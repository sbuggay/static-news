<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708074052207" as="style"/><link rel="stylesheet" href="styles.css?v=1708074052207"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/">Our next-generation model: Gemini 1.5</a>Â <span class="domain">(<a href="https://blog.google">blog.google</a>)</span></div><div class="subtext"><span>todsacerdoti</span> | <span>258 comments</span></div><br/><div><div id="39384513" class="c"><input type="checkbox" id="c-39384513" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#39384960">next</a><span>|</span><label class="collapse" for="c-39384513">[-]</label><label class="expand" for="c-39384513">[106 more]</label></div><br/><div class="children"><div class="content">The white paper is worth a read. The things that stand out to me are:<p>1. They don&#x27;t talk about how they get to 10M token context<p>2. They don&#x27;t talk about how they get to 10M token context<p>3. The 10M context ability wipes out most RAG stack complexity immediately. (I imagine creating caching abilities is going to be important for a lot of long token chatting features now, though). This is going to make things much, much simpler for a lot of use cases.<p>4. They are pretty clear that 1.5 Pro is better than GPT-4 in general, and therefore we have a new LLM-as-judge leader, which is pretty interesting.<p>5. It seems like 1.5 Ultra is going to be highly capable. 1.5 Pro is already very very capable. They are running up against very high scores on many tests, and took a minute to call out some tests where they scored badly as mostly returning false negatives.<p>Upshot, 1.5 Pro looks like it <i>should</i> set the bar for a bunch of workflow tasks, if we can ever get our hands on it. I&#x27;ve found 1.0 Ultra to be very capable, if a bit slow. Open models downstream should see a significant uptick in quality using it, which is great.<p>Time to dust out my coding test again, I think, which is: &quot;here is a tarball of a repository. Write a new module that does X&quot;.<p>I really want to know how they&#x27;re getting to 10M context, though. There are some intriguing clues in their results that this isn&#x27;t just a single ultra-long vector; for instance, their audio and video &quot;needle&quot; tests, which just include inserting an image that says &quot;the magic word is: xxx&quot;, or an audio clip that says the same thing, have perfect recall across up to 10M tokens. The text insertion occasionally fails. I&#x27;d speculate that this means there is some sort of compression going on; a full video frame with text on it is going to use a lot more tokens than the text needle.</div><br/><div id="39384823" class="c"><input type="checkbox" id="c-39384823" checked=""/><div class="controls bullet"><span class="by">swalsh</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39384813">next</a><span>|</span><label class="collapse" for="c-39384823">[-]</label><label class="expand" for="c-39384823">[28 more]</label></div><br/><div class="children"><div class="content">&quot;The 10M context ability wipes out most RAG stack complexity immediately.&quot;<p>I&#x27;m skeptical, my past experience is just becaues the context has room to stuff whatever you want in it, the more you stuff in the context the less accurate your results are. There seems to be this balance of providing enough that you&#x27;ll get high quality answers, but not too much that the model is overwhelmed.<p>I think a large part of developing better models is not just a better architectures that support larger and larger context sizes, but also capable models that can properly leverage that context.  That&#x27;s the test for me.</div><br/><div id="39385142" class="c"><input type="checkbox" id="c-39385142" checked=""/><div class="controls bullet"><span class="by">HereBePandas</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384823">parent</a><span>|</span><a href="#39394435">next</a><span>|</span><label class="collapse" for="c-39385142">[-]</label><label class="expand" for="c-39385142">[15 more]</label></div><br/><div class="children"><div class="content">They explicitly address this in page 11 of the report. Basically perfect recall for up to 1M tokens; way better than GPT-4.</div><br/><div id="39385510" class="c"><input type="checkbox" id="c-39385510" checked=""/><div class="controls bullet"><span class="by">westoncb</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385142">parent</a><span>|</span><a href="#39385648">next</a><span>|</span><label class="collapse" for="c-39385510">[-]</label><label class="expand" for="c-39385510">[13 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think recall really addresses it sufficiently: the main issue I see is answers getting &quot;muddy&quot;. Like it&#x27;s getting pulled in too many directions and averaging.</div><br/><div id="39385684" class="c"><input type="checkbox" id="c-39385684" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385510">parent</a><span>|</span><a href="#39394481">next</a><span>|</span><label class="collapse" for="c-39385684">[-]</label><label class="expand" for="c-39385684">[11 more]</label></div><br/><div class="children"><div class="content">I&#x27;d urge caution in extending generalizations about &quot;muddiness&quot; to a new context architecture. Let&#x27;s use the thing first.</div><br/><div id="39386928" class="c"><input type="checkbox" id="c-39386928" checked=""/><div class="controls bullet"><span class="by">westoncb</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385684">parent</a><span>|</span><a href="#39393542">next</a><span>|</span><label class="collapse" for="c-39386928">[-]</label><label class="expand" for="c-39386928">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not saying it applies to the new architecture, I&#x27;m saying that&#x27;s a big issue I&#x27;ve observed in existing models and that so far we have no info on whether it&#x27;s solved in the new one (i.e. accurate recall doesn&#x27;t imply much in that regard).</div><br/><div id="39388752" class="c"><input type="checkbox" id="c-39388752" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39386928">parent</a><span>|</span><a href="#39386939">next</a><span>|</span><label class="collapse" for="c-39388752">[-]</label><label class="expand" for="c-39388752">[3 more]</label></div><br/><div class="children"><div class="content">Ah, apologies for the misunderstanding. What tests would you suggest to evaluate &quot;muddiness&quot;?<p>What comes to my mind: run the usual gamut of tests, but with the excess context window saturated with irrelevant(?) data. Measure test answer accuracy&#x2F;verbosity as a function of context saturation percentage. If there&#x27;s little correlation between these two variables (e.g. 9% saturation is just as accurate&#x2F;succinct as 99% saturation), then &quot;muddiness&quot; isn&#x27;t an issue.</div><br/><div id="39389775" class="c"><input type="checkbox" id="c-39389775" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39388752">parent</a><span>|</span><a href="#39393449">next</a><span>|</span><label class="collapse" for="c-39389775">[-]</label><label class="expand" for="c-39389775">[1 more]</label></div><br/><div class="children"><div class="content">Manual testing on complex documents. A big legal contract for example. An issue can be referred to in 7 different places in a 100 page document. Does it give a coherent answer?<p>A handful of examples show whether it can do it. For example, GPT-4 turbo is downright awful at something like that.</div><br/></div></div><div id="39393449" class="c"><input type="checkbox" id="c-39393449" checked=""/><div class="controls bullet"><span class="by">somenameforme</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39388752">parent</a><span>|</span><a href="#39389775">prev</a><span>|</span><a href="#39386939">next</a><span>|</span><label class="collapse" for="c-39393449">[-]</label><label class="expand" for="c-39393449">[1 more]</label></div><br/><div class="children"><div class="content">You need to use relevant data. The question isn&#x27;t random sorting&#x2F;pruning, but being able to apply large numbers of related hints&#x2F;references&#x2F;definitions in a meaningful way. To me this would be the entire point of a large context window. For entirely different topics you can always just start a new instance.</div><br/></div></div></div></div><div id="39386939" class="c"><input type="checkbox" id="c-39386939" checked=""/><div class="controls bullet"><span class="by">westoncb</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39386928">parent</a><span>|</span><a href="#39388752">prev</a><span>|</span><a href="#39393542">next</a><span>|</span><label class="collapse" for="c-39386939">[-]</label><label class="expand" for="c-39386939">[1 more]</label></div><br/><div class="children"><div class="content">Would be awesome if it is solved but seems like a much deeper problem tbh.</div><br/></div></div></div></div><div id="39393542" class="c"><input type="checkbox" id="c-39393542" checked=""/><div class="controls bullet"><span class="by">mlsu</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385684">parent</a><span>|</span><a href="#39386928">prev</a><span>|</span><a href="#39388936">next</a><span>|</span><label class="collapse" for="c-39393542">[-]</label><label class="expand" for="c-39393542">[1 more]</label></div><br/><div class="children"><div class="content">I am skeptical of benchmarks in general, to be honest. It seems to be extremely difficult to come up with benchmarks for these things (it may be true of intelligence as a quality...). It&#x27;s almost an anti-signal to proclaim good results on benchmarks. The best barometer of model quality has been vibes, in places like &#x2F;r&#x2F;localllama where cracked posters are actively testing the newest models out.<p>Based on Google&#x27;s track record in the area of text chatbots, I am extremely skeptical of their claims about coherency across a 1M+ context window.<p>Of course none of this even matters anyway because the weights are closed the architecture is closed nobody has access to the model. I&#x27;ll believe it when I see it.</div><br/></div></div><div id="39388936" class="c"><input type="checkbox" id="c-39388936" checked=""/><div class="controls bullet"><span class="by">smeagull</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385684">parent</a><span>|</span><a href="#39393542">prev</a><span>|</span><a href="#39390939">next</a><span>|</span><label class="collapse" for="c-39388936">[-]</label><label class="expand" for="c-39388936">[2 more]</label></div><br/><div class="children"><div class="content">I believe that&#x27;s a limitation of using vectors of high dimensions. It&#x27;ll be muddy.</div><br/><div id="39393141" class="c"><input type="checkbox" id="c-39393141" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39388936">parent</a><span>|</span><a href="#39390939">next</a><span>|</span><label class="collapse" for="c-39393141">[-]</label><label class="expand" for="c-39393141">[1 more]</label></div><br/><div class="children"><div class="content">Not unlike trying to keep the whole contents of the document in your own mind :)</div><br/></div></div></div></div><div id="39390939" class="c"><input type="checkbox" id="c-39390939" checked=""/><div class="controls bullet"><span class="by">caesil</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385684">parent</a><span>|</span><a href="#39388936">prev</a><span>|</span><a href="#39394481">next</a><span>|</span><label class="collapse" for="c-39390939">[-]</label><label class="expand" for="c-39390939">[2 more]</label></div><br/><div class="children"><div class="content">Unfortunately Google&#x27;s track record with language models is one of overpromising and underdelivering.</div><br/><div id="39391658" class="c"><input type="checkbox" id="c-39391658" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39390939">parent</a><span>|</span><a href="#39394481">next</a><span>|</span><label class="collapse" for="c-39391658">[-]</label><label class="expand" for="c-39391658">[1 more]</label></div><br/><div class="children"><div class="content">This is only specifically for web interface LLMs in the past few years that it&#x27;s been lack luster.  However, this statements is not correct for their overall history.  W2V based lang models and BERT&#x2F;Transformer models in the early days (*publicly available, but not in web interface) were <i>far</i> ahead of the curve, as they were the ones that produced these innovations.
Effectively, Deepmmind&#x2F;Google are academics (where the <i>real</i> innovations are made, but they do struggle to produce corporate products (where openai shines).</div><br/></div></div></div></div></div></div><div id="39394481" class="c"><input type="checkbox" id="c-39394481" checked=""/><div class="controls bullet"><span class="by">andy_ppp</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385510">parent</a><span>|</span><a href="#39385684">prev</a><span>|</span><a href="#39385648">next</a><span>|</span><label class="collapse" for="c-39394481">[-]</label><label class="expand" for="c-39394481">[1 more]</label></div><br/><div class="children"><div class="content">Did you think the extraction of information from a the Buster Keaton film was muddy? I thought it was incredibly impressive to be this precise.</div><br/></div></div></div></div></div></div><div id="39394435" class="c"><input type="checkbox" id="c-39394435" checked=""/><div class="controls bullet"><span class="by">koliber</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384823">parent</a><span>|</span><a href="#39385142">prev</a><span>|</span><a href="#39385944">next</a><span>|</span><label class="collapse" for="c-39394435">[-]</label><label class="expand" for="c-39394435">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are able to utilize âall the worldsâ knowledge during training and give seemingly magical answers. While providing context in the query is different than training models, is it possible that more context will give more materials to the LLM and it will be able to pick out the relevant bits on its own?<p>What if it was possible, with each query, to fine tune the model on the provided context, and then use that JIT fine-tuned model to answer the query?</div><br/></div></div><div id="39385944" class="c"><input type="checkbox" id="c-39385944" checked=""/><div class="controls bullet"><span class="by">chuckcode</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384823">parent</a><span>|</span><a href="#39394435">prev</a><span>|</span><a href="#39387347">next</a><span>|</span><label class="collapse" for="c-39385944">[-]</label><label class="expand" for="c-39385944">[1 more]</label></div><br/><div class="children"><div class="content">Would like to see the latency and cost of parsing entire 10M context before throwing out the RAG stack which is relatively cheap and fast.</div><br/></div></div><div id="39387347" class="c"><input type="checkbox" id="c-39387347" checked=""/><div class="controls bullet"><span class="by">theolivenbaum</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384823">parent</a><span>|</span><a href="#39385944">prev</a><span>|</span><a href="#39391630">next</a><span>|</span><label class="collapse" for="c-39387347">[-]</label><label class="expand" for="c-39387347">[1 more]</label></div><br/><div class="children"><div class="content">Also unless they significantly change their pricing model, we&#x27;re talking about 0.5$ per API call at current prices</div><br/></div></div><div id="39391630" class="c"><input type="checkbox" id="c-39391630" checked=""/><div class="controls bullet"><span class="by">patja</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384823">parent</a><span>|</span><a href="#39387347">prev</a><span>|</span><a href="#39388229">next</a><span>|</span><label class="collapse" for="c-39391630">[-]</label><label class="expand" for="c-39391630">[2 more]</label></div><br/><div class="children"><div class="content">I think there are also a lot of people who are only interested in RAG if they can self-host and keep their documents private.</div><br/><div id="39393073" class="c"><input type="checkbox" id="c-39393073" checked=""/><div class="controls bullet"><span class="by">jimmySixDOF</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39391630">parent</a><span>|</span><a href="#39388229">next</a><span>|</span><label class="collapse" for="c-39393073">[-]</label><label class="expand" for="c-39393073">[1 more]</label></div><br/><div class="children"><div class="content">Yes and the ability to have direct attribution matters so you know exactly where your responses come from.   And costs as others point out, but RAG is not gone in fact it just got easier and a lot more powerful.</div><br/></div></div></div></div><div id="39388229" class="c"><input type="checkbox" id="c-39388229" checked=""/><div class="controls bullet"><span class="by">aik</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384823">parent</a><span>|</span><a href="#39391630">prev</a><span>|</span><a href="#39386530">next</a><span>|</span><label class="collapse" for="c-39388229">[-]</label><label class="expand" for="c-39388229">[1 more]</label></div><br/><div class="children"><div class="content">Have to consider cost for all of this.  Big value of RAG already even given the size of GPT-4âa largest context size is it decreases cost very significantly.</div><br/></div></div><div id="39386530" class="c"><input type="checkbox" id="c-39386530" checked=""/><div class="controls bullet"><span class="by">tkellogg</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384823">parent</a><span>|</span><a href="#39388229">prev</a><span>|</span><a href="#39385874">next</a><span>|</span><label class="collapse" for="c-39386530">[-]</label><label class="expand" for="c-39386530">[2 more]</label></div><br/><div class="children"><div class="content">costs rise on a per-token basis. So you <i>CAN</i> use 10M tokens, but it&#x27;s probably not usually a good idea. A database lookup is still better than a few billion math operations.</div><br/><div id="39387864" class="c"><input type="checkbox" id="c-39387864" checked=""/><div class="controls bullet"><span class="by">sjwhevvvvvsj</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39386530">parent</a><span>|</span><a href="#39385874">next</a><span>|</span><label class="collapse" for="c-39387864">[-]</label><label class="expand" for="c-39387864">[1 more]</label></div><br/><div class="children"><div class="content">I think the unspoken goal is to just lay off your employees and dump every doc and email theyâve ever written as one big context.<p>Now that Google has tasted the previously forbidden fruit of layoffs themselves, I think their primary goal in ML is now headcount reduction.</div><br/></div></div></div></div><div id="39385874" class="c"><input type="checkbox" id="c-39385874" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384823">parent</a><span>|</span><a href="#39386530">prev</a><span>|</span><a href="#39384813">next</a><span>|</span><label class="collapse" for="c-39385874">[-]</label><label class="expand" for="c-39385874">[4 more]</label></div><br/><div class="children"><div class="content">also costs are always based on context token, you dont want to put in 10m of context for every request (its just nice to have that option when you want to do big things that dont scale)</div><br/><div id="39390385" class="c"><input type="checkbox" id="c-39390385" checked=""/><div class="controls bullet"><span class="by">1024core</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385874">parent</a><span>|</span><a href="#39384813">next</a><span>|</span><label class="collapse" for="c-39390385">[-]</label><label class="expand" for="c-39390385">[3 more]</label></div><br/><div class="children"><div class="content">How much would a lawyer charge to review your 10M-token legal document?</div><br/><div id="39391168" class="c"><input type="checkbox" id="c-39391168" checked=""/><div class="controls bullet"><span class="by">hereonout2</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39390385">parent</a><span>|</span><a href="#39384813">next</a><span>|</span><label class="collapse" for="c-39391168">[-]</label><label class="expand" for="c-39391168">[2 more]</label></div><br/><div class="children"><div class="content">10M tokens is  something like 14 copies of war and peace, or maybe the entire harry potter series seven times over. That&#x27;d be some legal document!</div><br/><div id="39393191" class="c"><input type="checkbox" id="c-39393191" checked=""/><div class="controls bullet"><span class="by">xp84</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39391168">parent</a><span>|</span><a href="#39384813">next</a><span>|</span><label class="collapse" for="c-39393191">[-]</label><label class="expand" for="c-39393191">[1 more]</label></div><br/><div class="children"><div class="content">Hmm I donât know but I feel like the U.S. Congress has bills that would push that limit.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39384813" class="c"><input type="checkbox" id="c-39384813" checked=""/><div class="controls bullet"><span class="by">usaar333</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39384823">prev</a><span>|</span><a href="#39384581">next</a><span>|</span><label class="collapse" for="c-39384813">[-]</label><label class="expand" for="c-39384813">[18 more]</label></div><br/><div class="children"><div class="content">&gt;  They are pretty clear that 1.5 Pro is better than GPT-4 in general, and therefore we have a new LLM-as-judge leader, which is pretty interesting.<p>They try to push that, but it&#x27;s not the most convincing.  Look at Table 8 for text evaluations (math, etc.) - they don&#x27;t even attempt a comparison with GPT-4.<p>GPT-4 is higher than any Gemini model on both MMLU and GSM8K.  Gemini Pro seems slightly better than GPT-4 original in Human Eval (67-&gt;71).  Gemini Pro does crush naive GPT-4 on math (though not with code interpreter and this is the original model).<p>All in 1.5 Pro seems maybe a bit better than 1.0 Ultra.  Given that in the wild people seem to find GPT-4 better for say coding than Gemini Ultra, my current update is Pro 1.5 is about equal to GPT-4.<p>But we&#x27;ll see once released.</div><br/><div id="39385702" class="c"><input type="checkbox" id="c-39385702" checked=""/><div class="controls bullet"><span class="by">panarky</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384813">parent</a><span>|</span><a href="#39386707">next</a><span>|</span><label class="collapse" for="c-39385702">[-]</label><label class="expand" for="c-39385702">[15 more]</label></div><br/><div class="children"><div class="content"><i>&gt; people seem to find GPT-4 better for say coding than Gemini Ultra</i><p>For my use cases, Gemini Ultra performs significantly better than GPT-4.<p>My prompts are long and complex, with a paragraph or two about the general objective followed by 15 to 20 numbered requirements. Often I&#x27;ll include existing functions the new code needs to work with, or functions that must be refactored to handle the new requirements.<p>I took 20 prompts that I&#x27;d run with GPT-4 and fed them to Gemini Ultra. Gemini gave a clearly better result in 16 out of 20 cases.<p>Where GPT-4 might miss one or two requirements, Gemini usually got them all. Where GPT-4 might require multiple chat turns to point out its errors and omissions and tell it to fix them, Gemini often returned the result I wanted in one shot. Where GPT-4 hallucinated a method that doesn&#x27;t exist, or had been deprecated years ago, Gemini used correct methods. Where GPT-4 called methods of third-party packages it assumed were installed, Gemini either used native code or explicitly called out the dependency.<p>For the 4 out of 20 prompts where Gemini did worse, one was a weird rejection where I&#x27;d included an image in the prompt and Gemini refused to work with it because it had unrecognizable human forms in the distance. Another was a simple bash script to split a text file, and it came up with a technically correct but complex one-liner, while GPT-4 just used split with simple options to get the same result.<p>For now I subscribe to both. But I&#x27;m using Gemini for almost all coding work, only checking in with GPT-4 when Gemini stumbles, which isn&#x27;t often. If I continue to get solid results I&#x27;ll drop the GPT-4 subscription.</div><br/><div id="39385868" class="c"><input type="checkbox" id="c-39385868" checked=""/><div class="controls bullet"><span class="by">sho_hn</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385702">parent</a><span>|</span><a href="#39391957">next</a><span>|</span><label class="collapse" for="c-39385868">[-]</label><label class="expand" for="c-39385868">[9 more]</label></div><br/><div class="children"><div class="content">I have a very similar prompting style to yours and share this experience.<p>I am an experienced programmer and usually have a fairly exact idea of what I want, so I write detailed requirements and use the models more as typing accelerators.<p>GPT-4 is useful in this regard, but I also tried about a dozen older prompts on Gemini Advanced&#x2F;Ultra recently and in every case preferred the Ultra output. The code was usually more complete and prod-ready, with higher sophistication in its construction and somewhat higher density. It was just closer to what I would have hand-written.<p>It&#x27;s increasingly clear though LLM use has a couple of different major modes among end-user behavior. Knowledge base vs. reasoning, exploratory vs. completion, instruction following vs. getting suggestions, etc.<p>For programming I want an obedient instruction-following completer with great reasoning. Gemini Ultra seems to do this better than GPT-4 for me.</div><br/><div id="39387921" class="c"><input type="checkbox" id="c-39387921" checked=""/><div class="controls bullet"><span class="by">sjwhevvvvvsj</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385868">parent</a><span>|</span><a href="#39390993">next</a><span>|</span><label class="collapse" for="c-39387921">[-]</label><label class="expand" for="c-39387921">[6 more]</label></div><br/><div class="children"><div class="content">Iâm going to have to try Gemini for code again. It just occurred to me as a Xoogler that if they used Googleâs code base as the training data itâs going to be unbeatable. Now did they do that?  No idea, but quality wins over quantity, even with LLM.</div><br/><div id="39389436" class="c"><input type="checkbox" id="c-39389436" checked=""/><div class="controls bullet"><span class="by">barrkel</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39387921">parent</a><span>|</span><a href="#39390253">next</a><span>|</span><label class="collapse" for="c-39389436">[-]</label><label class="expand" for="c-39389436">[4 more]</label></div><br/><div class="children"><div class="content">There is no way NTK data is in the training set, and google3 is NTK.</div><br/><div id="39391942" class="c"><input type="checkbox" id="c-39391942" checked=""/><div class="controls bullet"><span class="by">sjwhevvvvvsj</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39389436">parent</a><span>|</span><a href="#39391873">next</a><span>|</span><label class="collapse" for="c-39391942">[-]</label><label class="expand" for="c-39391942">[1 more]</label></div><br/><div class="children"><div class="content">I dunno, leadership is desperate and they can de-NTK if and when they feel like it.</div><br/></div></div><div id="39391873" class="c"><input type="checkbox" id="c-39391873" checked=""/><div class="controls bullet"><span class="by">cpeterso</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39389436">parent</a><span>|</span><a href="#39391942">prev</a><span>|</span><a href="#39390253">next</a><span>|</span><label class="collapse" for="c-39391873">[-]</label><label class="expand" for="c-39391873">[2 more]</label></div><br/><div class="children"><div class="content">What is âNTKâ?</div><br/><div id="39392266" class="c"><input type="checkbox" id="c-39392266" checked=""/><div class="controls bullet"><span class="by">mjamaloney</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39391873">parent</a><span>|</span><a href="#39390253">next</a><span>|</span><label class="collapse" for="c-39392266">[-]</label><label class="expand" for="c-39392266">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Need To Know&quot;  I.e. data that isn&#x27;t open within the company.</div><br/></div></div></div></div></div></div></div></div><div id="39390993" class="c"><input type="checkbox" id="c-39390993" checked=""/><div class="controls bullet"><span class="by">lyu07282</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385868">parent</a><span>|</span><a href="#39387921">prev</a><span>|</span><a href="#39391957">next</a><span>|</span><label class="collapse" for="c-39390993">[-]</label><label class="expand" for="c-39390993">[2 more]</label></div><br/><div class="children"><div class="content">It constantly hallucinates APIs for me, I really wonder why people&#x27;s perceptions are so radically different. For me it&#x27;s basically unusable for coding. Perhaps I&#x27;m getting a cheaper model because I live in a poorer country.</div><br/><div id="39391408" class="c"><input type="checkbox" id="c-39391408" checked=""/><div class="controls bullet"><span class="by">sho_hn</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39390993">parent</a><span>|</span><a href="#39391957">next</a><span>|</span><label class="collapse" for="c-39391408">[-]</label><label class="expand" for="c-39391408">[1 more]</label></div><br/><div class="children"><div class="content">Are you using Gemini Advanced? (The paid tier.) The free one is indeed very bad.</div><br/></div></div></div></div></div></div><div id="39391957" class="c"><input type="checkbox" id="c-39391957" checked=""/><div class="controls bullet"><span class="by">koreth1</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385702">parent</a><span>|</span><a href="#39385868">prev</a><span>|</span><a href="#39385918">next</a><span>|</span><label class="collapse" for="c-39391957">[-]</label><label class="expand" for="c-39391957">[2 more]</label></div><br/><div class="children"><div class="content">&gt; My prompts are long and complex, with a paragraph or two about the general objective followed by 15 to 20 numbered requirements. Often I&#x27;ll include existing functions the new code needs to work with, or functions that must be refactored to handle the new requirements.<p>I guess this is a tough request if you&#x27;re working on a proprietary code base, but I would <i>love</i> to see some concrete examples of the prompts and the code they produce.<p>I keep trying this kind of prompting with various LLM tools including GPT-4 (haven&#x27;t tried Gemini Ultra yet, I admit) and it nearly always takes me longer to explain the detailed requirements and clean up the generated code than it would have taken me to write the code directly.<p>But plenty of people seem to have an experience more like yours, so I really wonder whether (a) we&#x27;re just asking it to write very different kinds of code, or (b) I&#x27;m bad at writing LLM-friendly requirements.</div><br/><div id="39393396" class="c"><input type="checkbox" id="c-39393396" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39391957">parent</a><span>|</span><a href="#39385918">next</a><span>|</span><label class="collapse" for="c-39393396">[-]</label><label class="expand" for="c-39393396">[1 more]</label></div><br/><div class="children"><div class="content">Not OP but here is a verbatim prompt I put into these LLMs. I&#x27;m learning to make flutter apps, and I like to try make various UIs so I can learn how to compose some things. I agree that Gemini Ultra (aka the paid &quot;advanced&quot; mode) is def better than ChatGPT-4 for this prompt. Mine is a bit more terse than OP&#x27;s huge prompt with numbered requirements, but I still got a super valid and meaningful response from Gemini, while GPT4 told me it was a tricky problem, and gave me some generic code snippets, that explicitly don&#x27;t solve the problem asked.<p>&gt; I&#x27;m building a note-taking app in flutter. I want to create a way to link between notes (like a web hyperlink) that opens a different note when a user clicks on it. They should be able to click on the link while editing the note, without having to switch modalities (eg. no edit-save-view flow nor a preview page). How can I accomplish this?<p>I also included a follow-up prompt after getting the first answer, which again for Gemini was super meaningful, and already included valid code to start with. Gemini also showed me many more projects and examples from the broader internet.<p>&gt; Can you write a complete Widget that can implement this functionality? Please hard-code the note text below: &lt;redacted from HN since its long&gt;</div><br/></div></div></div></div><div id="39385918" class="c"><input type="checkbox" id="c-39385918" checked=""/><div class="controls bullet"><span class="by">Dayshine</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385702">parent</a><span>|</span><a href="#39391957">prev</a><span>|</span><a href="#39391609">next</a><span>|</span><label class="collapse" for="c-39385918">[-]</label><label class="expand" for="c-39385918">[1 more]</label></div><br/><div class="children"><div class="content">Is there any chance you could share an example of the kind of prompt you&#x27;re writing?<p>I&#x27;m always reluctant to write long prompts because I often find GPT4 just doesn&#x27;t get it, and then I&#x27;ve wasted ten minutes writing a prompt</div><br/></div></div><div id="39391609" class="c"><input type="checkbox" id="c-39391609" checked=""/><div class="controls bullet"><span class="by">qingcharles</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385702">parent</a><span>|</span><a href="#39385918">prev</a><span>|</span><a href="#39391783">next</a><span>|</span><label class="collapse" for="c-39391609">[-]</label><label class="expand" for="c-39391609">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve found Gemini generally equal with the .Net and HTML coding I&#x27;ve been doing.<p>I&#x27;ve never had Gemini give me a better result than GPT, though, so it does not surpass it for my needs.<p>The UI is more responsive, though, which is worth something.</div><br/></div></div><div id="39391783" class="c"><input type="checkbox" id="c-39391783" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385702">parent</a><span>|</span><a href="#39391609">prev</a><span>|</span><a href="#39386707">next</a><span>|</span><label class="collapse" for="c-39391783">[-]</label><label class="expand" for="c-39391783">[1 more]</label></div><br/><div class="children"><div class="content">How do you interact with Gemini for coding work? I am trying to paste my code in the web interface and when I hit submit, the interface says &quot;something went wrong&quot; and the code does not appear in the chat window. I signed up for Gemini Advanced and that didn&#x27;t help. Do you use AI Studio? I am just looking in to that now.</div><br/></div></div></div></div><div id="39386707" class="c"><input type="checkbox" id="c-39386707" checked=""/><div class="controls bullet"><span class="by">spott</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384813">parent</a><span>|</span><a href="#39385702">prev</a><span>|</span><a href="#39385518">next</a><span>|</span><label class="collapse" for="c-39386707">[-]</label><label class="expand" for="c-39386707">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Gemini Pro seems slightly better than GPT-4 original in Human Eval (67-&gt;71).<p>Though they talk a bunch about how hard it was to filter out Human Eval, so this probably doesn&#x27;t matter much.</div><br/></div></div><div id="39385518" class="c"><input type="checkbox" id="c-39385518" checked=""/><div class="controls bullet"><span class="by">cchance</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384813">parent</a><span>|</span><a href="#39386707">prev</a><span>|</span><a href="#39384581">next</a><span>|</span><label class="collapse" for="c-39385518">[-]</label><label class="expand" for="c-39385518">[1 more]</label></div><br/><div class="children"><div class="content">I mean i don&#x27;t see GPT4 watching a 44 minute movie and being able to exactly pinpoint a guy taking a paper out of his pocket..</div><br/></div></div></div></div><div id="39384581" class="c"><input type="checkbox" id="c-39384581" checked=""/><div class="controls bullet"><span class="by">CharlieDigital</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39384813">prev</a><span>|</span><a href="#39385106">next</a><span>|</span><label class="collapse" for="c-39384581">[-]</label><label class="expand" for="c-39384581">[14 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>    &gt; The 10M context ability wipes out most RAG stack complexity immediately.
</code></pre>
Remains to be seen.<p>Large contexts are not always better.  For starters, it takes longer to process.  But secondly, even with RAG and the large context of GPT4 Turbo, providing it a more relevant and accurate context always yields better output.<p>What you get with RAG is faster response times and more accurate answers by pre-filtering out the noise.</div><br/><div id="39385090" class="c"><input type="checkbox" id="c-39385090" checked=""/><div class="controls bullet"><span class="by">killerstorm</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384581">parent</a><span>|</span><a href="#39384741">next</a><span>|</span><label class="collapse" for="c-39385090">[-]</label><label class="expand" for="c-39385090">[4 more]</label></div><br/><div class="children"><div class="content">Hopefully we can get a better RAG out of it. Currently people do incredibly primitive stuff like chunking text into chunks of a fixed size and adding them to vector DB.<p>An actually useful RAG would be to convert text to Q&amp;A and use Q&#x27;s embeddings as an index. Large context can make use of in-context learning to make better Q&amp;A.</div><br/><div id="39386016" class="c"><input type="checkbox" id="c-39386016" checked=""/><div class="controls bullet"><span class="by">mediaman</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385090">parent</a><span>|</span><a href="#39384741">next</a><span>|</span><label class="collapse" for="c-39386016">[-]</label><label class="expand" for="c-39386016">[3 more]</label></div><br/><div class="children"><div class="content">A lot of people in RAG already do this. I do this with my product: we process each page and create lists of potential questions that the page would answer, and then embed that.<p>We also embed the actual text, though, because I found that only doing the questions resulted in inferior performance.</div><br/><div id="39386200" class="c"><input type="checkbox" id="c-39386200" checked=""/><div class="controls bullet"><span class="by">CharlieDigital</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39386016">parent</a><span>|</span><a href="#39384741">next</a><span>|</span><label class="collapse" for="c-39386200">[-]</label><label class="expand" for="c-39386200">[2 more]</label></div><br/><div class="children"><div class="content">So in this case, what your workflow might look like is:<p><pre><code>    1. Get text from page&#x2F;section&#x2F;chunk
    2. Generate possible questions related to the page&#x2F;section&#x2F;chunk
    3. Generate an embedding using { each possible question + page&#x2F;section&#x2F;chunk }
    4. Incoming question targets the embedding and matches against { question + source }
</code></pre>
Is this roughly it?  How many questions do you generate?   Do you save a separate embedding for each question?  Or just stuff all of the questions back with the page&#x2F;section&#x2F;chunk?</div><br/><div id="39394489" class="c"><input type="checkbox" id="c-39394489" checked=""/><div class="controls bullet"><span class="by">mediaman</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39386200">parent</a><span>|</span><a href="#39384741">next</a><span>|</span><label class="collapse" for="c-39394489">[-]</label><label class="expand" for="c-39394489">[1 more]</label></div><br/><div class="children"><div class="content">Right now I just throw the different questions together in a single embedding for a given chunk, with the idea that thereâs enough dimensionality to capture them all. But I havenât tested embedding each question, matching on that vector, and then returning the corresponding chunk. That seems like itâd be worth testing out.</div><br/></div></div></div></div></div></div></div></div><div id="39384741" class="c"><input type="checkbox" id="c-39384741" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384581">parent</a><span>|</span><a href="#39385090">prev</a><span>|</span><a href="#39385106">next</a><span>|</span><label class="collapse" for="c-39384741">[-]</label><label class="expand" for="c-39384741">[9 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t forget that Gemini also has access to the internet, so a lot of RAGging becomes pointless anyway.</div><br/><div id="39384888" class="c"><input type="checkbox" id="c-39384888" checked=""/><div class="controls bullet"><span class="by">beppo</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384741">parent</a><span>|</span><a href="#39385077">next</a><span>|</span><label class="collapse" for="c-39384888">[-]</label><label class="expand" for="c-39384888">[6 more]</label></div><br/><div class="children"><div class="content">Internet search <i>is</i> a form of RAG, though. 10M tokens is very impressive, but you&#x27;re not fitting a database, let alone the entire internet into a prompt anytime soon.</div><br/><div id="39385113" class="c"><input type="checkbox" id="c-39385113" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384888">parent</a><span>|</span><a href="#39385077">next</a><span>|</span><label class="collapse" for="c-39385113">[-]</label><label class="expand" for="c-39385113">[5 more]</label></div><br/><div class="children"><div class="content">You shouldn&#x27;t fit an entire database in the context anyway.<p>btw, 10M tokens is 78 times more context window than the newest GPT-4-turbo (128K). In a way, you don&#x27;t need 78 GPT-4 API calls, only one batch call to Gemini 1.5.</div><br/><div id="39385564" class="c"><input type="checkbox" id="c-39385564" checked=""/><div class="controls bullet"><span class="by">cchance</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385113">parent</a><span>|</span><a href="#39385492">next</a><span>|</span><label class="collapse" for="c-39385564">[-]</label><label class="expand" for="c-39385564">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t get this why is it people think that you need to put an entire database in the short-term memory of the AI to be useful? When you work with a DB are you memorizing the entire f*cking database, no, you know the summaries of it and how to access and use it.<p>People also seem to forget that the average is 1b words that are read by people in their entire LIFETIME, and at 10m, with nearly 100% recall thats pretty damn amazing, i&#x27;m pretty sure I don&#x27;t have perfect recall of 10m words myself lol</div><br/><div id="39391064" class="c"><input type="checkbox" id="c-39391064" checked=""/><div class="controls bullet"><span class="by">choilive</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385564">parent</a><span>|</span><a href="#39389879">next</a><span>|</span><label class="collapse" for="c-39391064">[-]</label><label class="expand" for="c-39391064">[1 more]</label></div><br/><div class="children"><div class="content">You certainly don&#x27;t need that much context for it to be useful, but it definitely opens up a LOT more possibilities without the compromises of implementing some type of RAG. 
In addition, don&#x27;t we want our AI to have superhuman capabilities? The ability to work on 10M+ tokens of context at a time could enable superhuman performance in many tasks. Why stop at 10M tokens? Imagine if AI could work on 1B tokens of context like you said?</div><br/></div></div><div id="39389879" class="c"><input type="checkbox" id="c-39389879" checked=""/><div class="controls bullet"><span class="by">Qwero</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385564">parent</a><span>|</span><a href="#39391064">prev</a><span>|</span><a href="#39385492">next</a><span>|</span><label class="collapse" for="c-39389879">[-]</label><label class="expand" for="c-39389879">[1 more]</label></div><br/><div class="children"><div class="content">It increases the use cases.<p>It can also be a good alternative for fine-tuning.<p>And the use case of a code base is a good example: if the ai understands the whole context, it can do basically everything.<p>Let me pay 5â¬ for a android app rewritten into iOS.</div><br/></div></div></div></div><div id="39385492" class="c"><input type="checkbox" id="c-39385492" checked=""/><div class="controls bullet"><span class="by">rvnx</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385113">parent</a><span>|</span><a href="#39385564">prev</a><span>|</span><a href="#39385077">next</a><span>|</span><label class="collapse" for="c-39385492">[-]</label><label class="expand" for="c-39385492">[1 more]</label></div><br/><div class="children"><div class="content">Well it&#x27;s nice, just sad nobody can use it</div><br/></div></div></div></div></div></div><div id="39385077" class="c"><input type="checkbox" id="c-39385077" checked=""/><div class="controls bullet"><span class="by">CharlieDigital</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384741">parent</a><span>|</span><a href="#39384888">prev</a><span>|</span><a href="#39385106">next</a><span>|</span><label class="collapse" for="c-39385077">[-]</label><label class="expand" for="c-39385077">[2 more]</label></div><br/><div class="children"><div class="content">This may be useful in a generalized use case, but a problem is that many of those results again will add noise.<p>For any use case where you want contextual results, you need to be able to either filter the search scope or use RAG to pre-define the acceptable corpus.</div><br/><div id="39388368" class="c"><input type="checkbox" id="c-39388368" checked=""/><div class="controls bullet"><span class="by">panarky</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385077">parent</a><span>|</span><a href="#39385106">next</a><span>|</span><label class="collapse" for="c-39388368">[-]</label><label class="expand" for="c-39388368">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; you need to be able to either filter the search scope or use RAG ...</i><p>Unless you can get nearly perfect recall with millions of tokens, which is the claim made here.</div><br/></div></div></div></div></div></div></div></div><div id="39385106" class="c"><input type="checkbox" id="c-39385106" checked=""/><div class="controls bullet"><span class="by">tveita</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39384581">prev</a><span>|</span><a href="#39386656">next</a><span>|</span><label class="collapse" for="c-39385106">[-]</label><label class="expand" for="c-39385106">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The 10M context ability wipes out most RAG stack complexity immediately.<p>The video queries they show take around 1 minute each, this probably burns a ton of GPU. I appreciate how clearly they highlight that the video is sped up though, they&#x27;re clearly trying to avoid repeating the &quot;fake demo&quot; fiasco from the original Gemini videos.</div><br/></div></div><div id="39386656" class="c"><input type="checkbox" id="c-39386656" checked=""/><div class="controls bullet"><span class="by">TweedBeetle</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39385106">prev</a><span>|</span><a href="#39385505">next</a><span>|</span><label class="collapse" for="c-39386656">[-]</label><label class="expand" for="c-39386656">[2 more]</label></div><br/><div class="children"><div class="content">Regarding how theyâre getting to 10M context, I think itâs possible they are using the new SAMBA architecture.<p>Hereâs the paper:
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.00752" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.00752</a><p>And hereâs a great podcast episode on it:
<a href="https:&#x2F;&#x2F;www.cognitiverevolution.ai&#x2F;emergency-pod-mamba-memory-and-the-ssm-moment&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.cognitiverevolution.ai&#x2F;emergency-pod-mamba-memor...</a></div><br/><div id="39386778" class="c"><input type="checkbox" id="c-39386778" checked=""/><div class="controls bullet"><span class="by">LightMachine</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39386656">parent</a><span>|</span><a href="#39385505">next</a><span>|</span><label class="collapse" for="c-39386778">[-]</label><label class="expand" for="c-39386778">[1 more]</label></div><br/><div class="children"><div class="content">As a Brazilian, I approve that choice. Vambora amigos!</div><br/></div></div></div></div><div id="39385505" class="c"><input type="checkbox" id="c-39385505" checked=""/><div class="controls bullet"><span class="by">cchance</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39386656">prev</a><span>|</span><a href="#39390590">next</a><span>|</span><label class="collapse" for="c-39385505">[-]</label><label class="expand" for="c-39385505">[3 more]</label></div><br/><div class="children"><div class="content">The youtube video of the Multimodal analysis of a video is insane, imagine feeding in movies or tv shows and being able to autosummary or find information about them dynamically, how the hell is all this possible already? AI is moving insanely fast.</div><br/><div id="39393465" class="c"><input type="checkbox" id="c-39393465" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385505">parent</a><span>|</span><a href="#39390590">next</a><span>|</span><label class="collapse" for="c-39393465">[-]</label><label class="expand" for="c-39393465">[2 more]</label></div><br/><div class="children"><div class="content">&gt; imagine feeding in movies or tv shows<p>Google themselves have such a huge footprint of various businesses, that they alone would be an amazing customer for this, never mind all the other cool opportunities from third parties...<p>Imagine that they can ingest the entirety of YouTube and then dump that into Google Search&#x27;s index AND use it to generate training data for their next LLM.<p>Imagine that they can hook it up to your security cameras (Nest Cam), and then ask questions about what happened last night.<p>Imagine that you can ask Gemini how to do something (eg. fix appliance), and it can go and look up a YouTube video on how to accomplish that ask, and explain it to you.<p>Imagine that it can apply summarization and descriptions to every photo AND video in your personal Google Photos library. You can ask it to find a video of your son&#x27;s first steps, or a graduation&#x2F;diploma walk for your 3rd child (by name) and it can actually do that.<p>Imagine that Google Meet video calls can have the entire convo itself fed into an LLM (live?), instead of just a transcription. You can have an AI assistant there with you that can interject and discuss, based on both the audio and video feed.</div><br/><div id="39394189" class="c"><input type="checkbox" id="c-39394189" checked=""/><div class="controls bullet"><span class="by">anhner</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39393465">parent</a><span>|</span><a href="#39390590">next</a><span>|</span><label class="collapse" for="c-39394189">[-]</label><label class="expand" for="c-39394189">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to see that applied to the Google ecosystem, the question is - why haven&#x27;t they already done this?</div><br/></div></div></div></div></div></div><div id="39390590" class="c"><input type="checkbox" id="c-39390590" checked=""/><div class="controls bullet"><span class="by">nestorD</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39385505">prev</a><span>|</span><a href="#39384770">next</a><span>|</span><label class="collapse" for="c-39390590">[-]</label><label class="expand" for="c-39390590">[4 more]</label></div><br/><div class="children"><div class="content">Regarding the 10M tokens context, RingAttention has been shown [0] recently (by researchers, not ML engineers in a FAANG) to be able to scale to comparable (1M) context sizes (it does take work and a <i>lot</i> of GPUs).<p>[0]: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39367141">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39367141</a></div><br/><div id="39391415" class="c"><input type="checkbox" id="c-39391415" checked=""/><div class="controls bullet"><span class="by">jebarker</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39390590">parent</a><span>|</span><a href="#39384770">next</a><span>|</span><label class="collapse" for="c-39391415">[-]</label><label class="expand" for="c-39391415">[3 more]</label></div><br/><div class="children"><div class="content">&gt; researchers, not ML engineers in a FAANG<p>Why did you point out this distinction?</div><br/><div id="39392846" class="c"><input type="checkbox" id="c-39392846" checked=""/><div class="controls bullet"><span class="by">nestorD</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39391415">parent</a><span>|</span><a href="#39394079">next</a><span>|</span><label class="collapse" for="c-39392846">[-]</label><label class="expand" for="c-39392846">[1 more]</label></div><br/><div class="children"><div class="content">It means they have significantly less means (to get a lot of GPUs letting them scale up in context length) and are likely less well-versed in optimization (which also helps with scaling up)[0].<p>I believe those two things together are likely enough to explain the difference between a 1M context length and a 10M context length.<p>[0]: Which is not looking down on that particular research team, the vast majority of people have less means and optimization know-how than Google.</div><br/></div></div><div id="39394079" class="c"><input type="checkbox" id="c-39394079" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39391415">parent</a><span>|</span><a href="#39392846">prev</a><span>|</span><a href="#39384770">next</a><span>|</span><label class="collapse" for="c-39394079">[-]</label><label class="expand" for="c-39394079">[1 more]</label></div><br/><div class="children"><div class="content">Probably to indicate that its research and not productized?</div><br/></div></div></div></div></div></div><div id="39384770" class="c"><input type="checkbox" id="c-39384770" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39390590">prev</a><span>|</span><a href="#39384976">next</a><span>|</span><label class="collapse" for="c-39384770">[-]</label><label class="expand" for="c-39384770">[7 more]</label></div><br/><div class="children"><div class="content">&gt; 1. They don&#x27;t talk about how they get to 10M token context<p>&gt; 2. They don&#x27;t talk about how they get to 10M token context<p>Yes. I wonder if they&#x27;re using a &quot;linear RNN&quot; type of model like Linear Attention, Mamba, RWKV, etc.<p>Like Transformers with standard attention, these models train efficiently in parallel, but their compute is O(N) instead of O(NÂ²), so <i>in theory</i> they can be extended to much longer sequences much efficiently. They have shown a lot of promise recently at smaller model sizes.<p>Does anyone here have any insight or knowledge about the internals of Gemini 1.5?</div><br/><div id="39385533" class="c"><input type="checkbox" id="c-39385533" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384770">parent</a><span>|</span><a href="#39384905">next</a><span>|</span><label class="collapse" for="c-39385533">[-]</label><label class="expand" for="c-39385533">[1 more]</label></div><br/><div class="children"><div class="content">The fact they are getting perfect recall with millions of tokens rules out any of the existing linear attention methods.</div><br/></div></div><div id="39384832" class="c"><input type="checkbox" id="c-39384832" checked=""/><div class="controls bullet"><span class="by">candiodari</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384770">parent</a><span>|</span><a href="#39384905">prev</a><span>|</span><a href="#39384976">next</a><span>|</span><label class="collapse" for="c-39384832">[-]</label><label class="expand" for="c-39384832">[4 more]</label></div><br/><div class="children"><div class="content">They do give a hint:<p>&quot;This includes making Gemini 1.5 more efficient to train and serve, with a new Mixture-of-Experts (MoE) architecture.&quot;<p>One thing you could do with MoE is giving each expert different subsets of the input tokens. And that would definitely do what they claim here: it would allow search. You want to find where someone said &quot;the password is X&quot; in a 50 hour audio file, this would be perfect.<p>If your question is &quot;what is the first AND last thing person X said&quot; ... it&#x27;s going to suck badly. Anything that requires taking 2 things into account that aren&#x27;t right next to eachother is just not going to work.</div><br/><div id="39386752" class="c"><input type="checkbox" id="c-39386752" checked=""/><div class="controls bullet"><span class="by">spott</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384832">parent</a><span>|</span><a href="#39386088">next</a><span>|</span><label class="collapse" for="c-39386752">[-]</label><label class="expand" for="c-39386752">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Anything that requires taking 2 things into account that aren&#x27;t right next to eachother is just not going to work.<p>They kinda address that in the technical report[0].  On page 12 they show results from a &quot;multiple needle in a haystack&quot; evaluation.<p><a href="https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemini&#x2F;gemini_v1_5_report.pdf" rel="nofollow">https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemini&#x2F;gemini_...</a></div><br/></div></div><div id="39386088" class="c"><input type="checkbox" id="c-39386088" checked=""/><div class="controls bullet"><span class="by">declaredapple</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384832">parent</a><span>|</span><a href="#39386752">prev</a><span>|</span><a href="#39386153">next</a><span>|</span><label class="collapse" for="c-39386088">[-]</label><label class="expand" for="c-39386088">[1 more]</label></div><br/><div class="children"><div class="content">&gt; One thing you could do with MoE is giving each expert different subsets of the input tokens.<p>Don&#x27;t MoE&#x27;s route tokens to experts after the attention step? That wouldn&#x27;t solve the n^2 issue the attention step has.<p>If you split the tokens <i>before</i> the attention step, that would mean those tokens would have no relationship to each other - it would be like inferring two prompts in parallel. That would defeat the point of a 10M context</div><br/></div></div><div id="39386153" class="c"><input type="checkbox" id="c-39386153" checked=""/><div class="controls bullet"><span class="by">deskamess</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384832">parent</a><span>|</span><a href="#39386088">prev</a><span>|</span><a href="#39384976">next</a><span>|</span><label class="collapse" for="c-39386153">[-]</label><label class="expand" for="c-39386153">[1 more]</label></div><br/><div class="children"><div class="content">Is MOE then basically divide and conquer? I have no deep knowledge of this so I assumed MOE was where each expert analyzed the problem in a different way and then there was some map-reduce like operation on the generated expert results. Kinda like random forest but for inference.</div><br/></div></div></div></div></div></div><div id="39384976" class="c"><input type="checkbox" id="c-39384976" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39384770">prev</a><span>|</span><a href="#39388842">next</a><span>|</span><label class="collapse" for="c-39384976">[-]</label><label class="expand" for="c-39384976">[4 more]</label></div><br/><div class="children"><div class="content">Is 10M token context correct?  The blog post I see 1M but I&#x27;m not sure if these are different things<p>Edit:  Ah, I see, it&#x27;s 1M reliably in production, up to 10M in research:<p>&gt; <i>Through a series of machine learning innovations, weâve increased 1.5 Proâs context window capacity far beyond the original 32,000 tokens for Gemini 1.0. We can now run up to 1 million tokens in production.</i><p>&gt; <i>This means 1.5 Pro can process vast amounts of information in one go â including 1 hour of video, 11 hours of audio, codebases with over 30,000 lines of code or over 700,000 words. In our research, weâve also successfully tested up to 10 million tokens.</i></div><br/><div id="39392534" class="c"><input type="checkbox" id="c-39392534" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384976">parent</a><span>|</span><a href="#39385265">next</a><span>|</span><label class="collapse" for="c-39392534">[-]</label><label class="expand" for="c-39392534">[2 more]</label></div><br/><div class="children"><div class="content">How could one hour of video fit in 1M tokens? 1 hour at 30fps is 3600*30=100k frames. Each frame is converted in 256 tokens. So either they are not processing each frame, or each frame is converted into fewer tokens.</div><br/><div id="39393453" class="c"><input type="checkbox" id="c-39393453" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39392534">parent</a><span>|</span><a href="#39385265">next</a><span>|</span><label class="collapse" for="c-39393453">[-]</label><label class="expand" for="c-39393453">[1 more]</label></div><br/><div class="children"><div class="content">The model can probably perform fine at 1 frame per second (3600*256=921600 tokens), and they could probably use some sort of compression.</div><br/></div></div></div></div><div id="39385265" class="c"><input type="checkbox" id="c-39385265" checked=""/><div class="controls bullet"><span class="by">huytersd</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39384976">parent</a><span>|</span><a href="#39392534">prev</a><span>|</span><a href="#39388842">next</a><span>|</span><label class="collapse" for="c-39385265">[-]</label><label class="expand" for="c-39385265">[1 more]</label></div><br/><div class="children"><div class="content">I know how Iâm going to evaluate this model. Upload my codebase and ask it to âfind all the bugsâ.</div><br/></div></div></div></div><div id="39388842" class="c"><input type="checkbox" id="c-39388842" checked=""/><div class="controls bullet"><span class="by">kristjansson</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39384976">prev</a><span>|</span><a href="#39388107">next</a><span>|</span><label class="collapse" for="c-39388842">[-]</label><label class="expand" for="c-39388842">[1 more]</label></div><br/><div class="children"><div class="content">For other&#x27;s reference, the paper: <a href="https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemini&#x2F;gemini_v1_5_report.pdf" rel="nofollow">https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemini&#x2F;gemini_...</a></div><br/></div></div><div id="39388107" class="c"><input type="checkbox" id="c-39388107" checked=""/><div class="controls bullet"><span class="by">nborwankar</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39388842">prev</a><span>|</span><a href="#39386829">next</a><span>|</span><label class="collapse" for="c-39388107">[-]</label><label class="expand" for="c-39388107">[1 more]</label></div><br/><div class="children"><div class="content">Re RAG arenât you ignoring the fact that no one wants to put confidential company data into such LLMâs. Private RAG infrastructure remains a need for the same reason that privacy of data of all sorts remains a need. 
Huge context solves the problem for large open source context material but thatâs only part of the picture.</div><br/></div></div><div id="39386829" class="c"><input type="checkbox" id="c-39386829" checked=""/><div class="controls bullet"><span class="by">AaronFriel</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39388107">prev</a><span>|</span><a href="#39385221">next</a><span>|</span><label class="collapse" for="c-39386829">[-]</label><label class="expand" for="c-39386829">[6 more]</label></div><br/><div class="children"><div class="content">There will always be more data that <i>could</i> be relevant than fits in a context window, and especially for multi-turn conversations, huge contexts incur huge costs.<p>GPT-4 Turbo, using its full 128k context, costs around $1.28 per API call.<p>At that pricing, 1m tokens is $10, and 10m tokens is an eye-watering $100 per API call.<p>Of course prices will go down, but the price advantage of working with less will remain.</div><br/><div id="39389511" class="c"><input type="checkbox" id="c-39389511" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39386829">parent</a><span>|</span><a href="#39389427">next</a><span>|</span><label class="collapse" for="c-39389511">[-]</label><label class="expand" for="c-39389511">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see a problem with this pricing. At 1m tokens you can upload the whole proceedings of a trial and ask it to draw an analysis. Paying $10 for that sounds like a steal.</div><br/><div id="39393872" class="c"><input type="checkbox" id="c-39393872" checked=""/><div class="controls bullet"><span class="by">ithkuil</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39389511">parent</a><span>|</span><a href="#39389998">next</a><span>|</span><label class="collapse" for="c-39393872">[-]</label><label class="expand" for="c-39393872">[1 more]</label></div><br/><div class="children"><div class="content">Unfortunately the whole context has to be reprocessed fully for each query, which means that if you &quot;chat&quot; with the model you&#x27;ll incur in that $10 fee for every interaction which quickly sums up.<p>It may still be worth it for some use cases</div><br/></div></div><div id="39389998" class="c"><input type="checkbox" id="c-39389998" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39389511">parent</a><span>|</span><a href="#39393872">prev</a><span>|</span><a href="#39389923">next</a><span>|</span><label class="collapse" for="c-39389998">[-]</label><label class="expand" for="c-39389998">[1 more]</label></div><br/><div class="children"><div class="content">While it&#x27;s hard to say what&#x27;s possible on the cutting edge, historically models tend to get dumber as the context size gets bigger. So you&#x27;d get a much more intelligent analysis of a 10,000 token excerpt of the trial than a million token complete transcript of the trial.  I have not spent the money testing big token sizes in GPT 4 turbo, but it would not surprise me if it gets dumber.  Think of it this way, if the model is limited to 3,000 token replies, if an analysis would require a more detailed response than 3,000 tokens, it cannot provide it, it&#x27;ll just give you insufficient information. What it&#x27;ll probably do is ignore parts of the trial transcript because it can&#x27;t analyze all that information in 3,000 tokens. And asking a followup question is another million tokens.</div><br/></div></div><div id="39389923" class="c"><input type="checkbox" id="c-39389923" checked=""/><div class="controls bullet"><span class="by">AaronFriel</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39389511">parent</a><span>|</span><a href="#39389998">prev</a><span>|</span><a href="#39389427">next</a><span>|</span><label class="collapse" for="c-39389923">[-]</label><label class="expand" for="c-39389923">[1 more]</label></div><br/><div class="children"><div class="content">Of course, if you get exactly the answer you want in the first reply.</div><br/></div></div></div></div><div id="39389427" class="c"><input type="checkbox" id="c-39389427" checked=""/><div class="controls bullet"><span class="by">7734128</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39386829">parent</a><span>|</span><a href="#39389511">prev</a><span>|</span><a href="#39385221">next</a><span>|</span><label class="collapse" for="c-39389427">[-]</label><label class="expand" for="c-39389427">[1 more]</label></div><br/><div class="children"><div class="content">Would the price really increase linearly? Isn&#x27;t the demands on compute and memory increasing steeper than that as a function of context length?</div><br/></div></div></div></div><div id="39385221" class="c"><input type="checkbox" id="c-39385221" checked=""/><div class="controls bullet"><span class="by">resouer</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39386829">prev</a><span>|</span><a href="#39386613">next</a><span>|</span><label class="collapse" for="c-39385221">[-]</label><label class="expand" for="c-39385221">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The 10M context ability wipes out most RAG stack complexity immediately.<p>This may not be true. My experience of the complexity of RAG lays in how to properly connect to various unstructured data sources and perform data transformation pipeline for large scale data set (which means GB, TB or even PB). It&#x27;s in the critical path rather a &quot;nice to have&quot;, because the quality of data and the pipeline is a major factor for the final generated the result. i.e., in RAG, the importance of R &gt;&gt;&gt; G.</div><br/></div></div><div id="39386613" class="c"><input type="checkbox" id="c-39386613" checked=""/><div class="controls bullet"><span class="by">localhost</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39385221">prev</a><span>|</span><a href="#39391175">next</a><span>|</span><label class="collapse" for="c-39386613">[-]</label><label class="expand" for="c-39386613">[4 more]</label></div><br/><div class="children"><div class="content">RE: RAG - they haven&#x27;t released pricing, but if input tokens are priced at GPT-4 levels - $0.01&#x2F;1K then sending 10M tokens will cost you $100.</div><br/><div id="39392602" class="c"><input type="checkbox" id="c-39392602" checked=""/><div class="controls bullet"><span class="by">campers</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39386613">parent</a><span>|</span><a href="#39386736">next</a><span>|</span><label class="collapse" for="c-39392602">[-]</label><label class="expand" for="c-39392602">[1 more]</label></div><br/><div class="children"><div class="content">In the announcements today they also halved the pricing of Gemini 1.0 Pro to $0.000125 &#x2F; 1K characters, which is a quarter of GPT3.5 Turbo so it could potentially be a bit lower than GPT-4 pricing.</div><br/></div></div><div id="39386736" class="c"><input type="checkbox" id="c-39386736" checked=""/><div class="controls bullet"><span class="by">s-macke</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39386613">parent</a><span>|</span><a href="#39392602">prev</a><span>|</span><a href="#39391175">next</a><span>|</span><label class="collapse" for="c-39386736">[-]</label><label class="expand" for="c-39386736">[2 more]</label></div><br/><div class="children"><div class="content">If you think the current APIs will stay that way, then you&#x27;re right. But when they start offering dedicated chat instances or caching options, you could be back in the penny region.<p>You probably need a couple GB to cache a conversation. That&#x27;s not so easy at the moment because you have to transfer that data to and from the GPUs and store the data somewhere.</div><br/><div id="39390925" class="c"><input type="checkbox" id="c-39390925" checked=""/><div class="controls bullet"><span class="by">localhost</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39386736">parent</a><span>|</span><a href="#39391175">next</a><span>|</span><label class="collapse" for="c-39390925">[-]</label><label class="expand" for="c-39390925">[1 more]</label></div><br/><div class="children"><div class="content">The tokens need to be fed into the model along with the prompt and this takes time. Naive attention is O(N^2). They probably use at least flash attention, and likely something more exotic to their hardware.<p>You&#x27;ll notice in their video [1] that they never show the prompts running interactively. This is for a roughly 800K context. They claim that &quot;the model took around 60s to respond to each of these prompts&quot;.<p>This is not really usable as an interactive experience. I don&#x27;t want to wait 1 minute for an answer each time I have a question.<p>[1] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SSnsmqIj1MI" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SSnsmqIj1MI</a></div><br/></div></div></div></div></div></div><div id="39391175" class="c"><input type="checkbox" id="c-39391175" checked=""/><div class="controls bullet"><span class="by">bschne</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39386613">prev</a><span>|</span><a href="#39386672">next</a><span>|</span><label class="collapse" for="c-39391175">[-]</label><label class="expand" for="c-39391175">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The 10M context ability wipes out most RAG stack complexity immediately.<p>1. People mention accuracy issues with longer contexts
2. People mention processing time issues with longer contexts
3. Something people haven&#x27;t mentioned in this thread is cost -- even thought prompt tokens are usually cheaper than generated tokens, and Gemini seems to be cheaper than GPT-4, putting a whole knowledge base or 80-page document in the context is going to make every time you run that prompt quite expensive</div><br/></div></div><div id="39386672" class="c"><input type="checkbox" id="c-39386672" checked=""/><div class="controls bullet"><span class="by">renonce</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39391175">prev</a><span>|</span><a href="#39389578">next</a><span>|</span><label class="collapse" for="c-39386672">[-]</label><label class="expand" for="c-39386672">[3 more]</label></div><br/><div class="children"><div class="content">&gt; They don&#x27;t talk about how they get to 10M token context<p>I don&#x27;t know how either but maybe <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39367141">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39367141</a><p>Anyway I mean, there is plenty of public research on this so it&#x27;s probably just a matter of time for everyone else to catch up</div><br/><div id="39387417" class="c"><input type="checkbox" id="c-39387417" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39386672">parent</a><span>|</span><a href="#39389578">next</a><span>|</span><label class="collapse" for="c-39387417">[-]</label><label class="expand" for="c-39387417">[2 more]</label></div><br/><div class="children"><div class="content">Why do you think this specific variant (RingAttention)? There are so many different variants for this.<p>As far as I know, the problem in most cases is that while the context length might be high in theory, the actual ability to use it is still limited. E.g. recurrent networks even have infinite context, but they actually only use 10-20 frames as context (longer only in very specific settings; or maybe if you scale them up).</div><br/><div id="39392241" class="c"><input type="checkbox" id="c-39392241" checked=""/><div class="controls bullet"><span class="by">renonce</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39387417">parent</a><span>|</span><a href="#39389578">next</a><span>|</span><label class="collapse" for="c-39392241">[-]</label><label class="expand" for="c-39392241">[1 more]</label></div><br/><div class="children"><div class="content">There are ways to test the neural networkâs ability to recall from a very long sequence. For example, if you insert a random sentence like âX is Sam Altmanâ somewhere in the text, will the model be able to answer the question âWho is X?â, or maybe somewhat indirectly âWho is X (in another language)â or âWhich sentence was inserted out of context?â âWhich celebrity was mentioned in the text?â<p>Anyways the ability to generalize to longer context length is evidenced by such tests. If every token of the modelâs output is able to answer questions in such a way that any sentence from the input would be taken into account, this gives evidence that the full context window indeed matters. Currently I find Claude 2 to perform very well on such tasks, so that sets my expectation of how a language model with an extremely long context window should look like.</div><br/></div></div></div></div></div></div><div id="39389578" class="c"><input type="checkbox" id="c-39389578" checked=""/><div class="controls bullet"><span class="by">joshsabol46</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39386672">prev</a><span>|</span><a href="#39392342">next</a><span>|</span><label class="collapse" for="c-39389578">[-]</label><label class="expand" for="c-39389578">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The 10M context ability wipes out most RAG stack complexity immediately.<p>RAG is needed for the same reason you don&#x27;t `SELECT *` all of your queries.</div><br/></div></div><div id="39392342" class="c"><input type="checkbox" id="c-39392342" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39389578">prev</a><span>|</span><a href="#39392021">next</a><span>|</span><label class="collapse" for="c-39392342">[-]</label><label class="expand" for="c-39392342">[1 more]</label></div><br/><div class="children"><div class="content">&gt;3. The 10M context ability wipes out most RAG stack complexity immediately.<p>I&#x27;d imagine RAG would still be much more efficient computationally</div><br/></div></div><div id="39392021" class="c"><input type="checkbox" id="c-39392021" checked=""/><div class="controls bullet"><span class="by">lqcfcjx</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39392342">prev</a><span>|</span><a href="#39385448">next</a><span>|</span><label class="collapse" for="c-39392021">[-]</label><label class="expand" for="c-39392021">[1 more]</label></div><br/><div class="children"><div class="content">This might be a stupid question - even if there&#x27;s no quality degradation from 10M context, will it be extremely slow in reference?</div><br/></div></div><div id="39385448" class="c"><input type="checkbox" id="c-39385448" checked=""/><div class="controls bullet"><span class="by">ren_engineer</span><span>|</span><a href="#39384513">parent</a><span>|</span><a href="#39392021">prev</a><span>|</span><a href="#39384960">next</a><span>|</span><label class="collapse" for="c-39385448">[-]</label><label class="expand" for="c-39385448">[4 more]</label></div><br/><div class="children"><div class="content">RAG would still be useful for cost savings assuming they charge per token, plus I&#x27;m guessing using the full-context length would be slower than using RAG to get what you need for a smaller prompt</div><br/><div id="39385543" class="c"><input type="checkbox" id="c-39385543" checked=""/><div class="controls bullet"><span class="by">nostrebored</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385448">parent</a><span>|</span><a href="#39384960">next</a><span>|</span><label class="collapse" for="c-39385543">[-]</label><label class="expand" for="c-39385543">[3 more]</label></div><br/><div class="children"><div class="content">This is going to be the real differentiator.<p>HN is very focused on technical feasibility (which remains to be seen!), but in every LLM opportunity, the CIO&#x2F;CFO&#x2F;CEO are going to be concerned with the cost modeling.<p>The way that LLMs are billed now, if you can densely pack the context with relevant information, you will come out ahead commercially. I don&#x27;t see this changing with the way that LLM inference works.<p>Maybe this changes with managed vector search offerings that are opaque to the user. The context goes to a preprocessing layer, an efficient cache understands which parts haven&#x27;t been embedded (new bloom filter use case?), embeds the other chunks, and extracts the intent of the prompt.</div><br/><div id="39385967" class="c"><input type="checkbox" id="c-39385967" checked=""/><div class="controls bullet"><span class="by">mediaman</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385543">parent</a><span>|</span><a href="#39386336">next</a><span>|</span><label class="collapse" for="c-39385967">[-]</label><label class="expand" for="c-39385967">[1 more]</label></div><br/><div class="children"><div class="content">Agreed with this.<p>The leading ability AI (in terms of cognitive power) will, generally, cost more per token than lower cognitive power AI.<p>That means that at a given budget you can choose more cognitive power with fewer tokens, or less cognitive power with more tokens. For most use cases, there&#x27;s no real point in giving up cognitive power to include useless tokens that have no hope of helping with a given question.<p>So then you&#x27;re back to the question of: how do we reduce the number of tokens, so that we can get higher cognitive power?<p>And that&#x27;s the entire field of information retrieval, which is the most important part of RAG.</div><br/></div></div><div id="39386336" class="c"><input type="checkbox" id="c-39386336" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#39384513">root</a><span>|</span><a href="#39385543">parent</a><span>|</span><a href="#39385967">prev</a><span>|</span><a href="#39384960">next</a><span>|</span><label class="collapse" for="c-39386336">[-]</label><label class="expand" for="c-39386336">[1 more]</label></div><br/><div class="children"><div class="content">The way that LLMs are billed now, if you can densely pack the context with relevant information, you will come out ahead commercially. I don&#x27;t see this changing with the way that LLM inference works.<p>Really? Because to my understanding the compute necessary to generate a token grows linearly with the context, and doesn&#x27;t the OpenAI billing reflect that by seperating prompt and output tokens?</div><br/></div></div></div></div></div></div></div></div><div id="39384960" class="c"><input type="checkbox" id="c-39384960" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#39384513">prev</a><span>|</span><a href="#39383799">next</a><span>|</span><label class="collapse" for="c-39384960">[-]</label><label class="expand" for="c-39384960">[1 more]</label></div><br/><div class="children"><div class="content">One interesting tidbit from the technical report:<p>&gt;HumanEval is an industry standard open-source evaluation benchmark (Chen et al., 2021), but we
found controlling for accidental leakage on webpages and open-source code repositories to be a non-trivial task, even with conservative filtering heuristics. An analysis of the test data leakage of
Gemini 1.0 Ultra showed that continued pretraining on a dataset containing even a single epoch of
the test split for HumanEval boosted scores from 74.4% to 89.0%, highlighting the danger of data contamination. We found that this sharp increase persisted even when examples were embedded
in extraneous formats (e.g. JSON, HTML). We invite researchers assessing coding abilities of these
models head-to-head to always maintain a small set of truly held-out test functions that are written
in-house, thereby minimizing the risk of leakage. The Natural2Code benchmark, which we announced
and used in the evaluation of Gemini 1.0 series of models, was created to fill this gap. It follows the
exact same format of HumanEval but with a different set of prompts and tests.</div><br/></div></div><div id="39383799" class="c"><input type="checkbox" id="c-39383799" checked=""/><div class="controls bullet"><span class="by">alphabetting</span><span>|</span><a href="#39384960">prev</a><span>|</span><a href="#39384028">next</a><span>|</span><label class="collapse" for="c-39383799">[-]</label><label class="expand" for="c-39383799">[35 more]</label></div><br/><div class="children"><div class="content">Massive whoa if true from technical report<p>&quot;Studying the limits of Gemini 1.5 Pro&#x27;s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (&gt;99%) up to at least 10M tokens&quot;<p><a href="https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemini&#x2F;gemini_v1_5_report.pdf" rel="nofollow">https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemini&#x2F;gemini_...</a></div><br/><div id="39384145" class="c"><input type="checkbox" id="c-39384145" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#39383799">parent</a><span>|</span><a href="#39384521">next</a><span>|</span><label class="collapse" for="c-39384145">[-]</label><label class="expand" for="c-39384145">[6 more]</label></div><br/><div class="children"><div class="content">10M tokens is absolutely jaw dropping. For reference, this is approximately thirty books of 500 pages each.<p>Having 99% retrieval is nuts too. Models tend to unwind pretty badly as the context (tokens) grows.<p>Put these together and you are getting into the territory of dumping all your company documents, or all your departments documents into a single GPT (or whatever google will call it) and everyone working with that. Wild.</div><br/><div id="39384593" class="c"><input type="checkbox" id="c-39384593" checked=""/><div class="controls bullet"><span class="by">kranke155</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39384145">parent</a><span>|</span><a href="#39384521">next</a><span>|</span><label class="collapse" for="c-39384593">[-]</label><label class="expand" for="c-39384593">[5 more]</label></div><br/><div class="children"><div class="content">Seems like Google caught up. Demis is again showing an incredible ability to lead a team to make groundbreaking work.</div><br/><div id="39385312" class="c"><input type="checkbox" id="c-39385312" checked=""/><div class="controls bullet"><span class="by">huytersd</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39384593">parent</a><span>|</span><a href="#39384521">next</a><span>|</span><label class="collapse" for="c-39385312">[-]</label><label class="expand" for="c-39385312">[4 more]</label></div><br/><div class="children"><div class="content">If any of this is remotely true, not only did it catch up, itâs wiping the floor with how useful it can be compared to GPT4. Not going to make a judgement until I can actually try it out though.</div><br/><div id="39386265" class="c"><input type="checkbox" id="c-39386265" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39385312">parent</a><span>|</span><a href="#39384521">next</a><span>|</span><label class="collapse" for="c-39386265">[-]</label><label class="expand" for="c-39386265">[3 more]</label></div><br/><div class="children"><div class="content">In the demo videos gemini needs about a minute to answer long context questions. Which is better than reading thousands of pages yourself. But if it has to compete with classical search and skimming it might need some optimization.</div><br/><div id="39389034" class="c"><input type="checkbox" id="c-39389034" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39386265">parent</a><span>|</span><a href="#39387282">next</a><span>|</span><label class="collapse" for="c-39389034">[-]</label><label class="expand" for="c-39389034">[1 more]</label></div><br/><div class="children"><div class="content">Replacing grep or `ctrl+F` with Gemini would be the user&#x27;s fault, not Gemini&#x27;s. If classical search for a job already a performant solution, <i>use classical search</i>. Save your tokens for jobs worthy of solving with a general intelligence!</div><br/></div></div><div id="39387282" class="c"><input type="checkbox" id="c-39387282" checked=""/><div class="controls bullet"><span class="by">huytersd</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39386265">parent</a><span>|</span><a href="#39389034">prev</a><span>|</span><a href="#39384521">next</a><span>|</span><label class="collapse" for="c-39387282">[-]</label><label class="expand" for="c-39387282">[1 more]</label></div><br/><div class="children"><div class="content">Thatâs a compute problem, something that involves just throwing money at the problem.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39384521" class="c"><input type="checkbox" id="c-39384521" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#39383799">parent</a><span>|</span><a href="#39384145">prev</a><span>|</span><a href="#39385590">next</a><span>|</span><label class="collapse" for="c-39384521">[-]</label><label class="expand" for="c-39384521">[3 more]</label></div><br/><div class="children"><div class="content">Another whoa for me<p>&gt;Finally, we highlight surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person learning from the same content.<p>Results - <a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;qXcVNOM" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;qXcVNOM</a></div><br/><div id="39384999" class="c"><input type="checkbox" id="c-39384999" checked=""/><div class="controls bullet"><span class="by">usaar333</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39384521">parent</a><span>|</span><a href="#39385590">next</a><span>|</span><label class="collapse" for="c-39384999">[-]</label><label class="expand" for="c-39384999">[2 more]</label></div><br/><div class="children"><div class="content">I think this somewhat is mostly due to the ability to handle high context lengths better.  Note how Claude 2.1 already highly outperforms GPT-4 on this task.</div><br/><div id="39389100" class="c"><input type="checkbox" id="c-39389100" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39384999">parent</a><span>|</span><a href="#39385590">next</a><span>|</span><label class="collapse" for="c-39389100">[-]</label><label class="expand" for="c-39389100">[1 more]</label></div><br/><div class="children"><div class="content">GPT-4V turbo outperforms Claude on long contexts, IIRC. Unless that&#x27;s mistaken, I&#x27;d suspect a different explanation for that task.</div><br/></div></div></div></div></div></div><div id="39385590" class="c"><input type="checkbox" id="c-39385590" checked=""/><div class="controls bullet"><span class="by">cchance</span><span>|</span><a href="#39383799">parent</a><span>|</span><a href="#39384521">prev</a><span>|</span><a href="#39383891">next</a><span>|</span><label class="collapse" for="c-39385590">[-]</label><label class="expand" for="c-39385590">[1 more]</label></div><br/><div class="children"><div class="content">Did you watch the video of the Gemini 1.5 video recall after it processed the 44 minute video... holy shit</div><br/></div></div><div id="39383891" class="c"><input type="checkbox" id="c-39383891" checked=""/><div class="controls bullet"><span class="by">megaman821</span><span>|</span><a href="#39383799">parent</a><span>|</span><a href="#39385590">prev</a><span>|</span><a href="#39393745">next</a><span>|</span><label class="collapse" for="c-39383891">[-]</label><label class="expand" for="c-39383891">[8 more]</label></div><br/><div class="children"><div class="content">So, will this outperform any RAG approach as long as the data fits inside the context window?</div><br/><div id="39384322" class="c"><input type="checkbox" id="c-39384322" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39383891">parent</a><span>|</span><a href="#39384006">next</a><span>|</span><label class="collapse" for="c-39384322">[-]</label><label class="expand" for="c-39384322">[1 more]</label></div><br/><div class="children"><div class="content">A perfect RAG system would probably outperform everything in a larger context due to prompt dilution, but in the real world putting everything in context will win a lot of the time.  The large context system will also almost certainly be more usable due to elimination of retrieval latency.  The large context system might lose on price&#x2F;performance though.</div><br/></div></div><div id="39384006" class="c"><input type="checkbox" id="c-39384006" checked=""/><div class="controls bullet"><span class="by">TheGeminon</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39383891">parent</a><span>|</span><a href="#39384322">prev</a><span>|</span><a href="#39383950">next</a><span>|</span><label class="collapse" for="c-39384006">[-]</label><label class="expand" for="c-39384006">[2 more]</label></div><br/><div class="children"><div class="content">Outperform is dependent on the RAG approach (and this would be a RAG approach anyways, you can already do this with smaller context sizes). A simplistic one, probably, but dumping in data that you don&#x27;t need dilutes the useful information, so I would imagine there would be at least _some_ degradation.<p>But there is also the downside of &quot;tuning&quot; the RAG to return less tokens you will miss extra context that could be useful to the model.</div><br/><div id="39384140" class="c"><input type="checkbox" id="c-39384140" checked=""/><div class="controls bullet"><span class="by">megaman821</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39384006">parent</a><span>|</span><a href="#39383950">next</a><span>|</span><label class="collapse" for="c-39384140">[-]</label><label class="expand" for="c-39384140">[1 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t their needle&#x2F;haystack benchmark seem to suggest there is almost no dilution? They pushed that demo out to 10M tokens.</div><br/></div></div></div></div><div id="39383950" class="c"><input type="checkbox" id="c-39383950" checked=""/><div class="controls bullet"><span class="by">ArcaneMoose</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39383891">parent</a><span>|</span><a href="#39384006">prev</a><span>|</span><a href="#39389091">next</a><span>|</span><label class="collapse" for="c-39383950">[-]</label><label class="expand" for="c-39383950">[1 more]</label></div><br/><div class="children"><div class="content">Cost would still be a big concern</div><br/></div></div><div id="39389091" class="c"><input type="checkbox" id="c-39389091" checked=""/><div class="controls bullet"><span class="by">chasd00</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39383891">parent</a><span>|</span><a href="#39383950">prev</a><span>|</span><a href="#39383966">next</a><span>|</span><label class="collapse" for="c-39389091">[-]</label><label class="expand" for="c-39389091">[1 more]</label></div><br/><div class="children"><div class="content">are you going to upload 10M tokens to Gemini on every request? That&#x27;s a lot of data moving around when the user is expecting a near realtime response. Seems like it would still be better to only set the context with information relevant to the user&#x27;s prompt which is what plain rag does.</div><br/></div></div><div id="39383966" class="c"><input type="checkbox" id="c-39383966" checked=""/><div class="controls bullet"><span class="by">saliagato</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39383891">parent</a><span>|</span><a href="#39389091">prev</a><span>|</span><a href="#39393745">next</a><span>|</span><label class="collapse" for="c-39383966">[-]</label><label class="expand" for="c-39383966">[2 more]</label></div><br/><div class="children"><div class="content">basically, yes. Pinecone? Dead. Azure AI Search? Dead. Quadrant? Dead.</div><br/><div id="39384027" class="c"><input type="checkbox" id="c-39384027" checked=""/><div class="controls bullet"><span class="by">_boffin_</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39383966">parent</a><span>|</span><a href="#39393745">next</a><span>|</span><label class="collapse" for="c-39384027">[-]</label><label class="expand" for="c-39384027">[1 more]</label></div><br/><div class="children"><div class="content">Prompt token cost still a variable.</div><br/></div></div></div></div></div></div><div id="39384278" class="c"><input type="checkbox" id="c-39384278" checked=""/><div class="controls bullet"><span class="by">matsemann</span><span>|</span><a href="#39383799">parent</a><span>|</span><a href="#39393745">prev</a><span>|</span><a href="#39383885">next</a><span>|</span><label class="collapse" for="c-39384278">[-]</label><label class="expand" for="c-39384278">[9 more]</label></div><br/><div class="children"><div class="content">Could you (or someone) explain what this means?</div><br/><div id="39387463" class="c"><input type="checkbox" id="c-39387463" checked=""/><div class="controls bullet"><span class="by">ehsankia</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39384278">parent</a><span>|</span><a href="#39385581">next</a><span>|</span><label class="collapse" for="c-39387463">[-]</label><label class="expand" for="c-39387463">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s how much text it can consider at a time when generating a response. Basically the size of the prompt. A token is not quite a word but you can think of it as roughly that. Previously, the best most LLMs could do is around 32K. This new model does 1M, and in testing they could put it up to 10M with near perfect retrieval.<p>As the other comment mentions, you can paste the content of entire books or documents and ask very pointed question about it. Last year, Anthropic was showing off their 100K context window, and that&#x27;s exactly what they did, they gave it the content of The Great Gatsby and asked it questions about specific lines of the book.<p>Similarly, imagine giving it hundreds of documents and asking it to spot some specific detail in there.</div><br/><div id="39390797" class="c"><input type="checkbox" id="c-39390797" checked=""/><div class="controls bullet"><span class="by">liamYC</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39387463">parent</a><span>|</span><a href="#39385581">next</a><span>|</span><label class="collapse" for="c-39390797">[-]</label><label class="expand" for="c-39390797">[1 more]</label></div><br/><div class="children"><div class="content">Awesome explanation, thanks for the comparison</div><br/></div></div></div></div><div id="39385581" class="c"><input type="checkbox" id="c-39385581" checked=""/><div class="controls bullet"><span class="by">FergusArgyll</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39384278">parent</a><span>|</span><a href="#39387463">prev</a><span>|</span><a href="#39383885">next</a><span>|</span><label class="collapse" for="c-39385581">[-]</label><label class="expand" for="c-39385581">[6 more]</label></div><br/><div class="children"><div class="content">The input you give it can be very long. This can qualitatively change the experience. Imagine, for example, copy pasting the entire lord of the rings plus another 100 books you like and asking it to write a similar book...</div><br/><div id="39386068" class="c"><input type="checkbox" id="c-39386068" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39385581">parent</a><span>|</span><a href="#39390194">next</a><span>|</span><label class="collapse" for="c-39386068">[-]</label><label class="expand" for="c-39386068">[2 more]</label></div><br/><div class="children"><div class="content">I just googled it, and the LOTR trilogy apparently has a total of 480,000 words, which brings home how huge 1M is! It&#x27;d be fascinating to see how well Gemini could summarize the plot or reason about it.<p>One point I&#x27;m unclear on is how these huge context sizes are implemented by the various models. Are any of them the actual raw &quot;width of the model&quot; that is propagated through it, or are these all hierarchical summarization and chunk embedding index lookup type tricks?</div><br/><div id="39389377" class="c"><input type="checkbox" id="c-39389377" checked=""/><div class="controls bullet"><span class="by">mburns</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39386068">parent</a><span>|</span><a href="#39390194">next</a><span>|</span><label class="collapse" for="c-39389377">[-]</label><label class="expand" for="c-39389377">[1 more]</label></div><br/><div class="children"><div class="content">For another reference, Shakespeareâs complete works are ~885k words.<p>The Encyclopedia Britannica is ~44M words.</div><br/></div></div></div></div><div id="39390194" class="c"><input type="checkbox" id="c-39390194" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39385581">parent</a><span>|</span><a href="#39386068">prev</a><span>|</span><a href="#39385886">next</a><span>|</span><label class="collapse" for="c-39390194">[-]</label><label class="expand" for="c-39390194">[2 more]</label></div><br/><div class="children"><div class="content">Reading Lord of the Rings, and writing a quality book in the same style, are almost wholly unrelated tasks. Over 150 million copies of Lord of the Rings have been sold, but few readers are capable of &quot;writing a similar book&quot; in terms of quality.  There&#x27;s no reason to think this would work well.</div><br/><div id="39392184" class="c"><input type="checkbox" id="c-39392184" checked=""/><div class="controls bullet"><span class="by">pfooti</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39390194">parent</a><span>|</span><a href="#39385886">next</a><span>|</span><label class="collapse" for="c-39392184">[-]</label><label class="expand" for="c-39392184">[1 more]</label></div><br/><div class="children"><div class="content">I mean, Terry Brooks did it with the Sword of Shannara. (&#x2F;s)</div><br/></div></div></div></div><div id="39385886" class="c"><input type="checkbox" id="c-39385886" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39385581">parent</a><span>|</span><a href="#39390194">prev</a><span>|</span><a href="#39383885">next</a><span>|</span><label class="collapse" for="c-39385886">[-]</label><label class="expand" for="c-39385886">[1 more]</label></div><br/><div class="children"><div class="content">I doubt itâs smart enough to write another (coherent, good) book based on 103 books. But you could ask it questions about the books and it would search and synthesize good answers.</div><br/></div></div></div></div></div></div><div id="39383873" class="c"><input type="checkbox" id="c-39383873" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#39383799">parent</a><span>|</span><a href="#39383885">prev</a><span>|</span><a href="#39384028">next</a><span>|</span><label class="collapse" for="c-39383873">[-]</label><label class="expand" for="c-39383873">[5 more]</label></div><br/><div class="children"><div class="content">Until I can talk to it, I care exactly zero.</div><br/><div id="39384656" class="c"><input type="checkbox" id="c-39384656" checked=""/><div class="controls bullet"><span class="by">peterisza</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39383873">parent</a><span>|</span><a href="#39388756">prev</a><span>|</span><a href="#39384028">next</a><span>|</span><label class="collapse" for="c-39384656">[-]</label><label class="expand" for="c-39384656">[3 more]</label></div><br/><div class="children"><div class="content">you can buy their stock if you think they&#x27;ll make a lot of money with their tech</div><br/><div id="39385140" class="c"><input type="checkbox" id="c-39385140" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39384656">parent</a><span>|</span><a href="#39384028">next</a><span>|</span><label class="collapse" for="c-39385140">[-]</label><label class="expand" for="c-39385140">[2 more]</label></div><br/><div class="children"><div class="content">Well that&#x27;s really the right question .. what can, and will, Google do with this that can move their corporate earnings needle in a meaningful way? Obviously they can sell API access and integrate it into their Google docs suite, as well as their new Project IDX IDE, but do any of these have potential to make a meaningful impact ?<p>It&#x27;s also not obvious how these huge models will fare against increasingly capable open source ones like Mixtral, perhaps especially since Google are confirming here that MoE is the path forward, which perhaps helps limit how big these models need to be.</div><br/><div id="39385890" class="c"><input type="checkbox" id="c-39385890" checked=""/><div class="controls bullet"><span class="by">plaidfuji</span><span>|</span><a href="#39383799">root</a><span>|</span><a href="#39385140">parent</a><span>|</span><a href="#39384028">next</a><span>|</span><label class="collapse" for="c-39385890">[-]</label><label class="expand" for="c-39385890">[1 more]</label></div><br/><div class="children"><div class="content">In the long run it could move the needle in enterprise market share of Workspace and GCP. They have a lot of room to grow and IMO have a far superior product to O365&#x2F;Azure which could be exacerbated by strong AI products. Only problem is this sales cycle can take a decade or more, and Google hasnât historically been patient or strategic about things like this.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39384028" class="c"><input type="checkbox" id="c-39384028" checked=""/><div class="controls bullet"><span class="by">gpjanik</span><span>|</span><a href="#39383799">prev</a><span>|</span><a href="#39394652">next</a><span>|</span><label class="collapse" for="c-39384028">[-]</label><label class="expand" for="c-39384028">[7 more]</label></div><br/><div class="children"><div class="content">0 trust to what they put out until I see it live. After the last &quot;launch&quot; video which was fundamentally a marketing edit not showing the real product, I don&#x27;t trust anything coming out of Google that isn&#x27;t an instantly testable input form.</div><br/><div id="39392381" class="c"><input type="checkbox" id="c-39392381" checked=""/><div class="controls bullet"><span class="by">dingclancy</span><span>|</span><a href="#39384028">parent</a><span>|</span><a href="#39391876">next</a><span>|</span><label class="collapse" for="c-39392381">[-]</label><label class="expand" for="c-39392381">[1 more]</label></div><br/><div class="children"><div class="content">Essentially, the focus seems to be on leveraging the media buzz around Gemini 1.0 by highlighting the development of version 1.5. While GPT-4&#x27;s position relative to Gemini 1.5 remains unclear, and the specifics of ChatGPT 4.5 are yet to be disclosed, it&#x27;s worth noting that no official release has taken place until the functionality is directly accessible in user chats.<p>Google appears to be making strides in catching up.<p>When it comes to my personal workflow and accomplishing tasks, I still find ChatGPT to be the most effective tool. My familiarity with its features has made it indispensable. The integration of mentions and tailored GPTs seamlessly enhances my workflow.<p>While Gemini may match the foundational capabilities of LLMs, it falls short in delivering a product that efficiently aids in task completion.</div><br/></div></div><div id="39391876" class="c"><input type="checkbox" id="c-39391876" checked=""/><div class="controls bullet"><span class="by">frays</span><span>|</span><a href="#39384028">parent</a><span>|</span><a href="#39392381">prev</a><span>|</span><a href="#39390956">next</a><span>|</span><label class="collapse" for="c-39391876">[-]</label><label class="expand" for="c-39391876">[1 more]</label></div><br/><div class="children"><div class="content">I completely share the same views as you after their last video - and it appears that they&#x27;ve learnt their lesson this time.<p>If you watch the videos in the blog post, you can see it&#x27;s a screen recording on  a computer without any editing&#x2F;stitching of different scenes together.<p>It&#x27;s good to be sceptical but as engineers we should all remain open.</div><br/></div></div><div id="39390956" class="c"><input type="checkbox" id="c-39390956" checked=""/><div class="controls bullet"><span class="by">tfsh</span><span>|</span><a href="#39384028">parent</a><span>|</span><a href="#39391876">prev</a><span>|</span><a href="#39384760">next</a><span>|</span><label class="collapse" for="c-39390956">[-]</label><label class="expand" for="c-39390956">[1 more]</label></div><br/><div class="children"><div class="content">The videos shown in these demos have clearly learnt from that as they&#x27;re using a real live product, filmed on their computers with timers in the bottom showing how long the computations take.</div><br/></div></div><div id="39392076" class="c"><input type="checkbox" id="c-39392076" checked=""/><div class="controls bullet"><span class="by">replwoacause</span><span>|</span><a href="#39384028">parent</a><span>|</span><a href="#39384760">prev</a><span>|</span><a href="#39390711">next</a><span>|</span><label class="collapse" for="c-39392076">[-]</label><label class="expand" for="c-39392076">[1 more]</label></div><br/><div class="children"><div class="content">100%. Google continues to underwhelm. Not buying it until I can try it.</div><br/></div></div></div></div><div id="39394652" class="c"><input type="checkbox" id="c-39394652" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#39384028">prev</a><span>|</span><a href="#39384217">next</a><span>|</span><label class="collapse" for="c-39394652">[-]</label><label class="expand" for="c-39394652">[1 more]</label></div><br/><div class="children"><div class="content">This got my trying Gemini, but doing so is such a hassle that I&#x27;m almost ready to give up. Trying out ChatGPT is as simple as signing up (either for pro, or the API), and getting a single API key.<p>Google requires me to navigate their absolutely insane console (seriously, I thought the AWS console was bad, but GCP takes the cake), only to tell me there is not even a way to get an API key... I had to ask Gemini through the built in interface to figure that out.</div><br/></div></div><div id="39384217" class="c"><input type="checkbox" id="c-39384217" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#39394652">prev</a><span>|</span><a href="#39384587">next</a><span>|</span><label class="collapse" for="c-39384217">[-]</label><label class="expand" for="c-39384217">[21 more]</label></div><br/><div class="children"><div class="content">&gt;Finally, we highlight surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person learning from the same content.<p>Results - <a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;qXcVNOM" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;qXcVNOM</a><p>From the technical report
<a href="https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemini&#x2F;gemini_v1_5_report.pdf" rel="nofollow">https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemini&#x2F;gemini_...</a></div><br/><div id="39385222" class="c"><input type="checkbox" id="c-39385222" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#39384217">parent</a><span>|</span><a href="#39384885">next</a><span>|</span><label class="collapse" for="c-39385222">[-]</label><label class="expand" for="c-39385222">[4 more]</label></div><br/><div class="children"><div class="content">what if we ask it to translate an undeciphered language</div><br/><div id="39386271" class="c"><input type="checkbox" id="c-39386271" checked=""/><div class="controls bullet"><span class="by">dougmwne</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39385222">parent</a><span>|</span><a href="#39387422">next</a><span>|</span><label class="collapse" for="c-39386271">[-]</label><label class="expand" for="c-39386271">[1 more]</label></div><br/><div class="children"><div class="content">It produces basically random translations. This is covered in the 0-shot case where no translation manual was included in the context. Due to how rare this language is, itâs essentially untranslated in the training corpus.</div><br/></div></div><div id="39387422" class="c"><input type="checkbox" id="c-39387422" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39385222">parent</a><span>|</span><a href="#39386271">prev</a><span>|</span><a href="#39384885">next</a><span>|</span><label class="collapse" for="c-39387422">[-]</label><label class="expand" for="c-39387422">[2 more]</label></div><br/><div class="children"><div class="content">If you mean to dump random passages of text with no parallel corpora or grammar instructions then it won&#x27;t do better than random.<p>That said, I think that if you gave a LLM language text to predict during training, I believe that even if no parallel corpora exists during training, we could have a LLM that could still translate that language to some other language it also trained on.</div><br/><div id="39387992" class="c"><input type="checkbox" id="c-39387992" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39387422">parent</a><span>|</span><a href="#39384885">next</a><span>|</span><label class="collapse" for="c-39387992">[-]</label><label class="expand" for="c-39387992">[1 more]</label></div><br/><div class="children"><div class="content">What if we added a bunch of linguistic analysis books or something</div><br/></div></div></div></div></div></div><div id="39384885" class="c"><input type="checkbox" id="c-39384885" checked=""/><div class="controls bullet"><span class="by">poulpy123</span><span>|</span><a href="#39384217">parent</a><span>|</span><a href="#39385222">prev</a><span>|</span><a href="#39384587">next</a><span>|</span><label class="collapse" for="c-39384885">[-]</label><label class="expand" for="c-39384885">[16 more]</label></div><br/><div class="children"><div class="content">&gt; at a similar level to a person learning from the same content.<p>That&#x27;s an incredibly low bar</div><br/><div id="39386351" class="c"><input type="checkbox" id="c-39386351" checked=""/><div class="controls bullet"><span class="by">elevatedastalt</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39384885">parent</a><span>|</span><a href="#39385008">next</a><span>|</span><label class="collapse" for="c-39386351">[-]</label><label class="expand" for="c-39386351">[5 more]</label></div><br/><div class="children"><div class="content">:muffled sounds of goalposts being shifted in the distance:<p>Just a few years ago we used to clap if an NLP model could handle negation reliably or could generate even a paragraph of text in English that was natural sounding.<p>Now we are at a stage where it is basically producing reams of natural sounding text, performing surprisingly well on reasoning problems and translation of languages with barely any data despite being a markov chain on steroids, and what does it hear? &quot;That&#x27;s an incredibly low bar&quot;.</div><br/><div id="39386754" class="c"><input type="checkbox" id="c-39386754" checked=""/><div class="controls bullet"><span class="by">glenstein</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39386351">parent</a><span>|</span><a href="#39385008">next</a><span>|</span><label class="collapse" for="c-39386754">[-]</label><label class="expand" for="c-39386754">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m going to keep beating this dead horse, but if you were a philosophy nerd in the 80s, 90s, 00s etc you may know that debates RAGED over whether computers could ever, even in principle do things that are now being accomplished on a weekly basis.<p>And as you say, the goalposts keep getting moved. It used to be claimed that computers could never play chess at the highest levels because that required &quot;insight&quot;. And whatever a computer could do, it could never to that extra special thing, that could only be described in magical undefined terms.<p>I just hope there&#x27;s a moment of reckoning for decades upon decades of arguments, deemed academically respectable, that insisted that days like these would never come.</div><br/><div id="39388628" class="c"><input type="checkbox" id="c-39388628" checked=""/><div class="controls bullet"><span class="by">empath-nirvana</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39386754">parent</a><span>|</span><a href="#39387093">next</a><span>|</span><label class="collapse" for="c-39388628">[-]</label><label class="expand" for="c-39388628">[2 more]</label></div><br/><div class="children"><div class="content">Forget goalpost shifting, people frequently refuse to admit that it can do things that it obviously does, because they&#x27;ve never used it themselves.</div><br/><div id="39388954" class="c"><input type="checkbox" id="c-39388954" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39388628">parent</a><span>|</span><a href="#39387093">next</a><span>|</span><label class="collapse" for="c-39388954">[-]</label><label class="expand" for="c-39388954">[1 more]</label></div><br/><div class="children"><div class="content">Listen, you little ...</div><br/></div></div></div></div><div id="39387093" class="c"><input type="checkbox" id="c-39387093" checked=""/><div class="controls bullet"><span class="by">elevatedastalt</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39386754">parent</a><span>|</span><a href="#39388628">prev</a><span>|</span><a href="#39385008">next</a><span>|</span><label class="collapse" for="c-39387093">[-]</label><label class="expand" for="c-39387093">[1 more]</label></div><br/><div class="children"><div class="content">Honestly. I am ok with having greater and greater goals to accomplish but this sort of dismissive attitude really puts me off.</div><br/></div></div></div></div></div></div><div id="39385008" class="c"><input type="checkbox" id="c-39385008" checked=""/><div class="controls bullet"><span class="by">ithkuil</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39384885">parent</a><span>|</span><a href="#39386351">prev</a><span>|</span><a href="#39388647">next</a><span>|</span><label class="collapse" for="c-39385008">[-]</label><label class="expand" for="c-39385008">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s incredible how fast goalposts are moving.<p>The same feat one year ago would have been almost unbelievable.</div><br/></div></div><div id="39388647" class="c"><input type="checkbox" id="c-39388647" checked=""/><div class="controls bullet"><span class="by">zacmps</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39384885">parent</a><span>|</span><a href="#39385008">prev</a><span>|</span><a href="#39385036">next</a><span>|</span><label class="collapse" for="c-39388647">[-]</label><label class="expand" for="c-39388647">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The author (the human learner) has some formal experience in linguistics and has studied a variety of languages both formally and informally, though no Austronesian or Papuan languages<p>From the language benchmark (parentheses mine).</div><br/></div></div><div id="39385036" class="c"><input type="checkbox" id="c-39385036" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39384885">parent</a><span>|</span><a href="#39388647">prev</a><span>|</span><a href="#39390217">next</a><span>|</span><label class="collapse" for="c-39385036">[-]</label><label class="expand" for="c-39385036">[6 more]</label></div><br/><div class="children"><div class="content">Since when are we expecting super-human capabilities?</div><br/><div id="39385245" class="c"><input type="checkbox" id="c-39385245" checked=""/><div class="controls bullet"><span class="by">andsoitis</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39385036">parent</a><span>|</span><a href="#39391245">next</a><span>|</span><label class="collapse" for="c-39385245">[-]</label><label class="expand" for="c-39385245">[4 more]</label></div><br/><div class="children"><div class="content">And in fact it already is super human. Show me a single human who can translate amongst 10+ languages across specialized domains in the blink of an eye.</div><br/><div id="39388587" class="c"><input type="checkbox" id="c-39388587" checked=""/><div class="controls bullet"><span class="by">empath-nirvana</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39385245">parent</a><span>|</span><a href="#39393839">next</a><span>|</span><label class="collapse" for="c-39388587">[-]</label><label class="expand" for="c-39388587">[2 more]</label></div><br/><div class="children"><div class="content">Chat GPT has been super human in a lot of tasks even since 3.5.<p>People point out mistakes it makes that no human would make, but that doesn&#x27;t negate the super-human performance it has at other tasks -- and the _breadth_ of what it can do is far beyond any single person.</div><br/><div id="39388797" class="c"><input type="checkbox" id="c-39388797" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39388587">parent</a><span>|</span><a href="#39393839">next</a><span>|</span><label class="collapse" for="c-39388797">[-]</label><label class="expand" for="c-39388797">[1 more]</label></div><br/><div class="children"><div class="content">Where exactly does it have super-human performance? Above average and expert-level? Sure, I&#x27;d agree, but I haven&#x27;t experienced anything above that.</div><br/></div></div></div></div><div id="39393839" class="c"><input type="checkbox" id="c-39393839" checked=""/><div class="controls bullet"><span class="by">newzisforsukas</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39385245">parent</a><span>|</span><a href="#39388587">prev</a><span>|</span><a href="#39391245">next</a><span>|</span><label class="collapse" for="c-39393839">[-]</label><label class="expand" for="c-39393839">[1 more]</label></div><br/><div class="children"><div class="content">indeed, or a human who can analyze a hundred page text document in less than a minute and provide answers in less than a second.<p>the issue remains on accuracy. i think a human in that scenario is still more accurate with their responses, and i do not yet see that being overcome in this multi-year llm battle.</div><br/></div></div></div></div><div id="39391245" class="c"><input type="checkbox" id="c-39391245" checked=""/><div class="controls bullet"><span class="by">coffeebeqn</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39385036">parent</a><span>|</span><a href="#39385245">prev</a><span>|</span><a href="#39390217">next</a><span>|</span><label class="collapse" for="c-39391245">[-]</label><label class="expand" for="c-39391245">[1 more]</label></div><br/><div class="children"><div class="content">The model does already have superhuman ability by knowing hundreds of languages</div><br/></div></div></div></div><div id="39390217" class="c"><input type="checkbox" id="c-39390217" checked=""/><div class="controls bullet"><span class="by">JyB</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39384885">parent</a><span>|</span><a href="#39385036">prev</a><span>|</span><a href="#39393813">next</a><span>|</span><label class="collapse" for="c-39390217">[-]</label><label class="expand" for="c-39390217">[1 more]</label></div><br/><div class="children"><div class="content">Jarring you&#x27;re not adding more context to your comment.</div><br/></div></div><div id="39393813" class="c"><input type="checkbox" id="c-39393813" checked=""/><div class="controls bullet"><span class="by">newzisforsukas</span><span>|</span><a href="#39384217">root</a><span>|</span><a href="#39384885">parent</a><span>|</span><a href="#39390217">prev</a><span>|</span><a href="#39384587">next</a><span>|</span><label class="collapse" for="c-39393813">[-]</label><label class="expand" for="c-39393813">[1 more]</label></div><br/><div class="children"><div class="content">you are insane if you actually think this.</div><br/></div></div></div></div></div></div><div id="39384587" class="c"><input type="checkbox" id="c-39384587" checked=""/><div class="controls bullet"><span class="by">zippothrowaway</span><span>|</span><a href="#39384217">prev</a><span>|</span><a href="#39385175">next</a><span>|</span><label class="collapse" for="c-39384587">[-]</label><label class="expand" for="c-39384587">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve always been suspicious of any announcement from Demis Hassabis since way back in his video game days when he did a monthly article in Edge magainze about the game he was developing. &quot;Infinite Polygons&quot; became a running joke in the industry because of his obvious snake-oil. The game itself, Republic [1], was an uninteresting failure.<p>He learned how to promote himself from working for Peter &quot;Project Milo&quot; Molyneux and I see similar patterns of hype.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Republic:_The_Revolution#Marketing" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Republic:_The_Revolution#Marke...</a></div><br/><div id="39390013" class="c"><input type="checkbox" id="c-39390013" checked=""/><div class="controls bullet"><span class="by">Qwero</span><span>|</span><a href="#39384587">parent</a><span>|</span><a href="#39391096">next</a><span>|</span><label class="collapse" for="c-39390013">[-]</label><label class="expand" for="c-39390013">[1 more]</label></div><br/><div class="children"><div class="content">Funny read about his game.<p>Nonetheless while still underwhelming in comparison to gpt-4 (excluding this announcement as I haven&#x27;t tried it yet), alpha go, zero and especially fold were tremendous!</div><br/></div></div><div id="39391096" class="c"><input type="checkbox" id="c-39391096" checked=""/><div class="controls bullet"><span class="by">COAGULOPATH</span><span>|</span><a href="#39384587">parent</a><span>|</span><a href="#39390013">prev</a><span>|</span><a href="#39389710">next</a><span>|</span><label class="collapse" for="c-39391096">[-]</label><label class="expand" for="c-39391096">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, it&#x27;s funny. I used to think &quot;Demis Hassabis...where have I heard that name before?&quot; And then I realized I saw him in the manuals for old Bullfrog games.</div><br/></div></div><div id="39389710" class="c"><input type="checkbox" id="c-39389710" checked=""/><div class="controls bullet"><span class="by">pradn</span><span>|</span><a href="#39384587">parent</a><span>|</span><a href="#39391096">prev</a><span>|</span><a href="#39385175">next</a><span>|</span><label class="collapse" for="c-39389710">[-]</label><label class="expand" for="c-39389710">[1 more]</label></div><br/><div class="children"><div class="content">The line between delusional and visionary is thin! I know I&#x27;m too grounded in &quot;expected value&quot; math to do super outlier stuff like starting a video game company...</div><br/></div></div></div></div><div id="39385175" class="c"><input type="checkbox" id="c-39385175" checked=""/><div class="controls bullet"><span class="by">killthebuddha</span><span>|</span><a href="#39384587">prev</a><span>|</span><a href="#39389461">next</a><span>|</span><label class="collapse" for="c-39385175">[-]</label><label class="expand" for="c-39385175">[5 more]</label></div><br/><div class="children"><div class="content">10M tokens is an absolute game changer, especially if there&#x27;s no noticeable decay in quality with prompt size. We&#x27;re going to see things like entire domain specific languages embedded in prompts. IMO people will start thinking of the prompt itself as a sort of runtime rather than a static input.<p>Back when OpenAI still supported raw text completion with text-davinci-003 I spent some time experimenting with tiny prompt-embedded DSLs. The results were very, very, interesting IMO. In a lot of ways, text-davinci-003 with embedded functions still feels to me like the &quot;smartest&quot; language model I&#x27;ve ever interacted with.<p>I&#x27;m not sure how close we are to &quot;superintelligence&quot; but for baseline general intelligence we very well could have already made the prerequisite technological breakthroughs.</div><br/><div id="39386096" class="c"><input type="checkbox" id="c-39386096" checked=""/><div class="controls bullet"><span class="by">empath-nirvana</span><span>|</span><a href="#39385175">parent</a><span>|</span><a href="#39389461">next</a><span>|</span><label class="collapse" for="c-39386096">[-]</label><label class="expand" for="c-39386096">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty slow, though looks like up to 60 seconds for some of the answers, and uses god knows how much compute, so there&#x27;s probably going to be some trade offs -- you&#x27;re going to want to make sure that that much context is actually useful for what you want.</div><br/><div id="39389129" class="c"><input type="checkbox" id="c-39389129" checked=""/><div class="controls bullet"><span class="by">drusepth</span><span>|</span><a href="#39385175">root</a><span>|</span><a href="#39386096">parent</a><span>|</span><a href="#39389461">next</a><span>|</span><label class="collapse" for="c-39389129">[-]</label><label class="expand" for="c-39389129">[3 more]</label></div><br/><div class="children"><div class="content">TBF: when talking about the first &quot;superintelligence&quot;, I&#x27;d expect it to take unreasonable amounts of compute and&#x2F;or be slow -- that can always be optimized. Bringing it into existence in the first place is the hardest part.</div><br/><div id="39390266" class="c"><input type="checkbox" id="c-39390266" checked=""/><div class="controls bullet"><span class="by">unshavedyak</span><span>|</span><a href="#39385175">root</a><span>|</span><a href="#39389129">parent</a><span>|</span><a href="#39389461">next</a><span>|</span><label class="collapse" for="c-39390266">[-]</label><label class="expand" for="c-39390266">[2 more]</label></div><br/><div class="children"><div class="content">Yea. Of course for some tasks we need speed, but i&#x27;ve been kinda surprised that we haven&#x27;t seen very slow models which perform far better than faster models. We&#x27;re treading new territory, and everyone seems to make models that are &quot;fast enough&quot;.<p>I wanna see how far this tech can scale, regardless of speed. I don&#x27;t care if it takes 24h to formulate a response. Are there &quot;easy&quot; variables which drastically improve output?<p>I suspect not. I imagine people have tried that. Though i&#x27;m still curious as to why.</div><br/><div id="39393555" class="c"><input type="checkbox" id="c-39393555" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#39385175">root</a><span>|</span><a href="#39390266">parent</a><span>|</span><a href="#39389461">next</a><span>|</span><label class="collapse" for="c-39393555">[-]</label><label class="expand" for="c-39393555">[1 more]</label></div><br/><div class="children"><div class="content">I think the problem is that 24 hours of compute to run a response would be incredibly expensive. I mean hell how would that even be trained.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39389461" class="c"><input type="checkbox" id="c-39389461" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#39385175">prev</a><span>|</span><a href="#39384034">next</a><span>|</span><label class="collapse" for="c-39389461">[-]</label><label class="expand" for="c-39389461">[4 more]</label></div><br/><div class="children"><div class="content">I gotta say, I&#x27;ve been trying out Gemini recently and it&#x27;s embarrassingly bad. I can&#x27;t take anything google puts out seriously when their current offerings are so so much worse than ChatGPT (or even local llama!).<p>As a particularly egregious example, yesterday night I gave Gemini a list of drinks and other cocktail ingredients I had laying around and asked for some recommendations for cute drinks that I could make. It&#x27;s response:<p>&gt; I&#x27;m just a language model, so I can&#x27;t help you with that.<p>ChatGPT 3.5 came up with several delicious options with clear instructions, but it&#x27;s not just this instance, I&#x27;ve NEVER gotten a response from Gemini that I even felt was <i>more useful</i> than just a freaking bing search! Much less better than ChatGPT. I&#x27;m just going to assume they&#x27;re using cherrypicked metrics to make themselves feel better until proven otherwise. I have zero confidence in Google&#x27;s AI plays, and I assume all their competent talent is now at OpenAI or Anthropic.</div><br/><div id="39394429" class="c"><input type="checkbox" id="c-39394429" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#39389461">parent</a><span>|</span><a href="#39392327">next</a><span>|</span><label class="collapse" for="c-39394429">[-]</label><label class="expand" for="c-39394429">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think &quot;I&#x27;m just a language model, I can&#x27;t help you with that&quot; comes from Gemini. Google has a seperate censorship model that blocks you from receiving Gemini&#x27;s response in certain situations.<p>When Gemeni (Ultra) refuses to do something itself it is more verbose and specific as to why it won&#x27;t do it, it my experience.</div><br/></div></div><div id="39392327" class="c"><input type="checkbox" id="c-39392327" checked=""/><div class="controls bullet"><span class="by">samspenc</span><span>|</span><a href="#39389461">parent</a><span>|</span><a href="#39394429">prev</a><span>|</span><a href="#39384034">next</a><span>|</span><label class="collapse" for="c-39392327">[-]</label><label class="expand" for="c-39392327">[2 more]</label></div><br/><div class="children"><div class="content">My experiences are similar, but I think we are talking about the Gemini free model, available on the Google Gemini website. I think the rest of the comments are saying the paid versions (Pro &#x2F; Ultra) are significantly better, though I haven&#x27;t tested it myself to compare.</div><br/><div id="39394625" class="c"><input type="checkbox" id="c-39394625" checked=""/><div class="controls bullet"><span class="by">shellfishgene</span><span>|</span><a href="#39389461">root</a><span>|</span><a href="#39392327">parent</a><span>|</span><a href="#39384034">next</a><span>|</span><label class="collapse" for="c-39394625">[-]</label><label class="expand" for="c-39394625">[1 more]</label></div><br/><div class="children"><div class="content">I have the 2 months trial for the paid version, and find myself going back to free ChatGPT often. Gemini loves to put everything in bullet point lists and short paragraphs with subheadings for example, even when asking for a letter. I&#x27;m not a heavy user, but it seems to not quite get what I want often.
Not important but annoying: It starts almost every answer with &quot;Absolutely!&quot;, even when it doesn&#x27;t match the question (e.g. &quot;How does x work?&quot;).</div><br/></div></div></div></div></div></div><div id="39384034" class="c"><input type="checkbox" id="c-39384034" checked=""/><div class="controls bullet"><span class="by">losvedir</span><span>|</span><a href="#39389461">prev</a><span>|</span><a href="#39384699">next</a><span>|</span><label class="collapse" for="c-39384034">[-]</label><label class="expand" for="c-39384034">[7 more]</label></div><br/><div class="children"><div class="content">If I understand correctly, they&#x27;re releasing this for Pro but not Ultra, which I think is akin to GPT 3.5 vs 4? Sigh, the naming is confusing...<p>But my main takeaway is the huge context window! Up to a million, with more than 100k tokens right now? Even just GPT 3.5 level prediction with such a huge context window opens up a lot of interesting capabilities. RAG can be super powerful with that much to work with.</div><br/><div id="39385667" class="c"><input type="checkbox" id="c-39385667" checked=""/><div class="controls bullet"><span class="by">cchance</span><span>|</span><a href="#39384034">parent</a><span>|</span><a href="#39384371">next</a><span>|</span><label class="collapse" for="c-39385667">[-]</label><label class="expand" for="c-39385667">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s sizes<p>Nano&#x2F;Pro&#x2F;Ultra are model SIZES. 
1.0&#x2F;1.5 is generations of the architecture.</div><br/></div></div><div id="39384371" class="c"><input type="checkbox" id="c-39384371" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#39384034">parent</a><span>|</span><a href="#39385667">prev</a><span>|</span><a href="#39388972">next</a><span>|</span><label class="collapse" for="c-39384371">[-]</label><label class="expand" for="c-39384371">[3 more]</label></div><br/><div class="children"><div class="content">The announcement suggests that 1.5 Pro is similar to 1.0 Ultra.</div><br/><div id="39384497" class="c"><input type="checkbox" id="c-39384497" checked=""/><div class="controls bullet"><span class="by">benopal64</span><span>|</span><a href="#39384034">root</a><span>|</span><a href="#39384371">parent</a><span>|</span><a href="#39388972">next</a><span>|</span><label class="collapse" for="c-39384497">[-]</label><label class="expand" for="c-39384497">[2 more]</label></div><br/><div class="children"><div class="content">I am reaching a bit, however, I think its a bit of a marketing technique. The Pro 1.5 being compared to the Ultra 1.0 model seems to imply that they will be releasing a Ultra 1.5 model which will presumably have similar characteristics to the new Pro 1.5 model (MOE architecture w&#x2F; a huge context window).</div><br/><div id="39384612" class="c"><input type="checkbox" id="c-39384612" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#39384034">root</a><span>|</span><a href="#39384497">parent</a><span>|</span><a href="#39388972">next</a><span>|</span><label class="collapse" for="c-39384612">[-]</label><label class="expand" for="c-39384612">[1 more]</label></div><br/><div class="children"><div class="content">Apparently the technical report implies that Ultra 1.5 is a step-up again, I&#x27;m not sure it&#x27;s just context length, that seems to be orthogonal in everything I&#x27;ve read so far.</div><br/></div></div></div></div></div></div><div id="39388972" class="c"><input type="checkbox" id="c-39388972" checked=""/><div class="controls bullet"><span class="by">amf12</span><span>|</span><a href="#39384034">parent</a><span>|</span><a href="#39384371">prev</a><span>|</span><a href="#39385653">next</a><span>|</span><label class="collapse" for="c-39388972">[-]</label><label class="expand" for="c-39388972">[1 more]</label></div><br/><div class="children"><div class="content">Maybe this analogy would help: iPhone 15, iPhone Pro 15, iPhone Pro Max 15 and then iPhone Pro 15.5</div><br/></div></div><div id="39385653" class="c"><input type="checkbox" id="c-39385653" checked=""/><div class="controls bullet"><span class="by">ygouzerh</span><span>|</span><a href="#39384034">parent</a><span>|</span><a href="#39388972">prev</a><span>|</span><a href="#39384699">next</a><span>|</span><label class="collapse" for="c-39385653">[-]</label><label class="expand" for="c-39385653">[1 more]</label></div><br/><div class="children"><div class="content">So Pro and Ultra are from my understanding link to the number of parameters. More parameters means more reasonning capabilities, but more compute needed.<p>So Pro is like the light and fast version and Ultra the advanced and expensive one.</div><br/></div></div></div></div><div id="39384699" class="c"><input type="checkbox" id="c-39384699" checked=""/><div class="controls bullet"><span class="by">sonium</span><span>|</span><a href="#39384034">prev</a><span>|</span><a href="#39384326">next</a><span>|</span><label class="collapse" for="c-39384699">[-]</label><label class="expand" for="c-39384699">[6 more]</label></div><br/><div class="children"><div class="content">I just watched the demo with the Apollo 11 transcript. (sidenote: maybe Gemini is named after the space program?).<p>Wouldn&#x27;t the transcript or at least a timeline of Apollo 11 be part of the training corpus?
So even without the 400 pages in the context window just given the drawing I would assume a prompt like &quot;In the context of Apoll 11, what moment does the drawing refer to?&quot; would yield the same result.</div><br/><div id="39385253" class="c"><input type="checkbox" id="c-39385253" checked=""/><div class="controls bullet"><span class="by">technics256</span><span>|</span><a href="#39384699">parent</a><span>|</span><a href="#39386335">next</a><span>|</span><label class="collapse" for="c-39385253">[-]</label><label class="expand" for="c-39385253">[1 more]</label></div><br/><div class="children"><div class="content">Gemini is named that way because of the collaboration between Google brain and deep mind</div><br/></div></div><div id="39386335" class="c"><input type="checkbox" id="c-39386335" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#39384699">parent</a><span>|</span><a href="#39385253">prev</a><span>|</span><a href="#39387365">next</a><span>|</span><label class="collapse" for="c-39386335">[-]</label><label class="expand" for="c-39386335">[1 more]</label></div><br/><div class="children"><div class="content">Correct except that it spits out the timestamp</div><br/></div></div><div id="39387365" class="c"><input type="checkbox" id="c-39387365" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#39384699">parent</a><span>|</span><a href="#39386335">prev</a><span>|</span><a href="#39388720">next</a><span>|</span><label class="collapse" for="c-39387365">[-]</label><label class="expand" for="c-39387365">[2 more]</label></div><br/><div class="children"><div class="content">Gemini is named after the spacecraft that put the second person into orbit - pretty aptly named, but not sure if this was the intention.</div><br/><div id="39394343" class="c"><input type="checkbox" id="c-39394343" checked=""/><div class="controls bullet"><span class="by">DrNosferatu</span><span>|</span><a href="#39384699">root</a><span>|</span><a href="#39387365">parent</a><span>|</span><a href="#39388720">next</a><span>|</span><label class="collapse" for="c-39394343">[-]</label><label class="expand" for="c-39394343">[1 more]</label></div><br/><div class="children"><div class="content">Google needs their Apollo.</div><br/></div></div></div></div><div id="39388720" class="c"><input type="checkbox" id="c-39388720" checked=""/><div class="controls bullet"><span class="by">empath-nirvana</span><span>|</span><a href="#39384699">parent</a><span>|</span><a href="#39387365">prev</a><span>|</span><a href="#39384326">next</a><span>|</span><label class="collapse" for="c-39388720">[-]</label><label class="expand" for="c-39388720">[1 more]</label></div><br/><div class="children"><div class="content">i asked chatgpt4 to identify three humorous moments in the apollo 11 transcript and it hallucinated all 3 of them (i think -- i can&#x27;t find what it&#x27;s referring to).  Presumably it&#x27;s in it&#x27;s corpus, too.<p>&gt; The &quot;Snoopy&quot; Moment:
During the mission, the crew had a small, black-and-white cartoon Snoopy doll as a semi-official mascot, representing safety and mission success. At one point, Collins joked about &quot;Snoopy&quot; floating into his view in the spacecraft, which was a light moment reflecting the camaraderie and the use of humor to ease the intense focus required for their mission.<p>The &quot;Biohazard&quot; Joke:
After the successful moon landing and upon preparing for re-entry into Earth&#x27;s atmosphere, the crew humorously discussed among themselves the potential of being quarantined back on Earth due to unknown lunar pathogens. They joked about the extensive debriefing they&#x27;d have to go through and the possibility of being a biohazard. This was a light-hearted take on the serious precautions NASA was taking to prevent the hypothetical contamination of Earth with lunar microbes.<p>The &quot;Mailbox&quot; Comment:
In the midst of their groundbreaking mission, there was an exchange where one of the astronauts joked about expecting to find a mailbox on the Moon, or asking where they should leave a package, playing on the surreal experience of being on the lunar surface, far from the ordinary elements of Earthly life. This comment highlighted the astronauts&#x27; ability to find humor in the extraordinary circumstances of their journey.</div><br/></div></div></div></div><div id="39384326" class="c"><input type="checkbox" id="c-39384326" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#39384699">prev</a><span>|</span><a href="#39384461">next</a><span>|</span><label class="collapse" for="c-39384326">[-]</label><label class="expand" for="c-39384326">[12 more]</label></div><br/><div class="children"><div class="content">The context window size - if it really works as advertised - is pretty ground-breaking. It would replace the need to RAG or fine tune for one-off (or few-off) analys{is,es} of input streams cheaper and faster. I wonder how they got past the input token stuffing problems everyone else runs into.</div><br/><div id="39385134" class="c"><input type="checkbox" id="c-39385134" checked=""/><div class="controls bullet"><span class="by">lumost</span><span>|</span><a href="#39384326">parent</a><span>|</span><a href="#39386836">next</a><span>|</span><label class="collapse" for="c-39385134">[-]</label><label class="expand" for="c-39385134">[4 more]</label></div><br/><div class="children"><div class="content">They are almost certainly using some form of sparse attention. If you linearize the attention operation, you can scale up to around 1-10M tokens depending on hardware before hitting memory constraints. Linearization works off the assumption that for a subsequence of X tokens out M tokens, where M os much greater than X there are likely only K tokens which are useful for the attention operation.<p>There are a bunch of techniques to do this, but it&#x27;s unclear how well any of them scale.</div><br/><div id="39386022" class="c"><input type="checkbox" id="c-39386022" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39384326">root</a><span>|</span><a href="#39385134">parent</a><span>|</span><a href="#39386836">next</a><span>|</span><label class="collapse" for="c-39386022">[-]</label><label class="expand" for="c-39386022">[3 more]</label></div><br/><div class="children"><div class="content">Not &quot;almost&quot;, but certainly. Dense attention is quadratic, not even Google would be able to run it at an acceptable speed. Their model is not recurrent - they did not have the time yet (or resources - believe it or not, Google of 2023-24 is very compute constrained) to train newer SSM or recurrent based models at practical parameter counts. Then there&#x27;s the fact that those models are far harder to train due to instabilities, which is one of the reasons why you don&#x27;t yet see FOSS recurrent&#x2F;SSM models that are SOTA at their size or tokens&#x2F;sec. With sparse attention, however, long context recall will be far from perfect, and the longer the context the worse the recall. That&#x27;s better than no recall at all (as in a fully dense attention model which will simply lop off the preceding parts of the conversation), but not by a hell of a lot.</div><br/><div id="39390886" class="c"><input type="checkbox" id="c-39390886" checked=""/><div class="controls bullet"><span class="by">kiraaa</span><span>|</span><a href="#39384326">root</a><span>|</span><a href="#39386022">parent</a><span>|</span><a href="#39386836">next</a><span>|</span><label class="collapse" for="c-39390886">[-]</label><label class="expand" for="c-39390886">[2 more]</label></div><br/><div class="children"><div class="content">maybe they are using ring attention, on top of their 128k model.</div><br/><div id="39392313" class="c"><input type="checkbox" id="c-39392313" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39384326">root</a><span>|</span><a href="#39390886">parent</a><span>|</span><a href="#39386836">next</a><span>|</span><label class="collapse" for="c-39392313">[-]</label><label class="expand" for="c-39392313">[1 more]</label></div><br/><div class="children"><div class="content">More likely some clever take on RAG. Thereâs no way that 1M context is all available at all times. More likely parts of it are retrievable on demand. Hence the retrieval-like use cases you see in the demos. The goal is to find a thing, not to find patterns at a distance</div><br/></div></div></div></div></div></div></div></div><div id="39386836" class="c"><input type="checkbox" id="c-39386836" checked=""/><div class="controls bullet"><span class="by">nbardy</span><span>|</span><a href="#39384326">parent</a><span>|</span><a href="#39385134">prev</a><span>|</span><a href="#39387287">next</a><span>|</span><label class="collapse" for="c-39386836">[-]</label><label class="expand" for="c-39386836">[1 more]</label></div><br/><div class="children"><div class="content">RAG will stick around, at some point you want to retrieve grounded information samples to inject in the context window. RAG+long context just gives you more room for grounded context.<p>Think building huge relevant context on topics before answering.</div><br/></div></div><div id="39387287" class="c"><input type="checkbox" id="c-39387287" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#39384326">parent</a><span>|</span><a href="#39386836">prev</a><span>|</span><a href="#39386455">next</a><span>|</span><label class="collapse" for="c-39387287">[-]</label><label class="expand" for="c-39387287">[1 more]</label></div><br/><div class="children"><div class="content">Tbh, I haven&#x27;t read the paper, but I think it&#x27;s pretty self-evident that large contexts aren&#x27;t cheap - the AI has to comb through every word of the context for each successive generated token at least once, so it&#x27;s going to be at least linear.</div><br/></div></div><div id="39386455" class="c"><input type="checkbox" id="c-39386455" checked=""/><div class="controls bullet"><span class="by">popinman322</span><span>|</span><a href="#39384326">parent</a><span>|</span><a href="#39387287">prev</a><span>|</span><a href="#39384887">next</a><span>|</span><label class="collapse" for="c-39386455">[-]</label><label class="expand" for="c-39386455">[1 more]</label></div><br/><div class="children"><div class="content">vs RAG: RAG is good for searching across &gt;billions of tokens and providing up-to-date information to a static model. Even with huge context lengths it&#x27;s a good idea to submit high quality inputs to prevent the model from going off on tangents, getting stuck on contradictory information, etc..<p>vs fine tuning: smaller, fine-tuned models can perform better than huge models in a decent number of tasks. Not strictly fine-tuning, but for throughput limited tasks it&#x27;ll likely still be better to prune a 70B model down to 2B, keeping only the components you need for accurate inference.<p>I can see this model being good for taking huge inputs and compressing them down for smaller models to use.</div><br/></div></div><div id="39384887" class="c"><input type="checkbox" id="c-39384887" checked=""/><div class="controls bullet"><span class="by">jcuenod</span><span>|</span><a href="#39384326">parent</a><span>|</span><a href="#39386455">prev</a><span>|</span><a href="#39390556">next</a><span>|</span><label class="collapse" for="c-39384887">[-]</label><label class="expand" for="c-39384887">[3 more]</label></div><br/><div class="children"><div class="content">It won&#x27;t remove the use of RAG at all. That&#x27;s like saying, &quot;wow, now that I&#x27;ve upgraded my 128GB HDD to 1TB, I&#x27;ll never run out of space again.&quot;</div><br/><div id="39385058" class="c"><input type="checkbox" id="c-39385058" checked=""/><div class="controls bullet"><span class="by">madisonmay</span><span>|</span><a href="#39384326">root</a><span>|</span><a href="#39384887">parent</a><span>|</span><a href="#39385910">next</a><span>|</span><label class="collapse" for="c-39385058">[-]</label><label class="expand" for="c-39385058">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more like saying &quot;I&#x27;ve upgraded to 128GB of RAM, I&#x27;ll never use my disk again&quot;.</div><br/></div></div><div id="39385910" class="c"><input type="checkbox" id="c-39385910" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#39384326">root</a><span>|</span><a href="#39384887">parent</a><span>|</span><a href="#39385058">prev</a><span>|</span><a href="#39390556">next</a><span>|</span><label class="collapse" for="c-39385910">[-]</label><label class="expand" for="c-39385910">[1 more]</label></div><br/><div class="children"><div class="content">10 TB for an accurate proportion.<p>And I think people who buy a laptop with a 1TB SSD generally don&#x27;t run out of space, at least I don&#x27;t.</div><br/></div></div></div></div><div id="39390556" class="c"><input type="checkbox" id="c-39390556" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#39384326">parent</a><span>|</span><a href="#39384887">prev</a><span>|</span><a href="#39384461">next</a><span>|</span><label class="collapse" for="c-39390556">[-]</label><label class="expand" for="c-39390556">[1 more]</label></div><br/><div class="children"><div class="content">Saw testing earlier that suggested the context does indeed work right</div><br/></div></div></div></div><div id="39384461" class="c"><input type="checkbox" id="c-39384461" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#39384326">prev</a><span>|</span><a href="#39385291">next</a><span>|</span><label class="collapse" for="c-39384461">[-]</label><label class="expand" for="c-39384461">[2 more]</label></div><br/><div class="children"><div class="content">This is the first time I&#x27;ve been legitimately impressed by one of Google&#x27;s LLMs (with the obvious caveat that I&#x27;m taking the results reported in their tech report at face value).</div><br/><div id="39392096" class="c"><input type="checkbox" id="c-39392096" checked=""/><div class="controls bullet"><span class="by">replwoacause</span><span>|</span><a href="#39384461">parent</a><span>|</span><a href="#39385291">next</a><span>|</span><label class="collapse" for="c-39392096">[-]</label><label class="expand" for="c-39392096">[1 more]</label></div><br/><div class="children"><div class="content">Itâs just marketing at this point, nothing to be impressed by. Itâs a mistake to take at face value.</div><br/></div></div></div></div><div id="39385291" class="c"><input type="checkbox" id="c-39385291" checked=""/><div class="controls bullet"><span class="by">Yusefmosiah</span><span>|</span><a href="#39384461">prev</a><span>|</span><a href="#39387105">next</a><span>|</span><label class="collapse" for="c-39385291">[-]</label><label class="expand" for="c-39385291">[3 more]</label></div><br/><div class="children"><div class="content">I see a lot of talk about retrieval over long context. Some even think this replaces RAG.<p>I don&#x27;t care if the model can tell me which page in the book or which code file has a particular concept. RAG already does this. I want the model to notice how a concept is distributed throughout a text, and be able to connect, compare, contrast, synthesize, and understand all the ways that a book touches on a theme, or to rewrite multiple code files in one pass, without introducing bugs.<p>How does Gemini 1.5&#x27;s reasoning compare to GPT-4? GPT-4 already has superhuman memory; its bottleneck is its relatively weak reasoning.</div><br/><div id="39386366" class="c"><input type="checkbox" id="c-39386366" checked=""/><div class="controls bullet"><span class="by">sinuhe69</span><span>|</span><a href="#39385291">parent</a><span>|</span><a href="#39389621">next</a><span>|</span><label class="collapse" for="c-39386366">[-]</label><label class="expand" for="c-39386366">[1 more]</label></div><br/><div class="children"><div class="content">In my experience (I work mostly and deeply with Bard&#x2F;Gemini), the reasoning capability of Gemini is quite good. Gemini Pro is already much better than ChatGPT 3.5, but they still make quite a few mistakes along the way. What is more worrying is that when these models made mistakes, they tried really hard to justify their reasoning (errors), practically misleading the users. Because of their high mimicry ability, users really have to pay attention to validate and eventually spot the errors. Of course, this is still far below the human level, so I&#x27;m not sure whether they add value or are more of a burden.</div><br/></div></div><div id="39389621" class="c"><input type="checkbox" id="c-39389621" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#39385291">parent</a><span>|</span><a href="#39386366">prev</a><span>|</span><a href="#39387105">next</a><span>|</span><label class="collapse" for="c-39389621">[-]</label><label class="expand" for="c-39389621">[1 more]</label></div><br/><div class="children"><div class="content">The most impressive demonstration of long context is this in my opinion,<p><a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;qXcVNOM" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;qXcVNOM</a><p>Testing language translation abilities of an extremely obscure language after passing in one grammar book as context.</div><br/></div></div></div></div><div id="39387105" class="c"><input type="checkbox" id="c-39387105" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#39385291">prev</a><span>|</span><a href="#39384428">next</a><span>|</span><label class="collapse" for="c-39387105">[-]</label><label class="expand" for="c-39387105">[8 more]</label></div><br/><div class="children"><div class="content">The long context length is of course incredible, but I&#x27;m more shocked that the <i>Pro</i> model is now on par with Ultra (~GPT-4, at least the original release). That implies when they release 1.5 Ultra, we&#x27;ll finally have a GPT-4 killer. And assuming that 1.5 Pro is priced similarly to the current Pro, that&#x27;s a 4x price advantage per-token.<p>Not surprising that OpenAI shipped a blog post today about their video generation â I think they&#x27;re feeling considerable heat right now.</div><br/><div id="39387493" class="c"><input type="checkbox" id="c-39387493" checked=""/><div class="controls bullet"><span class="by">topicseed</span><span>|</span><a href="#39387105">parent</a><span>|</span><a href="#39391031">next</a><span>|</span><label class="collapse" for="c-39387493">[-]</label><label class="expand" for="c-39387493">[3 more]</label></div><br/><div class="children"><div class="content">Gemini 1 Ultra was also said to be on par with ChatGPT 4 and it&#x27;s not really there so let&#x27;s see for ourselves when we can get our hands on it.</div><br/><div id="39387873" class="c"><input type="checkbox" id="c-39387873" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#39387105">root</a><span>|</span><a href="#39387493">parent</a><span>|</span><a href="#39391031">next</a><span>|</span><label class="collapse" for="c-39387873">[-]</label><label class="expand" for="c-39387873">[2 more]</label></div><br/><div class="children"><div class="content">Ultra benchmarked around the original release of GPT-4, not the current model. My understanding is that was fairly accurate â it&#x27;s close to current GPT-4 but not quite equal. However, close-to-GPT-4 but 4x cheaper and 10x context length would be very impressive and IMO useful.</div><br/><div id="39393464" class="c"><input type="checkbox" id="c-39393464" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39387105">root</a><span>|</span><a href="#39387873">parent</a><span>|</span><a href="#39391031">next</a><span>|</span><label class="collapse" for="c-39393464">[-]</label><label class="expand" for="c-39393464">[1 more]</label></div><br/><div class="children"><div class="content">No, it benchmarked around the original release of GPT-4 <i>given 32 attempts</i> versus GPT-4&#x27;s 5.</div><br/></div></div></div></div></div></div><div id="39391031" class="c"><input type="checkbox" id="c-39391031" checked=""/><div class="controls bullet"><span class="by">tigershark</span><span>|</span><a href="#39387105">parent</a><span>|</span><a href="#39387493">prev</a><span>|</span><a href="#39384428">next</a><span>|</span><label class="collapse" for="c-39391031">[-]</label><label class="expand" for="c-39391031">[4 more]</label></div><br/><div class="children"><div class="content">Feeling the heat? Did you actually watch the videos?
That was a huge leap forward compared to anything existing at the moment.
Order of magnitudes away from a blog post discussing a model that maybe will finally be on par with chat gtp 4...</div><br/><div id="39393463" class="c"><input type="checkbox" id="c-39393463" checked=""/><div class="controls bullet"><span class="by">mupuff1234</span><span>|</span><a href="#39387105">root</a><span>|</span><a href="#39391031">parent</a><span>|</span><a href="#39384428">next</a><span>|</span><label class="collapse" for="c-39393463">[-]</label><label class="expand" for="c-39393463">[3 more]</label></div><br/><div class="children"><div class="content">The openai announcement is also more or less a blog post, isn&#x27;t it?<p>Do we know how much time or money does it take to create a movie clip?</div><br/><div id="39394418" class="c"><input type="checkbox" id="c-39394418" checked=""/><div class="controls bullet"><span class="by">tigershark</span><span>|</span><a href="#39387105">root</a><span>|</span><a href="#39393463">parent</a><span>|</span><a href="#39384428">next</a><span>|</span><label class="collapse" for="c-39394418">[-]</label><label class="expand" for="c-39394418">[2 more]</label></div><br/><div class="children"><div class="content">There was Sam Altman taking live prompt requests on twitter and generating videos.
They were not the same quality as some of the ones in the website, but they were still incredibly impressive.</div><br/><div id="39394470" class="c"><input type="checkbox" id="c-39394470" checked=""/><div class="controls bullet"><span class="by">mupuff1234</span><span>|</span><a href="#39387105">root</a><span>|</span><a href="#39394418">parent</a><span>|</span><a href="#39384428">next</a><span>|</span><label class="collapse" for="c-39394470">[-]</label><label class="expand" for="c-39394470">[1 more]</label></div><br/><div class="children"><div class="content">And how much compute were those requests using?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39384428" class="c"><input type="checkbox" id="c-39384428" checked=""/><div class="controls bullet"><span class="by">Alifatisk</span><span>|</span><a href="#39387105">prev</a><span>|</span><a href="#39385026">next</a><span>|</span><label class="collapse" for="c-39384428">[-]</label><label class="expand" for="c-39384428">[2 more]</label></div><br/><div class="children"><div class="content">I remember one of the biggest advantages with Google Bard was the heavily limited context window. I am glad Google is now actually delivering some exciting news now with Gemini and this gigantic token size.<p>Sure it&#x27;s a bummer that they slap the &quot;Join the waiting list&quot;, but it&#x27;s still interesting to read about their progress and competing with ClosedAi (OpenAi).<p>One last thing I hope they fix is the heavily morally and ethically guardrail, sometimes I can barely ask proper questions without it triggering Gemini to educate me about what&#x27;s right and wrong. And when I try the same prompt with ChatGPT and Bing ai, they happily answer.</div><br/><div id="39389711" class="c"><input type="checkbox" id="c-39389711" checked=""/><div class="controls bullet"><span class="by">elevatedastalt</span><span>|</span><a href="#39384428">parent</a><span>|</span><a href="#39385026">next</a><span>|</span><label class="collapse" for="c-39389711">[-]</label><label class="expand" for="c-39389711">[1 more]</label></div><br/><div class="children"><div class="content">&quot;biggest advantages with Google Bard&quot;<p>Did you mean disadvantages?</div><br/></div></div></div></div><div id="39385026" class="c"><input type="checkbox" id="c-39385026" checked=""/><div class="controls bullet"><span class="by">aubanel</span><span>|</span><a href="#39384428">prev</a><span>|</span><a href="#39383870">next</a><span>|</span><label class="collapse" for="c-39385026">[-]</label><label class="expand" for="c-39385026">[1 more]</label></div><br/><div class="children"><div class="content">For reference, here is the technical report: <a href="https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemini&#x2F;gemini_v1_5_report.pdf" rel="nofollow">https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemini&#x2F;gemini_...</a></div><br/></div></div><div id="39383870" class="c"><input type="checkbox" id="c-39383870" checked=""/><div class="controls bullet"><span class="by">prakhar897</span><span>|</span><a href="#39385026">prev</a><span>|</span><a href="#39384939">next</a><span>|</span><label class="collapse" for="c-39383870">[-]</label><label class="expand" for="c-39383870">[9 more]</label></div><br/><div class="children"><div class="content">Can anyone explain how context length is tested? Do they prompt something like:<p>&quot;Remember val=&quot;XXXX&quot; .........10M tokens later....... Print val&quot;</div><br/><div id="39384422" class="c"><input type="checkbox" id="c-39384422" checked=""/><div class="controls bullet"><span class="by">halflings</span><span>|</span><a href="#39383870">parent</a><span>|</span><a href="#39385715">next</a><span>|</span><label class="collapse" for="c-39384422">[-]</label><label class="expand" for="c-39384422">[1 more]</label></div><br/><div class="children"><div class="content">Yep that&#x27;s pretty much it! That&#x27;s what they call needle in a haystack.
See:
<a href="https:&#x2F;&#x2F;github.com&#x2F;gkamradt&#x2F;LLMTest_NeedleInAHaystack">https:&#x2F;&#x2F;github.com&#x2F;gkamradt&#x2F;LLMTest_NeedleInAHaystack</a></div><br/></div></div><div id="39385715" class="c"><input type="checkbox" id="c-39385715" checked=""/><div class="controls bullet"><span class="by">cchance</span><span>|</span><a href="#39383870">parent</a><span>|</span><a href="#39384422">prev</a><span>|</span><a href="#39383945">next</a><span>|</span><label class="collapse" for="c-39385715">[-]</label><label class="expand" for="c-39385715">[5 more]</label></div><br/><div class="children"><div class="content">yep they hide things throughout the prompt and then ask it about that specific thing, imagine hiding passwords in a giant block of text and then being like, what was bobs password 10 million tokens later.<p>According to this it&#x27;s remembering with 99% accuracy, which if you think about it is NUTS, can you imagine reading a 22x 1000 page books, and remembering every single word that was said with 100% accuracy lol</div><br/><div id="39386319" class="c"><input type="checkbox" id="c-39386319" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#39383870">root</a><span>|</span><a href="#39385715">parent</a><span>|</span><a href="#39383945">next</a><span>|</span><label class="collapse" for="c-39386319">[-]</label><label class="expand" for="c-39386319">[4 more]</label></div><br/><div class="children"><div class="content">Interestingly, there&#x27;s a decent chance I&#x27;d remember if there was an out of context passage saying &quot;the password is FooBar&quot;. I wonder if it would be better to test with minor edits? E.g., &quot;what color shirt was X wearing when...&quot;</div><br/><div id="39393687" class="c"><input type="checkbox" id="c-39393687" checked=""/><div class="controls bullet"><span class="by">kenjackson</span><span>|</span><a href="#39383870">root</a><span>|</span><a href="#39386319">parent</a><span>|</span><a href="#39392288">next</a><span>|</span><label class="collapse" for="c-39393687">[-]</label><label class="expand" for="c-39393687">[1 more]</label></div><br/><div class="children"><div class="content">I think instead you could just do a full doc of relationships.  &quot;Tina and Chris have five children named ...&quot;<p>Then you can ask it who is Tina&#x27;s (great)^57 grandmother&#x27;s twice removed cousin on her father&#x27;s side?<p>It would have to be able to remember the context of the relationships up and down the document and there&#x27;d be nothing to key into as you could ask about any relationship.</div><br/></div></div><div id="39392288" class="c"><input type="checkbox" id="c-39392288" checked=""/><div class="controls bullet"><span class="by">ambichook</span><span>|</span><a href="#39383870">root</a><span>|</span><a href="#39386319">parent</a><span>|</span><a href="#39393687">prev</a><span>|</span><a href="#39383945">next</a><span>|</span><label class="collapse" for="c-39392288">[-]</label><label class="expand" for="c-39392288">[2 more]</label></div><br/><div class="children"><div class="content">i feel you would recognise that more as a quirk of how humans think, remember that LLMs think fundamentally differently to you and i. i would be curious about someone making a benchmark like that and using it to compare as an experiment however</div><br/><div id="39393012" class="c"><input type="checkbox" id="c-39393012" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#39383870">root</a><span>|</span><a href="#39392288">parent</a><span>|</span><a href="#39383945">next</a><span>|</span><label class="collapse" for="c-39393012">[-]</label><label class="expand" for="c-39393012">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not trying to anthropomorphize the model, but it&#x27;s not hard to imagine that a model would attribute significance to something completely out of context, and hence &quot;focus&quot; on it when computing attention.<p>Another possible synthetic benchmark would be to present a list of key value pairs and then ask it for the value corresponding to different keys. Or present a long list of distinct facts and then ask it about them. This latter one could probably be sourced from something like a trivia question and answers data set. I bet there&#x27;s something like that from Jeopardy.</div><br/></div></div></div></div></div></div></div></div><div id="39383945" class="c"><input type="checkbox" id="c-39383945" checked=""/><div class="controls bullet"><span class="by">NhanH</span><span>|</span><a href="#39383870">parent</a><span>|</span><a href="#39385715">prev</a><span>|</span><a href="#39384008">next</a><span>|</span><label class="collapse" for="c-39383945">[-]</label><label class="expand" for="c-39383945">[1 more]</label></div><br/><div class="children"><div class="content">Yep, thatâs actually a common one</div><br/></div></div><div id="39384008" class="c"><input type="checkbox" id="c-39384008" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#39383870">parent</a><span>|</span><a href="#39383945">prev</a><span>|</span><a href="#39384939">next</a><span>|</span><label class="collapse" for="c-39384008">[-]</label><label class="expand" for="c-39384008">[1 more]</label></div><br/><div class="children"><div class="content">Very simplified 
There are arrays (matrices) that are length 10M inside the model.<p>Itâs difficult to make that array longer because training time explodes.</div><br/></div></div></div></div><div id="39384939" class="c"><input type="checkbox" id="c-39384939" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#39383870">prev</a><span>|</span><a href="#39383918">next</a><span>|</span><label class="collapse" for="c-39384939">[-]</label><label class="expand" for="c-39384939">[1 more]</label></div><br/><div class="children"><div class="content">Based on what I&#x27;ve seen so far, I think the probability that this is actually better than GPT4 on the kind of real world coding tasks that I use it for is less than 1%. Literally everything from Google on this has been vaporware or laughably bad in actual practice in my personal experience. Which is totally insane to me given their financial resources, human resources, and multi-year lead in AI&#x2F;DL research, but that&#x27;s what seems to have happened. I certainly hope that they can develop and actually release a capable model, but at this point, I think you have to be deeply skeptical of everything they say until such a model is available for real by the public and you can try it on actual, real tasks and not fake benchmark nonsense and waitlists.</div><br/></div></div><div id="39383918" class="c"><input type="checkbox" id="c-39383918" checked=""/><div class="controls bullet"><span class="by">guybedo</span><span>|</span><a href="#39384939">prev</a><span>|</span><a href="#39384009">next</a><span>|</span><label class="collapse" for="c-39383918">[-]</label><label class="expand" for="c-39383918">[2 more]</label></div><br/><div class="children"><div class="content">looks interesting enough that i wanted to give Gemini a try and join the waitlist.<p>And i thought it would be easy, what a rookie mistake.<p>Looks like &quot;France&quot; isn&#x27;t on the list of available regions for Ai Studio ?<p>Now i&#x27;m trying to use Vertex AI, not even sure what&#x27;s the difference with Ai Studio, but it seems it&#x27;s available.<p>So far i&#x27;ve been struggling for 15 minutes through a maze of google cloud pages: console, docs, signups. No end in sight, looks like i won&#x27;t be able to try it out</div><br/><div id="39384123" class="c"><input type="checkbox" id="c-39384123" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#39383918">parent</a><span>|</span><a href="#39384009">next</a><span>|</span><label class="collapse" for="c-39384123">[-]</label><label class="expand" for="c-39384123">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not available outside of a private preview yet. The page says you can use 1.0 ultra in vertex but it&#x27;s not available to me in the UK.<p>I can&#x27;t get on the waitlist, because the waitlist link redirects to aistudio and I can&#x27;t use that.<p>I should stop expecting that I can use literally anything google announces.</div><br/></div></div></div></div><div id="39384009" class="c"><input type="checkbox" id="c-39384009" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#39383918">prev</a><span>|</span><a href="#39383938">next</a><span>|</span><label class="collapse" for="c-39384009">[-]</label><label class="expand" for="c-39384009">[1 more]</label></div><br/><div class="children"><div class="content">The technical report: <a href="https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemini&#x2F;gemini_v1_5_report.pdf" rel="nofollow">https:&#x2F;&#x2F;storage.googleapis.com&#x2F;deepmind-media&#x2F;gemini&#x2F;gemini_...</a></div><br/></div></div><div id="39383938" class="c"><input type="checkbox" id="c-39383938" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39384009">prev</a><span>|</span><a href="#39383836">next</a><span>|</span><label class="collapse" for="c-39383938">[-]</label><label class="expand" for="c-39383938">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to know how much a 1 million token prompt is likely to cost - both in terms of cash and in terms of raw energy usage.</div><br/><div id="39388742" class="c"><input type="checkbox" id="c-39388742" checked=""/><div class="controls bullet"><span class="by">empath-nirvana</span><span>|</span><a href="#39383938">parent</a><span>|</span><a href="#39384594">next</a><span>|</span><label class="collapse" for="c-39388742">[-]</label><label class="expand" for="c-39388742">[1 more]</label></div><br/><div class="children"><div class="content">When you account for this, you have to consider how much it would cost to have a human perform the same task.</div><br/></div></div><div id="39384594" class="c"><input type="checkbox" id="c-39384594" checked=""/><div class="controls bullet"><span class="by">bearjaws</span><span>|</span><a href="#39383938">parent</a><span>|</span><a href="#39388742">prev</a><span>|</span><a href="#39383836">next</a><span>|</span><label class="collapse" for="c-39384594">[-]</label><label class="expand" for="c-39384594">[1 more]</label></div><br/><div class="children"><div class="content">Cannot emphasize enough, even with the improvements in context handling I imagine 128k tokens costs as much as 16k tokens did previously.<p>So 1M tokens is going to be astronomical.</div><br/></div></div></div></div><div id="39383836" class="c"><input type="checkbox" id="c-39383836" checked=""/><div class="controls bullet"><span class="by">ranulo</span><span>|</span><a href="#39383938">prev</a><span>|</span><a href="#39383982">next</a><span>|</span><label class="collapse" for="c-39383836">[-]</label><label class="expand" for="c-39383836">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This new generation also delivers a breakthrough in long-context understanding. Weâve been able to significantly increase the amount of information our models can process â running up to 1 million tokens consistently, achieving the longest context window of any large-scale foundation model yet.<p>Sweet, this opens up so many possibilities.</div><br/></div></div><div id="39383982" class="c"><input type="checkbox" id="c-39383982" checked=""/><div class="controls bullet"><span class="by">foliveira</span><span>|</span><a href="#39383836">prev</a><span>|</span><a href="#39389328">next</a><span>|</span><label class="collapse" for="c-39383982">[-]</label><label class="expand" for="c-39383982">[5 more]</label></div><br/><div class="children"><div class="content">&gt;&quot;Gemini 1.5 Pro (...) matches or surpasses Gemini 1.0 Ultraâs state-of-the-art performance across a broad set of benchmarks.&quot;<p>So Pro is better than Ultra, but only if the version numbers are higher?</div><br/><div id="39385017" class="c"><input type="checkbox" id="c-39385017" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#39383982">parent</a><span>|</span><a href="#39384591">next</a><span>|</span><label class="collapse" for="c-39385017">[-]</label><label class="expand" for="c-39385017">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that usually the case with many products? Like the M3 Pro CPU in the new Macs is more powerful than the M1 Max in the old Macs.<p>The Nano &lt; Pro &lt; Ultra is an in-revision thing. For their LLMs it&#x27;s a size thing. Then there&#x27;s newer releases of Nano, Pro, and Ultra. Some Pro might be better than some older Ultra.<p>A lot of people seem confused about this but it feels so easy to understand that it&#x27;s confusing to me that anyone could have trouble.</div><br/><div id="39385839" class="c"><input type="checkbox" id="c-39385839" checked=""/><div class="controls bullet"><span class="by">devindotcom</span><span>|</span><a href="#39383982">root</a><span>|</span><a href="#39385017">parent</a><span>|</span><a href="#39384591">next</a><span>|</span><label class="collapse" for="c-39385839">[-]</label><label class="expand" for="c-39385839">[2 more]</label></div><br/><div class="children"><div class="content">Apple didn&#x27;t release the M3 Pro a week after the M1 Max</div><br/><div id="39385942" class="c"><input type="checkbox" id="c-39385942" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#39383982">root</a><span>|</span><a href="#39385839">parent</a><span>|</span><a href="#39384591">next</a><span>|</span><label class="collapse" for="c-39385942">[-]</label><label class="expand" for="c-39385942">[1 more]</label></div><br/><div class="children"><div class="content">Adam Osborneâs wife was one of my dadâs patients so Iâm not unacquainted with the risk of early announcements. But surely they do not prevent comprehension.</div><br/></div></div></div></div></div></div><div id="39384591" class="c"><input type="checkbox" id="c-39384591" checked=""/><div class="controls bullet"><span class="by">denysvitali</span><span>|</span><a href="#39383982">parent</a><span>|</span><a href="#39385017">prev</a><span>|</span><a href="#39389328">next</a><span>|</span><label class="collapse" for="c-39384591">[-]</label><label class="expand" for="c-39384591">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but you&#x27;d have to wait for Gemini Pro Max next year to see the real improvements</div><br/></div></div></div></div><div id="39392338" class="c"><input type="checkbox" id="c-39392338" checked=""/><div class="controls bullet"><span class="by">noisy_boy</span><span>|</span><a href="#39389328">prev</a><span>|</span><a href="#39384910">next</a><span>|</span><label class="collapse" for="c-39392338">[-]</label><label class="expand" for="c-39392338">[1 more]</label></div><br/><div class="children"><div class="content">I feel sad for those who are in law school right now.</div><br/></div></div><div id="39384910" class="c"><input type="checkbox" id="c-39384910" checked=""/><div class="controls bullet"><span class="by">EZ-E</span><span>|</span><a href="#39392338">prev</a><span>|</span><a href="#39383987">next</a><span>|</span><label class="collapse" for="c-39384910">[-]</label><label class="expand" for="c-39384910">[3 more]</label></div><br/><div class="children"><div class="content">Remember AI Dungeon and how it was frustrating about how it would forget what happened previously?
With a 10M context window, am I right to assume it would be possible to weave a story  which would span with multiple multiple books worth of content? (more or less 1400 pages)</div><br/><div id="39386446" class="c"><input type="checkbox" id="c-39386446" checked=""/><div class="controls bullet"><span class="by">dougmwne</span><span>|</span><a href="#39384910">parent</a><span>|</span><a href="#39389388">next</a><span>|</span><label class="collapse" for="c-39386446">[-]</label><label class="expand" for="c-39386446">[1 more]</label></div><br/><div class="children"><div class="content">Pretty much! Check out this demo of finding a scene in a 1400 page book based on a stick figure drawing. Mind blowing, right?<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;JeffDean&#x2F;status&#x2F;1758148159942091114" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;JeffDean&#x2F;status&#x2F;1758148159942091114</a></div><br/></div></div><div id="39389388" class="c"><input type="checkbox" id="c-39389388" checked=""/><div class="controls bullet"><span class="by">VikingCoder</span><span>|</span><a href="#39384910">parent</a><span>|</span><a href="#39386446">prev</a><span>|</span><a href="#39383987">next</a><span>|</span><label class="collapse" for="c-39389388">[-]</label><label class="expand" for="c-39389388">[1 more]</label></div><br/><div class="children"><div class="content">Dear Google,<p>Teach Gemini how to be a Dungeon Master, and run free adventures at Comic Con.<p>Then offer it up as a subscription.<p>Sincerely,<p>Everyone</div><br/></div></div></div></div><div id="39383987" class="c"><input type="checkbox" id="c-39383987" checked=""/><div class="controls bullet"><span class="by">thiago_fm</span><span>|</span><a href="#39384910">prev</a><span>|</span><a href="#39384435">next</a><span>|</span><label class="collapse" for="c-39383987">[-]</label><label class="expand" for="c-39383987">[1 more]</label></div><br/><div class="children"><div class="content">I like that they are rushing with this and don&#x27;t care enough to make it Gemini 2 or even really release it, to me it looks like they are concerned to share progress.<p>Hope they do a good job and once OpenAI releases GPT 5 they are competitive with it with their offerings, it will be better for everyone.</div><br/></div></div><div id="39384435" class="c"><input type="checkbox" id="c-39384435" checked=""/><div class="controls bullet"><span class="by">CrypticShift</span><span>|</span><a href="#39383987">prev</a><span>|</span><a href="#39383998">next</a><span>|</span><label class="collapse" for="c-39384435">[-]</label><label class="expand" for="c-39384435">[1 more]</label></div><br/><div class="children"><div class="content">Most data accumulates gradually (e.g., one email at a time, one line of text at a time across various documents). Is this huge 10M scale of context window relevant to a gradual, yet constant, influx of data (like a prompt over a whole google workspace) ?</div><br/></div></div><div id="39383998" class="c"><input type="checkbox" id="c-39383998" checked=""/><div class="controls bullet"><span class="by">kaspermarstal</span><span>|</span><a href="#39384435">prev</a><span>|</span><label class="collapse" for="c-39383998">[-]</label><label class="expand" for="c-39383998">[3 more]</label></div><br/><div class="children"><div class="content">Incredible. RAG will be obsolete in a year or two.</div><br/><div id="39385154" class="c"><input type="checkbox" id="c-39385154" checked=""/><div class="controls bullet"><span class="by">jeanloolz</span><span>|</span><a href="#39383998">parent</a><span>|</span><a href="#39384710">next</a><span>|</span><label class="collapse" for="c-39385154">[-]</label><label class="expand" for="c-39385154">[1 more]</label></div><br/><div class="children"><div class="content">Obsolete if you don&#x27;t take cost in consideration. Having 10 millions of token going through each layer of the LLM is going to cost a lot of money each time. At gpt4 rate that could mean 200 dollars for each inference</div><br/></div></div><div id="39384710" class="c"><input type="checkbox" id="c-39384710" checked=""/><div class="controls bullet"><span class="by">hackernoteng</span><span>|</span><a href="#39383998">parent</a><span>|</span><a href="#39385154">prev</a><span>|</span><label class="collapse" for="c-39384710">[-]</label><label class="expand" for="c-39384710">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s already obsolete.  It doesn&#x27;t work except for trivial cases which have no real value.</div><br/></div></div></div></div></div></div></div></div></div></body></html>