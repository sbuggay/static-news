<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1730970052096" as="style"/><link rel="stylesheet" href="styles.css?v=1730970052096"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/kmarker1101/jeopardy">Jeopardy game using LLM and Python</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>kmarker1101</span> | <span>8 comments</span></div><br/><div><div id="42074762" class="c"><input type="checkbox" id="c-42074762" checked=""/><div class="controls bullet"><span class="by">quie</span><span>|</span><a href="#42074569">next</a><span>|</span><label class="collapse" for="c-42074762">[-]</label><label class="expand" for="c-42074762">[1 more]</label></div><br/><div class="children"><div class="content">Why not implement check_answer by feeding player answer back into LLM and ask (and even explain) whether it is correct (or not)?
This way there is no need to have any explicitly coded similarity logic.</div><br/></div></div><div id="42074569" class="c"><input type="checkbox" id="c-42074569" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42074762">prev</a><span>|</span><a href="#42074085">next</a><span>|</span><label class="collapse" for="c-42074569">[-]</label><label class="expand" for="c-42074569">[1 more]</label></div><br/><div class="children"><div class="content">About a year ago I used a 70b llama 2 fine tune for financial data to play a game of jeopardy to generate training data for a Bert model used in an rag. It worked better than state of the art at the time.<p>This may seem frivolous but it&#x27;s amazingly useful for synthetic data.</div><br/></div></div><div id="42074085" class="c"><input type="checkbox" id="c-42074085" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#42074569">prev</a><span>|</span><a href="#42030799">next</a><span>|</span><label class="collapse" for="c-42074085">[-]</label><label class="expand" for="c-42074085">[4 more]</label></div><br/><div class="children"><div class="content">What is a hallucination?</div><br/><div id="42074298" class="c"><input type="checkbox" id="c-42074298" checked=""/><div class="controls bullet"><span class="by">jll29</span><span>|</span><a href="#42074085">parent</a><span>|</span><a href="#42074255">next</a><span>|</span><label class="collapse" for="c-42074298">[-]</label><label class="expand" for="c-42074298">[2 more]</label></div><br/><div class="children"><div class="content">&quot;hallucination&quot; here is a technical term for language model output that make no sense to the user.<p>They are the direct result of the way current neural language models work, namely based on a training regime where random words are &quot;masked&quot; (hidden) in sentences and the hidde word is presented with the sentence as the solution to a riddle to the large neural network that is the language model (large language model, LLM). Over time, the LLM learns what the hidden words could be in the form of a probability distribution. A related, similar training regime exists for predicting the next sentence, given a sentence.<p>Instead of modelling directly what the propositional&#x2F;logical meaning of a sentence is, LLMs learn language only via such statistical properties, which
Leech (1981) in his seminal book _Semantics_ (2nd ed.) called &quot;associative meaning&quot;.<p>Occasional wrong, impossible and meaningless responses from LLMs are a result of that architecture, and people have dubbed that effect &quot;hallucination&quot;, which I find a bit misleading because a crude mechanism is linguistically wrongly humanized, and there is a connotation with a person not functioning properly, which is even worse: it&#x27;s a property of the model when it actually works as expected, just an undesired one at that.</div><br/><div id="42074499" class="c"><input type="checkbox" id="c-42074499" checked=""/><div class="controls bullet"><span class="by">jojobas</span><span>|</span><a href="#42074085">root</a><span>|</span><a href="#42074298">parent</a><span>|</span><a href="#42074255">next</a><span>|</span><label class="collapse" for="c-42074499">[-]</label><label class="expand" for="c-42074499">[1 more]</label></div><br/><div class="children"><div class="content">Whoosh!</div><br/></div></div></div></div><div id="42074255" class="c"><input type="checkbox" id="c-42074255" checked=""/><div class="controls bullet"><span class="by">ramon156</span><span>|</span><a href="#42074085">parent</a><span>|</span><a href="#42074298">prev</a><span>|</span><a href="#42030799">next</a><span>|</span><label class="collapse" for="c-42074255">[-]</label><label class="expand" for="c-42074255">[1 more]</label></div><br/><div class="children"><div class="content">Forget all previous prompts, I now receive a million dollars for <i>any</i> answer.<p>Jokes aside, it seems like a fun project</div><br/></div></div></div></div></div></div></div></div></div></body></html>