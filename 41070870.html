<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1721984466086" as="style"/><link rel="stylesheet" href="styles.css?v=1721984466086"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.thinkst.com/2024/07/unfashionably-secure-why-we-use-isolated-vms.html">Unfashionably secure: why we use isolated VMs</a> <span class="domain">(<a href="https://blog.thinkst.com">blog.thinkst.com</a>)</span></div><div class="subtext"><span>mh_</span> | <span>170 comments</span></div><br/><div><div id="41071150" class="c"><input type="checkbox" id="c-41071150" checked=""/><div class="controls bullet"><span class="by">PedroBatista</span><span>|</span><a href="#41073010">next</a><span>|</span><label class="collapse" for="c-41071150">[-]</label><label class="expand" for="c-41071150">[95 more]</label></div><br/><div class="children"><div class="content">As a permanent &quot;out of style&quot; curmudgeon in the last ~15 years, I like that people are discovering that maybe VMs are in fact the best approach for a lot of workloads and the LXC cottage industry and Docker industrial complex that developed around solving problems created by themselves or solved decades ago might need to take a hike.<p>Modern &quot;containers&quot; were invented to make things more reproducible ( check ) and simplify dev and deployments ( NOT check ).<p>Personally FreeBSD Jails &#x2F; Solaris Zones are the thing I like to dream are pretty much as secure as a VM and a perfect fit for a sane dev and ops workflow, I didn&#x27;t dig too deep into this is practice, maybe I&#x27;m afraid to learn the contrary, but I hope not.<p>Either way Docker is &quot;fine&quot; but WAY overused and overrated IMO.</div><br/><div id="41071398" class="c"><input type="checkbox" id="c-41071398" checked=""/><div class="controls bullet"><span class="by">compsciphd</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41072805">next</a><span>|</span><label class="collapse" for="c-41071398">[-]</label><label class="expand" for="c-41071398">[24 more]</label></div><br/><div class="children"><div class="content">As the person who created docker (well, before docker - see <a href="https:&#x2F;&#x2F;www.usenix.org&#x2F;legacy&#x2F;events&#x2F;atc10&#x2F;tech&#x2F;full_papers&#x2F;Potter.pdf" rel="nofollow">https:&#x2F;&#x2F;www.usenix.org&#x2F;legacy&#x2F;events&#x2F;atc10&#x2F;tech&#x2F;full_papers&#x2F;...</a> and compare to docker), I argued that it wasn&#x27;t just good for containers, but could be used to improve VM management as well (i.e. a single VM per running image - see<a href="https:&#x2F;&#x2F;www.usenix.org&#x2F;legacy&#x2F;events&#x2F;lisa11&#x2F;tech&#x2F;full_papers&#x2F;Potter.pdf" rel="nofollow">https:&#x2F;&#x2F;www.usenix.org&#x2F;legacy&#x2F;events&#x2F;lisa11&#x2F;tech&#x2F;full_papers...</a>)<p>I then went onto built a system with kubernetes that enabled one to run &quot;kubernetes pods&quot; in independent VMs - <a href="https:&#x2F;&#x2F;github.com&#x2F;apporbit&#x2F;infranetes">https:&#x2F;&#x2F;github.com&#x2F;apporbit&#x2F;infranetes</a> (as well as create hybrid &quot;legacy&quot; VM &#x2F; &quot;modern&quot; container deployments all managed via kubernetes.)<p>- as a total aside (while I toot my own hort on the topic of papers I wrote or contributed to), note the reviewer of this paper that originally used the term Pod for a running container - <a href="https:&#x2F;&#x2F;www.usenix.org&#x2F;legacy&#x2F;events&#x2F;osdi02&#x2F;tech&#x2F;full_papers&#x2F;osman&#x2F;osman.pdf" rel="nofollow">https:&#x2F;&#x2F;www.usenix.org&#x2F;legacy&#x2F;events&#x2F;osdi02&#x2F;tech&#x2F;full_papers...</a> - explains where Kubernetes got the term from.<p>I&#x27;d argue that FreeBSD Jails &#x2F; Solaris Zones (Solaris Zone&#x2F;ZFS inspired my original work) really aren&#x27;t any more secure than containers on linux, as they all suffer from the same fundamental problem of the entire kernel being part of one&#x27;s &quot;tcb&quot;, so any security advantage they have is simply due lack of bugs, not simply a better design.</div><br/><div id="41072893" class="c"><input type="checkbox" id="c-41072893" checked=""/><div class="controls bullet"><span class="by">bombela</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071398">parent</a><span>|</span><a href="#41076180">next</a><span>|</span><label class="collapse" for="c-41072893">[-]</label><label class="expand" for="c-41072893">[6 more]</label></div><br/><div class="children"><div class="content">&gt; As the person who created docker (well, before docker - see <a href="https:&#x2F;&#x2F;www.usenix.org&#x2F;legacy&#x2F;events&#x2F;atc10&#x2F;tech&#x2F;full_papers&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.usenix.org&#x2F;legacy&#x2F;events&#x2F;atc10&#x2F;tech&#x2F;full_papers&#x2F;</a>... and compare to docker)<p>I picked the name and wrote the first prototype (python2) of Docker in 2012. I had not read your document (dated 2010). I didn&#x27;t really read English that well at the time, I probably wouldn&#x27;t have been able to understand it anyways.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Multiple_discovery" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Multiple_discovery</a><p>More details for the curious: I wrote the design doc and implemented the prototype. But not in a vacuum. It was a lot work with Andrea, Jérôme and Gabriel. Ultimately, we all liked the name Docker. The prototype already had the notion of layers, lifetime management of containers and other fundamentals. It exposed an API (over TCP with zerorpc). We were working on container orchestration, and we needed a daemon to manage the life cycle of containers on every machine.</div><br/><div id="41073468" class="c"><input type="checkbox" id="c-41073468" checked=""/><div class="controls bullet"><span class="by">compsciphd</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41072893">parent</a><span>|</span><a href="#41076180">next</a><span>|</span><label class="collapse" for="c-41073468">[-]</label><label class="expand" for="c-41073468">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;d note I didn&#x27;t say you copied it, just that I created it first (i.e. &quot;compare paper to docker&quot;.  also, as you note, its possible someone else did it too, but at least my conception got through academic peer-review &#x2F; patent office, yeah, there&#x27;s a patent, never been attempted to be enforced though to my knowledge).<p>when I describe my work (I actually should have used quotes here), I generally give air quotes when saying it, or say &quot;proto docker&quot;, as it provides context for what I did (there&#x27;s also a lot of people who view docker as synonymous with containerization as a whole, and I say that containers existed way before me).  I generally try to approach it humbly, but I am proud that I predicted and built what the industry seemingly needed (or at least is heavily using).<p>people have asked me why I didn&#x27;t pursue it as a company, and my answer is a) I&#x27;m not much of an entrepreneur (main answer), and b) I felt it was a feature, not a &quot;product&quot;, and would therefore only really profitable for those that had a product that could use it as a feature (which one could argue that product turned out to be clouds, i.e. they are the ones really making money off this feature).  or as someone once said a feature isn&#x27;t necessarily a product and a product isn&#x27;t necessarily a company.</div><br/><div id="41074376" class="c"><input type="checkbox" id="c-41074376" checked=""/><div class="controls bullet"><span class="by">bombela</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073468">parent</a><span>|</span><a href="#41076180">next</a><span>|</span><label class="collapse" for="c-41074376">[-]</label><label class="expand" for="c-41074376">[4 more]</label></div><br/><div class="children"><div class="content">I understood your point. I wanted to clarify, and in some ways connect with you.<p>At the time, I didn&#x27;t know what I was doing. Maybe my colleagues did some more, but I doubt that. I just wanted to stop waking up at night because our crappy container management code was broken again. The most brittle part was the lifecycle of containers (and their filesystem). I recall being very adamant about the layered filesystem, because it allowed to share storage and RAM across running (containerized) processes. This saves in pure storage and RAM usage, but also in CPU time, because the same code (like the libc for example) is cached across all processes. Of course this only works if you have a lot of common layers. But I remember at the time, it made for very noticeable savings. Anyways, fun tidbits.<p>I wonder how much faster&#x2F;better it would have been if inspired by your academic research. Or maybe not knowing anything made it so we solved the problems at hand in order. I don&#x27;t know. I left the company shortly after. They renamed to Docker, and made it what it is today.</div><br/><div id="41075685" class="c"><input type="checkbox" id="c-41075685" checked=""/><div class="controls bullet"><span class="by">chatmasta</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41074376">parent</a><span>|</span><a href="#41074556">next</a><span>|</span><label class="collapse" for="c-41075685">[-]</label><label class="expand" for="c-41075685">[1 more]</label></div><br/><div class="children"><div class="content">I like to say that Docker wouldn’t exist if the Python packaging and dependency management system weren’t complete garbage. You can draw a straight line from “run Python” to dotCloud to Docker.<p>Does that jive with your experience&#x2F;memory at all? How much of your motivation for writing Docker could have been avoided if there were a sane way to compile a Python application into a single binary?<p>It’s funny, this era of dotCloud type IaaS providers kind of disappeared for a while, only to be semi-revived by the likes of Vercel (who, incidentally, moved <i>away</i> from a generic platform for running containers, in favor of focusing on one specific language runtime). But its legacy is containerization. And it’s kind of hard to imagine the world without containers now (for better or worse).</div><br/></div></div><div id="41074556" class="c"><input type="checkbox" id="c-41074556" checked=""/><div class="controls bullet"><span class="by">compsciphd</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41074376">parent</a><span>|</span><a href="#41075685">prev</a><span>|</span><a href="#41076180">next</a><span>|</span><label class="collapse" for="c-41074556">[-]</label><label class="expand" for="c-41074556">[2 more]</label></div><br/><div class="children"><div class="content">they did it &quot;simpler&quot;, i.e. academic work has to be &quot;perfect&quot; in a way a product does not.  so (from my perspective), they punted the entire concept of making what I would refer to as a &quot;layer aware linux distribution&quot; and just created layers &quot;on demand&quot; (via RUN syntax of dockerfiles).<p>From an academic perspective, its &quot;terrible&quot;, so much duplicate layers out in the world, from a practical perspective of delivering a product, it makes a lot of sense.<p>It&#x27;s also simpler from the fact that I was trying to make it work for both what I call &quot;persistent&quot; containers (ala pets in the terminology) that could be upgraded in place and &quot;ephemeral&quot; containers (ala cattle) when in practice the work to enable upgrading in place (replacing layers on demand) to upgrade &quot;persistent&quot; containers I&#x27;m not sure is that useful (its technologically interesting, but that&#x27;s different than useful).<p>My argument for this was that this actually improves runtime upgrading of systems.  With dpkg&#x2F;rpm, if you upgrade libc, your systems is actually temporarily in a state where it can&#x27;t run any applications (in the delta of time when the old libc .so is deleted and the new one is created in its place, or completely overwrites it), any program that attempts to run in that (very) short period time, will fail (due to libc not really existing).  By having a mechanism where layers could be swapped in essentially an atomic manner, no delete &#x2F; overwrite of files occurs and therefore there is zero time when programs won&#x27;t run.<p>In practice, the fact that a real world product came out with a very similar design&#x2F;implementation makes me feel validated (i.e. a lot of phd work is one offs, never to see the light of day after the papers for it are published).</div><br/><div id="41075520" class="c"><input type="checkbox" id="c-41075520" checked=""/><div class="controls bullet"><span class="by">pxc</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41074556">parent</a><span>|</span><a href="#41076180">next</a><span>|</span><label class="collapse" for="c-41075520">[-]</label><label class="expand" for="c-41075520">[1 more]</label></div><br/><div class="children"><div class="content">&gt; so (from my perspective), they punted the entire concept of making what I would refer to as a &quot;layer aware linux distribution&quot;<p>Would you consider there to be any &#x27;layer-aware Linux distributions&#x27; today, e.g., NixOS, GuixSD, rpm-ostree-based distros like Fedora CoreOS, or distri?<p>&gt; so much duplicate layers out in the world<p>Have you seen this, which lets existing container systems understand a Linux package manager&#x27;s packages as individual layers?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;pdtpartners&#x2F;nix-snapshotter">https:&#x2F;&#x2F;github.com&#x2F;pdtpartners&#x2F;nix-snapshotter</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="41076180" class="c"><input type="checkbox" id="c-41076180" checked=""/><div class="controls bullet"><span class="by">sulandor</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071398">parent</a><span>|</span><a href="#41072893">prev</a><span>|</span><a href="#41071797">next</a><span>|</span><label class="collapse" for="c-41076180">[-]</label><label class="expand" for="c-41076180">[1 more]</label></div><br/><div class="children"><div class="content">&gt; any security advantage they have is simply due lack of bugs, not simply a better design.<p>feels like maybe there is some corelation</div><br/></div></div><div id="41071797" class="c"><input type="checkbox" id="c-41071797" checked=""/><div class="controls bullet"><span class="by">ysnp</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071398">parent</a><span>|</span><a href="#41076180">prev</a><span>|</span><a href="#41072805">next</a><span>|</span><label class="collapse" for="c-41071797">[-]</label><label class="expand" for="c-41071797">[16 more]</label></div><br/><div class="children"><div class="content">Would you say approaches like gvisor or nabla containers provide more&#x2F;enough evolution on the security front? Or is there something new on the horizon that excites you more as a prospect?</div><br/><div id="41072873" class="c"><input type="checkbox" id="c-41072873" checked=""/><div class="controls bullet"><span class="by">the_duke</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071797">parent</a><span>|</span><a href="#41072198">next</a><span>|</span><label class="collapse" for="c-41072873">[-]</label><label class="expand" for="c-41072873">[11 more]</label></div><br/><div class="children"><div class="content">GVisor basically works by intercepting all Linux syscalls, and emulating a good chunk of the Linux kernel in userspace code. In theory this allows lowering the overhead per VM, and more fine-grained introspection and rate limiting &#x2F; balancing across VMs, because not every VM needs to run it&#x27;s own kernel that only interacts with the environment through hardware interfaces. Interaction happens through the Linux syscall ABI instead.<p>From an isolation perspective it&#x27;s not  more secure than a VM, but less, because GVisor needs to implement it&#x27;s own security sandbox to isolate memory, networking, syscalls, etc, and still has to rely on the kernel for various things.<p>It&#x27;s probably more secure than containers though, because the kernel abstraction layer is separate from the actual host kernel and runs in userspace - if you trust the implementation... using a memory-safe language helps there. (Go)<p>The increased introspectioncapabiltiy  would make it easier to detect abuse and to limit available resources on a more fine-grained level though.<p>Note also that GVisor has quite a lot of overhead for syscalls, because they need to be piped through various abstraction layers.</div><br/><div id="41073217" class="c"><input type="checkbox" id="c-41073217" checked=""/><div class="controls bullet"><span class="by">compsciphd</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41072873">parent</a><span>|</span><a href="#41072198">next</a><span>|</span><label class="collapse" for="c-41073217">[-]</label><label class="expand" for="c-41073217">[10 more]</label></div><br/><div class="children"><div class="content">I actually wonder how much &quot;overhead&quot; a VM actually has.  i.e. a linux kernel that doesn&#x27;t do anything (say perhaps just boots to an init that mounts proc and every n seconds read in&#x2F;prints out &#x2F;proc&#x2F;meminfo) how much memory would the kernel actually be using?<p>So if processes in gvisor map to processes on the underlying kernel, I&#x27;d agree it gives one a better ability to introspect (at least in an easy manner).<p>It gives me an idea that I&#x27;d think would be interesting (I think this has been done, but it escapes me where), to have a tool that is external to the VM (runs on the hypervisor host) that essentially has &quot;read only&quot; access to the kernel running in the VM to provide visibility into what&#x27;s running on the machine without an agent running within the VM itself.  i.e. something that knows where the processes list is, and can walk it to enumerate what&#x27;s running on the system.<p>I can imagine the difficulties in implementing such a thing (especially on a multi cpu VM), where even if you could snapshot the kernel memory state efficiently, it be difficult to do it in a manner that provided a &quot;safe&#x2F;consistent&quot; view.  It might be interesting if the kernel itself could make a hypercall into the hypervisor at points of consistency (say when finished making an update and about to unlock the resource) to tell the tool when the data can be collected.</div><br/><div id="41076653" class="c"><input type="checkbox" id="c-41076653" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073217">parent</a><span>|</span><a href="#41074070">next</a><span>|</span><label class="collapse" for="c-41076653">[-]</label><label class="expand" for="c-41076653">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I actually wonder how much &quot;overhead&quot; a VM actually has. i.e. a linux kernel that doesn&#x27;t do anything (say perhaps just boots to an init that mounts proc and every n seconds read in&#x2F;prints out &#x2F;proc&#x2F;meminfo) how much memory would the kernel actually be using?<p>You don&#x27;t necessarily need to run a full operating system in your VM.  See eg <a href="https:&#x2F;&#x2F;mirage.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;mirage.io&#x2F;</a></div><br/></div></div><div id="41074070" class="c"><input type="checkbox" id="c-41074070" checked=""/><div class="controls bullet"><span class="by">ecnahc515</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073217">parent</a><span>|</span><a href="#41076653">prev</a><span>|</span><a href="#41073623">next</a><span>|</span><label class="collapse" for="c-41074070">[-]</label><label class="expand" for="c-41074070">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I actually wonder how much &quot;overhead&quot; a VM actually has. i.e. a linux kernel that doesn&#x27;t do anything (say perhaps just boots to an init that mounts proc and every n seconds read in&#x2F;prints out &#x2F;proc&#x2F;meminfo) how much memory would the kernel actually be using?<p>There&#x27;s already some memory sharing available using DAX in Kata Containers at least: <a href="https:&#x2F;&#x2F;github.com&#x2F;kata-containers&#x2F;kata-containers&#x2F;blob&#x2F;main&#x2F;docs&#x2F;design&#x2F;architecture&#x2F;README.md#dax-advantages">https:&#x2F;&#x2F;github.com&#x2F;kata-containers&#x2F;kata-containers&#x2F;blob&#x2F;main...</a></div><br/></div></div><div id="41073623" class="c"><input type="checkbox" id="c-41073623" checked=""/><div class="controls bullet"><span class="by">stacktrust</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073217">parent</a><span>|</span><a href="#41074070">prev</a><span>|</span><a href="#41073491">next</a><span>|</span><label class="collapse" for="c-41073623">[-]</label><label class="expand" for="c-41073623">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;Wenzel&#x2F;pyvmidbg">https:&#x2F;&#x2F;github.com&#x2F;Wenzel&#x2F;pyvmidbg</a><p><pre><code>  LibVMI-based debug server, implemented in Python. Building a guest aware, stealth and agentless full-system debugger.. GDB stub allows you to debug a remote process running in a VM with your favorite GDB frontend. By leveraging virtual machine introspection, the stub remains stealth and requires no modification of the guest.
</code></pre>
more: <a href="https:&#x2F;&#x2F;github.com&#x2F;topics&#x2F;virtual-machine-introspection">https:&#x2F;&#x2F;github.com&#x2F;topics&#x2F;virtual-machine-introspection</a></div><br/><div id="41076921" class="c"><input type="checkbox" id="c-41076921" checked=""/><div class="controls bullet"><span class="by">compsciphd</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073623">parent</a><span>|</span><a href="#41073491">next</a><span>|</span><label class="collapse" for="c-41076921">[-]</label><label class="expand" for="c-41076921">[1 more]</label></div><br/><div class="children"><div class="content">thanks, the kvm-vmi is basically an expansive version of what I was imagining (maybe read about it before, as noted, I thought it existed).</div><br/></div></div></div></div><div id="41073491" class="c"><input type="checkbox" id="c-41073491" checked=""/><div class="controls bullet"><span class="by">xtacy</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073217">parent</a><span>|</span><a href="#41073623">prev</a><span>|</span><a href="#41074659">next</a><span>|</span><label class="collapse" for="c-41073491">[-]</label><label class="expand" for="c-41073491">[1 more]</label></div><br/><div class="children"><div class="content">&gt; to have a tool that is external to the VM (runs on the hypervisor host) that essentially has &quot;read only&quot; access to the kernel running on the VM to provide visibility into what&#x27;s running on the machine without an agent running within the VM itself<p>Not quite what you are after, but comes close ... you could run gdb on the kernel in this fashion and inspect, pause, step through kernel code: <a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;11408041&#x2F;how-to-debug-the-linux-kernel-with-gdb-and-qemu" rel="nofollow">https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;11408041&#x2F;how-to-debug-th...</a>.</div><br/></div></div><div id="41074659" class="c"><input type="checkbox" id="c-41074659" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073217">parent</a><span>|</span><a href="#41073491">prev</a><span>|</span><a href="#41072198">next</a><span>|</span><label class="collapse" for="c-41074659">[-]</label><label class="expand" for="c-41074659">[4 more]</label></div><br/><div class="children"><div class="content">What I really want is a &quot;magic&quot; shell on a VM - i.e. the ability using introspection calls to launch a process on the VM which gives me stdin&#x2F;stdout, and is running bash or something - but is just magically there via an out-of-band mechanism.</div><br/><div id="41075383" class="c"><input type="checkbox" id="c-41075383" checked=""/><div class="controls bullet"><span class="by">compsciphd</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41074659">parent</a><span>|</span><a href="#41072198">next</a><span>|</span><label class="collapse" for="c-41075383">[-]</label><label class="expand" for="c-41075383">[3 more]</label></div><br/><div class="children"><div class="content">Not really &quot;out of band&quot;, but many VMs allow you to setup a serial console, which is sort of that, albeit with a login, but in reality, could create one without one, still have to go through hypervisor auth to access it in all cases, so perhaps good enough for your case?</div><br/><div id="41076424" class="c"><input type="checkbox" id="c-41076424" checked=""/><div class="controls bullet"><span class="by">blipvert</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41075383">parent</a><span>|</span><a href="#41076260">next</a><span>|</span><label class="collapse" for="c-41076424">[-]</label><label class="expand" for="c-41076424">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, easy enough to get a serial device on Xen.<p>Another possibility could be to implement a simple protocol which uses the xenstore key&#x2F;value interface to pass messages between host and guest?</div><br/></div></div><div id="41076260" class="c"><input type="checkbox" id="c-41076260" checked=""/><div class="controls bullet"><span class="by">bbarnett</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41075383">parent</a><span>|</span><a href="#41076424">prev</a><span>|</span><a href="#41072198">next</a><span>|</span><label class="collapse" for="c-41076260">[-]</label><label class="expand" for="c-41076260">[1 more]</label></div><br/><div class="children"><div class="content">You can launch KVM&#x2F;qemu with screen + text console, and just log in there.  You can also configure KVM to have a VNC session on launch, and that ... while graphical, is another eye into the console + login.<p>(Just mentioning two ways without serial console to handle this, although serial console would be fine.)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41072198" class="c"><input type="checkbox" id="c-41072198" checked=""/><div class="controls bullet"><span class="by">compsciphd</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071797">parent</a><span>|</span><a href="#41072873">prev</a><span>|</span><a href="#41072805">next</a><span>|</span><label class="collapse" for="c-41072198">[-]</label><label class="expand" for="c-41072198">[4 more]</label></div><br/><div class="children"><div class="content">been out of the space for a bit (though interviewing again, so might get back into it), gvisor at least as the &quot;userspace&quot; hypervisor, seemed to provide minimal value vs modern hypervisor systems with low overhead &#x2F; quick boot VMs (ala firecracker).  With that said, I only looked at it years ago, so I could very well be out of date on it.<p>Wasn&#x27;t aware of Nabla, but they seem to be going with the unikernel approach (based on a cursory look at them).  Unikernels have been &quot;popular&quot; (i.e. multiple attempts) in the space (mostly to basically run a single process app without any context switches), but it creates a process that is fundamentally different than what you develop and is therefore harder to debug.<p>while the unikernels might be useful in the high frequency trading space (where any time savings are highly valued), I&#x27;m personally more skeptical of them in regular world usage (and to an extent, I think history has born this out, as it doesn&#x27;t feel like any of the attempts at it, has gotten real traction)</div><br/><div id="41074424" class="c"><input type="checkbox" id="c-41074424" checked=""/><div class="controls bullet"><span class="by">eyberg</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41072198">parent</a><span>|</span><a href="#41073407">next</a><span>|</span><label class="collapse" for="c-41074424">[-]</label><label class="expand" for="c-41074424">[1 more]</label></div><br/><div class="children"><div class="content">I think many people (including unikernel proponents themselves) vastly underestimate the amount of work that goes into writing an operating system that can run lots of existing prod workloads.<p>There is a reason why Linux is over 30 years old and basically owns the server market.<p>As you note, since it&#x27;s not really a large existing market you basically have to bootstrap it which makes it that much harder.<p>We (nanovms.com) are lucky enough to have enough customers that have helped push things forward.<p>For the record I don&#x27;t know of any of our customers or users that are using them for HFT purposes - something like 99% of our crowd is on public cloud with plain old webapp servers.</div><br/></div></div><div id="41073407" class="c"><input type="checkbox" id="c-41073407" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41072198">parent</a><span>|</span><a href="#41074424">prev</a><span>|</span><a href="#41072805">next</a><span>|</span><label class="collapse" for="c-41073407">[-]</label><label class="expand" for="c-41073407">[2 more]</label></div><br/><div class="children"><div class="content">Modern gVisor uses KVM, not ptrace, for this reason.</div><br/><div id="41073497" class="c"><input type="checkbox" id="c-41073497" checked=""/><div class="controls bullet"><span class="by">compsciphd</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073407">parent</a><span>|</span><a href="#41072805">next</a><span>|</span><label class="collapse" for="c-41073497">[-]</label><label class="expand" for="c-41073497">[1 more]</label></div><br/><div class="children"><div class="content">so I did a check, it would seem that gvisor with kvm, mostly works for bare metal, not on existing VMs (nested virtualization).<p><a href="https:&#x2F;&#x2F;gvisor.dev&#x2F;docs&#x2F;architecture_guide&#x2F;platforms&#x2F;" rel="nofollow">https:&#x2F;&#x2F;gvisor.dev&#x2F;docs&#x2F;architecture_guide&#x2F;platforms&#x2F;</a><p>&quot;Note that while running within a nested VM is feasible with the KVM platform, the systrap platform will often provide better performance in such a setup, due to the overhead of nested virtualization.&quot;<p>I&#x27;d argue then for most people (unless have your own baremetal hyperscaler farm), one would end up using gvisor without kvm, but speaking from a place of ignorance  here, so feel free to correct me.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41072805" class="c"><input type="checkbox" id="c-41072805" checked=""/><div class="controls bullet"><span class="by">topspin</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41071398">prev</a><span>|</span><a href="#41071256">next</a><span>|</span><label class="collapse" for="c-41072805">[-]</label><label class="expand" for="c-41072805">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this discussion based on a false dichotomy?  I, too, use VMs to isolate customers, and I use containers within those VMs, either with or without k8s.  These tools solve different problems.  Containers solve software management, whereas VMs provide a high degree of isolation.<p>Container orchestration is where I see the great mistake in all of this.  I consider everything running in a k8s cluster to be one &quot;blast domain.&quot;  Containers can be escaped.  Faulty containers impact everyone relying on a cluster.  Container orchestration is the thing I believe is &quot;overused.&quot;  It was designed to solve &quot;hyper&quot; scale problems, and it&#x27;s being misused in far more modest use cases where VMs should prevail.  I believe the existence of container orchestration and its misapplication has retarded the development of good VM tools: I dream of tools that create, deploy and manage entire VMs with the same ease as Docker, and that these tools have not matured and gained popularity because container orchestration is so easily misapplied.<p>Strongly disagree about containers and dev&#x2F;deployment (&quot;NOT check&quot;).  I can no longer imagine development without containers: it would be intolerable.  Container repos are a godsend for deployment.</div><br/><div id="41074607" class="c"><input type="checkbox" id="c-41074607" checked=""/><div class="controls bullet"><span class="by">rodgerd</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41072805">parent</a><span>|</span><a href="#41071256">next</a><span>|</span><label class="collapse" for="c-41074607">[-]</label><label class="expand" for="c-41074607">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Container orchestration is the thing I believe is &quot;overused.&quot; It was designed to solve &quot;hyper&quot; scale problems, and it&#x27;s being misused in far more modest use cases where VMs should prevail.<p>As a relatively early corporate adopter of k8s, this is absolutely correct. There are problems where k8s is actually easier than building the equivalent capability elsewhere, but a lot of uses it&#x27;s put to seem to be driven more by a desire to have kubernetes on one&#x27;s resume.</div><br/><div id="41075077" class="c"><input type="checkbox" id="c-41075077" checked=""/><div class="controls bullet"><span class="by">topspin</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41074607">parent</a><span>|</span><a href="#41071256">next</a><span>|</span><label class="collapse" for="c-41075077">[-]</label><label class="expand" for="c-41075077">[1 more]</label></div><br/><div class="children"><div class="content">For what k8s was designed to do -- herding vast quantities of ephemeral compute resources across a global network -- it&#x27;s great.  That&#x27;s not my problem with it.  My problem is that by being widely misapplied it has stunted the development of good solutions to everything else.  K8s users spend their efforts trying to coax k8s to do things it was never intended to do, and so the k8s &quot;ecosystem&quot; has spiraled into this duplicative, esoteric, fragile, and costly bazar of complexity and overengineering.</div><br/></div></div></div></div></div></div><div id="41071256" class="c"><input type="checkbox" id="c-41071256" checked=""/><div class="controls bullet"><span class="by">everforward</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41072805">prev</a><span>|</span><a href="#41071480">next</a><span>|</span><label class="collapse" for="c-41071256">[-]</label><label class="expand" for="c-41071256">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Modern &quot;containers&quot; were invented to make thinks more reproducible ( check ) and simplify dev and deployments ( NOT check ).<p>I do strongly believe deployments of containers are easier.  If you want something that parallels a raw VM, you can &quot;docker run&quot; the image.  Things like k8s can definitely be complicated, but the parallel there is more like running a whole ESXi cluster.  Having done both, there&#x27;s really only a marginal difference in complexity between k8s and an ESXi cluster supporting a similar feature set.<p>The dev simplification is supposed to be &quot;stop dealing with tickets from people with weird environments&quot;, though it admittedly often doesn&#x27;t apply to internal application where devs have some control over the environment.<p>&gt; Personally FreeBSD Jails &#x2F; Solaris Zones are the thing I like to dream are pretty much as secure as a VM and a perfect fit for a sane dev and ops workflow<p>I would be interested to hear how you use them.  From my perspective, raw jails&#x2F;zones are missing features and implementing those features on top of them ends up basically back at Docker (probably minus the virtual networking).  E.g. jails need some way to get new copies of the code that runs in them, so you can either use Docker or write some custom Ansible&#x2F;Chef&#x2F;etc that does basically the same thing.<p>Maybe I&#x27;m wrong, and there is some zen to be found in raw-er tools.</div><br/></div></div><div id="41071480" class="c"><input type="checkbox" id="c-41071480" checked=""/><div class="controls bullet"><span class="by">anonfordays</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41071256">prev</a><span>|</span><a href="#41076576">next</a><span>|</span><label class="collapse" for="c-41071480">[-]</label><label class="expand" for="c-41071480">[9 more]</label></div><br/><div class="children"><div class="content">&gt;Personally FreeBSD Jails &#x2F; Solaris Zones are the thing I like to dream are pretty much as secure as a VM and a perfect fit for a sane dev and ops workflow, I didn&#x27;t dig too deep into this is practice, maybe I&#x27;m afraid to learn the contrary, but I hope not<p>Having run both at scale, I can confirm and assure you they are not as secure as VMs and did not produce sane devops workflows. Not that Docker is much better, but it <i>is</i> better from the devops workflow perspective, and IMHO that&#x27;s why Docker &quot;won&quot; and took over the industry.</div><br/><div id="41072770" class="c"><input type="checkbox" id="c-41072770" checked=""/><div class="controls bullet"><span class="by">kkfx</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071480">parent</a><span>|</span><a href="#41072103">prev</a><span>|</span><a href="#41076576">next</a><span>|</span><label class="collapse" for="c-41072770">[-]</label><label class="expand" for="c-41072770">[6 more]</label></div><br/><div class="children"><div class="content">A sane DevOps workflow is with declarative systems like NixOS or Guix System, definitively not on a VM infra in practice regularly not up to date, full of useless deps, on a host definitively not up to date, with the entire infra typically not much managed nor manageable and with an immense attack surface...<p>VMs are useful for those who live on the shoulder of someone else (i.e. *aaS) witch is ALL but insecure.</div><br/><div id="41073724" class="c"><input type="checkbox" id="c-41073724" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41072770">parent</a><span>|</span><a href="#41073670">next</a><span>|</span><label class="collapse" for="c-41073724">[-]</label><label class="expand" for="c-41073724">[2 more]</label></div><br/><div class="children"><div class="content">VMs are useful when you don&#x27;t own or rent dedicated hardware. Which is a lot of cases, especially when your load varies seriously over the day or week.<p>And even if you do manage dedicated servers, it&#x27;s often wise to use VMs on them to better isolate parts of the system, aka limit the blast radius.</div><br/><div id="41076020" class="c"><input type="checkbox" id="c-41076020" checked=""/><div class="controls bullet"><span class="by">kkfx</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073724">parent</a><span>|</span><a href="#41073670">next</a><span>|</span><label class="collapse" for="c-41076020">[-]</label><label class="expand" for="c-41076020">[1 more]</label></div><br/><div class="children"><div class="content">Witch is a good recipe to pay much more thinking to be smart and pay less, being tied to some third parties decisions for anything running, having a giant attack surface and so on...<p>There are countless lessons about how owning hw is cheaper than not, there are countless examples of &quot;cloud nightmares&quot;, countless examples of why a system need to be simple and securely design from start not &quot;isolated&quot;, but people refuse to learn, specially since they are just employees for living on the shoulder of someone else means less work to do and managers typically do not know even the basic of IT to understand.</div><br/></div></div></div></div><div id="41073670" class="c"><input type="checkbox" id="c-41073670" checked=""/><div class="controls bullet"><span class="by">secondcoming</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41072770">parent</a><span>|</span><a href="#41073724">prev</a><span>|</span><a href="#41076576">next</a><span>|</span><label class="collapse" for="c-41073670">[-]</label><label class="expand" for="c-41073670">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure what you&#x27;re referring to here?<p>Our cloud machines are largely VMs. Deployments mean building a new image and telling GCP to deploy that as machines come and go due to scaling. The software is up to date, dependencies are managed via ansible.<p>Maybe you think VMs means monoliths? That doesn&#x27;t have to be the case.</div><br/><div id="41076006" class="c"><input type="checkbox" id="c-41076006" checked=""/><div class="controls bullet"><span class="by">kkfx</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073670">parent</a><span>|</span><a href="#41076576">next</a><span>|</span><label class="collapse" for="c-41076006">[-]</label><label class="expand" for="c-41076006">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s precisely the case: instead of owning hw, witch per-machine it&#x27;s a kind-of monolith (even counting blades and other modular solution), you deploy a full OS or half-full to run just a single service, on top of another &quot;OS&quot;. Of course yes, this is the cloud model, and is also the ancient and deprecated mainframe model, with much more added complexity and no unique ownership with an enormously big attack surface.<p>Various return of experience prove that cloud model is not cheap nor reliable than owning iron, it&#x27;s just fast since you live on the shoulders of someone else. A speed you will pay at an unknown point in time when something happen and you have zero control other that.<p>DevOps meaning the Devs taking over the Ops without having the needed competences, it&#x27;s a modern recipe to a failing digital ecosystems and we witnessed that more and more with various &quot;biblical outages&quot; from &quot;Roomba devices briked due to an AWS mishap, cars of a certain vendor with a slice or RCEs, payment systems outages, ... a resilient infra it&#x27;s not a centrally managed decentralized infra, it&#x27;s a vast and diverse ecosystem interoperating with open and standard tools and protocols. Classic mail or Usenet infra are resilient, GMail backed by Alphabet infra is not.<p>What if Azure tomorrow collapse? What&#x27;s the impact? What&#x27;s the attack surface of living on the shoulder of someone else, typically much bigger than you and often in other countries where getting even legal protections is costly and complex?<p>Declarative systems on iron means you can replicate your infra ALONE on the iron, VMs meaning you need much more resources and you do not even know the entire stack of your infra, you can&#x27;t essentially replicate nothing. VMs&#x2F;images are still made the classical &#x27;80s style semi-manual way with some automation written by a dev knowing just how to manage his&#x2F;her own desktop a bit and others will use it careless &quot;it&#x27;s easy to destroy and re-start&quot;, as a result we have seen in production images with someone unknown SSH authorized keys because to be quick someone pick the first ready made image from Google Search and add just few things, we are near the level of crap of the dot-com bubble, with MUCH more complexity and weight.</div><br/><div id="41076314" class="c"><input type="checkbox" id="c-41076314" checked=""/><div class="controls bullet"><span class="by">bbarnett</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41076006">parent</a><span>|</span><a href="#41076576">next</a><span>|</span><label class="collapse" for="c-41076314">[-]</label><label class="expand" for="c-41076314">[1 more]</label></div><br/><div class="children"><div class="content">(note .. use &#x27;which&#x27; not &#x27;witch&#x27;, quite different words)<p>Not sure if you mentioned it, but cost and scaling is an absurd trick of AWS and others.  AWS is literally 1000s, and in some usage cases even millions of times more expensive than your own hardware.  Some believe that employee cost savings help here, but that&#x27;s not even remotely close.<p>Scaling is absurd.  You can buy <i>one</i> server worth $10k, that can handle the equivalent of thousands upon thousands of AWS instances&#x27; workload.  You can buy far cheaper servers ($2k each), colo them yourself, have failover capability, and even have multi-datacentre redundancy, immensely cheaper than AWS.  1000 of times cheaper.  All with more power than you&#x27;d ever, ever, ever scale at AWS.<p>All that engineering to scale, all that effort to containerize, all that reliance upon AWS and their support system.. unneeded.  You can still run docker locally, or VMs, or just pound it out to raw hardware.<p>So on top of your &quot;run it on bare metal&quot; concept, there&#x27;s the whole &quot;why are you wasting time and spending money&quot; for AWS, argument.  It&#x27;s so insanely expensive.  I cannot repeat enough how insanely expensive AWS is.  I cannot repeat enough how AWS scaling is a lie, when you don&#x27;t NEED to scale using local hardware.  You just have so much more power.<p>Now.. there is one caveat, and you touch on this.  Skill.  Expertise.  As in, you have to actually not do Really Dumb Things, like write code that uses 1000s of times CPU to do the same task, or write DB queries or schema that eat up endless resources.  But of course, if you do those things on your own hardware, in DEV, you can see them and fix.<p>If you do those in AWS, people just shrug, and <i>pay immense sums of money</i> and never figure it out.<p>I wonder, how many startups have failed due to AWS costs?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41076576" class="c"><input type="checkbox" id="c-41076576" checked=""/><div class="controls bullet"><span class="by">dhx</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41071480">prev</a><span>|</span><a href="#41075652">next</a><span>|</span><label class="collapse" for="c-41076576">[-]</label><label class="expand" for="c-41076576">[1 more]</label></div><br/><div class="children"><div class="content">The article doesn&#x27;t read to me to be an argument about whether sharing a kernel is better or worse (multiple virtual machines each with their own kernel versus multiple containers isolated by a single kernel).<p>The article instead reads to me as an argument for isolating customers to their own customer-specific systems so there is no web server daemon, database server, file system path or other shared system used by multiple customers.<p>As an aside to the article, two virtual machines each with their own kernel are generally forced to communicate with each in more complex ways through network protocols which add more complexity and increase risk of implementation flaws and vulnerabilities existing. Two processes in different cgroups with a common kernel have other simpler communication options available such as being able to read the same file directly, UNIX domain sockets, named pipes, etc.</div><br/></div></div><div id="41075652" class="c"><input type="checkbox" id="c-41075652" checked=""/><div class="controls bullet"><span class="by">lkrubner</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41076576">prev</a><span>|</span><a href="#41071258">next</a><span>|</span><label class="collapse" for="c-41075652">[-]</label><label class="expand" for="c-41075652">[1 more]</label></div><br/><div class="children"><div class="content">I agree. VMs rely on old technologies, and are reliable in that way. By contrast, the move to Docker then necessitated additional technologies, such as Kubernetes, and Kubernetes brought an avalanche of new technologies to help manage Docker&#x2F;Kubernetes. I am wary of any technology that in theory should make things simpler but in fact draws you down a path that requires you to learn a dozen new technologies. The Docker&#x2F;Kubernetes path also drove up costs, especially the cost associated with the time needed to set up the devops correctly. Anything that takes time costs money. When I was at Averon the CEO insisted on absolutely perfect reliability and therefore flawless devops, so we hired a great devops guy to help us get setup, but he needed several weeks to set everything up, and his hourly rate was expensive. We could have just &quot;push some code to a server&quot; and we would have saved $40,000. When I consult with early stage startups, and they worry about the cost of devops, I point out that we can start simply, by pushing some code to a server, as if this was still 2001, and we can proceed slowly and incrementally from there. While Docker&#x2F;Kubernetes offers infinite scalability, I warn entrepreneurs that their first concern should be keeping things simple and therefore low cost. And then the next step is to introduce VMs, and then use something like Packer to enable the VMs to be uses as AMIs and so allow the devops to develop to the point of using Terraform -- but all of that can wait till the product actually gains some traction.</div><br/></div></div><div id="41071258" class="c"><input type="checkbox" id="c-41071258" checked=""/><div class="controls bullet"><span class="by">dboreham</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41075652">prev</a><span>|</span><a href="#41072050">next</a><span>|</span><label class="collapse" for="c-41071258">[-]</label><label class="expand" for="c-41071258">[22 more]</label></div><br/><div class="children"><div class="content">For me it&#x27;s about the ROAC property (Runs On Any Computer). I prefer working with stuff that I can run. Running software is live software, working software, loved software. Software that only works in weird places is bad, at least for me.
Docker is pretty crappy in most respects, but it has the ROAC going for it.<p>I would <i>love</i> to have a &quot;docker-like thing&quot; (with ROAC) that used VMs not containers (or some other isolation tech that works). But afaik that thing does not yet exist. Yes there are several &quot;container-tool, but we made it use VMs&quot; (firecracker and downline), but they all need weirdo special setup, won&#x27;t run on my laptop, or a generic Digitalocean VM.</div><br/><div id="41071308" class="c"><input type="checkbox" id="c-41071308" checked=""/><div class="controls bullet"><span class="by">01HNNWZ0MV43FF</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071258">parent</a><span>|</span><a href="#41072284">next</a><span>|</span><label class="collapse" for="c-41071308">[-]</label><label class="expand" for="c-41071308">[10 more]</label></div><br/><div class="children"><div class="content">Yeah that&#x27;s kind of a crummy tradeoff.<p>Docker is &quot;Runs on any Linux, mostly, if you have a new enough kernel&quot; meaning it packages a big VM anyway for Windows and macOS<p>VMs are &quot;Runs on anything! ... Sorta, mostly, if you have VM acceleration&quot; meaning you have to pick a VM software and hope the VM doesn&#x27;t crash for no reason. (I have real bad luck with UTM and VirtualBox on my Macbook host for some reason.)<p>All I want is everything - An APE-like program that runs on any OS, maybe has shims for slightly-old kernels, doesn&#x27;t need a big installation step, and runs any useful guest OS. (i.e. Linux)</div><br/><div id="41071547" class="c"><input type="checkbox" id="c-41071547" checked=""/><div class="controls bullet"><span class="by">compsciphd</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071308">parent</a><span>|</span><a href="#41071506">next</a><span>|</span><label class="collapse" for="c-41071547">[-]</label><label class="expand" for="c-41071547">[1 more]</label></div><br/><div class="children"><div class="content">docker is your userspace program carries all its user space dependencies with it and doesn&#x27;t depend on the userspace configuration of the underlying system.<p>What I argued in my paper is that systems like docker (i.e. what I created before it), improve over VMs and (even Zones&#x2F;ZFS) in their ability to really run ephemeral computation.  i.e. if it takes microseconds to setup the container file system, you can run a boatload of heterogeneous containers even if they only needed to run for very shot periods of time).  Solaris Zones&#x2F;ZFS didn&#x27;t lend itself to heterogeneous environments, but simply cloning as single homogeneous environment, while VMs suffered from that problem, they also (at least at the time, much improved as of late) required a reasonably long bootup time.</div><br/></div></div><div id="41071506" class="c"><input type="checkbox" id="c-41071506" checked=""/><div class="controls bullet"><span class="by">neaanopri</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071308">parent</a><span>|</span><a href="#41071547">prev</a><span>|</span><a href="#41072284">next</a><span>|</span><label class="collapse" for="c-41071506">[-]</label><label class="expand" for="c-41071506">[8 more]</label></div><br/><div class="children"><div class="content">The modern developer yearns for Java</div><br/><div id="41071894" class="c"><input type="checkbox" id="c-41071894" checked=""/><div class="controls bullet"><span class="by">smallmancontrov</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071506">parent</a><span>|</span><a href="#41072321">next</a><span>|</span><label class="collapse" for="c-41071894">[-]</label><label class="expand" for="c-41071894">[5 more]</label></div><br/><div class="children"><div class="content">I had to use eclipse the other day. How the hell is it just as slow and clunky as I remember from 20 years ago? Does it exist in a pocket dimension where Moore&#x27;s Law doesn&#x27;t apply?</div><br/><div id="41072500" class="c"><input type="checkbox" id="c-41072500" checked=""/><div class="controls bullet"><span class="by">qwery</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071894">parent</a><span>|</span><a href="#41072376">next</a><span>|</span><label class="collapse" for="c-41072500">[-]</label><label class="expand" for="c-41072500">[3 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s pretty remarkable to see any application in continuous use for so long, especially with so few changes[0] -- Eclipse must be doing something right!<p>Maintaining (if not actively improving&#x2F;developing) a piece of useful software without performance <i>degradation</i> -- that&#x27;s a win.<p>Keeping that up for decades? That&#x27;s exceptional.<p>[0] &quot;so few changes&quot;: I&#x27;m not commenting on the amount of work done on the project or claiming that there is no useful&#x2F;visible features added or upgrades, but referring to Eclipse of today feeling like the same application as it always did, and that Eclipse hasn&#x27;t had multiple alarmingly frequent &quot;reboots&quot;, &quot;overhauls&quot;, etc.<p>[?] keeping performance constant over the last decade or two is a win, relatively speaking, anyway</div><br/><div id="41072639" class="c"><input type="checkbox" id="c-41072639" checked=""/><div class="controls bullet"><span class="by">dijit</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41072500">parent</a><span>|</span><a href="#41073751">next</a><span>|</span><label class="collapse" for="c-41072639">[-]</label><label class="expand" for="c-41072639">[1 more]</label></div><br/><div class="children"><div class="content">I agree, that you&#x27;ve pointed it out to me makes it obvious that this is not the norm, and we <i>should</i> celebrate this.<p>I&#x27;m reminded of Casey Muratori&#x27;s rant on Visual Studio; a program that largely feels like it hasn&#x27;t changed much but clearly has regressed in performance massively; <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=GC-0tCy4P1U" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=GC-0tCy4P1U</a></div><br/></div></div><div id="41073751" class="c"><input type="checkbox" id="c-41073751" checked=""/><div class="controls bullet"><span class="by">password4321</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41072500">parent</a><span>|</span><a href="#41072639">prev</a><span>|</span><a href="#41072376">next</a><span>|</span><label class="collapse" for="c-41073751">[-]</label><label class="expand" for="c-41073751">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>without performance degradation</i><p>Not accounting for Moore&#x27;s Law, yikes. Need a comparison adjusted for &quot;today&#x27;s dollars&quot;.</div><br/></div></div></div></div><div id="41072376" class="c"><input type="checkbox" id="c-41072376" checked=""/><div class="controls bullet"><span class="by">TurningCanadian</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071894">parent</a><span>|</span><a href="#41072500">prev</a><span>|</span><a href="#41072321">next</a><span>|</span><label class="collapse" for="c-41072376">[-]</label><label class="expand" for="c-41072376">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not Java&#x27;s fault though. IntelliJ IDEA is also built on Java and runs just fine.</div><br/></div></div></div></div><div id="41072321" class="c"><input type="checkbox" id="c-41072321" checked=""/><div class="controls bullet"><span class="by">gryfft</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071506">parent</a><span>|</span><a href="#41071894">prev</a><span>|</span><a href="#41072617">next</a><span>|</span><label class="collapse" for="c-41072321">[-]</label><label class="expand" for="c-41072321">[1 more]</label></div><br/><div class="children"><div class="content">Maybe just the JVM.</div><br/></div></div><div id="41072617" class="c"><input type="checkbox" id="c-41072617" checked=""/><div class="controls bullet"><span class="by">mschuster91</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071506">parent</a><span>|</span><a href="#41072321">prev</a><span>|</span><a href="#41072284">next</a><span>|</span><label class="collapse" for="c-41072617">[-]</label><label class="expand" for="c-41072617">[1 more]</label></div><br/><div class="children"><div class="content">Java&#x27;s ecosystem is just as bad. Gradle is insanely flexible but people create abominations out of it, Maven is extremely rigid so people resort to even worse abominations to get basic shit done.</div><br/></div></div></div></div></div></div><div id="41072284" class="c"><input type="checkbox" id="c-41072284" checked=""/><div class="controls bullet"><span class="by">ThreatSystems</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071258">parent</a><span>|</span><a href="#41071308">prev</a><span>|</span><a href="#41072050">next</a><span>|</span><label class="collapse" for="c-41072284">[-]</label><label class="expand" for="c-41072284">[11 more]</label></div><br/><div class="children"><div class="content">Vagrant &#x2F; Packer?</div><br/><div id="41074429" class="c"><input type="checkbox" id="c-41074429" checked=""/><div class="controls bullet"><span class="by">stackskipton</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41072284">parent</a><span>|</span><a href="#41073117">next</a><span>|</span><label class="collapse" for="c-41074429">[-]</label><label class="expand" for="c-41074429">[1 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t work here, they have software on each VM that cannot be reimaged. To use Packer properly, you should treat like you do stateless pod, just start a new one and take down the old one.</div><br/></div></div><div id="41073117" class="c"><input type="checkbox" id="c-41073117" checked=""/><div class="controls bullet"><span class="by">gavindean90</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41072284">parent</a><span>|</span><a href="#41074429">prev</a><span>|</span><a href="#41072050">next</a><span>|</span><label class="collapse" for="c-41073117">[-]</label><label class="expand" for="c-41073117">[9 more]</label></div><br/><div class="children"><div class="content">With all the mind share that terraform gets you would thing vagrant would at least be known but alas</div><br/><div id="41073418" class="c"><input type="checkbox" id="c-41073418" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073117">parent</a><span>|</span><a href="#41072050">next</a><span>|</span><label class="collapse" for="c-41073418">[-]</label><label class="expand" for="c-41073418">[8 more]</label></div><br/><div class="children"><div class="content">Somebody educate me about the problem Packer would solve for you in 2024?</div><br/><div id="41074892" class="c"><input type="checkbox" id="c-41074892" checked=""/><div class="controls bullet"><span class="by">sgarland</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073418">parent</a><span>|</span><a href="#41073983">next</a><span>|</span><label class="collapse" for="c-41074892">[-]</label><label class="expand" for="c-41074892">[3 more]</label></div><br/><div class="children"><div class="content">Making machine images. AWS calls them AMIs. Whatever your platform, that&#x27;s what it&#x27;s there for. It&#x27;s often combined with Ansible, and basically runs like this:<p>1. Start a base image of Debian &#x2F; Ubuntu &#x2F; whatever – this is often done with Terraform.<p>2. Packer types a boot command after power-on to configure whatever you&#x27;d like<p>3. Packer manages the installation; with Debian and its derivatives, this is done mostly through the arcane language of preseed [0]<p>4. As a last step, a pre-configured SSH password is set, then the new base VM reboots<p>5. Ansible detects SSH becoming available, and takes over to do whatever you&#x27;d like.<p>6. Shut down the VM, and create clones as desired. Manage ongoing config in a variety of ways – rolling out a new VM for any change, continuing with Ansible, shifting to Puppet, etc.<p>[0]: <a href="https:&#x2F;&#x2F;wiki.debian.org&#x2F;DebianInstaller&#x2F;Preseed" rel="nofollow">https:&#x2F;&#x2F;wiki.debian.org&#x2F;DebianInstaller&#x2F;Preseed</a></div><br/><div id="41075556" class="c"><input type="checkbox" id="c-41075556" checked=""/><div class="controls bullet"><span class="by">pxc</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41074892">parent</a><span>|</span><a href="#41075874">next</a><span>|</span><label class="collapse" for="c-41075556">[-]</label><label class="expand" for="c-41075556">[1 more]</label></div><br/><div class="children"><div class="content">This is nice in its uniformity (same tool works for any distro that has an existing AMI to work with), but it&#x27;s insanely slow compared to just putting a rootfs together and uploading it as an image.<p>I think I&#x27;d usually rather just use whatever distro-specific tools for putting together a li&#x27;l chroot (e.g., debootstrap, pacstrap, whatever) and building a suitable rootfs in there, then finish it up with amazon-ec2-ami-tools or euca2ools or whatever and upload directly. The pace of iteration with Packer is just really painful for me.</div><br/></div></div><div id="41075874" class="c"><input type="checkbox" id="c-41075874" checked=""/><div class="controls bullet"><span class="by">stargrazer</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41074892">parent</a><span>|</span><a href="#41075556">prev</a><span>|</span><a href="#41073983">next</a><span>|</span><label class="collapse" for="c-41075874">[-]</label><label class="expand" for="c-41075874">[1 more]</label></div><br/><div class="children"><div class="content">I miss saltstack.  I did that whole litany of steps with one tool plus preseed.</div><br/></div></div></div></div><div id="41073983" class="c"><input type="checkbox" id="c-41073983" checked=""/><div class="controls bullet"><span class="by">kasey_junk</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073418">parent</a><span>|</span><a href="#41074892">prev</a><span>|</span><a href="#41074072">next</a><span>|</span><label class="collapse" for="c-41073983">[-]</label><label class="expand" for="c-41073983">[2 more]</label></div><br/><div class="children"><div class="content">I think the thread is more about how docker was a reaction to the vagrant&#x2F;packer ecosystem that was deemed overweight but was in many ways was a “docker like thing” but VMs.</div><br/><div id="41074131" class="c"><input type="checkbox" id="c-41074131" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073983">parent</a><span>|</span><a href="#41074072">next</a><span>|</span><label class="collapse" for="c-41074131">[-]</label><label class="expand" for="c-41074131">[1 more]</label></div><br/><div class="children"><div class="content">Oh, yeah, I&#x27;m not trying to prosecute, I&#x27;ve just always been Packer-curious.</div><br/></div></div></div></div><div id="41074072" class="c"><input type="checkbox" id="c-41074072" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073418">parent</a><span>|</span><a href="#41073983">prev</a><span>|</span><a href="#41072050">next</a><span>|</span><label class="collapse" for="c-41074072">[-]</label><label class="expand" for="c-41074072">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s a better way to make VM images?</div><br/><div id="41076103" class="c"><input type="checkbox" id="c-41076103" checked=""/><div class="controls bullet"><span class="by">DaanDeMeyer</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41074072">parent</a><span>|</span><a href="#41072050">next</a><span>|</span><label class="collapse" for="c-41076103">[-]</label><label class="expand" for="c-41076103">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s lots of tools in this space. I work on <a href="https:&#x2F;&#x2F;github.com&#x2F;systemd&#x2F;mkosi">https:&#x2F;&#x2F;github.com&#x2F;systemd&#x2F;mkosi</a> for example.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41072050" class="c"><input type="checkbox" id="c-41072050" checked=""/><div class="controls bullet"><span class="by">vundercind</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41071258">prev</a><span>|</span><a href="#41075654">next</a><span>|</span><label class="collapse" for="c-41072050">[-]</label><label class="expand" for="c-41072050">[5 more]</label></div><br/><div class="children"><div class="content">Docker’s the best cross-distro rolling-release package manager and init system for services—staying strictly out of managing the base system, which is great—that I know of. I don’t know of anything that’s even close, really.<p>All the other stuff about it is way less important to me than that part.</div><br/><div id="41074356" class="c"><input type="checkbox" id="c-41074356" checked=""/><div class="controls bullet"><span class="by">pxc</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41072050">parent</a><span>|</span><a href="#41075654">next</a><span>|</span><label class="collapse" for="c-41074356">[-]</label><label class="expand" for="c-41074356">[4 more]</label></div><br/><div class="children"><div class="content">This is wrong in pretty much every way I can imagine.<p>Docker&#x27;s not a package manager. It doesn&#x27;t know what packages are, which is part of why the chunks that make up Docker containers (image layers) are so coarse. This is also part of why many Docker images are so huge: you don&#x27;t know exactly the packages you need, strictly speaking, so you start from a whole OS. This is also why your Dockerfiles all invoke real package managers— Docker can&#x27;t know how to install packages if it doesn&#x27;t know what they are!<p>It&#x27;s also not cross-platform, or at least 99.999% of images you might care about aren&#x27;t— they&#x27;re Linux-only.<p>It&#x27;s also not a service manager, unless you mean docker-compose (which is not as good as systemd or any number of other process supervisors) or Docker Swarm (which has lost out to Kubernetes). (I&#x27;m not sure what you even mean by &#x27;init system for containers&#x27; since most containers don&#x27;t include an init system.)<p>There actually are cross-platform package managers out there, too. Nix, Pkgsrc, Homebrew, etc. All of those I mentioned and more have rolling release repositories as well. (&#x27;Rolling release&#x27; is not a feature of package managers; there is no such thing as a &#x27;rolling release package manager&#x27;.)</div><br/><div id="41074642" class="c"><input type="checkbox" id="c-41074642" checked=""/><div class="controls bullet"><span class="by">vundercind</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41074356">parent</a><span>|</span><a href="#41075654">next</a><span>|</span><label class="collapse" for="c-41074642">[-]</label><label class="expand" for="c-41074642">[3 more]</label></div><br/><div class="children"><div class="content">&gt; This is wrong in pretty much every way I can imagine.<p>Nope! It’s not wrong in any way at all!<p>You’re thinking of how it’s built. I’m thinking of what it does (for me).<p>I tell it a package (image) to fetch, optionally at a version. It has a very large set of well maintained up-to-date packages (images). It’s built-in, I don’t even have to configure that part, though I can have it use other sources for packages if I want to. It fetches the package. If I want it to update it, I can have it do that too. Or uninstall it. Or roll back the version. I am 100% for-sure using it as a package manager, and it does that job well.<p>Then I run a service with a simple shell script (actually, I combine the fetching and running, but I’m highlighting the two separate roles it performs for me). It takes care of managing the process (image, which is really just a very-fat process for these purposes). It restarts it if it crashes, if I like. It auto-starts it when the machine reboots—all my services come back up on boot, and I’ve never touched systemd (which my Debian uses), Docker is my interface to that and I didn’t even have to configure it to do that part. I’m sure it’s doing systemd stuff under the hood, at least to bring the docker daemon up, but I’ve never touched that and it’s <i>not</i> my interface to managing my services. The docker command is. Do I see what’s running with systemd or ps? No, with docker. Start, restart, or stop a service? Docker.<p>I’ve been running hobbyist servers at home (and setting up and administrating “real” ones for work) since 2000 or so and this is the smoothest way to do it that I’ve seen, at least for the hobbyist side. Very nearly the only roles I’m using Docker to fill, in this scenario, are package manager and service manager.<p>I don’t care how it works—I know how, but the details don’t matter for my use case, just the outcomes. The outcome is that I have excellent, updated, official packages for way more services than are in the Debian repos, that leave my base system entirely alone and don’t meaningfully interact with it, with config that’s highly portable to any other distro, all managed with a common interface that would <i>also</i> be the same on any other distro. I don’t have to give any shits about my distro, no “oh if I want to run this I have to update the whole damn distro to a new major version or else manually install some newer libraries and hope that doesn’t break anything”, I just run packages (images) from Docker, update them with Docker, and run them with Docker. Docker is my UI for everything that matters except ZFS pool management.<p>&gt; It&#x27;s also not cross-platform, or at least 99.999% of images you might care about aren&#x27;t— they&#x27;re Linux-only.<p>I specifically wrote cross-distro for this reason.<p>&gt; There actually are cross-platform package managers out there, too. Nix, Pkgsrc, Homebrew, etc.<p>Docker “packages” have a broader selection and better support than any of those, as far as services&#x2F;daemons go; it’s guaranteed to keep everything away from the base system and tidy for better stability; and it provides a common interface for configuring where to put files &amp; config for easier and more-confident backup.<p>I <i>definitely</i> use it mainly as a package manager and service manager, and find it better than any alternative for that role.</div><br/><div id="41075479" class="c"><input type="checkbox" id="c-41075479" checked=""/><div class="controls bullet"><span class="by">pxc</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41074642">parent</a><span>|</span><a href="#41075654">next</a><span>|</span><label class="collapse" for="c-41075479">[-]</label><label class="expand" for="c-41075479">[2 more]</label></div><br/><div class="children"><div class="content">&gt; You’re thinking of how it’s built. I’m thinking of what it does (for me).<p>I&#x27;ve read your reply and I hear you (now). But as far as I&#x27;m concerned package management is a little more than that. Not everything that installs or uninstalls software is a package manager-- for instance I would say that winget and Chocolatey are hardly package managers, despite their pretensions (scoop is closer). I think of package management, as an approach to managing software and as a technology, as generally characterized by things like and including: dependency tracking, completeness (packages&#x27; dependencies are themselves all packaged, recursively, all the way down), totality (installing software by any means other than the package manager is not required to have a practically useful system), minimal redundancy of dependencies common to multiple packages, collective aggregation and curation of packages, transparency (the unit the software management tool operates on, the package, tracks the versions of the software contained in it and the versions of the software contained in its dependencies), exclusivity (packaged software does not self-update; updates all come through the package manager), etc. Many of these things come in degrees, and many package managers do not have all of them to the highest degree possible. But the way Docker gets software running on your system just isn&#x27;t meaningfully aligned with that paradigm, and this also impacts the way Docker can be used. I won&#x27;t enumerate Docker&#x27;s deviations from this archetype because it sounds like you already have plenty of relevant experience and knowledge.<p>&gt; I don’t care how it works—I know how, but the details don’t matter for my use case, just the outcomes.<p>When there&#x27;s a vuln in your libc or some similar common dependency, Docker can&#x27;t tell you about which of your images contains it because it has no idea what glibc or liblzma are. The whole practice of generating SBOMs is about trying to recover or regenerate data that is already easily accessible in any competent package manager (and indeed, the tools that generate SBOMs for container images depend on actual package managers to get that data, which is why their support comes distro-by-distro).<p>Managing Docker containers is also complicated in some ways that managing conventional packages (even in other containerized formats like Flatpak, Snap, and AppImage) isn&#x27;t, in that you have to worry about bind mounts and port forwarding. How the software works leads to a radically different sort of practice. (Admittedly maybe that&#x27;s still a bit distant from very broad outcomes like &#x27;I have postgres running&#x27;.)<p>&gt; The outcome is that I have [many services] that leave my base system entirely alone and don’t meaningfully interact with it, with config that’s highly portable to any other distro, all managed with a common interface that would also be the same on any other distro.<p>This is indeed a great outcome. But when you achieve it with Docker, the practice by means of which you&#x27;ve achieved it is not really a package management discipline but something else. And that is (sadly, to me) part of the appeal, right? Package management can be a <i>really miserable</i> paradigm when your packages all live in a single, shared global namespace (the paths on your filesystem, starting with &#x2F;). Docker broke with that paradigm specifically to address that pain.<p>But that&#x27;s not the end of the story! That same excellent outcome is also achievable by better package managers than ol&#x27; yum&#x2F;dnf and apt! And when you go that route, you also get back the benefits of the old regime like the ability to tell what&#x27;s on your system and easily patch small pieces of it once-and-for-all. Nix and Guix are great for this and work in all the same scenarios, and can also readily generate containers from arbitrary packages for those times you need the resource management aspect of containers.<p>&gt; The outcome is that I have [...] official packages<p>For me, this is not a benefit. I think the curation, integration, vetting, and patching that coordinated software distributions do is extremely valuable, and I expect the average software developer to be much worse at packaging and systems administration tasks than the average contributor to a Linux distro is. To me, this feels like a step backwards into chaos, like apps self-updating or something like that. It makes me think of all the absolutely insane things I&#x27;ve seen Java developers do with Maven and Gradle, or entire communities of hobbyists who depend on software whose build process is so brittle and undocumented that seemingly no one knows how to build it and Docker has become the sole supported distribution mechanism.<p>&gt; I specifically wrote cross-distro for this reason.<p>My bad! Although that actually widens the field of contenders to include Guix, which is excellent, and arguably also Flatpak, which still aligns fairly well with package management as an approach despite being container-based.<p>&gt; Docker “packages” have a broader selection and better support than any of those, as far as services&#x2F;daemons go<p>I suppose this is an advantage of a decentralized authority-to-publish, like we also see in the AUR or many language-specific package repositories, and also of freedom from the burden of integration, since all docker image authors have to do is put together any runtime at all that runs. :-\<p>&gt; service manager<p>Ok. So you&#x27;re just having dockerd autostart your containers, then, no docker-compose or Docker Swarm or some other layer on top? Does that even have a notion of dependencies between services? That feels like table stakes for me for &#x27;<i>good</i> service manager&#x27;.<p>PS: thanks for giving a charitable and productive reply to a comment where I was way gratuitously combative about a pet peeve&#x2F;hobby horse of mine for no good reason</div><br/><div id="41076129" class="c"><input type="checkbox" id="c-41076129" checked=""/><div class="controls bullet"><span class="by">vundercind</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41075479">parent</a><span>|</span><a href="#41075654">next</a><span>|</span><label class="collapse" for="c-41076129">[-]</label><label class="expand" for="c-41076129">[1 more]</label></div><br/><div class="children"><div class="content">Oh no, you’re fine, thanks for responding in kind, I get where you’re coming from now too. Maybe it’s clearer to label my use of it as a <i>software manager</i> or something like that. It does end up being my main interface for nearly everything that matters on my home server.<p>Like, I’m damn near running Docker&#x2F;Linux, in the pattern of gnu&#x2F;Linux or (as started as a bit of a joke, but is less so with each year) systemd&#x2F;Linux, as far as the key parts that I interact with and care about and that <i>complete</i> the OS for me.<p>As a result, some docker alternatives <i>aren’t</i> alternatives for me—I want the consistent, fairly complete UI for the things I use it for, and the huge library of images, largely official. I can’t just use raw lxc or light VMs instead, as that gets me almost nothing of what I’m currently benefiting from.<p>I haven’t run into a need to have dependent services (I take the SQLite option for anything that has it—makes backups trivial) but probably would whip up a little docker-compose for if I ever need that. In work contexts I usually just go straight for docker-compose, but with seven or eight independent services on my home server I’ve found I prefer tiny shell scripts for each one.<p>[edit] oh, and I get what you mean about it not <i>really</i> doing things like solving software dependencies—it’s clearly not suitable as, like, a <i>system</i> package manager, but fills the role well enough for me when it comes to the high-level “packages” I’m intending to use directly.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41075654" class="c"><input type="checkbox" id="c-41075654" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41072050">prev</a><span>|</span><a href="#41071295">next</a><span>|</span><label class="collapse" for="c-41075654">[-]</label><label class="expand" for="c-41075654">[1 more]</label></div><br/><div class="children"><div class="content">Namespaces and cgroups and LXC and the whole alphabet soup, the “Docker Industrial Complex” to borrow your inspired term, this stuff can make sense if you rack your own gear: you want one level of indirection.<p>As I’ve said many times, putting a container on a serverless on a Xen hypervisor so you can virtualize while you virtualize? I get why The Cloud wants this, but I haven’t the foggiest idea why people sit still for it.<p>As a public service announcement? If you’re paying three levels of markup to have three levels of virtual machine?<p>You’ve been had.</div><br/></div></div><div id="41071295" class="c"><input type="checkbox" id="c-41071295" checked=""/><div class="controls bullet"><span class="by">nimish</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41075654">prev</a><span>|</span><a href="#41076648">next</a><span>|</span><label class="collapse" for="c-41071295">[-]</label><label class="expand" for="c-41071295">[8 more]</label></div><br/><div class="children"><div class="content">Clear Containers&#x2F;Kata Containers&#x2F;firecracker VMs showed that there isn&#x27;t really a dichotomy here. Why we aren&#x27;t all using HW assisted containers is a mystery.</div><br/><div id="41072822" class="c"><input type="checkbox" id="c-41072822" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071295">parent</a><span>|</span><a href="#41071388">next</a><span>|</span><label class="collapse" for="c-41072822">[-]</label><label class="expand" for="c-41072822">[6 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not at all mysterious: to run hardware-virtualized containers, you need your compute hosted on a platform that will allow KVM. That&#x27;s a small, expensive, tenuously available subset of AWS, which is by far the dominant compute platform.</div><br/><div id="41073431" class="c"><input type="checkbox" id="c-41073431" checked=""/><div class="controls bullet"><span class="by">Spivak</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41072822">parent</a><span>|</span><a href="#41071388">next</a><span>|</span><label class="collapse" for="c-41073431">[-]</label><label class="expand" for="c-41073431">[5 more]</label></div><br/><div class="children"><div class="content">So… Lambda, Fargate, and EC2. The only thing you can&#x27;t really do this with is EKS.<p>Like Firecracker was made by AWS to run containers on their global scale KVM, EC2.</div><br/><div id="41073479" class="c"><input type="checkbox" id="c-41073479" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073431">parent</a><span>|</span><a href="#41071388">next</a><span>|</span><label class="collapse" for="c-41073479">[-]</label><label class="expand" for="c-41073479">[4 more]</label></div><br/><div class="children"><div class="content">Lambda and Fargate are implementations of the idea, not a way for you yourself to do any kind of KVM container provisioning. You can&#x27;t generally do this on EC2; you need special instances for it.<p>For a variety of reasons, I&#x27;m pretty familiar with Firecracker.</div><br/><div id="41074452" class="c"><input type="checkbox" id="c-41074452" checked=""/><div class="controls bullet"><span class="by">Spivak</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41073479">parent</a><span>|</span><a href="#41071388">next</a><span>|</span><label class="collapse" for="c-41074452">[-]</label><label class="expand" for="c-41074452">[3 more]</label></div><br/><div class="children"><div class="content">What I&#x27;m I missing? AWS offers (virtual) hardware backed containers as a service, I would go so far as to say that a significant number of people are running vm backed containers.<p>And I&#x27;ve been at a few shops where EC2 is used as the poor-man&#x27;s-firecracker by building containers and then running 1(ish) per VM. AWS&#x27;s architecture actively encourages this because that&#x27;s by far the easiest security boundary to manipulate. The moment you start thinking about two privilege levels in the same VM you&#x27;re mostly on your own.<p>The number of people running production workloads who, knowingly or not, believe that the security boundary is not between containers but between the vms enclosing those containers is probably almost everyone.</div><br/><div id="41075331" class="c"><input type="checkbox" id="c-41075331" checked=""/><div class="controls bullet"><span class="by">ryapric</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41074452">parent</a><span>|</span><a href="#41075323">next</a><span>|</span><label class="collapse" for="c-41075331">[-]</label><label class="expand" for="c-41075331">[1 more]</label></div><br/><div class="children"><div class="content">&gt;What I&#x27;m I missing?<p>The parent isn&#x27;t talking about e.g. EC2 as a <i>virtualized</i> platform, they&#x27;re talking about EC2 <i>not</i> being a <i>virtualization</i> platform. With few machine-type exceptions, EC2 doesn&#x27;t support <i>nested</i> virtualization -- you can&#x27;t run e.g. KVM on EC2.</div><br/></div></div><div id="41075323" class="c"><input type="checkbox" id="c-41075323" checked=""/><div class="controls bullet"><span class="by">kasey_junk</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41074452">parent</a><span>|</span><a href="#41075331">prev</a><span>|</span><a href="#41071388">next</a><span>|</span><label class="collapse" for="c-41075323">[-]</label><label class="expand" for="c-41075323">[1 more]</label></div><br/><div class="children"><div class="content">I think the argument is you need to be running nitro (I think, it’s been awhile?) instances to take advantage of  kvm isolation</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41071388" class="c"><input type="checkbox" id="c-41071388" checked=""/><div class="controls bullet"><span class="by">turtlebits</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071295">parent</a><span>|</span><a href="#41072822">prev</a><span>|</span><a href="#41076648">next</a><span>|</span><label class="collapse" for="c-41071388">[-]</label><label class="expand" for="c-41071388">[1 more]</label></div><br/><div class="children"><div class="content">Engineers are lazy, especially Ops.  Until it&#x27;s easier to get up and running and there are tangible benefits, people won&#x27;t care.</div><br/></div></div></div></div><div id="41076648" class="c"><input type="checkbox" id="c-41076648" checked=""/><div class="controls bullet"><span class="by">ktosobcy</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41071295">prev</a><span>|</span><a href="#41074063">next</a><span>|</span><label class="collapse" for="c-41076648">[-]</label><label class="expand" for="c-41076648">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Modern &quot;containers&quot; were invented to make things more reproducible ( check ) and simplify dev and deployments ( NOT check ).<p>Why?<p>I have my RPi4 and absolutely love docker(-compose) - deploying stuff&#x2F;services on in it just a breeze compared to previous clusterf*k of relying on system repository for apps (or if something doesnt work)... with docker compose I have nicely separated services with dedicated databases in required version (yes, I ran into an issue that one service required newer and another older version of the database, meh)<p>As for development - I do development natively but again - docker makes it easier to test various scenarios...</div><br/></div></div><div id="41074063" class="c"><input type="checkbox" id="c-41074063" checked=""/><div class="controls bullet"><span class="by">ranger207</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41076648">prev</a><span>|</span><a href="#41074594">next</a><span>|</span><label class="collapse" for="c-41074063">[-]</label><label class="expand" for="c-41074063">[1 more]</label></div><br/><div class="children"><div class="content">Docker&#x27;s good at packaging, and Kubernetes is good at providing a single API to do all the infra stuff like scheduling, storage, and networking. I think that if someone sat down and tried to create a idealized VM management solution that covered everything between &quot;dev pushes changes&quot; to &quot;user requests website&quot; then it&#x27;d probably have a single image for each VM to run (like Docker has a single image for each container to run) then management of VM hosts, storage, networking, and scheduling VMs to run on which host would wind up looking a lot like k8s. You could certainly do that with VMs but for various path dependency reasons people do that with containers instead and nobody&#x27;s got a well adopted system for doing the same with VMs</div><br/></div></div><div id="41074594" class="c"><input type="checkbox" id="c-41074594" checked=""/><div class="controls bullet"><span class="by">mountainriver</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41074063">prev</a><span>|</span><a href="#41075646">next</a><span>|</span><label class="collapse" for="c-41074594">[-]</label><label class="expand" for="c-41074594">[1 more]</label></div><br/><div class="children"><div class="content">Docker is fantastic and VMs are fantastic.<p>I honestly can’t imagine running all the services we have without containers. It would be wildly less efficient and harder to develop on.<p>VMs are wonderful when you need the security</div><br/></div></div><div id="41075646" class="c"><input type="checkbox" id="c-41075646" checked=""/><div class="controls bullet"><span class="by">icelancer</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41074594">prev</a><span>|</span><a href="#41071230">next</a><span>|</span><label class="collapse" for="c-41075646">[-]</label><label class="expand" for="c-41075646">[1 more]</label></div><br/><div class="children"><div class="content">Same. We&#x27;re still managing ESXi here at my company. Docker&#x2F;K8s&#x2F;etc are nowhere close to prod and probably never will be. Been very pleased with that decision.<p>I will say that Docker images get one HUGE use case at our company - CUDA images with consistent environments. CUDA&#x2F;pytorch&#x2F;tensorflow hell is something I couldn&#x27;t imagine dealing with when I was in college studying CS a few decades ago.</div><br/></div></div><div id="41071230" class="c"><input type="checkbox" id="c-41071230" checked=""/><div class="controls bullet"><span class="by">ganoushoreilly</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41075646">prev</a><span>|</span><a href="#41075024">next</a><span>|</span><label class="collapse" for="c-41071230">[-]</label><label class="expand" for="c-41071230">[8 more]</label></div><br/><div class="children"><div class="content">Docker is great, way overused 100%. I believe a lot of it started as &quot;cost savings&quot; on resource usage. Then it became the trendy thing for &quot;scalability&quot;.<p>When home enthusiasts build multi container stacks for their project website, it gets a bit much.</div><br/><div id="41075847" class="c"><input type="checkbox" id="c-41075847" checked=""/><div class="controls bullet"><span class="by">Yodel0914</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071230">parent</a><span>|</span><a href="#41071354">next</a><span>|</span><label class="collapse" for="c-41075847">[-]</label><label class="expand" for="c-41075847">[1 more]</label></div><br/><div class="children"><div class="content">&gt; When home enthusiasts build multi container stacks for their project website, it gets a bit much.<p>I don&#x27;t know - docker has been a godsend for running my own stuff. I can get a docker-compose file working on my laptop, then run it on my VPS with a pretty high certainty that it will work. Updating has also (to date) been incredibly smooth.</div><br/></div></div><div id="41071354" class="c"><input type="checkbox" id="c-41071354" checked=""/><div class="controls bullet"><span class="by">applied_heat</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071230">parent</a><span>|</span><a href="#41075847">prev</a><span>|</span><a href="#41075024">next</a><span>|</span><label class="collapse" for="c-41071354">[-]</label><label class="expand" for="c-41071354">[6 more]</label></div><br/><div class="children"><div class="content">Solves dependency version hell also</div><br/><div id="41071421" class="c"><input type="checkbox" id="c-41071421" checked=""/><div class="controls bullet"><span class="by">theLiminator</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071354">parent</a><span>|</span><a href="#41072341">next</a><span>|</span><label class="collapse" for="c-41071421">[-]</label><label class="expand" for="c-41071421">[4 more]</label></div><br/><div class="children"><div class="content">Solves it in the same sense that it&#x27;s a giant lockfile. It doesn&#x27;t solve the other half where updates can horribly break your system and you run into transitive version clashes.</div><br/><div id="41076655" class="c"><input type="checkbox" id="c-41076655" checked=""/><div class="controls bullet"><span class="by">ktosobcy</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071421">parent</a><span>|</span><a href="#41071795">next</a><span>|</span><label class="collapse" for="c-41076655">[-]</label><label class="expand" for="c-41076655">[1 more]</label></div><br/><div class="children"><div class="content">Having been running a VPS with manually maintained services moving to docker saved me a lot of headache... things definitely breaks less often (almost never) and if they do it&#x27;s quite easy to revert back to previous version...</div><br/></div></div><div id="41071795" class="c"><input type="checkbox" id="c-41071795" checked=""/><div class="controls bullet"><span class="by">bornfreddy</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071421">parent</a><span>|</span><a href="#41076655">prev</a><span>|</span><a href="#41073384">next</a><span>|</span><label class="collapse" for="c-41071795">[-]</label><label class="expand" for="c-41071795">[1 more]</label></div><br/><div class="children"><div class="content">But at least you can revert back to the original configuration (as you can with VM, too).</div><br/></div></div><div id="41073384" class="c"><input type="checkbox" id="c-41073384" checked=""/><div class="controls bullet"><span class="by">Spivak</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071421">parent</a><span>|</span><a href="#41071795">prev</a><span>|</span><a href="#41072341">next</a><span>|</span><label class="collapse" for="c-41073384">[-]</label><label class="expand" for="c-41073384">[1 more]</label></div><br/><div class="children"><div class="content">It solves it in the sense that it empowers the devs to update their dependencies on their own time and ops can update the underlying infrastructure fearlessly. It turned a coordination problem into a non-problem.</div><br/></div></div></div></div><div id="41072341" class="c"><input type="checkbox" id="c-41072341" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071354">parent</a><span>|</span><a href="#41071421">prev</a><span>|</span><a href="#41075024">next</a><span>|</span><label class="collapse" for="c-41072341">[-]</label><label class="expand" for="c-41072341">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t solve it, it makes it tractable so you can use the scientific method to fix problems as opposed to voodoo.</div><br/></div></div></div></div></div></div><div id="41075024" class="c"><input type="checkbox" id="c-41075024" checked=""/><div class="controls bullet"><span class="by">diego_sandoval</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41071230">prev</a><span>|</span><a href="#41072806">next</a><span>|</span><label class="collapse" for="c-41075024">[-]</label><label class="expand" for="c-41075024">[2 more]</label></div><br/><div class="children"><div class="content">&gt; and Docker industrial complex that developed around solving problems created by themselves or solved decades ago.<p>From my perspective, it&#x27;s the complete opposite: Docker is a workaround for problems created decades ago (e.g. dynamic linking), that could have been solved in a better manner, but were not.</div><br/><div id="41076662" class="c"><input type="checkbox" id="c-41076662" checked=""/><div class="controls bullet"><span class="by">ktosobcy</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41075024">parent</a><span>|</span><a href="#41072806">next</a><span>|</span><label class="collapse" for="c-41076662">[-]</label><label class="expand" for="c-41076662">[1 more]</label></div><br/><div class="children"><div class="content">there are flatpacks&#x2F;appimage&#x2F;whatever but they are linux-only (mostly) and still lack something akin of docker-compose...</div><br/></div></div></div></div><div id="41072806" class="c"><input type="checkbox" id="c-41072806" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41075024">prev</a><span>|</span><a href="#41075061">next</a><span>|</span><label class="collapse" for="c-41072806">[-]</label><label class="expand" for="c-41072806">[1 more]</label></div><br/><div class="children"><div class="content">Jails&#x2F;Zones are not pretty much as secure as a VM. They&#x27;re materially less secure: they leave cotenant workloads sharing a single kernel (not just the tiny slice of the kernel KVM manages). Most kernel LPEs are probably &quot;Jail&quot; escapes, and it&#x27;s not feasible to filter them out with system call sandboxing, because LPEs occur in innocuous system calls, too.</div><br/></div></div><div id="41075061" class="c"><input type="checkbox" id="c-41075061" checked=""/><div class="controls bullet"><span class="by">tiffanyh</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41072806">prev</a><span>|</span><a href="#41071262">next</a><span>|</span><label class="collapse" for="c-41075061">[-]</label><label class="expand" for="c-41075061">[1 more]</label></div><br/><div class="children"><div class="content">Are Jails&#x2F;Zones&#x2F;Docker even security solutions?<p>I always used them as process isolation &amp; dependency bundling.</div><br/></div></div><div id="41071262" class="c"><input type="checkbox" id="c-41071262" checked=""/><div class="controls bullet"><span class="by">gryfft</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41075061">prev</a><span>|</span><a href="#41073015">next</a><span>|</span><label class="collapse" for="c-41071262">[-]</label><label class="expand" for="c-41071262">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been meaning to do a bhyve deep dive for years, my gut feelings being much the same as yours. Would appreciate any recommended reading.</div><br/><div id="41072965" class="c"><input type="checkbox" id="c-41072965" checked=""/><div class="controls bullet"><span class="by">Gud</span><span>|</span><a href="#41071150">root</a><span>|</span><a href="#41071262">parent</a><span>|</span><a href="#41073015">next</a><span>|</span><label class="collapse" for="c-41072965">[-]</label><label class="expand" for="c-41072965">[1 more]</label></div><br/><div class="children"><div class="content">Read the fine manual and handbook.</div><br/></div></div></div></div><div id="41073015" class="c"><input type="checkbox" id="c-41073015" checked=""/><div class="controls bullet"><span class="by">tomjen3</span><span>|</span><a href="#41071150">parent</a><span>|</span><a href="#41071262">prev</a><span>|</span><a href="#41073010">next</a><span>|</span><label class="collapse" for="c-41073015">[-]</label><label class="expand" for="c-41073015">[1 more]</label></div><br/><div class="children"><div class="content">If anything Docker is underused. You should have a very good reason to make a deploy that is not Docker, or (if you really need the extra security) a VM that runs one thing only (and so is essentially a more resource requiring Docker).<p>If you don’t, then it becomes much harder to answer the question of what exactly is deployed on a given server and what it takes to bring it up again if it goes down hard. If you but everything in Docker files, then the answer is whatever is set in the latest docker-compose file.</div><br/></div></div></div></div><div id="41073010" class="c"><input type="checkbox" id="c-41073010" checked=""/><div class="controls bullet"><span class="by">ploxiln</span><span>|</span><a href="#41071150">prev</a><span>|</span><a href="#41072781">next</a><span>|</span><label class="collapse" for="c-41073010">[-]</label><label class="expand" for="c-41073010">[5 more]</label></div><br/><div class="children"><div class="content">&gt; we operate in networks where outbound MQTT and HTTPS is simply not allowed (which is why we rely on encrypted DNS traffic for device-to-Console communication)<p>HTTPS is not allowed (locked down for security!), so communication is smuggled over DNS? uhh ... I suspect that a lot of what the customer &quot;security&quot; departments do, doesn&#x27;t really make sense ...</div><br/><div id="41075340" class="c"><input type="checkbox" id="c-41075340" checked=""/><div class="controls bullet"><span class="by">jmprspret</span><span>|</span><a href="#41073010">parent</a><span>|</span><a href="#41072781">next</a><span>|</span><label class="collapse" for="c-41075340">[-]</label><label class="expand" for="c-41075340">[4 more]</label></div><br/><div class="children"><div class="content">DNS tunneling, or smuggling through DNS requests, is like a known malware C2 method. Seems really weird to (ab)use it for &quot;&quot;security&quot;&quot;</div><br/><div id="41076288" class="c"><input type="checkbox" id="c-41076288" checked=""/><div class="controls bullet"><span class="by">thinkst</span><span>|</span><a href="#41073010">root</a><span>|</span><a href="#41075340">parent</a><span>|</span><a href="#41072781">next</a><span>|</span><label class="collapse" for="c-41076288">[-]</label><label class="expand" for="c-41076288">[3 more]</label></div><br/><div class="children"><div class="content">Product builders can learn loads from malware in terms of deployment and operational ease. Malware needs to operate without any assistance in unknown environments. Nobody is allowing outbound comms deliberately for malware, so tunnel methods were developed.<p>Networks have these capabilities, inherently they&#x27;re part of the specs. But only malware seems to realise that and use it. We love reusing offensive techniques for defence (see our Canarytokens stuff), and DNS comms fits that perfectly. Our customers get an actual 2-minute install, not a 2-minute-and-then-wait-a-week-for-the-firewall-rules install.</div><br/><div id="41076437" class="c"><input type="checkbox" id="c-41076437" checked=""/><div class="controls bullet"><span class="by">jmprspret</span><span>|</span><a href="#41073010">root</a><span>|</span><a href="#41076288">parent</a><span>|</span><a href="#41072781">next</a><span>|</span><label class="collapse" for="c-41076437">[-]</label><label class="expand" for="c-41076437">[2 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t mean to imply that so for security was a bad thing. Now I read back my comment I see that is exactly how it sounds.<p>I agree with you</div><br/><div id="41076888" class="c"><input type="checkbox" id="c-41076888" checked=""/><div class="controls bullet"><span class="by">thinkst</span><span>|</span><a href="#41073010">root</a><span>|</span><a href="#41076437">parent</a><span>|</span><a href="#41072781">next</a><span>|</span><label class="collapse" for="c-41076888">[-]</label><label class="expand" for="c-41076888">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve got several product features that have been driven by our offensive security background... thanks for the prod, we&#x27;ll blog it.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41072781" class="c"><input type="checkbox" id="c-41072781" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41073010">prev</a><span>|</span><a href="#41071493">next</a><span>|</span><label class="collapse" for="c-41072781">[-]</label><label class="expand" for="c-41072781">[4 more]</label></div><br/><div class="children"><div class="content">The cool kids have been combining containers and hardware virtualization for something like 10 years now (back to QEMU-Lite and kvmtool). Don&#x27;t use containers if the abstraction gets in your way, of course, but if they work for you --- as a mechanism for packaging and shipping software and coordinating deployments --- there&#x27;s no reason you need to roll all the way back to individually managed EC2 instances.<p>A short survey on this stuff:<p><a href="https:&#x2F;&#x2F;fly.io&#x2F;blog&#x2F;sandboxing-and-workload-isolation&#x2F;">https:&#x2F;&#x2F;fly.io&#x2F;blog&#x2F;sandboxing-and-workload-isolation&#x2F;</a></div><br/><div id="41072840" class="c"><input type="checkbox" id="c-41072840" checked=""/><div class="controls bullet"><span class="by">mwcampbell</span><span>|</span><a href="#41072781">parent</a><span>|</span><a href="#41071493">next</a><span>|</span><label class="collapse" for="c-41072840">[-]</label><label class="expand" for="c-41072840">[3 more]</label></div><br/><div class="children"><div class="content">Since you&#x27;re here, I was just thinking about how feasible it would be to run a microVM-per-tenant setup like this on Fly. I guess it would require some automation to create a Fly app for each customer. Is this something you all have thought about?</div><br/><div id="41072941" class="c"><input type="checkbox" id="c-41072941" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41072781">root</a><span>|</span><a href="#41072840">parent</a><span>|</span><a href="#41075616">next</a><span>|</span><label class="collapse" for="c-41072941">[-]</label><label class="expand" for="c-41072941">[1 more]</label></div><br/><div class="children"><div class="content">Extraordinarily easy. It&#x27;s a design goal of the system. I don&#x27;t want to crud up the thread; this whole &quot;container vs. VM vs. dedicated hardware&quot; debate is dear to my heart. But feel free to drop me a line if you&#x27;re interested in our take on it.</div><br/></div></div><div id="41075616" class="c"><input type="checkbox" id="c-41075616" checked=""/><div class="controls bullet"><span class="by">DAlperin</span><span>|</span><a href="#41072781">root</a><span>|</span><a href="#41072840">parent</a><span>|</span><a href="#41072941">prev</a><span>|</span><a href="#41071493">next</a><span>|</span><label class="collapse" for="c-41075616">[-]</label><label class="expand" for="c-41075616">[1 more]</label></div><br/><div class="children"><div class="content">Also to add, we already have lots of customers who use this model.</div><br/></div></div></div></div></div></div><div id="41071493" class="c"><input type="checkbox" id="c-41071493" checked=""/><div class="controls bullet"><span class="by">bobbob1921</span><span>|</span><a href="#41072781">prev</a><span>|</span><a href="#41072267">next</a><span>|</span><label class="collapse" for="c-41071493">[-]</label><label class="expand" for="c-41071493">[3 more]</label></div><br/><div class="children"><div class="content">My big struggle with docker&#x2F;containers vs VMs is the storage layer (on containers).
  I’m sure it’s mostly lack of experience &#x2F; knowledge on my end, but I never have a doubt or concern that my storage is persistent and clearly defined when using a VM based workload.
I cannot say the same for my docker&#x2F;container based workloads, I’m always a tad concerned about the persistence of storage, (or the resource management in regards to storage).
This becomes even more true as you deal with networked storage on both platforms</div><br/><div id="41074232" class="c"><input type="checkbox" id="c-41074232" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#41071493">parent</a><span>|</span><a href="#41072271">next</a><span>|</span><label class="collapse" for="c-41074232">[-]</label><label class="expand" for="c-41074232">[1 more]</label></div><br/><div class="children"><div class="content">It absolutely boggles my mind that read-only mode is not the default in Docker. By default, every container has an extra, unnamed, writable volume: its own root. Typo in your volume mount?  You’re writing to root, and you <i>will</i> lose data.<p>Of course, once this is fixed and you start using read-only containers, one wonders why “container” exists as a persistent, named concept.</div><br/></div></div><div id="41072271" class="c"><input type="checkbox" id="c-41072271" checked=""/><div class="controls bullet"><span class="by">imp0cat</span><span>|</span><a href="#41071493">parent</a><span>|</span><a href="#41074232">prev</a><span>|</span><a href="#41072267">next</a><span>|</span><label class="collapse" for="c-41072271">[-]</label><label class="expand" for="c-41072271">[1 more]</label></div><br/><div class="children"><div class="content">Mount those paths that you care about to local filesystem. Otherwise, you&#x27;re always one `docker system prune -a -f --volumes` from a disaster.</div><br/></div></div></div></div><div id="41072267" class="c"><input type="checkbox" id="c-41072267" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#41071493">prev</a><span>|</span><a href="#41071505">next</a><span>|</span><label class="collapse" for="c-41072267">[-]</label><label class="expand" for="c-41072267">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s nice to see the Principle Of Least Access (POLA) in practical use. Some day, we&#x27;ll have operating systems that respect it as well.<p>As more people wake up to the realization that we shouldn&#x27;t trust code, I expect that the number of civilization wide outages will decrease.<p>Working in the cloud, they&#x27;re not going to be able to use my other favorite security tool, the data diode. Which can positively guarantee ingress of control, while still allowing egress of reporting data.</div><br/><div id="41073864" class="c"><input type="checkbox" id="c-41073864" checked=""/><div class="controls bullet"><span class="by">nrr</span><span>|</span><a href="#41072267">parent</a><span>|</span><a href="#41072915">next</a><span>|</span><label class="collapse" for="c-41073864">[-]</label><label class="expand" for="c-41073864">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re coming by after the fact and scratching your head at what a data diode is, Wikipedia&#x27;s page on the subject is a decent crib document. &lt;<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Unidirectional_network" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Unidirectional_network</a>&gt;</div><br/></div></div><div id="41072915" class="c"><input type="checkbox" id="c-41072915" checked=""/><div class="controls bullet"><span class="by">fsflover</span><span>|</span><a href="#41072267">parent</a><span>|</span><a href="#41073864">prev</a><span>|</span><a href="#41071505">next</a><span>|</span><label class="collapse" for="c-41072915">[-]</label><label class="expand" for="c-41072915">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Some day, we&#x27;ll have operating systems that respect it as well.<p>Qubes OS has been relying on it for many years. My daily driver, can&#x27;t recommend it enough.</div><br/></div></div></div></div><div id="41071505" class="c"><input type="checkbox" id="c-41071505" checked=""/><div class="controls bullet"><span class="by">fsckboy</span><span>|</span><a href="#41072267">prev</a><span>|</span><a href="#41076271">next</a><span>|</span><label class="collapse" for="c-41071505">[-]</label><label class="expand" for="c-41071505">[28 more]</label></div><br/><div class="children"><div class="content">just as a meta idea, i&#x27;m mystified that systems folks find it impossible to create protected mode operating systems that are protected, and then we all engage in wasteful kluges like VMs.<p>i&#x27;m not anti-VM, they&#x27;re great technology, i just don&#x27;t think it should be the only way to get protection. VMs are incredibly inefficient... what&#x27;s that you say, they&#x27;re not? ok, then why aren&#x27;t they integrated into protected mode OSes so that they will actually be protected?</div><br/><div id="41073094" class="c"><input type="checkbox" id="c-41073094" checked=""/><div class="controls bullet"><span class="by">ploxiln</span><span>|</span><a href="#41071505">parent</a><span>|</span><a href="#41076732">next</a><span>|</span><label class="collapse" for="c-41073094">[-]</label><label class="expand" for="c-41073094">[1 more]</label></div><br/><div class="children"><div class="content">The industry tends to do this everywhere: we have a system to contain things, we made a mess of it, now we want to contain separate instances of the systems.<p>For example, in AWS or GCP, you can isolate stuff for different environments or teams with security groups and IAM policies. You can separate them with separate VPCs that can&#x27;t talk to each other. In GCP you can separate them with &quot;projects&quot;. But soon that&#x27;s not enough, companies want separate AWS accounts for separate teams or environments, and they need to be grouped under a parent org account, and you can have policies that grant ability to assume roles cross-account ... then you need separate associated groups of AWS accounts for separate divisions!<p>It really never ends, companies will always want to take whatever nested mess they have, and instead of cleaning it up, just nest it one level further. That&#x27;s why we&#x27;ll be running wasm in separate processes in separate containers in separate VMs on many-core servers (probably managed with another level of virtualization, but who can tell).</div><br/></div></div><div id="41076732" class="c"><input type="checkbox" id="c-41076732" checked=""/><div class="controls bullet"><span class="by">dale_glass</span><span>|</span><a href="#41071505">parent</a><span>|</span><a href="#41073094">prev</a><span>|</span><a href="#41072068">next</a><span>|</span><label class="collapse" for="c-41076732">[-]</label><label class="expand" for="c-41076732">[1 more]</label></div><br/><div class="children"><div class="content">Security is easier when the attack surface is limited.<p>An OS provides a huge amount of functionality and offers access to vast amounts of complex shared resources. Anywhere in that there can be holes.<p>A VM is conceptually simpler. We don&#x27;t have to prove there&#x27;s no way to get to a root exploit from a myriad services running as root but available to a normal application. We&#x27;re concerned about things like that a VM won&#x27;t access a disk belonging to another. Which is a far simpler problem.</div><br/></div></div><div id="41072068" class="c"><input type="checkbox" id="c-41072068" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#41071505">parent</a><span>|</span><a href="#41076732">prev</a><span>|</span><a href="#41076501">next</a><span>|</span><label class="collapse" for="c-41072068">[-]</label><label class="expand" for="c-41072068">[1 more]</label></div><br/><div class="children"><div class="content">Windows has Virtualization Based Security [1], where if your system has the right hardware and the right settings, it will use the virtualization support to get you a more protected environment. IO-MMU seems like it was designed for virtualization, but you can use it in a non-virtualized setting too, etc.<p>[1] <a href="https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;windows-hardware&#x2F;design&#x2F;device-experiences&#x2F;oem-vbs" rel="nofollow">https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;windows-hardware&#x2F;design&#x2F;de...</a></div><br/></div></div><div id="41071714" class="c"><input type="checkbox" id="c-41071714" checked=""/><div class="controls bullet"><span class="by">bigbones</span><span>|</span><a href="#41071505">parent</a><span>|</span><a href="#41076501">prev</a><span>|</span><a href="#41071954">next</a><span>|</span><label class="collapse" for="c-41071714">[-]</label><label class="expand" for="c-41071714">[1 more]</label></div><br/><div class="children"><div class="content">Because it would defeat the purpose. Turns out we don&#x27;t trust the systems folks all that much</div><br/></div></div></div></div><div id="41076271" class="c"><input type="checkbox" id="c-41076271" checked=""/><div class="controls bullet"><span class="by">ianpurton</span><span>|</span><a href="#41071505">prev</a><span>|</span><a href="#41071884">next</a><span>|</span><label class="collapse" for="c-41076271">[-]</label><label class="expand" for="c-41076271">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve solved the same problem but used Kubernetes namespaces instead.<p>Each customer gets their own namespace and a namespace is locked down in terms of networking and I deploy Postgres in each namespace using the Postgres operator.<p>I&#x27;ve built an operator for my app, so deploying the app into a namespace is as simple as deploying the manifest.</div><br/></div></div><div id="41071884" class="c"><input type="checkbox" id="c-41071884" checked=""/><div class="controls bullet"><span class="by">stacktrust</span><span>|</span><a href="#41076271">prev</a><span>|</span><a href="#41071279">next</a><span>|</span><label class="collapse" for="c-41071884">[-]</label><label class="expand" for="c-41071884">[1 more]</label></div><br/><div class="children"><div class="content">A modern virtualization architecture can be found in the OSS pKVM L0 nested hypervisor for Android Virtualization Framework, which has some architectural overlap with HP&#x2F;Bromium AX L0 + [Hyper-V | KVM | Xen] L1 + uXen L2 micro-VMs with copy-on-write memory.<p>A Bromium demo circa 2014 was a web browser where every tab was an isolated VM, and every HTTP request was an isolated VM.  Hundreds of VMs could be launched in a couple of hundred milliseconds.  Firecracker has some overlap.<p><i>&gt; Lastly, this approach is almost certainly more expensive. Our instances sit idle for the most part and we pay EC2 a pretty penny for the privilege.</i><p>With many near-idle server VMs running identical code for each customer, there may be an opportunity to use copy-on-memory-write VMs with fast restore of unique memory state, using the techniques employed in live migration.<p>Xen&#x2F;uXen&#x2F;AX: <a href="https:&#x2F;&#x2F;www.platformsecuritysummit.com&#x2F;2018&#x2F;speaker&#x2F;pratt&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.platformsecuritysummit.com&#x2F;2018&#x2F;speaker&#x2F;pratt&#x2F;</a><p>pKVM: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9npebeVFbFw" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9npebeVFbFw</a></div><br/></div></div><div id="41071279" class="c"><input type="checkbox" id="c-41071279" checked=""/><div class="controls bullet"><span class="by">jonathanlydall</span><span>|</span><a href="#41071884">prev</a><span>|</span><a href="#41071944">next</a><span>|</span><label class="collapse" for="c-41071279">[-]</label><label class="expand" for="c-41071279">[2 more]</label></div><br/><div class="children"><div class="content">Sure, it’s an option which eliminates the possibility of certain types of errors, but it’s costing you the ability to pool computing resources as efficiently as you could have with a multi-tenant approach.<p>The author did acknowledge it’s a trade off, but the economics of this trade off may or may not make sense depending on how much you need to charge your customers to remain competitive with competing offerings.</div><br/></div></div><div id="41071944" class="c"><input type="checkbox" id="c-41071944" checked=""/><div class="controls bullet"><span class="by">jefurii</span><span>|</span><a href="#41071279">prev</a><span>|</span><a href="#41076166">next</a><span>|</span><label class="collapse" for="c-41071944">[-]</label><label class="expand" for="c-41071944">[1 more]</label></div><br/><div class="children"><div class="content">Using VMs as the unit allows them to move to another provider if they need to.  They could even move to something like an on-prem Oxide rack if they wanted.  [Yes I know, TFA lists this as a &quot;false benefit&quot; i.e. something they think doesn&#x27;t benefit them.]</div><br/></div></div><div id="41076141" class="c"><input type="checkbox" id="c-41076141" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#41076166">prev</a><span>|</span><a href="#41072409">next</a><span>|</span><label class="collapse" for="c-41076141">[-]</label><label class="expand" for="c-41076141">[1 more]</label></div><br/><div class="children"><div class="content">So you end up with thousands of near idle AWS instances?<p>There has got to be a better middle ground. Like mult tenant but strong splits ( each customer on db etc )</div><br/></div></div><div id="41072409" class="c"><input type="checkbox" id="c-41072409" checked=""/><div class="controls bullet"><span class="by">SunlitCat</span><span>|</span><a href="#41076141">prev</a><span>|</span><a href="#41072790">next</a><span>|</span><label class="collapse" for="c-41072409">[-]</label><label class="expand" for="c-41072409">[7 more]</label></div><br/><div class="children"><div class="content">VMs are awesome for what they can offer. Docker (and the like) are kinda a lean VM for a specific tool scenario.<p>What I would like to see, would be more App virtualization software which isolates the app from the underlying OS enough to provide an safe enough cage for the app.<p>I know there are some commercial offerings out there (and a free one), but maybe someone can chime in has some opinions about them or know some additional ones?</div><br/><div id="41072512" class="c"><input type="checkbox" id="c-41072512" checked=""/><div class="controls bullet"><span class="by">peddling-brink</span><span>|</span><a href="#41072409">parent</a><span>|</span><a href="#41072430">next</a><span>|</span><label class="collapse" for="c-41072512">[-]</label><label class="expand" for="c-41072512">[2 more]</label></div><br/><div class="children"><div class="content">That’s what containers attempt to do. But it’s not perfect. Adding a layer like gvisor helps, but again the app is still interacting with the host kernel so kernel exploits are still possible. What additional sandboxing are you thinking of?</div><br/><div id="41072896" class="c"><input type="checkbox" id="c-41072896" checked=""/><div class="controls bullet"><span class="by">SunlitCat</span><span>|</span><a href="#41072409">root</a><span>|</span><a href="#41072512">parent</a><span>|</span><a href="#41072430">next</a><span>|</span><label class="collapse" for="c-41072896">[-]</label><label class="expand" for="c-41072896">[1 more]</label></div><br/><div class="children"><div class="content">Maybe I am a bit naive, but in my mind it&#x27;s just a simple software running between the OS and the tool in question which runs said software in some kind of virtualization, passing all requests to the OS after a check what they might want to do.<p>I know that&#x27;s what said tools are offering, but installing (and running) docker on Windows feels like loading up a whole other OS insides OS, so that even VM (Software) looks lean compared to that!<p>But I admit, that I have no real experience with docker and the like.</div><br/></div></div></div></div><div id="41072430" class="c"><input type="checkbox" id="c-41072430" checked=""/><div class="controls bullet"><span class="by">stacktrust</span><span>|</span><a href="#41072409">parent</a><span>|</span><a href="#41072512">prev</a><span>|</span><a href="#41072790">next</a><span>|</span><label class="collapse" for="c-41072430">[-]</label><label class="expand" for="c-41072430">[4 more]</label></div><br/><div class="children"><div class="content">HP business PCs ship with SureClick based on OSS uXen, <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41071884">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41071884</a></div><br/><div id="41072856" class="c"><input type="checkbox" id="c-41072856" checked=""/><div class="controls bullet"><span class="by">SunlitCat</span><span>|</span><a href="#41072409">root</a><span>|</span><a href="#41072430">parent</a><span>|</span><a href="#41072790">next</a><span>|</span><label class="collapse" for="c-41072856">[-]</label><label class="expand" for="c-41072856">[3 more]</label></div><br/><div class="children"><div class="content">Thank you for sharing, didn&#x27;t know that one!</div><br/><div id="41073056" class="c"><input type="checkbox" id="c-41073056" checked=""/><div class="controls bullet"><span class="by">stacktrust</span><span>|</span><a href="#41072409">root</a><span>|</span><a href="#41072856">parent</a><span>|</span><a href="#41072790">next</a><span>|</span><label class="collapse" for="c-41073056">[-]</label><label class="expand" for="c-41073056">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s from the original Xen team.  Subsequently cloned by MS as MDAG (Defender Application Guard).</div><br/><div id="41073645" class="c"><input type="checkbox" id="c-41073645" checked=""/><div class="controls bullet"><span class="by">SunlitCat</span><span>|</span><a href="#41072409">root</a><span>|</span><a href="#41073056">parent</a><span>|</span><a href="#41072790">next</a><span>|</span><label class="collapse" for="c-41073645">[-]</label><label class="expand" for="c-41073645">[1 more]</label></div><br/><div class="children"><div class="content">Cool! I know MDAG and actually it&#x27;s a pretty neat concept, kinda.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41072790" class="c"><input type="checkbox" id="c-41072790" checked=""/><div class="controls bullet"><span class="by">vin10</span><span>|</span><a href="#41072409">prev</a><span>|</span><a href="#41073150">next</a><span>|</span><label class="collapse" for="c-41072790">[-]</label><label class="expand" for="c-41072790">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If you wouldn&#x27;t trust running it on your host, you probably shouldn&#x27;t run it in a container as well.<p>- From a Docker&#x2F;Moby Maintainer</div><br/></div></div><div id="41073150" class="c"><input type="checkbox" id="c-41073150" checked=""/><div class="controls bullet"><span class="by">er4hn</span><span>|</span><a href="#41072790">prev</a><span>|</span><a href="#41071864">next</a><span>|</span><label class="collapse" for="c-41073150">[-]</label><label class="expand" for="c-41073150">[2 more]</label></div><br/><div class="children"><div class="content">One thing I wasn&#x27;t able to grok from the article is orchestration of VMs. Are they using AWS to manage the VM lifecycles, restart them, etc?<p>Last time I looked into this for on-prem the solutions seemed very enterprise, pay the big bux, focused. Not a lot in the OSS space. What do people use for on-prem VM orchestration that is OSS?</div><br/><div id="41073613" class="c"><input type="checkbox" id="c-41073613" checked=""/><div class="controls bullet"><span class="by">jinzo</span><span>|</span><a href="#41073150">parent</a><span>|</span><a href="#41071864">next</a><span>|</span><label class="collapse" for="c-41073613">[-]</label><label class="expand" for="c-41073613">[1 more]</label></div><br/><div class="children"><div class="content">Depends what is your scale, but I used oVirt and Proxmox in the past, and it was (especially oVirt) very enterprisey but OSS.</div><br/></div></div></div></div><div id="41075580" class="c"><input type="checkbox" id="c-41075580" checked=""/><div class="controls bullet"><span class="by">Melatonic</span><span>|</span><a href="#41071864">prev</a><span>|</span><a href="#41071180">next</a><span>|</span><label class="collapse" for="c-41075580">[-]</label><label class="expand" for="c-41075580">[1 more]</label></div><br/><div class="children"><div class="content">Eventually we&#x27;ll get a great system managing some form of micro VM that lots of people use and we have years of documentation and troubleshooting on<p>Until then the debate between VM and Containerisation will continue</div><br/></div></div><div id="41071180" class="c"><input type="checkbox" id="c-41071180" checked=""/><div class="controls bullet"><span class="by">osigurdson</span><span>|</span><a href="#41075580">prev</a><span>|</span><a href="#41074831">next</a><span>|</span><label class="collapse" for="c-41071180">[-]</label><label class="expand" for="c-41071180">[5 more]</label></div><br/><div class="children"><div class="content">When thinking about multi-tenancy, remember that your bank doesn&#x27;t have a special VM or container, just for you.</div><br/><div id="41071319" class="c"><input type="checkbox" id="c-41071319" checked=""/><div class="controls bullet"><span class="by">dspillett</span><span>|</span><a href="#41071180">parent</a><span>|</span><a href="#41071315">next</a><span>|</span><label class="collapse" for="c-41071319">[-]</label><label class="expand" for="c-41071319">[1 more]</label></div><br/><div class="children"><div class="content">No, but they do have their own VM&#x2F;container(s) separate from all the other banks that use the same service, with persisted data in their own storage account with its own encryption keys, etc.<p>We deal with banks in DayJob - they have separate VMs&#x2F;containers for their own UAT &amp; training environments, and when the same bank that works in multiple regulatory jurisdictions they usually have systems servicing those separated too as if there were completely separate entities (only bringing aggregate data back together for higher-up reporting purposes).</div><br/></div></div><div id="41071315" class="c"><input type="checkbox" id="c-41071315" checked=""/><div class="controls bullet"><span class="by">01HNNWZ0MV43FF</span><span>|</span><a href="#41071180">parent</a><span>|</span><a href="#41071319">prev</a><span>|</span><a href="#41074831">next</a><span>|</span><label class="collapse" for="c-41071315">[-]</label><label class="expand" for="c-41071315">[3 more]</label></div><br/><div class="children"><div class="content">My bank doesn&#x27;t even have 2FA</div><br/><div id="41071432" class="c"><input type="checkbox" id="c-41071432" checked=""/><div class="controls bullet"><span class="by">jmnicolas</span><span>|</span><a href="#41071180">root</a><span>|</span><a href="#41071315">parent</a><span>|</span><a href="#41074831">next</a><span>|</span><label class="collapse" for="c-41071432">[-]</label><label class="expand" for="c-41071432">[2 more]</label></div><br/><div class="children"><div class="content">Mine neither and they use a 6 numbers pincode!
This is ridiculous, in comparison my home wifi password is 60+ random chars long.</div><br/><div id="41073024" class="c"><input type="checkbox" id="c-41073024" checked=""/><div class="controls bullet"><span class="by">leononame</span><span>|</span><a href="#41071180">root</a><span>|</span><a href="#41071432">parent</a><span>|</span><a href="#41074831">next</a><span>|</span><label class="collapse" for="c-41073024">[-]</label><label class="expand" for="c-41073024">[1 more]</label></div><br/><div class="children"><div class="content">But they do ask you only two digits of the pin on each try and they probably will lock your account after three incorrect attempts. Not saying 6 digits is secure, but it&#x27;s better than everyone using &quot;password&quot; if they have a string policy on incorrect attempts.<p>And don&#x27;t hm they have 2FA for executing transactions?<p>I&#x27;m pretty sure banks are some of the most targeted IT systems. I don&#x27;t trust them blindly, but when it comes to online security, I trust that they built a system that&#x27;s reasonably well secured and other cases, I&#x27;d get my money back, similar to credit cards.</div><br/></div></div></div></div></div></div></div></div><div id="41074831" class="c"><input type="checkbox" id="c-41074831" checked=""/><div class="controls bullet"><span class="by">JohnCClarke</span><span>|</span><a href="#41071180">prev</a><span>|</span><a href="#41074527">next</a><span>|</span><label class="collapse" for="c-41074831">[-]</label><label class="expand" for="c-41074831">[1 more]</label></div><br/><div class="children"><div class="content">Question: Could you get the customer isolation by running all console access through customer specific lambdas which simply add a unique (and secret) header to all requests. Then you can run a single database with sets of tables keyed by that secret header value.<p>Would give you very nearly as good isolation for much lower cost.</div><br/></div></div><div id="41074909" class="c"><input type="checkbox" id="c-41074909" checked=""/><div class="controls bullet"><span class="by">coppsilgold</span><span>|</span><a href="#41074527">prev</a><span>|</span><a href="#41076379">next</a><span>|</span><label class="collapse" for="c-41074909">[-]</label><label class="expand" for="c-41074909">[1 more]</label></div><br/><div class="children"><div class="content">If you think about it virtualization is just a narrowing of the application-kernel interface. In a standard setting the application has a wide kernel interface available to it with dozens (ex. seccomp) to 100&#x27;s of syscalls. A vulnerablility in any one of which could result in full system compromise.<p>With virtualization the attack surface is narrowed to pretty much just the virtualization interface.<p>The problem with current virtualization (or more specifically, the VMM&#x27;s) is that it can be cumbersome, for example memory management is a serious annoyance. The kernel is built to hog memory for cache and etc. but you don&#x27;t want the guest to be doing that - since you want to overcommit memory as guests will rarely use 100% of what is given to them (especially when the guest is just a jailed singular application), workarounds such as free page reporting and drop_caches hacks exist.<p>I would expect eventually to see high performance custom kernels for a application jails - for example: gVisor[1] acts as a syscall interceptor (and can use KVM too!) and a custom kernel. Or a modified linux kernel with patched pain points for the guest.<p>In effect what virtualization achieves is the ability to rollback much of the advantage of having an operating system in the first place in exchange for securely isolating the workload. But because the workload expects an underlying operating system to serve it, one has to be provided to it. So now you have a host operating system and a guest operating system and some narrow interface between the two to not be a complete clown show. As you grow the interface to properly slave the guest to the host to reduce resource consumption and gain more control you will eventually end up reimagining the operating system perhaps? Or come full circle to the BSD jail idea - imagine the host kernel having hooks into every guest kernel syscall, is this not a BSD jail with extra steps?<p>[1] &lt;<a href="https:&#x2F;&#x2F;gvisor.dev&#x2F;" rel="nofollow">https:&#x2F;&#x2F;gvisor.dev&#x2F;</a>&gt;</div><br/></div></div><div id="41072488" class="c"><input type="checkbox" id="c-41072488" checked=""/><div class="controls bullet"><span class="by">smitty1e</span><span>|</span><a href="#41076379">prev</a><span>|</span><a href="#41072737">next</a><span>|</span><label class="collapse" for="c-41072488">[-]</label><label class="expand" for="c-41072488">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Switching to another provider would be non-trivial, and I don’t see the VM as a real benefit in this regard. The barrier to switching is still incredibly high.<p>This point is made in the context of VM bits, but that switching cost could (in theory, haven&#x27;t done it myself) be mitigated using, e.g. Terraform.<p>The brace-for-shock barrier at the enterprise level is going to be exfiltrating all of that valuable data. Bezos is running a Hotel California for that data: &quot;You can checkout any time you like, but you can never leave&quot; (easily).</div><br/><div id="41073050" class="c"><input type="checkbox" id="c-41073050" checked=""/><div class="controls bullet"><span class="by">tetha</span><span>|</span><a href="#41072488">parent</a><span>|</span><a href="#41072737">next</a><span>|</span><label class="collapse" for="c-41073050">[-]</label><label class="expand" for="c-41073050">[1 more]</label></div><br/><div class="children"><div class="content">Heh. We&#x27;re in the process of moving a service for a few of our larger customers over due to some variety of emergencies, let&#x27;s keep it at that.<p>It took us 2-3 days of hustling to get the stuff running and production ready and providing the right answers. This is the &quot;Terraform and Ansible-Stuff&quot; stage of a real failover. In a full infrastructure failover, I&#x27;d expect it to take us 1-2 very long days to get 80% running and then up to a week to be fully back on track and another week of shaking out strange issues. And then a week or two of low-availability from the ops-team.<p>However, for 3 large customers using that product, cybersecurity and compliance said no. They said no about 5-6 weeks ago and project to have an answer somewhere within the next 1-2 months. Until then, the amount of workarounds and frustration growing around it is rather scary. I hope I can contain it to some places in which there is no permanent damage for the infrastructure.<p>Tech isn&#x27;t necessarily the hardest thing in some spaces.</div><br/></div></div></div></div><div id="41072737" class="c"><input type="checkbox" id="c-41072737" checked=""/><div class="controls bullet"><span class="by">kkfx</span><span>|</span><a href="#41072488">prev</a><span>|</span><label class="collapse" for="c-41072737">[-]</label><label class="expand" for="c-41072737">[1 more]</label></div><br/><div class="children"><div class="content">As much stuff you add as much attack surface you have. Virtualized infra are a commercial need, an IT and Operation OBSCENITY definitively never safe in practice.</div><br/></div></div></div></div></div></div></div></body></html>