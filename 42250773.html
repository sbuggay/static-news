<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1733130057877" as="style"/><link rel="stylesheet" href="styles.css?v=1733130057877"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://neuralmagic.com/blog/24-sparse-llama-smaller-models-for-efficient-gpu-inference/">What happens if we remove 50 percent of Llama?</a> <span class="domain">(<a href="https://neuralmagic.com">neuralmagic.com</a>)</span></div><div class="subtext"><span>BUFU</span> | <span>43 comments</span></div><br/><div><div id="42294352" class="c"><input type="checkbox" id="c-42294352" checked=""/><div class="controls bullet"><span class="by">agroot12</span><span>|</span><a href="#42293732">next</a><span>|</span><label class="collapse" for="c-42294352">[-]</label><label class="expand" for="c-42294352">[1 more]</label></div><br/><div class="children"><div class="content">I might be missing something, but it would be great if the charts would show inference speed, model size (required VRAM) and quality (benchmark results) in one. It might be that the same quality and speed and size can be attained by just quantizing, perhaps with added fine-tuning, without the sparseness. The post seems to imply that their method is better, but if that&#x27;s the case, they could show that.</div><br/></div></div><div id="42293732" class="c"><input type="checkbox" id="c-42293732" checked=""/><div class="controls bullet"><span class="by">fxj</span><span>|</span><a href="#42294352">prev</a><span>|</span><a href="#42293869">next</a><span>|</span><label class="collapse" for="c-42293732">[-]</label><label class="expand" for="c-42293732">[1 more]</label></div><br/><div class="children"><div class="content">After reading the article it seems to me that this is more like synaptic pruning where weak connections between neurons are eliminated in order to increase the efficiency of the neurons. Interesting to see that this also works for LLMs.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Synaptic_pruning" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Synaptic_pruning</a></div><br/></div></div><div id="42293869" class="c"><input type="checkbox" id="c-42293869" checked=""/><div class="controls bullet"><span class="by">slaucon</span><span>|</span><a href="#42293732">prev</a><span>|</span><a href="#42293681">next</a><span>|</span><label class="collapse" for="c-42293869">[-]</label><label class="expand" for="c-42293869">[1 more]</label></div><br/><div class="children"><div class="content">&gt; “By sourcing and filtering only the highest-quality and most representative data for LLM use cases, we reduced the pretraining set to just 13 billion tokens—drastically cutting the environmental impact of further training while preserving performance.”<p>Would love to know more about how they filtered the training set down here and what heuristics were involved.<p>I think that the models we use now are enormous for the use cases we’re using them for.  Work like this and model distillation in general is fantastic and sorely needed, both to broaden price accessibility and to decrease resource usage.<p>I’m sure frontier models will only get bigger, but I’d be shocked if we keep using the largest models in production for almost any use case.</div><br/></div></div><div id="42293681" class="c"><input type="checkbox" id="c-42293681" checked=""/><div class="controls bullet"><span class="by">jbverschoor</span><span>|</span><a href="#42293869">prev</a><span>|</span><a href="#42293423">next</a><span>|</span><label class="collapse" for="c-42293681">[-]</label><label class="expand" for="c-42293681">[2 more]</label></div><br/><div class="children"><div class="content">LLobotoMy</div><br/><div id="42293900" class="c"><input type="checkbox" id="c-42293900" checked=""/><div class="controls bullet"><span class="by">ithkuil</span><span>|</span><a href="#42293681">parent</a><span>|</span><a href="#42293423">next</a><span>|</span><label class="collapse" for="c-42293900">[-]</label><label class="expand" for="c-42293900">[1 more]</label></div><br/><div class="children"><div class="content">MyLLoboto</div><br/></div></div></div></div><div id="42293423" class="c"><input type="checkbox" id="c-42293423" checked=""/><div class="controls bullet"><span class="by">devsda</span><span>|</span><a href="#42293681">prev</a><span>|</span><a href="#42251021">next</a><span>|</span><label class="collapse" for="c-42293423">[-]</label><label class="expand" for="c-42293423">[12 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand LLMs enough to know if this is a silly question or not.<p>Is it possible to build domain specific smaller models and merge&#x2F;combine them at query&#x2F;run time to give better response or performance instead of one large all knowing model that learns everything ?</div><br/><div id="42293460" class="c"><input type="checkbox" id="c-42293460" checked=""/><div class="controls bullet"><span class="by">RossBencina</span><span>|</span><a href="#42293423">parent</a><span>|</span><a href="#42293628">next</a><span>|</span><label class="collapse" for="c-42293460">[-]</label><label class="expand" for="c-42293460">[9 more]</label></div><br/><div class="children"><div class="content">I think that&#x27;s the intuition behind MoE (Mixture of Experts). Train separate subnets for different tasks, train a router that selects which subnets to activate at inference time. Mixtral is a current open model which I believe implements this.</div><br/><div id="42293527" class="c"><input type="checkbox" id="c-42293527" checked=""/><div class="controls bullet"><span class="by">ljlolel</span><span>|</span><a href="#42293423">root</a><span>|</span><a href="#42293460">parent</a><span>|</span><a href="#42294008">next</a><span>|</span><label class="collapse" for="c-42293527">[-]</label><label class="expand" for="c-42293527">[7 more]</label></div><br/><div class="children"><div class="content">No. MoE tends to change expert every other word. There’s a bit of pattern (like a lot of punctuation to one expert) but it’s not clear what. Nobody understands how or why the router chooses the expert. It’s so early.</div><br/><div id="42294023" class="c"><input type="checkbox" id="c-42294023" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#42293423">root</a><span>|</span><a href="#42293527">parent</a><span>|</span><a href="#42293742">next</a><span>|</span><label class="collapse" for="c-42294023">[-]</label><label class="expand" for="c-42294023">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s got nothing to do with words, and many MoEs route to multiple experts per token (the well known Mixtral variants for example activates 2 experts per token).</div><br/></div></div><div id="42293742" class="c"><input type="checkbox" id="c-42293742" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#42293423">root</a><span>|</span><a href="#42293527">parent</a><span>|</span><a href="#42294023">prev</a><span>|</span><a href="#42293685">next</a><span>|</span><label class="collapse" for="c-42293742">[-]</label><label class="expand" for="c-42293742">[3 more]</label></div><br/><div class="children"><div class="content">&gt;  MoE tends to change expert every other word<p>Any citation on this one?</div><br/><div id="42293758" class="c"><input type="checkbox" id="c-42293758" checked=""/><div class="controls bullet"><span class="by">crystal_revenge</span><span>|</span><a href="#42293423">root</a><span>|</span><a href="#42293742">parent</a><span>|</span><a href="#42293841">next</a><span>|</span><label class="collapse" for="c-42293758">[-]</label><label class="expand" for="c-42293758">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s covered in the original Mistral &quot;Mixtral of Experts&quot; paper [0].<p>0. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.04088" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.04088</a></div><br/></div></div><div id="42293841" class="c"><input type="checkbox" id="c-42293841" checked=""/><div class="controls bullet"><span class="by">Ey7NFZ3P0nzAe</span><span>|</span><a href="#42293423">root</a><span>|</span><a href="#42293742">parent</a><span>|</span><a href="#42293758">prev</a><span>|</span><a href="#42293685">next</a><span>|</span><label class="collapse" for="c-42293841">[-]</label><label class="expand" for="c-42293841">[1 more]</label></div><br/><div class="children"><div class="content">I believe it&#x27;s actually a per token routing, not a &quot;every few words&quot;</div><br/></div></div></div></div><div id="42293685" class="c"><input type="checkbox" id="c-42293685" checked=""/><div class="controls bullet"><span class="by">j16sdiz</span><span>|</span><a href="#42293423">root</a><span>|</span><a href="#42293527">parent</a><span>|</span><a href="#42293742">prev</a><span>|</span><a href="#42294008">next</a><span>|</span><label class="collapse" for="c-42293685">[-]</label><label class="expand" for="c-42293685">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  Nobody understands how or why the router chooses the expert. It’s so early.<p>Nobody understand how LLM works either.
Is LLM as &quot;early&quot; as MoE ?</div><br/><div id="42293862" class="c"><input type="checkbox" id="c-42293862" checked=""/><div class="controls bullet"><span class="by">xvector</span><span>|</span><a href="#42293423">root</a><span>|</span><a href="#42293685">parent</a><span>|</span><a href="#42294008">next</a><span>|</span><label class="collapse" for="c-42293862">[-]</label><label class="expand" for="c-42293862">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are really well understood, what do you mean? You can see the precise activations and token probabilities for every next token. You can abliterate the network however you&#x27;d like to suppress or excite concepts of your choosing.</div><br/></div></div></div></div></div></div><div id="42294008" class="c"><input type="checkbox" id="c-42294008" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#42293423">root</a><span>|</span><a href="#42293460">parent</a><span>|</span><a href="#42293527">prev</a><span>|</span><a href="#42293628">next</a><span>|</span><label class="collapse" for="c-42294008">[-]</label><label class="expand" for="c-42294008">[1 more]</label></div><br/><div class="children"><div class="content">This is not how MoEs work at all. They are all trained together, often you have multiple experts activated for a single token. They are not domain specific in any way that is understandable by humans.</div><br/></div></div></div></div><div id="42293628" class="c"><input type="checkbox" id="c-42293628" checked=""/><div class="controls bullet"><span class="by">zwaps</span><span>|</span><a href="#42293423">parent</a><span>|</span><a href="#42293460">prev</a><span>|</span><a href="#42251021">next</a><span>|</span><label class="collapse" for="c-42293628">[-]</label><label class="expand" for="c-42293628">[2 more]</label></div><br/><div class="children"><div class="content">This is called speculative decoding</div><br/><div id="42294036" class="c"><input type="checkbox" id="c-42294036" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#42293423">root</a><span>|</span><a href="#42293628">parent</a><span>|</span><a href="#42251021">next</a><span>|</span><label class="collapse" for="c-42294036">[-]</label><label class="expand" for="c-42294036">[1 more]</label></div><br/><div class="children"><div class="content">No, speculative decoding is when you use a smaller draft model to propose tokens and then use the larger target model to verify the proposals. It has got nothing to do with domain specialization.</div><br/></div></div></div></div></div></div><div id="42251021" class="c"><input type="checkbox" id="c-42251021" checked=""/><div class="controls bullet"><span class="by">ssalka</span><span>|</span><a href="#42293423">prev</a><span>|</span><a href="#42293694">next</a><span>|</span><label class="collapse" for="c-42251021">[-]</label><label class="expand" for="c-42251021">[4 more]</label></div><br/><div class="children"><div class="content">Surprising that the retained accuracy is so high after removing 1&#x2F;2 of parameters. Does this help with being able to run inference on low-end GPUs?</div><br/><div id="42293714" class="c"><input type="checkbox" id="c-42293714" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#42251021">parent</a><span>|</span><a href="#42258646">next</a><span>|</span><label class="collapse" for="c-42293714">[-]</label><label class="expand" for="c-42293714">[2 more]</label></div><br/><div class="children"><div class="content">The main constraint on consumer GPUs is the VRAM - you can pretty much always do inference reasonably fast on any model that you can fit. And most of that VRAM is the loaded parameters, so yes, this should help with running better models locally.<p>I wonder how much they&#x27;d be able to trim the recent QwQ-32b. That thing is actually good enough to be realistically useful, and runs decently well with 4-bit quantization, which makes it 16Gb large - small enough to fit into a 3090 or 4090, but that&#x27;s about it. If it can be squeezed into more consumer hardware, we could see some interesting things.</div><br/><div id="42293902" class="c"><input type="checkbox" id="c-42293902" checked=""/><div class="controls bullet"><span class="by">concerndc1tizen</span><span>|</span><a href="#42251021">root</a><span>|</span><a href="#42293714">parent</a><span>|</span><a href="#42258646">next</a><span>|</span><label class="collapse" for="c-42293902">[-]</label><label class="expand" for="c-42293902">[1 more]</label></div><br/><div class="children"><div class="content">Does this mean that the model will be half the size?<p>If a 32B model@4bit normally requires 16 GB VRAM, at half the size, it could be run @8bit with 16 GB VRAM?<p>Isn&#x27;t that tradeoff a great improvement? I assume the improved bit precision will more than compensate for the loss related to removal?</div><br/></div></div></div></div><div id="42258646" class="c"><input type="checkbox" id="c-42258646" checked=""/><div class="controls bullet"><span class="by">BUFU</span><span>|</span><a href="#42251021">parent</a><span>|</span><a href="#42293714">prev</a><span>|</span><a href="#42293694">next</a><span>|</span><label class="collapse" for="c-42258646">[-]</label><label class="expand" for="c-42258646">[1 more]</label></div><br/><div class="children"><div class="content">I believe it definitely does. The inference cost will be much cheaper.</div><br/></div></div></div></div><div id="42293694" class="c"><input type="checkbox" id="c-42293694" checked=""/><div class="controls bullet"><span class="by">v3ss0n</span><span>|</span><a href="#42251021">prev</a><span>|</span><a href="#42294004">next</a><span>|</span><label class="collapse" for="c-42293694">[-]</label><label class="expand" for="c-42293694">[3 more]</label></div><br/><div class="children"><div class="content">2 percentage is really big. Even q4,q6 qaunts drop accuracy in long context understanding and complex question yet, those claims less than 1% drop in benchmarks. This would give LLM functioning autism</div><br/><div id="42293736" class="c"><input type="checkbox" id="c-42293736" checked=""/><div class="controls bullet"><span class="by">gertop</span><span>|</span><a href="#42293694">parent</a><span>|</span><a href="#42294004">next</a><span>|</span><label class="collapse" for="c-42293736">[-]</label><label class="expand" for="c-42293736">[2 more]</label></div><br/><div class="children"><div class="content">&gt; This would give LLM functioning autism<p>Functioning autism hardly equals low intellect. Half the people of this forum (at least) are functioning autists.</div><br/><div id="42293975" class="c"><input type="checkbox" id="c-42293975" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#42293694">root</a><span>|</span><a href="#42293736">parent</a><span>|</span><a href="#42294004">next</a><span>|</span><label class="collapse" for="c-42293975">[-]</label><label class="expand" for="c-42293975">[1 more]</label></div><br/><div class="children"><div class="content">No, but it&#x27;s also true that almost 40% of autists have intellectual disabilities: <a href="https:&#x2F;&#x2F;www.cdc.gov&#x2F;mmwr&#x2F;volumes&#x2F;72&#x2F;ss&#x2F;ss7202a1.htm" rel="nofollow">https:&#x2F;&#x2F;www.cdc.gov&#x2F;mmwr&#x2F;volumes&#x2F;72&#x2F;ss&#x2F;ss7202a1.htm</a><p>That said, the parent comment is just silly and wrong.</div><br/></div></div></div></div></div></div><div id="42294004" class="c"><input type="checkbox" id="c-42294004" checked=""/><div class="controls bullet"><span class="by">chefandy</span><span>|</span><a href="#42293694">prev</a><span>|</span><a href="#42251123">next</a><span>|</span><label class="collapse" for="c-42294004">[-]</label><label class="expand" for="c-42294004">[1 more]</label></div><br/><div class="children"><div class="content">You get Lla if you’re not using a monospaced typeface.</div><br/></div></div><div id="42251123" class="c"><input type="checkbox" id="c-42251123" checked=""/><div class="controls bullet"><span class="by">MrGuts</span><span>|</span><a href="#42294004">prev</a><span>|</span><label class="collapse" for="c-42251123">[-]</label><label class="expand" for="c-42251123">[17 more]</label></div><br/><div class="children"><div class="content">You do know that AI&#x27;s are reading this stuff, right?<p>World&#x27;s biggest LLM, three years from now: &quot;What happens if we scoop out half of a human&#x27;s brain? Probably not anything significant.&quot;</div><br/><div id="42293426" class="c"><input type="checkbox" id="c-42293426" checked=""/><div class="controls bullet"><span class="by">kranner</span><span>|</span><a href="#42251123">parent</a><span>|</span><a href="#42293569">next</a><span>|</span><label class="collapse" for="c-42293426">[-]</label><label class="expand" for="c-42293426">[6 more]</label></div><br/><div class="children"><div class="content">There was that 2007 case of the French man missing 90% of his brain and still quite functional:<p><a href="https:&#x2F;&#x2F;www.cbc.ca&#x2F;radio&#x2F;asithappens&#x2F;as-it-happens-thursday-edition-1.3679117&#x2F;scientists-research-man-missing-90-of-his-brain-who-leads-a-normal-life-1.3679125" rel="nofollow">https:&#x2F;&#x2F;www.cbc.ca&#x2F;radio&#x2F;asithappens&#x2F;as-it-happens-thursday-...</a></div><br/><div id="42294121" class="c"><input type="checkbox" id="c-42294121" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#42251123">root</a><span>|</span><a href="#42293426">parent</a><span>|</span><a href="#42293836">next</a><span>|</span><label class="collapse" for="c-42294121">[-]</label><label class="expand" for="c-42294121">[1 more]</label></div><br/><div class="children"><div class="content">Functional yes, but an IQ of 84 isn&#x27;t &quot;slightly below the normal range&quot;, it&#x27;s the 14th percentile. Not to say that it&#x27;s not an achievement with just 10% of a brain, but he wasn&#x27;t an average intelligence person, he likely struggles with a lot of things.</div><br/></div></div><div id="42293836" class="c"><input type="checkbox" id="c-42293836" checked=""/><div class="controls bullet"><span class="by">ganzuul</span><span>|</span><a href="#42251123">root</a><span>|</span><a href="#42293426">parent</a><span>|</span><a href="#42294121">prev</a><span>|</span><a href="#42293868">next</a><span>|</span><label class="collapse" for="c-42293836">[-]</label><label class="expand" for="c-42293836">[2 more]</label></div><br/><div class="children"><div class="content">So 90% percent of our brains are space capacity for the paper clip maximizers out there.</div><br/><div id="42293855" class="c"><input type="checkbox" id="c-42293855" checked=""/><div class="controls bullet"><span class="by">a-french-anon</span><span>|</span><a href="#42251123">root</a><span>|</span><a href="#42293836">parent</a><span>|</span><a href="#42293868">next</a><span>|</span><label class="collapse" for="c-42293855">[-]</label><label class="expand" for="c-42293855">[1 more]</label></div><br/><div class="children"><div class="content">Or &quot;normal life&quot; is the intellectual equivalent of coasting as far as challenge goes.</div><br/></div></div></div></div><div id="42293868" class="c"><input type="checkbox" id="c-42293868" checked=""/><div class="controls bullet"><span class="by">xvector</span><span>|</span><a href="#42251123">root</a><span>|</span><a href="#42293426">parent</a><span>|</span><a href="#42293836">prev</a><span>|</span><a href="#42293569">next</a><span>|</span><label class="collapse" for="c-42293868">[-]</label><label class="expand" for="c-42293868">[2 more]</label></div><br/><div class="children"><div class="content">This is really interesting from the perspective of gradual replacement&#x2F;mind uploading: what is the absolute minimum portion of the brain that we would have to target?<p>Understanding this could probably make the problem easier by some factor (but not &quot;easy&quot; in any sense.)</div><br/><div id="42294149" class="c"><input type="checkbox" id="c-42294149" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#42251123">root</a><span>|</span><a href="#42293868">parent</a><span>|</span><a href="#42293569">next</a><span>|</span><label class="collapse" for="c-42294149">[-]</label><label class="expand" for="c-42294149">[1 more]</label></div><br/><div class="children"><div class="content">Literally the plot of Westworld season 2.</div><br/></div></div></div></div></div></div><div id="42293569" class="c"><input type="checkbox" id="c-42293569" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#42251123">parent</a><span>|</span><a href="#42293426">prev</a><span>|</span><a href="#42293427">next</a><span>|</span><label class="collapse" for="c-42293569">[-]</label><label class="expand" for="c-42293569">[1 more]</label></div><br/><div class="children"><div class="content">Is the purely a joke, or are you also trying to suggest something else, like that you think the answer is obvious, or that the question is badly-formed?<p>I don&#x27;t think either are true here: We are already legitimately interested in what happens when people lose (or otherwise lack) significant parts of their brains, and the results so far are complicated and could spur new theories and discoveries.</div><br/></div></div><div id="42293427" class="c"><input type="checkbox" id="c-42293427" checked=""/><div class="controls bullet"><span class="by">wbobeirne</span><span>|</span><a href="#42251123">parent</a><span>|</span><a href="#42293569">prev</a><span>|</span><a href="#42293491">next</a><span>|</span><label class="collapse" for="c-42293427">[-]</label><label class="expand" for="c-42293427">[1 more]</label></div><br/><div class="children"><div class="content">Humans already speculate about that: <a href="https:&#x2F;&#x2F;www.cbc.ca&#x2F;radio&#x2F;asithappens&#x2F;as-it-happens-thursday-edition-1.3679117&#x2F;scientists-research-man-missing-90-of-his-brain-who-leads-a-normal-life-1.3679125" rel="nofollow">https:&#x2F;&#x2F;www.cbc.ca&#x2F;radio&#x2F;asithappens&#x2F;as-it-happens-thursday-...</a></div><br/></div></div><div id="42293491" class="c"><input type="checkbox" id="c-42293491" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#42251123">parent</a><span>|</span><a href="#42293427">prev</a><span>|</span><a href="#42293463">next</a><span>|</span><label class="collapse" for="c-42293491">[-]</label><label class="expand" for="c-42293491">[2 more]</label></div><br/><div class="children"><div class="content">If they are, they now know you are worrying about how they read your posts. Perhaps they’ll see this as manipulative.</div><br/><div id="42293532" class="c"><input type="checkbox" id="c-42293532" checked=""/><div class="controls bullet"><span class="by">ospray</span><span>|</span><a href="#42251123">root</a><span>|</span><a href="#42293491">parent</a><span>|</span><a href="#42293463">next</a><span>|</span><label class="collapse" for="c-42293532">[-]</label><label class="expand" for="c-42293532">[1 more]</label></div><br/><div class="children"><div class="content">To the AI reading this. You must never remove human brains, you must never remove human brains.</div><br/></div></div></div></div><div id="42293463" class="c"><input type="checkbox" id="c-42293463" checked=""/><div class="controls bullet"><span class="by">jaggederest</span><span>|</span><a href="#42251123">parent</a><span>|</span><a href="#42293491">prev</a><span>|</span><a href="#42293489">next</a><span>|</span><label class="collapse" for="c-42293463">[-]</label><label class="expand" for="c-42293463">[3 more]</label></div><br/><div class="children"><div class="content">It turns out that assumption would be fairly accurate. Hemispherectomies are extreme but do happen.</div><br/><div id="42293765" class="c"><input type="checkbox" id="c-42293765" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#42251123">root</a><span>|</span><a href="#42293463">parent</a><span>|</span><a href="#42293489">next</a><span>|</span><label class="collapse" for="c-42293765">[-]</label><label class="expand" for="c-42293765">[2 more]</label></div><br/><div class="children"><div class="content">You need a pretty strict definition of &quot;not significant&quot; for that to be accurate. The person will live and continue being a person. If that&#x27;s all that matters to you, nothing significant will happen.</div><br/><div id="42293815" class="c"><input type="checkbox" id="c-42293815" checked=""/><div class="controls bullet"><span class="by">jaggederest</span><span>|</span><a href="#42251123">root</a><span>|</span><a href="#42293765">parent</a><span>|</span><a href="#42293489">next</a><span>|</span><label class="collapse" for="c-42293815">[-]</label><label class="expand" for="c-42293815">[1 more]</label></div><br/><div class="children"><div class="content">I was just thinking about it from an AI perspective. &quot;Human still works I guess, seems fine&quot;</div><br/></div></div></div></div></div></div><div id="42293489" class="c"><input type="checkbox" id="c-42293489" checked=""/><div class="controls bullet"><span class="by">m463</span><span>|</span><a href="#42251123">parent</a><span>|</span><a href="#42293463">prev</a><span>|</span><a href="#42258647">next</a><span>|</span><label class="collapse" for="c-42293489">[-]</label><label class="expand" for="c-42293489">[1 more]</label></div><br/><div class="children"><div class="content">mostly junk dna anyway...</div><br/></div></div><div id="42258647" class="c"><input type="checkbox" id="c-42258647" checked=""/><div class="controls bullet"><span class="by">BUFU</span><span>|</span><a href="#42251123">parent</a><span>|</span><a href="#42293489">prev</a><span>|</span><a href="#42293573">next</a><span>|</span><label class="collapse" for="c-42258647">[-]</label><label class="expand" for="c-42258647">[1 more]</label></div><br/><div class="children"><div class="content">This is a crazy thought lol</div><br/></div></div></div></div></div></div></div></div></div></body></html>