<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1693731658834" as="style"/><link rel="stylesheet" href="styles.css?v=1693731658834"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2308.15022">Recursively summarizing enables long-term dialogue memory in LLMs</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>PaulHoule</span> | <span>136 comments</span></div><br/><div><div id="37367909" class="c"><input type="checkbox" id="c-37367909" checked=""/><div class="controls bullet"><span class="by">gillh</span><span>|</span><a href="#37368109">next</a><span>|</span><label class="collapse" for="c-37367909">[-]</label><label class="expand" for="c-37367909">[1 more]</label></div><br/><div class="children"><div class="content">We have been doing this at CodeRabbit[0] for incrementally reviewing PRs and allowing conversations in the context of code changes, giving the impression that the bot has much more context than it has. It&#x27;s one of the few tricks we use to scale the AI to code review even large PRs (100+ files).<p>For each commit, we summarize diff for each file. Then, we create a summary of summaries, which is incrementally updated as further commits are made on a pull request. This summary of summaries is saved, hidden inside a comment on a pull request, and is used while reviewing each file and answering the user&#x27;s queries.<p>Some of our code is in the open source. Here is the link to the relevant prompt for recursive summarization - <a href="https:&#x2F;&#x2F;github.com&#x2F;coderabbitai&#x2F;ai-pr-reviewer&#x2F;blob&#x2F;main&#x2F;src&#x2F;prompts.ts#L55">https:&#x2F;&#x2F;github.com&#x2F;coderabbitai&#x2F;ai-pr-reviewer&#x2F;blob&#x2F;main&#x2F;src...</a><p>[0]: coderabbit.ai</div><br/></div></div><div id="37368109" class="c"><input type="checkbox" id="c-37368109" checked=""/><div class="controls bullet"><span class="by">Roark66</span><span>|</span><a href="#37367909">prev</a><span>|</span><a href="#37363931">next</a><span>|</span><label class="collapse" for="c-37368109">[-]</label><label class="expand" for="c-37368109">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Code and scripts will be released later.<p>At this stage I&#x27;ll not believe a single claim without &quot;code and scripts&quot;. It may be true, it may be bullshit. Who knows. Without a low effort way to replicate the experiments I consider this paper(and others like it) something written just so authors can put it in their CVs.<p>I&#x27;ve been waiting for 6+ months for other &quot;code to be released later&quot; papers in this(LLMs) space and there is no indication it will ever be released. Some of these papers are so brazen to even include broken links that lead to parked domains.<p>It&#x27;s time the community started recognising this behaviour.</div><br/><div id="37368293" class="c"><input type="checkbox" id="c-37368293" checked=""/><div class="controls bullet"><span class="by">phire</span><span>|</span><a href="#37368109">parent</a><span>|</span><a href="#37363931">next</a><span>|</span><label class="collapse" for="c-37368293">[-]</label><label class="expand" for="c-37368293">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it&#x27;s a really simple idea that doesn&#x27;t require much code. Really shouldn&#x27;t be hard to clean it up and publish.<p>I was actually playing with similar ideas a while back. I didn&#x27;t have any code at all, I was just experimenting with prompts directly in the API dashboard. The idea showed some promise, but it didn&#x27;t seem like the API cost would be worth it. I suspect you would be much better off with a vector embeddings approach.</div><br/></div></div></div></div><div id="37363931" class="c"><input type="checkbox" id="c-37363931" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#37368109">prev</a><span>|</span><a href="#37365344">next</a><span>|</span><label class="collapse" for="c-37363931">[-]</label><label class="expand" for="c-37363931">[86 more]</label></div><br/><div class="children"><div class="content">To my intuition, all of these ways of building memory in “text space” seem super hacky.<p>It seems intuitive to me that memory would be best stored in dense embedding space that can preserve full semantic meaning for the model rather than as some hacked on process of continually regenerating summaries.<p>And similarly, the model needs to be trained in a setting where it is <i>aware</i> of the memory and how to use it. Preferably that would be from the very beginning (ie. the train on text).</div><br/><div id="37364228" class="c"><input type="checkbox" id="c-37364228" checked=""/><div class="controls bullet"><span class="by">nottheengineer</span><span>|</span><a href="#37363931">parent</a><span>|</span><a href="#37365588">next</a><span>|</span><label class="collapse" for="c-37364228">[-]</label><label class="expand" for="c-37364228">[56 more]</label></div><br/><div class="children"><div class="content">It does seem hacky, but then again the whole concept of conversational LLMs is. You&#x27;re just asking it to add an extra word to a given conversation and after a bit, it spits out an end token that tells your application to hand control back to the user.<p>I think latent space and text space aren&#x27;t as far apart as you think. LLMs are pretty stupid, but very good at speech. They are good at writing code because that&#x27;s very similar, but fall apart in things that need some actual abstract thinking, like math.<p>Those text space hacks do tend to work and stuff like &quot;think step by step&quot; has become common because of that.<p>LoRAs are closer to what you mean and they&#x27;re great at packing a lot of understanding into very little data. But adjusting weights for a single conversation just isn&#x27;t feasible yet, so we&#x27;re exploring text space for that purpose. Maybe someome will transfer the methods we discover in text space to embedding space to make them more efficient, but that&#x27;s for the future.</div><br/><div id="37364901" class="c"><input type="checkbox" id="c-37364901" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37364228">parent</a><span>|</span><a href="#37366045">next</a><span>|</span><label class="collapse" for="c-37364901">[-]</label><label class="expand" for="c-37364901">[54 more]</label></div><br/><div class="children"><div class="content">&gt;They are good at writing code because that&#x27;s very similar, but fall apart in things that need some actual abstract thinking, like math.<p>Pretty odd assertion. LLMs are not &quot;good at speech, bad at abstract thinking&quot;.<p>What do these have to do with speech ?<p><a href="https:&#x2F;&#x2F;general-pattern-machines.github.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;general-pattern-machines.github.io&#x2F;</a><p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.09196" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.09196</a><p>It doesn&#x27;t even hold with your example because GPT-4 is pretty good at Math, nowhere near &quot;falling apart&quot;.</div><br/><div id="37365065" class="c"><input type="checkbox" id="c-37365065" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37364901">parent</a><span>|</span><a href="#37365184">next</a><span>|</span><label class="collapse" for="c-37365065">[-]</label><label class="expand" for="c-37365065">[29 more]</label></div><br/><div class="children"><div class="content">&gt; GPT-4 is pretty good at Math, nowhere near &quot;falling apart&quot;.<p>its good at tasks which were included into training dataset in some variations.</div><br/><div id="37365196" class="c"><input type="checkbox" id="c-37365196" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365065">parent</a><span>|</span><a href="#37365179">next</a><span>|</span><label class="collapse" for="c-37365196">[-]</label><label class="expand" for="c-37365196">[14 more]</label></div><br/><div class="children"><div class="content">I have played around with GPT-4 and some fairly simple but completely new math ideas. It was fabulous at identifying special cases I overlooked, that disproved conjectures.</div><br/><div id="37365336" class="c"><input type="checkbox" id="c-37365336" checked=""/><div class="controls bullet"><span class="by">sudokuist</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365196">parent</a><span>|</span><a href="#37365179">next</a><span>|</span><label class="collapse" for="c-37365336">[-]</label><label class="expand" for="c-37365336">[13 more]</label></div><br/><div class="children"><div class="content">Example?</div><br/><div id="37365470" class="c"><input type="checkbox" id="c-37365470" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365336">parent</a><span>|</span><a href="#37365179">next</a><span>|</span><label class="collapse" for="c-37365470">[-]</label><label class="expand" for="c-37365470">[12 more]</label></div><br/><div class="children"><div class="content">I was playing around with prime numbers, and simple made up relationships between them, such as between the square of a prime N vs. the set of primes smaller than N, etc.<p>It caught me out with specific examples that violated my conjectures. In one case the conjecture held for all but one case, another conjecture was generally true but not for 2 and 3.<p>In one case it thought a conjecture I made was wrong, and I had to push it to think through why it thought it was wrong until it realized the conjecture was right. As soon as it had its epiphany, it corrected all its logic around that concept.<p>It was very simple stuff, but an interesting exercise.<p>The part I enjoyed the most was seeing GPT-4&#x27;s understanding move and change as we pushed back on each other&#x27;s views. You miss out on that impressive aspect of GPT-4 in simpler sessions.</div><br/><div id="37365671" class="c"><input type="checkbox" id="c-37365671" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365470">parent</a><span>|</span><a href="#37365482">next</a><span>|</span><label class="collapse" for="c-37365671">[-]</label><label class="expand" for="c-37365671">[5 more]</label></div><br/><div class="children"><div class="content">its hard to judge how deep and unique your conjectures were.<p>I did similar testing of GPT4, and my observation is that it starts failing after 3-4 levels of reasoning depth.</div><br/><div id="37368419" class="c"><input type="checkbox" id="c-37368419" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365671">parent</a><span>|</span><a href="#37366465">next</a><span>|</span><label class="collapse" for="c-37368419">[-]</label><label class="expand" for="c-37368419">[1 more]</label></div><br/><div class="children"><div class="content">Nice to see number of levels of reasoning depth mentioned. I personally believe the size of a (well-trained) LLM determines how many steps of reasoning in sequence it can approximate. Newer models get deeper and deeper, giving them deeper reasoning context windows. My hypothesis is that you don&#x27;t need infinite reasoning depth, just a bit more than GPT-4 has. I think once you can tie your output together with thinking in terms of ~10+ reasoning steps you&#x27;ll be very close to hunan performance.</div><br/></div></div><div id="37366465" class="c"><input type="checkbox" id="c-37366465" checked=""/><div class="controls bullet"><span class="by">tux1968</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365671">parent</a><span>|</span><a href="#37368419">prev</a><span>|</span><a href="#37365482">next</a><span>|</span><label class="collapse" for="c-37366465">[-]</label><label class="expand" for="c-37366465">[3 more]</label></div><br/><div class="children"><div class="content">&gt; failing after 3-4 levels of reasoning depth.<p>That sounds more like an implementation or resource limitation, rather than an inherent limitation of the technique in general.</div><br/><div id="37366766" class="c"><input type="checkbox" id="c-37366766" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366465">parent</a><span>|</span><a href="#37365482">next</a><span>|</span><label class="collapse" for="c-37366766">[-]</label><label class="expand" for="c-37366766">[2 more]</label></div><br/><div class="children"><div class="content">it is not obvious to me how you came to such conclusion.<p>LLMs got lots of investments: 10s of billions of dollars and tons of compute, maybe more than any other tech in history, and can&#x27;t crack 3 steps reasoning. It sounds like tech limitation..</div><br/><div id="37367705" class="c"><input type="checkbox" id="c-37367705" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366766">parent</a><span>|</span><a href="#37365482">next</a><span>|</span><label class="collapse" for="c-37367705">[-]</label><label class="expand" for="c-37367705">[1 more]</label></div><br/><div class="children"><div class="content">None of these systems or their training sets have been specifically tailored to tackle abstract reasoning or math, so that seems like a premature conclusion. The fact that they&#x27;re decent at programming despite that is interesting.</div><br/></div></div></div></div></div></div></div></div><div id="37365482" class="c"><input type="checkbox" id="c-37365482" checked=""/><div class="controls bullet"><span class="by">sudokuist</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365470">parent</a><span>|</span><a href="#37365671">prev</a><span>|</span><a href="#37365179">next</a><span>|</span><label class="collapse" for="c-37365482">[-]</label><label class="expand" for="c-37365482">[6 more]</label></div><br/><div class="children"><div class="content">Have you tried formalizing your ideas with Isabelle? It has a constraint solver and will often find counterexamples to false arithmetical propositions[1].<p>1: <a href="https:&#x2F;&#x2F;isabelle.in.tum.de&#x2F;overview.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;isabelle.in.tum.de&#x2F;overview.html</a></div><br/><div id="37366144" class="c"><input type="checkbox" id="c-37366144" checked=""/><div class="controls bullet"><span class="by">mannykannot</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365482">parent</a><span>|</span><a href="#37365696">next</a><span>|</span><label class="collapse" for="c-37366144">[-]</label><label class="expand" for="c-37366144">[2 more]</label></div><br/><div class="children"><div class="content">I have not been able to figure out how that would help in the context of this discussion. As I see it, what’s very interesting  here is that an LLM is able to do this.</div><br/><div id="37366225" class="c"><input type="checkbox" id="c-37366225" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366144">parent</a><span>|</span><a href="#37365696">next</a><span>|</span><label class="collapse" for="c-37366225">[-]</label><label class="expand" for="c-37366225">[1 more]</label></div><br/><div class="children"><div class="content">I think the point is that LLM is not right tool for deep reasoning, and isabelle and others are much better such tools, even community trying to apply LLM in this area following current wave of hype.</div><br/></div></div></div></div><div id="37365696" class="c"><input type="checkbox" id="c-37365696" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365482">parent</a><span>|</span><a href="#37366144">prev</a><span>|</span><a href="#37365540">next</a><span>|</span><label class="collapse" for="c-37365696">[-]</label><label class="expand" for="c-37365696">[2 more]</label></div><br/><div class="children"><div class="content">curious why you referred specifically on isabelle, which looks ancient and over engineered, there are many other tools and langs in this area.<p>I am not criticizing, but curious about your opinion.</div><br/><div id="37367755" class="c"><input type="checkbox" id="c-37367755" checked=""/><div class="controls bullet"><span class="by">c-cube</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365696">parent</a><span>|</span><a href="#37365540">next</a><span>|</span><label class="collapse" for="c-37367755">[-]</label><label class="expand" for="c-37367755">[1 more]</label></div><br/><div class="children"><div class="content">Isabelle is good at counter examples in ways few other proof assistants are. In general its automation is excellent, partly because it uses a less powerful logic (HOL instead of CIC; more expressive logics are harder to write automation for). It&#x27;s not obsolete.</div><br/></div></div></div></div><div id="37365540" class="c"><input type="checkbox" id="c-37365540" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365482">parent</a><span>|</span><a href="#37365696">prev</a><span>|</span><a href="#37365179">next</a><span>|</span><label class="collapse" for="c-37365540">[-]</label><label class="expand" for="c-37365540">[1 more]</label></div><br/><div class="children"><div class="content">I have not, thanks for the tip.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37365179" class="c"><input type="checkbox" id="c-37365179" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365065">parent</a><span>|</span><a href="#37365196">prev</a><span>|</span><a href="#37365267">next</a><span>|</span><label class="collapse" for="c-37365179">[-]</label><label class="expand" for="c-37365179">[9 more]</label></div><br/><div class="children"><div class="content">Kinda able to do some math tasks some of the time whereas you can use techniques from the arithmetic textbook to get the right answer all of the time with millions of times less CPU even including the overhead of round-tripping to ASCII numerals which is shockingly large compared to what a multiply costs.<p>Kinda &quot;the problem&quot; with LLMS is that they successfully seduce people by seeming to get the right answer to anything 80% of the time.</div><br/><div id="37366175" class="c"><input type="checkbox" id="c-37366175" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365179">parent</a><span>|</span><a href="#37365321">next</a><span>|</span><label class="collapse" for="c-37366175">[-]</label><label class="expand" for="c-37366175">[2 more]</label></div><br/><div class="children"><div class="content">The arithmetic issues are well documented and understood; it&#x27;s a problem of sub-token manipulation, which has nothing to do with reasoning. (Similar to calling blind people unintelligent because they can&#x27;t read the iq test.)<p>And the better llms can easily write code to do the attention that they suck at...</div><br/><div id="37368116" class="c"><input type="checkbox" id="c-37368116" checked=""/><div class="controls bullet"><span class="by">bigyikes</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366175">parent</a><span>|</span><a href="#37365321">next</a><span>|</span><label class="collapse" for="c-37368116">[-]</label><label class="expand" for="c-37368116">[1 more]</label></div><br/><div class="children"><div class="content">Excellent anology. LLMs are capable of many extraordinary things, and it’s a shame people dismiss them because they fail to live up to some specific test they invented.</div><br/></div></div></div></div><div id="37365321" class="c"><input type="checkbox" id="c-37365321" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365179">parent</a><span>|</span><a href="#37366175">prev</a><span>|</span><a href="#37365267">next</a><span>|</span><label class="collapse" for="c-37365321">[-]</label><label class="expand" for="c-37365321">[6 more]</label></div><br/><div class="children"><div class="content">Math is a lot more than just Arithmetic.</div><br/><div id="37365383" class="c"><input type="checkbox" id="c-37365383" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365321">parent</a><span>|</span><a href="#37365267">next</a><span>|</span><label class="collapse" for="c-37365383">[-]</label><label class="expand" for="c-37365383">[5 more]</label></div><br/><div class="children"><div class="content">Yeah but if you can only do arithmetic right X% of the time you aren&#x27;t going to get other answers right as often as would really be useful.<p>That said,  LLMs have a magic ability to &quot;short circuit&quot; and get the right answer despite not being able to get the steps right.  I remember scoping out designs for NLP systems about 5 years ago and frequently conclude that &quot;that won&#x27;t work&quot; because information was lost at an early stage but in retrospect by short circuiting a system like that can outperform its parts but it still faces a ceiling on how accurate the answers are because the reasoning <i>is not</i> sound.</div><br/><div id="37365541" class="c"><input type="checkbox" id="c-37365541" checked=""/><div class="controls bullet"><span class="by">btilly</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365383">parent</a><span>|</span><a href="#37365267">next</a><span>|</span><label class="collapse" for="c-37365541">[-]</label><label class="expand" for="c-37365541">[4 more]</label></div><br/><div class="children"><div class="content">Human reasoning is amazingly not sound.<p>When you add in various patterns, double-checks, and memorized previous results, what human reasoning can do is astounding. But it is very, vary far from sound.</div><br/><div id="37366245" class="c"><input type="checkbox" id="c-37366245" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365541">parent</a><span>|</span><a href="#37365267">next</a><span>|</span><label class="collapse" for="c-37366245">[-]</label><label class="expand" for="c-37366245">[3 more]</label></div><br/><div class="children"><div class="content">all currently available reasoning approaches are limited.<p>I guess the topic is how far GPT in reasoning is from human. We can take some simple tests:<p>- can GPT play chess as well as humans, as benchmark of reasoning games?<p>- did GPT prove some nontrivial math theorems or solve some math problems where humans couldn&#x27;t find solution yet?</div><br/><div id="37366670" class="c"><input type="checkbox" id="c-37366670" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366245">parent</a><span>|</span><a href="#37365267">next</a><span>|</span><label class="collapse" for="c-37366670">[-]</label><label class="expand" for="c-37366670">[2 more]</label></div><br/><div class="children"><div class="content">One thing I thought was amusing was that there was a burst of articles about Cyc that got mentioned when Doug Lenat died including this arXiv paper<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.04445" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.04445</a><p>and that one said that Cyc had over 1,100 special purpose reasoning engines.  The general purpose resolution solver was nowhere near fast enough to be really useful.<p>Early one there was<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;General_Problem_Solver" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;General_Problem_Solver</a><p>which would be capable in principle of finding a winning move in a chess position but because it worked by exhaustive search it would practically take too long.  The thing is that a good chess playing program is not generally intelligent just as a chess grandmaster isn&#x27;t necessarily good at anything other than chess,  it just has special purpose heuristics (as opposed to algorithms) that find good chess move.<p>ChatGPT-like systems will be greatly improved by coupling them to other systems such as &quot;write a Python&#x2F;SQL script then run it&quot;,  &quot;run a query against bing and summarize the results&quot;,  and &quot;go find the chess engine and ask it what move to make&quot;,  that is,  like Cyc,  it will get a swiss army knife of tools that help it do things it&#x27;s not good at but it doesn&#x27;t create general intelligence any more than Cyc did.<p>Robert Penrose in the <i>Emperor&#x27;s New Mind</i> suggests that there must be some quantum magic in the human mind because the human mind is able to solve any math problem whereas any machine is limited by Gödel&#x27;s theorem.  It&#x27;s silly,  however,  because we don&#x27;t humans are capable of proving any theorem:  look at how we struggled with Fermat for nearly 360 years or how<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Collatz_conjecture" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Collatz_conjecture</a><p>seems not even tantalizingly out of reach.<p>The difference might be that humans <i>feel bad</i> when they get the wrong answer whereas ChatGPT certainly doesn&#x27;t.  (as much as its empty apology can be satisifying to people)  This isn&#x27;t just an attribute of humans,  working with other animals such as horses I&#x27;m convinced that they feel bad when they screw up too.</div><br/><div id="37367753" class="c"><input type="checkbox" id="c-37367753" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366670">parent</a><span>|</span><a href="#37365267">next</a><span>|</span><label class="collapse" for="c-37367753">[-]</label><label class="expand" for="c-37367753">[1 more]</label></div><br/><div class="children"><div class="content">&gt; it will get a swiss army knife of tools that help it do things it&#x27;s not good at but it doesn&#x27;t create general intelligence any more than Cyc did<p>How do you know general intelligence is its own thing and not just a Swiss army knife of tools?<p>&gt; because the human mind is able to solve any math problem whereas any machine is limited by Gödel&#x27;s theorem<p>Any machine can be programmed to solve any problem at all, if the proof system is inconsistent. Which is probably exactly the case with humans. We work around it because different humans have different inconsistencies, so checking each other&#x27;s work is how we average out those inconsistencies.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="37365267" class="c"><input type="checkbox" id="c-37365267" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365065">parent</a><span>|</span><a href="#37365179">prev</a><span>|</span><a href="#37365184">next</a><span>|</span><label class="collapse" for="c-37365267">[-]</label><label class="expand" for="c-37365267">[5 more]</label></div><br/><div class="children"><div class="content">No it&#x27;s just pretty good in general lol.</div><br/><div id="37365403" class="c"><input type="checkbox" id="c-37365403" checked=""/><div class="controls bullet"><span class="by">cerved</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365267">parent</a><span>|</span><a href="#37365184">next</a><span>|</span><label class="collapse" for="c-37365403">[-]</label><label class="expand" for="c-37365403">[4 more]</label></div><br/><div class="children"><div class="content">my experience is that it&#x27;s pretty subpar</div><br/><div id="37368127" class="c"><input type="checkbox" id="c-37368127" checked=""/><div class="controls bullet"><span class="by">bigyikes</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365403">parent</a><span>|</span><a href="#37366248">next</a><span>|</span><label class="collapse" for="c-37368127">[-]</label><label class="expand" for="c-37368127">[2 more]</label></div><br/><div class="children"><div class="content">Care to share a GPT conversation you’ve had? I’m interested in what sorts of prompts lead you to this opinion. My experience is the opposite.</div><br/><div id="37368749" class="c"><input type="checkbox" id="c-37368749" checked=""/><div class="controls bullet"><span class="by">cerved</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37368127">parent</a><span>|</span><a href="#37366248">next</a><span>|</span><label class="collapse" for="c-37368749">[-]</label><label class="expand" for="c-37368749">[1 more]</label></div><br/><div class="children"><div class="content">A bit too much of a hassle. But if you&#x27;re willing to share some of your good experiences, I&#x27;m curious</div><br/></div></div></div></div><div id="37366248" class="c"><input type="checkbox" id="c-37366248" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365403">parent</a><span>|</span><a href="#37368127">prev</a><span>|</span><a href="#37365184">next</a><span>|</span><label class="collapse" for="c-37366248">[-]</label><label class="expand" for="c-37366248">[1 more]</label></div><br/><div class="children"><div class="content">Are you using code interpreter? It&#x27;s better.<p>The mobile app doesn&#x27;t offer it though, and also has a system prompt that causes some strange behavior - sometimes it will put emojis in the text and then apologize for using emojis.</div><br/></div></div></div></div></div></div></div></div><div id="37365184" class="c"><input type="checkbox" id="c-37365184" checked=""/><div class="controls bullet"><span class="by">sudokuist</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37364901">parent</a><span>|</span><a href="#37365065">prev</a><span>|</span><a href="#37366658">next</a><span>|</span><label class="collapse" for="c-37365184">[-]</label><label class="expand" for="c-37365184">[5 more]</label></div><br/><div class="children"><div class="content">Can LLMs solve sudoku yet?</div><br/><div id="37366896" class="c"><input type="checkbox" id="c-37366896" checked=""/><div class="controls bullet"><span class="by">JieJie</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365184">parent</a><span>|</span><a href="#37366658">next</a><span>|</span><label class="collapse" for="c-37366896">[-]</label><label class="expand" for="c-37366896">[4 more]</label></div><br/><div class="children"><div class="content">These folks think so.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;jieyilong&#x2F;tree-of-thought-puzzle-solver">https:&#x2F;&#x2F;github.com&#x2F;jieyilong&#x2F;tree-of-thought-puzzle-solver</a></div><br/><div id="37367134" class="c"><input type="checkbox" id="c-37367134" checked=""/><div class="controls bullet"><span class="by">sudokuist</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366896">parent</a><span>|</span><a href="#37366658">next</a><span>|</span><label class="collapse" for="c-37367134">[-]</label><label class="expand" for="c-37367134">[3 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t have 9x9 puzzles. Any guesses as to why they only tried 3x3, 4x4, and 5x5 but not 9x9?<p>This work is interesting. I wouldn&#x27;t have guessed 3x3 puzzles would be solvable by a large Markov chain. It would be interesting to know how large of a context is necessary to solve 9x9 puzzles. No existing model can currently solve 9x9 puzzles even though the recursive backtracking algorithm can solve any given puzzle in less than a second.</div><br/><div id="37367979" class="c"><input type="checkbox" id="c-37367979" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37367134">parent</a><span>|</span><a href="#37367902">next</a><span>|</span><label class="collapse" for="c-37367979">[-]</label><label class="expand" for="c-37367979">[1 more]</label></div><br/><div class="children"><div class="content">Why are people so intent on incorrectly asserting these models are Markov chains?  It makes sense to use the analogy as an educational tool for exposition, but it more often seems that many use it as a way to minimize the notion that these models could ever possibly be useful for anyone.  Is this just simply to make it more intuitive for others that it&#x27;s a sequence model?  Because it seems about as helpful as &#x27;email is just bits&#x27; when everyone and their grandma knows about the relation between transformers, GAT, and circulant matrices.</div><br/></div></div><div id="37367902" class="c"><input type="checkbox" id="c-37367902" checked=""/><div class="controls bullet"><span class="by">JieJie</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37367134">parent</a><span>|</span><a href="#37367979">prev</a><span>|</span><a href="#37366658">next</a><span>|</span><label class="collapse" for="c-37367902">[-]</label><label class="expand" for="c-37367902">[1 more]</label></div><br/><div class="children"><div class="content">Well, you just said sudoku.<p>As others have pointed out, maybe intelligence derived from language just isn&#x27;t very good at math? It&#x27;s not like linear algebra comes naturally to humans, we have to be specially trained. I&#x27;ve been taking Khan Academy classes and believe me, math sure doesn&#x27;t come naturally to me.<p>I realize tempers are high on this subject, but I literally just wanted to point it out, in case you hadn&#x27;t seen it. I wasn&#x27;t trying to dunk on you or anything.</div><br/></div></div></div></div></div></div></div></div><div id="37366658" class="c"><input type="checkbox" id="c-37366658" checked=""/><div class="controls bullet"><span class="by">dongping</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37364901">parent</a><span>|</span><a href="#37365184">prev</a><span>|</span><a href="#37366045">next</a><span>|</span><label class="collapse" for="c-37366658">[-]</label><label class="expand" for="c-37366658">[19 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure about its math, but GPT-4 fails miserably at simple arithmetic questions like 897*394=?<p>The GPT-3.5 turbo is fined-tuned for arithmetic according to ClosedAI (noted in one of the change logs), so it is sometimes slightly better, but nevertheless always fails equations like 4897*394=?</div><br/><div id="37368449" class="c"><input type="checkbox" id="c-37368449" checked=""/><div class="controls bullet"><span class="by">Closi</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366658">parent</a><span>|</span><a href="#37366851">next</a><span>|</span><label class="collapse" for="c-37368449">[-]</label><label class="expand" for="c-37368449">[1 more]</label></div><br/><div class="children"><div class="content">&gt; [GPT] always fails equations like 4897 x 394=?<p>In some ways, I think we should treat GPT like a human without access to a calculator.<p>If you ask a human what 4897 x 394 is, they will struggle.</div><br/></div></div><div id="37366851" class="c"><input type="checkbox" id="c-37366851" checked=""/><div class="controls bullet"><span class="by">mistercow</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366658">parent</a><span>|</span><a href="#37368449">prev</a><span>|</span><a href="#37368080">next</a><span>|</span><label class="collapse" for="c-37366851">[-]</label><label class="expand" for="c-37366851">[2 more]</label></div><br/><div class="children"><div class="content">Arithmetic is a pretty pathological case for ChatGPT because of BPE. Digits just tokenize in a way that makes arithmetic way more complicated.<p>That said, I just fed both of your examples into GPT-4 and it answered them correctly without using CoT.</div><br/><div id="37368710" class="c"><input type="checkbox" id="c-37368710" checked=""/><div class="controls bullet"><span class="by">dongping</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366851">parent</a><span>|</span><a href="#37368080">next</a><span>|</span><label class="collapse" for="c-37368710">[-]</label><label class="expand" for="c-37368710">[1 more]</label></div><br/><div class="children"><div class="content">This was the response that I got from the GPT-4 API yesterday:<p>{&#x27;id&#x27;: &#x27;chatcmpl-7uVF5xGqR1oEzXITw3WZYsnB4Yzt8&#x27;, &#x27;object&#x27;: &#x27;chat.completion&#x27;, &#x27;created&#x27;: 1693700819, &#x27;model&#x27;: &#x27;gpt-4-0613&#x27;, &#x27;choices&#x27;: [{&#x27;index&#x27;: 0, &#x27;message&#x27;: {&#x27;role&#x27;: &#x27;assistant&#x27;, &#x27;content&#x27;: &#x27;353538&#x27;}, &#x27;finish_reason&#x27;: &#x27;stop&#x27;}], &#x27;usage&#x27;: {&#x27;prompt_tokens&#x27;: 11, &#x27;completion_tokens&#x27;: 2, &#x27;total_tokens&#x27;: 13}}<p>Maybe they fine-tuned the ChatGPT version better, or fed it to an calculator.</div><br/></div></div></div></div><div id="37368080" class="c"><input type="checkbox" id="c-37368080" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366658">parent</a><span>|</span><a href="#37366851">prev</a><span>|</span><a href="#37367536">next</a><span>|</span><label class="collapse" for="c-37368080">[-]</label><label class="expand" for="c-37368080">[1 more]</label></div><br/><div class="children"><div class="content">It absolutely FAILS for the simple problem of 1+1 in what I like to call &#x27;bubble math&#x27;.<p>1+1=1<p>Or actually, 1+1=1 and 1+1=2, with some probability for each outcome.<p>Because bubbles can be put together and either merge into one, or stay as two bubbles with a shared wall.<p>Obviously this can be extended and formalized, but hopefully it also displays that mathematics isn&#x27;t even guaranteed to provide the same answer for 1+1, since it depends on the context and rules you set up (mod, etc).<p>I should also mention that GPT-4 does quite astoundingly good at this type of problem wherein new rules are made up on the fly.  So in-context learning is powerful, and the idea that it &#x27;just regurgitates training data&#x27; for simple problems is quite false.</div><br/></div></div><div id="37367536" class="c"><input type="checkbox" id="c-37367536" checked=""/><div class="controls bullet"><span class="by">hboon</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366658">parent</a><span>|</span><a href="#37368080">prev</a><span>|</span><a href="#37366691">next</a><span>|</span><label class="collapse" for="c-37367536">[-]</label><label class="expand" for="c-37367536">[2 more]</label></div><br/><div class="children"><div class="content">I think I understand your logic, but ChatGPT+GPT-4 gave me the correct answer for &quot;What is 897*394?&quot;<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;00f94e43-c353-400a-858a-50c10cf8eb4e" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;00f94e43-c353-400a-858a-50c10c...</a><p>(GPT-3 gave the wrong numeric answer though)</div><br/><div id="37368729" class="c"><input type="checkbox" id="c-37368729" checked=""/><div class="controls bullet"><span class="by">dongping</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37367536">parent</a><span>|</span><a href="#37366691">next</a><span>|</span><label class="collapse" for="c-37368729">[-]</label><label class="expand" for="c-37368729">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for testing it. I canceled my ChatGPT Plus a few months ago (when they changed the color from black to purple IIRC).<p>So I only tested that with the GPT-4 API, with the following results:<p>{&#x27;id&#x27;: &#x27;chatcmpl-7uVF5xGqR1oEzXITw3WZYsnB4Yzt8&#x27;, &#x27;object&#x27;: &#x27;chat.completion&#x27;, &#x27;created&#x27;: 1693700819, &#x27;model&#x27;: &#x27;gpt-4-0613&#x27;, &#x27;choices&#x27;: [{&#x27;index&#x27;: 0, &#x27;message&#x27;: {&#x27;role&#x27;: &#x27;assistant&#x27;, &#x27;content&#x27;: &#x27;353538&#x27;}, &#x27;finish_reason&#x27;: &#x27;stop&#x27;}], &#x27;usage&#x27;: {&#x27;prompt_tokens&#x27;: 11, &#x27;completion_tokens&#x27;: 2, &#x27;total_tokens&#x27;: 13}}</div><br/></div></div></div></div><div id="37366691" class="c"><input type="checkbox" id="c-37366691" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366658">parent</a><span>|</span><a href="#37367536">prev</a><span>|</span><a href="#37366045">next</a><span>|</span><label class="collapse" for="c-37366691">[-]</label><label class="expand" for="c-37366691">[12 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m not sure about its math, but GPT-4 fails miserably at simple arithmetic questions like 897*394=?<p>That&#x27;s, um, about 300,000?<p>...<p>353,418 actually. But I&#x27;m not going to blame the AI too much for failing at something I can&#x27;t do either.</div><br/><div id="37366785" class="c"><input type="checkbox" id="c-37366785" checked=""/><div class="controls bullet"><span class="by">dongping</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366691">parent</a><span>|</span><a href="#37367136">next</a><span>|</span><label class="collapse" for="c-37366785">[-]</label><label class="expand" for="c-37366785">[2 more]</label></div><br/><div class="children"><div class="content">One can resort to traditional vertical multiplication (which requires patience), or do<p>897*394 = (900-3) * (400-6) = 900*400 - 6*900 - 400*3 + 3*6 =  360,000 - (5,400 + 1,200) + 18 = 360,018 - 6,600 = 353,418</div><br/><div id="37367684" class="c"><input type="checkbox" id="c-37367684" checked=""/><div class="controls bullet"><span class="by">6510</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366785">parent</a><span>|</span><a href="#37367136">next</a><span>|</span><label class="collapse" for="c-37367684">[-]</label><label class="expand" for="c-37367684">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>   8*3=24 and 800*300 =240000
   8*9=72 and 800* 90 = 72000
   8*4=32 and 800*  4 =  3200
   9*3=27 and  90*300 = 27000
   9*9=81 and  90* 90 =  8100
   9*4=36 and  90*  4 =   360
   7*3=21 and   7*300 =  2100
   7*9=63 and   7* 90 =   630
   7*4=28 and   7*  4 =    28
   --------------------------
                       353418</code></pre></div><br/></div></div></div></div><div id="37367136" class="c"><input type="checkbox" id="c-37367136" checked=""/><div class="controls bullet"><span class="by">dash2</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366691">parent</a><span>|</span><a href="#37366785">prev</a><span>|</span><a href="#37366045">next</a><span>|</span><label class="collapse" for="c-37367136">[-]</label><label class="expand" for="c-37367136">[9 more]</label></div><br/><div class="children"><div class="content">But you are smart enough to use a computer or calculator. And AI <i>is</i> a computer. So the naive expectation would be that it would be capable of doing as well as a computer.<p>Also, you probably could do long multiplication with paper and pencil if you needed to. So a reasoning AI (which has read many many descriptions of how to do long multiplication) should be able to also.</div><br/><div id="37368676" class="c"><input type="checkbox" id="c-37368676" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37367136">parent</a><span>|</span><a href="#37367703">next</a><span>|</span><label class="collapse" for="c-37368676">[-]</label><label class="expand" for="c-37368676">[1 more]</label></div><br/><div class="children"><div class="content">An AI is a program running on a computer.<p>Minecraft runs on a computer too, but you don&#x27;t expect the Minecraft NPCs to be able to do math.<p>So it&#x27;s a <i>very</i> naive assumption.<p>Most people struggle with long multiplication despite not only having learnt the rules, but having had extensive reinforcement training in applying the rules.<p>Getting people conditioned to stay on task for repetitive and detail oriented tasks is <i>difficult</i>. There&#x27;s little reason to believe it&#x27;d be easier to get AIs to stay on task, in part because there&#x27;s a tension between wanting predictability and wanting creativity and problem solving. Ultimately I think the best solution is the same as for humans: tool use. Recognise that the effort required to do some things &quot;manually&quot; is not worth it.</div><br/></div></div><div id="37367703" class="c"><input type="checkbox" id="c-37367703" checked=""/><div class="controls bullet"><span class="by">ambrozk</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37367136">parent</a><span>|</span><a href="#37368676">prev</a><span>|</span><a href="#37368482">next</a><span>|</span><label class="collapse" for="c-37367703">[-]</label><label class="expand" for="c-37367703">[4 more]</label></div><br/><div class="children"><div class="content">&gt; And AI <i>is</i> a computer. So the naive expectation would be that it would be capable of doing as well as a computer.<p>Why would you judge an AI against the expectations of a naive person who doesn&#x27;t understand capabilities AIs are likely to have? If an alien came down to earth and concluded humans weren&#x27;t intelligent because the first person it met couldn&#x27;t simulate quantum systems in their head, would that be fair?</div><br/><div id="37368341" class="c"><input type="checkbox" id="c-37368341" checked=""/><div class="controls bullet"><span class="by">dash2</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37367703">parent</a><span>|</span><a href="#37367903">next</a><span>|</span><label class="collapse" for="c-37368341">[-]</label><label class="expand" for="c-37368341">[2 more]</label></div><br/><div class="children"><div class="content">The original question was whether LLM&#x27;s are &quot;smart&quot; in a human-like way. I think that if you gave a human a computer, he&#x27;d be able to solve 3-digit multiplications. If LLM&#x27;s were human-like smart, they could do this too.</div><br/><div id="37368492" class="c"><input type="checkbox" id="c-37368492" checked=""/><div class="controls bullet"><span class="by">Timon3</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37368341">parent</a><span>|</span><a href="#37367903">next</a><span>|</span><label class="collapse" for="c-37368492">[-]</label><label class="expand" for="c-37368492">[1 more]</label></div><br/><div class="children"><div class="content">Did someone train LLMs with &quot;access&quot; to a computer? If not, why would you expect them to be able to use something they have never seen?</div><br/></div></div></div></div><div id="37367903" class="c"><input type="checkbox" id="c-37367903" checked=""/><div class="controls bullet"><span class="by">walleeee</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37367703">parent</a><span>|</span><a href="#37368341">prev</a><span>|</span><a href="#37368482">next</a><span>|</span><label class="collapse" for="c-37367903">[-]</label><label class="expand" for="c-37367903">[1 more]</label></div><br/><div class="children"><div class="content">I dunno, I simulate quantum systems (you, myself, my friends) in my head all the time</div><br/></div></div></div></div><div id="37368482" class="c"><input type="checkbox" id="c-37368482" checked=""/><div class="controls bullet"><span class="by">Timon3</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37367136">parent</a><span>|</span><a href="#37367703">prev</a><span>|</span><a href="#37368491">next</a><span>|</span><label class="collapse" for="c-37368482">[-]</label><label class="expand" for="c-37368482">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But you are smart enough to use a computer or calculator. And AI is a computer. So the naive expectation would be that it would be capable of doing as well as a computer.<p>I disagree. The AI runs on a computer, but it isn&#x27;t one (in the classical sense). Otherwise you could reduce humans the same way - technically our cells are small (non-classical) computers, and we&#x27;re made up of chemistry. Yet you don&#x27;t expect humans to be perfect at resolving chemical reactions, or computing complex mathematics in their heads.</div><br/></div></div><div id="37368491" class="c"><input type="checkbox" id="c-37368491" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37367136">parent</a><span>|</span><a href="#37368482">prev</a><span>|</span><a href="#37367252">next</a><span>|</span><label class="collapse" for="c-37368491">[-]</label><label class="expand" for="c-37368491">[1 more]</label></div><br/><div class="children"><div class="content">They can reason through it they just sometimes make mistakes along the way, which is not surprising. More relevant to your comment is that if you give gpt4 a calculator it&#x27;ll use it in these cases.</div><br/></div></div><div id="37367252" class="c"><input type="checkbox" id="c-37367252" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37367136">parent</a><span>|</span><a href="#37368491">prev</a><span>|</span><a href="#37366045">next</a><span>|</span><label class="collapse" for="c-37367252">[-]</label><label class="expand" for="c-37367252">[1 more]</label></div><br/><div class="children"><div class="content">I am indeed smart enough to do that. And so is the AI, if you use the right AI. (I.e, code interpreter.)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37366045" class="c"><input type="checkbox" id="c-37366045" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37364228">parent</a><span>|</span><a href="#37364901">prev</a><span>|</span><a href="#37365588">next</a><span>|</span><label class="collapse" for="c-37366045">[-]</label><label class="expand" for="c-37366045">[1 more]</label></div><br/><div class="children"><div class="content">FWIU recently there&#x27;s?:<p>- Increase the input prompt token limit (2023-09: 32K tokens in: OpenAI GPT-4 Enterprise, Giraffe (LLama 2))<p>- Fine tune [a &quot;LoRA&quot; atop a foundation model]<p>- TODO: ~Checkpoint w&#x2F; Copy-on-Write</div><br/></div></div></div></div><div id="37365588" class="c"><input type="checkbox" id="c-37365588" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#37363931">parent</a><span>|</span><a href="#37364228">prev</a><span>|</span><a href="#37364931">next</a><span>|</span><label class="collapse" for="c-37365588">[-]</label><label class="expand" for="c-37365588">[4 more]</label></div><br/><div class="children"><div class="content">Consider what happens if we use this method in our head. Recursively summarize the discussion so far.  It will improve our memory.  It seem &#x27;hacky&quot; to summarize things in your head but I think that is a big part of how memory works.</div><br/><div id="37366411" class="c"><input type="checkbox" id="c-37366411" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365588">parent</a><span>|</span><a href="#37366185">next</a><span>|</span><label class="collapse" for="c-37366411">[-]</label><label class="expand" for="c-37366411">[2 more]</label></div><br/><div class="children"><div class="content">This would be more equivalent to having some little spool in our head that writes down our summarized thoughts and then when we need to remember something we take the spool out of our head and look at it to reorient ourselves.</div><br/><div id="37366646" class="c"><input type="checkbox" id="c-37366646" checked=""/><div class="controls bullet"><span class="by">jonas21</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366411">parent</a><span>|</span><a href="#37366185">next</a><span>|</span><label class="collapse" for="c-37366646">[-]</label><label class="expand" for="c-37366646">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that what people are doing when they take notes and read them later to refresh their memory?</div><br/></div></div></div></div><div id="37366185" class="c"><input type="checkbox" id="c-37366185" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365588">parent</a><span>|</span><a href="#37366411">prev</a><span>|</span><a href="#37364931">next</a><span>|</span><label class="collapse" for="c-37366185">[-]</label><label class="expand" for="c-37366185">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what memory is. Whether sensory information (what you remember for a place) or the points from a meeting (where we only remember the big picture items, not what was said minute to minute) it&#x27;s all summarized&#x2F;compressed.</div><br/></div></div></div></div><div id="37364931" class="c"><input type="checkbox" id="c-37364931" checked=""/><div class="controls bullet"><span class="by">eganist</span><span>|</span><a href="#37363931">parent</a><span>|</span><a href="#37365588">prev</a><span>|</span><a href="#37364327">next</a><span>|</span><label class="collapse" for="c-37364931">[-]</label><label class="expand" for="c-37364931">[13 more]</label></div><br/><div class="children"><div class="content">&gt; rather than as some hacked on process of continually regenerating summaries.<p>incidentally, this isn&#x27;t far off from how the human brain is believed to work (at least with long term memories).<p><a href="https:&#x2F;&#x2F;news.northwestern.edu&#x2F;stories&#x2F;2012&#x2F;09&#x2F;your-memory-is-like-the-telephone-game" rel="nofollow noreferrer">https:&#x2F;&#x2F;news.northwestern.edu&#x2F;stories&#x2F;2012&#x2F;09&#x2F;your-memory-is...</a></div><br/><div id="37366416" class="c"><input type="checkbox" id="c-37366416" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37364931">parent</a><span>|</span><a href="#37365224">next</a><span>|</span><label class="collapse" for="c-37366416">[-]</label><label class="expand" for="c-37366416">[1 more]</label></div><br/><div class="children"><div class="content">&gt; incidentally, this isn&#x27;t far off from how the human brain is believed to work (at least with long term memories).<p>Not as <i>literal words printed somewhere in our mind</i> it isn&#x27;t. This is more akin to things like the funnel transformer. [0] Nevermind that we hardly understand how our minds actually work.<p>[0]: <a href="https:&#x2F;&#x2F;github.com&#x2F;laiguokun&#x2F;Funnel-Transformer">https:&#x2F;&#x2F;github.com&#x2F;laiguokun&#x2F;Funnel-Transformer</a></div><br/></div></div><div id="37365224" class="c"><input type="checkbox" id="c-37365224" checked=""/><div class="controls bullet"><span class="by">sudokuist</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37364931">parent</a><span>|</span><a href="#37366416">prev</a><span>|</span><a href="#37364327">next</a><span>|</span><label class="collapse" for="c-37365224">[-]</label><label class="expand" for="c-37365224">[11 more]</label></div><br/><div class="children"><div class="content">No one knows how the brain works and how it is connected with the body. Did you know your gut is directly connected with cognition? An unhealthy digestive system has been linked with several neurodegenerative diseases. Also, walking and cardio in general is known to create new neurons and prevent cognitive degeneration.<p>It&#x27;s always funny to me when people on online forums confidently proclaim to know what cognition and thinking is all about and that furthermore it can be reduced to symbol shuffling on computers. No one has any clue how to make computers intelligent and anyone that claims to know is either doing marketing for some &quot;AI&quot; company or thoroughly confused about what counts as biological intelligence.</div><br/><div id="37366189" class="c"><input type="checkbox" id="c-37366189" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365224">parent</a><span>|</span><a href="#37367914">next</a><span>|</span><label class="collapse" for="c-37366189">[-]</label><label class="expand" for="c-37366189">[5 more]</label></div><br/><div class="children"><div class="content">&gt;<i>No one knows how the brain works and how it is connected with the body</i><p>Yes, but we have some informed theories.<p>Nobody knew how physics works at Newton&#x27;s time either, but they did know enough to model to satisifcation lots of phenomena like ballistics...<p>&gt;<i>Did you know your gut is directly connected with cognition?</i><p>Well, it is widely reported even in pop-science articles for several decades!<p>&gt;<i>No one has any clue how to make computers intelligent</i><p>For not having &quot;any clue&quot;, sure the LLM guys did quite well in getting some of the way to the goalposts...</div><br/><div id="37366359" class="c"><input type="checkbox" id="c-37366359" checked=""/><div class="controls bullet"><span class="by">sudokuist</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366189">parent</a><span>|</span><a href="#37367914">next</a><span>|</span><label class="collapse" for="c-37366359">[-]</label><label class="expand" for="c-37366359">[4 more]</label></div><br/><div class="children"><div class="content">Turns out Markov chains with a large context can do a lot and yet no one has figured out why LLMs can not solve sudoku puzzles. Why do you think that&#x27;s the case if the goalposts have moved so much?</div><br/><div id="37366404" class="c"><input type="checkbox" id="c-37366404" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366359">parent</a><span>|</span><a href="#37367914">next</a><span>|</span><label class="collapse" for="c-37366404">[-]</label><label class="expand" for="c-37366404">[3 more]</label></div><br/><div class="children"><div class="content">Perhaps because intelligence is multi-faceted, and the aspect required for Sudoku puzzles is not modelled well enough with an LLM style backend.</div><br/><div id="37366458" class="c"><input type="checkbox" id="c-37366458" checked=""/><div class="controls bullet"><span class="by">sudokuist</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366404">parent</a><span>|</span><a href="#37367914">next</a><span>|</span><label class="collapse" for="c-37366458">[-]</label><label class="expand" for="c-37366458">[2 more]</label></div><br/><div class="children"><div class="content">Perhaps.</div><br/><div id="37368136" class="c"><input type="checkbox" id="c-37368136" checked=""/><div class="controls bullet"><span class="by">bigyikes</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37366458">parent</a><span>|</span><a href="#37367914">next</a><span>|</span><label class="collapse" for="c-37368136">[-]</label><label class="expand" for="c-37368136">[1 more]</label></div><br/><div class="children"><div class="content">“Perhaps”? Are you suggesting that intelligence is not multi-faceted? What exactly did the user you’re replying to say that is questionable?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37367914" class="c"><input type="checkbox" id="c-37367914" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365224">parent</a><span>|</span><a href="#37366189">prev</a><span>|</span><a href="#37365297">next</a><span>|</span><label class="collapse" for="c-37367914">[-]</label><label class="expand" for="c-37367914">[1 more]</label></div><br/><div class="children"><div class="content">For someone correctly claiming we don&#x27;t know how the brain works, you seem to have a remarkable trust in the brain-gut axis pop sci articles.  It&#x27;s always good to remember that much of the &#x27;established&#x27; data in neuroscience (such as the A beta hypothesis) are very well within the possibility of being based on a decent amount of fraud, as has been shown recently.<p>Not to say all brain-gut axis data is irrelevant, but, as with almost all of biology, the effect sizes of observables anyone typically cares about are pretty small.<p>However, the visual system in rodents is probably the best data source we have that maps to any formal theory right now, along with temporal difference learning and RL in neuroscience.</div><br/></div></div><div id="37365297" class="c"><input type="checkbox" id="c-37365297" checked=""/><div class="controls bullet"><span class="by">eganist</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365224">parent</a><span>|</span><a href="#37367914">prev</a><span>|</span><a href="#37365561">next</a><span>|</span><label class="collapse" for="c-37365297">[-]</label><label class="expand" for="c-37365297">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re 100% right, no one knows how the brain works. And all the elements you described are probably relevant — including things you didn&#x27;t mention, such as personality changes tied to heart transplants, etc.<p>But that&#x27;s probably reading a little too deeply and seriously into what I said.</div><br/></div></div><div id="37365561" class="c"><input type="checkbox" id="c-37365561" checked=""/><div class="controls bullet"><span class="by">btilly</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365224">parent</a><span>|</span><a href="#37365297">prev</a><span>|</span><a href="#37366238">next</a><span>|</span><label class="collapse" for="c-37365561">[-]</label><label class="expand" for="c-37365561">[1 more]</label></div><br/><div class="children"><div class="content">Nobody knows, but this model works well enough to, for instance, treat PTSD. Read through <a href="https:&#x2F;&#x2F;www.ptsd.va.gov&#x2F;understand_tx&#x2F;emdr.asp" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.ptsd.va.gov&#x2F;understand_tx&#x2F;emdr.asp</a> to verify that.</div><br/></div></div><div id="37366238" class="c"><input type="checkbox" id="c-37366238" checked=""/><div class="controls bullet"><span class="by">doctor_eval</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37365224">parent</a><span>|</span><a href="#37365561">prev</a><span>|</span><a href="#37364327">next</a><span>|</span><label class="collapse" for="c-37366238">[-]</label><label class="expand" for="c-37366238">[1 more]</label></div><br/><div class="children"><div class="content">This is why the Turing Test tried to abstract away everything other than the text of a conversation. Because ultimately the mechanism is going to be inscrutable, and it’s the output that counts.</div><br/></div></div></div></div></div></div><div id="37364327" class="c"><input type="checkbox" id="c-37364327" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#37363931">parent</a><span>|</span><a href="#37364931">prev</a><span>|</span><a href="#37367124">next</a><span>|</span><label class="collapse" for="c-37364327">[-]</label><label class="expand" for="c-37364327">[2 more]</label></div><br/><div class="children"><div class="content">I would note that almost everything in computing we use today is super hacky stuff sufficiently abstracted and error handled such that it seems like it’s not at all a hack.</div><br/><div id="37368150" class="c"><input type="checkbox" id="c-37368150" checked=""/><div class="controls bullet"><span class="by">bigyikes</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37364327">parent</a><span>|</span><a href="#37367124">next</a><span>|</span><label class="collapse" for="c-37368150">[-]</label><label class="expand" for="c-37368150">[1 more]</label></div><br/><div class="children"><div class="content">I would note that all of biology is hacks on top of hacks! Have you seen a human being? They have this useless fucking organ that tends to burst and kill them.</div><br/></div></div></div></div><div id="37367124" class="c"><input type="checkbox" id="c-37367124" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#37363931">parent</a><span>|</span><a href="#37364327">prev</a><span>|</span><a href="#37364406">next</a><span>|</span><label class="collapse" for="c-37367124">[-]</label><label class="expand" for="c-37367124">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m wondering how well the data is preserved if you put it in a buffer like that.<p>It reminds me of the game where someone tells A some story, then A tells it to B, B to C, etc. and then of course the end result is that the story became completely different.</div><br/></div></div><div id="37364406" class="c"><input type="checkbox" id="c-37364406" checked=""/><div class="controls bullet"><span class="by">tinco</span><span>|</span><a href="#37363931">parent</a><span>|</span><a href="#37367124">prev</a><span>|</span><a href="#37367297">next</a><span>|</span><label class="collapse" for="c-37364406">[-]</label><label class="expand" for="c-37364406">[2 more]</label></div><br/><div class="children"><div class="content">Why do you have the intuition that a dense embedding space could preserve full semantic meaning? From what I understand from embeddings is that they are inherently lossy. At least with a textual summary you could have an agent verify the summary actually accurately represents the information that it&#x27;s meant to summarize.</div><br/><div id="37365030" class="c"><input type="checkbox" id="c-37365030" checked=""/><div class="controls bullet"><span class="by">chefandy</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37364406">parent</a><span>|</span><a href="#37367297">next</a><span>|</span><label class="collapse" for="c-37365030">[-]</label><label class="expand" for="c-37365030">[1 more]</label></div><br/><div class="children"><div class="content">From a technical perspective, sure: it&#x27;s clear why it functions like that, and there&#x27;s no technical reason it shouldn&#x27;t. From a user interface perspective— likely what most people would judge with intuition— that doesn&#x27;t matter. Processes that mimick familiar interaction patterns cause dissonance when they don&#x27;t match our mental model of those interactions, and familiar conversation is about as familiar as you get. People we interact with know the things we explicitly tell them or guide them through figuring out, and people intuitively interact with these applications as they would with people because they&#x27;ve deliberately adopted the voice of an individual person. Additionally, for SaaS products, we&#x27;re used to them maintaining state automatically.<p>(As annoying as our input can be, this is why dev teams that try to solve problems for non-dev users have designers.)</div><br/></div></div></div></div><div id="37367297" class="c"><input type="checkbox" id="c-37367297" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#37363931">parent</a><span>|</span><a href="#37364406">prev</a><span>|</span><a href="#37365044">next</a><span>|</span><label class="collapse" for="c-37367297">[-]</label><label class="expand" for="c-37367297">[1 more]</label></div><br/><div class="children"><div class="content">It seems to me that there&#x27;s a major advantage, though: you can read it and fix any mistakes.</div><br/></div></div><div id="37365044" class="c"><input type="checkbox" id="c-37365044" checked=""/><div class="controls bullet"><span class="by">digitcatphd</span><span>|</span><a href="#37363931">parent</a><span>|</span><a href="#37367297">prev</a><span>|</span><a href="#37367897">next</a><span>|</span><label class="collapse" for="c-37365044">[-]</label><label class="expand" for="c-37365044">[1 more]</label></div><br/><div class="children"><div class="content">Most things right now seem so.
Seems like we are going through many rounds of iteration and guessing what works long term and what is a short term fix is frustrating</div><br/></div></div><div id="37367897" class="c"><input type="checkbox" id="c-37367897" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#37363931">parent</a><span>|</span><a href="#37365044">prev</a><span>|</span><a href="#37365165">next</a><span>|</span><label class="collapse" for="c-37367897">[-]</label><label class="expand" for="c-37367897">[1 more]</label></div><br/><div class="children"><div class="content">Sure but airplanes don&#x27;t look like birds</div><br/></div></div><div id="37365305" class="c"><input type="checkbox" id="c-37365305" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#37363931">parent</a><span>|</span><a href="#37365165">prev</a><span>|</span><a href="#37364549">next</a><span>|</span><label class="collapse" for="c-37365305">[-]</label><label class="expand" for="c-37365305">[1 more]</label></div><br/><div class="children"><div class="content">But there are hardware limitations for storing them in memory</div><br/></div></div><div id="37364549" class="c"><input type="checkbox" id="c-37364549" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#37363931">parent</a><span>|</span><a href="#37365305">prev</a><span>|</span><a href="#37365344">next</a><span>|</span><label class="collapse" for="c-37364549">[-]</label><label class="expand" for="c-37364549">[2 more]</label></div><br/><div class="children"><div class="content">It seems to me that sparse encodings would be more efficient and practical for medium term memory. Isn&#x27;t rhe problem wirh dense embedding is memory usage?</div><br/><div id="37366422" class="c"><input type="checkbox" id="c-37366422" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#37363931">root</a><span>|</span><a href="#37364549">parent</a><span>|</span><a href="#37365344">next</a><span>|</span><label class="collapse" for="c-37366422">[-]</label><label class="expand" for="c-37366422">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Isn&#x27;t rhe problem wirh dense embedding is memory usage?<p>Not necessarily, given that a dense embedding can encode the equivalent of many many words or even higher order concepts not easily expressed in word space.</div><br/></div></div></div></div></div></div><div id="37365344" class="c"><input type="checkbox" id="c-37365344" checked=""/><div class="controls bullet"><span class="by">TuringNYC</span><span>|</span><a href="#37363931">prev</a><span>|</span><a href="#37364310">next</a><span>|</span><label class="collapse" for="c-37365344">[-]</label><label class="expand" for="c-37365344">[1 more]</label></div><br/><div class="children"><div class="content">A bit of personal anecdote -- at work, we have thousands of &quot;Briefings&quot; which are hour-long (sometimes day-long) in-person panels. We&#x27;ve successfully summarized each and every briefing. The messy transcripts are well summarized into five paragraph of text.<p>More topical, we also 1:many categorized each briefing into topics and sub-topics with topics ending up with several dozen briefings and sub-topics of a dozen briefings. For this <i>we summarized the subset of associated summaries</i> and tested this comprehensively, and had great results with LLMs.<p>I was originally skeptical this would work, but it worked beautifully. Given a sufficiently large context window, we would not have done so, but thankfully this was not a problem.</div><br/></div></div><div id="37364310" class="c"><input type="checkbox" id="c-37364310" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#37365344">prev</a><span>|</span><a href="#37366331">next</a><span>|</span><label class="collapse" for="c-37364310">[-]</label><label class="expand" for="c-37364310">[9 more]</label></div><br/><div class="children"><div class="content">I tried to build memory using recursive summarization months ago using open source models and what I found is that with a naive implementation, it would often get stuck on a certain topic forever because certain bits would survive all summarization rounds.</div><br/><div id="37368068" class="c"><input type="checkbox" id="c-37368068" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#37364310">parent</a><span>|</span><a href="#37364526">next</a><span>|</span><label class="collapse" for="c-37368068">[-]</label><label class="expand" for="c-37368068">[1 more]</label></div><br/><div class="children"><div class="content">Not only that, it’s provable that this approach doesn’t scale.<p>That is to say, specifically, that it is not possible, by any means, to take any block of text and reduce it to a smaller block of text without losing some information.<p>That is infinite compression; if you could do this, you could reduce any dataset to 1 bit and recover the data seamlessly.<p>You can’t.<p>What that means is that this approach is fundamentally just improving performance, it’s not solving the problem.<p>When you compress a conversation into a summary, some information will be lost. You can tweak and fold and do whatever clever things you want, but fundamentally you are losing information.<p>…and the process is recursive. So at some point you will be summarising a set of summaries… and you will lose some information you’ve summarised, to some extent.<p>You can’t <i>not</i> lose information with this process.<p>So… while, it probably helps in trivial cases, putting recursive summaries into your prompt is kind of daft, and almost certainly doesn’t actually work when you try use it to do useful things.<p>It probably <i>looks like it’s working</i> when you don’t use the recursive summaries heavily, because at that point you’re not losing <i>much</i> information.<p>…but, I’m going to bet that it doesn’t scale, and people using it will find that out pretty quickly as they use it.</div><br/></div></div><div id="37364526" class="c"><input type="checkbox" id="c-37364526" checked=""/><div class="controls bullet"><span class="by">washadjeffmad</span><span>|</span><a href="#37364310">parent</a><span>|</span><a href="#37368068">prev</a><span>|</span><a href="#37364397">next</a><span>|</span><label class="collapse" for="c-37364526">[-]</label><label class="expand" for="c-37364526">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, unless this substantially mitigates amplification, even when using manual chunk sizing on known materials, the context still hangs onto its &quot;dying thoughts&quot; in a way that remarkably resembles Alzheimer&#x27;s.</div><br/><div id="37364567" class="c"><input type="checkbox" id="c-37364567" checked=""/><div class="controls bullet"><span class="by">cushpush</span><span>|</span><a href="#37364310">root</a><span>|</span><a href="#37364526">parent</a><span>|</span><a href="#37364397">next</a><span>|</span><label class="collapse" for="c-37364567">[-]</label><label class="expand" for="c-37364567">[1 more]</label></div><br/><div class="children"><div class="content">plaque buildup?</div><br/></div></div></div></div><div id="37364397" class="c"><input type="checkbox" id="c-37364397" checked=""/><div class="controls bullet"><span class="by">arketyp</span><span>|</span><a href="#37364310">parent</a><span>|</span><a href="#37364526">prev</a><span>|</span><a href="#37364543">next</a><span>|</span><label class="collapse" for="c-37364397">[-]</label><label class="expand" for="c-37364397">[1 more]</label></div><br/><div class="children"><div class="content">Reminds me of a &quot;bad trip&quot; or OCD patterns. I sometimes think about how little it takes to derail the human mind, by trauma or ontogenically, and how wishful the idea of a human-like AI then seems.</div><br/></div></div><div id="37364543" class="c"><input type="checkbox" id="c-37364543" checked=""/><div class="controls bullet"><span class="by">keskival</span><span>|</span><a href="#37364310">parent</a><span>|</span><a href="#37364397">prev</a><span>|</span><a href="#37366331">next</a><span>|</span><label class="collapse" for="c-37364543">[-]</label><label class="expand" for="c-37364543">[4 more]</label></div><br/><div class="children"><div class="content">You can just advice it to forget (that is, skip in summarization) things which seem irrelevant.</div><br/><div id="37365537" class="c"><input type="checkbox" id="c-37365537" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37364310">root</a><span>|</span><a href="#37364543">parent</a><span>|</span><a href="#37366331">next</a><span>|</span><label class="collapse" for="c-37365537">[-]</label><label class="expand" for="c-37365537">[3 more]</label></div><br/><div class="children"><div class="content">Or you could use techniques from the 1970s that let you skip the kind of highly repetitive text that causes them to get stuck without losing important concepts (IDF)</div><br/><div id="37366711" class="c"><input type="checkbox" id="c-37366711" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#37364310">root</a><span>|</span><a href="#37365537">parent</a><span>|</span><a href="#37366331">next</a><span>|</span><label class="collapse" for="c-37366711">[-]</label><label class="expand" for="c-37366711">[2 more]</label></div><br/><div class="children"><div class="content">This is interesting. How would you incorporate IDF into an LLM?</div><br/><div id="37367447" class="c"><input type="checkbox" id="c-37367447" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37364310">root</a><span>|</span><a href="#37366711">parent</a><span>|</span><a href="#37366331">next</a><span>|</span><label class="collapse" for="c-37367447">[-]</label><label class="expand" for="c-37367447">[1 more]</label></div><br/><div class="children"><div class="content">BM25F (which is based on IDF) to quickly identify relevant past responses based on the question and reply they got.<p>At each summarization step you can go through and find which Q&#x2F;A pairs have the highest relevance across all of the other questions in the current batch, which helps solve LLMs tendencies to get stuck in high similar repetitions</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37366331" class="c"><input type="checkbox" id="c-37366331" checked=""/><div class="controls bullet"><span class="by">Michelangelo11</span><span>|</span><a href="#37364310">prev</a><span>|</span><a href="#37366696">next</a><span>|</span><label class="collapse" for="c-37366331">[-]</label><label class="expand" for="c-37366331">[3 more]</label></div><br/><div class="children"><div class="content">Kind of a disappointing paper, really. Essentially zero details about their technique, just tables showing that, with the methodology they used, it does indeed achieve good results.<p>I get that that&#x27;s par for the course for science these days, but to me as a developer working on LLMs, the paper is essentially valueless (of course, I&#x27;m sure it incrementally raises the authors&#x27; prestige in their corner of academia, as was no doubt its purpose).</div><br/><div id="37368118" class="c"><input type="checkbox" id="c-37368118" checked=""/><div class="controls bullet"><span class="by">Roark66</span><span>|</span><a href="#37366331">parent</a><span>|</span><a href="#37366893">next</a><span>|</span><label class="collapse" for="c-37368118">[-]</label><label class="expand" for="c-37368118">[1 more]</label></div><br/><div class="children"><div class="content">Also:
&gt;Code and scripts will be released later.<p>Hey, wanna team up and author a few articles like that?&lt;&#x2F;sarcasm&gt; I wonder how many it&#x27;ll take before one can put a title of a &quot;ML researcher&quot; on top of their CV.</div><br/></div></div><div id="37366893" class="c"><input type="checkbox" id="c-37366893" checked=""/><div class="controls bullet"><span class="by">glandium</span><span>|</span><a href="#37366331">parent</a><span>|</span><a href="#37368118">prev</a><span>|</span><a href="#37366696">next</a><span>|</span><label class="collapse" for="c-37366893">[-]</label><label class="expand" for="c-37366893">[1 more]</label></div><br/><div class="children"><div class="content">Check the very last page. Their prompt is there.</div><br/></div></div></div></div><div id="37366696" class="c"><input type="checkbox" id="c-37366696" checked=""/><div class="controls bullet"><span class="by">roseway4</span><span>|</span><a href="#37366331">prev</a><span>|</span><a href="#37368134">next</a><span>|</span><label class="collapse" for="c-37366696">[-]</label><label class="expand" for="c-37366696">[5 more]</label></div><br/><div class="children"><div class="content">I’m struggling to understand what’s novel here. LLM-based summarization of chat history memory is a well-established technique implemented by many LLM frameworks. Summarizing on every message is, as proposed in the paper, a major performance bottleneck and adds significant latency to the chat loop.<p>Many implementations utilize a fixed sized buffer, progressively summarizing batches of older memories when they fall out of the buffer. Ideally, this is also done out of band to the chat loop.<p>I’m an author of Zep[0], an open source long-term memory store, and this is how we implemented summarization.<p>0: <a href="https:&#x2F;&#x2F;github.com&#x2F;getzep&#x2F;zep">https:&#x2F;&#x2F;github.com&#x2F;getzep&#x2F;zep</a></div><br/><div id="37366843" class="c"><input type="checkbox" id="c-37366843" checked=""/><div class="controls bullet"><span class="by">anotherpaulg</span><span>|</span><a href="#37366696">parent</a><span>|</span><a href="#37367129">next</a><span>|</span><label class="collapse" for="c-37366843">[-]</label><label class="expand" for="c-37366843">[1 more]</label></div><br/><div class="children"><div class="content">Aider does this too, using a background thread to summarize the messages older than the last N.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;paul-gauthier&#x2F;aider&#x2F;blob&#x2F;main&#x2F;aider&#x2F;history.py">https:&#x2F;&#x2F;github.com&#x2F;paul-gauthier&#x2F;aider&#x2F;blob&#x2F;main&#x2F;aider&#x2F;histo...</a></div><br/></div></div><div id="37367129" class="c"><input type="checkbox" id="c-37367129" checked=""/><div class="controls bullet"><span class="by">px43</span><span>|</span><a href="#37366696">parent</a><span>|</span><a href="#37366843">prev</a><span>|</span><a href="#37367374">next</a><span>|</span><label class="collapse" for="c-37367129">[-]</label><label class="expand" for="c-37367129">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I&#x27;m pretty novice, but I took that hourish long Andrew Ng class on LangChain, and they covered recursive summarization as a standard memory management technique.<p><a href="https:&#x2F;&#x2F;www.deeplearning.ai&#x2F;short-courses&#x2F;langchain-for-llm-application-development&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.deeplearning.ai&#x2F;short-courses&#x2F;langchain-for-llm-...</a></div><br/></div></div><div id="37367325" class="c"><input type="checkbox" id="c-37367325" checked=""/><div class="controls bullet"><span class="by">chrgy</span><span>|</span><a href="#37366696">parent</a><span>|</span><a href="#37367374">prev</a><span>|</span><a href="#37368134">next</a><span>|</span><label class="collapse" for="c-37367325">[-]</label><label class="expand" for="c-37367325">[1 more]</label></div><br/><div class="children"><div class="content">Exactly, There is nothing Novel here, even a middle school Chatgpt user would have known this.</div><br/></div></div></div></div><div id="37368134" class="c"><input type="checkbox" id="c-37368134" checked=""/><div class="controls bullet"><span class="by">samanator</span><span>|</span><a href="#37366696">prev</a><span>|</span><a href="#37365277">next</a><span>|</span><label class="collapse" for="c-37368134">[-]</label><label class="expand" for="c-37368134">[1 more]</label></div><br/><div class="children"><div class="content">The paper&#x27;s implementation is to essentially append memory text as part of the prompt.<p>Why don&#x27;t they use a storage&#x2F;retrieval system that doesn&#x27;t consume context window tokens? E.g. Storage could be to automatically categorize data with tags at insertion time (i.e. upon user prompt) and retrieval could be a query that filters using a tag guessed by the LLM (before responding to the user).<p>With a few initial rules like hard coded tag names&#x2F;styles, my intuition is that this would produce great results.</div><br/></div></div><div id="37365277" class="c"><input type="checkbox" id="c-37365277" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#37368134">prev</a><span>|</span><a href="#37368397">next</a><span>|</span><label class="collapse" for="c-37365277">[-]</label><label class="expand" for="c-37365277">[1 more]</label></div><br/><div class="children"><div class="content">This is a little sideways to the article&#x2F;discussion.<p>The short memory is a real limitation, but I have noticed most critiques of GPT-4&#x27;s abilities apply as much, or more, to humans.<p>I don&#x27;t think anyone alive could convince me they were GPT-4 in a Reverse Turing Test situation. GPT-4&#x27;s fast organized responses alone hammer human abilities.<p>But even a team of humans, with 60 minutes to answer each question, could find it difficult to match GPT-4&#x27;s responses to interesting queries. It would be a fun competition.</div><br/></div></div><div id="37368397" class="c"><input type="checkbox" id="c-37368397" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#37365277">prev</a><span>|</span><a href="#37364042">next</a><span>|</span><label class="collapse" for="c-37368397">[-]</label><label class="expand" for="c-37368397">[2 more]</label></div><br/><div class="children"><div class="content">How is this original work? I bet you can find 100 implementations of some kind of recursive summarization memory for LLMs on  Github, one of them on mine.</div><br/><div id="37368435" class="c"><input type="checkbox" id="c-37368435" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#37368397">parent</a><span>|</span><a href="#37364042">next</a><span>|</span><label class="collapse" for="c-37368435">[-]</label><label class="expand" for="c-37368435">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not into ML at all, and this was the first idea I had when I learned about the limitations of the context length. Even commented on it here[1] some time later.<p>Of course, it&#x27;s one thing to have a good idea, another is to actually implement it well...<p>[1]: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36531771">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36531771</a></div><br/></div></div></div></div><div id="37364042" class="c"><input type="checkbox" id="c-37364042" checked=""/><div class="controls bullet"><span class="by">MadsRC</span><span>|</span><a href="#37368397">prev</a><span>|</span><a href="#37363927">next</a><span>|</span><label class="collapse" for="c-37364042">[-]</label><label class="expand" for="c-37364042">[1 more]</label></div><br/><div class="children"><div class="content">This seems very close to Langchain&#x27;s &quot;summary&quot; memory functionality (which seems to have existed since March 2023?) - granted I haven&#x27;t read the paper just yet</div><br/></div></div><div id="37363927" class="c"><input type="checkbox" id="c-37363927" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37364042">prev</a><span>|</span><a href="#37364184">next</a><span>|</span><label class="collapse" for="c-37363927">[-]</label><label class="expand" for="c-37363927">[1 more]</label></div><br/><div class="children"><div class="content">I mentioned this in a comment a few weeks ago, but people are oversimplifying the summarization part: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37117515">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37117515</a><p>For a given use-case, long term memory has a subtly different value proposition.<p>If I&#x27;m building a home assistant, I should be using NER to identify names and build an understanding of how they like to be spoken to in messages, or places and how they tend to be commuted to<p>If I&#x27;m building a CS bot, I should be identifying queries that resulted in extended exchanges, or led to a sudden abandoned cart<p>Summarization at the generic summarization level is enough for flashy demoes, but to build truly useful products right now you need to go a step further</div><br/></div></div><div id="37364184" class="c"><input type="checkbox" id="c-37364184" checked=""/><div class="controls bullet"><span class="by">caprock</span><span>|</span><a href="#37363927">prev</a><span>|</span><a href="#37366554">next</a><span>|</span><label class="collapse" for="c-37364184">[-]</label><label class="expand" for="c-37364184">[1 more]</label></div><br/><div class="children"><div class="content">This sounds a lot like the chunking concept from learning theory.</div><br/></div></div><div id="37366554" class="c"><input type="checkbox" id="c-37366554" checked=""/><div class="controls bullet"><span class="by">slavboj</span><span>|</span><a href="#37364184">prev</a><span>|</span><a href="#37365560">next</a><span>|</span><label class="collapse" for="c-37366554">[-]</label><label class="expand" for="c-37366554">[1 more]</label></div><br/><div class="children"><div class="content">In human terms, we would call this &quot;reading the minutes&quot;.</div><br/></div></div><div id="37365560" class="c"><input type="checkbox" id="c-37365560" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#37366554">prev</a><span>|</span><a href="#37364779">next</a><span>|</span><label class="collapse" for="c-37365560">[-]</label><label class="expand" for="c-37365560">[2 more]</label></div><br/><div class="children"><div class="content">This wouldn’t work for ChatGPT right? All it cares about is the input context</div><br/><div id="37365739" class="c"><input type="checkbox" id="c-37365739" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#37365560">parent</a><span>|</span><a href="#37364779">next</a><span>|</span><label class="collapse" for="c-37365739">[-]</label><label class="expand" for="c-37365739">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT is used in the paper. The problem they&#x27;re solving is how to throw a loop around the LLM (like ChatGPT) to generate an appropriate input context.</div><br/></div></div></div></div><div id="37364779" class="c"><input type="checkbox" id="c-37364779" checked=""/><div class="controls bullet"><span class="by">wintermutestwin</span><span>|</span><a href="#37365560">prev</a><span>|</span><a href="#37365314">next</a><span>|</span><label class="collapse" for="c-37364779">[-]</label><label class="expand" for="c-37364779">[11 more]</label></div><br/><div class="children"><div class="content">Probably my ignorance talking, but I don&#x27;t understand why there isn&#x27;t a layer on top of an LLM that uses good old computer logic to keep track of and validate data.<p>I am baffled by the fact that when I ask ChatGPT to look up some stock data (for example) it spits out errors. How does it not have a built in ability to check its own output?</div><br/><div id="37364997" class="c"><input type="checkbox" id="c-37364997" checked=""/><div class="controls bullet"><span class="by">a-r-t</span><span>|</span><a href="#37364779">parent</a><span>|</span><a href="#37364951">next</a><span>|</span><label class="collapse" for="c-37364997">[-]</label><label class="expand" for="c-37364997">[5 more]</label></div><br/><div class="children"><div class="content">Because if you had a logic layer capable of validating LLM&#x27;s output, you wouldn&#x27;t need the LLM.</div><br/><div id="37366098" class="c"><input type="checkbox" id="c-37366098" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#37364779">root</a><span>|</span><a href="#37364997">parent</a><span>|</span><a href="#37366740">next</a><span>|</span><label class="collapse" for="c-37366098">[-]</label><label class="expand" for="c-37366098">[2 more]</label></div><br/><div class="children"><div class="content">I think we still need an LLM to enable the system as a whole to understand vague and half-baked human input.<p>I can easily ask an LLM to write be a function in a random programming language, then feed the output to a compiler, and pipe errors from the compiler back to the LLM.<p>What doesn&#x27;t work so well is typing &quot;pong in java&quot; into a bash shell.<p>This isn&#x27;t a perfect solution (not even for small projects), but it does demonstrate that automated validation can improve the output.</div><br/><div id="37366784" class="c"><input type="checkbox" id="c-37366784" checked=""/><div class="controls bullet"><span class="by">a-r-t</span><span>|</span><a href="#37364779">root</a><span>|</span><a href="#37366098">parent</a><span>|</span><a href="#37366740">next</a><span>|</span><label class="collapse" for="c-37366784">[-]</label><label class="expand" for="c-37366784">[1 more]</label></div><br/><div class="children"><div class="content">This is what ChatGPT&#x27;s Code Interpreter does (writes code in Python and then runs it to check for errors). I&#x27;m not sure if it&#x27;s enabled for everyone yet though.</div><br/></div></div></div></div><div id="37366740" class="c"><input type="checkbox" id="c-37366740" checked=""/><div class="controls bullet"><span class="by">turmeric_root</span><span>|</span><a href="#37364779">root</a><span>|</span><a href="#37364997">parent</a><span>|</span><a href="#37366098">prev</a><span>|</span><a href="#37365411">next</a><span>|</span><label class="collapse" for="c-37366740">[-]</label><label class="expand" for="c-37366740">[1 more]</label></div><br/><div class="children"><div class="content">c&#x27;mon just write a function that takes in text and tells you whether or not it&#x27;s true, how hard could it be</div><br/></div></div><div id="37365411" class="c"><input type="checkbox" id="c-37365411" checked=""/><div class="controls bullet"><span class="by">kendalf89</span><span>|</span><a href="#37364779">root</a><span>|</span><a href="#37364997">parent</a><span>|</span><a href="#37366740">prev</a><span>|</span><a href="#37364951">next</a><span>|</span><label class="collapse" for="c-37365411">[-]</label><label class="expand" for="c-37365411">[1 more]</label></div><br/><div class="children"><div class="content">Even something as simple as censoring swear words would be in line with what openAI are trying to accomplish but they keep lobotomizing the model instead.</div><br/></div></div></div></div><div id="37364951" class="c"><input type="checkbox" id="c-37364951" checked=""/><div class="controls bullet"><span class="by">jamilton</span><span>|</span><a href="#37364779">parent</a><span>|</span><a href="#37364997">prev</a><span>|</span><a href="#37366747">next</a><span>|</span><label class="collapse" for="c-37364951">[-]</label><label class="expand" for="c-37364951">[1 more]</label></div><br/><div class="children"><div class="content">What do you mean by &quot;check its own output&quot;?</div><br/></div></div><div id="37365683" class="c"><input type="checkbox" id="c-37365683" checked=""/><div class="controls bullet"><span class="by">tomr75</span><span>|</span><a href="#37364779">parent</a><span>|</span><a href="#37366747">prev</a><span>|</span><a href="#37366683">next</a><span>|</span><label class="collapse" for="c-37365683">[-]</label><label class="expand" for="c-37365683">[1 more]</label></div><br/><div class="children"><div class="content">people have built their own solutions to validate data like this<p>won&#x27;t be one size fits all due to various types of data  - and if you could validate all types - why do you need the llm!</div><br/></div></div><div id="37366683" class="c"><input type="checkbox" id="c-37366683" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37364779">parent</a><span>|</span><a href="#37365683">prev</a><span>|</span><a href="#37366675">next</a><span>|</span><label class="collapse" for="c-37366683">[-]</label><label class="expand" for="c-37366683">[1 more]</label></div><br/><div class="children"><div class="content">It does have the ability, you haven&#x27;t allowed it to.<p>If you&#x27;re completing the sentence &quot;What is the current price of Apple?...&quot; based on the internet as a training source, the most likely reply is not:<p>&quot;Well, let&#x27;s think about how we&#x27;d go about this, I should do a search for AAPL, and then...&quot;<p>The most likely completion is:
&quot;The price of AAPL is $X&quot;<p>OpenAI had to &quot;artificially&quot; bias it to say &quot;I can&#x27;t answer that&quot; or it&#x27;d happily tell you a million things like that.<p>—<p>On the other hand, give the LLM room to plan, it will plan. If you ask it &quot;To answer x what do you need&quot;, it does better at answering x<p>The most likely completion to &quot;How would you tell me Apple&#x27;s current price&quot;, is a logical step by step process. And that&#x27;s how it gets a chance to check itself.<p>I think people underestimate how much logic the model can do in a single pass, LLMs need to output things that we assume can be done with working memory</div><br/></div></div><div id="37366675" class="c"><input type="checkbox" id="c-37366675" checked=""/><div class="controls bullet"><span class="by">tayo42</span><span>|</span><a href="#37364779">parent</a><span>|</span><a href="#37366683">prev</a><span>|</span><a href="#37365314">next</a><span>|</span><label class="collapse" for="c-37366675">[-]</label><label class="expand" for="c-37366675">[1 more]</label></div><br/><div class="children"><div class="content">There is, I&#x27;ve seen it called react pattern. I&#x27;m pretty sure this is what chat gpts plug-ins feature is</div><br/></div></div></div></div><div id="37365314" class="c"><input type="checkbox" id="c-37365314" checked=""/><div class="controls bullet"><span class="by">ipnon</span><span>|</span><a href="#37364779">prev</a><span>|</span><a href="#37363724">next</a><span>|</span><label class="collapse" for="c-37365314">[-]</label><label class="expand" for="c-37365314">[2 more]</label></div><br/><div class="children"><div class="content">Very interesting academies these researchers are based at … makes you wonder what kind of new applications this research enables.</div><br/><div id="37366121" class="c"><input type="checkbox" id="c-37366121" checked=""/><div class="controls bullet"><span class="by">Philpax</span><span>|</span><a href="#37365314">parent</a><span>|</span><a href="#37363724">next</a><span>|</span><label class="collapse" for="c-37366121">[-]</label><label class="expand" for="c-37366121">[1 more]</label></div><br/><div class="children"><div class="content">What are you saying?</div><br/></div></div></div></div><div id="37363724" class="c"><input type="checkbox" id="c-37363724" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#37365314">prev</a><span>|</span><a href="#37367308">next</a><span>|</span><label class="collapse" for="c-37363724">[-]</label><label class="expand" for="c-37363724">[4 more]</label></div><br/><div class="children"><div class="content">7 people got together and wrote this paper<p>Can someone summarize why this is different&#x2F;better&#x2F;more important than what we’ve seen 4 months ago with AutoGPT or even longer ago with the guy who got ChatGPT to make a compression algorithm that other ChatGPT sessions could read and resume conversations from</div><br/><div id="37364582" class="c"><input type="checkbox" id="c-37364582" checked=""/><div class="controls bullet"><span class="by">keskival</span><span>|</span><a href="#37363724">parent</a><span>|</span><a href="#37363924">next</a><span>|</span><label class="collapse" for="c-37364582">[-]</label><label class="expand" for="c-37364582">[1 more]</label></div><br/><div class="children"><div class="content">It seems to be exactly the same as what everyone has been doing for a long time already, and they fail in citing existing implementations.</div><br/></div></div><div id="37363924" class="c"><input type="checkbox" id="c-37363924" checked=""/><div class="controls bullet"><span class="by">codefreakxff</span><span>|</span><a href="#37363724">parent</a><span>|</span><a href="#37364582">prev</a><span>|</span><a href="#37363754">next</a><span>|</span><label class="collapse" for="c-37363924">[-]</label><label class="expand" for="c-37363924">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t read it yet to be certain, but my impression is that rather than using lossless compression of entire chats, this uses lossy summarization to get the gist of chats. There will be tradeoffs between the two methods, hopefully this paper covers that.</div><br/></div></div></div></div></div></div></div></div></div></body></html>