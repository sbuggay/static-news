<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1722243658788" as="style"/><link rel="stylesheet" href="styles.css?v=1722243658788"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.codingconfessions.com/p/simultaneous-multithreading">How simultaneous multithreading works under the hood</a> <span class="domain">(<a href="https://blog.codingconfessions.com">blog.codingconfessions.com</a>)</span></div><div class="subtext"><span>rbanffy</span> | <span>87 comments</span></div><br/><div><div id="41095211" class="c"><input type="checkbox" id="c-41095211" checked=""/><div class="controls bullet"><span class="by">shaggie76</span><span>|</span><a href="#41094781">next</a><span>|</span><label class="collapse" for="c-41095211">[-]</label><label class="expand" for="c-41095211">[24 more]</label></div><br/><div class="children"><div class="content">A grossly over-simplified argument for SMT that resonated with me was that it could keep a precious ALU busy while a thread stalls on a cache miss.<p>I gather in the early days the LPDDR used on laptops was slower too and since cores were scarce so this was more valuable there. Lately, though, we often have more cores than we can scale with and the value is harder to appreciate. We even avoid scheduling work on a shared with an important thread to avoid cache-contention because we know the single-threaded performance will be the bottleneck.<p>A while back I was testing Efficient&#x2F;Performance cores and SMT cores for MT rendering with DirectX 12; on my i7-12700K I found no benefit to either: just using P-cores took about the same time to render a complex scene as P+SMT and P+E+SMT. It&#x27;s not always a wash, though: on the Xbox Series X we found the same test marginally faster when we scheduled work for SMT too.</div><br/><div id="41095252" class="c"><input type="checkbox" id="c-41095252" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#41095211">parent</a><span>|</span><a href="#41095603">next</a><span>|</span><label class="collapse" for="c-41095252">[-]</label><label class="expand" for="c-41095252">[6 more]</label></div><br/><div class="children"><div class="content">Rendering is one of the scenarios which was either same or slower with SMT since the beginning. This is because rendering is already math heavy, and your FPU is always active, esp. dividers (which is always the most expensive operation for processors).<p>SMT shines while waiting for I&#x2F;O or doing some simple integer stuff. If both your threads can saturate the FPU, SMT is generally slower because of the extra tagging added to the data inside the CPU to note what belongs where.</div><br/><div id="41095694" class="c"><input type="checkbox" id="c-41095694" checked=""/><div class="controls bullet"><span class="by">rcxdude</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41095252">parent</a><span>|</span><a href="#41095666">next</a><span>|</span><label class="collapse" for="c-41095694">[-]</label><label class="expand" for="c-41095694">[3 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re waiting for IO, you&#x27;re likely getting booted off the processor by the OS anyway. SMT is most useful when your code doesn&#x27;t have enough instruction-level parallelism but is still mostly compute bound.</div><br/><div id="41096584" class="c"><input type="checkbox" id="c-41096584" checked=""/><div class="controls bullet"><span class="by">corysama</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41095694">parent</a><span>|</span><a href="#41095666">next</a><span>|</span><label class="collapse" for="c-41096584">[-]</label><label class="expand" for="c-41096584">[2 more]</label></div><br/><div class="children"><div class="content">I believe &quot;I&#x2F;O&quot; here is referring to data movement between DRAM and registers. Not drives or NICs.</div><br/><div id="41098933" class="c"><input type="checkbox" id="c-41098933" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41096584">parent</a><span>|</span><a href="#41095666">next</a><span>|</span><label class="collapse" for="c-41098933">[-]</label><label class="expand" for="c-41098933">[1 more]</label></div><br/><div class="children"><div class="content">Yes, exactly. One exception can be Infiniband, since it can put the received data to RAM directly, without CPU intervention.</div><br/></div></div></div></div></div></div><div id="41095666" class="c"><input type="checkbox" id="c-41095666" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41095252">parent</a><span>|</span><a href="#41095694">prev</a><span>|</span><a href="#41095603">next</a><span>|</span><label class="collapse" for="c-41095666">[-]</label><label class="expand" for="c-41095666">[2 more]</label></div><br/><div class="children"><div class="content">But the way you make rendering embarrassingly parallel is the way you make web servers parallel; treat the system as a large number of discrete tasks with deadlines you work toward and avoid letting them interact with each other as much as possible.<p>You don’t worry about how long it takes to render one frame of a digital movie, you worry about how many CPU hours it takes to render five minutes of the movie.</div><br/><div id="41098947" class="c"><input type="checkbox" id="c-41098947" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41095666">parent</a><span>|</span><a href="#41095603">next</a><span>|</span><label class="collapse" for="c-41098947">[-]</label><label class="expand" for="c-41098947">[1 more]</label></div><br/><div class="children"><div class="content">Yes, however in an SMT enabled processor, there are one physical FPU per two logical cores. FPU is already busy with other thread&#x27;s work, so the threads in that SMT enabled core take turns for their computation in the FPU, creating a bottleneck here.<p>As a result, you don&#x27;t get any speed boost at best case, or lose some time in the worst case.<p>Since SMT doesn&#x27;t magically increase the number of FPUs available in a processor, if what you&#x27;re doing is math heavy, SMT just doesn&#x27;t help. Same is true for scientific simulation, too. I observed the same effect, and verified that indeed saturating the FPU with a thread makes SMT moot.</div><br/></div></div></div></div></div></div><div id="41095603" class="c"><input type="checkbox" id="c-41095603" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41095211">parent</a><span>|</span><a href="#41095252">prev</a><span>|</span><a href="#41095782">next</a><span>|</span><label class="collapse" for="c-41095603">[-]</label><label class="expand" for="c-41095603">[7 more]</label></div><br/><div class="children"><div class="content">At this point, especially with backside power, I wonder how much cache stalls on one processor result in less thermal throttling both on that processor and neighboring ones.<p>Maybe we should just be letting these procs take their little naps?</div><br/><div id="41096125" class="c"><input type="checkbox" id="c-41096125" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41095603">parent</a><span>|</span><a href="#41095782">next</a><span>|</span><label class="collapse" for="c-41096125">[-]</label><label class="expand" for="c-41096125">[6 more]</label></div><br/><div class="children"><div class="content">This leads, in the extreme, to the idea of a huge array of very simple cores, which I believe is something that has been tried but never really caught on.</div><br/><div id="41096425" class="c"><input type="checkbox" id="c-41096425" checked=""/><div class="controls bullet"><span class="by">orbat</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41096125">parent</a><span>|</span><a href="#41096144">next</a><span>|</span><label class="collapse" for="c-41096425">[-]</label><label class="expand" for="c-41096425">[2 more]</label></div><br/><div class="children"><div class="content">That description reminds me of GreenArrays&#x27; (<a href="https:&#x2F;&#x2F;www.greenarraychips.com" rel="nofollow">https:&#x2F;&#x2F;www.greenarraychips.com</a>) Forth chips that have 144 cores – although they call them &quot;computers&quot; because they&#x27;re more independent than regular CPU cores, and eg. each has its own memory and so on. Each &quot;computer&quot; is very simple and small – with a 180nm geometry they can cram 8 of them in 1mm^2, and the chip is fairly energy-efficient.<p>Programming for these chips apparently a bit of a nightmare though. Because the &quot;computers&quot; are so simple, even eg. calculating MD5 turns into a fairly tricky proposition as you have to spread out the algorithm to multiple computers with very small amounts of memory, so something that would be very simple on a more classic processor turns into a very low level multithreaded ordeal</div><br/><div id="41098074" class="c"><input type="checkbox" id="c-41098074" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41096425">parent</a><span>|</span><a href="#41096144">next</a><span>|</span><label class="collapse" for="c-41098074">[-]</label><label class="expand" for="c-41098074">[1 more]</label></div><br/><div class="children"><div class="content">Worth noting that the GreenArrays chip is 15 years old. 144 cores was a BIG DEAL back then. I wonder what a similar architecture compiled with a modern process could achieve. 1440 cores? More?</div><br/></div></div></div></div><div id="41096144" class="c"><input type="checkbox" id="c-41096144" checked=""/><div class="controls bullet"><span class="by">makerofthings</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41096125">parent</a><span>|</span><a href="#41096425">prev</a><span>|</span><a href="#41095782">next</a><span>|</span><label class="collapse" for="c-41096144">[-]</label><label class="expand" for="c-41096144">[3 more]</label></div><br/><div class="children"><div class="content">Sounds like gpu to me.</div><br/><div id="41096724" class="c"><input type="checkbox" id="c-41096724" checked=""/><div class="controls bullet"><span class="by">ColonelPhantom</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41096144">parent</a><span>|</span><a href="#41096366">next</a><span>|</span><label class="collapse" for="c-41096724">[-]</label><label class="expand" for="c-41096724">[1 more]</label></div><br/><div class="children"><div class="content">GPUs are actually SMT&#x27;d to the extreme. For example, Intel&#x27;s Xe-HPG has 8-wide SMT. Other vendors have even bigger SMT: RDNA2 can have up to 16 threads in flight per core.</div><br/></div></div><div id="41096366" class="c"><input type="checkbox" id="c-41096366" checked=""/><div class="controls bullet"><span class="by">pavlov</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41096144">parent</a><span>|</span><a href="#41096724">prev</a><span>|</span><a href="#41095782">next</a><span>|</span><label class="collapse" for="c-41096366">[-]</label><label class="expand" for="c-41096366">[1 more]</label></div><br/><div class="children"><div class="content">The Xeon Phi was a “manycore” x86 design with lots of tiny CPU cores, something like the original Pentium, but with the addition of 512-bit SIMD and hyperthreading:<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Xeon_Phi" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Xeon_Phi</a></div><br/></div></div></div></div></div></div></div></div><div id="41095782" class="c"><input type="checkbox" id="c-41095782" checked=""/><div class="controls bullet"><span class="by">gonzo</span><span>|</span><a href="#41095211">parent</a><span>|</span><a href="#41095603">prev</a><span>|</span><a href="#41095553">next</a><span>|</span><label class="collapse" for="c-41095782">[-]</label><label class="expand" for="c-41095782">[2 more]</label></div><br/><div class="children"><div class="content">Intel’s hyperthreading is really a write pipe hack.<p>It’s not so much cache misses as allowing the core to run something else while the write completes.<p>This is why some code scales poorly and other code achieves near linear speed ups.</div><br/><div id="41096810" class="c"><input type="checkbox" id="c-41096810" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41095782">parent</a><span>|</span><a href="#41095553">next</a><span>|</span><label class="collapse" for="c-41096810">[-]</label><label class="expand" for="c-41096810">[1 more]</label></div><br/><div class="children"><div class="content">Why would the core have to wait for the write to complete?<p>A core stalls on a write only if the store buffer is full. As hyper threads share the write buffer, SMT makes store stalls more likely, not less ( but still unlikely to be the bottleneck).</div><br/></div></div></div></div><div id="41095553" class="c"><input type="checkbox" id="c-41095553" checked=""/><div class="controls bullet"><span class="by">gary_0</span><span>|</span><a href="#41095211">parent</a><span>|</span><a href="#41095782">prev</a><span>|</span><a href="#41096751">next</a><span>|</span><label class="collapse" for="c-41095553">[-]</label><label class="expand" for="c-41095553">[5 more]</label></div><br/><div class="children"><div class="content">I wonder if instead of having SMT, processors could briefly power off the unused ALUs&#x2F;FPUs while waiting for something further up the pipeline, and focus on reducing heat and power consumption rather than maximizing utilization.</div><br/><div id="41096532" class="c"><input type="checkbox" id="c-41096532" checked=""/><div class="controls bullet"><span class="by">usrusr</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41095553">parent</a><span>|</span><a href="#41095679">next</a><span>|</span><label class="collapse" for="c-41096532">[-]</label><label class="expand" for="c-41096532">[2 more]</label></div><br/><div class="children"><div class="content">I consider SMT a relic left over from the days when CPU design was all about performance per square millimeter. We are in the process of substituting that goal with that of performance per watt, or in the process of slowly realizing that our goals have shifted quite a while ago.<p>I really don&#x27;t expect SMT to stay much longer. Even more so with timing visibility crosstalk issues lurking and big&#x2F;small architectures offering more parallelism per chip area where single thread performance isn&#x27;t in the spotlight. Or perhaps the marketing challenge of removing a feature that had once been the pride of the company is so big that SMT stays forever.</div><br/><div id="41098725" class="c"><input type="checkbox" id="c-41098725" checked=""/><div class="controls bullet"><span class="by">cherioo</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41096532">parent</a><span>|</span><a href="#41095679">next</a><span>|</span><label class="collapse" for="c-41098725">[-]</label><label class="expand" for="c-41098725">[1 more]</label></div><br/><div class="children"><div class="content">Intel is removing SMT from their next gen mobile processor.<p>My guess is this will help them improve ST perf. We will see how well it works, and if amd will follow</div><br/></div></div></div></div><div id="41095679" class="c"><input type="checkbox" id="c-41095679" checked=""/><div class="controls bullet"><span class="by">rcxdude</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41095553">parent</a><span>|</span><a href="#41096532">prev</a><span>|</span><a href="#41095638">next</a><span>|</span><label class="collapse" for="c-41095679">[-]</label><label class="expand" for="c-41095679">[1 more]</label></div><br/><div class="children"><div class="content">They basically do: it&#x27;s pretty common to clock gate inactive parts of the ALU, which reduces their power consumption greatly. Modern processor power usage is very workload-dependent for this reason.</div><br/></div></div><div id="41095638" class="c"><input type="checkbox" id="c-41095638" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41095211">root</a><span>|</span><a href="#41095553">parent</a><span>|</span><a href="#41095679">prev</a><span>|</span><a href="#41096751">next</a><span>|</span><label class="collapse" for="c-41095638">[-]</label><label class="expand" for="c-41095638">[1 more]</label></div><br/><div class="children"><div class="content">Could you, do they, put the “extra” LUs right next to the parts of the chip with the highest average thermal dissipation to even out the thermal load across the chip?<p>Or stack them vertically, so the least consistently used parts of the chip are farthest away from the heat sink, delaying throttling.</div><br/></div></div></div></div><div id="41096751" class="c"><input type="checkbox" id="c-41096751" checked=""/><div class="controls bullet"><span class="by">bobmcnamara</span><span>|</span><a href="#41095211">parent</a><span>|</span><a href="#41095553">prev</a><span>|</span><a href="#41097125">next</a><span>|</span><label class="collapse" for="c-41096751">[-]</label><label class="expand" for="c-41096751">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I gather in the early days the LPDDR used on laptops was slower too<p>Oddly, the latency hasn&#x27;t improved too much - CAS is often 5-10ns for DDR2&#x2F;3&#x2F;4&#x2F;5. Bus width, transfers per second, queueing, power per bit transfered and stored have, but if a program depends on something that&#x27;s not in cache and was poorly predicted, the RAM latency is the issue.</div><br/></div></div><div id="41097125" class="c"><input type="checkbox" id="c-41097125" checked=""/><div class="controls bullet"><span class="by">kijiki</span><span>|</span><a href="#41095211">parent</a><span>|</span><a href="#41096751">prev</a><span>|</span><a href="#41096121">next</a><span>|</span><label class="collapse" for="c-41097125">[-]</label><label class="expand" for="c-41097125">[1 more]</label></div><br/><div class="children"><div class="content">&gt; on the Xbox Series X we found the same test marginally faster when we scheduled work for SMT too.<p>This makes sense, the Series X has GDDR RAM, and so has substantially worse latency than DDR&#x2F;LPDDR. SMT can help cover that latency, and the higher GDDR bandwidth mitigates the higher memory bandwidth needed to feed both threads.</div><br/></div></div><div id="41096121" class="c"><input type="checkbox" id="c-41096121" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#41095211">parent</a><span>|</span><a href="#41097125">prev</a><span>|</span><a href="#41094781">next</a><span>|</span><label class="collapse" for="c-41096121">[-]</label><label class="expand" for="c-41096121">[1 more]</label></div><br/><div class="children"><div class="content">Anecdotally, mkp224o (.onion vanity address miner, supposedly compute-bound with little memory access) runs about 5-10% faster on my 24-core AMD with 48 threads than with 24 threads. However, I haven&#x27;t tried the same benchmark with SMT disabled in firmware.</div><br/></div></div></div></div><div id="41094781" class="c"><input type="checkbox" id="c-41094781" checked=""/><div class="controls bullet"><span class="by">pavlov</span><span>|</span><a href="#41095211">prev</a><span>|</span><a href="#41097558">next</a><span>|</span><label class="collapse" for="c-41094781">[-]</label><label class="expand" for="c-41094781">[18 more]</label></div><br/><div class="children"><div class="content">Intel’s next generation Arrow Lake CPUs are supposed to remove hyperthreading (i.e. SMT) completely.<p>The performance gains were always heavily application-dependent, so maybe it’s better to simplify.<p>Here’s a recent discussion of when and where it makes sense:
<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39097124">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39097124</a></div><br/><div id="41095110" class="c"><input type="checkbox" id="c-41095110" checked=""/><div class="controls bullet"><span class="by">PaulKeeble</span><span>|</span><a href="#41094781">parent</a><span>|</span><a href="#41096368">next</a><span>|</span><label class="collapse" for="c-41095110">[-]</label><label class="expand" for="c-41095110">[13 more]</label></div><br/><div class="children"><div class="content">Most programs end up with some limitation on the number of threads they can reasonably used. When you have a lot less Cores than that SMT makes a lot of sense to better utilise the resources of the CPU. However once you get to the point where you have enough cores SMT no longer makes any sense. I am not convinced we are necessarily there yet but the P&#x2F;E cores Intel are using are an alternative towards a similar goal and makes a lot of sense on the desktop given how many workloads are single&#x2F;low threaded. I can see the value in not having to deal with SMT and E core distinctions in application optimisation.<p>AMD on the other hand intends to keep mostly homogenous cores for now and continue to use SMT. I doubt its going to be simple to work out which strategy in practice is the best, its going to vary widely by application.</div><br/><div id="41095305" class="c"><input type="checkbox" id="c-41095305" checked=""/><div class="controls bullet"><span class="by">variadix</span><span>|</span><a href="#41094781">root</a><span>|</span><a href="#41095110">parent</a><span>|</span><a href="#41095721">next</a><span>|</span><label class="collapse" for="c-41095305">[-]</label><label class="expand" for="c-41095305">[7 more]</label></div><br/><div class="children"><div class="content">It is my understanding that SMT should be beneficial regardless of core count, as SMT should enable two threads that can stall waiting for memory fetches to fully utilize a single ALU, i.e. SMT improves ALU utilization in memory bound applications with multiple threads by interleaving ALU usage when each thread is waiting on memory. Maybe larger caches are reducing the benefits of SMT, but it should be beneficial as long as there are many threads who are generally bound by memory latency.</div><br/><div id="41097016" class="c"><input type="checkbox" id="c-41097016" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#41094781">root</a><span>|</span><a href="#41095305">parent</a><span>|</span><a href="#41095498">next</a><span>|</span><label class="collapse" for="c-41097016">[-]</label><label class="expand" for="c-41097016">[1 more]</label></div><br/><div class="children"><div class="content">In a CPU with many cores, when some cores stall by waiting for memory loads, other cores can proceed by using data from their caches and this is even more likely to happen than for the SMT threads that share the same cache memory.<p>When there are enough cores, they will keep the common memory interface busy all the time, so adding SMT is unlikely to increase the performance in a memory-throughput limited application when there already are enough cores.<p>Keeping busy all the ALUs in a compute-limited application can usually be done well enough by out-of-order execution, because the modern CPUs have very big execution windows from which to choose instructions to be executed.<p>So when there already are many cores, in many cases SMT may provide negligible advantages. On server computers there are much more opportunities for SMT to improve their efficiency, but on non-server computers I have encountered only one widespread application for which SMT is clearly beneficial, which is the compilation of big software projects (i.e. with thousands of source files).<p>The big cores of Intel are optimized for single-thread performance. This optimization criterion results in bad multi-threaded performance. The reason is that the MT performance is limited by the maximum permissible chip area and by the maximum possible power consumption. A big core has very poor performance per area and performance per power ratios.<p>Adding SMT to such a big core improves the multi-threaded performance, but it is not the best way of improving it, because in the same area and power consumption  used by a big core one can implement 3 to 5 efficient cores, so such a replacement of a big core with multiple efficient cores will increase the multi-threaded performance much more than adding SMT. So unlike for the case of a CPU that uses only big cores, in hybrid CPUs, SMT does not make sense, because a better MT performance is obtained by keeping only a few big cores, to provide high single-thread performance, and by replacing the other big cores with smaller, more efficient cores.</div><br/></div></div><div id="41095498" class="c"><input type="checkbox" id="c-41095498" checked=""/><div class="controls bullet"><span class="by">t-3</span><span>|</span><a href="#41094781">root</a><span>|</span><a href="#41095305">parent</a><span>|</span><a href="#41097016">prev</a><span>|</span><a href="#41098081">next</a><span>|</span><label class="collapse" for="c-41095498">[-]</label><label class="expand" for="c-41095498">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Maybe larger caches are reducing the benefits of SMT, but it should be beneficial as long as there are many threads who are generally bound by memory latency.<p>I thought the reason SMT sometimes resulted in lower performance was that it halved the available cache per thread though - shouldn&#x27;t larger caches make SMT <i>more</i> effective?</div><br/><div id="41096075" class="c"><input type="checkbox" id="c-41096075" checked=""/><div class="controls bullet"><span class="by">jmb99</span><span>|</span><a href="#41094781">root</a><span>|</span><a href="#41095498">parent</a><span>|</span><a href="#41098081">next</a><span>|</span><label class="collapse" for="c-41096075">[-]</label><label class="expand" for="c-41096075">[3 more]</label></div><br/><div class="children"><div class="content">My understanding is that a larger cache can make SMT more effective, but like usual, only in certain cases.<p>Let’s imagine we have 8 cores with SMT, and we’re running a task that (in theory) scales roughly linearly up to 16 threads. If each thread’s working memory is around half as much as there is cache available to each thread, but each working set is only used briefly, then SMT is going to be hugely beneficial: while one hyperthread is committing and fetching memory, the other one’s cache is already full with a new working set and can begin computing. Increasing cache will increase the allowable working set size without causing cache contention between hyperthreads.<p>Alternatively, if the working set is sufficiently large per thread (probably &gt;2&#x2F;3 the amount of cache available), SMT becomes substantially less useful. When the first hyperthread finishes its work, the second hyperthread has to still wait for some (or all) of its working set to be fetched from main memory (or higher cache levels if lucky). This may take just as long as simply keeping hyperthread #1 fed with new working sets. Increasing cache in this scenario will increase SMT performance almost linearly, until each hyperthread’s working set can be prefetched into the lowest cache levels while the other hyperthread is busy working.<p>Also consider the situation where the working set is much, much smaller than the available cache, but lots of computing must be done to it. In this case, a single hyperthread can continually be fed with new data, since the old set can be purged to main memory and the next set can be loaded into cache long before the current set is processed. SMT provides no benefit here no matter how large you grow the cache (unless the tasks use wildly different components of the core and they can be run at instruction-level parallelism - but that’s tricky to get right and you may run into thermal or power throttling before you can actually get enough performance to make it worthwhile).<p>Of course the real world is way more complicated than that. Many tasks do not scale linearly with more threads. Sometimes running on 6 “real” cores vs 12 SMT threads can result in no performance gain, but running on 8 “real” cores is 1&#x2F;3 faster. And sometimes SMT will give you a non-linear speedup but a few more (non-SMT) cores will give you a better (but still non-linear) speedup. So short answer: yes, sometimes more cache makes SMT more viable, if your tasks can be 2x parallelized, have working sets around the size of the cache, and work on the same set for a notable chunk of the time required to store the old set and fetch the next one.<p>And of course all of this requires the processor and&#x2F;or compiler to be smart enough to ensure the cache is properly fed new data from main memory. This is frequently the case these days, but not always.</div><br/><div id="41096890" class="c"><input type="checkbox" id="c-41096890" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#41094781">root</a><span>|</span><a href="#41096075">parent</a><span>|</span><a href="#41098081">next</a><span>|</span><label class="collapse" for="c-41096890">[-]</label><label class="expand" for="c-41096890">[2 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s say your workload consists solely in traversing a single linked list. This list fits perfectly in L1.<p>As an L1 load takes 4 cycles and you can&#x27;t start the next load untill you completed the previous one, the CPU will stall doing nothing 3&#x2F;4th of cycles. A 4-way SMT could in principle make use of all the wasted cycles.<p>Of course no load is even close to purely traversing a linked list, but a lot of non-hpc real world load do spend a lot of time in latency limited sections that can benefit from SMT, so it is not just cache misses.</div><br/><div id="41097989" class="c"><input type="checkbox" id="c-41097989" checked=""/><div class="controls bullet"><span class="by">jmb99</span><span>|</span><a href="#41094781">root</a><span>|</span><a href="#41096890">parent</a><span>|</span><a href="#41098081">next</a><span>|</span><label class="collapse" for="c-41097989">[-]</label><label class="expand" for="c-41097989">[1 more]</label></div><br/><div class="children"><div class="content">&gt; so it is not just cache misses.<p>Agreed 100%. SMT is waaaay more complex than just cache. I was just trying to illustrate in simple scenarios where increasing cache would and would not be beneficial to SMT.</div><br/></div></div></div></div></div></div></div></div><div id="41098081" class="c"><input type="checkbox" id="c-41098081" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#41094781">root</a><span>|</span><a href="#41095305">parent</a><span>|</span><a href="#41095498">prev</a><span>|</span><a href="#41095721">next</a><span>|</span><label class="collapse" for="c-41098081">[-]</label><label class="expand" for="c-41098081">[1 more]</label></div><br/><div class="children"><div class="content">Depends greatly on the work load.</div><br/></div></div></div></div><div id="41095721" class="c"><input type="checkbox" id="c-41095721" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41094781">root</a><span>|</span><a href="#41095110">parent</a><span>|</span><a href="#41095305">prev</a><span>|</span><a href="#41096368">next</a><span>|</span><label class="collapse" for="c-41095721">[-]</label><label class="expand" for="c-41095721">[5 more]</label></div><br/><div class="children"><div class="content">I’d like to see the math for why it doesn’t work out to have a model where n real cores share a set of logic units for rare instructions and a few common ones where say the average number of instructions per clock is 2.66 so four cores each have 2 apiece and then share 3 between them.<p>When this whole idea first came up that’s how I thought it was going to work, but we’ve had two virtual processors sharing all of their logic units in common instead.</div><br/><div id="41096749" class="c"><input type="checkbox" id="c-41096749" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#41094781">root</a><span>|</span><a href="#41095721">parent</a><span>|</span><a href="#41096923">next</a><span>|</span><label class="collapse" for="c-41096749">[-]</label><label class="expand" for="c-41096749">[1 more]</label></div><br/><div class="children"><div class="content">It is difficult to share an execution unit between many cores, because in that case it must be placed at a great distance from at least the majority of those cores.<p>The communication between distant places on a chip requires additional area and power consumption and time. It may make the shared execution unit significantly slower than a non-shared unit and it may decrease the PPA (performance per power and area) of the CPU.<p>Such a sharing works only for a completely self-contained execution unit, which includes its own registers and which can perform a complex sequence of operations independently of the core that has requested it. In such cases the communication between a core and the shared unit is reduced to sending the initial operands and receiving the final results, while between these messages the shared unit operates for a long time without external communication. An example of such a shared execution unit is the SME accelerator of the Apple CPUs, which executes matrix operations.</div><br/></div></div><div id="41096923" class="c"><input type="checkbox" id="c-41096923" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#41094781">root</a><span>|</span><a href="#41095721">parent</a><span>|</span><a href="#41096749">prev</a><span>|</span><a href="#41096368">next</a><span>|</span><label class="collapse" for="c-41096923">[-]</label><label class="expand" for="c-41096923">[3 more]</label></div><br/><div class="children"><div class="content">Aside for large vector ALUs, execution units are cheap in transistor count. Caches, TLBs, memory for the various predictors, register files, reorder buffers, cost probably a significantly larger amount of transistors.<p>In any case, execution units are clusters around ports, so, AFAIK, you wouldn&#x27;t really be able to share at the instruction level, bu only groups of related instructions.<p>Still, some sharing is possible, AMD tried to share FPUs in Bulldozer, but it didn&#x27;t work well. Some other CPUs share cryptographic accelerators. IIRC Apple shares the matrix unit across cores.</div><br/><div id="41097887" class="c"><input type="checkbox" id="c-41097887" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41094781">root</a><span>|</span><a href="#41096923">parent</a><span>|</span><a href="#41096368">next</a><span>|</span><label class="collapse" for="c-41097887">[-]</label><label class="expand" for="c-41097887">[2 more]</label></div><br/><div class="children"><div class="content">Are predictors and TLBs shared between SMTs?</div><br/><div id="41098275" class="c"><input type="checkbox" id="c-41098275" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#41094781">root</a><span>|</span><a href="#41097887">parent</a><span>|</span><a href="#41096368">next</a><span>|</span><label class="collapse" for="c-41098275">[-]</label><label class="expand" for="c-41098275">[1 more]</label></div><br/><div class="children"><div class="content">As far as I know yes, most structures are dynamically shared between hypertreads.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41096368" class="c"><input type="checkbox" id="c-41096368" checked=""/><div class="controls bullet"><span class="by">YesBox</span><span>|</span><a href="#41094781">parent</a><span>|</span><a href="#41095110">prev</a><span>|</span><a href="#41095689">next</a><span>|</span><label class="collapse" for="c-41096368">[-]</label><label class="expand" for="c-41096368">[1 more]</label></div><br/><div class="children"><div class="content">Im creating a game + engine and speaking from personal experience&#x2F;my use case, hyperthreading was less performant than (praying to the CPU thread allocation god) each thread utilizing its own core. I decided to max out the number of threads by using std::thread::hardware_concurrency() &#x2F; 2 - 1. (i.e. number of cores - 1 ).<p>I&#x27;m working with a std::vector</div><br/></div></div><div id="41095689" class="c"><input type="checkbox" id="c-41095689" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41094781">parent</a><span>|</span><a href="#41096368">prev</a><span>|</span><a href="#41096837">next</a><span>|</span><label class="collapse" for="c-41095689">[-]</label><label class="expand" for="c-41095689">[2 more]</label></div><br/><div class="children"><div class="content">On common industry benchmarks at least every second generation of Intel hyperthreading ended up being slower than turning it off. Even when it worked it was barely double digit percent improvements, and there were periods when it was worse for consecutive generation. Why do they keep trying?</div><br/><div id="41095747" class="c"><input type="checkbox" id="c-41095747" checked=""/><div class="controls bullet"><span class="by">tedunangst</span><span>|</span><a href="#41094781">root</a><span>|</span><a href="#41095689">parent</a><span>|</span><a href="#41096837">next</a><span>|</span><label class="collapse" for="c-41095747">[-]</label><label class="expand" for="c-41095747">[1 more]</label></div><br/><div class="children"><div class="content">Because the benchmarks don&#x27;t measure what people do with computers.</div><br/></div></div></div></div><div id="41096837" class="c"><input type="checkbox" id="c-41096837" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#41094781">parent</a><span>|</span><a href="#41095689">prev</a><span>|</span><a href="#41097558">next</a><span>|</span><label class="collapse" for="c-41096837">[-]</label><label class="expand" for="c-41096837">[1 more]</label></div><br/><div class="children"><div class="content">Even on server parts?</div><br/></div></div></div></div><div id="41097558" class="c"><input type="checkbox" id="c-41097558" checked=""/><div class="controls bullet"><span class="by">leloctai</span><span>|</span><a href="#41094781">prev</a><span>|</span><a href="#41096493">next</a><span>|</span><label class="collapse" for="c-41097558">[-]</label><label class="expand" for="c-41097558">[3 more]</label></div><br/><div class="children"><div class="content">Does anyone know how to search for this type of detailed technical articles?<p>I&#x27;ve searched for this exact topic. As expected with kind of end user facing techs, the search results are always end user oriented article that explain nothing.</div><br/><div id="41098596" class="c"><input type="checkbox" id="c-41098596" checked=""/><div class="controls bullet"><span class="by">stareatgoats</span><span>|</span><a href="#41097558">parent</a><span>|</span><a href="#41097962">next</a><span>|</span><label class="collapse" for="c-41098596">[-]</label><label class="expand" for="c-41098596">[1 more]</label></div><br/><div class="children"><div class="content">I find that LLMs with web access are a good fit for this kind of search, at least to point me in the right direction. The URLs provided are mostly hallucinations, however.</div><br/></div></div><div id="41097962" class="c"><input type="checkbox" id="c-41097962" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#41097558">parent</a><span>|</span><a href="#41098596">prev</a><span>|</span><a href="#41096493">next</a><span>|</span><label class="collapse" for="c-41097962">[-]</label><label class="expand" for="c-41097962">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;hn.algolia.com" rel="nofollow">https:&#x2F;&#x2F;hn.algolia.com</a>, assuming that most articles of that sort end up getting submitted to or mentioned on HN.</div><br/></div></div></div></div><div id="41096493" class="c"><input type="checkbox" id="c-41096493" checked=""/><div class="controls bullet"><span class="by">sweetjuly</span><span>|</span><a href="#41097558">prev</a><span>|</span><a href="#41094694">next</a><span>|</span><label class="collapse" for="c-41096493">[-]</label><label class="expand" for="c-41096493">[2 more]</label></div><br/><div class="children"><div class="content">&gt; As we have seen, enabling SMT on a CPU core requires sharing many of the buffers and execution resources between the two logical processors. Even if there is only one thread running on an SMT enabled core, these resources remain unavailable to that thread which reduces its potential performance.<p>This isn&#x27;t true (anymore?). We&#x27;ve seen a variety of SMT cores which partition the ROB, fetch&#x2F;decode bandwidth, etc. when running in SMT mode but allow full use when not in SMT mode.</div><br/><div id="41097956" class="c"><input type="checkbox" id="c-41097956" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#41096493">parent</a><span>|</span><a href="#41094694">next</a><span>|</span><label class="collapse" for="c-41097956">[-]</label><label class="expand" for="c-41097956">[1 more]</label></div><br/><div class="children"><div class="content">This is exactly how the x200 series Phi processors work, you get far more resources per thread in non-smt mode than the 4-way smt mode.</div><br/></div></div></div></div><div id="41094694" class="c"><input type="checkbox" id="c-41094694" checked=""/><div class="controls bullet"><span class="by">mhh__</span><span>|</span><a href="#41096493">prev</a><span>|</span><a href="#41095658">next</a><span>|</span><label class="collapse" for="c-41094694">[-]</label><label class="expand" for="c-41094694">[1 more]</label></div><br/><div class="children"><div class="content">Good summary overall, although seemed a little muddled in places.<p>Would love to know some of the tricks of the trade from insiders (not relating to security at least)</div><br/></div></div><div id="41095658" class="c"><input type="checkbox" id="c-41095658" checked=""/><div class="controls bullet"><span class="by">mgaunard</span><span>|</span><a href="#41094694">prev</a><span>|</span><a href="#41095838">next</a><span>|</span><label class="collapse" for="c-41095658">[-]</label><label class="expand" for="c-41095658">[1 more]</label></div><br/><div class="children"><div class="content">The whole point of SMT is to maximize utilization of a superscalar execution engine.<p>I wonder if that trend means people think superscalar is less important than it used to be.</div><br/></div></div><div id="41095838" class="c"><input type="checkbox" id="c-41095838" checked=""/><div class="controls bullet"><span class="by">superjan</span><span>|</span><a href="#41095658">prev</a><span>|</span><a href="#41096020">next</a><span>|</span><label class="collapse" for="c-41095838">[-]</label><label class="expand" for="c-41095838">[5 more]</label></div><br/><div class="children"><div class="content">What I think is worth knowing is that compute units in GPU’s also use SMT, usually at a level of 7 to 10 threads per CU. This helps to hide latency.</div><br/><div id="41096862" class="c"><input type="checkbox" id="c-41096862" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#41095838">parent</a><span>|</span><a href="#41096020">next</a><span>|</span><label class="collapse" for="c-41096862">[-]</label><label class="expand" for="c-41096862">[4 more]</label></div><br/><div class="children"><div class="content">Most GPUs do not use SMT, but the predecessor of SMT, fine-grained multi-threading. In every clock cycle the instruction that is initiated is selected from the many threads that are available, depending on which of them need resources that are not busy. Most GPUs do not initiate multiple instructions per clock cycle (even if multiple instructions may proceed concurrently after initiation) or if they initiate multiple instructions per clock cycle those may have to belong to distinct classes of instructions, which use distinct execution resources, for example scalar instructions and vector instructions.<p>SMT, i.e. <i>simultaneous</i> multi-threading, means that in every clock cycle many instructions are simultaneously initiated from all threads and then those instructions compete for the multiple execution units of a superscalar CPU, in order to keep busy as many of those execution units as possible. For each of the concurrent execution units, e.g. for each of the 6 integer adders available in the latest CPUs, a decision is made separately from the others about which instruction to be executed from the queues that hold instructions belonging to all simultaneous threads.</div><br/><div id="41097042" class="c"><input type="checkbox" id="c-41097042" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#41095838">root</a><span>|</span><a href="#41096862">parent</a><span>|</span><a href="#41096020">next</a><span>|</span><label class="collapse" for="c-41097042">[-]</label><label class="expand" for="c-41097042">[3 more]</label></div><br/><div class="children"><div class="content">As far as I know hypertreads on intel share fetchers and decoders, so each clock cycle only one thread is feeding in instructions to the pipeline. That&#x27;s no different to even for a simple barrel processor.<p>It is true that once fetched an OoO CPU does a significant amount of scheduling and it is possible that in a given clock cycle instructions from both threads are getting fed to an execution unit. But I don&#x27;t think that&#x27;s the essence of SMT.<p>For example the original larrabee is described as 4-way SMT, but as P5-derived it was a simple in-order design with very limited superscalar capabilities. I very much doubt that at any time instructions from more than one thread were at the execution stage.</div><br/><div id="41097091" class="c"><input type="checkbox" id="c-41097091" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#41095838">root</a><span>|</span><a href="#41097042">parent</a><span>|</span><a href="#41096020">next</a><span>|</span><label class="collapse" for="c-41097091">[-]</label><label class="expand" for="c-41097091">[2 more]</label></div><br/><div class="children"><div class="content">The fetchers and decoders are shared, but they fetch and decode many instructions per clock cycle (up to 8 or 9 instructions per clock cycle in the latest Intel cores, i.e. Lion Cove and Skymont).<p>While the shared Intel decoders alternate between the threads and the queue that stores micro-operations before they are dispatched is also partitioned between threads, this front-end is decoupled from the schedulers that select micro-operations for execution, which may choose in any clock cycle as many uops as there are execution units and in any combination between the SMT threads.<p>Even in the first Intel CPU with SMT, Pentium 4, up to 3 instructions were fetched and decoded in each clock cycle and there were places where up to 7 instructions in any combination between the 2 SMT threads were executed during the same clock cycle.<p>In modern CPUs the concurrency is much greater.</div><br/><div id="41098272" class="c"><input type="checkbox" id="c-41098272" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#41095838">root</a><span>|</span><a href="#41097091">parent</a><span>|</span><a href="#41096020">next</a><span>|</span><label class="collapse" for="c-41098272">[-]</label><label class="expand" for="c-41098272">[1 more]</label></div><br/><div class="children"><div class="content">They fetch and decode many instructions, but only from a single cache line fetched by the fetcher hence they can&#x27;t decide instructions for more than one hyperthread at a time.<p>Except the newer *mont cores that have truly separate decoders and fetchers  and could indeed decode for two hypertreads separately.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41096020" class="c"><input type="checkbox" id="c-41096020" checked=""/><div class="controls bullet"><span class="by">written-beyond</span><span>|</span><a href="#41095838">prev</a><span>|</span><a href="#41095656">next</a><span>|</span><label class="collapse" for="c-41096020">[-]</label><label class="expand" for="c-41096020">[1 more]</label></div><br/><div class="children"><div class="content">Poor AMD their bulldozer architecture got so much flak for not including SMT and now everyone&#x27;s moving away from it.<p>Yes yes I know bulldozer had a bunch more issues than just no SMT. It actually had the exact opposite with multiple cores sharing the same ALU or something like that. But still they could have been onto something if they had made it marginally more performant.</div><br/></div></div><div id="41095656" class="c"><input type="checkbox" id="c-41095656" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#41096020">prev</a><span>|</span><a href="#41094938">next</a><span>|</span><label class="collapse" for="c-41095656">[-]</label><label class="expand" for="c-41095656">[3 more]</label></div><br/><div class="children"><div class="content">One of the biggest mistakes users have is a mental model of SMT that imagines the existence of one &quot;real core&quot; and one inferior one. The threads are coequal in all observable respects.</div><br/><div id="41097839" class="c"><input type="checkbox" id="c-41097839" checked=""/><div class="controls bullet"><span class="by">MBCook</span><span>|</span><a href="#41095656">parent</a><span>|</span><a href="#41097952">next</a><span>|</span><label class="collapse" for="c-41097839">[-]</label><label class="expand" for="c-41097839">[1 more]</label></div><br/><div class="children"><div class="content">I suspect that’s a result of the performance. While both threads are capable of the same tasks, you don’t get the 2x performance you would with a “real” second thread, really a second core.<p>So conceptually it’s a little more like having 1.25 single threaded cores, or whatever the ratio is for your application, if you only care about performance in the end.</div><br/></div></div><div id="41097952" class="c"><input type="checkbox" id="c-41097952" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#41095656">parent</a><span>|</span><a href="#41097839">prev</a><span>|</span><a href="#41094938">next</a><span>|</span><label class="collapse" for="c-41097952">[-]</label><label class="expand" for="c-41097952">[1 more]</label></div><br/><div class="children"><div class="content">I mean, Intel&#x27;s new CPUs certainly have both real cores (&quot;P-cores&quot;) and inferior cores (&quot;E-cores&quot;). I suspect the reason they introduced E-cores is primarily thermal- and die-space-related, not actually power usage or performance. I always make sure to buy the chips without E-cores because they are better.</div><br/></div></div></div></div><div id="41094938" class="c"><input type="checkbox" id="c-41094938" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41095656">prev</a><span>|</span><a href="#41095015">next</a><span>|</span><label class="collapse" for="c-41094938">[-]</label><label class="expand" for="c-41094938">[1 more]</label></div><br/><div class="children"><div class="content">It seems a bit high-level, kind of skimming over a bunch of architecture concepts with a couple references to the fact that this might be duplicated when hyperthreading, this might not…<p>IMO a blog post should be more actionable. This isn’t a textbook chapter. For example we go through the frontend. When discussing the trace cache we have:<p>&gt; … Instruction decoding is an expensive operation and some instructions need to be executed frequently. Having this cache helps the processor cut down the instruction execution latency.<p>…<p>&gt; Trace cache is shared dynamically between the two logical processors on an as needed basis.<p>…<p>&gt; Each entry in the cache is tagged with the thread information to distinguish the instructions of the two threads. The access to the trace cache is arbitrated between the two logical processors each cycle.<p>So the threads share a trace cache, but keep track of which hyperthread used which instructions—but we don’t really know, practically, if we prefer threads that are running very similar computations or if that is a non-issue (that is, does the fact that they share the trace cache mean one thread can benefit from things the other has cached? Or does the tagging keep them separated?).<p>In general, often they say “this is split equally between the two threads” or “this is shared,” which makes me wonder “if I disable SMT does the now single-thread get access to twice as much of this resource, and are there cases where that matters.”<p>This is somewhat covered in:<p>&gt; As we have seen, enabling SMT on a CPU core requires sharing many of the buffers and execution resources between the two logical processors. Even if there is only one thread running on an SMT enabled core, these resources remain unavailable to that thread which reduces its potential performance.<p>But this seems a bit fuzzy to me, I mean, we talk about caches which are shared dynamically between the two threads so at least <i>some</i> resources will be more readily available if only a single thread is running.<p>It also could be interesting—if the author is an expert, perhaps they could share their experience as to which pipeline stages are often bottlenecks that get tighter with hyperthreads on, and which aren’t? We have a sort of even focus on each stage without many hints as to which practically matter. Or how we can help them out. Also it is largely based on a 2002 whitepaper so I guess the specific pipeline stages must have evolved a bit since then.<p>Or maybe they could share some battle stories, favorite tools, some examples of applications and why they put pressure on particular stages, things which surprisingly didn’t scale when hyperthreads were enabled (I’m not asking for all these things, just any would be good).</div><br/></div></div><div id="41094717" class="c"><input type="checkbox" id="c-41094717" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#41096367">prev</a><span>|</span><label class="collapse" for="c-41094717">[-]</label><label class="expand" for="c-41094717">[24 more]</label></div><br/><div class="children"><div class="content">Tangent: Ever since I became familiar with Erlang and the impressive BEAM, any other async method seems subpar and contrived, and that includes Python, Go, Rust, etc.<p>It&#x27;s just weird how there&#x27;s a correct way to do async and parallelism (which erlang does) and literally no other language does it.</div><br/><div id="41094961" class="c"><input type="checkbox" id="c-41094961" checked=""/><div class="controls bullet"><span class="by">lastofus</span><span>|</span><a href="#41094717">parent</a><span>|</span><a href="#41095177">next</a><span>|</span><label class="collapse" for="c-41094961">[-]</label><label class="expand" for="c-41094961">[2 more]</label></div><br/><div class="children"><div class="content">Other languages do sometimes implement this at the library level. Clojure&#x27;s core.async comes to mind (though there are subtle differences). There&#x27;s downsides to this approach though.<p>The data going into each &quot;mailbox&quot; either needs to be immutable, or deep copied to be thread safe. This obviously comes at a cost. Sometimes you just have a huge amount of state that different threads need to work on, and the above solution isn&#x27;t viable. Erlang has ets&#x2F;dets to help deal with this. You will notice ets&#x2F;dets looks nothing like the mailbox&#x2F;process pattern.<p>Erlang is great, but it is hardly the &quot;one true way&quot;. As with most things, tradeoffs are a thing, and usually the right solution comes down to &quot;it depends&quot;.</div><br/><div id="41095189" class="c"><input type="checkbox" id="c-41095189" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41094961">parent</a><span>|</span><a href="#41095177">next</a><span>|</span><label class="collapse" for="c-41095189">[-]</label><label class="expand" for="c-41095189">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The data going into each &quot;mailbox&quot; either needs to be immutable, or deep copied to be thread safe<p>Or moved. The mailbox&#x2F;process pattern works great in Rust because you can simply move ownership of a value. Kind of like if in C you send the pointer and then delete your copy.<p>Of course doing this across threads doesn&#x27;t work with every type of value (what Rust&#x27;s type system encodes as a value being `Send`). For example you can&#x27;t send a reference-counted value if the reference counter isn&#x27;t thread-safe. But that&#x27;s rarely an issue and easily solved with a good type system.</div><br/></div></div></div></div><div id="41095177" class="c"><input type="checkbox" id="c-41095177" checked=""/><div class="controls bullet"><span class="by">mrkeen</span><span>|</span><a href="#41094717">parent</a><span>|</span><a href="#41094961">prev</a><span>|</span><a href="#41094760">next</a><span>|</span><label class="collapse" for="c-41095177">[-]</label><label class="expand" for="c-41095177">[8 more]</label></div><br/><div class="children"><div class="content">The actor model (share-nothing) is one way to address the problem of shared, mutable state.<p>But what if I want to have my cake and eat it too?  What if I want to have thread-safe, shared, mutable state.  Is it not conceivable that there&#x27;s a better approach than share-nothing?</div><br/><div id="41095545" class="c"><input type="checkbox" id="c-41095545" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41095177">parent</a><span>|</span><a href="#41094760">next</a><span>|</span><label class="collapse" for="c-41095545">[-]</label><label class="expand" for="c-41095545">[7 more]</label></div><br/><div class="children"><div class="content">As much as I hate to say it, I think Java probably has the best eaten cake implementation by far. Volatile makes sure variables stay sane on the memory side, if you only write to a variable from one thread and read it from others, then it just sort of magically works? Plus the executors to handle thread reuse for async tasks. I assume C# has the same concepts given that it&#x27;s just a carbon copy with title case naming.<p>Python can&#x27;t execute in on two cores at once, so it functionally has no multithreading, JS can share data between threads, but must convert it all to string because pointless performance penalties are great to have. Golang has that weird FIFO channel thing (probably sockets in disguise for these last two). C&#x2F;C++ has a segfault.</div><br/><div id="41096024" class="c"><input type="checkbox" id="c-41096024" checked=""/><div class="controls bullet"><span class="by">throwitaway1123</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41095545">parent</a><span>|</span><a href="#41095807">next</a><span>|</span><label class="collapse" for="c-41096024">[-]</label><label class="expand" for="c-41096024">[3 more]</label></div><br/><div class="children"><div class="content">&gt; JS can share data between threads, but must convert it all to string<p>To be more precise, you can send data to web workers and worker threads by copying via the structured clone algorithm (unlike JSON this supports almost all data types), and you can also move certain transferable objects between threads which is a zero-copy (and therefore much faster) operation.</div><br/><div id="41096124" class="c"><input type="checkbox" id="c-41096124" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41096024">parent</a><span>|</span><a href="#41095807">next</a><span>|</span><label class="collapse" for="c-41096124">[-]</label><label class="expand" for="c-41096124">[2 more]</label></div><br/><div class="children"><div class="content">Ah yeah dataviews, but you still need to convert from json to those and that takes about as much overhead, plus they&#x27;re much harder to deal with complexity-wise being annoying single type buffers and all. For any other language it would work better, but because JS mainly deals with data arriving from elsewhere it means it needs to be converted every single time instead of just maintaining a local copy for thread comms.</div><br/><div id="41096450" class="c"><input type="checkbox" id="c-41096450" checked=""/><div class="controls bullet"><span class="by">throwitaway1123</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41096124">parent</a><span>|</span><a href="#41095807">next</a><span>|</span><label class="collapse" for="c-41096450">[-]</label><label class="expand" for="c-41096450">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Ah yeah dataviews, but you still need to convert from json to those and that takes about as much overhead<p>You don&#x27;t necessarily need to have an intermediate JSON representation. Many of the built in APIs in Node and browsers return array buffers natively. For example:<p><pre><code>  const buffer = await fetch(&#x27;foo.wav&#x27;).then(res =&gt; res.arrayBuffer())
  new Worker(&#x27;worker.js&#x27;).postMessage(buffer, [buffer])
</code></pre>
This completely transfers the buffer to the worker thread, after which it is detached (unusable from the sending side) [1][2].<p>[1] <a href="https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;API&#x2F;Worker&#x2F;postMessage#transfer" rel="nofollow">https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;API&#x2F;Worker&#x2F;post...</a><p>[2] <a href="https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;JavaScript&#x2F;Reference&#x2F;Global_Objects&#x2F;ArrayBuffer&#x2F;detached" rel="nofollow">https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;JavaScript&#x2F;Refe...</a></div><br/></div></div></div></div></div></div><div id="41095807" class="c"><input type="checkbox" id="c-41095807" checked=""/><div class="controls bullet"><span class="by">mrkeen</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41095545">parent</a><span>|</span><a href="#41096024">prev</a><span>|</span><a href="#41096451">next</a><span>|</span><label class="collapse" for="c-41095807">[-]</label><label class="expand" for="c-41095807">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Volatile makes sure variables stay sane on the memory side<p>This doesn&#x27;t get you from shared-mutable-hell to shared-mutable-safe, it gets you from shared-mutable-relaxedmemorymodel-hell to shared-mutable-hell.  It&#x27;s the kind of hell you don&#x27;t come across until you start being too smart for synchronisation primitives and start taking a stab at lockfree&#x2F;lockless wizardry.<p>&gt; if you only write to a variable from one thread and read it from others, then it just sort of magically works<p>I&#x27;m not necessarily convinced by that - but either way that&#x27;s a huge blow to &#x27;shared&#x27; if you are only allowed one writer.<p>&gt; Plus the executors to handle thread reuse for async tasks.<p>What does this solve with regard to the shared-mutable problem?  This is like &quot;Erlang has BEAM to handle the actors&quot; or something - so what?</div><br/><div id="41096207" class="c"><input type="checkbox" id="c-41096207" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41095807">parent</a><span>|</span><a href="#41096451">next</a><span>|</span><label class="collapse" for="c-41096207">[-]</label><label class="expand" for="c-41096207">[1 more]</label></div><br/><div class="children"><div class="content">Well it doesn&#x27;t get you there because shared-mutable-safe doesn&#x27;t exist, at least I doubt it can without major tradeoffs. You either err on the side of complete safety with a system that is borderline unusable for anything practical, or you let people do whatever they want and let them deal with their problems once they actually have them.<p>&gt; either way that&#x27;s a huge blow to &#x27;shared&#x27; if you are only allowed one writer<p>Yeah for full N thread reading and editing you&#x27;d need N vars per var which is annoying, but that kind of every-thread-is-main setup is something that is exceedingly rare. There&#x27;s almost always a few fixed main ones and lots running specific tasks that don&#x27;t really need to know about all the other ones.</div><br/></div></div></div></div><div id="41096451" class="c"><input type="checkbox" id="c-41096451" checked=""/><div class="controls bullet"><span class="by">neonsunset</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41095545">parent</a><span>|</span><a href="#41095807">prev</a><span>|</span><a href="#41094760">next</a><span>|</span><label class="collapse" for="c-41096451">[-]</label><label class="expand" for="c-41096451">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I assume C# has the same concepts given that it&#x27;s just a carbon copy with title case naming.<p>Better not comment than look clueless. Moreover, this applies to the use of volatile keyword in Java as well.</div><br/></div></div></div></div></div></div><div id="41094760" class="c"><input type="checkbox" id="c-41094760" checked=""/><div class="controls bullet"><span class="by">lawn</span><span>|</span><a href="#41094717">parent</a><span>|</span><a href="#41095177">prev</a><span>|</span><a href="#41095162">next</a><span>|</span><label class="collapse" for="c-41094760">[-]</label><label class="expand" for="c-41094760">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a huge fan of the BEAM, but I wonder if you&#x27;re not overselling it a little? Surely there are trade-offs here that sometimes aren&#x27;t worth it and alternative ways are better?<p>For example, the BEAM isn&#x27;t optimized for throughput and if that&#x27;s a high priority for you then you might want to choose something else (maybe Rust).</div><br/><div id="41094770" class="c"><input type="checkbox" id="c-41094770" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41094760">parent</a><span>|</span><a href="#41095162">next</a><span>|</span><label class="collapse" for="c-41094770">[-]</label><label class="expand" for="c-41094770">[6 more]</label></div><br/><div class="children"><div class="content">&gt; For example, the BEAM isn&#x27;t optimized for throughput...<p>weird take, given that Erlang powers telecom systems with literally millions of connections at a time.</div><br/><div id="41096330" class="c"><input type="checkbox" id="c-41096330" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41094770">parent</a><span>|</span><a href="#41094839">next</a><span>|</span><label class="collapse" for="c-41096330">[-]</label><label class="expand" for="c-41096330">[1 more]</label></div><br/><div class="children"><div class="content">The original use that Erlang was built for was controlling telecom switches. In that application, Erlang supervises phone lines, responding to on&#x2F;off hook and dialed numbers etc, but does not touch the voice path at all --- it only controls the relays&#x2F;etc to connect the voice path as desired. That&#x27;s not a high throughput job at all.<p>I used Erlang at WhatsApp. Having a million chat connections was impressive, but that&#x27;s not high throughput either. Most of those connections were idle. As we added more things to the chat connection, we ended up with a significantly lower target per machine (and then at Facebook, the servers got a lot smaller and the target connections per machine was way less).<p>We did have some Erlang machines that pushed 20gbps for https downloads, but I don&#x27;t think that&#x27;s that impressive; serving static files with https at 20gbps with a 2690v4 isn&#x27;t hard to do if you have clients to pull the files.<p>IMHO, Erlang is built for reliability&#x2F;fault tolerance first. Process isolation happens to be good for both fault tolerance and enabling parallel processing. I find it to be a really nice environment to run lots of processes in, but it&#x27;s clearly not trying to win performance prizes. If you need throughput, you need to at least process the data outside of Erlang (TLS protocol happens in Erlang, TLS crypto happens in C), and sometimes it&#x27;s better to keep the data out of Erlang all together. Erlang is better suited for &#x27;control plane&#x27; than &#x27;data plane&#x27; applications; but it&#x27;s 2024 and we have an abundance of compute power, so you can shoehorn a lot into a less than high performance environment ;)</div><br/></div></div><div id="41094839" class="c"><input type="checkbox" id="c-41094839" checked=""/><div class="controls bullet"><span class="by">deagle50</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41094770">parent</a><span>|</span><a href="#41096330">prev</a><span>|</span><a href="#41094928">next</a><span>|</span><label class="collapse" for="c-41094839">[-]</label><label class="expand" for="c-41094839">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s not throughput</div><br/></div></div><div id="41094928" class="c"><input type="checkbox" id="c-41094928" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41094770">parent</a><span>|</span><a href="#41094839">prev</a><span>|</span><a href="#41095213">next</a><span>|</span><label class="collapse" for="c-41094928">[-]</label><label class="expand" for="c-41094928">[1 more]</label></div><br/><div class="children"><div class="content">but the traffic on each connection doesn&#x27;t need bandwidth in the throughput sense mentioned</div><br/></div></div><div id="41095213" class="c"><input type="checkbox" id="c-41095213" checked=""/><div class="controls bullet"><span class="by">JackSlateur</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41094770">parent</a><span>|</span><a href="#41094928">prev</a><span>|</span><a href="#41095162">next</a><span>|</span><label class="collapse" for="c-41095213">[-]</label><label class="expand" for="c-41095213">[2 more]</label></div><br/><div class="children"><div class="content">I worked in telecoms, no erlang found. Perhaps we were too modern ?</div><br/><div id="41096194" class="c"><input type="checkbox" id="c-41096194" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41095213">parent</a><span>|</span><a href="#41095162">next</a><span>|</span><label class="collapse" for="c-41096194">[-]</label><label class="expand" for="c-41096194">[1 more]</label></div><br/><div class="children"><div class="content">I worked in telecoms. We had a third party component written in Erlang. I&#x27;d say it was about the second most reliable component of the system. It was susceptible to some memory leaks, but they usually turned out to be caused by misuse of the C client library.<p>The most reliable component was written in C, with what must have been a space shuttle level of effort behind it. No memory allocation, except in code paths that can return an error to the user who asked for something that caused the system to need more memory (we probably got this wrong on our side of the API, and didn&#x27;t test out of memory scenarios, and they probably would have just resulted in OOM kills anyway). Every line of code commented, sometimes leading to the infamous &quot;&#x2F;&#x2F;add 1 to i&quot; but most times showing deep design thought. State machines documented with a paragraph for every state transition explaining non-obvious concerns.</div><br/></div></div></div></div></div></div></div></div><div id="41095162" class="c"><input type="checkbox" id="c-41095162" checked=""/><div class="controls bullet"><span class="by">OnlyMortal</span><span>|</span><a href="#41094717">parent</a><span>|</span><a href="#41094760">prev</a><span>|</span><a href="#41094765">next</a><span>|</span><label class="collapse" for="c-41095162">[-]</label><label class="expand" for="c-41095162">[4 more]</label></div><br/><div class="children"><div class="content">What are you talking about?<p>In C++ you have ASIO (Boost) that’s mostly used for IPC but can be used as a general purpose async event queue. There is io_uring support too. You can sit a pool of threads as the consumers for events if you want to scale.<p>C++ has had a defacto support for threads for ages (Boost) and it has been rolled into the standard library since 2011.<p>If you’re using compute clusters you also have MPI in Boost. That’s a scatter&#x2F;gather model.<p>There’s also OpenMP to parallelize loops if you’re so inclined.</div><br/><div id="41095212" class="c"><input type="checkbox" id="c-41095212" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41095162">parent</a><span>|</span><a href="#41094765">next</a><span>|</span><label class="collapse" for="c-41095212">[-]</label><label class="expand" for="c-41095212">[3 more]</label></div><br/><div class="children"><div class="content">I should look this up but I’m lazy.<p>I’m familiar with MPI in Fortran&#x2F;C, and IIRC they had some MPI C++ primitives a that never really got a ton of traction (that’s just the informal impression I got skimming their docs, though).<p>How’s MPI in C++ boost work? MPI communicates big dumb arrays best I think, so maybe they did all the plumbing work for translating Boost objects into big dumb arrays, communicating those, and then reconstructing them on the other side?</div><br/><div id="41095657" class="c"><input type="checkbox" id="c-41095657" checked=""/><div class="controls bullet"><span class="by">OnlyMortal</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41095212">parent</a><span>|</span><a href="#41094765">next</a><span>|</span><label class="collapse" for="c-41095657">[-]</label><label class="expand" for="c-41095657">[2 more]</label></div><br/><div class="children"><div class="content">It’s still a dumb way to do computation. The Boost library hides the implementation away though, on Linux and Macintosh, it requires gomp.<p>Really, it’s a wrapper but makes it “easier” for scientists to use who are, in general, not good coders.<p>Source: I’ve seen CERN research code.</div><br/><div id="41096210" class="c"><input type="checkbox" id="c-41096210" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41094717">root</a><span>|</span><a href="#41095657">parent</a><span>|</span><a href="#41094765">next</a><span>|</span><label class="collapse" for="c-41096210">[-]</label><label class="expand" for="c-41096210">[1 more]</label></div><br/><div class="children"><div class="content">What’s a dumb way to do computation? Using objects? I’m generally suspicious of divergence from the ideal form of computation (math, applied to a big dumb array) but C++ is quite popular.</div><br/></div></div></div></div></div></div></div></div><div id="41094765" class="c"><input type="checkbox" id="c-41094765" checked=""/><div class="controls bullet"><span class="by">JackSlateur</span><span>|</span><a href="#41094717">parent</a><span>|</span><a href="#41095162">prev</a><span>|</span><a href="#41095262">next</a><span>|</span><label class="collapse" for="c-41094765">[-]</label><label class="expand" for="c-41094765">[1 more]</label></div><br/><div class="children"><div class="content">Could you share more intels about this ? Links or whatever<p>I&#x27;d like to learn more</div><br/></div></div><div id="41095262" class="c"><input type="checkbox" id="c-41095262" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#41094717">parent</a><span>|</span><a href="#41094765">prev</a><span>|</span><label class="collapse" for="c-41095262">[-]</label><label class="expand" for="c-41095262">[1 more]</label></div><br/><div class="children"><div class="content">Go does something similar and ponylang is basically compiled Erlang.</div><br/></div></div></div></div></div></div></div></div></div></body></html>