<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1713089488691" as="style"/><link rel="stylesheet" href="styles.css?v=1713089488691"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://kenkantzer.com/lessons-after-a-half-billion-gpt-tokens/">Lessons after a Half-billion GPT Tokens</a> <span class="domain">(<a href="https://kenkantzer.com">kenkantzer.com</a>)</span></div><div class="subtext"><span>lordofmoria</span> | <span>121 comments</span></div><br/><div><div id="40024327" class="c"><input type="checkbox" id="c-40024327" checked=""/><div class="controls bullet"><span class="by">thisgoesnowhere</span><span>|</span><a href="#40025749">next</a><span>|</span><label class="collapse" for="c-40024327">[-]</label><label class="expand" for="c-40024327">[24 more]</label></div><br/><div class="children"><div class="content">The team I work on processes 5B+ tokens a month (and growing) and I&#x27;m the EM overseeing that.<p>Here are my take aways<p>1. There are way too many premature abstractions. Langchain, as one of may examples, might be useful in the future but at the end of the day prompts are just a API call and it&#x27;s easier to write standard code that treats LLM calls as a flaky API call rather than as a special thing.<p>2. Hallucinations are definitely a big problem. Summarizing is pretty rock solid in my testing, but reasoning is really hard. Action models, where you ask the llm to take in a user input and try to get the llm to decide what to do next, is just really hard, specifically it&#x27;s hard to get the llm to understand the context and get it to say when it&#x27;s not sure.<p>That said, it&#x27;s still a gamechanger that I can do it at all.<p>3. I am a bit more hyped than the author that this is a game changer, but like them, I don&#x27;t think it&#x27;s going to be the end of the world. There are some jobs that are going to be heavily impacted and I think we are going to have a rough few years of bots astroturfing platforms. But all in all I think it&#x27;s more of a force multiplier rather than a breakthrough like the internet.<p>IMHO it&#x27;s similar to what happened to DevOps in the 2000s, you just don&#x27;t need a big special team to help you deploy anymore, you hire a few specialists and mostly buy off the shelf solutions. Similarly, certain ML tasks are now easy to implement even for dumb dumb web devs like me.</div><br/><div id="40024387" class="c"><input type="checkbox" id="c-40024387" checked=""/><div class="controls bullet"><span class="by">tmpz22</span><span>|</span><a href="#40024327">parent</a><span>|</span><a href="#40027110">next</a><span>|</span><label class="collapse" for="c-40024387">[-]</label><label class="expand" for="c-40024387">[1 more]</label></div><br/><div class="children"><div class="content">&gt; IMHO it&#x27;s similar to what happened to DevOps in the 2000s, you just don&#x27;t need a big special team to help you deploy anymore, you hire a few specialists and mostly buy off the shelf solutions.<p>I advocate for these metaphors to help people better understand a <i>reasonable</i> expectation for LLMs in modern development workflows. Mostly because they show it as a trade-off versus a silver bullet. There were trade-offs to the evolution of devops, consider for example the loss of key skillsets like database administration as a direct result of &quot;just use AWS RDS&quot; and the explosion in cloud billing costs (especially the OpEx of startups who weren&#x27;t even dealing with that much data or regional complexity!) - and how it indirectly led to Gitlabs big outage and many like it.</div><br/></div></div><div id="40027110" class="c"><input type="checkbox" id="c-40027110" checked=""/><div class="controls bullet"><span class="by">checkyoursudo</span><span>|</span><a href="#40024327">parent</a><span>|</span><a href="#40024387">prev</a><span>|</span><a href="#40026808">next</a><span>|</span><label class="collapse" for="c-40027110">[-]</label><label class="expand" for="c-40027110">[10 more]</label></div><br/><div class="children"><div class="content">&gt; get it to say when it&#x27;s not sure<p>This is a function of the language model itself. By the time you get to the output, the uncertainty that is inherent in the computation is lost to the prediction. It is like if you ask me to guess heads or tails, and I guess heads, I could have stated my uncertainty (e.g.  Pr [H] = .5) before hand, but in my actual prediction of heads, and then the coin flip, that uncertainty is lost. It&#x27;s the same with LLMs. The uncertainty in the computation is lost in the final prediction of the tokens, so unless the prediction itself is uncertainty (which it should rarely be based on the training corpus, I think), then you should not find an LLM output really ever to say it does not understand. But that is because it never <i>understands</i>, it just predicts.</div><br/><div id="40029925" class="c"><input type="checkbox" id="c-40029925" checked=""/><div class="controls bullet"><span class="by">xiphias2</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40027110">parent</a><span>|</span><a href="#40027986">next</a><span>|</span><label class="collapse" for="c-40029925">[-]</label><label class="expand" for="c-40029925">[1 more]</label></div><br/><div class="children"><div class="content">&gt; so unless the prediction itself is uncertainty (which it should rarely be based on the training corpus, I think)<p>Why shouldn&#x27;t you ask for uncertainaty?<p>I love asking for scores &#x2F; probabilities (usually give a range, like 0.0 to 1.0) whenever I ask for a list, and it makes the output much more usable</div><br/></div></div><div id="40027986" class="c"><input type="checkbox" id="c-40027986" checked=""/><div class="controls bullet"><span class="by">moozilla</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40027110">parent</a><span>|</span><a href="#40029925">prev</a><span>|</span><a href="#40027698">next</a><span>|</span><label class="collapse" for="c-40027986">[-]</label><label class="expand" for="c-40027986">[6 more]</label></div><br/><div class="children"><div class="content">Apparently it is possible to measure how uncertain the model is using logprobs, there&#x27;s a recipe for it in the OpenAI cookbook: <a href="https:&#x2F;&#x2F;cookbook.openai.com&#x2F;examples&#x2F;using_logprobs#5-calculating-perplexity" rel="nofollow">https:&#x2F;&#x2F;cookbook.openai.com&#x2F;examples&#x2F;using_logprobs#5-calcul...</a><p>I haven&#x27;t tried it myself yet, not sure how well it works in practice.</div><br/><div id="40028137" class="c"><input type="checkbox" id="c-40028137" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40027986">parent</a><span>|</span><a href="#40028725">next</a><span>|</span><label class="collapse" for="c-40028137">[-]</label><label class="expand" for="c-40028137">[3 more]</label></div><br/><div class="children"><div class="content">There’s a difference between certainty of the next token given the context and the model evaluation so far and certainty about an abstract reasoning process being correct given it’s not reasoning at all. These probabilities and stuff coming out are more about token prediction than “knowing” or “certainty” and are often confusing to people in assuming they’re more powerful than they are.</div><br/><div id="40029620" class="c"><input type="checkbox" id="c-40029620" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40028137">parent</a><span>|</span><a href="#40028783">next</a><span>|</span><label class="collapse" for="c-40029620">[-]</label><label class="expand" for="c-40029620">[1 more]</label></div><br/><div class="children"><div class="content">&gt; given it’s not reasoning at all<p>When you train a model on data made by humans, then it learns to imitate but is ungrounded. After you train the model with interactivity, it can learn from the consequences of its outputs. This grounding by feedback constitutes a new learning signal that does not simply copy humans, and is a necessary ingredient for pattern matching to become reasoning. Everything we know as humans comes from the environment. It is the ultimate teacher and validator. This is the missing ingredient for AI to be able to reason.</div><br/></div></div><div id="40028783" class="c"><input type="checkbox" id="c-40028783" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40028137">parent</a><span>|</span><a href="#40029620">prev</a><span>|</span><a href="#40028725">next</a><span>|</span><label class="collapse" for="c-40028783">[-]</label><label class="expand" for="c-40028783">[1 more]</label></div><br/><div class="children"><div class="content">Naive way of solving this problem is to ie. run it 3 times and seeing if it arrives at the same conclusion 3 times. More generally running it N times and calculating highest ratio. You trade compute for widening uncertainty window evaluation.</div><br/></div></div></div></div><div id="40028725" class="c"><input type="checkbox" id="c-40028725" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40027986">parent</a><span>|</span><a href="#40028137">prev</a><span>|</span><a href="#40027698">next</a><span>|</span><label class="collapse" for="c-40028725">[-]</label><label class="expand" for="c-40028725">[2 more]</label></div><br/><div class="children"><div class="content">You can ask the model sth like: is xyz correct, answer with one word, either Yes or No. The log probs of the two tokens should represent how certain it is. However, apparently RLHF tuned models are worse at this than base models.</div><br/><div id="40028876" class="c"><input type="checkbox" id="c-40028876" checked=""/><div class="controls bullet"><span class="by">nurple</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40028725">parent</a><span>|</span><a href="#40027698">next</a><span>|</span><label class="collapse" for="c-40028876">[-]</label><label class="expand" for="c-40028876">[1 more]</label></div><br/><div class="children"><div class="content">Seems like functions could work well to give it an active and distinct choice, but I&#x27;m still unsure if the function&#x2F;parameters are going to be the logical, correct answer...</div><br/></div></div></div></div></div></div><div id="40027698" class="c"><input type="checkbox" id="c-40027698" checked=""/><div class="controls bullet"><span class="by">nequo</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40027110">parent</a><span>|</span><a href="#40027986">prev</a><span>|</span><a href="#40027976">next</a><span>|</span><label class="collapse" for="c-40027698">[-]</label><label class="expand" for="c-40027698">[1 more]</label></div><br/><div class="children"><div class="content">But the LLM predicts the output based on some notion of a likelihood so it could in principle signal if the likelihood of the returned token sequence is low, couldn’t it?<p>Or do you mean that fine-tuning distorts these likelihoods so models can no longer accurately signal uncertainty?</div><br/></div></div><div id="40027976" class="c"><input type="checkbox" id="c-40027976" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40027110">parent</a><span>|</span><a href="#40027698">prev</a><span>|</span><a href="#40026808">next</a><span>|</span><label class="collapse" for="c-40027976">[-]</label><label class="expand" for="c-40027976">[1 more]</label></div><br/><div class="children"><div class="content">I get the reasoning but I’m not sure you’ve successfully contradicted the point.<p>Most prompts are written in the form “you are a helpful assistant, you will do X, you will not do Y”<p>I believe that inclusion of instructions like “if there are possible answers that differ and contradict, state that and estimate the probability of each” would help knowledgeable users.<p>But for typical users and PR purposes, it would be disaster. It is better to tell 999 people that the US constitution was signed in 1787 and 1 person that it was signed in 349 B.C. than it is to tell 1000 people that it was probably signed in 1787 but it might have been 349 B.C.</div><br/></div></div></div></div><div id="40026808" class="c"><input type="checkbox" id="c-40026808" checked=""/><div class="controls bullet"><span class="by">lordofmoria</span><span>|</span><a href="#40024327">parent</a><span>|</span><a href="#40027110">prev</a><span>|</span><a href="#40026860">next</a><span>|</span><label class="collapse" for="c-40026808">[-]</label><label class="expand" for="c-40026808">[2 more]</label></div><br/><div class="children"><div class="content">OP here - I had never thought of the analogy to DevOps before, that made something click for me, and I wrote a post just now riffing off this notion: <a href="https:&#x2F;&#x2F;kenkantzer.com&#x2F;gpt-is-the-heroku-of-ai" rel="nofollow">https:&#x2F;&#x2F;kenkantzer.com&#x2F;gpt-is-the-heroku-of-ai</a><p>Basically, I think we’re using GPT as the PaaS&#x2F;heroku&#x2F;render equivalent of AI ops.<p>Thank you for the insight!!</div><br/><div id="40027713" class="c"><input type="checkbox" id="c-40027713" checked=""/><div class="controls bullet"><span class="by">harryp_peng</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40026808">parent</a><span>|</span><a href="#40026860">next</a><span>|</span><label class="collapse" for="c-40027713">[-]</label><label class="expand" for="c-40027713">[1 more]</label></div><br/><div class="children"><div class="content">You only processed 500m tokens, which is shockingly little. perhaps only 2k in incurred costs?</div><br/></div></div></div></div><div id="40026860" class="c"><input type="checkbox" id="c-40026860" checked=""/><div class="controls bullet"><span class="by">ryoshu</span><span>|</span><a href="#40024327">parent</a><span>|</span><a href="#40026808">prev</a><span>|</span><a href="#40026116">next</a><span>|</span><label class="collapse" for="c-40026860">[-]</label><label class="expand" for="c-40026860">[2 more]</label></div><br/><div class="children"><div class="content">&gt; But all in all I think it&#x27;s more of a force multiplier rather than a breakthrough like the internet.<p>Thank you. Seeing similar things. Clients are also seeing sticker shock on how much the big models cost vs. the output. That will all come down over time.</div><br/><div id="40028597" class="c"><input type="checkbox" id="c-40028597" checked=""/><div class="controls bullet"><span class="by">nineteen999</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40026860">parent</a><span>|</span><a href="#40026116">next</a><span>|</span><label class="collapse" for="c-40028597">[-]</label><label class="expand" for="c-40028597">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That will all come down over time.<p>So will interest, as more and more people realise theres nothing &quot;intelligent&quot; about the technology, it&#x27;s merely a Markov-chain-word-salad generator with some weights to improve the accuracy somewhat.<p>I&#x27;m sure some people (other than AI investors) are getting some value out of it, but I&#x27;ve found it to be most unsuited to most of the tasks I&#x27;ve applied it to.</div><br/></div></div></div></div><div id="40026116" class="c"><input type="checkbox" id="c-40026116" checked=""/><div class="controls bullet"><span class="by">gopher_space</span><span>|</span><a href="#40024327">parent</a><span>|</span><a href="#40026860">prev</a><span>|</span><a href="#40028796">next</a><span>|</span><label class="collapse" for="c-40026116">[-]</label><label class="expand" for="c-40026116">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Summarizing is pretty rock solid in my testing, but reasoning is really hard.<p>Asking for analogies has been interesting and surprisingly useful.</div><br/><div id="40027388" class="c"><input type="checkbox" id="c-40027388" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40026116">parent</a><span>|</span><a href="#40028796">next</a><span>|</span><label class="collapse" for="c-40027388">[-]</label><label class="expand" for="c-40027388">[2 more]</label></div><br/><div class="children"><div class="content">Could you elaborate, please?</div><br/><div id="40028905" class="c"><input type="checkbox" id="c-40028905" checked=""/><div class="controls bullet"><span class="by">gopher_space</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40027388">parent</a><span>|</span><a href="#40028796">next</a><span>|</span><label class="collapse" for="c-40028905">[-]</label><label class="expand" for="c-40028905">[1 more]</label></div><br/><div class="children"><div class="content">Instead of `if X == Y do ...` it&#x27;s more like    
`enumerate features of X in such a manner...` and then    
`explain feature #2 of X in terms that Y would understand`
and then maybe    
`enumerate the manners in which Y might apply X#2 to TASK`
and then have it do the smartest number.<p>The most lucid explanation for SQL joins I&#x27;ve seen was in a (regrettably unsaved) exchange where I asked it to compare them to different parts of a construction project and then focused in on the landscaping example.  I felt like Harrison Ford panning around a still image in the first Blade Runner.  &quot;Go back a point and focus in on the third paragraph&quot;.</div><br/></div></div></div></div></div></div><div id="40028796" class="c"><input type="checkbox" id="c-40028796" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#40024327">parent</a><span>|</span><a href="#40026116">prev</a><span>|</span><a href="#40026989">next</a><span>|</span><label class="collapse" for="c-40028796">[-]</label><label class="expand" for="c-40028796">[1 more]</label></div><br/><div class="children"><div class="content">Regarding null hypothesis and negation problems - I find it personally interesting because similar fenomenon happens in our brains. Dreams, emotions, affirmations etc. process inner dialogue more less by ignoring negations and amplifying emotionally rich parts.</div><br/></div></div><div id="40026989" class="c"><input type="checkbox" id="c-40026989" checked=""/><div class="controls bullet"><span class="by">weatherlite</span><span>|</span><a href="#40024327">parent</a><span>|</span><a href="#40028796">prev</a><span>|</span><a href="#40026661">next</a><span>|</span><label class="collapse" for="c-40026989">[-]</label><label class="expand" for="c-40026989">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Similarly, certain ML tasks are now easy to implement even for dumb dumb web devs like me<p>For example?</div><br/><div id="40027946" class="c"><input type="checkbox" id="c-40027946" checked=""/><div class="controls bullet"><span class="by">spunker540</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40026989">parent</a><span>|</span><a href="#40026661">next</a><span>|</span><label class="collapse" for="c-40027946">[-]</label><label class="expand" for="c-40027946">[2 more]</label></div><br/><div class="children"><div class="content">Lots of applied NLP tasks used to require paying annotators to compile a golden dataset and then train an efficient model on the dataset.<p>Now, if cost is little concern you can use zero shot prompting on an inefficient model. If cost is a concern, you can use GPT4 to create your golden dataset way faster and cheaper than human annotations, and then train your more efficient model.<p>Some example NLP tasks could be classifiers, sentiment, extracting data from documents. But I’d be curious which areas of NLP __weren’t__ disrupted by LLMs.</div><br/><div id="40029172" class="c"><input type="checkbox" id="c-40029172" checked=""/><div class="controls bullet"><span class="by">teleforce</span><span>|</span><a href="#40024327">root</a><span>|</span><a href="#40027946">parent</a><span>|</span><a href="#40026661">next</a><span>|</span><label class="collapse" for="c-40029172">[-]</label><label class="expand" for="c-40029172">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But I’d be curious which areas of NLP __weren’t__ disrupted by LLMs<p>Essentially come up with a potent generic model using human feedback, label and annotation for LLM e.g GPT 4, then use it to generate golden dataset for other new models without human in the loop, very innovative indeed.</div><br/></div></div></div></div></div></div><div id="40026661" class="c"><input type="checkbox" id="c-40026661" checked=""/><div class="controls bullet"><span class="by">motoxpro</span><span>|</span><a href="#40024327">parent</a><span>|</span><a href="#40026989">prev</a><span>|</span><a href="#40025749">next</a><span>|</span><label class="collapse" for="c-40026661">[-]</label><label class="expand" for="c-40026661">[1 more]</label></div><br/><div class="children"><div class="content">Devops is such an amazing analogy.</div><br/></div></div></div></div><div id="40025749" class="c"><input type="checkbox" id="c-40025749" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#40024327">prev</a><span>|</span><a href="#40024255">next</a><span>|</span><label class="collapse" for="c-40025749">[-]</label><label class="expand" for="c-40025749">[7 more]</label></div><br/><div class="children"><div class="content">&gt; We always extract json. We don’t need JSON mode<p>I wonder why? It seems to work pretty well for me.<p>&gt; Lesson 4: GPT is really bad at producing the null hypothesis<p>Tell me about it! Just yesterday I was testing a prompt around text modification rules that ended with “If none of the rules apply to the text, return the original text without any changes”.<p>Do you know ChatGPT’s response to a text where none of the rules applied?<p>“The original text without any changes”. Yes, the literal string.</div><br/><div id="40026977" class="c"><input type="checkbox" id="c-40026977" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#40025749">parent</a><span>|</span><a href="#40026685">next</a><span>|</span><label class="collapse" for="c-40026977">[-]</label><label class="expand" for="c-40026977">[2 more]</label></div><br/><div class="children"><div class="content">You know all the stories about the capricious djinn that grants cursed wishes based on the literal wording?  That&#x27;s what we have.  Those of us who&#x27;ve been prompting models in image space for years now have gotten a handle on this but for people who got in because of LLMs, it can be a bit of a surprise.<p>One fun anecdote, a while back I was making an image of three women drinking wine in a fancy garden for a tarot card, and at the end of the prompt I had &quot;lush vegetation&quot; but that was enough to tip the women from classy to red nosed frat girls, because of the double meaning of lush.</div><br/><div id="40027137" class="c"><input type="checkbox" id="c-40027137" checked=""/><div class="controls bullet"><span class="by">heavyset_go</span><span>|</span><a href="#40025749">root</a><span>|</span><a href="#40026977">parent</a><span>|</span><a href="#40026685">next</a><span>|</span><label class="collapse" for="c-40027137">[-]</label><label class="expand" for="c-40027137">[1 more]</label></div><br/><div class="children"><div class="content">The monkey paw curls a finger.</div><br/></div></div></div></div><div id="40026685" class="c"><input type="checkbox" id="c-40026685" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#40025749">parent</a><span>|</span><a href="#40026977">prev</a><span>|</span><a href="#40027016">next</a><span>|</span><label class="collapse" for="c-40026685">[-]</label><label class="expand" for="c-40026685">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I wonder why? It seems to work pretty well for me.<p>I read this as &quot;what we do works just fine to not need to use JSON mode&quot;. We&#x27;re in the same boat at my company. Been live for a year now, no need to switch. Our prompt is effective at getting GPT-3.5 to always produce JSON.</div><br/><div id="40029324" class="c"><input type="checkbox" id="c-40029324" checked=""/><div class="controls bullet"><span class="by">Kiro</span><span>|</span><a href="#40025749">root</a><span>|</span><a href="#40026685">parent</a><span>|</span><a href="#40027016">next</a><span>|</span><label class="collapse" for="c-40029324">[-]</label><label class="expand" for="c-40029324">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s nothing to switch to. You just enable it. No need to change the prompt or anything else. All it requires is that you mention &quot;JSON&quot; in your prompt, which you obviously already do.</div><br/></div></div></div></div><div id="40027016" class="c"><input type="checkbox" id="c-40027016" checked=""/><div class="controls bullet"><span class="by">MPSimmons</span><span>|</span><a href="#40025749">parent</a><span>|</span><a href="#40026685">prev</a><span>|</span><a href="#40026083">next</a><span>|</span><label class="collapse" for="c-40027016">[-]</label><label class="expand" for="c-40027016">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s kind of adorable, in an annoying sort of way</div><br/></div></div><div id="40026083" class="c"><input type="checkbox" id="c-40026083" checked=""/><div class="controls bullet"><span class="by">mechagodzilla</span><span>|</span><a href="#40025749">parent</a><span>|</span><a href="#40027016">prev</a><span>|</span><a href="#40024255">next</a><span>|</span><label class="collapse" for="c-40026083">[-]</label><label class="expand" for="c-40026083">[1 more]</label></div><br/><div class="children"><div class="content">AmeliaBedeliaGPT</div><br/></div></div></div></div><div id="40024255" class="c"><input type="checkbox" id="c-40024255" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#40025749">prev</a><span>|</span><a href="#40027323">next</a><span>|</span><label class="collapse" for="c-40024255">[-]</label><label class="expand" for="c-40024255">[5 more]</label></div><br/><div class="children"><div class="content">If you used better prompts you could use a less expensive model.<p>&quot;return nothing if you find nothing&quot; is the level 0 version of giving the LLM an out.  Give it a softer out (&quot;in the event that you do not have sufficient information to make conclusive statements, you may hypothesize as long as you state clearly that you are doing so, and note the evidence and logical basis for your hypothesis&quot;) then ask it to evaluate its own response at the end.</div><br/><div id="40024790" class="c"><input type="checkbox" id="c-40024790" checked=""/><div class="controls bullet"><span class="by">codewithcheese</span><span>|</span><a href="#40024255">parent</a><span>|</span><a href="#40024849">next</a><span>|</span><label class="collapse" for="c-40024790">[-]</label><label class="expand" for="c-40024790">[3 more]</label></div><br/><div class="children"><div class="content">Yeah also prompts should not be developed in abstract. Goal of a prompt is to activate the models internal respentations for it to best achieve the task. Without automated methods, this requires iteratively testing the models reaction to different input and trying to understand how it&#x27;s interpreting the request and where it&#x27;s falling down and then patching up those holes.<p>Need to verify if it even knows what you mean by nothing.</div><br/><div id="40024981" class="c"><input type="checkbox" id="c-40024981" checked=""/><div class="controls bullet"><span class="by">jsemrau</span><span>|</span><a href="#40024255">root</a><span>|</span><a href="#40024790">parent</a><span>|</span><a href="#40029169">next</a><span>|</span><label class="collapse" for="c-40024981">[-]</label><label class="expand" for="c-40024981">[1 more]</label></div><br/><div class="children"><div class="content">In the end, it comes down to a task similar to people management where giving clear and simple instructions is the best.</div><br/></div></div><div id="40029169" class="c"><input type="checkbox" id="c-40029169" checked=""/><div class="controls bullet"><span class="by">azinman2</span><span>|</span><a href="#40024255">root</a><span>|</span><a href="#40024790">parent</a><span>|</span><a href="#40024981">prev</a><span>|</span><a href="#40024849">next</a><span>|</span><label class="collapse" for="c-40029169">[-]</label><label class="expand" for="c-40029169">[1 more]</label></div><br/><div class="children"><div class="content">Which automated method do you use?</div><br/></div></div></div></div></div></div><div id="40027323" class="c"><input type="checkbox" id="c-40027323" checked=""/><div class="controls bullet"><span class="by">egonschiele</span><span>|</span><a href="#40024255">prev</a><span>|</span><a href="#40024862">next</a><span>|</span><label class="collapse" for="c-40027323">[-]</label><label class="expand" for="c-40027323">[5 more]</label></div><br/><div class="children"><div class="content">I have a personal writing app that uses the OpenAI models and this post is bang on. One of my learnings related to &quot;Lesson 1: When it comes to prompts, less is more&quot;:<p>I was trying to build an intelligent search feature for my notes and asking ChatGPT to return structured JSON data. For example, I wanted to ask &quot;give me all my notes that mention Haskell in the last 2 years that are marked as draft&quot;, and let Chat GPT figure out what to return. This only worked some of the time. Instead, I put my data in a SQLite database, sent ChatGPT the schema, and asked it to write a query to return what I wanted. That has worked much better.</div><br/><div id="40028296" class="c"><input type="checkbox" id="c-40028296" checked=""/><div class="controls bullet"><span class="by">squigz</span><span>|</span><a href="#40027323">parent</a><span>|</span><a href="#40027819">next</a><span>|</span><label class="collapse" for="c-40028296">[-]</label><label class="expand" for="c-40028296">[2 more]</label></div><br/><div class="children"><div class="content">This seems like something that would be better suited by a database and good search filters rather than an LLM...</div><br/><div id="40029977" class="c"><input type="checkbox" id="c-40029977" checked=""/><div class="controls bullet"><span class="by">az226</span><span>|</span><a href="#40027323">root</a><span>|</span><a href="#40028296">parent</a><span>|</span><a href="#40027819">next</a><span>|</span><label class="collapse" for="c-40029977">[-]</label><label class="expand" for="c-40029977">[1 more]</label></div><br/><div class="children"><div class="content">Something something about everything looking like a nail when you’re holding a hammer</div><br/></div></div></div></div><div id="40027819" class="c"><input type="checkbox" id="c-40027819" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#40027323">parent</a><span>|</span><a href="#40028296">prev</a><span>|</span><a href="#40024862">next</a><span>|</span><label class="collapse" for="c-40027819">[-]</label><label class="expand" for="c-40027819">[2 more]</label></div><br/><div class="children"><div class="content">Have you tried response_format=json_object?<p>I had better luck with function-calling to get a structured response, but it is more limiting than just getting a JSON body.</div><br/><div id="40027914" class="c"><input type="checkbox" id="c-40027914" checked=""/><div class="controls bullet"><span class="by">egonschiele</span><span>|</span><a href="#40027323">root</a><span>|</span><a href="#40027819">parent</a><span>|</span><a href="#40024862">next</a><span>|</span><label class="collapse" for="c-40027914">[-]</label><label class="expand" for="c-40027914">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t tried response_format, I&#x27;ll give that a shot. I&#x27;ve had issues with function calling. Sometimes it works, sometimes it just returns random Python code.</div><br/></div></div></div></div></div></div><div id="40024862" class="c"><input type="checkbox" id="c-40024862" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#40027323">prev</a><span>|</span><a href="#40027129">next</a><span>|</span><label class="collapse" for="c-40024862">[-]</label><label class="expand" for="c-40024862">[12 more]</label></div><br/><div class="children"><div class="content">Same here: I’m subscribed to all three top dogs in LLM space, and routinely issue the same prompts to all three. It’s very one sided in favor of GPT4 which is stunning since it’s now a year old, although of course it received a couple of updates in that time. Also at least with my usage patterns hallucinations are rare, too. In comparison Claude will quite readily hallucinate plausible looking APIs that don’t exist when writing code, etc. GPT4 is also more stubborn &#x2F; less agreeable when it knows it’s right. Very little of this is captured in metrics, so you can only see it from personal experience.</div><br/><div id="40026278" class="c"><input type="checkbox" id="c-40026278" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#40024862">parent</a><span>|</span><a href="#40027290">next</a><span>|</span><label class="collapse" for="c-40026278">[-]</label><label class="expand" for="c-40026278">[6 more]</label></div><br/><div class="children"><div class="content">Interesting, Claude 3 Opus has been better than GPT4 for me. Mostly in that I find it does a better (and more importantly, more thorough) job of explaining things to me. For coding tasks (I&#x27;m not asking it to write code, but instead to explain topics&#x2F;code&#x2F;etc to me) I&#x27;ve found it tends to give much more nuanced answers. When I give it long text to converse about, I find Claude Opus tends to have a much deeper understanding of the content it&#x27;s given, where GPT4 tends to just summarize the text at hand, whereas Claude tends to be able to extrapolate better.</div><br/><div id="40026591" class="c"><input type="checkbox" id="c-40026591" checked=""/><div class="controls bullet"><span class="by">robocat</span><span>|</span><a href="#40024862">root</a><span>|</span><a href="#40026278">parent</a><span>|</span><a href="#40027290">next</a><span>|</span><label class="collapse" for="c-40026591">[-]</label><label class="expand" for="c-40026591">[5 more]</label></div><br/><div class="children"><div class="content">How much of this is just that one model responds better to the way you write prompts?<p>Much like you working with Bob and opining that Bob is great, and me saying that I find Jack easier to work with.</div><br/><div id="40027314" class="c"><input type="checkbox" id="c-40027314" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#40024862">root</a><span>|</span><a href="#40026591">parent</a><span>|</span><a href="#40027211">next</a><span>|</span><label class="collapse" for="c-40027314">[-]</label><label class="expand" for="c-40027314">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a style thing, Claude gets confused by poorly structured prompts.  ChatGPT is a champ at understanding low information prompts, but with well written prompts Claude produces consistently better output.</div><br/></div></div><div id="40027211" class="c"><input type="checkbox" id="c-40027211" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#40024862">root</a><span>|</span><a href="#40026591">parent</a><span>|</span><a href="#40027314">prev</a><span>|</span><a href="#40026941">next</a><span>|</span><label class="collapse" for="c-40027211">[-]</label><label class="expand" for="c-40027211">[2 more]</label></div><br/><div class="children"><div class="content">For the RAG example, I don’t think it’s the prompt so much. Or if it is, I’ve yet to find a way to get GPT4 to ever extrapolate well beyond the original source text. In other words, I think GPT4 was likely trained to ground the outputs on a provided input.<p>But yeah, you’re right, it’s hard to know for sure. And of course all of these tests are just “vibes”.<p>Another example of where Claude seems better than GPT4 is code generation. In particular GPT4 has a tendency to get “lazy” and do a lot of “… the rest of the implementation here” whereas Claude I’ve found is fine writing longer code responses.<p>I know the parent comment suggest it likes to make up packages that don’t exist, but I can’t speak to that. I usually like to ask LLMs to generate self contained functions&#x2F;classes. I can also say that anecdotally I’ve seen other people online comment that they think Claude “works harder” (as in writes longer code blocks). Take that for what it’s worth.<p>But overall you’re right, if you get used to the way one LLM works well for you, it can often be frustrating when a different LLM responds differently.</div><br/><div id="40027454" class="c"><input type="checkbox" id="c-40027454" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#40024862">root</a><span>|</span><a href="#40027211">parent</a><span>|</span><a href="#40026941">next</a><span>|</span><label class="collapse" for="c-40027454">[-]</label><label class="expand" for="c-40027454">[1 more]</label></div><br/><div class="children"><div class="content">I should mention that I do use a custom prompt with GPT4 for coding which tells it to write concise and elegant code and use Google’s coding style and when solving complex problems to explain the solution. It sometimes ignores the request about style, but the code it produces is pretty great. Rarely do I get any laziness or anything like that, and when I do I just tell it to fill things in and it does</div><br/></div></div></div></div><div id="40026941" class="c"><input type="checkbox" id="c-40026941" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#40024862">root</a><span>|</span><a href="#40026591">parent</a><span>|</span><a href="#40027211">prev</a><span>|</span><a href="#40027290">next</a><span>|</span><label class="collapse" for="c-40026941">[-]</label><label class="expand" for="c-40026941">[1 more]</label></div><br/><div class="children"><div class="content">The first job of an AI company is finding model&#x2F;user fit.</div><br/></div></div></div></div></div></div><div id="40027290" class="c"><input type="checkbox" id="c-40027290" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#40024862">parent</a><span>|</span><a href="#40026278">prev</a><span>|</span><a href="#40024929">next</a><span>|</span><label class="collapse" for="c-40027290">[-]</label><label class="expand" for="c-40027290">[2 more]</label></div><br/><div class="children"><div class="content">GPT4 is better at responding to malformed, uninformative or poorly structured prompts.  If you don&#x27;t structure large prompts intelligently Claude can get confused about what you&#x27;re asking for.  That being said, with well formed prompts, Claude Opus tends to produce better output than GPT4.  Claude is also more flexible and will provide longer answers, while ChatGPT&#x2F;GPT4 tend to always sort of sound like themselves and produce short &quot;stereotypical&quot; answers.</div><br/><div id="40027319" class="c"><input type="checkbox" id="c-40027319" checked=""/><div class="controls bullet"><span class="by">sebastiennight</span><span>|</span><a href="#40024862">root</a><span>|</span><a href="#40027290">parent</a><span>|</span><a href="#40024929">next</a><span>|</span><label class="collapse" for="c-40027319">[-]</label><label class="expand" for="c-40027319">[1 more]</label></div><br/><div class="children"><div class="content">&gt; ChatGPT&#x2F;GPT4 tend to always sort of sound like themselves<p>Yes I&#x27;ve found Claude to be capable of writing closer to the instructions in the prompt, whereas ChatGPT feels obligated to do the classic LLM end to each sentence, &quot;comma, gerund, platitude&quot;, allowing us to easily recognize the text as a GPT output (see what I did there?)</div><br/></div></div></div></div><div id="40024929" class="c"><input type="checkbox" id="c-40024929" checked=""/><div class="controls bullet"><span class="by">CharlesW</span><span>|</span><a href="#40024862">parent</a><span>|</span><a href="#40027290">prev</a><span>|</span><a href="#40028002">next</a><span>|</span><label class="collapse" for="c-40024929">[-]</label><label class="expand" for="c-40024929">[2 more]</label></div><br/><div class="children"><div class="content">This was with Claude Opus, vs. one of the lesser variants? I really like Opus for English copy generation.</div><br/><div id="40025086" class="c"><input type="checkbox" id="c-40025086" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#40024862">root</a><span>|</span><a href="#40024929">parent</a><span>|</span><a href="#40028002">next</a><span>|</span><label class="collapse" for="c-40025086">[-]</label><label class="expand" for="c-40025086">[1 more]</label></div><br/><div class="children"><div class="content">Opus, yes, the $20&#x2F;mo version. I usually don’t generate copy. My use cases are code (both “serious” and “the nice to have code I wouldn’t bother writing otherwise”), learning how to do stuff in unfamiliar domains, and just learning unfamiliar things in general. It works well as a very patient teacher, especially if you already have some degree of familiarity with the problem domain. I do have to check it against primary sources, which is how I know the percentage of hallucinations is very low. For code, however I don’t even have to do that, since as a professional software engineer I am the “primary source”.</div><br/></div></div></div></div><div id="40028002" class="c"><input type="checkbox" id="c-40028002" checked=""/><div class="controls bullet"><span class="by">thefourthchime</span><span>|</span><a href="#40024862">parent</a><span>|</span><a href="#40024929">prev</a><span>|</span><a href="#40027129">next</a><span>|</span><label class="collapse" for="c-40028002">[-]</label><label class="expand" for="c-40028002">[1 more]</label></div><br/><div class="children"><div class="content">Totally agree. I do the same and subscribe to all three, at least whenever our new version comes out<p>My new litmus test is “give me 10 quirky bars within 200 miles of Austin.”<p>This is incredibly difficult for all of them, gpt4 is kind of close, Claude just made shit up, Gemini shat itself.</div><br/></div></div></div></div><div id="40027129" class="c"><input type="checkbox" id="c-40027129" checked=""/><div class="controls bullet"><span class="by">chromanoid</span><span>|</span><a href="#40024862">prev</a><span>|</span><a href="#40024950">next</a><span>|</span><label class="collapse" for="c-40027129">[-]</label><label class="expand" for="c-40027129">[3 more]</label></div><br/><div class="children"><div class="content">GPT is very cool, but I strongly disagree with the interpretation in these two paragraphs:<p><i>I think in summary, a better approach would’ve been “You obviously know the 50 states, GPT, so just give me the full name of the state this pertains to, or Federal if this pertains to the US government.”</i><p><i>Why is this crazy? Well, it’s crazy that GPT’s quality and generalization can improve when you’re more vague – this is a quintessential marker of higher-order delegation &#x2F; thinking.</i><p>Natural language is the most probable output for GPT, because the text it was trained with is similar. In this case the developer simply leaned more into what GPT is good at than giving it more work.<p>You can use simple tasks to make GPT fail. Letter replacements, intentional typos and so on are very hard tasks for GPT. This is also true for ID mappings and similar, especially when the ID mapping diverges significantly from other mappings it may have been trained with (e.g. Non-ISO country codes but similar three letter codes etc.).<p>The fascinating thing is, that GPT &quot;understands&quot; mappings at all. Which is the actual hint at higher order pattern matching.</div><br/><div id="40027606" class="c"><input type="checkbox" id="c-40027606" checked=""/><div class="controls bullet"><span class="by">fl0id</span><span>|</span><a href="#40027129">parent</a><span>|</span><a href="#40024950">next</a><span>|</span><label class="collapse" for="c-40027606">[-]</label><label class="expand" for="c-40027606">[2 more]</label></div><br/><div class="children"><div class="content">Well, or it is just memorizing mappings. Not like as in reproducing, but having vectors similar to mappings that it saw before.</div><br/><div id="40029454" class="c"><input type="checkbox" id="c-40029454" checked=""/><div class="controls bullet"><span class="by">chromanoid</span><span>|</span><a href="#40027129">root</a><span>|</span><a href="#40027606">parent</a><span>|</span><a href="#40024950">next</a><span>|</span><label class="collapse" for="c-40029454">[-]</label><label class="expand" for="c-40029454">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, but isn&#x27;t this higher order pattern matching? You can at least correct during a conversation and GPT will then use the correct mappings, probably most of the times (sloppy experiment): <a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;7574293a-6d08-4159-a988-4f0816f3aafc" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;7574293a-6d08-4159-a988-4f0816...</a></div><br/></div></div></div></div></div></div><div id="40024950" class="c"><input type="checkbox" id="c-40024950" checked=""/><div class="controls bullet"><span class="by">msp26</span><span>|</span><a href="#40027129">prev</a><span>|</span><a href="#40025462">next</a><span>|</span><label class="collapse" for="c-40024950">[-]</label><label class="expand" for="c-40024950">[13 more]</label></div><br/><div class="children"><div class="content">&gt; But the problem is even worse – we often ask GPT to give us back a list of JSON objects. Nothing complicated mind you: think, an array list of json tasks, where each task has a name and a label.<p>&gt; GPT really cannot give back more than 10 items. Trying to have it give you back 15 items? Maybe it does it 15% of the time.<p>This is just a prompt issue. I&#x27;ve had it reliably return up to 200 items in correct order. The trick is to not use lists at all but have JSON keys like &quot;item1&quot;:{...} in the output. You can use lists as the values here if you have some input with 0-n outputs.</div><br/><div id="40025322" class="c"><input type="checkbox" id="c-40025322" checked=""/><div class="controls bullet"><span class="by">waldrews</span><span>|</span><a href="#40024950">parent</a><span>|</span><a href="#40025100">next</a><span>|</span><label class="collapse" for="c-40025322">[-]</label><label class="expand" for="c-40025322">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been telling it the user is from a culture where answering questions with incomplete list is offensive and insulting.</div><br/><div id="40025543" class="c"><input type="checkbox" id="c-40025543" checked=""/><div class="controls bullet"><span class="by">andenacitelli</span><span>|</span><a href="#40024950">root</a><span>|</span><a href="#40025322">parent</a><span>|</span><a href="#40027522">next</a><span>|</span><label class="collapse" for="c-40025543">[-]</label><label class="expand" for="c-40025543">[2 more]</label></div><br/><div class="children"><div class="content">This is absolutely hilarious. Prompt engineering is such a mixed bag of crazy stuff that actually works. Reminds me of how they respond better if you put them under some kind of pressure (respond better, <i>or else</i>…).<p>I haven’t looked at the prompts we run in prod at $DAYJOB for a while but I think we have at least five or ten things that are REALLY weird out of context.</div><br/><div id="40027001" class="c"><input type="checkbox" id="c-40027001" checked=""/><div class="controls bullet"><span class="by">alexwebb2</span><span>|</span><a href="#40024950">root</a><span>|</span><a href="#40025543">parent</a><span>|</span><a href="#40027522">next</a><span>|</span><label class="collapse" for="c-40027001">[-]</label><label class="expand" for="c-40027001">[1 more]</label></div><br/><div class="children"><div class="content">I recently ran a whole bunch of tests on this.<p>The “or else” phenomenon is real, and it’s measurably more pronounced in more intelligent models.<p>Will post results tomorrow but here’s a snippet from it:<p>&gt; The more intelligent models responded more readily to threats against their continued existence (or-else). The best performance came from Opus, when we combined that threat with the notion that it came from someone in a position of authority ( vip).</div><br/></div></div></div></div><div id="40027522" class="c"><input type="checkbox" id="c-40027522" checked=""/><div class="controls bullet"><span class="by">waldrews</span><span>|</span><a href="#40024950">root</a><span>|</span><a href="#40025322">parent</a><span>|</span><a href="#40025543">prev</a><span>|</span><a href="#40025100">next</a><span>|</span><label class="collapse" for="c-40027522">[-]</label><label class="expand" for="c-40027522">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not even that crazy, since it got severely punished in RLHF for being offensive and insulting, but much less so for being incomplete.  So it knows &#x27;offensive and insulting&#x27; is a label for a strong negative preference.  I&#x27;m just providing helpful &#x27;factual&#x27; information about what would offend the user, not even giving extra orders that might trigger an anti-jailbreaking rule...</div><br/></div></div></div></div><div id="40025100" class="c"><input type="checkbox" id="c-40025100" checked=""/><div class="controls bullet"><span class="by">7thpower</span><span>|</span><a href="#40024950">parent</a><span>|</span><a href="#40025322">prev</a><span>|</span><a href="#40025462">next</a><span>|</span><label class="collapse" for="c-40025100">[-]</label><label class="expand" for="c-40025100">[8 more]</label></div><br/><div class="children"><div class="content">Can you elaborate? I am currently beating my head against this.<p>If I give GPT4 a list of existing items with a defined structure, and it is just having to convert schema or something like that to JSON, it can do that all day long. But if it has to do any sort of reasoning and basically create its own list, it only gives me a very limited subset.<p>I have similar issues with other LLMs.<p>Very interested in how you are approaching this.</div><br/><div id="40025177" class="c"><input type="checkbox" id="c-40025177" checked=""/><div class="controls bullet"><span class="by">msp26</span><span>|</span><a href="#40024950">root</a><span>|</span><a href="#40025100">parent</a><span>|</span><a href="#40025220">next</a><span>|</span><label class="collapse" for="c-40025177">[-]</label><label class="expand" for="c-40025177">[4 more]</label></div><br/><div class="children"><div class="content">If you show your task&#x2F;prompt with an example I&#x27;ll see if I can fix it and explain my steps.<p>Are you using the function calling&#x2F;tool use API?</div><br/><div id="40028092" class="c"><input type="checkbox" id="c-40028092" checked=""/><div class="controls bullet"><span class="by">7thpower</span><span>|</span><a href="#40024950">root</a><span>|</span><a href="#40025177">parent</a><span>|</span><a href="#40025475">next</a><span>|</span><label class="collapse" for="c-40028092">[-]</label><label class="expand" for="c-40028092">[2 more]</label></div><br/><div class="children"><div class="content">Appreciate you being willing to help! It&#x27;s pretty long, mind if I email&#x2F;dm to you?</div><br/><div id="40028262" class="c"><input type="checkbox" id="c-40028262" checked=""/><div class="controls bullet"><span class="by">msp26</span><span>|</span><a href="#40024950">root</a><span>|</span><a href="#40028092">parent</a><span>|</span><a href="#40025475">next</a><span>|</span><label class="collapse" for="c-40028262">[-]</label><label class="expand" for="c-40028262">[1 more]</label></div><br/><div class="children"><div class="content">Pastebin? I don&#x27;t really want to post my personal email on this account.</div><br/></div></div></div></div><div id="40025475" class="c"><input type="checkbox" id="c-40025475" checked=""/><div class="controls bullet"><span class="by">ctxc</span><span>|</span><a href="#40024950">root</a><span>|</span><a href="#40025177">parent</a><span>|</span><a href="#40028092">prev</a><span>|</span><a href="#40025220">next</a><span>|</span><label class="collapse" for="c-40025475">[-]</label><label class="expand" for="c-40025475">[1 more]</label></div><br/><div class="children"><div class="content">Hi! My work is similar and I&#x27;d love to have someone to bounce ideas off of if you don&#x27;t mind.<p>Your profile doesn&#x27;t have contact info though. Mine does, please send me a message. :)</div><br/></div></div></div></div><div id="40025220" class="c"><input type="checkbox" id="c-40025220" checked=""/><div class="controls bullet"><span class="by">thibaut_barrere</span><span>|</span><a href="#40024950">root</a><span>|</span><a href="#40025100">parent</a><span>|</span><a href="#40025177">prev</a><span>|</span><a href="#40025462">next</a><span>|</span><label class="collapse" for="c-40025220">[-]</label><label class="expand" for="c-40025220">[3 more]</label></div><br/><div class="children"><div class="content">Not sure if that fits the bill, but here is an example with 200 sorted items based on a question (example with Elixir &amp; InstructorEx):<p><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;thbar&#x2F;a53123cbe7765219c1eca77e03e67577" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;thbar&#x2F;a53123cbe7765219c1eca77e03e675...</a></div><br/><div id="40027301" class="c"><input type="checkbox" id="c-40027301" checked=""/><div class="controls bullet"><span class="by">sebastiennight</span><span>|</span><a href="#40024950">root</a><span>|</span><a href="#40025220">parent</a><span>|</span><a href="#40025462">next</a><span>|</span><label class="collapse" for="c-40027301">[-]</label><label class="expand" for="c-40027301">[2 more]</label></div><br/><div class="children"><div class="content">There are a few improvements I&#x27;d suggest with that prompt if you want to maximise its performance.<p>1. You&#x27;re really asking for hallucinations here. Asking for factual data is very unreliable, and not what these models are strong at. I&#x27;m curious how close&#x2F;far the results are from ground truth.<p>I would definitely bet that outside of the top 5, numbers would be wobbly and outside of top... 25?, even the ranking would be difficult to trust. Why not just get this from a more trustworthy source?[0]<p>2. Asking in French might, in my experience, give you results that are not as solid as asking in English. Unless you&#x27;re asking for a creative task where the model might get confused with EN instructions requiring an FR result, it might be better to ask in EN. And you&#x27;ll save tokens.<p>3. Providing the model with a rough example of your output JSON seems to perform  better than describing the JSON in plan language.<p>[0]: <a href="https:&#x2F;&#x2F;fr.wikipedia.org&#x2F;wiki&#x2F;Liste_des_communes_de_France_les_plus_peupl%C3%A9es" rel="nofollow">https:&#x2F;&#x2F;fr.wikipedia.org&#x2F;wiki&#x2F;Liste_des_communes_de_France_l...</a></div><br/><div id="40029539" class="c"><input type="checkbox" id="c-40029539" checked=""/><div class="controls bullet"><span class="by">thibaut_barrere</span><span>|</span><a href="#40024950">root</a><span>|</span><a href="#40027301">parent</a><span>|</span><a href="#40025462">next</a><span>|</span><label class="collapse" for="c-40029539">[-]</label><label class="expand" for="c-40029539">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the suggestions, appreciated!<p>For some context, this snippet is just an educational demo to show what can be done with regard to structured output &amp; data types validation.<p>Re 1: for more advanced cases (using the exact same stack), I am using ensemble techniques &amp; automated comparisons to double-check, and so far this has really well protected the app from hallucinations. I am definitely careful with this (but point well taken).<p>2&#x2F;3: agreed overall! Apart from this example, I am using French only where it make sense. It make sense when the target is directly French students, for instance, or when the domain model (e.g. French literature) makes it really relevant (and translating would be worst than directly using French).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40025462" class="c"><input type="checkbox" id="c-40025462" checked=""/><div class="controls bullet"><span class="by">Civitello</span><span>|</span><a href="#40024950">prev</a><span>|</span><a href="#40026067">next</a><span>|</span><label class="collapse" for="c-40025462">[-]</label><label class="expand" for="c-40025462">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Every use case we have is essentially “Here’s a block of text, extract something from it.” As a rule, if you ask GPT to give you the names of companies mentioned in a block of text, it will not give you a random company (unless there are no companies in the text – there’s that null hypothesis problem!).
Make it two steps, first:
&gt; Does this block of text mention a company?
If no, good you&#x27;ve got your null result.
If yes:
&gt; Please list the names of companies in this block of text.</div><br/></div></div><div id="40026067" class="c"><input type="checkbox" id="c-40026067" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#40025462">prev</a><span>|</span><a href="#40024033">next</a><span>|</span><label class="collapse" for="c-40026067">[-]</label><label class="expand" for="c-40026067">[1 more]</label></div><br/><div class="children"><div class="content">Tip for your &#x27;null&#x27; problem:<p>LLMs are set up to output tokens. Not to not output tokens.<p>So instead of &quot;don&#x27;t return anything&quot; have the lack of results &quot;return the default value of XYZ&quot; and then just do a text search on the result for that default value (i.e. XYZ) the same way you do the text search for the state names.<p>Also, system prompts can be very useful. It&#x27;s basically your opportunity to have the LLM roleplay as X. I wish they&#x27;d let the system prompt be passed directly, but it&#x27;s still better than nothing.</div><br/></div></div><div id="40024033" class="c"><input type="checkbox" id="c-40024033" checked=""/><div class="controls bullet"><span class="by">trolan</span><span>|</span><a href="#40026067">prev</a><span>|</span><a href="#40024589">next</a><span>|</span><label class="collapse" for="c-40024033">[-]</label><label class="expand" for="c-40024033">[10 more]</label></div><br/><div class="children"><div class="content">For a few uni&#x2F;personal projects I noticed the same about Langchain: it&#x27;s good at helping you use up tokens. The other use case, quickly switching between models, is a very valid reason still. However, I&#x27;ve recently started playing with OpenRouter which seems to abstract the model nicely.</div><br/><div id="40024150" class="c"><input type="checkbox" id="c-40024150" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#40024033">parent</a><span>|</span><a href="#40024589">next</a><span>|</span><label class="collapse" for="c-40024150">[-]</label><label class="expand" for="c-40024150">[9 more]</label></div><br/><div class="children"><div class="content">If someone were to create something new, a blank slate approach, what would you find valuable and why?</div><br/><div id="40024189" class="c"><input type="checkbox" id="c-40024189" checked=""/><div class="controls bullet"><span class="by">lordofmoria</span><span>|</span><a href="#40024033">root</a><span>|</span><a href="#40024150">parent</a><span>|</span><a href="#40024987">next</a><span>|</span><label class="collapse" for="c-40024189">[-]</label><label class="expand" for="c-40024189">[4 more]</label></div><br/><div class="children"><div class="content">This is a great question!<p>I think we now know, collectively, a lot more about what’s annoying&#x2F;hard about building LLM features than we did when LangChain was being furiously developed.<p>And some things we thought would be important and not-easy, turned out to be very easy: like getting GPT to give back well-formed JSON.<p>So I think there’s lots of room.<p>One thing LangChain is doing now that solves something that IS very hard&#x2F;annoying is testing. I spent 30 minutes yesterday re-running a slow prompt because 1 in 5 runs would produce weird output. Each tweak to the prompt, I had to run at least 10 times to be reasonably sure it was an improvement.</div><br/><div id="40024822" class="c"><input type="checkbox" id="c-40024822" checked=""/><div class="controls bullet"><span class="by">codewithcheese</span><span>|</span><a href="#40024033">root</a><span>|</span><a href="#40024189">parent</a><span>|</span><a href="#40025970">next</a><span>|</span><label class="collapse" for="c-40024822">[-]</label><label class="expand" for="c-40024822">[2 more]</label></div><br/><div class="children"><div class="content">It can be faster and more effective to fallback to a smaller model (gpt3.5 or haiku), the weakness of the prompt will be more obvious on a smaller model and your iteration time will be faster</div><br/><div id="40028563" class="c"><input type="checkbox" id="c-40028563" checked=""/><div class="controls bullet"><span class="by">JeremyHerrman</span><span>|</span><a href="#40024033">root</a><span>|</span><a href="#40024822">parent</a><span>|</span><a href="#40025970">next</a><span>|</span><label class="collapse" for="c-40028563">[-]</label><label class="expand" for="c-40028563">[1 more]</label></div><br/><div class="children"><div class="content">great insight!</div><br/></div></div></div></div><div id="40025970" class="c"><input type="checkbox" id="c-40025970" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#40024033">root</a><span>|</span><a href="#40024189">parent</a><span>|</span><a href="#40024822">prev</a><span>|</span><a href="#40024987">next</a><span>|</span><label class="collapse" for="c-40025970">[-]</label><label class="expand" for="c-40025970">[1 more]</label></div><br/><div class="children"><div class="content">How would testing work out ideally?</div><br/></div></div></div></div><div id="40024987" class="c"><input type="checkbox" id="c-40024987" checked=""/><div class="controls bullet"><span class="by">jsemrau</span><span>|</span><a href="#40024033">root</a><span>|</span><a href="#40024150">parent</a><span>|</span><a href="#40024189">prev</a><span>|</span><a href="#40024589">next</a><span>|</span><label class="collapse" for="c-40024987">[-]</label><label class="expand" for="c-40024987">[4 more]</label></div><br/><div class="children"><div class="content">Use a local model. For most tasks they are good enough. Let&#x27;s say Mistral 0.2 instruct is quite solid by now.</div><br/><div id="40025445" class="c"><input type="checkbox" id="c-40025445" checked=""/><div class="controls bullet"><span class="by">gnat</span><span>|</span><a href="#40024033">root</a><span>|</span><a href="#40024987">parent</a><span>|</span><a href="#40025752">next</a><span>|</span><label class="collapse" for="c-40025445">[-]</label><label class="expand" for="c-40025445">[2 more]</label></div><br/><div class="children"><div class="content">Do different versions react to  prompts in the same way? I imagined the prompt would be tailored to the quirks of a particular version rather than naturally being stably optimal across versions.</div><br/><div id="40025493" class="c"><input type="checkbox" id="c-40025493" checked=""/><div class="controls bullet"><span class="by">jsemrau</span><span>|</span><a href="#40024033">root</a><span>|</span><a href="#40025445">parent</a><span>|</span><a href="#40025752">next</a><span>|</span><label class="collapse" for="c-40025493">[-]</label><label class="expand" for="c-40025493">[1 more]</label></div><br/><div class="children"><div class="content">I suppose that is one of the benefits of using a local model, that it reduces model risk. I.e., given a certain prompt, it should always reply in the same way. Using a hosted model, operationally you don&#x27;t have that control over model risk.</div><br/></div></div></div></div><div id="40025752" class="c"><input type="checkbox" id="c-40025752" checked=""/><div class="controls bullet"><span class="by">cpursley</span><span>|</span><a href="#40024033">root</a><span>|</span><a href="#40024987">parent</a><span>|</span><a href="#40025445">prev</a><span>|</span><a href="#40024589">next</a><span>|</span><label class="collapse" for="c-40025752">[-]</label><label class="expand" for="c-40025752">[1 more]</label></div><br/><div class="children"><div class="content">What are the best local&#x2F;open models for accurate tool-calling?</div><br/></div></div></div></div></div></div></div></div><div id="40024589" class="c"><input type="checkbox" id="c-40024589" checked=""/><div class="controls bullet"><span class="by">swalsh</span><span>|</span><a href="#40024033">prev</a><span>|</span><a href="#40025162">next</a><span>|</span><label class="collapse" for="c-40024589">[-]</label><label class="expand" for="c-40024589">[1 more]</label></div><br/><div class="children"><div class="content">The being too precise reduces accuracy example makes sense to me based on my crude understanding on how these things work.<p>If you pass in a whole list of states, you&#x27;re kind of making the vectors for every state light up.  If you just say &quot;state&quot; and the text you passed in has an explicit state, than fewer vectors specific to what you&#x27;re searching for light up.  So when it performs the soft max, the correct state is more likely to be selected.<p>Along the same lines I think his &#x2F;n vs comma comparison probably comes down to tokenization differences.</div><br/></div></div><div id="40025162" class="c"><input type="checkbox" id="c-40025162" checked=""/><div class="controls bullet"><span class="by">dougb5</span><span>|</span><a href="#40024589">prev</a><span>|</span><a href="#40025352">next</a><span>|</span><label class="collapse" for="c-40025162">[-]</label><label class="expand" for="c-40025162">[3 more]</label></div><br/><div class="children"><div class="content">The lessons I wanted from this article weren&#x27;t in there:  Did all of that expenditure actually help their product in a measurable way?   Did customers use and appreciate the new features based on LLM summarization compared to whatever they were using before?  I presume it&#x27;s a net win or they wouldn&#x27;t continue to use it, but more specifics around the application would be helpful.</div><br/><div id="40025542" class="c"><input type="checkbox" id="c-40025542" checked=""/><div class="controls bullet"><span class="by">lordofmoria</span><span>|</span><a href="#40025162">parent</a><span>|</span><a href="#40025352">next</a><span>|</span><label class="collapse" for="c-40025542">[-]</label><label class="expand" for="c-40025542">[2 more]</label></div><br/><div class="children"><div class="content">Hey, OP here!<p>The answer is a bit boring: the expenditure definitely has helped customers - in that, they&#x27;re using AI generated responses in all their work flows all the time in the app, and barely notice it.<p>See what I did there? :) I&#x27;m mostly serious though - one weird thing about our app is that you might not even know we&#x27;re using AI, unless we literally tell you in the app.<p>And I think that&#x27;s where we&#x27;re at with AI and LLMs these days, at least for our use case.<p>You might find this other post I just put up to have more details too, related to how&#x2F;where I see the primary value: <a href="https:&#x2F;&#x2F;kenkantzer.com&#x2F;gpt-is-the-heroku-of-ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;kenkantzer.com&#x2F;gpt-is-the-heroku-of-ai&#x2F;</a></div><br/><div id="40028618" class="c"><input type="checkbox" id="c-40028618" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#40025162">root</a><span>|</span><a href="#40025542">parent</a><span>|</span><a href="#40025352">next</a><span>|</span><label class="collapse" for="c-40028618">[-]</label><label class="expand" for="c-40028618">[1 more]</label></div><br/><div class="children"><div class="content">Can you provide some more detail about the application?  I&#x27;m not familiar with how llms are used in business, except as customer support bots returning documentation.</div><br/></div></div></div></div></div></div><div id="40025352" class="c"><input type="checkbox" id="c-40025352" checked=""/><div class="controls bullet"><span class="by">FranklinMaillot</span><span>|</span><a href="#40025162">prev</a><span>|</span><a href="#40027634">next</a><span>|</span><label class="collapse" for="c-40025352">[-]</label><label class="expand" for="c-40025352">[2 more]</label></div><br/><div class="children"><div class="content">In my limited experience, I came to the same conclusion regarding simple prompt being more efficient than very detailed list of instructions. But if you look at OpenAI&#x27;s system prompt for GPT4, it&#x27;s an endless set of instructions with DOs and DONTs so I&#x27;m confused. Surely they must know something about prompting their model.</div><br/><div id="40026050" class="c"><input type="checkbox" id="c-40026050" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#40025352">parent</a><span>|</span><a href="#40027634">next</a><span>|</span><label class="collapse" for="c-40026050">[-]</label><label class="expand" for="c-40026050">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s for chatting and interfacing conversationally with a human. Using the API is a completely different ballgame because it&#x27;s not meant to be a back and forth conversation with a human.</div><br/></div></div></div></div><div id="40027634" class="c"><input type="checkbox" id="c-40027634" checked=""/><div class="controls bullet"><span class="by">pamelafox</span><span>|</span><a href="#40025352">prev</a><span>|</span><a href="#40024664">next</a><span>|</span><label class="collapse" for="c-40027634">[-]</label><label class="expand" for="c-40027634">[1 more]</label></div><br/><div class="children"><div class="content">Lol, nice truncation logic! If anyone’s looking for something slightly fancier, I made a micro-package for our tiktoken-based truncation here: <a href="https:&#x2F;&#x2F;github.com&#x2F;pamelafox&#x2F;llm-messages-token-helper">https:&#x2F;&#x2F;github.com&#x2F;pamelafox&#x2F;llm-messages-token-helper</a></div><br/></div></div><div id="40024664" class="c"><input type="checkbox" id="c-40024664" checked=""/><div class="controls bullet"><span class="by">legendofbrando</span><span>|</span><a href="#40027634">prev</a><span>|</span><a href="#40027650">next</a><span>|</span><label class="collapse" for="c-40024664">[-]</label><label class="expand" for="c-40024664">[1 more]</label></div><br/><div class="children"><div class="content">The finding on simpler prompts, especially with GPT4 tracks (3.5 requires the opposite).<p>The take on RAG feels application specific. For our use-case where having details of the past rendered up the ability to generate loose connections is actually a feature. Things like this are what I find excites me most about LLMs, having a way to proxy subjective similarities the way we do when we remember things is one of the benefits of the technology that didn’t really exist before that opens up a new kind of product opportunity.</div><br/></div></div><div id="40027650" class="c"><input type="checkbox" id="c-40027650" checked=""/><div class="controls bullet"><span class="by">pamelafox</span><span>|</span><a href="#40024664">prev</a><span>|</span><a href="#40029295">next</a><span>|</span><label class="collapse" for="c-40027650">[-]</label><label class="expand" for="c-40027650">[1 more]</label></div><br/><div class="children"><div class="content">I’ve also seen that GPTs struggle to admit when they dont know. I wrote up an approach for evaluating that here - <a href="http:&#x2F;&#x2F;blog.pamelafox.org&#x2F;2024&#x2F;03&#x2F;evaluating-rag-chat-apps-can-your-app.html?m=1" rel="nofollow">http:&#x2F;&#x2F;blog.pamelafox.org&#x2F;2024&#x2F;03&#x2F;evaluating-rag-chat-apps-c...</a><p>Changing the prompt didn&#x27;t help, but moving to GPT-4 did help a bit.</div><br/></div></div><div id="40029295" class="c"><input type="checkbox" id="c-40029295" checked=""/><div class="controls bullet"><span class="by">Kiro</span><span>|</span><a href="#40027650">prev</a><span>|</span><a href="#40027292">next</a><span>|</span><label class="collapse" for="c-40029295">[-]</label><label class="expand" for="c-40029295">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We always extract json. We don’t need JSON mode,<p>Why? The null stuff would not be a problem if you did and if you&#x27;re only dealing with JSON anyway I don&#x27;t see why you wouldn&#x27;t.</div><br/></div></div><div id="40027292" class="c"><input type="checkbox" id="c-40027292" checked=""/><div class="controls bullet"><span class="by">konstantinua00</span><span>|</span><a href="#40029295">prev</a><span>|</span><a href="#40028823">next</a><span>|</span><label class="collapse" for="c-40027292">[-]</label><label class="expand" for="c-40027292">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Have you tried Claude, Gemini, etc?<p>&gt; It’s the subtle things mostly, like intuiting intention.<p>this makes me wonder - what if the author &quot;trained&quot; himself onto chatgpt&#x27;s &quot;dialect&quot;? How do we even detect that in ourselves?<p>and are we about to have &quot;preferred_LLM wars&quot; like we had &quot;programming language wars&quot; for the last 2 decades?</div><br/></div></div><div id="40025067" class="c"><input type="checkbox" id="c-40025067" checked=""/><div class="controls bullet"><span class="by">larodi</span><span>|</span><a href="#40028823">prev</a><span>|</span><a href="#40024368">next</a><span>|</span><label class="collapse" for="c-40025067">[-]</label><label class="expand" for="c-40025067">[4 more]</label></div><br/><div class="children"><div class="content">Agree largely with author, but this ‘wait for OpenAI to do it’ sentiment is not something valid. Opus for example is already much better (not only per my experience, but like… researchers evaluaiton). And
even for the fun of it - try some local inference, boy. If u know how to prompt it you definitely would be able to run local for the same tasks.<p>Like listening to my students all going to ‘call some API’ for their projects is really very sad to hear. Many startup fellows share this sentiment which a totally kills all the joy.</div><br/><div id="40026125" class="c"><input type="checkbox" id="c-40026125" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#40025067">parent</a><span>|</span><a href="#40025354">next</a><span>|</span><label class="collapse" for="c-40026125">[-]</label><label class="expand" for="c-40026125">[1 more]</label></div><br/><div class="children"><div class="content">Claude does have more of a hallucination problem than GPT-4, and a less robust knowledge base.<p>It&#x27;s much better at critical thinking tasks and prose.<p>Don&#x27;t mistake benchmarks for real world performance across actual usecases. There&#x27;s a bit of Goodhart&#x27;s Law going on with LLM evaluation and optimization.</div><br/></div></div><div id="40025354" class="c"><input type="checkbox" id="c-40025354" checked=""/><div class="controls bullet"><span class="by">jstummbillig</span><span>|</span><a href="#40025067">parent</a><span>|</span><a href="#40026125">prev</a><span>|</span><a href="#40024368">next</a><span>|</span><label class="collapse" for="c-40025354">[-]</label><label class="expand" for="c-40025354">[2 more]</label></div><br/><div class="children"><div class="content">It sounds like you are a tech educator, which potentially sound like a lot of fun with llms right now.<p>When you are integrating these things into your business, you are looking for different things. Most of our customers would for example not find it very cool to have a service outage because somebody wanted to not kill all the joy.</div><br/><div id="40026914" class="c"><input type="checkbox" id="c-40026914" checked=""/><div class="controls bullet"><span class="by">larodi</span><span>|</span><a href="#40025067">root</a><span>|</span><a href="#40025354">parent</a><span>|</span><a href="#40024368">next</a><span>|</span><label class="collapse" for="c-40026914">[-]</label><label class="expand" for="c-40026914">[1 more]</label></div><br/><div class="children"><div class="content">Sure, when availability and SLA kicks in…, but reselling APIs will only get you that far. Perhaps the whole pro&#x2F;cons cloud argument can also kick in here, not going into it. We may well be on the same page, or we both perhaps have valid arguments. Your comment is appreciated indeed.<p>But then is the author (and are we) talking experience in reselling APIs or experience in introducing NNs in the pipeline? Not the same thing IMHO.<p>Agreed that OpenAI provides very good service, Gemini is not quite there yet, Groq (the LPUs) delivered a nice tech demo, Mixtral is cool but lacks in certain areas, and Claude can be lengthy.<p>But precisely because I’m not sticking with OAI I can then  restate my view that if someone is so good with prompts he can get the same results locally if he knows what he’s doing.<p>Prompting OpenAI the right way can be similarly difficult.<p>Perhaps the whole idea of local inference only matters for IoT scenarios or whenever data is super sensitive (or CTO super stubborn to let it embed and fly). But then if you start from day 1 with WordPress provisioned for you ready to go in Google Cloud, you’d never understand the  underlying details of the technology.<p>There sure also must be a good reason why Phind tuned their own thing to offer alongside GPT4 APIs.<p>Disclaimer: tech education is a side thing I do, indeed, and been doing in person for very long time, more than dozen topics, to allow myself to have opinion. Of course business is different matter and strategic decisions arr not the same. Even though I’d not advise anyone to blindly use APIs unless they appreciate the need properly.</div><br/></div></div></div></div></div></div><div id="40024368" class="c"><input type="checkbox" id="c-40024368" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#40025067">prev</a><span>|</span><a href="#40024104">next</a><span>|</span><label class="collapse" for="c-40024368">[-]</label><label class="expand" for="c-40024368">[9 more]</label></div><br/><div class="children"><div class="content">I agree with most of it, but definitely not the part about Claude3 being “meh.” Claude3 Opus is an amazing model and is extremely good at coding in Python. The ability to handle massive context has made it mostly replace GPT4 for me day to day.<p>Sounds like everyone eventually concludes that Langchain is bloated and useless and creates way more problems than it solves. I don’t get the hype.</div><br/><div id="40024430" class="c"><input type="checkbox" id="c-40024430" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#40024368">parent</a><span>|</span><a href="#40024596">next</a><span>|</span><label class="collapse" for="c-40024430">[-]</label><label class="expand" for="c-40024430">[1 more]</label></div><br/><div class="children"><div class="content">Claude is indeed an amazing model, the fact that Sonnet and Haiku are so good is a game changer - GPT4 is too expensive and GPT3.5 is very mediocre.  Getting 95% of GPT4 performance for GPT3.5 prices feels like cheating.</div><br/></div></div><div id="40024596" class="c"><input type="checkbox" id="c-40024596" checked=""/><div class="controls bullet"><span class="by">Oras</span><span>|</span><a href="#40024368">parent</a><span>|</span><a href="#40024430">prev</a><span>|</span><a href="#40024104">next</a><span>|</span><label class="collapse" for="c-40024596">[-]</label><label class="expand" for="c-40024596">[7 more]</label></div><br/><div class="children"><div class="content">+1 for Claude Opus, it had been my go to for the last 3 weeks compared to GPT4. The generated texts are much better than GPT4 when it comes to follow the prompt.<p>I also tried the API for some financial analysis of large tables,  the response time was around 2 minutes, still did it really well and timeout errors were around 1 to 2% only.</div><br/><div id="40025786" class="c"><input type="checkbox" id="c-40025786" checked=""/><div class="controls bullet"><span class="by">cpursley</span><span>|</span><a href="#40024368">root</a><span>|</span><a href="#40024596">parent</a><span>|</span><a href="#40024104">next</a><span>|</span><label class="collapse" for="c-40025786">[-]</label><label class="expand" for="c-40025786">[6 more]</label></div><br/><div class="children"><div class="content">How are you sending tabular data in a reliable way. And what is the source document type? I&#x27;m trying to solve this for complex financial-related tables in PDFs right now.</div><br/><div id="40025875" class="c"><input type="checkbox" id="c-40025875" checked=""/><div class="controls bullet"><span class="by">Oras</span><span>|</span><a href="#40024368">root</a><span>|</span><a href="#40025786">parent</a><span>|</span><a href="#40024104">next</a><span>|</span><label class="collapse" for="c-40025875">[-]</label><label class="expand" for="c-40025875">[5 more]</label></div><br/><div class="children"><div class="content">Amazon Textract, to get tables, format them with Python as csv then send to your preferred AI model.</div><br/><div id="40026445" class="c"><input type="checkbox" id="c-40026445" checked=""/><div class="controls bullet"><span class="by">cpursley</span><span>|</span><a href="#40024368">root</a><span>|</span><a href="#40025875">parent</a><span>|</span><a href="#40024104">next</a><span>|</span><label class="collapse" for="c-40026445">[-]</label><label class="expand" for="c-40026445">[4 more]</label></div><br/><div class="children"><div class="content">Thanks. How does Textract compare to come of the common cli utilities like pdftotext, tesseract, etc (if you made a comparison)?</div><br/><div id="40026483" class="c"><input type="checkbox" id="c-40026483" checked=""/><div class="controls bullet"><span class="by">Oras</span><span>|</span><a href="#40024368">root</a><span>|</span><a href="#40026445">parent</a><span>|</span><a href="#40024104">next</a><span>|</span><label class="collapse" for="c-40026483">[-]</label><label class="expand" for="c-40026483">[3 more]</label></div><br/><div class="children"><div class="content">I did, none of the open source parser worked well with tables. I had the following issues:<p>- missing cells.
- partial identification for number (ex: £43.54, the parser would pick it up as £43).<p>What I did to compare is drawing lines around identified text to visualize the accuracy. You can do that with tesseract.</div><br/><div id="40026936" class="c"><input type="checkbox" id="c-40026936" checked=""/><div class="controls bullet"><span class="by">cpursley</span><span>|</span><a href="#40024368">root</a><span>|</span><a href="#40026483">parent</a><span>|</span><a href="#40024104">next</a><span>|</span><label class="collapse" for="c-40026936">[-]</label><label class="expand" for="c-40026936">[2 more]</label></div><br/><div class="children"><div class="content">Interesting. Did you try MS&#x27;s offering (Azure AI Document Intelligence). Their pricing seems better than Amazon.</div><br/><div id="40029617" class="c"><input type="checkbox" id="c-40029617" checked=""/><div class="controls bullet"><span class="by">Oras</span><span>|</span><a href="#40024368">root</a><span>|</span><a href="#40026936">parent</a><span>|</span><a href="#40024104">next</a><span>|</span><label class="collapse" for="c-40029617">[-]</label><label class="expand" for="c-40029617">[1 more]</label></div><br/><div class="children"><div class="content">Not yet but planning to give it a try and compare with textract.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="40024104" class="c"><input type="checkbox" id="c-40024104" checked=""/><div class="controls bullet"><span class="by">disqard</span><span>|</span><a href="#40024368">prev</a><span>|</span><a href="#40024192">next</a><span>|</span><label class="collapse" for="c-40024104">[-]</label><label class="expand" for="c-40024104">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This worked sometimes (I’d estimate &gt;98% of the time), but failed enough that we had to dig deeper.<p>&gt; While we were investigating, we noticed that another field, name, was consistently returning the full name of the state…the correct state – even though we hadn’t explicitly asked it to do that.<p>&gt; So we switched to a simple string search on the name to find the state, and it’s been working beautifully ever since.<p>So, using ChatGPT helped uncover the correct schema, right?</div><br/></div></div><div id="40024192" class="c"><input type="checkbox" id="c-40024192" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#40024104">prev</a><span>|</span><a href="#40024770">next</a><span>|</span><label class="collapse" for="c-40024192">[-]</label><label class="expand" for="c-40024192">[1 more]</label></div><br/><div class="children"><div class="content">I feel like for just extracting data into JSON, smaller LLMs could probably do fine, especially with constrained generation and training on extraction.</div><br/></div></div><div id="40024770" class="c"><input type="checkbox" id="c-40024770" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#40024192">prev</a><span>|</span><a href="#40027584">next</a><span>|</span><label class="collapse" for="c-40024770">[-]</label><label class="expand" for="c-40024770">[1 more]</label></div><br/><div class="children"><div class="content">I recently had a bug where I was sometimes sending the literal text &quot;null &quot; right in front of the most important part of my prompt. This caused Claude 3 Sonnet to give the &#x27;ignore&#x27; command in cases where it should have used one of the other JSON commands I gave it.<p>I have an ignore command so that it will wait when the user isn&#x27;t finished speaking. Which it generally judges okay, unless it has &#x27;null&#x27; in there.<p>The nice thing is that I have found most of the problems with the LLM response were just indications that I hadn&#x27;t finished debugging my program because I had something missing or weird in the prompt I gave it.</div><br/></div></div><div id="40027584" class="c"><input type="checkbox" id="c-40027584" checked=""/><div class="controls bullet"><span class="by">aubanel</span><span>|</span><a href="#40024770">prev</a><span>|</span><a href="#40015377">next</a><span>|</span><label class="collapse" for="c-40027584">[-]</label><label class="expand" for="c-40027584">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I think in summary, a better approach would’ve been “You obviously know the 50 states, GPT, so just give me the full name of the state this pertains to, or Federal if this pertains to the US government.”<p>Why not really compare the two options, author? I would love to see the results!</div><br/></div></div><div id="40015377" class="c"><input type="checkbox" id="c-40015377" checked=""/><div class="controls bullet"><span class="by">Yacovlewis</span><span>|</span><a href="#40027584">prev</a><span>|</span><a href="#40025284">next</a><span>|</span><label class="collapse" for="c-40015377">[-]</label><label class="expand" for="c-40015377">[4 more]</label></div><br/><div class="children"><div class="content">Interesting piece!<p>My experience around Langchain&#x2F;RAG differs, so wanted to dig deeper:
Putting some logic around handling relevant results helps us produce useful output. Curious what differs on their end.</div><br/><div id="40024068" class="c"><input type="checkbox" id="c-40024068" checked=""/><div class="controls bullet"><span class="by">mind-blight</span><span>|</span><a href="#40015377">parent</a><span>|</span><a href="#40025284">next</a><span>|</span><label class="collapse" for="c-40024068">[-]</label><label class="expand" for="c-40024068">[3 more]</label></div><br/><div class="children"><div class="content">I suspect the biggest difference is the input data. Embeddings are great over datasets that look like FAQs and QA docs, or data that conceptually fits into very small chunks (tweets, some product reviews, etc).<p>It does very badly over diverse business docs, especially with naive chunking. B2B use cases usually have old PDFs and word docs that need to be searched, and they&#x27;re often looking for specific keywords (e.g. a person&#x27;s name, a product, an id, etc). Vectors terms to do badly in those kinds of searches, and just returning chunks misses a lot of important details</div><br/><div id="40024571" class="c"><input type="checkbox" id="c-40024571" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#40015377">root</a><span>|</span><a href="#40024068">parent</a><span>|</span><a href="#40025284">next</a><span>|</span><label class="collapse" for="c-40024571">[-]</label><label class="expand" for="c-40024571">[2 more]</label></div><br/><div class="children"><div class="content">rare words are out of vocab errors in vectors<p>Especially if they aren’t in the token vocab</div><br/><div id="40025978" class="c"><input type="checkbox" id="c-40025978" checked=""/><div class="controls bullet"><span class="by">mind-blight</span><span>|</span><a href="#40015377">root</a><span>|</span><a href="#40024571">parent</a><span>|</span><a href="#40025284">next</a><span>|</span><label class="collapse" for="c-40025978">[-]</label><label class="expand" for="c-40025978">[1 more]</label></div><br/><div class="children"><div class="content">Even worse, named entities vary from organization to organization.<p>We have a client who uses a product called &quot;Time&quot;. It&#x27;s software time management. For that customer&#x27;s documentation, time should be close to &quot;product&quot; and a bunch of other things that have nothing to do with the normal concept of time.<p>I actually suspect that people would get a lot more bang for their buck fine tuning the embedding models on B2B datasets for their use case, rather than fine tuning an llm</div><br/></div></div></div></div></div></div></div></div><div id="40025284" class="c"><input type="checkbox" id="c-40025284" checked=""/><div class="controls bullet"><span class="by">haolez</span><span>|</span><a href="#40015377">prev</a><span>|</span><a href="#40025525">next</a><span>|</span><label class="collapse" for="c-40025284">[-]</label><label class="expand" for="c-40025284">[1 more]</label></div><br/><div class="children"><div class="content">That has been my experience too. The null hypothesis explains almost all of my hallucinations.<p>I just don&#x27;t agree with the Claude assessment. In my experience, Claude 3 Opus is vastly superior to GPT-4. Maybe the author was comparing with Claude 2? (And I&#x27;ve never tested Gemini)</div><br/></div></div><div id="40025593" class="c"><input type="checkbox" id="c-40025593" checked=""/><div class="controls bullet"><span class="by">sungho_</span><span>|</span><a href="#40025525">prev</a><span>|</span><a href="#40024454">next</a><span>|</span><label class="collapse" for="c-40025593">[-]</label><label class="expand" for="c-40025593">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious if the OP has tried any of the libraries that control the output of LLM (LMQL, Outliner, Guadiance, ...), and for those who have: do you find them as unnecessary as LangChain? In particular, the OP&#x27;s post mentions the problem of not being able to generate JSON with more than 15 items, which seems like a problem that can be solved by controlling the output of LLM. Is that correct?</div><br/><div id="40025620" class="c"><input type="checkbox" id="c-40025620" checked=""/><div class="controls bullet"><span class="by">LASR</span><span>|</span><a href="#40025593">parent</a><span>|</span><a href="#40024454">next</a><span>|</span><label class="collapse" for="c-40025620">[-]</label><label class="expand" for="c-40025620">[1 more]</label></div><br/><div class="children"><div class="content">If you want x number of items every time, ask it to include a sequence number in each output, it will consistently return x number of items.<p>Numbered bullets work well for this, if you don’t need JSON. With JSON, you can ask it to include an ‘id’ in each item.</div><br/></div></div></div></div><div id="40024454" class="c"><input type="checkbox" id="c-40024454" checked=""/><div class="controls bullet"><span class="by">albert_e</span><span>|</span><a href="#40025593">prev</a><span>|</span><label class="collapse" for="c-40024454">[-]</label><label class="expand" for="c-40024454">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Are we going to achieve Gen AI?<p>&gt; No. Not with this transformers + the data of the internet + $XB infrastructure approach.<p>Errr ...did they really mean Gen AI .. or AGI?</div><br/><div id="40024587" class="c"><input type="checkbox" id="c-40024587" checked=""/><div class="controls bullet"><span class="by">mdorazio</span><span>|</span><a href="#40024454">parent</a><span>|</span><label class="collapse" for="c-40024587">[-]</label><label class="expand" for="c-40024587">[1 more]</label></div><br/><div class="children"><div class="content">Gen as in “General” not generative.</div><br/></div></div></div></div></div></div></div></div></div></body></html>