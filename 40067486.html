<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1713430860368" as="style"/><link rel="stylesheet" href="styles.css?v=1713430860368"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://bawolf.substack.com/p/embeddings-are-a-good-starting-point">Embeddings are a good starting point for the AI curious app developer</a> <span class="domain">(<a href="https://bawolf.substack.com">bawolf.substack.com</a>)</span></div><div class="subtext"><span>bryantwolf</span> | <span>159 comments</span></div><br/><div><div id="40067955" class="c"><input type="checkbox" id="c-40067955" checked=""/><div class="controls bullet"><span class="by">thisiszilff</span><span>|</span><a href="#40070542">next</a><span>|</span><label class="collapse" for="c-40067955">[-]</label><label class="expand" for="c-40067955">[38 more]</label></div><br/><div class="children"><div class="content">One straightforward way to get started is to understand embedding without any AI&#x2F;deep learning magic. Just pick a vocabulary of words (say, some 50k words), pick a unique index between 0 and 49,999 for each of the words, and then produce embedding by adding +1 to the given index for a given word each time it occurs in a text. Then normalize the embedding so it adds up to one.<p>Presto -- embeddings! And you can use cosine similarity with them and all that good stuff and the results aren&#x27;t totally terrible.<p>The rest of &quot;embeddings&quot; builds on top of this basic strategy (smaller vectors, filtering out words&#x2F;tokens that occur frequently enough that they don&#x27;t signify similarity, handling synonyms or words that are related to one another, etc. etc.). But stripping out the deep learning bits really does make it easier to understand.</div><br/><div id="40069520" class="c"><input type="checkbox" id="c-40069520" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40067955">parent</a><span>|</span><a href="#40068262">next</a><span>|</span><label class="collapse" for="c-40069520">[-]</label><label class="expand" for="c-40069520">[14 more]</label></div><br/><div class="children"><div class="content">Those would really just be identifiers. I think the key property of embeddings is that the dimensions each individually mean&#x2F;measure something, and therefore the dot product of two embeddings (similarity of direction of the vectors) is a meaningful similarity measure of the things being represented.<p>The classic example is word embeddings such as word2vec, or GloVE, where due to the embeddings being meaningful in this way, one can see vector relationships such as &quot;man - woman&quot; = &quot;king - queen&quot;.</div><br/><div id="40070202" class="c"><input type="checkbox" id="c-40070202" checked=""/><div class="controls bullet"><span class="by">thisiszilff</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40069520">parent</a><span>|</span><a href="#40069752">next</a><span>|</span><label class="collapse" for="c-40070202">[-]</label><label class="expand" for="c-40070202">[7 more]</label></div><br/><div class="children"><div class="content">&gt; I think the key property of embeddings is that the dimensions each individually mean&#x2F;measure something, and therefore the dot product of two embeddings (similarity of direction of the vectors) is a meaningful similarity measure of the things being represented.<p>In this case each dimension is the presence of a word in a particular text. So when you take the dot product of two texts you are effectively counting the number of words the two texts have in common (subject to some normalization constants depending on how you normalize the embedding). Cosine similarity still works for even these super naive embeddings which makes it slightly easier to understand before getting into any mathy stuff.<p>You are 100% right this won&#x27;t give you the word embedding analogies like king - man = queen or stuff like that. This embedding has no concept of relationships between words.</div><br/><div id="40070381" class="c"><input type="checkbox" id="c-40070381" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40070202">parent</a><span>|</span><a href="#40070551">next</a><span>|</span><label class="collapse" for="c-40070381">[-]</label><label class="expand" for="c-40070381">[5 more]</label></div><br/><div class="children"><div class="content">But that doesn&#x27;t seem to be what you are describing in terms of using incrementing indices and adding occurrence counts.<p>If you want to create a bag of words text embedding then you set the number of embedding dimensions to the vocabulary size and the value of each dimension to the global count of the corresponding word.</div><br/><div id="40070456" class="c"><input type="checkbox" id="c-40070456" checked=""/><div class="controls bullet"><span class="by">thisiszilff</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40070381">parent</a><span>|</span><a href="#40070551">next</a><span>|</span><label class="collapse" for="c-40070456">[-]</label><label class="expand" for="c-40070456">[4 more]</label></div><br/><div class="children"><div class="content">Heh -- my explanation isn&#x27;t the clearest I realize, but yes, it is BoW.<p>Eg fix your vocab of 50k words (or whatever) and enumerate it.<p>Then to make an embedding for some piece of text<p>1. initialize an all zero vector of size 50k
2. for each word in the text, add one to the index of the corresponding word (per our enumeration). If the word isn&#x27;t in the 50k words in your vocabulary, then discard it
3. (optionally), normalize the embedding to 1 (though you don&#x27;t really need this and can leave it off for the toy example).
 initialize an embedding (for a single text) as an all zero vector of size 50k</div><br/><div id="40071005" class="c"><input type="checkbox" id="c-40071005" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40070456">parent</a><span>|</span><a href="#40070551">next</a><span>|</span><label class="collapse" for="c-40071005">[-]</label><label class="expand" for="c-40071005">[3 more]</label></div><br/><div class="children"><div class="content">This is not the best way to understand where modern embeddings are coming from.</div><br/><div id="40071792" class="c"><input type="checkbox" id="c-40071792" checked=""/><div class="controls bullet"><span class="by">codetrotter</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40071005">parent</a><span>|</span><a href="#40070551">next</a><span>|</span><label class="collapse" for="c-40071792">[-]</label><label class="expand" for="c-40071792">[2 more]</label></div><br/><div class="children"><div class="content">True, but what is the best way?</div><br/><div id="40072106" class="c"><input type="checkbox" id="c-40072106" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40071792">parent</a><span>|</span><a href="#40070551">next</a><span>|</span><label class="collapse" for="c-40072106">[-]</label><label class="expand" for="c-40072106">[1 more]</label></div><br/><div class="children"><div class="content">Are you talking about sentence&#x2F;text chunk embeddings, or just embeddings in general?<p>If you need high quality text embeddings (e. g to use with a vector DB for text chunk retrieval), they they are going to come from the output of a language model, either a local one or using an embeddings API.<p>Other embeddings are normally going to be learnt in end-to-end fashion.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40069752" class="c"><input type="checkbox" id="c-40069752" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40069520">parent</a><span>|</span><a href="#40070202">prev</a><span>|</span><a href="#40068262">next</a><span>|</span><label class="collapse" for="c-40069752">[-]</label><label class="expand" for="c-40069752">[6 more]</label></div><br/><div class="children"><div class="content">They&#x27;re not, I get why you think that though.<p>They&#x27;re making a vector for a text that&#x27;s the term frequencies in the document.<p>It&#x27;s one step simpler than tfidf which is a great starting point.</div><br/><div id="40069872" class="c"><input type="checkbox" id="c-40069872" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40069752">parent</a><span>|</span><a href="#40069812">next</a><span>|</span><label class="collapse" for="c-40069872">[-]</label><label class="expand" for="c-40069872">[2 more]</label></div><br/><div class="children"><div class="content">OK, sounds counter-intuitive, but I&#x27;ll take your word for it!<p>It seems odd since the basis of word similarity captured in this type of way is that word meanings are associated with local context, which doesn&#x27;t seem related to these global occurrence counts.<p>Perhaps it works because two words with similar occurrence counts are more likely to often appear close to each other than two words where one has a high count, and another a small count? But this wouldn&#x27;t seem to work for small counts, and anyways the counts are just being added to the base index rather than making similar-count words closer in the embedding space.<p>Do you have any explanation for why this captures any similarity in meaning?</div><br/><div id="40070246" class="c"><input type="checkbox" id="c-40070246" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40069872">parent</a><span>|</span><a href="#40069812">next</a><span>|</span><label class="collapse" for="c-40070246">[-]</label><label class="expand" for="c-40070246">[1 more]</label></div><br/><div class="children"><div class="content">&gt; rather than making similar-count words closer in the embedding space.<p>Ah I think I see the confusion here. They are describing creating an embedding of a <i>document</i> or piece of text. At the base, the embedding of a single word would just be a single 1. There is absolutely no help with word similarity.<p>The problem of multiple meanings isn&#x27;t solved by this approach at all, at least not directly.<p>Talking about the &quot;gravity of a situation&quot; in a political piece makes the text <i>a bit more similar</i> to physics discussions about gravity. But most of the words won&#x27;t match as well, so your document vector is still more similar to other political pieces than physics.<p>Going up the scale, here&#x27;s a few basic starting points that were (are?) the backbone of many production text AI&#x2F;ML systems.<p>1. Bag of words. Here your vector has a 1 for words that are present, and 0 for ones that aren&#x27;t.<p>2. Bag of words with a count. A little better, now we&#x27;ve got the information that you said &quot;gravity&quot; fifty times not once. Normalise it so text length doesn&#x27;t matter and everything fits into 0-1.<p>3. TF-IDF. It&#x27;s not very useful to know that you said a common word a lot. Most texts do, what we care about is ones that say it more than you&#x27;d expect so we take into account how often the words appear in the entire corpus.<p>These don&#x27;t help with words, but given how simple they are they are shockingly useful. They have their stupid moments, although one benefit is that it&#x27;s very easy to debug why they cause a problem.</div><br/></div></div></div></div><div id="40069812" class="c"><input type="checkbox" id="c-40069812" checked=""/><div class="controls bullet"><span class="by">OmarShehata</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40069752">parent</a><span>|</span><a href="#40069872">prev</a><span>|</span><a href="#40068262">next</a><span>|</span><label class="collapse" for="c-40069812">[-]</label><label class="expand" for="c-40069812">[3 more]</label></div><br/><div class="children"><div class="content">Are you saying it&#x27;s pure chance that operations like &quot;man - woman&quot; = &quot;king - queen&quot; (and many, many other similar relationships and analogies) work?<p>If not please explain this comment to those of us ignorant in these matters :)</div><br/><div id="40070024" class="c"><input type="checkbox" id="c-40070024" checked=""/><div class="controls bullet"><span class="by">StrangeDoctor</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40069812">parent</a><span>|</span><a href="#40073152">next</a><span>|</span><label class="collapse" for="c-40070024">[-]</label><label class="expand" for="c-40070024">[1 more]</label></div><br/><div class="children"><div class="content">It’s not pure chance that the above calculus shakes out, but it doesn’t have to be that way. If you are embedding on a word by word level then it can happen, if it’s a little smaller or larger than word by word it’s not immediately clear what the calculation is doing.<p>But the main difference here is you get 1 embedding for the document in question, not an embedding per word like word2vec. So it’s something more like “document about OS&#x2F;2 warp” - “wiki page for ibm” + “wiki page for Microsoft” = “document on windows 3.1”</div><br/></div></div><div id="40073152" class="c"><input type="checkbox" id="c-40073152" checked=""/><div class="controls bullet"><span class="by">napoleongl</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40069812">parent</a><span>|</span><a href="#40070024">prev</a><span>|</span><a href="#40068262">next</a><span>|</span><label class="collapse" for="c-40073152">[-]</label><label class="expand" for="c-40073152">[1 more]</label></div><br/><div class="children"><div class="content">3Blue1Brown has some other examples in his videos about transformers, most notably I think is that hitler-Germany+italy ~ Mussolini!<p><a href="https:&#x2F;&#x2F;www.3blue1brown.com&#x2F;lessons&#x2F;gpt" rel="nofollow">https:&#x2F;&#x2F;www.3blue1brown.com&#x2F;lessons&#x2F;gpt</a></div><br/></div></div></div></div></div></div></div></div><div id="40068262" class="c"><input type="checkbox" id="c-40068262" checked=""/><div class="controls bullet"><span class="by">pstorm</span><span>|</span><a href="#40067955">parent</a><span>|</span><a href="#40069520">prev</a><span>|</span><a href="#40068522">next</a><span>|</span><label class="collapse" for="c-40068262">[-]</label><label class="expand" for="c-40068262">[9 more]</label></div><br/><div class="children"><div class="content">I&#x27;m trying to understand this approach. Maybe I am expecting too much out of this basic approach, but how does this create a similarity between words with indices close to each other? Wouldn&#x27;t it just be a popularity contest - the more common words have higher indices and vice versa? For instance, &quot;king&quot; and &quot;prince&quot; wouldn&#x27;t necessarily have similar indices, but they are semantically very similar.</div><br/><div id="40068314" class="c"><input type="checkbox" id="c-40068314" checked=""/><div class="controls bullet"><span class="by">svieira</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40068262">parent</a><span>|</span><a href="#40072803">next</a><span>|</span><label class="collapse" for="c-40068314">[-]</label><label class="expand" for="c-40068314">[1 more]</label></div><br/><div class="children"><div class="content">You are expecting too much out of this basic approach.  The &quot;simple&quot; similarity search in word2vec (used in <a href="https:&#x2F;&#x2F;semantle.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;semantle.com&#x2F;</a> if you haven&#x27;t seen it) is based on _multiple_ embeddings like this one (it&#x27;s a simple neural network not a simple embedding).</div><br/></div></div><div id="40072803" class="c"><input type="checkbox" id="c-40072803" checked=""/><div class="controls bullet"><span class="by">bschmidt1</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40068262">parent</a><span>|</span><a href="#40068314">prev</a><span>|</span><a href="#40068806">next</a><span>|</span><label class="collapse" for="c-40072803">[-]</label><label class="expand" for="c-40072803">[1 more]</label></div><br/><div class="children"><div class="content">This is a simple example where it scores their frequency. If you scored every word by their frequency only you might have embeddings like this:<p><pre><code>  act: [0.1]
  as:  [0.4]
  at:  [0.3]
  ...
</code></pre>
That&#x27;s a very simple 1D embedding, and like you said would only give you popularity. But say you wanted other stuff like its: Vulgarity, prevalence over time, whether its slang or not, how likely it is to start or end a sentence, etc. you would need more than 1 number. In text-embedding-ada-002 there are 1536 numbers in the array (vector), so it&#x27;s like:<p><pre><code>  act: [0.1, 0.1, 0.3, 0.0001, 0.000003, 0.003, ... (1536 items)]
  ...
</code></pre>
The numbers don&#x27;t mean anything in-and-of-themselves. The values don&#x27;t represent qualities of the words, they&#x27;re just numbers in relation to others in the training data. They&#x27;re different numbers in different training data because all the words are scored in relation to each other, like a graph. So when you compute them you arrive at words and meanings in the training data as you would arrive at a point in a coordinate space if you subtracted one [x,y,z] from another [x,y,z] in 3D.<p>So the rage about a vector db is that it&#x27;s a database for arrays of numbers (vectors) designed for computing them against each other, optimized for that instead of say a SQL or NoSQL which are all about retrieval etc.<p>So king vs prince etc. - When you take into account the 1536 numbers, you can imagine how compared to other words in training data they would actually be similar, always used in the same kinds of ways, and are indeed semantically similar - you&#x27;d be able to &quot;arrive&quot; at that fact, and arrive at antonyms, synonyms, their French alternatives, etc. but the system doesn&#x27;t &quot;know&quot; that stuff. Throw in Burger King training data and talk about French fries a lot though, and you&#x27;d mess up the embeddings when it comes arriving at the French version of a king! You might get &quot;pomme de terre&quot;.</div><br/></div></div><div id="40068806" class="c"><input type="checkbox" id="c-40068806" checked=""/><div class="controls bullet"><span class="by">jncfhnb</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40068262">parent</a><span>|</span><a href="#40072803">prev</a><span>|</span><a href="#40068635">next</a><span>|</span><label class="collapse" for="c-40068806">[-]</label><label class="expand" for="c-40068806">[1 more]</label></div><br/><div class="children"><div class="content">King doesn’t need to appear commonly with prince. It just needs to appear in the same context as prince.<p>It also leaves out the old “tf idf” normalization of considering how common a word is broadly (less interesting) vs in that particular document. Kind of like a shittier attention. Used to make a big difference.</div><br/></div></div><div id="40068635" class="c"><input type="checkbox" id="c-40068635" checked=""/><div class="controls bullet"><span class="by">sdwr</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40068262">parent</a><span>|</span><a href="#40068806">prev</a><span>|</span><a href="#40068308">next</a><span>|</span><label class="collapse" for="c-40068635">[-]</label><label class="expand" for="c-40068635">[3 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t even work as described for popularity - one word starts at 49,999 and one starts at 0.</div><br/><div id="40068732" class="c"><input type="checkbox" id="c-40068732" checked=""/><div class="controls bullet"><span class="by">itronitron</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40068635">parent</a><span>|</span><a href="#40068308">next</a><span>|</span><label class="collapse" for="c-40068732">[-]</label><label class="expand" for="c-40068732">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, that is a poorly written description. I think they meant that each word gets  a unique index location into an array, and the value at that word&#x27;s index location is incremented whenever the word occurs.</div><br/></div></div></div></div><div id="40068308" class="c"><input type="checkbox" id="c-40068308" checked=""/><div class="controls bullet"><span class="by">zachrose</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40068262">parent</a><span>|</span><a href="#40068635">prev</a><span>|</span><a href="#40068974">next</a><span>|</span><label class="collapse" for="c-40068308">[-]</label><label class="expand" for="c-40068308">[1 more]</label></div><br/><div class="children"><div class="content">Maybe the idea is to order your vocabulary into some kind of “semantic rainbow”? Like a one-dimensional embedding?</div><br/></div></div><div id="40068974" class="c"><input type="checkbox" id="c-40068974" checked=""/><div class="controls bullet"><span class="by">im3w1l</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40068262">parent</a><span>|</span><a href="#40068308">prev</a><span>|</span><a href="#40068522">next</a><span>|</span><label class="collapse" for="c-40068974">[-]</label><label class="expand" for="c-40068974">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a document embedding, not a word embedding.</div><br/></div></div></div></div><div id="40068522" class="c"><input type="checkbox" id="c-40068522" checked=""/><div class="controls bullet"><span class="by">mschulkind</span><span>|</span><a href="#40067955">parent</a><span>|</span><a href="#40068262">prev</a><span>|</span><a href="#40068592">next</a><span>|</span><label class="collapse" for="c-40068522">[-]</label><label class="expand" for="c-40068522">[2 more]</label></div><br/><div class="children"><div class="content">Aren&#x27;t you just describing a bag-of-words model?<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bag-of-words_model" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bag-of-words_model</a></div><br/><div id="40070122" class="c"><input type="checkbox" id="c-40070122" checked=""/><div class="controls bullet"><span class="by">thisiszilff</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40068522">parent</a><span>|</span><a href="#40068592">next</a><span>|</span><label class="collapse" for="c-40070122">[-]</label><label class="expand" for="c-40070122">[1 more]</label></div><br/><div class="children"><div class="content">Yes! And the follow up that cosine similarity (for BoW) is a super simple similarity metric based on counting up the number of words the two vectors have in common.</div><br/></div></div></div></div><div id="40068592" class="c"><input type="checkbox" id="c-40068592" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40067955">parent</a><span>|</span><a href="#40068522">prev</a><span>|</span><a href="#40068304">next</a><span>|</span><label class="collapse" for="c-40068592">[-]</label><label class="expand" for="c-40068592">[6 more]</label></div><br/><div class="children"><div class="content">Is that really an embedding?  I normally think of an embedding as an approximate lower-dimensional matrix of coefficients that operate on a reduced set of composite variables that map the data from a nonlinear to linear space.</div><br/><div id="40070108" class="c"><input type="checkbox" id="c-40070108" checked=""/><div class="controls bullet"><span class="by">thisiszilff</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40068592">parent</a><span>|</span><a href="#40068304">next</a><span>|</span><label class="collapse" for="c-40070108">[-]</label><label class="expand" for="c-40070108">[5 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right that what I described isn&#x27;t what people commonly think about as embeddings (given we are more advanced now the above description), but broadly an embedding is anything (in nlp at least) that maps text into a fixed length vector. When you make embedding like this, the nice thing is that cosine similarity has an easy to understand similarity meaning: count the number of words two documents have in common (subject to some normalization constant).<p>Most fancy modern embedding strategies basically start with this and then proceed to build on top of it to reduce dimensions, represent words as vectors in their own right, pass this into some neural layer, etc.</div><br/><div id="40071385" class="c"><input type="checkbox" id="c-40071385" checked=""/><div class="controls bullet"><span class="by">light_hue_1</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40070108">parent</a><span>|</span><a href="#40068304">next</a><span>|</span><label class="collapse" for="c-40071385">[-]</label><label class="expand" for="c-40071385">[4 more]</label></div><br/><div class="children"><div class="content">A lot of people here are trying to describe to you that no, this is not at all the starting point of modern embeddings. This has none of the properties of embeddings.<p>What you&#x27;re describing is an idea from the 90s that was a dead end. Bag of words representations.<p>It has no relationship to modern methods. It&#x27;s based on totally different theory (bow instead of the distributional hypothesis).<p>There is no conceptual or practical path from what you describe to what modern embeddings are. It&#x27;s horribly misleading.</div><br/><div id="40071763" class="c"><input type="checkbox" id="c-40071763" checked=""/><div class="controls bullet"><span class="by">thisiszilff</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40071385">parent</a><span>|</span><a href="#40073698">next</a><span>|</span><label class="collapse" for="c-40071763">[-]</label><label class="expand" for="c-40071763">[2 more]</label></div><br/><div class="children"><div class="content">Eh, I disagree. When I began working in ML everything was about word2vec and glove and the state of the art for embedding documents was adding together all the word embeddings and it made no sense to me but it worked.<p>Learning about BoW and simple ways of convert text to fixed length vectors that can be used in ML algos clarified a whole for me, especially the fact that embeddings aren’t magic they are just a way to convert text to a fixed length vector.<p>BoW and tf-idf vectors are still workhorses for routine text classification tasks despite their limitations, so they aren’t really a dead end. Similarity a lot of things that follow BoW make a whole lot more sense if you think of them as addressing limitations of BoW.</div><br/><div id="40072097" class="c"><input type="checkbox" id="c-40072097" checked=""/><div class="controls bullet"><span class="by">light_hue_1</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40071763">parent</a><span>|</span><a href="#40073698">next</a><span>|</span><label class="collapse" for="c-40072097">[-]</label><label class="expand" for="c-40072097">[1 more]</label></div><br/><div class="children"><div class="content">Well, you&#x27;ve fooled yourself into thinking you understand something when you don&#x27;t. I say this as someone with a PhD in the topic, who has taught many students, and published dozens of papers in the space.<p>The operation of adding BoW vectors together has nothing to do with the operation of adding together word embeddings. Well, aside from both nominally being addition.<p>It&#x27;s like saying you understand what&#x27;s happening because you can add velocity vectors and then you go on to add the binary vectors that represent two binary programs and expect the result to give you a program with the average behavior of both. Obviously that doesn&#x27;t happen, you get a nonsense binary.<p>They may both be arrays of numbers but mathematically there&#x27;s no relationship between the two. Thinking that there&#x27;s a relationship between them leads to countless nonsense conclusions: the idea that you can keep adding word embeddings to create document embeddings like you keep adding BoWs, the notion that average BoWs mean the same thing as average word embeddings, the notion that normalizing BoWs is the same as normalizing word embeddings and will lead to the same kind of search results, etc. The errors you get with BoWs are totally different from the errors you get with word or sentence or document embeddings. And how you fix those errors is totally different.<p>No. Nothing at all makes sense about word embeddings from the point of BoW.<p>Also, yes BoW is a total dead end. They have been completely supplanted. There&#x27;s never any case where someone should use them.</div><br/></div></div></div></div><div id="40073698" class="c"><input type="checkbox" id="c-40073698" checked=""/><div class="controls bullet"><span class="by">danieldk</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40071385">parent</a><span>|</span><a href="#40071763">prev</a><span>|</span><a href="#40068304">next</a><span>|</span><label class="collapse" for="c-40073698">[-]</label><label class="expand" for="c-40073698">[1 more]</label></div><br/><div class="children"><div class="content"><i>There is no conceptual or practical path from what you describe to what modern embeddings are.</i><p>There certainly is. At least there is a strong relation between bag of word representations and methods like word2vec. I am sure you know all of this, but I think it&#x27;s worth expanding a bit on this, since the top-level comment describes things in a rather confusing way.<p>In traditional Information Retrieval, two kinds of vectors were typically used: document vectors and term vectors. If you make a |D| x |T| matrix (where |D| is the number of documents and |T| is the number of terms that occur in all documents), we can go through a corpus and note in each |T|-length row for a particular the frequency of each term in that document (<i>frequency</i> here means the raw counts or something like TF-IDF). Each row is a document vector, each column a term vector. The cosine similarity between two document vectors will tell you whether two documents are similar, because similar documents are likely to have similar terms. The cosine similarity between two term vectors will tell you whether two terms are similar, because similar terms tend to occur in similar documents. The top-level comment seems to have explained document vectors in a clumsy way.<p>Over time (we are talking 70-90s), people have found that term vectors did not really work well, because documents are often too coarse-grained as context. So, term vectors were defined as |T| x |T| matrices where if you have such a matrix C, C[i][j] contains the frequency of how often the j-th term occurs in the context of the i-th term. Since this type of matrix is not bound to documents, you can choose the context size based on the goals you have in mind. For instance, you could only count terms that are within 10 (text) distance of the occurrences of the term i.<p>One refinement is that rather than raw frequencies, we can use some other measure. One issue with raw frequencies is that a frequent word like <i>the</i> will co-occur with pretty much every word, so it&#x27;s frequency in the term vector is not particularly informative, but it&#x27;s large frequency will have an outsized influence on e.g. dot products. So, people would typically use pointwise mutual information (PMI) instead. It&#x27;s beyond the scope of a comment to explain PMI, but intuitively you can think of the PMI of two words to mean: how much more often do the words cooccur than chance? This will result in low PMIs for e.g. <i>PMI(information, the)</i> but a high PMI for <i>PMI(information, retrieval)</i>. Then it&#x27;s also common practice to replace negative PMI values by zero, which leads to PPMI (positive PMI).<p>So, what do we have now? A |T|x|T| matrix with PPMI scores, where each row (or column) can be used as a word vector. However, it&#x27;s a bit unwieldy, because the vectors are large (|T|) and typically somewhat sparse. So people started to apply dimensionality reduction, e.g. by applying Singular Value Decomposition (SVD, I&#x27;ll skip the details here of how to use it for dimensionality reduction). So, suppose that we use SVD to reduce the vector dimensionality to 300, we are left with a |T|x300 matrix and we finally have dense vectors, similar to e.g. word2vec.<p>Now, the interesting thing is that people have found that word2vec&#x27;s skipgram with negative sampling (SGNS) is implicitly vectorizing a PMI-based word-context matrix [1], exactly like the IR folks were doing before. Conversely, if you matrix-multiply the word and context embedding matrices that come out of word2vec SGNS, you get an approximation of the |T|x|T| PMI matrix (or |T|x|C| if a different vocab is used for the context).<p>Summarized, there is a strong conceptual relation between bag-of-word representations of old days and word2vec.<p>Whether it&#x27;s an interesting route didactically for understanding embeddings is up for debate. It&#x27;s not like the mathematics behind word2vec are complex (understanding the dot product and the logistic function goes a long way) and understanding word2vec in terms of &#x27;neural net building blocks&#x27; makes it easier to go from word2vec to modern architectures. But in an exhaustive course about word representations, it certainly makes sense to link word embeddings to prior work in IR.<p>[1] <a href="https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper_files&#x2F;paper&#x2F;2014&#x2F;file&#x2F;feab05aa91085b7a8012516bc3533958-Paper.pdf" rel="nofollow">https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper_files&#x2F;paper&#x2F;2014&#x2F;file&#x2F;f...</a></div><br/></div></div></div></div></div></div></div></div><div id="40068304" class="c"><input type="checkbox" id="c-40068304" checked=""/><div class="controls bullet"><span class="by">afro88</span><span>|</span><a href="#40067955">parent</a><span>|</span><a href="#40068592">prev</a><span>|</span><a href="#40071522">next</a><span>|</span><label class="collapse" for="c-40068304">[-]</label><label class="expand" for="c-40068304">[3 more]</label></div><br/><div class="children"><div class="content">How does this enable cosine similarity usage? I don&#x27;t get the link between incrementing a word&#x27;s index by it&#x27;s count in a text and how this ends up with words that have similar meaning to have a high cosine similarity value</div><br/><div id="40068521" class="c"><input type="checkbox" id="c-40068521" checked=""/><div class="controls bullet"><span class="by">twelfthnight</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40068304">parent</a><span>|</span><a href="#40068379">next</a><span>|</span><label class="collapse" for="c-40068521">[-]</label><label class="expand" for="c-40068521">[1 more]</label></div><br/><div class="children"><div class="content">I think they are talking about bag-of-words. If you apply a dimensionality reduction technique like SVD or even random projection on bag-of-words, you can effectively create a basic embedding. Check out latent semantic indexing &#x2F; latent semantic analysis.</div><br/></div></div><div id="40068379" class="c"><input type="checkbox" id="c-40068379" checked=""/><div class="controls bullet"><span class="by">sell_dennis</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40068304">parent</a><span>|</span><a href="#40068521">prev</a><span>|</span><a href="#40071522">next</a><span>|</span><label class="collapse" for="c-40068379">[-]</label><label class="expand" for="c-40068379">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right, that approach doesn&#x27;t enable getting embeddings for an individual word. But it would work for comparing similarity of documents - not that well of course, but it&#x27;s a toy example that might feel more intuitive</div><br/></div></div></div></div><div id="40071522" class="c"><input type="checkbox" id="c-40071522" checked=""/><div class="controls bullet"><span class="by">wjholden</span><span>|</span><a href="#40067955">parent</a><span>|</span><a href="#40068304">prev</a><span>|</span><a href="#40069489">next</a><span>|</span><label class="collapse" for="c-40071522">[-]</label><label class="expand" for="c-40071522">[2 more]</label></div><br/><div class="children"><div class="content">Really appreciate you explaining this idea, I want to try this! It wasn&#x27;t clear to me until I read the discussion that you meant that you&#x27;d have similarity of entire documents, not among words.</div><br/><div id="40072078" class="c"><input type="checkbox" id="c-40072078" checked=""/><div class="controls bullet"><span class="by">thisiszilff</span><span>|</span><a href="#40067955">root</a><span>|</span><a href="#40071522">parent</a><span>|</span><a href="#40069489">next</a><span>|</span><label class="collapse" for="c-40072078">[-]</label><label class="expand" for="c-40072078">[1 more]</label></div><br/><div class="children"><div class="content">Yes! And that’s an oversight on my part — word embeddings are interesting but I usually deal with documents when doing nlp work and only deal with word embeddings when thinking about how to combine them into a document embedding.<p>Give it a shot! I’d grab a corpus like <a href="https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;datasets&#x2F;real_world.html#the-20-newsgroups-text-dataset" rel="nofollow">https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;datasets&#x2F;real_world.html#the...</a> to play with and see what you get. It’s not going to be amazing, but it’s a great way to build some baseline intuition for nlp work with text that you can do on a laptop.</div><br/></div></div></div></div></div></div><div id="40070542" class="c"><input type="checkbox" id="c-40070542" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#40067955">prev</a><span>|</span><a href="#40070233">next</a><span>|</span><label class="collapse" for="c-40070542">[-]</label><label class="expand" for="c-40070542">[2 more]</label></div><br/><div class="children"><div class="content">Without getting into any big debates about whether or not RAG is medium-term interesting or whatever, you can ‘pip install sentence-transformers faiss’ and just immediately start having fun. I recommend using straightforward cosine similarity to just crush the NYT’s recommender as a fun project for two reasons: there’s an API and plenty of corpus, and it’s like, whoa, that’s better than the New York Times.<p>He’s trying to sell a SaaS product (Pinecone), but he’s doing it the right way: it’s ok to be an influencer if you know what you’re taking about.<p>James Briggs has great stuff on this: <a href="https:&#x2F;&#x2F;youtube.com&#x2F;@jamesbriggs" rel="nofollow">https:&#x2F;&#x2F;youtube.com&#x2F;@jamesbriggs</a></div><br/><div id="40072308" class="c"><input type="checkbox" id="c-40072308" checked=""/><div class="controls bullet"><span class="by">aeth0s</span><span>|</span><a href="#40070542">parent</a><span>|</span><a href="#40070233">next</a><span>|</span><label class="collapse" for="c-40072308">[-]</label><label class="expand" for="c-40072308">[1 more]</label></div><br/><div class="children"><div class="content">&gt; crush the NYT’s recommender as a fun project for two reasons<p>Could you share what recommender you&#x27;re referring to here, and how you can evaluate &quot;crushing&quot; it?<p>Sounds fun!</div><br/></div></div></div></div><div id="40070233" class="c"><input type="checkbox" id="c-40070233" checked=""/><div class="controls bullet"><span class="by">mrkeen</span><span>|</span><a href="#40070542">prev</a><span>|</span><a href="#40068401">next</a><span>|</span><label class="collapse" for="c-40070233">[-]</label><label class="expand" for="c-40070233">[1 more]</label></div><br/><div class="children"><div class="content">Given<p><pre><code>  not because they’re sufficiently advanced technology indistinguishable from magic, but the opposite.

  Unlike LLMs, working with embeddings feels like regular deterministic code.

  &lt;h3&gt;Creating embeddings&lt;&#x2F;h3&gt;
</code></pre>
I was hoping for a bit more than:<p><pre><code>  They’re a bit of a black box

  Next, we chose an embedding model. OpenAI’s embedding models will probably work just fine.</code></pre></div><br/></div></div><div id="40068401" class="c"><input type="checkbox" id="c-40068401" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#40070233">prev</a><span>|</span><a href="#40073706">next</a><span>|</span><label class="collapse" for="c-40068401">[-]</label><label class="expand" for="c-40068401">[10 more]</label></div><br/><div class="children"><div class="content">One of the challenges here is handling homonyms. If I search in the app for &quot;king&quot;, most of the top ten results are &quot;ruler&quot; icons - showing a measuring stick. Rodent returns mostly computer mice, etc.<p><a href="https:&#x2F;&#x2F;www.v0.app&#x2F;search?q=king" rel="nofollow">https:&#x2F;&#x2F;www.v0.app&#x2F;search?q=king</a><p><a href="https:&#x2F;&#x2F;www.v0.app&#x2F;search?q=rodent" rel="nofollow">https:&#x2F;&#x2F;www.v0.app&#x2F;search?q=rodent</a><p>This isn&#x27;t a criticism of the app - I&#x27;d rather get a few funny mismatches in exchange for being able to find related icons. But it&#x27;s an interesting puzzle to think about.</div><br/><div id="40068809" class="c"><input type="checkbox" id="c-40068809" checked=""/><div class="controls bullet"><span class="by">itronitron</span><span>|</span><a href="#40068401">parent</a><span>|</span><a href="#40068771">next</a><span>|</span><label class="collapse" for="c-40068809">[-]</label><label class="expand" for="c-40068809">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; If I search in the app for &quot;king&quot;, most of the top ten results are &quot;ruler&quot; icons<p>I believe that&#x27;s the measure of a man.</div><br/></div></div><div id="40068771" class="c"><input type="checkbox" id="c-40068771" checked=""/><div class="controls bullet"><span class="by">charlieyuan</span><span>|</span><a href="#40068401">parent</a><span>|</span><a href="#40068809">prev</a><span>|</span><a href="#40070550">next</a><span>|</span><label class="collapse" for="c-40068771">[-]</label><label class="expand" for="c-40068771">[1 more]</label></div><br/><div class="children"><div class="content">Good call out! We think of this as a two part problem.<p>1. The intent of the user. Is it a description of the look of the icon or the utility of the icon?
2. How best to rank the results which is a combination of intent, CTR of past search queries, bootstrapping popularity via usage on open source projects etc.<p>- Charlie of v0.app</div><br/></div></div><div id="40070550" class="c"><input type="checkbox" id="c-40070550" checked=""/><div class="controls bullet"><span class="by">joshspankit</span><span>|</span><a href="#40068401">parent</a><span>|</span><a href="#40068771">prev</a><span>|</span><a href="#40071519">next</a><span>|</span><label class="collapse" for="c-40070550">[-]</label><label class="expand" for="c-40070550">[1 more]</label></div><br/><div class="children"><div class="content">This is imo the worst part of embedding search.<p>Somehow Amazon continues to be the leader in muddy results which is a sign that it’s a huge problem domain and not easily fixable even if you have massive resources.</div><br/></div></div><div id="40071519" class="c"><input type="checkbox" id="c-40071519" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#40068401">parent</a><span>|</span><a href="#40070550">prev</a><span>|</span><a href="#40069427">next</a><span>|</span><label class="collapse" for="c-40071519">[-]</label><label class="expand" for="c-40071519">[1 more]</label></div><br/><div class="children"><div class="content">Wouldn’t it help to provide  affordances guiding the user to submit a question rather than a keyword? Then, “Why are kings selected by primogeniture?” probably wouldn’t be near passages about measuring sticks in the embedding space. (Of course, this idea doesn’t work for icon search.)</div><br/></div></div><div id="40069427" class="c"><input type="checkbox" id="c-40069427" checked=""/><div class="controls bullet"><span class="by">dceddia</span><span>|</span><a href="#40068401">parent</a><span>|</span><a href="#40071519">prev</a><span>|</span><a href="#40069022">next</a><span>|</span><label class="collapse" for="c-40069427">[-]</label><label class="expand" for="c-40069427">[1 more]</label></div><br/><div class="children"><div class="content">I was reading this article and thinking about things like, in the case of doing transcription, if you heard the spoken word “sign” in isolation you couldn’t be sure whether it meant road sign, spiritual sign, +&#x2F;- sign, or even the sine function. This seems like a similar problem where you pretty much require context to make a good guess, otherwise the best it could do is go off of how many times the word appears in the dataset right? Is there something smarter it could do?</div><br/></div></div><div id="40069022" class="c"><input type="checkbox" id="c-40069022" checked=""/><div class="controls bullet"><span class="by">bryantwolf</span><span>|</span><a href="#40068401">parent</a><span>|</span><a href="#40069427">prev</a><span>|</span><a href="#40072616">next</a><span>|</span><label class="collapse" for="c-40069022">[-]</label><label class="expand" for="c-40069022">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, these can be cute, but they&#x27;re not ideal. I think the user feedback mechanism could help naturally align this over time, but it would also be gameable. It&#x27;s all interesting stuff</div><br/><div id="40069857" class="c"><input type="checkbox" id="c-40069857" checked=""/><div class="controls bullet"><span class="by">jonnycoder</span><span>|</span><a href="#40068401">root</a><span>|</span><a href="#40069022">parent</a><span>|</span><a href="#40072616">next</a><span>|</span><label class="collapse" for="c-40069857">[-]</label><label class="expand" for="c-40069857">[2 more]</label></div><br/><div class="children"><div class="content">As the op, you can do both semantic search (embedding) and keyword search. Some RAG techniques call out using both for better results. Nice product by the way!</div><br/><div id="40071378" class="c"><input type="checkbox" id="c-40071378" checked=""/><div class="controls bullet"><span class="by">bryantwolf</span><span>|</span><a href="#40068401">root</a><span>|</span><a href="#40069857">parent</a><span>|</span><a href="#40072616">next</a><span>|</span><label class="collapse" for="c-40071378">[-]</label><label class="expand" for="c-40071378">[1 more]</label></div><br/><div class="children"><div class="content">Hybrid searches are great, though I&#x27;m not sure they would help here. Neither &#x27;crown&#x27; nor &#x27;ruler&#x27; would come back from a text search for &#x27;king,&#x27; right?<p>I bet if we put a better description into the embedding for &#x27;ruler,&#x27; we&#x27;d avoid this. Something like &quot;a straight strip or cylinder of plastic, wood, metal, or other rigid material, typically marked at regular intervals, to draw straight lines or measure distances.&quot; (stolen from a Google search). We might be able to ask a language model to look at the icon and give a good description we can put into the embedding.</div><br/></div></div></div></div></div></div></div></div><div id="40073706" class="c"><input type="checkbox" id="c-40073706" checked=""/><div class="controls bullet"><span class="by">primitivesuave</span><span>|</span><a href="#40068401">prev</a><span>|</span><a href="#40068468">next</a><span>|</span><label class="collapse" for="c-40073706">[-]</label><label class="expand" for="c-40073706">[2 more]</label></div><br/><div class="children"><div class="content">I learned how to use embeddings by building semantic search for the Bhagavad Gita. I simply saved the embeddings for all 700 verses into a big file which is stored in a Lambda function, and compared against incoming queries with a single query to OpenAI&#x27;s embedding endpoint.<p>Shameless plug in case anyone wants to test it out - <a href="https:&#x2F;&#x2F;gita.pub" rel="nofollow">https:&#x2F;&#x2F;gita.pub</a></div><br/><div id="40074000" class="c"><input type="checkbox" id="c-40074000" checked=""/><div class="controls bullet"><span class="by">forgingahead</span><span>|</span><a href="#40073706">parent</a><span>|</span><a href="#40068468">next</a><span>|</span><label class="collapse" for="c-40074000">[-]</label><label class="expand" for="c-40074000">[1 more]</label></div><br/><div class="children"><div class="content">Really nice and beautiful site!</div><br/></div></div></div></div><div id="40068468" class="c"><input type="checkbox" id="c-40068468" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#40073706">prev</a><span>|</span><a href="#40072733">next</a><span>|</span><label class="collapse" for="c-40068468">[-]</label><label class="expand" for="c-40068468">[28 more]</label></div><br/><div class="children"><div class="content">One of my biggest annoyances with the modern AI tooling hype is that you need to use a vector store for just working with embeddings. You don&#x27;t.<p>The reason vector stores are important for production use-cases are mostly latency-related for larger sets of data (100k+ records), but if you&#x27;re working on a toy project just learning how to use embeddings, you can compute cosine distance with a couple lines of numpy by doing a dot product of a normalized query vectors with a matrix of normalized records.<p>Best of all, it gives you a reason to use Python&#x27;s @ operator, which with numpy matrices does a dot product.</div><br/><div id="40069049" class="c"><input type="checkbox" id="c-40069049" checked=""/><div class="controls bullet"><span class="by">hereonout2</span><span>|</span><a href="#40068468">parent</a><span>|</span><a href="#40069006">next</a><span>|</span><label class="collapse" for="c-40069049">[-]</label><label class="expand" for="c-40069049">[9 more]</label></div><br/><div class="children"><div class="content">100k records is still pretty small!<p>It feels a bit like the hype that happended with &quot;big data&quot;. People ended up creating spark clusters to query a few million records. Or  using Hadoop for a dataset you could process with awk.<p>Professionally I&#x27;ve only ever worked with dataset sizes in the region of low millions and have never needed specialist tooling to cope.<p>I assume these tools do serve a purpose but perhaps one that only kicks in at a scale approaching billions.</div><br/><div id="40070183" class="c"><input type="checkbox" id="c-40070183" checked=""/><div class="controls bullet"><span class="by">hot_gril</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40069049">parent</a><span>|</span><a href="#40069410">next</a><span>|</span><label class="collapse" for="c-40070183">[-]</label><label class="expand" for="c-40070183">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been in the &quot;mid-sized&quot; area a lot where Numpy etc cannot handle it, so I had to go to Postgres or more specialized tooling like Spark. But I always started with the simple thing and only moved up if it didn&#x27;t suffice.<p>Similarly, I read how Postgres won&#x27;t scale for a backend application and I should use Citus, Spanner, or some NoSQL thing. But that day has not yet arrived.</div><br/><div id="40070719" class="c"><input type="checkbox" id="c-40070719" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40070183">parent</a><span>|</span><a href="#40070876">next</a><span>|</span><label class="collapse" for="c-40070719">[-]</label><label class="expand" for="c-40070719">[2 more]</label></div><br/><div class="children"><div class="content">Numpy might not be able to handle a full o(n^2) comparison of vectors but you can use a lib with hsnw and it can have great performance on medium  
(and large) datasets.</div><br/><div id="40071917" class="c"><input type="checkbox" id="c-40071917" checked=""/><div class="controls bullet"><span class="by">hot_gril</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40070719">parent</a><span>|</span><a href="#40070876">next</a><span>|</span><label class="collapse" for="c-40071917">[-]</label><label class="expand" for="c-40071917">[1 more]</label></div><br/><div class="children"><div class="content">If I remember correctly, the simplest thing Numpy choked on was large sparse matrix multiplication. There are also other things like text search that Numpy won&#x27;t help with and you don&#x27;t want to do in Python if it&#x27;s a large set.</div><br/></div></div></div></div><div id="40070876" class="c"><input type="checkbox" id="c-40070876" checked=""/><div class="controls bullet"><span class="by">cornel_io</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40070183">parent</a><span>|</span><a href="#40070719">prev</a><span>|</span><a href="#40069410">next</a><span>|</span><label class="collapse" for="c-40070876">[-]</label><label class="expand" for="c-40070876">[2 more]</label></div><br/><div class="children"><div class="content">Right on: I&#x27;ve used a single Postgres database on AWS to handle 1M+ concurrent users. If you&#x27;re Google, sure, not gonna cut it, but for most people these things scale vertically a <i>lot</i> further than you&#x27;d expect (especially if, like me, you grew up in the pre-SSD days and couldn&#x27;t get hundreds of gigs of RAM on a cloud instance).<p>Even when you do pass that point, you can often shard to achieve horizontal scalability to at least some degree, since the real heavy lifting is usually easy to break out on a per-user basis. Some apps won&#x27;t permit that (if you&#x27;ve got cross-user joins then it&#x27;s going to be a bit of a headache), but at that point you&#x27;ve at least earned the right to start building up a more complex stack and complicating your queries to let things grow horizontally.<p>Horizontal scaling <i>is</i> a huge headache, any way you cut it, and TBH going with something like Spanner is just as much of a headache because you have to understand its limitations extremely well if you want <i>it</i> to scale. It doesn&#x27;t just magically make all your SQL infinitely scalable, things that are hard to shard are typically also hard to make fast on Spanner. What it&#x27;s really good at is taking an app with huge traffic where a) all the hot queries <i>would</i> be easy to shard, but b) you don&#x27;t want the complexity of adding sharding logic (+re-sharding, migration, failure handling, etc), and c) the tough to shard queries are low frequency enough that you don&#x27;t really care if they&#x27;re slow (I guess also d) you don&#x27;t care that it&#x27;s hella expensive compared to a normal Postgres or MySQL box). You still need to understand a lot more than when using a normal DB, but it can add a lot of value in those cases.</div><br/><div id="40071842" class="c"><input type="checkbox" id="c-40071842" checked=""/><div class="controls bullet"><span class="by">hot_gril</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40070876">parent</a><span>|</span><a href="#40069410">next</a><span>|</span><label class="collapse" for="c-40071842">[-]</label><label class="expand" for="c-40071842">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t even say whether or not Google benefits from Spanner, vs multiple Postgres DBs with application-level sharding. Reworking your systems to work with a horizontally-scaling DB is eerily similar to doing application-level sharding, and just because something is huge doesn&#x27;t mean it&#x27;s better with DB-level sharding.<p>The unique nice thing in Spanner is TrueTime, which enables the closest semblance of a multi-master DB by making an atomic clock the ground truth (see Generals&#x27; Problem). So you essentially don&#x27;t have to worry about a regional failure causing unavailability (or inconsistency if you choose it) for one DB, since those clocks are a lot more reliable than machines. But there are probably downsides.</div><br/></div></div></div></div></div></div><div id="40069410" class="c"><input type="checkbox" id="c-40069410" checked=""/><div class="controls bullet"><span class="by">smahs</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40069049">parent</a><span>|</span><a href="#40070183">prev</a><span>|</span><a href="#40069466">next</a><span>|</span><label class="collapse" for="c-40069410">[-]</label><label class="expand" for="c-40069410">[2 more]</label></div><br/><div class="children"><div class="content">This sentiment is pretty common I guess. Outside of a niche, the massive scale for which a vast majority of the data tech was designed doesn&#x27;t exist and KISS wins outright. Though I guess that&#x27;s evolution, we want to test the limits in pursuit of grandeur before mastering the utility (ex. pyramids).</div><br/><div id="40069890" class="c"><input type="checkbox" id="c-40069890" checked=""/><div class="controls bullet"><span class="by">jonnycoder</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40069410">parent</a><span>|</span><a href="#40069466">next</a><span>|</span><label class="collapse" for="c-40069890">[-]</label><label class="expand" for="c-40069890">[1 more]</label></div><br/><div class="children"><div class="content">KISS doesn&#x27;t get me employed though. I narrowly missed being the chosen candidate for a State job which called for Apache Spark experience. I missed two questions relating to Spark and &quot;what is a parquet file?&quot; but otherwise did great on the remaining behavioral questions (the hiring manager gave me feedback after requesting it). Too bad they did not have a question about processing data using command lines tools.</div><br/></div></div></div></div><div id="40069466" class="c"><input type="checkbox" id="c-40069466" checked=""/><div class="controls bullet"><span class="by">mritchie712</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40069049">parent</a><span>|</span><a href="#40069410">prev</a><span>|</span><a href="#40069006">next</a><span>|</span><label class="collapse" for="c-40069466">[-]</label><label class="expand" for="c-40069466">[1 more]</label></div><br/><div class="children"><div class="content">yeah, glad the hype around big data is dead. Not a lot of solid numbers in here, but this post covers it well[0].<p>We have duckdb embedded in our product[1] and it works perfectly well for billions of rows of a data without the hadoop overhead.<p>0 - <a href="https:&#x2F;&#x2F;motherduck.com&#x2F;blog&#x2F;big-data-is-dead&#x2F;" rel="nofollow">https:&#x2F;&#x2F;motherduck.com&#x2F;blog&#x2F;big-data-is-dead&#x2F;</a><p>1 - <a href="https:&#x2F;&#x2F;www.definite.app&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.definite.app&#x2F;</a></div><br/></div></div></div></div><div id="40069006" class="c"><input type="checkbox" id="c-40069006" checked=""/><div class="controls bullet"><span class="by">bryantwolf</span><span>|</span><a href="#40068468">parent</a><span>|</span><a href="#40069049">prev</a><span>|</span><a href="#40068869">next</a><span>|</span><label class="collapse" for="c-40069006">[-]</label><label class="expand" for="c-40069006">[3 more]</label></div><br/><div class="children"><div class="content">As an individual, I love the idea of pushing to simplify even further to understand these core concepts. For the ecosystem, I like that vector stores make these features accessible to environments outside of Python.</div><br/><div id="40069242" class="c"><input type="checkbox" id="c-40069242" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40069006">parent</a><span>|</span><a href="#40068869">next</a><span>|</span><label class="collapse" for="c-40069242">[-]</label><label class="expand" for="c-40069242">[2 more]</label></div><br/><div class="children"><div class="content">If you ask ChatGPT to give you a cosine similarity function that works against two arrays of floating numbers in any programming language you&#x27;ll get the code that you need.<p>Here&#x27;s one in JavaScript (my prompt was &quot;cosine similarity function for two javascript arrays of floating point numbers&quot;):<p><pre><code>    function cosineSimilarity(vecA, vecB) {
        if (vecA.length !== vecB.length) {
            throw &quot;Vectors do not have the same dimensions&quot;;
        }
        let dotProduct = 0.0;
        let normA = 0.0;
        let normB = 0.0;
        for (let i = 0; i &lt; vecA.length; i++) {
            dotProduct += vecA[i] * vecB[i];
            normA += vecA[i] ** 2;
            normB += vecB[i] ** 2;
        }
        if (normA === 0 || normB === 0) {
            throw &quot;One of the vectors is zero, cannot compute similarity&quot;;
        }
        return dotProduct &#x2F; (Math.sqrt(normA) * Math.sqrt(normB));
    }
</code></pre>
Vector stores really aren&#x27;t necessary if you&#x27;re dealing with less than a few hundred thousand vectors - load them up in a bunch of in-memory arrays and run a function like that against them using brute-force.</div><br/><div id="40069846" class="c"><input type="checkbox" id="c-40069846" checked=""/><div class="controls bullet"><span class="by">bryantwolf</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40069242">parent</a><span>|</span><a href="#40068869">next</a><span>|</span><label class="collapse" for="c-40069846">[-]</label><label class="expand" for="c-40069846">[1 more]</label></div><br/><div class="children"><div class="content">I love it!</div><br/></div></div></div></div></div></div><div id="40068869" class="c"><input type="checkbox" id="c-40068869" checked=""/><div class="controls bullet"><span class="by">christiangenco</span><span>|</span><a href="#40068468">parent</a><span>|</span><a href="#40069006">prev</a><span>|</span><a href="#40070060">next</a><span>|</span><label class="collapse" for="c-40068869">[-]</label><label class="expand" for="c-40068869">[2 more]</label></div><br/><div class="children"><div class="content">Yup. I was just playing around with this in Javascript yesterday and with ChatGPT&#x27;s help it was surprisingly simple to go from text =&gt; embedding (via. `openai.embeddings.create`) and then to compare the embedding similarity with the cosine distance (which ChatGPT wrote for me): <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;christiangenco&#x2F;3e23925885e3127f2c1775871b8f52f1" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;christiangenco&#x2F;3e23925885e3127f2c177...</a><p>Seems like the next standard feature in every app is going to be natural language search powered by embeddings.</div><br/><div id="40069002" class="c"><input type="checkbox" id="c-40069002" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40068869">parent</a><span>|</span><a href="#40070060">next</a><span>|</span><label class="collapse" for="c-40069002">[-]</label><label class="expand" for="c-40069002">[1 more]</label></div><br/><div class="children"><div class="content">For posterity, OpenAI embeddings come pre-normalized so you can immediately dot-product.<p>Most embeddings providers do normalization by default, and SentenceTransformers has a normalize_embeddings parameter which does that. (it&#x27;s a wrapper around PyTorch&#x27;s F.normalize)</div><br/></div></div></div></div><div id="40070060" class="c"><input type="checkbox" id="c-40070060" checked=""/><div class="controls bullet"><span class="by">ertgbnm</span><span>|</span><a href="#40068468">parent</a><span>|</span><a href="#40068869">prev</a><span>|</span><a href="#40068711">next</a><span>|</span><label class="collapse" for="c-40070060">[-]</label><label class="expand" for="c-40070060">[3 more]</label></div><br/><div class="children"><div class="content">When I&#x27;m messing around, I normally have everything in a Pandas DataFrame already so I just add embeddings as a column and calculate cosine similarity on the fly. Even with a hundred thousand rows, it&#x27;s fast enough to calculate before I can even move my eyes down on the screen to read the output.<p>I regret ever messing around with Pinecone for my tiny and infrequently used set ups.</div><br/><div id="40070490" class="c"><input type="checkbox" id="c-40070490" checked=""/><div class="controls bullet"><span class="by">m1117</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40070060">parent</a><span>|</span><a href="#40070194">next</a><span>|</span><label class="collapse" for="c-40070490">[-]</label><label class="expand" for="c-40070490">[1 more]</label></div><br/><div class="children"><div class="content">Actually, I had a pretty good experience with Pinecone.</div><br/></div></div><div id="40070194" class="c"><input type="checkbox" id="c-40070194" checked=""/><div class="controls bullet"><span class="by">laurshelly</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40070060">parent</a><span>|</span><a href="#40070490">prev</a><span>|</span><a href="#40068711">next</a><span>|</span><label class="collapse" for="c-40070194">[-]</label><label class="expand" for="c-40070194">[1 more]</label></div><br/><div class="children"><div class="content">Could not agree more. For some reason Pandas seems to get phased out as developers advance.</div><br/></div></div></div></div><div id="40068711" class="c"><input type="checkbox" id="c-40068711" checked=""/><div class="controls bullet"><span class="by">twelfthnight</span><span>|</span><a href="#40068468">parent</a><span>|</span><a href="#40070060">prev</a><span>|</span><a href="#40069142">next</a><span>|</span><label class="collapse" for="c-40068711">[-]</label><label class="expand" for="c-40068711">[4 more]</label></div><br/><div class="children"><div class="content">Even in production my guess is most teams would be better off just rolling their own embedding model (huggingface) + caching (redis&#x2F;rocksdb) + FAISS (nearest neighbor) and be good to go. I suppose there is some expertise needed, but working with a vector database vendor has major drawbacks too.</div><br/><div id="40068773" class="c"><input type="checkbox" id="c-40068773" checked=""/><div class="controls bullet"><span class="by">danielbln</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40068711">parent</a><span>|</span><a href="#40068759">next</a><span>|</span><label class="collapse" for="c-40068773">[-]</label><label class="expand" for="c-40068773">[1 more]</label></div><br/><div class="children"><div class="content">Or you just shove it into Postgres + pg_vector and just use the DBMS you already use anyway.</div><br/></div></div><div id="40068759" class="c"><input type="checkbox" id="c-40068759" checked=""/><div class="controls bullet"><span class="by">hackernoteng</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40068711">parent</a><span>|</span><a href="#40068773">prev</a><span>|</span><a href="#40069142">next</a><span>|</span><label class="collapse" for="c-40068759">[-]</label><label class="expand" for="c-40068759">[2 more]</label></div><br/><div class="children"><div class="content">Using Postgres with pgvector is trivial and cheap.  Its also available on AWS RDS.</div><br/><div id="40068859" class="c"><input type="checkbox" id="c-40068859" checked=""/><div class="controls bullet"><span class="by">jonplackett</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40068759">parent</a><span>|</span><a href="#40069142">next</a><span>|</span><label class="collapse" for="c-40068859">[-]</label><label class="expand" for="c-40068859">[1 more]</label></div><br/><div class="children"><div class="content">Also on supabase!</div><br/></div></div></div></div></div></div><div id="40069142" class="c"><input type="checkbox" id="c-40069142" checked=""/><div class="controls bullet"><span class="by">leobg</span><span>|</span><a href="#40068468">parent</a><span>|</span><a href="#40068711">prev</a><span>|</span><a href="#40068892">next</a><span>|</span><label class="collapse" for="c-40069142">[-]</label><label class="expand" for="c-40069142">[1 more]</label></div><br/><div class="children"><div class="content">hnswlib, usearch. Both handle tens of millions of vectors easily. The latter even without holding them in RAM.</div><br/></div></div><div id="40068892" class="c"><input type="checkbox" id="c-40068892" checked=""/><div class="controls bullet"><span class="by">itronitron</span><span>|</span><a href="#40068468">parent</a><span>|</span><a href="#40069142">prev</a><span>|</span><a href="#40072733">next</a><span>|</span><label class="collapse" for="c-40068892">[-]</label><label class="expand" for="c-40068892">[5 more]</label></div><br/><div class="children"><div class="content">Does anyone know the provenance for when vectors started to be called embeddings?</div><br/><div id="40072825" class="c"><input type="checkbox" id="c-40072825" checked=""/><div class="controls bullet"><span class="by">nmfisher</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40068892">parent</a><span>|</span><a href="#40069087">next</a><span>|</span><label class="collapse" for="c-40072825">[-]</label><label class="expand" for="c-40072825">[1 more]</label></div><br/><div class="children"><div class="content">In an NLP context, earliest I could find was ICML 2008:<p><a href="http:&#x2F;&#x2F;machinelearning.org&#x2F;archive&#x2F;icml2008&#x2F;papers&#x2F;391.pdf" rel="nofollow">http:&#x2F;&#x2F;machinelearning.org&#x2F;archive&#x2F;icml2008&#x2F;papers&#x2F;391.pdf</a><p>I&#x27;m sure there are earlier instances, though - the strict mathematical definition of embedding has surely been around for a lot longer.<p>(interestingly, the word2vec papers don&#x27;t use the term either, so I guess it didn&#x27;t enter &quot;common&quot; usage until the mid-late 2010s)</div><br/></div></div><div id="40069087" class="c"><input type="checkbox" id="c-40069087" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40068892">parent</a><span>|</span><a href="#40072825">prev</a><span>|</span><a href="#40072733">next</a><span>|</span><label class="collapse" for="c-40069087">[-]</label><label class="expand" for="c-40069087">[3 more]</label></div><br/><div class="children"><div class="content">I think it was due to GloVe embeddings back then: I don&#x27;t recall them ever being called GloVe vectors, although the &quot;Ve&quot; does stand for vector so it could have been RAS syndrome.</div><br/><div id="40069133" class="c"><input type="checkbox" id="c-40069133" checked=""/><div class="controls bullet"><span class="by">itronitron</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40069087">parent</a><span>|</span><a href="#40072733">next</a><span>|</span><label class="collapse" for="c-40069133">[-]</label><label class="expand" for="c-40069133">[2 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; <a href="https:&#x2F;&#x2F;nlp.stanford.edu&#x2F;projects&#x2F;glove&#x2F;" rel="nofollow">https:&#x2F;&#x2F;nlp.stanford.edu&#x2F;projects&#x2F;glove&#x2F;</a><p>A quick scan of the project website yields zero uses of &#x27;embedding&#x27; and 23 of &#x27;vector&#x27;</div><br/><div id="40069154" class="c"><input type="checkbox" id="c-40069154" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#40068468">root</a><span>|</span><a href="#40069133">parent</a><span>|</span><a href="#40072733">next</a><span>|</span><label class="collapse" for="c-40069154">[-]</label><label class="expand" for="c-40069154">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s how I remember it when I was working with them back in the day (word embeddings): I could be wrong.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40072733" class="c"><input type="checkbox" id="c-40072733" checked=""/><div class="controls bullet"><span class="by">suprgeek</span><span>|</span><a href="#40068468">prev</a><span>|</span><a href="#40072675">next</a><span>|</span><label class="collapse" for="c-40072733">[-]</label><label class="expand" for="c-40072733">[1 more]</label></div><br/><div class="children"><div class="content">Great project and excellent initiative to learn about embeddings.
Two possible avenues to explore more.
Your system backend could be thought of as being composed of two parts:
|Icons-&gt;Embedder-&gt;|PGVector|-&gt;Retriever-&gt;Display Result|<p>1. In the embedder part trying out different embedding models and&#x2F;or vector dimensions to explore if the Recall@K &amp; Precision@K for your data set (icons) improves. Models make a surprising amount of difference to the quality of the results.  Try the MTEB Leaderboard for ideas on which models to explore.<p>2. In the Information Retriever part  you can try a couple of approaches:
a.after you retrieve from PGVector see if you can use a reranker like Cohere to get better results <a href="https:&#x2F;&#x2F;cohere.com&#x2F;blog&#x2F;rerank" rel="nofollow">https:&#x2F;&#x2F;cohere.com&#x2F;blog&#x2F;rerank</a><p>b.You could try a &quot;fusion ranking&quot; similar  to the one you do but structured such that 50% of the weight is for a plain old keyword search in the metadata and 50% is for the embedding based search<p>Finally something more interesting to noodle on - what if the embeddings were based on the icon images and the model knew how to search for a textual descriptions in the latent space?</div><br/></div></div><div id="40072675" class="c"><input type="checkbox" id="c-40072675" checked=""/><div class="controls bullet"><span class="by">voxelc4L</span><span>|</span><a href="#40072733">prev</a><span>|</span><a href="#40074041">next</a><span>|</span><label class="collapse" for="c-40072675">[-]</label><label class="expand" for="c-40072675">[1 more]</label></div><br/><div class="children"><div class="content">It begs the question though, doesn&#x27;t it...? Embeddings require a neural network or some reasonable facsimile to produce the embedding in the first place. Compression to a vector (a semantic space of some sort) still needs to happen – and that&#x27;s the crux of the understanding&#x2F;meaning. To just say &quot;embeddings are cool let&#x27;s use them&quot; is ignoring the core problem of semantics&#x2F;meaning&#x2F;information-in-context etc. Knowing where an embedding came from is pretty damn important.<p>Embeddings live a very biased existence. They are the product of a network (or some algorithm) that was trained (or built) with specific data (and&#x2F;or code) and assume particular biases intrinsically (network structure&#x2F;algorithm) or extrinsically (e.g., data used to train a network) which they impose on the translation of data into some n-dimensional space. Any engineered solution always lives with such limitations, but with the advent of more and more sophisticated methods for the generation of them, I feel like it&#x27;s becoming more about the result than the process. This strikes me as problematic on a global scale... might be fine for local problems but could be not-so-great in an ever changing world.</div><br/></div></div><div id="40074041" class="c"><input type="checkbox" id="c-40074041" checked=""/><div class="controls bullet"><span class="by">pantulis</span><span>|</span><a href="#40072675">prev</a><span>|</span><a href="#40071401">next</a><span>|</span><label class="collapse" for="c-40074041">[-]</label><label class="expand" for="c-40074041">[2 more]</label></div><br/><div class="children"><div class="content">I strongly agree with the title of the article. RAG is very interesting right now just as an example of how technology moves from being just fresh out of academia to being engineered and commoditized into regular out of the shelf tools.   On the other hand I don&#x27;t think it&#x27;s <i>that</i> important to understand how embeddings are calculated, for the beginner it&#x27;s more important to showcase why they work and why they enable simple reasoning like &quot;queen = woman + (king - men)&quot;  and the possible use cases.</div><br/></div></div><div id="40071401" class="c"><input type="checkbox" id="c-40071401" checked=""/><div class="controls bullet"><span class="by">adamgordonbell</span><span>|</span><a href="#40074041">prev</a><span>|</span><a href="#40067877">next</a><span>|</span><label class="collapse" for="c-40071401">[-]</label><label class="expand" for="c-40071401">[1 more]</label></div><br/><div class="children"><div class="content">My problem with this is that it doesn&#x27;t explain a lot.<p>You can manually make a vector of a word and then step wise get up to word2vec approach and then document embedding. My post[1] does some of the first part and this great word2vec post[2] dives into it in more detail.<p>[1] <a href="https:&#x2F;&#x2F;earthly.dev&#x2F;blog&#x2F;cosine_similarity_text_embeddings&#x2F;" rel="nofollow">https:&#x2F;&#x2F;earthly.dev&#x2F;blog&#x2F;cosine_similarity_text_embeddings&#x2F;</a><p>[2] <a href="https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-word2vec&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-word2vec&#x2F;</a></div><br/></div></div><div id="40067877" class="c"><input type="checkbox" id="c-40067877" checked=""/><div class="controls bullet"><span class="by">dullcrisp</span><span>|</span><a href="#40071401">prev</a><span>|</span><a href="#40068791">next</a><span>|</span><label class="collapse" for="c-40067877">[-]</label><label class="expand" for="c-40067877">[19 more]</label></div><br/><div class="children"><div class="content">Is there any easy way to run the embedding logic locally? Maybe even locally to the database? My understanding is that they’re hitting OpenAI’s API to get the embedding for each search query and then storing that in the database. I wouldn’t want my search function to be dependent on OpenAI if I could help it.</div><br/><div id="40068734" class="c"><input type="checkbox" id="c-40068734" checked=""/><div class="controls bullet"><span class="by">laktek</span><span>|</span><a href="#40067877">parent</a><span>|</span><a href="#40068337">next</a><span>|</span><label class="collapse" for="c-40068734">[-]</label><label class="expand" for="c-40068734">[5 more]</label></div><br/><div class="children"><div class="content">If you are building using Supabase stack (Postgres as DB with pgVector), we just released a built-in embedding generation API yesterday. This works both locally (in CPUs) and you can deploy it without any modifications.<p>Check this video on building Semantic Search in Supabase: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;w4Rr_1whU-U" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;w4Rr_1whU-U</a><p>Also, the blog on announcement with links to text versions of the tutorials: <a href="https:&#x2F;&#x2F;supabase.com&#x2F;blog&#x2F;ai-inference-now-available-in-supabase-edge-functions">https:&#x2F;&#x2F;supabase.com&#x2F;blog&#x2F;ai-inference-now-available-in-supa...</a></div><br/><div id="40073477" class="c"><input type="checkbox" id="c-40073477" checked=""/><div class="controls bullet"><span class="by">_bramses</span><span>|</span><a href="#40067877">root</a><span>|</span><a href="#40068734">parent</a><span>|</span><a href="#40068911">next</a><span>|</span><label class="collapse" for="c-40073477">[-]</label><label class="expand" for="c-40073477">[2 more]</label></div><br/><div class="children"><div class="content">neat! one thing i’d really love tooling for: supporting multi user apps where each has their own siloed data and embeddings. i find myself having to set up databases from scratch for all my clients, which results in a lot of repetitive work. i’d love to have the ability one day to easily add users to the same db and let them get to embedding without having to have any knowledge going in</div><br/><div id="40073801" class="c"><input type="checkbox" id="c-40073801" checked=""/><div class="controls bullet"><span class="by">kiwicopple</span><span>|</span><a href="#40067877">root</a><span>|</span><a href="#40073477">parent</a><span>|</span><a href="#40068911">next</a><span>|</span><label class="collapse" for="c-40073801">[-]</label><label class="expand" for="c-40073801">[1 more]</label></div><br/><div class="children"><div class="content">This is possible in supabase. You can store all the data in a table and restrict access with Row Level Security<p>You also have various ways to separate the data for indexes&#x2F;performance<p>- use metadata filtering first (eg: filter by customer ID prior to running a semantic search). This is fast in postgres since its a relational DB<p>- pgvector supports partial indexes - create one per customer based on a customer ID column<p>- use table partitions<p>- use Foreign Data Wrappers (more involved but scales horizontally)</div><br/></div></div></div></div><div id="40068911" class="c"><input type="checkbox" id="c-40068911" checked=""/><div class="controls bullet"><span class="by">jonplackett</span><span>|</span><a href="#40067877">root</a><span>|</span><a href="#40068734">parent</a><span>|</span><a href="#40073477">prev</a><span>|</span><a href="#40068337">next</a><span>|</span><label class="collapse" for="c-40068911">[-]</label><label class="expand" for="c-40068911">[2 more]</label></div><br/><div class="children"><div class="content">So handy! I already got some embeddings working with  supabase pgvector and OpenAI and it worked great.<p>What would the cost of running this be like compared to the OpenAI embedding api?</div><br/><div id="40069884" class="c"><input type="checkbox" id="c-40069884" checked=""/><div class="controls bullet"><span class="by">laktek</span><span>|</span><a href="#40067877">root</a><span>|</span><a href="#40068911">parent</a><span>|</span><a href="#40068337">next</a><span>|</span><label class="collapse" for="c-40069884">[-]</label><label class="expand" for="c-40069884">[1 more]</label></div><br/><div class="children"><div class="content">There are no extra costs other than the what we&#x27;d normally charge for Edge Function invocations (you get up to 500K in the free plan and 2M in the Pro plan)</div><br/></div></div></div></div></div></div><div id="40068337" class="c"><input type="checkbox" id="c-40068337" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#40067877">parent</a><span>|</span><a href="#40068734">prev</a><span>|</span><a href="#40068218">next</a><span>|</span><label class="collapse" for="c-40068337">[-]</label><label class="expand" for="c-40068337">[1 more]</label></div><br/><div class="children"><div class="content">Support for _some_ embedding models works in Ollama (and llama.cpp - Bert models specifically)<p><pre><code>  ollama pull all-minilm

  curl http:&#x2F;&#x2F;localhost:11434&#x2F;api&#x2F;embeddings -d &#x27;{
    &quot;model&quot;: &quot;all-minilm&quot;,
    &quot;prompt&quot;: &quot;Here is an article about llamas...&quot;
  }&#x27;
</code></pre>
Embedding models run quite well even on CPU since they are smaller models. There are other implementations with a library form factor like transformers.js <a href="https:&#x2F;&#x2F;xenova.github.io&#x2F;transformers.js&#x2F;" rel="nofollow">https:&#x2F;&#x2F;xenova.github.io&#x2F;transformers.js&#x2F;</a> and sentence-transformers <a href="https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;sentence-transformers&#x2F;" rel="nofollow">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;sentence-transformers&#x2F;</a></div><br/></div></div><div id="40068218" class="c"><input type="checkbox" id="c-40068218" checked=""/><div class="controls bullet"><span class="by">ngalstyan4</span><span>|</span><a href="#40067877">parent</a><span>|</span><a href="#40068337">prev</a><span>|</span><a href="#40068091">next</a><span>|</span><label class="collapse" for="c-40068218">[-]</label><label class="expand" for="c-40068218">[3 more]</label></div><br/><div class="children"><div class="content">We provide this functionality in Lantern cloud via our Lantern Extras extension: &lt;<a href="https:&#x2F;&#x2F;github.com&#x2F;lanterndata&#x2F;lantern_extras">https:&#x2F;&#x2F;github.com&#x2F;lanterndata&#x2F;lantern_extras</a>&gt;<p>You can generate CLIP embeddings locally on the DB server via:<p><pre><code>  SELECT abstract,
       introduction,
       figure1,
       clip_text(abstract) AS abstract_ai,
       clip_text(introduction) AS introduction_ai,
       clip_image(figure1) AS figure1_ai
  INTO papers_augmented
  FROM papers;
</code></pre>
Then you can search for embeddings via:<p><pre><code>  SELECT abstract, introduction FROM papers_augmented ORDER BY clip_text(query) &lt;=&gt; abstract_ai LIMIT 10;
</code></pre>
The approach significantly decreases search latency and results in cleaner code.
As an added bonus, EXPLAIN ANALYZE can now tell percentage of time spent in embedding generation vs search.<p>The linked library enables embedding generation for a dozen open source models and proprietary APIs (list here: &lt;<a href="https:&#x2F;&#x2F;lantern.dev&#x2F;docs&#x2F;develop&#x2F;generate">https:&#x2F;&#x2F;lantern.dev&#x2F;docs&#x2F;develop&#x2F;generate</a>&gt;, and adding new ones is really easy.</div><br/><div id="40068844" class="c"><input type="checkbox" id="c-40068844" checked=""/><div class="controls bullet"><span class="by">charlieyuan</span><span>|</span><a href="#40067877">root</a><span>|</span><a href="#40068218">parent</a><span>|</span><a href="#40068091">next</a><span>|</span><label class="collapse" for="c-40068844">[-]</label><label class="expand" for="c-40068844">[2 more]</label></div><br/><div class="children"><div class="content">Lantern seems really cool! Interestingly we did try CLIP (openclip) image embeddings but the results were poor for 24px by 24px icons. Any ideas?<p>Charlie @ v0.app</div><br/><div id="40069731" class="c"><input type="checkbox" id="c-40069731" checked=""/><div class="controls bullet"><span class="by">ngalstyan4</span><span>|</span><a href="#40067877">root</a><span>|</span><a href="#40068844">parent</a><span>|</span><a href="#40068091">next</a><span>|</span><label class="collapse" for="c-40069731">[-]</label><label class="expand" for="c-40069731">[1 more]</label></div><br/><div class="children"><div class="content">I have tried CLIP on my personal photo album collection and it worked really well there - I could write detailed scene descriptions of past road trips, and the photos I had in mind would pop up. Probably the model is better for everyday photos than for icons</div><br/></div></div></div></div></div></div><div id="40068091" class="c"><input type="checkbox" id="c-40068091" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40067877">parent</a><span>|</span><a href="#40068218">prev</a><span>|</span><a href="#40067993">next</a><span>|</span><label class="collapse" for="c-40068091">[-]</label><label class="expand" for="c-40068091">[1 more]</label></div><br/><div class="children"><div class="content">There are a bunch of embedding models you can run on your own machine. My LLM tool had plugins for some of those:<p>- <a href="https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;plugins&#x2F;directory.html#embedding-models" rel="nofollow">https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;plugins&#x2F;directory.html#em...</a><p>Here&#x27;s how to use them: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Sep&#x2F;4&#x2F;llm-embeddings&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Sep&#x2F;4&#x2F;llm-embeddings&#x2F;</a></div><br/></div></div><div id="40067993" class="c"><input type="checkbox" id="c-40067993" checked=""/><div class="controls bullet"><span class="by">dvt</span><span>|</span><a href="#40067877">parent</a><span>|</span><a href="#40068091">prev</a><span>|</span><a href="#40073074">next</a><span>|</span><label class="collapse" for="c-40067993">[-]</label><label class="expand" for="c-40067993">[2 more]</label></div><br/><div class="children"><div class="content">Yes, I use fastembed-rs[1] in a project I&#x27;m working on and it runs flawlessly. You can store the embeddings in any boring database (it&#x27;s just an array of f32s at the end of the day). But for fast vector math (which you need for similarity search), a vector database is recommended, e.g. the pgvector[2] postgres extension.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;Anush008&#x2F;fastembed-rs">https:&#x2F;&#x2F;github.com&#x2F;Anush008&#x2F;fastembed-rs</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;pgvector&#x2F;pgvector">https:&#x2F;&#x2F;github.com&#x2F;pgvector&#x2F;pgvector</a></div><br/><div id="40069608" class="c"><input type="checkbox" id="c-40069608" checked=""/><div class="controls bullet"><span class="by">J_Shelby_J</span><span>|</span><a href="#40067877">root</a><span>|</span><a href="#40067993">parent</a><span>|</span><a href="#40073074">next</a><span>|</span><label class="collapse" for="c-40069608">[-]</label><label class="expand" for="c-40069608">[1 more]</label></div><br/><div class="children"><div class="content">Fun timing!<p>I literally just published my first crate: candle_embed[1]<p>It uses Candle under the hood (the crate is more of a user friendly wrapper) and lets you use any model on HF like the new SoTA model from Snowflake[2].<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;ShelbyJenkins&#x2F;candle_embed">https:&#x2F;&#x2F;github.com&#x2F;ShelbyJenkins&#x2F;candle_embed</a>
[2] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Snowflake&#x2F;snowflake-arctic-embed-l" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;Snowflake&#x2F;snowflake-arctic-embed-l</a></div><br/></div></div></div></div><div id="40073074" class="c"><input type="checkbox" id="c-40073074" checked=""/><div class="controls bullet"><span class="by">internet101010</span><span>|</span><a href="#40067877">parent</a><span>|</span><a href="#40067993">prev</a><span>|</span><a href="#40070082">next</a><span>|</span><label class="collapse" for="c-40073074">[-]</label><label class="expand" for="c-40073074">[1 more]</label></div><br/><div class="children"><div class="content">Open WebUI has langchain built-in and integrates perfectly with ollama. They have several variations of docker compose files on their github.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;open-webui&#x2F;open-webui">https:&#x2F;&#x2F;github.com&#x2F;open-webui&#x2F;open-webui</a></div><br/></div></div><div id="40069916" class="c"><input type="checkbox" id="c-40069916" checked=""/><div class="controls bullet"><span class="by">jonnycoder</span><span>|</span><a href="#40067877">parent</a><span>|</span><a href="#40070082">prev</a><span>|</span><a href="#40068901">next</a><span>|</span><label class="collapse" for="c-40069916">[-]</label><label class="expand" for="c-40069916">[1 more]</label></div><br/><div class="children"><div class="content">The MTEB leaderboard has you covered. That is a goto for finding the leading embedding models and I believe many of them can run locally.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard</a></div><br/></div></div><div id="40068901" class="c"><input type="checkbox" id="c-40068901" checked=""/><div class="controls bullet"><span class="by">bryantwolf</span><span>|</span><a href="#40067877">parent</a><span>|</span><a href="#40069916">prev</a><span>|</span><a href="#40068132">next</a><span>|</span><label class="collapse" for="c-40068901">[-]</label><label class="expand" for="c-40068901">[1 more]</label></div><br/><div class="children"><div class="content">This is a good call out. OpenAI embeddings were simple to stand up, pretty good, cheap at this scale, and accessible to everyone. I think that makes them a good starting point for many people. That said, they&#x27;re closed-source, and there are open-source embeddings you can run on your infrastructure to reduce external dependencies.</div><br/></div></div><div id="40068132" class="c"><input type="checkbox" id="c-40068132" checked=""/><div class="controls bullet"><span class="by">notakash</span><span>|</span><a href="#40067877">parent</a><span>|</span><a href="#40068901">prev</a><span>|</span><a href="#40068074">next</a><span>|</span><label class="collapse" for="c-40068132">[-]</label><label class="expand" for="c-40068132">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re building an iOS app, I&#x27;ve had success storing vectors in coredata and using a tiny coreml model that runs on device for embedding and then doing cosine similarity.</div><br/></div></div></div></div><div id="40068791" class="c"><input type="checkbox" id="c-40068791" checked=""/><div class="controls bullet"><span class="by">gchadwick</span><span>|</span><a href="#40067877">prev</a><span>|</span><a href="#40068588">next</a><span>|</span><label class="collapse" for="c-40068791">[-]</label><label class="expand" for="c-40068791">[4 more]</label></div><br/><div class="children"><div class="content">For an article extolling the benefits of embeddings for developers looking to dip their toe into the waters of AI it&#x27;s odd they don&#x27;t actually have an intro to embeddings or to vector databases. They just assume the reader already knows these concepts and dives on in to how they use them.<p>Sure many do know these concepts already but they&#x27;re probably not the people wondering about a &#x27;good starting point for the AI curious app developer&#x27;.</div><br/><div id="40069259" class="c"><input type="checkbox" id="c-40069259" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40068791">parent</a><span>|</span><a href="#40068867">next</a><span>|</span><label class="collapse" for="c-40069259">[-]</label><label class="expand" for="c-40069259">[1 more]</label></div><br/><div class="children"><div class="content">I published this pretty comprehensive intro to embeddings last year: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Oct&#x2F;23&#x2F;embeddings&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Oct&#x2F;23&#x2F;embeddings&#x2F;</a></div><br/></div></div><div id="40068867" class="c"><input type="checkbox" id="c-40068867" checked=""/><div class="controls bullet"><span class="by">charlieyuan</span><span>|</span><a href="#40068791">parent</a><span>|</span><a href="#40069259">prev</a><span>|</span><a href="#40069835">next</a><span>|</span><label class="collapse" for="c-40068867">[-]</label><label class="expand" for="c-40068867">[1 more]</label></div><br/><div class="children"><div class="content">Apologies!<p>Here&#x27;s a good primer on embeddings from openai:
<a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;embeddings" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;embeddings</a></div><br/></div></div><div id="40069835" class="c"><input type="checkbox" id="c-40069835" checked=""/><div class="controls bullet"><span class="by">gk1</span><span>|</span><a href="#40068791">parent</a><span>|</span><a href="#40068867">prev</a><span>|</span><a href="#40068588">next</a><span>|</span><label class="collapse" for="c-40069835">[-]</label><label class="expand" for="c-40069835">[1 more]</label></div><br/><div class="children"><div class="content">To add to the other recommendations, here&#x27;s a primer on vector DB&#x27;s: <a href="https:&#x2F;&#x2F;www.pinecone.io&#x2F;learn&#x2F;vector-database&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.pinecone.io&#x2F;learn&#x2F;vector-database&#x2F;</a></div><br/></div></div></div></div><div id="40068588" class="c"><input type="checkbox" id="c-40068588" checked=""/><div class="controls bullet"><span class="by">kaycebasques</span><span>|</span><a href="#40068791">prev</a><span>|</span><a href="#40072601">next</a><span>|</span><label class="collapse" for="c-40068588">[-]</label><label class="expand" for="c-40068588">[1 more]</label></div><br/><div class="children"><div class="content">I have been saying similar things to my fellow technical writers ever since the ChatGPT explosion. We now have a tool that makes semantic search on arbitrary, diverse input much easier. Improved semantic search could make a lot of common technical writing workflows much more efficient. E.g. speeding up the mandatory research that you must do before it&#x27;s even possible to write an effective doc.</div><br/></div></div><div id="40072601" class="c"><input type="checkbox" id="c-40072601" checked=""/><div class="controls bullet"><span class="by">thomasfromcdnjs</span><span>|</span><a href="#40068588">prev</a><span>|</span><a href="#40069216">next</a><span>|</span><label class="collapse" for="c-40072601">[-]</label><label class="expand" for="c-40072601">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been adding embeddings to every project I work on for the purpose of vector similarity searches.<p>I was just trying to order uber eats and wondering why they don&#x27;t have a better search based off embeddings.<p>Almost finished building a feature on JSON Resume, that takes your hosted resume and WhoIsHiring job posts and uses embeddings to return relevant results -&gt; <a href="https:&#x2F;&#x2F;registry.jsonresume.org&#x2F;thomasdavis&#x2F;jobs" rel="nofollow">https:&#x2F;&#x2F;registry.jsonresume.org&#x2F;thomasdavis&#x2F;jobs</a></div><br/></div></div><div id="40069216" class="c"><input type="checkbox" id="c-40069216" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#40072601">prev</a><span>|</span><a href="#40068138">next</a><span>|</span><label class="collapse" for="c-40069216">[-]</label><label class="expand" for="c-40069216">[15 more]</label></div><br/><div class="children"><div class="content">My smooth brain might not understand this properly, but the idea is we generate embeddings, store them, then use retrieval each time we want to use them.<p>For simple things we might not need to worry about storing much, we can generate the embeddings and just cache them or send them straight to retrieval as an array or something...<p>The storing of embeddings seems the hard part, do I need a special database or PG extension? Is there any reason I can&#x27;t store them as a blobs in SQlite if I don&#x27;t have THAT much data, and I don&#x27;t care too much about speed? Do embeddings generated ever &#x27;expire&#x27;?</div><br/><div id="40070063" class="c"><input type="checkbox" id="c-40070063" checked=""/><div class="controls bullet"><span class="by">bryantwolf</span><span>|</span><a href="#40069216">parent</a><span>|</span><a href="#40069601">next</a><span>|</span><label class="collapse" for="c-40070063">[-]</label><label class="expand" for="c-40070063">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;d have to update the embedding every time the data used to generate it changes. For example, if you had an embedding for user profiles and they updated their bio, you would want to make a new embedding.<p>I don&#x27;t expect to have to change the embeddings for each icon all that often, so storing them seemed like a good choice. However, you probably don&#x27;t need to cache the embedding for each search query since there will be long-tail ones that don&#x27;t change that much.<p>The reason to use pgvector over blobs is if you want to use the distance functions in your queries.</div><br/></div></div><div id="40069601" class="c"><input type="checkbox" id="c-40069601" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#40069216">parent</a><span>|</span><a href="#40070063">prev</a><span>|</span><a href="#40069293">next</a><span>|</span><label class="collapse" for="c-40069601">[-]</label><label class="expand" for="c-40069601">[5 more]</label></div><br/><div class="children"><div class="content">Yes, you can shove the embeddings in a BLOB, but then you can&#x27;t do the kinds of query operations you expect to be able to do with embeddings.</div><br/><div id="40071110" class="c"><input type="checkbox" id="c-40071110" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40069216">root</a><span>|</span><a href="#40069601">parent</a><span>|</span><a href="#40069653">next</a><span>|</span><label class="collapse" for="c-40071110">[-]</label><label class="expand" for="c-40071110">[1 more]</label></div><br/><div class="children"><div class="content">You can run similarity scores with a custom SQLite function.<p>I use a Python one usually, but it&#x27;s also possible to build a much faster one in C: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Mar&#x2F;23&#x2F;building-c-extensions-for-sqlite-with-chatgpt-code-interpreter&#x2F;#more-ambitious" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Mar&#x2F;23&#x2F;building-c-extensions-...</a></div><br/></div></div><div id="40069653" class="c"><input type="checkbox" id="c-40069653" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#40069216">root</a><span>|</span><a href="#40069601">parent</a><span>|</span><a href="#40071110">prev</a><span>|</span><a href="#40069293">next</a><span>|</span><label class="collapse" for="c-40069653">[-]</label><label class="expand" for="c-40069653">[3 more]</label></div><br/><div class="children"><div class="content">Right like you could use it sort of like cache and send the blobs to OpenAI to use their similarity API, but you couldn&#x27;t really use SQL to do cosine similarity operations?<p>My understanding of what&#x27;s going on at a technical level might be a bit limited.</div><br/><div id="40070493" class="c"><input type="checkbox" id="c-40070493" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#40069216">root</a><span>|</span><a href="#40069653">parent</a><span>|</span><a href="#40069293">next</a><span>|</span><label class="collapse" for="c-40070493">[-]</label><label class="expand" for="c-40070493">[2 more]</label></div><br/><div class="children"><div class="content">Yes.<p>Although if you really wanted to, and normalized your data like a good little Edgar F. Codd devotee, you could write something like this:<p>SELECT SUM(v.dot) &#x2F; (SQRT(SUM(v.v1)) * SQRT(SUM(v.v2))) FROM (SELECT v1.dimension as dim, v1.value as v1, v2.value as v2, v1.value * v2.value as dot FROM vectors as v1 INNER JOIN vectors as v2 ON v1.dimension = v2.dimension WHERE v1.vector_id = &quot;?&quot; AND v2.vector_id = &quot;?&quot;) as v;<p>This assumes one table called &quot;vectors&quot; with columns vector_id, dimension, and value; vector_id and dimension being primary. The inner query grabs two vectors as separate columns with some self-join trickery, computes the product of each component, and then the outer query computes aggregate functions on the inner query to do the actual cosine similarity.<p>No I have not tested this on an actual database engine, I probably screwed up the SQL somehow. And obviously it&#x27;s easier to just have a database (or Postgres extension) that recognizes vector data as a distinct data type and gives you a dedicated cosine-similarity function.</div><br/><div id="40070751" class="c"><input type="checkbox" id="c-40070751" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#40069216">root</a><span>|</span><a href="#40070493">parent</a><span>|</span><a href="#40069293">next</a><span>|</span><label class="collapse" for="c-40070751">[-]</label><label class="expand" for="c-40070751">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the explanation! Appreciate that you took the time to give an example. Makes a lot more sense why we reach for specific tools for this.</div><br/></div></div></div></div></div></div></div></div><div id="40069293" class="c"><input type="checkbox" id="c-40069293" checked=""/><div class="controls bullet"><span class="by">laborcontract</span><span>|</span><a href="#40069216">parent</a><span>|</span><a href="#40069601">prev</a><span>|</span><a href="#40069267">next</a><span>|</span><label class="collapse" for="c-40069293">[-]</label><label class="expand" for="c-40069293">[2 more]</label></div><br/><div class="children"><div class="content">A KV store is both good enough and highly performant. I use Redis for storing embeddings and expire them after a while. Unless you have a highly specialized use case it’s not economical to persistently store chunk embedding.<p>Redis also does have vector search capability as well. However, the most popular answer you’ll get here is to use Postgres (pgvectpr).</div><br/><div id="40069367" class="c"><input type="checkbox" id="c-40069367" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#40069216">root</a><span>|</span><a href="#40069293">parent</a><span>|</span><a href="#40069267">next</a><span>|</span><label class="collapse" for="c-40069367">[-]</label><label class="expand" for="c-40069367">[1 more]</label></div><br/><div class="children"><div class="content">Redis sounds like a good option. I like that it’s not more infrastructure, I already have redis setup for my app so I’m not adding more to the stack.</div><br/></div></div></div></div><div id="40069267" class="c"><input type="checkbox" id="c-40069267" checked=""/><div class="controls bullet"><span class="by">H1Supreme</span><span>|</span><a href="#40069216">parent</a><span>|</span><a href="#40069293">prev</a><span>|</span><a href="#40069281">next</a><span>|</span><label class="collapse" for="c-40069267">[-]</label><label class="expand" for="c-40069267">[2 more]</label></div><br/><div class="children"><div class="content">Vector databases are used to store embeddings.</div><br/><div id="40069402" class="c"><input type="checkbox" id="c-40069402" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#40069216">root</a><span>|</span><a href="#40069267">parent</a><span>|</span><a href="#40069281">next</a><span>|</span><label class="collapse" for="c-40069402">[-]</label><label class="expand" for="c-40069402">[1 more]</label></div><br/><div class="children"><div class="content">But why is that? I’m sure it’s the ‘best’ way to do things, but it also means more infrastructure which for simple apps isn’t worth the hassle.<p>I should use redis for queues but often I’ll just use a table in a SQLite database. For small scale projects I find it works fine, I’m wondering what an equivalent simple option for embeddings would be.</div><br/></div></div></div></div><div id="40069540" class="c"><input type="checkbox" id="c-40069540" checked=""/><div class="controls bullet"><span class="by">alexgarcia-xyz</span><span>|</span><a href="#40069216">parent</a><span>|</span><a href="#40069281">prev</a><span>|</span><a href="#40071094">next</a><span>|</span><label class="collapse" for="c-40069540">[-]</label><label class="expand" for="c-40069540">[2 more]</label></div><br/><div class="children"><div class="content">Re storing vectors in BLOB columns: ya, if it&#x27;s not a lot of data and it&#x27;s fast enough for you, then there&#x27;s no problem doing it like that. I&#x27;d even just store then in JSON&#x2F;npy files first and see how long you can get away with it. Once that gets too slow, then try SQLite&#x2F;redis&#x2F;valkey, and when that gets too slow, look into pgvector or other vector database solutions.<p>For SQLite specifically, very large BLOB columns might effect query performance, especially for large embeddings. For example, a 1536-dimension vector from OpenAI would take 1536 * 4  = 6144 bytes of space, if stored in a compact BLOB format. That&#x27;s larger than SQLite default page size of 4096, so that extra data will overflow into overflow pages. Which again, isn&#x27;t too big of a deal, but if the original table had small values before, then table scans can be slower.<p>One solution is to move it to a separate table, ex on an original `users` table, you can make a new `CREATE TABLE users_embeddings(user_id, embedding)` table and just LEFT JOIN that when you need it. Or you can use new techniques like Matryoshka embeddings[0] or scalar&#x2F;binary quantization[1] to reduce the size of individual vectors, at the cost of lower accuracy. Or you can bump the page size of your SQLite database with `PRAGMA page_size=8192`.<p>I also have a SQLite extension for vector search[2], but there&#x27;s a number of usability&#x2F;ergonomic issues with it. I&#x27;m making a new one that I hope to release soon, which will hopefully be a great middle ground between &quot;store vectors in a .npy files&quot; and &quot;use pgvector&quot;.<p>Re &quot;do embeddings ever expire&quot;: nope! As long as you have access to the same model, the same text input should give the same embedding output. It&#x27;s not like LLMs that have temperatures&#x2F;meta prompts&#x2F;a million other dials that make outputs non-deterministic, most embedding models should be deterministic and should work forever.<p>[0] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;matryoshka" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;matryoshka</a><p>[1] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;embedding-quantization" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;embedding-quantization</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;asg017&#x2F;sqlite-vss">https:&#x2F;&#x2F;github.com&#x2F;asg017&#x2F;sqlite-vss</a></div><br/><div id="40069719" class="c"><input type="checkbox" id="c-40069719" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#40069216">root</a><span>|</span><a href="#40069540">parent</a><span>|</span><a href="#40071094">next</a><span>|</span><label class="collapse" for="c-40069719">[-]</label><label class="expand" for="c-40069719">[1 more]</label></div><br/><div class="children"><div class="content">This is very useful appreciate the insight. Storing embeddings in a table and joining when needed feels like a really nice solution for what I&#x27;m trying to do.</div><br/></div></div></div></div><div id="40071094" class="c"><input type="checkbox" id="c-40071094" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40069216">parent</a><span>|</span><a href="#40069540">prev</a><span>|</span><a href="#40068138">next</a><span>|</span><label class="collapse" for="c-40071094">[-]</label><label class="expand" for="c-40071094">[1 more]</label></div><br/><div class="children"><div class="content">I store them as blobs in SQLite. It works fine - depending on the model they take up 1-2KB each.</div><br/></div></div></div></div><div id="40068138" class="c"><input type="checkbox" id="c-40068138" checked=""/><div class="controls bullet"><span class="by">patrick-fitz</span><span>|</span><a href="#40069216">prev</a><span>|</span><a href="#40068499">next</a><span>|</span><label class="collapse" for="c-40068138">[-]</label><label class="expand" for="c-40068138">[2 more]</label></div><br/><div class="children"><div class="content">Nice project! I find it can be hard to think of a idea that is well suited to use AI. Using embeddings for search is definitely a good option to start with.</div><br/><div id="40068538" class="c"><input type="checkbox" id="c-40068538" checked=""/><div class="controls bullet"><span class="by">ParanoidShroom</span><span>|</span><a href="#40068138">parent</a><span>|</span><a href="#40068499">next</a><span>|</span><label class="collapse" for="c-40068538">[-]</label><label class="expand" for="c-40068538">[1 more]</label></div><br/><div class="children"><div class="content">I made a reverse image search when I learned about embeddings.
It&#x27;s pretty fun to work with images
<a href="https:&#x2F;&#x2F;medium.com&#x2F;@christophe.smet1&#x2F;finding-dirty-xtc-with-applied-machine-learning-61d1f9a2e857" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@christophe.smet1&#x2F;finding-dirty-xtc-with-...</a></div><br/></div></div></div></div><div id="40068499" class="c"><input type="checkbox" id="c-40068499" checked=""/><div class="controls bullet"><span class="by">dvaun</span><span>|</span><a href="#40068138">prev</a><span>|</span><a href="#40069629">next</a><span>|</span><label class="collapse" for="c-40068499">[-]</label><label class="expand" for="c-40068499">[2 more]</label></div><br/><div class="children"><div class="content">I’d love to build a suite of local tooling to play around with different embedding approaches.<p>I’ve had great results using SentenceTransformers for quick one-off tasks at work for unique data asks.<p>I’m curious about clustering within the embeddings and seeing what different approaches can yield and what applications they work best for.</div><br/><div id="40068720" class="c"><input type="checkbox" id="c-40068720" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#40068499">parent</a><span>|</span><a href="#40069629">next</a><span>|</span><label class="collapse" for="c-40068720">[-]</label><label class="expand" for="c-40068720">[1 more]</label></div><br/><div class="children"><div class="content">If I have 50,000 historical articles and 5,000 new articles I apply SBERT and then k-means with N=20 I get great results in terms of articles about Ukraine, sports, chemistry, and nerdcore from Lobsters ending up in distinct clusters.<p>I’ve used DBSCAN for finding duplicate content,  this is less successful.  With the parameters I am using it is rare for there to be a false positives, but there aren’t that many true positives.  I’m sure I could do do better if I tuned it up but I’m not sure if there is an operating point I’d really like.</div><br/></div></div></div></div><div id="40069629" class="c"><input type="checkbox" id="c-40069629" checked=""/><div class="controls bullet"><span class="by">clementmas</span><span>|</span><a href="#40068499">prev</a><span>|</span><a href="#40068463">next</a><span>|</span><label class="collapse" for="c-40069629">[-]</label><label class="expand" for="c-40069629">[2 more]</label></div><br/><div class="children"><div class="content">Embeddings are indeed a good starting point. Next step is choosing the model and the database. The comments here have been taken over by database companies so I&#x27;m skeptical about the opinions. I wish MySQL had a cosine search feature built in</div><br/><div id="40069658" class="c"><input type="checkbox" id="c-40069658" checked=""/><div class="controls bullet"><span class="by">bootsmann</span><span>|</span><a href="#40069629">parent</a><span>|</span><a href="#40068463">next</a><span>|</span><label class="collapse" for="c-40069658">[-]</label><label class="expand" for="c-40069658">[1 more]</label></div><br/><div class="children"><div class="content">pg_vector has you covered</div><br/></div></div></div></div><div id="40068463" class="c"><input type="checkbox" id="c-40068463" checked=""/><div class="controls bullet"><span class="by">EcommerceFlow</span><span>|</span><a href="#40069629">prev</a><span>|</span><a href="#40069064">next</a><span>|</span><label class="collapse" for="c-40068463">[-]</label><label class="expand" for="c-40068463">[1 more]</label></div><br/><div class="children"><div class="content">Embeddings have a special place in my heart since I learned about them 2 years ago. Working in SEO, it felt like everything finally &quot;clicked&quot; and I understood, on a lower level, how Google search actually works, how they&#x27;re able to show specific content snippets directly on the search results page, etc. I never found any &quot;SEO Guru&quot; discussing this at all back then (maybe even now?), even though this was complete gold. It explains &quot;topical authority&quot; and gave you clues on how Google itself understands it.</div><br/></div></div><div id="40069064" class="c"><input type="checkbox" id="c-40069064" checked=""/><div class="controls bullet"><span class="by">hot_gril</span><span>|</span><a href="#40068463">prev</a><span>|</span><a href="#40068479">next</a><span>|</span><label class="collapse" for="c-40069064">[-]</label><label class="expand" for="c-40069064">[4 more]</label></div><br/><div class="children"><div class="content">This is where I got started too. Glove embedding stored in Postgres.<p>Pgvector is nice, and it&#x27;s cool seeing quick tutorials using it. Back then, we only had cube, which didn&#x27;t do cosine similarity indexing out of the box (you had to normalize vectors and use euclidean indexes) and only supported up to 100 dimensions. And there were maybe other inconveniences I don&#x27;t remember, cause front page AI tutorials weren&#x27;t using it.</div><br/><div id="40069143" class="c"><input type="checkbox" id="c-40069143" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#40069064">parent</a><span>|</span><a href="#40068479">next</a><span>|</span><label class="collapse" for="c-40069143">[-]</label><label class="expand" for="c-40069143">[3 more]</label></div><br/><div class="children"><div class="content">PGvector is very nice indeed. And you get to store your vectors close to the rest of your data. I&#x27;m yet to understand the unique use case for dedicated vector dbs. It seems so annoying, having to query your vectors in a separate database without being able to easily join&#x2F;filter based on the rest of your tables.<p>I stored ~6 million hacker news posts, their metadata, and the vector embeddings in a cheap 20$&#x2F;month vm running pgvector. Querying is very fast. Maybe there&#x27;s some penalty to pay when you get to the billion+ row counts, but I&#x27;m happy so far.</div><br/><div id="40071419" class="c"><input type="checkbox" id="c-40071419" checked=""/><div class="controls bullet"><span class="by">brianjking</span><span>|</span><a href="#40069064">root</a><span>|</span><a href="#40069143">parent</a><span>|</span><a href="#40069280">next</a><span>|</span><label class="collapse" for="c-40071419">[-]</label><label class="expand" for="c-40071419">[1 more]</label></div><br/><div class="children"><div class="content">As I&#x27;m trying to work on some pricing info for PGVector - can you share some more info about the hacker news posts you&#x27;ve embedded?<p>* Which embedding model? (or number of dimensions)
* When you say 6 million posts - it&#x27;s just the URL of the post, title, and author, or do you mean you&#x27;ve also embedded the linked URL (be it HN or elsewhere)?<p>Cheers!</div><br/></div></div><div id="40069280" class="c"><input type="checkbox" id="c-40069280" checked=""/><div class="controls bullet"><span class="by">hot_gril</span><span>|</span><a href="#40069064">root</a><span>|</span><a href="#40069143">parent</a><span>|</span><a href="#40071419">prev</a><span>|</span><a href="#40068479">next</a><span>|</span><label class="collapse" for="c-40069280">[-]</label><label class="expand" for="c-40069280">[1 more]</label></div><br/><div class="children"><div class="content">You can also store vectors or matrices in a split-up fashion as separate rows in a table, which is particularly useful if they&#x27;re sparse. I&#x27;ve handled huge sparse matrix expressions (add, subtract, multiply, transpose) that way, cause numpy couldn&#x27;t deal with them.</div><br/></div></div></div></div></div></div><div id="40068479" class="c"><input type="checkbox" id="c-40068479" checked=""/><div class="controls bullet"><span class="by">cargobuild</span><span>|</span><a href="#40069064">prev</a><span>|</span><a href="#40070266">next</a><span>|</span><label class="collapse" for="c-40068479">[-]</label><label class="expand" for="c-40068479">[3 more]</label></div><br/><div class="children"><div class="content">seeing comments about using pgvector... at pinecone, we spent some time understanding it&#x27;s limitations and pain points. pinecone eliminates these pain points entirely and makes things simple at any scale.
check it out: <a href="https:&#x2F;&#x2F;www.pinecone.io&#x2F;blog&#x2F;pinecone-vs-pgvector&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.pinecone.io&#x2F;blog&#x2F;pinecone-vs-pgvector&#x2F;</a></div><br/><div id="40068535" class="c"><input type="checkbox" id="c-40068535" checked=""/><div class="controls bullet"><span class="by">gregorymichael</span><span>|</span><a href="#40068479">parent</a><span>|</span><a href="#40070266">next</a><span>|</span><label class="collapse" for="c-40068535">[-]</label><label class="expand" for="c-40068535">[2 more]</label></div><br/><div class="children"><div class="content">Has Pinecone gotten any cheaper? Last time I tried it was $75&#x2F;month for the starter plan &#x2F; single vector store.</div><br/><div id="40068648" class="c"><input type="checkbox" id="c-40068648" checked=""/><div class="controls bullet"><span class="by">cargobuild</span><span>|</span><a href="#40068479">root</a><span>|</span><a href="#40068535">parent</a><span>|</span><a href="#40070266">next</a><span>|</span><label class="collapse" for="c-40068648">[-]</label><label class="expand" for="c-40068648">[1 more]</label></div><br/><div class="children"><div class="content">yep. pinecone serverless has reduced costs significantly for many workloads.</div><br/></div></div></div></div></div></div><div id="40070266" class="c"><input type="checkbox" id="c-40070266" checked=""/><div class="controls bullet"><span class="by">mehulashah</span><span>|</span><a href="#40068479">prev</a><span>|</span><a href="#40069467">next</a><span>|</span><label class="collapse" for="c-40070266">[-]</label><label class="expand" for="c-40070266">[1 more]</label></div><br/><div class="children"><div class="content">I think he is saying: embeddings are deterministic, so they are more predictable in production.<p>They’re still magic, with little explain ability or adaptability when they don’t work.</div><br/></div></div><div id="40069467" class="c"><input type="checkbox" id="c-40069467" checked=""/><div class="controls bullet"><span class="by">aidenn0</span><span>|</span><a href="#40070266">prev</a><span>|</span><a href="#40068248">next</a><span>|</span><label class="collapse" for="c-40069467">[-]</label><label class="expand" for="c-40069467">[5 more]</label></div><br/><div class="children"><div class="content">Can someone give a qualitative explanation of what the vector of a word with 2 unrelated meanings would look like compared to the vector of a synonym of each of those meanings?</div><br/><div id="40069546" class="c"><input type="checkbox" id="c-40069546" checked=""/><div class="controls bullet"><span class="by">base698</span><span>|</span><a href="#40069467">parent</a><span>|</span><a href="#40068248">next</a><span>|</span><label class="collapse" for="c-40069546">[-]</label><label class="expand" for="c-40069546">[4 more]</label></div><br/><div class="children"><div class="content">If you think about it like a point on a graph, and the vectors as just 2D points (x,y), then the synonyms would be close and the unrelated meanings would be further away.</div><br/><div id="40069636" class="c"><input type="checkbox" id="c-40069636" checked=""/><div class="controls bullet"><span class="by">aidenn0</span><span>|</span><a href="#40069467">root</a><span>|</span><a href="#40069546">parent</a><span>|</span><a href="#40068248">next</a><span>|</span><label class="collapse" for="c-40069636">[-]</label><label class="expand" for="c-40069636">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m guessing 2 dimensions isn&#x27;t for this.<p>Here&#x27;s a concrete example: &quot;bow&quot; would need to be close to &quot;ribbon&quot; (as in a bow on a present) and also close to &quot;gun&quot; (as a weapon that shoots a projectile), but &quot;ribbon&quot; and &quot;gun&quot; would seem to need be far from each other.  How does something like word2vec resolve this?  Any transitive relationship would seem to fall afoul of this.</div><br/><div id="40070511" class="c"><input type="checkbox" id="c-40070511" checked=""/><div class="controls bullet"><span class="by">base698</span><span>|</span><a href="#40069467">root</a><span>|</span><a href="#40069636">parent</a><span>|</span><a href="#40068248">next</a><span>|</span><label class="collapse" for="c-40070511">[-]</label><label class="expand" for="c-40070511">[2 more]</label></div><br/><div class="children"><div class="content">Yes, only more sophisticated embeddings can capture that and it&#x27;s over 300+ dimensions.</div><br/><div id="40072771" class="c"><input type="checkbox" id="c-40072771" checked=""/><div class="controls bullet"><span class="by">aidenn0</span><span>|</span><a href="#40069467">root</a><span>|</span><a href="#40070511">parent</a><span>|</span><a href="#40068248">next</a><span>|</span><label class="collapse" for="c-40072771">[-]</label><label class="expand" for="c-40072771">[1 more]</label></div><br/><div class="children"><div class="content">But we still need a measure of &quot;closeness&quot; that is non-transitive.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40068248" class="c"><input type="checkbox" id="c-40068248" checked=""/><div class="controls bullet"><span class="by">LunaSea</span><span>|</span><a href="#40069467">prev</a><span>|</span><a href="#40069314">next</a><span>|</span><label class="collapse" for="c-40068248">[-]</label><label class="expand" for="c-40068248">[1 more]</label></div><br/><div class="children"><div class="content">Does anyone have examples of word (ngram) disambiguation when doing Approximate Nearest Neighbour (ANN) on word vector embeddings?</div><br/></div></div><div id="40069314" class="c"><input type="checkbox" id="c-40069314" checked=""/><div class="controls bullet"><span class="by">thorum</span><span>|</span><a href="#40068248">prev</a><span>|</span><a href="#40068129">next</a><span>|</span><label class="collapse" for="c-40069314">[-]</label><label class="expand" for="c-40069314">[4 more]</label></div><br/><div class="children"><div class="content">Can embeddings be used to capture stylistic features of text, rather than semantic? Like writing style?</div><br/><div id="40069404" class="c"><input type="checkbox" id="c-40069404" checked=""/><div class="controls bullet"><span class="by">levocardia</span><span>|</span><a href="#40069314">parent</a><span>|</span><a href="#40069416">next</a><span>|</span><label class="collapse" for="c-40069404">[-]</label><label class="expand" for="c-40069404">[2 more]</label></div><br/><div class="children"><div class="content">Probably, but you might need something more sophisticated than cosine distance. For example, you might take a dataset of business letters, diary entries, and fiction stories and train some classifier on top of the embeddings of each of the three types of text, then run (embeddings --&gt; your classifier) on new text.  But at that point you might just want to ask an LLM directly with a prompt like - &quot;Classify the style of the following text as business, personal, or fiction: $YOUR TEXT$&quot;</div><br/><div id="40069900" class="c"><input type="checkbox" id="c-40069900" checked=""/><div class="controls bullet"><span class="by">vladimirzaytsev</span><span>|</span><a href="#40069314">root</a><span>|</span><a href="#40069404">parent</a><span>|</span><a href="#40069416">next</a><span>|</span><label class="collapse" for="c-40069900">[-]</label><label class="expand" for="c-40069900">[1 more]</label></div><br/><div class="children"><div class="content">You may get way more accurate results from relatively small models as well as logits for each class if you ask one question per class instead.</div><br/></div></div></div></div><div id="40069416" class="c"><input type="checkbox" id="c-40069416" checked=""/><div class="controls bullet"><span class="by">vladimirzaytsev</span><span>|</span><a href="#40069314">parent</a><span>|</span><a href="#40069404">prev</a><span>|</span><a href="#40068129">next</a><span>|</span><label class="collapse" for="c-40069416">[-]</label><label class="expand" for="c-40069416">[1 more]</label></div><br/><div class="children"><div class="content">Likely not, embeddings are very crude. Embeddings of a text is just an average of &quot;meanings&quot; of words.<p>As is embeddings lack a lot of tricks that made transformers so efficient.</div><br/></div></div></div></div><div id="40068129" class="c"><input type="checkbox" id="c-40068129" checked=""/><div class="controls bullet"><span class="by">m1117</span><span>|</span><a href="#40069314">prev</a><span>|</span><a href="#40068864">next</a><span>|</span><label class="collapse" for="c-40068129">[-]</label><label class="expand" for="c-40068129">[5 more]</label></div><br/><div class="children"><div class="content">ah pgvector is kind of annoying to start with, you have to set it up and maintain, and then it starts falling apart when you have more vectors</div><br/><div id="40073989" class="c"><input type="checkbox" id="c-40073989" checked=""/><div class="controls bullet"><span class="by">ntry01</span><span>|</span><a href="#40068129">parent</a><span>|</span><a href="#40068229">next</a><span>|</span><label class="collapse" for="c-40073989">[-]</label><label class="expand" for="c-40073989">[1 more]</label></div><br/><div class="children"><div class="content">on the other hand, if you have postgres already, it may be easier to add pgvector than to add another dependency to your stack (especially if you are using something like supabase)<p>another benefit is that you can easily filter your embeddings by other field, so everything is kept in one place and could help with perfomance<p>it&#x27;s a good place to start in those cases and if it is successful and you need extreme performance you can always move to other specialized tools like qdrant, pinecone or weaviate which were purpose-built for vectors</div><br/></div></div><div id="40068229" class="c"><input type="checkbox" id="c-40068229" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#40068129">parent</a><span>|</span><a href="#40073989">prev</a><span>|</span><a href="#40068808">next</a><span>|</span><label class="collapse" for="c-40068229">[-]</label><label class="expand" for="c-40068229">[2 more]</label></div><br/><div class="children"><div class="content">Can you elaborate more on the falling apart?  I can see pgvector being intimidating for users with no experience standing up a DB, but I don&#x27;t see how Postgres or pgvector would fall apart.  Note, my reason for asking is I&#x27;m planning on going all in with Postgres, so pgvector makes sense for me.</div><br/><div id="40068486" class="c"><input type="checkbox" id="c-40068486" checked=""/><div class="controls bullet"><span class="by">cargobuild</span><span>|</span><a href="#40068129">root</a><span>|</span><a href="#40068229">parent</a><span>|</span><a href="#40068808">next</a><span>|</span><label class="collapse" for="c-40068486">[-]</label><label class="expand" for="c-40068486">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.pinecone.io&#x2F;blog&#x2F;pinecone-vs-pgvector&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.pinecone.io&#x2F;blog&#x2F;pinecone-vs-pgvector&#x2F;</a> check it out :)</div><br/></div></div></div></div><div id="40068808" class="c"><input type="checkbox" id="c-40068808" checked=""/><div class="controls bullet"><span class="by">hackernoteng</span><span>|</span><a href="#40068129">parent</a><span>|</span><a href="#40068229">prev</a><span>|</span><a href="#40068864">next</a><span>|</span><label class="collapse" for="c-40068808">[-]</label><label class="expand" for="c-40068808">[1 more]</label></div><br/><div class="children"><div class="content">What is &quot;more vectors&quot;?  How many are we talking about?  We&#x27;ve been using pgvector in production for more than 1 year without any issues.  We dont have a ton of vectors, less than 100,000, and we filter queries by other fields so our total per cosine function is probably more like max of 5000.  Performance is fine and no issues.</div><br/></div></div></div></div><div id="40073327" class="c"><input type="checkbox" id="c-40073327" checked=""/><div class="controls bullet"><span class="by">Nuella19</span><span>|</span><a href="#40068864">prev</a><span>|</span><label class="collapse" for="c-40073327">[-]</label><label class="expand" for="c-40073327">[1 more]</label></div><br/><div class="children"><div class="content">I always stood against people trying to hack their partner&#x27;s phone, until my cheating husband gave me every reason to spy on him. I&#x27;ve been suspecting his attitude lately and I really loved my man, so I was eager to find out the reason behind his sudden change of attitude. I contacted Vladimir Hacker who was recommended by a friend and after a few hours of contacting him, he gave me remote access to my husband&#x27;s phone and I saw all his daily activities and I was able to confirm he was cheating. You can reach him on gmail through; remotespyhacker @gmail com.</div><br/></div></div></div></div></div></div></div></body></html>