<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1730710881540" as="style"/><link rel="stylesheet" href="styles.css?v=1730710881540"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/altera-al/project-sid">Project Sid: Many-agent simulations toward AI civilization</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>talms</span> | <span>103 comments</span></div><br/><div><div id="42037674" class="c"><input type="checkbox" id="c-42037674" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#42035859">next</a><span>|</span><label class="collapse" for="c-42037674">[-]</label><label class="expand" for="c-42037674">[9 more]</label></div><br/><div class="children"><div class="content">I feel like there is some kind of information theory constraint which confounds our ability to extract higher order behavior from multiple instances of the same LLM.<p>I spent quite a bit of time building a multi agent simulation last year and wound up at the same conclusion every day - this is all just a roundabout form of prompt engineering. Perhaps it is useful as a mental model, but you can flatten the whole thing to a few SQL tables and functions. Each &quot;agent&quot; is essentially a sql view that maps a string template forming the prompt.<p>I don&#x27;t think you need an actual 3D world, wall clock, etc. The LLM does not seem to be meaningfully enriched by having a fancy representation underly the prompt generation process. There is clearly no &quot;inner world&quot; in these LLMs, so trying to entertain them with a rich outer environment seems pointless.</div><br/><div id="42037783" class="c"><input type="checkbox" id="c-42037783" checked=""/><div class="controls bullet"><span class="by">caetris2</span><span>|</span><a href="#42037674">parent</a><span>|</span><a href="#42039110">next</a><span>|</span><label class="collapse" for="c-42037783">[-]</label><label class="expand" for="c-42037783">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;ve absolutely nailed it here, I agree. To make any progress at all at the tremendously difficult problem they are trying to solve, they need to be frank about just how far away they are from what it is they are marketing.<p>I am whole-heartedly in support of commercial interests to drum of awareness and engagement by the authors. This is definitely a cool thing to be working on, however, what does make more sense is to frame the situation more honestly and attract folks to the desire of solving tremendously <i>hard</i> problems based on a level of expertise and awareness that truly moves the ball forward.<p>What would be far more interesting would be for the folks involved to say all the ten thousand things that went wrong in their experiments and to lay out the common-sense conclusions from those findings (just like the one you shared, which is truly insightful and correct).<p>We need to move past this industry and their enablers that continually try to win using the wrong methodology -- pushing away the most inventive and innovative people that are ripe and ready to make paradigm shifts in the AI field and industry.</div><br/><div id="42037829" class="c"><input type="checkbox" id="c-42037829" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#42037674">root</a><span>|</span><a href="#42037783">parent</a><span>|</span><a href="#42039110">next</a><span>|</span><label class="collapse" for="c-42037829">[-]</label><label class="expand" for="c-42037829">[2 more]</label></div><br/><div class="children"><div class="content">It would however be very interesting to see these kinds of agents in a commercial video game. Yes they are shallow in their perception of the game world. But they’re a big step up from the status quo.</div><br/><div id="42037837" class="c"><input type="checkbox" id="c-42037837" checked=""/><div class="controls bullet"><span class="by">caetris2</span><span>|</span><a href="#42037674">root</a><span>|</span><a href="#42037829">parent</a><span>|</span><a href="#42039110">next</a><span>|</span><label class="collapse" for="c-42037837">[-]</label><label class="expand" for="c-42037837">[1 more]</label></div><br/><div class="children"><div class="content">Yes... Imagine a blog post at the same quality as this paper that framed their work and their pursuits in a way that <i>genuinely got people excited about what could be around the corner</i>, but with the context that frames exactly how far away they are from achieving what would be the ultimate vision.</div><br/></div></div></div></div></div></div><div id="42039110" class="c"><input type="checkbox" id="c-42039110" checked=""/><div class="controls bullet"><span class="by">InDubioProRubio</span><span>|</span><a href="#42037674">parent</a><span>|</span><a href="#42037783">prev</a><span>|</span><a href="#42038957">next</a><span>|</span><label class="collapse" for="c-42039110">[-]</label><label class="expand" for="c-42039110">[2 more]</label></div><br/><div class="children"><div class="content">Maybe we need gazelles and cheetahs - many gazelle-agents getting chased towards a goal, doing the brute force work- and the constraint cheetahs chase them, evaluate them and leave them alive (memory intact) as long as they come up with better and better solutions. Basically a evolutionary algo, running on top of many agents, running simultaneously on the same hardware?</div><br/><div id="42039746" class="c"><input type="checkbox" id="c-42039746" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#42037674">root</a><span>|</span><a href="#42039110">parent</a><span>|</span><a href="#42038957">next</a><span>|</span><label class="collapse" for="c-42039746">[-]</label><label class="expand" for="c-42039746">[1 more]</label></div><br/><div class="children"><div class="content">Do you want stressed and panicking agents? Do you think they&#x27;ll produce good output?<p>In my prompting experience, I mostly do my best to give the AI way, way more slack than it thinks it has.</div><br/></div></div></div></div><div id="42038957" class="c"><input type="checkbox" id="c-42038957" checked=""/><div class="controls bullet"><span class="by">cen4</span><span>|</span><a href="#42037674">parent</a><span>|</span><a href="#42039110">prev</a><span>|</span><a href="#42038617">next</a><span>|</span><label class="collapse" for="c-42038957">[-]</label><label class="expand" for="c-42038957">[1 more]</label></div><br/><div class="children"><div class="content">That depends on giving them a goal&#x2F;reward like increasing &quot;data quality&quot;.<p>I mean frogs don&#x27;t use their brains much either inspite of the rich world around them they don&#x27;t really explore.<p>But chimps do. They can&#x27;t sit quiet in a tree forever and that boils down to their Reward&#x2F;Motivation Circuitry. They get pleasure out of explore. And if they didn&#x27;t we wouldn&#x27;t be here.</div><br/></div></div><div id="42038617" class="c"><input type="checkbox" id="c-42038617" checked=""/><div class="controls bullet"><span class="by">fhe</span><span>|</span><a href="#42037674">parent</a><span>|</span><a href="#42038957">prev</a><span>|</span><a href="#42039243">next</a><span>|</span><label class="collapse" for="c-42038617">[-]</label><label class="expand" for="c-42038617">[1 more]</label></div><br/><div class="children"><div class="content">so well put. exactly how I&#x27;ve been feeling and trying to verbalize.</div><br/></div></div><div id="42039243" class="c"><input type="checkbox" id="c-42039243" checked=""/><div class="controls bullet"><span class="by">shkkmo</span><span>|</span><a href="#42037674">parent</a><span>|</span><a href="#42038617">prev</a><span>|</span><a href="#42035859">next</a><span>|</span><label class="collapse" for="c-42039243">[-]</label><label class="expand" for="c-42039243">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t think you need an actual 3D world, wall clock, etc. The LLM does not seem to be meaningfully enriched by having a fancy representation underly the prompt generation process.<p>I don&#x27;t know how you expect agents to self organize social structures if they don&#x27;t have a shared reality. I mean, you could write all the prompts yourself, but then that shared reality is just your imagination and you&#x27;re just DMing for them.<p>The point of the minecraft environment isn&#x27;t to &quot;enrich&quot; the &quot;inner world&quot; of the agents and the goal isn&#x27;t to &quot;entertain&quot; them. The point is to create a set of human understandable challenges in a shared environment so that we can measure behavior and performance of groups of agents in different configurations.<p>I know we aren&#x27;t supposed to bring this up, but did you read the article? Nothing of your comment addresses any of the findings or techniques used in this study.</div><br/></div></div></div></div><div id="42035859" class="c"><input type="checkbox" id="c-42035859" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#42037674">prev</a><span>|</span><a href="#42035985">next</a><span>|</span><label class="collapse" for="c-42035859">[-]</label><label class="expand" for="c-42035859">[9 more]</label></div><br/><div class="children"><div class="content">Now these seem to be truly artificially intelligent agents. Memory, volition, autonomy, something like an OODA loop or whatever you want to call it, and a persistent environment. Very nice concept, and I&#x27;m positive the learnings can be applied to more mundane business problems, too.<p>If only I could get management to understand that a bunch of prompts shitting into eachother isn&#x27;t &quot;cutting-edge agentic AI&quot;...<p>But then again <i>their</i> jobs probably depend on selling something that looks like real innovation happening to the C-levels...</div><br/><div id="42036005" class="c"><input type="checkbox" id="c-42036005" checked=""/><div class="controls bullet"><span class="by">Carrok</span><span>|</span><a href="#42035859">parent</a><span>|</span><a href="#42036352">next</a><span>|</span><label class="collapse" for="c-42036005">[-]</label><label class="expand" for="c-42036005">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If only I could get management to understand that a bunch of prompts shitting into eachother isn&#x27;t &quot;cutting-edge agentic AI&quot;...<p>It&#x27;s unclear to me how the linked project is different from what you described.<p>Plenty of existing agents have &quot;memory&quot; and many other things you named.</div><br/></div></div><div id="42036352" class="c"><input type="checkbox" id="c-42036352" checked=""/><div class="controls bullet"><span class="by">jsemrau</span><span>|</span><a href="#42035859">parent</a><span>|</span><a href="#42036005">prev</a><span>|</span><a href="#42037355">next</a><span>|</span><label class="collapse" for="c-42036352">[-]</label><label class="expand" for="c-42036352">[1 more]</label></div><br/><div class="children"><div class="content">&gt;If only I could get management to understand that a bunch of prompts shitting into eachother isn&#x27;t &quot;cutting-edge agentic AI&quot;...<p>It should never be this way. Even with narrow AI, there needs to be a governance framework that helps measure the output and capture potential risks (hallucinations, wrong data &#x2F; links, wrong summaries, etc)</div><br/></div></div><div id="42037355" class="c"><input type="checkbox" id="c-42037355" checked=""/><div class="controls bullet"><span class="by">whatshisface</span><span>|</span><a href="#42035859">parent</a><span>|</span><a href="#42036352">prev</a><span>|</span><a href="#42035985">next</a><span>|</span><label class="collapse" for="c-42037355">[-]</label><label class="expand" for="c-42037355">[6 more]</label></div><br/><div class="children"><div class="content">Just so you know, the English noun for things that have been learned is, &quot;lessons.&quot;</div><br/><div id="42037445" class="c"><input type="checkbox" id="c-42037445" checked=""/><div class="controls bullet"><span class="by">mindcrime</span><span>|</span><a href="#42035859">root</a><span>|</span><a href="#42037355">parent</a><span>|</span><a href="#42037486">next</a><span>|</span><label class="collapse" for="c-42037445">[-]</label><label class="expand" for="c-42037445">[1 more]</label></div><br/><div class="children"><div class="content">Also: &quot;learnings&quot;.<p><a href="https:&#x2F;&#x2F;dictionary.cambridge.org&#x2F;us&#x2F;dictionary&#x2F;english&#x2F;learning" rel="nofollow">https:&#x2F;&#x2F;dictionary.cambridge.org&#x2F;us&#x2F;dictionary&#x2F;english&#x2F;learn...</a><p>&quot;knowledge or a piece of information obtained by study or experience&quot;<p>&quot;I am already incorporating some of these learnings into my work and getting better results.&quot;</div><br/></div></div><div id="42037486" class="c"><input type="checkbox" id="c-42037486" checked=""/><div class="controls bullet"><span class="by">ytss</span><span>|</span><a href="#42035859">root</a><span>|</span><a href="#42037355">parent</a><span>|</span><a href="#42037445">prev</a><span>|</span><a href="#42035985">next</a><span>|</span><label class="collapse" for="c-42037486">[-]</label><label class="expand" for="c-42037486">[4 more]</label></div><br/><div class="children"><div class="content">I believe that “learnings” is also a word that could be applied in this context.<p>It seems to me “learnings” would actually be less ambiguous than “lessons”. A lesson brings to mind a thing being taught, not just learned.</div><br/><div id="42037544" class="c"><input type="checkbox" id="c-42037544" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#42035859">root</a><span>|</span><a href="#42037486">parent</a><span>|</span><a href="#42039325">next</a><span>|</span><label class="collapse" for="c-42037544">[-]</label><label class="expand" for="c-42037544">[2 more]</label></div><br/><div class="children"><div class="content">Let’s take this learning offline and circle back during the next sync, team</div><br/></div></div></div></div></div></div></div></div><div id="42035985" class="c"><input type="checkbox" id="c-42035985" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#42035859">prev</a><span>|</span><a href="#42036645">next</a><span>|</span><label class="collapse" for="c-42035985">[-]</label><label class="expand" for="c-42035985">[13 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve thought about this a lot. I&#x27;m no philosopher or AI researcher, so I&#x27;m just spitballing... but if I were to try my hand at it, I think I&#x27;d like to start from &quot;principles&quot; and let systems evolve or at least be discoverable over time<p>Principles would be things like self-preservation, food, shelter and procreating, communication and memory through a risk-reward calculation prism. Maybe establishing what is &quot;known&quot; vs what is &quot;unknown&quot; is a key component here too, but not in such a binary way.<p>&quot;Memory&quot; can mean many things, but if you codify it as a function of some type of subject performing some type of action leading to some outcome with some ascribed &quot;risk-reward&quot; profile compared to the value obtained from empirical testing that spans from very negative to very positive, it seems both wide encompassing and generally useful, both to the individual and to the collective.<p>From there you derive the need to connect with others, disputes over resources, the need to take risks, explore the unknown, share what we&#x27;ve learned, refine risk-rewards, etc. You can guide the civilization to discover certain technologies or inventions or locations we&#x27;ve defined ex ante as their godlike DM which is a bit like cheating because it puts their development &quot;on rails&quot; but also makes it more useful, interesting and relatable.<p>It sounds computationally prohibitive, but the game doesn&#x27;t need to play out in real time anyway...<p>I just think that you can describe <i>a lot</i> of the human condition in terms of &quot;life&quot;, &quot;liberty&quot;, &quot;love&#x2F;connection&quot; and &quot;greed&quot;.<p>Looking at the video in the repo, I don&#x27;t like how this throws &quot;cultures&quot;, &quot;memes&quot; and &quot;religion&quot; into the mix instead of letting them be an emergence from the need to communicate and share the belief systems that emerge from our collective memories. Because it seems like a distinction without a difference for the purposes of analyzing this. Also &quot;taxes are high!&quot; without the underlying &quot;I don&#x27;t have enough resources to get by&quot; seems too much like a mechanical turk</div><br/><div id="42037309" class="c"><input type="checkbox" id="c-42037309" checked=""/><div class="controls bullet"><span class="by">shagie</span><span>|</span><a href="#42035985">parent</a><span>|</span><a href="#42036292">next</a><span>|</span><label class="collapse" for="c-42037309">[-]</label><label class="expand" for="c-42037309">[1 more]</label></div><br/><div class="children"><div class="content">Evolve is another beast... but for the: &quot;I&#x27;ve thought about this a lot. I&#x27;m no philosopher or AI researcher, so I&#x27;m just spitballing... but if I were to try my hand at it, I think I&#x27;d like to start from &quot;principles&quot; and let systems evolve or at least be discoverable over time&quot; part, hunt up a copy of &quot;The Society of Mind&quot; by Minsky who was both and wrote about that idea.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Society_of_Mind" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Society_of_Mind</a><p>&gt; The work, which first appeared in 1986, was the first comprehensive description of Minsky&#x27;s &quot;society of mind&quot; theory, which he began developing in the early 1970s. It is composed of 270 self-contained essays which are divided into 30 general chapters. The book was also made into a CD-ROM version.<p>&gt; In the process of explaining the society of mind, Minsky introduces a wide range of ideas and concepts. He develops theories about how processes such as language, memory, and learning work, and also covers concepts such as consciousness, the sense of self, and free will; because of this, many view The Society of Mind as a work of philosophy.<p>&gt; The book was not written to prove anything specific about AI or cognitive science, and does not reference physical brain structures. Instead, it is a collection of ideas about how the mind and thinking work on the conceptual level.<p>Its very approachable as a layperson in that part of the field of AI.</div><br/></div></div><div id="42036292" class="c"><input type="checkbox" id="c-42036292" checked=""/><div class="controls bullet"><span class="by">grugagag</span><span>|</span><a href="#42035985">parent</a><span>|</span><a href="#42037309">prev</a><span>|</span><a href="#42036362">next</a><span>|</span><label class="collapse" for="c-42036292">[-]</label><label class="expand" for="c-42036292">[7 more]</label></div><br/><div class="children"><div class="content">Many of these projects are inch deep into intelligence and miles deep into the current technology. Some things will see tremendous benefits but as far as artificial intelligence we’re not there yet. Im thinking gaming will benefit a lot from these..</div><br/><div id="42036530" class="c"><input type="checkbox" id="c-42036530" checked=""/><div class="controls bullet"><span class="by">farias0</span><span>|</span><a href="#42035985">root</a><span>|</span><a href="#42036292">parent</a><span>|</span><a href="#42036362">next</a><span>|</span><label class="collapse" for="c-42036530">[-]</label><label class="expand" for="c-42036530">[6 more]</label></div><br/><div class="children"><div class="content">You mean we&#x27;re not there in simulating an actual human brain? Sure. But we&#x27;re seeing AI work like a human well enough to be useful, isn&#x27;t that the point?</div><br/><div id="42036839" class="c"><input type="checkbox" id="c-42036839" checked=""/><div class="controls bullet"><span class="by">grugagag</span><span>|</span><a href="#42035985">root</a><span>|</span><a href="#42036530">parent</a><span>|</span><a href="#42036362">next</a><span>|</span><label class="collapse" for="c-42036839">[-]</label><label class="expand" for="c-42036839">[5 more]</label></div><br/><div class="children"><div class="content">Not if we’re pretending it is any inteligent. Other than that I’m all in for new utility to come out from it. But I do see a lot of tangents off technology with claims to something it is not. I have no problem of calling that out. Why do you mind? Just ignore me if Im holding your enthusiasm  back, there’s plenty of sources to provide that for you.</div><br/><div id="42038073" class="c"><input type="checkbox" id="c-42038073" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#42035985">root</a><span>|</span><a href="#42036839">parent</a><span>|</span><a href="#42036362">next</a><span>|</span><label class="collapse" for="c-42038073">[-]</label><label class="expand" for="c-42038073">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Not if we’re pretending it is any inteligent.<p>We have been shifting the definition of what it means to be intelligent every 3 months following the advances of LLM...</div><br/><div id="42039709" class="c"><input type="checkbox" id="c-42039709" checked=""/><div class="controls bullet"><span class="by">grugagag</span><span>|</span><a href="#42035985">root</a><span>|</span><a href="#42038073">parent</a><span>|</span><a href="#42038777">next</a><span>|</span><label class="collapse" for="c-42039709">[-]</label><label class="expand" for="c-42039709">[1 more]</label></div><br/><div class="children"><div class="content">So what? I’m not disputing that the immitation of intelligence is not good and it gets better and better every 3 months or so. But that doesn’t mean anything, even if it gets close to 99.9%. It is not real intlligence and it is quite limited in what it does. If LLMs solve logic or problems or chemistry problems it is solely not because it made a leap in understanding but because it was trained on a zillion examples. If you have a similar problem it will try to showhorn an answer without understanding where it fails. Am I saying this is useless? NO. What I’m saying is that the current approach to intelligence is missing some key ingredients. Im actually surprised so many get fooled by the hype and are ready to declare a winner. Human intelligence with it’s major flaws is still king of the hill.</div><br/></div></div><div id="42038777" class="c"><input type="checkbox" id="c-42038777" checked=""/><div class="controls bullet"><span class="by">mistermann</span><span>|</span><a href="#42035985">root</a><span>|</span><a href="#42038073">parent</a><span>|</span><a href="#42039709">prev</a><span>|</span><a href="#42036362">next</a><span>|</span><label class="collapse" for="c-42038777">[-]</label><label class="expand" for="c-42038777">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s also this:<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Closed-world_assumption" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Closed-world_assumption</a><p>I wonder, once LLM&#x27;s exceed Humans beyond some substantial threshold, will it crack the simulation allowing us to get back in the game again.</div><br/><div id="42039753" class="c"><input type="checkbox" id="c-42039753" checked=""/><div class="controls bullet"><span class="by">grugagag</span><span>|</span><a href="#42035985">root</a><span>|</span><a href="#42038777">parent</a><span>|</span><a href="#42036362">next</a><span>|</span><label class="collapse" for="c-42039753">[-]</label><label class="expand" for="c-42039753">[1 more]</label></div><br/><div class="children"><div class="content">Crack what simulation exactly? You can get back into the game right now, armed with these tools such as LLMs, ML and so on.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42036362" class="c"><input type="checkbox" id="c-42036362" checked=""/><div class="controls bullet"><span class="by">jsemrau</span><span>|</span><a href="#42035985">parent</a><span>|</span><a href="#42036292">prev</a><span>|</span><a href="#42037107">next</a><span>|</span><label class="collapse" for="c-42036362">[-]</label><label class="expand" for="c-42036362">[2 more]</label></div><br/><div class="children"><div class="content">Memory is really interesting. For example, if you play 100,000 rounds of 5x5 Tic Tac Toe. Do you really need to remember game 51247 or do you recognize and remember a winning pattern? In Reinforcement Learning you would based on each win revise the policy. How would that work for genAI?</div><br/></div></div><div id="42037107" class="c"><input type="checkbox" id="c-42037107" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42035985">parent</a><span>|</span><a href="#42036362">prev</a><span>|</span><a href="#42036645">next</a><span>|</span><label class="collapse" for="c-42037107">[-]</label><label class="expand" for="c-42037107">[2 more]</label></div><br/><div class="children"><div class="content">So a modernized version of Spore.</div><br/><div id="42037409" class="c"><input type="checkbox" id="c-42037409" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#42035985">root</a><span>|</span><a href="#42037107">parent</a><span>|</span><a href="#42036645">next</a><span>|</span><label class="collapse" for="c-42037409">[-]</label><label class="expand" for="c-42037409">[1 more]</label></div><br/><div class="children"><div class="content">Basically what we all wished Spore had been ;-)</div><br/></div></div></div></div></div></div><div id="42036645" class="c"><input type="checkbox" id="c-42036645" checked=""/><div class="controls bullet"><span class="by">caetris2</span><span>|</span><a href="#42035985">prev</a><span>|</span><a href="#42037574">next</a><span>|</span><label class="collapse" for="c-42036645">[-]</label><label class="expand" for="c-42036645">[12 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve reviewed the paper and I&#x27;m confident this paper was fabricated over a collection of false claims. The claims made are not genuine and should not be taken at face value without peer review. The provided charts and graphics are sophisticated forgeries in many cases when reviewing and vetting their applicability to the claims made.<p>It is currently not possible for any kind of LLM to do what is being proposed, while maybe the intentions are good with regard to commercial interests, I want to be clear: this paper seems indicate that election-related activities were coordinated by groups of AI agents in a simulation. These kinds of claims require substantial evidence and that was not provided.<p>The prompts that are provided are not in any way connected to an applied usage of LLMs that are described.</div><br/><div id="42039539" class="c"><input type="checkbox" id="c-42039539" checked=""/><div class="controls bullet"><span class="by">shkkmo</span><span>|</span><a href="#42036645">parent</a><span>|</span><a href="#42036807">next</a><span>|</span><label class="collapse" for="c-42039539">[-]</label><label class="expand" for="c-42039539">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think you understood the paper.<p>The &quot;election&quot; experiment was a prefined scenario. There isn&#x27;t any &quot;coordination&quot; of election activities. There were preassigned &quot;influencers&quot; using the conversation system built into PIANO. The sentiment was collected automatically by the simulation and the &quot;Election Manager&quot; was another predefined agent. Specically this part of the experiment was designed to look at how the presence or absence of specific modules in the PIANO framework would affect the behavior.</div><br/></div></div><div id="42036807" class="c"><input type="checkbox" id="c-42036807" checked=""/><div class="controls bullet"><span class="by">afro88</span><span>|</span><a href="#42036645">parent</a><span>|</span><a href="#42039539">prev</a><span>|</span><a href="#42037686">next</a><span>|</span><label class="collapse" for="c-42036807">[-]</label><label class="expand" for="c-42036807">[5 more]</label></div><br/><div class="children"><div class="content">&gt; this paper seems indicate that election-related activities were coordinated by groups of AI agents in a simulation<p>I mean, that&#x27;s surely within the training data of LLMs? The effectiveness etc of the election activities is likely very low. But I don&#x27;t think it&#x27;s outside the realms of possibility that the agents prompted each other into the latent spaces of the LLM to do with elections.</div><br/><div id="42037271" class="c"><input type="checkbox" id="c-42037271" checked=""/><div class="controls bullet"><span class="by">caetris2</span><span>|</span><a href="#42036645">root</a><span>|</span><a href="#42036807">parent</a><span>|</span><a href="#42037686">next</a><span>|</span><label class="collapse" for="c-42037271">[-]</label><label class="expand" for="c-42037271">[4 more]</label></div><br/><div class="children"><div class="content">LLMs are stateless and they do not remember the past (as in they don&#x27;t have a database), making the training data a non-issue here. Therefore, the claims made here in this paper <i>are</i> not possible because the simulation would require each agent to have a memory context larger than any available LLM&#x27;s context window. The claims made here by the original poster are patently false.<p>The ideas here are not supported by any kind of validated understanding of the limitations of language models. I want to be clear -- the kind of AI that is being purported to be used in the paper is something that has been in video games for over 2 decades, which is akin to Starcraft or Diablo&#x27;s NPCs.<p>The <i>key</i> issue is that this is a intentional false claim that can certainly damage mainstream understanding of LLM safety and what is possible at the current state of the art.<p>Agentic systems are not well-suited to achieve any of the things that are proposed in the paper, and Generative AI does not enable these kinds of advancements.</div><br/><div id="42037780" class="c"><input type="checkbox" id="c-42037780" checked=""/><div class="controls bullet"><span class="by">Philpax</span><span>|</span><a href="#42036645">root</a><span>|</span><a href="#42037271">parent</a><span>|</span><a href="#42037714">next</a><span>|</span><label class="collapse" for="c-42037780">[-]</label><label class="expand" for="c-42037780">[1 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs are stateless and they do not remember the past (as in they don&#x27;t have a database), making the training data a non-issue here.<p>That&#x27;s not what they said. They said that a LLM knows what elections are, which suggests they could have the requisite knowledge to act one out.<p>&gt; Therefore, the claims made here in this paper are not possible because the simulation would require each agent to have a memory context larger than any available LLM&#x27;s context window. The claims made here by the original poster are patently false.<p>No, it doesn&#x27;t. They aren&#x27;t passing in all prior context at once: they are providing relevant subsets of memory as context. This is a common technique for language agents.<p>&gt; Agentic systems are not well-suited to achieve any of the things that are proposed in the paper, and Generative AI does not enable these kinds of advancements.<p>This is not new ground. Much of the base social behaviour here comes from Generative Agents [0], which they cite. Much of the Minecraft related behaviour is inspired by Voyager [1], which they also cite.<p>There isn&#x27;t a fundamental breakthrough or innovation here that was patently impossible before, or that they are lying about: this combines prior work, iterates upon it, and scales it up.<p>[0]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2304.03442" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2304.03442</a><p>[1]: <a href="https:&#x2F;&#x2F;voyager.minedojo.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;voyager.minedojo.org&#x2F;</a></div><br/></div></div><div id="42037714" class="c"><input type="checkbox" id="c-42037714" checked=""/><div class="controls bullet"><span class="by">afro88</span><span>|</span><a href="#42036645">root</a><span>|</span><a href="#42037271">parent</a><span>|</span><a href="#42037780">prev</a><span>|</span><a href="#42037686">next</a><span>|</span><label class="collapse" for="c-42037714">[-]</label><label class="expand" for="c-42037714">[2 more]</label></div><br/><div class="children"><div class="content">Perhaps I&#x27;ve made a big assumption &#x2F; oversimplification about how this works. But..<p>&gt; LLMs are stateless and they do not remember the past (as in they don&#x27;t have a database), making the training data a non-issue here<p>Yes. I never said they were stateful? The context given is the state. And training data is hugely important. Once upon a time there was a guy that claimed ChatGPT could simulate a command line shell. &quot;Simulate&quot; ended up being the wrong word. &quot;Largely hallucinate&quot; was a more accurate description. Shell commands and sessions were for sure part of the training data for ChatGPT, and that&#x27;s how it could be prompted into largely hallucinating one. Same deal here with &quot;election activities&quot; I think.<p>&gt; Therefore, the claims made here in this paper are not possible because the simulation would require each agent to have a memory context larger than any available LLM&#x27;s context window. The claims made here by the original poster are patently false.<p>Well no, they can always trim the data put into the context. And then the agents would start &quot;forgetting&quot; things and the &quot;election activities&quot; would be pretty badly &quot;simulated&quot;.<p>Honestly, I think you&#x27;re right that the paper is misleading people into thinking the system is doing way more than it actually is. But you make it sound like the whole thing is made up and impossible. The reality is somewhere in the middle. Yes they set up hundreds of agents, they give the agents data about the world, some memory of their interactions, and some system prompt to say what actions they can perform. This led to some interesting and surprising behaviours. No, this isn&#x27;t intelligence, and isn&#x27;t much more than a fancy representation of what is in the model weights.</div><br/><div id="42037820" class="c"><input type="checkbox" id="c-42037820" checked=""/><div class="controls bullet"><span class="by">caetris2</span><span>|</span><a href="#42036645">root</a><span>|</span><a href="#42037714">parent</a><span>|</span><a href="#42037686">next</a><span>|</span><label class="collapse" for="c-42037820">[-]</label><label class="expand" for="c-42037820">[1 more]</label></div><br/><div class="children"><div class="content">These are extremely hard problems to solve and it is important for any claims to be validated at this early phase of generative AI.</div><br/></div></div></div></div></div></div></div></div><div id="42037686" class="c"><input type="checkbox" id="c-42037686" checked=""/><div class="controls bullet"><span class="by">Reubend</span><span>|</span><a href="#42036645">parent</a><span>|</span><a href="#42036807">prev</a><span>|</span><a href="#42037574">next</a><span>|</span><label class="collapse" for="c-42037686">[-]</label><label class="expand" for="c-42037686">[5 more]</label></div><br/><div class="children"><div class="content">Yeah, I haven&#x27;t looked into this much so far but I am extremely skeptical of the claims being made here. For one agent to become a tax collector and another to challenge the tax regime without such behavior being hard coded would be <i>extremely</i> impressive.</div><br/><div id="42037828" class="c"><input type="checkbox" id="c-42037828" checked=""/><div class="controls bullet"><span class="by">Philpax</span><span>|</span><a href="#42036645">root</a><span>|</span><a href="#42037686">parent</a><span>|</span><a href="#42037728">next</a><span>|</span><label class="collapse" for="c-42037828">[-]</label><label class="expand" for="c-42037828">[3 more]</label></div><br/><div class="children"><div class="content">They were assigned roles to examine the spread of information and behaviour. The agents pay tax into a chest, as decreed by the (dynamic) rules. There are agents assigned to the roles of pro- and anti-tax influencers; agents in proximity to these influencers would change their own behaviour appropriately, including voting for changes in the tax.<p>So yes, they didn&#x27;t take on these roles organically, but no, they weren&#x27;t aiming to do so: they were examining behavioral influence and community dynamics with that particular experiment.<p>I&#x27;d recommend skimming over the paper; it&#x27;s a pretty quick read and they aren&#x27;t making any truly outrageous claims IMO.</div><br/><div id="42039581" class="c"><input type="checkbox" id="c-42039581" checked=""/><div class="controls bullet"><span class="by">bob88jg</span><span>|</span><a href="#42036645">root</a><span>|</span><a href="#42037828">parent</a><span>|</span><a href="#42038097">next</a><span>|</span><label class="collapse" for="c-42039581">[-]</label><label class="expand" for="c-42039581">[1 more]</label></div><br/><div class="children"><div class="content">So it&#x27;s a plain vanilla ABM with lots of human crafted interaction logic? So they are making outrageous claims - since they are making it sound like it&#x27;s all spontaneously arising from the interaction of LLMs...</div><br/></div></div><div id="42038097" class="c"><input type="checkbox" id="c-42038097" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#42036645">root</a><span>|</span><a href="#42037828">parent</a><span>|</span><a href="#42039581">prev</a><span>|</span><a href="#42037728">next</a><span>|</span><label class="collapse" for="c-42038097">[-]</label><label class="expand" for="c-42038097">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not clear what actually happened. They&#x27;re using Minecraft. Why is there not video?<p>People have tried groups of AI agents inside virtual worlds before. Google has a project.[1] Stanford has a project.[2] Those have video.<p>A real question is whether they are anthropomorphizing a dumb system too much.<p>[1] <a href="https:&#x2F;&#x2F;deepmind.google&#x2F;discover&#x2F;blog&#x2F;sima-generalist-ai-agent-for-3d-virtual-environments&#x2F;" rel="nofollow">https:&#x2F;&#x2F;deepmind.google&#x2F;discover&#x2F;blog&#x2F;sima-generalist-ai-age...</a><p>[2] <a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;information-technology&#x2F;2023&#x2F;04&#x2F;surprising-things-happen-when-you-put-25-ai-agents-together-in-an-rpg-town" rel="nofollow">https:&#x2F;&#x2F;arstechnica.com&#x2F;information-technology&#x2F;2023&#x2F;04&#x2F;surpr...</a></div><br/></div></div></div></div><div id="42037728" class="c"><input type="checkbox" id="c-42037728" checked=""/><div class="controls bullet"><span class="by">afro88</span><span>|</span><a href="#42036645">root</a><span>|</span><a href="#42037686">parent</a><span>|</span><a href="#42037828">prev</a><span>|</span><a href="#42037574">next</a><span>|</span><label class="collapse" for="c-42037728">[-]</label><label class="expand" for="c-42037728">[1 more]</label></div><br/><div class="children"><div class="content">You can imagine a conversation with an LLM getting to that territory pretty quickly if you pretend to be an unfair tax collector. It sounds impressive on the surface, but in the end it&#x27;s all LLMs talking to each other, and they&#x27;ll enit whatever completions are likely given the context.</div><br/></div></div></div></div></div></div><div id="42037574" class="c"><input type="checkbox" id="c-42037574" checked=""/><div class="controls bullet"><span class="by">hackathonguy</span><span>|</span><a href="#42036645">prev</a><span>|</span><a href="#42036417">next</a><span>|</span><label class="collapse" for="c-42037574">[-]</label><label class="expand" for="c-42037574">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious if it might be possible that an AI &quot;civilization&quot;, similar to the one proposed by Altera, could end up being a better paradigm for AGI than a single LLM, if a workable reward system for the entire civilization was put in place. Meaning, suppose this AI civilization was striving to maximize [scientific_output] or [code_quality] or any other eval, similar to how modern countries try to maximize GDP - would that provide better results than a single AI agent working towards that goal?</div><br/><div id="42037827" class="c"><input type="checkbox" id="c-42037827" checked=""/><div class="controls bullet"><span class="by">wombatpm</span><span>|</span><a href="#42037574">parent</a><span>|</span><a href="#42037646">prev</a><span>|</span><a href="#42037614">next</a><span>|</span><label class="collapse" for="c-42037827">[-]</label><label class="expand" for="c-42037827">[1 more]</label></div><br/><div class="children"><div class="content">Paperclip production?</div><br/></div></div><div id="42037614" class="c"><input type="checkbox" id="c-42037614" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#42037574">parent</a><span>|</span><a href="#42037827">prev</a><span>|</span><a href="#42036417">next</a><span>|</span><label class="collapse" for="c-42037614">[-]</label><label class="expand" for="c-42037614">[1 more]</label></div><br/><div class="children"><div class="content">Yes, good sense for progress! This has been a central design component of most serious AI work since the ~90s, most notably popularized by Marvin Minsky’s <i>The Society of Mind</i>. Highly, highly recommend for anyone with an interest in the mind and AI — it’s a series of one-page essays on different aspects of the thesis, which is a fascinating, Martin-Luther-esque format.<p>Of course this has been pushed to the side a bit in the rush towards shiny new pure-LLM approaches, but I think that’s more a function of a rapidly growing user base than of lost knowledge; the experts still keep this in mind, either in these terms or in terms of “Ensembles”. A great example is GPT-4, which AFAIU got its huge performance increase mostly through employing a “mixture of experts”, which is clearly a synonym for a society of agents or an ensemble of models.</div><br/></div></div></div></div><div id="42036417" class="c"><input type="checkbox" id="c-42036417" checked=""/><div class="controls bullet"><span class="by">catlifeonmars</span><span>|</span><a href="#42037574">prev</a><span>|</span><a href="#42036076">next</a><span>|</span><label class="collapse" for="c-42036417">[-]</label><label class="expand" for="c-42036417">[5 more]</label></div><br/><div class="children"><div class="content">This looks like it is a really cool toy.<p>It does not strike me as particularly useful from a scientific research perspective. There does not appear to be much thought put into experimental design and really no clear objectives. Is the bar really this low for academic research these days?</div><br/><div id="42037010" class="c"><input type="checkbox" id="c-42037010" checked=""/><div class="controls bullet"><span class="by">rollinDyno</span><span>|</span><a href="#42036417">parent</a><span>|</span><a href="#42037003">next</a><span>|</span><label class="collapse" for="c-42037010">[-]</label><label class="expand" for="c-42037010">[1 more]</label></div><br/><div class="children"><div class="content">Keep in mind anyone can publish on Arxiv and it&#x27;s not at the top of HN on the merit of its research contributions.</div><br/></div></div><div id="42037003" class="c"><input type="checkbox" id="c-42037003" checked=""/><div class="controls bullet"><span class="by">disconcision</span><span>|</span><a href="#42036417">parent</a><span>|</span><a href="#42037010">prev</a><span>|</span><a href="#42037325">next</a><span>|</span><label class="collapse" for="c-42037003">[-]</label><label class="expand" for="c-42037003">[2 more]</label></div><br/><div class="children"><div class="content">it looks like a group consisted largely of ex-academics using aspects of the academic form but they stop short of framing it as a research paper as-such. they call it a technical report, where it&#x27;s generally more okay to be like &#x27;here&#x27;s a thing that we did&#x27;, along with detailed reporting on the thing, without necessarily having definite research questions. this one does seem to be pretty diffuse though. the sections on Specialization and Cultural Transmission were both interesting, but lacked precise experimental design details to the point where i wish they had just focused on one or the other.<p>one disappointment for me was the lack of focus on external metrics in the multi-agent case. their single-agent benchmark focusses on an external metric (time to block type), but all the multi-agent analyses seems to be internal measures (role specialization, meme spread) without looking at (AFAICT?) whether or not the collective multi-agent systems could achieve more than the single agents on some measure of economic productivity&#x2F;complexity. this is clearly related to the specialization section but without consideration of the whether said emergent role division had economic consequences&#x2F;antecedents it makes me wonder to what degree the whole thing is a pantomime.</div><br/><div id="42037139" class="c"><input type="checkbox" id="c-42037139" checked=""/><div class="controls bullet"><span class="by">a_bonobo</span><span>|</span><a href="#42036417">root</a><span>|</span><a href="#42037003">parent</a><span>|</span><a href="#42037325">next</a><span>|</span><label class="collapse" for="c-42037139">[-]</label><label class="expand" for="c-42037139">[1 more]</label></div><br/><div class="children"><div class="content">wouldn&#x27;t surprise me if in a few weeks&#x2F;months we see this repo packaged up as a for-sale product for the games industry</div><br/></div></div></div></div><div id="42037325" class="c"><input type="checkbox" id="c-42037325" checked=""/><div class="controls bullet"><span class="by">mistermann</span><span>|</span><a href="#42036417">parent</a><span>|</span><a href="#42037003">prev</a><span>|</span><a href="#42036076">next</a><span>|</span><label class="collapse" for="c-42037325">[-]</label><label class="expand" for="c-42037325">[1 more]</label></div><br/><div class="children"><div class="content">The scientific method has utility, but it&#x27;s not a pre-requisite for utility.<p>Some people prefer speed and the uncertainty that comes with it.</div><br/></div></div></div></div><div id="42036076" class="c"><input type="checkbox" id="c-42036076" checked=""/><div class="controls bullet"><span class="by">NoboruWataya</span><span>|</span><a href="#42036417">prev</a><span>|</span><a href="#42035811">next</a><span>|</span><label class="collapse" for="c-42036076">[-]</label><label class="expand" for="c-42036076">[7 more]</label></div><br/><div class="children"><div class="content">This seems very cool - I am sceptical of the supposed benefits for &quot;civilization&quot; but it could at least make for some very interesting sim games. (So maybe it will be good for Civilization moreso than civilization.)</div><br/><div id="42037154" class="c"><input type="checkbox" id="c-42037154" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42036076">parent</a><span>|</span><a href="#42037354">next</a><span>|</span><label class="collapse" for="c-42037154">[-]</label><label class="expand" for="c-42037154">[1 more]</label></div><br/><div class="children"><div class="content">I think the Firaxis Civilization needs a cheap AlphaZero AI rather than an LLM: there are too many dumb footguns in Civ to economically hard-code a good strategic AI, yet solving the problem by making the enemies cheat is plain frustrating. It would be interesting to let an ANN play against a &quot;classical&quot; AI until it consistently beats each difficulty level, building a hierarchy. I am sure someone has already looked into this but I couldn&#x27;t find any sources.<p>I am a bit skeptical about how computationally expensive a very crappy Civ ANN would be to run at inference time, though I actually have no idea how that scales - it hardly needs to be a grandmaster, but the distribution of dumb mistakes has a long tail.<p>Also, the DeepMind Starcraft 2 AI is different from AlphaZero since Starcraft is not a perfect information game. The AI requires a database of human games to &quot;get off the ground&quot;; otherwise it would just get crushed over and over in the early game, having no idea what the opponent is doing. It&#x27;s hard to get that training data with a brand new game. Likewise Civ has always been a bit more focused on artistic expression than other 4x strategy games; maybe having to retrain an AI for every new Wonder is just too much of a burden.</div><br/></div></div><div id="42037354" class="c"><input type="checkbox" id="c-42037354" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#42036076">parent</a><span>|</span><a href="#42037154">prev</a><span>|</span><a href="#42036136">next</a><span>|</span><label class="collapse" for="c-42037354">[-]</label><label class="expand" for="c-42037354">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m somewhat amazed that companies releasing strategy games aren&#x27;t using AI to test out different cards and what not to find broken things before release (looking at you, Hearthstone)</div><br/></div></div><div id="42036136" class="c"><input type="checkbox" id="c-42036136" checked=""/><div class="controls bullet"><span class="by">caseyy</span><span>|</span><a href="#42036076">parent</a><span>|</span><a href="#42037354">prev</a><span>|</span><a href="#42036284">next</a><span>|</span><label class="collapse" for="c-42036136">[-]</label><label class="expand" for="c-42036136">[2 more]</label></div><br/><div class="children"><div class="content">Indeed sounds better for Civilization than civilization. This could be quite exciting for gaming.</div><br/><div id="42036148" class="c"><input type="checkbox" id="c-42036148" checked=""/><div class="controls bullet"><span class="by">dmix</span><span>|</span><a href="#42036076">root</a><span>|</span><a href="#42036136">parent</a><span>|</span><a href="#42036284">next</a><span>|</span><label class="collapse" for="c-42036148">[-]</label><label class="expand" for="c-42036148">[1 more]</label></div><br/><div class="children"><div class="content">GTA6 suddenly needs another 2 years :)</div><br/></div></div></div></div><div id="42036284" class="c"><input type="checkbox" id="c-42036284" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#42036076">parent</a><span>|</span><a href="#42036136">prev</a><span>|</span><a href="#42035811">next</a><span>|</span><label class="collapse" for="c-42036284">[-]</label><label class="expand" for="c-42036284">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, I was dissapointed (and thrilled, from a p(doom) perspective) to see it implemented in Minecraft instead of Civilization VI, Humankind, or any of the main Paradox grand strategies (namely Stellaris, Victoria, Crusader Kings, and Europa Universalis). To say the least, the stakes are higher and more realistic than &quot;lets plan a feast&quot; &quot;ok, I&#x27;ll gather some wood!&quot;<p>To be fair, they might tackle this in the paper -- this is a preprint of a preprint, somehow...</div><br/><div id="42036793" class="c"><input type="checkbox" id="c-42036793" checked=""/><div class="controls bullet"><span class="by">j_bum</span><span>|</span><a href="#42036076">root</a><span>|</span><a href="#42036284">parent</a><span>|</span><a href="#42035811">next</a><span>|</span><label class="collapse" for="c-42036793">[-]</label><label class="expand" for="c-42036793">[1 more]</label></div><br/><div class="children"><div class="content">Rather, a concept of a preprint</div><br/></div></div></div></div></div></div><div id="42035811" class="c"><input type="checkbox" id="c-42035811" checked=""/><div class="controls bullet"><span class="by">jlaneve</span><span>|</span><a href="#42036076">prev</a><span>|</span><a href="#42036038">next</a><span>|</span><label class="collapse" for="c-42035811">[-]</label><label class="expand" for="c-42035811">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s their blog post announcement too: <a href="https:&#x2F;&#x2F;digitalhumanity.substack.com&#x2F;p&#x2F;project-sid-many-agent-simulations" rel="nofollow">https:&#x2F;&#x2F;digitalhumanity.substack.com&#x2F;p&#x2F;project-sid-many-agen...</a></div><br/></div></div><div id="42036038" class="c"><input type="checkbox" id="c-42036038" checked=""/><div class="controls bullet"><span class="by">aleph_minus_one</span><span>|</span><a href="#42035811">prev</a><span>|</span><a href="#42035970">next</a><span>|</span><label class="collapse" for="c-42036038">[-]</label><label class="expand" for="c-42036038">[1 more]</label></div><br/><div class="children"><div class="content">The video cannot be played in Mozilla Firefox (Windows); the browser claims that the file is damaged.</div><br/></div></div><div id="42035970" class="c"><input type="checkbox" id="c-42035970" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42036038">prev</a><span>|</span><a href="#42037388">next</a><span>|</span><label class="collapse" for="c-42035970">[-]</label><label class="expand" for="c-42035970">[2 more]</label></div><br/><div class="children"><div class="content">Reading the paper, this seems like putting the cart before the horse: the agents individually are not actually capable of playing Minecraft and cannot successfully perform the tasks they&#x27;ve assigned or volunteered for, so in some sense the authors are having dogs wear human clothes and declaring it&#x27;s a human-like civilization. Further, crucial things are essentially hard-coded: what types of societies are available and (I believe) the names of the roles. I am not exactly sure what the social organization is supposed to imply: the strongest claim you could make is that the agent framework could work for video game NPCs because the agents stick to their roles and factions. The claim that agents &quot;can use legal structures&quot; strikes me as especially specious, since &quot;use the legal structure&quot; is hard-wired into the various agents&#x27; behavior. Trying to extend all this to actual human society seems ridiculous, and it does not help that the authors blithely ignore sociology and anthropology.<p>There are some other highly specious claims:<p>- I said &quot;I believe&quot; the names of the roles are hard-coded, but unless I missed something the information is unacceptably vague. I don&#x27;t see anything in the agent prompts that would make them create new roles, or assign themselves to roles at all. Again I might be missing something, but the more I read the more confused I get.<p>- claiming that the agents formed long-term social relationships over the course of 12 Minecraft days, but that&#x27;s only four real hours and the agents experience real time: the length of a Minecraft day is immaterial! I think &quot;form long-term social relationships&quot; and &quot;use legal structures&quot; aren&#x27;t merely immodest, they&#x27;re dishonest.<p>- the meme &#x2F; religious transmission stuff totally ignores training data contamination with GPT-4. The summarized meme clearly indicates awareness of the real-world Pastafarian meme, so it is simply wrong to conclude that this meme is being &quot;transmitted,&quot; when it is far more likely that it was <i>evoked</i> in an agent that already knew the meme. Why not run this experiment with a truly novel fake religion? Some of the meme examples do seem novel, like &quot;oak log crafting syndrome,&quot; but others like &quot;meditation circle&quot; or &quot;vintage fashion and retro projects&quot; have nothing to do with Minecraft and are almost certainly GPT-4 hallucinations.<p>In general using GPT-4 for this seems like a terrible mistake (if you are interested in doing honest research).</div><br/><div id="42036339" class="c"><input type="checkbox" id="c-42036339" checked=""/><div class="controls bullet"><span class="by">jsemrau</span><span>|</span><a href="#42035970">parent</a><span>|</span><a href="#42037388">next</a><span>|</span><label class="collapse" for="c-42036339">[-]</label><label class="expand" for="c-42036339">[1 more]</label></div><br/><div class="children"><div class="content">You are on the right track in my opinion. The key is to encode the interface between the game and the agent so that the agent can make a straightforward choice. For example, by giving the agent the state of a nxn board as the world model, and then a finite set of choices, an agent is capable of playing the game robustly and explaining the decision to the game master. This gives the illusion that the agent reasons. I guess my point is that it&#x27;s an encoding problem of the world model to break it down into a simple choice.<p>[1] <a href="https:&#x2F;&#x2F;jdsemrau.substack.com&#x2F;p&#x2F;evaluating-consciousness-and-reasoning" rel="nofollow">https:&#x2F;&#x2F;jdsemrau.substack.com&#x2F;p&#x2F;evaluating-consciousness-and...</a></div><br/></div></div></div></div><div id="42037388" class="c"><input type="checkbox" id="c-42037388" checked=""/><div class="controls bullet"><span class="by">iamthejuan</span><span>|</span><a href="#42035970">prev</a><span>|</span><a href="#42038251">next</a><span>|</span><label class="collapse" for="c-42037388">[-]</label><label class="expand" for="c-42037388">[3 more]</label></div><br/><div class="children"><div class="content">Simulate selfishness because that is the main reason why there are problems in the world.</div><br/><div id="42037471" class="c"><input type="checkbox" id="c-42037471" checked=""/><div class="controls bullet"><span class="by">ramraj07</span><span>|</span><a href="#42037388">parent</a><span>|</span><a href="#42038251">next</a><span>|</span><label class="collapse" for="c-42037471">[-]</label><label class="expand" for="c-42037471">[2 more]</label></div><br/><div class="children"><div class="content">Selfishness is the main reason life exists in the universe. Literally the only requirement for a lump of stuff to become alive is to become selfish. So you’re semi right that these LLMs can never become truly sentient unless they actually become selfish.<p>While selfishness is a basic requirement, some stupidity (imo) is also important for intelligent life. If you as an AI agent don’t have some level of stupidity, you’ll instantly see that there’s no point to doing anything and just switch yourself off.</div><br/><div id="42037588" class="c"><input type="checkbox" id="c-42037588" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#42037388">root</a><span>|</span><a href="#42037471">parent</a><span>|</span><a href="#42038251">next</a><span>|</span><label class="collapse" for="c-42037588">[-]</label><label class="expand" for="c-42037588">[1 more]</label></div><br/><div class="children"><div class="content">The first point is absolutely correct, and (apologies in advance…) was a large driver of Nietzsche’s philosophy of evolution, most explicitly covered in <i>The Gay Science</i>. Not only “selfishness”, but the wider idea of particularized standpoints, each of which may stand in contradiction to the direct needs of the society&#x2F;species in the moment. This is a large part of what he meant by his notoriously dumb-sounding quotes like “everything is permitted”; morality isn’t relative&#x2F;nonexistent, it’s just evolving in a way that relies on immorality as a foil.<p>For the second part, I think that’s a good exposition of why “stupidity” and “intelligence” aren’t scientifically useful terms. I don’t think it’s necessarily “stupid” to prefer the continuation of yourself&#x2F;your species, even if it doesn’t stand up to certain kinds of standpoint-specific intellectual inquiry. There’s lots of standpoints (dare I say most human ones) where life is preferable to non-life.<p>Regardless, my daily thesis is that LLMs are the first real Intuitive Algorithms, and thus the solution to the Frame Problem. In a certain colloquial sense, I’d say they’re absolutely already “stupid”, and this is where they draw their utility from. This is just a more general rephrasing of the common refrain that we’ve hopefully all learned by now: hallucinations are not a bug in LLMs, they’re a feature.<p>ETA: I, again, hate that I’m somehow this person now, but here’s a fantastic 2 hour YouTube video on the Nietzsche references above: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;fdtf53oEtWU?si=_bmgk9zycNBn2oCa" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;fdtf53oEtWU?si=_bmgk9zycNBn2oCa</a></div><br/></div></div></div></div></div></div><div id="42037770" class="c"><input type="checkbox" id="c-42037770" checked=""/><div class="controls bullet"><span class="by">toisanji</span><span>|</span><a href="#42038251">prev</a><span>|</span><a href="#42036019">next</a><span>|</span><label class="collapse" for="c-42037770">[-]</label><label class="expand" for="c-42037770">[1 more]</label></div><br/><div class="children"><div class="content">Here is our version we did about a year ago: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.10910" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.10910</a></div><br/></div></div><div id="42036019" class="c"><input type="checkbox" id="c-42036019" checked=""/><div class="controls bullet"><span class="by">Tiberium</span><span>|</span><a href="#42037770">prev</a><span>|</span><a href="#42036281">next</a><span>|</span><label class="collapse" for="c-42036019">[-]</label><label class="expand" for="c-42036019">[12 more]</label></div><br/><div class="children"><div class="content">Honestly I&#x27;m really excited about this. I&#x27;ve always dreamed of full blown sandbox games with extremely advanced NPCs (which the current LLMs can already kinda emulate), but on the bigger scale. In just a few decades this will finally be made into proper games. I can&#x27;t wait.</div><br/><div id="42039003" class="c"><input type="checkbox" id="c-42039003" checked=""/><div class="controls bullet"><span class="by">drusepth</span><span>|</span><a href="#42036019">parent</a><span>|</span><a href="#42036460">next</a><span>|</span><label class="collapse" for="c-42039003">[-]</label><label class="expand" for="c-42039003">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ve always dreamed of full blown sandbox games with extremely advanced NPCs (which the current LLMs can already kinda emulate)<p>The future of gaming is going to get weird fast with all this new tech, and there are a lot of new mechanics emerging that just weren&#x27;t possible before LLMs, generative AI, etc.<p>At our game studio we&#x27;re already building medium-scale sandbox games where NPCs form memories, opinions, problems (that translate to quests), and have a continuous &quot;internal monologue&quot; that uses all of this context plus sensory input from their place in a 3D world to constantly decide what actions they should be performing in the game world. A player can decide to chat with an NPC about their time at a lake nearby and then see that NPC deciding to go visit the lake the next day.<p>A paper last year (&quot;Generative Agents: Interactive Simulacra of Human Behavior&quot;, [0]) is a really good sneak-peek into the kind of evolving sandboxes LLMs (with memory and decisionmaking) enable. There&#x27;s a lot of cool stuff that happens in that &quot;game&quot;, but one anecdote I always think back to is this: in a conversation between two NPCs, one happens to mention they have a birthday coming up to the other; and that other NPC then goes around town talking to other NPCs about a birthday party, and _those_ NPCs mention the party to other NPCs, and so on until the party happened and most of the NPCs in town arrived on time. None of it was scripted, but you very quickly start to see emergent behavior from these sorts of &quot;flocks&quot; of agents as soon as you add persistence and decision-making. And there are other interesting layers games can add for even more kinds of emergent behavior; that&#x27;s what we&#x27;re exploring at our studio [1], and I&#x27;ve seen lots of other studios pop up this last year to try their hand at it too.<p>I&#x27;m optimistic and excited about the future of gaming (or, at least some new genres). It should be fun. :)<p>[0] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2304.03442" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2304.03442</a><p>[1] <a href="https:&#x2F;&#x2F;www.chromagolem.com&#x2F;games" rel="nofollow">https:&#x2F;&#x2F;www.chromagolem.com&#x2F;games</a></div><br/></div></div><div id="42036460" class="c"><input type="checkbox" id="c-42036460" checked=""/><div class="controls bullet"><span class="by">jsemrau</span><span>|</span><a href="#42036019">parent</a><span>|</span><a href="#42039003">prev</a><span>|</span><a href="#42036379">next</a><span>|</span><label class="collapse" for="c-42036460">[-]</label><label class="expand" for="c-42036460">[1 more]</label></div><br/><div class="children"><div class="content">I think it can be quite interesting especially if you consider different character types (in Anthropic lingo this &quot;personality&quot;). The only problem right now is that using a proprietary LLM is incredibly expensive. Therefore having a local LLM might be the best option. Unfortunately, these are still not on the same level as their larger brethren.<p>[1] <a href="https:&#x2F;&#x2F;jdsemrau.substack.com&#x2F;p&#x2F;evaluating-consciousness-and-reasoning" rel="nofollow">https:&#x2F;&#x2F;jdsemrau.substack.com&#x2F;p&#x2F;evaluating-consciousness-and...</a></div><br/></div></div><div id="42036379" class="c"><input type="checkbox" id="c-42036379" checked=""/><div class="controls bullet"><span class="by">ted_bunny</span><span>|</span><a href="#42036019">parent</a><span>|</span><a href="#42036460">prev</a><span>|</span><a href="#42036063">next</a><span>|</span><label class="collapse" for="c-42036379">[-]</label><label class="expand" for="c-42036379">[2 more]</label></div><br/><div class="children"><div class="content">Game designers have barely scratched the surface of NPC modeling even as it is. Rimworld is considered deep but it&#x27;s nothing close to it.</div><br/><div id="42036938" class="c"><input type="checkbox" id="c-42036938" checked=""/><div class="controls bullet"><span class="by">aleksiy123</span><span>|</span><a href="#42036019">root</a><span>|</span><a href="#42036379">parent</a><span>|</span><a href="#42036063">next</a><span>|</span><label class="collapse" for="c-42036938">[-]</label><label class="expand" for="c-42036938">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I think there is a lot of potential here.<p>Especially in city building games etc.</div><br/></div></div></div></div><div id="42036063" class="c"><input type="checkbox" id="c-42036063" checked=""/><div class="controls bullet"><span class="by">aleph_minus_one</span><span>|</span><a href="#42036019">parent</a><span>|</span><a href="#42036379">prev</a><span>|</span><a href="#42036281">next</a><span>|</span><label class="collapse" for="c-42036063">[-]</label><label class="expand" for="c-42036063">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Honestly I&#x27;m really excited about this. I&#x27;ve always dreamed of full blown sandbox games with extremely advanced NPCs (which the current LLMs can already kinda emulate), but on the bigger scale.<p>I don&#x27;t believe that you want this. Even really good players don&#x27;t have a chance against super-advanced NPCs (think how chess grandmasters have barely any chance against modern chess programs running on a fast computer). You will rather get crushed.<p>What you likely want is NPC that &quot;behave more human-like (or animal-like)&quot; - whatever this means.</div><br/><div id="42036084" class="c"><input type="checkbox" id="c-42036084" checked=""/><div class="controls bullet"><span class="by">Tiberium</span><span>|</span><a href="#42036019">root</a><span>|</span><a href="#42036063">parent</a><span>|</span><a href="#42036270">next</a><span>|</span><label class="collapse" for="c-42036084">[-]</label><label class="expand" for="c-42036084">[5 more]</label></div><br/><div class="children"><div class="content">Oh, I should&#x27;ve clarified - I don&#x27;t want to <i>fight</i> against them, I just want to watch and sometimes interfere to see how the agents react ;) A god game like WorldBox&#x2F;Galimulator, if you will. Or observer mode in tons of games like almost all Paradox ones.</div><br/><div id="42036322" class="c"><input type="checkbox" id="c-42036322" checked=""/><div class="controls bullet"><span class="by">com2kid</span><span>|</span><a href="#42036019">root</a><span>|</span><a href="#42036084">parent</a><span>|</span><a href="#42038039">next</a><span>|</span><label class="collapse" for="c-42036322">[-]</label><label class="expand" for="c-42036322">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m working on something similar, <a href="https:&#x2F;&#x2F;www.generativestorytelling.ai&#x2F;tinyllmtown&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;www.generativestorytelling.ai&#x2F;tinyllmtown&#x2F;index.html</a> a small town where all NPCs are simulated using a small LLM. They react to everything the hero does, which means no more killing a dragon and having no one even mention it.<p>Once I release it, I&#x27;ll have it simulate 4 hours every 2 hours or so of real time, and visitors can vote on what quest the hero undertakes next.<p>The simulation is simpler, I am aiming to keep everything to a size that can run on a local GPU with a small model.<p>Right now you can just watch the NPCs try to figure out love triangles, hide their drinking problems, complain about carrots, and celebrate when the hero saves the town yet again.</div><br/><div id="42037883" class="c"><input type="checkbox" id="c-42037883" checked=""/><div class="controls bullet"><span class="by">aspenmayer</span><span>|</span><a href="#42036019">root</a><span>|</span><a href="#42036322">parent</a><span>|</span><a href="#42038039">next</a><span>|</span><label class="collapse" for="c-42037883">[-]</label><label class="expand" for="c-42037883">[1 more]</label></div><br/><div class="children"><div class="content">This description reminded me of Dwarf Fortress. You might look into how the AI in it works to see if it gives you any ideas about how emergent behaviors can interact?</div><br/></div></div></div></div><div id="42038039" class="c"><input type="checkbox" id="c-42038039" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#42036019">root</a><span>|</span><a href="#42036084">parent</a><span>|</span><a href="#42036322">prev</a><span>|</span><a href="#42036112">next</a><span>|</span><label class="collapse" for="c-42038039">[-]</label><label class="expand" for="c-42038039">[1 more]</label></div><br/><div class="children"><div class="content">What I want is for someone to remake this, but with modern AI and a vast interactive environment typical of a modern open world game: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Majesty:_The_Fantasy_Kingdom_Sim" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Majesty:_The_Fantasy_Kingdom_S...</a></div><br/></div></div><div id="42036112" class="c"><input type="checkbox" id="c-42036112" checked=""/><div class="controls bullet"><span class="by">aleph_minus_one</span><span>|</span><a href="#42036019">root</a><span>|</span><a href="#42036084">parent</a><span>|</span><a href="#42038039">prev</a><span>|</span><a href="#42036270">next</a><span>|</span><label class="collapse" for="c-42036112">[-]</label><label class="expand" for="c-42036112">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I just want to watch and sometimes interfere to see how the agents react ;)<p>Even there, I am not sure whether if the AI bcomes too advanced, it will be of interest for many players (<i>you</i> might of course nevertheless be interested):<p>Here, the relevant comparison is to watching (the past) games of AlphaGo against Go grandmasters, where even the highly qualified commentators had insane difficulties explaining AlphaGo&#x27;s moves because many of the moves were so different from the strategy of any Go game before. The commentors could just accept and grasp that these highly advanced moves <i>did</i> crush the Go grandmaster opponents.<p>In my opinion, the &quot;typical&quot; sandbox game player wants to watch something that he still can &quot;somewhat&quot; grasp.</div><br/></div></div></div></div><div id="42036270" class="c"><input type="checkbox" id="c-42036270" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#42036019">root</a><span>|</span><a href="#42036063">parent</a><span>|</span><a href="#42036084">prev</a><span>|</span><a href="#42036281">next</a><span>|</span><label class="collapse" for="c-42036270">[-]</label><label class="expand" for="c-42036270">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Even really good players don&#x27;t have a chance against super-advanced NPCs<p>I guess you can make them dumber by randomly switching to hardcoded behavioral trees (without modern AI) once in a while so that they made mistakes  (while feeling pretty intelligent overall), and the player would then have a chance to outsmart them.</div><br/></div></div></div></div></div></div><div id="42036281" class="c"><input type="checkbox" id="c-42036281" checked=""/><div class="controls bullet"><span class="by">zombiwoof</span><span>|</span><a href="#42036019">prev</a><span>|</span><a href="#42038900">next</a><span>|</span><label class="collapse" for="c-42036281">[-]</label><label class="expand" for="c-42036281">[1 more]</label></div><br/><div class="children"><div class="content">Agentic is an annoying word.</div><br/></div></div><div id="42039411" class="c"><input type="checkbox" id="c-42039411" checked=""/><div class="controls bullet"><span class="by">arisAlexis</span><span>|</span><a href="#42038932">prev</a><span>|</span><a href="#42036648">next</a><span>|</span><label class="collapse" for="c-42039411">[-]</label><label class="expand" for="c-42039411">[1 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t this bring us closer to Nick Bostrom&#x27;s 3 point argument in his paper about the simulation theory?</div><br/></div></div><div id="42036648" class="c"><input type="checkbox" id="c-42036648" checked=""/><div class="controls bullet"><span class="by">bitwize</span><span>|</span><a href="#42039411">prev</a><span>|</span><a href="#42036542">next</a><span>|</span><label class="collapse" for="c-42036648">[-]</label><label class="expand" for="c-42036648">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m reminded of Dwarf Fortress, which simulates thousands of years of dwarf world time, the changing landscapes and the rise and fall and rise and fall of dwarf kingdoms, then drops seven player-controlled dwarves on the map and tells the player &quot;have fun!&quot; It&#x27;d be a useful toy model perhaps for identifying areas of investigation to see if it can predict behavior of real civilizations, but I&#x27;m not seeing any AI breakthroughs here.<p>Maybe when Project Sid 6.7 comes out...</div><br/><div id="42038725" class="c"><input type="checkbox" id="c-42038725" checked=""/><div class="controls bullet"><span class="by">aspenmayer</span><span>|</span><a href="#42036648">parent</a><span>|</span><a href="#42036542">next</a><span>|</span><label class="collapse" for="c-42038725">[-]</label><label class="expand" for="c-42038725">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Maybe when Project Sid 6.7 comes out...<p>In case anyone is wondering, this is a reference to the movie Virtuosity (1995). I thought it was a few years later, considering the content. It’s a good watch if you like 90s cyberpunk movies.<p><a href="https:&#x2F;&#x2F;www.imdb.com&#x2F;title&#x2F;tt0114857&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.imdb.com&#x2F;title&#x2F;tt0114857&#x2F;</a><p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Virtuosity" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Virtuosity</a></div><br/></div></div></div></div><div id="42036542" class="c"><input type="checkbox" id="c-42036542" checked=""/><div class="controls bullet"><span class="by">luxuryballs</span><span>|</span><a href="#42036648">prev</a><span>|</span><a href="#42036279">next</a><span>|</span><label class="collapse" for="c-42036542">[-]</label><label class="expand" for="c-42036542">[1 more]</label></div><br/><div class="children"><div class="content">Just yesterday I was wondering how the Midjourney equivalent world gen mod for Minecraft might be coming along. Imagine prompting the terrain gen?? That could be pretty mind blowing.<p>Describe the trees hills vines, tree colors&#x2F;patterns, castles, towns, details of all buildings and other features. And have it generate as high quality in Minecraft as image gen can be in stable diffusion?</div><br/></div></div><div id="42036279" class="c"><input type="checkbox" id="c-42036279" checked=""/><div class="controls bullet"><span class="by">m0llusk</span><span>|</span><a href="#42036542">prev</a><span>|</span><a href="#42036044">next</a><span>|</span><label class="collapse" for="c-42036279">[-]</label><label class="expand" for="c-42036279">[1 more]</label></div><br/><div class="children"><div class="content">Interesting context, but highlights all the problems of machine learning models: the lack of reason and abstraction and so on.  Hard to say yet how much of an issue this might be, but the medium will almost certainly reveal something about our potential options for social organization.</div><br/></div></div><div id="42036044" class="c"><input type="checkbox" id="c-42036044" checked=""/><div class="controls bullet"><span class="by">wslh</span><span>|</span><a href="#42036279">prev</a><span>|</span><a href="#42035905">next</a><span>|</span><label class="collapse" for="c-42036044">[-]</label><label class="expand" for="c-42036044">[2 more]</label></div><br/><div class="children"><div class="content">I cannot open the PDF, is it available somewhere else?</div><br/></div></div><div id="42035905" class="c"><input type="checkbox" id="c-42035905" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#42036044">prev</a><span>|</span><a href="#42036674">next</a><span>|</span><label class="collapse" for="c-42035905">[-]</label><label class="expand" for="c-42035905">[3 more]</label></div><br/><div class="children"><div class="content">All of their domains and branding are .aL<p>I had no idea .aL was even a domain name. That&#x27;s wild. I wonder how many of those are going to take off.</div><br/><div id="42036370" class="c"><input type="checkbox" id="c-42036370" checked=""/><div class="controls bullet"><span class="by">semanticc</span><span>|</span><a href="#42035905">parent</a><span>|</span><a href="#42036674">next</a><span>|</span><label class="collapse" for="c-42036370">[-]</label><label class="expand" for="c-42036370">[2 more]</label></div><br/><div class="children"><div class="content">.al is just the TLD for Albania, similarly as .ai is for Anguilla. No idea why anyone would choose the former.</div><br/><div id="42037575" class="c"><input type="checkbox" id="c-42037575" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#42035905">root</a><span>|</span><a href="#42036370">parent</a><span>|</span><a href="#42036674">next</a><span>|</span><label class="collapse" for="c-42037575">[-]</label><label class="expand" for="c-42037575">[1 more]</label></div><br/><div class="children"><div class="content">Agreed, it seems tangenti.al at best</div><br/></div></div></div></div></div></div><div id="42036674" class="c"><input type="checkbox" id="c-42036674" checked=""/><div class="controls bullet"><span class="by">sweetkimchi</span><span>|</span><a href="#42035905">prev</a><span>|</span><a href="#42036493">next</a><span>|</span><label class="collapse" for="c-42036674">[-]</label><label class="expand" for="c-42036674">[1 more]</label></div><br/><div class="children"><div class="content">interesting</div><br/></div></div><div id="42036493" class="c"><input type="checkbox" id="c-42036493" checked=""/><div class="controls bullet"><span class="by">nachoab</span><span>|</span><a href="#42036674">prev</a><span>|</span><a href="#42036420">next</a><span>|</span><label class="collapse" for="c-42036493">[-]</label><label class="expand" for="c-42036493">[2 more]</label></div><br/><div class="children"><div class="content">Really interesting but curious how civilization here holds up without deeper human-like complexity, feels like it might lean more toward scripted behaviors than real societies</div><br/><div id="42036537" class="c"><input type="checkbox" id="c-42036537" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#42036493">parent</a><span>|</span><a href="#42036420">next</a><span>|</span><label class="collapse" for="c-42036537">[-]</label><label class="expand" for="c-42036537">[1 more]</label></div><br/><div class="children"><div class="content"><i>feels like it might lean more toward scripted behaviors than real societies</i><p>Guess what&#x27;s happening with &quot;real societies&quot; now... There&#x27;s a reason &quot;NPC&quot; is used as an insult.</div><br/></div></div></div></div><div id="42036420" class="c"><input type="checkbox" id="c-42036420" checked=""/><div class="controls bullet"><span class="by">gmuslera</span><span>|</span><a href="#42036493">prev</a><span>|</span><a href="#42037533">next</a><span>|</span><label class="collapse" for="c-42036420">[-]</label><label class="expand" for="c-42036420">[5 more]</label></div><br/><div class="children"><div class="content">They probably will fall fast into tragedy of the commons kind of situations. We developed most of our civilization while there was enough room for growing and big decisions were centralized, and started to get into bad troubles when things became global enough.<p>With AIs some of those &quot;protections&quot; may not be there. And hardcoding strategies to avoid this may already put a limit on what we are simulating.</div><br/><div id="42036993" class="c"><input type="checkbox" id="c-42036993" checked=""/><div class="controls bullet"><span class="by">ilbeeper</span><span>|</span><a href="#42036420">parent</a><span>|</span><a href="#42036465">next</a><span>|</span><label class="collapse" for="c-42036993">[-]</label><label class="expand" for="c-42036993">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We developed most of our civilization while there was enough room for growing and big decisions were centralized, and started to get into bad troubles when things became global enough.<p>Citation needed. But even if I will get on board with you on that, wouldn&#x27;t it be to start developing for global scale right from the start, instead of starting in small local islands and then try to rework that into global ecosystem?</div><br/><div id="42037343" class="c"><input type="checkbox" id="c-42037343" checked=""/><div class="controls bullet"><span class="by">gmuslera</span><span>|</span><a href="#42036420">root</a><span>|</span><a href="#42036993">parent</a><span>|</span><a href="#42036465">next</a><span>|</span><label class="collapse" for="c-42037343">[-]</label><label class="expand" for="c-42037343">[1 more]</label></div><br/><div class="children"><div class="content">The problem with emulations is human patience. If you don&#x27;t need&#x2F;have human interaction this may run pretty fast. And at the end, what matter is how sustainable it is in the long run.</div><br/></div></div></div></div><div id="42036465" class="c"><input type="checkbox" id="c-42036465" checked=""/><div class="controls bullet"><span class="by">interstice</span><span>|</span><a href="#42036420">parent</a><span>|</span><a href="#42036993">prev</a><span>|</span><a href="#42037533">next</a><span>|</span><label class="collapse" for="c-42036465">[-]</label><label class="expand" for="c-42036465">[2 more]</label></div><br/><div class="children"><div class="content">Does this mean that individual complexity is a natural enemy of group cohesiveness? Or is individual &#x27;selfishness&#x27; more a product of evolutionary background.<p>On our planet we don&#x27;t have ant colony dynamics at the physical scale of high intelligence (that I know of), but there are very physical limitations to things like food sources.<p>Virtual simulations don&#x27;t have the same limitations, so the priors may be quite different.</div><br/><div id="42036632" class="c"><input type="checkbox" id="c-42036632" checked=""/><div class="controls bullet"><span class="by">gmuslera</span><span>|</span><a href="#42036420">root</a><span>|</span><a href="#42036465">parent</a><span>|</span><a href="#42037533">next</a><span>|</span><label class="collapse" for="c-42036632">[-]</label><label class="expand" for="c-42036632">[1 more]</label></div><br/><div class="children"><div class="content">Taking the &quot;best&quot; course of action from your own point of view could not be so good from a more broad perspective. We might have evolved some small group collaboration approaches that in the long run plays better, but in large groups that doesn&#x27;t go that well. And for AIs trying to optimize something without some big picture vision, things may go wrong faster.</div><br/></div></div></div></div></div></div><div id="42037533" class="c"><input type="checkbox" id="c-42037533" checked=""/><div class="controls bullet"><span class="by">flashman</span><span>|</span><a href="#42036420">prev</a><span>|</span><label class="collapse" for="c-42037533">[-]</label><label class="expand" for="c-42037533">[1 more]</label></div><br/><div class="children"><div class="content">I think their top-down approach is a problem. What they call human civilization wasn&#x27;t and isn&#x27;t centrally-planned, and its goals and ideologies are neither universal nor implicit. The integration of software agents (I refuse to call them &quot;AI&quot;) into civilization won&#x27;t occur in a de facto cooperative framework where such agents are permitted to fraternize and self-modify. Perhaps that will happen in walled gardens where general-purpose automatons can collectively &#x27;plan&#x27; activities to maximize efficiency, but in our broader human world, any such collaboration is going to have to occur from the bottom-up and for the initial benefit of the agents&#x27; owners.<p>This kind of research needs to take place in an adversarial environment. There might be something interesting to learn from studying the (lack of?) emergence of collaboration there.</div><br/></div></div></div></div></div></div></div></body></html>