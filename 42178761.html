<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1732006876076" as="style"/><link rel="stylesheet" href="styles.css?v=1732006876076"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://cerebras.ai/blog/llama-405b-inference">Llama 3.1 405B now runs at 969 tokens/s on Cerebras Inference</a> <span class="domain">(<a href="https://cerebras.ai">cerebras.ai</a>)</span></div><div class="subtext"><span>benchmarkist</span> | <span>110 comments</span></div><br/><div><div id="42179476" class="c"><input type="checkbox" id="c-42179476" checked=""/><div class="controls bullet"><span class="by">zackangelo</span><span>|</span><a href="#42181071">next</a><span>|</span><label class="collapse" for="c-42179476">[-]</label><label class="expand" for="c-42179476">[49 more]</label></div><br/><div class="children"><div class="content">This is astonishingly fast. I’m struggling to get over 100 tok&#x2F;s on my own Llama 3.1 70b implementation on an 8x H100 cluster.<p>I’m curious how they’re doing it. Obviously the standard bag of tricks (eg, speculative decoding, flash attention) won’t get you close. It seems like at a minimum you’d have to do multi-node inference and maybe some kind of sparse attention mechanism?</div><br/><div id="42179501" class="c"><input type="checkbox" id="c-42179501" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#42179476">parent</a><span>|</span><a href="#42179503">next</a><span>|</span><label class="collapse" for="c-42179501">[-]</label><label class="expand" for="c-42179501">[23 more]</label></div><br/><div class="children"><div class="content">Cerebras makes CPUs with ~1 million cores, and they&#x27;re inferring on that not on GPUs. It&#x27;s an entirely different architecture which means no network involved. It&#x27;s possible they&#x27;re doing this significantly from CPU caches rather than HBM as well.<p>I recommend the TechTechPotato YouTube videos on Cerebras to understand more of their chip design.</div><br/><div id="42179599" class="c"><input type="checkbox" id="c-42179599" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179501">parent</a><span>|</span><a href="#42179717">next</a><span>|</span><label class="collapse" for="c-42179599">[-]</label><label class="expand" for="c-42179599">[1 more]</label></div><br/><div class="children"><div class="content">&gt; TechTechPotato YouTube videos on Cerebras<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;@TechTechPotato&#x2F;search?query=cerebras" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;@TechTechPotato&#x2F;search?query=cerebra...</a> for anyone also looking. there are quite a lot of them.</div><br/></div></div><div id="42179717" class="c"><input type="checkbox" id="c-42179717" checked=""/><div class="controls bullet"><span class="by">accrual</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179501">parent</a><span>|</span><a href="#42179599">prev</a><span>|</span><a href="#42179509">next</a><span>|</span><label class="collapse" for="c-42179717">[-]</label><label class="expand" for="c-42179717">[19 more]</label></div><br/><div class="children"><div class="content">I hope we can buy Cerebras cards one day. Imagine buying a ~$500 AI card for your desktop and having easy access to 70B+ models (the price is speculative&#x2F;made up).</div><br/><div id="42180050" class="c"><input type="checkbox" id="c-42180050" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179717">parent</a><span>|</span><a href="#42180265">next</a><span>|</span><label class="collapse" for="c-42180050">[-]</label><label class="expand" for="c-42180050">[12 more]</label></div><br/><div class="children"><div class="content">I believe pricing was mid 6 figures per machine. They&#x27;re also like 8U and water cooled I believe. I doubt it would be possible to deploy one outside of a fairly top tier colo facility where they have the ability to support water cooling. Also imagine learning a new CUDA but that is designed for another completely different compute model.</div><br/><div id="42180527" class="c"><input type="checkbox" id="c-42180527" checked=""/><div class="controls bullet"><span class="by">trsohmers</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180050">parent</a><span>|</span><a href="#42180470">next</a><span>|</span><label class="collapse" for="c-42180527">[-]</label><label class="expand" for="c-42180527">[4 more]</label></div><br/><div class="children"><div class="content">Based on their S1 filing and public statements, the average cost per WSE system for their (~90% of their total revenue) largest customer is ~$1.36M, and I’ve heard “retail” pricing of $2.5M per system. They are also 15U and due to power and additional support equipment take up an entire rack.<p>The other thing people don’t seem to be getting in this thread that just to hold the weights for 405B at FP16 requires 19 of their systems since it is SRAM only… rounding up to 20 to account for program code + KV cache for the user context would mean 20 systems&#x2F;racks, so well over $20M. The full rack (including support equipment) also consumes 23kW, so we are talking nearly half a megawatt and ~$30M for them to be getting this performance on Llama 405B</div><br/><div id="42181290" class="c"><input type="checkbox" id="c-42181290" checked=""/><div class="controls bullet"><span class="by">meowface</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180527">parent</a><span>|</span><a href="#42180544">next</a><span>|</span><label class="collapse" for="c-42181290">[-]</label><label class="expand" for="c-42181290">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for the breakdown. Bit of an emotional journey.<p>&quot;$500 in the future...? Oh, $30 million now, so that might be a while...&quot;</div><br/></div></div><div id="42180544" class="c"><input type="checkbox" id="c-42180544" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180527">parent</a><span>|</span><a href="#42181290">prev</a><span>|</span><a href="#42180470">next</a><span>|</span><label class="collapse" for="c-42180544">[-]</label><label class="expand" for="c-42180544">[2 more]</label></div><br/><div class="children"><div class="content">Thank you, far better answer than mine! Those are indeed wild numbers, although interestingly &quot;only&quot; 23kw, I&#x27;d expect the same level of compute in GPUs to be quite a lot more than that, or at least higher power density.</div><br/><div id="42180615" class="c"><input type="checkbox" id="c-42180615" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180544">parent</a><span>|</span><a href="#42180470">next</a><span>|</span><label class="collapse" for="c-42180615">[-]</label><label class="expand" for="c-42180615">[1 more]</label></div><br/><div class="children"><div class="content">You get ~400TFLOP&#x2F;s in H100 for 350W. You need (2 * token&#x2F;s * param count) FLOP&#x2F;s. For 405b, 969tok&#x2F;s you just need 784 TFLOP&#x2F;s which is just 2 H100s.<p>The limiting factor with GPU for inference is memory bandwidth. For 969 tok&#x2F;s in int8, you need 392 TB&#x2F;s memory bandwidth or 200 H100s.</div><br/></div></div></div></div></div></div><div id="42180470" class="c"><input type="checkbox" id="c-42180470" checked=""/><div class="controls bullet"><span class="by">bboygravity</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180050">parent</a><span>|</span><a href="#42180527">prev</a><span>|</span><a href="#42181229">next</a><span>|</span><label class="collapse" for="c-42180470">[-]</label><label class="expand" for="c-42180470">[5 more]</label></div><br/><div class="children"><div class="content">That means it&#x27;ll be close to affordable in 3 to 5 years if we follow the curve we&#x27;ve been on for the past decades.</div><br/><div id="42180845" class="c"><input type="checkbox" id="c-42180845" checked=""/><div class="controls bullet"><span class="by">schoen</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180470">parent</a><span>|</span><a href="#42180967">next</a><span>|</span><label class="collapse" for="c-42180845">[-]</label><label class="expand" for="c-42180845">[3 more]</label></div><br/><div class="children"><div class="content">How have power and cooling been doing with respect to chip improvements? Have power requirements per operation been coming down rapidly, as other features have improved?<p>My recollection from PC CPUs is that we&#x27;ve gotten many more operations per second, and many more operations per second per dollar, but that the power and corresponding cooling requirements for the CPUs have tended to go up as well. I don&#x27;t really know what power per operation has looked like there. (I guess it&#x27;s clearly improved, though, because it seems like the power consumption of a desktop PC has only increased by a single order of magnitude, while the computational capacity has increased by more than that.)<p>A reason that I wonder about this in this context is that people are saying that the power and cooling requirements for these devices are currently enormous (by individual or hobbyist standards, not by data center standards!). If we imagine a Moore&#x27;s Law-style improvement where the hardware itself becomes 1&#x2F;10 or 1&#x2F;100 of its current price, would we expect the overall power consumption to be similarly reduced, or to remain closer to its current levels?</div><br/><div id="42180977" class="c"><input type="checkbox" id="c-42180977" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180845">parent</a><span>|</span><a href="#42180967">next</a><span>|</span><label class="collapse" for="c-42180977">[-]</label><label class="expand" for="c-42180977">[2 more]</label></div><br/><div class="children"><div class="content">Mooers law in the consumer space seems to be pretty much asymptoting now, as indicated by Apple&#x27;s amazing Macbooks with an astounding 8GB of RAM.
Data center compute is arguable, as it tends to be catered to some niche, making it confusing (cerebras as an example vs GPU datacenters vs more standard HPC). Also Clusters and even GPUs don&#x27;t really fit in to Mooers law as originally framed.</div><br/><div id="42181319" class="c"><input type="checkbox" id="c-42181319" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180977">parent</a><span>|</span><a href="#42180967">next</a><span>|</span><label class="collapse" for="c-42181319">[-]</label><label class="expand" for="c-42181319">[1 more]</label></div><br/><div class="children"><div class="content">Apple doesn’t sell those anymore.</div><br/></div></div></div></div></div></div><div id="42180967" class="c"><input type="checkbox" id="c-42180967" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180470">parent</a><span>|</span><a href="#42180845">prev</a><span>|</span><a href="#42181229">next</a><span>|</span><label class="collapse" for="c-42180967">[-]</label><label class="expand" for="c-42180967">[1 more]</label></div><br/><div class="children"><div class="content">It will also mean 405B models will be uninteresting in 3 to 5 years if we follow the curve we&#x27;ve been on for the past decades.</div><br/></div></div></div></div><div id="42181229" class="c"><input type="checkbox" id="c-42181229" checked=""/><div class="controls bullet"><span class="by">szundi</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180050">parent</a><span>|</span><a href="#42180470">prev</a><span>|</span><a href="#42180442">next</a><span>|</span><label class="collapse" for="c-42181229">[-]</label><label class="expand" for="c-42181229">[1 more]</label></div><br/><div class="children"><div class="content">Parent wishes 70b not 405b though</div><br/></div></div><div id="42180442" class="c"><input type="checkbox" id="c-42180442" checked=""/><div class="controls bullet"><span class="by">initplus</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180050">parent</a><span>|</span><a href="#42181229">prev</a><span>|</span><a href="#42180265">next</a><span>|</span><label class="collapse" for="c-42180442">[-]</label><label class="expand" for="c-42180442">[1 more]</label></div><br/><div class="children"><div class="content">Yeah you can see the cooling requirements by looking at their product images.
<a href="https:&#x2F;&#x2F;cerebras.ai&#x2F;wp-content&#x2F;uploads&#x2F;2021&#x2F;04&#x2F;Cerebras_Product_CS1-1.png" rel="nofollow">https:&#x2F;&#x2F;cerebras.ai&#x2F;wp-content&#x2F;uploads&#x2F;2021&#x2F;04&#x2F;Cerebras_Prod...</a><p>Thing is nearly all cooling. And look at the diameter on the water cooling pipes. Airflow guides on the fans are solid steel. Apparently the chip itself measures 21.5cm^2. Insane.</div><br/></div></div></div></div><div id="42180265" class="c"><input type="checkbox" id="c-42180265" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179717">parent</a><span>|</span><a href="#42180050">prev</a><span>|</span><a href="#42179769">next</a><span>|</span><label class="collapse" for="c-42180265">[-]</label><label class="expand" for="c-42180265">[1 more]</label></div><br/><div class="children"><div class="content">You still have to pay for the memory. The Cerebras chip is fast because they use 700x more SRAM than, say, A100 GPUs. Loading the whole model in SRAM every time you compute one token is the expensive bit.</div><br/></div></div><div id="42179769" class="c"><input type="checkbox" id="c-42179769" checked=""/><div class="controls bullet"><span class="by">chessgecko</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179717">parent</a><span>|</span><a href="#42180265">prev</a><span>|</span><a href="#42179834">next</a><span>|</span><label class="collapse" for="c-42179769">[-]</label><label class="expand" for="c-42179769">[4 more]</label></div><br/><div class="children"><div class="content">One day is doing some heavy heavy lifting here, we’re currently off by ~3-4 orders of magnitude…</div><br/><div id="42179793" class="c"><input type="checkbox" id="c-42179793" checked=""/><div class="controls bullet"><span class="by">accrual</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179769">parent</a><span>|</span><a href="#42179975">next</a><span>|</span><label class="collapse" for="c-42179793">[-]</label><label class="expand" for="c-42179793">[2 more]</label></div><br/><div class="children"><div class="content">Thank you, for the reality check! :)</div><br/><div id="42179931" class="c"><input type="checkbox" id="c-42179931" checked=""/><div class="controls bullet"><span class="by">thomashop</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179793">parent</a><span>|</span><a href="#42179975">next</a><span>|</span><label class="collapse" for="c-42179931">[-]</label><label class="expand" for="c-42179931">[1 more]</label></div><br/><div class="children"><div class="content">We have moved 2 orders of magnitude in the last year. Not that unreasonable</div><br/></div></div></div></div><div id="42179975" class="c"><input type="checkbox" id="c-42179975" checked=""/><div class="controls bullet"><span class="by">grahamj</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179769">parent</a><span>|</span><a href="#42179793">prev</a><span>|</span><a href="#42179834">next</a><span>|</span><label class="collapse" for="c-42179975">[-]</label><label class="expand" for="c-42179975">[1 more]</label></div><br/><div class="children"><div class="content">So 1000-10000 days? ;)</div><br/></div></div></div></div><div id="42179834" class="c"><input type="checkbox" id="c-42179834" checked=""/><div class="controls bullet"><span class="by">killingtime74</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179717">parent</a><span>|</span><a href="#42179769">prev</a><span>|</span><a href="#42179509">next</a><span>|</span><label class="collapse" for="c-42179834">[-]</label><label class="expand" for="c-42179834">[1 more]</label></div><br/><div class="children"><div class="content">Maybe not $500, but $500,000</div><br/></div></div></div></div><div id="42179509" class="c"><input type="checkbox" id="c-42179509" checked=""/><div class="controls bullet"><span class="by">zackangelo</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179501">parent</a><span>|</span><a href="#42179717">prev</a><span>|</span><a href="#42179503">next</a><span>|</span><label class="collapse" for="c-42179509">[-]</label><label class="expand" for="c-42179509">[2 more]</label></div><br/><div class="children"><div class="content">Ah, makes a lot more sense now.</div><br/><div id="42179747" class="c"><input type="checkbox" id="c-42179747" checked=""/><div class="controls bullet"><span class="by">StrangeDoctor</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179509">parent</a><span>|</span><a href="#42179503">next</a><span>|</span><label class="collapse" for="c-42179747">[-]</label><label class="expand" for="c-42179747">[1 more]</label></div><br/><div class="children"><div class="content">also the WSE3 pulls 15kw. <a href="https:&#x2F;&#x2F;www.eetimes.com&#x2F;cerebras-third-gen-wafer-scale-chip-doubles-performance" rel="nofollow">https:&#x2F;&#x2F;www.eetimes.com&#x2F;cerebras-third-gen-wafer-scale-chip-...</a><p>but 8x h100 are ~2.6-5.2kw (I get conflicting info, I think based on pice vs smx) so anywhere between roughly even and up to 2x efficient.</div><br/></div></div></div></div></div></div><div id="42179503" class="c"><input type="checkbox" id="c-42179503" checked=""/><div class="controls bullet"><span class="by">parsimo2010</span><span>|</span><a href="#42179476">parent</a><span>|</span><a href="#42179501">prev</a><span>|</span><a href="#42180035">next</a><span>|</span><label class="collapse" for="c-42179503">[-]</label><label class="expand" for="c-42179503">[5 more]</label></div><br/><div class="children"><div class="content">They are doing it with custom silicon with several times more area than 8x H100s.  I’m sure they are doing some sort of optimization at execution&#x2F;runtime, but the primary difference is the sheer transistor count.<p><a href="https:&#x2F;&#x2F;cerebras.ai&#x2F;product-chip&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cerebras.ai&#x2F;product-chip&#x2F;</a></div><br/><div id="42179580" class="c"><input type="checkbox" id="c-42179580" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179503">parent</a><span>|</span><a href="#42180035">next</a><span>|</span><label class="collapse" for="c-42179580">[-]</label><label class="expand" for="c-42179580">[4 more]</label></div><br/><div class="children"><div class="content">To be specific, a single WSE-3 has the same die area as about 57 H100s. It&#x27;s a big chip.</div><br/><div id="42179826" class="c"><input type="checkbox" id="c-42179826" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179580">parent</a><span>|</span><a href="#42179681">next</a><span>|</span><label class="collapse" for="c-42179826">[-]</label><label class="expand" for="c-42179826">[1 more]</label></div><br/><div class="children"><div class="content">It is worth splitting out the stacked memory silicon layers on both too (if Cerebras is set up with external DRAM memory). HBM is over 10 layers now so the die area is a good bit more than the chip area, but different process nodes are involved.</div><br/></div></div><div id="42180319" class="c"><input type="checkbox" id="c-42180319" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179580">parent</a><span>|</span><a href="#42179681">prev</a><span>|</span><a href="#42180035">next</a><span>|</span><label class="collapse" for="c-42180319">[-]</label><label class="expand" for="c-42180319">[1 more]</label></div><br/><div class="children"><div class="content">Amazing!</div><br/></div></div></div></div></div></div><div id="42180035" class="c"><input type="checkbox" id="c-42180035" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42179476">parent</a><span>|</span><a href="#42179503">prev</a><span>|</span><a href="#42179754">next</a><span>|</span><label class="collapse" for="c-42180035">[-]</label><label class="expand" for="c-42180035">[9 more]</label></div><br/><div class="children"><div class="content">They have a chip the size of a dinner plate. Take a look at the pictures: <a href="https:&#x2F;&#x2F;cerebras.ai&#x2F;product-chip&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cerebras.ai&#x2F;product-chip&#x2F;</a></div><br/><div id="42180150" class="c"><input type="checkbox" id="c-42180150" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180035">parent</a><span>|</span><a href="#42180116">next</a><span>|</span><label class="collapse" for="c-42180150">[-]</label><label class="expand" for="c-42180150">[3 more]</label></div><br/><div class="children"><div class="content">21 petabytes per second. Can push the whole internet over that chip xD</div><br/><div id="42180630" class="c"><input type="checkbox" id="c-42180630" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180150">parent</a><span>|</span><a href="#42180744">next</a><span>|</span><label class="collapse" for="c-42180630">[-]</label><label class="expand" for="c-42180630">[1 more]</label></div><br/><div class="children"><div class="content">The number for that is I believe 1 terabit or 125GB&#x2F;s -- 21 petabytes is the speed from the SRAM (~registers) to the cores (~ALU) for the whole chip. It&#x27;s not especially impressive for SRAM speeds. The impressive thing is that they have an enormous amount of SRAM</div><br/></div></div><div id="42180744" class="c"><input type="checkbox" id="c-42180744" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180150">parent</a><span>|</span><a href="#42180630">prev</a><span>|</span><a href="#42180116">next</a><span>|</span><label class="collapse" for="c-42180744">[-]</label><label class="expand" for="c-42180744">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s their on chip cache bandwidth. Usually that stuff isn&#x27;t even measured in bandwidth but latency.</div><br/></div></div></div></div><div id="42180116" class="c"><input type="checkbox" id="c-42180116" checked=""/><div class="controls bullet"><span class="by">pram</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180035">parent</a><span>|</span><a href="#42180150">prev</a><span>|</span><a href="#42180490">next</a><span>|</span><label class="collapse" for="c-42180116">[-]</label><label class="expand" for="c-42180116">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to see the heatsink for this lol</div><br/><div id="42180317" class="c"><input type="checkbox" id="c-42180317" checked=""/><div class="controls bullet"><span class="by">futureshock</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180116">parent</a><span>|</span><a href="#42180490">next</a><span>|</span><label class="collapse" for="c-42180317">[-]</label><label class="expand" for="c-42180317">[1 more]</label></div><br/><div class="children"><div class="content">They call it the “engine block”!<p><a href="https:&#x2F;&#x2F;www.servethehome.com&#x2F;a-cerebras-cs-2-engine-block-bare-on-the-sc22-show-floor&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.servethehome.com&#x2F;a-cerebras-cs-2-engine-block-ba...</a></div><br/></div></div></div></div><div id="42180490" class="c"><input type="checkbox" id="c-42180490" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180035">parent</a><span>|</span><a href="#42180116">prev</a><span>|</span><a href="#42179754">next</a><span>|</span><label class="collapse" for="c-42180490">[-]</label><label class="expand" for="c-42180490">[3 more]</label></div><br/><div class="children"><div class="content">what kind of yield do they get on that size?</div><br/><div id="42180601" class="c"><input type="checkbox" id="c-42180601" checked=""/><div class="controls bullet"><span class="by">petra</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180490">parent</a><span>|</span><a href="#42180600">next</a><span>|</span><label class="collapse" for="c-42180601">[-]</label><label class="expand" for="c-42180601">[1 more]</label></div><br/><div class="children"><div class="content">Part of their technology is managing&#x2F;bypassing defects.</div><br/></div></div><div id="42180600" class="c"><input type="checkbox" id="c-42180600" checked=""/><div class="controls bullet"><span class="by">bufferoverflow</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180490">parent</a><span>|</span><a href="#42180601">prev</a><span>|</span><a href="#42179754">next</a><span>|</span><label class="collapse" for="c-42180600">[-]</label><label class="expand" for="c-42180600">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s near 100%. Discussed here:<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;f4Dly8I8lMY?t=95" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;f4Dly8I8lMY?t=95</a></div><br/></div></div></div></div></div></div><div id="42179754" class="c"><input type="checkbox" id="c-42179754" checked=""/><div class="controls bullet"><span class="by">boroboro4</span><span>|</span><a href="#42179476">parent</a><span>|</span><a href="#42180035">prev</a><span>|</span><a href="#42179493">next</a><span>|</span><label class="collapse" for="c-42179754">[-]</label><label class="expand" for="c-42179754">[1 more]</label></div><br/><div class="children"><div class="content">There are two big tricks: their chips are enormous, and they use sram as their memory, which is vastly faster than hbm ram being used by GPUs. In fact this is main reason it’s <i>so</i> fast. Groq has the speed because of the same reason.</div><br/></div></div><div id="42179493" class="c"><input type="checkbox" id="c-42179493" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#42179476">parent</a><span>|</span><a href="#42179754">prev</a><span>|</span><a href="#42179489">next</a><span>|</span><label class="collapse" for="c-42179493">[-]</label><label class="expand" for="c-42179493">[3 more]</label></div><br/><div class="children"><div class="content">Cerebras is a chip company. They are not using GPUs. Their chip uses wafer scale integration which means it&#x27;s the physical size of a whole wafer, dozens of GPUs in one.<p>They have limited memory on chip (all SRAM) and it&#x27;s not clear how much HBM bandwidth they have per wafer. It&#x27;s a completely different optimization problem than running on GPU clusters.</div><br/><div id="42180735" class="c"><input type="checkbox" id="c-42180735" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179493">parent</a><span>|</span><a href="#42179489">next</a><span>|</span><label class="collapse" for="c-42180735">[-]</label><label class="expand" for="c-42180735">[2 more]</label></div><br/><div class="children"><div class="content">they have about 125GB&#x2F;s of off-chip bandwidth</div><br/><div id="42180792" class="c"><input type="checkbox" id="c-42180792" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180735">parent</a><span>|</span><a href="#42179489">next</a><span>|</span><label class="collapse" for="c-42180792">[-]</label><label class="expand" for="c-42180792">[1 more]</label></div><br/><div class="children"><div class="content">Do they just not do HBM at all or</div><br/></div></div></div></div></div></div><div id="42179489" class="c"><input type="checkbox" id="c-42179489" checked=""/><div class="controls bullet"><span class="by">yalok</span><span>|</span><a href="#42179476">parent</a><span>|</span><a href="#42179493">prev</a><span>|</span><a href="#42180569">next</a><span>|</span><label class="collapse" for="c-42179489">[-]</label><label class="expand" for="c-42179489">[4 more]</label></div><br/><div class="children"><div class="content">how much memory do you need to run fp8 llama 3 70b - can it potentially fit 1 H100 GPU with 96GB RAM?<p>In other words, if you wanted to run 8 separate 70b models on your cluster, each of which would fit into 1 GPU, how much larger your overall token output could be than parallelizing 1 model per 8 GPUs and having things slowed down a bit due to NVLink?</div><br/><div id="42180112" class="c"><input type="checkbox" id="c-42180112" checked=""/><div class="controls bullet"><span class="by">qingcharles</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179489">parent</a><span>|</span><a href="#42179533">next</a><span>|</span><label class="collapse" for="c-42180112">[-]</label><label class="expand" for="c-42180112">[2 more]</label></div><br/><div class="children"><div class="content">It should work, I believe. And anything that doesn&#x27;t fit you can leave on your system RAM.<p>Looks like an H100 runs about $30K online for one. Are there any issues with just sticking one of these in a stock desktop PC and running llama.cpp?</div><br/><div id="42180911" class="c"><input type="checkbox" id="c-42180911" checked=""/><div class="controls bullet"><span class="by">joha4270</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42180112">parent</a><span>|</span><a href="#42179533">next</a><span>|</span><label class="collapse" for="c-42180911">[-]</label><label class="expand" for="c-42180911">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Are there any issues with just sticking one of these in a stock desktop PC and running llama.cpp?<p>Cooling might be a challenge. The H100 has a heatsink designed to make use of the case fans. So you need a fairly high airflow through a part which is itself passive.<p>On a server this isn&#x27;t too big a problem, you have fans in one end and GPU&#x27;s blocking the exit on the other end, but in a desktop you probably need to get creative with cardboard&#x2F;3d printed shrouds to force enough air through it.</div><br/></div></div></div></div><div id="42179533" class="c"><input type="checkbox" id="c-42179533" checked=""/><div class="controls bullet"><span class="by">zackangelo</span><span>|</span><a href="#42179476">root</a><span>|</span><a href="#42179489">parent</a><span>|</span><a href="#42180112">prev</a><span>|</span><a href="#42180569">next</a><span>|</span><label class="collapse" for="c-42179533">[-]</label><label class="expand" for="c-42179533">[1 more]</label></div><br/><div class="children"><div class="content">It’s been a minute so my memory might be off but I think when I ran 70b at fp16 it just barely fit on a 2x A100 80GB cluster but quickly OOMed as the context&#x2F;kv cache grew.<p>So if I had to guess a 96GB H100 could probably run it at fp8 as long as you didn’t need a big context window. If you’re doing speculative decoding it probably won’t fit because you also need weights and kv cache for the draft model.</div><br/></div></div></div></div><div id="42180569" class="c"><input type="checkbox" id="c-42180569" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#42179476">parent</a><span>|</span><a href="#42179489">prev</a><span>|</span><a href="#42179794">next</a><span>|</span><label class="collapse" for="c-42180569">[-]</label><label class="expand" for="c-42180569">[1 more]</label></div><br/><div class="children"><div class="content">Imagine if you could take Llama 3.1 405B and break it down to a tree of logical gates, optimizing out all the things like multiplies by 0 in one of the bits, etc... then load it into a massive FPGA like chip that had no von Neumann bottleneck, was just pure compute without memory access latency with a conservative 1 Ghz clock rate.<p>Such a system would be limited by the latency across the reported 126 layers worth of math involved, before it could generate the next token, which might be as much as 100 uSec. So it would be 10x faster, but you could have thousands of other independent streams pipelined through in parallel because you&#x27;d get a token per clock cycle out the end.<p>In summary, 1 Gigatoken&#x2F;second, divided into 100,000 separate users each getting 10k tokens&#x2F;second.<p>This is the future I want to build.</div><br/></div></div><div id="42179794" class="c"><input type="checkbox" id="c-42179794" checked=""/><div class="controls bullet"><span class="by">mmaunder</span><span>|</span><a href="#42179476">parent</a><span>|</span><a href="#42180569">prev</a><span>|</span><a href="#42180144">next</a><span>|</span><label class="collapse" for="c-42179794">[-]</label><label class="expand" for="c-42179794">[1 more]</label></div><br/><div class="children"><div class="content">Nah. Try vLLM and 405B FP8 on that hardware. And make sure you’re benchmarking with some concurrency for max TPS.</div><br/></div></div><div id="42180144" class="c"><input type="checkbox" id="c-42180144" checked=""/><div class="controls bullet"><span class="by">hendler</span><span>|</span><a href="#42179476">parent</a><span>|</span><a href="#42179794">prev</a><span>|</span><a href="#42181071">next</a><span>|</span><label class="collapse" for="c-42180144">[-]</label><label class="expand" for="c-42180144">[1 more]</label></div><br/><div class="children"><div class="content">Check out BaseTen for performant use of GPUs</div><br/></div></div></div></div><div id="42181071" class="c"><input type="checkbox" id="c-42181071" checked=""/><div class="controls bullet"><span class="by">shreezus</span><span>|</span><a href="#42179476">prev</a><span>|</span><a href="#42181280">next</a><span>|</span><label class="collapse" for="c-42181071">[-]</label><label class="expand" for="c-42181071">[3 more]</label></div><br/><div class="children"><div class="content">This is seriously impressive performance. I think there&#x27;s a high probability Nvidia attempts to acquire Cerebras.</div><br/><div id="42181168" class="c"><input type="checkbox" id="c-42181168" checked=""/><div class="controls bullet"><span class="by">gorkempacaci</span><span>|</span><a href="#42181071">parent</a><span>|</span><a href="#42181280">next</a><span>|</span><label class="collapse" for="c-42181168">[-]</label><label class="expand" for="c-42181168">[2 more]</label></div><br/><div class="children"><div class="content">They&#x27;re considering an IPO. I&#x27;d say an acquisition is unlikely. Even then, they&#x27;d be worth more to Facebook or MS.</div><br/><div id="42181220" class="c"><input type="checkbox" id="c-42181220" checked=""/><div class="controls bullet"><span class="by">szundi</span><span>|</span><a href="#42181071">root</a><span>|</span><a href="#42181168">parent</a><span>|</span><a href="#42181280">next</a><span>|</span><label class="collapse" for="c-42181220">[-]</label><label class="expand" for="c-42181220">[1 more]</label></div><br/><div class="children"><div class="content">No, they would make a capital infusion on paper and then make Cerebras buy more hw from that money on paper, thus showing huge revenues on Nvidia books.<p>Makes sense… or whatever</div><br/></div></div></div></div></div></div><div id="42181280" class="c"><input type="checkbox" id="c-42181280" checked=""/><div class="controls bullet"><span class="by">frogfish</span><span>|</span><a href="#42181071">prev</a><span>|</span><a href="#42179527">next</a><span>|</span><label class="collapse" for="c-42181280">[-]</label><label class="expand" for="c-42181280">[1 more]</label></div><br/><div class="children"><div class="content">Genuinely curious and willing to learn: what are the different inference approaches broadly? Is there any difference in the approach between Cerebras and simplismart.ai which claims to be the fastest?</div><br/></div></div><div id="42179527" class="c"><input type="checkbox" id="c-42179527" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#42181280">prev</a><span>|</span><a href="#42181295">next</a><span>|</span><label class="collapse" for="c-42179527">[-]</label><label class="expand" for="c-42179527">[9 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure if they&#x27;re comparing apples to apples on the latency here. There are roughly three parts to the latency: the <i>throughput</i> of the context&#x2F;prompt, the time spent queueing for hardware access, and the other standard API overheads (network, etc).<p>From what I understand, several, maybe all, of the comparison services are not based on provisioned capacity, which means that the measurements include the queue time. For LLMs this can be significant. The Cerebras number on the other hand almost certainly doesn&#x27;t have some unbounded amount of queue time included, as I expect they had guaranteed hardware access.<p>The throughput here is amazing, but to get that throughput <i>at a good latency</i> for end-users means over-provisioning, and it&#x27;s unclear what queueing will do to this. Additionally, does that latency depend on the machine being ready with the model, or does that include loading the model if necessary? If using a fine-tuned model does this change the latency?<p>I&#x27;m sure it&#x27;s a clear win for batch workloads where you can keep Cerebras machines running at 100% utilisation and get 1k tokens&#x2F;s constantly.</div><br/><div id="42179788" class="c"><input type="checkbox" id="c-42179788" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#42179527">parent</a><span>|</span><a href="#42181295">next</a><span>|</span><label class="collapse" for="c-42179788">[-]</label><label class="expand" for="c-42179788">[8 more]</label></div><br/><div class="children"><div class="content">Everyone presumes this is under ideal conditions...and it&#x27;s incredible.<p>It&#x27;s bs=1. At 1,000 t&#x2F;s. Of a 405B parameter model. Wild.</div><br/><div id="42180069" class="c"><input type="checkbox" id="c-42180069" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#42179527">root</a><span>|</span><a href="#42179788">parent</a><span>|</span><a href="#42179907">next</a><span>|</span><label class="collapse" for="c-42180069">[-]</label><label class="expand" for="c-42180069">[6 more]</label></div><br/><div class="children"><div class="content">Cerebras&#x27; benchmark is most likely under ideal conditions, but I&#x27;m not sure it&#x27;s possible to test public cloud APIs under ideal conditions as it&#x27;s shared infrastructure so you just don&#x27;t know if a request is &quot;ideal&quot;. I think you can only test these things across significant numbers of requests, and that still assumes that shared resource usage doesn&#x27;t change much.</div><br/><div id="42180185" class="c"><input type="checkbox" id="c-42180185" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#42179527">root</a><span>|</span><a href="#42180069">parent</a><span>|</span><a href="#42179907">next</a><span>|</span><label class="collapse" for="c-42180185">[-]</label><label class="expand" for="c-42180185">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not talking about that. I and many others here have spun up 8x or more H100 clusters and run this exact model. Zero other traffic. You won&#x27;t come anywhere close to this.</div><br/><div id="42180762" class="c"><input type="checkbox" id="c-42180762" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#42179527">root</a><span>|</span><a href="#42180185">parent</a><span>|</span><a href="#42180357">next</a><span>|</span><label class="collapse" for="c-42180762">[-]</label><label class="expand" for="c-42180762">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  I&#x27;m not talking about that. I and many others here have spun up 8x or more H100 clusters and run this exact model. Zero other traffic. You won&#x27;t come anywhere close to this.

</code></pre>
8x H100 can also do fine tuning right? Does Cerebras offer fine tuning support?</div><br/></div></div><div id="42180357" class="c"><input type="checkbox" id="c-42180357" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#42179527">root</a><span>|</span><a href="#42180185">parent</a><span>|</span><a href="#42180762">prev</a><span>|</span><a href="#42179907">next</a><span>|</span><label class="collapse" for="c-42180357">[-]</label><label class="expand" for="c-42180357">[3 more]</label></div><br/><div class="children"><div class="content">In that case I&#x27;m misunderstanding you. Are you saying that it&#x27;s &quot;BS&quot; that they are reaching ~1k tokens&#x2F;s? If so, you may be misunderstanding what a Cerebras machine is. Also 8xH100 is still ~half the price of a single Cerebras machine, and that&#x27;s even accounting for H100s being massively over priced. You&#x27;ve got easily twice the value in a Cerebras machine, they have nearly 1m cores on a single die.</div><br/><div id="42180469" class="c"><input type="checkbox" id="c-42180469" checked=""/><div class="controls bullet"><span class="by">sam_dam_gai</span><span>|</span><a href="#42179527">root</a><span>|</span><a href="#42180357">parent</a><span>|</span><a href="#42179907">next</a><span>|</span><label class="collapse" for="c-42180469">[-]</label><label class="expand" for="c-42180469">[2 more]</label></div><br/><div class="children"><div class="content">Ha ha. He probably means ”at a batch size of 1”, i.e. not even using some amortization tricks to get better numbers.</div><br/><div id="42180534" class="c"><input type="checkbox" id="c-42180534" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#42179527">root</a><span>|</span><a href="#42180469">parent</a><span>|</span><a href="#42179907">next</a><span>|</span><label class="collapse" for="c-42180534">[-]</label><label class="expand" for="c-42180534">[1 more]</label></div><br/><div class="children"><div class="content">Ah! That does make more sense!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42179907" class="c"><input type="checkbox" id="c-42179907" checked=""/><div class="controls bullet"><span class="by">colordrops</span><span>|</span><a href="#42179527">root</a><span>|</span><a href="#42179788">parent</a><span>|</span><a href="#42180069">prev</a><span>|</span><a href="#42181295">next</a><span>|</span><label class="collapse" for="c-42179907">[-]</label><label class="expand" for="c-42179907">[1 more]</label></div><br/><div class="children"><div class="content">Right, I&#x27;d assume most LLM benchmarks are run on dedicated hardware.</div><br/></div></div></div></div></div></div><div id="42181295" class="c"><input type="checkbox" id="c-42181295" checked=""/><div class="controls bullet"><span class="by">leobg</span><span>|</span><a href="#42179527">prev</a><span>|</span><a href="#42179539">next</a><span>|</span><label class="collapse" for="c-42181295">[-]</label><label class="expand" for="c-42181295">[1 more]</label></div><br/><div class="children"><div class="content">Cerebras features in the internal OpenAI emails that recently came out. One example:<p>Ilya Sutskever to Elon Musk, Sam Altman, (cc: Greg Brockman, Sam Teller, Shivon Zilis) - Sep 20, 2017 2:08 PM<p>&gt; In the event we decide to buy Cerebras, my strong sense is that it&#x27;ll be done through Tesla. But why do it this way if we could also do it from within OpenAI?</div><br/></div></div><div id="42179539" class="c"><input type="checkbox" id="c-42179539" checked=""/><div class="controls bullet"><span class="by">LASR</span><span>|</span><a href="#42181295">prev</a><span>|</span><a href="#42180721">next</a><span>|</span><label class="collapse" for="c-42179539">[-]</label><label class="expand" for="c-42179539">[4 more]</label></div><br/><div class="children"><div class="content">What you can do with current-gen models, along with RAG, multi-agent &amp; code interpreters, the wall is very much model latency, and not accuracy any more.<p>There are so many interactive experiences that could be made possible at this level of token throughput from 405B class models.</div><br/><div id="42179814" class="c"><input type="checkbox" id="c-42179814" checked=""/><div class="controls bullet"><span class="by">TechDebtDevin</span><span>|</span><a href="#42179539">parent</a><span>|</span><a href="#42180721">next</a><span>|</span><label class="collapse" for="c-42179814">[-]</label><label class="expand" for="c-42179814">[3 more]</label></div><br/><div class="children"><div class="content">Like what..</div><br/><div id="42180145" class="c"><input type="checkbox" id="c-42180145" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#42179539">root</a><span>|</span><a href="#42179814">parent</a><span>|</span><a href="#42180079">next</a><span>|</span><label class="collapse" for="c-42180145">[-]</label><label class="expand" for="c-42180145">[1 more]</label></div><br/><div class="children"><div class="content">You can create massive variants of OpenAI&#x27;s 01 model. The &quot;Chain of Thought&quot; tools become way more useful when you can get when you can iterate 100x faster. Right now, flagship LLMs stream responses back, and barely beat the speed a human can read, so adding CoT makes it really slow for human-in-the-loop experiences. You can really get a lot more interesting &quot;thoughts&quot; (or workflow steps, or whatever) when it can do more, without slowing down the human experience of using the tool.<p>You can also get a lot fancier with tool-usage when you can start getting an LLM to use and reply to tools at a speed closer to the speed of a normal network service.<p>I&#x27;ve never timed it, but I&#x27;m guessing current LLMs don&#x27;t handle &quot;live video&quot; type applications well. Imagine an LLM you could actually video chat with - it&#x27;d be useful for walking someone through a procedure, or advanced automation of GUI applications, etc.<p>AND the holy-grail of AI applications that would combine all of this - Robotics. Today, Cerebras chips are probably too power hungry for battery powered robotic assistants, but one could imagine a Star-Wars style robot assistant many years from now. You can have a robot that can navigate some space (home setting, or work setting) and it can see its environment and behavior, processing the video in real-time. Then, can reason about the world and its given task, by explicitly thinking through steps, and critically self-challenging the steps.</div><br/></div></div><div id="42180079" class="c"><input type="checkbox" id="c-42180079" checked=""/><div class="controls bullet"><span class="by">davidfiala</span><span>|</span><a href="#42179539">root</a><span>|</span><a href="#42179814">parent</a><span>|</span><a href="#42180145">prev</a><span>|</span><a href="#42180721">next</a><span>|</span><label class="collapse" for="c-42180079">[-]</label><label class="expand" for="c-42180079">[1 more]</label></div><br/><div class="children"><div class="content">Imagine increasing the quality and FPS of those AI-generated minecraft clones and experiencing even more high-quality, realtime AI-generated gameplay<p>(yeah, I know they are doing textual tokens. but just sayin..)<p>edit: context is <a href="https:&#x2F;&#x2F;oasisaiminecraft.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;oasisaiminecraft.com&#x2F;</a></div><br/></div></div></div></div></div></div><div id="42180721" class="c"><input type="checkbox" id="c-42180721" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#42179539">prev</a><span>|</span><a href="#42180575">next</a><span>|</span><label class="collapse" for="c-42180721">[-]</label><label class="expand" for="c-42180721">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d like to see a tokens &#x2F; second &#x2F; watt comparison.</div><br/></div></div><div id="42180575" class="c"><input type="checkbox" id="c-42180575" checked=""/><div class="controls bullet"><span class="by">owenpalmer</span><span>|</span><a href="#42180721">prev</a><span>|</span><a href="#42179871">next</a><span>|</span><label class="collapse" for="c-42180575">[-]</label><label class="expand" for="c-42180575">[4 more]</label></div><br/><div class="children"><div class="content">The fact that such a boost is possible with new hardware, I wonder what the ceiling is for improving performance for training via hardware as well.</div><br/><div id="42180710" class="c"><input type="checkbox" id="c-42180710" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#42180575">parent</a><span>|</span><a href="#42180618">next</a><span>|</span><label class="collapse" for="c-42180710">[-]</label><label class="expand" for="c-42180710">[1 more]</label></div><br/><div class="children"><div class="content">Not enormous without significant changes to the ML. There are two pieces to this: improving efficiency and improving flops.<p>Improving flops is the most obvious way to improve speed, but I think we&#x27;re pretty close to physical limits for a given process node and datatype precision. It&#x27;s hard to give proof positive of this, but there are a few lines of evidence. One is that the fundamental operation of LLMs, matrix multiplications, are really simple (unlike e.g. CPU work) and so all the e.g. control flow logic is pretty minimized. We&#x27;re largely spending electricity on doing the matrix multiplications themselves, and the matrix multiplications are in fact electricity-bound[1]. There are gains to be made by changing precision, but this is difficult and we&#x27;re close to tapped out on it in my opinion (already very low precisions (fp8 can&#x27;t represent 17), new research showing limitations).<p>Efficiency in LLM training is measured with a very punishing standard, &quot;Model Flops Utilization&quot; (MFU), where we divide the theoretical number of flops the hardware could provide with the theoretical number of flops necessary to implement the mathematical operation. We&#x27;re able to get 30% without thinking (just FSDP) and 50-60% are not implausible&#x2F;unheard of. The inefficiency is largely because 1) the hardware can&#x27;t provide the number of flops it says on the tin for various reasons and 2) we have to synchronize terabytes of data across tens of thousands of machines. The theoretical limit here is 2x but in practice there&#x27;s not a ton to eke out here.<p>There will be gains but they will be mostly focused on reducing NVIDIA&#x27;s margin (TPU), on improving process node, on reducing datatype (B100), or on enlarging the size of a chip to reduce costly cross-chip communication (B100). There&#x27;s not room for a 10x (again at constant precision and process node).<p>[1]: <a href="https:&#x2F;&#x2F;www.thonking.ai&#x2F;p&#x2F;strangely-matrix-multiplications" rel="nofollow">https:&#x2F;&#x2F;www.thonking.ai&#x2F;p&#x2F;strangely-matrix-multiplications</a></div><br/></div></div><div id="42180618" class="c"><input type="checkbox" id="c-42180618" checked=""/><div class="controls bullet"><span class="by">bufferoverflow</span><span>|</span><a href="#42180575">parent</a><span>|</span><a href="#42180710">prev</a><span>|</span><a href="#42179871">next</a><span>|</span><label class="collapse" for="c-42180618">[-]</label><label class="expand" for="c-42180618">[2 more]</label></div><br/><div class="children"><div class="content">The ultimate solution would be to convert an LLM to a pure ASIC.<p>My guess is that would 10X the performance. But then it&#x27;s a very very expensive solution.</div><br/><div id="42180716" class="c"><input type="checkbox" id="c-42180716" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#42180575">root</a><span>|</span><a href="#42180618">parent</a><span>|</span><a href="#42179871">next</a><span>|</span><label class="collapse" for="c-42180716">[-]</label><label class="expand" for="c-42180716">[1 more]</label></div><br/><div class="children"><div class="content">Why would converting a specific LLM to an ASIC help you? LLMs are like 99% matrix multiplications by work and we already have things that amount to ASICs for matrix multiplications (e.g. TPU) that aren&#x27;t cheaper than e.g. H100</div><br/></div></div></div></div></div></div><div id="42179871" class="c"><input type="checkbox" id="c-42179871" checked=""/><div class="controls bullet"><span class="by">fillskills</span><span>|</span><a href="#42180575">prev</a><span>|</span><a href="#42179482">next</a><span>|</span><label class="collapse" for="c-42179871">[-]</label><label class="expand" for="c-42179871">[5 more]</label></div><br/><div class="children"><div class="content">No mention of their direct competitor Groq?</div><br/><div id="42179927" class="c"><input type="checkbox" id="c-42179927" checked=""/><div class="controls bullet"><span class="by">icelancer</span><span>|</span><a href="#42179871">parent</a><span>|</span><a href="#42180988">next</a><span>|</span><label class="collapse" for="c-42179927">[-]</label><label class="expand" for="c-42179927">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a happily-paying customer of Groq but they aren&#x27;t competitive against Cerebras in the 405b space (literally at all).<p>Groq has paying customers below the enterprise-level and actually serves all their models to everyone in a wide berth, unlike Cerebras who is very selective, so they have that going for them. But in terms of sheer speed and in the largest models, Groq doesn&#x27;t really compare.</div><br/><div id="42180137" class="c"><input type="checkbox" id="c-42180137" checked=""/><div class="controls bullet"><span class="by">hendler</span><span>|</span><a href="#42179871">root</a><span>|</span><a href="#42179927">parent</a><span>|</span><a href="#42180988">next</a><span>|</span><label class="collapse" for="c-42180137">[-]</label><label class="expand" for="c-42180137">[2 more]</label></div><br/><div class="children"><div class="content">Is this because 405b doesn&#x27;t fit on Groq? If they perform better, I would also have liked to have seen.</div><br/><div id="42180341" class="c"><input type="checkbox" id="c-42180341" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#42179871">root</a><span>|</span><a href="#42180137">parent</a><span>|</span><a href="#42180988">next</a><span>|</span><label class="collapse" for="c-42180341">[-]</label><label class="expand" for="c-42180341">[1 more]</label></div><br/><div class="children"><div class="content">When 405b first launched Groq ran it, it&#x27;s not currently running due to capacity issues though</div><br/></div></div></div></div></div></div><div id="42180988" class="c"><input type="checkbox" id="c-42180988" checked=""/><div class="controls bullet"><span class="by">guyomes</span><span>|</span><a href="#42179871">parent</a><span>|</span><a href="#42179927">prev</a><span>|</span><a href="#42179482">next</a><span>|</span><label class="collapse" for="c-42180988">[-]</label><label class="expand" for="c-42180988">[1 more]</label></div><br/><div class="children"><div class="content">Sambanova is not often mentioned either [0]. One of his co-founder is known as “father of the multi-core processor” [1].<p>[0]: <a href="https:&#x2F;&#x2F;sambanova.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;sambanova.ai&#x2F;</a><p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kunle_Olukotun" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kunle_Olukotun</a></div><br/></div></div></div></div><div id="42179482" class="c"><input type="checkbox" id="c-42179482" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#42179871">prev</a><span>|</span><a href="#42179943">next</a><span>|</span><label class="collapse" for="c-42179482">[-]</label><label class="expand" for="c-42179482">[6 more]</label></div><br/><div class="children"><div class="content">I am wondering how much cost is needed for serving at such a latency. Of course for customers, static cost depends on the pricing strategy. But still, the cost really determines how widely this can be adopted. Is it only for those business that <i>really</i> need the latency, or this can be generally deployed.</div><br/><div id="42179803" class="c"><input type="checkbox" id="c-42179803" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#42179482">parent</a><span>|</span><a href="#42179566">next</a><span>|</span><label class="collapse" for="c-42179803">[-]</label><label class="expand" for="c-42179803">[4 more]</label></div><br/><div class="children"><div class="content">Maybe it could become standard for everyone to make giant chips and use SRAM?<p>How many SRAM manufacturers are there? Or does it somehow need to be fully integrated into the chip?</div><br/><div id="42179856" class="c"><input type="checkbox" id="c-42179856" checked=""/><div class="controls bullet"><span class="by">AlotOfReading</span><span>|</span><a href="#42179482">root</a><span>|</span><a href="#42179803">parent</a><span>|</span><a href="#42180718">next</a><span>|</span><label class="collapse" for="c-42179856">[-]</label><label class="expand" for="c-42179856">[2 more]</label></div><br/><div class="children"><div class="content">SRAM is usually produced on the same wafer as the rest of the logic. SRAM on an external chip would lose many of the advantages without being significantly cheaper.</div><br/><div id="42180689" class="c"><input type="checkbox" id="c-42180689" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#42179482">root</a><span>|</span><a href="#42179856">parent</a><span>|</span><a href="#42180718">next</a><span>|</span><label class="collapse" for="c-42180689">[-]</label><label class="expand" for="c-42180689">[1 more]</label></div><br/><div class="children"><div class="content">Yes, the limiting factor for bandwidth is generally the number of pins which are not cheap and you can only have few 1000s in a chip. The absolute state of the art is 36 Gb&#x2F;s&#x2F;pin[1], and your $30 RAM could have 6 Gb&#x2F;s&#x2F;pin[2].<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GDDR7_SDRAM" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GDDR7_SDRAM</a><p>[2]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;DDR5_SDRAM" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;DDR5_SDRAM</a></div><br/></div></div></div></div><div id="42180718" class="c"><input type="checkbox" id="c-42180718" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#42179482">root</a><span>|</span><a href="#42179803">parent</a><span>|</span><a href="#42179856">prev</a><span>|</span><a href="#42179566">next</a><span>|</span><label class="collapse" for="c-42180718">[-]</label><label class="expand" for="c-42180718">[1 more]</label></div><br/><div class="children"><div class="content">the cost is not the memory technology per se but primarily the wires. SRAM is fast because it&#x27;s directly inside the chip and so the connections with the logic that does the work is cheap because it&#x27;s close.</div><br/></div></div></div></div></div></div><div id="42179943" class="c"><input type="checkbox" id="c-42179943" checked=""/><div class="controls bullet"><span class="by">bargle0</span><span>|</span><a href="#42179482">prev</a><span>|</span><a href="#42181139">next</a><span>|</span><label class="collapse" for="c-42179943">[-]</label><label class="expand" for="c-42179943">[3 more]</label></div><br/><div class="children"><div class="content">Their hardware is cool and bizarre. It has to be seen in person to be believed. It reminds me of the old days when supercomputers were weird.</div><br/><div id="42179953" class="c"><input type="checkbox" id="c-42179953" checked=""/><div class="controls bullet"><span class="by">IAmNotACellist</span><span>|</span><a href="#42179943">parent</a><span>|</span><a href="#42181139">next</a><span>|</span><label class="collapse" for="c-42179953">[-]</label><label class="expand" for="c-42179953">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t leave us hanging, show us a weird computer!</div><br/><div id="42181092" class="c"><input type="checkbox" id="c-42181092" checked=""/><div class="controls bullet"><span class="by">campers</span><span>|</span><a href="#42179943">root</a><span>|</span><a href="#42179953">parent</a><span>|</span><a href="#42181139">next</a><span>|</span><label class="collapse" for="c-42181092">[-]</label><label class="expand" for="c-42181092">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230812020202&#x2F;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pzyZpauU3Ig" rel="nofollow">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230812020202&#x2F;https:&#x2F;&#x2F;www.youtu...</a></div><br/></div></div></div></div></div></div><div id="42181139" class="c"><input type="checkbox" id="c-42181139" checked=""/><div class="controls bullet"><span class="by">gorkempacaci</span><span>|</span><a href="#42179943">prev</a><span>|</span><a href="#42180200">next</a><span>|</span><label class="collapse" for="c-42181139">[-]</label><label class="expand" for="c-42181139">[2 more]</label></div><br/><div class="children"><div class="content">nvidia hates this one little trick</div><br/><div id="42181177" class="c"><input type="checkbox" id="c-42181177" checked=""/><div class="controls bullet"><span class="by">zurfer</span><span>|</span><a href="#42181139">parent</a><span>|</span><a href="#42180200">next</a><span>|</span><label class="collapse" for="c-42181177">[-]</label><label class="expand" for="c-42181177">[1 more]</label></div><br/><div class="children"><div class="content">I laughed and upvoted, but if anything I bet they put their best people on it to replicate this offering.<p>What I take away from this is: we are just getting started. I remember in 2023 begging OpenAI to give us more than 7 tokens&#x2F;second on GPT-4.</div><br/></div></div></div></div><div id="42180200" class="c"><input type="checkbox" id="c-42180200" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#42181139">prev</a><span>|</span><a href="#42179727">next</a><span>|</span><label class="collapse" for="c-42180200">[-]</label><label class="expand" for="c-42180200">[2 more]</label></div><br/><div class="children"><div class="content">Normally, I don&#x27;t think 1000 tokens&#x2F;s is that much more useful than 50 tokens&#x2F;s.<p>However, given that CoT makes models a lot smarter, I think Cerebras chips will be in huge demand from now on. You can have a lot more CoT runs when the inference is 20x faster.<p>Also, I assume financial applications such as hedge funds would be buying these things in bulk now.</div><br/><div id="42180756" class="c"><input type="checkbox" id="c-42180756" checked=""/><div class="controls bullet"><span class="by">deadmutex</span><span>|</span><a href="#42180200">parent</a><span>|</span><a href="#42179727">next</a><span>|</span><label class="collapse" for="c-42180756">[-]</label><label class="expand" for="c-42180756">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Also, I assume financial applications such as hedge funds would be buying these things in bulk now.<p>Please elaborate.. why?</div><br/></div></div></div></div><div id="42179727" class="c"><input type="checkbox" id="c-42179727" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#42180200">prev</a><span>|</span><a href="#42179843">next</a><span>|</span><label class="collapse" for="c-42179727">[-]</label><label class="expand" for="c-42179727">[6 more]</label></div><br/><div class="children"><div class="content">So out of all AI chip startups, Cerebras is probably the real deal</div><br/><div id="42179935" class="c"><input type="checkbox" id="c-42179935" checked=""/><div class="controls bullet"><span class="by">icelancer</span><span>|</span><a href="#42179727">parent</a><span>|</span><a href="#42179835">next</a><span>|</span><label class="collapse" for="c-42179935">[-]</label><label class="expand" for="c-42179935">[3 more]</label></div><br/><div class="children"><div class="content">Groq is legitimate. Cerebras so far doesn&#x27;t scale (wide) nearly as good as Groq. We&#x27;ll see how it goes.</div><br/><div id="42180942" class="c"><input type="checkbox" id="c-42180942" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#42179727">root</a><span>|</span><a href="#42179935">parent</a><span>|</span><a href="#42180141">next</a><span>|</span><label class="collapse" for="c-42180942">[-]</label><label class="expand" for="c-42180942">[1 more]</label></div><br/><div class="children"><div class="content">How exactly does groq scale wide well?  Last I heard it was 9 racks!! to run llama-2 70b<p>Which is why they throttle your requests</div><br/></div></div><div id="42180141" class="c"><input type="checkbox" id="c-42180141" checked=""/><div class="controls bullet"><span class="by">hendler</span><span>|</span><a href="#42179727">root</a><span>|</span><a href="#42179935">parent</a><span>|</span><a href="#42180942">prev</a><span>|</span><a href="#42179835">next</a><span>|</span><label class="collapse" for="c-42180141">[-]</label><label class="expand" for="c-42180141">[1 more]</label></div><br/><div class="children"><div class="content">Google TPUs, Amazon, a YC funded ASIC&#x2F;FPGA company, a Chinese Co. all have custom hardware too that might scale well.</div><br/></div></div></div></div><div id="42179835" class="c"><input type="checkbox" id="c-42179835" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#42179727">parent</a><span>|</span><a href="#42179935">prev</a><span>|</span><a href="#42179843">next</a><span>|</span><label class="collapse" for="c-42179835">[-]</label><label class="expand" for="c-42179835">[2 more]</label></div><br/><div class="children"><div class="content">just in time for their ipo</div><br/><div id="42179878" class="c"><input type="checkbox" id="c-42179878" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#42179727">root</a><span>|</span><a href="#42179835">parent</a><span>|</span><a href="#42179843">next</a><span>|</span><label class="collapse" for="c-42179878">[-]</label><label class="expand" for="c-42179878">[1 more]</label></div><br/><div class="children"><div class="content">It got cancelled&#x2F;postponed.</div><br/></div></div></div></div></div></div><div id="42179843" class="c"><input type="checkbox" id="c-42179843" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#42179727">prev</a><span>|</span><a href="#42180121">next</a><span>|</span><label class="collapse" for="c-42179843">[-]</label><label class="expand" for="c-42179843">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m so curious to see some multi-agent systems running with inference this fast.</div><br/><div id="42179877" class="c"><input type="checkbox" id="c-42179877" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#42179843">parent</a><span>|</span><a href="#42180121">next</a><span>|</span><label class="collapse" for="c-42179877">[-]</label><label class="expand" for="c-42179877">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no good open source agent models at the moment unfortunately.</div><br/></div></div></div></div><div id="42180121" class="c"><input type="checkbox" id="c-42180121" checked=""/><div class="controls bullet"><span class="by">easeout</span><span>|</span><a href="#42179843">prev</a><span>|</span><a href="#42180550">next</a><span>|</span><label class="collapse" for="c-42180121">[-]</label><label class="expand" for="c-42180121">[4 more]</label></div><br/><div class="children"><div class="content">How does binning work when your chip is the entire wafer?</div><br/><div id="42180161" class="c"><input type="checkbox" id="c-42180161" checked=""/><div class="controls bullet"><span class="by">shrubble</span><span>|</span><a href="#42180121">parent</a><span>|</span><a href="#42180550">next</a><span>|</span><label class="collapse" for="c-42180161">[-]</label><label class="expand" for="c-42180161">[3 more]</label></div><br/><div class="children"><div class="content">They expect that some of the cores on the wafer will fail, so they have redundant links all throughout the chip, so they can seal off&#x2F;turn off any cores that fail and still have enough cores to do useful work.</div><br/><div id="42180731" class="c"><input type="checkbox" id="c-42180731" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#42180121">root</a><span>|</span><a href="#42180161">parent</a><span>|</span><a href="#42180550">next</a><span>|</span><label class="collapse" for="c-42180731">[-]</label><label class="expand" for="c-42180731">[2 more]</label></div><br/><div class="children"><div class="content">My understanding is that they mask off or otherwise disable a whole row+column of cores when one dies</div><br/><div id="42180846" class="c"><input type="checkbox" id="c-42180846" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#42180121">root</a><span>|</span><a href="#42180731">parent</a><span>|</span><a href="#42180550">next</a><span>|</span><label class="collapse" for="c-42180846">[-]</label><label class="expand" for="c-42180846">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s way too wasteful.<p>Take a look at <a href="https:&#x2F;&#x2F;fuse.wikichip.org&#x2F;news&#x2F;3010&#x2F;a-look-at-cerebras-wafer-scale-engine-half-square-foot-silicon-chip&#x2F;2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;fuse.wikichip.org&#x2F;news&#x2F;3010&#x2F;a-look-at-cerebras-wafer...</a> and specifically the diagram <a href="https:&#x2F;&#x2F;fuse.wikichip.org&#x2F;wp-content&#x2F;uploads&#x2F;2019&#x2F;11&#x2F;hc31-cerebras-mem-yield.png" rel="nofollow">https:&#x2F;&#x2F;fuse.wikichip.org&#x2F;wp-content&#x2F;uploads&#x2F;2019&#x2F;11&#x2F;hc31-ce...</a><p>The fabric can effectively route signals diagonally to work around an individual defective core, with a displacement of one position for cores in the same row from that defect over to the nearest spare core. That&#x27;s how they get away with a claimed &quot;1–1.5%&quot; of spare cores.</div><br/></div></div></div></div></div></div></div></div><div id="42180550" class="c"><input type="checkbox" id="c-42180550" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#42180121">prev</a><span>|</span><a href="#42179744">next</a><span>|</span><label class="collapse" for="c-42180550">[-]</label><label class="expand" for="c-42180550">[1 more]</label></div><br/><div class="children"><div class="content">Damn that&#x27;s a big model and that&#x27;s really fast inference.</div><br/></div></div><div id="42179744" class="c"><input type="checkbox" id="c-42179744" checked=""/><div class="controls bullet"><span class="by">germanjoey</span><span>|</span><a href="#42180550">prev</a><span>|</span><a href="#42179444">next</a><span>|</span><label class="collapse" for="c-42179744">[-]</label><label class="expand" for="c-42179744">[1 more]</label></div><br/><div class="children"><div class="content">Pretty amazing speed, especially considering this is bf16. But how many racks is this using? The used 4 racks for 70B, so this, what, at least 24? A whole data center for one model?!</div><br/></div></div><div id="42179444" class="c"><input type="checkbox" id="c-42179444" checked=""/><div class="controls bullet"><span class="by">jadbox</span><span>|</span><a href="#42179744">prev</a><span>|</span><a href="#42180257">next</a><span>|</span><label class="collapse" for="c-42179444">[-]</label><label class="expand" for="c-42179444">[1 more]</label></div><br/><div class="children"><div class="content">Not open beta until Q1 2025</div><br/></div></div><div id="42180257" class="c"><input type="checkbox" id="c-42180257" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42179444">prev</a><span>|</span><a href="#42179970">next</a><span>|</span><label class="collapse" for="c-42180257">[-]</label><label class="expand" for="c-42180257">[1 more]</label></div><br/><div class="children"><div class="content">Holy bananas, the title alone is almost its own language.</div><br/></div></div><div id="42179970" class="c"><input type="checkbox" id="c-42179970" checked=""/><div class="controls bullet"><span class="by">kuprel</span><span>|</span><a href="#42180257">prev</a><span>|</span><a href="#42180666">next</a><span>|</span><label class="collapse" for="c-42179970">[-]</label><label class="expand" for="c-42179970">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if Cerebras could generate video decent quality in real time</div><br/></div></div><div id="42180666" class="c"><input type="checkbox" id="c-42180666" checked=""/><div class="controls bullet"><span class="by">xwww</span><span>|</span><a href="#42179970">prev</a><span>|</span><a href="#42180484">next</a><span>|</span><label class="collapse" for="c-42180666">[-]</label><label class="expand" for="c-42180666">[1 more]</label></div><br/><div class="children"><div class="content">Transistor（GPU）-&gt; Integrated Circuit (WSE-3)</div><br/></div></div></div></div></div></div></div></body></html>