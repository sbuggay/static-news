<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1696410064615" as="style"/><link rel="stylesheet" href="styles.css?v=1696410064615"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://resobscura.substack.com/p/translating-latin-demonology-manuals">Translating Latin demonology manuals with GPT-4 and Claude</a> <span class="domain">(<a href="https://resobscura.substack.com">resobscura.substack.com</a>)</span></div><div class="subtext"><span>benbreen</span> | <span>70 comments</span></div><br/><div><div id="37762007" class="c"><input type="checkbox" id="c-37762007" checked=""/><div class="controls bullet"><span class="by">klntsky</span><span>|</span><a href="#37762547">next</a><span>|</span><label class="collapse" for="c-37762007">[-]</label><label class="expand" for="c-37762007">[3 more]</label></div><br/><div class="children"><div class="content">Everyone is excited about LLM abilities to help with language learning, while completely ignoring the fact that for most people LLMs will make the learning unneeded. There will be less experts in the field, and therefore we will lose the part of language and foreign literature understanding not captured by statmodels. Which is a huge part (subtle contexts in poetry, etc)</div><br/><div id="37762203" class="c"><input type="checkbox" id="c-37762203" checked=""/><div class="controls bullet"><span class="by">Drakim</span><span>|</span><a href="#37762007">parent</a><span>|</span><a href="#37762547">next</a><span>|</span><label class="collapse" for="c-37762203">[-]</label><label class="expand" for="c-37762203">[2 more]</label></div><br/><div class="children"><div class="content">You aren&#x27;t wrong, but this has been a dilemma with every new technology. The camera had that effect, modern metalworking had that effect, even tractors had that effect.<p>It&#x27;s definitely a problem we should be talking about, but we can&#x27;t go back in time or remain frozen, the genie never goes back in the bottle. We have to move forward towards the future while salvaging the parts of the past we want to bring with us.</div><br/><div id="37762559" class="c"><input type="checkbox" id="c-37762559" checked=""/><div class="controls bullet"><span class="by">HPsquared</span><span>|</span><a href="#37762007">root</a><span>|</span><a href="#37762203">parent</a><span>|</span><a href="#37762547">next</a><span>|</span><label class="collapse" for="c-37762559">[-]</label><label class="expand" for="c-37762559">[1 more]</label></div><br/><div class="children"><div class="content">This line of thought goes back to Socrates and his supposed views on writing. (That it weakens the memory)</div><br/></div></div></div></div></div></div><div id="37762547" class="c"><input type="checkbox" id="c-37762547" checked=""/><div class="controls bullet"><span class="by">lofaszvanitt</span><span>|</span><a href="#37762007">prev</a><span>|</span><a href="#37759440">next</a><span>|</span><label class="collapse" for="c-37762547">[-]</label><label class="expand" for="c-37762547">[1 more]</label></div><br/><div class="children"><div class="content">I always wondered why these texts are so difficult to interpret... ... ... why certain symbols, like a crow have ambivalent meanings. In some cultures the crow is evil, while in others, it&#x27;s benevolent.<p>GPT4 to the rescue, let&#x27;s see what&#x27;ll happen if everyone has the means to summon demons, curse others and the like.</div><br/></div></div><div id="37759440" class="c"><input type="checkbox" id="c-37759440" checked=""/><div class="controls bullet"><span class="by">anigbrowl</span><span>|</span><a href="#37762547">prev</a><span>|</span><a href="#37761673">next</a><span>|</span><label class="collapse" for="c-37759440">[-]</label><label class="expand" for="c-37759440">[1 more]</label></div><br/><div class="children"><div class="content">This Dan Brown&#x2F;William Gibson crossover sucks my soul right out of the petabyte SSD I bought in a dark alley.</div><br/></div></div><div id="37761673" class="c"><input type="checkbox" id="c-37761673" checked=""/><div class="controls bullet"><span class="by">iamnotsure</span><span>|</span><a href="#37759440">prev</a><span>|</span><a href="#37759485">next</a><span>|</span><label class="collapse" for="c-37761673">[-]</label><label class="expand" for="c-37761673">[1 more]</label></div><br/><div class="children"><div class="content">Latin demonology manuals were about GPT-T and Calude in the first place.</div><br/></div></div><div id="37759485" class="c"><input type="checkbox" id="c-37759485" checked=""/><div class="controls bullet"><span class="by">coyotespike</span><span>|</span><a href="#37761673">prev</a><span>|</span><a href="#37761853">next</a><span>|</span><label class="collapse" for="c-37759485">[-]</label><label class="expand" for="c-37759485">[14 more]</label></div><br/><div class="children"><div class="content">This is really fun. Google Translate is incompetent at Latin, and I&#x27;m informed that so far ChatGPT still makes errors of grammar and word choice when generating Latin.<p>This experiment helps show we can use GPT-4&#x2F;Claude to parse and summarize Latin, but doesn&#x27;t yet show that we can rely on them to the level of a human expert.<p>I&#x27;m confident we&#x27;ll get there pretty soon - and then will be able to rely on LLMs to generate Comprehensible Input and thereby greatly accelerate language learning.</div><br/><div id="37759575" class="c"><input type="checkbox" id="c-37759575" checked=""/><div class="controls bullet"><span class="by">huytersd</span><span>|</span><a href="#37759485">parent</a><span>|</span><a href="#37759544">next</a><span>|</span><label class="collapse" for="c-37759575">[-]</label><label class="expand" for="c-37759575">[4 more]</label></div><br/><div class="children"><div class="content">You can include in the prompt a requirement to highlight sections the LLM was not sure about&#x2F;needs to be verified.</div><br/><div id="37760395" class="c"><input type="checkbox" id="c-37760395" checked=""/><div class="controls bullet"><span class="by">topherclay</span><span>|</span><a href="#37759485">root</a><span>|</span><a href="#37759575">parent</a><span>|</span><a href="#37759544">next</a><span>|</span><label class="collapse" for="c-37760395">[-]</label><label class="expand" for="c-37760395">[3 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t that work just as well as including in the prompt a requirement  for it to not make any mistakes?</div><br/><div id="37760425" class="c"><input type="checkbox" id="c-37760425" checked=""/><div class="controls bullet"><span class="by">schoen</span><span>|</span><a href="#37759485">root</a><span>|</span><a href="#37760395">parent</a><span>|</span><a href="#37761142">next</a><span>|</span><label class="collapse" for="c-37760425">[-]</label><label class="expand" for="c-37760425">[1 more]</label></div><br/><div class="children"><div class="content">With some LLMs, emphasizing the possibility and appropriateness of saying &quot;I don&#x27;t know&quot; has reduced the frequency of hallucinations.</div><br/></div></div><div id="37761142" class="c"><input type="checkbox" id="c-37761142" checked=""/><div class="controls bullet"><span class="by">apstls</span><span>|</span><a href="#37759485">root</a><span>|</span><a href="#37760395">parent</a><span>|</span><a href="#37760425">prev</a><span>|</span><a href="#37759544">next</a><span>|</span><label class="collapse" for="c-37761142">[-]</label><label class="expand" for="c-37761142">[1 more]</label></div><br/><div class="children"><div class="content">If you assume the LLM can gauge its “confidence” in the last n tokens it generated, which seems within the realm of reason (from a layman’s perspective), then I would think this idea would work better the significant majority of the time. It’s providing an additional dimension of context related to the output (which we’re assuming is sound, or at least not entirely nonsensical), which alone seems like enough of a justification to do this. It’s unclear (to me, at least) exactly what effect adding a “no mistake” requirement to the prompt would have on the LLM’s output; I could see it skipping ranges of tokens that it’s unsure about, which seems less preferable to having it provide a best guess and make clear that it’s only a guess, but I could also certainly see it operating as it otherwise would have without the “no mistake” instruction, giving the same dubious output to a user that may now have an unwarranted increase in confidence in the LLM’s output.<p>I’ve spent a decent amount of free time doing what feels like coercing, tricking, or otherwise manipulating GPT-4 and Llama2 into doing my bidding - with my bidding being mostly toy ideas for little tools to make random small tasks easier and one or two more interesting ideas that are fun to mess around with, but would probably require some medical-grade antianxiety meds to even consider using in a real production setting (ie a universal ORM.) Even though I’m not developing (or I guess we now call it prompt engineering) in a rigorous or serious way, I’ve found that making the LLM _actively_ reconsider and validate its output works very well, with the effectiveness seeming to be a rough  function of “how actively” you trick it into doing so. Giving a list of “be sure to consider these things” at the end of your prompt often works, but also very often doesn’t; adding another step to the process you’re asking them to perform comprised of subtasks that map to the list of gotchas, but reframed as actions you are requiring them to perform, is often the remedy for cases where the simple suggestion list isn’t enough, and is basically a more active variant of the same idea as providing the gotcha list. Dialing it up a bit more, requiring them to provide an update after they complete each subtask to confirm they indeed performed it and to provide a summary of what they found makes their retrospective assessment even more actively involved, and has been a pretty damn reliable trick for ironing out kinks and known failure modes in my prompts.<p>All that being said, I think the simple fact that you’re now actively requiring them to reflect on their confidence in their output, and therefore the correctness of their output, may lead to this idea improving the quality of output&#x2F;results as an unintended side effect that would alone make it worth doing.</div><br/></div></div></div></div></div></div><div id="37759544" class="c"><input type="checkbox" id="c-37759544" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#37759485">parent</a><span>|</span><a href="#37759575">prev</a><span>|</span><a href="#37760303">next</a><span>|</span><label class="collapse" for="c-37759544">[-]</label><label class="expand" for="c-37759544">[8 more]</label></div><br/><div class="children"><div class="content">I think that mostly depends on getting more high quality Latin into the training set, but I&#x27;m guessing the new amount of that being generated&#x2F;discovered is relatively small. Then again, new techniques for training models could prove me wrong.</div><br/><div id="37759611" class="c"><input type="checkbox" id="c-37759611" checked=""/><div class="controls bullet"><span class="by">benbreen</span><span>|</span><a href="#37759485">root</a><span>|</span><a href="#37759544">parent</a><span>|</span><a href="#37759623">next</a><span>|</span><label class="collapse" for="c-37759611">[-]</label><label class="expand" for="c-37759611">[4 more]</label></div><br/><div class="children"><div class="content">This is the result when I asked GPT-4 to write &quot;a new, original passage in latin which is inspired by Del Rio&#x27;s style. Make the Latin elegant and lucid, with witty turns of phrase,&quot; then asked Claude to translate it back to English. I am no Latin expert, but I know enough to say that this is clearly lucid, makes sense -- though overwrought -- and has no glaring errors. I will ask my classicist friends to weigh in on the matter of style though.<p>GPT-4 improvising: 
&quot;Quemadmodum divini philosophi docent, mysteria naturae introrsus latitant, neque enim sine causa Democritus &#x27;naturam rerum&#x27; dixit esse clausam in occulto. Verum, si Angelorum bonorum opera ut in Psalmis celebratur, &#x27;Benedicite, omnes Angeli Domini, Domino,&#x27; quo pacto maleficae entitates—daemones scilicet—invisibiles nec tamen inactiones sunt? Mihi consentaneum videtur quod Tertullianus et Ambrosius prodiderunt, angeli esse nuntios divinos, sed daemonas esse nuntios corruptionis. Ne confundamus! Nam verbum &#x27;daemon&#x27; Graecis non semper malum significat, sed Romanis—o mores!—semper sinistram portendit.<p>In huius argumenti speculo, miror qua de causa daemones semper in tenebris vel in locis desertis, vel in nocturna silentia se abscondant. An non, ut Seneca loquitur, &#x27;magna vis veritatis est, quae se per se defendet&#x27;? Atqui daemones a veritate abhorrent, quasi nox a lucem fugiat.<p>Ergo, si volumus a daemonum insidiis nosmetipsos protegere, debemus, ut Virgilius canit, &#x27;forsan et haec olim meminisse juvabit,&#x27; non oblivisci quod lux aeterna sit telum acerrimum contra tenebras. Haec ita dicam, et iam concludo: sicut angeli ad caelum pertinent, sic daemones ad abyssum—et utrique suum locum noverunt. Caveamus igitur ne, dum caelum quaerimus, abyssum inveniamus.&quot;<p>Claude translation: &quot;As the divine philosophers teach, the mysteries of nature lie hidden within, for not without reason did Democritus call nature &#x27;enclosed in the dark&#x27;. Yet, if by the work of good Angels as celebrated in the Psalms, &#x27;Bless the Lord, all you Angels of the Lord,&#x27; in what manner are evil entities—namely demons—invisible yet not inactive? It seems fitting to me what Tertullian and Ambrose handed down, that angels are divine messengers, but demons are messengers of corruption. Let us not confuse the two! For the word &#x27;daemon&#x27; does not always signify something evil for the Greeks, but for the Romans—oh the times!—it always portends something sinister.
In the mirror of this argument, I wonder for what reason demons always hide themselves in shadows or deserted places or the silences of the night. Does not, as Seneca says, &#x27;truth have great power, which will defend itself&#x27;? Yet demons shrink from the truth, as the night flees from the light.
Therefore, if we wish to protect ourselves from the snares of demons, we must, as Virgil sings, &#x27;perhaps one day it will please us to remember these things,&#x27; not forget that eternal light is the sharpest weapon against the darkness. I will say this much, and now conclude: just as angels belong to heaven, so demons belong to the abyss—and both know their proper places. Let us beware, therefore, lest while seeking heaven we find the abyss.&quot;</div><br/><div id="37760402" class="c"><input type="checkbox" id="c-37760402" checked=""/><div class="controls bullet"><span class="by">schoen</span><span>|</span><a href="#37759485">root</a><span>|</span><a href="#37759611">parent</a><span>|</span><a href="#37759623">next</a><span>|</span><label class="collapse" for="c-37760402">[-]</label><label class="expand" for="c-37760402">[3 more]</label></div><br/><div class="children"><div class="content">This is very impressive!<p>Problems I see in the Latin:<p>(1) The sentence beginning &quot;verum, si Angelorum&quot; loses the thread in the middle (as you can also see in the English translation). The if-clause (protasis) has no main verb, while there is no then-clause (apodosis) at all.<p>(2) I think &quot;inactiones&quot; is hallucinated. This word isn&#x27;t found in Wiktionary, Lewis &amp; Short&#x27;s <i>Latin Dictionary</i>, or Souter&#x27;s <i>Glossary of Later Latin</i>. If it existed, it also wouldn&#x27;t be an adjective meaning &quot;inactive&quot;.<p>(3) In the indirect discourse governed by &quot;prodiderunt&quot;, the subject should also be accusative (&quot;angelos esse nuntios divinos&quot;).<p>(4) Since &quot;confundo&quot; is generally transitive, it&#x27;s a bit questionable to say &quot;ne confundamus&quot; without an object like &quot;eos&quot;, although I&#x27;m not positive it&#x27;s not idiomatic.<p>(5) I would think &quot;sinistrum&quot; rather than &quot;sinistram&quot; if the intention is to refer to &quot;something sinister&quot; rather than &quot;the left-hand side&quot;.<p>(6) &quot;nox a lucem fugiat&quot; is wrong (the preposition &quot;a&quot; always governs the ablative, not the accusative), and should probably be either &quot;nox lucem fugiat&quot; (less likely to me &quot;nox a luce fugiat&quot;). For rhetorical parallelism with &quot;a veritate&quot; it would definitely be &quot;a luce&quot;.<p>(7) The orthography is also inconsistent in writing &quot;iam&quot; but then quoting the <i>Aeneid</i> as saying &quot;juvabit&quot; (if you write consonantal i as j, you would expect to do it consistently everywhere).<p>Again, it&#x27;s very impressive. That&#x27;s a lot fewer mistakes than I would make writing a Latin text of this length!</div><br/><div id="37760632" class="c"><input type="checkbox" id="c-37760632" checked=""/><div class="controls bullet"><span class="by">benbreen</span><span>|</span><a href="#37759485">root</a><span>|</span><a href="#37760402">parent</a><span>|</span><a href="#37759623">next</a><span>|</span><label class="collapse" for="c-37760632">[-]</label><label class="expand" for="c-37760632">[2 more]</label></div><br/><div class="children"><div class="content">This is awesome, thank you! Or should I say gratias tibi :)</div><br/><div id="37762438" class="c"><input type="checkbox" id="c-37762438" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#37759485">root</a><span>|</span><a href="#37760632">parent</a><span>|</span><a href="#37759623">next</a><span>|</span><label class="collapse" for="c-37762438">[-]</label><label class="expand" for="c-37762438">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Or should I say gratias tibi<p>You might want to get a verb in there too.</div><br/></div></div></div></div></div></div></div></div><div id="37759623" class="c"><input type="checkbox" id="c-37759623" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#37759485">root</a><span>|</span><a href="#37759544">parent</a><span>|</span><a href="#37759611">prev</a><span>|</span><a href="#37760303">next</a><span>|</span><label class="collapse" for="c-37759623">[-]</label><label class="expand" for="c-37759623">[3 more]</label></div><br/><div class="children"><div class="content">The embedding method that nearly all LLM&#x27;s use puts them at a severe disadvantage because they can&#x27;t &#x27;see&#x27; the spelling of common words.  That makes it hard to infer things like &#x27;past tense words end with an e&#x27;.<p>With small modifications, the exact characters could be exposed to the model, in addition to the current tokens, but it would require a full retraining, which would cost $$$$$$$$.</div><br/><div id="37761346" class="c"><input type="checkbox" id="c-37761346" checked=""/><div class="controls bullet"><span class="by">omneity</span><span>|</span><a href="#37759485">root</a><span>|</span><a href="#37759623">parent</a><span>|</span><a href="#37759891">next</a><span>|</span><label class="collapse" for="c-37761346">[-]</label><label class="expand" for="c-37761346">[1 more]</label></div><br/><div class="children"><div class="content">You remind me of the ELMo architecture.<p><a href="https:&#x2F;&#x2F;paperswithcode.com&#x2F;method&#x2F;elmo" rel="nofollow noreferrer">https:&#x2F;&#x2F;paperswithcode.com&#x2F;method&#x2F;elmo</a></div><br/></div></div><div id="37759891" class="c"><input type="checkbox" id="c-37759891" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#37759485">root</a><span>|</span><a href="#37759623">parent</a><span>|</span><a href="#37761346">prev</a><span>|</span><a href="#37760303">next</a><span>|</span><label class="collapse" for="c-37759891">[-]</label><label class="expand" for="c-37759891">[1 more]</label></div><br/><div class="children"><div class="content">So, next week on HF?</div><br/></div></div></div></div></div></div><div id="37760303" class="c"><input type="checkbox" id="c-37760303" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#37759485">parent</a><span>|</span><a href="#37759544">prev</a><span>|</span><a href="#37761853">next</a><span>|</span><label class="collapse" for="c-37760303">[-]</label><label class="expand" for="c-37760303">[1 more]</label></div><br/><div class="children"><div class="content">You get really good results if you prompt it with: “You’re an expert in Latin translation”.</div><br/></div></div></div></div><div id="37761853" class="c"><input type="checkbox" id="c-37761853" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#37759485">prev</a><span>|</span><a href="#37759943">next</a><span>|</span><label class="collapse" for="c-37761853">[-]</label><label class="expand" for="c-37761853">[3 more]</label></div><br/><div class="children"><div class="content">Coming soon, layoffs in medieval history departments.</div><br/><div id="37762010" class="c"><input type="checkbox" id="c-37762010" checked=""/><div class="controls bullet"><span class="by">padraic7a</span><span>|</span><a href="#37761853">parent</a><span>|</span><a href="#37759943">next</a><span>|</span><label class="collapse" for="c-37762010">[-]</label><label class="expand" for="c-37762010">[2 more]</label></div><br/><div class="children"><div class="content">Why?<p>You think scholars are mostly translating?</div><br/><div id="37762076" class="c"><input type="checkbox" id="c-37762076" checked=""/><div class="controls bullet"><span class="by">vintermann</span><span>|</span><a href="#37761853">root</a><span>|</span><a href="#37762010">parent</a><span>|</span><a href="#37759943">next</a><span>|</span><label class="collapse" for="c-37762076">[-]</label><label class="expand" for="c-37762076">[1 more]</label></div><br/><div class="children"><div class="content">If only. There are far too many documents to read, transcribe and translate compared to the number of people paid to do that sort of thing!</div><br/></div></div></div></div></div></div><div id="37759943" class="c"><input type="checkbox" id="c-37759943" checked=""/><div class="controls bullet"><span class="by">staplung</span><span>|</span><a href="#37761853">prev</a><span>|</span><a href="#37760308">next</a><span>|</span><label class="collapse" for="c-37759943">[-]</label><label class="expand" for="c-37759943">[2 more]</label></div><br/><div class="children"><div class="content">I always figured our AI overlords would kill us in a Terminator kind of way, not in a &quot;No! You must not read from the book!&quot; sort of way.<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=E0DIsPBczcI">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=E0DIsPBczcI</a></div><br/><div id="37760467" class="c"><input type="checkbox" id="c-37760467" checked=""/><div class="controls bullet"><span class="by">phendrenad2</span><span>|</span><a href="#37759943">parent</a><span>|</span><a href="#37760308">next</a><span>|</span><label class="collapse" for="c-37760467">[-]</label><label class="expand" for="c-37760467">[1 more]</label></div><br/><div class="children"><div class="content">Digital Devil Saga indeed</div><br/></div></div></div></div><div id="37760308" class="c"><input type="checkbox" id="c-37760308" checked=""/><div class="controls bullet"><span class="by">user_named</span><span>|</span><a href="#37759943">prev</a><span>|</span><a href="#37762157">next</a><span>|</span><label class="collapse" for="c-37760308">[-]</label><label class="expand" for="c-37760308">[3 more]</label></div><br/><div class="children"><div class="content">I tried using BingGPT to translate simple Chinese text from screenshots. The results were complete hallucinations, different each time for the same screenshot.<p>I wouldn&#x27;t trust these translations at all.</div><br/><div id="37760822" class="c"><input type="checkbox" id="c-37760822" checked=""/><div class="controls bullet"><span class="by">mediaman</span><span>|</span><a href="#37760308">parent</a><span>|</span><a href="#37760461">next</a><span>|</span><label class="collapse" for="c-37760822">[-]</label><label class="expand" for="c-37760822">[1 more]</label></div><br/><div class="children"><div class="content">That’s a completely different test. You’re using the vision multimodal ability to decipher Chinese script, essentially adding an OCR step to the process, and it’s not good at OCR of Chinese script.<p>Try feeding it actual Chinese characters. From what I understand, it’s somewhat competent.</div><br/></div></div><div id="37760461" class="c"><input type="checkbox" id="c-37760461" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37760308">parent</a><span>|</span><a href="#37760822">prev</a><span>|</span><a href="#37762157">next</a><span>|</span><label class="collapse" for="c-37760461">[-]</label><label class="expand" for="c-37760461">[1 more]</label></div><br/><div class="children"><div class="content">Image input in Bing basically can&#x27;t handle non English text. Has nothing to do with its Chinese translation ability, which is great.</div><br/></div></div></div></div><div id="37762157" class="c"><input type="checkbox" id="c-37762157" checked=""/><div class="controls bullet"><span class="by">hoseja</span><span>|</span><a href="#37760308">prev</a><span>|</span><a href="#37760435">next</a><span>|</span><label class="collapse" for="c-37762157">[-]</label><label class="expand" for="c-37762157">[1 more]</label></div><br/><div class="children"><div class="content">What could <i>possibly</i> go wrong.</div><br/></div></div><div id="37760435" class="c"><input type="checkbox" id="c-37760435" checked=""/><div class="controls bullet"><span class="by">swader999</span><span>|</span><a href="#37762157">prev</a><span>|</span><a href="#37759683">next</a><span>|</span><label class="collapse" for="c-37760435">[-]</label><label class="expand" for="c-37760435">[1 more]</label></div><br/><div class="children"><div class="content">Real demons prefer Latin.</div><br/></div></div><div id="37759683" class="c"><input type="checkbox" id="c-37759683" checked=""/><div class="controls bullet"><span class="by">keymasta</span><span>|</span><a href="#37760435">prev</a><span>|</span><a href="#37760543">next</a><span>|</span><label class="collapse" for="c-37759683">[-]</label><label class="expand" for="c-37759683">[8 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been working on a language-learning app, and gpt-4 has made things doable that didn&#x27;t seem to be doable without it. For example, translating to lesser known languages like Cree. Online you can find a dictionary of words, or phrases, but I wasn&#x27;t able to find a translator for arbitrary statements. It&#x27;s not listed on google translate, nor yandex, et al. But, it&#x27;s possible with gpt.<p>My strategy in automated translation is to translate, and then translate back to English. That way we can be <i>fairly</i> sure the translation is accurate. Of course if this app &quot;has wings&quot; I would open source the translations to allow corrections, and&#x2F;or hire native translators for languages I don&#x27;t speak (which is the vast majority of languages).<p>Last tip is that for me, I was able to make a pretty good automated system for this. What I did was spell out a monstrosity of a system-prompt which ensures a few things. One, it will always give me the most descriptive romanization for non-ASCII languages. Two, it will give me output that is essentially .csv data. Three, if it encounters a made up language like Pirate, it will try (and do a pretty good job). The rest is just parsing my final translation file to find prompt&#x2F;language pairs which aren&#x27;t saved yet and piping that to the monstrous prompt which queries for translations.<p>Then you can run a separate program for validation. See if it passes my game of telephone test.<p>This way I and others could learn common phrases in languages which are hard to otherwise access. It even works for dead languages&#x2F;dialects&#x2F;character-voices, etc. For me, it&#x27;s pretty amazing actually. Please note that I never said perfect. But it&#x27;s pretty damn close actually.<p>All in all this workflow allows what would have been unimaginable even two years ago.<p>In doing tests today I found it interesting&#x2F;useful to note how gpt-4 is &quot;thinking&quot; about translating the word &quot;settings&quot;. Here is its thoughts. This is not the exact system prompt I&#x27;m using for my app.<p><pre><code>  system_prompt = &quot;You are an expert in translating lesser-known languages. When translating you will include both the native writing system, and the romanization into the latin alphabet. When you romanize text you always include any accents or pronunciation marks.&quot;

  user_prompt = &quot;Translate the following into Cree. [Hello, goodbye, settings]&quot;

  # Output
  In Cree language, your words will be translated into:

  1. Hello - ᑌᔭᔭᑎ, romanized as &quot;Tānsi&quot;
  2. Goodbye - ᐊᔭᙱᐂᒥᑎ, romanized as &quot;Ayāwāw&quot;

  &quot;Settings&quot; is a bit more complex, because it implies technological context that doesn&#x27;t necessarily have a direct equivalent in Cree. However, a possible option is:

  3. Settings - ᓂᐹᕗᓂᑕᐚᓇᐠ, romanized as &quot;Nipāvunitawānāk&quot;, which might refer to &quot;adjustments&quot;.</code></pre></div><br/><div id="37762325" class="c"><input type="checkbox" id="c-37762325" checked=""/><div class="controls bullet"><span class="by">unhammer</span><span>|</span><a href="#37759683">parent</a><span>|</span><a href="#37761159">next</a><span>|</span><label class="collapse" for="c-37762325">[-]</label><label class="expand" for="c-37762325">[1 more]</label></div><br/><div class="children"><div class="content">I really hope you check with native Cree speakers before you publish a system like this. Smaller language communities are already being hurt by low-quality generated text. See also <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35300603">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35300603</a> for an experience of GPT with the language Northern Saami.<p>Also, back-translation was never a good evaluation method. Say I don&#x27;t know English; and I make a new system that calls Google Translate word for word. I back-translate the Norwegian input &quot;den sykkelen kunne jeg gjerne hatt&quot; and my word-for-word system gives &quot;it the bike could I happily hat&quot;. Now I evaluate that whole sentence by putting it into Google Translate and presto I get &quot;den sykkelen kunne jeg gjerne hatt&quot; – can I conclude that word-for-word Google Translate is perfect? Or even worse, if you use the same system in both directions, anything that&#x27;s looks like a bijection if you squint will do (reverse then reverse, transliteration into Canadian syllabics then back, encrypt&#x2F;decrypt, map each word to a number, etc.).<p>If you have a bilingual corpus of Cree text, then you can get some automated measures (some popular ones listed at <a href="https:&#x2F;&#x2F;wiki.apertium.org&#x2F;wiki&#x2F;Evaluation" rel="nofollow noreferrer">https:&#x2F;&#x2F;wiki.apertium.org&#x2F;wiki&#x2F;Evaluation</a> ), but it would have to be a corpus you didn&#x27;t find online since otherwise GPT-4 has it in its training data and it the numbers will not be valid for sentences that are not in that corpus.</div><br/></div></div><div id="37761159" class="c"><input type="checkbox" id="c-37761159" checked=""/><div class="controls bullet"><span class="by">lionkor</span><span>|</span><a href="#37759683">parent</a><span>|</span><a href="#37762325">prev</a><span>|</span><a href="#37760475">next</a><span>|</span><label class="collapse" for="c-37761159">[-]</label><label class="expand" for="c-37761159">[1 more]</label></div><br/><div class="children"><div class="content">Why would you use an AI for something you know nothing about? that&#x27;s, like, the one use case that will not work and results in unchecked hallucinations</div><br/></div></div><div id="37760475" class="c"><input type="checkbox" id="c-37760475" checked=""/><div class="controls bullet"><span class="by">schoen</span><span>|</span><a href="#37759683">parent</a><span>|</span><a href="#37761159">prev</a><span>|</span><a href="#37760022">next</a><span>|</span><label class="collapse" for="c-37760475">[-]</label><label class="expand" for="c-37760475">[3 more]</label></div><br/><div class="children"><div class="content">I think there&#x27;s some hallucination creeping in here!<p>(1) This HN discussion is the <i>only</i> Google search result for each of these syllabics strings.<p>(2) I tried using <a href="https:&#x2F;&#x2F;syllabics.atlas-ling.ca&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;syllabics.atlas-ling.ca&#x2F;</a> to transliterate these to Roman letters, and none of these was transliterated in the same way as the GPT-4 output (although the third one is somewhat close).<p>(3) I searched and found that &quot;hello&quot; in Cree is likely written ᑖᓂᓯ (not ᑌᔭᔭᑎ), while correctly romanized as &quot;tān[i]si&quot;.<p>Your approach is clever, but I think the language model is still ultimately overconfident (and confused) here somehow.</div><br/><div id="37760786" class="c"><input type="checkbox" id="c-37760786" checked=""/><div class="controls bullet"><span class="by">keymasta</span><span>|</span><a href="#37759683">root</a><span>|</span><a href="#37760475">parent</a><span>|</span><a href="#37760022">next</a><span>|</span><label class="collapse" for="c-37760786">[-]</label><label class="expand" for="c-37760786">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, for Cree it is definitely more suspect than trustworthy. Another thing I noticed was that on another attempt I actually received different translations, so.. it&#x27;s hard to say how this is going to be refined to be usable, or if it indeed is at all.<p>And wow, yes we are all alone on google results for those strings.<p>EDIT 1: Another thought occurs to me, if it&#x27;s getting the transliteration right, and not the syllabics, maybe I seperate the tasks and go english -&gt; transliteration -&gt; syllabic. I will have to see if that approach works better.<p>Another idea might be to use that syllabics site to bring it from transliteration -&gt; syllabic. I noticed that they were correct if translated there.<p>EDIT 2: By updating the system prompt I was able to get it to translate properly. I had to remind it to be correct!<p><pre><code>  You are an expert in translating Cree. When translating you will include both the native writing system, and the romanization into the latin alphabet. When you romanize text you always include any accents or pronunciation marks. You use syllabics properly and in the modern usage

  Hello - ᑕᓂᓯ (Tânisi)
  Goodbye - ᐅᑲᕆ (Okaawii)
  Settings - ᐅᑌᕁ ᐟ (Otēw with Roman orthography)</code></pre></div><br/><div id="37761792" class="c"><input type="checkbox" id="c-37761792" checked=""/><div class="controls bullet"><span class="by">schoen</span><span>|</span><a href="#37759683">root</a><span>|</span><a href="#37760786">parent</a><span>|</span><a href="#37760022">next</a><span>|</span><label class="collapse" for="c-37761792">[-]</label><label class="expand" for="c-37761792">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I had to remind it to be correct!<p>It&#x27;s so funny to encounter the effects of language models producing the <i>highest-probability</i> completions of a prompt, and how those aren&#x27;t necessarily the same as the <i>most correct</i> completions.<p>I also saw something like this with people asking GPT models to write poetry, and they wrote mediocre poetry. But then when asked to write <i>good</i> poetry, they wrote better poetry!</div><br/></div></div></div></div></div></div><div id="37760022" class="c"><input type="checkbox" id="c-37760022" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#37759683">parent</a><span>|</span><a href="#37760475">prev</a><span>|</span><a href="#37760543">next</a><span>|</span><label class="collapse" for="c-37760022">[-]</label><label class="expand" for="c-37760022">[2 more]</label></div><br/><div class="children"><div class="content">This is such a cool example. &quot;Adjustments&quot; makes so much sense, doesn&#x27;t have the very modern &quot;settings&quot; implication of a device UI&#x2F;machinery</div><br/><div id="37760788" class="c"><input type="checkbox" id="c-37760788" checked=""/><div class="controls bullet"><span class="by">keymasta</span><span>|</span><a href="#37759683">root</a><span>|</span><a href="#37760022">parent</a><span>|</span><a href="#37760543">next</a><span>|</span><label class="collapse" for="c-37760788">[-]</label><label class="expand" for="c-37760788">[1 more]</label></div><br/><div class="children"><div class="content">Yeah. Hopefully it actually makes sense!</div><br/></div></div></div></div></div></div><div id="37760543" class="c"><input type="checkbox" id="c-37760543" checked=""/><div class="controls bullet"><span class="by">akomtu</span><span>|</span><a href="#37759683">prev</a><span>|</span><a href="#37760483">next</a><span>|</span><label class="collapse" for="c-37760543">[-]</label><label class="expand" for="c-37760543">[2 more]</label></div><br/><div class="children"><div class="content">Gpt4 does an ok job translating texts that aren&#x27;t complex, but if you read the original and its translation side by side, you&#x27;ll see that gpt4 still makes dumb mistakes every few sentences, hallucinates stuff when it runs into cryptic words it&#x27;s not familiar with, and sometimes omits important passages. Gpt4 is like a very productive, but clueless newbie.</div><br/><div id="37761467" class="c"><input type="checkbox" id="c-37761467" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37760543">parent</a><span>|</span><a href="#37760483">next</a><span>|</span><label class="collapse" for="c-37761467">[-]</label><label class="expand" for="c-37761467">[1 more]</label></div><br/><div class="children"><div class="content">&gt; gpt4 still makes dumb mistakes every few sentences, hallucinates stuff when it runs into cryptic words it&#x27;s not familiar with, and sometimes omits important passages<p>This, but in all domains where you are an expert - it becomes apparent that GPT-4 makes stupid mistakes. It makes mistakes frequently. Can&#x27;t seriously see it replacing humans soon, it is far from having four 9&#x27;s of reliability.<p>Probably needs 99.99% accuracy to work alone unsupervised by humans, because at each decision step it incurs the error rate again and again, so error rate is growing exponentially in sequence length. Coupled with the &quot;forward only&quot; thinking pattern of LLMs that doesn&#x27;t allow backtracking and planning this error rate kills autonomy.<p>Recently, GPT-4-Vision was found to have poor OCR accuracy. Smart but stupid, same story.</div><br/></div></div></div></div><div id="37760483" class="c"><input type="checkbox" id="c-37760483" checked=""/><div class="controls bullet"><span class="by">Barrin92</span><span>|</span><a href="#37760543">prev</a><span>|</span><a href="#37761019">next</a><span>|</span><label class="collapse" for="c-37760483">[-]</label><label class="expand" for="c-37760483">[2 more]</label></div><br/><div class="children"><div class="content"><i>&quot;Many people equate the word &quot;daemon&quot; with the word &quot;demon&quot;, implying some kind of satanic connection between UNIX and the underworld. This is an egregious misunderstanding. &quot;Daemon&quot; is actually a much older form of &quot;demon&quot;; daemons have no particular bias towards good or evil, but rather serve to help define a person&#x27;s character or personality. The ancient Greeks&#x27; concept of a &quot;personal daemon&quot; was similar to the modern concept of a &quot;guardian angel&quot;—eudaemonia is the state of being helped or protected by a kindly spirit. As a rule, UNIX systems seem to be infested with both daemons and demons.&quot;</i><p>that naming convention might turn out to be more prescient than people thought. Can&#x27;t wait until my Catholic school education pays off and I chant at my computer in Latin</div><br/><div id="37760597" class="c"><input type="checkbox" id="c-37760597" checked=""/><div class="controls bullet"><span class="by">akomtu</span><span>|</span><a href="#37760483">parent</a><span>|</span><a href="#37761019">next</a><span>|</span><label class="collapse" for="c-37760597">[-]</label><label class="expand" for="c-37760597">[1 more]</label></div><br/><div class="children"><div class="content">Linux actually got the &quot;demons&quot; right: those manifest as usually hidden activity in someone&#x27;s brain, and in special circumstances can take control over the entire system (e.g. with a deadlock if it&#x27;s a neutral demon, or by other means if it&#x27;s malicious). Those Greek daemons, in contrast, never possess or control anyone: they may inspire, but only if the subject is consciously seeking such inspiration (&quot;the gates must be opened from within&quot;).</div><br/></div></div></div></div><div id="37761019" class="c"><input type="checkbox" id="c-37761019" checked=""/><div class="controls bullet"><span class="by">szundi</span><span>|</span><a href="#37760483">prev</a><span>|</span><a href="#37758593">next</a><span>|</span><label class="collapse" for="c-37761019">[-]</label><label class="expand" for="c-37761019">[1 more]</label></div><br/><div class="children"><div class="content">Oh no, more demons summoned! AI ruins the world.</div><br/></div></div><div id="37758593" class="c"><input type="checkbox" id="c-37758593" checked=""/><div class="controls bullet"><span class="by">kevingadd</span><span>|</span><a href="#37761019">prev</a><span>|</span><a href="#37758868">next</a><span>|</span><label class="collapse" for="c-37758593">[-]</label><label class="expand" for="c-37758593">[16 more]</label></div><br/><div class="children"><div class="content">&quot;It&#x27;s clear that GPT-4 and Claude are skilled translators&quot; on what basis? What makes a predictor LLM better at translating Latin than a system trained specifically for translation?<p>I&#x27;m sure they can do a decent job but it&#x27;s weird to me that someone would leap to GPT-style tech despite its known tendency to hallucinate&#x2F;make stuff up instead of translation-oriented tools like DeepL or Google Translate (I say this as someone who despises both of those tools due to their quality issues)<p>I can&#x27;t imagine there are vast swaths of Latin in GPT&#x27;s training set.</div><br/><div id="37758672" class="c"><input type="checkbox" id="c-37758672" checked=""/><div class="controls bullet"><span class="by">benbreen</span><span>|</span><a href="#37758593">parent</a><span>|</span><a href="#37758760">next</a><span>|</span><label class="collapse" for="c-37758672">[-]</label><label class="expand" for="c-37758672">[1 more]</label></div><br/><div class="children"><div class="content">Well, that&#x27;s just it - I use Google Translate all the time to translate historical texts, and for whatever reason, GPT-4 and Claude both work better. Since I deal with texts that feature archaic orthography like the long s (ſ) the main advantage over Google Translate is that LLMs can make educated guesses about what a word <i>should</i> be. But even in terms of pure translation ability — assuming all orthographic issues have been corrected — the LLMs do a better job in the languages I&#x27;ve tested and which I can read (early modern Portuguese, Spanish and French, plus Latin).<p>The post by David Bell which I linked to gets into this for French - I agree with him that ChatGPT (I guess he was using GPT 3.5) has a tendency to &quot;overtranslate.&quot; But it is super impressive as a translator overall IMO: <a href="https:&#x2F;&#x2F;davidabell.substack.com&#x2F;p&#x2F;playing-around-with-machine-translation" rel="nofollow noreferrer">https:&#x2F;&#x2F;davidabell.substack.com&#x2F;p&#x2F;playing-around-with-machin...</a></div><br/></div></div><div id="37758760" class="c"><input type="checkbox" id="c-37758760" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37758593">parent</a><span>|</span><a href="#37758672">prev</a><span>|</span><a href="#37758727">next</a><span>|</span><label class="collapse" for="c-37758760">[-]</label><label class="expand" for="c-37758760">[1 more]</label></div><br/><div class="children"><div class="content">&gt;on what basis? What makes a predictor LLM better at translating Latin than a system trained specifically for translation?<p>They just are. Sure it sounds a bit strange if you&#x27;ve never thought about it but they are.<p>&gt;I&#x27;m sure they can do a decent job but it&#x27;s weird to me that someone would leap to GPT-style tech despite its known tendency to hallucinate&#x2F;make stuff up instead of translation-oriented tools like DeepL or Google Translate<p>1. They don&#x27;t just potentially do a decent job. For a couple dozen languages, GPT-4 is by far the best translator you can get your hands on. Google, Deepl are not as good.<p>2. Tasks like summarization and translation have very low hallucination rates. Not something to be particularly worried about with languages that have sufficient presence in training.<p>&gt;I can&#x27;t imagine there are vast swaths of Latin in GPT&#x27;s training set.<p>Doesn&#x27;t matter. There is incredible generalization for predict the next token models as far as proficiency is concerned. a model trained on 500b tokens on English and 50b tokens of french will not speak french like a model trained on only 50b tokens of french but much much better.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2108.13349" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2108.13349</a><p>It also doesn&#x27;t need to see translation pairs for every language in its corpus to learn how to translate that language pair(but this is the case for traditional models too)</div><br/></div></div><div id="37758727" class="c"><input type="checkbox" id="c-37758727" checked=""/><div class="controls bullet"><span class="by">lukebitts</span><span>|</span><a href="#37758593">parent</a><span>|</span><a href="#37758760">prev</a><span>|</span><a href="#37760337">next</a><span>|</span><label class="collapse" for="c-37758727">[-]</label><label class="expand" for="c-37758727">[4 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t attempted latin translations, but anything from my native language to english and back has been 100% perfect, miles better than anything google translate can do</div><br/><div id="37759105" class="c"><input type="checkbox" id="c-37759105" checked=""/><div class="controls bullet"><span class="by">marginalia_nu</span><span>|</span><a href="#37758593">root</a><span>|</span><a href="#37758727">parent</a><span>|</span><a href="#37760337">next</a><span>|</span><label class="collapse" for="c-37759105">[-]</label><label class="expand" for="c-37759105">[3 more]</label></div><br/><div class="children"><div class="content">Latin is tricky though.  Google translate is notoriously bad at latin grammar, much worse than most living languages.<p>Not exactly sure why, maybe small corpus, maybe because it&#x27;s a pro-drop language without fixed word order and an very complex set of conjugation rules.</div><br/><div id="37759394" class="c"><input type="checkbox" id="c-37759394" checked=""/><div class="controls bullet"><span class="by">duskwuff</span><span>|</span><a href="#37758593">root</a><span>|</span><a href="#37759105">parent</a><span>|</span><a href="#37760337">next</a><span>|</span><label class="collapse" for="c-37759394">[-]</label><label class="expand" for="c-37759394">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Not exactly sure why, maybe small corpus<p>That&#x27;s probably most of the problem. The entire corpus of ancient Latin is on the order of 9-10M words. If you printed it at the same density as a typical English novel, it wouldn&#x27;t even fill a bookcase.</div><br/><div id="37759859" class="c"><input type="checkbox" id="c-37759859" checked=""/><div class="controls bullet"><span class="by">marginalia_nu</span><span>|</span><a href="#37758593">root</a><span>|</span><a href="#37759394">parent</a><span>|</span><a href="#37760337">next</a><span>|</span><label class="collapse" for="c-37759859">[-]</label><label class="expand" for="c-37759859">[1 more]</label></div><br/><div class="children"><div class="content">Why would the corpus need to be limited to antique latin? The language was in use for much longer than antiquity. You&#x27;ve got everything from Thomas Aquinas to Isaac Newton to pad with.</div><br/></div></div></div></div></div></div></div></div><div id="37760337" class="c"><input type="checkbox" id="c-37760337" checked=""/><div class="controls bullet"><span class="by">svat</span><span>|</span><a href="#37758593">parent</a><span>|</span><a href="#37758727">prev</a><span>|</span><a href="#37758625">next</a><span>|</span><label class="collapse" for="c-37760337">[-]</label><label class="expand" for="c-37760337">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a great question. But note that Google Translate is <i>also</i> trained on &quot;predict the missing token&quot;: <a href="https:&#x2F;&#x2F;blog.research.google&#x2F;2022&#x2F;05&#x2F;24-new-languages-google-translate.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;blog.research.google&#x2F;2022&#x2F;05&#x2F;24-new-languages-google...</a> &#x2F; <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.03983" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.03983</a> (search the blog post around “Surprisingly, this simple procedure produces high quality zero-shot translations.”)<p>This was in May 2022, as part of Google Translate adding support for several low-resource languages (including Sanskrit). I was already very surprised that simply training on predicting tokens does translation so well — then a few months later ChatGPT came out, trained (roughly) the same way and doing a lot of things besides translation.</div><br/></div></div><div id="37758625" class="c"><input type="checkbox" id="c-37758625" checked=""/><div class="controls bullet"><span class="by">vore</span><span>|</span><a href="#37758593">parent</a><span>|</span><a href="#37760337">prev</a><span>|</span><a href="#37760465">next</a><span>|</span><label class="collapse" for="c-37758625">[-]</label><label class="expand" for="c-37758625">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think they&#x27;re arguing that an LLM is better at translation that an actual translator, just that they are pretty good at it. DeepL and Google Translate definitely also make things up though, so I don&#x27;t think that&#x27;s a good comparison...</div><br/><div id="37758912" class="c"><input type="checkbox" id="c-37758912" checked=""/><div class="controls bullet"><span class="by">mjn</span><span>|</span><a href="#37758593">root</a><span>|</span><a href="#37758625">parent</a><span>|</span><a href="#37760465">next</a><span>|</span><label class="collapse" for="c-37758912">[-]</label><label class="expand" for="c-37758912">[2 more]</label></div><br/><div class="children"><div class="content">&gt; DeepL and Google Translate definitely also make things up<p>I think what they make up is different, but this is a good point. They have a particularly odd tendency to either do something like autocorrect where it wasn&#x27;t appropriate (translate a different word that is similar in spelling to the requested word), or to make up false friends, doing something like transliterate + then autocorrect in the target language.<p>One example, which I blogged about 5 years ago but is still mistranslated, is the word &quot;ribbit&quot; (what a frog does): <a href="https:&#x2F;&#x2F;www.kmjn.org&#x2F;notes&#x2F;google_translates_ribbit.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.kmjn.org&#x2F;notes&#x2F;google_translates_ribbit.html</a><p>In 2018, if you translated it to Greek with Google Translate, it gave you κουνέλι (kouneli), which is Greek for <i>rabbit</i>. A word that is one letter away from ribbit but not close to a similar meaning. When I tried it just now, it translates it to ραβδί (rabdi), which means <i>stick</i> and is completely unrelated to the correct answer, but I guess starts with similar letters as <i>ribbit</i>?</div><br/><div id="37759612" class="c"><input type="checkbox" id="c-37759612" checked=""/><div class="controls bullet"><span class="by">20after4</span><span>|</span><a href="#37758593">root</a><span>|</span><a href="#37758912">parent</a><span>|</span><a href="#37760465">next</a><span>|</span><label class="collapse" for="c-37759612">[-]</label><label class="expand" for="c-37759612">[1 more]</label></div><br/><div class="children"><div class="content">Google search has a horrible tendency to do the same thing to my search terms.  Autocorrect is (usually) great when typing on a touch screen but it&#x27;s horrible when it decides it knows what I mean better than I do.</div><br/></div></div></div></div></div></div><div id="37760465" class="c"><input type="checkbox" id="c-37760465" checked=""/><div class="controls bullet"><span class="by">phire</span><span>|</span><a href="#37758593">parent</a><span>|</span><a href="#37758625">prev</a><span>|</span><a href="#37759469">next</a><span>|</span><label class="collapse" for="c-37760465">[-]</label><label class="expand" for="c-37760465">[1 more]</label></div><br/><div class="children"><div class="content">For some important context, The &quot;Attention is all you need&quot; paper that established the transformer architecture that most LLMs use, is a paper about explicitly machine translation.<p>It the idea of using transformers for non-translation tasks was only briefly explored at the end of the paper. So it really shouldn&#x27;t be surprising that LLMs are still good at translating.<p>Yes, the hallucinations are less than ideal, but the extra freedom is part of what makes their translation abilities so good when they do get it right.  
And it&#x27;s not look google translate is completely free of &quot;hallucination&quot; type issues. It&#x27;s well known that dedicated machine translation models will assume (aka hallucinate) genders when going from non-gendered to gendered languages.</div><br/></div></div><div id="37759469" class="c"><input type="checkbox" id="c-37759469" checked=""/><div class="controls bullet"><span class="by">anigbrowl</span><span>|</span><a href="#37758593">parent</a><span>|</span><a href="#37760465">prev</a><span>|</span><a href="#37759774">next</a><span>|</span><label class="collapse" for="c-37759469">[-]</label><label class="expand" for="c-37759469">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s probably great at Latin and Greek for reasons that should be obvious (plenty of public domain raw material, vast reams of scholarship dating back centuries). It&#x27;s less good with some other languages, eg some Japanese companies have decided to train their own models due to dissatisfaction with ChatGPT&#x27;s shortcomings.</div><br/></div></div><div id="37759774" class="c"><input type="checkbox" id="c-37759774" checked=""/><div class="controls bullet"><span class="by">j16sdiz</span><span>|</span><a href="#37758593">parent</a><span>|</span><a href="#37759469">prev</a><span>|</span><a href="#37759901">next</a><span>|</span><label class="collapse" for="c-37759774">[-]</label><label class="expand" for="c-37759774">[1 more]</label></div><br/><div class="children"><div class="content">LLM based translates use&#x2F;add contextual information.<p>It just choose better word when the original is ambiguous.<p>Hallucinating in translation task is quite low (much lower than creative, fact finding or information retrieval task)</div><br/></div></div><div id="37759901" class="c"><input type="checkbox" id="c-37759901" checked=""/><div class="controls bullet"><span class="by">imchillyb</span><span>|</span><a href="#37758593">parent</a><span>|</span><a href="#37759774">prev</a><span>|</span><a href="#37758815">next</a><span>|</span><label class="collapse" for="c-37759901">[-]</label><label class="expand" for="c-37759901">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What makes a predictor LLM better at translating Latin than a system trained specifically for translation?<p>Contextual awareness that is baked into the models.  Large Language Models are at their core transformation engines.  For the operation of transformative text there must be awareness of context.  This alone makes LLMs great candidates for translation tasks.</div><br/></div></div><div id="37758815" class="c"><input type="checkbox" id="c-37758815" checked=""/><div class="controls bullet"><span class="by">ogogmad</span><span>|</span><a href="#37758593">parent</a><span>|</span><a href="#37759901">prev</a><span>|</span><a href="#37758868">next</a><span>|</span><label class="collapse" for="c-37758815">[-]</label><label class="expand" for="c-37758815">[1 more]</label></div><br/><div class="children"><div class="content">Translation-specialised models like Google Translate don&#x27;t actually understand what they&#x27;re translating. But models like GPT do. This fact is intuitive and easy for anyone to test.</div><br/></div></div></div></div><div id="37758868" class="c"><input type="checkbox" id="c-37758868" checked=""/><div class="controls bullet"><span class="by">neilv</span><span>|</span><a href="#37758593">prev</a><span>|</span><a href="#37758699">next</a><span>|</span><label class="collapse" for="c-37758868">[-]</label><label class="expand" for="c-37758868">[4 more]</label></div><br/><div class="children"><div class="content">Institutional review board time.  We&#x27;ve already been warned that a computer merely enumerating the names of God can end existence.  Then surely a computer can also summon ancient demons.</div><br/><div id="37758892" class="c"><input type="checkbox" id="c-37758892" checked=""/><div class="controls bullet"><span class="by">atourgates</span><span>|</span><a href="#37758868">parent</a><span>|</span><a href="#37759650">next</a><span>|</span><label class="collapse" for="c-37758892">[-]</label><label class="expand" for="c-37758892">[1 more]</label></div><br/><div class="children"><div class="content">The author has no sense of self preservation!<p>&gt; Demonology is a topic I’ve been interested in ever since I read Stuart Clark’s brilliant Thinking with Demons. However, it’s also one I find intimidating to do real research in, since so many of the sources involved are written in a highly erudite form of Latin.<p>Should be:<p>&gt; since I don&#x27;t want to end up with a creature from the netherworld in my study.</div><br/></div></div><div id="37759650" class="c"><input type="checkbox" id="c-37759650" checked=""/><div class="controls bullet"><span class="by">coffeeshopgoth</span><span>|</span><a href="#37758868">parent</a><span>|</span><a href="#37758892">prev</a><span>|</span><a href="#37758915">next</a><span>|</span><label class="collapse" for="c-37759650">[-]</label><label class="expand" for="c-37759650">[1 more]</label></div><br/><div class="children"><div class="content">I mean, my computer has Legion written right on the front of it, so this just makes sense.</div><br/></div></div></div></div><div id="37758906" class="c"><input type="checkbox" id="c-37758906" checked=""/><div class="controls bullet"><span class="by">tomohelix</span><span>|</span><a href="#37758699">prev</a><span>|</span><label class="collapse" for="c-37758906">[-]</label><label class="expand" for="c-37758906">[5 more]</label></div><br/><div class="children"><div class="content">Slightly off topic but this is hilarious. We are already crafting &quot;chants&quot; and &quot;spells&quot; for LLM, i.e. prompt engineering, now we are teaching it demonology too? Some priest from the middle ages would have a heart attack.<p>Now I know how the AI apocalypse would look like. GPT-42 would summon hordes of demons from the pit of Hell to bring about the end of days. Who need all that pesky nuclear codes when you can call upon Satan?</div><br/><div id="37759098" class="c"><input type="checkbox" id="c-37759098" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#37758906">parent</a><span>|</span><a href="#37758914">next</a><span>|</span><label class="collapse" for="c-37759098">[-]</label><label class="expand" for="c-37759098">[2 more]</label></div><br/><div class="children"><div class="content">Technically the origins of the Gnostic demiurge as a concept comes remarkably close to the modern trend of AI.<p>A lot of the imagery was inspired by the Orphic Phanes, a dual gendered being of creation and light that gestated in the cosmic egg of the universe.<p>There were groups that combined the idea of an eventual creator with naturalism such that you had the idea of an original dual gendered primordial &#x27;man&#x27; which then brought forth a &#x27;son of man&#x27; that ended up creating us in the images of that original.<p>They allegedly patterned this on the Phrygian mysteries of a spontaneous original being that was likened to a tumor springing into existence.<p>They even tied this all into concepts around the dependence of the soul on the body, suggesting an original physical world and a non-physical replica created by the demiurge where death was functionally escapable.<p>The later influence of Neoplatonism flips it all back to a perfect form original and corrupted physical incarnation - the version of Gnosticism most well known today.<p>But for a tiny slice of time in antiquity you had people talking about the ideas of a naturally occurring man in a physical world giving rise to a creator of light which then recreated the physical original man in a non-physical twin of the cosmos in order to effectively allow them to escape death.<p>Which is a pretty wild set of beliefs for the time, but less so in an age where we are moving towards humanity bringing forth AI literally in light (optoelectronics is a likely next hardware shift), creating digital twins of the world around us, and increasingly making that world inhospitable to our continued existence.<p>Some of the nuances of what they were thinking about are pretty wild to consider in modern contexts.</div><br/><div id="37762609" class="c"><input type="checkbox" id="c-37762609" checked=""/><div class="controls bullet"><span class="by">fu7kt</span><span>|</span><a href="#37758906">root</a><span>|</span><a href="#37759098">parent</a><span>|</span><a href="#37758914">next</a><span>|</span><label class="collapse" for="c-37762609">[-]</label><label class="expand" for="c-37762609">[1 more]</label></div><br/><div class="children"><div class="content">Last year I partnered with a friends company to try to build a general AI management platform and I&#x27;ve always felt that this stuff is very occult in nature but with all the buzz around new LLMs and stuff I leaned in really hard on the occult imagery for the design part of it. The project didn&#x27;t continue, but I&#x27;m going to keep that in my pocket and try to reform it.<p>I put the design docs up just for posterity.<p>[1] project tombstone: <a href="https:&#x2F;&#x2F;orphanim.ai" rel="nofollow noreferrer">https:&#x2F;&#x2F;orphanim.ai</a><p>[2] Design and Style Guide <a href="https:&#x2F;&#x2F;docs.google.com&#x2F;document&#x2F;d&#x2F;1JsEeok9NmRGDAF3o0AX7h9ozyMJVUvvEEanM677nyu0&#x2F;edit?usp=sharing" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.google.com&#x2F;document&#x2F;d&#x2F;1JsEeok9NmRGDAF3o0AX7h9oz...</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>