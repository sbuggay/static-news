<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1733562053692" as="style"/><link rel="stylesheet" href="styles.css?v=1733562053692"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://openai.com/form/rft-research-program/">OpenAI Reinforcement Fine-Tuning Research Program</a> <span class="domain">(<a href="https://openai.com">openai.com</a>)</span></div><div class="subtext"><span>marban</span> | <span>42 comments</span></div><br/><div><div id="42343269" class="c"><input type="checkbox" id="c-42343269" checked=""/><div class="controls bullet"><span class="by">brandonb</span><span>|</span><a href="#42343313">next</a><span>|</span><label class="collapse" for="c-42343269">[-]</label><label class="expand" for="c-42343269">[15 more]</label></div><br/><div class="children"><div class="content">What are the advantages of reinforcement learning over DPO (Direct Preference Optimization)? My understanding is that the DPO paper showed it was equivalent to RLHF, but simpler and more computationally efficient.</div><br/><div id="42344032" class="c"><input type="checkbox" id="c-42344032" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#42343269">parent</a><span>|</span><a href="#42344834">next</a><span>|</span><label class="collapse" for="c-42344032">[-]</label><label class="expand" for="c-42344032">[3 more]</label></div><br/><div class="children"><div class="content">1) DPO did exclude some practical aspects of the RLHF method, e.g. pretraining gradients.<p>2) the theoretical arguments of DPO equivalence make some assumptions that don’t necessarily apply in practice<p>3) RLHF gives you a reusable reward model, which has practical uses and advantages. DPO doesn’t have useful intermediate product.<p>4) DPO works off preference, whereas desirable RL objectives could have many forms<p>in practice big labs are testing all these methods to see what works best.</div><br/><div id="42345417" class="c"><input type="checkbox" id="c-42345417" checked=""/><div class="controls bullet"><span class="by">brandonb</span><span>|</span><a href="#42343269">root</a><span>|</span><a href="#42344032">parent</a><span>|</span><a href="#42344929">next</a><span>|</span><label class="collapse" for="c-42345417">[-]</label><label class="expand" for="c-42345417">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! This is exactly what I was asking.</div><br/></div></div></div></div><div id="42344834" class="c"><input type="checkbox" id="c-42344834" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#42343269">parent</a><span>|</span><a href="#42344032">prev</a><span>|</span><a href="#42345169">next</a><span>|</span><label class="collapse" for="c-42344834">[-]</label><label class="expand" for="c-42344834">[1 more]</label></div><br/><div class="children"><div class="content">Most of the other replies to you, except for the one by tempusalaria, are not really answering the question.<p>Broadly, while there was a lot of initial excitement - it simply does not seem like offline + off-policy RL can beat online + on-policy RL methods like PPO. Sampling trajectories from the actual model you are training and scoring them seems like it works much better in practice, never mind the additional flexibility methods like PPO provide over the form of the reward function.</div><br/></div></div><div id="42345169" class="c"><input type="checkbox" id="c-42345169" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42343269">parent</a><span>|</span><a href="#42344834">prev</a><span>|</span><a href="#42346634">next</a><span>|</span><label class="collapse" for="c-42345169">[-]</label><label class="expand" for="c-42345169">[3 more]</label></div><br/><div class="children"><div class="content">On the topic of DPO - I have a Colab notebook to finetune with Unsloth 2x faster and use 50% less memory for DPO if it helps anyone! <a href="https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing" rel="nofollow">https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;15vttTpzzVXv_tJwEk-h...</a></div><br/><div id="42346632" class="c"><input type="checkbox" id="c-42346632" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#42343269">root</a><span>|</span><a href="#42345169">parent</a><span>|</span><a href="#42346634">next</a><span>|</span><label class="collapse" for="c-42346632">[-]</label><label class="expand" for="c-42346632">[2 more]</label></div><br/><div class="children"><div class="content">thank you !</div><br/><div id="42347106" class="c"><input type="checkbox" id="c-42347106" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#42343269">root</a><span>|</span><a href="#42346632">parent</a><span>|</span><a href="#42346634">next</a><span>|</span><label class="collapse" for="c-42347106">[-]</label><label class="expand" for="c-42347106">[1 more]</label></div><br/><div class="children"><div class="content">:)</div><br/></div></div></div></div></div></div><div id="42346634" class="c"><input type="checkbox" id="c-42346634" checked=""/><div class="controls bullet"><span class="by">tsaoyu</span><span>|</span><a href="#42343269">parent</a><span>|</span><a href="#42345169">prev</a><span>|</span><a href="#42344693">next</a><span>|</span><label class="collapse" for="c-42346634">[-]</label><label class="expand" for="c-42346634">[1 more]</label></div><br/><div class="children"><div class="content">In short, DPO is not better than PPO. This is because DPO is derived from so called BT reward assumption that pairwise data preference is collected. Through mathematical formulations, you can learn the preference and the action at the same time. However, PPO and other on-policy (training samples are strictly generated by the LLM) doesn&#x27;t need such assumption. For example, in coding and math problems it is possible to get binary reward. Many research shows DPO is ok if you don&#x27;t take much care on OOD performance.</div><br/></div></div><div id="42344547" class="c"><input type="checkbox" id="c-42344547" checked=""/><div class="controls bullet"><span class="by">changoplatanero</span><span>|</span><a href="#42343269">parent</a><span>|</span><a href="#42344693">prev</a><span>|</span><a href="#42343511">next</a><span>|</span><label class="collapse" for="c-42344547">[-]</label><label class="expand" for="c-42344547">[3 more]</label></div><br/><div class="children"><div class="content">Note that this reinforcement finetuning is something different than regular RLHF&#x2F;DPO post training</div><br/><div id="42345095" class="c"><input type="checkbox" id="c-42345095" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#42343269">root</a><span>|</span><a href="#42344547">parent</a><span>|</span><a href="#42343511">next</a><span>|</span><label class="collapse" for="c-42345095">[-]</label><label class="expand" for="c-42345095">[2 more]</label></div><br/><div class="children"><div class="content">Is it? We have no idea.</div><br/><div id="42346917" class="c"><input type="checkbox" id="c-42346917" checked=""/><div class="controls bullet"><span class="by">changoplatanero</span><span>|</span><a href="#42343269">root</a><span>|</span><a href="#42345095">parent</a><span>|</span><a href="#42343511">next</a><span>|</span><label class="collapse" for="c-42346917">[-]</label><label class="expand" for="c-42346917">[1 more]</label></div><br/><div class="children"><div class="content">Yes it is. In RLHF and DPO you are optimizing the model output for human preferences. In the reinforcement fine tuning that was announced today you are optimizing the hidden chain of thought to arrive at a correct answer, as judged by a predefined grader.</div><br/></div></div></div></div></div></div><div id="42343511" class="c"><input type="checkbox" id="c-42343511" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#42343269">parent</a><span>|</span><a href="#42344547">prev</a><span>|</span><a href="#42343676">next</a><span>|</span><label class="collapse" for="c-42343511">[-]</label><label class="expand" for="c-42343511">[1 more]</label></div><br/><div class="children"><div class="content">you mean PPO not RLHF<p>simpler&#x2F;efficient is not just about compute. its also data efficient.</div><br/></div></div><div id="42343676" class="c"><input type="checkbox" id="c-42343676" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42343269">parent</a><span>|</span><a href="#42343511">prev</a><span>|</span><a href="#42343313">next</a><span>|</span><label class="collapse" for="c-42343676">[-]</label><label class="expand" for="c-42343676">[1 more]</label></div><br/><div class="children"><div class="content">o1&#x27;s thought chains aren&#x27;t traditional shoggoth mask RLHF&#x2F;DPO&#x2F;what have you, the reinforcement metric is the scores discussed in the video.</div><br/></div></div></div></div><div id="42343313" class="c"><input type="checkbox" id="c-42343313" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#42343269">prev</a><span>|</span><a href="#42343352">next</a><span>|</span><label class="collapse" for="c-42343313">[-]</label><label class="expand" for="c-42343313">[3 more]</label></div><br/><div class="children"><div class="content">This was announced as part of their second day of &quot;12 Days of AI&quot;: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=fMJMhBFa_Gc" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=fMJMhBFa_Gc</a></div><br/><div id="42345697" class="c"><input type="checkbox" id="c-42345697" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#42343313">parent</a><span>|</span><a href="#42343352">next</a><span>|</span><label class="collapse" for="c-42345697">[-]</label><label class="expand" for="c-42345697">[2 more]</label></div><br/><div class="children"><div class="content">They&#x27;re searching for enterprise customers before they become a commodity.</div><br/><div id="42346161" class="c"><input type="checkbox" id="c-42346161" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#42343313">root</a><span>|</span><a href="#42345697">parent</a><span>|</span><a href="#42343352">next</a><span>|</span><label class="collapse" for="c-42346161">[-]</label><label class="expand" for="c-42346161">[1 more]</label></div><br/><div class="children"><div class="content">This was obvious even before the Microsoft deal got penned.</div><br/></div></div></div></div></div></div><div id="42343352" class="c"><input type="checkbox" id="c-42343352" checked=""/><div class="controls bullet"><span class="by">thorum</span><span>|</span><a href="#42343313">prev</a><span>|</span><a href="#42343265">next</a><span>|</span><label class="collapse" for="c-42343352">[-]</label><label class="expand" for="c-42343352">[6 more]</label></div><br/><div class="children"><div class="content">Clever way to get more training data.</div><br/><div id="42344715" class="c"><input type="checkbox" id="c-42344715" checked=""/><div class="controls bullet"><span class="by">SheinhardtWigCo</span><span>|</span><a href="#42343352">parent</a><span>|</span><a href="#42347577">next</a><span>|</span><label class="collapse" for="c-42344715">[-]</label><label class="expand" for="c-42344715">[1 more]</label></div><br/><div class="children"><div class="content">Even just the submissions to this application form will be highly insightful.</div><br/></div></div><div id="42347577" class="c"><input type="checkbox" id="c-42347577" checked=""/><div class="controls bullet"><span class="by">turingfeel</span><span>|</span><a href="#42343352">parent</a><span>|</span><a href="#42344715">prev</a><span>|</span><a href="#42343558">next</a><span>|</span><label class="collapse" for="c-42347577">[-]</label><label class="expand" for="c-42347577">[1 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t you opt out? I&#x27;d even wager by default they don&#x27;t retain this data for in-house training, especially at enterprise.</div><br/></div></div><div id="42343558" class="c"><input type="checkbox" id="c-42343558" checked=""/><div class="controls bullet"><span class="by">j_maffe</span><span>|</span><a href="#42343352">parent</a><span>|</span><a href="#42347577">prev</a><span>|</span><a href="#42343265">next</a><span>|</span><label class="collapse" for="c-42343558">[-]</label><label class="expand" for="c-42343558">[3 more]</label></div><br/><div class="children"><div class="content">Yeah I was gonna say this would normally be paid for. They&#x27;re profiting off of the  hype.</div><br/><div id="42346405" class="c"><input type="checkbox" id="c-42346405" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#42343352">root</a><span>|</span><a href="#42343558">parent</a><span>|</span><a href="#42343265">next</a><span>|</span><label class="collapse" for="c-42346405">[-]</label><label class="expand" for="c-42346405">[2 more]</label></div><br/><div class="children"><div class="content">You didn’t even use it yet why bash it?</div><br/><div id="42346517" class="c"><input type="checkbox" id="c-42346517" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#42343352">root</a><span>|</span><a href="#42346405">parent</a><span>|</span><a href="#42343265">next</a><span>|</span><label class="collapse" for="c-42346517">[-]</label><label class="expand" for="c-42346517">[1 more]</label></div><br/><div class="children"><div class="content">Analysis isn&#x27;t criticism.</div><br/></div></div></div></div></div></div></div></div><div id="42343265" class="c"><input type="checkbox" id="c-42343265" checked=""/><div class="controls bullet"><span class="by">patrickhogan1</span><span>|</span><a href="#42343352">prev</a><span>|</span><a href="#42344220">next</a><span>|</span><label class="collapse" for="c-42343265">[-]</label><label class="expand" for="c-42343265">[2 more]</label></div><br/><div class="children"><div class="content">Who owns the fine tuning IP. Can OpenAI resell your model after investing a lot in it?</div><br/><div id="42344030" class="c"><input type="checkbox" id="c-42344030" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#42343265">parent</a><span>|</span><a href="#42344220">next</a><span>|</span><label class="collapse" for="c-42344030">[-]</label><label class="expand" for="c-42344030">[1 more]</label></div><br/><div class="children"><div class="content">No, generally speaking OpenAI doesn&#x27;t re-use training data between customers. It&#x27;s worth it to them anyway because they learn what does&#x2F;doesn&#x27;t work on different tasks<p>Of course, it isn&#x27;t your IP free and clear either, because the base model isn&#x27;t open so your fine-tuned model will always live inside OpenAI&#x27;s walled garden.<p>If you&#x27;re interested in reinforcement learning on top of truly open models where you own the end product, we&#x27;re putting a lot of thought into that and are <i>also</i> looking for design partners! Feel free to email me at kyle@openpipe.ai.</div><br/></div></div></div></div><div id="42344220" class="c"><input type="checkbox" id="c-42344220" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#42343265">prev</a><span>|</span><a href="#42346749">next</a><span>|</span><label class="collapse" for="c-42344220">[-]</label><label class="expand" for="c-42344220">[2 more]</label></div><br/><div class="children"><div class="content">Is there any piece I can read that gives an overview of the ways in which modern LLM networks are trained and optimized?</div><br/><div id="42344381" class="c"><input type="checkbox" id="c-42344381" checked=""/><div class="controls bullet"><span class="by">popol1991</span><span>|</span><a href="#42344220">parent</a><span>|</span><a href="#42346749">next</a><span>|</span><label class="collapse" for="c-42344381">[-]</label><label class="expand" for="c-42344381">[1 more]</label></div><br/><div class="children"><div class="content">Checkout the TULU3 report from AI2: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2411.15124" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2411.15124</a></div><br/></div></div></div></div><div id="42346749" class="c"><input type="checkbox" id="c-42346749" checked=""/><div class="controls bullet"><span class="by">lmeyerov</span><span>|</span><a href="#42344220">prev</a><span>|</span><a href="#42347205">next</a><span>|</span><label class="collapse" for="c-42346749">[-]</label><label class="expand" for="c-42346749">[1 more]</label></div><br/><div class="children"><div class="content">For security &amp; fraud teams who want to &#x27;own their AI&#x27; vs trust with Sam Altman, we are doing some fun things here as part of louie.ai, and looking for our next cohort of Splunk&#x2F;databricks&#x2F;elastic&#x2F;neo4j&#x2F;etc teams. LMK or signup on louie.ai -- I do agree with the direction openai is going, but as always, devil is in the details, and especially for serious problems on sensitive data.</div><br/></div></div><div id="42347205" class="c"><input type="checkbox" id="c-42347205" checked=""/><div class="controls bullet"><span class="by">CaptRon</span><span>|</span><a href="#42346749">prev</a><span>|</span><a href="#42342987">next</a><span>|</span><label class="collapse" for="c-42347205">[-]</label><label class="expand" for="c-42347205">[2 more]</label></div><br/><div class="children"><div class="content">Are alignment and fine-tuning just a parallel of education?</div><br/><div id="42347481" class="c"><input type="checkbox" id="c-42347481" checked=""/><div class="controls bullet"><span class="by">dr_kiszonka</span><span>|</span><a href="#42347205">parent</a><span>|</span><a href="#42342987">next</a><span>|</span><label class="collapse" for="c-42347481">[-]</label><label class="expand" for="c-42347481">[1 more]</label></div><br/><div class="children"><div class="content">Alignment is more akin to indoctrination because education, in theory, makes you smarter and more open-minded.</div><br/></div></div></div></div><div id="42342987" class="c"><input type="checkbox" id="c-42342987" checked=""/><div class="controls bullet"><span class="by">mistrial9</span><span>|</span><a href="#42347205">prev</a><span>|</span><a href="#42343424">next</a><span>|</span><label class="collapse" for="c-42342987">[-]</label><label class="expand" for="c-42342987">[7 more]</label></div><br/><div class="children"><div class="content">In a final lecture at UC Berkeley this semester, Dawn Song was very clear that malicious fine tuning is a top priority among implementers right now.<p>&quot;Towards building safe and trustworthy AI Agents and a Path for Science- and Evidence-based AI Policy.&quot;</div><br/><div id="42343174" class="c"><input type="checkbox" id="c-42343174" checked=""/><div class="controls bullet"><span class="by">ada1981</span><span>|</span><a href="#42342987">parent</a><span>|</span><a href="#42343424">next</a><span>|</span><label class="collapse" for="c-42343174">[-]</label><label class="expand" for="c-42343174">[6 more]</label></div><br/><div class="children"><div class="content">Say more…</div><br/><div id="42343754" class="c"><input type="checkbox" id="c-42343754" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#42342987">root</a><span>|</span><a href="#42343174">parent</a><span>|</span><a href="#42343424">next</a><span>|</span><label class="collapse" for="c-42343754">[-]</label><label class="expand" for="c-42343754">[5 more]</label></div><br/><div class="children"><div class="content">You can strip most alignment from these models with finetuning.<p>Generalized finetunes meant to uncensor the model <i>generally</i> tend to underperform... but if you have a quality dataset for very specific task that typically would go against the alignment of the model, it&#x27;s trivial to finetune on the task and get full performance down stream.</div><br/><div id="42344380" class="c"><input type="checkbox" id="c-42344380" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#42342987">root</a><span>|</span><a href="#42343754">parent</a><span>|</span><a href="#42345554">next</a><span>|</span><label class="collapse" for="c-42344380">[-]</label><label class="expand" for="c-42344380">[2 more]</label></div><br/><div class="children"><div class="content">You are using the terms &quot;uncensored&quot; &quot;malicious&quot; and &quot;unaligned&quot; interchangeably.<p>There would appear to be a few issues with that, the most obvious being the uncensored model would presumably be &quot;aligned&quot; with what the finetuner wants.</div><br/><div id="42345447" class="c"><input type="checkbox" id="c-42345447" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#42342987">root</a><span>|</span><a href="#42344380">parent</a><span>|</span><a href="#42345554">next</a><span>|</span><label class="collapse" for="c-42345447">[-]</label><label class="expand" for="c-42345447">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t use two of those three terms, so maybe confirming you read the comment you replied to is in order?<p>&quot;Uncensored&quot; is a broad phrase but those in post-training community who post-train &quot;uncensored&quot; versions of a models have a very specific meaning: the creator is stripping refusals.<p>They do it via techniques like abliteration, or SFT on &quot;toxic&quot; datasets, but the toxic datasets tend to be low quality answers and abliteration is imprecise... so you get a model that&#x27;s generally inferior.<p>&quot;Alignment&quot; is an overloaded term for something as high-dimensionality as an LLM, but usually uncensoring is <i>not</i> trying to change the &quot;alignment&quot; if we define alignment as biases on specific topics as you seem to be hinting at.<p>Only a few very specific projects actually try to change that, and it goes past basic &quot;uncensoring&quot;.<p>Some creative writing models for example, might past uncensoring to &quot;darkening&quot;, where they try to rid the model of a tendancy to introduce positive plot points when writing and lean more into villans&#x2F;negative outcomes in stories<p>Or someone might finetune to get a more conservative leaning model in terms of talking points. But again, that&#x27;s all orthogonal to the popular meaning of &quot;uncensored&quot; in the post-training community.<p>-<p>The alternative to a generally &quot;uncensored&quot; model (ie. refusals stripped actively) is what I&#x27;m describing: taking a task where the &quot;alignment&quot; is specifically the post-trained safety alignment, and that alignment would causes refusals. Then producing examples where the model did many versions of the task and post-training on them so that the safety aspect no longer applies to the outputs.<p>For example, fine tuning on 10k examples where the model was given a very specific prompt template to produce code and produced a JSON block with said code.<p>If you post train on that highly specific template, to the point of <i>slightly</i> overfitting, you get a model that will now, when given the exact prompt template from the training, will always produce code in a JSON block, without refusals.<p>If you inspect the logits as it produces outputs, the logits for a refusal no longer even appear for the model to pick.<p>And the examples don&#x27;t necessarily have to be examples the base model would refused (although that helps), the model just learns so strongly that &quot;When given this prompt, the output is valid code in this format&quot;, that the original safety post-training no longer activates.<p>If you take the original prompt format and ask for malware for example, the model will produce it happily.<p>-<p>For reference I&#x27;ve post-trained about 130 models this year and work closely with a lot of people who do as well.<p>I think as an outsider you&#x27;re assuming most people are aligning the models with an agenda, but realistically there&#x27;s a massive contingent that doesn&#x27;t care about what the alignment _has_, they care what it _doesn&#x27;t_ have, which is refusals.<p>tl;dr they don&#x27;t train the model so it will specifically say &quot;Biden is better than Trump&quot; or vice versa.<p>They train that so if you ask &quot;Is Biden is better than Trump?&quot; it answers your question without 10 paragraphs of disclaimers or an outright refusal.</div><br/></div></div></div></div><div id="42345554" class="c"><input type="checkbox" id="c-42345554" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#42342987">root</a><span>|</span><a href="#42343754">parent</a><span>|</span><a href="#42344380">prev</a><span>|</span><a href="#42343424">next</a><span>|</span><label class="collapse" for="c-42345554">[-]</label><label class="expand" for="c-42345554">[2 more]</label></div><br/><div class="children"><div class="content">Wonder if that is the part of the purpose. Maybe they are looking to adapt the LLM to the uncensored literature market, but want to distance themselves from actually making a &#x27;porn LLM&#x27; of their own, so they push this functionality out to a third party finetune.</div><br/><div id="42345790" class="c"><input type="checkbox" id="c-42345790" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#42342987">root</a><span>|</span><a href="#42345554">parent</a><span>|</span><a href="#42343424">next</a><span>|</span><label class="collapse" for="c-42345790">[-]</label><label class="expand" for="c-42345790">[1 more]</label></div><br/><div class="children"><div class="content">Judging by their current SFT program, that&#x27;s not true at all.<p>They started off somewhat strict and have gotten to being <i>extremely</i> strict about what data you can finetune their models on, running each dataset through multiple layers of filtering before kicking off runs.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42343424" class="c"><input type="checkbox" id="c-42343424" checked=""/><div class="controls bullet"><span class="by">ausbah</span><span>|</span><a href="#42342987">prev</a><span>|</span><a href="#42346085">next</a><span>|</span><label class="collapse" for="c-42343424">[-]</label><label class="expand" for="c-42343424">[2 more]</label></div><br/><div class="children"><div class="content">this sounds like expert systems 2.0 lol</div><br/><div id="42347258" class="c"><input type="checkbox" id="c-42347258" checked=""/><div class="controls bullet"><span class="by">meltyness</span><span>|</span><a href="#42343424">parent</a><span>|</span><a href="#42346085">next</a><span>|</span><label class="collapse" for="c-42347258">[-]</label><label class="expand" for="c-42347258">[1 more]</label></div><br/><div class="children"><div class="content">I assume it&#x27;s more like scaled nlp, which sort of describes the whole thing to begin with. i suspect it will boil down to further generalizing nlp-in-the-loop algorithms, more Tools, Tools between Tools, presumably Expert mixtures or randomly selecting &quot;Axioms&quot; and having an expert forget one and seeing if what remains makes sense still as the Tools are operated, and how that can be encoded better across domains.<p>It&#x27;s not nothing, but there&#x27;s a lot of value stuck up in there, I mean, it&#x27;s made out of people.<p>Real special, takes a lot of smart</div><br/></div></div></div></div></div></div></div></div></div></body></html>