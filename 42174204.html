<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1732006876076" as="style"/><link rel="stylesheet" href="styles.css?v=1732006876076"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a>Launch HN: Regatta Storage (YC F24) – Turn S3 into a local-like, POSIX cloud FS</a> </div><div class="subtext"><span>huntaub</span> | <span>116 comments</span></div><br/><div><div id="42176699" class="c"><input type="checkbox" id="c-42176699" checked=""/><div class="controls bullet"><span class="by">garganzol</span><span>|</span><a href="#42174697">next</a><span>|</span><label class="collapse" for="c-42176699">[-]</label><label class="expand" for="c-42176699">[10 more]</label></div><br/><div class="children"><div class="content">I used the same approach based on Rclone for a long time. I wondered what makes Regatta Storage different than Rclone. Here is the answer: &quot;When performing mutating operations on the file system (including writes, renames, and directory changes), Regatta first stages this data on its high-speed caching layer to provide strong consistency to other file clients.&quot; [0].<p>Rclone, on the contrary, has no layer that would guarantee consistency among parallel clients.<p>[0] <a href="https:&#x2F;&#x2F;docs.regattastorage.com&#x2F;details&#x2F;architecture#overview">https:&#x2F;&#x2F;docs.regattastorage.com&#x2F;details&#x2F;architecture#overvie...</a></div><br/><div id="42176790" class="c"><input type="checkbox" id="c-42176790" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176699">parent</a><span>|</span><a href="#42178556">next</a><span>|</span><label class="collapse" for="c-42176790">[-]</label><label class="expand" for="c-42176790">[3 more]</label></div><br/><div class="children"><div class="content">This is exactly right, and something that we think is particularly important for applications that care about data consistency. Often times, we see that customers want to be able to quickly hand off tasks from one instance to another which can be incredibly complex if you don&#x27;t have guarantees that your new operations will be seen by the second instance!</div><br/><div id="42177109" class="c"><input type="checkbox" id="c-42177109" checked=""/><div class="controls bullet"><span class="by">wanderingmind</span><span>|</span><a href="#42176699">root</a><span>|</span><a href="#42176790">parent</a><span>|</span><a href="#42178556">next</a><span>|</span><label class="collapse" for="c-42177109">[-]</label><label class="expand" for="c-42177109">[2 more]</label></div><br/><div class="children"><div class="content">Might be useful to show the differences with Rclone, s3fs as a table to make it obvious</div><br/><div id="42177304" class="c"><input type="checkbox" id="c-42177304" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176699">root</a><span>|</span><a href="#42177109">parent</a><span>|</span><a href="#42178556">next</a><span>|</span><label class="collapse" for="c-42177304">[-]</label><label class="expand" for="c-42177304">[1 more]</label></div><br/><div class="children"><div class="content">I agree, I plan to put up a table soon.</div><br/></div></div></div></div></div></div><div id="42178556" class="c"><input type="checkbox" id="c-42178556" checked=""/><div class="controls bullet"><span class="by">benatkin</span><span>|</span><a href="#42176699">parent</a><span>|</span><a href="#42176790">prev</a><span>|</span><a href="#42177002">next</a><span>|</span><label class="collapse" for="c-42178556">[-]</label><label class="expand" for="c-42178556">[5 more]</label></div><br/><div class="children"><div class="content">The headline seems misleading, then.<p>rclone can work with AWS&#x27; different offerings, some of which at least partially address this: <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;new-amazon-s3-express-one-zone-high-performance-storage-class&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;new-amazon-s3-express-one-z...</a></div><br/><div id="42178584" class="c"><input type="checkbox" id="c-42178584" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176699">root</a><span>|</span><a href="#42178556">parent</a><span>|</span><a href="#42177002">next</a><span>|</span><label class="collapse" for="c-42178584">[-]</label><label class="expand" for="c-42178584">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not totally sure what you mean. I don&#x27;t think that S3 Express One Zone offers any additional atomic semantics in the file system world.</div><br/><div id="42179130" class="c"><input type="checkbox" id="c-42179130" checked=""/><div class="controls bullet"><span class="by">benatkin</span><span>|</span><a href="#42176699">root</a><span>|</span><a href="#42178584">parent</a><span>|</span><a href="#42177002">next</a><span>|</span><label class="collapse" for="c-42179130">[-]</label><label class="expand" for="c-42179130">[3 more]</label></div><br/><div class="children"><div class="content">For the misleading part, I probably should have said confusing because I don&#x27;t think you intended that, I mean that instead of introducing your caching layer you make it about S3, where the Object Storage provider seems totally interchangeable. Though it seems to work for a lot of your audience, from what I can tell from other comments here.<p>As for Express One Zone providing consistency, it would make more groups of operations consistent, provided that the clients could access the endpoints with low latency. It wouldn&#x27;t be a guarantee but it would be practical for some applications. It depends on what the problem is - for instance, do you want someone to never see noticeably stale data? I can definitely see that happening with Express One Zone if it&#x27;s as described.</div><br/><div id="42179162" class="c"><input type="checkbox" id="c-42179162" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176699">root</a><span>|</span><a href="#42179130">parent</a><span>|</span><a href="#42177002">next</a><span>|</span><label class="collapse" for="c-42179162">[-]</label><label class="expand" for="c-42179162">[2 more]</label></div><br/><div class="children"><div class="content">Yes, I think this is something that I’m actually struggling with. What’s the most exciting part for users? Is it the fact that we’re building a super fast file system or is it that we have this synchronization to S3? Ultimately, there just isn’t space for it all — but I appreciate the feedback.</div><br/><div id="42179229" class="c"><input type="checkbox" id="c-42179229" checked=""/><div class="controls bullet"><span class="by">benatkin</span><span>|</span><a href="#42176699">root</a><span>|</span><a href="#42179162">parent</a><span>|</span><a href="#42177002">next</a><span>|</span><label class="collapse" for="c-42179229">[-]</label><label class="expand" for="c-42179229">[1 more]</label></div><br/><div class="children"><div class="content">I think they both go together. It might take about 10 minutes to give a good high level explanation of it, including how the S3 syncing works - that the S3 lags slightly behind the caching layer for reads, and that you can still write to S3. 2-way sync. I imagine that S3 would be treated sort of like another client if updates came from S3 and the clients at the same time. It would probably be not so great to write to S3 if you aren&#x27;t writing to somewhere that&#x27;s being actively edited, but if you want to write to a dormant area of S3 directly, that&#x27;s fine.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42177002" class="c"><input type="checkbox" id="c-42177002" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#42176699">parent</a><span>|</span><a href="#42178556">prev</a><span>|</span><a href="#42174697">next</a><span>|</span><label class="collapse" for="c-42177002">[-]</label><label class="expand" for="c-42177002">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, this was my thought as well.  I use and love rclone and it wasn&#x27;t immediately clear what this offered above that</div><br/></div></div></div></div><div id="42174697" class="c"><input type="checkbox" id="c-42174697" checked=""/><div class="controls bullet"><span class="by">memset</span><span>|</span><a href="#42176699">prev</a><span>|</span><a href="#42180521">next</a><span>|</span><label class="collapse" for="c-42174697">[-]</label><label class="expand" for="c-42174697">[9 more]</label></div><br/><div class="children"><div class="content">This is honestly the coolest thing I&#x27;ve seen coming out of YC in years. I have a bunch of questions which are basically related to &quot;how does it work&quot; and please pardon me if my questions are silly or naive!<p>1. If I had a local disk which was 10 GB, what happens when I try to contend with data in the 50 GB range (as in, more that could be cached locally?) Would I immediately see degradation, or thrashing, at the 10 GB mark?<p>2. Does this only work in practice on AWS instances? As in, I could run it on a different cloud, but in practice we only really get fast speeds due to running everything within AWS?<p>3. I&#x27;ve always had trouble with FUSE in different kinds of docker environments. And it looks like you&#x27;re using both FUSE and NFS mounts. How does all of that work?<p>4. Is the idea that I could literally run Clickhouse or Postgres with a regatta volume as the backing store?<p>5. I have to ask - how do you think about open source here?<p>6. Can I mount on multiple servers? What are the limits there? (ie, a lambda function.)<p>I haven&#x27;t played with the so maybe doing so would help answer questions. But I&#x27;m really excited about this! I have tried using EFS for small projects in the past but - and maybe I was holding it wrong - I could not for the life of me figure out what I needed to get faster bandwidth, probably because I didn&#x27;t know how to turn the knobs correctly.</div><br/><div id="42174791" class="c"><input type="checkbox" id="c-42174791" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174697">parent</a><span>|</span><a href="#42180521">next</a><span>|</span><label class="collapse" for="c-42174791">[-]</label><label class="expand" for="c-42174791">[8 more]</label></div><br/><div class="children"><div class="content">Wow, thanks for the nice note! No questions are silly, and I&#x27;ll also note that we now have a docs site (<a href="https:&#x2F;&#x2F;docs.regattastorage.com">https:&#x2F;&#x2F;docs.regattastorage.com</a>) and feel free to email me (hleath [at] regattastorage.com) if I don&#x27;t fully address your questions.<p>&gt; If I had a local disk which was 10 GB, what happens when I try to contend with data in the 50 GB range (as in, more that could be cached locally?) Would I immediately see degradation, or thrashing, at the 10 GB mark?<p>We don&#x27;t actually do caching on <i>your instance&#x27;s</i> disk. Instead, data is cached in the Linux page cache (in memory) like a regular hard drive, and Regatta provides a durable, shared cache that automatically expands with the working set size of your application. For example, if you were trying to work with data in the 50 GiB range, Regatta would automatically cache all 50 GiB -- allowing you to access it with sub-millisecond latency.<p>&gt; Does this only work in practice on AWS instances? As in, I could run it on a different cloud, but in practice we only really get fast speeds due to running everything within AWS?<p>For now, yes -- the speed is highly dependent on latency -- which is highly dependent on distance between your instance and Regatta. Today, we are only in AWS, but we are looking to launch in other clouds by the end of the year. Shoot me an email if there&#x27;s somewhere specifically that you&#x27;re interested in.<p>&gt; I&#x27;ve always had trouble with FUSE in different kinds of docker environments. And it looks like you&#x27;re using both FUSE and NFS mounts. How does all of that work?<p>There are a couple of different questions bundled together in this. Today, Regatta exposes an NFSv3 file system that you can mount. We are working on a new protocol which will be mounted via FUSE. However, in Docker environments, we also provide a CSI driver (for use with K8s) and a Docker volume plugin (for use with just Docker) that handles the mounting for you. We haven&#x27;t released these publicly yet, so shoot me an email if you want early access.<p>&gt; Is the idea that I could literally run Clickhouse or Postgres with a regatta volume as the backing store?<p>Yes, you should be able to run a database on Regatta.<p>&gt; I have to ask - how do you think about open source here?<p>We are in the process of open sourcing all of the client code (CSI driver, mount helper, FUSE), but we don&#x27;t have plans currently to open source the server code. We see the value of Regatta in managing the infrastructure so you don&#x27;t have to, and if we release it via open-source, it would be difficult to run on your own.<p>&gt; Can I mount on multiple servers? What are the limits there? (ie, a lambda function.)<p>Yes, you can mount on multiple servers simultaneously! We haven&#x27;t specifically stress-tested the number of clients we support, but we should be good for O(100s) of mounts. Unfortunately, AWS locks down Lambda so we can&#x27;t mount arbitrary file systems in that environment specifically.<p>&gt; efs performance<p>Yes, the challenge here is specifically around the semantics of NFS itself and the latency of the EFS service. We think we have a path to solving both of these in the next month or two.</div><br/><div id="42175084" class="c"><input type="checkbox" id="c-42175084" checked=""/><div class="controls bullet"><span class="by">memset</span><span>|</span><a href="#42174697">root</a><span>|</span><a href="#42174791">parent</a><span>|</span><a href="#42177548">next</a><span>|</span><label class="collapse" for="c-42175084">[-]</label><label class="expand" for="c-42175084">[2 more]</label></div><br/><div class="children"><div class="content">Thank you for the detailed answers! Honestly, this project inspires me to work on infrastructure problems.<p>So you are saying that regatta&#x27;s own SaaS infrastructure provides the disk caching layer. So you all make sure the pipe between my AWS instance and your servers are very fast and &quot;infinitely scalable&quot;, and then the sync to S3 happens after the fact.</div><br/><div id="42175112" class="c"><input type="checkbox" id="c-42175112" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174697">root</a><span>|</span><a href="#42175084">parent</a><span>|</span><a href="#42177548">next</a><span>|</span><label class="collapse" for="c-42175112">[-]</label><label class="expand" for="c-42175112">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s exactly right!</div><br/></div></div></div></div><div id="42177548" class="c"><input type="checkbox" id="c-42177548" checked=""/><div class="controls bullet"><span class="by">gizmo</span><span>|</span><a href="#42174697">root</a><span>|</span><a href="#42174791">parent</a><span>|</span><a href="#42175084">prev</a><span>|</span><a href="#42179460">next</a><span>|</span><label class="collapse" for="c-42177548">[-]</label><label class="expand" for="c-42177548">[2 more]</label></div><br/><div class="children"><div class="content">Do I understand correctly that the data gets decrypted at your Regatta AWS instances, before the data ends up in the customer&#x27;s S3 bucket? It sounds like the SSL pipe used for NFS is terminated at Regatta servers. Can customers run the Regatta service on their own hardware?<p>Or does Regatta only have access to filesystem metadata -- enough to do POSIX stuffs like locks, mv, rm -- but the file contents themselves remain encrypted end-to-end?</div><br/><div id="42177649" class="c"><input type="checkbox" id="c-42177649" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174697">root</a><span>|</span><a href="#42177548">parent</a><span>|</span><a href="#42179460">next</a><span>|</span><label class="collapse" for="c-42177649">[-]</label><label class="expand" for="c-42177649">[1 more]</label></div><br/><div class="children"><div class="content">This is correct, we encrypt data in-transit to the Regatta servers (using TLS), and we encrypt any data that the Regatta servers are storing. Of course, when Regatta communicates with S3, that&#x27;s also encrypted with TLS (just like using the AWS SDK). However, we don&#x27;t pass the encrypted data to S3, otherwise you wouldn&#x27;t be able to read it from the bucket directly and use it in other applications!</div><br/></div></div></div></div><div id="42179460" class="c"><input type="checkbox" id="c-42179460" checked=""/><div class="controls bullet"><span class="by">0x1ceb00da</span><span>|</span><a href="#42174697">root</a><span>|</span><a href="#42174791">parent</a><span>|</span><a href="#42177548">prev</a><span>|</span><a href="#42179172">next</a><span>|</span><label class="collapse" for="c-42179460">[-]</label><label class="expand" for="c-42179460">[2 more]</label></div><br/><div class="children"><div class="content">Are you planning to support android? How? AFAIK android doesn&#x27;t have FUSE or NFS.</div><br/><div id="42179573" class="c"><input type="checkbox" id="c-42179573" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174697">root</a><span>|</span><a href="#42179460">parent</a><span>|</span><a href="#42179172">next</a><span>|</span><label class="collapse" for="c-42179573">[-]</label><label class="expand" for="c-42179573">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that I&#x27;m planning support for Android, did I mistakingly mention it somewhere?</div><br/></div></div></div></div></div></div></div></div><div id="42180521" class="c"><input type="checkbox" id="c-42180521" checked=""/><div class="controls bullet"><span class="by">daviesliu</span><span>|</span><a href="#42174697">prev</a><span>|</span><a href="#42175067">next</a><span>|</span><label class="collapse" for="c-42180521">[-]</label><label class="expand" for="c-42180521">[3 more]</label></div><br/><div class="children"><div class="content">Founder of JuiceFS here, congrats to the Launch! I&#x27;m super excited to see more people doing creative things in the using-S3-as-file-system space. When we started JuiceFS back in 2017, applied YC for 2 times but no luck.<p>We are still working hard on it, hoping that we can help people with different workloads with different tech!</div><br/><div id="42181262" class="c"><input type="checkbox" id="c-42181262" checked=""/><div class="controls bullet"><span class="by">gumbojuice</span><span>|</span><a href="#42180521">parent</a><span>|</span><a href="#42180528">next</a><span>|</span><label class="collapse" for="c-42181262">[-]</label><label class="expand" for="c-42181262">[1 more]</label></div><br/><div class="children"><div class="content">As someone who is already happily using JuuceFS, perhaps you can provide a short list of differences (conceptual and&#x2F;or technical).  Thanks for a great product.</div><br/></div></div><div id="42180528" class="c"><input type="checkbox" id="c-42180528" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42180521">parent</a><span>|</span><a href="#42181262">prev</a><span>|</span><a href="#42175067">next</a><span>|</span><label class="collapse" for="c-42180528">[-]</label><label class="expand" for="c-42180528">[1 more]</label></div><br/><div class="children"><div class="content">Wow, thanks for coming out! I hope that you&#x27;re heartened to see the number of people who immediately think of JuiceFS when they see our launch. I totally agree with you, storage is such an interesting space to work in, and I&#x27;m excited that there are so many great products out there to fit the different needs of customers.</div><br/></div></div></div></div><div id="42175067" class="c"><input type="checkbox" id="c-42175067" checked=""/><div class="controls bullet"><span class="by">mritchie712</span><span>|</span><a href="#42180521">prev</a><span>|</span><a href="#42175213">next</a><span>|</span><label class="collapse" for="c-42175067">[-]</label><label class="expand" for="c-42175067">[9 more]</label></div><br/><div class="children"><div class="content">Pretty sure we&#x27;re in your target market. We [0] currently use GCP Filestore to host DuckDB. Here&#x27;s the pricing and performance at 10 TiB. Can you give me an idea on the pricing and performance for Regatta?<p>Service Tier: Zonal<p>Location: us-central1<p>10 TiB instance at $0.35&#x2F;TiB&#x2F;hr<p>Monthly cost: $2,560.00<p>Performance Estimate:<p>Read IOPS: 92,000<p>Write IOPS: 26,000<p>Read Throughput: 2,600 MiB&#x2F;s<p>Write Throughput: 880 MiB&#x2F;s<p>0 - <a href="https:&#x2F;&#x2F;www.definite.app&#x2F;blog&#x2F;duckdb-datawarehouse" rel="nofollow">https:&#x2F;&#x2F;www.definite.app&#x2F;blog&#x2F;duckdb-datawarehouse</a></div><br/><div id="42175238" class="c"><input type="checkbox" id="c-42175238" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175067">parent</a><span>|</span><a href="#42175360">next</a><span>|</span><label class="collapse" for="c-42175238">[-]</label><label class="expand" for="c-42175238">[3 more]</label></div><br/><div class="children"><div class="content">Yes, you should be in our target market. I don&#x27;t think that I can give a cost estimate without having a good sense of what <i>percentage</i> of your data you&#x27;re actively using at any given time, but we should absolutely support the performance numbers that you&#x27;re talking about. I&#x27;d love to chat more in detail, feel free to send me a note at hleath [at] regattastorage.com.</div><br/><div id="42175518" class="c"><input type="checkbox" id="c-42175518" checked=""/><div class="controls bullet"><span class="by">mritchie712</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42175238">parent</a><span>|</span><a href="#42175360">next</a><span>|</span><label class="collapse" for="c-42175518">[-]</label><label class="expand" for="c-42175518">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll send you a note!<p>Found this in the docs:<p>&gt; By default, Regatta file systems can provide up to 10 Gbps of throughput and 10,000 IOPS across all connected clients.<p>Is that the lower bound? The 50 TiB filestore instance has 104 Gbps read through put (albeit at a relatively high price point).</div><br/><div id="42175541" class="c"><input type="checkbox" id="c-42175541" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42175518">parent</a><span>|</span><a href="#42175360">next</a><span>|</span><label class="collapse" for="c-42175541">[-]</label><label class="expand" for="c-42175541">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s just the limit that we apply to new file systems. We should be able to support your 104 Gbps of read throughput.</div><br/></div></div></div></div></div></div><div id="42175360" class="c"><input type="checkbox" id="c-42175360" checked=""/><div class="controls bullet"><span class="by">_bare_metal</span><span>|</span><a href="#42175067">parent</a><span>|</span><a href="#42175238">prev</a><span>|</span><a href="#42175213">next</a><span>|</span><label class="collapse" for="c-42175360">[-]</label><label class="expand" for="c-42175360">[5 more]</label></div><br/><div class="children"><div class="content">Out of curiosity, why not go bare metal in a managed colocation? Is that for the geographic spread? Or unpredictable load?<p>Every few months of this spend is like buying a server<p>Edit: back at my pc and checked, relevant bare metal is ~$500&#x2F;m, amortized:<p><a href="https:&#x2F;&#x2F;baremetalsavings.com&#x2F;c&#x2F;LtxKMNj" rel="nofollow">https:&#x2F;&#x2F;baremetalsavings.com&#x2F;c&#x2F;LtxKMNj</a><p>Edit 2: for 100tb..</div><br/><div id="42175534" class="c"><input type="checkbox" id="c-42175534" checked=""/><div class="controls bullet"><span class="by">mritchie712</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42175360">parent</a><span>|</span><a href="#42176875">next</a><span>|</span><label class="collapse" for="c-42175534">[-]</label><label class="expand" for="c-42175534">[1 more]</label></div><br/><div class="children"><div class="content">agreed, one month of 50 TiB is $12,800!<p>we&#x27;re using Filestore out of convenience right now, but actively exploring alternatives.</div><br/></div></div><div id="42176875" class="c"><input type="checkbox" id="c-42176875" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42175360">parent</a><span>|</span><a href="#42175534">prev</a><span>|</span><a href="#42175213">next</a><span>|</span><label class="collapse" for="c-42176875">[-]</label><label class="expand" for="c-42176875">[3 more]</label></div><br/><div class="children"><div class="content">Hiring someone who knows how to manage bare metal (with failover and stuff) may take time %)</div><br/><div id="42177892" class="c"><input type="checkbox" id="c-42177892" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42176875">parent</a><span>|</span><a href="#42175213">next</a><span>|</span><label class="collapse" for="c-42177892">[-]</label><label class="expand" for="c-42177892">[2 more]</label></div><br/><div class="children"><div class="content">You pay a datacenter to put it in a rack and add connect power and uplinks, then treat it like a big ec2 instance (minus the built-in firewall). Now you just need someone who knows how to secure an ec2 instance and run your preferred software there (with failover and stuff).<p>If you run a single-digit number of servers and replace them every 5 years you will probably never get a hardware failure. If you&#x27;re unlucky and it still happens get someone to diagnose what&#x27;s wrong, ship replacement parts to the data center and pay their tech to install them in your server.<p>Bare metal at scale is difficult. A small number of bare metal servers is easy. If your needs are average enough you can even just rent them so you don&#x27;t have capital costs and aren&#x27;t responsible for fixing hardware issues.</div><br/></div></div></div></div></div></div></div></div><div id="42175213" class="c"><input type="checkbox" id="c-42175213" checked=""/><div class="controls bullet"><span class="by">jitl</span><span>|</span><a href="#42175067">prev</a><span>|</span><a href="#42176750">next</a><span>|</span><label class="collapse" for="c-42175213">[-]</label><label class="expand" for="c-42175213">[2 more]</label></div><br/><div class="children"><div class="content">I’m very interested in this as a backing disk for SQLite&#x2F;DuckDB&#x2F;parquet, but I really want my cached reads to come straight from instance-local NVMe storage, and to have a way to “pin” and “unpin” some subdirectories from local cache.<p>Why local storage? We’re going to have multiple processes reading &amp; writing to the files and need locking &amp; shared memory semantics you can’t get w&#x2F; NFS. I could implement pin&#x2F;unpin myself in user space by copying stuff between &#x2F;mnt&#x2F;magic-nfs and &#x2F;mnt&#x2F;instance-nvme but at that point I’d just use S3 myself.<p>Any thoughts about providing a custom file system or how to assemble this out of parts on top of the NFS mount?</div><br/><div id="42175408" class="c"><input type="checkbox" id="c-42175408" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175213">parent</a><span>|</span><a href="#42176750">next</a><span>|</span><label class="collapse" for="c-42175408">[-]</label><label class="expand" for="c-42175408">[1 more]</label></div><br/><div class="children"><div class="content">Hey -- I think this is something that&#x27;s in-scope for our custom protocol that we&#x27;re working on. I&#x27;d love to chat more about your needs to make sure that we build something that will work great for you. Would you mind shooting an email to hleath [at] regattastorage.com and we can chat more?</div><br/></div></div></div></div><div id="42176750" class="c"><input type="checkbox" id="c-42176750" checked=""/><div class="controls bullet"><span class="by">convivialdingo</span><span>|</span><a href="#42175213">prev</a><span>|</span><a href="#42180611">next</a><span>|</span><label class="collapse" for="c-42176750">[-]</label><label class="expand" for="c-42176750">[5 more]</label></div><br/><div class="children"><div class="content">Wow, looks like a great product!  That&#x27;s a great idea to use NFS as the protocol.  I honestly hadn&#x27;t thought of that.<p>Perfect.<p>For IBM, I wrote a crypto filesystem that works similarly in concept, except it was a kernel filesystem.  We crypto split the blocks up into 4 parts, stored into cache. A background daemon listened to events and sync&#x27;ed blocks to S3 orchestrated with a shared journal.<p>It&#x27;s pure magic when you mount a filesystem on clean machine and all your data is &quot;just there.&quot;</div><br/><div id="42176804" class="c"><input type="checkbox" id="c-42176804" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176750">parent</a><span>|</span><a href="#42180611">next</a><span>|</span><label class="collapse" for="c-42176804">[-]</label><label class="expand" for="c-42176804">[4 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s pure magic when you mount a filesystem on clean machine and all your data is &quot;just there.&quot;<p>I totally agree! I am hoping that Regatta can power a future where teams don&#x27;t need more than ~8 GiB of local storage for their operating system, and can store the rest on something like Regatta to get rid of the waste of overprovisioned block volumes.</div><br/><div id="42177044" class="c"><input type="checkbox" id="c-42177044" checked=""/><div class="controls bullet"><span class="by">lijok</span><span>|</span><a href="#42176750">root</a><span>|</span><a href="#42176804">parent</a><span>|</span><a href="#42180611">next</a><span>|</span><label class="collapse" for="c-42177044">[-]</label><label class="expand" for="c-42177044">[3 more]</label></div><br/><div class="children"><div class="content">That would sell like hot cakes to the public sector.</div><br/><div id="42177636" class="c"><input type="checkbox" id="c-42177636" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176750">root</a><span>|</span><a href="#42177044">parent</a><span>|</span><a href="#42180422">next</a><span>|</span><label class="collapse" for="c-42177636">[-]</label><label class="expand" for="c-42177636">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s hope so, I&#x27;d love to help teams take storage infrastructure management off of their plate! If you&#x27;re in the public sector and interested in trying out Regatta, please shoot me an email at hleath [at] regattastorage.com.</div><br/></div></div><div id="42180422" class="c"><input type="checkbox" id="c-42180422" checked=""/><div class="controls bullet"><span class="by">shaklee3</span><span>|</span><a href="#42176750">root</a><span>|</span><a href="#42177044">parent</a><span>|</span><a href="#42177636">prev</a><span>|</span><a href="#42180611">next</a><span>|</span><label class="collapse" for="c-42180422">[-]</label><label class="expand" for="c-42180422">[1 more]</label></div><br/><div class="children"><div class="content">The public sector is typically air-gapped, so not really.</div><br/></div></div></div></div></div></div></div></div><div id="42180611" class="c"><input type="checkbox" id="c-42180611" checked=""/><div class="controls bullet"><span class="by">mbrt</span><span>|</span><a href="#42176750">prev</a><span>|</span><a href="#42179784">next</a><span>|</span><label class="collapse" for="c-42180611">[-]</label><label class="expand" for="c-42180611">[5 more]</label></div><br/><div class="children"><div class="content">Wow, coincidentally I posted GlassBD (<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42164058">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42164058</a>) a couple of days ago. Making S3 strongly consistent is not trivial, so I&#x27;m curious about how you achieved this.<p>If the caching layer can return success before writing through to s3, it means you built a strongly consistent distributed in memory database.<p>Or, the consistency guarantee is actually less, or data is partitioned and cannot be quickly shared across clients.<p>I&#x27;m really curious to understand how this was implemented.</div><br/><div id="42180672" class="c"><input type="checkbox" id="c-42180672" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42180611">parent</a><span>|</span><a href="#42179784">next</a><span>|</span><label class="collapse" for="c-42180672">[-]</label><label class="expand" for="c-42180672">[4 more]</label></div><br/><div class="children"><div class="content">Hey, thanks for reaching out. The caching layer does return success before writing to S3 -- that&#x27;s how we get good performance for all operations, including those which aren&#x27;t possible to do in S3 efficiently (such as random writes, renames, or file appends). Because the caching layer is durable, we can safely asynchronously apply these changes to the S3 bucket. Most operations appear in the S3 bucket within a minute!</div><br/><div id="42180782" class="c"><input type="checkbox" id="c-42180782" checked=""/><div class="controls bullet"><span class="by">mbrt</span><span>|</span><a href="#42180611">root</a><span>|</span><a href="#42180672">parent</a><span>|</span><a href="#42179784">next</a><span>|</span><label class="collapse" for="c-42180782">[-]</label><label class="expand" for="c-42180782">[3 more]</label></div><br/><div class="children"><div class="content">Very nice, I like the approach. I assume data is partitioned and each file is handled by an elected leader? If data is replicated, you still need a consensus algorithm on updates.<p>How are concurrent updates to the same file handled? Either only one client can open in write at any one time, or you need fencing tokens.</div><br/><div id="42180873" class="c"><input type="checkbox" id="c-42180873" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42180611">root</a><span>|</span><a href="#42180782">parent</a><span>|</span><a href="#42179784">next</a><span>|</span><label class="collapse" for="c-42180873">[-]</label><label class="expand" for="c-42180873">[2 more]</label></div><br/><div class="children"><div class="content">Without getting too much into internals which could change at any time, yes. You have to replicate, partition, and serve consensus over data to achieve high-durability and availability.<p>For concurrent updates, the standard practice for remote file systems is to use file locking to coordinate concurrent writes. Otherwise, NFS doesn&#x27;t have any guarantees about WRITE operation ordering. If you&#x27;re talking about concurrent writes which occur from NFS and S3 simultaneously, this leads to undefined behavior. We think that this is okay if we do a good job at detecting and alerting the user if this occurs because we don&#x27;t think that there are applications currently written to do this kind of simultaneous data editing (because Regatta didn&#x27;t exist yet).</div><br/><div id="42180979" class="c"><input type="checkbox" id="c-42180979" checked=""/><div class="controls bullet"><span class="by">mbrt</span><span>|</span><a href="#42180611">root</a><span>|</span><a href="#42180873">parent</a><span>|</span><a href="#42179784">next</a><span>|</span><label class="collapse" for="c-42180979">[-]</label><label class="expand" for="c-42180979">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the details!<p>Consistency at the individual file can be guaranteed this way, but I don&#x27;t think this works across multiple files (as you need a global total order of operations). In any case, this is a pragmatic solution, and I like the tradeoffs. Comparing against NFS rather than Spanner seems the right way to look at it.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42179784" class="c"><input type="checkbox" id="c-42179784" checked=""/><div class="controls bullet"><span class="by">zX41ZdbW</span><span>|</span><a href="#42180611">prev</a><span>|</span><a href="#42180315">next</a><span>|</span><label class="collapse" for="c-42179784">[-]</label><label class="expand" for="c-42179784">[2 more]</label></div><br/><div class="children"><div class="content">That is interesting, but I haven&#x27;t read how it is implemented yet.<p>The hard part is a cache layer with immediate consistency. It likely requires RAFT (or, otherwise, works incorrectly). Integration of this cache layer with S3 (offloading cold data to S3) is easy (not interesting).<p>It should not be compared to s3fs, mountpoint, geesefs, etc., because they lack consistency and also slow and also don&#x27;t support full filesystem semantics, and break often.<p>It could be compared with AWS EFS. Which is also slow (but I didn&#x27;t try to tune it up to maximum numbers).<p>For ClickHouse, this system is unneeded because ClickHouse is already distributed (it supports full replication or shared storage + cache), and it does not require full filesystem semantics (it pairs with blob storages nicely).</div><br/><div id="42179882" class="c"><input type="checkbox" id="c-42179882" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42179784">parent</a><span>|</span><a href="#42180315">next</a><span>|</span><label class="collapse" for="c-42179882">[-]</label><label class="expand" for="c-42179882">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the note, great to hear from you! I think that what Clickhouse does is great, and I expect that more applications want to take advantage of the low prices of S3 cold storage without needing to build their own application-level abstractions. I&#x27;m hopeful that this allows more of these next-generation serverless data products to exist.</div><br/></div></div></div></div><div id="42180315" class="c"><input type="checkbox" id="c-42180315" checked=""/><div class="controls bullet"><span class="by">IgorPartola</span><span>|</span><a href="#42179784">prev</a><span>|</span><a href="#42176558">next</a><span>|</span><label class="collapse" for="c-42180315">[-]</label><label class="expand" for="c-42180315">[2 more]</label></div><br/><div class="children"><div class="content">Is this meaningfully different from <a href="https:&#x2F;&#x2F;github.com&#x2F;s3ql&#x2F;s3ql">https:&#x2F;&#x2F;github.com&#x2F;s3ql&#x2F;s3ql</a> ?<p>S3 semantics are generally fairly terrible for file storage (no atomic move&#x2F;rename is just one example) but using it as block storage a la ZFS is quite clever.</div><br/><div id="42180411" class="c"><input type="checkbox" id="c-42180411" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42180315">parent</a><span>|</span><a href="#42176558">next</a><span>|</span><label class="collapse" for="c-42180411">[-]</label><label class="expand" for="c-42180411">[1 more]</label></div><br/><div class="children"><div class="content">Hey, thanks for the question. From what I can tell (and this could be wrong), but it looks like s3ql is using S3 as a block layer. Regatta, on the other hand, allows you to read and write files in their native format. I agree that it&#x27;s harder to implement than just using S3 for block storage, but I think that it unlocks a lot of potential use cases for customers. With Regatta, we make these semantics performant, which is a huge improvement on the prior art.</div><br/></div></div></div></div><div id="42176558" class="c"><input type="checkbox" id="c-42176558" checked=""/><div class="controls bullet"><span class="by">ragulpr</span><span>|</span><a href="#42180315">prev</a><span>|</span><a href="#42176756">next</a><span>|</span><label class="collapse" for="c-42176558">[-]</label><label class="expand" for="c-42176558">[2 more]</label></div><br/><div class="children"><div class="content">Love this idea! Biggest hurdle though have been to have predictable Auth&amp;IO across multiple Python&#x2F;Scala versions and all other things (Spark, orchestrators, CLI&#x27;s of teams of varying types of OS etc etc) add to that access logs.<p>SF3s&#x2F;boto&#x2F;botocore versions x Scala&#x2F;Spark x parquet x iceberg x k8s etc readers own assumptions makes reading from S3 alone a maintenance and compatibility nightmare.<p>Will the mounted system _really_ be accessible as local fs and seen as such to all running processes? No surprises? No need for python specific filesystem like S3Fs?<p>If so then you will win 100% I wouldn&#x27;t even care about speed&#x2F;cost if it&#x27;s up to par with s3</div><br/><div id="42176673" class="c"><input type="checkbox" id="c-42176673" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176558">parent</a><span>|</span><a href="#42176756">next</a><span>|</span><label class="collapse" for="c-42176673">[-]</label><label class="expand" for="c-42176673">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, that&#x27;s exactly right. I had some... experiences with Spark recently, that convinced me that this is something that could really help. I also really like the idea that organizations can continue to use S3 as the source of truth for their data (as you mention, it means that you can continue to use Access Logs, which would capture <i>all</i> usage of your S3 bucket across your applications).<p>&gt; Will the mounted system _really_ be accessible as local fs and seen as such to all running processes? No surprises? No need for python specific filesystem like S3Fs?<p>Ha, well it depends on what you mean by surprises. We won&#x27;t have a Python-specific file system. Our client is going to come in two flavors. Today, you can mount Regatta over NFSv3 (which we wrap in TLS to make it secure). This works for some workloads, but doesn&#x27;t provide like-for-like performance with EBS. Over the next month, we plan to release the &quot;custom protocol&quot; that I wrote about above, that we expect to send to customers in the form of a FUSE file system.<p>Either way, it should be one package, you shouldn&#x27;t need to worry about versioning, and it will appear as a real, local file system. :D</div><br/></div></div></div></div><div id="42176756" class="c"><input type="checkbox" id="c-42176756" checked=""/><div class="controls bullet"><span class="by">random3</span><span>|</span><a href="#42176558">prev</a><span>|</span><a href="#42180894">next</a><span>|</span><label class="collapse" for="c-42176756">[-]</label><label class="expand" for="c-42176756">[3 more]</label></div><br/><div class="children"><div class="content">In (March?) 2007 (correction 2008) myself and two other engineers in front of Bruce Chizen - Adobe&#x27;s CEO in a small conference room in Bucharest demoed a photo taken with an iPhone automagically showing as a file on a Mac. I implemented the local FUSE talking to Ozzy - Adobe&#x27;s distributed object store back then, using an equivalent of a Linux inode structure. It worked like a charm and if I remember correctly it took us a few days to build it. It was a success just as much as Adobe&#x27;s later choices around <a href="http:&#x2F;&#x2F;Photoshop.com" rel="nofollow">http:&#x2F;&#x2F;Photoshop.com</a> were a huge failure. A few months later Dropbox launched.<p>That kickstarted about a decade in (actual) research and development led by my team which positioned the Bucharest center as one of the most prolific centers in distributed systems within Adobe and of Adobe within Romania.<p>But I didn&#x27;t come up with the concept, it was Richard Jones that inspired us with the Gmail drive that used FUSE with gmail attachments back in 2004 when I got my first while still in college  <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GMail_Drive" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GMail_Drive</a>. I guess I&#x27;m old, but I find it funny to see Launch HN: Regatta Storage (YC F24) – Turn S3 into a local-like, POSIX cloud FS</div><br/><div id="42176840" class="c"><input type="checkbox" id="c-42176840" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176756">parent</a><span>|</span><a href="#42180894">next</a><span>|</span><label class="collapse" for="c-42176840">[-]</label><label class="expand" for="c-42176840">[2 more]</label></div><br/><div class="children"><div class="content">The funny thing about storage is that all of the problems are the same! Ultimately, there is no problem that cannot be solved with caching, journaling, write-ahead logging, etc. I think what makes the problem space so interesting is how a million different products can make a million different trade offs with these tools to deliver on their customer needs. File systems are awesome.</div><br/><div id="42177269" class="c"><input type="checkbox" id="c-42177269" checked=""/><div class="controls bullet"><span class="by">random3</span><span>|</span><a href="#42176756">root</a><span>|</span><a href="#42176840">parent</a><span>|</span><a href="#42180894">next</a><span>|</span><label class="collapse" for="c-42177269">[-]</label><label class="expand" for="c-42177269">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The funny thing about storage is that all of the problems are the same!<p>they <i>are</i> all the same and they are all more than what would at the surface seem that it&#x27;s &quot;just files&quot; the whole OS, especially Linux&#x2F;UNIX is &quot;just files&quot; and if you look deeper at databases you can see how it boils down to the file formats (something that was visible with LevelDB but maybe less so with RocksDB, I guess)</div><br/></div></div></div></div></div></div><div id="42180894" class="c"><input type="checkbox" id="c-42180894" checked=""/><div class="controls bullet"><span class="by">highwaylights</span><span>|</span><a href="#42176756">prev</a><span>|</span><a href="#42174866">next</a><span>|</span><label class="collapse" for="c-42180894">[-]</label><label class="expand" for="c-42180894">[1 more]</label></div><br/><div class="children"><div class="content">I have a feeling Amazon is about to throw a big bag of money at you and that this will be the fastest acquisition in HN history.  Congratulations on your successful launch!</div><br/></div></div><div id="42174866" class="c"><input type="checkbox" id="c-42174866" checked=""/><div class="controls bullet"><span class="by">mikecwang</span><span>|</span><a href="#42180894">prev</a><span>|</span><a href="#42176227">next</a><span>|</span><label class="collapse" for="c-42174866">[-]</label><label class="expand" for="c-42174866">[7 more]</label></div><br/><div class="children"><div class="content">Does it mean I can use Lambda + SQLite + Regatta to build a real pay-as-you-go ACID SQL storage?<p>Edit: an production-ready (high durability) ACID SQL storage</div><br/><div id="42174881" class="c"><input type="checkbox" id="c-42174881" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174866">parent</a><span>|</span><a href="#42175822">next</a><span>|</span><label class="collapse" for="c-42174881">[-]</label><label class="expand" for="c-42174881">[3 more]</label></div><br/><div class="children"><div class="content">Yes! This is my expectation. Lots of the big companies have already done this with in-house architecture. With Regatta, we want to democratize building stateless applications that can take advantage of the low-cost storage of S3.</div><br/><div id="42181154" class="c"><input type="checkbox" id="c-42181154" checked=""/><div class="controls bullet"><span class="by">hades32</span><span>|</span><a href="#42174866">root</a><span>|</span><a href="#42174881">parent</a><span>|</span><a href="#42174921">next</a><span>|</span><label class="collapse" for="c-42181154">[-]</label><label class="expand" for="c-42181154">[1 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t that limit the concurrency of the lambdas to 1? Since they would hold a lock on the db file</div><br/></div></div><div id="42174921" class="c"><input type="checkbox" id="c-42174921" checked=""/><div class="controls bullet"><span class="by">mikecwang</span><span>|</span><a href="#42174866">root</a><span>|</span><a href="#42174881">parent</a><span>|</span><a href="#42181154">prev</a><span>|</span><a href="#42175822">next</a><span>|</span><label class="collapse" for="c-42174921">[-]</label><label class="expand" for="c-42174921">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s some real tech in YC these days!</div><br/></div></div></div></div><div id="42175822" class="c"><input type="checkbox" id="c-42175822" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#42174866">parent</a><span>|</span><a href="#42174881">prev</a><span>|</span><a href="#42176227">next</a><span>|</span><label class="collapse" for="c-42175822">[-]</label><label class="expand" for="c-42175822">[3 more]</label></div><br/><div class="children"><div class="content">Curious as to why you would want to build that yourself when so many solutions already exist (Supabase, NeonDB, AWS Aurora or RDS, etc.)?</div><br/><div id="42175971" class="c"><input type="checkbox" id="c-42175971" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174866">root</a><span>|</span><a href="#42175822">parent</a><span>|</span><a href="#42176227">next</a><span>|</span><label class="collapse" for="c-42175971">[-]</label><label class="expand" for="c-42175971">[2 more]</label></div><br/><div class="children"><div class="content">One of my hopes for Regatta is that we&#x27;re able to power the next generation of these data platforms. These things work because the designers had specialized storage knowledge that allowed them to carefully build serverless data products. I hope that Regatta is generic enough to allow anyone to build a serverless data product moving forward, without having to think about their storage infrastructure.</div><br/><div id="42177203" class="c"><input type="checkbox" id="c-42177203" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#42174866">root</a><span>|</span><a href="#42175971">parent</a><span>|</span><a href="#42176227">next</a><span>|</span><label class="collapse" for="c-42177203">[-]</label><label class="expand" for="c-42177203">[1 more]</label></div><br/><div class="children"><div class="content">That makes a lot of sense.  If you eliminate the need for storage expertise the problem becomes a lot easier!<p>BTW I sent you an email.</div><br/></div></div></div></div></div></div></div></div><div id="42176227" class="c"><input type="checkbox" id="c-42176227" checked=""/><div class="controls bullet"><span class="by">hitekker</span><span>|</span><a href="#42174866">prev</a><span>|</span><a href="#42175379">next</a><span>|</span><label class="collapse" for="c-42176227">[-]</label><label class="expand" for="c-42176227">[2 more]</label></div><br/><div class="children"><div class="content">This looks quite compelling.<p>But it&#x27;s not clear how it handles file update conflicts.
For example: if User A updates File X on one computer, and User B  updates File X on another computer, what does the final file look like in S3?</div><br/><div id="42176263" class="c"><input type="checkbox" id="c-42176263" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176227">parent</a><span>|</span><a href="#42175379">next</a><span>|</span><label class="collapse" for="c-42176263">[-]</label><label class="expand" for="c-42176263">[1 more]</label></div><br/><div class="children"><div class="content">Hey there, our file system is strongly consistent for all connected file system clients. For example, if User A and User B are both connected via Regatta, then this works like any other NFS file system (in that they can use file locks, atomic renames or other techniques to ensure that one write wins). However, if User A and User B are accessing the data through different protocols (for example User A is using Regatta and User B is accessing the data through S3), then it&#x27;s possible to get undefined behavior by attempting to simultaneously update the same piece of data from both places. We think that these applications are rare, and (almost by definition) likely don&#x27;t exist right now. For the most part, customers use file storage as a &quot;stage&quot; in a broader workflow (for example, customers may ingest data through S3 and then process it on a file system), and that is totally consistent.</div><br/></div></div></div></div><div id="42175379" class="c"><input type="checkbox" id="c-42175379" checked=""/><div class="controls bullet"><span class="by">debarshri</span><span>|</span><a href="#42176227">prev</a><span>|</span><a href="#42174305">next</a><span>|</span><label class="collapse" for="c-42175379">[-]</label><label class="expand" for="c-42175379">[7 more]</label></div><br/><div class="children"><div class="content">There are quite some noteworthy alternatives like s3fs, rclone, goofys etc.</div><br/><div id="42175395" class="c"><input type="checkbox" id="c-42175395" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175379">parent</a><span>|</span><a href="#42174305">next</a><span>|</span><label class="collapse" for="c-42175395">[-]</label><label class="expand" for="c-42175395">[6 more]</label></div><br/><div class="children"><div class="content">This is accurate! A lot of people have spent a lot of time trying to build a good file system abstraction on cheap, S3 storage. However, Regatta differs from these solutions in two important ways. First, Regatta is a shared, durable caching layer that sits between your instances and S3. This means that Regatta is able to efficiently perform operations (like directory renames) and provide strong consistency to other file system clients (whereas s3fs and other FUSE file systems would need to actually perform those operations in S3 for other clients to see the output). Secondly, Regatta is designed to support all file system operations. This means that you can do file locking, random writes, appends, and renames -- even when they aren&#x27;t efficient to perform on S3.</div><br/><div id="42175570" class="c"><input type="checkbox" id="c-42175570" checked=""/><div class="controls bullet"><span class="by">aidos</span><span>|</span><a href="#42175379">root</a><span>|</span><a href="#42175395">parent</a><span>|</span><a href="#42174305">next</a><span>|</span><label class="collapse" for="c-42175570">[-]</label><label class="expand" for="c-42175570">[5 more]</label></div><br/><div class="children"><div class="content">Super interesting product. I have a couple of questions:<p>In terms of storing in s3 - is that in your buckets? Sound like the plan is to run the caching on your infrastructure, are there plans to allow customers to run those instances themselves?<p>Presumably the format within s3 is your own bespoke format? What does the migration strategy look like for people looking to move into or out of your infrastructure? They effectively pull everything down from their s3 to the local “filesystem”?</div><br/><div id="42175591" class="c"><input type="checkbox" id="c-42175591" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175379">root</a><span>|</span><a href="#42175570">parent</a><span>|</span><a href="#42174305">next</a><span>|</span><label class="collapse" for="c-42175591">[-]</label><label class="expand" for="c-42175591">[4 more]</label></div><br/><div class="children"><div class="content">I love this because it allows me to highlight the parts of the system that I&#x27;m most excited about. The Regatta caching runs on our infrastructure, but it connects to buckets that our customers control. We read and write data into the customer&#x27;s bucket in a regular, native (not bespoke) format -- so you can connect a Regatta file system directly to a bucket that already exists, with data in it, and use that data from a file system without any data migration!</div><br/><div id="42176644" class="c"><input type="checkbox" id="c-42176644" checked=""/><div class="controls bullet"><span class="by">aidos</span><span>|</span><a href="#42175379">root</a><span>|</span><a href="#42175591">parent</a><span>|</span><a href="#42176352">next</a><span>|</span><label class="collapse" for="c-42176644">[-]</label><label class="expand" for="c-42176644">[2 more]</label></div><br/><div class="children"><div class="content">Oh interesting! So you map exactly to the structure in s3? It’s like fuse backed by s3 with good performance?</div><br/><div id="42177662" class="c"><input type="checkbox" id="c-42177662" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175379">root</a><span>|</span><a href="#42176644">parent</a><span>|</span><a href="#42176352">next</a><span>|</span><label class="collapse" for="c-42177662">[-]</label><label class="expand" for="c-42177662">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s exactly right -- I like to think that we deliver on the promise of those open-source S3 adapters. We provide enterprise-grade performance.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42174305" class="c"><input type="checkbox" id="c-42174305" checked=""/><div class="controls bullet"><span class="by">koolba</span><span>|</span><a href="#42175379">prev</a><span>|</span><a href="#42180323">next</a><span>|</span><label class="collapse" for="c-42174305">[-]</label><label class="expand" for="c-42174305">[8 more]</label></div><br/><div class="children"><div class="content">Neat stuff. I think everybody with an interest in NFS has toyed with this idea at some point.<p>&gt; Under the hood, customers mount a Regatta file system by connecting to our fleet of caching instances over NFSv3 (soon, our custom protocol). Our instances then connect to the customer’s S3 bucket on the backend, and provide sub-millisecond cached-read and write performance. This durable cache allows us to provide a strongly consistent, efficient view of the file system to all connected file clients. We can perform challenging operations (like directory renaming) quickly and durably, while they asynchronously propagate to the S3 bucket.<p>How do you handle the cache server crashing before syncing to S3? Do the cache servers have local disk as well?<p>Ditto for how to handle intermittent S3 availability issues?<p>What are the fsync guarantees for file append operations and directories?</div><br/><div id="42174365" class="c"><input type="checkbox" id="c-42174365" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174305">parent</a><span>|</span><a href="#42180323">next</a><span>|</span><label class="collapse" for="c-42174365">[-]</label><label class="expand" for="c-42174365">[7 more]</label></div><br/><div class="children"><div class="content">Thanks for the question!<p>&gt; How do you handle the cache server crashing before syncing to S3? Do the cache servers have local disk as well?<p>Our caching layer is highly durable, which is (in my opinion) the key for doing this kind of staging. This means that once a write is complete to Regatta, we guarantee that it will eventually complete on S3.<p>For this reason, server crashes and intermittent S3 availability issues are not a problem because we have the writes stored safely.<p>&gt; What are the fsync guarantees for file append operations and directories?<p>We have strong, read-after-write consistency for all connected file system clients -- including for operations which aren&#x27;t possible to perform on S3 efficiently (such as renames, appends, etc). We asynchronously push those writes to S3, so there may be a few minutes before you can access them directly from the bucket. But, during this time, the file system interface will always reflect the up-to-date view.</div><br/><div id="42175912" class="c"><input type="checkbox" id="c-42175912" checked=""/><div class="controls bullet"><span class="by">the_duke</span><span>|</span><a href="#42174305">root</a><span>|</span><a href="#42174365">parent</a><span>|</span><a href="#42175879">next</a><span>|</span><label class="collapse" for="c-42175912">[-]</label><label class="expand" for="c-42175912">[2 more]</label></div><br/><div class="children"><div class="content">So, I assume you use a journal in the cache server.<p>A few related questions:<p>* Do you use a single leader for a specific file system, or do you have a cluster solution with consensus to enable scaling&#x2F;redundancy?<p>* How do you guarantee read-after-write consistency? Do you stream the journal to all clients and wait for them to ack before the write finishes? Or at least wait for everyone to ack the latest revisions for files, while the content is streamed out separately&#x2F;requested on demand?<p>* If the above is true, I assume this is strictly viable for single-DC usage due to latency? Do you support different mount options for different consistency guarantees?</div><br/><div id="42175986" class="c"><input type="checkbox" id="c-42175986" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174305">root</a><span>|</span><a href="#42175912">parent</a><span>|</span><a href="#42175879">next</a><span>|</span><label class="collapse" for="c-42175986">[-]</label><label class="expand" for="c-42175986">[1 more]</label></div><br/><div class="children"><div class="content">These are questions that are super specific to our implementation, that I&#x27;m hesitant to share publicly because they could change any at any time. I can share that we&#x27;re designed to horizontally scale the performance of each file system, and our custom protocol will enable Lustre-like scale out performance. As for single- vs. multi-DC, I think that you&#x27;d be surprised at how much latency budget there is (a cross-DC round trip in AWS can be anywhere from 200us-700us, and EBS gp3 latencies are around 1000us).</div><br/></div></div></div></div><div id="42175879" class="c"><input type="checkbox" id="c-42175879" checked=""/><div class="controls bullet"><span class="by">koolba</span><span>|</span><a href="#42174305">root</a><span>|</span><a href="#42174365">parent</a><span>|</span><a href="#42175912">prev</a><span>|</span><a href="#42174934">next</a><span>|</span><label class="collapse" for="c-42175879">[-]</label><label class="expand" for="c-42175879">[2 more]</label></div><br/><div class="children"><div class="content">Is it fair to say this is best suited for small files that will be written infrequently?<p>There’s no partial write for s3 so editing a small range of a 1 GiB file would repeatedly upload the full file to the backing s3 right?<p>Or is the s3 representation not the same hierarchy as the presented mount point? (ie something opaque like a log structured &#x2F; append only chunked list)</div><br/><div id="42176003" class="c"><input type="checkbox" id="c-42176003" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174305">root</a><span>|</span><a href="#42175879">parent</a><span>|</span><a href="#42174934">next</a><span>|</span><label class="collapse" for="c-42176003">[-]</label><label class="expand" for="c-42176003">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s hard to define &quot;best&quot;, and in many cases, the answers to these questions depend heavily on the workload and the caching parameters (how long do we wait before flushing to S3, etc). We are designed to provide good file system performance, even if customers are repeatedly writing small pieces of data to a 1 GiB file, so &quot;best&quot; in this case is a question of whether or not it&#x27;s cost efficient.</div><br/></div></div></div></div><div id="42174934" class="c"><input type="checkbox" id="c-42174934" checked=""/><div class="controls bullet"><span class="by">paulgb</span><span>|</span><a href="#42174305">root</a><span>|</span><a href="#42174365">parent</a><span>|</span><a href="#42175879">prev</a><span>|</span><a href="#42180323">next</a><span>|</span><label class="collapse" for="c-42174934">[-]</label><label class="expand" for="c-42174934">[2 more]</label></div><br/><div class="children"><div class="content">Congrats on the launch, this is really cool! Is the durable cache an attached disk, or are you using a separate AWS product for that?</div><br/><div id="42174959" class="c"><input type="checkbox" id="c-42174959" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174305">root</a><span>|</span><a href="#42174934">parent</a><span>|</span><a href="#42180323">next</a><span>|</span><label class="collapse" for="c-42174959">[-]</label><label class="expand" for="c-42174959">[1 more]</label></div><br/><div class="children"><div class="content">Without getting too much into the details of the system, our durable cache is designed for 5 9s of durability (and we&#x27;re working on a version that will provide 11 9s of durability soon). You can&#x27;t achieve those durability numbers on a single attached NVMe device without some kind of replication.</div><br/></div></div></div></div></div></div></div></div><div id="42180323" class="c"><input type="checkbox" id="c-42180323" checked=""/><div class="controls bullet"><span class="by">amitizle</span><span>|</span><a href="#42174305">prev</a><span>|</span><a href="#42174818">next</a><span>|</span><label class="collapse" for="c-42180323">[-]</label><label class="expand" for="c-42180323">[2 more]</label></div><br/><div class="children"><div class="content">Why are these solutions always using NFS?
I&#x27;m asking out of curiosity, not judgement.<p>I&#x27;ve looked for a solution to write many small files fast (safely). Think about cloning thr Linux kernel git repo.
Whatever I tested, the NFS protocol was always a bottleneck.</div><br/><div id="42180421" class="c"><input type="checkbox" id="c-42180421" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42180323">parent</a><span>|</span><a href="#42174818">next</a><span>|</span><label class="collapse" for="c-42180421">[-]</label><label class="expand" for="c-42180421">[1 more]</label></div><br/><div class="children"><div class="content">We choose NFS purely because it&#x27;s the fastest way to get broad compatibility with most operating systems (NFSv3, for example is supported on both Linux and Windows). However, I have great news for you! We&#x27;re simultaneously working on a custom protocol (over FUSE today) that is going to solve the small file problem for things like cloning the Linux kernel git repo. You can actually see in our demo video (<a href="https:&#x2F;&#x2F;youtu.be&#x2F;xh1q5p7E4JY?feature=shared&amp;t=170" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;xh1q5p7E4JY?feature=shared&amp;t=170</a>) that we untar the Linux kernel on Regatta in under 12 seconds. We&#x27;re hopeful that this performance makes file storage useful for a broader set of workloads.</div><br/></div></div></div></div><div id="42174818" class="c"><input type="checkbox" id="c-42174818" checked=""/><div class="controls bullet"><span class="by">count</span><span>|</span><a href="#42180323">prev</a><span>|</span><a href="#42179324">next</a><span>|</span><label class="collapse" for="c-42174818">[-]</label><label class="expand" for="c-42174818">[7 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see any other question about it, so maybe I just missed the obvious answer, but how do you handle POSIX ACLs?  If the data is stored as an object in S3, but exposed via filesystem, where are you keeping (if at all?) the filesystem ACLs and metadata?<p>Also, NFSv3 and not 4?</div><br/><div id="42174874" class="c"><input type="checkbox" id="c-42174874" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174818">parent</a><span>|</span><a href="#42179324">next</a><span>|</span><label class="collapse" for="c-42174874">[-]</label><label class="expand" for="c-42174874">[6 more]</label></div><br/><div class="children"><div class="content">Great call out. Some kinds of data, like ACLs and specific kinds of metadata, don&#x27;t live in S3. Full disclosure, we don&#x27;t support ACLs today (but plan to soon). We keep file system metadata in the durable cache. For some files (where users haven&#x27;t changed permissions, etc), we are able to release that cached metadata when the file is no longer in use. For other files (where permissions have been changed by the user), that metadata must live in the cache long-term.<p>We selected NFSv3 due to it&#x27;s broad compatibility with different compute environments. For example, Windows has an NFSv3 client in it, but doesn&#x27;t have an NFSv4 client. There are lots of enterprise workloads which needs simultaneous access to file data from both Windows and Linux, and supporting NFSv3 was the easiest path to support those workloads.</div><br/><div id="42177171" class="c"><input type="checkbox" id="c-42177171" checked=""/><div class="controls bullet"><span class="by">secabeen</span><span>|</span><a href="#42174818">root</a><span>|</span><a href="#42174874">parent</a><span>|</span><a href="#42175271">next</a><span>|</span><label class="collapse" for="c-42177171">[-]</label><label class="expand" for="c-42177171">[3 more]</label></div><br/><div class="children"><div class="content">Do you pay for metadata accesses?  Does running a `find` across the filesystem cost anything?  What about system calls that don&#x27;t transfer data?  Can I move or rename a file without paying to copy and then delete the associated S3 object?</div><br/><div id="42177334" class="c"><input type="checkbox" id="c-42177334" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174818">root</a><span>|</span><a href="#42177171">parent</a><span>|</span><a href="#42175271">next</a><span>|</span><label class="collapse" for="c-42177334">[-]</label><label class="expand" for="c-42177334">[2 more]</label></div><br/><div class="children"><div class="content">Today, we only charge for cache usage (storage) and data transfer between Regatta and S3. If your metadata access doesn&#x27;t require transfer to S3, then it doesn&#x27;t cost anything! However, renames <i>do</i> require transfer to S3 (because we have to move the object on the backend).</div><br/><div id="42181161" class="c"><input type="checkbox" id="c-42181161" checked=""/><div class="controls bullet"><span class="by">hades32</span><span>|</span><a href="#42174818">root</a><span>|</span><a href="#42177334">parent</a><span>|</span><a href="#42175271">next</a><span>|</span><label class="collapse" for="c-42181161">[-]</label><label class="expand" for="c-42181161">[1 more]</label></div><br/><div class="children"><div class="content">does that mean you pay for the storage twice (i.e. S3 and Regatta) or is the cache size tunable?</div><br/></div></div></div></div></div></div><div id="42175271" class="c"><input type="checkbox" id="c-42175271" checked=""/><div class="controls bullet"><span class="by">count</span><span>|</span><a href="#42174818">root</a><span>|</span><a href="#42174874">parent</a><span>|</span><a href="#42177171">prev</a><span>|</span><a href="#42179324">next</a><span>|</span><label class="collapse" for="c-42175271">[-]</label><label class="expand" for="c-42175271">[2 more]</label></div><br/><div class="children"><div class="content">Thanks, I keep hoping someone comes up with some magic :)<p>Is the intent to run this in-vpc?<p>And how do you differentiate from AWS Storage Gateway?</div><br/><div id="42175368" class="c"><input type="checkbox" id="c-42175368" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174818">root</a><span>|</span><a href="#42175271">parent</a><span>|</span><a href="#42179324">next</a><span>|</span><label class="collapse" for="c-42175368">[-]</label><label class="expand" for="c-42175368">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to hear more about what you&#x27;re excited to do when the magic arrives. :D<p>We are running it as a managed SaaS, so our customers connect to the caching layer that runs in the Regatta VPC. This allows us to manage the infrastructure for them and keep costs low.<p>Storage Gateway is an interesting product, and I worked closely with that team for several years -- so mad respect for them. It was designed to be an appliance that you run on servers in your own data center (of course, many customers now deploy it to EC2). Because of this, it&#x27;s designed to operate in an environment with &quot;finite storage&quot; -- for example, different workload pattterns can thrash the cache, which results in poor performance to clients, and it&#x27;s not designed to run in a high-availability cluster in the cloud. Regatta solves these problems with durable cache storage that&#x27;s safe to data in long-term, and is designed for high-availability.</div><br/></div></div></div></div></div></div></div></div><div id="42179324" class="c"><input type="checkbox" id="c-42179324" checked=""/><div class="controls bullet"><span class="by">jmspring</span><span>|</span><a href="#42174818">prev</a><span>|</span><a href="#42176823">next</a><span>|</span><label class="collapse" for="c-42179324">[-]</label><label class="expand" for="c-42179324">[2 more]</label></div><br/><div class="children"><div class="content">I wish you luck, having looked at doing something similar years back, I don’t see the market.  In the case of what I was involved in, it pivoted to enterprise backup.</div><br/><div id="42179894" class="c"><input type="checkbox" id="c-42179894" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42179324">parent</a><span>|</span><a href="#42176823">next</a><span>|</span><label class="collapse" for="c-42179894">[-]</label><label class="expand" for="c-42179894">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for your note! We&#x27;re really hopeful that our &quot;local-like performance&quot; is part of the story that distinguishes us from other file system solutions. I envision a world where people don&#x27;t have to overprovision block storage volumes, and can just use this instead -- with the ability to easiy grab their data from S3.</div><br/></div></div></div></div><div id="42176823" class="c"><input type="checkbox" id="c-42176823" checked=""/><div class="controls bullet"><span class="by">alfalfasprout</span><span>|</span><a href="#42179324">prev</a><span>|</span><a href="#42179993">next</a><span>|</span><label class="collapse" for="c-42176823">[-]</label><label class="expand" for="c-42176823">[2 more]</label></div><br/><div class="children"><div class="content">Can you comment on how this is different from <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;mountpoint-for-amazon-s3-generally-available-and-ready-for-production-workloads&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;mountpoint-for-amazon-s3-ge...</a> ?</div><br/><div id="42176847" class="c"><input type="checkbox" id="c-42176847" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176823">parent</a><span>|</span><a href="#42179993">next</a><span>|</span><label class="collapse" for="c-42176847">[-]</label><label class="expand" for="c-42176847">[1 more]</label></div><br/><div class="children"><div class="content">Sure can, full disclosure, copied from a comment below:<p>Thanks for the question! Mountpoint for Amazon S3 is a FUSE layer that doesn&#x27;t support full POSIX semantics. For example, you can&#x27;t use Mountpoint for Amazon S3 for random writes to existing files, appends, or renames. This means that you have to carefully instrument your application to understand whether or not it&#x27;s compatible with Mountpoint, which can be error-prone. Regatta, on the other hand, provides full POSIX compatibility for the file interface, which means that it works out-of-the-box with all file based applications.</div><br/></div></div></div></div><div id="42179993" class="c"><input type="checkbox" id="c-42179993" checked=""/><div class="controls bullet"><span class="by">up2isomorphism</span><span>|</span><a href="#42176823">prev</a><span>|</span><a href="#42176321">next</a><span>|</span><label class="collapse" for="c-42179993">[-]</label><label class="expand" for="c-42179993">[2 more]</label></div><br/><div class="children"><div class="content">The main reason of adopting object storage is to avoid the burden associated with POSIX file system APIs. And this renders the major motivation using an object storage pointless.<p>Also using a translation layer on top of S3 will not save your costs.</div><br/><div id="42180303" class="c"><input type="checkbox" id="c-42180303" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42179993">parent</a><span>|</span><a href="#42176321">next</a><span>|</span><label class="collapse" for="c-42180303">[-]</label><label class="expand" for="c-42180303">[1 more]</label></div><br/><div class="children"><div class="content">Hey there, thanks for your note. I think that the answer here (as with all good questions) is &quot;it depends&quot;.<p>I agree with you, Object Storage accels at making the storage interface super simple to use (POSIX is incredibly complex). However, that doesn&#x27;t change the reality that nearly all software still reads and writes data from a local file system interface.<p>The specifics of whether or not using a translation layer will save you costs comes down a lot to what you&#x27;re comparing it to. If you have an EBS volume that&#x27;s 20% full, then I guarantee you that Regatta&#x27;s storage costs will be cheaper than EBS, even if you don&#x27;t ever tier to S3. It&#x27;s just a cherry on top for workloads which may have unpredictable access patterns and don&#x27;t want all of their data to be hot when not in use.</div><br/></div></div></div></div><div id="42176321" class="c"><input type="checkbox" id="c-42176321" checked=""/><div class="controls bullet"><span class="by">whinvik</span><span>|</span><a href="#42179993">prev</a><span>|</span><a href="#42175235">next</a><span>|</span><label class="collapse" for="c-42176321">[-]</label><label class="expand" for="c-42176321">[2 more]</label></div><br/><div class="children"><div class="content">I am not your target audience but I have been thinking of building a very minified version of this using [0] Pooch and [1] S3FS.<p>Right now we spend a lot of time downloading various stuff from HTTP or S3 links and then figuring out folder structures to keep them in our S3 buckets. Pooch really simplifies the caching for this by having a deterministic path on your local storage for downloaded files, but has no S3 backend.<p>So a combination of 2 would be to just have 1 call to a link that would embed the caching both locally and on our S3 buckets deterministically.<p>[0] <a href="https:&#x2F;&#x2F;www.fatiando.org&#x2F;pooch&#x2F;latest&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.fatiando.org&#x2F;pooch&#x2F;latest&#x2F;</a>
[1] <a href="https:&#x2F;&#x2F;s3fs.readthedocs.io&#x2F;en&#x2F;latest&#x2F;" rel="nofollow">https:&#x2F;&#x2F;s3fs.readthedocs.io&#x2F;en&#x2F;latest&#x2F;</a></div><br/><div id="42176384" class="c"><input type="checkbox" id="c-42176384" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176321">parent</a><span>|</span><a href="#42175235">next</a><span>|</span><label class="collapse" for="c-42176384">[-]</label><label class="expand" for="c-42176384">[1 more]</label></div><br/><div class="children"><div class="content">I think this is a great insight, and something that I think about often. The challenge that I see is that the scientist archetype (whether it&#x27;s data science, AI researcher, or anything else) isn&#x27;t really interested in doing software development for these kinds of things. They just want the data to <i>be there</i>, and it&#x27;s super nice to be able to click through the S3 console to be able to see and share the data their using. I think that what you&#x27;re doing is a great idea for folks who are accessing their data primarily through Python programs!</div><br/></div></div></div></div><div id="42175235" class="c"><input type="checkbox" id="c-42175235" checked=""/><div class="controls bullet"><span class="by">weinzierl</span><span>|</span><a href="#42176321">prev</a><span>|</span><a href="#42174429">next</a><span>|</span><label class="collapse" for="c-42175235">[-]</label><label class="expand" for="c-42175235">[4 more]</label></div><br/><div class="children"><div class="content">The title says POSIX but then it talks about NFS. So, what is it? Does it guarantee <i>all</i> POSIX semantics or not?</div><br/><div id="42175277" class="c"><input type="checkbox" id="c-42175277" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175235">parent</a><span>|</span><a href="#42174429">next</a><span>|</span><label class="collapse" for="c-42175277">[-]</label><label class="expand" for="c-42175277">[3 more]</label></div><br/><div class="children"><div class="content">You are correct in that NFS is not strictly-speaking POSIX compliant to the letter of the law, due to the caching behavior. This is an NFSv3 file system, so it shares those semantics. The point that I&#x27;m trying to emphasize is that the file system supports standard file operations which aren&#x27;t possible through other FUSE adapters, or possible to perform efficiently on S3 (such as append, rename, and symbolic links) -- which provides broad compatibility with file-based applications.</div><br/><div id="42175728" class="c"><input type="checkbox" id="c-42175728" checked=""/><div class="controls bullet"><span class="by">weinzierl</span><span>|</span><a href="#42175235">root</a><span>|</span><a href="#42175277">parent</a><span>|</span><a href="#42174429">next</a><span>|</span><label class="collapse" for="c-42175728">[-]</label><label class="expand" for="c-42175728">[2 more]</label></div><br/><div class="children"><div class="content">Which <i>is</i> nice and useful of course but there is ton of things that can&#x27;t reliably be done with that (like running any database you that comes to mind) which makes it important to be precise here.</div><br/><div id="42175820" class="c"><input type="checkbox" id="c-42175820" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175235">root</a><span>|</span><a href="#42175728">parent</a><span>|</span><a href="#42174429">next</a><span>|</span><label class="collapse" for="c-42175820">[-]</label><label class="expand" for="c-42175820">[1 more]</label></div><br/><div class="children"><div class="content">Is there something specific that you worry about when running a database on a networked file system? I would imagine that any database which is correctly fsync&#x27;ing the data to the write-ahead-log should work just fine.</div><br/></div></div></div></div></div></div></div></div><div id="42174429" class="c"><input type="checkbox" id="c-42174429" checked=""/><div class="controls bullet"><span class="by">remram</span><span>|</span><a href="#42175235">prev</a><span>|</span><a href="#42177534">next</a><span>|</span><label class="collapse" for="c-42174429">[-]</label><label class="expand" for="c-42174429">[2 more]</label></div><br/><div class="children"><div class="content">Is this like JuiceFS? <a href="https:&#x2F;&#x2F;juicefs.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;juicefs.com&#x2F;</a></div><br/><div id="42174449" class="c"><input type="checkbox" id="c-42174449" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174429">parent</a><span>|</span><a href="#42177534">next</a><span>|</span><label class="collapse" for="c-42174449">[-]</label><label class="expand" for="c-42174449">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s similar to JuiceFS, but JuiceFS writes and reads data from S3 in a proprietary block format. This means that you cannot connect JuiceFS to existing data sets in S3, and you cannot use data written through JuiceFS from the S3 API directly. On the other hand, Regatta reads and writes data to S3 using it&#x27;s native format -- so you can do these things!</div><br/></div></div></div></div><div id="42177534" class="c"><input type="checkbox" id="c-42177534" checked=""/><div class="controls bullet"><span class="by">neeleshs</span><span>|</span><a href="#42174429">prev</a><span>|</span><a href="#42174733">next</a><span>|</span><label class="collapse" for="c-42177534">[-]</label><label class="expand" for="c-42177534">[3 more]</label></div><br/><div class="children"><div class="content">Pretty cool. I&#x27;m excited about databases using this. Feels like  Neon&#x27;s PostgreSQL storage, but generalized to an FS.<p>Is this like FUSE with a cache? How does cache invalidation work?<p>All the best!</div><br/><div id="42177618" class="c"><input type="checkbox" id="c-42177618" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42177534">parent</a><span>|</span><a href="#42174733">next</a><span>|</span><label class="collapse" for="c-42177618">[-]</label><label class="expand" for="c-42177618">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, I like to think of it in a similar vein. We want to empower people to create stateless workflows where they may have previously needed to think about state management. Today, Regatta is an NFS file system where the cache lives on our shared infrastructure. However, when we complete the work on our custom protocol, that will be a FUSE file system which offers additional caching on your instances to enable truly local-like performance.</div><br/><div id="42179091" class="c"><input type="checkbox" id="c-42179091" checked=""/><div class="controls bullet"><span class="by">neeleshs</span><span>|</span><a href="#42177534">root</a><span>|</span><a href="#42177618">parent</a><span>|</span><a href="#42174733">next</a><span>|</span><label class="collapse" for="c-42179091">[-]</label><label class="expand" for="c-42179091">[1 more]</label></div><br/><div class="children"><div class="content">I am now inspired to build a toy project to learn how it all works!</div><br/></div></div></div></div></div></div><div id="42174733" class="c"><input type="checkbox" id="c-42174733" checked=""/><div class="controls bullet"><span class="by">mmastrac</span><span>|</span><a href="#42177534">prev</a><span>|</span><a href="#42174836">next</a><span>|</span><label class="collapse" for="c-42174733">[-]</label><label class="expand" for="c-42174733">[2 more]</label></div><br/><div class="children"><div class="content">I have a few qualms with this app:<p>1. For a Linux user, you can already build such a system yourself quite trivially by getting an FTP account, mounting it locally with curlftpfs, and then using SVN or CVS on the mounted filesystem. From Windows or Mac, this FTP account could be accessed through built-in software.<p>... I&#x27;m kidding, this is quite useful.<p>I really wish that NFSv3 and Linux had built-in file hashing ioctls that could delegate some of this expensive work to the backend as it would make it much easier to use something like this as a backup accelerator.</div><br/><div id="42174811" class="c"><input type="checkbox" id="c-42174811" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174733">parent</a><span>|</span><a href="#42174836">next</a><span>|</span><label class="collapse" for="c-42174811">[-]</label><label class="expand" for="c-42174811">[1 more]</label></div><br/><div class="children"><div class="content">Ha, thank you for the FTP comment, I was hoping someone would make it.<p>&gt; I really wish that NFSv3 and Linux had built-in file hashing ioctls that could delegate some of this expensive work to the backend as it would make it much easier to use something like this as a backup accelerator.<p>Tell me a bit more about what you mean here. We&#x27;re interested in really pushing the limits of what a storage system can do, so I&#x27;d be potentially interested.</div><br/></div></div></div></div><div id="42174836" class="c"><input type="checkbox" id="c-42174836" checked=""/><div class="controls bullet"><span class="by">bithive123</span><span>|</span><a href="#42174733">prev</a><span>|</span><a href="#42175229">next</a><span>|</span><label class="collapse" for="c-42174836">[-]</label><label class="expand" for="c-42174836">[4 more]</label></div><br/><div class="children"><div class="content">How does this compare to Amazon&#x27;s own offering in this space, the &quot;AWS Storage Gateway&quot;?  It can also back various storage protocols with S3, using SSDs for cache, etc.  (<a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;storagegateway&#x2F;features&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;storagegateway&#x2F;features&#x2F;</a>)</div><br/><div id="42174924" class="c"><input type="checkbox" id="c-42174924" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174836">parent</a><span>|</span><a href="#42175229">next</a><span>|</span><label class="collapse" for="c-42174924">[-]</label><label class="expand" for="c-42174924">[3 more]</label></div><br/><div class="children"><div class="content">Great question! We fill the same role as AWS Storage Gateway (and I used to work closely with that team when I was at AWS, lots of respect for what they do). AWS Storage Gateway is built primarily as an appliance to be installed on instances in your own data center to ease migration to the cloud. Many customers do deploy Storage Gateway on EC2 because they want these features in the cloud itself. However, the &quot;appliance&quot; design of Storage Gateway makes it unsuitable for this purpose. For example, Storage Gateway is not designed to run in a cluster for high-availability and doesn&#x27;t have access to durable, long-term storage to stage and cache writes.<p>On the other hand, Regatta is designed as a cloud-native gateway product. Regatta&#x27;s elastic, durable caching layer allows us to efficiently cache large data sets without thrashing, and always efficiently perform writes. Because Regatta is designed to be highly-available, customers don&#x27;t have to worry about downtime for patching or deployments.</div><br/><div id="42175521" class="c"><input type="checkbox" id="c-42175521" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#42174836">root</a><span>|</span><a href="#42174924">parent</a><span>|</span><a href="#42175229">next</a><span>|</span><label class="collapse" for="c-42175521">[-]</label><label class="expand" for="c-42175521">[2 more]</label></div><br/><div class="children"><div class="content">S3 File Gateway sounds a lot like your product.</div><br/><div id="42175552" class="c"><input type="checkbox" id="c-42175552" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174836">root</a><span>|</span><a href="#42175521">parent</a><span>|</span><a href="#42175229">next</a><span>|</span><label class="collapse" for="c-42175552">[-]</label><label class="expand" for="c-42175552">[1 more]</label></div><br/><div class="children"><div class="content">Also true! If you look at their site, they&#x27;re really targeting folks to deploy it into their data centers to provide on-premises caching of resources in AWS, rather than providing a high-speed cache within AWS for file-based applications.<p><a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;storagegateway&#x2F;file&#x2F;s3&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;storagegateway&#x2F;file&#x2F;s3&#x2F;</a></div><br/></div></div></div></div></div></div></div></div><div id="42175229" class="c"><input type="checkbox" id="c-42175229" checked=""/><div class="controls bullet"><span class="by">austinpena</span><span>|</span><a href="#42174836">prev</a><span>|</span><a href="#42175870">next</a><span>|</span><label class="collapse" for="c-42175229">[-]</label><label class="expand" for="c-42175229">[2 more]</label></div><br/><div class="children"><div class="content">Reminds me of <a href="https:&#x2F;&#x2F;www.lucidlink.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.lucidlink.com&#x2F;</a> for video editors. I quite like the experience with them.</div><br/><div id="42175295" class="c"><input type="checkbox" id="c-42175295" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175229">parent</a><span>|</span><a href="#42175870">next</a><span>|</span><label class="collapse" for="c-42175295">[-]</label><label class="expand" for="c-42175295">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s exactly right, I&#x27;ve spoken with a ton of folks who have had a good experience with Lucid Link. I think that we are in a slightly different part of the market (in that we aren&#x27;t targeting video editors, and more of data-intensive applications which may use thousands of IOPS), but I appreciate that the technology is likely similar.</div><br/></div></div></div></div><div id="42175870" class="c"><input type="checkbox" id="c-42175870" checked=""/><div class="controls bullet"><span class="by">inopinatus</span><span>|</span><a href="#42175229">prev</a><span>|</span><a href="#42180320">next</a><span>|</span><label class="collapse" for="c-42175870">[-]</label><label class="expand" for="c-42175870">[2 more]</label></div><br/><div class="children"><div class="content">I rejected EFS as a common caching and shared files layer, despite being technologically an excellent fit for my stack, because it is astronomically expensive. The value created didn’t match the cost.<p>When I got in touch about that, I was confronted with a wall of TCO papers, which tells me the product managers evidently believe their target segment to be Gartner-following corporate drones. This was a further deterrent.<p>We threw that idea away and used memcached instead, with common static files in a package in S3.<p>I guess I’m suggesting, don’t be like EFS when it comes to pricing or reaching customers.</div><br/><div id="42176102" class="c"><input type="checkbox" id="c-42176102" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175870">parent</a><span>|</span><a href="#42180320">next</a><span>|</span><label class="collapse" for="c-42176102">[-]</label><label class="expand" for="c-42176102">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s certainly my hope to be cost effective, but I understand the worry and I&#x27;m sorry that you had that experience with the PMs of that time. At the end of the day, I see my target customers as those who aren&#x27;t interested in running their own infrastructure and having to manage availability and durability (in memcached case, things like needing to pre-warm the cache). I understand that it still may be possible to be more cost effective if you&#x27;re willing to trade off ease of use for dealing with those other concerns.</div><br/></div></div></div></div><div id="42180320" class="c"><input type="checkbox" id="c-42180320" checked=""/><div class="controls bullet"><span class="by">tw04</span><span>|</span><a href="#42175870">prev</a><span>|</span><label class="collapse" for="c-42180320">[-]</label><label class="expand" for="c-42180320">[2 more]</label></div><br/><div class="children"><div class="content">At first glance it’s not clear how this is unique from Nasuni.</div><br/><div id="42180426" class="c"><input type="checkbox" id="c-42180426" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42180320">parent</a><span>|</span><label class="collapse" for="c-42180426">[-]</label><label class="expand" for="c-42180426">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the question. Full disclosure, I&#x27;m grabbing this response from another comment:<p>I have mutual friends with some of the Nasuni folks, and I have a lot of respect for what they do. In particular, Nasuni stores data in a proprietary block format in your S3 bucket, so you can&#x27;t connect it to existing data sets or use that data directly from S3 out the other side. Whereas with Regatta, we store data in its native format in S3 so you can do these things.</div><br/></div></div></div></div></div></div></div></div></div></body></html>