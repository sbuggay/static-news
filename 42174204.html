<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1732093259552" as="style"/><link rel="stylesheet" href="styles.css?v=1732093259552"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a>Launch HN: Regatta Storage (YC F24) – Turn S3 into a local-like, POSIX cloud FS</a> </div><div class="subtext"><span>huntaub</span> | <span>143 comments</span></div><br/><div><div id="42176699" class="c"><input type="checkbox" id="c-42176699" checked=""/><div class="controls bullet"><span class="by">garganzol</span><span>|</span><a href="#42174697">next</a><span>|</span><label class="collapse" for="c-42176699">[-]</label><label class="expand" for="c-42176699">[13 more]</label></div><br/><div class="children"><div class="content">I used the same approach based on Rclone for a long time. I wondered what makes Regatta Storage different than Rclone. Here is the answer: &quot;When performing mutating operations on the file system (including writes, renames, and directory changes), Regatta first stages this data on its high-speed caching layer to provide strong consistency to other file clients.&quot; [0].<p>Rclone, on the contrary, has no layer that would guarantee consistency among parallel clients.<p>[0] <a href="https:&#x2F;&#x2F;docs.regattastorage.com&#x2F;details&#x2F;architecture#overview">https:&#x2F;&#x2F;docs.regattastorage.com&#x2F;details&#x2F;architecture#overvie...</a></div><br/><div id="42176790" class="c"><input type="checkbox" id="c-42176790" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176699">parent</a><span>|</span><a href="#42178556">next</a><span>|</span><label class="collapse" for="c-42176790">[-]</label><label class="expand" for="c-42176790">[3 more]</label></div><br/><div class="children"><div class="content">This is exactly right, and something that we think is particularly important for applications that care about data consistency. Often times, we see that customers want to be able to quickly hand off tasks from one instance to another which can be incredibly complex if you don&#x27;t have guarantees that your new operations will be seen by the second instance!</div><br/><div id="42177109" class="c"><input type="checkbox" id="c-42177109" checked=""/><div class="controls bullet"><span class="by">wanderingmind</span><span>|</span><a href="#42176699">root</a><span>|</span><a href="#42176790">parent</a><span>|</span><a href="#42178556">next</a><span>|</span><label class="collapse" for="c-42177109">[-]</label><label class="expand" for="c-42177109">[2 more]</label></div><br/><div class="children"><div class="content">Might be useful to show the differences with Rclone, s3fs as a table to make it obvious</div><br/><div id="42177304" class="c"><input type="checkbox" id="c-42177304" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176699">root</a><span>|</span><a href="#42177109">parent</a><span>|</span><a href="#42178556">next</a><span>|</span><label class="collapse" for="c-42177304">[-]</label><label class="expand" for="c-42177304">[1 more]</label></div><br/><div class="children"><div class="content">I agree, I plan to put up a table soon.</div><br/></div></div></div></div></div></div><div id="42178556" class="c"><input type="checkbox" id="c-42178556" checked=""/><div class="controls bullet"><span class="by">benatkin</span><span>|</span><a href="#42176699">parent</a><span>|</span><a href="#42176790">prev</a><span>|</span><a href="#42177002">next</a><span>|</span><label class="collapse" for="c-42178556">[-]</label><label class="expand" for="c-42178556">[5 more]</label></div><br/><div class="children"><div class="content">The headline seems misleading, then.<p>rclone can work with AWS&#x27; different offerings, some of which at least partially address this: <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;new-amazon-s3-express-one-zone-high-performance-storage-class&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;new-amazon-s3-express-one-z...</a></div><br/><div id="42178584" class="c"><input type="checkbox" id="c-42178584" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176699">root</a><span>|</span><a href="#42178556">parent</a><span>|</span><a href="#42177002">next</a><span>|</span><label class="collapse" for="c-42178584">[-]</label><label class="expand" for="c-42178584">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not totally sure what you mean. I don&#x27;t think that S3 Express One Zone offers any additional atomic semantics in the file system world.</div><br/><div id="42179130" class="c"><input type="checkbox" id="c-42179130" checked=""/><div class="controls bullet"><span class="by">benatkin</span><span>|</span><a href="#42176699">root</a><span>|</span><a href="#42178584">parent</a><span>|</span><a href="#42177002">next</a><span>|</span><label class="collapse" for="c-42179130">[-]</label><label class="expand" for="c-42179130">[3 more]</label></div><br/><div class="children"><div class="content">For the misleading part, I probably should have said confusing because I don&#x27;t think you intended that, I mean that instead of introducing your caching layer you make it about S3, where the Object Storage provider seems totally interchangeable. Though it seems to work for a lot of your audience, from what I can tell from other comments here.<p>As for Express One Zone providing consistency, it would make more groups of operations consistent, provided that the clients could access the endpoints with low latency. It wouldn&#x27;t be a guarantee but it would be practical for some applications. It depends on what the problem is - for instance, do you want someone to never see noticeably stale data? I can definitely see that happening with Express One Zone if it&#x27;s as described.</div><br/><div id="42179162" class="c"><input type="checkbox" id="c-42179162" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176699">root</a><span>|</span><a href="#42179130">parent</a><span>|</span><a href="#42177002">next</a><span>|</span><label class="collapse" for="c-42179162">[-]</label><label class="expand" for="c-42179162">[2 more]</label></div><br/><div class="children"><div class="content">Yes, I think this is something that I’m actually struggling with. What’s the most exciting part for users? Is it the fact that we’re building a super fast file system or is it that we have this synchronization to S3? Ultimately, there just isn’t space for it all — but I appreciate the feedback.</div><br/><div id="42179229" class="c"><input type="checkbox" id="c-42179229" checked=""/><div class="controls bullet"><span class="by">benatkin</span><span>|</span><a href="#42176699">root</a><span>|</span><a href="#42179162">parent</a><span>|</span><a href="#42177002">next</a><span>|</span><label class="collapse" for="c-42179229">[-]</label><label class="expand" for="c-42179229">[1 more]</label></div><br/><div class="children"><div class="content">I think they both go together. It might take about 10 minutes to give a good high level explanation of it, including how the S3 syncing works - that the S3 lags slightly behind the caching layer for reads, and that you can still write to S3. 2-way sync. I imagine that S3 would be treated sort of like another client if updates came from S3 and the clients at the same time. It would probably be not so great to write to S3 if you aren&#x27;t writing to somewhere that&#x27;s being actively edited, but if you want to write to a dormant area of S3 directly, that&#x27;s fine.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42177002" class="c"><input type="checkbox" id="c-42177002" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#42176699">parent</a><span>|</span><a href="#42178556">prev</a><span>|</span><a href="#42182970">next</a><span>|</span><label class="collapse" for="c-42177002">[-]</label><label class="expand" for="c-42177002">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, this was my thought as well.  I use and love rclone and it wasn&#x27;t immediately clear what this offered above that</div><br/></div></div><div id="42182970" class="c"><input type="checkbox" id="c-42182970" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#42176699">parent</a><span>|</span><a href="#42177002">prev</a><span>|</span><a href="#42174697">next</a><span>|</span><label class="collapse" for="c-42182970">[-]</label><label class="expand" for="c-42182970">[3 more]</label></div><br/><div class="children"><div class="content">I suppose rclone doesn&#x27;t provide byte range file locking? Running sqlite over rclone would be a disaster.</div><br/><div id="42188761" class="c"><input type="checkbox" id="c-42188761" checked=""/><div class="controls bullet"><span class="by">garganzol</span><span>|</span><a href="#42176699">root</a><span>|</span><a href="#42182970">parent</a><span>|</span><a href="#42184404">next</a><span>|</span><label class="collapse" for="c-42188761">[-]</label><label class="expand" for="c-42188761">[1 more]</label></div><br/><div class="children"><div class="content">Running sqlite over rclone is not a disaster as long as you run only a single instance working with that database. Rclone provides no support for locking semantics.</div><br/></div></div><div id="42184404" class="c"><input type="checkbox" id="c-42184404" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176699">root</a><span>|</span><a href="#42182970">parent</a><span>|</span><a href="#42188761">prev</a><span>|</span><a href="#42174697">next</a><span>|</span><label class="collapse" for="c-42184404">[-]</label><label class="expand" for="c-42184404">[1 more]</label></div><br/><div class="children"><div class="content">That would be my expectation, you need <i>something</i> in the middle to actually broker the file locks.</div><br/></div></div></div></div></div></div><div id="42174697" class="c"><input type="checkbox" id="c-42174697" checked=""/><div class="controls bullet"><span class="by">memset</span><span>|</span><a href="#42176699">prev</a><span>|</span><a href="#42183927">next</a><span>|</span><label class="collapse" for="c-42174697">[-]</label><label class="expand" for="c-42174697">[11 more]</label></div><br/><div class="children"><div class="content">This is honestly the coolest thing I&#x27;ve seen coming out of YC in years. I have a bunch of questions which are basically related to &quot;how does it work&quot; and please pardon me if my questions are silly or naive!<p>1. If I had a local disk which was 10 GB, what happens when I try to contend with data in the 50 GB range (as in, more that could be cached locally?) Would I immediately see degradation, or thrashing, at the 10 GB mark?<p>2. Does this only work in practice on AWS instances? As in, I could run it on a different cloud, but in practice we only really get fast speeds due to running everything within AWS?<p>3. I&#x27;ve always had trouble with FUSE in different kinds of docker environments. And it looks like you&#x27;re using both FUSE and NFS mounts. How does all of that work?<p>4. Is the idea that I could literally run Clickhouse or Postgres with a regatta volume as the backing store?<p>5. I have to ask - how do you think about open source here?<p>6. Can I mount on multiple servers? What are the limits there? (ie, a lambda function.)<p>I haven&#x27;t played with the so maybe doing so would help answer questions. But I&#x27;m really excited about this! I have tried using EFS for small projects in the past but - and maybe I was holding it wrong - I could not for the life of me figure out what I needed to get faster bandwidth, probably because I didn&#x27;t know how to turn the knobs correctly.</div><br/><div id="42174791" class="c"><input type="checkbox" id="c-42174791" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174697">parent</a><span>|</span><a href="#42183927">next</a><span>|</span><label class="collapse" for="c-42174791">[-]</label><label class="expand" for="c-42174791">[10 more]</label></div><br/><div class="children"><div class="content">Wow, thanks for the nice note! No questions are silly, and I&#x27;ll also note that we now have a docs site (<a href="https:&#x2F;&#x2F;docs.regattastorage.com">https:&#x2F;&#x2F;docs.regattastorage.com</a>) and feel free to email me (hleath [at] regattastorage.com) if I don&#x27;t fully address your questions.<p>&gt; If I had a local disk which was 10 GB, what happens when I try to contend with data in the 50 GB range (as in, more that could be cached locally?) Would I immediately see degradation, or thrashing, at the 10 GB mark?<p>We don&#x27;t actually do caching on <i>your instance&#x27;s</i> disk. Instead, data is cached in the Linux page cache (in memory) like a regular hard drive, and Regatta provides a durable, shared cache that automatically expands with the working set size of your application. For example, if you were trying to work with data in the 50 GiB range, Regatta would automatically cache all 50 GiB -- allowing you to access it with sub-millisecond latency.<p>&gt; Does this only work in practice on AWS instances? As in, I could run it on a different cloud, but in practice we only really get fast speeds due to running everything within AWS?<p>For now, yes -- the speed is highly dependent on latency -- which is highly dependent on distance between your instance and Regatta. Today, we are only in AWS, but we are looking to launch in other clouds by the end of the year. Shoot me an email if there&#x27;s somewhere specifically that you&#x27;re interested in.<p>&gt; I&#x27;ve always had trouble with FUSE in different kinds of docker environments. And it looks like you&#x27;re using both FUSE and NFS mounts. How does all of that work?<p>There are a couple of different questions bundled together in this. Today, Regatta exposes an NFSv3 file system that you can mount. We are working on a new protocol which will be mounted via FUSE. However, in Docker environments, we also provide a CSI driver (for use with K8s) and a Docker volume plugin (for use with just Docker) that handles the mounting for you. We haven&#x27;t released these publicly yet, so shoot me an email if you want early access.<p>&gt; Is the idea that I could literally run Clickhouse or Postgres with a regatta volume as the backing store?<p>Yes, you should be able to run a database on Regatta.<p>&gt; I have to ask - how do you think about open source here?<p>We are in the process of open sourcing all of the client code (CSI driver, mount helper, FUSE), but we don&#x27;t have plans currently to open source the server code. We see the value of Regatta in managing the infrastructure so you don&#x27;t have to, and if we release it via open-source, it would be difficult to run on your own.<p>&gt; Can I mount on multiple servers? What are the limits there? (ie, a lambda function.)<p>Yes, you can mount on multiple servers simultaneously! We haven&#x27;t specifically stress-tested the number of clients we support, but we should be good for O(100s) of mounts. Unfortunately, AWS locks down Lambda so we can&#x27;t mount arbitrary file systems in that environment specifically.<p>&gt; efs performance<p>Yes, the challenge here is specifically around the semantics of NFS itself and the latency of the EFS service. We think we have a path to solving both of these in the next month or two.</div><br/><div id="42177548" class="c"><input type="checkbox" id="c-42177548" checked=""/><div class="controls bullet"><span class="by">gizmo</span><span>|</span><a href="#42174697">root</a><span>|</span><a href="#42174791">parent</a><span>|</span><a href="#42175084">next</a><span>|</span><label class="collapse" for="c-42177548">[-]</label><label class="expand" for="c-42177548">[2 more]</label></div><br/><div class="children"><div class="content">Do I understand correctly that the data gets decrypted at your Regatta AWS instances, before the data ends up in the customer&#x27;s S3 bucket? It sounds like the SSL pipe used for NFS is terminated at Regatta servers. Can customers run the Regatta service on their own hardware?<p>Or does Regatta only have access to filesystem metadata -- enough to do POSIX stuffs like locks, mv, rm -- but the file contents themselves remain encrypted end-to-end?</div><br/><div id="42177649" class="c"><input type="checkbox" id="c-42177649" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174697">root</a><span>|</span><a href="#42177548">parent</a><span>|</span><a href="#42175084">next</a><span>|</span><label class="collapse" for="c-42177649">[-]</label><label class="expand" for="c-42177649">[1 more]</label></div><br/><div class="children"><div class="content">This is correct, we encrypt data in-transit to the Regatta servers (using TLS), and we encrypt any data that the Regatta servers are storing. Of course, when Regatta communicates with S3, that&#x27;s also encrypted with TLS (just like using the AWS SDK). However, we don&#x27;t pass the encrypted data to S3, otherwise you wouldn&#x27;t be able to read it from the bucket directly and use it in other applications!</div><br/></div></div></div></div><div id="42175084" class="c"><input type="checkbox" id="c-42175084" checked=""/><div class="controls bullet"><span class="by">memset</span><span>|</span><a href="#42174697">root</a><span>|</span><a href="#42174791">parent</a><span>|</span><a href="#42177548">prev</a><span>|</span><a href="#42179460">next</a><span>|</span><label class="collapse" for="c-42175084">[-]</label><label class="expand" for="c-42175084">[4 more]</label></div><br/><div class="children"><div class="content">Thank you for the detailed answers! Honestly, this project inspires me to work on infrastructure problems.<p>So you are saying that regatta&#x27;s own SaaS infrastructure provides the disk caching layer. So you all make sure the pipe between my AWS instance and your servers are very fast and &quot;infinitely scalable&quot;, and then the sync to S3 happens after the fact.</div><br/><div id="42175112" class="c"><input type="checkbox" id="c-42175112" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174697">root</a><span>|</span><a href="#42175084">parent</a><span>|</span><a href="#42179460">next</a><span>|</span><label class="collapse" for="c-42175112">[-]</label><label class="expand" for="c-42175112">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s exactly right!</div><br/><div id="42183374" class="c"><input type="checkbox" id="c-42183374" checked=""/><div class="controls bullet"><span class="by">gregw2</span><span>|</span><a href="#42174697">root</a><span>|</span><a href="#42175112">parent</a><span>|</span><a href="#42179460">next</a><span>|</span><label class="collapse" for="c-42183374">[-]</label><label class="expand" for="c-42183374">[2 more]</label></div><br/><div class="children"><div class="content">So Regatta has an in memory cache? Does the posix disk write only suceed when the data is in more than one availability zone?</div><br/><div id="42184421" class="c"><input type="checkbox" id="c-42184421" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174697">root</a><span>|</span><a href="#42183374">parent</a><span>|</span><a href="#42179460">next</a><span>|</span><label class="collapse" for="c-42184421">[-]</label><label class="expand" for="c-42184421">[1 more]</label></div><br/><div class="children"><div class="content">Hey there! Today, we are replicating cache data within a single availability zone, but we’re working on a multi-availability zone product. If you have a need for multi-AZ, please shoot me an email at hleath [at] regattastorage.com, I’d love to learn more</div><br/></div></div></div></div></div></div></div></div><div id="42179460" class="c"><input type="checkbox" id="c-42179460" checked=""/><div class="controls bullet"><span class="by">0x1ceb00da</span><span>|</span><a href="#42174697">root</a><span>|</span><a href="#42174791">parent</a><span>|</span><a href="#42175084">prev</a><span>|</span><a href="#42179172">next</a><span>|</span><label class="collapse" for="c-42179460">[-]</label><label class="expand" for="c-42179460">[2 more]</label></div><br/><div class="children"><div class="content">Are you planning to support android? How? AFAIK android doesn&#x27;t have FUSE or NFS.</div><br/><div id="42179573" class="c"><input type="checkbox" id="c-42179573" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174697">root</a><span>|</span><a href="#42179460">parent</a><span>|</span><a href="#42179172">next</a><span>|</span><label class="collapse" for="c-42179573">[-]</label><label class="expand" for="c-42179573">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that I&#x27;m planning support for Android, did I mistakingly mention it somewhere?</div><br/></div></div></div></div></div></div></div></div><div id="42183927" class="c"><input type="checkbox" id="c-42183927" checked=""/><div class="controls bullet"><span class="by">cuno</span><span>|</span><a href="#42174697">prev</a><span>|</span><a href="#42175067">next</a><span>|</span><label class="collapse" for="c-42183927">[-]</label><label class="expand" for="c-42183927">[3 more]</label></div><br/><div class="children"><div class="content">Founder of cunoFS here, brilliant to see lots of activity in this space, and congrats on the launch! As you&#x27;ll know, there&#x27;s a whole galaxy of design decisions when building file storage, and as a storage geek it&#x27;s fun to see what different choices people make!<p>I see you&#x27;ve made some similar decisions to what we did for similar reasons I think - making sure files are stored 1:1 exactly as an object without some proprietary backend scrambling, offering strong consistency and POSIX semantics on the file storage, with eventual consistency between S3 and POSIX interfaces, and targeting high performance. Looks like we differ on the managed service vs traditional download and install model, and the client-first vs server-first approach (though some of our users also run cunoFS on an NFS&#x2F;SMB gateway server), and caching is a paid feature for us versus an included feature for yours.<p>Look forward to meeting and seeing you at storage conferences!</div><br/><div id="42184394" class="c"><input type="checkbox" id="c-42184394" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42183927">parent</a><span>|</span><a href="#42187403">next</a><span>|</span><label class="collapse" for="c-42184394">[-]</label><label class="expand" for="c-42184394">[1 more]</label></div><br/><div class="children"><div class="content">Great to hear from you, I think cunoFS is doing a lot of things right! It’s certainly a fun problem space!</div><br/></div></div><div id="42187403" class="c"><input type="checkbox" id="c-42187403" checked=""/><div class="controls bullet"><span class="by">Andys</span><span>|</span><a href="#42183927">parent</a><span>|</span><a href="#42184394">prev</a><span>|</span><a href="#42175067">next</a><span>|</span><label class="collapse" for="c-42187403">[-]</label><label class="expand" for="c-42187403">[1 more]</label></div><br/><div class="children"><div class="content">Is that Gweo? Didn&#x27;t know you were in the storage space, good to see you!</div><br/></div></div></div></div><div id="42175067" class="c"><input type="checkbox" id="c-42175067" checked=""/><div class="controls bullet"><span class="by">mritchie712</span><span>|</span><a href="#42183927">prev</a><span>|</span><a href="#42186660">next</a><span>|</span><label class="collapse" for="c-42175067">[-]</label><label class="expand" for="c-42175067">[15 more]</label></div><br/><div class="children"><div class="content">Pretty sure we&#x27;re in your target market. We [0] currently use GCP Filestore to host DuckDB. Here&#x27;s the pricing and performance at 10 TiB. Can you give me an idea on the pricing and performance for Regatta?<p>Service Tier: Zonal<p>Location: us-central1<p>10 TiB instance at $0.35&#x2F;TiB&#x2F;hr<p>Monthly cost: $2,560.00<p>Performance Estimate:<p>Read IOPS: 92,000<p>Write IOPS: 26,000<p>Read Throughput: 2,600 MiB&#x2F;s<p>Write Throughput: 880 MiB&#x2F;s<p>0 - <a href="https:&#x2F;&#x2F;www.definite.app&#x2F;blog&#x2F;duckdb-datawarehouse" rel="nofollow">https:&#x2F;&#x2F;www.definite.app&#x2F;blog&#x2F;duckdb-datawarehouse</a></div><br/><div id="42175238" class="c"><input type="checkbox" id="c-42175238" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175067">parent</a><span>|</span><a href="#42175360">next</a><span>|</span><label class="collapse" for="c-42175238">[-]</label><label class="expand" for="c-42175238">[3 more]</label></div><br/><div class="children"><div class="content">Yes, you should be in our target market. I don&#x27;t think that I can give a cost estimate without having a good sense of what <i>percentage</i> of your data you&#x27;re actively using at any given time, but we should absolutely support the performance numbers that you&#x27;re talking about. I&#x27;d love to chat more in detail, feel free to send me a note at hleath [at] regattastorage.com.</div><br/><div id="42175518" class="c"><input type="checkbox" id="c-42175518" checked=""/><div class="controls bullet"><span class="by">mritchie712</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42175238">parent</a><span>|</span><a href="#42175360">next</a><span>|</span><label class="collapse" for="c-42175518">[-]</label><label class="expand" for="c-42175518">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll send you a note!<p>Found this in the docs:<p>&gt; By default, Regatta file systems can provide up to 10 Gbps of throughput and 10,000 IOPS across all connected clients.<p>Is that the lower bound? The 50 TiB filestore instance has 104 Gbps read through put (albeit at a relatively high price point).</div><br/><div id="42175541" class="c"><input type="checkbox" id="c-42175541" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42175518">parent</a><span>|</span><a href="#42175360">next</a><span>|</span><label class="collapse" for="c-42175541">[-]</label><label class="expand" for="c-42175541">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s just the limit that we apply to new file systems. We should be able to support your 104 Gbps of read throughput.</div><br/></div></div></div></div></div></div><div id="42175360" class="c"><input type="checkbox" id="c-42175360" checked=""/><div class="controls bullet"><span class="by">_bare_metal</span><span>|</span><a href="#42175067">parent</a><span>|</span><a href="#42175238">prev</a><span>|</span><a href="#42186660">next</a><span>|</span><label class="collapse" for="c-42175360">[-]</label><label class="expand" for="c-42175360">[11 more]</label></div><br/><div class="children"><div class="content">Out of curiosity, why not go bare metal in a managed colocation? Is that for the geographic spread? Or unpredictable load?<p>Every few months of this spend is like buying a server<p>Edit: back at my pc and checked, relevant bare metal is ~$500&#x2F;m, amortized:<p><a href="https:&#x2F;&#x2F;baremetalsavings.com&#x2F;c&#x2F;LtxKMNj" rel="nofollow">https:&#x2F;&#x2F;baremetalsavings.com&#x2F;c&#x2F;LtxKMNj</a><p>Edit 2: for 100tb..</div><br/><div id="42189500" class="c"><input type="checkbox" id="c-42189500" checked=""/><div class="controls bullet"><span class="by">nthh</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42175360">parent</a><span>|</span><a href="#42175534">next</a><span>|</span><label class="collapse" for="c-42189500">[-]</label><label class="expand" for="c-42189500">[1 more]</label></div><br/><div class="children"><div class="content">This is compelling but it would be useful to compare upfront costs here. Investing $20,000+ in a server isn&#x27;t feasible for many. I&#x27;d also be curious to know how much a failsafe (perhaps &quot;heatable&quot; cold storage, at least for the example) would cost.</div><br/></div></div><div id="42175534" class="c"><input type="checkbox" id="c-42175534" checked=""/><div class="controls bullet"><span class="by">mritchie712</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42175360">parent</a><span>|</span><a href="#42189500">prev</a><span>|</span><a href="#42176875">next</a><span>|</span><label class="collapse" for="c-42175534">[-]</label><label class="expand" for="c-42175534">[1 more]</label></div><br/><div class="children"><div class="content">agreed, one month of 50 TiB is $12,800!<p>we&#x27;re using Filestore out of convenience right now, but actively exploring alternatives.</div><br/></div></div><div id="42176875" class="c"><input type="checkbox" id="c-42176875" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42175360">parent</a><span>|</span><a href="#42175534">prev</a><span>|</span><a href="#42186660">next</a><span>|</span><label class="collapse" for="c-42176875">[-]</label><label class="expand" for="c-42176875">[8 more]</label></div><br/><div class="children"><div class="content">Hiring someone who knows how to manage bare metal (with failover and stuff) may take time %)</div><br/><div id="42177892" class="c"><input type="checkbox" id="c-42177892" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42176875">parent</a><span>|</span><a href="#42186660">next</a><span>|</span><label class="collapse" for="c-42177892">[-]</label><label class="expand" for="c-42177892">[7 more]</label></div><br/><div class="children"><div class="content">You pay a datacenter to put it in a rack and add connect power and uplinks, then treat it like a big ec2 instance (minus the built-in firewall). Now you just need someone who knows how to secure an ec2 instance and run your preferred software there (with failover and stuff).<p>If you run a single-digit number of servers and replace them every 5 years you will probably never get a hardware failure. If you&#x27;re unlucky and it still happens get someone to diagnose what&#x27;s wrong, ship replacement parts to the data center and pay their tech to install them in your server.<p>Bare metal at scale is difficult. A small number of bare metal servers is easy. If your needs are average enough you can even just rent them so you don&#x27;t have capital costs and aren&#x27;t responsible for fixing hardware issues.</div><br/><div id="42184135" class="c"><input type="checkbox" id="c-42184135" checked=""/><div class="controls bullet"><span class="by">tempest_</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42177892">parent</a><span>|</span><a href="#42185150">next</a><span>|</span><label class="collapse" for="c-42184135">[-]</label><label class="expand" for="c-42184135">[1 more]</label></div><br/><div class="children"><div class="content">We run on our own stuff at our shop.<p>Some things that are hidden in the cloud providers cost are redundant networking, redundant internet connection, redundant disks.<p>Likely still cheaper than the cloud obviously but you will need to stomach down time for that stuff if something breaks.</div><br/></div></div><div id="42185150" class="c"><input type="checkbox" id="c-42185150" checked=""/><div class="controls bullet"><span class="by">kingnothing</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42177892">parent</a><span>|</span><a href="#42184135">prev</a><span>|</span><a href="#42183127">next</a><span>|</span><label class="collapse" for="c-42185150">[-]</label><label class="expand" for="c-42185150">[2 more]</label></div><br/><div class="children"><div class="content">Are you going to risk your entire business over &quot;probably never get a hardware failure&quot; that, if it hits, might result in days of downtime to resolve? I wouldn&#x27;t.</div><br/><div id="42189295" class="c"><input type="checkbox" id="c-42189295" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42185150">parent</a><span>|</span><a href="#42183127">next</a><span>|</span><label class="collapse" for="c-42189295">[-]</label><label class="expand" for="c-42189295">[1 more]</label></div><br/><div class="children"><div class="content">Just pay 2x for the hardware and have a hot standby, 1990s-style. Practice switching between the boxes every month or so; should be imperceptible for the customers and a nearly non-event for the ops.</div><br/></div></div></div></div><div id="42183127" class="c"><input type="checkbox" id="c-42183127" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42177892">parent</a><span>|</span><a href="#42185150">prev</a><span>|</span><a href="#42179198">next</a><span>|</span><label class="collapse" for="c-42183127">[-]</label><label class="expand" for="c-42183127">[2 more]</label></div><br/><div class="children"><div class="content">sounds like an opportunity for someone (you?) to offer an abstraction slightly above bare metal to do the stuff you said to do, charging higher than bare metal but lower than the other stuff. how much daylight is there between those prices?</div><br/><div id="42189476" class="c"><input type="checkbox" id="c-42189476" checked=""/><div class="controls bullet"><span class="by">nthh</span><span>|</span><a href="#42175067">root</a><span>|</span><a href="#42183127">parent</a><span>|</span><a href="#42179198">next</a><span>|</span><label class="collapse" for="c-42189476">[-]</label><label class="expand" for="c-42189476">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure there are companies in this space providing private clouds on bare metal, I wonder how that would be to operate at scale though.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42186660" class="c"><input type="checkbox" id="c-42186660" checked=""/><div class="controls bullet"><span class="by">boulos</span><span>|</span><a href="#42175067">prev</a><span>|</span><a href="#42175213">next</a><span>|</span><label class="collapse" for="c-42186660">[-]</label><label class="expand" for="c-42186660">[2 more]</label></div><br/><div class="children"><div class="content">I love this space, and I have tried and failed to get cloud providers to work on it directly :). We could <i>not</i> get the Avere folks to admit that their block-based thing on object store was a mistake, but they were also the only real game in town.<p>That said, I feel like writeback caching is a bit ... risky? That is, you aren&#x27;t treating the object store as the source of truth. If your caching layer goes down after a write is ack&#x27;ed but before it&#x27;s &quot;replicated&quot; to S3, people lose their data, right?<p>I think you&#x27;ll end up wanting to offer customers the ability to do strongly-consistent writes (and cache invalidation). You&#x27;ll also likely end up wanting to add <i>operator</i> control for &quot;oh and don&#x27;t cache these, just pass through to the backing store&quot; (e.g., some final output that isn&#x27;t intended to get reused anytime soon).<p>Finally, don&#x27;t sleep on NFSv4.1! It ticks a bunch of <i>compliance</i> boxes for various industries, and then they will pay you :). Supporting FUSE is great for folks who can do it, but you&#x27;d want them to <i>start</i> by just pointing their NFS client at you, then &quot;upgrading&quot; to FUSE for better performance.</div><br/><div id="42186783" class="c"><input type="checkbox" id="c-42186783" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42186660">parent</a><span>|</span><a href="#42175213">next</a><span>|</span><label class="collapse" for="c-42186783">[-]</label><label class="expand" for="c-42186783">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That is, you aren&#x27;t treating the object store as the source of truth. If your caching layer goes down after a write is ack&#x27;ed but before it&#x27;s &quot;replicated&quot; to S3, people lose their data, right?<p>This is exactly why we&#x27;re building our caching layer to be highly-durable, like S3 itself. We will make sure that the data in the cache is safe, even if servers go down. This is what gives us the confidence to respond to the client before the data is in S3. The big difference between the data living in our cache and the data living in S3 is cost and performance, not necessarily durability.<p>&gt; I think you&#x27;ll end up wanting to offer customers the ability to do strongly-consistent writes (and cache invalidation). You&#x27;ll also likely end up wanting to add operator control for &quot;oh and don&#x27;t cache these, just pass through to the backing store&quot; (e.g., some final output that isn&#x27;t intended to get reused anytime soon).<p>I think this is exactly right. I think that storage systems are too often too hands off about the data (oh, give us the bytes and we will store them for you). I believe that there are gains to be had by asking the users to tell you more about what they&#x27;re doing. If you have a directory which is only used to read files and a directory which is only used to write files, then you probably want to have different cache strategies for those directories? I believe we can deliver this with good enough UX for most people to use.<p>&gt; Finally, don&#x27;t sleep on NFSv4.1! It ticks a bunch of compliance boxes for various industries, and then they will pay you :). Supporting FUSE is great for folks who can do it, but you&#x27;d want them to start by just pointing their NFS client at you, then &quot;upgrading&quot; to FUSE for better performance.<p>I certainly don&#x27;t, and this is why we are supporting NFSv3 right now. That&#x27;s not going away any time soon. We want to offer something that&#x27;s highly compatible with the industry at large today (NFS-based, we can talk specifics about whether or not that should be v3 or v4) and then something that is high-performance for the early adopters who can use something like FUSE. I think that both things are required to get the breadth of customers that we&#x27;re looking for.</div><br/></div></div></div></div><div id="42175213" class="c"><input type="checkbox" id="c-42175213" checked=""/><div class="controls bullet"><span class="by">jitl</span><span>|</span><a href="#42186660">prev</a><span>|</span><a href="#42180521">next</a><span>|</span><label class="collapse" for="c-42175213">[-]</label><label class="expand" for="c-42175213">[8 more]</label></div><br/><div class="children"><div class="content">I’m very interested in this as a backing disk for SQLite&#x2F;DuckDB&#x2F;parquet, but I really want my cached reads to come straight from instance-local NVMe storage, and to have a way to “pin” and “unpin” some subdirectories from local cache.<p>Why local storage? We’re going to have multiple processes reading &amp; writing to the files and need locking &amp; shared memory semantics you can’t get w&#x2F; NFS. I could implement pin&#x2F;unpin myself in user space by copying stuff between &#x2F;mnt&#x2F;magic-nfs and &#x2F;mnt&#x2F;instance-nvme but at that point I’d just use S3 myself.<p>Any thoughts about providing a custom file system or how to assemble this out of parts on top of the NFS mount?</div><br/><div id="42175408" class="c"><input type="checkbox" id="c-42175408" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175213">parent</a><span>|</span><a href="#42180521">next</a><span>|</span><label class="collapse" for="c-42175408">[-]</label><label class="expand" for="c-42175408">[7 more]</label></div><br/><div class="children"><div class="content">Hey -- I think this is something that&#x27;s in-scope for our custom protocol that we&#x27;re working on. I&#x27;d love to chat more about your needs to make sure that we build something that will work great for you. Would you mind shooting an email to hleath [at] regattastorage.com and we can chat more?</div><br/><div id="42184410" class="c"><input type="checkbox" id="c-42184410" checked=""/><div class="controls bullet"><span class="by">juancampa</span><span>|</span><a href="#42175213">root</a><span>|</span><a href="#42175408">parent</a><span>|</span><a href="#42180521">next</a><span>|</span><label class="collapse" for="c-42184410">[-]</label><label class="expand" for="c-42184410">[6 more]</label></div><br/><div class="children"><div class="content">We&#x27;re also interested in SQLite shared by multiple processes on something like Regatta but my concerns are the issues described in the SQLite documentation about NFS [1]. Notably &quot;SQLite relies on exclusive locks for write operations, and those have been known to operate incorrectly for some network filesystems.&quot;<p>[1] <a href="https:&#x2F;&#x2F;sqlite.org&#x2F;useovernet.html" rel="nofollow">https:&#x2F;&#x2F;sqlite.org&#x2F;useovernet.html</a></div><br/><div id="42184481" class="c"><input type="checkbox" id="c-42184481" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175213">root</a><span>|</span><a href="#42184410">parent</a><span>|</span><a href="#42180521">next</a><span>|</span><label class="collapse" for="c-42184481">[-]</label><label class="expand" for="c-42184481">[5 more]</label></div><br/><div class="children"><div class="content">Ah, yes — there are some specific file locking concerns with NFSv3 (notably that locks aren’t built as leases like in NFSv4). Let me do a double click here, but I know we will be able to support locks correctly with our custom protocol when we launch it by the end of the year.</div><br/><div id="42185966" class="c"><input type="checkbox" id="c-42185966" checked=""/><div class="controls bullet"><span class="by">juancampa</span><span>|</span><a href="#42175213">root</a><span>|</span><a href="#42184481">parent</a><span>|</span><a href="#42186463">next</a><span>|</span><label class="collapse" for="c-42185966">[-]</label><label class="expand" for="c-42185966">[2 more]</label></div><br/><div class="children"><div class="content">One more question. How does it handle large files that are frequently modified in arbitrary locations (like a SQLite file)? Will it only upload the &quot;diffs&quot; to S3? I&#x27;m guessing it doesn&#x27;t have to scan the whole file to determine what&#x27;s changed since it can keep track of what&#x27;s &quot;dirty&quot;.<p>I ask because last time I checked, S3 wouldn&#x27;t let you &quot;patch&quot; an object. So you&#x27;d have to push the diff as separate objects and then &quot;reconstruct&quot; the original file client-side as different chunks are read, right?</div><br/><div id="42186260" class="c"><input type="checkbox" id="c-42186260" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175213">root</a><span>|</span><a href="#42185966">parent</a><span>|</span><a href="#42186463">next</a><span>|</span><label class="collapse" for="c-42186260">[-]</label><label class="expand" for="c-42186260">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s correct re: the S3 API. What we do is we &quot;merge&quot; multiple write requests together to minimize the cost to you and the number of requests to S3. For example, if you write a file 1,000 times in the span of a minute, we would merge that into a single PutObject request to S3. Of course, we force flush the data every few minutes (even if it&#x27;s being written frequently) in order to make sure that there&#x27;s an up-to-date copy in S3.</div><br/></div></div></div></div><div id="42186463" class="c"><input type="checkbox" id="c-42186463" checked=""/><div class="controls bullet"><span class="by">mdaniel</span><span>|</span><a href="#42175213">root</a><span>|</span><a href="#42184481">parent</a><span>|</span><a href="#42185966">prev</a><span>|</span><a href="#42180521">next</a><span>|</span><label class="collapse" for="c-42186463">[-]</label><label class="expand" for="c-42186463">[2 more]</label></div><br/><div class="children"><div class="content">I would really enjoy hearing why SMBv4 or the hundreds of other protocols are somehow insufficient for your needs. The thought of &quot;how hard can a custom protocol be?!&quot; makes me shudder, to say nothing of the burden -- ours and yours -- of maintaining endpoint implementations for all the bazillions of places one would want to consume a network mount</div><br/><div id="42187584" class="c"><input type="checkbox" id="c-42187584" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175213">root</a><span>|</span><a href="#42186463">parent</a><span>|</span><a href="#42180521">next</a><span>|</span><label class="collapse" for="c-42187584">[-]</label><label class="expand" for="c-42187584">[1 more]</label></div><br/><div class="children"><div class="content">Ultimately, we&#x27;re just working on a different problem space than these protocols. That&#x27;s not to say that all of the existing protocols are bad, I absolutely believe that these protocols are great. Our ultimate goal, though, is to replace block storage, with a file-layer protocol. This sort of requires different semantics than what the existing file protocols support.<p>I don&#x27;t at all disagree that it&#x27;s a hard problem! That&#x27;s part of what makes it so fun to work on.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42180521" class="c"><input type="checkbox" id="c-42180521" checked=""/><div class="controls bullet"><span class="by">daviesliu</span><span>|</span><a href="#42175213">prev</a><span>|</span><a href="#42188489">next</a><span>|</span><label class="collapse" for="c-42180521">[-]</label><label class="expand" for="c-42180521">[7 more]</label></div><br/><div class="children"><div class="content">Founder of JuiceFS here, congrats to the Launch! I&#x27;m super excited to see more people doing creative things in the using-S3-as-file-system space. When we started JuiceFS back in 2017, applied YC for 2 times but no luck.<p>We are still working hard on it, hoping that we can help people with different workloads with different tech!</div><br/><div id="42180528" class="c"><input type="checkbox" id="c-42180528" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42180521">parent</a><span>|</span><a href="#42182079">next</a><span>|</span><label class="collapse" for="c-42180528">[-]</label><label class="expand" for="c-42180528">[1 more]</label></div><br/><div class="children"><div class="content">Wow, thanks for coming out! I hope that you&#x27;re heartened to see the number of people who immediately think of JuiceFS when they see our launch. I totally agree with you, storage is such an interesting space to work in, and I&#x27;m excited that there are so many great products out there to fit the different needs of customers.</div><br/></div></div><div id="42182079" class="c"><input type="checkbox" id="c-42182079" checked=""/><div class="controls bullet"><span class="by">dsvf</span><span>|</span><a href="#42180521">parent</a><span>|</span><a href="#42180528">prev</a><span>|</span><a href="#42181262">next</a><span>|</span><label class="collapse" for="c-42182079">[-]</label><label class="expand" for="c-42182079">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a happy and satisfied JuiceFS user here, so I too would be interested in the difference between these. Is the Regatta key point caching?</div><br/><div id="42182252" class="c"><input type="checkbox" id="c-42182252" checked=""/><div class="controls bullet"><span class="by">ChocolateGod</span><span>|</span><a href="#42180521">root</a><span>|</span><a href="#42182079">parent</a><span>|</span><a href="#42185151">next</a><span>|</span><label class="collapse" for="c-42182252">[-]</label><label class="expand" for="c-42182252">[1 more]</label></div><br/><div class="children"><div class="content">Also a user of JuiceFS, replaced a GlusterFS cluster a few years ago, far cheaper and easier to scale with no issues or changes needed to the applications using GlusterFS.</div><br/></div></div><div id="42185151" class="c"><input type="checkbox" id="c-42185151" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42180521">root</a><span>|</span><a href="#42182079">parent</a><span>|</span><a href="#42182252">prev</a><span>|</span><a href="#42181262">next</a><span>|</span><label class="collapse" for="c-42185151">[-]</label><label class="expand" for="c-42185151">[2 more]</label></div><br/><div class="children"><div class="content">I know that I&#x27;ve answered this question a couple times in the thread, so I don&#x27;t know if my words add extra value here. But, I agree that it would be interesting to hear what Davies is thinking.</div><br/><div id="42185864" class="c"><input type="checkbox" id="c-42185864" checked=""/><div class="controls bullet"><span class="by">dsvf</span><span>|</span><a href="#42180521">root</a><span>|</span><a href="#42185151">parent</a><span>|</span><a href="#42181262">next</a><span>|</span><label class="collapse" for="c-42185864">[-]</label><label class="expand" for="c-42185864">[1 more]</label></div><br/><div class="children"><div class="content">Yes, your input into the thread cleared many things up for me, thanks!</div><br/></div></div></div></div></div></div><div id="42181262" class="c"><input type="checkbox" id="c-42181262" checked=""/><div class="controls bullet"><span class="by">gumbojuice</span><span>|</span><a href="#42180521">parent</a><span>|</span><a href="#42182079">prev</a><span>|</span><a href="#42188489">next</a><span>|</span><label class="collapse" for="c-42181262">[-]</label><label class="expand" for="c-42181262">[1 more]</label></div><br/><div class="children"><div class="content">As someone who is already happily using JuuceFS, perhaps you can provide a short list of differences (conceptual and&#x2F;or technical).  Thanks for a great product.</div><br/></div></div></div></div><div id="42188489" class="c"><input type="checkbox" id="c-42188489" checked=""/><div class="controls bullet"><span class="by">aloukissas</span><span>|</span><a href="#42180521">prev</a><span>|</span><a href="#42176750">next</a><span>|</span><label class="collapse" for="c-42188489">[-]</label><label class="expand" for="c-42188489">[2 more]</label></div><br/><div class="children"><div class="content">This is fantastic! Interestingly, I was one of the early engineers at Maginatics [1], a company that built exactly this in 2011 - and Netflix was one of our earliest beta customers. We strived to be both SMB3 and POSIX compatible, but leaning into SMB3 semantics. We had some pretty great optimizations that gave almost local disk performance (e.g. using file and directory leases [2], async metadata ops, data and metadata caching, etc). EFS was just coming out at that point (Azure I think also had something similar in the works).<p>I&#x27;ll be looking closely in what you&#x27;re building!<p>[1] <a href="https:&#x2F;&#x2F;www.dell.com&#x2F;en-us&#x2F;blog&#x2F;welcoming-spanning-maginatics-emc-family&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.dell.com&#x2F;en-us&#x2F;blog&#x2F;welcoming-spanning-maginatic...</a><p>[2] <a href="https:&#x2F;&#x2F;www.slideshare.net&#x2F;slideshow&#x2F;maginatics-sdcdwl&#x2F;39257996" rel="nofollow">https:&#x2F;&#x2F;www.slideshare.net&#x2F;slideshow&#x2F;maginatics-sdcdwl&#x2F;39257...</a></div><br/><div id="42189699" class="c"><input type="checkbox" id="c-42189699" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42188489">parent</a><span>|</span><a href="#42176750">next</a><span>|</span><label class="collapse" for="c-42189699">[-]</label><label class="expand" for="c-42189699">[1 more]</label></div><br/><div class="children"><div class="content">Awesome! Great to meet you, so happy that so many folks in the space are here.</div><br/></div></div></div></div><div id="42176750" class="c"><input type="checkbox" id="c-42176750" checked=""/><div class="controls bullet"><span class="by">convivialdingo</span><span>|</span><a href="#42188489">prev</a><span>|</span><a href="#42176756">next</a><span>|</span><label class="collapse" for="c-42176750">[-]</label><label class="expand" for="c-42176750">[6 more]</label></div><br/><div class="children"><div class="content">Wow, looks like a great product!  That&#x27;s a great idea to use NFS as the protocol.  I honestly hadn&#x27;t thought of that.<p>Perfect.<p>For IBM, I wrote a crypto filesystem that works similarly in concept, except it was a kernel filesystem.  We crypto split the blocks up into 4 parts, stored into cache. A background daemon listened to events and sync&#x27;ed blocks to S3 orchestrated with a shared journal.<p>It&#x27;s pure magic when you mount a filesystem on clean machine and all your data is &quot;just there.&quot;</div><br/><div id="42176804" class="c"><input type="checkbox" id="c-42176804" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176750">parent</a><span>|</span><a href="#42176756">next</a><span>|</span><label class="collapse" for="c-42176804">[-]</label><label class="expand" for="c-42176804">[5 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s pure magic when you mount a filesystem on clean machine and all your data is &quot;just there.&quot;<p>I totally agree! I am hoping that Regatta can power a future where teams don&#x27;t need more than ~8 GiB of local storage for their operating system, and can store the rest on something like Regatta to get rid of the waste of overprovisioned block volumes.</div><br/><div id="42177044" class="c"><input type="checkbox" id="c-42177044" checked=""/><div class="controls bullet"><span class="by">lijok</span><span>|</span><a href="#42176750">root</a><span>|</span><a href="#42176804">parent</a><span>|</span><a href="#42176756">next</a><span>|</span><label class="collapse" for="c-42177044">[-]</label><label class="expand" for="c-42177044">[4 more]</label></div><br/><div class="children"><div class="content">That would sell like hot cakes to the public sector.</div><br/><div id="42177636" class="c"><input type="checkbox" id="c-42177636" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176750">root</a><span>|</span><a href="#42177044">parent</a><span>|</span><a href="#42180422">next</a><span>|</span><label class="collapse" for="c-42177636">[-]</label><label class="expand" for="c-42177636">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s hope so, I&#x27;d love to help teams take storage infrastructure management off of their plate! If you&#x27;re in the public sector and interested in trying out Regatta, please shoot me an email at hleath [at] regattastorage.com.</div><br/></div></div><div id="42180422" class="c"><input type="checkbox" id="c-42180422" checked=""/><div class="controls bullet"><span class="by">shaklee3</span><span>|</span><a href="#42176750">root</a><span>|</span><a href="#42177044">parent</a><span>|</span><a href="#42177636">prev</a><span>|</span><a href="#42176756">next</a><span>|</span><label class="collapse" for="c-42180422">[-]</label><label class="expand" for="c-42180422">[2 more]</label></div><br/><div class="children"><div class="content">The public sector is typically air-gapped, so not really.</div><br/><div id="42184524" class="c"><input type="checkbox" id="c-42184524" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176750">root</a><span>|</span><a href="#42180422">parent</a><span>|</span><a href="#42176756">next</a><span>|</span><label class="collapse" for="c-42184524">[-]</label><label class="expand" for="c-42184524">[1 more]</label></div><br/><div class="children"><div class="content">I think it depends which part of the public sector! AWS GovCloud is not airgapped, but I certainly know of deployments which are.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42176756" class="c"><input type="checkbox" id="c-42176756" checked=""/><div class="controls bullet"><span class="by">random3</span><span>|</span><a href="#42176750">prev</a><span>|</span><a href="#42176558">next</a><span>|</span><label class="collapse" for="c-42176756">[-]</label><label class="expand" for="c-42176756">[4 more]</label></div><br/><div class="children"><div class="content">In (March?) 2007 (correction 2008) myself and two other engineers in front of Bruce Chizen - Adobe&#x27;s CEO in a small conference room in Bucharest demoed a photo taken with an iPhone automagically showing as a file on a Mac. I implemented the local FUSE talking to Ozzy - Adobe&#x27;s distributed object store back then, using an equivalent of a Linux inode structure. It worked like a charm and if I remember correctly it took us a few days to build it. It was a success just as much as Adobe&#x27;s later choices around <a href="http:&#x2F;&#x2F;Photoshop.com" rel="nofollow">http:&#x2F;&#x2F;Photoshop.com</a> were a huge failure. A few months later Dropbox launched.<p>That kickstarted about a decade in (actual) research and development led by my team which positioned the Bucharest center as one of the most prolific centers in distributed systems within Adobe and of Adobe within Romania.<p>But I didn&#x27;t come up with the concept, it was Richard Jones that inspired us with the Gmail drive that used FUSE with gmail attachments back in 2004 when I got my first while still in college  <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GMail_Drive" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GMail_Drive</a>. I guess I&#x27;m old, but I find it funny to see Launch HN: Regatta Storage (YC F24) – Turn S3 into a local-like, POSIX cloud FS</div><br/><div id="42176840" class="c"><input type="checkbox" id="c-42176840" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176756">parent</a><span>|</span><a href="#42189051">next</a><span>|</span><label class="collapse" for="c-42176840">[-]</label><label class="expand" for="c-42176840">[2 more]</label></div><br/><div class="children"><div class="content">The funny thing about storage is that all of the problems are the same! Ultimately, there is no problem that cannot be solved with caching, journaling, write-ahead logging, etc. I think what makes the problem space so interesting is how a million different products can make a million different trade offs with these tools to deliver on their customer needs. File systems are awesome.</div><br/><div id="42177269" class="c"><input type="checkbox" id="c-42177269" checked=""/><div class="controls bullet"><span class="by">random3</span><span>|</span><a href="#42176756">root</a><span>|</span><a href="#42176840">parent</a><span>|</span><a href="#42189051">next</a><span>|</span><label class="collapse" for="c-42177269">[-]</label><label class="expand" for="c-42177269">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The funny thing about storage is that all of the problems are the same!<p>they <i>are</i> all the same and they are all more than what would at the surface seem that it&#x27;s &quot;just files&quot; the whole OS, especially Linux&#x2F;UNIX is &quot;just files&quot; and if you look deeper at databases you can see how it boils down to the file formats (something that was visible with LevelDB but maybe less so with RocksDB, I guess)</div><br/></div></div></div></div><div id="42189051" class="c"><input type="checkbox" id="c-42189051" checked=""/><div class="controls bullet"><span class="by">mikeshi42</span><span>|</span><a href="#42176756">parent</a><span>|</span><a href="#42176840">prev</a><span>|</span><a href="#42176558">next</a><span>|</span><label class="collapse" for="c-42189051">[-]</label><label class="expand" for="c-42189051">[1 more]</label></div><br/><div class="children"><div class="content">wow gmail drive is a walk down memory lane :) really is amazing how far we&#x27;ve come since then!</div><br/></div></div></div></div><div id="42176558" class="c"><input type="checkbox" id="c-42176558" checked=""/><div class="controls bullet"><span class="by">ragulpr</span><span>|</span><a href="#42176756">prev</a><span>|</span><a href="#42176321">next</a><span>|</span><label class="collapse" for="c-42176558">[-]</label><label class="expand" for="c-42176558">[2 more]</label></div><br/><div class="children"><div class="content">Love this idea! Biggest hurdle though have been to have predictable Auth&amp;IO across multiple Python&#x2F;Scala versions and all other things (Spark, orchestrators, CLI&#x27;s of teams of varying types of OS etc etc) add to that access logs.<p>SF3s&#x2F;boto&#x2F;botocore versions x Scala&#x2F;Spark x parquet x iceberg x k8s etc readers own assumptions makes reading from S3 alone a maintenance and compatibility nightmare.<p>Will the mounted system _really_ be accessible as local fs and seen as such to all running processes? No surprises? No need for python specific filesystem like S3Fs?<p>If so then you will win 100% I wouldn&#x27;t even care about speed&#x2F;cost if it&#x27;s up to par with s3</div><br/><div id="42176673" class="c"><input type="checkbox" id="c-42176673" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176558">parent</a><span>|</span><a href="#42176321">next</a><span>|</span><label class="collapse" for="c-42176673">[-]</label><label class="expand" for="c-42176673">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, that&#x27;s exactly right. I had some... experiences with Spark recently, that convinced me that this is something that could really help. I also really like the idea that organizations can continue to use S3 as the source of truth for their data (as you mention, it means that you can continue to use Access Logs, which would capture <i>all</i> usage of your S3 bucket across your applications).<p>&gt; Will the mounted system _really_ be accessible as local fs and seen as such to all running processes? No surprises? No need for python specific filesystem like S3Fs?<p>Ha, well it depends on what you mean by surprises. We won&#x27;t have a Python-specific file system. Our client is going to come in two flavors. Today, you can mount Regatta over NFSv3 (which we wrap in TLS to make it secure). This works for some workloads, but doesn&#x27;t provide like-for-like performance with EBS. Over the next month, we plan to release the &quot;custom protocol&quot; that I wrote about above, that we expect to send to customers in the form of a FUSE file system.<p>Either way, it should be one package, you shouldn&#x27;t need to worry about versioning, and it will appear as a real, local file system. :D</div><br/></div></div></div></div><div id="42176321" class="c"><input type="checkbox" id="c-42176321" checked=""/><div class="controls bullet"><span class="by">whinvik</span><span>|</span><a href="#42176558">prev</a><span>|</span><a href="#42191010">next</a><span>|</span><label class="collapse" for="c-42176321">[-]</label><label class="expand" for="c-42176321">[2 more]</label></div><br/><div class="children"><div class="content">I am not your target audience but I have been thinking of building a very minified version of this using [0] Pooch and [1] S3FS.<p>Right now we spend a lot of time downloading various stuff from HTTP or S3 links and then figuring out folder structures to keep them in our S3 buckets. Pooch really simplifies the caching for this by having a deterministic path on your local storage for downloaded files, but has no S3 backend.<p>So a combination of 2 would be to just have 1 call to a link that would embed the caching both locally and on our S3 buckets deterministically.<p>[0] <a href="https:&#x2F;&#x2F;www.fatiando.org&#x2F;pooch&#x2F;latest&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.fatiando.org&#x2F;pooch&#x2F;latest&#x2F;</a>
[1] <a href="https:&#x2F;&#x2F;s3fs.readthedocs.io&#x2F;en&#x2F;latest&#x2F;" rel="nofollow">https:&#x2F;&#x2F;s3fs.readthedocs.io&#x2F;en&#x2F;latest&#x2F;</a></div><br/><div id="42176384" class="c"><input type="checkbox" id="c-42176384" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176321">parent</a><span>|</span><a href="#42191010">next</a><span>|</span><label class="collapse" for="c-42176384">[-]</label><label class="expand" for="c-42176384">[1 more]</label></div><br/><div class="children"><div class="content">I think this is a great insight, and something that I think about often. The challenge that I see is that the scientist archetype (whether it&#x27;s data science, AI researcher, or anything else) isn&#x27;t really interested in doing software development for these kinds of things. They just want the data to <i>be there</i>, and it&#x27;s super nice to be able to click through the S3 console to be able to see and share the data their using. I think that what you&#x27;re doing is a great idea for folks who are accessing their data primarily through Python programs!</div><br/></div></div></div></div><div id="42191010" class="c"><input type="checkbox" id="c-42191010" checked=""/><div class="controls bullet"><span class="by">nisten</span><span>|</span><a href="#42176321">prev</a><span>|</span><a href="#42174866">next</a><span>|</span><label class="collapse" for="c-42191010">[-]</label><label class="expand" for="c-42191010">[1 more]</label></div><br/><div class="children"><div class="content">Ok that&#x27;s cool but like... you could&#x27;ve just given me a bashscript to do the same thing instead of the pitchdeck-followup baggage of the n-th try at recreating the dropbox lottery shot from a decade and a half ago...</div><br/></div></div><div id="42174866" class="c"><input type="checkbox" id="c-42174866" checked=""/><div class="controls bullet"><span class="by">mikecwang</span><span>|</span><a href="#42191010">prev</a><span>|</span><a href="#42187886">next</a><span>|</span><label class="collapse" for="c-42174866">[-]</label><label class="expand" for="c-42174866">[9 more]</label></div><br/><div class="children"><div class="content">Does it mean I can use Lambda + SQLite + Regatta to build a real pay-as-you-go ACID SQL storage?<p>Edit: an production-ready (high durability) ACID SQL storage</div><br/><div id="42174881" class="c"><input type="checkbox" id="c-42174881" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174866">parent</a><span>|</span><a href="#42175822">next</a><span>|</span><label class="collapse" for="c-42174881">[-]</label><label class="expand" for="c-42174881">[5 more]</label></div><br/><div class="children"><div class="content">Yes! This is my expectation. Lots of the big companies have already done this with in-house architecture. With Regatta, we want to democratize building stateless applications that can take advantage of the low-cost storage of S3.</div><br/><div id="42174921" class="c"><input type="checkbox" id="c-42174921" checked=""/><div class="controls bullet"><span class="by">mikecwang</span><span>|</span><a href="#42174866">root</a><span>|</span><a href="#42174881">parent</a><span>|</span><a href="#42181154">next</a><span>|</span><label class="collapse" for="c-42174921">[-]</label><label class="expand" for="c-42174921">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s some real tech in YC these days!</div><br/></div></div><div id="42181154" class="c"><input type="checkbox" id="c-42181154" checked=""/><div class="controls bullet"><span class="by">hades32</span><span>|</span><a href="#42174866">root</a><span>|</span><a href="#42174881">parent</a><span>|</span><a href="#42174921">prev</a><span>|</span><a href="#42175822">next</a><span>|</span><label class="collapse" for="c-42181154">[-]</label><label class="expand" for="c-42181154">[3 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t that limit the concurrency of the lambdas to 1? Since they would hold a lock on the db file</div><br/><div id="42184041" class="c"><input type="checkbox" id="c-42184041" checked=""/><div class="controls bullet"><span class="by">datadeft</span><span>|</span><a href="#42174866">root</a><span>|</span><a href="#42181154">parent</a><span>|</span><a href="#42175822">next</a><span>|</span><label class="collapse" for="c-42184041">[-]</label><label class="expand" for="c-42184041">[2 more]</label></div><br/><div class="children"><div class="content">Well these are the details that many of us is interested.</div><br/><div id="42184548" class="c"><input type="checkbox" id="c-42184548" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174866">root</a><span>|</span><a href="#42184041">parent</a><span>|</span><a href="#42175822">next</a><span>|</span><label class="collapse" for="c-42184548">[-]</label><label class="expand" for="c-42184548">[1 more]</label></div><br/><div class="children"><div class="content">This is super dependent on the application, and not something that I could answer without being an expert in SQLite. If SQLite only allows a single reader or writer, then yes. This could still be a good choice for applications which elect a “leader” to serve the database, though.</div><br/></div></div></div></div></div></div></div></div><div id="42175822" class="c"><input type="checkbox" id="c-42175822" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#42174866">parent</a><span>|</span><a href="#42174881">prev</a><span>|</span><a href="#42187886">next</a><span>|</span><label class="collapse" for="c-42175822">[-]</label><label class="expand" for="c-42175822">[3 more]</label></div><br/><div class="children"><div class="content">Curious as to why you would want to build that yourself when so many solutions already exist (Supabase, NeonDB, AWS Aurora or RDS, etc.)?</div><br/><div id="42175971" class="c"><input type="checkbox" id="c-42175971" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174866">root</a><span>|</span><a href="#42175822">parent</a><span>|</span><a href="#42187886">next</a><span>|</span><label class="collapse" for="c-42175971">[-]</label><label class="expand" for="c-42175971">[2 more]</label></div><br/><div class="children"><div class="content">One of my hopes for Regatta is that we&#x27;re able to power the next generation of these data platforms. These things work because the designers had specialized storage knowledge that allowed them to carefully build serverless data products. I hope that Regatta is generic enough to allow anyone to build a serverless data product moving forward, without having to think about their storage infrastructure.</div><br/><div id="42177203" class="c"><input type="checkbox" id="c-42177203" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#42174866">root</a><span>|</span><a href="#42175971">parent</a><span>|</span><a href="#42187886">next</a><span>|</span><label class="collapse" for="c-42177203">[-]</label><label class="expand" for="c-42177203">[1 more]</label></div><br/><div class="children"><div class="content">That makes a lot of sense.  If you eliminate the need for storage expertise the problem becomes a lot easier!<p>BTW I sent you an email.</div><br/></div></div></div></div></div></div></div></div><div id="42187886" class="c"><input type="checkbox" id="c-42187886" checked=""/><div class="controls bullet"><span class="by">itissid</span><span>|</span><a href="#42174866">prev</a><span>|</span><a href="#42189747">next</a><span>|</span><label class="collapse" for="c-42187886">[-]</label><label class="expand" for="c-42187886">[2 more]</label></div><br/><div class="children"><div class="content">Noob Question: When an average person buys 2TB storage from a cloud provider one pays upfront for the entire thing. Would pricing for a product be made more competitive(vs dropbox) using such a solution?<p>It takes somtimes years to fill it up with photos, vidoes and other documents. Sounds like one could build a great killer low amortized – pay as you fill it up – service for people to compete with dropbox.</div><br/><div id="42187965" class="c"><input type="checkbox" id="c-42187965" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42187886">parent</a><span>|</span><a href="#42189747">next</a><span>|</span><label class="collapse" for="c-42187965">[-]</label><label class="expand" for="c-42187965">[1 more]</label></div><br/><div class="children"><div class="content">This is true, and I think that there are consumer (or at least &quot;run on your laptop&quot;) versions of this that could make sense. However, the technology underneath would have to look <i>very</i> different. For example, these protocols are designed for online file systems (e.g. you must be connected to the file system directly in order to list what&#x27;s in a directory). This works great in a data center, but doesn&#x27;t work great on your laptop.<p>On the other hand, something like Dropbox is actually a program running on your laptop that <i>simulates</i> a file system, and then does the synchronization at the file level as needed. I think that there&#x27;s probably some latent demand for a similar product for developers to access their S3 buckets easily from their laptops, and it&#x27;s something we might look into as we get farther along.</div><br/></div></div></div></div><div id="42189747" class="c"><input type="checkbox" id="c-42189747" checked=""/><div class="controls bullet"><span class="by">objectivefs</span><span>|</span><a href="#42187886">prev</a><span>|</span><a href="#42176227">next</a><span>|</span><label class="collapse" for="c-42189747">[-]</label><label class="expand" for="c-42189747">[2 more]</label></div><br/><div class="children"><div class="content">Congratulations on your launch from ObjectiveFS! There is a lot of interest in 1-to-1 filesystems for mixed workloads, hope you can capture a nice share of that.<p>Using NFS and being able to use an existing bucket is a nice way to make it easy to get started and try things out. For applications that need full consistency between the S3 and the filesystem view, you can even provide an S3 proxy endpoint on your durable cache that removes any synchronization delays.</div><br/><div id="42190111" class="c"><input type="checkbox" id="c-42190111" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42189747">parent</a><span>|</span><a href="#42176227">next</a><span>|</span><label class="collapse" for="c-42190111">[-]</label><label class="expand" for="c-42190111">[1 more]</label></div><br/><div class="children"><div class="content">Thank you so much! It’s been amazing to see what you all have built over the years, and it’s (of course) been inspirational for me.</div><br/></div></div></div></div><div id="42176227" class="c"><input type="checkbox" id="c-42176227" checked=""/><div class="controls bullet"><span class="by">hitekker</span><span>|</span><a href="#42189747">prev</a><span>|</span><a href="#42179784">next</a><span>|</span><label class="collapse" for="c-42176227">[-]</label><label class="expand" for="c-42176227">[2 more]</label></div><br/><div class="children"><div class="content">This looks quite compelling.<p>But it&#x27;s not clear how it handles file update conflicts.
For example: if User A updates File X on one computer, and User B  updates File X on another computer, what does the final file look like in S3?</div><br/><div id="42176263" class="c"><input type="checkbox" id="c-42176263" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42176227">parent</a><span>|</span><a href="#42179784">next</a><span>|</span><label class="collapse" for="c-42176263">[-]</label><label class="expand" for="c-42176263">[1 more]</label></div><br/><div class="children"><div class="content">Hey there, our file system is strongly consistent for all connected file system clients. For example, if User A and User B are both connected via Regatta, then this works like any other NFS file system (in that they can use file locks, atomic renames or other techniques to ensure that one write wins). However, if User A and User B are accessing the data through different protocols (for example User A is using Regatta and User B is accessing the data through S3), then it&#x27;s possible to get undefined behavior by attempting to simultaneously update the same piece of data from both places. We think that these applications are rare, and (almost by definition) likely don&#x27;t exist right now. For the most part, customers use file storage as a &quot;stage&quot; in a broader workflow (for example, customers may ingest data through S3 and then process it on a file system), and that is totally consistent.</div><br/></div></div></div></div><div id="42179784" class="c"><input type="checkbox" id="c-42179784" checked=""/><div class="controls bullet"><span class="by">zX41ZdbW</span><span>|</span><a href="#42176227">prev</a><span>|</span><a href="#42175379">next</a><span>|</span><label class="collapse" for="c-42179784">[-]</label><label class="expand" for="c-42179784">[3 more]</label></div><br/><div class="children"><div class="content">That is interesting, but I haven&#x27;t read how it is implemented yet.<p>The hard part is a cache layer with immediate consistency. It likely requires RAFT (or, otherwise, works incorrectly). Integration of this cache layer with S3 (offloading cold data to S3) is easy (not interesting).<p>It should not be compared to s3fs, mountpoint, geesefs, etc., because they lack consistency and also slow and also don&#x27;t support full filesystem semantics, and break often.<p>It could be compared with AWS EFS. Which is also slow (but I didn&#x27;t try to tune it up to maximum numbers).<p>For ClickHouse, this system is unneeded because ClickHouse is already distributed (it supports full replication or shared storage + cache), and it does not require full filesystem semantics (it pairs with blob storages nicely).</div><br/><div id="42179882" class="c"><input type="checkbox" id="c-42179882" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42179784">parent</a><span>|</span><a href="#42188267">next</a><span>|</span><label class="collapse" for="c-42179882">[-]</label><label class="expand" for="c-42179882">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the note, great to hear from you! I think that what Clickhouse does is great, and I expect that more applications want to take advantage of the low prices of S3 cold storage without needing to build their own application-level abstractions. I&#x27;m hopeful that this allows more of these next-generation serverless data products to exist.</div><br/></div></div><div id="42188267" class="c"><input type="checkbox" id="c-42188267" checked=""/><div class="controls bullet"><span class="by">dangoodmanUT</span><span>|</span><a href="#42179784">parent</a><span>|</span><a href="#42179882">prev</a><span>|</span><a href="#42175379">next</a><span>|</span><label class="collapse" for="c-42188267">[-]</label><label class="expand" for="c-42188267">[1 more]</label></div><br/><div class="children"><div class="content">there are more ways to achieve consensus than raft</div><br/></div></div></div></div><div id="42175379" class="c"><input type="checkbox" id="c-42175379" checked=""/><div class="controls bullet"><span class="by">debarshri</span><span>|</span><a href="#42179784">prev</a><span>|</span><a href="#42174305">next</a><span>|</span><label class="collapse" for="c-42175379">[-]</label><label class="expand" for="c-42175379">[7 more]</label></div><br/><div class="children"><div class="content">There are quite some noteworthy alternatives like s3fs, rclone, goofys etc.</div><br/><div id="42175395" class="c"><input type="checkbox" id="c-42175395" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175379">parent</a><span>|</span><a href="#42174305">next</a><span>|</span><label class="collapse" for="c-42175395">[-]</label><label class="expand" for="c-42175395">[6 more]</label></div><br/><div class="children"><div class="content">This is accurate! A lot of people have spent a lot of time trying to build a good file system abstraction on cheap, S3 storage. However, Regatta differs from these solutions in two important ways. First, Regatta is a shared, durable caching layer that sits between your instances and S3. This means that Regatta is able to efficiently perform operations (like directory renames) and provide strong consistency to other file system clients (whereas s3fs and other FUSE file systems would need to actually perform those operations in S3 for other clients to see the output). Secondly, Regatta is designed to support all file system operations. This means that you can do file locking, random writes, appends, and renames -- even when they aren&#x27;t efficient to perform on S3.</div><br/><div id="42175570" class="c"><input type="checkbox" id="c-42175570" checked=""/><div class="controls bullet"><span class="by">aidos</span><span>|</span><a href="#42175379">root</a><span>|</span><a href="#42175395">parent</a><span>|</span><a href="#42174305">next</a><span>|</span><label class="collapse" for="c-42175570">[-]</label><label class="expand" for="c-42175570">[5 more]</label></div><br/><div class="children"><div class="content">Super interesting product. I have a couple of questions:<p>In terms of storing in s3 - is that in your buckets? Sound like the plan is to run the caching on your infrastructure, are there plans to allow customers to run those instances themselves?<p>Presumably the format within s3 is your own bespoke format? What does the migration strategy look like for people looking to move into or out of your infrastructure? They effectively pull everything down from their s3 to the local “filesystem”?</div><br/><div id="42175591" class="c"><input type="checkbox" id="c-42175591" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175379">root</a><span>|</span><a href="#42175570">parent</a><span>|</span><a href="#42174305">next</a><span>|</span><label class="collapse" for="c-42175591">[-]</label><label class="expand" for="c-42175591">[4 more]</label></div><br/><div class="children"><div class="content">I love this because it allows me to highlight the parts of the system that I&#x27;m most excited about. The Regatta caching runs on our infrastructure, but it connects to buckets that our customers control. We read and write data into the customer&#x27;s bucket in a regular, native (not bespoke) format -- so you can connect a Regatta file system directly to a bucket that already exists, with data in it, and use that data from a file system without any data migration!</div><br/><div id="42176644" class="c"><input type="checkbox" id="c-42176644" checked=""/><div class="controls bullet"><span class="by">aidos</span><span>|</span><a href="#42175379">root</a><span>|</span><a href="#42175591">parent</a><span>|</span><a href="#42176352">next</a><span>|</span><label class="collapse" for="c-42176644">[-]</label><label class="expand" for="c-42176644">[2 more]</label></div><br/><div class="children"><div class="content">Oh interesting! So you map exactly to the structure in s3? It’s like fuse backed by s3 with good performance?</div><br/><div id="42177662" class="c"><input type="checkbox" id="c-42177662" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175379">root</a><span>|</span><a href="#42176644">parent</a><span>|</span><a href="#42176352">next</a><span>|</span><label class="collapse" for="c-42177662">[-]</label><label class="expand" for="c-42177662">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s exactly right -- I like to think that we deliver on the promise of those open-source S3 adapters. We provide enterprise-grade performance.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42174305" class="c"><input type="checkbox" id="c-42174305" checked=""/><div class="controls bullet"><span class="by">koolba</span><span>|</span><a href="#42175379">prev</a><span>|</span><a href="#42183182">next</a><span>|</span><label class="collapse" for="c-42174305">[-]</label><label class="expand" for="c-42174305">[8 more]</label></div><br/><div class="children"><div class="content">Neat stuff. I think everybody with an interest in NFS has toyed with this idea at some point.<p>&gt; Under the hood, customers mount a Regatta file system by connecting to our fleet of caching instances over NFSv3 (soon, our custom protocol). Our instances then connect to the customer’s S3 bucket on the backend, and provide sub-millisecond cached-read and write performance. This durable cache allows us to provide a strongly consistent, efficient view of the file system to all connected file clients. We can perform challenging operations (like directory renaming) quickly and durably, while they asynchronously propagate to the S3 bucket.<p>How do you handle the cache server crashing before syncing to S3? Do the cache servers have local disk as well?<p>Ditto for how to handle intermittent S3 availability issues?<p>What are the fsync guarantees for file append operations and directories?</div><br/><div id="42174365" class="c"><input type="checkbox" id="c-42174365" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174305">parent</a><span>|</span><a href="#42183182">next</a><span>|</span><label class="collapse" for="c-42174365">[-]</label><label class="expand" for="c-42174365">[7 more]</label></div><br/><div class="children"><div class="content">Thanks for the question!<p>&gt; How do you handle the cache server crashing before syncing to S3? Do the cache servers have local disk as well?<p>Our caching layer is highly durable, which is (in my opinion) the key for doing this kind of staging. This means that once a write is complete to Regatta, we guarantee that it will eventually complete on S3.<p>For this reason, server crashes and intermittent S3 availability issues are not a problem because we have the writes stored safely.<p>&gt; What are the fsync guarantees for file append operations and directories?<p>We have strong, read-after-write consistency for all connected file system clients -- including for operations which aren&#x27;t possible to perform on S3 efficiently (such as renames, appends, etc). We asynchronously push those writes to S3, so there may be a few minutes before you can access them directly from the bucket. But, during this time, the file system interface will always reflect the up-to-date view.</div><br/><div id="42175912" class="c"><input type="checkbox" id="c-42175912" checked=""/><div class="controls bullet"><span class="by">the_duke</span><span>|</span><a href="#42174305">root</a><span>|</span><a href="#42174365">parent</a><span>|</span><a href="#42175879">next</a><span>|</span><label class="collapse" for="c-42175912">[-]</label><label class="expand" for="c-42175912">[2 more]</label></div><br/><div class="children"><div class="content">So, I assume you use a journal in the cache server.<p>A few related questions:<p>* Do you use a single leader for a specific file system, or do you have a cluster solution with consensus to enable scaling&#x2F;redundancy?<p>* How do you guarantee read-after-write consistency? Do you stream the journal to all clients and wait for them to ack before the write finishes? Or at least wait for everyone to ack the latest revisions for files, while the content is streamed out separately&#x2F;requested on demand?<p>* If the above is true, I assume this is strictly viable for single-DC usage due to latency? Do you support different mount options for different consistency guarantees?</div><br/><div id="42175986" class="c"><input type="checkbox" id="c-42175986" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174305">root</a><span>|</span><a href="#42175912">parent</a><span>|</span><a href="#42175879">next</a><span>|</span><label class="collapse" for="c-42175986">[-]</label><label class="expand" for="c-42175986">[1 more]</label></div><br/><div class="children"><div class="content">These are questions that are super specific to our implementation, that I&#x27;m hesitant to share publicly because they could change any at any time. I can share that we&#x27;re designed to horizontally scale the performance of each file system, and our custom protocol will enable Lustre-like scale out performance. As for single- vs. multi-DC, I think that you&#x27;d be surprised at how much latency budget there is (a cross-DC round trip in AWS can be anywhere from 200us-700us, and EBS gp3 latencies are around 1000us).</div><br/></div></div></div></div><div id="42175879" class="c"><input type="checkbox" id="c-42175879" checked=""/><div class="controls bullet"><span class="by">koolba</span><span>|</span><a href="#42174305">root</a><span>|</span><a href="#42174365">parent</a><span>|</span><a href="#42175912">prev</a><span>|</span><a href="#42174934">next</a><span>|</span><label class="collapse" for="c-42175879">[-]</label><label class="expand" for="c-42175879">[2 more]</label></div><br/><div class="children"><div class="content">Is it fair to say this is best suited for small files that will be written infrequently?<p>There’s no partial write for s3 so editing a small range of a 1 GiB file would repeatedly upload the full file to the backing s3 right?<p>Or is the s3 representation not the same hierarchy as the presented mount point? (ie something opaque like a log structured &#x2F; append only chunked list)</div><br/><div id="42176003" class="c"><input type="checkbox" id="c-42176003" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174305">root</a><span>|</span><a href="#42175879">parent</a><span>|</span><a href="#42174934">next</a><span>|</span><label class="collapse" for="c-42176003">[-]</label><label class="expand" for="c-42176003">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s hard to define &quot;best&quot;, and in many cases, the answers to these questions depend heavily on the workload and the caching parameters (how long do we wait before flushing to S3, etc). We are designed to provide good file system performance, even if customers are repeatedly writing small pieces of data to a 1 GiB file, so &quot;best&quot; in this case is a question of whether or not it&#x27;s cost efficient.</div><br/></div></div></div></div><div id="42174934" class="c"><input type="checkbox" id="c-42174934" checked=""/><div class="controls bullet"><span class="by">paulgb</span><span>|</span><a href="#42174305">root</a><span>|</span><a href="#42174365">parent</a><span>|</span><a href="#42175879">prev</a><span>|</span><a href="#42183182">next</a><span>|</span><label class="collapse" for="c-42174934">[-]</label><label class="expand" for="c-42174934">[2 more]</label></div><br/><div class="children"><div class="content">Congrats on the launch, this is really cool! Is the durable cache an attached disk, or are you using a separate AWS product for that?</div><br/><div id="42174959" class="c"><input type="checkbox" id="c-42174959" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174305">root</a><span>|</span><a href="#42174934">parent</a><span>|</span><a href="#42183182">next</a><span>|</span><label class="collapse" for="c-42174959">[-]</label><label class="expand" for="c-42174959">[1 more]</label></div><br/><div class="children"><div class="content">Without getting too much into the details of the system, our durable cache is designed for 5 9s of durability (and we&#x27;re working on a version that will provide 11 9s of durability soon). You can&#x27;t achieve those durability numbers on a single attached NVMe device without some kind of replication.</div><br/></div></div></div></div></div></div></div></div><div id="42183182" class="c"><input type="checkbox" id="c-42183182" checked=""/><div class="controls bullet"><span class="by">siscia</span><span>|</span><a href="#42174305">prev</a><span>|</span><a href="#42182488">next</a><span>|</span><label class="collapse" for="c-42183182">[-]</label><label class="expand" for="c-42183182">[7 more]</label></div><br/><div class="children"><div class="content">How do you handle wrote concurrency?<p>If you different processes write on the same file at the same time, what do I read after?</div><br/><div id="42184442" class="c"><input type="checkbox" id="c-42184442" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42183182">parent</a><span>|</span><a href="#42182488">next</a><span>|</span><label class="collapse" for="c-42184442">[-]</label><label class="expand" for="c-42184442">[6 more]</label></div><br/><div class="children"><div class="content">All connected file system clients see read-after-write consistency, so you see the up to date file data!</div><br/><div id="42186443" class="c"><input type="checkbox" id="c-42186443" checked=""/><div class="controls bullet"><span class="by">mdaniel</span><span>|</span><a href="#42183182">root</a><span>|</span><a href="#42184442">parent</a><span>|</span><a href="#42182488">next</a><span>|</span><label class="collapse" for="c-42186443">[-]</label><label class="expand" for="c-42186443">[5 more]</label></div><br/><div class="children"><div class="content">I heard you about the &quot;limited hands, infinite wishlist&quot; but nowadays when I see someone making bold claims about transactions and consistency over the network, I grab my popcorn bucket and eagerly await the Jepsen report about it<p>The good news is that you, personally, don&#x27;t have to spend the time to create the Jepsen test harness, you can pay them to run the test but I have <i>no idea</i> what kind of O($) we&#x27;re talking here. Still, it could be worth it to inspire confidence, and is almost an imperative if you&#x27;re going to (ahem) roll your own protocol for network file access :-&#x2F;</div><br/><div id="42186798" class="c"><input type="checkbox" id="c-42186798" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42183182">root</a><span>|</span><a href="#42186443">parent</a><span>|</span><a href="#42188537">next</a><span>|</span><label class="collapse" for="c-42186798">[-]</label><label class="expand" for="c-42186798">[2 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve actually been thinking about getting Jepsen to do this, so I&#x27;m happy to hear that you also think that it would inspire confidence!</div><br/><div id="42187606" class="c"><input type="checkbox" id="c-42187606" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#42183182">root</a><span>|</span><a href="#42186798">parent</a><span>|</span><a href="#42188537">next</a><span>|</span><label class="collapse" for="c-42187606">[-]</label><label class="expand" for="c-42187606">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s exactly right!</div><br/></div></div></div></div><div id="42188537" class="c"><input type="checkbox" id="c-42188537" checked=""/><div class="controls bullet"><span class="by">geertj</span><span>|</span><a href="#42183182">root</a><span>|</span><a href="#42186443">parent</a><span>|</span><a href="#42186798">prev</a><span>|</span><a href="#42182488">next</a><span>|</span><label class="collapse" for="c-42188537">[-]</label><label class="expand" for="c-42188537">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I grab my popcorn bucket and eagerly await the Jepsen report about it<p>I am the same, as distributed consensus is notoriously hard especially when it fronts distributed storage.<p>However, it is not imposssible.. Hunter and I were both in the EFS team at AWS (I am still there), and he was deeply involved in all aspects of our consensus and replication layers. So if anyone can do it, Hunter is!</div><br/><div id="42190148" class="c"><input type="checkbox" id="c-42190148" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42183182">root</a><span>|</span><a href="#42188537">parent</a><span>|</span><a href="#42182488">next</a><span>|</span><label class="collapse" for="c-42190148">[-]</label><label class="expand" for="c-42190148">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for the kind words, Geert!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42182488" class="c"><input type="checkbox" id="c-42182488" checked=""/><div class="controls bullet"><span class="by">unit149</span><span>|</span><a href="#42183182">prev</a><span>|</span><a href="#42174818">next</a><span>|</span><label class="collapse" for="c-42182488">[-]</label><label class="expand" for="c-42182488">[2 more]</label></div><br/><div class="children"><div class="content">Was taking a look at pricing features - melted down, paying per month doesn&#x27;t seem like a bad option; still, the API features 1 hour SLA support for enterprise tier subscribers.<p>S3 bucket systems for cloud hosting services are typically encrypted through AES-256. SSE-S3 or SSE-KMS are available upon request.<p>[1]: <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;new-amazon-s3-encryption-security-features&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;new-amazon-s3-encryption-se...</a><p>Having the API hosted on Regatta&#x27;s servers but integrating a POSIX-compliant bring-your-own compute would tighten up instance storage fees for the end-user.<p>[1]:<a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;new-amazon-s3-encryption-security-features&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;blogs&#x2F;aws&#x2F;new-amazon-s3-encryption-se...</a></div><br/><div id="42184511" class="c"><input type="checkbox" id="c-42184511" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42182488">parent</a><span>|</span><a href="#42174818">next</a><span>|</span><label class="collapse" for="c-42184511">[-]</label><label class="expand" for="c-42184511">[1 more]</label></div><br/><div class="children"><div class="content">All data cached in Regatta is <i>also</i> encrypted with AES-256<p>Re: bring your own compute: It’s certainly something we’re thinking about. We are in discussions with a lot of customers running GPU clusters with orphaned NVMe resources that they would like to install Regatta on. We’d love to get more details on who’s out there looking for this, so please shoot me an email at hleath [at] regattastorage.com</div><br/></div></div></div></div><div id="42174818" class="c"><input type="checkbox" id="c-42174818" checked=""/><div class="controls bullet"><span class="by">count</span><span>|</span><a href="#42182488">prev</a><span>|</span><a href="#42185334">next</a><span>|</span><label class="collapse" for="c-42174818">[-]</label><label class="expand" for="c-42174818">[8 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see any other question about it, so maybe I just missed the obvious answer, but how do you handle POSIX ACLs?  If the data is stored as an object in S3, but exposed via filesystem, where are you keeping (if at all?) the filesystem ACLs and metadata?<p>Also, NFSv3 and not 4?</div><br/><div id="42174874" class="c"><input type="checkbox" id="c-42174874" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174818">parent</a><span>|</span><a href="#42185334">next</a><span>|</span><label class="collapse" for="c-42174874">[-]</label><label class="expand" for="c-42174874">[7 more]</label></div><br/><div class="children"><div class="content">Great call out. Some kinds of data, like ACLs and specific kinds of metadata, don&#x27;t live in S3. Full disclosure, we don&#x27;t support ACLs today (but plan to soon). We keep file system metadata in the durable cache. For some files (where users haven&#x27;t changed permissions, etc), we are able to release that cached metadata when the file is no longer in use. For other files (where permissions have been changed by the user), that metadata must live in the cache long-term.<p>We selected NFSv3 due to it&#x27;s broad compatibility with different compute environments. For example, Windows has an NFSv3 client in it, but doesn&#x27;t have an NFSv4 client. There are lots of enterprise workloads which needs simultaneous access to file data from both Windows and Linux, and supporting NFSv3 was the easiest path to support those workloads.</div><br/><div id="42177171" class="c"><input type="checkbox" id="c-42177171" checked=""/><div class="controls bullet"><span class="by">secabeen</span><span>|</span><a href="#42174818">root</a><span>|</span><a href="#42174874">parent</a><span>|</span><a href="#42175271">next</a><span>|</span><label class="collapse" for="c-42177171">[-]</label><label class="expand" for="c-42177171">[4 more]</label></div><br/><div class="children"><div class="content">Do you pay for metadata accesses?  Does running a `find` across the filesystem cost anything?  What about system calls that don&#x27;t transfer data?  Can I move or rename a file without paying to copy and then delete the associated S3 object?</div><br/><div id="42177334" class="c"><input type="checkbox" id="c-42177334" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174818">root</a><span>|</span><a href="#42177171">parent</a><span>|</span><a href="#42175271">next</a><span>|</span><label class="collapse" for="c-42177334">[-]</label><label class="expand" for="c-42177334">[3 more]</label></div><br/><div class="children"><div class="content">Today, we only charge for cache usage (storage) and data transfer between Regatta and S3. If your metadata access doesn&#x27;t require transfer to S3, then it doesn&#x27;t cost anything! However, renames <i>do</i> require transfer to S3 (because we have to move the object on the backend).</div><br/><div id="42181161" class="c"><input type="checkbox" id="c-42181161" checked=""/><div class="controls bullet"><span class="by">hades32</span><span>|</span><a href="#42174818">root</a><span>|</span><a href="#42177334">parent</a><span>|</span><a href="#42175271">next</a><span>|</span><label class="collapse" for="c-42181161">[-]</label><label class="expand" for="c-42181161">[2 more]</label></div><br/><div class="children"><div class="content">does that mean you pay for the storage twice (i.e. S3 and Regatta) or is the cache size tunable?</div><br/><div id="42184615" class="c"><input type="checkbox" id="c-42184615" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174818">root</a><span>|</span><a href="#42181161">parent</a><span>|</span><a href="#42175271">next</a><span>|</span><label class="collapse" for="c-42184615">[-]</label><label class="expand" for="c-42184615">[1 more]</label></div><br/><div class="children"><div class="content">That’s correct — you pay for the storage yourself in S3, and then you pay for the storage when it’s in the Regatta cache. We may expose the ability to limit the cache size in the future for teams who need controllable costs more than the highest performance.</div><br/></div></div></div></div></div></div></div></div><div id="42175271" class="c"><input type="checkbox" id="c-42175271" checked=""/><div class="controls bullet"><span class="by">count</span><span>|</span><a href="#42174818">root</a><span>|</span><a href="#42174874">parent</a><span>|</span><a href="#42177171">prev</a><span>|</span><a href="#42185334">next</a><span>|</span><label class="collapse" for="c-42175271">[-]</label><label class="expand" for="c-42175271">[2 more]</label></div><br/><div class="children"><div class="content">Thanks, I keep hoping someone comes up with some magic :)<p>Is the intent to run this in-vpc?<p>And how do you differentiate from AWS Storage Gateway?</div><br/><div id="42175368" class="c"><input type="checkbox" id="c-42175368" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174818">root</a><span>|</span><a href="#42175271">parent</a><span>|</span><a href="#42185334">next</a><span>|</span><label class="collapse" for="c-42175368">[-]</label><label class="expand" for="c-42175368">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to hear more about what you&#x27;re excited to do when the magic arrives. :D<p>We are running it as a managed SaaS, so our customers connect to the caching layer that runs in the Regatta VPC. This allows us to manage the infrastructure for them and keep costs low.<p>Storage Gateway is an interesting product, and I worked closely with that team for several years -- so mad respect for them. It was designed to be an appliance that you run on servers in your own data center (of course, many customers now deploy it to EC2). Because of this, it&#x27;s designed to operate in an environment with &quot;finite storage&quot; -- for example, different workload pattterns can thrash the cache, which results in poor performance to clients, and it&#x27;s not designed to run in a high-availability cluster in the cloud. Regatta solves these problems with durable cache storage that&#x27;s safe to data in long-term, and is designed for high-availability.</div><br/></div></div></div></div></div></div></div></div><div id="42185334" class="c"><input type="checkbox" id="c-42185334" checked=""/><div class="controls bullet"><span class="by">Melonotromo</span><span>|</span><a href="#42174818">prev</a><span>|</span><a href="#42188429">next</a><span>|</span><label class="collapse" for="c-42185334">[-]</label><label class="expand" for="c-42185334">[4 more]</label></div><br/><div class="children"><div class="content">Your pricepoint is very bad. The overprovicioning statement in your Post indicated that you would be a &#x27;cheap&#x27; alternative but 100gb for $5?<p>I&#x27;m also not sure that its a good architecture to have your servers inbetween my S3. If i&#x27;m on one cloud provider, the traffic between their S3 compatible solution and my infrastructure is most of the time in the same cloud provider. And if not, i will for sure have a local cache rcloning the stuff from left to right.<p>I also don&#x27;t get your calculator at all.</div><br/><div id="42185428" class="c"><input type="checkbox" id="c-42185428" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42185334">parent</a><span>|</span><a href="#42188429">next</a><span>|</span><label class="collapse" for="c-42185428">[-]</label><label class="expand" for="c-42185428">[3 more]</label></div><br/><div class="children"><div class="content">Thanks for the feedback. If price is the single blocker for teams to try the product, I&#x27;d love to discuss more. Please send me an email at hleath [at] regattastorage.com.<p>&gt; If i&#x27;m on one cloud provider, the traffic between their S3 compatible solution and my infrastructure is most of the time in the same cloud provider<p>This is exactly right, and it&#x27;s why we&#x27;re working to deploy our infrastructure to every major cloud. We don&#x27;t want customers paying egress costs or cross-cloud latency to use Regatta.<p>&gt; I also don&#x27;t get your calculator at all.<p>This could probably use a bit more explanation on the website. We&#x27;re comparing to the usage of local devices. We find that, most often, teams will only use 15% of the EBS volumes that they&#x27;ve purchased (over a monthly time period). This means that instead of paying $0.125&#x2F;GiB-mo of storage (like io2 offers), they&#x27;re actually paying $0.833&#x2F;GiB-mo of actual bytes stored ($0.125&#x2F;15%). Whereas on Regatta, they&#x27;re only paying for what they use -- which is a combination of our caching layer ($0.20) and S3 ($0.025). That averages out closer to $0.10&#x2F;GiB stored, depending on the amount of data that you use.</div><br/><div id="42185478" class="c"><input type="checkbox" id="c-42185478" checked=""/><div class="controls bullet"><span class="by">Melonotromo</span><span>|</span><a href="#42185334">root</a><span>|</span><a href="#42185428">parent</a><span>|</span><a href="#42188429">next</a><span>|</span><label class="collapse" for="c-42185478">[-]</label><label class="expand" for="c-42185478">[2 more]</label></div><br/><div class="children"><div class="content">What is then your initial latency if i start an AI job &#x27;fresh&#x27;? You still need to hit the backend right? How long do you then keep this data in your cache?<p>Btw. while your experience works well for Netflix, in my company (also very big), we have LoBs and while different teams utilize their storage in a different way, none of us are aligned on a level that we would benefit directly from your solution.<p>From a pure curiosity point of view: Do you have already enough customers which have savings? What are their use cases? The size of their setups?</div><br/><div id="42186239" class="c"><input type="checkbox" id="c-42186239" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42185334">root</a><span>|</span><a href="#42185478">parent</a><span>|</span><a href="#42188429">next</a><span>|</span><label class="collapse" for="c-42186239">[-]</label><label class="expand" for="c-42186239">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What is then your initial latency if i start an AI job &#x27;fresh&#x27;? You still need to hit the backend right? How long do you then keep this data in your cache?<p>That&#x27;s correct, and it&#x27;s something that we can tune if there&#x27;s a specific need. For AI use cases specifically, we&#x27;re working on adding functionality to &quot;pre-load&quot; the cache with your data. For example, you would be able to call an API that says &quot;I&#x27;m about to start a job and I need this directory on the cache&quot;. We would then be able to fan out our infrastructure to download that data <i>very</i> quickly (think hundreds of GiB&#x2F;s) -- much faster than any individual instance could download the data. Then your job would be able to access the data set at low-latency. Does that sound like it would make sense for you?<p>&gt; Btw. while your experience works well for Netflix, in my company (also very big), we have LoBs and while different teams utilize their storage in a different way, none of us are aligned on a level that we would benefit directly from your solution.<p>I&#x27;m not totally sure what you mean here. I don&#x27;t anticipate that a large organization would have to 100% buy-in to Regatta in order to get benefits. In fact, this is the reason why we are so intent on having a serverless product that &quot;scales to 0&quot;. That would allow each of your teams to independently try Regatta without needing to spend hundreds of thousands of dollars on something Day 1 for the entire company.<p>&gt; From a pure curiosity point of view: Do you have already enough customers which have savings? What are their use cases? The size of their setups?<p>These are pretty intimate details about the business, and I don&#x27;t think I can share very specific data. However, yes -- we do have customers who are realizing massive savings (50%+) over their existing set ups.</div><br/></div></div></div></div></div></div></div></div><div id="42188429" class="c"><input type="checkbox" id="c-42188429" checked=""/><div class="controls bullet"><span class="by">craigkilgo</span><span>|</span><a href="#42185334">prev</a><span>|</span><a href="#42174429">next</a><span>|</span><label class="collapse" for="c-42188429">[-]</label><label class="expand" for="c-42188429">[2 more]</label></div><br/><div class="children"><div class="content">How did you choose the name Regatta?</div><br/><div id="42190146" class="c"><input type="checkbox" id="c-42190146" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42188429">parent</a><span>|</span><a href="#42174429">next</a><span>|</span><label class="collapse" for="c-42190146">[-]</label><label class="expand" for="c-42190146">[1 more]</label></div><br/><div class="children"><div class="content">I spent several years working in the Northeast, and I developed an appreciation (but not a skill) for sailing. In some sense, I think of Regatta as high-speed sailing on top of customer data lakes.</div><br/></div></div></div></div><div id="42174429" class="c"><input type="checkbox" id="c-42174429" checked=""/><div class="controls bullet"><span class="by">remram</span><span>|</span><a href="#42188429">prev</a><span>|</span><a href="#42175229">next</a><span>|</span><label class="collapse" for="c-42174429">[-]</label><label class="expand" for="c-42174429">[2 more]</label></div><br/><div class="children"><div class="content">Is this like JuiceFS? <a href="https:&#x2F;&#x2F;juicefs.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;juicefs.com&#x2F;</a></div><br/><div id="42174449" class="c"><input type="checkbox" id="c-42174449" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174429">parent</a><span>|</span><a href="#42175229">next</a><span>|</span><label class="collapse" for="c-42174449">[-]</label><label class="expand" for="c-42174449">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s similar to JuiceFS, but JuiceFS writes and reads data from S3 in a proprietary block format. This means that you cannot connect JuiceFS to existing data sets in S3, and you cannot use data written through JuiceFS from the S3 API directly. On the other hand, Regatta reads and writes data to S3 using it&#x27;s native format -- so you can do these things!</div><br/></div></div></div></div><div id="42175229" class="c"><input type="checkbox" id="c-42175229" checked=""/><div class="controls bullet"><span class="by">austinpena</span><span>|</span><a href="#42174429">prev</a><span>|</span><a href="#42180315">next</a><span>|</span><label class="collapse" for="c-42175229">[-]</label><label class="expand" for="c-42175229">[2 more]</label></div><br/><div class="children"><div class="content">Reminds me of <a href="https:&#x2F;&#x2F;www.lucidlink.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.lucidlink.com&#x2F;</a> for video editors. I quite like the experience with them.</div><br/><div id="42175295" class="c"><input type="checkbox" id="c-42175295" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42175229">parent</a><span>|</span><a href="#42180315">next</a><span>|</span><label class="collapse" for="c-42175295">[-]</label><label class="expand" for="c-42175295">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s exactly right, I&#x27;ve spoken with a ton of folks who have had a good experience with Lucid Link. I think that we are in a slightly different part of the market (in that we aren&#x27;t targeting video editors, and more of data-intensive applications which may use thousands of IOPS), but I appreciate that the technology is likely similar.</div><br/></div></div></div></div><div id="42180315" class="c"><input type="checkbox" id="c-42180315" checked=""/><div class="controls bullet"><span class="by">IgorPartola</span><span>|</span><a href="#42175229">prev</a><span>|</span><a href="#42180323">next</a><span>|</span><label class="collapse" for="c-42180315">[-]</label><label class="expand" for="c-42180315">[2 more]</label></div><br/><div class="children"><div class="content">Is this meaningfully different from <a href="https:&#x2F;&#x2F;github.com&#x2F;s3ql&#x2F;s3ql">https:&#x2F;&#x2F;github.com&#x2F;s3ql&#x2F;s3ql</a> ?<p>S3 semantics are generally fairly terrible for file storage (no atomic move&#x2F;rename is just one example) but using it as block storage a la ZFS is quite clever.</div><br/><div id="42180411" class="c"><input type="checkbox" id="c-42180411" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42180315">parent</a><span>|</span><a href="#42180323">next</a><span>|</span><label class="collapse" for="c-42180411">[-]</label><label class="expand" for="c-42180411">[1 more]</label></div><br/><div class="children"><div class="content">Hey, thanks for the question. From what I can tell (and this could be wrong), but it looks like s3ql is using S3 as a block layer. Regatta, on the other hand, allows you to read and write files in their native format. I agree that it&#x27;s harder to implement than just using S3 for block storage, but I think that it unlocks a lot of potential use cases for customers. With Regatta, we make these semantics performant, which is a huge improvement on the prior art.</div><br/></div></div></div></div><div id="42180323" class="c"><input type="checkbox" id="c-42180323" checked=""/><div class="controls bullet"><span class="by">amitizle</span><span>|</span><a href="#42180315">prev</a><span>|</span><a href="#42174733">next</a><span>|</span><label class="collapse" for="c-42180323">[-]</label><label class="expand" for="c-42180323">[2 more]</label></div><br/><div class="children"><div class="content">Why are these solutions always using NFS?
I&#x27;m asking out of curiosity, not judgement.<p>I&#x27;ve looked for a solution to write many small files fast (safely). Think about cloning thr Linux kernel git repo.
Whatever I tested, the NFS protocol was always a bottleneck.</div><br/><div id="42180421" class="c"><input type="checkbox" id="c-42180421" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42180323">parent</a><span>|</span><a href="#42174733">next</a><span>|</span><label class="collapse" for="c-42180421">[-]</label><label class="expand" for="c-42180421">[1 more]</label></div><br/><div class="children"><div class="content">We choose NFS purely because it&#x27;s the fastest way to get broad compatibility with most operating systems (NFSv3, for example is supported on both Linux and Windows). However, I have great news for you! We&#x27;re simultaneously working on a custom protocol (over FUSE today) that is going to solve the small file problem for things like cloning the Linux kernel git repo. You can actually see in our demo video (<a href="https:&#x2F;&#x2F;youtu.be&#x2F;xh1q5p7E4JY?feature=shared&amp;t=170" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;xh1q5p7E4JY?feature=shared&amp;t=170</a>) that we untar the Linux kernel on Regatta in under 12 seconds. We&#x27;re hopeful that this performance makes file storage useful for a broader set of workloads.</div><br/></div></div></div></div><div id="42174733" class="c"><input type="checkbox" id="c-42174733" checked=""/><div class="controls bullet"><span class="by">mmastrac</span><span>|</span><a href="#42180323">prev</a><span>|</span><label class="collapse" for="c-42174733">[-]</label><label class="expand" for="c-42174733">[2 more]</label></div><br/><div class="children"><div class="content">I have a few qualms with this app:<p>1. For a Linux user, you can already build such a system yourself quite trivially by getting an FTP account, mounting it locally with curlftpfs, and then using SVN or CVS on the mounted filesystem. From Windows or Mac, this FTP account could be accessed through built-in software.<p>... I&#x27;m kidding, this is quite useful.<p>I really wish that NFSv3 and Linux had built-in file hashing ioctls that could delegate some of this expensive work to the backend as it would make it much easier to use something like this as a backup accelerator.</div><br/><div id="42174811" class="c"><input type="checkbox" id="c-42174811" checked=""/><div class="controls bullet"><span class="by">huntaub</span><span>|</span><a href="#42174733">parent</a><span>|</span><label class="collapse" for="c-42174811">[-]</label><label class="expand" for="c-42174811">[1 more]</label></div><br/><div class="children"><div class="content">Ha, thank you for the FTP comment, I was hoping someone would make it.<p>&gt; I really wish that NFSv3 and Linux had built-in file hashing ioctls that could delegate some of this expensive work to the backend as it would make it much easier to use something like this as a backup accelerator.<p>Tell me a bit more about what you mean here. We&#x27;re interested in really pushing the limits of what a storage system can do, so I&#x27;d be potentially interested.</div><br/></div></div></div></div></div></div></div></div></div></body></html>