<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1712048452241" as="style"/><link rel="stylesheet" href="styles.css?v=1712048452241"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://justine.lol/matmul/">LLaMA now goes faster on CPUs</a> <span class="domain">(<a href="https://justine.lol">justine.lol</a>)</span></div><div class="subtext"><span>lawrencechen</span> | <span>359 comments</span></div><br/><div><div id="39891772" class="c"><input type="checkbox" id="c-39891772" checked=""/><div class="controls bullet"><span class="by">speps</span><span>|</span><a href="#39894316">next</a><span>|</span><label class="collapse" for="c-39891772">[-]</label><label class="expand" for="c-39891772">[23 more]</label></div><br/><div class="children"><div class="content">Regarding this bit at the end:<p>&gt; I learned how to write math kernels by renting Vast VMs and watching Gautham Venkatasubramanian and mrdomino develop CUDA kernels in a tmux session. They&#x27;ve been focusing on solving a much more important challenge for llamafile, which is helping it not have a mandatory dependency on the cuBLAS<p>If I&#x27;m reading this right, they&#x27;re trying to rewrite cuBLAS within CUDA itself. I&#x27;m guessing the next step would be removing CUDA dependency and go with directly using Vulkan or Metal compute shaders. Am I correct?</div><br/><div id="39891938" class="c"><input type="checkbox" id="c-39891938" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#39891772">parent</a><span>|</span><a href="#39891952">next</a><span>|</span><label class="collapse" for="c-39891938">[-]</label><label class="expand" for="c-39891938">[18 more]</label></div><br/><div class="children"><div class="content">Yes, but none of these have performance portability across GPU vendors, so it&#x27;s probably seen as pointless. You would need an AMD Vulkan shader, an nvidia one, and intel one, etc. It&#x27;s not like C code on CPUs.</div><br/><div id="39892517" class="c"><input type="checkbox" id="c-39892517" checked=""/><div class="controls bullet"><span class="by">radarsat1</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39891938">parent</a><span>|</span><a href="#39892849">next</a><span>|</span><label class="collapse" for="c-39892517">[-]</label><label class="expand" for="c-39892517">[6 more]</label></div><br/><div class="children"><div class="content">Depending on how many individual tweaks are necessary for hardware variants of course... but at this level of code &amp; complexity it actually seems pretty reasonable to write 3 or 4 versions of things for different vendors. More work yes, but not pointless.</div><br/><div id="39892862" class="c"><input type="checkbox" id="c-39892862" checked=""/><div class="controls bullet"><span class="by">treffer</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39892517">parent</a><span>|</span><a href="#39892849">next</a><span>|</span><label class="collapse" for="c-39892862">[-]</label><label class="expand" for="c-39892862">[5 more]</label></div><br/><div class="children"><div class="content">A nice example of this is fftw which has hundreds (if not thousands) of generated methods to do the fft math. The whole project is a code generator.<p>It can then after compilation benchmark these, generate a wisdom file for the hardware and pick the right implementation.<p>Compared with that &quot;a few&quot; implementations of the core math kernel seem like an easy thing to do.</div><br/><div id="39900855" class="c"><input type="checkbox" id="c-39900855" checked=""/><div class="controls bullet"><span class="by">mananaysiempre</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39892862">parent</a><span>|</span><a href="#39894222">next</a><span>|</span><label class="collapse" for="c-39900855">[-]</label><label class="expand" for="c-39900855">[1 more]</label></div><br/><div class="children"><div class="content">Metalibm[1,2] is a different idea, but kind of related: if you need a special (trigonometric, exponential, ...) function only with limited accuracy or only on a specific domain, you can have an approximation generated for your specific needs.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;metalibm&#x2F;metalibm">https:&#x2F;&#x2F;github.com&#x2F;metalibm&#x2F;metalibm</a><p>[2] <a href="https:&#x2F;&#x2F;indico.cern.ch&#x2F;event&#x2F;166141&#x2F;sessions&#x2F;125685&#x2F;attachments&#x2F;201415&#x2F;282783&#x2F;DeDinechin-2012-CERN.pdf" rel="nofollow">https:&#x2F;&#x2F;indico.cern.ch&#x2F;event&#x2F;166141&#x2F;sessions&#x2F;125685&#x2F;attachme...</a></div><br/></div></div><div id="39894222" class="c"><input type="checkbox" id="c-39894222" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39892862">parent</a><span>|</span><a href="#39900855">prev</a><span>|</span><a href="#39897679">next</a><span>|</span><label class="collapse" for="c-39894222">[-]</label><label class="expand" for="c-39894222">[1 more]</label></div><br/><div class="children"><div class="content">ATLAS was an automatically tuned BLAS, but it’s been mostly supplanted by ones using the hand-tuned kernel strategy.</div><br/></div></div><div id="39897679" class="c"><input type="checkbox" id="c-39897679" checked=""/><div class="controls bullet"><span class="by">touisteur</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39892862">parent</a><span>|</span><a href="#39894222">prev</a><span>|</span><a href="#39893904">next</a><span>|</span><label class="collapse" for="c-39897679">[-]</label><label class="expand" for="c-39897679">[1 more]</label></div><br/><div class="children"><div class="content">Apache TVM does something similar for auto-optimization and last time I checked it wasn&#x27;t always a win against OpenVINO (depending on the network and batch-size) and it came with lots of limitations (which may have been lifted since) - stuff like dynamic batch size.<p>I wish we had superoptom</div><br/></div></div><div id="39893904" class="c"><input type="checkbox" id="c-39893904" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39892862">parent</a><span>|</span><a href="#39897679">prev</a><span>|</span><a href="#39892849">next</a><span>|</span><label class="collapse" for="c-39893904">[-]</label><label class="expand" for="c-39893904">[1 more]</label></div><br/><div class="children"><div class="content">Not exactly comparable, as you said, the FFTW implementations are auto-generated but it doesn&#x27;t sound like these few implementations will be.</div><br/></div></div></div></div></div></div><div id="39892849" class="c"><input type="checkbox" id="c-39892849" checked=""/><div class="controls bullet"><span class="by">TuringNYC</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39891938">parent</a><span>|</span><a href="#39892517">prev</a><span>|</span><a href="#39893827">next</a><span>|</span><label class="collapse" for="c-39892849">[-]</label><label class="expand" for="c-39892849">[8 more]</label></div><br/><div class="children"><div class="content">To me it makes sense to have an interface that can be implemented individually for AMD, Metal, etc. Then, leave it up to the individual manufacturers to implement those interfaces.<p>I&#x27;m sitting in an office with a massive number of Macbook Pro Max laptops usually sitting idle and I wish Apple would realize the final coup they could achieve if I could also run the typically-NVIDIA workloads on these hefty, yet underutilized, Mx machines.</div><br/><div id="39893369" class="c"><input type="checkbox" id="c-39893369" checked=""/><div class="controls bullet"><span class="by">jorvi</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39892849">parent</a><span>|</span><a href="#39893827">next</a><span>|</span><label class="collapse" for="c-39893369">[-]</label><label class="expand" for="c-39893369">[7 more]</label></div><br/><div class="children"><div class="content">Apple could unlock so much compute if they give customers a sort of “Apple@Home” deal. Allow Apple to run distributed AI workloads on your mostly idle extremely overpowered Word&#x2F;Excel&#x2F;VSCode machine, and you get compensation dropped straight into your Apple account’s linked creditcard.</div><br/><div id="39894120" class="c"><input type="checkbox" id="c-39894120" checked=""/><div class="controls bullet"><span class="by">TuringNYC</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39893369">parent</a><span>|</span><a href="#39893684">next</a><span>|</span><label class="collapse" for="c-39894120">[-]</label><label class="expand" for="c-39894120">[3 more]</label></div><br/><div class="children"><div class="content">BTW, at our day-job, we&#x27;ve been running a &quot;cluster&quot; of M1 Pro Max machines running Ollama and LLMs. Corporate rules prevent remote access onto machines, so we created a quick and dirty pull system where individual developers can start pulling from a central queue, running LLM workloads via the Ollama local service, and contributing things back centrally.<p>Sounds kludge, but introduce enough constraints and you end up with this as the best solution.</div><br/><div id="39897232" class="c"><input type="checkbox" id="c-39897232" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39894120">parent</a><span>|</span><a href="#39893684">next</a><span>|</span><label class="collapse" for="c-39897232">[-]</label><label class="expand" for="c-39897232">[2 more]</label></div><br/><div class="children"><div class="content">Do you have price-performance numbers you can share on that? Like compared against local or cloud machines with RTX and A100 GPU’s?</div><br/><div id="39897544" class="c"><input type="checkbox" id="c-39897544" checked=""/><div class="controls bullet"><span class="by">TuringNYC</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39897232">parent</a><span>|</span><a href="#39893684">next</a><span>|</span><label class="collapse" for="c-39897544">[-]</label><label class="expand" for="c-39897544">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; Do you have price-performance numbers you can share on that? Like compared against local or cloud machines with RTX and A100 GPU’s?<p>Good question, the account is muddy --<p>1. Electricity is a parent company responsibility, so while that is a factor in OpEx price, it isnt a factor for us. I dont think it even gets submetered. Obviously, one wouldnt want to abuse this, but maxing out Macbooks dont seem close to abuse territory<p>2. The M1&#x2F;M2&#x2F;M3 machines are already purchased, so while that is major CapEx, it is a sunk cost and also an underutilized resource most of the day. We assume no wear and tear from maxing out the cores, not sure if that is a perfect assumption but good enough.<p>3. Local servers are out of the question at a big company outside of infra groups, it would take years to provision them and I dont think there is even a means to anymore.<p>The real question is cloud. Cloud with RTX&#x2F;A100 would be far more expensive, though I&#x27;m sure performant. (TPM calculation left to the reader :-) I&#x27;d leave those for fine tuning, not for inference workloads. Non-production Inference is particularly bad because you cant easily justify reserved capacity without some constant throughput. If we could mix environments, it might make sense to go all cloud on NVIDIA but having separate environments with separate compliance requirements makes that hard.<p>Jokes aside, I think a TPM calculation would be worthwhile and perhaps I can do a quick writeup on this and submit to HN.</div><br/></div></div></div></div></div></div><div id="39893684" class="c"><input type="checkbox" id="c-39893684" checked=""/><div class="controls bullet"><span class="by">newswasboring</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39893369">parent</a><span>|</span><a href="#39894120">prev</a><span>|</span><a href="#39893644">next</a><span>|</span><label class="collapse" for="c-39893684">[-]</label><label class="expand" for="c-39893684">[2 more]</label></div><br/><div class="children"><div class="content">If Apple were doing an Apple@Home kind of deal they might actually want to give away some machines for free or super cheap (I realize that doesn&#x27;t fit their brand) and then get the rights perpetually to run compute on them. Kind of like advertising but it might be doing something actually helpful for someone else.</div><br/><div id="39897593" class="c"><input type="checkbox" id="c-39897593" checked=""/><div class="controls bullet"><span class="by">TuringNYC</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39893684">parent</a><span>|</span><a href="#39893644">next</a><span>|</span><label class="collapse" for="c-39897593">[-]</label><label class="expand" for="c-39897593">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; If Apple were doing an Apple@Home kind of deal they might actually want to give away some machines for free or super cheap<p>In such a case, my guess is that the machines being free would be trumped by the increased cost of electricity.</div><br/></div></div></div></div></div></div></div></div><div id="39893827" class="c"><input type="checkbox" id="c-39893827" checked=""/><div class="controls bullet"><span class="by">surge</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39891938">parent</a><span>|</span><a href="#39892849">prev</a><span>|</span><a href="#39891952">next</a><span>|</span><label class="collapse" for="c-39893827">[-]</label><label class="expand" for="c-39893827">[3 more]</label></div><br/><div class="children"><div class="content">Maybe its a dumb question, but isn&#x27;t something like OpenCL meant to solve this problem?</div><br/><div id="39893959" class="c"><input type="checkbox" id="c-39893959" checked=""/><div class="controls bullet"><span class="by">jvanderbot</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39893827">parent</a><span>|</span><a href="#39899257">next</a><span>|</span><label class="collapse" for="c-39893959">[-]</label><label class="expand" for="c-39893959">[1 more]</label></div><br/><div class="children"><div class="content">From my understanding, using triangle &#x2F; shaders to do HPC has given way to a specific, more general purpose GPU programming paradigm which is CUDA.<p>Of course this knowledge is superficial and probably outdated, but if I&#x27;m not too far off base, it&#x27;s probably more work to translate a general CUDA-like layer or CUDA libs to OpenCL.</div><br/></div></div><div id="39899257" class="c"><input type="checkbox" id="c-39899257" checked=""/><div class="controls bullet"><span class="by">VHRanger</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39893827">parent</a><span>|</span><a href="#39893959">prev</a><span>|</span><a href="#39891952">next</a><span>|</span><label class="collapse" for="c-39899257">[-]</label><label class="expand" for="c-39899257">[1 more]</label></div><br/><div class="children"><div class="content">In theory, yes.<p>In practice, OpenCL became a giant mess. Some vendors put speed bumps by not supporting the transition from 2 to 3, or having shitty drivers for it.<p>It also sat at the wrong level of abstraction for high performance compute, which is why CUDA ended up being used.<p>Vulkan would have been reasonable to write compute shaders in, if there wasn&#x27;t a ton of alternatives out there already now</div><br/></div></div></div></div></div></div><div id="39891952" class="c"><input type="checkbox" id="c-39891952" checked=""/><div class="controls bullet"><span class="by">larodi</span><span>|</span><a href="#39891772">parent</a><span>|</span><a href="#39891938">prev</a><span>|</span><a href="#39903245">next</a><span>|</span><label class="collapse" for="c-39891952">[-]</label><label class="expand" for="c-39891952">[3 more]</label></div><br/><div class="children"><div class="content">llama.cpp (or rather G.Gerganov et. al.) are trying to avoid cuBLAS entirely, using ins own kernels. not sure how jart&#x27;s effort relates, and whether jart intends to upstream these into llama.cpp which seems to still be the underlying tech behind the llamafile.</div><br/><div id="39892283" class="c"><input type="checkbox" id="c-39892283" checked=""/><div class="controls bullet"><span class="by">homarp</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39891952">parent</a><span>|</span><a href="#39903245">next</a><span>|</span><label class="collapse" for="c-39892283">[-]</label><label class="expand" for="c-39892283">[2 more]</label></div><br/><div class="children"><div class="content">Here are links to the most recent pull requests  sent<p><pre><code>    https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;6414
    https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;6412</code></pre></div><br/><div id="39893320" class="c"><input type="checkbox" id="c-39893320" checked=""/><div class="controls bullet"><span class="by">speps</span><span>|</span><a href="#39891772">root</a><span>|</span><a href="#39892283">parent</a><span>|</span><a href="#39903245">next</a><span>|</span><label class="collapse" for="c-39893320">[-]</label><label class="expand" for="c-39893320">[1 more]</label></div><br/><div class="children"><div class="content">This doesn&#x27;t relate to GPU kernels unfortunately.</div><br/></div></div></div></div></div></div></div></div><div id="39894316" class="c"><input type="checkbox" id="c-39894316" checked=""/><div class="controls bullet"><span class="by">marshallward</span><span>|</span><a href="#39891772">prev</a><span>|</span><a href="#39890454">next</a><span>|</span><label class="collapse" for="c-39894316">[-]</label><label class="expand" for="c-39894316">[13 more]</label></div><br/><div class="children"><div class="content">There is an implication here that the Fortran implementation of `SGEMM` is somehow inadequate.  But any modern Fortran compiler will quite easily apply the AVX and FMA optimizations presented here without any additional changes.  Both GNU and Intel make these substitutions with the correct flags.<p>The unrolling optimization is also just another flag away (`-funroll-all-loops`).  The Intel Compiler will even do this without prompting.  In fact, it appears to only do a modest 2x unroll on my machine, suggesting that the extreme unroll in this article would have been overkill.<p>Parallelization certainly a lot to ask of Fortran 77 source, but there there is little stopping you from adding OpenMP statements to the `SGEMM` function.  In fact, modern Fortran even offers its own parallelization constructs if you&#x27;re willing to go there.<p>Which is to say: Let&#x27;s not belittle this old Fortran 77 function.  Yes it is old, and does not even resemble modern Fortran.  But the whole point of Fortran is to free the developer from these platform-specific details, and hand the job off to the compiler.  If you don&#x27;t like that approach, then you&#x27;re welcome to go to C or C++.  But this little block of Fortran code is already capable of doing just about everything in this article.</div><br/><div id="39894636" class="c"><input type="checkbox" id="c-39894636" checked=""/><div class="controls bullet"><span class="by">steppi</span><span>|</span><a href="#39894316">parent</a><span>|</span><a href="#39896349">next</a><span>|</span><label class="collapse" for="c-39894636">[-]</label><label class="expand" for="c-39894636">[4 more]</label></div><br/><div class="children"><div class="content">The Fortran implementation is just a reference implementation. The goal of reference BLAS [0] is to provide relatively simple and easy to understand implementations which demonstrate the interface and are intended to give correct results to test against. Perhaps an exceptional Fortran compiler which doesn&#x27;t yet exist could generate code which rivals hand (or automatically) tuned optimized BLAS libraries like OpenBLAS [1], MKL [2], ATLAS [3], and those based on BLIS [4], but in practice this is not observed.<p>Justine observed that the threading model for LLaMA makes it impractical to integrate one of these optimized BLAS libraries, so she wrote her own hand-tuned implementations following the same principles they use.<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Basic_Linear_Algebra_Subprograms" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Basic_Linear_Algebra_Subprogra...</a><p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;OpenMathLib&#x2F;OpenBLAS">https:&#x2F;&#x2F;github.com&#x2F;OpenMathLib&#x2F;OpenBLAS</a><p>[2] <a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;tools&#x2F;oneapi&#x2F;onemkl.html#gs.6q6b7q" rel="nofollow">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;tools&#x2F;onea...</a><p>[3] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Automatically_Tuned_Linear_Algebra_Software" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Automatically_Tuned_Linear_Alg...</a><p>[4]<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;BLIS_(software)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;BLIS_(software)</a></div><br/><div id="39895125" class="c"><input type="checkbox" id="c-39895125" checked=""/><div class="controls bullet"><span class="by">marshallward</span><span>|</span><a href="#39894316">root</a><span>|</span><a href="#39894636">parent</a><span>|</span><a href="#39896349">next</a><span>|</span><label class="collapse" for="c-39895125">[-]</label><label class="expand" for="c-39895125">[3 more]</label></div><br/><div class="children"><div class="content">Fair enough, this is not meant to be some endorsement of the standard Fortran BLAS implementations over the optimized versions cited above.   Only that the mainstream compilers cited above appear capable of applying these optimizations to the standard BLAS Fortran without any additional effort.<p>I am basing these comments on quick inspection of the assembly output.  Timings would be equally interesting to compare at each stage, but I&#x27;m only willing to go so far for a Hacker News comment.  So all I will say is perhaps let&#x27;s keep an open mind about the capability of simple Fortran code.</div><br/><div id="39895797" class="c"><input type="checkbox" id="c-39895797" checked=""/><div class="controls bullet"><span class="by">steppi</span><span>|</span><a href="#39894316">root</a><span>|</span><a href="#39895125">parent</a><span>|</span><a href="#39896349">next</a><span>|</span><label class="collapse" for="c-39895797">[-]</label><label class="expand" for="c-39895797">[2 more]</label></div><br/><div class="children"><div class="content">Check out <i>The Science of Programming Matrix Computations</i> by Robert A. van de Geijn and Enrique S. Quintana-Ort. Chapter 5 walks through how to write an optimized GEMM. It involves clever use of block multiplication, choosing block sizes for optimal cache behavior for specific chips. Modern compilers just aren&#x27;t able to do such things now. I&#x27;ve spent a little time debugging things in scipy.linalg by swapping out OpenBLAS with reference BLAS and have found the slowdown from using reference BLAS is typically at least an order of magnitude.<p>[0] <a href="https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;rvdg&#x2F;tmp&#x2F;TSoPMC.pdf" rel="nofollow">https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;rvdg&#x2F;tmp&#x2F;TSoPMC.pdf</a></div><br/><div id="39899901" class="c"><input type="checkbox" id="c-39899901" checked=""/><div class="controls bullet"><span class="by">marshallward</span><span>|</span><a href="#39894316">root</a><span>|</span><a href="#39895797">parent</a><span>|</span><a href="#39896349">next</a><span>|</span><label class="collapse" for="c-39899901">[-]</label><label class="expand" for="c-39899901">[1 more]</label></div><br/><div class="children"><div class="content">You are right, I just tested this out and my speed from BLAS to OpenBLAS went from 6 GFLOP&#x2F;s to 150 GFLOP&#x2F;s.  I can only imagine what BLIS and MKL would give.  I apologize for my ignorance.  Apparently my faith in the compilers was wildly misplaced.</div><br/></div></div></div></div></div></div></div></div><div id="39896349" class="c"><input type="checkbox" id="c-39896349" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#39894316">parent</a><span>|</span><a href="#39894636">prev</a><span>|</span><a href="#39894984">next</a><span>|</span><label class="collapse" for="c-39896349">[-]</label><label class="expand" for="c-39896349">[6 more]</label></div><br/><div class="children"><div class="content">using AVX&#x2F;FMA and unrolling loops does extremely little in the way of compiling to fast (&gt;80% peak) GEMM code.  These are very much intro steps that don&#x27;t take into account <i>many</i> important ideas related to cache hierarchy, uop interactions,  and even instruction decode time.  The Fortran implementation is entirely and unquestionably inadequate for real high performance GEMMs.</div><br/><div id="39900056" class="c"><input type="checkbox" id="c-39900056" checked=""/><div class="controls bullet"><span class="by">marshallward</span><span>|</span><a href="#39894316">root</a><span>|</span><a href="#39896349">parent</a><span>|</span><a href="#39896823">next</a><span>|</span><label class="collapse" for="c-39900056">[-]</label><label class="expand" for="c-39900056">[4 more]</label></div><br/><div class="children"><div class="content">I just did a test of OpenBLAS with Intel-compiled BLAS, and it was about 6 GFLOP&#x2F;s vs 150 GFLOP&#x2F;s, so I must admit that I was wrong here.  Maybe in some sense 4% is not bad, but it&#x27;s certainly not good.  My faith in current compilers has certainly been shattered quite a bit today.<p>Anyway, I have come to eat crow.  Thank you for your insight and helping me to get a much better perspective on this problem.  I mostly work with scalar and vector updates, and do not work with matrices very often.</div><br/><div id="39901912" class="c"><input type="checkbox" id="c-39901912" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39894316">root</a><span>|</span><a href="#39900056">parent</a><span>|</span><a href="#39896823">next</a><span>|</span><label class="collapse" for="c-39901912">[-]</label><label class="expand" for="c-39901912">[3 more]</label></div><br/><div class="children"><div class="content">The inequality between matrix multiplication implementations is enormous. It gets even more extreme on GPU where I&#x27;ve seen the difference between naïve and cuBLAS going as high as 1000x. Possibly 10000x. I have a lot of faith in myself as an optimization person to be able to beat compilers. I can even beat MKL and hipBLAS if I focus on specific shapes in sizes. But trying to beat cuBLAS at anything makes me feel like Saddam Hussein when they pulled him out of that bunker.</div><br/><div id="39901979" class="c"><input type="checkbox" id="c-39901979" checked=""/><div class="controls bullet"><span class="by">marshallward</span><span>|</span><a href="#39894316">root</a><span>|</span><a href="#39901912">parent</a><span>|</span><a href="#39896823">next</a><span>|</span><label class="collapse" for="c-39901979">[-]</label><label class="expand" for="c-39901979">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure there&#x27;s more to it, but just comparing the profile output shows aggressive use of prefetch and broadcast instructions.</div><br/><div id="39902120" class="c"><input type="checkbox" id="c-39902120" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39894316">root</a><span>|</span><a href="#39901979">parent</a><span>|</span><a href="#39896823">next</a><span>|</span><label class="collapse" for="c-39902120">[-]</label><label class="expand" for="c-39902120">[1 more]</label></div><br/><div class="children"><div class="content">BLIS does that in their kernels. I&#x27;ve tried doing that but was never able to get something better than half as good as MKL. The BLIS technique of tiling across k also requires atomics or an array of locks to write output.</div><br/></div></div></div></div></div></div></div></div><div id="39896823" class="c"><input type="checkbox" id="c-39896823" checked=""/><div class="controls bullet"><span class="by">marshallward</span><span>|</span><a href="#39894316">root</a><span>|</span><a href="#39896349">parent</a><span>|</span><a href="#39900056">prev</a><span>|</span><a href="#39894984">next</a><span>|</span><label class="collapse" for="c-39896823">[-]</label><label class="expand" for="c-39896823">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t disagree, but where are those techniques presented in the article?  It seems like she exploits the particular shape of her matrix to align better with cache.  No BLAS library is going to figure that out.<p>I am not trying to say that a simple 50+ year old matrix solver is somehow competitive with existing BLAS libraries.  But I disagreed with its portrayal in the article, which associated the block with NumPy performance.  Give that to a 2024 Fortran compiler, and it&#x27;s going to get enough right to produce reasonable bytecode.</div><br/></div></div></div></div><div id="39894984" class="c"><input type="checkbox" id="c-39894984" checked=""/><div class="controls bullet"><span class="by">pklausler</span><span>|</span><a href="#39894316">parent</a><span>|</span><a href="#39896349">prev</a><span>|</span><a href="#39890454">next</a><span>|</span><label class="collapse" for="c-39894984">[-]</label><label class="expand" for="c-39894984">[2 more]</label></div><br/><div class="children"><div class="content">Modern Fortran&#x27;s only parallel feature is coarrays, which operate at the whole program level.<p>DO CONCURRENT is a serial construct with an unspecified order of iterations, not a parallel construct.  A DO CONCURRENT loop imposes requirements that allow an arbitrary order of iterations but which are not sufficient for safe parallelization.</div><br/><div id="39895144" class="c"><input type="checkbox" id="c-39895144" checked=""/><div class="controls bullet"><span class="by">marshallward</span><span>|</span><a href="#39894316">root</a><span>|</span><a href="#39894984">parent</a><span>|</span><a href="#39890454">next</a><span>|</span><label class="collapse" for="c-39895144">[-]</label><label class="expand" for="c-39895144">[1 more]</label></div><br/><div class="children"><div class="content">How do you feel about Nvidia endorsing do concurrent migration to GPUs?  Would that be classified as parallelization?</div><br/></div></div></div></div></div></div><div id="39890454" class="c"><input type="checkbox" id="c-39890454" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#39894316">prev</a><span>|</span><a href="#39890694">next</a><span>|</span><label class="collapse" for="c-39890454">[-]</label><label class="expand" for="c-39890454">[202 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s a good idea for everyone to download and be able to run a LLM locally, even if you have the minimum of requirements. As a pseudo-backup of a large chunk of human knowledge.</div><br/><div id="39890656" class="c"><input type="checkbox" id="c-39890656" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890615">next</a><span>|</span><label class="collapse" for="c-39890656">[-]</label><label class="expand" for="c-39890656">[87 more]</label></div><br/><div class="children"><div class="content">I strongly recommend that people run LLMs locally for a different reason.<p>The ones you can run on your own machine tend to be bad - really bad. They hallucinate wildly and fail at all sorts of tasks that the larger hosted ones succeed at.<p>This makes them a fantastic tool for learning more about how LLMs work and what they&#x27;re useful for. Interacting with a weak-but-functional LLM that runs on your own computer is a great way to get a much more solid mental model for what these things actually are.</div><br/><div id="39891030" class="c"><input type="checkbox" id="c-39891030" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39890763">next</a><span>|</span><label class="collapse" for="c-39891030">[-]</label><label class="expand" for="c-39891030">[53 more]</label></div><br/><div class="children"><div class="content">The other reason is to find out what a detuned model is capable of. The canonical example is how to make cocaine, which ChatGPT will admonish you for even asking, while llama2-uncensored will happily describe the process which is only really interesting if you&#x27;re an amateur chemist and want to be Scarface-that-knocks. (the recipe is relatively easy, it&#x27;s getting access to the raw ingredients that&#x27;s the hard part, same as with nukes.)<p>if you accidentally use the word&quot;hack&quot; when trying to get ChatGPT to write some code for you. it&#x27;ll stop and tell you that hacking is bad, and not a colloquial expression, and refuse to go further.<p>privacy reasons are another reason to try a local LLM. for the extremely paranoid (justified or not), a local LLM gives users a place to ask questions without the text being fed to a server somewhere for later lawsuit discovery (Google searches are routinely subpoenaed, it&#x27;s only a matter of time until ChatGPT chats are as well.)<p>There&#x27;s an uncensored model for vision available as well. The censored vision models won&#x27;t play the shallow game of hot or not with you.<p>There are uncensored image generation models as well, but, ah, those are NSFW and not for polite company. (As well as there&#x27;s multiple thesis&#x27; worth of content on what that&#x27;ll do to society.)</div><br/><div id="39893303" class="c"><input type="checkbox" id="c-39893303" checked=""/><div class="controls bullet"><span class="by">bambax</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891030">parent</a><span>|</span><a href="#39891350">next</a><span>|</span><label class="collapse" for="c-39893303">[-]</label><label class="expand" for="c-39893303">[11 more]</label></div><br/><div class="children"><div class="content">&gt; <i>if you accidentally use the word &quot;hack&quot; [with] ChatGPT...</i><p>Side note: ChatGPT is now completely useless for most creative tasks. I&#x27;m trying to use it, via NovelCrafter, to help flesh out a story where a minor character committed suicide. ChatGPT refuses to respond, mentioning &quot;self harm&quot; as a reason.<p>The character in question killed himself before the story even begins (and for very good reasons, story-wise); it&#x27;s not like one&#x27;s asking about ways to commit suicide.<p>This is insane, ridiculous, and different from what all other actors of the industry do, including Claude or Mistral. It seems OpenAI is trying to shoot itself in the foot and doing a pretty good job at it.</div><br/><div id="39896497" class="c"><input type="checkbox" id="c-39896497" checked=""/><div class="controls bullet"><span class="by">luma</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39893303">parent</a><span>|</span><a href="#39893399">next</a><span>|</span><label class="collapse" for="c-39896497">[-]</label><label class="expand" for="c-39896497">[2 more]</label></div><br/><div class="children"><div class="content">OpenAI is angling for enterprise users who have different notions about safety.  Writing novels isn&#x27;t the use case, powering customer service chatbots that will never ever ever say &quot;just kill yourself&quot; is.</div><br/><div id="39903539" class="c"><input type="checkbox" id="c-39903539" checked=""/><div class="controls bullet"><span class="by">antonvs</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39896497">parent</a><span>|</span><a href="#39893399">next</a><span>|</span><label class="collapse" for="c-39903539">[-]</label><label class="expand" for="c-39903539">[1 more]</label></div><br/><div class="children"><div class="content">My contrarian tendencies now have me thinking of scenarios where a customer service chatbot might need to say &quot;just kill yourself&quot;.<p>Perhaps the HR support line for OpenAI developers tasked with implementing the censorship system?</div><br/></div></div></div></div><div id="39893399" class="c"><input type="checkbox" id="c-39893399" checked=""/><div class="controls bullet"><span class="by">marpstar</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39893303">parent</a><span>|</span><a href="#39896497">prev</a><span>|</span><a href="#39900185">next</a><span>|</span><label class="collapse" for="c-39893399">[-]</label><label class="expand" for="c-39893399">[1 more]</label></div><br/><div class="children"><div class="content">I’ve been frustrated by this, too. Trying to ask for ways to support a close family member who experienced sexual trauma. ChatGPT won’t touch the topic.</div><br/></div></div><div id="39900185" class="c"><input type="checkbox" id="c-39900185" checked=""/><div class="controls bullet"><span class="by">barfingclouds</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39893303">parent</a><span>|</span><a href="#39893399">prev</a><span>|</span><a href="#39893750">next</a><span>|</span><label class="collapse" for="c-39900185">[-]</label><label class="expand" for="c-39900185">[2 more]</label></div><br/><div class="children"><div class="content">Darn I guess you’ll have to go back to living in the dark ages and actually write it yourself</div><br/><div id="39902801" class="c"><input type="checkbox" id="c-39902801" checked=""/><div class="controls bullet"><span class="by">bambax</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39900185">parent</a><span>|</span><a href="#39893750">next</a><span>|</span><label class="collapse" for="c-39902801">[-]</label><label class="expand" for="c-39902801">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39894775">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39894775</a></div><br/></div></div></div></div></div></div><div id="39891350" class="c"><input type="checkbox" id="c-39891350" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891030">parent</a><span>|</span><a href="#39893303">prev</a><span>|</span><a href="#39899563">next</a><span>|</span><label class="collapse" for="c-39891350">[-]</label><label class="expand" for="c-39891350">[28 more]</label></div><br/><div class="children"><div class="content">&gt; if you accidentally use the word&quot;hack&quot; when trying to get ChatGPT to write some code for you. it&#x27;ll stop and tell you that hacking is bad, and not a colloquial expression, and refuse to go further.<p>Is that 3.5 or 4? I asked 4 for an example of code which &quot;is a hack&quot;, it misunderstood me as asking for hacking code rather than buggy code, but then it did actually answer on the first try.<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;ca2c320c-f4ba-41bf-8f40-f7faf26a4379" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;ca2c320c-f4ba-41bf-8f40-f7faf2...</a></div><br/><div id="39891468" class="c"><input type="checkbox" id="c-39891468" checked=""/><div class="controls bullet"><span class="by">semi-extrinsic</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891350">parent</a><span>|</span><a href="#39891452">next</a><span>|</span><label class="collapse" for="c-39891468">[-]</label><label class="expand" for="c-39891468">[24 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t use LLMs for my coding, I manage just fine with LSP and Treesitter. So genuine question: is that answer representative of the output quality of these things? Because both answers are pretty crappy and assume the user has already done the difficult things, and is asking for help on the easy things.</div><br/><div id="39891538" class="c"><input type="checkbox" id="c-39891538" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891468">parent</a><span>|</span><a href="#39891514">next</a><span>|</span><label class="collapse" for="c-39891538">[-]</label><label class="expand" for="c-39891538">[1 more]</label></div><br/><div class="children"><div class="content">The response seems pretty reasonable; it&#x27;s answering the question it was asked. If you want to ask it how to do the difficult part, ask it about that instead. Expecting it to get the answer right in the first pass is like expecting your code to compile the very first time. You have to have more of a conversation with it to coax the difference out of  you&#x27;re thinking and what you&#x27;re actually saying.<p>If you&#x27;re looking to read a more advanced example of its capabilities and limitations, try<p><a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Mar&#x2F;23&#x2F;building-c-extensions-for-sqlite-with-chatgpt-code-interpreter&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Mar&#x2F;23&#x2F;building-c-extensions-...</a></div><br/></div></div><div id="39891514" class="c"><input type="checkbox" id="c-39891514" checked=""/><div class="controls bullet"><span class="by">lpapez</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891468">parent</a><span>|</span><a href="#39891538">prev</a><span>|</span><a href="#39892633">next</a><span>|</span><label class="collapse" for="c-39891514">[-]</label><label class="expand" for="c-39891514">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not representative.<p>The models are capable of much much more, and they are being significantly nerfed over time by these ineffective attempts to introduce safeguards.<p>Recently I&#x27;ve asked GPT4 to quote me some code to which it replied that it is not allowed to do so - even though it was perfectly happy to quote anything until recently. When prompted to quote the source code, but output it as PHP comments, it happily complied because it saw that as &quot;derivative work&quot; which it is allowed to do.</div><br/><div id="39892005" class="c"><input type="checkbox" id="c-39892005" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891514">parent</a><span>|</span><a href="#39892633">next</a><span>|</span><label class="collapse" for="c-39892005">[-]</label><label class="expand" for="c-39892005">[1 more]</label></div><br/><div class="children"><div class="content">My point is that there aren&#x27;t any safeguards in the reply. In fact I didn&#x27;t even want it to give me hacking info and it did it anyway.</div><br/></div></div></div></div><div id="39892633" class="c"><input type="checkbox" id="c-39892633" checked=""/><div class="controls bullet"><span class="by">rpigab</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891468">parent</a><span>|</span><a href="#39891514">prev</a><span>|</span><a href="#39892000">next</a><span>|</span><label class="collapse" for="c-39892633">[-]</label><label class="expand" for="c-39892633">[2 more]</label></div><br/><div class="children"><div class="content">I asked ChatGPT for some dataviz task (I barely ever do dataviz myself) and it recommended some nice Python libraries to use, some I had already heard of and some I hadn&#x27;t, and provided the code.<p>I&#x27;m grateful because I thought code LLMs only sped up the &quot;RTFM&quot; part, but it made me find those libs so I didn&#x27;t have to Google around for (and sometimes it&#x27;s hard to guess if they&#x27;re the right tool for the job, and they might be behind in SEO).</div><br/><div id="39893365" class="c"><input type="checkbox" id="c-39893365" checked=""/><div class="controls bullet"><span class="by">miki123211</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39892633">parent</a><span>|</span><a href="#39892000">next</a><span>|</span><label class="collapse" for="c-39893365">[-]</label><label class="expand" for="c-39893365">[1 more]</label></div><br/><div class="children"><div class="content">There are three things I find LLMs really excellent at for coding:<p>1. Being the &quot;senior developer&quot; who spend their whole career working with a technology you&#x27;re very junior at. No matter what you do and how long your programming career is, you&#x27;re inevitably going to run into one of these sooner or later. Whether it&#x27;s build scripts, frontend code, interfacing with third-party APIs or something else entirely, you aren&#x27;t an expert at every technology you work with.<p>2. Writing the &quot;boring&quot; parts of your program, and every program has some of these. If you&#x27;re writing a service to fooize a bar really efficiently, Copilot won&#x27;t help you with the core bar fooization algorithm, but will make you a lot faster at coding up user authentication, rate limiting for different plans, billing in whatever obscure payment method your country uses etc.<p>3. Telling you what to even Google for. This is where raw Chat GPT comes into play, not Copilot. Let&#x27;s say you need a sorting algorithm that preserves the order of equal elements from the original list. This is called stable sorting, and Googling for stable sorting is a good way to find what you&#x27;re looking for, but Chat GPT is usually a better way to tell you what it&#x27;s called based on the problem description.</div><br/></div></div></div></div><div id="39892000" class="c"><input type="checkbox" id="c-39892000" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891468">parent</a><span>|</span><a href="#39892633">prev</a><span>|</span><a href="#39891874">next</a><span>|</span><label class="collapse" for="c-39892000">[-]</label><label class="expand" for="c-39892000">[1 more]</label></div><br/><div class="children"><div class="content">I asked a stupid question and got a stupid answer. Relatively speaking the answer was stupider than it should have been, so yes, it was wrong.<p>I asked it to try again and got a better result though, just didn&#x27;t include it.</div><br/></div></div><div id="39891874" class="c"><input type="checkbox" id="c-39891874" checked=""/><div class="controls bullet"><span class="by">yunohn</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891468">parent</a><span>|</span><a href="#39892000">prev</a><span>|</span><a href="#39891452">next</a><span>|</span><label class="collapse" for="c-39891874">[-]</label><label class="expand" for="c-39891874">[17 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t use LLMs for my coding, I manage just fine with LSP and Treesitter.<p>You’re literally comparing apples to oranges.</div><br/><div id="39893108" class="c"><input type="checkbox" id="c-39893108" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891874">parent</a><span>|</span><a href="#39891918">next</a><span>|</span><label class="collapse" for="c-39893108">[-]</label><label class="expand" for="c-39893108">[15 more]</label></div><br/><div class="children"><div class="content">You need to read more than just the first sentence of a comment.  They only said that part so the reader would know that they have never used an LLM for coding, so they would have more context for the question:<p>&gt; So genuine question: is that answer representative of the output quality of these things?</div><br/><div id="39894217" class="c"><input type="checkbox" id="c-39894217" checked=""/><div class="controls bullet"><span class="by">yunohn</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39893108">parent</a><span>|</span><a href="#39891918">next</a><span>|</span><label class="collapse" for="c-39894217">[-]</label><label class="expand" for="c-39894217">[14 more]</label></div><br/><div class="children"><div class="content">Yes, I did read it. I’m kind of tired of HNers loudly proclaiming they are ignoring LLMs more than a year into this paradigm shift.<p>Is it that hard to input a prompt into the free version of ChatGPT and see how it helps with programming?</div><br/><div id="39894877" class="c"><input type="checkbox" id="c-39894877" checked=""/><div class="controls bullet"><span class="by">jpc0</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39894217">parent</a><span>|</span><a href="#39891918">next</a><span>|</span><label class="collapse" for="c-39894877">[-]</label><label class="expand" for="c-39894877">[13 more]</label></div><br/><div class="children"><div class="content">I did exactly that and found it lackluster for the domain I asked it for.<p>And most use I&#x27;ve seen on it realistically a good LSP covers.<p>Or to put it a other way. It&#x27;s no good at writing algorithms or data structures ( or at least no better thab I would have with a first drafy but the first draft puts me ahead of the LLM in understanding that actual problem at hand, handing it off to an LLM doesn&#x27;t help me get to the final solution faster).<p>So that leaves writing boiler plate but concidering my experience with it writing more complex stuff, I would need to read over the boilerplate code to ensure it&#x27;s correct which in that case I may as well have written it.</div><br/><div id="39895096" class="c"><input type="checkbox" id="c-39895096" checked=""/><div class="controls bullet"><span class="by">yunohn</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39894877">parent</a><span>|</span><a href="#39891918">next</a><span>|</span><label class="collapse" for="c-39895096">[-]</label><label class="expand" for="c-39895096">[12 more]</label></div><br/><div class="children"><div class="content">&gt; found it lackluster for the domain I asked it for<p>Fair, that is possible depending on your domain.<p>&gt; It&#x27;s no good at writing algorithms or data structures<p>In my experience, this is untrue. I’ve gotten it to write algorithms with various constraints I had. You can even tell it to use specific function signatures instead of any stdlib, and make changes to tweak behavior.<p>&gt; And most use I&#x27;ve seen on it realistically a good LSP covers.<p>Again, I really don’t understand this comparison. LSPs and LLMs go hand in hand.<p>I think it’s more of a workflow clash. One really needs to change how they operate to effectively use LLMs for programming. If you’re just typing nonstop, maybe it would feel like Copilot is just an LSP. But, if you try harder, LLMs are game changers when:<p>- maybe you like rubber ducking<p>- need to learn a new concept and implement it<p>- or need to glue things together<p>- or for new projects or features<p>- or filling in boilerplate based on existing context.</div><br/><div id="39896410" class="c"><input type="checkbox" id="c-39896410" checked=""/><div class="controls bullet"><span class="by">jpc0</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39895096">parent</a><span>|</span><a href="#39891918">next</a><span>|</span><label class="collapse" for="c-39896410">[-]</label><label class="expand" for="c-39896410">[11 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;c8c19f42-240f-44e7-baf4-50ee5e7f5000" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;c8c19f42-240f-44e7-baf4-50ee5e...</a><p><a href="https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;s9Yvnjz7K" rel="nofollow">https:&#x2F;&#x2F;godbolt.org&#x2F;z&#x2F;s9Yvnjz7K</a><p>I mean I could write the algorithm by hand pretty quickly in C++ and would follow the exact same thought pattern but also deal with the edge cases. And factoring in the loss of productivity from the context switch that is a net negative. This algorithm is also not generic over enough cases but that is just up to the prompt.<p>If I can&#x27;t trust it to write `strip_whitespace` correctly which is like 5 lines of code, can I trust it to do more without a thorough review of the code and writing a ton of unit tests... Well I was going to do that anyway.<p>The argument that I just need to learn better prompt engineering to make the LLM do what I want just doesn&#x27;t sit with me when instead I could just spend the time writing the code. As I said your last point is absolutely the place I can see LLMs being actually useful but then I need to spend a significant amount of time in code review for generated code from an &quot;employee&quot; who is known to make up interfaces or entire libraries that doesn&#x27;t exist.</div><br/><div id="39897031" class="c"><input type="checkbox" id="c-39897031" checked=""/><div class="controls bullet"><span class="by">mrtranscendence</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39896410">parent</a><span>|</span><a href="#39891918">next</a><span>|</span><label class="collapse" for="c-39897031">[-]</label><label class="expand" for="c-39897031">[10 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a Python-slinging data scientist so C++ isn&#x27;t my jam (to say the least), but I changed the prompt to the following and asked it to GPT-4:<p>&gt; Write me an algorithm in C++ which finds the begin and end iterator of a sequence where leading and trailing whitespace is stripped. Please write secure code that handles any possible edge cases.<p>It gave me this:<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;55a4afe2-5db2-4dd1-b516-a3cacd93b350" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;55a4afe2-5db2-4dd1-b516-a3cacd...</a><p>I&#x27;m not sure what other edge cases there might be, however. This only covers one of them.<p>In general, I&#x27;ve found LLMs to be <i>marginally</i> helpful. Like, I can&#x27;t ever remember how to get matplotlib to give me the plot I want, and 9 times out of 10 GPT-4 easily gives me the code I want. Anything even <i>slightly</i> off the beaten path, though, and it quickly becomes absolutely useless.</div><br/><div id="39897576" class="c"><input type="checkbox" id="c-39897576" checked=""/><div class="controls bullet"><span class="by">jpc0</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39897031">parent</a><span>|</span><a href="#39891918">next</a><span>|</span><label class="collapse" for="c-39897576">[-]</label><label class="expand" for="c-39897576">[9 more]</label></div><br/><div class="children"><div class="content">My guess is that this was generated using GPT4?<p>Free GPT I get <a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;f533429d-63ca-4505-8dc8-b8d2e799ca49" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;f533429d-63ca-4505-8dc8-b8d2e7...</a> which has exactly the same problem as my previous example and doesn&#x27;t consider the string of all whitespace.<p>Sure GPT4 is better at that, it wasn&#x27;t the argument made.<p>The example you gave absolutely was the code I would write on a first draft since it does cover the edge cases (assuming we aren&#x27;t dealing with the full UTF charset and all that could be considered a space there).<p>However this is code that is trivial to write in any language and the &quot;Is it that hard to input a prompt into the free version of ChatGPT and see how it helps with programming? &quot; argument doesn&#x27;t hold up. Am I to believe it will implement something more complex correctly. This is also code that would absolutely be in hundreds of codebases so GPT has tons of context for it.</div><br/><div id="39898218" class="c"><input type="checkbox" id="c-39898218" checked=""/><div class="controls bullet"><span class="by">mrtranscendence</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39897576">parent</a><span>|</span><a href="#39897888">next</a><span>|</span><label class="collapse" for="c-39898218">[-]</label><label class="expand" for="c-39898218">[2 more]</label></div><br/><div class="children"><div class="content">I think you have the mistaken impression that I was arguing with you (certainly my comment makes it clear that I don&#x27;t feel that LLMs are a panacea). I merely thought that you might be curious how GPT-4 would respond.<p>&gt; My guess is that this was generated using GPT4?<p>This is a good guess, since I stated outright that I used GPT-4, and then mentioned GPT-4 later on in the comment.</div><br/><div id="39898478" class="c"><input type="checkbox" id="c-39898478" checked=""/><div class="controls bullet"><span class="by">jpc0</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39898218">parent</a><span>|</span><a href="#39897888">next</a><span>|</span><label class="collapse" for="c-39898478">[-]</label><label class="expand" for="c-39898478">[1 more]</label></div><br/><div class="children"><div class="content">I was curious and yes I was mistaken.</div><br/></div></div></div></div><div id="39897888" class="c"><input type="checkbox" id="c-39897888" checked=""/><div class="controls bullet"><span class="by">yunohn</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39897576">parent</a><span>|</span><a href="#39898218">prev</a><span>|</span><a href="#39891918">next</a><span>|</span><label class="collapse" for="c-39897888">[-]</label><label class="expand" for="c-39897888">[6 more]</label></div><br/><div class="children"><div class="content">Yeah honestly, I think you have a completely different expectation and style of usage than what is optimal with LLMs. I don’t have the energy to convince you further, but maybe one day it’ll click for you? No worries either way.</div><br/><div id="39898487" class="c"><input type="checkbox" id="c-39898487" checked=""/><div class="controls bullet"><span class="by">jpc0</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39897888">parent</a><span>|</span><a href="#39891918">next</a><span>|</span><label class="collapse" for="c-39898487">[-]</label><label class="expand" for="c-39898487">[5 more]</label></div><br/><div class="children"><div class="content">Could you maybe give me an example of what is concidered an optimal use of LLMs.<p>Maybe a prompt to GPT</div><br/><div id="39899151" class="c"><input type="checkbox" id="c-39899151" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39898487">parent</a><span>|</span><a href="#39899515">next</a><span>|</span><label class="collapse" for="c-39899151">[-]</label><label class="expand" for="c-39899151">[2 more]</label></div><br/><div class="children"><div class="content">Simonw&#x27;s blog has some examples I&#x27;d consider show off its usefulness and limitations,
eg<p><a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Mar&#x2F;23&#x2F;building-c-extensions-for-sqlite-with-chatgpt-code-interpreter&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Mar&#x2F;23&#x2F;building-c-extensions-...</a><p>(linked previously above)</div><br/><div id="39903574" class="c"><input type="checkbox" id="c-39903574" checked=""/><div class="controls bullet"><span class="by">jpc0</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39899151">parent</a><span>|</span><a href="#39899515">next</a><span>|</span><label class="collapse" for="c-39903574">[-]</label><label class="expand" for="c-39903574">[1 more]</label></div><br/><div class="children"><div class="content">Are you happy with the C code generated there?<p>I&#x27;m not sure there isn&#x27;t a buffer overflow in the vector_decode code he showed there, likewise I don&#x27;t see any error checks on the code and I am not familiar with the sqlite api to even know whether errors can be propagated upwards and what error conditions would mean in that code.<p>This code is probably fine for a quick side project but doesn&#x27;t pass my smell test for anything close to production ready code.<p>I definitely would want to see a lot of unit tests around the decode and encode functions with fuzzing and to be honestly that would be the bulk of the work here. That and documentation on this code. Even though he encode function looks correct at first glance.<p>I also don&#x27;t see an easy way to actually unit test this code as it is without actually running it through sqlite which outs a lot of dependencies on the unit test.<p>I would either need to spend a lot more time massaging gpt to get this to a point where I would be fine shipping the code or you know just write it myself.</div><br/></div></div></div></div><div id="39899515" class="c"><input type="checkbox" id="c-39899515" checked=""/><div class="controls bullet"><span class="by">yunohn</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39898487">parent</a><span>|</span><a href="#39899151">prev</a><span>|</span><a href="#39891918">next</a><span>|</span><label class="collapse" for="c-39899515">[-]</label><label class="expand" for="c-39899515">[2 more]</label></div><br/><div class="children"><div class="content">Like sibling commenter mentioned, simonw’s blog is a great resource.<p>Regarding your point around being able to whip up the code yourself - the point is to have a decent starting point to save time and energy. Like you said, you know the edge cases so you could skip the boring parts using GPT and focus purely on fixing those. Though, with more prompting (especially providing examples), GPT can also handle that for you.<p>I have nearly 2 decades of experience as a developer and it took me a while to reorient my flow around LLMs. But now that I have, it’s truly gamechanging.<p>And since you asked, here’s my system prompt:<p>You are an experienced developer who follows industry standards and best practices. Write lean code and explain briefly using bullet points or numbered lists. Elaborate only when explaining concepts or making choices. Always mention which file and where to store provided code.<p>Tech Stack: &lt; insert all the languages, frameworks, etc you’d like to use &gt;<p>If I provide code, highlight and explain problematic code. Also show and explain the corrected code.<p>Take a deep breath and think step by step.<p>Also, always use GPT4 and customize the above to your style and liking.</div><br/><div id="39903542" class="c"><input type="checkbox" id="c-39903542" checked=""/><div class="controls bullet"><span class="by">jpc0</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39899515">parent</a><span>|</span><a href="#39891918">next</a><span>|</span><label class="collapse" for="c-39903542">[-]</label><label class="expand" for="c-39903542">[1 more]</label></div><br/><div class="children"><div class="content">I will definitely try this out when I have time later in the day.<p>There is some code I would really prefer not to write that is a decent test case for this and won&#x27;t expose company code to GPT. Will give feedback when I am done. Maybe you are correct.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39891918" class="c"><input type="checkbox" id="c-39891918" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891874">parent</a><span>|</span><a href="#39893108">prev</a><span>|</span><a href="#39891452">next</a><span>|</span><label class="collapse" for="c-39891918">[-]</label><label class="expand" for="c-39891918">[1 more]</label></div><br/><div class="children"><div class="content">I think the point was like &quot;when it comes to programming assistance, auto-completion&#x2F;linting&#x2F;and whatever else LSP does and syntax assist from Treesitter, are enough for me&quot;.<p>Though it does come a little off as a comparison. How about programming assistance via asking a colleague for help, Stack Overflow, or online references, code examples, and other such things, which are closer to what the LLM would provide than LSP and treesitter?</div><br/></div></div></div></div></div></div><div id="39891452" class="c"><input type="checkbox" id="c-39891452" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891350">parent</a><span>|</span><a href="#39891468">prev</a><span>|</span><a href="#39899563">next</a><span>|</span><label class="collapse" for="c-39891452">[-]</label><label class="expand" for="c-39891452">[3 more]</label></div><br/><div class="children"><div class="content">Interesting. It was 4. I can&#x27;t share the chat I had where ChatGPT refused to help because I used the wrong words, because I can&#x27;t find it (ChatGPT conversation history search when?), but I just remember it refusing to do something because it thought I was trying to break some sort of moral and ethical boundary writing a chrome extension when all I wanted to do is move some divs around or some such.</div><br/><div id="39892920" class="c"><input type="checkbox" id="c-39892920" checked=""/><div class="controls bullet"><span class="by">BytesAndGears</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891452">parent</a><span>|</span><a href="#39899563">next</a><span>|</span><label class="collapse" for="c-39892920">[-]</label><label class="expand" for="c-39892920">[2 more]</label></div><br/><div class="children"><div class="content">One time I wanted to learn about transmitter antenna design, just because I’m curious. ChatGPT 4 refused to give me basic information because you could use that to break some FCC regulations (I’m not even living in the US currently)</div><br/><div id="39897297" class="c"><input type="checkbox" id="c-39897297" checked=""/><div class="controls bullet"><span class="by">lodovic</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39892920">parent</a><span>|</span><a href="#39899563">next</a><span>|</span><label class="collapse" for="c-39897297">[-]</label><label class="expand" for="c-39897297">[1 more]</label></div><br/><div class="children"><div class="content">I usually get around that with &quot;I&#x27;m writing a research paper&quot; or &quot;I&#x27;m writing a novel and need to depict this as accurate as possible&quot;</div><br/></div></div></div></div></div></div></div></div><div id="39899563" class="c"><input type="checkbox" id="c-39899563" checked=""/><div class="controls bullet"><span class="by">anukin</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891030">parent</a><span>|</span><a href="#39891350">prev</a><span>|</span><a href="#39892590">next</a><span>|</span><label class="collapse" for="c-39899563">[-]</label><label class="expand" for="c-39899563">[2 more]</label></div><br/><div class="children"><div class="content">Which uncensored model is willing to play hot or not? I just knew about llava. Are there other such models now?</div><br/><div id="39901949" class="c"><input type="checkbox" id="c-39901949" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39899563">parent</a><span>|</span><a href="#39892590">next</a><span>|</span><label class="collapse" for="c-39901949">[-]</label><label class="expand" for="c-39901949">[1 more]</label></div><br/><div class="children"><div class="content">Llava just integrates Clip with a llama model. Koboldcpp can now do this with many models out of the box:<p>* <a href="https:&#x2F;&#x2F;github.com&#x2F;LostRuins&#x2F;koboldcpp&#x2F;releases&#x2F;tag&#x2F;v1.61.2">https:&#x2F;&#x2F;github.com&#x2F;LostRuins&#x2F;koboldcpp&#x2F;releases&#x2F;tag&#x2F;v1.61.2</a></div><br/></div></div></div></div><div id="39892590" class="c"><input type="checkbox" id="c-39892590" checked=""/><div class="controls bullet"><span class="by">gryn</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891030">parent</a><span>|</span><a href="#39899563">prev</a><span>|</span><a href="#39892513">next</a><span>|</span><label class="collapse" for="c-39892590">[-]</label><label class="expand" for="c-39892590">[2 more]</label></div><br/><div class="children"><div class="content">&gt; There&#x27;s an uncensored model for vision available as well.<p>you mean the LLava based variants ?</div><br/><div id="39892790" class="c"><input type="checkbox" id="c-39892790" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39892590">parent</a><span>|</span><a href="#39892513">next</a><span>|</span><label class="collapse" for="c-39892790">[-]</label><label class="expand" for="c-39892790">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;SkunkworksAI&#x2F;BakLLaVA-1" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;SkunkworksAI&#x2F;BakLLaVA-1</a></div><br/></div></div></div></div><div id="39892513" class="c"><input type="checkbox" id="c-39892513" checked=""/><div class="controls bullet"><span class="by">supposemaybe</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891030">parent</a><span>|</span><a href="#39892590">prev</a><span>|</span><a href="#39891932">next</a><span>|</span><label class="collapse" for="c-39892513">[-]</label><label class="expand" for="c-39892513">[3 more]</label></div><br/><div class="children"><div class="content">Links to all these models you speak of?</div><br/><div id="39892828" class="c"><input type="checkbox" id="c-39892828" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39892513">parent</a><span>|</span><a href="#39891932">next</a><span>|</span><label class="collapse" for="c-39892828">[-]</label><label class="expand" for="c-39892828">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;georgesung&#x2F;llama2_7b_chat_uncensored" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;georgesung&#x2F;llama2_7b_chat_uncensored</a><p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;SkunkworksAI&#x2F;BakLLaVA-1" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;SkunkworksAI&#x2F;BakLLaVA-1</a><p>you&#x27;ll have to brave 4chan yourself to find links to the NSFW ones, I don&#x27;t actually have them.</div><br/><div id="39893261" class="c"><input type="checkbox" id="c-39893261" checked=""/><div class="controls bullet"><span class="by">supposemaybe</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39892828">parent</a><span>|</span><a href="#39891932">next</a><span>|</span><label class="collapse" for="c-39893261">[-]</label><label class="expand" for="c-39893261">[1 more]</label></div><br/><div class="children"><div class="content">I just can’t brave the venture to 4chan, I may get mugged or worse.</div><br/></div></div></div></div></div></div><div id="39891932" class="c"><input type="checkbox" id="c-39891932" checked=""/><div class="controls bullet"><span class="by">kevingadd</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891030">parent</a><span>|</span><a href="#39892513">prev</a><span>|</span><a href="#39890763">next</a><span>|</span><label class="collapse" for="c-39891932">[-]</label><label class="expand" for="c-39891932">[6 more]</label></div><br/><div class="children"><div class="content">If you want to be an amateur chemist I recommend not getting your instructions from an LLM that might be hallucinating. Chemistry can be very dangerous if you&#x27;re following incorrect instructions.</div><br/><div id="39892503" class="c"><input type="checkbox" id="c-39892503" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891932">parent</a><span>|</span><a href="#39892610">next</a><span>|</span><label class="collapse" for="c-39892503">[-]</label><label class="expand" for="c-39892503">[1 more]</label></div><br/><div class="children"><div class="content">From experience as a failed organic chemist (who happily switched to computational chemistry for reasons of self preservation) I can tell you it&#x27;s plenty dangerous when you&#x27;re following correct instructions :^)</div><br/></div></div><div id="39892610" class="c"><input type="checkbox" id="c-39892610" checked=""/><div class="controls bullet"><span class="by">rpigab</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891932">parent</a><span>|</span><a href="#39892503">prev</a><span>|</span><a href="#39890763">next</a><span>|</span><label class="collapse" for="c-39892610">[-]</label><label class="expand" for="c-39892610">[4 more]</label></div><br/><div class="children"><div class="content">Yes, just as the best professional cooks recommend avoiding to boil cow eggs, as they can explode.</div><br/><div id="39896049" class="c"><input type="checkbox" id="c-39896049" checked=""/><div class="controls bullet"><span class="by">slowmovintarget</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39892610">parent</a><span>|</span><a href="#39890763">next</a><span>|</span><label class="collapse" for="c-39896049">[-]</label><label class="expand" for="c-39896049">[3 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t explode, the shell simply cracks and then you get egg soup.<p>Now microwaving eggs... that&#x27;s a different matter.</div><br/><div id="39896273" class="c"><input type="checkbox" id="c-39896273" checked=""/><div class="controls bullet"><span class="by">rpigab</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39896049">parent</a><span>|</span><a href="#39890763">next</a><span>|</span><label class="collapse" for="c-39896273">[-]</label><label class="expand" for="c-39896273">[2 more]</label></div><br/><div class="children"><div class="content">I was talking about cow eggs specifically! When ChatGPT et al got out, one of the funniest things to do was ask it about the best recipes for cow egg omelette or camel egg salad, and the LLM would provide. Sadly, most of it got patched somehow.</div><br/><div id="39897916" class="c"><input type="checkbox" id="c-39897916" checked=""/><div class="controls bullet"><span class="by">slowmovintarget</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39896273">parent</a><span>|</span><a href="#39890763">next</a><span>|</span><label class="collapse" for="c-39897916">[-]</label><label class="expand" for="c-39897916">[1 more]</label></div><br/><div class="children"><div class="content">Oops... Yep, I missed that too. (On the internet, no one knows you&#x27;re a dog.)<p>That&#x27;s funny. It makes me wonder how these statistical mad libs machines will handle the gradual boundaries nature gives us. Almost all mammals give birth live, but not all. Nearly all mammals had mammalian parents, but not all.<p>Daniel Dennett was making this argument for why we haven&#x27;t developed reasonable models for the nature of consciousness. It&#x27;s because we&#x27;re so sure there will be an absolute classification, and not a gradual accumulation of interacting systems that together yield the phenomenon.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39890763" class="c"><input type="checkbox" id="c-39890763" checked=""/><div class="controls bullet"><span class="by">devsda</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39891030">prev</a><span>|</span><a href="#39891106">next</a><span>|</span><label class="collapse" for="c-39890763">[-]</label><label class="expand" for="c-39890763">[6 more]</label></div><br/><div class="children"><div class="content">For someone interested in learning about LLMs, running them locally is a good way to understand the internals.<p>For everyone else, I wish they  experience these (locally or elsewhere) <i>weak</i> LLMs atleast once before using the commercial ones just to understand various failure modes and to introduce a healthy dose of skepticism towards the results instead of blindly trusting them to be the facts&#x2F;truth.</div><br/><div id="39890770" class="c"><input type="checkbox" id="c-39890770" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890763">parent</a><span>|</span><a href="#39891348">next</a><span>|</span><label class="collapse" for="c-39890770">[-]</label><label class="expand" for="c-39890770">[1 more]</label></div><br/><div class="children"><div class="content">Completely agree. Playing around with a weak LLM is a great way to give yourself a little bit of extra healthy skepticism for when you work with the strong ones.</div><br/></div></div><div id="39891348" class="c"><input type="checkbox" id="c-39891348" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890763">parent</a><span>|</span><a href="#39890770">prev</a><span>|</span><a href="#39891274">next</a><span>|</span><label class="collapse" for="c-39891348">[-]</label><label class="expand" for="c-39891348">[1 more]</label></div><br/><div class="children"><div class="content">This skepticism is completely justified since ChatGPT 3.5 is also happily hallucinating things that don&#x27;t exist. For example how to integrate a different system Python interpreter into pyenv. Though maybe ChatGPT 4 doesn&#x27;t :)</div><br/></div></div><div id="39891274" class="c"><input type="checkbox" id="c-39891274" checked=""/><div class="controls bullet"><span class="by">mmahemoff</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890763">parent</a><span>|</span><a href="#39891348">prev</a><span>|</span><a href="#39891106">next</a><span>|</span><label class="collapse" for="c-39891274">[-]</label><label class="expand" for="c-39891274">[3 more]</label></div><br/><div class="children"><div class="content">How do you learn about the internals by running LLMs locally? Are you playing with The code, runtime params, or just interacting via chat?</div><br/><div id="39891364" class="c"><input type="checkbox" id="c-39891364" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891274">parent</a><span>|</span><a href="#39892630">next</a><span>|</span><label class="collapse" for="c-39891364">[-]</label><label class="expand" for="c-39891364">[1 more]</label></div><br/><div class="children"><div class="content">The abstractions are relatively brittle. If you don&#x27;t have a powerful GPU, you will be forced to consider how to split the model between CPU and GPU, how much context size you need, whether to quantize the model, and the tradeoffs implied by these things. To understand these, you have to develop a basic model how an LLM works.</div><br/></div></div><div id="39892630" class="c"><input type="checkbox" id="c-39892630" checked=""/><div class="controls bullet"><span class="by">barrkel</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891274">parent</a><span>|</span><a href="#39891364">prev</a><span>|</span><a href="#39891106">next</a><span>|</span><label class="collapse" for="c-39892630">[-]</label><label class="expand" for="c-39892630">[1 more]</label></div><br/><div class="children"><div class="content">By interacting with it. You see the contours of its capabilities much more clearly, learn to recognize failure modes, understand how prior conversation can set the course of future conversation in a way that&#x27;s almost impossible to correct without starting over or editing the conversation history.</div><br/></div></div></div></div></div></div><div id="39891106" class="c"><input type="checkbox" id="c-39891106" checked=""/><div class="controls bullet"><span class="by">tgma</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39890763">prev</a><span>|</span><a href="#39890986">next</a><span>|</span><label class="collapse" for="c-39891106">[-]</label><label class="expand" for="c-39891106">[17 more]</label></div><br/><div class="children"><div class="content">If you have an &gt;=M1-class machine with sufficient RAM, the medium-sized models that are on the order of 30GB in size perform decently on many tasks to be quite useful without leaking your data.</div><br/><div id="39891340" class="c"><input type="checkbox" id="c-39891340" checked=""/><div class="controls bullet"><span class="by">noman-land</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891106">parent</a><span>|</span><a href="#39891112">next</a><span>|</span><label class="collapse" for="c-39891340">[-]</label><label class="expand" for="c-39891340">[13 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using Mixtral 8x7b as a llamafile on an M1 regularly for coding help and general Q&amp;A. It&#x27;s really something wonderful to just run a single command and have this incredible offline resource.</div><br/><div id="39891386" class="c"><input type="checkbox" id="c-39891386" checked=""/><div class="controls bullet"><span class="by">tgma</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891340">parent</a><span>|</span><a href="#39891710">next</a><span>|</span><label class="collapse" for="c-39891386">[-]</label><label class="expand" for="c-39891386">[1 more]</label></div><br/><div class="children"><div class="content">I concur; in my experience Mixtral is one of the best ~30G models (likely the best pro laptop-size model currently) and Gemma is quite good compared to other below 8GB models.</div><br/></div></div><div id="39891710" class="c"><input type="checkbox" id="c-39891710" checked=""/><div class="controls bullet"><span class="by">tchvil</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891340">parent</a><span>|</span><a href="#39891386">prev</a><span>|</span><a href="#39891112">next</a><span>|</span><label class="collapse" for="c-39891710">[-]</label><label class="expand" for="c-39891710">[11 more]</label></div><br/><div class="children"><div class="content">By any chance, do you have a good link to some help with the installation?</div><br/><div id="39891828" class="c"><input type="checkbox" id="c-39891828" checked=""/><div class="controls bullet"><span class="by">yaantc</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891710">parent</a><span>|</span><a href="#39903278">next</a><span>|</span><label class="collapse" for="c-39891828">[-]</label><label class="expand" for="c-39891828">[2 more]</label></div><br/><div class="children"><div class="content">Use llamafile [1], it can be as simple as downloading a file (for mixtral, [2]), making it executable and running it. The repo README has all the info, it&#x27;s simple and downloading the model is what takes the most time.<p>In my case I got the runtime detection issue (explained in the README &quot;gotcha&quot; section). Solved my running &quot;assimilate&quot; [3] on the downloaded llamafile.<p><pre><code>    [1] https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile&#x2F;
    [2] https:&#x2F;&#x2F;huggingface.co&#x2F;jartine&#x2F;Mixtral-8x7B-Instruct-v0.1-llamafile&#x2F;resolve&#x2F;main&#x2F;mixtral-8x7b-instruct-v0.1.Q5_K_M.llamafile?download=true
    [3] https:&#x2F;&#x2F;cosmo.zip&#x2F;pub&#x2F;cosmos&#x2F;bin&#x2F;assimilate</code></pre></div><br/><div id="39894382" class="c"><input type="checkbox" id="c-39894382" checked=""/><div class="controls bullet"><span class="by">tchvil</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891828">parent</a><span>|</span><a href="#39903278">next</a><span>|</span><label class="collapse" for="c-39894382">[-]</label><label class="expand" for="c-39894382">[1 more]</label></div><br/><div class="children"><div class="content">Thank you !</div><br/></div></div></div></div><div id="39903278" class="c"><input type="checkbox" id="c-39903278" checked=""/><div class="controls bullet"><span class="by">firewolf34</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891710">parent</a><span>|</span><a href="#39891828">prev</a><span>|</span><a href="#39891829">next</a><span>|</span><label class="collapse" for="c-39903278">[-]</label><label class="expand" for="c-39903278">[1 more]</label></div><br/><div class="children"><div class="content">Check out PrivateGPT on GitHub. Pretty much just works put of the box. I got Mistral7B running on a GTX 970 in about 30 minutes flat first try. Yep, that&#x27;s the triple-digit GTX 970.</div><br/></div></div><div id="39891829" class="c"><input type="checkbox" id="c-39891829" checked=""/><div class="controls bullet"><span class="by">tgma</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891710">parent</a><span>|</span><a href="#39903278">prev</a><span>|</span><a href="#39893841">next</a><span>|</span><label class="collapse" for="c-39891829">[-]</label><label class="expand" for="c-39891829">[2 more]</label></div><br/><div class="children"><div class="content">Either <a href="https:&#x2F;&#x2F;lmstudio.ai" rel="nofollow">https:&#x2F;&#x2F;lmstudio.ai</a> (desktop app with nice GUI) or <a href="https:&#x2F;&#x2F;ollama.com">https:&#x2F;&#x2F;ollama.com</a> (command-like more like a docker container that you can also hook up to a web UI via <a href="https:&#x2F;&#x2F;openwebui.com" rel="nofollow">https:&#x2F;&#x2F;openwebui.com</a>) should be super straightforward to get running.</div><br/><div id="39894414" class="c"><input type="checkbox" id="c-39894414" checked=""/><div class="controls bullet"><span class="by">tchvil</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891829">parent</a><span>|</span><a href="#39893841">next</a><span>|</span><label class="collapse" for="c-39894414">[-]</label><label class="expand" for="c-39894414">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for letting me know it was possible on an M1. I&#x27;ll try all this now.</div><br/></div></div></div></div><div id="39893841" class="c"><input type="checkbox" id="c-39893841" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891710">parent</a><span>|</span><a href="#39891829">prev</a><span>|</span><a href="#39891112">next</a><span>|</span><label class="collapse" for="c-39893841">[-]</label><label class="expand" for="c-39893841">[5 more]</label></div><br/><div class="children"><div class="content">I am the author of Msty [1]. My goal is to make it as straightforward as possible with just one click (once you download the app). If you try it, let me know what you think.<p>1: <a href="https:&#x2F;&#x2F;msty.app" rel="nofollow">https:&#x2F;&#x2F;msty.app</a></div><br/><div id="39903667" class="c"><input type="checkbox" id="c-39903667" checked=""/><div class="controls bullet"><span class="by">omnibrain</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39893841">parent</a><span>|</span><a href="#39894411">next</a><span>|</span><label class="collapse" for="c-39903667">[-]</label><label class="expand" for="c-39903667">[1 more]</label></div><br/><div class="children"><div class="content">Looks great. Can you recommend what GPU to get to just play with the models for a bit? (I want to have perform it fast, otherwise I lose interest too quickly).
Are consumer GPUs like the RTX 4080 Super sufficient, or do I need anything else?</div><br/></div></div><div id="39894411" class="c"><input type="checkbox" id="c-39894411" checked=""/><div class="controls bullet"><span class="by">tchvil</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39893841">parent</a><span>|</span><a href="#39903667">prev</a><span>|</span><a href="#39899933">next</a><span>|</span><label class="collapse" for="c-39894411">[-]</label><label class="expand" for="c-39894411">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll try in a week+ when I&#x27;m back to a fast connection. Thank you.</div><br/></div></div><div id="39899933" class="c"><input type="checkbox" id="c-39899933" checked=""/><div class="controls bullet"><span class="by">yunohn</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39893841">parent</a><span>|</span><a href="#39894411">prev</a><span>|</span><a href="#39891112">next</a><span>|</span><label class="collapse" for="c-39899933">[-]</label><label class="expand" for="c-39899933">[2 more]</label></div><br/><div class="children"><div class="content">Why is this both free and closed source? Ideally, when you advertise privacy-first, I’d like to see a GitHub link with real source code. Or I’d rather pay for it to ensure you have a financial incentive to not sell my data.</div><br/><div id="39899987" class="c"><input type="checkbox" id="c-39899987" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39899933">parent</a><span>|</span><a href="#39891112">next</a><span>|</span><label class="collapse" for="c-39899987">[-]</label><label class="expand" for="c-39899987">[1 more]</label></div><br/><div class="children"><div class="content">It will be paid down the road, but we are not there yet. It’s all offline, data is locally saved. You own it, we don’t have it even if you ask for it.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39891112" class="c"><input type="checkbox" id="c-39891112" checked=""/><div class="controls bullet"><span class="by">bongobingo1</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891106">parent</a><span>|</span><a href="#39891340">prev</a><span>|</span><a href="#39892549">next</a><span>|</span><label class="collapse" for="c-39891112">[-]</label><label class="expand" for="c-39891112">[2 more]</label></div><br/><div class="children"><div class="content">What is sufficient RAM in that case? 30gb+? Or can you get by streaming it?</div><br/><div id="39891191" class="c"><input type="checkbox" id="c-39891191" checked=""/><div class="controls bullet"><span class="by">AaronFriel</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891112">parent</a><span>|</span><a href="#39892549">next</a><span>|</span><label class="collapse" for="c-39891191">[-]</label><label class="expand" for="c-39891191">[1 more]</label></div><br/><div class="children"><div class="content">30gb+, yeah. You can&#x27;t get by streaming the model&#x27;s parameters: NVMe isn&#x27;t fast enough. Consumer GPUs and Apple Silicon processors boast memory bandwidths in the hundreds of gigabytes per second.<p>To a first order approximation, LLMs are bandwidth constrained. We can estimate single batch throughput as Memory Bandwidth &#x2F; (Active Parameters * Parameter Size).<p>An 8-bit quantized Llama 2 70B conveniently uses 70GiB of VRAM (and then some, let&#x27;s ignore that.) The M3 Max with 96GiB of VRAM and 300GiB&#x2F;s bandwidth would have a peak throughput around 4.2 tokens per second.<p>Quantized models trade reduced quality for lower VRAM requirements and may also offer higher throughput with optimized kernels, largely as a consequence of transfering less data from VRAM into the GPU die for each parameter.<p>Mixture of Expert models reduce active parameters for higher throughput, but disk is still far too slow to page in layers.</div><br/></div></div></div></div><div id="39892549" class="c"><input type="checkbox" id="c-39892549" checked=""/><div class="controls bullet"><span class="by">supposemaybe</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891106">parent</a><span>|</span><a href="#39891112">prev</a><span>|</span><a href="#39890986">next</a><span>|</span><label class="collapse" for="c-39892549">[-]</label><label class="expand" for="c-39892549">[1 more]</label></div><br/><div class="children"><div class="content">It’s an awful thing for many to accept, but just downloading and setting up an LLM which doesn’t connect to the web doesn’t mean that your conversations with said LLM won’t be a severely interesting piece of telemetry that Microsoft and (likely Apple) would swipe to help deliver a ‘better service’ to you.</div><br/></div></div></div></div><div id="39890986" class="c"><input type="checkbox" id="c-39890986" checked=""/><div class="controls bullet"><span class="by">tracerbulletx</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39891106">prev</a><span>|</span><a href="#39890833">next</a><span>|</span><label class="collapse" for="c-39890986">[-]</label><label class="expand" for="c-39890986">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t really think this is true, you can&#x27;t really extrapolate the strengths and weaknesses of bigger models from the behavior of smaller&#x2F;quantized models and in fact a lot of small models are actually great at lots of things and better at creative writing. If you want to know how they work, just learn how they work, it takes like 5 hours of watching Youtube videos if you&#x27;re a programmer.</div><br/><div id="39891065" class="c"><input type="checkbox" id="c-39891065" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890986">parent</a><span>|</span><a href="#39890833">next</a><span>|</span><label class="collapse" for="c-39891065">[-]</label><label class="expand" for="c-39891065">[2 more]</label></div><br/><div class="children"><div class="content">Sure, you can&#x27;t extrapolate the strengths and weaknesses of the larger ones from the smaller ones - but you still get a much firmer idea of what &quot;they&#x27;re fancy autocomplete&quot; actually means.<p>If nothing else it does a great job of demystifying them. They feel a lot less intimidating once you&#x27;ve seen a small one running on your computer write a terrible haiku and hallucinate some non-existent API methods.</div><br/><div id="39891105" class="c"><input type="checkbox" id="c-39891105" checked=""/><div class="controls bullet"><span class="by">fzzzy</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891065">parent</a><span>|</span><a href="#39890833">next</a><span>|</span><label class="collapse" for="c-39891105">[-]</label><label class="expand" for="c-39891105">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s funny that you say this, because the first thing I tried after ChatGPT came out (3.5-turbo was it?) was writing a haiku. It couldn&#x27;t do it at all. Also, after 4 came out, it hallucinated an api that wasted a day for me. It&#x27;s an api that absolutely should have existed, but didn&#x27;t. Now, I frequently apply llm to things that are easily verifiable, and just double check everything.</div><br/></div></div></div></div></div></div><div id="39890833" class="c"><input type="checkbox" id="c-39890833" checked=""/><div class="controls bullet"><span class="by">kersplody</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39890986">prev</a><span>|</span><a href="#39893075">next</a><span>|</span><label class="collapse" for="c-39890833">[-]</label><label class="expand" for="c-39890833">[1 more]</label></div><br/><div class="children"><div class="content">Local LLMs are also a fantastic too for creative endeavors. Without prompt injection and having the ability to modify the amount of noise and &quot;creativity&quot; in the output, absolutely bonkers things pop out.</div><br/></div></div><div id="39893075" class="c"><input type="checkbox" id="c-39893075" checked=""/><div class="controls bullet"><span class="by">jonnycomputer</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39890833">prev</a><span>|</span><a href="#39896958">next</a><span>|</span><label class="collapse" for="c-39893075">[-]</label><label class="expand" for="c-39893075">[1 more]</label></div><br/><div class="children"><div class="content">They are not so bad as you are making out, tbh.<p>And privacy is a good enough reason to use local LLMs over commercial ones.</div><br/></div></div><div id="39896958" class="c"><input type="checkbox" id="c-39896958" checked=""/><div class="controls bullet"><span class="by">hylaride</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39893075">prev</a><span>|</span><a href="#39901301">next</a><span>|</span><label class="collapse" for="c-39896958">[-]</label><label class="expand" for="c-39896958">[1 more]</label></div><br/><div class="children"><div class="content"><i>The ones you can run on your own machine tend to be bad - really bad. They hallucinate wildly and fail at all sorts of tasks that the larger hosted ones succeed at.</i><p>Totally. I recently asked a locally-run &quot;speed&quot; LLM for the best restaurants in my (major) city, but it spit out restaurants opened by chefs from said city in other cities.  It&#x27;s not a thing you&#x27;d want to rely on for important work, but is still quite something.</div><br/></div></div><div id="39901301" class="c"><input type="checkbox" id="c-39901301" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39896958">prev</a><span>|</span><a href="#39900177">next</a><span>|</span><label class="collapse" for="c-39901301">[-]</label><label class="expand" for="c-39901301">[1 more]</label></div><br/><div class="children"><div class="content">Who cares, a local LLM still knows way way more practical knowledge than you, and without internet would provide a ton of useful information. Not surprised by this typical techy attitude - something has to be &#x27;perfect&#x27; to be useful.</div><br/></div></div><div id="39900177" class="c"><input type="checkbox" id="c-39900177" checked=""/><div class="controls bullet"><span class="by">barfingclouds</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39901301">prev</a><span>|</span><a href="#39896176">next</a><span>|</span><label class="collapse" for="c-39900177">[-]</label><label class="expand" for="c-39900177">[1 more]</label></div><br/><div class="children"><div class="content">Why not just interact with a virtual one that’s equally weak? You get all the same benefits</div><br/></div></div><div id="39896176" class="c"><input type="checkbox" id="c-39896176" checked=""/><div class="controls bullet"><span class="by">gfodor</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39900177">prev</a><span>|</span><a href="#39894921">next</a><span>|</span><label class="collapse" for="c-39896176">[-]</label><label class="expand" for="c-39896176">[1 more]</label></div><br/><div class="children"><div class="content">I mean kinda. But there&#x27;s a good chance this is also misleading. Lots of people have been fooled into thinking LLMs are inherently stupid because they have had bad experiences with GPT-3.5. The whole point is that the mistakes they make and even more fundamentally <i>what they&#x27;re doing</i> changes as you scale them up.</div><br/></div></div><div id="39894921" class="c"><input type="checkbox" id="c-39894921" checked=""/><div class="controls bullet"><span class="by">gardenhedge</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39896176">prev</a><span>|</span><a href="#39890615">next</a><span>|</span><label class="collapse" for="c-39894921">[-]</label><label class="expand" for="c-39894921">[1 more]</label></div><br/><div class="children"><div class="content">You can just chat to ChatGPT for awhile about something you know about and you&#x27;ll learn that.</div><br/></div></div></div></div><div id="39890615" class="c"><input type="checkbox" id="c-39890615" checked=""/><div class="controls bullet"><span class="by">gpm</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890656">prev</a><span>|</span><a href="#39890470">next</a><span>|</span><label class="collapse" for="c-39890615">[-]</label><label class="expand" for="c-39890615">[10 more]</label></div><br/><div class="children"><div class="content">If you want to download a backup of a large chunk of human knowledge... download wikipedia. It&#x27;s a similar size to a small LLM and can actually distinguish between real life and fantasy: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wikipedia:Database_download" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wikipedia:Database_download</a><p>If you just want to play around with an LLM though, absolutely.</div><br/><div id="39891447" class="c"><input type="checkbox" id="c-39891447" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890615">parent</a><span>|</span><a href="#39894304">next</a><span>|</span><label class="collapse" for="c-39891447">[-]</label><label class="expand" for="c-39891447">[6 more]</label></div><br/><div class="children"><div class="content">Kiwix provides prepackaged highly compressed archives of Wikipedia, Project Gutenberg, and many other useful things: <a href="https:&#x2F;&#x2F;download.kiwix.org&#x2F;zim&#x2F;" rel="nofollow">https:&#x2F;&#x2F;download.kiwix.org&#x2F;zim&#x2F;</a>.<p>Between that and dirt cheap storage prices, it is possible to have a local, offline copy of more human knowledge than one can sensibly consume in a lifetime. Hell, it&#x27;s possible to have it all on one&#x27;s <i>smartphone</i> (just get one with an SD card slot and shove a 1+ Tb one in there).</div><br/><div id="39893498" class="c"><input type="checkbox" id="c-39893498" checked=""/><div class="controls bullet"><span class="by">claritise</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891447">parent</a><span>|</span><a href="#39895040">next</a><span>|</span><label class="collapse" for="c-39893498">[-]</label><label class="expand" for="c-39893498">[4 more]</label></div><br/><div class="children"><div class="content">Just create a RAG with wikipedia as the corpus and a low parameter model to run it and you can basically have an instantly queryable corpus of human knowledge runnable on an old raspberry pi.</div><br/><div id="39894325" class="c"><input type="checkbox" id="c-39894325" checked=""/><div class="controls bullet"><span class="by">CaptainOfCoit</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39893498">parent</a><span>|</span><a href="#39894423">next</a><span>|</span><label class="collapse" for="c-39894325">[-]</label><label class="expand" for="c-39894325">[1 more]</label></div><br/><div class="children"><div class="content">&gt; a low parameter model<p>&gt; on an old raspberry pi<p>I bet the LLM responses will be great... You&#x27;re better off just opening up a raw text dump of Wikipedia markup files in vim.</div><br/></div></div><div id="39894423" class="c"><input type="checkbox" id="c-39894423" checked=""/><div class="controls bullet"><span class="by">boywitharupee</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39893498">parent</a><span>|</span><a href="#39894325">prev</a><span>|</span><a href="#39895040">next</a><span>|</span><label class="collapse" for="c-39894423">[-]</label><label class="expand" for="c-39894423">[2 more]</label></div><br/><div class="children"><div class="content">but which model to tokenize with? is there a leaderboard for models that are good for RAG?</div><br/><div id="39896727" class="c"><input type="checkbox" id="c-39896727" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39894423">parent</a><span>|</span><a href="#39895040">next</a><span>|</span><label class="collapse" for="c-39896727">[-]</label><label class="expand" for="c-39896727">[1 more]</label></div><br/><div class="children"><div class="content">“For RAG” is ambiguous.<p>First there is a leaderboard for embeddings. [1]<p>Even then, it depends how you use them. Some embeddings pack the highest signal in the beginning so you can truncate the vector, while most can not. You might want that truncated version for a fast dirty index. Same with using multiple models of differing vector sizes for the same content.<p>Do you preprocess your text? There will be a model there. Likely the same model you would use to process the query.<p>There is a model for asking questions from context. Sometimes that is a different model. [2]</div><br/></div></div></div></div></div></div><div id="39895040" class="c"><input type="checkbox" id="c-39895040" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891447">parent</a><span>|</span><a href="#39893498">prev</a><span>|</span><a href="#39894304">next</a><span>|</span><label class="collapse" for="c-39895040">[-]</label><label class="expand" for="c-39895040">[1 more]</label></div><br/><div class="children"><div class="content">Pretty neat to have laying around, thanks</div><br/></div></div></div></div><div id="39894304" class="c"><input type="checkbox" id="c-39894304" checked=""/><div class="controls bullet"><span class="by">CaptainOfCoit</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890615">parent</a><span>|</span><a href="#39891447">prev</a><span>|</span><a href="#39890470">next</a><span>|</span><label class="collapse" for="c-39894304">[-]</label><label class="expand" for="c-39894304">[3 more]</label></div><br/><div class="children"><div class="content">&gt; actually distinguish between real life and fantasy<p>Are LLMs unable to distinguish between real life and fantasy? What prompts have you thrown at them to make this determination? Sending a small fairy tale and asking the LLM if it thinks it&#x27;s a real story or fake one?</div><br/><div id="39894497" class="c"><input type="checkbox" id="c-39894497" checked=""/><div class="controls bullet"><span class="by">gpm</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39894304">parent</a><span>|</span><a href="#39890470">next</a><span>|</span><label class="collapse" for="c-39894497">[-]</label><label class="expand" for="c-39894497">[2 more]</label></div><br/><div class="children"><div class="content">... having them talk about events from sci fi stories in response to questions about the real world. Having them confidently lie about pretty much everything. Etc.</div><br/><div id="39894731" class="c"><input type="checkbox" id="c-39894731" checked=""/><div class="controls bullet"><span class="by">CaptainOfCoit</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39894497">parent</a><span>|</span><a href="#39890470">next</a><span>|</span><label class="collapse" for="c-39894731">[-]</label><label class="expand" for="c-39894731">[1 more]</label></div><br/><div class="children"><div class="content">What are the specific prompts you&#x27;re using? You might get those answers when you&#x27;re not being specific enough (or use models that aren&#x27;t state of the art).<p>&quot;Shit in, shit out&quot; as the saying goes, but applied to conversations with LLMs where the prompts often aren&#x27;t prescriptive enough.</div><br/></div></div></div></div></div></div></div></div><div id="39890470" class="c"><input type="checkbox" id="c-39890470" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890615">prev</a><span>|</span><a href="#39892548">next</a><span>|</span><label class="collapse" for="c-39890470">[-]</label><label class="expand" for="c-39890470">[34 more]</label></div><br/><div class="children"><div class="content">I contend that most human knowledge is not written down or if it is written down it’s not publicly available on the internet and so does not exist in these datasets.<p>There’s so much subtle knowledge like the way a mother learns to calm her child or the way a carpenter learns to work different kinds of wood which may be written down in part, but may also be learned through lived experience or transferred from human to human such that little of it gets written down and posted online.</div><br/><div id="39891101" class="c"><input type="checkbox" id="c-39891101" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39890527">next</a><span>|</span><label class="collapse" for="c-39891101">[-]</label><label class="expand" for="c-39891101">[7 more]</label></div><br/><div class="children"><div class="content">That&#x27;s where humans suck. The classic &quot;you&#x27;re not doing it right&quot; then proceeds to quickly show how to do it without verbalizing any info on learning process, pitfalls, failure modes, etc, as if just showing it was enough for themselves to learn. Most people do[n&#x27;t do] that, not even a sign of reflection.<p>My worst case was with a guy who asked me to write an arbitrage betting bot. When I asked how to calculate coeffs, he pointed at two values and said &quot;look, there &lt;x&gt;, there &lt;y&gt; <i>thinks for a minute</i> then it&#x27;s &lt;z&gt;!&quot;. When I asked how exactly did he calculate it, he simply repeated with different numbers.</div><br/><div id="39891439" class="c"><input type="checkbox" id="c-39891439" checked=""/><div class="controls bullet"><span class="by">Aerroon</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891101">parent</a><span>|</span><a href="#39891388">next</a><span>|</span><label class="collapse" for="c-39891439">[-]</label><label class="expand" for="c-39891439">[3 more]</label></div><br/><div class="children"><div class="content">People often don&#x27;t know how to verbalize them in the first place. Some of these topics are very complex, but our intuition gets us halfway there.<p>Once upon a time I was good at a video game. Everyone realized that positioning is extremely important in this game.<p>I have good positioning in that game and was asked many times to make a guide about positioning. I never did, because I don&#x27;t really know how. There is too much information they you need to convey to cover all the various situations.<p>I think you would first have to come up with a framework on positioning to be able to really teach this to someone else. Some kind of base truths&#x2F;patterns that you can then use to convey the meaning. I believe the same thing applies to a lot of these processes that aren&#x27;t verbalized.</div><br/><div id="39892897" class="c"><input type="checkbox" id="c-39892897" checked=""/><div class="controls bullet"><span class="by">snovv_crash</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891439">parent</a><span>|</span><a href="#39891388">next</a><span>|</span><label class="collapse" for="c-39892897">[-]</label><label class="expand" for="c-39892897">[2 more]</label></div><br/><div class="children"><div class="content">Often for this kind of problem writing a closed form solution is simply intractable. However, it&#x27;s often still possible to express the cost function of at least a big portion of what goes into a human-optimal solution. From here you can sample your space, do gradient descent or whatever to find some acceptable solution that has a more human-intuitive property.</div><br/><div id="39893285" class="c"><input type="checkbox" id="c-39893285" checked=""/><div class="controls bullet"><span class="by">michaelt</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39892897">parent</a><span>|</span><a href="#39891388">next</a><span>|</span><label class="collapse" for="c-39893285">[-]</label><label class="expand" for="c-39893285">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not necessarily that it&#x27;s intractable - just that a thing can be very hard to describe, under some circumstances.<p>Imagine someone learning English has written &quot;The experiment reached it&#x27;s conclusion&quot; and you have to correct their grammar. Almost any english speaker can correct &quot;it&#x27;s&quot; to &quot;its&quot; but unless they (and the person they&#x27;re correcting) know a bunch of terms like &#x27;noun&#x27; and &#x27;pronoun&#x27; and &#x27;possessive&#x27; they&#x27;ll have a very hard time explaining why.</div><br/></div></div></div></div></div></div><div id="39891388" class="c"><input type="checkbox" id="c-39891388" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891101">parent</a><span>|</span><a href="#39891439">prev</a><span>|</span><a href="#39894379">next</a><span>|</span><label class="collapse" for="c-39891388">[-]</label><label class="expand" for="c-39891388">[2 more]</label></div><br/><div class="children"><div class="content">&gt; When I asked how exactly did he calculate it, he simply repeated with different numbers.<p>Now you know how an LLM feels during training!</div><br/><div id="39892114" class="c"><input type="checkbox" id="c-39892114" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891388">parent</a><span>|</span><a href="#39894379">next</a><span>|</span><label class="collapse" for="c-39892114">[-]</label><label class="expand" for="c-39892114">[1 more]</label></div><br/><div class="children"><div class="content">Probably during inference, as well.</div><br/></div></div></div></div><div id="39894379" class="c"><input type="checkbox" id="c-39894379" checked=""/><div class="controls bullet"><span class="by">Shorel</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891101">parent</a><span>|</span><a href="#39891388">prev</a><span>|</span><a href="#39890527">next</a><span>|</span><label class="collapse" for="c-39894379">[-]</label><label class="expand" for="c-39894379">[1 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t say this is where humans suck. On the contrary, this how we find human language is such a fantastic tool to serialize and deserialize human mental processes.<p>Language is so good, that an artificial language tool, without any understanding of these mental processes, can appear semi-intelligent to us.<p>A few people unable to do this serialization doesn&#x27;t mean much on the larger scale. Just that their ideas and mental processes will be forgotten.</div><br/></div></div></div></div><div id="39890527" class="c"><input type="checkbox" id="c-39890527" checked=""/><div class="controls bullet"><span class="by">spacephysics</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39891101">prev</a><span>|</span><a href="#39890542">next</a><span>|</span><label class="collapse" for="c-39890527">[-]</label><label class="expand" for="c-39890527">[5 more]</label></div><br/><div class="children"><div class="content">For sure agree, however as the storage of information evolves, it’s becoming more efficient over time<p>From oral tradition to tablets to scrolls to books to mass produced books to digital and now these LLMs, I think it’s still a good idea to preserve what we have the best we can. Not as a replacement, but a hedge against a potential library of Alexandria incident.<p>I could imagine a time in the near future where the models are domain-specific, and just like there are trusted encyclopedia publishers there are trusted model publishers that guarantee a certain level of accuracy.<p>It’s not like reading a book, but I for sure had an easier time learning golang talking with ChatGPT than a book</div><br/><div id="39891219" class="c"><input type="checkbox" id="c-39891219" checked=""/><div class="controls bullet"><span class="by">nyokodo</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890527">parent</a><span>|</span><a href="#39890542">next</a><span>|</span><label class="collapse" for="c-39891219">[-]</label><label class="expand" for="c-39891219">[4 more]</label></div><br/><div class="children"><div class="content">&gt; a hedge against a potential library of Alexandria incident<p>What would cause a Library of Alexandria incident wiping out all human knowledge elsewhere, that would also allow you to run a local LLM?</div><br/><div id="39892865" class="c"><input type="checkbox" id="c-39892865" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891219">parent</a><span>|</span><a href="#39897278">next</a><span>|</span><label class="collapse" for="c-39892865">[-]</label><label class="expand" for="c-39892865">[1 more]</label></div><br/><div class="children"><div class="content">To run a local LLM you need the device it currently runs on and electricity. There are actually quite a lot of ways to generate electricity, but to name one, a diesel generator that can run on vegetable oil.<p>What you&#x27;re really asking is, what could cause a modern Library of Alexandria incident? But the fact is we keep the only copy of too many things on the servers of the major cloud providers. Which are then intended to have their own internal redundancy, but that doesn&#x27;t protect you against a targeted attack or a systemic failure when all the copies are under the same roof and you lose every redundant copy at once from a single mistake replicated in a monoculture.</div><br/></div></div><div id="39897278" class="c"><input type="checkbox" id="c-39897278" checked=""/><div class="controls bullet"><span class="by">spacephysics</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891219">parent</a><span>|</span><a href="#39892865">prev</a><span>|</span><a href="#39892578">next</a><span>|</span><label class="collapse" for="c-39897278">[-]</label><label class="expand" for="c-39897278">[1 more]</label></div><br/><div class="children"><div class="content">A more dooms-day prepping would call for some heavy lead-faraday cage to store the storage mediums in the event of an EMP&#x2F;major solar flare.<p>Or more Sci-fi related, some hyper computer virus that ends up infecting all internet connected devices.<p>Not too far fetched if we can conceive of some AI enabled worm that mutates depending on the target, I could imagine a model of sorts being feasible within the next 5-10 years</div><br/></div></div></div></div></div></div><div id="39890542" class="c"><input type="checkbox" id="c-39890542" checked=""/><div class="controls bullet"><span class="by">_ache_</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39890527">prev</a><span>|</span><a href="#39894821">next</a><span>|</span><label class="collapse" for="c-39890542">[-]</label><label class="expand" for="c-39890542">[2 more]</label></div><br/><div class="children"><div class="content">I think you underestimate the amount of information contained in books and the extent to which our society (as a whole) depends on them.</div><br/><div id="39891546" class="c"><input type="checkbox" id="c-39891546" checked=""/><div class="controls bullet"><span class="by">Barrin92</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890542">parent</a><span>|</span><a href="#39894821">next</a><span>|</span><label class="collapse" for="c-39891546">[-]</label><label class="expand" for="c-39891546">[1 more]</label></div><br/><div class="children"><div class="content">society depends much more on social networks, mentorship and tacit knowledge than books. It&#x27;s easy to test this. Just run the thought experiment by a few people, if you could get only one, would you take an Ivy league degree without the education or the education without the degree?<p>Venture capital in tech is a good example of this. The book knowledge is effectively globally distributed and almost free, effectively success happens in a few geographically concentrated counties.</div><br/></div></div></div></div><div id="39894821" class="c"><input type="checkbox" id="c-39894821" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39890542">prev</a><span>|</span><a href="#39890794">next</a><span>|</span><label class="collapse" for="c-39894821">[-]</label><label class="expand" for="c-39894821">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I contend that most human knowledge is not written down<p>Yes - the available training data is essentially mostly a combination of declarative knowledge (facts - including human-generated artifacts) and procedural knowledge (how to do things). What is missing is the learning process of taking a description of how to do something, and trying to apply that yourself in a specific situation.<p>No amount of reading books, or reading other people&#x27;s blogs on how they did something, can avoid the need for hands-on experience if you want to learn how to do it yourself.<p>It&#x27;s not just a matter of information that might be missing or unclear in instructional material, including how to cope with every type of failure and unexpected outcome, but crucially how to do this <i>yourself</i> - if you are to be the actor, then it&#x27;s the predictive process in <i>your</i> mind that matters.<p>Partly for this reason, and partly because current AI&#x27;s (transformer-based LLMs) don&#x27;t support online learning (try &amp; fail skill acquisition), I think we&#x27;re going to see two distinct phases of AI.<p>1) The current &quot;GenAI&quot; phase where AI can only produce mash-ups of things it saw in it&#x27;s pre-training data, augmented by similar &quot;book learning&quot; provided in-context which can be utilized by in-context learning. I&#x27;d characterize what this type of AI to be useful for, and capable of, as &quot;automation&quot;. Applying that book (incl. anecdotal) knowledge to new situations where mash-up is all you need.<p>2) The second phase is where we have something closer to AGI, even if still below human level, which is no longer just a pre-trained transformer, but also has online learning and is agentic - taking actions predicated on innate traits like curiosity and boredom, so that given the book knowledge it can (&amp; will!) then learn to apply that by experimentation&#x2F;practice and learning from its own mistakes.<p>There will no doubt be advances beyond this &quot;phase two&quot; as well, but it seems we&#x27;re likely to be stuck at &quot;phase one&quot; for a while (even as models become much better at phase one capabilities), until architectures fundamentally advance beyond transformers to allow this type of on-the-job training and skill acquisition.</div><br/></div></div><div id="39890794" class="c"><input type="checkbox" id="c-39890794" checked=""/><div class="controls bullet"><span class="by">nicklecompte</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39894821">prev</a><span>|</span><a href="#39890588">next</a><span>|</span><label class="collapse" for="c-39890794">[-]</label><label class="expand" for="c-39890794">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not even &quot;human knowledge&quot; that can&#x27;t be written down - it seems all vertebrates understand causality, quantity (in the sense of intuitively understanding what numbers are), and object permanence. Good luck writing those concepts down in a way that GPT can use!<p>In general AI in 2024 is not even close to understanding these ideas, nor does any AI developer have a clue how to build an AI with this understanding. The best we can do is imitating object permanence for a small subset of perceptible objects, a limitation not found in dogs or spiders.</div><br/></div></div><div id="39890588" class="c"><input type="checkbox" id="c-39890588" checked=""/><div class="controls bullet"><span class="by">skeledrew</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39890794">prev</a><span>|</span><a href="#39890652">next</a><span>|</span><label class="collapse" for="c-39890588">[-]</label><label class="expand" for="c-39890588">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d content that those are skills (gained through experience) rather than knowledge (gained through rote learning).</div><br/><div id="39891765" class="c"><input type="checkbox" id="c-39891765" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890588">parent</a><span>|</span><a href="#39890652">next</a><span>|</span><label class="collapse" for="c-39891765">[-]</label><label class="expand" for="c-39891765">[1 more]</label></div><br/><div class="children"><div class="content">I think it’s worth expanding your definition of knowledge.</div><br/></div></div></div></div><div id="39890652" class="c"><input type="checkbox" id="c-39890652" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39890588">prev</a><span>|</span><a href="#39890519">next</a><span>|</span><label class="collapse" for="c-39890652">[-]</label><label class="expand" for="c-39890652">[1 more]</label></div><br/><div class="children"><div class="content">Yes but it contains enough hints to help someone find their way on the these types of tasks.</div><br/></div></div><div id="39890519" class="c"><input type="checkbox" id="c-39890519" checked=""/><div class="controls bullet"><span class="by">mickdarling</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39890652">prev</a><span>|</span><a href="#39892548">next</a><span>|</span><label class="collapse" for="c-39890519">[-]</label><label class="expand" for="c-39890519">[14 more]</label></div><br/><div class="children"><div class="content">Wait till all the videos ever created are tokenized and ingested into a training dataset. Carpentry techniques are certainly there.  The subtleties of parenting maybe harder to derive from that, but maybe lots of little snippets of people’s lives will add up to a general understanding of parenting. There have certainly been bigger surprises in the field.</div><br/><div id="39890555" class="c"><input type="checkbox" id="c-39890555" checked=""/><div class="controls bullet"><span class="by">oblio</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890519">parent</a><span>|</span><a href="#39891292">next</a><span>|</span><label class="collapse" for="c-39890555">[-]</label><label class="expand" for="c-39890555">[12 more]</label></div><br/><div class="children"><div class="content">What about smells or tastes? Or feelings?<p>I can&#x27;t help but feel we&#x27;re at the &quot;aliens watch people eat from space and recreate chemically identical food that has no taste&quot; phase of AI development.</div><br/><div id="39890617" class="c"><input type="checkbox" id="c-39890617" checked=""/><div class="controls bullet"><span class="by">skeledrew</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890555">parent</a><span>|</span><a href="#39890618">next</a><span>|</span><label class="collapse" for="c-39890617">[-]</label><label class="expand" for="c-39890617">[7 more]</label></div><br/><div class="children"><div class="content">If the food is chemically identical then the taste would be the same though, since taste (and smell) is about chemistry. I do get what you&#x27;re saying though.</div><br/><div id="39898153" class="c"><input type="checkbox" id="c-39898153" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890617">parent</a><span>|</span><a href="#39891233">next</a><span>|</span><label class="collapse" for="c-39898153">[-]</label><label class="expand" for="c-39898153">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Knowledge_argument" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Knowledge_argument</a></div><br/><div id="39901457" class="c"><input type="checkbox" id="c-39901457" checked=""/><div class="controls bullet"><span class="by">skeledrew</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39898153">parent</a><span>|</span><a href="#39891233">next</a><span>|</span><label class="collapse" for="c-39901457">[-]</label><label class="expand" for="c-39901457">[1 more]</label></div><br/><div class="children"><div class="content">An interesting thought experiment, but there&#x27;s a flaw in it, an implicit fallacy that&#x27;s probably a straw man. On its own, the argument would likely stand that Mary gains new knowledge on actually being exposed to color.<p>However, there is a broader context: this is supporting an argument against physicalism, and in this light it falls apart. There are a couple of missing bits required to complete the experiment in this context. The understanding that knowledge comes in 2 varieties: direct (actual experience) and indirect (description by one with the actual experience using shared language). This understanding brings proper clarity to the original argument, as we are aware - I think - that language is used to create compressed representations of things; something like a perceptual hash function.<p>The other key bit, which I guess we&#x27;ve only considered and extensively explored after the argument was formulated, is that all information coming in via the senses goes to the brain as electrical signals. And we actually have experimental data showing that sensory information can be emulated using machines. Thus, the original argument, to be relevant to the context, should be completed by giving Mary access to a machine that she can program to emulate the electrical signals that represent color experience.<p>I posit that without access to that hypothetical machine, given the context of the experiment, it cannot be said that Mary has &quot;learned everything there is to learn about color&quot;. And once she has comprehensively and correctly utilized said machine on herself, she will gain no new knowledge when she is exposed to the world of color. Therefore this experiment cannot be used as an argument against physicalism as originally intended.</div><br/></div></div></div></div><div id="39891233" class="c"><input type="checkbox" id="c-39891233" checked=""/><div class="controls bullet"><span class="by">nyokodo</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890617">parent</a><span>|</span><a href="#39898153">prev</a><span>|</span><a href="#39891457">next</a><span>|</span><label class="collapse" for="c-39891233">[-]</label><label class="expand" for="c-39891233">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If the food is chemically identical…<p>If it were 99.9% chemically identical but they left out the salt and spices…</div><br/><div id="39898732" class="c"><input type="checkbox" id="c-39898732" checked=""/><div class="controls bullet"><span class="by">skeledrew</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891233">parent</a><span>|</span><a href="#39891457">next</a><span>|</span><label class="collapse" for="c-39898732">[-]</label><label class="expand" for="c-39898732">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say that, when it comes to chemistry, only 100% reproduction can be considered identical. Anything less is to be deemed similar to some degree.<p>And so without the correct amount of salt and&#x2F;or spices, we&#x27;re talking about food that&#x27;s very similar, and not identical.</div><br/></div></div></div></div><div id="39891457" class="c"><input type="checkbox" id="c-39891457" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890617">parent</a><span>|</span><a href="#39891233">prev</a><span>|</span><a href="#39890618">next</a><span>|</span><label class="collapse" for="c-39891457">[-]</label><label class="expand" for="c-39891457">[2 more]</label></div><br/><div class="children"><div class="content">Their perception is very likely to be totally different.<p>* They might not perceive some substances at all, others that we don&#x27;t notice might make it unpalatable.<p>* Some substances might be perceived differently than us, or be indistinguishable from others.<p>* And some might require getting used to.<p>Note that all of the above phenomena also occur in humans because of genetics, cultural background, or experiences!</div><br/><div id="39898979" class="c"><input type="checkbox" id="c-39898979" checked=""/><div class="controls bullet"><span class="by">skeledrew</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891457">parent</a><span>|</span><a href="#39890618">next</a><span>|</span><label class="collapse" for="c-39898979">[-]</label><label class="expand" for="c-39898979">[1 more]</label></div><br/><div class="children"><div class="content">This may come off as pedantic, but &quot;identical&quot; is a very strong term when it comes to something like chemistry. The smallest chemical difference can manifest as a large physical difference. Consider that genetically, humans are about 60% similar to the fruit fly, yet phenotically, the similarity could be considered under 1%.</div><br/></div></div></div></div></div></div><div id="39890618" class="c"><input type="checkbox" id="c-39890618" checked=""/><div class="controls bullet"><span class="by">mickdarling</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890555">parent</a><span>|</span><a href="#39890617">prev</a><span>|</span><a href="#39892313">next</a><span>|</span><label class="collapse" for="c-39890618">[-]</label><label class="expand" for="c-39890618">[3 more]</label></div><br/><div class="children"><div class="content">Well, I have synesthetic smell&#x2F;color senses, so I don’t even know what other humans experience, nor they me. But, I have described it in detail to many people and they seem to get the idea, and can even predict how certain smells will “look” to me.  All that took was using words to describe things.</div><br/><div id="39891229" class="c"><input type="checkbox" id="c-39891229" checked=""/><div class="controls bullet"><span class="by">nyokodo</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890618">parent</a><span>|</span><a href="#39892313">next</a><span>|</span><label class="collapse" for="c-39891229">[-]</label><label class="expand" for="c-39891229">[2 more]</label></div><br/><div class="children"><div class="content">&gt; All that took was using words to describe things.<p>All that took was words and a shared experience of smelling.</div><br/><div id="39896352" class="c"><input type="checkbox" id="c-39896352" checked=""/><div class="controls bullet"><span class="by">mickdarling</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891229">parent</a><span>|</span><a href="#39892313">next</a><span>|</span><label class="collapse" for="c-39896352">[-]</label><label class="expand" for="c-39896352">[1 more]</label></div><br/><div class="children"><div class="content">How rude, what do our bathing habits have to do with this? ;-)<p>But, fair point. The gist I was trying to get across is that I don&#x27;t even know what a plant smells like to you, and you don&#x27;t know what a plant smells like to me. Those aren&#x27;t comparable with any objective data. We make guesses, and we try to get close with our descriptions, which are in words. That&#x27;s the best we can do and we share our senses. Asking more from computers seems overly picky to me.</div><br/></div></div></div></div></div></div><div id="39892313" class="c"><input type="checkbox" id="c-39892313" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890555">parent</a><span>|</span><a href="#39890618">prev</a><span>|</span><a href="#39891292">next</a><span>|</span><label class="collapse" for="c-39892313">[-]</label><label class="expand" for="c-39892313">[1 more]</label></div><br/><div class="children"><div class="content">I think we can safely say that any taste, smell, sensation or emotion of any importance has been described 1000 times over in the text corpus of GPT. Even though it is fragmented, by sheer volume there is enough signal in the training set, otherwise it would not be able to generate coherent text. In this case I think the map (language) is asymptotically close to the territory (sensations &amp; experience in general).</div><br/></div></div></div></div><div id="39891292" class="c"><input type="checkbox" id="c-39891292" checked=""/><div class="controls bullet"><span class="by">andersa</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890519">parent</a><span>|</span><a href="#39890555">prev</a><span>|</span><a href="#39892548">next</a><span>|</span><label class="collapse" for="c-39891292">[-]</label><label class="expand" for="c-39891292">[1 more]</label></div><br/><div class="children"><div class="content">What makes you think they aren&#x27;t already?</div><br/></div></div></div></div></div></div><div id="39892548" class="c"><input type="checkbox" id="c-39892548" checked=""/><div class="controls bullet"><span class="by">kalleboo</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890470">prev</a><span>|</span><a href="#39890818">next</a><span>|</span><label class="collapse" for="c-39892548">[-]</label><label class="expand" for="c-39892548">[2 more]</label></div><br/><div class="children"><div class="content">I had downloaded some LLMs to run locally just to experiment when a freak hailstorm suddenly left me without internet for over a week. It was really interesting to use a local LLM as a replacement for Google.<p>It gave me a new mental model for LLMs rather than a &quot;spicy autocomplete&quot; or whatever, I now think of it as &quot;a lossy compressed database of knowledge&quot;. Like you ran the internet through JPEG at 30% quality.</div><br/><div id="39894791" class="c"><input type="checkbox" id="c-39894791" checked=""/><div class="controls bullet"><span class="by">pizzafeelsright</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39892548">parent</a><span>|</span><a href="#39890818">next</a><span>|</span><label class="collapse" for="c-39894791">[-]</label><label class="expand" for="c-39894791">[1 more]</label></div><br/><div class="children"><div class="content">Feels like that really smart friend who is probably correct but ya just don&#x27;t know.</div><br/></div></div></div></div><div id="39890818" class="c"><input type="checkbox" id="c-39890818" checked=""/><div class="controls bullet"><span class="by">creatonez</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39892548">prev</a><span>|</span><a href="#39899411">next</a><span>|</span><label class="collapse" for="c-39890818">[-]</label><label class="expand" for="c-39890818">[6 more]</label></div><br/><div class="children"><div class="content">Maybe I&#x27;m seeing things through a modern lens, but if I were trying to restart civilization and was <i>only</i> left with ChatGPT, I would be enraged and very much not grateful for this.</div><br/><div id="39891251" class="c"><input type="checkbox" id="c-39891251" checked=""/><div class="controls bullet"><span class="by">nyokodo</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890818">parent</a><span>|</span><a href="#39891955">next</a><span>|</span><label class="collapse" for="c-39891251">[-]</label><label class="expand" for="c-39891251">[3 more]</label></div><br/><div class="children"><div class="content">&gt; if I were trying to restart civilization and was only left with ChatGPT<p>In this scenario you’d need to also be left with a big chunk of compute, and power infrastructure. Since ChatGPT is the front end of the model you’d also need to have the internet still going in a minimum capacity.</div><br/><div id="39894754" class="c"><input type="checkbox" id="c-39894754" checked=""/><div class="controls bullet"><span class="by">CaptainOfCoit</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891251">parent</a><span>|</span><a href="#39891888">next</a><span>|</span><label class="collapse" for="c-39894754">[-]</label><label class="expand" for="c-39894754">[1 more]</label></div><br/><div class="children"><div class="content">If we&#x27;re playing this game, you forgot to mention that they also need: A monitor, a keyboard, roof over their head (to prevent rain from entering your electronic), etc etc...<p>But really, didn&#x27;t you catch the meaning of parents message, or are you being purposefully obtuse?</div><br/></div></div></div></div><div id="39891955" class="c"><input type="checkbox" id="c-39891955" checked=""/><div class="controls bullet"><span class="by">devsda</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890818">parent</a><span>|</span><a href="#39891251">prev</a><span>|</span><a href="#39899411">next</a><span>|</span><label class="collapse" for="c-39891955">[-]</label><label class="expand" for="c-39891955">[2 more]</label></div><br/><div class="children"><div class="content">I think re-imagining the &quot;Dr. Stone&quot; series with the main character replaced by an LLM will be a funny &amp; interesting series if we decide to stay true to LLMs nature and make it hallucinate as well.<p>Given the way LLMs are right now, I suspect there will be lot of failed experiments and the kingdom of science will not advance that quick.</div><br/><div id="39893233" class="c"><input type="checkbox" id="c-39893233" checked=""/><div class="controls bullet"><span class="by">latexr</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891955">parent</a><span>|</span><a href="#39899411">next</a><span>|</span><label class="collapse" for="c-39893233">[-]</label><label class="expand" for="c-39893233">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the kingdom of science will not advance that quick.<p>It’s more likely that it wouldn’t even start. The first step to any development was figuring out nitric acid as the cure to the petrification. Good luck getting any LLM to figure that out. Even if it did, good luck getting any of the other characters to know what to do with that information that early on.</div><br/></div></div></div></div></div></div><div id="39899411" class="c"><input type="checkbox" id="c-39899411" checked=""/><div class="controls bullet"><span class="by">TrevorJ</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890818">prev</a><span>|</span><a href="#39890546">next</a><span>|</span><label class="collapse" for="c-39899411">[-]</label><label class="expand" for="c-39899411">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a very underrated side effect of this whole LLM thing: We&#x27;ve created a super compact representation of human knowledge in a form that requires a FAR less complex tech stack to get the information &#x27;out&#x27; of in the future.<p>A year ago, a lot of this information only existed on the internet, and would have been nearly impossible to recover in any cohesive unfragmented form if the lights were to ever go out on our civilization.<p>Now the problem space has moved simply to &quot;find a single solitary PC that will still boot up&quot;, and boom, you have access to everything.<p>I think we just created our Rosetta stone.</div><br/></div></div><div id="39890546" class="c"><input type="checkbox" id="c-39890546" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39899411">prev</a><span>|</span><a href="#39890739">next</a><span>|</span><label class="collapse" for="c-39890546">[-]</label><label class="expand" for="c-39890546">[19 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see LLMs as a large chunk of knowledge, I see them as an emergent alien intelligence snapshotted at the moment it appeared to stop learning. It&#x27;s further hobbled by the limited context window it has to use, and the probabilistic output structure that allows for outside random influences to pick its next word.<p>Both the context window and output structure are, in my opinion, massive impedance mismatches for the emergent intellect embedded in the weights of the model.<p>If there were a way to match the impedance, I strongly suspect we&#x27;d already have AGI on our hands.</div><br/><div id="39891739" class="c"><input type="checkbox" id="c-39891739" checked=""/><div class="controls bullet"><span class="by">mlsu</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890546">parent</a><span>|</span><a href="#39890657">next</a><span>|</span><label class="collapse" for="c-39891739">[-]</label><label class="expand" for="c-39891739">[1 more]</label></div><br/><div class="children"><div class="content">Disagree. The input&#x2F;output structure (tokens) is the interface for both inference <i>and</i> for training. There is an emergent intellect embedded in the weights of the model. However, it is <i>only</i> accessible through the autoregressive token interface.<p>This is a fundamental limitation, much more fundamental than appears at first. It means that the only way to touch the model, and for the model to touch the world, is through the tokenizer (also, btw, why tokenizer is so essential to model performance). Touching the world through a tokenizer is actually quite limited.<p>So there is an intelligence in there for sure, but it is locked in an ontology that is tied to its interface. This is even more of a limitation than e.g. weights being frozen.</div><br/></div></div><div id="39890657" class="c"><input type="checkbox" id="c-39890657" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890546">parent</a><span>|</span><a href="#39891739">prev</a><span>|</span><a href="#39890738">next</a><span>|</span><label class="collapse" for="c-39890657">[-]</label><label class="expand" for="c-39890657">[15 more]</label></div><br/><div class="children"><div class="content">What is alien about them ?<p>LLMs are of this earth and created by our species. Seems quite familiar to me.</div><br/><div id="39890965" class="c"><input type="checkbox" id="c-39890965" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890657">parent</a><span>|</span><a href="#39899970">next</a><span>|</span><label class="collapse" for="c-39890965">[-]</label><label class="expand" for="c-39890965">[7 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t think, they don&#x27;t reason, they don&#x27;t understand. Except they do. But it&#x27;s hard for human words for thought processes to apply when giving it an endless string of AAAAA&#x27;s makes it go bananas.<p>That&#x27;s not familiar behavior. Nor is the counting reddit derived output. It&#x27;s also not familiar for a single person to have the breadth and depth of knowledge that ChatGPT has. Sure, some people know more than others, but even without hitting the Internet, it has a ridiculous amount of knowledge, far surpassing a human, making it, to me, alien. though, it&#x27;s inability to do math sometimes is humanizing to me for some reason.<p>ChatGPT&#x27;s memory is also unhuman. It has a context window which is a thing, but also it only knows about things you&#x27;ve told it in each chat. Make a new chat and it&#x27;s totally forgotten the nickname you gave it.<p>I don&#x27;t think of HR Geiger&#x27;s work, though made by a human, as familiar to me. it feels quite alien to me, and it&#x27;s not just me,
either. Dali, Bosch, and Escher are other human artists who&#x27;s work can be unfamiliar and alien. So being created by our species doesn&#x27;t automatically imbue something with familiar human processes.<p>So it dot products, it matrix multiplies, instead of reasoning and understanding. It&#x27;s the Chinese room experiment on steroids; it turns out a sufficiently large corpus on a sufficiently large machine does make it look like something&quot;understands&quot;.</div><br/><div id="39891582" class="c"><input type="checkbox" id="c-39891582" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890965">parent</a><span>|</span><a href="#39891318">next</a><span>|</span><label class="collapse" for="c-39891582">[-]</label><label class="expand" for="c-39891582">[1 more]</label></div><br/><div class="children"><div class="content">The context window is comparable to human short-term memory. LLMs are missing episodic memory and means to migrate knowledge between the different layers and into its weights.<p>Math is mostly impeded by the tokenization, but it would still make more sense to adapt them to use RAG to process questions that are clearly calculations or chains of logical inference. With proper prompt engineering, they can process the latter though, and deviating from strictly logical reasoning is sometimes exactly what we want.<p>The ability to reset the text and to change that history is a powerful tool! It can make the model roleplay and even help circumvent alignment.<p>I think that LLMs could one day serve as the language center of an AGI.</div><br/></div></div><div id="39891318" class="c"><input type="checkbox" id="c-39891318" checked=""/><div class="controls bullet"><span class="by">trimethylpurine</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890965">parent</a><span>|</span><a href="#39891582">prev</a><span>|</span><a href="#39891684">next</a><span>|</span><label class="collapse" for="c-39891318">[-]</label><label class="expand" for="c-39891318">[2 more]</label></div><br/><div class="children"><div class="content">The word &quot;alien&quot; works in this context but, as the previous commenter mentioned, it also carries the implication of foreign origin. You could use &quot;uncanny&quot; instead. Maybe that&#x27;s less arbitrary and more specific to these examples.<p>&quot;Alien&quot; still works, but then you might have to add all the context at length, as you&#x27;ve done in this last comment.</div><br/><div id="39891423" class="c"><input type="checkbox" id="c-39891423" checked=""/><div class="controls bullet"><span class="by">fire_lake</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891318">parent</a><span>|</span><a href="#39891684">next</a><span>|</span><label class="collapse" for="c-39891423">[-]</label><label class="expand" for="c-39891423">[1 more]</label></div><br/><div class="children"><div class="content">Hype people do this all the time - take a word that has a particular meaning in a narrow context and move it to a broader context where people will give it a sexier meaning.<p><pre><code>    AI researchers unveil alien intelligence
</code></pre>
Is way better headline.</div><br/></div></div></div></div><div id="39891684" class="c"><input type="checkbox" id="c-39891684" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890965">parent</a><span>|</span><a href="#39891318">prev</a><span>|</span><a href="#39893336">next</a><span>|</span><label class="collapse" for="c-39891684">[-]</label><label class="expand" for="c-39891684">[2 more]</label></div><br/><div class="children"><div class="content">In all fairness, going up to SMS random human and yelling AAAAAAAAAAAAAA… at them for long enough will produce some out-of-distribution responses too.</div><br/><div id="39895016" class="c"><input type="checkbox" id="c-39895016" checked=""/><div class="controls bullet"><span class="by">cloudwalk9</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891684">parent</a><span>|</span><a href="#39893336">next</a><span>|</span><label class="collapse" for="c-39895016">[-]</label><label class="expand" for="c-39895016">[1 more]</label></div><br/><div class="children"><div class="content">Makes me think that TikTok and YT pranksters are accidentally producing psychological data on what makes people tick under scenarios of extreme deliberate annoyance. Although the quality (and importance) of that data is obviously highly variable and probably not very high, and depends on what the prank is.</div><br/></div></div></div></div><div id="39893336" class="c"><input type="checkbox" id="c-39893336" checked=""/><div class="controls bullet"><span class="by">inference-lord</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890965">parent</a><span>|</span><a href="#39891684">prev</a><span>|</span><a href="#39899970">next</a><span>|</span><label class="collapse" for="c-39893336">[-]</label><label class="expand" for="c-39893336">[1 more]</label></div><br/><div class="children"><div class="content">Do you find a large database or spreadsheet that hold more information than you can &quot;alien&quot; too?</div><br/></div></div></div></div><div id="39899970" class="c"><input type="checkbox" id="c-39899970" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890657">parent</a><span>|</span><a href="#39890965">prev</a><span>|</span><a href="#39890748">next</a><span>|</span><label class="collapse" for="c-39899970">[-]</label><label class="expand" for="c-39899970">[1 more]</label></div><br/><div class="children"><div class="content">Alien meaning unfamiliar, not necessarily extraterrestrial.<p>Aliens are people from other countries, for example.<p>Exotic would be another good word to use.</div><br/></div></div><div id="39890748" class="c"><input type="checkbox" id="c-39890748" checked=""/><div class="controls bullet"><span class="by">jfoster</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890657">parent</a><span>|</span><a href="#39899970">prev</a><span>|</span><a href="#39890738">next</a><span>|</span><label class="collapse" for="c-39890748">[-]</label><label class="expand" for="c-39890748">[6 more]</label></div><br/><div class="children"><div class="content">They can write in a way similar to how a human might write, but they&#x27;re not human.<p>The chat interfaces (Claude, ChatGPT) certainly have a particular style of writing, but the underlying LLMs are definitely capable of impersonating as our species in the medium of text.</div><br/><div id="39892771" class="c"><input type="checkbox" id="c-39892771" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890748">parent</a><span>|</span><a href="#39890964">prev</a><span>|</span><a href="#39890738">next</a><span>|</span><label class="collapse" for="c-39892771">[-]</label><label class="expand" for="c-39892771">[4 more]</label></div><br/><div class="children"><div class="content">But they&#x27;re extremely relatable to us because it&#x27;s regurgitating us.<p>I saw this talk with Geoffrey Hinton the other day and he said he was astonished at the capabilities of ChatGPT-4 because he asked it what the relationship between a compost heap and a nuclear bomb was, and he couldn&#x27;t believe it answered, he really thought it was proof the thing could reason. Totally mind blown.<p>However I got it right away with zero effort.<p>Either I&#x27;m a super genius or this has been discussed before and made it&#x27;s way into the training data.<p>Usual disclaimer: I don&#x27;t think this invalidates the usefulness of AI or LLMs, just that we might be bamboozling ourselves into the idea that we&#x27;ve created an alien intelligence.</div><br/><div id="39897039" class="c"><input type="checkbox" id="c-39897039" checked=""/><div class="controls bullet"><span class="by">EMM_386</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39892771">parent</a><span>|</span><a href="#39890738">next</a><span>|</span><label class="collapse" for="c-39897039">[-]</label><label class="expand" for="c-39897039">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Either I&#x27;m a super genius or this has been discussed before and made it&#x27;s way into the training data.<p>If an LLM can tell you the relatonship between a compost heap and nuclear bomb, that doesn&#x27;t mean that was in the training data.<p>It could be because a compost heap &quot;generates heat&quot;, and a nuclear bomb also &quot;generates heat&quot; and due to that relationship they have something in common.  The model will pick up on these similar patterns.  They tokens are positioned closer to each other in the high dimensional vector space.<p>But for any given &quot;what does x have in common with y&quot;, that doesn&#x27;t necessarily mean someone has asked that before and it&#x27;s in the training data.   Is that reasoning?  I don&#x27;t know ... how does the brain do it?</div><br/><div id="39898195" class="c"><input type="checkbox" id="c-39898195" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39897039">parent</a><span>|</span><a href="#39897587">next</a><span>|</span><label class="collapse" for="c-39898195">[-]</label><label class="expand" for="c-39898195">[1 more]</label></div><br/><div class="children"><div class="content">I mean that’s what sucks about Open AI isn’t it ? They won’t tell us what is in the training data so we don’t know. All I’m saying is that it wouldn’t be surprising if this was discussed previously somewhere in a pop science book.<p>That answer was close btw !</div><br/></div></div><div id="39897587" class="c"><input type="checkbox" id="c-39897587" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39897039">parent</a><span>|</span><a href="#39898195">prev</a><span>|</span><a href="#39890738">next</a><span>|</span><label class="collapse" for="c-39897587">[-]</label><label class="expand" for="c-39897587">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  how does the brain do it?<p>It&#x27;s a lot of organic matmuls. ;)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39890738" class="c"><input type="checkbox" id="c-39890738" checked=""/><div class="controls bullet"><span class="by">namarie</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890546">parent</a><span>|</span><a href="#39890657">prev</a><span>|</span><a href="#39890739">next</a><span>|</span><label class="collapse" for="c-39890738">[-]</label><label class="expand" for="c-39890738">[2 more]</label></div><br/><div class="children"><div class="content">I can agree on the context windows,  but what other output structure would you have?</div><br/><div id="39897527" class="c"><input type="checkbox" id="c-39897527" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890738">parent</a><span>|</span><a href="#39890739">next</a><span>|</span><label class="collapse" for="c-39897527">[-]</label><label class="expand" for="c-39897527">[1 more]</label></div><br/><div class="children"><div class="content">Working with pure bytes is one option that&#x27;s being researched. That way you&#x27;re not really constrained by anything at all. Sound, images, text, video, etc. Anything goes in, anything comes out. It&#x27;s hard to say if it&#x27;s feasible with current compute yet without tokenizers to reduce dimensionality.</div><br/></div></div></div></div></div></div><div id="39890739" class="c"><input type="checkbox" id="c-39890739" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890546">prev</a><span>|</span><a href="#39896840">next</a><span>|</span><label class="collapse" for="c-39890739">[-]</label><label class="expand" for="c-39890739">[19 more]</label></div><br/><div class="children"><div class="content">It is invaluable to have a chunk of human knowledge that can tell you things like the Brooklyn Nets won the 1986 Cricket World Cup by scoring 46 yards in only 3 frames</div><br/><div id="39891479" class="c"><input type="checkbox" id="c-39891479" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890739">parent</a><span>|</span><a href="#39890981">next</a><span>|</span><label class="collapse" for="c-39891479">[-]</label><label class="expand" for="c-39891479">[3 more]</label></div><br/><div class="children"><div class="content">The facts LLMs learned from training are fuzzy, unreliable, and quickly outdated. You actually want retrieval-augmented generation (RAG) where a model queries an external system for facts or to perform calculations and postprocesses the results to generate an answer for you.</div><br/><div id="39894596" class="c"><input type="checkbox" id="c-39894596" checked=""/><div class="controls bullet"><span class="by">unshavedyak</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891479">parent</a><span>|</span><a href="#39890981">next</a><span>|</span><label class="collapse" for="c-39894596">[-]</label><label class="expand" for="c-39894596">[2 more]</label></div><br/><div class="children"><div class="content">Is there a name for the reverse? I&#x27;m interested in having a local LLM monitor an incoming, stateful data stream. Imagine chats. It should have the capability of tracking the current day, active participants, active topics, etc - and then use that stateful world view to associate metadata with incoming streams during indexing.<p>Then after all is indexed you can pursue RAG on a richer set of metadata. Though i&#x27;ve got no idea what that stateful world view is.</div><br/><div id="39903307" class="c"><input type="checkbox" id="c-39903307" checked=""/><div class="controls bullet"><span class="by">firewolf34</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39894596">parent</a><span>|</span><a href="#39890981">next</a><span>|</span><label class="collapse" for="c-39903307">[-]</label><label class="expand" for="c-39903307">[1 more]</label></div><br/><div class="children"><div class="content">This is an interesting idea but I&#x27;m having trouble understanding what you&#x27;re to achieve. Do you mean the LLM would simply continuously update it&#x27;s context window with incoming data feeds realtime, and you use it as an interface?  That&#x27;s pretty akin to summarization task, yes? Or are you augmenting the streams with &quot;metadata&quot; you mentioned?</div><br/></div></div></div></div></div></div><div id="39890981" class="c"><input type="checkbox" id="c-39890981" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890739">parent</a><span>|</span><a href="#39891479">prev</a><span>|</span><a href="#39896840">next</a><span>|</span><label class="collapse" for="c-39890981">[-]</label><label class="expand" for="c-39890981">[15 more]</label></div><br/><div class="children"><div class="content">According to ChatGPT<p>&gt; Australia won the 1987 Cricket World Cup. The 1986 date is incorrect; there was no Cricket World Cup in 1986. The tournament took place in 1987, and Australia defeated England in the final to win their first title.<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;e9360faa-1157-4806-80ea-563489f946f3" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;e9360faa-1157-4806-80ea-563489...</a><p>I&#x27;m no cricket fan, so someone will have to correct Wikipedia if that&#x27;s wrong.<p>If you want to point out that LLMs hallucinate, you might want to speak plainly and just come out and say it, or at least give a real world example and not one where it didn&#x27;t.</div><br/><div id="39891129" class="c"><input type="checkbox" id="c-39891129" checked=""/><div class="controls bullet"><span class="by">vlunkr</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890981">parent</a><span>|</span><a href="#39891164">next</a><span>|</span><label class="collapse" for="c-39891129">[-]</label><label class="expand" for="c-39891129">[13 more]</label></div><br/><div class="children"><div class="content">We’re not talking about running chatGPT locally though, are we?</div><br/><div id="39891147" class="c"><input type="checkbox" id="c-39891147" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891129">parent</a><span>|</span><a href="#39891164">next</a><span>|</span><label class="collapse" for="c-39891147">[-]</label><label class="expand" for="c-39891147">[12 more]</label></div><br/><div class="children"><div class="content"><i>sigh</i> your going to make me open my laptop, aren&#x27;t you.</div><br/><div id="39891197" class="c"><input type="checkbox" id="c-39891197" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891147">parent</a><span>|</span><a href="#39891164">next</a><span>|</span><label class="collapse" for="c-39891197">[-]</label><label class="expand" for="c-39891197">[11 more]</label></div><br/><div class="children"><div class="content">I ran &#x27;who won the 1986 Cricket World Cup&#x27; against llama2-uncensored (the local model I have pre-downloaded) and hilarious got 5 different answers asking it 5 times:<p><pre><code>    &gt;&gt;&gt; who won the 1986 Cricket World Cup
    India
    
    &gt;&gt;&gt; who won the 1986 Cricket World Cup
    Australia
    
    &gt;&gt;&gt; who won the 1986 Cricket World Cup
    New Zealand
    
    &gt;&gt;&gt; who won the 1986 Cricket World Cup
    West Indies
    
    &gt;&gt;&gt; who won the 1986 Cricket World Cup
    England
</code></pre>
Which proves GP&#x27;s point about hallucinations, though none of those are<p>&gt; Brooklyn Nets won the 1986 Cricket World Cup by scoring 46 yards in only 3 frames<p>LLM&#x27;s hallucinations are insidous because they have the ring of truth around them. yards and frames aren&#x27;t cricket terms, so we&#x27;re off to the races with them.</div><br/><div id="39893952" class="c"><input type="checkbox" id="c-39893952" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891197">parent</a><span>|</span><a href="#39891587">next</a><span>|</span><label class="collapse" for="c-39893952">[-]</label><label class="expand" for="c-39893952">[1 more]</label></div><br/><div class="children"><div class="content">You should specify the model size and temperature.<p>For fact retrieval you need to use temperature 0.<p>If you don&#x27;t get the right facts then try 34b, 70b, Mixtral, Falcon 180b, or another highly ranked one that has come out recently like DBRX.</div><br/></div></div><div id="39891587" class="c"><input type="checkbox" id="c-39891587" checked=""/><div class="controls bullet"><span class="by">beefnugs</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891197">parent</a><span>|</span><a href="#39893952">prev</a><span>|</span><a href="#39891365">next</a><span>|</span><label class="collapse" for="c-39891587">[-]</label><label class="expand" for="c-39891587">[4 more]</label></div><br/><div class="children"><div class="content">Actually isn&#x27;t this good? It means we can run something multiple times to prove itself a bad answer?</div><br/><div id="39893299" class="c"><input type="checkbox" id="c-39893299" checked=""/><div class="controls bullet"><span class="by">latexr</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891587">parent</a><span>|</span><a href="#39891365">next</a><span>|</span><label class="collapse" for="c-39893299">[-]</label><label class="expand" for="c-39893299">[3 more]</label></div><br/><div class="children"><div class="content">You can ask LLMs the same question and they might sometimes get it wrong and other times get it right. Having different answers is no indication that none of them is correct.<p>Furthermore, even if an LLM always gives the same answer to a question, there’s no guarantee the answer is correct.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Propaganda" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Propaganda</a><p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Big_lie#Alleged_quotation" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Big_lie#Alleged_quotation</a></div><br/><div id="39896847" class="c"><input type="checkbox" id="c-39896847" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39893299">parent</a><span>|</span><a href="#39891365">next</a><span>|</span><label class="collapse" for="c-39896847">[-]</label><label class="expand" for="c-39896847">[2 more]</label></div><br/><div class="children"><div class="content">An LLM will always give the same output for the same input. It’s sorta like a random number generator that gives the same list of “random” numbers for the same seed. LLMs get a seed too.</div><br/><div id="39899109" class="c"><input type="checkbox" id="c-39899109" checked=""/><div class="controls bullet"><span class="by">latexr</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39896847">parent</a><span>|</span><a href="#39891365">next</a><span>|</span><label class="collapse" for="c-39899109">[-]</label><label class="expand" for="c-39899109">[1 more]</label></div><br/><div class="children"><div class="content">That’s irrelevant for the matter. The person I replied to obviously did not have seeded responses in mind.</div><br/></div></div></div></div></div></div></div></div><div id="39891365" class="c"><input type="checkbox" id="c-39891365" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891197">parent</a><span>|</span><a href="#39891587">prev</a><span>|</span><a href="#39891164">next</a><span>|</span><label class="collapse" for="c-39891365">[-]</label><label class="expand" for="c-39891365">[5 more]</label></div><br/><div class="children"><div class="content">If you want factual answers from a local model it might help to turn the temperature down.</div><br/><div id="39891792" class="c"><input type="checkbox" id="c-39891792" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891365">parent</a><span>|</span><a href="#39891449">next</a><span>|</span><label class="collapse" for="c-39891792">[-]</label><label class="expand" for="c-39891792">[1 more]</label></div><br/><div class="children"><div class="content">It would also help if I had more VRAM and wasn&#x27;t running a 7B parameter 4-bit quantized model.</div><br/></div></div><div id="39891455" class="c"><input type="checkbox" id="c-39891455" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891365">parent</a><span>|</span><a href="#39891449">prev</a><span>|</span><a href="#39891164">next</a><span>|</span><label class="collapse" for="c-39891455">[-]</label><label class="expand" for="c-39891455">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If you want factual answers from a local model it might help to turn the temperature down.<p>This makes sense. If you interact with a language model and it says something wrong it is your fault</div><br/><div id="39892011" class="c"><input type="checkbox" id="c-39892011" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891455">parent</a><span>|</span><a href="#39891164">next</a><span>|</span><label class="collapse" for="c-39892011">[-]</label><label class="expand" for="c-39892011">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re not &quot;interacting with a language model&quot;, you&#x27;re running a program (llama.cpp) with a sampling algorithm which is not set to maximum factualness by default.<p>It&#x27;s like how you have to set x264 to the anime tuning or the film tuning depending on what you run it on.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39896840" class="c"><input type="checkbox" id="c-39896840" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890739">prev</a><span>|</span><a href="#39891250">next</a><span>|</span><label class="collapse" for="c-39896840">[-]</label><label class="expand" for="c-39896840">[1 more]</label></div><br/><div class="children"><div class="content">Language models are an inefficient way to store knowledge; if you want to have a “pseudo-backup of a large chunk of human knowledge,” download a wikipedia dump, not an LLM.<p>If you want a friendly but fallible UI to that dump, download an LLM and build a simple ReAct framework around it with prompting to use the wikipedia dump for reference.</div><br/></div></div><div id="39891250" class="c"><input type="checkbox" id="c-39891250" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39896840">prev</a><span>|</span><a href="#39890507">next</a><span>|</span><label class="collapse" for="c-39891250">[-]</label><label class="expand" for="c-39891250">[5 more]</label></div><br/><div class="children"><div class="content">It seems to be an unbelievably inefficient way to back up knowledge.</div><br/><div id="39891642" class="c"><input type="checkbox" id="c-39891642" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891250">parent</a><span>|</span><a href="#39890507">next</a><span>|</span><label class="collapse" for="c-39891642">[-]</label><label class="expand" for="c-39891642">[4 more]</label></div><br/><div class="children"><div class="content">Are they though? They are lossy compressing trillions of tokens into a few dozen GB. The decompression action is fuzzy and inefficient though.</div><br/><div id="39891846" class="c"><input type="checkbox" id="c-39891846" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891642">parent</a><span>|</span><a href="#39890507">next</a><span>|</span><label class="collapse" for="c-39891846">[-]</label><label class="expand" for="c-39891846">[3 more]</label></div><br/><div class="children"><div class="content">And it requires massive computational power to decompress, which I don&#x27;t expect to be available in a catastrophic situation where humans have lost a large chunk of important knowledge.</div><br/><div id="39891945" class="c"><input type="checkbox" id="c-39891945" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891846">parent</a><span>|</span><a href="#39890507">next</a><span>|</span><label class="collapse" for="c-39891945">[-]</label><label class="expand" for="c-39891945">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t necessarily agree. It requires massive computing power, but running models smaller than 70G parameters is possible on consumer hardware, albeit slowly.</div><br/><div id="39900176" class="c"><input type="checkbox" id="c-39900176" checked=""/><div class="controls bullet"><span class="by">threecheese</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891945">parent</a><span>|</span><a href="#39890507">next</a><span>|</span><label class="collapse" for="c-39900176">[-]</label><label class="expand" for="c-39900176">[1 more]</label></div><br/><div class="children"><div class="content">Parent may be thinking more along the lines of a “hope we can print all the knowledge“ type catastrophe. Though if there is zero compute it’ll be tough reading all those disks!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39890507" class="c"><input type="checkbox" id="c-39890507" checked=""/><div class="controls bullet"><span class="by">texuf</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39891250">prev</a><span>|</span><a href="#39891923">next</a><span>|</span><label class="collapse" for="c-39890507">[-]</label><label class="expand" for="c-39890507">[7 more]</label></div><br/><div class="children"><div class="content">Any recommendations for the latest and greatest way to run these locally?</div><br/><div id="39893859" class="c"><input type="checkbox" id="c-39893859" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890507">parent</a><span>|</span><a href="#39891794">next</a><span>|</span><label class="collapse" for="c-39893859">[-]</label><label class="expand" for="c-39893859">[1 more]</label></div><br/><div class="children"><div class="content">I am the author of Msty [1]. My goal is to make it as straightforward as possible with just one click (once you download the app). If you end up trying it, I would love to hear your feedback.<p>1: <a href="https:&#x2F;&#x2F;msty.app" rel="nofollow">https:&#x2F;&#x2F;msty.app</a></div><br/></div></div><div id="39891794" class="c"><input type="checkbox" id="c-39891794" checked=""/><div class="controls bullet"><span class="by">slowmotiony</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890507">parent</a><span>|</span><a href="#39893859">prev</a><span>|</span><a href="#39890577">next</a><span>|</span><label class="collapse" for="c-39891794">[-]</label><label class="expand" for="c-39891794">[1 more]</label></div><br/><div class="children"><div class="content">I use a tool called LM Studio, makes it trivial to run these models on a Mac. You can also use it as a local API so it kinda acts like a drop-in replacement for the openAI API.</div><br/></div></div><div id="39890577" class="c"><input type="checkbox" id="c-39890577" checked=""/><div class="controls bullet"><span class="by">etc-hosts</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890507">parent</a><span>|</span><a href="#39891794">prev</a><span>|</span><a href="#39891001">next</a><span>|</span><label class="collapse" for="c-39890577">[-]</label><label class="expand" for="c-39890577">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;justine.lol&#x2F;oneliners&#x2F;" rel="nofollow">https:&#x2F;&#x2F;justine.lol&#x2F;oneliners&#x2F;</a></div><br/><div id="39900222" class="c"><input type="checkbox" id="c-39900222" checked=""/><div class="controls bullet"><span class="by">threecheese</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890577">parent</a><span>|</span><a href="#39891001">next</a><span>|</span><label class="collapse" for="c-39900222">[-]</label><label class="expand" for="c-39900222">[1 more]</label></div><br/><div class="children"><div class="content">This looks amazing, but the docs mention .llamafiles exceed the Windows executable size limit, and there are workarounds to externalize the weights. Do you think this is an impediment to its becoming popular? Or is MS consumer hardware just far enough behind (w&#x2F;o dedi gpu) that “there’s time”?</div><br/></div></div></div></div><div id="39891001" class="c"><input type="checkbox" id="c-39891001" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890507">parent</a><span>|</span><a href="#39890577">prev</a><span>|</span><a href="#39890512">next</a><span>|</span><label class="collapse" for="c-39891001">[-]</label><label class="expand" for="c-39891001">[1 more]</label></div><br/><div class="children"><div class="content">ollama</div><br/></div></div><div id="39890512" class="c"><input type="checkbox" id="c-39890512" checked=""/><div class="controls bullet"><span class="by">speps</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890507">parent</a><span>|</span><a href="#39891001">prev</a><span>|</span><a href="#39891923">next</a><span>|</span><label class="collapse" for="c-39890512">[-]</label><label class="expand" for="c-39890512">[1 more]</label></div><br/><div class="children"><div class="content">llamafile as per TFA...</div><br/></div></div></div></div><div id="39891923" class="c"><input type="checkbox" id="c-39891923" checked=""/><div class="controls bullet"><span class="by">LunaSea</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890507">prev</a><span>|</span><a href="#39891011">next</a><span>|</span><label class="collapse" for="c-39891923">[-]</label><label class="expand" for="c-39891923">[3 more]</label></div><br/><div class="children"><div class="content">I wonder how the Chinese government will manage to sensor LLMs within China?</div><br/><div id="39892822" class="c"><input type="checkbox" id="c-39892822" checked=""/><div class="controls bullet"><span class="by">popol12</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891923">parent</a><span>|</span><a href="#39891011">next</a><span>|</span><label class="collapse" for="c-39892822">[-]</label><label class="expand" for="c-39892822">[2 more]</label></div><br/><div class="children"><div class="content">The same way Facebook&#x2F;Google&#x2F;openAI &amp; others censored their own LLMs, I guess ?</div><br/><div id="39893772" class="c"><input type="checkbox" id="c-39893772" checked=""/><div class="controls bullet"><span class="by">LunaSea</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39892822">parent</a><span>|</span><a href="#39891011">next</a><span>|</span><label class="collapse" for="c-39893772">[-]</label><label class="expand" for="c-39893772">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s only for SaaS LLMs, but if you can simply download and run one on your hardware, things become difficult.</div><br/></div></div></div></div></div></div><div id="39891011" class="c"><input type="checkbox" id="c-39891011" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39891923">prev</a><span>|</span><a href="#39890772">next</a><span>|</span><label class="collapse" for="c-39891011">[-]</label><label class="expand" for="c-39891011">[3 more]</label></div><br/><div class="children"><div class="content">And why would I need to backup human knowledge as an individual</div><br/><div id="39892285" class="c"><input type="checkbox" id="c-39892285" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891011">parent</a><span>|</span><a href="#39890772">next</a><span>|</span><label class="collapse" for="c-39892285">[-]</label><label class="expand" for="c-39892285">[2 more]</label></div><br/><div class="children"><div class="content">You remember those fantasies where you got up from your seat at the pub and punched the lights out of this guy for being rude? A lot of us have fantasies of being the all powerful oracle that guides a reboot of civilization using knowledge of science and engineering.</div><br/><div id="39893323" class="c"><input type="checkbox" id="c-39893323" checked=""/><div class="controls bullet"><span class="by">latexr</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39892285">parent</a><span>|</span><a href="#39890772">next</a><span>|</span><label class="collapse" for="c-39893323">[-]</label><label class="expand" for="c-39893323">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the all powerful oracle that guides a reboot of civilization using knowledge of science and engineering.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dr._Stone" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dr._Stone</a></div><br/></div></div></div></div></div></div><div id="39890772" class="c"><input type="checkbox" id="c-39890772" checked=""/><div class="controls bullet"><span class="by">TheCaptain4815</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39891011">prev</a><span>|</span><a href="#39890694">next</a><span>|</span><label class="collapse" for="c-39890772">[-]</label><label class="expand" for="c-39890772">[4 more]</label></div><br/><div class="children"><div class="content">It’s kind of crazy really. Before LLMs, any type of world scale disaster you’d hope for what? Wikipedia backups? Now, a single LLM ran locally would be much more effective. Imagine the local models in 5 years!</div><br/><div id="39891460" class="c"><input type="checkbox" id="c-39891460" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890772">parent</a><span>|</span><a href="#39891069">next</a><span>|</span><label class="collapse" for="c-39891460">[-]</label><label class="expand" for="c-39891460">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a lot more than just Wikipedia that gets archived, and yes, that is a far more sensible way to go about it. For one thing, the compute required to then read it back is orders of magnitude less (a 15 year old smartphone can handle it just fine). For another, you don&#x27;t have to wonder how much of what you got back is hallucinated - data is either there or it&#x27;s corrupted and unreadable.</div><br/></div></div><div id="39891069" class="c"><input type="checkbox" id="c-39891069" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890772">parent</a><span>|</span><a href="#39891460">prev</a><span>|</span><a href="#39890909">next</a><span>|</span><label class="collapse" for="c-39891069">[-]</label><label class="expand" for="c-39891069">[1 more]</label></div><br/><div class="children"><div class="content">The processing required to run current language models with a useful amount of knowledge encoded in them is way more than I imagine would be available in a &quot;world scale disaster&quot;.</div><br/></div></div><div id="39890909" class="c"><input type="checkbox" id="c-39890909" checked=""/><div class="controls bullet"><span class="by">danmur</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890772">parent</a><span>|</span><a href="#39891069">prev</a><span>|</span><a href="#39890694">next</a><span>|</span><label class="collapse" for="c-39890909">[-]</label><label class="expand" for="c-39890909">[1 more]</label></div><br/><div class="children"><div class="content">Uh yeah I would, and still am, take the Wikipedia backup for doomsday scenarios. I&#x27;m not even sure how that would be a competition</div><br/></div></div></div></div></div></div><div id="39890694" class="c"><input type="checkbox" id="c-39890694" checked=""/><div class="controls bullet"><span class="by">ajtulloch</span><span>|</span><a href="#39890454">prev</a><span>|</span><a href="#39896402">next</a><span>|</span><label class="collapse" for="c-39890694">[-]</label><label class="expand" for="c-39890694">[3 more]</label></div><br/><div class="children"><div class="content">- <a href="https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;flame&#x2F;laff&#x2F;pfhp&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;flame&#x2F;laff&#x2F;pfhp&#x2F;index.html</a> (e.g. here <a href="https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;flame&#x2F;laff&#x2F;pfhp&#x2F;week2-blocking-for-registers.html" rel="nofollow">https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;flame&#x2F;laff&#x2F;pfhp&#x2F;week2-blocki...</a>)<p>- <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;nadavrot&#x2F;5b35d44e8ba3dd718e595e40184d03f0" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;nadavrot&#x2F;5b35d44e8ba3dd718e595e40184...</a><p>might be of interest</div><br/><div id="39890777" class="c"><input type="checkbox" id="c-39890777" checked=""/><div class="controls bullet"><span class="by">kpw94</span><span>|</span><a href="#39890694">parent</a><span>|</span><a href="#39896402">next</a><span>|</span><label class="collapse" for="c-39890777">[-]</label><label class="expand" for="c-39890777">[2 more]</label></div><br/><div class="children"><div class="content">Great links,
especially last one referencing the Goto paper:<p><a href="https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;pingali&#x2F;CS378&#x2F;2008sp&#x2F;papers&#x2F;gotoPaper.pdf" rel="nofollow">https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;pingali&#x2F;CS378&#x2F;2008sp&#x2F;papers&#x2F;...</a><p>&gt;&gt; I believe the trick with CPU math kernels is exploiting instruction level parallelism with fewer memory references<p>It&#x27;s the collection of tricks to minimize all sort of cache misses (L1, L2, TLB, page miss etc), improve register reuse, leverage SIMD instructions, transpose one of the matrices if it provides better spatial locality, etc.</div><br/><div id="39891977" class="c"><input type="checkbox" id="c-39891977" checked=""/><div class="controls bullet"><span class="by">larodi</span><span>|</span><a href="#39890694">root</a><span>|</span><a href="#39890777">parent</a><span>|</span><a href="#39896402">next</a><span>|</span><label class="collapse" for="c-39891977">[-]</label><label class="expand" for="c-39891977">[1 more]</label></div><br/><div class="children"><div class="content">The trick is indeed to somehow imagine how the CPU works with the Lx caches and keep as much info in them as possible. So its not only about exploiting fancy instructions, but also thinking in engineering terms. Most of the software written in higher level langs cannot effectively use L1&#x2F;L2 and thus results in this constant slowing down otherwise similarly (from asymptotic analysis perspective) complexity algos.</div><br/></div></div></div></div></div></div><div id="39896402" class="c"><input type="checkbox" id="c-39896402" checked=""/><div class="controls bullet"><span class="by">aaronscott</span><span>|</span><a href="#39890694">prev</a><span>|</span><a href="#39896164">next</a><span>|</span><label class="collapse" for="c-39896402">[-]</label><label class="expand" for="c-39896402">[9 more]</label></div><br/><div class="children"><div class="content">&gt; I like to define my subroutines using a modern language like C++, which goes 47 gigaflops. This means C++ is three orders of a magnitude faster than Python. That&#x27;s twenty years of progress per Moore&#x27;s law.<p>This is great. I love the idea of measuring performance differences in “years of Moore’s law.”<p>Twenty years puts the delta in an easy to understand framework.</div><br/><div id="39896704" class="c"><input type="checkbox" id="c-39896704" checked=""/><div class="controls bullet"><span class="by">JohnKemeny</span><span>|</span><a href="#39896402">parent</a><span>|</span><a href="#39896164">next</a><span>|</span><label class="collapse" for="c-39896704">[-]</label><label class="expand" for="c-39896704">[8 more]</label></div><br/><div class="children"><div class="content">I doubt that you get Python to run faster than C++ at 2004 hardware.</div><br/><div id="39896800" class="c"><input type="checkbox" id="c-39896800" checked=""/><div class="controls bullet"><span class="by">mrtranscendence</span><span>|</span><a href="#39896402">root</a><span>|</span><a href="#39896704">parent</a><span>|</span><a href="#39898390">next</a><span>|</span><label class="collapse" for="c-39896800">[-]</label><label class="expand" for="c-39896800">[6 more]</label></div><br/><div class="children"><div class="content">Python on 2024 hardware vs C++ on 2004 hardware ... I don&#x27;t think it&#x27;s obvious that C++ always wins here, though it would depend on the use case, how much of the Python is underpinned by native libraries, and the specific hardware in question.</div><br/><div id="39896875" class="c"><input type="checkbox" id="c-39896875" checked=""/><div class="controls bullet"><span class="by">JohnKemeny</span><span>|</span><a href="#39896402">root</a><span>|</span><a href="#39896800">parent</a><span>|</span><a href="#39898390">next</a><span>|</span><label class="collapse" for="c-39896875">[-]</label><label class="expand" for="c-39896875">[5 more]</label></div><br/><div class="children"><div class="content">If we allow native libraries, it&#x27;s not clear that C++ would win, even on modern hardware.</div><br/><div id="39897507" class="c"><input type="checkbox" id="c-39897507" checked=""/><div class="controls bullet"><span class="by">michaelt</span><span>|</span><a href="#39896402">root</a><span>|</span><a href="#39896875">parent</a><span>|</span><a href="#39902282">next</a><span>|</span><label class="collapse" for="c-39897507">[-]</label><label class="expand" for="c-39897507">[1 more]</label></div><br/><div class="children"><div class="content">I think we all know that, when someone writes &quot;C++ is three orders of a magnitude faster than Python&quot; they&#x27;re not including native libraries.</div><br/></div></div><div id="39902282" class="c"><input type="checkbox" id="c-39902282" checked=""/><div class="controls bullet"><span class="by">MichaelDickens</span><span>|</span><a href="#39896402">root</a><span>|</span><a href="#39896875">parent</a><span>|</span><a href="#39897507">prev</a><span>|</span><a href="#39897708">next</a><span>|</span><label class="collapse" for="c-39902282">[-]</label><label class="expand" for="c-39902282">[1 more]</label></div><br/><div class="children"><div class="content">C++ with well-optimized libraries should always outperform Python with well-optimized libraries, right? They should be ~identical in the highly optimized inner loops, but Python has more overhead. But naive hand-written C++ could easily perform worse than something like Numpy.<p>(I&#x27;ve only tested this once, and my naive hand-written C++ was still twice as fast as Numpy, but that was only on one specific task.)</div><br/></div></div><div id="39897708" class="c"><input type="checkbox" id="c-39897708" checked=""/><div class="controls bullet"><span class="by">mrtranscendence</span><span>|</span><a href="#39896402">root</a><span>|</span><a href="#39896875">parent</a><span>|</span><a href="#39902282">prev</a><span>|</span><a href="#39898390">next</a><span>|</span><label class="collapse" for="c-39897708">[-]</label><label class="expand" for="c-39897708">[2 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t <i>not</i> include native libraries, at least if you want your benchmark to be realistic. Almost every Python library where performance matters is written (at least partially) in a compiled language.</div><br/><div id="39898517" class="c"><input type="checkbox" id="c-39898517" checked=""/><div class="controls bullet"><span class="by">bornfreddy</span><span>|</span><a href="#39896402">root</a><span>|</span><a href="#39897708">parent</a><span>|</span><a href="#39898390">next</a><span>|</span><label class="collapse" for="c-39898517">[-]</label><label class="expand" for="c-39898517">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but many people like the sound of &quot;X-times faster than Python&quot; while conveniently forgetting that the same thing can be (and usually is) done in Python + numpy &amp; co. even faster.<p>I have come to appreciate &quot;slowness&quot; of Python. It trades speed for legibility, which is a great compromise once you have <i>really fast</i> native libraries one import away. Best of both worlds.</div><br/></div></div></div></div></div></div></div></div><div id="39898390" class="c"><input type="checkbox" id="c-39898390" checked=""/><div class="controls bullet"><span class="by">bevekspldnw</span><span>|</span><a href="#39896402">root</a><span>|</span><a href="#39896704">parent</a><span>|</span><a href="#39896800">prev</a><span>|</span><a href="#39896164">next</a><span>|</span><label class="collapse" for="c-39898390">[-]</label><label class="expand" for="c-39898390">[1 more]</label></div><br/><div class="children"><div class="content">Honestly depends on what you are doing. Most of my python work is data collection and analysis on top of Postgres.<p>Being smart in how I use Postgres indexing (and when to disable it outright) has more performance impact than the actual language doing the plumbing.</div><br/></div></div></div></div></div></div><div id="39896164" class="c"><input type="checkbox" id="c-39896164" checked=""/><div class="controls bullet"><span class="by">TimPC</span><span>|</span><a href="#39896402">prev</a><span>|</span><a href="#39890810">next</a><span>|</span><label class="collapse" for="c-39896164">[-]</label><label class="expand" for="c-39896164">[2 more]</label></div><br/><div class="children"><div class="content">Strange title. My first read of the title thought the author was arguing the model is now faster on CPU than GPU. Would be much nicer if they titled this something closer to &quot;Performance Improvement for LLaMa on CPU&quot;.</div><br/><div id="39899459" class="c"><input type="checkbox" id="c-39899459" checked=""/><div class="controls bullet"><span class="by">utopcell</span><span>|</span><a href="#39896164">parent</a><span>|</span><a href="#39890810">next</a><span>|</span><label class="collapse" for="c-39899459">[-]</label><label class="expand" for="c-39899459">[1 more]</label></div><br/><div class="children"><div class="content">Same here.</div><br/></div></div></div></div><div id="39890810" class="c"><input type="checkbox" id="c-39890810" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#39896164">prev</a><span>|</span><a href="#39896366">next</a><span>|</span><label class="collapse" for="c-39890810">[-]</label><label class="expand" for="c-39890810">[7 more]</label></div><br/><div class="children"><div class="content">&gt; You don&#x27;t need a large computer to run a large language model<p>While running tiny llama does indeed count as running a language model, I’m skeptical that the capabilities of doing so match what most people would consider a baseline requirement to be useful.<p>Running 10 param model is also “technically” running an LM, and I can do it by hand with a piece of paper.<p>That doesn’t mean “you don’t need a computer to run an LM”…<p>I’m not sure where LM becomes LLM, but… I personally think it’s more about capability than parameter count.<p>I don’t <i>realllly</i> believe you can do a lot of useful LLM work on a pi</div><br/><div id="39890860" class="c"><input type="checkbox" id="c-39890860" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#39890810">parent</a><span>|</span><a href="#39895120">next</a><span>|</span><label class="collapse" for="c-39890860">[-]</label><label class="expand" for="c-39890860">[3 more]</label></div><br/><div class="children"><div class="content">Tinyllama isn&#x27;t going to be doing what ChatGPT does, but it still beats the pants off what we had for completion or sentiment analysis 5 years ago.  And now a Pi can run it decently fast.</div><br/><div id="39892812" class="c"><input type="checkbox" id="c-39892812" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#39890810">root</a><span>|</span><a href="#39890860">parent</a><span>|</span><a href="#39895120">next</a><span>|</span><label class="collapse" for="c-39892812">[-]</label><label class="expand" for="c-39892812">[2 more]</label></div><br/><div class="children"><div class="content">You can fine-tune a 60mm parameter (e.g. distilBERT) discriminative (not generative) language model and it&#x27;s one or two order of magnitude more efficient for classification tasks like sentiment analysis, and probably similar if not more accurate.</div><br/><div id="39894570" class="c"><input type="checkbox" id="c-39894570" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#39890810">root</a><span>|</span><a href="#39892812">parent</a><span>|</span><a href="#39895120">next</a><span>|</span><label class="collapse" for="c-39894570">[-]</label><label class="expand" for="c-39894570">[1 more]</label></div><br/><div class="children"><div class="content">Yup, I&#x27;m not saying TinyLLAMA is minimal, efficient, etc (indeed, that is just saying that you can take models even smaller).  And a whole lot of what we just throw LLMs at is not the right tool for the job, but it&#x27;s expedient and surprisingly works.</div><br/></div></div></div></div></div></div><div id="39891672" class="c"><input type="checkbox" id="c-39891672" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890810">parent</a><span>|</span><a href="#39895120">prev</a><span>|</span><a href="#39892159">next</a><span>|</span><label class="collapse" for="c-39891672">[-]</label><label class="expand" for="c-39891672">[1 more]</label></div><br/><div class="children"><div class="content">Some newer models trained more recently have been repeatedly shown to have comparable performance as larger models. And the Mixture of Experts architecture makes it possible to train large models that know how to selectively activate only the parts that are relevant for the current context, which drastically reduces compute demand. Smaller models can also level the playing field by being faster to process content retrieved by RAG. Via the same mechanism, they could also access larger, more powerful models for tasks that exceed their capabilities.</div><br/></div></div><div id="39892159" class="c"><input type="checkbox" id="c-39892159" checked=""/><div class="controls bullet"><span class="by">SoothingSorbet</span><span>|</span><a href="#39890810">parent</a><span>|</span><a href="#39891672">prev</a><span>|</span><a href="#39896366">next</a><span>|</span><label class="collapse" for="c-39892159">[-]</label><label class="expand" for="c-39892159">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve gotten some useful stuff out of 7B param LLMs, and that should fit on a Pi quantized.</div><br/></div></div></div></div><div id="39896366" class="c"><input type="checkbox" id="c-39896366" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39890810">prev</a><span>|</span><a href="#39893427">next</a><span>|</span><label class="collapse" for="c-39896366">[-]</label><label class="expand" for="c-39896366">[2 more]</label></div><br/><div class="children"><div class="content">As someone who has tried to beat MKL-DNN, and was unsuccessful at doing so even for constrained matrix sizes, I’m curious how they pulled off such a massive improvement.<p>But as someone who routinely estimates picojoules per flop at $DAY_JOB - there’s simply no way this is energy efficient. That is not even physically possible with a CPU.</div><br/><div id="39898386" class="c"><input type="checkbox" id="c-39898386" checked=""/><div class="controls bullet"><span class="by">janwas</span><span>|</span><a href="#39896366">parent</a><span>|</span><a href="#39893427">next</a><span>|</span><label class="collapse" for="c-39898386">[-]</label><label class="expand" for="c-39898386">[1 more]</label></div><br/><div class="children"><div class="content">I think the previous code was using dot products, f32 instead of bf16.</div><br/></div></div></div></div><div id="39893427" class="c"><input type="checkbox" id="c-39893427" checked=""/><div class="controls bullet"><span class="by">tiffanyh</span><span>|</span><a href="#39896366">prev</a><span>|</span><a href="#39893221">next</a><span>|</span><label class="collapse" for="c-39893427">[-]</label><label class="expand" for="c-39893427">[8 more]</label></div><br/><div class="children"><div class="content">Pixar uses CPUs …<p>I wonder if we’ll end up in a situation like rendered movies.<p>Where the big studios like Pixar uses CPUs (not GPUs) to render their movies due to the cost&#x2F;perf (and access to larger amounts of RAM).<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25616372">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25616372</a></div><br/><div id="39900237" class="c"><input type="checkbox" id="c-39900237" checked=""/><div class="controls bullet"><span class="by">cthalupa</span><span>|</span><a href="#39893427">parent</a><span>|</span><a href="#39893668">next</a><span>|</span><label class="collapse" for="c-39900237">[-]</label><label class="expand" for="c-39900237">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s entirely the cost&#x2F;perf of access to the larger amounts of VRAM that keeps rendering on CPUs now. GPUs are strictly better in almost every way for rendering (We could have some arguments about technical precision, FP calculations, etc. but with modern cards these arguments are largely semantics, you can have output that is accurate to the level that any human watching for entertainment purposes will not be able to determine any physical inaccuracies that arise from a GPU render vs. CPU.), except the need for large amounts of VRAM being quite expensive at current.<p>But that&#x27;s already been changing, and we are seeing studios moving to fully GPU based pipelines. Wylie Co, who are a major visual effects company (Dune part 1 and 2, marvel movies, the last of us, a bunch of others) are now a 100% GPU shop. The trend is towards more and more GPU rendering, not less.<p>With AI providing another strong incentive towards increasing the amount of VRAM on GPUs, I don&#x27;t see any reason to believe that trend will reverse.</div><br/></div></div><div id="39893668" class="c"><input type="checkbox" id="c-39893668" checked=""/><div class="controls bullet"><span class="by">CaptainOfCoit</span><span>|</span><a href="#39893427">parent</a><span>|</span><a href="#39900237">prev</a><span>|</span><a href="#39893646">next</a><span>|</span><label class="collapse" for="c-39893668">[-]</label><label class="expand" for="c-39893668">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure how true that is anymore, from the outside it seems they&#x27;re at least moving to a CPU&#x2F;GPU hybrid (which makes a lot of sense), at least judging by new features landing in RenderMan that continues to add more support for GPUs (like XPU).</div><br/><div id="39893685" class="c"><input type="checkbox" id="c-39893685" checked=""/><div class="controls bullet"><span class="by">tiffanyh</span><span>|</span><a href="#39893427">root</a><span>|</span><a href="#39893668">parent</a><span>|</span><a href="#39893646">next</a><span>|</span><label class="collapse" for="c-39893685">[-]</label><label class="expand" for="c-39893685">[2 more]</label></div><br/><div class="children"><div class="content">Isn’t this more of a function that RenderMan is a sold product.<p>And it’s expected to at least support GPUs.</div><br/><div id="39893806" class="c"><input type="checkbox" id="c-39893806" checked=""/><div class="controls bullet"><span class="by">CaptainOfCoit</span><span>|</span><a href="#39893427">root</a><span>|</span><a href="#39893685">parent</a><span>|</span><a href="#39893646">next</a><span>|</span><label class="collapse" for="c-39893806">[-]</label><label class="expand" for="c-39893806">[1 more]</label></div><br/><div class="children"><div class="content">Hard to know without getting information from people at Pixar really.<p>Not sure how much sense it would make for Pixar to spend a lot of engineering hours for things they wouldn&#x27;t touch in their own rendering pipeline. As far as I know, most of the feature development comes from their own rendering requirements rather than from outside customers.</div><br/></div></div></div></div></div></div><div id="39893646" class="c"><input type="checkbox" id="c-39893646" checked=""/><div class="controls bullet"><span class="by">kreco</span><span>|</span><a href="#39893427">parent</a><span>|</span><a href="#39893668">prev</a><span>|</span><a href="#39893221">next</a><span>|</span><label class="collapse" for="c-39893646">[-]</label><label class="expand" for="c-39893646">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Where the big studios like Pixar uses CPUs (not GPUs) to render their movies due to the cost&#x2F;perf (and access to larger amounts of RAM).<p>I wonder if (or when) this will change once integrated GPUs become &quot;mainstream&quot;, the CPU&#x2F;GPU share the same RAM AFAIK.</div><br/><div id="39893770" class="c"><input type="checkbox" id="c-39893770" checked=""/><div class="controls bullet"><span class="by">rockwotj</span><span>|</span><a href="#39893427">root</a><span>|</span><a href="#39893646">parent</a><span>|</span><a href="#39893221">next</a><span>|</span><label class="collapse" for="c-39893770">[-]</label><label class="expand" for="c-39893770">[2 more]</label></div><br/><div class="children"><div class="content">I expect GPU hardware to specialize like Google’s TPU. The TPU feels like ARM in these AI workloads where when you start to run these at scale, you’ll care about the cost perf tradeoff for most usecases.<p>&gt; CPU&#x2F;GPU share the same RAM AFAIK.<p>This depends on the GPU I believe Apple has integrated memory, but most GPUs from my limited experience writing kernels have their own memory. CUDA pretty heavily has a device memory vs host memory abstraction.</div><br/><div id="39895995" class="c"><input type="checkbox" id="c-39895995" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#39893427">root</a><span>|</span><a href="#39893770">parent</a><span>|</span><a href="#39893221">next</a><span>|</span><label class="collapse" for="c-39895995">[-]</label><label class="expand" for="c-39895995">[1 more]</label></div><br/><div class="children"><div class="content">On top of that, Nvidia has provided a unified addressing abstraction over PCI for a looooong time via CUDA: <a href="https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;unified-memory-in-cuda-6&#x2F;" rel="nofollow">https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;unified-memory-in-cuda-6&#x2F;</a><p>Customers like Pixar could probably push this even further, with a more recent Nvidia rack and Mellanox networking. Networking a couple Mac Studios over Thunderbolt doesn&#x27;t have a hope of competing, at that scale.</div><br/></div></div></div></div></div></div></div></div><div id="39893221" class="c"><input type="checkbox" id="c-39893221" checked=""/><div class="controls bullet"><span class="by">AbuAssar</span><span>|</span><a href="#39893427">prev</a><span>|</span><a href="#39890537">next</a><span>|</span><label class="collapse" for="c-39893221">[-]</label><label class="expand" for="c-39893221">[5 more]</label></div><br/><div class="children"><div class="content">regarding AMD zen4 with avx512:<p>&quot;Here we see that, despite only being twice the price, the 7995WX x86 ISA offers 7x more raw compute power than the M2 Ultra ARM ISA, and nearly the same token generation speed, which is likely thanks to its 384mb L3 cache. When I bought this chip, I had to expand support in llama.cpp for bfloat16 and AVX512 before I could fully test its capabilities. My work means you can now run LLaMA 2.8x faster on Zen4 than you could before.&quot;</div><br/><div id="39893525" class="c"><input type="checkbox" id="c-39893525" checked=""/><div class="controls bullet"><span class="by">reckless</span><span>|</span><a href="#39893221">parent</a><span>|</span><a href="#39890537">next</a><span>|</span><label class="collapse" for="c-39893525">[-]</label><label class="expand" for="c-39893525">[4 more]</label></div><br/><div class="children"><div class="content">Does this also count platform costs or just chip cost? I&#x27;d imagine the threadripper motherboard and ram costs aren&#x27;t insignificant</div><br/><div id="39896319" class="c"><input type="checkbox" id="c-39896319" checked=""/><div class="controls bullet"><span class="by">KennyBlanken</span><span>|</span><a href="#39893221">root</a><span>|</span><a href="#39893525">parent</a><span>|</span><a href="#39890537">next</a><span>|</span><label class="collapse" for="c-39896319">[-]</label><label class="expand" for="c-39896319">[3 more]</label></div><br/><div class="children"><div class="content">A complete desktop computer with the M2 Ultra w&#x2F;64GB of RAM and 1TB of SSD is $4k.<p>The 7995WX processor alone is $10k, the motherboard is <i>one grand</i>, the RAM is another $300. So you&#x27;re up to $11300, and you still don&#x27;t have a PSU, case, SSD, GPU....or heatsink that can handle the 300W TDP of the threadripper processor; you&#x27;re probably looking at a very large AIO radiator to keep it cool enough to get its quoted performance. So you&#x27;re probably up past $12k, 3x the price of the Studio...more like $14k if you want to have a GPU of similar capability to the M2 Ultra.<p>Just the usual &quot;aPPle cOMpuTeRs aRE EXpeNsIVE!&quot; nonsense.</div><br/><div id="39900182" class="c"><input type="checkbox" id="c-39900182" checked=""/><div class="controls bullet"><span class="by">juitpykyk</span><span>|</span><a href="#39893221">root</a><span>|</span><a href="#39896319">parent</a><span>|</span><a href="#39897246">next</a><span>|</span><label class="collapse" for="c-39900182">[-]</label><label class="expand" for="c-39900182">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re using the wrong CPU.<p>Consumer AMD 7950X supports AVX-512, it&#x27;s faster than M2 Ultra at half the cost.</div><br/></div></div><div id="39897246" class="c"><input type="checkbox" id="c-39897246" checked=""/><div class="controls bullet"><span class="by">incrudible</span><span>|</span><a href="#39893221">root</a><span>|</span><a href="#39896319">parent</a><span>|</span><a href="#39900182">prev</a><span>|</span><a href="#39890537">next</a><span>|</span><label class="collapse" for="c-39897246">[-]</label><label class="expand" for="c-39897246">[1 more]</label></div><br/><div class="children"><div class="content">So from a CPU perspective you get 7x the CPU throughput for 3x to 4x the price, plus upgradable RAM that is massively cheaper. The M2 uses the GPU for LLMs though, and there it sits in a weird spot where 64GB of (slower) RAM plus midrange GPU performance is not something that exists in the PC space. The closest thing would probably be a (faster) 48GB Quadro RTX which is in the $5000 ballpark. For other use cases where VRAM is not such a limiting factor, the comparably priced PC will blow the Mac out of the water, especially when it comes to GPU performance. The only reason we do not have cheap 96GB GDDR GPUs is that it would cannibalize NVIDIA&#x2F;AMDs high margin segment. If this was something that affected Apple, they would act the same.</div><br/></div></div></div></div></div></div></div></div><div id="39890537" class="c"><input type="checkbox" id="c-39890537" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#39893221">prev</a><span>|</span><a href="#39890910">next</a><span>|</span><label class="collapse" for="c-39890537">[-]</label><label class="expand" for="c-39890537">[1 more]</label></div><br/><div class="children"><div class="content">Super nice story on the matmul optimization that gave 810 gflops for 512x512.  Thanks for the write up and the contributions to llama.cpp and the community more broadly.</div><br/></div></div><div id="39890910" class="c"><input type="checkbox" id="c-39890910" checked=""/><div class="controls bullet"><span class="by">none_to_remain</span><span>|</span><a href="#39890537">prev</a><span>|</span><a href="#39890570">next</a><span>|</span><label class="collapse" for="c-39890910">[-]</label><label class="expand" for="c-39890910">[6 more]</label></div><br/><div class="children"><div class="content">From the example: &quot;--temp 0 turns off the random number generator (we don&#x27;t want improvisation for a spam filter)&quot;<p>I&#x27;ve been thinking for a while about how many applications of LLMs need this adjustment and aren&#x27;t getting it</div><br/><div id="39892268" class="c"><input type="checkbox" id="c-39892268" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39890910">parent</a><span>|</span><a href="#39890973">next</a><span>|</span><label class="collapse" for="c-39892268">[-]</label><label class="expand" for="c-39892268">[1 more]</label></div><br/><div class="children"><div class="content">I couldn&#x27;t disagree more, turning temp to zero is like taking a monte carlo method and only using one sample, or a particle filter with only one particle. Takes the entire concept and throws it out of the window so you can have predictability.<p>LLMs need to probabilistically explore the generation domain to converge on a good result for best performance. Similar issue with people benchmarking models by only having them output one single token (e.g. yes or no) outright, which prevents any real computation from occurring so the results are predictably poor.</div><br/></div></div><div id="39890973" class="c"><input type="checkbox" id="c-39890973" checked=""/><div class="controls bullet"><span class="by">mvkel</span><span>|</span><a href="#39890910">parent</a><span>|</span><a href="#39892268">prev</a><span>|</span><a href="#39890570">next</a><span>|</span><label class="collapse" for="c-39890973">[-]</label><label class="expand" for="c-39890973">[4 more]</label></div><br/><div class="children"><div class="content">Is that what it does, though?<p>I thought setting temperature to 0 would (extremely simple example) equate to a spam filter seeing:<p>- this is a spam email<p>But if the sender adapts and says<p>- th1s is a spam email<p>It wouldn&#x27;t be flagged as spam.</div><br/><div id="39891757" class="c"><input type="checkbox" id="c-39891757" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890910">root</a><span>|</span><a href="#39890973">parent</a><span>|</span><a href="#39891051">next</a><span>|</span><label class="collapse" for="c-39891757">[-]</label><label class="expand" for="c-39891757">[1 more]</label></div><br/><div class="children"><div class="content">The output of an autoregressive model is a probability for each token to appear next after the input sequence. Computing these is strictly deterministic from the prior context and the model&#x27;s weights.<p>Based on that probability distribution, a variety of text generation strategies are possible. The simplest (greedy decoding) is picking the token with the highest probability. To allow creativity, a random number generator is used to choose among the possible outputs, biased by the probabilities of course.<p>Temperature scales the output probabilities. As temperature increases, the probabilities approach 1&#x2F;dictionary size, and the output becomes completely random. For very small temperature values, text generation approaches greedy sampling.<p>If all you want is a spam filter, better replace the output layer of an LLM with one with just two outputs, and finetune that on a public collection of spam mails and some &quot;ham&quot; from your inbox.</div><br/></div></div><div id="39891051" class="c"><input type="checkbox" id="c-39891051" checked=""/><div class="controls bullet"><span class="by">none_to_remain</span><span>|</span><a href="#39890910">root</a><span>|</span><a href="#39890973">parent</a><span>|</span><a href="#39891757">prev</a><span>|</span><a href="#39890570">next</a><span>|</span><label class="collapse" for="c-39891051">[-]</label><label class="expand" for="c-39891051">[2 more]</label></div><br/><div class="children"><div class="content">My understanding is that temperature applies to the output side and allows for some randomness in the next predicted token. Here Justine has constrained the machine to start with either &quot;yes&quot; or &quot;no&quot; and to predict only one token. This makes the issue stark: leaving a non-zero temperature here would just add a chance of flipping a boolean.</div><br/><div id="39895033" class="c"><input type="checkbox" id="c-39895033" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39890910">root</a><span>|</span><a href="#39891051">parent</a><span>|</span><a href="#39890570">next</a><span>|</span><label class="collapse" for="c-39895033">[-]</label><label class="expand" for="c-39895033">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more nuanced than that, in practice: this is true for the shims you see from API providers (ex. OpenAI, Anthropic, Mistral).<p>With llama.cpp, it&#x27;s actually not a great idea to have temperature purely at 0: in practice, especially with smaller models, this leads to pure repeating or nonsense.<p>I can&#x27;t remember where I picked this up, but, a few years back, without _some_ randomness, the next likely token was always the last token.</div><br/></div></div></div></div></div></div></div></div><div id="39890570" class="c"><input type="checkbox" id="c-39890570" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#39890910">prev</a><span>|</span><a href="#39893712">next</a><span>|</span><label class="collapse" for="c-39890570">[-]</label><label class="expand" for="c-39890570">[9 more]</label></div><br/><div class="children"><div class="content">It fascinating to me that coming up on a year since Sapphire Rapids has been available in the public cloud, developers are still targeting AVX512 when they should be targeting VNNI and AMX.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;2555">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;2555</a></div><br/><div id="39890653" class="c"><input type="checkbox" id="c-39890653" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#39890570">parent</a><span>|</span><a href="#39890886">next</a><span>|</span><label class="collapse" for="c-39890653">[-]</label><label class="expand" for="c-39890653">[5 more]</label></div><br/><div class="children"><div class="content">This project in particular seems to care about the long tail of hardware; note that the very first machine in this post is a box from 2020 with spinning rust disk. Granted, adding support for newer extensions is likely also good, but cost&#x2F;benefit is in play.</div><br/><div id="39890805" class="c"><input type="checkbox" id="c-39890805" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#39890570">root</a><span>|</span><a href="#39890653">parent</a><span>|</span><a href="#39890886">next</a><span>|</span><label class="collapse" for="c-39890805">[-]</label><label class="expand" for="c-39890805">[4 more]</label></div><br/><div class="children"><div class="content">Is four years really &#x27;long tail&#x27; these days? Our VM host box is from 2010 (and I had to rebuild llama.cpp locally without AVX to get it working :P )</div><br/><div id="39890899" class="c"><input type="checkbox" id="c-39890899" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#39890570">root</a><span>|</span><a href="#39890805">parent</a><span>|</span><a href="#39891310">next</a><span>|</span><label class="collapse" for="c-39890899">[-]</label><label class="expand" for="c-39890899">[1 more]</label></div><br/><div class="children"><div class="content">For cutting-edge LLM work, probably? I mean, I run mine on older hardware than that, but I&#x27;m a total hobbyist...</div><br/></div></div><div id="39891310" class="c"><input type="checkbox" id="c-39891310" checked=""/><div class="controls bullet"><span class="by">d416</span><span>|</span><a href="#39890570">root</a><span>|</span><a href="#39890805">parent</a><span>|</span><a href="#39890899">prev</a><span>|</span><a href="#39895043">next</a><span>|</span><label class="collapse" for="c-39891310">[-]</label><label class="expand" for="c-39891310">[1 more]</label></div><br/><div class="children"><div class="content">It should be noted that while the HP  Prodesk was released in 2020, the CPU’s Skylake architecture was designed in 2014. Architecture is a significant factor in this style of engineering gymnastics to squeeze the most out of silicon.</div><br/></div></div><div id="39895043" class="c"><input type="checkbox" id="c-39895043" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39890570">root</a><span>|</span><a href="#39890805">parent</a><span>|</span><a href="#39891310">prev</a><span>|</span><a href="#39890886">next</a><span>|</span><label class="collapse" for="c-39895043">[-]</label><label class="expand" for="c-39895043">[1 more]</label></div><br/><div class="children"><div class="content">For LLMs...yeah. I imagine you&#x27;re measuring in tokens&#x2F;minute with that setup. So its possible, but...do you use it much? :)</div><br/></div></div></div></div></div></div><div id="39890886" class="c"><input type="checkbox" id="c-39890886" checked=""/><div class="controls bullet"><span class="by">luyu_wu</span><span>|</span><a href="#39890570">parent</a><span>|</span><a href="#39890653">prev</a><span>|</span><a href="#39891742">next</a><span>|</span><label class="collapse" for="c-39890886">[-]</label><label class="expand" for="c-39890886">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t believe that is the target for a local LLM... Pretty sure we&#x27;re talking about client-side computing, of which the newest supports only AVX-512 (and even that sketchily on Intel&#x27;s side).</div><br/></div></div><div id="39891742" class="c"><input type="checkbox" id="c-39891742" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39890570">parent</a><span>|</span><a href="#39890886">prev</a><span>|</span><a href="#39891196">next</a><span>|</span><label class="collapse" for="c-39891742">[-]</label><label class="expand" for="c-39891742">[1 more]</label></div><br/><div class="children"><div class="content">People with Sapphire Rapids options are not the target audience of these patches</div><br/></div></div><div id="39891196" class="c"><input type="checkbox" id="c-39891196" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#39890570">parent</a><span>|</span><a href="#39891742">prev</a><span>|</span><a href="#39893712">next</a><span>|</span><label class="collapse" for="c-39891196">[-]</label><label class="expand" for="c-39891196">[1 more]</label></div><br/><div class="children"><div class="content">Just buy a new AMD processor that supports AVX512.</div><br/></div></div></div></div><div id="39893712" class="c"><input type="checkbox" id="c-39893712" checked=""/><div class="controls bullet"><span class="by">s_Hogg</span><span>|</span><a href="#39890570">prev</a><span>|</span><a href="#39891285">next</a><span>|</span><label class="collapse" for="c-39893712">[-]</label><label class="expand" for="c-39893712">[15 more]</label></div><br/><div class="children"><div class="content">I&#x27;d pay good money to watch jart in conversation with Carmack</div><br/><div id="39900141" class="c"><input type="checkbox" id="c-39900141" checked=""/><div class="controls bullet"><span class="by">s_Hogg</span><span>|</span><a href="#39893712">parent</a><span>|</span><a href="#39899793">next</a><span>|</span><label class="collapse" for="c-39900141">[-]</label><label class="expand" for="c-39900141">[1 more]</label></div><br/><div class="children"><div class="content">To carry on, this is because they&#x27;re both very interested in &quot;knowledge in depth&quot;, rather than because of what they actually work on day-to-day. They&#x27;ve both made careers out of knowing what&#x27;s going on with the thing they&#x27;re building down to the most basic level possible.</div><br/></div></div><div id="39899793" class="c"><input type="checkbox" id="c-39899793" checked=""/><div class="controls bullet"><span class="by">objektif</span><span>|</span><a href="#39893712">parent</a><span>|</span><a href="#39900141">prev</a><span>|</span><a href="#39894208">next</a><span>|</span><label class="collapse" for="c-39899793">[-]</label><label class="expand" for="c-39899793">[2 more]</label></div><br/><div class="children"><div class="content">Why is he even relevant? What makes you believe that he would be good at solving AI related problems? He is a developer right?</div><br/><div id="39903151" class="c"><input type="checkbox" id="c-39903151" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#39893712">root</a><span>|</span><a href="#39899793">parent</a><span>|</span><a href="#39894208">next</a><span>|</span><label class="collapse" for="c-39903151">[-]</label><label class="expand" for="c-39903151">[1 more]</label></div><br/><div class="children"><div class="content">Actually he recently founded an AGI company. But this post is about optimization more than AI, which he is definitely good at (though he doesn&#x27;t claim to be the best).</div><br/></div></div></div></div><div id="39894208" class="c"><input type="checkbox" id="c-39894208" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#39893712">parent</a><span>|</span><a href="#39899793">prev</a><span>|</span><a href="#39891285">next</a><span>|</span><label class="collapse" for="c-39894208">[-]</label><label class="expand" for="c-39894208">[11 more]</label></div><br/><div class="children"><div class="content">Carmack is great but completely irrelevant here. He missed the entire AI&#x2F;LLM&#x2F;ML boat  to help Zuckerberg hawk virtual reality fantasies for years.</div><br/><div id="39895029" class="c"><input type="checkbox" id="c-39895029" checked=""/><div class="controls bullet"><span class="by">vinkelhake</span><span>|</span><a href="#39893712">root</a><span>|</span><a href="#39894208">parent</a><span>|</span><a href="#39895293">next</a><span>|</span><label class="collapse" for="c-39895029">[-]</label><label class="expand" for="c-39895029">[9 more]</label></div><br/><div class="children"><div class="content"><i>Completely irrelevant</i> is probably overstating it. He&#x27;s been working on AI for the last 4+ years.</div><br/><div id="39895273" class="c"><input type="checkbox" id="c-39895273" checked=""/><div class="controls bullet"><span class="by">cactusplant7374</span><span>|</span><a href="#39893712">root</a><span>|</span><a href="#39895029">parent</a><span>|</span><a href="#39895589">next</a><span>|</span><label class="collapse" for="c-39895273">[-]</label><label class="expand" for="c-39895273">[1 more]</label></div><br/><div class="children"><div class="content">He&#x27;s striving for AGI though, right? So he&#x27;s not really working on anything because he certainly hasn&#x27;t discovered AGI.</div><br/></div></div><div id="39895589" class="c"><input type="checkbox" id="c-39895589" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#39893712">root</a><span>|</span><a href="#39895029">parent</a><span>|</span><a href="#39895273">prev</a><span>|</span><a href="#39895293">next</a><span>|</span><label class="collapse" for="c-39895589">[-]</label><label class="expand" for="c-39895589">[7 more]</label></div><br/><div class="children"><div class="content">He literally squandered the last 10 years of his life working on <i>absolutely nothing</i> for Zuckerberg. And only after the rest of the world innovated on AI (transformers, etc) did he clearly feel embarrassed and had to proclaim he&#x27;s going to focus on AGI in a &quot;one-up&quot; way.</div><br/><div id="39896332" class="c"><input type="checkbox" id="c-39896332" checked=""/><div class="controls bullet"><span class="by">fkyoureadthedoc</span><span>|</span><a href="#39893712">root</a><span>|</span><a href="#39895589">parent</a><span>|</span><a href="#39896061">next</a><span>|</span><label class="collapse" for="c-39896332">[-]</label><label class="expand" for="c-39896332">[5 more]</label></div><br/><div class="children"><div class="content">He got paid a lot to do something he was presumably passionate about and enjoyed. It also might surprise you to find out that there&#x27;s quite a lot of people that just work as a means to an end, and find value and enjoyment primarily from other parts of their life.</div><br/><div id="39897455" class="c"><input type="checkbox" id="c-39897455" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#39893712">root</a><span>|</span><a href="#39896332">parent</a><span>|</span><a href="#39896061">next</a><span>|</span><label class="collapse" for="c-39897455">[-]</label><label class="expand" for="c-39897455">[4 more]</label></div><br/><div class="children"><div class="content">that&#x27;s great for him. i&#x27;m glad he enjoyed the $$$ playing with VR. that has nothing to do with my point about his irrelevance to this LLaMa discussion.</div><br/><div id="39899969" class="c"><input type="checkbox" id="c-39899969" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#39893712">root</a><span>|</span><a href="#39897455">parent</a><span>|</span><a href="#39896061">next</a><span>|</span><label class="collapse" for="c-39899969">[-]</label><label class="expand" for="c-39899969">[3 more]</label></div><br/><div class="children"><div class="content">He&#x27;s not irrelevant, though. Literally the first thing he did after leaving Meta was start an AI business, and the original point wasn&#x27;t even necessarily about AI. They just said they wanted to see two engineers in conversation, and you used it as an opportunity to denigrate one of their previous employers. <i>That&#x27;s</i> bewilderingly irrelevant.</div><br/><div id="39901455" class="c"><input type="checkbox" id="c-39901455" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#39893712">root</a><span>|</span><a href="#39899969">parent</a><span>|</span><a href="#39896061">next</a><span>|</span><label class="collapse" for="c-39901455">[-]</label><label class="expand" for="c-39901455">[2 more]</label></div><br/><div class="children"><div class="content">i can &quot;start an ai business&quot; too simply by incorporating.</div><br/><div id="39903161" class="c"><input type="checkbox" id="c-39903161" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#39893712">root</a><span>|</span><a href="#39901455">parent</a><span>|</span><a href="#39896061">next</a><span>|</span><label class="collapse" for="c-39903161">[-]</label><label class="expand" for="c-39903161">[1 more]</label></div><br/><div class="children"><div class="content">Can you raise $20m and get the likes of Rich Sutton to collaborate with you?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39896061" class="c"><input type="checkbox" id="c-39896061" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#39893712">root</a><span>|</span><a href="#39895589">parent</a><span>|</span><a href="#39896332">prev</a><span>|</span><a href="#39895293">next</a><span>|</span><label class="collapse" for="c-39896061">[-]</label><label class="expand" for="c-39896061">[1 more]</label></div><br/><div class="children"><div class="content">&gt; He literally squandered the last 10 years of his life working on absolutely nothing<p>Speak for yourself, the Oculus Quest is the coolest piece of sub-$500 tech in my home.</div><br/></div></div></div></div></div></div><div id="39895293" class="c"><input type="checkbox" id="c-39895293" checked=""/><div class="controls bullet"><span class="by">cactusplant7374</span><span>|</span><a href="#39893712">root</a><span>|</span><a href="#39894208">parent</a><span>|</span><a href="#39895029">prev</a><span>|</span><a href="#39891285">next</a><span>|</span><label class="collapse" for="c-39895293">[-]</label><label class="expand" for="c-39895293">[1 more]</label></div><br/><div class="children"><div class="content">Altman isn&#x27;t even relevant here. He is focusing on LLM&#x27;s instead of a framework that gets us to AGI. He can&#x27;t describe how we get there or any such theories around AGI. It&#x27;s a complete failure.</div><br/></div></div></div></div></div></div><div id="39891285" class="c"><input type="checkbox" id="c-39891285" checked=""/><div class="controls bullet"><span class="by">politelemon</span><span>|</span><a href="#39893712">prev</a><span>|</span><a href="#39893374">next</a><span>|</span><label class="collapse" for="c-39891285">[-]</label><label class="expand" for="c-39891285">[1 more]</label></div><br/><div class="children"><div class="content">This is great work. I&#x27;ve always thought it would be great if running LLM could be commoditized for regular average Joe hardware. I had thought that llamafile was like dockerfile for llama.cpp but looks like that&#x27;s a mistake?<p>Will definitely be giving this a try.</div><br/></div></div><div id="39893374" class="c"><input type="checkbox" id="c-39893374" checked=""/><div class="controls bullet"><span class="by">miki123211</span><span>|</span><a href="#39891285">prev</a><span>|</span><a href="#39891236">next</a><span>|</span><label class="collapse" for="c-39893374">[-]</label><label class="expand" for="c-39893374">[1 more]</label></div><br/><div class="children"><div class="content">If I&#x27;m reading the post correctly, Llamafile is faster than llama.cpp, despite the author upstreaming some of the changes. What&#x27;s the reason for this?</div><br/></div></div><div id="39891236" class="c"><input type="checkbox" id="c-39891236" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#39893374">prev</a><span>|</span><a href="#39894979">next</a><span>|</span><label class="collapse" for="c-39891236">[-]</label><label class="expand" for="c-39891236">[2 more]</label></div><br/><div class="children"><div class="content">Nice to see such speedups for CPUs. Are these changes available as a branch or pull request in llama.cpp itself?  I&#x27;d like to make use of them in that form if possible (as I&#x27;m used to using that).</div><br/><div id="39891814" class="c"><input type="checkbox" id="c-39891814" checked=""/><div class="controls bullet"><span class="by">dagaci</span><span>|</span><a href="#39891236">parent</a><span>|</span><a href="#39894979">next</a><span>|</span><label class="collapse" for="c-39891814">[-]</label><label class="expand" for="c-39891814">[1 more]</label></div><br/><div class="children"><div class="content">Yes, this is really a phenomenal effort! And what open source is about: Bringing improvements to so many use cases. So that Intel and AMD chip uses can start to perform while taking advantage of their high-performance capabilities, making even old parts competitive.<p>There are two PRs raised to merge to llama.cpp:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;6414">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;6414</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;6412">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;6412</a><p>Hopefully these can be accepted, without drama! as there are many downstream dependencies on llama.cpp can will also benefit.<p>Though of course everyone should also look directly at releases from llamafile <a href="https:&#x2F;&#x2F;github.com&#x2F;mozilla-Ocho&#x2F;llamafile">https:&#x2F;&#x2F;github.com&#x2F;mozilla-Ocho&#x2F;llamafile</a>.</div><br/></div></div></div></div><div id="39894979" class="c"><input type="checkbox" id="c-39894979" checked=""/><div class="controls bullet"><span class="by">hrkfmud50k</span><span>|</span><a href="#39891236">prev</a><span>|</span><a href="#39890621">next</a><span>|</span><label class="collapse" for="c-39894979">[-]</label><label class="expand" for="c-39894979">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s clearly optimal since my CPU is listed as only being capable of going 780 gigaflops<p>780 GFLOP is the iGPU spec.  Is this a valid comparison?<p><a href="https:&#x2F;&#x2F;nanoreview.net&#x2F;en&#x2F;cpu&#x2F;intel-core-i9-14900k" rel="nofollow">https:&#x2F;&#x2F;nanoreview.net&#x2F;en&#x2F;cpu&#x2F;intel-core-i9-14900k</a></div><br/></div></div><div id="39890621" class="c"><input type="checkbox" id="c-39890621" checked=""/><div class="controls bullet"><span class="by">aniijbod</span><span>|</span><a href="#39894979">prev</a><span>|</span><a href="#39892257">next</a><span>|</span><label class="collapse" for="c-39890621">[-]</label><label class="expand" for="c-39890621">[4 more]</label></div><br/><div class="children"><div class="content">A way of thinking about what&#x27;s inside any of the top LLMs right now: even if they never learn another single fact, even if they get ridiculously out of date as a result, even if they are even more riddled with errors and prone to biases than we know them to be, even if they are as prone to hallucinations as we know they they are and they never develop the capacity to cure themselves of this, they are more knowledgeable and capable of more reasoned response, despite their capacity for error, to more questions than any single human being that has ever lived.</div><br/><div id="39891806" class="c"><input type="checkbox" id="c-39891806" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890621">parent</a><span>|</span><a href="#39896079">next</a><span>|</span><label class="collapse" for="c-39891806">[-]</label><label class="expand" for="c-39891806">[1 more]</label></div><br/><div class="children"><div class="content">We shouldn&#x27;t choose LLMs for how many facts they support, but their capability to process human language. There is some overlap between these two though, but an LLM that just doesn&#x27;t know something can always be augmented with RAG capabilities.</div><br/></div></div><div id="39896079" class="c"><input type="checkbox" id="c-39896079" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#39890621">parent</a><span>|</span><a href="#39891806">prev</a><span>|</span><a href="#39890630">next</a><span>|</span><label class="collapse" for="c-39896079">[-]</label><label class="expand" for="c-39896079">[1 more]</label></div><br/><div class="children"><div class="content">If you ignore my capacity for error, I bet I&#x27;d put up a good score too. Hell, maybe Markov chains are smarter than LLMs by this definition.</div><br/></div></div><div id="39890630" class="c"><input type="checkbox" id="c-39890630" checked=""/><div class="controls bullet"><span class="by">JKCalhoun</span><span>|</span><a href="#39890621">parent</a><span>|</span><a href="#39896079">prev</a><span>|</span><a href="#39892257">next</a><span>|</span><label class="collapse" for="c-39890630">[-]</label><label class="expand" for="c-39890630">[1 more]</label></div><br/><div class="children"><div class="content">Picturing &quot;LLM Jeopardy&quot;. You know, a game show.</div><br/></div></div></div></div><div id="39892257" class="c"><input type="checkbox" id="c-39892257" checked=""/><div class="controls bullet"><span class="by">mijoharas</span><span>|</span><a href="#39890621">prev</a><span>|</span><a href="#39892181">next</a><span>|</span><label class="collapse" for="c-39892257">[-]</label><label class="expand" for="c-39892257">[4 more]</label></div><br/><div class="children"><div class="content">Has Justine written anywhere about her disassembly setup?<p>&gt; I configured Emacs so I can push a button, and the disassembly for the C++ code I&#x27;m working on will pop up on the screen in a few milliseconds.<p>I assume it&#x27;s something project specific rather than being able to get the disassembly for an arbitrary section of code or something?<p>It seems very handy, so I&#x27;d love to see the implementation (I couldn&#x27;t find anything googling)</div><br/><div id="39892954" class="c"><input type="checkbox" id="c-39892954" checked=""/><div class="controls bullet"><span class="by">pelletier</span><span>|</span><a href="#39892257">parent</a><span>|</span><a href="#39892181">next</a><span>|</span><label class="collapse" for="c-39892954">[-]</label><label class="expand" for="c-39892954">[3 more]</label></div><br/><div class="children"><div class="content">This is probably what they are referring to <a href="https:&#x2F;&#x2F;github.com&#x2F;jart&#x2F;disaster">https:&#x2F;&#x2F;github.com&#x2F;jart&#x2F;disaster</a></div><br/><div id="39896815" class="c"><input type="checkbox" id="c-39896815" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#39892257">root</a><span>|</span><a href="#39892954">parent</a><span>|</span><a href="#39894586">next</a><span>|</span><label class="collapse" for="c-39896815">[-]</label><label class="expand" for="c-39896815">[1 more]</label></div><br/><div class="children"><div class="content">Nice. I have been using rmsbolt for a similar feature, but it is very rough. I&#x27;ll need to give this as try.</div><br/></div></div><div id="39894586" class="c"><input type="checkbox" id="c-39894586" checked=""/><div class="controls bullet"><span class="by">mijoharas</span><span>|</span><a href="#39892257">root</a><span>|</span><a href="#39892954">parent</a><span>|</span><a href="#39896815">prev</a><span>|</span><a href="#39892181">next</a><span>|</span><label class="collapse" for="c-39894586">[-]</label><label class="expand" for="c-39894586">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! I need to get better at googling I guess.</div><br/></div></div></div></div></div></div><div id="39892181" class="c"><input type="checkbox" id="c-39892181" checked=""/><div class="controls bullet"><span class="by">isusmelj</span><span>|</span><a href="#39892257">prev</a><span>|</span><a href="#39897250">next</a><span>|</span><label class="collapse" for="c-39892181">[-]</label><label class="expand" for="c-39892181">[1 more]</label></div><br/><div class="children"><div class="content">Is there somewhere an overview of the progress we made on the software side for training and inference of LLMs? It feels like we squeezed 10-100x more out of the hardware since llama appeared. This crazy progress will probably saturate though as we reach theoretical limits, no?</div><br/></div></div><div id="39897250" class="c"><input type="checkbox" id="c-39897250" checked=""/><div class="controls bullet"><span class="by">rbnsl</span><span>|</span><a href="#39892181">prev</a><span>|</span><a href="#39890495">next</a><span>|</span><label class="collapse" for="c-39897250">[-]</label><label class="expand" for="c-39897250">[1 more]</label></div><br/><div class="children"><div class="content">Definitely wild we’re in the timeline you can run a 1.1 bn param model on a raspberry pi, but its still tough to justify because the 1.1 is kinda useless compared to the beefier models. Sick for home builds&#x2F;hobbyists though I might wanna get one of the new Pis just to try this out</div><br/></div></div><div id="39890495" class="c"><input type="checkbox" id="c-39890495" checked=""/><div class="controls bullet"><span class="by">1-6</span><span>|</span><a href="#39897250">prev</a><span>|</span><a href="#39890826">next</a><span>|</span><label class="collapse" for="c-39890495">[-]</label><label class="expand" for="c-39890495">[21 more]</label></div><br/><div class="children"><div class="content">Question is, how much of an improvement has it gotten to over a GPU or ASIC?</div><br/><div id="39891224" class="c"><input type="checkbox" id="c-39891224" checked=""/><div class="controls bullet"><span class="by">gpapilion</span><span>|</span><a href="#39890495">parent</a><span>|</span><a href="#39890904">next</a><span>|</span><label class="collapse" for="c-39891224">[-]</label><label class="expand" for="c-39891224">[3 more]</label></div><br/><div class="children"><div class="content">So... I was struggling with this for a while. I would says anywhere from 2x to an order of magnitude faster with a GPU. (I&#x27;ve been looking at a lot of GPU benchmarks lately, and they are REALLY hard to compare since they are all so specific)<p>I do think long term there gets to be more hope for CPUs here with inference largely because memory bandwidth becomes more important than the gpu. You can see this with reports of the MI-300 series outperforming h100, largely because it has more memory bandwidth. MCR dimms give you close to 2x the exiting memory bw in intel cpus, and when coupled with AMX you may be able to exceed v100 and might touch a100 performance levels.<p>HBM and the general GPU architecture gives it a huge memory advantage, especially with the chip to chip interface. Even adding HBM to a CPU, you are likely to find the CPU is unable to use the memory bw effectively unless it was specifically designed to use it. Then you&#x27;d still likely have limited performance with things like UPI being a really ugly bottleneck between CPUs.</div><br/><div id="39891871" class="c"><input type="checkbox" id="c-39891871" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39891224">parent</a><span>|</span><a href="#39890904">next</a><span>|</span><label class="collapse" for="c-39891871">[-]</label><label class="expand" for="c-39891871">[2 more]</label></div><br/><div class="children"><div class="content">If someone releases DDR5 or DDR6 based PIM, then most of the memory bandwidth advantage of GPUs evaporates overnight. I expect CPUs to be king at inference in the future.</div><br/><div id="39891973" class="c"><input type="checkbox" id="c-39891973" checked=""/><div class="controls bullet"><span class="by">gpapilion</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39891871">parent</a><span>|</span><a href="#39890904">next</a><span>|</span><label class="collapse" for="c-39891973">[-]</label><label class="expand" for="c-39891973">[1 more]</label></div><br/><div class="children"><div class="content">But then you&#x27;ll get GDDR6 delivered via HBM5 or whatever. I don&#x27;t think CPUs will ever really keep up with the memory bandwidth, because for most applications it doesn&#x27;t matter.<p>MCR DIMM is like 1&#x2F;2 the memory bandwidth that is possible with HBM4, plus it requires you to buy something like 2TB of memory. It might get there, but I&#x27;d keep my money on hbm and gpus.</div><br/></div></div></div></div></div></div><div id="39890904" class="c"><input type="checkbox" id="c-39890904" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#39890495">parent</a><span>|</span><a href="#39891224">prev</a><span>|</span><a href="#39890616">next</a><span>|</span><label class="collapse" for="c-39890904">[-]</label><label class="expand" for="c-39890904">[2 more]</label></div><br/><div class="children"><div class="content">I think that should be phrased more like &quot;what fraction of GPU speed can this reach?&quot;, because it&#x27;ll always be less than 1x.</div><br/></div></div><div id="39890616" class="c"><input type="checkbox" id="c-39890616" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39890495">parent</a><span>|</span><a href="#39890904">prev</a><span>|</span><a href="#39891758">next</a><span>|</span><label class="collapse" for="c-39890616">[-]</label><label class="expand" for="c-39890616">[13 more]</label></div><br/><div class="children"><div class="content">Nothing in software will ever beat an equivalent ASIC.</div><br/><div id="39890932" class="c"><input type="checkbox" id="c-39890932" checked=""/><div class="controls bullet"><span class="by">postalrat</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39890616">parent</a><span>|</span><a href="#39891087">next</a><span>|</span><label class="collapse" for="c-39890932">[-]</label><label class="expand" for="c-39890932">[2 more]</label></div><br/><div class="children"><div class="content">Sure there is. Software is easy to change.</div><br/><div id="39893306" class="c"><input type="checkbox" id="c-39893306" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39890932">parent</a><span>|</span><a href="#39891087">next</a><span>|</span><label class="collapse" for="c-39893306">[-]</label><label class="expand" for="c-39893306">[1 more]</label></div><br/><div class="children"><div class="content">By “beat” I meant in performance.<p>Obviously you can’t change an asic</div><br/></div></div></div></div><div id="39891087" class="c"><input type="checkbox" id="c-39891087" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39890616">parent</a><span>|</span><a href="#39890932">prev</a><span>|</span><a href="#39891671">next</a><span>|</span><label class="collapse" for="c-39891087">[-]</label><label class="expand" for="c-39891087">[2 more]</label></div><br/><div class="children"><div class="content">an asic is fixed function, so it&#x27;ll never be able to boot my pc and then be the CPU, even though an asic beats the pants off anything else computing Sha hashes for Bitcoin mining.</div><br/><div id="39893297" class="c"><input type="checkbox" id="c-39893297" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39891087">parent</a><span>|</span><a href="#39891671">next</a><span>|</span><label class="collapse" for="c-39893297">[-]</label><label class="expand" for="c-39893297">[1 more]</label></div><br/><div class="children"><div class="content">By “beat” I meant performance.<p>Obviously an ASIC is not a general purpose machine like a cpu.</div><br/></div></div></div></div><div id="39891671" class="c"><input type="checkbox" id="c-39891671" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39890616">parent</a><span>|</span><a href="#39891087">prev</a><span>|</span><a href="#39891758">next</a><span>|</span><label class="collapse" for="c-39891671">[-]</label><label class="expand" for="c-39891671">[8 more]</label></div><br/><div class="children"><div class="content">Most ASICs are cost or power optimizations.</div><br/><div id="39893302" class="c"><input type="checkbox" id="c-39893302" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39891671">parent</a><span>|</span><a href="#39891758">next</a><span>|</span><label class="collapse" for="c-39893302">[-]</label><label class="expand" for="c-39893302">[7 more]</label></div><br/><div class="children"><div class="content">Exactly. They’re much faster for their specific tasks and thus are more power efficient and potentially cost efficient</div><br/><div id="39894362" class="c"><input type="checkbox" id="c-39894362" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39893302">parent</a><span>|</span><a href="#39891758">next</a><span>|</span><label class="collapse" for="c-39894362">[-]</label><label class="expand" for="c-39894362">[6 more]</label></div><br/><div class="children"><div class="content">No. Eg of the hardware discussed on the article, the Raspberry Pi uses an ASIC that&#x27;s slow, cheap and low power vs the Intel or AMD chips.<p>In some cases ASICs are faster than general purpouse CPUs, but usually not.</div><br/><div id="39901714" class="c"><input type="checkbox" id="c-39901714" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39894362">parent</a><span>|</span><a href="#39896161">next</a><span>|</span><label class="collapse" for="c-39901714">[-]</label><label class="expand" for="c-39901714">[1 more]</label></div><br/><div class="children"><div class="content">I think we’re saying the same thing here.<p>You can make an ASIC which doesn’t have the same power draw as a CPU, but provides the same performance.<p>It doesn’t need to be faster than the fastest software implementation, but power per performance will always favor ASIC.</div><br/></div></div><div id="39896161" class="c"><input type="checkbox" id="c-39896161" checked=""/><div class="controls bullet"><span class="by">LtdJorge</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39894362">parent</a><span>|</span><a href="#39901714">prev</a><span>|</span><a href="#39891758">next</a><span>|</span><label class="collapse" for="c-39896161">[-]</label><label class="expand" for="c-39896161">[4 more]</label></div><br/><div class="children"><div class="content">Is the LLM running on an ASIC for the Pi here? I dout it.</div><br/><div id="39902652" class="c"><input type="checkbox" id="c-39902652" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39896161">parent</a><span>|</span><a href="#39891758">next</a><span>|</span><label class="collapse" for="c-39902652">[-]</label><label class="expand" for="c-39902652">[3 more]</label></div><br/><div class="children"><div class="content">The ARM cores and the VideoCore parts are all in the ASIC, it&#x27;s a SoC type ASIC.</div><br/><div id="39903079" class="c"><input type="checkbox" id="c-39903079" checked=""/><div class="controls bullet"><span class="by">LtdJorge</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39902652">parent</a><span>|</span><a href="#39891758">next</a><span>|</span><label class="collapse" for="c-39903079">[-]</label><label class="expand" for="c-39903079">[2 more]</label></div><br/><div class="children"><div class="content">Hmm, yeah it&#x27;s a SoC, but not an ASIC. Maybe you mean APU? ASICs are circuits thst can only do one thing, CPU cores are definitely not that.<p>Edit: an example ASIC the Pi has is the video encoder&#x2F;decoder, with JPEG also supported. I think it&#x27;s embedded in the GPU.</div><br/><div id="39903262" class="c"><input type="checkbox" id="c-39903262" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39903079">parent</a><span>|</span><a href="#39891758">next</a><span>|</span><label class="collapse" for="c-39903262">[-]</label><label class="expand" for="c-39903262">[1 more]</label></div><br/><div class="children"><div class="content">ASIC just means it&#x27;s an application specific IC (= chip), meaning it&#x27;s fabbed for that specific product (like in this case the Raspberry Pi). A functional block like a JPEG codec contained therein is not an ASIC. Quoth wikipedia:<p>&quot;Modern ASICs often include entire microprocessors, memory blocks including ROM, RAM, EEPROM, flash memory and other large building blocks. Such an ASIC is often termed a SoC (system-on-chip).&quot;<p>What you&#x27;re describing in the JPEG codec might be termed a fixed function IP block in semiconductor design terminology.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39891758" class="c"><input type="checkbox" id="c-39891758" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39890495">parent</a><span>|</span><a href="#39890616">prev</a><span>|</span><a href="#39892763">next</a><span>|</span><label class="collapse" for="c-39891758">[-]</label><label class="expand" for="c-39891758">[1 more]</label></div><br/><div class="children"><div class="content">From the article, passage about the 14900k:<p>&gt; For example, when I run my spam.sh shell script, it only takes 420 milliseconds, which is 7x faster than my Raspberry Pi 5. That&#x27;s right, when it comes to small workloads, this chip is able to finish before CUDA even gets started.<p>So… it depends :)</div><br/></div></div><div id="39892763" class="c"><input type="checkbox" id="c-39892763" checked=""/><div class="controls bullet"><span class="by">jchw</span><span>|</span><a href="#39890495">parent</a><span>|</span><a href="#39891758">prev</a><span>|</span><a href="#39890826">next</a><span>|</span><label class="collapse" for="c-39892763">[-]</label><label class="expand" for="c-39892763">[1 more]</label></div><br/><div class="children"><div class="content">I think I understand what you are thinking. You may be fixing &quot;than other ways of running them&quot; to the end of the title, but it&#x27;s actually &quot;than it was on CPU before now&quot;.</div><br/></div></div></div></div><div id="39890826" class="c"><input type="checkbox" id="c-39890826" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#39890495">prev</a><span>|</span><a href="#39891055">next</a><span>|</span><label class="collapse" for="c-39890826">[-]</label><label class="expand" for="c-39890826">[3 more]</label></div><br/><div class="children"><div class="content">Is it easy to find where the matvecs are, in LLaMA (if you are someone who  is curious and wants to poke around at the “engine” without understanding the “transmission,” so to speak)? I was hoping to mess around with this for Stable Diffusion, but it seemed like they were buried under quite a few layers of indirection. Which is entirely reasonable, the goal is to ship software, not satisfy people who’d just want to poke things and see what happens, haha.</div><br/><div id="39891042" class="c"><input type="checkbox" id="c-39891042" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890826">parent</a><span>|</span><a href="#39891055">next</a><span>|</span><label class="collapse" for="c-39891042">[-]</label><label class="expand" for="c-39891042">[2 more]</label></div><br/><div class="children"><div class="content">did you see tiny grad can run llama and stable diffusion? it&#x27;s an intentionally extremely simple framework vs pytorch or even micrograd, which helped me dig into the underlying math. though <a href="https:&#x2F;&#x2F;spreadsheets-are-all-you-need.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;spreadsheets-are-all-you-need.ai&#x2F;</a> is a good one for learning LLMs.</div><br/><div id="39891060" class="c"><input type="checkbox" id="c-39891060" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#39890826">root</a><span>|</span><a href="#39891042">parent</a><span>|</span><a href="#39891055">next</a><span>|</span><label class="collapse" for="c-39891060">[-]</label><label class="expand" for="c-39891060">[1 more]</label></div><br/><div class="children"><div class="content">I haven’t seen that. I’ll definitely have to take a look, thanks!</div><br/></div></div></div></div></div></div><div id="39891055" class="c"><input type="checkbox" id="c-39891055" checked=""/><div class="controls bullet"><span class="by">Ono-Sendai</span><span>|</span><a href="#39890826">prev</a><span>|</span><a href="#39895398">next</a><span>|</span><label class="collapse" for="c-39891055">[-]</label><label class="expand" for="c-39891055">[3 more]</label></div><br/><div class="children"><div class="content">Multithreading support in llama.cpp is probably still pretty busted, assuming it uses the same underlying NN inference code as whisper.cpp: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;whisper.cpp&#x2F;issues&#x2F;200#issuecomment-1484025515">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;whisper.cpp&#x2F;issues&#x2F;200#issuecom...</a></div><br/><div id="39891848" class="c"><input type="checkbox" id="c-39891848" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#39891055">parent</a><span>|</span><a href="#39895398">next</a><span>|</span><label class="collapse" for="c-39891848">[-]</label><label class="expand" for="c-39891848">[2 more]</label></div><br/><div class="children"><div class="content">From what I have heard they use manual spin locks. Generally, spin locks are not a good idea unless you want to dedicate the entire machine to a single application. If the process a spinlock waits on gets suspended, you&#x27;re burning CPU time for nothing. The OS thinks a spinlock making zero progress is actually a high priority process, so it is starving the suspended process from making progress.</div><br/><div id="39893669" class="c"><input type="checkbox" id="c-39893669" checked=""/><div class="controls bullet"><span class="by">Ono-Sendai</span><span>|</span><a href="#39891055">root</a><span>|</span><a href="#39891848">parent</a><span>|</span><a href="#39895398">next</a><span>|</span><label class="collapse" for="c-39893669">[-]</label><label class="expand" for="c-39893669">[1 more]</label></div><br/><div class="children"><div class="content">Yeah the code looks like a spinlock.  It behaves terribly under contention, resulting in performance falling off a cliff as the number of threads increases.
Adding more threads actually slows down the total performance.<p>I would fix it if I could be bothered.  Instead I will just use the Cuda whisper backend which is pretty nice and fast.</div><br/></div></div></div></div></div></div><div id="39895398" class="c"><input type="checkbox" id="c-39895398" checked=""/><div class="controls bullet"><span class="by">arendtio</span><span>|</span><a href="#39891055">prev</a><span>|</span><a href="#39890514">next</a><span>|</span><label class="collapse" for="c-39895398">[-]</label><label class="expand" for="c-39895398">[2 more]</label></div><br/><div class="children"><div class="content">Does someone else see llamafile using Wine on Linux?<p>Edit: After the download I did a simple chmod +x llava-v1.5-7b-q4.llamafile; .&#x2F;llava-v1.5-7b-q4.llamafile</div><br/><div id="39896987" class="c"><input type="checkbox" id="c-39896987" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39895398">parent</a><span>|</span><a href="#39890514">next</a><span>|</span><label class="collapse" for="c-39896987">[-]</label><label class="expand" for="c-39896987">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a simple fix for that.<p><pre><code>    sudo wget -O &#x2F;usr&#x2F;bin&#x2F;ape https:&#x2F;&#x2F;cosmo.zip&#x2F;pub&#x2F;cosmos&#x2F;bin&#x2F;ape-$(uname -m).elf
    sudo chmod +x &#x2F;usr&#x2F;bin&#x2F;ape
    sudo sh -c &quot;echo &#x27;:APE:M::MZqFpD::&#x2F;usr&#x2F;bin&#x2F;ape:&#x27; &gt;&#x2F;proc&#x2F;sys&#x2F;fs&#x2F;binfmt_misc&#x2F;register&quot;
    sudo sh -c &quot;echo &#x27;:APE-jart:M::jartsr::&#x2F;usr&#x2F;bin&#x2F;ape:&#x27; &gt;&#x2F;proc&#x2F;sys&#x2F;fs&#x2F;binfmt_misc&#x2F;register&quot;
</code></pre>
<a href="https:&#x2F;&#x2F;github.com&#x2F;mozilla-ocho&#x2F;llamafile&#x2F;?tab=readme-ov-file#gotchas">https:&#x2F;&#x2F;github.com&#x2F;mozilla-ocho&#x2F;llamafile&#x2F;?tab=readme-ov-fil...</a></div><br/></div></div></div></div><div id="39890514" class="c"><input type="checkbox" id="c-39890514" checked=""/><div class="controls bullet"><span class="by">discordance</span><span>|</span><a href="#39895398">prev</a><span>|</span><a href="#39891500">next</a><span>|</span><label class="collapse" for="c-39890514">[-]</label><label class="expand" for="c-39890514">[5 more]</label></div><br/><div class="children"><div class="content">&quot;As for disk speed, dd if=&#x2F;dev&#x2F;zero of=&#x2F;tmp&#x2F;output bs=128k count=50k; rm -f &#x2F;tmp&#x2F;output reports 1.6 GB&#x2F;s which is 3.6x slower than my Mac Studio, and 3x slower than my Intel (which has the same M.2 stick). I&#x27;m told that Intel and Apple are just better at this, but I wish I understood why. &quot;<p>Can anyone here answer why this is?</div><br/><div id="39890752" class="c"><input type="checkbox" id="c-39890752" checked=""/><div class="controls bullet"><span class="by">pstrateman</span><span>|</span><a href="#39890514">parent</a><span>|</span><a href="#39890853">next</a><span>|</span><label class="collapse" for="c-39890752">[-]</label><label class="expand" for="c-39890752">[1 more]</label></div><br/><div class="children"><div class="content">Apple made fsync a noop.<p>You have to make a different call to get sync on macos.<p>So tons is stuff is faster because it&#x27;s not actually writing to disk.</div><br/></div></div><div id="39890853" class="c"><input type="checkbox" id="c-39890853" checked=""/><div class="controls bullet"><span class="by">bishfish</span><span>|</span><a href="#39890514">parent</a><span>|</span><a href="#39890752">prev</a><span>|</span><a href="#39891500">next</a><span>|</span><label class="collapse" for="c-39890853">[-]</label><label class="expand" for="c-39890853">[3 more]</label></div><br/><div class="children"><div class="content">Plus he isn’t using oflag=direct, so since output file is small it isn’t even making it to disk.  I think it would only be sent to page cache.  I’m afraid he is testing CPU and memory (bus) speeds here.<p>oflag=direct will write direct and bypass page cache.</div><br/><div id="39891037" class="c"><input type="checkbox" id="c-39891037" checked=""/><div class="controls bullet"><span class="by">fweimer</span><span>|</span><a href="#39890514">root</a><span>|</span><a href="#39890853">parent</a><span>|</span><a href="#39901061">next</a><span>|</span><label class="collapse" for="c-39891037">[-]</label><label class="expand" for="c-39891037">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. Something is very fishy if this system only writes 1.6 GB&#x2F;s to the page cache. Probably that dd command line quoted in the article is incomplete.</div><br/></div></div><div id="39901061" class="c"><input type="checkbox" id="c-39901061" checked=""/><div class="controls bullet"><span class="by">otterley</span><span>|</span><a href="#39890514">root</a><span>|</span><a href="#39890853">parent</a><span>|</span><a href="#39891037">prev</a><span>|</span><a href="#39891500">next</a><span>|</span><label class="collapse" for="c-39901061">[-]</label><label class="expand" for="c-39901061">[1 more]</label></div><br/><div class="children"><div class="content">*she</div><br/></div></div></div></div></div></div><div id="39891500" class="c"><input type="checkbox" id="c-39891500" checked=""/><div class="controls bullet"><span class="by">seangrogg</span><span>|</span><a href="#39890514">prev</a><span>|</span><a href="#39897844">next</a><span>|</span><label class="collapse" for="c-39891500">[-]</label><label class="expand" for="c-39891500">[2 more]</label></div><br/><div class="children"><div class="content">Mmm, I wonder how well this would work on a mobile device. Maybe I&#x27;ll try grabbing my ubuntu touch here in a sec...</div><br/><div id="39892950" class="c"><input type="checkbox" id="c-39892950" checked=""/><div class="controls bullet"><span class="by">seangrogg</span><span>|</span><a href="#39891500">parent</a><span>|</span><a href="#39897844">next</a><span>|</span><label class="collapse" for="c-39892950">[-]</label><label class="expand" for="c-39892950">[1 more]</label></div><br/><div class="children"><div class="content">(For any who were curious: it does not for memory reasons)</div><br/></div></div></div></div><div id="39897844" class="c"><input type="checkbox" id="c-39897844" checked=""/><div class="controls bullet"><span class="by">JohnnyHerz</span><span>|</span><a href="#39891500">prev</a><span>|</span><a href="#39892987">next</a><span>|</span><label class="collapse" for="c-39897844">[-]</label><label class="expand" for="c-39897844">[1 more]</label></div><br/><div class="children"><div class="content">Awesomeness.
thank you for sharing!</div><br/></div></div><div id="39892987" class="c"><input type="checkbox" id="c-39892987" checked=""/><div class="controls bullet"><span class="by">6r17</span><span>|</span><a href="#39897844">prev</a><span>|</span><label class="collapse" for="c-39892987">[-]</label><label class="expand" for="c-39892987">[1 more]</label></div><br/><div class="children"><div class="content">today being today ; I must ask ; anyone has actually tried this ?</div><br/></div></div></div></div></div></div></div></body></html>