<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1711962074806" as="style"/><link rel="stylesheet" href="styles.css?v=1711962074806"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://justine.lol/matmul/">LLaMA Now Goes Faster on CPUs</a>Â <span class="domain">(<a href="https://justine.lol">justine.lol</a>)</span></div><div class="subtext"><span>lawrencechen</span> | <span>161 comments</span></div><br/><div><div id="39891772" class="c"><input type="checkbox" id="c-39891772" checked=""/><div class="controls bullet"><span class="by">speps</span><span>|</span><a href="#39890454">next</a><span>|</span><label class="collapse" for="c-39891772">[-]</label><label class="expand" for="c-39891772">[3 more]</label></div><br/><div class="children"><div class="content">Regarding this bit at the end:<p>&gt; I learned how to write math kernels by renting Vast VMs and watching Gautham Venkatasubramanian and mrdomino develop CUDA kernels in a tmux session. They&#x27;ve been focusing on solving a much more important challenge for llamafile, which is helping it not have a mandatory dependency on the cuBLAS<p>If I&#x27;m reading this right, they&#x27;re trying to rewrite cuBLAS within CUDA itself. I&#x27;m guessing the next step would be removing CUDA dependency and go with directly using Vulkan or Metal compute shaders. Am I correct?</div><br/><div id="39891952" class="c"><input type="checkbox" id="c-39891952" checked=""/><div class="controls bullet"><span class="by">larodi</span><span>|</span><a href="#39891772">parent</a><span>|</span><a href="#39891938">next</a><span>|</span><label class="collapse" for="c-39891952">[-]</label><label class="expand" for="c-39891952">[1 more]</label></div><br/><div class="children"><div class="content">llama.cpp (or rather G.Gerganov et. al.) are trying to avoid cuBLAS entirely, using ins own kernels. not sure how jart&#x27;s effort relates, and whether jart intends to upstream these into llama.cpp which seems to still be the underlying tech behind the llamafile.</div><br/></div></div><div id="39891938" class="c"><input type="checkbox" id="c-39891938" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#39891772">parent</a><span>|</span><a href="#39891952">prev</a><span>|</span><a href="#39890454">next</a><span>|</span><label class="collapse" for="c-39891938">[-]</label><label class="expand" for="c-39891938">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but none of these have performance portability across GPU vendors, so it&#x27;s probably seen as pointless. You would need an AMD Vulkan shader, an nvidia one, and intel one, etc. It&#x27;s not like C code on CPUs.</div><br/></div></div></div></div><div id="39890454" class="c"><input type="checkbox" id="c-39890454" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#39891772">prev</a><span>|</span><a href="#39890694">next</a><span>|</span><label class="collapse" for="c-39890454">[-]</label><label class="expand" for="c-39890454">[95 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s a good idea for everyone to download and be able to run a LLM locally, even if you have the minimum of requirements. As a pseudo-backup of a large chunk of human knowledge.</div><br/><div id="39890656" class="c"><input type="checkbox" id="c-39890656" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890615">next</a><span>|</span><label class="collapse" for="c-39890656">[-]</label><label class="expand" for="c-39890656">[29 more]</label></div><br/><div class="children"><div class="content">I strongly recommend that people run LLMs locally for a different reason.<p>The ones you can run on your own machine tend to be bad - really bad. They hallucinate wildly and fail at all sorts of tasks that the larger hosted ones succeed at.<p>This makes them a fantastic tool for learning more about how LLMs work and what they&#x27;re useful for. Interacting with a weak-but-functional LLM that runs on your own computer is a great way to get a much more solid mental model for what these things actually are.</div><br/><div id="39891030" class="c"><input type="checkbox" id="c-39891030" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39890763">next</a><span>|</span><label class="collapse" for="c-39891030">[-]</label><label class="expand" for="c-39891030">[11 more]</label></div><br/><div class="children"><div class="content">The other reason is to find out what a detuned model is capable of. The canonical example is how to make cocaine, which ChatGPT will admonish you for even asking, while llama2-uncensored will happily describe the process which is only really interesting if you&#x27;re an amateur chemist and want to be Scarface-that-knocks. (the recipe is relatively easy, it&#x27;s getting access to the raw ingredients that&#x27;s the hard part, same as with nukes.)<p>if you accidentally use the word&quot;hack&quot; when trying to get ChatGPT to write some code for you. it&#x27;ll stop and tell you that hacking is bad, and not a colloquial expression, and refuse to go further.<p>privacy reasons are another reason to try a local LLM. for the extremely paranoid (justified or not), a local LLM gives users a place to ask questions without the text being fed to a server somewhere for later lawsuit discovery (Google searches are routinely subpoenaed, it&#x27;s only a matter of time until ChatGPT chats are as well.)<p>There&#x27;s an uncensored model for vision available as well. The censored vision models won&#x27;t play the shallow game of hot or not with you.<p>There are uncensored image generation models as well, but, ah, those are NSFW and not for polite company. (As well as there&#x27;s multiple thesis&#x27; worth of content on what that&#x27;ll do to society.)</div><br/><div id="39891350" class="c"><input type="checkbox" id="c-39891350" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891030">parent</a><span>|</span><a href="#39891932">next</a><span>|</span><label class="collapse" for="c-39891350">[-]</label><label class="expand" for="c-39891350">[9 more]</label></div><br/><div class="children"><div class="content">&gt; if you accidentally use the word&quot;hack&quot; when trying to get ChatGPT to write some code for you. it&#x27;ll stop and tell you that hacking is bad, and not a colloquial expression, and refuse to go further.<p>Is that 3.5 or 4? I asked 4 for an example of code which &quot;is a hack&quot;, it misunderstood me as asking for hacking code rather than buggy code, but then it did actually answer on the first try.<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;ca2c320c-f4ba-41bf-8f40-f7faf26a4379" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;ca2c320c-f4ba-41bf-8f40-f7faf2...</a></div><br/><div id="39891468" class="c"><input type="checkbox" id="c-39891468" checked=""/><div class="controls bullet"><span class="by">semi-extrinsic</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891350">parent</a><span>|</span><a href="#39891452">next</a><span>|</span><label class="collapse" for="c-39891468">[-]</label><label class="expand" for="c-39891468">[7 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t use LLMs for my coding, I manage just fine with LSP and Treesitter. So genuine question: is that answer representative of the output quality of these things? Because both answers are pretty crappy and assume the user has already done the difficult things, and is asking for help on the easy things.</div><br/><div id="39891538" class="c"><input type="checkbox" id="c-39891538" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891468">parent</a><span>|</span><a href="#39892000">next</a><span>|</span><label class="collapse" for="c-39891538">[-]</label><label class="expand" for="c-39891538">[1 more]</label></div><br/><div class="children"><div class="content">The response seems pretty reasonable; it&#x27;s answering the question it was asked. If you want to ask it how to do the difficult part, ask it about that instead. Expecting it to get the answer right in the first pass is like expecting your code to compile the very first time. You have to have more of a conversation with it to coax the difference out of  you&#x27;re thinking and what you&#x27;re actually saying.<p>If you&#x27;re looking to read a more advanced example of its capabilities and limitations, try<p><a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Mar&#x2F;23&#x2F;building-c-extensions-for-sqlite-with-chatgpt-code-interpreter&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Mar&#x2F;23&#x2F;building-c-extensions-...</a></div><br/></div></div><div id="39892000" class="c"><input type="checkbox" id="c-39892000" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891468">parent</a><span>|</span><a href="#39891538">prev</a><span>|</span><a href="#39891514">next</a><span>|</span><label class="collapse" for="c-39892000">[-]</label><label class="expand" for="c-39892000">[1 more]</label></div><br/><div class="children"><div class="content">I asked a stupid question and got a stupid answer. Relatively speaking the answer was stupider than it should have been, so yes, it was wrong.<p>I asked it to try again and got a better result though, just didn&#x27;t include it.</div><br/></div></div><div id="39891514" class="c"><input type="checkbox" id="c-39891514" checked=""/><div class="controls bullet"><span class="by">lpapez</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891468">parent</a><span>|</span><a href="#39892000">prev</a><span>|</span><a href="#39891874">next</a><span>|</span><label class="collapse" for="c-39891514">[-]</label><label class="expand" for="c-39891514">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not representative.<p>The models are capable of much much more, and they are being significantly nerfed over time by these ineffective attempts to introduce safeguards.<p>Recently I&#x27;ve asked GPT4 to quote me some code to which it replied that it is not allowed to do so - even though it was perfectly happy to quote anything until recently. When prompted to quote the source code, but output it as PHP comments, it happily complied because it saw that as &quot;derivative work&quot; which it is allowed to do.</div><br/><div id="39892005" class="c"><input type="checkbox" id="c-39892005" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891514">parent</a><span>|</span><a href="#39891874">next</a><span>|</span><label class="collapse" for="c-39892005">[-]</label><label class="expand" for="c-39892005">[1 more]</label></div><br/><div class="children"><div class="content">My point is that there aren&#x27;t any safeguards in the reply. In fact I didn&#x27;t even want it to give me hacking info and it did it anyway.</div><br/></div></div></div></div><div id="39891874" class="c"><input type="checkbox" id="c-39891874" checked=""/><div class="controls bullet"><span class="by">yunohn</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891468">parent</a><span>|</span><a href="#39891514">prev</a><span>|</span><a href="#39891452">next</a><span>|</span><label class="collapse" for="c-39891874">[-]</label><label class="expand" for="c-39891874">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t use LLMs for my coding, I manage just fine with LSP and Treesitter.<p>Youâre literally comparing apples to oranges.</div><br/><div id="39891918" class="c"><input type="checkbox" id="c-39891918" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891874">parent</a><span>|</span><a href="#39891452">next</a><span>|</span><label class="collapse" for="c-39891918">[-]</label><label class="expand" for="c-39891918">[1 more]</label></div><br/><div class="children"><div class="content">I think the point was like &quot;when it comes to programming assistance, auto-completion&#x2F;linting&#x2F;and whatever else LSP does and syntax assist from Treesitter, are enough for me&quot;.<p>Though it does come a little off as a comparison. How about programming assistance via asking a colleague for help, Stack Overflow, or online references, code examples, and other such things, which are closer to what the LLM would provide than LSP and treesitter?</div><br/></div></div></div></div></div></div><div id="39891452" class="c"><input type="checkbox" id="c-39891452" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891350">parent</a><span>|</span><a href="#39891468">prev</a><span>|</span><a href="#39891932">next</a><span>|</span><label class="collapse" for="c-39891452">[-]</label><label class="expand" for="c-39891452">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. It was 4. I can&#x27;t share the chat I had where ChatGPT refused to help because I used the wrong words, because I can&#x27;t find it (ChatGPT conversation history search when?), but I just remember it refusing to do something because it thought I was trying to break some sort of moral and ethical boundary writing a chrome extension when all I wanted to do is move some divs around or some such.</div><br/></div></div></div></div><div id="39891932" class="c"><input type="checkbox" id="c-39891932" checked=""/><div class="controls bullet"><span class="by">kevingadd</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891030">parent</a><span>|</span><a href="#39891350">prev</a><span>|</span><a href="#39890763">next</a><span>|</span><label class="collapse" for="c-39891932">[-]</label><label class="expand" for="c-39891932">[1 more]</label></div><br/><div class="children"><div class="content">If you want to be an amateur chemist I recommend not getting your instructions from an LLM that might be hallucinating. Chemistry can be very dangerous if you&#x27;re following incorrect instructions.</div><br/></div></div></div></div><div id="39890763" class="c"><input type="checkbox" id="c-39890763" checked=""/><div class="controls bullet"><span class="by">devsda</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39891030">prev</a><span>|</span><a href="#39890986">next</a><span>|</span><label class="collapse" for="c-39890763">[-]</label><label class="expand" for="c-39890763">[5 more]</label></div><br/><div class="children"><div class="content">For someone interested in learning about LLMs, running them locally is a good way to understand the internals.<p>For everyone else, I wish they  experience these (locally or elsewhere) <i>weak</i> LLMs atleast once before using the commercial ones just to understand various failure modes and to introduce a healthy dose of skepticism towards the results instead of blindly trusting them to be the facts&#x2F;truth.</div><br/><div id="39890770" class="c"><input type="checkbox" id="c-39890770" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890763">parent</a><span>|</span><a href="#39891348">next</a><span>|</span><label class="collapse" for="c-39890770">[-]</label><label class="expand" for="c-39890770">[1 more]</label></div><br/><div class="children"><div class="content">Completely agree. Playing around with a weak LLM is a great way to give yourself a little bit of extra healthy skepticism for when you work with the strong ones.</div><br/></div></div><div id="39891348" class="c"><input type="checkbox" id="c-39891348" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890763">parent</a><span>|</span><a href="#39890770">prev</a><span>|</span><a href="#39891274">next</a><span>|</span><label class="collapse" for="c-39891348">[-]</label><label class="expand" for="c-39891348">[1 more]</label></div><br/><div class="children"><div class="content">This skepticism is completely justified since ChatGPT 3.5 is also happily hallucinating things that don&#x27;t exist. For example how to integrate a different system Python interpreter into pyenv. Though maybe ChatGPT 4 doesn&#x27;t :)</div><br/></div></div><div id="39891274" class="c"><input type="checkbox" id="c-39891274" checked=""/><div class="controls bullet"><span class="by">mmahemoff</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890763">parent</a><span>|</span><a href="#39891348">prev</a><span>|</span><a href="#39890986">next</a><span>|</span><label class="collapse" for="c-39891274">[-]</label><label class="expand" for="c-39891274">[2 more]</label></div><br/><div class="children"><div class="content">How do you learn about the internals by running LLMs locally? Are you playing with The code, runtime params, or just interacting via chat?</div><br/><div id="39891364" class="c"><input type="checkbox" id="c-39891364" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891274">parent</a><span>|</span><a href="#39890986">next</a><span>|</span><label class="collapse" for="c-39891364">[-]</label><label class="expand" for="c-39891364">[1 more]</label></div><br/><div class="children"><div class="content">The abstractions are relatively brittle. If you don&#x27;t have a powerful GPU, you will be forced to consider how to split the model between CPU and GPU, how much context size you need, whether to quantize the model, and the tradeoffs implied by these things. To understand these, you have to develop a basic model how an LLM works.</div><br/></div></div></div></div></div></div><div id="39890986" class="c"><input type="checkbox" id="c-39890986" checked=""/><div class="controls bullet"><span class="by">tracerbulletx</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39890763">prev</a><span>|</span><a href="#39891106">next</a><span>|</span><label class="collapse" for="c-39890986">[-]</label><label class="expand" for="c-39890986">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t really think this is true, you can&#x27;t really extrapolate the strengths and weaknesses of bigger models from the behavior of smaller&#x2F;quantized models and in fact a lot of small models are actually great at lots of things and better at creative writing. If you want to know how they work, just learn how they work, it takes like 5 hours of watching Youtube videos if you&#x27;re a programmer.</div><br/><div id="39891065" class="c"><input type="checkbox" id="c-39891065" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890986">parent</a><span>|</span><a href="#39891106">next</a><span>|</span><label class="collapse" for="c-39891065">[-]</label><label class="expand" for="c-39891065">[2 more]</label></div><br/><div class="children"><div class="content">Sure, you can&#x27;t extrapolate the strengths and weaknesses of the larger ones from the smaller ones - but you still get a much firmer idea of what &quot;they&#x27;re fancy autocomplete&quot; actually means.<p>If nothing else it does a great job of demystifying them. They feel a lot less intimidating once you&#x27;ve seen a small one running on your computer write a terrible haiku and hallucinate some non-existent API methods.</div><br/><div id="39891105" class="c"><input type="checkbox" id="c-39891105" checked=""/><div class="controls bullet"><span class="by">fzzzy</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891065">parent</a><span>|</span><a href="#39891106">next</a><span>|</span><label class="collapse" for="c-39891105">[-]</label><label class="expand" for="c-39891105">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s funny that you say this, because the first thing I tried after ChatGPT came out (3.5-turbo was it?) was writing a haiku. It couldn&#x27;t do it at all. Also, after 4 came out, it hallucinated an api that wasted a day for me. It&#x27;s an api that absolutely should have existed, but didn&#x27;t. Now, I frequently apply llm to things that are easily verifiable, and just double check everything.</div><br/></div></div></div></div></div></div><div id="39891106" class="c"><input type="checkbox" id="c-39891106" checked=""/><div class="controls bullet"><span class="by">tgma</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39890986">prev</a><span>|</span><a href="#39890833">next</a><span>|</span><label class="collapse" for="c-39891106">[-]</label><label class="expand" for="c-39891106">[8 more]</label></div><br/><div class="children"><div class="content">If you have an &gt;=M1-class machine with sufficient RAM, the medium-sized models that are on the order of 30GB in size perform decently on many tasks to be quite useful without leaking your data.</div><br/><div id="39891340" class="c"><input type="checkbox" id="c-39891340" checked=""/><div class="controls bullet"><span class="by">noman-land</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891106">parent</a><span>|</span><a href="#39891112">next</a><span>|</span><label class="collapse" for="c-39891340">[-]</label><label class="expand" for="c-39891340">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using Mixtral 8x7b as a llamafile on an M1 regularly for coding help and general Q&amp;A. It&#x27;s really something wonderful to just run a single command and have this incredible offline resource.</div><br/><div id="39891710" class="c"><input type="checkbox" id="c-39891710" checked=""/><div class="controls bullet"><span class="by">tchvil</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891340">parent</a><span>|</span><a href="#39891386">next</a><span>|</span><label class="collapse" for="c-39891710">[-]</label><label class="expand" for="c-39891710">[3 more]</label></div><br/><div class="children"><div class="content">By any chance, do you have a good link to some help with the installation?</div><br/><div id="39891828" class="c"><input type="checkbox" id="c-39891828" checked=""/><div class="controls bullet"><span class="by">yaantc</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891710">parent</a><span>|</span><a href="#39891829">next</a><span>|</span><label class="collapse" for="c-39891828">[-]</label><label class="expand" for="c-39891828">[1 more]</label></div><br/><div class="children"><div class="content">Use llamafile [1], it can be as simple as downloading a file (for mixtral, [2]), making it executable and running it. The repo README has all the info, it&#x27;s simple and downloading the model is what takes the most time.<p>In my case I got the runtime detection issue (explained in the README &quot;gotcha&quot; section). Solved my running &quot;assimilate&quot; [3] on the downloaded llamafile.<p><pre><code>    [1] https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile&#x2F;
    [2] https:&#x2F;&#x2F;huggingface.co&#x2F;jartine&#x2F;Mixtral-8x7B-Instruct-v0.1-llamafile&#x2F;resolve&#x2F;main&#x2F;mixtral-8x7b-instruct-v0.1.Q5_K_M.llamafile?download=true
    [3] https:&#x2F;&#x2F;cosmo.zip&#x2F;pub&#x2F;cosmos&#x2F;bin&#x2F;assimilate</code></pre></div><br/></div></div><div id="39891829" class="c"><input type="checkbox" id="c-39891829" checked=""/><div class="controls bullet"><span class="by">tgma</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891710">parent</a><span>|</span><a href="#39891828">prev</a><span>|</span><a href="#39891386">next</a><span>|</span><label class="collapse" for="c-39891829">[-]</label><label class="expand" for="c-39891829">[1 more]</label></div><br/><div class="children"><div class="content">Either <a href="https:&#x2F;&#x2F;lmstudio.ai" rel="nofollow">https:&#x2F;&#x2F;lmstudio.ai</a> (desktop app with nice GUI) or <a href="https:&#x2F;&#x2F;ollama.com">https:&#x2F;&#x2F;ollama.com</a> (command-like more like a docker container that you can also hook up to a web UI via <a href="https:&#x2F;&#x2F;openwebui.com" rel="nofollow">https:&#x2F;&#x2F;openwebui.com</a>) should be super straightforward to get running.</div><br/></div></div></div></div><div id="39891386" class="c"><input type="checkbox" id="c-39891386" checked=""/><div class="controls bullet"><span class="by">tgma</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891340">parent</a><span>|</span><a href="#39891710">prev</a><span>|</span><a href="#39891112">next</a><span>|</span><label class="collapse" for="c-39891386">[-]</label><label class="expand" for="c-39891386">[1 more]</label></div><br/><div class="children"><div class="content">I concur; in my experience Mixtral is one of the best ~30G models (likely the best pro laptop-size model currently) and Gemma is quite good compared to other below 8GB models.</div><br/></div></div></div></div><div id="39891112" class="c"><input type="checkbox" id="c-39891112" checked=""/><div class="controls bullet"><span class="by">bongobingo1</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891106">parent</a><span>|</span><a href="#39891340">prev</a><span>|</span><a href="#39890833">next</a><span>|</span><label class="collapse" for="c-39891112">[-]</label><label class="expand" for="c-39891112">[2 more]</label></div><br/><div class="children"><div class="content">What is sufficient RAM in that case? 30gb+? Or can you get by streaming it?</div><br/><div id="39891191" class="c"><input type="checkbox" id="c-39891191" checked=""/><div class="controls bullet"><span class="by">AaronFriel</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891112">parent</a><span>|</span><a href="#39890833">next</a><span>|</span><label class="collapse" for="c-39891191">[-]</label><label class="expand" for="c-39891191">[1 more]</label></div><br/><div class="children"><div class="content">30gb+, yeah. You can&#x27;t get by streaming the model&#x27;s parameters: NVMe isn&#x27;t fast enough. Consumer GPUs and Apple Silicon processors boast memory bandwidths in the hundreds of gigabytes per second.<p>To a first order approximation, LLMs are bandwidth constrained. We can estimate single batch throughput as Memory Bandwidth &#x2F; (Active Parameters * Parameter Size).<p>An 8-bit quantized Llama 2 70B conveniently uses 70GiB of VRAM (and then some, let&#x27;s ignore that.) The M3 Max with 96GiB of VRAM and 300GiB&#x2F;s bandwidth would have a peak throughput around 4.2 tokens per second.<p>Quantized models trade reduced quality for lower VRAM requirements and may also offer higher throughput with optimized kernels, largely as a consequence of transfering less data from VRAM into the GPU die for each parameter.<p>Mixture of Expert models reduce active parameters for higher throughput, but disk is still far too slow to page in layers.</div><br/></div></div></div></div></div></div><div id="39890833" class="c"><input type="checkbox" id="c-39890833" checked=""/><div class="controls bullet"><span class="by">kersplody</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890656">parent</a><span>|</span><a href="#39891106">prev</a><span>|</span><a href="#39890615">next</a><span>|</span><label class="collapse" for="c-39890833">[-]</label><label class="expand" for="c-39890833">[1 more]</label></div><br/><div class="children"><div class="content">Local LLMs are also a fantastic too for creative endeavors. Without prompt injection and having the ability to modify the amount of noise and &quot;creativity&quot; in the output, absolutely bonkers things pop out.</div><br/></div></div></div></div><div id="39890615" class="c"><input type="checkbox" id="c-39890615" checked=""/><div class="controls bullet"><span class="by">gpm</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890656">prev</a><span>|</span><a href="#39890470">next</a><span>|</span><label class="collapse" for="c-39890615">[-]</label><label class="expand" for="c-39890615">[2 more]</label></div><br/><div class="children"><div class="content">If you want to download a backup of a large chunk of human knowledge... download wikipedia. It&#x27;s a similar size to a small LLM and can actually distinguish between real life and fantasy: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wikipedia:Database_download" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wikipedia:Database_download</a><p>If you just want to play around with an LLM though, absolutely.</div><br/><div id="39891447" class="c"><input type="checkbox" id="c-39891447" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890615">parent</a><span>|</span><a href="#39890470">next</a><span>|</span><label class="collapse" for="c-39891447">[-]</label><label class="expand" for="c-39891447">[1 more]</label></div><br/><div class="children"><div class="content">Kiwix provides prepackaged highly compressed archives of Wikipedia, Project Gutenberg, and many other useful things: <a href="https:&#x2F;&#x2F;download.kiwix.org&#x2F;zim&#x2F;" rel="nofollow">https:&#x2F;&#x2F;download.kiwix.org&#x2F;zim&#x2F;</a>.<p>Between that and dirt cheap storage prices, it is possible to have a local, offline copy of more human knowledge than one can sensibly consume in a lifetime. Hell, it&#x27;s possible to have it all on one&#x27;s <i>smartphone</i> (just get one with an SD card slot and shove a 1+ Tb one in there).</div><br/></div></div></div></div><div id="39890470" class="c"><input type="checkbox" id="c-39890470" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890615">prev</a><span>|</span><a href="#39890739">next</a><span>|</span><label class="collapse" for="c-39890470">[-]</label><label class="expand" for="c-39890470">[20 more]</label></div><br/><div class="children"><div class="content">I contend that most human knowledge is not written down or if it is written down itâs not publicly available on the internet and so does not exist in these datasets.<p>Thereâs so much subtle knowledge like the way a mother learns to calm her child or the way a carpenter learns to work different kinds of wood which may be written down in part, but may also be learned through lived experience or transferred from human to human such that little of it gets written down and posted online.</div><br/><div id="39891101" class="c"><input type="checkbox" id="c-39891101" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39890527">next</a><span>|</span><label class="collapse" for="c-39891101">[-]</label><label class="expand" for="c-39891101">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s where humans suck. The classic &quot;you&#x27;re not doing it right&quot; then proceeds to quickly show how to do it without verbalizing any info on learning process, pitfalls, failure modes, etc, as if just showing it was enough for themselves to learn. Most people do[n&#x27;t do] that, not even a sign of reflection.<p>My worst case was with a guy who asked me to write an arbitrage betting bot. When I asked how to calculate coeffs, he pointed at two values and said &quot;look, there &lt;x&gt;, there &lt;y&gt; <i>thinks for a minute</i> then it&#x27;s &lt;z&gt;!&quot;. When I asked how exactly did he calculate it, he simply repeated with different numbers.</div><br/><div id="39891439" class="c"><input type="checkbox" id="c-39891439" checked=""/><div class="controls bullet"><span class="by">Aerroon</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891101">parent</a><span>|</span><a href="#39891388">next</a><span>|</span><label class="collapse" for="c-39891439">[-]</label><label class="expand" for="c-39891439">[1 more]</label></div><br/><div class="children"><div class="content">People often don&#x27;t know how to verbalize them in the first place. Some of these topics are very complex, but our intuition gets us halfway there.<p>Once upon a time I was good at a video game. Everyone realized that positioning is extremely important in this game.<p>I have good positioning in that game and was asked many times to make a guide about positioning. I never did, because I don&#x27;t really know how. There is too much information they you need to convey to cover all the various situations.<p>I think you would first have to come up with a framework on positioning to be able to really teach this to someone else. Some kind of base truths&#x2F;patterns that you can then use to convey the meaning. I believe the same thing applies to a lot of these processes that aren&#x27;t verbalized.</div><br/></div></div><div id="39891388" class="c"><input type="checkbox" id="c-39891388" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891101">parent</a><span>|</span><a href="#39891439">prev</a><span>|</span><a href="#39890527">next</a><span>|</span><label class="collapse" for="c-39891388">[-]</label><label class="expand" for="c-39891388">[1 more]</label></div><br/><div class="children"><div class="content">&gt; When I asked how exactly did he calculate it, he simply repeated with different numbers.<p>Now you know how an LLM feels during training!</div><br/></div></div></div></div><div id="39890527" class="c"><input type="checkbox" id="c-39890527" checked=""/><div class="controls bullet"><span class="by">spacephysics</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39891101">prev</a><span>|</span><a href="#39890542">next</a><span>|</span><label class="collapse" for="c-39890527">[-]</label><label class="expand" for="c-39890527">[2 more]</label></div><br/><div class="children"><div class="content">For sure agree, however as the storage of information evolves, itâs becoming more efficient over time<p>From oral tradition to tablets to scrolls to books to mass produced books to digital and now these LLMs, I think itâs still a good idea to preserve what we have the best we can. Not as a replacement, but a hedge against a potential library of Alexandria incident.<p>I could imagine a time in the near future where the models are domain-specific, and just like there are trusted encyclopedia publishers there are trusted model publishers that guarantee a certain level of accuracy.<p>Itâs not like reading a book, but I for sure had an easier time learning golang talking with ChatGPT than a book</div><br/><div id="39891219" class="c"><input type="checkbox" id="c-39891219" checked=""/><div class="controls bullet"><span class="by">nyokodo</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890527">parent</a><span>|</span><a href="#39890542">next</a><span>|</span><label class="collapse" for="c-39891219">[-]</label><label class="expand" for="c-39891219">[1 more]</label></div><br/><div class="children"><div class="content">&gt; a hedge against a potential library of Alexandria incident<p>What would cause a Library of Alexandria incident wiping out all human knowledge elsewhere, that would also allow you to run a local LLM?</div><br/></div></div></div></div><div id="39890542" class="c"><input type="checkbox" id="c-39890542" checked=""/><div class="controls bullet"><span class="by">_ache_</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39890527">prev</a><span>|</span><a href="#39890794">next</a><span>|</span><label class="collapse" for="c-39890542">[-]</label><label class="expand" for="c-39890542">[2 more]</label></div><br/><div class="children"><div class="content">I think you underestimate the amount of information contained in books and the extent to which our society (as a whole) depends on them.</div><br/><div id="39891546" class="c"><input type="checkbox" id="c-39891546" checked=""/><div class="controls bullet"><span class="by">Barrin92</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890542">parent</a><span>|</span><a href="#39890794">next</a><span>|</span><label class="collapse" for="c-39891546">[-]</label><label class="expand" for="c-39891546">[1 more]</label></div><br/><div class="children"><div class="content">society depends much more on social networks, mentorship and tacit knowledge than books. It&#x27;s easy to test this. Just run the thought experiment by a few people, if you could get only one, would you take an Ivy league degree without the education or the education without the degree?<p>Venture capital in tech is a good example of this. The book knowledge is effectively globally distributed and almost free, effectively success happens in a few geographically concentrated counties.</div><br/></div></div></div></div><div id="39890794" class="c"><input type="checkbox" id="c-39890794" checked=""/><div class="controls bullet"><span class="by">nicklecompte</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39890542">prev</a><span>|</span><a href="#39890588">next</a><span>|</span><label class="collapse" for="c-39890794">[-]</label><label class="expand" for="c-39890794">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not even &quot;human knowledge&quot; that can&#x27;t be written down - it seems all vertebrates understand causality, quantity (in the sense of intuitively understanding what numbers are), and object permanence. Good luck writing those concepts down in a way that GPT can use!<p>In general AI in 2024 is not even close to understanding these ideas, nor does any AI developer have a clue how to build an AI with this understanding. The best we can do is imitating object permanence for a small subset of perceptible objects, a limitation not found in dogs or spiders.</div><br/></div></div><div id="39890588" class="c"><input type="checkbox" id="c-39890588" checked=""/><div class="controls bullet"><span class="by">skeledrew</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39890794">prev</a><span>|</span><a href="#39890652">next</a><span>|</span><label class="collapse" for="c-39890588">[-]</label><label class="expand" for="c-39890588">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d content that those are skills (gained through experience) rather than knowledge (gained through rote learning).</div><br/><div id="39891765" class="c"><input type="checkbox" id="c-39891765" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890588">parent</a><span>|</span><a href="#39890652">next</a><span>|</span><label class="collapse" for="c-39891765">[-]</label><label class="expand" for="c-39891765">[1 more]</label></div><br/><div class="children"><div class="content">I think itâs worth expanding your definition of knowledge.</div><br/></div></div></div></div><div id="39890652" class="c"><input type="checkbox" id="c-39890652" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39890588">prev</a><span>|</span><a href="#39890519">next</a><span>|</span><label class="collapse" for="c-39890652">[-]</label><label class="expand" for="c-39890652">[1 more]</label></div><br/><div class="children"><div class="content">Yes but it contains enough hints to help someone find their way on the these types of tasks.</div><br/></div></div><div id="39890519" class="c"><input type="checkbox" id="c-39890519" checked=""/><div class="controls bullet"><span class="by">mickdarling</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890470">parent</a><span>|</span><a href="#39890652">prev</a><span>|</span><a href="#39890739">next</a><span>|</span><label class="collapse" for="c-39890519">[-]</label><label class="expand" for="c-39890519">[8 more]</label></div><br/><div class="children"><div class="content">Wait till all the videos ever created are tokenized and ingested into a training dataset. Carpentry techniques are certainly there.  The subtleties of parenting maybe harder to derive from that, but maybe lots of little snippets of peopleâs lives will add up to a general understanding of parenting. There have certainly been bigger surprises in the field.</div><br/><div id="39890555" class="c"><input type="checkbox" id="c-39890555" checked=""/><div class="controls bullet"><span class="by">oblio</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890519">parent</a><span>|</span><a href="#39891292">next</a><span>|</span><label class="collapse" for="c-39890555">[-]</label><label class="expand" for="c-39890555">[6 more]</label></div><br/><div class="children"><div class="content">What about smells or tastes? Or feelings?<p>I can&#x27;t help but feel we&#x27;re at the &quot;aliens watch people eat from space and recreate chemically identical food that has no taste&quot; phase of AI development.</div><br/><div id="39890618" class="c"><input type="checkbox" id="c-39890618" checked=""/><div class="controls bullet"><span class="by">mickdarling</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890555">parent</a><span>|</span><a href="#39890617">next</a><span>|</span><label class="collapse" for="c-39890618">[-]</label><label class="expand" for="c-39890618">[2 more]</label></div><br/><div class="children"><div class="content">Well, I have synesthetic smell&#x2F;color senses, so I donât even know what other humans experience, nor they me. But, I have described it in detail to many people and they seem to get the idea, and can even predict how certain smells will âlookâ to me.  All that took was using words to describe things.</div><br/><div id="39891229" class="c"><input type="checkbox" id="c-39891229" checked=""/><div class="controls bullet"><span class="by">nyokodo</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890618">parent</a><span>|</span><a href="#39890617">next</a><span>|</span><label class="collapse" for="c-39891229">[-]</label><label class="expand" for="c-39891229">[1 more]</label></div><br/><div class="children"><div class="content">&gt; All that took was using words to describe things.<p>All that took was words and a shared experience of smelling.</div><br/></div></div></div></div><div id="39890617" class="c"><input type="checkbox" id="c-39890617" checked=""/><div class="controls bullet"><span class="by">skeledrew</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890555">parent</a><span>|</span><a href="#39890618">prev</a><span>|</span><a href="#39891292">next</a><span>|</span><label class="collapse" for="c-39890617">[-]</label><label class="expand" for="c-39890617">[3 more]</label></div><br/><div class="children"><div class="content">If the food is chemically identical then the taste would be the same though, since taste (and smell) is about chemistry. I do get what you&#x27;re saying though.</div><br/><div id="39891457" class="c"><input type="checkbox" id="c-39891457" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890617">parent</a><span>|</span><a href="#39891233">next</a><span>|</span><label class="collapse" for="c-39891457">[-]</label><label class="expand" for="c-39891457">[1 more]</label></div><br/><div class="children"><div class="content">Their perception is very likely to be totally different.<p>* They might not perceive some substances at all, others that we don&#x27;t notice might make it unpalatable.<p>* Some substances might be perceived differently than us, or be indistinguishable from others.<p>* And some might require getting used to.<p>Note that all of the above phenomena also occur in humans because of genetics, cultural background, or experiences!</div><br/></div></div><div id="39891233" class="c"><input type="checkbox" id="c-39891233" checked=""/><div class="controls bullet"><span class="by">nyokodo</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890617">parent</a><span>|</span><a href="#39891457">prev</a><span>|</span><a href="#39891292">next</a><span>|</span><label class="collapse" for="c-39891233">[-]</label><label class="expand" for="c-39891233">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If the food is chemically identicalâ¦<p>If it were 99.9% chemically identical but they left out the salt and spicesâ¦</div><br/></div></div></div></div></div></div><div id="39891292" class="c"><input type="checkbox" id="c-39891292" checked=""/><div class="controls bullet"><span class="by">andersa</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890519">parent</a><span>|</span><a href="#39890555">prev</a><span>|</span><a href="#39890739">next</a><span>|</span><label class="collapse" for="c-39891292">[-]</label><label class="expand" for="c-39891292">[1 more]</label></div><br/><div class="children"><div class="content">What makes you think they aren&#x27;t already?</div><br/></div></div></div></div></div></div><div id="39890739" class="c"><input type="checkbox" id="c-39890739" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890470">prev</a><span>|</span><a href="#39890818">next</a><span>|</span><label class="collapse" for="c-39890739">[-]</label><label class="expand" for="c-39890739">[13 more]</label></div><br/><div class="children"><div class="content">It is invaluable to have a chunk of human knowledge that can tell you things like the Brooklyn Nets won the 1986 Cricket World Cup by scoring 46 yards in only 3 frames</div><br/><div id="39891479" class="c"><input type="checkbox" id="c-39891479" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890739">parent</a><span>|</span><a href="#39890981">next</a><span>|</span><label class="collapse" for="c-39891479">[-]</label><label class="expand" for="c-39891479">[1 more]</label></div><br/><div class="children"><div class="content">The facts LLMs learned from training are fuzzy, unreliable, and quickly outdated. You actually want retrieval-augmented generation (RAG) where a model queries an external system for facts or to perform calculations and postprocesses the results to generate an answer for you.</div><br/></div></div><div id="39890981" class="c"><input type="checkbox" id="c-39890981" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890739">parent</a><span>|</span><a href="#39891479">prev</a><span>|</span><a href="#39890818">next</a><span>|</span><label class="collapse" for="c-39890981">[-]</label><label class="expand" for="c-39890981">[11 more]</label></div><br/><div class="children"><div class="content">According to ChatGPT<p>&gt; Australia won the 1987 Cricket World Cup. The 1986 date is incorrect; there was no Cricket World Cup in 1986. The tournament took place in 1987, and Australia defeated England in the final to win their first title.<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;e9360faa-1157-4806-80ea-563489f946f3" rel="nofollow">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;e9360faa-1157-4806-80ea-563489...</a><p>I&#x27;m no cricket fan, so someone will have to correct Wikipedia if that&#x27;s wrong.<p>If you want to point out that LLMs hallucinate, you might want to speak plainly and just come out and say it, or at least give a real world example and not one where it didn&#x27;t.</div><br/><div id="39891129" class="c"><input type="checkbox" id="c-39891129" checked=""/><div class="controls bullet"><span class="by">vlunkr</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890981">parent</a><span>|</span><a href="#39891164">next</a><span>|</span><label class="collapse" for="c-39891129">[-]</label><label class="expand" for="c-39891129">[9 more]</label></div><br/><div class="children"><div class="content">Weâre not talking about running chatGPT locally though, are we?</div><br/><div id="39891147" class="c"><input type="checkbox" id="c-39891147" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891129">parent</a><span>|</span><a href="#39891164">next</a><span>|</span><label class="collapse" for="c-39891147">[-]</label><label class="expand" for="c-39891147">[8 more]</label></div><br/><div class="children"><div class="content"><i>sigh</i> your going to make me open my laptop, aren&#x27;t you.</div><br/><div id="39891197" class="c"><input type="checkbox" id="c-39891197" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891147">parent</a><span>|</span><a href="#39891164">next</a><span>|</span><label class="collapse" for="c-39891197">[-]</label><label class="expand" for="c-39891197">[7 more]</label></div><br/><div class="children"><div class="content">I ran &#x27;who won the 1986 Cricket World Cup&#x27; against llama2-uncensored (the local model I have pre-downloaded) and hilarious got 5 different answers asking it 5 times:<p><pre><code>    &gt;&gt;&gt; who won the 1986 Cricket World Cup
    India
    
    &gt;&gt;&gt; who won the 1986 Cricket World Cup
    Australia
    
    &gt;&gt;&gt; who won the 1986 Cricket World Cup
    New Zealand
    
    &gt;&gt;&gt; who won the 1986 Cricket World Cup
    West Indies
    
    &gt;&gt;&gt; who won the 1986 Cricket World Cup
    England
</code></pre>
Which proves GP&#x27;s point about hallucinations, though none of those are<p>&gt; Brooklyn Nets won the 1986 Cricket World Cup by scoring 46 yards in only 3 frames<p>LLM&#x27;s hallucinations are insidous because they have the ring of truth around them. yards and frames aren&#x27;t cricket terms, so we&#x27;re off to the races with them.</div><br/><div id="39891365" class="c"><input type="checkbox" id="c-39891365" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891197">parent</a><span>|</span><a href="#39891587">next</a><span>|</span><label class="collapse" for="c-39891365">[-]</label><label class="expand" for="c-39891365">[5 more]</label></div><br/><div class="children"><div class="content">If you want factual answers from a local model it might help to turn the temperature down.</div><br/><div id="39891792" class="c"><input type="checkbox" id="c-39891792" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891365">parent</a><span>|</span><a href="#39891455">next</a><span>|</span><label class="collapse" for="c-39891792">[-]</label><label class="expand" for="c-39891792">[1 more]</label></div><br/><div class="children"><div class="content">It would also help if I had more VRAM and wasn&#x27;t running a 7B parameter 4-bit quantized model.</div><br/></div></div><div id="39891455" class="c"><input type="checkbox" id="c-39891455" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891365">parent</a><span>|</span><a href="#39891792">prev</a><span>|</span><a href="#39891449">next</a><span>|</span><label class="collapse" for="c-39891455">[-]</label><label class="expand" for="c-39891455">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If you want factual answers from a local model it might help to turn the temperature down.<p>This makes sense. If you interact with a language model and it says something wrong it is your fault</div><br/><div id="39892011" class="c"><input type="checkbox" id="c-39892011" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891455">parent</a><span>|</span><a href="#39891449">next</a><span>|</span><label class="collapse" for="c-39892011">[-]</label><label class="expand" for="c-39892011">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re not &quot;interacting with a language model&quot;, you&#x27;re running a program (llama.cpp) with a sampling algorithm which is not set to maximum factualness by default.<p>It&#x27;s like how you have to set x264 to the anime tuning or the film tuning depending on what you run it on.</div><br/></div></div></div></div></div></div><div id="39891587" class="c"><input type="checkbox" id="c-39891587" checked=""/><div class="controls bullet"><span class="by">beefnugs</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891197">parent</a><span>|</span><a href="#39891365">prev</a><span>|</span><a href="#39891164">next</a><span>|</span><label class="collapse" for="c-39891587">[-]</label><label class="expand" for="c-39891587">[1 more]</label></div><br/><div class="children"><div class="content">Actually isn&#x27;t this good? It means we can run something multiple times to prove itself a bad answer?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39890818" class="c"><input type="checkbox" id="c-39890818" checked=""/><div class="controls bullet"><span class="by">creatonez</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890739">prev</a><span>|</span><a href="#39890546">next</a><span>|</span><label class="collapse" for="c-39890818">[-]</label><label class="expand" for="c-39890818">[4 more]</label></div><br/><div class="children"><div class="content">Maybe I&#x27;m seeing things through a modern lens, but if I were trying to restart civilization and was <i>only</i> left with ChatGPT, I would be enraged and very much not grateful for this.</div><br/><div id="39891955" class="c"><input type="checkbox" id="c-39891955" checked=""/><div class="controls bullet"><span class="by">devsda</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890818">parent</a><span>|</span><a href="#39891251">next</a><span>|</span><label class="collapse" for="c-39891955">[-]</label><label class="expand" for="c-39891955">[1 more]</label></div><br/><div class="children"><div class="content">I think re-imagining the &quot;Dr. Stone&quot; series with the main character replaced by an LLM will be a funny &amp; interesting series if we decide to stay true to LLMs nature and make it hallucinate as well.<p>Given the way LLMs are right now, I suspect there will be lot of failed experiments and the kingdom of science will not advance that quick.</div><br/></div></div><div id="39891251" class="c"><input type="checkbox" id="c-39891251" checked=""/><div class="controls bullet"><span class="by">nyokodo</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890818">parent</a><span>|</span><a href="#39891955">prev</a><span>|</span><a href="#39890546">next</a><span>|</span><label class="collapse" for="c-39891251">[-]</label><label class="expand" for="c-39891251">[2 more]</label></div><br/><div class="children"><div class="content">&gt; if I were trying to restart civilization and was only left with ChatGPT<p>In this scenario youâd need to also be left with a big chunk of compute, and power infrastructure. Since ChatGPT is the front end of the model youâd also need to have the internet still going in a minimum capacity.</div><br/></div></div></div></div><div id="39890546" class="c"><input type="checkbox" id="c-39890546" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890818">prev</a><span>|</span><a href="#39891250">next</a><span>|</span><label class="collapse" for="c-39890546">[-]</label><label class="expand" for="c-39890546">[11 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see LLMs as a large chunk of knowledge, I see them as an emergent alien intelligence snapshotted at the moment it appeared to stop learning. It&#x27;s further hobbled by the limited context window it has to use, and the probabilistic output structure that allows for outside random influences to pick its next word.<p>Both the context window and output structure are, in my opinion, massive impedance mismatches for the emergent intellect embedded in the weights of the model.<p>If there were a way to match the impedance, I strongly suspect we&#x27;d already have AGI on our hands.</div><br/><div id="39891739" class="c"><input type="checkbox" id="c-39891739" checked=""/><div class="controls bullet"><span class="by">mlsu</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890546">parent</a><span>|</span><a href="#39890738">next</a><span>|</span><label class="collapse" for="c-39891739">[-]</label><label class="expand" for="c-39891739">[1 more]</label></div><br/><div class="children"><div class="content">Disagree. The input&#x2F;output structure (tokens) is the interface for both inference <i>and</i> for training. There is an emergent intellect embedded in the weights of the model. However, it is <i>only</i> accessible through the autoregressive token interface.<p>This is a fundamental limitation, much more fundamental than appears at first. It means that the only way to touch the model, and for the model to touch the world, is through the tokenizer (also, btw, why tokenizer is so essential to model performance). Touching the world through a tokenizer is actually quite limited.<p>So there is an intelligence in there for sure, but it is locked in an ontology that is tied to its interface. This is even more of a limitation than e.g. weights being frozen.</div><br/></div></div><div id="39890738" class="c"><input type="checkbox" id="c-39890738" checked=""/><div class="controls bullet"><span class="by">namarie</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890546">parent</a><span>|</span><a href="#39891739">prev</a><span>|</span><a href="#39890657">next</a><span>|</span><label class="collapse" for="c-39890738">[-]</label><label class="expand" for="c-39890738">[1 more]</label></div><br/><div class="children"><div class="content">I can agree on the context windows,  but what other output structure would you have?</div><br/></div></div><div id="39890657" class="c"><input type="checkbox" id="c-39890657" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890546">parent</a><span>|</span><a href="#39890738">prev</a><span>|</span><a href="#39891250">next</a><span>|</span><label class="collapse" for="c-39890657">[-]</label><label class="expand" for="c-39890657">[8 more]</label></div><br/><div class="children"><div class="content">What is alien about them ?<p>LLMs are of this earth and created by our species. Seems quite familiar to me.</div><br/><div id="39890965" class="c"><input type="checkbox" id="c-39890965" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890657">parent</a><span>|</span><a href="#39890748">next</a><span>|</span><label class="collapse" for="c-39890965">[-]</label><label class="expand" for="c-39890965">[5 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t think, they don&#x27;t reason, they don&#x27;t understand. Except they do. But it&#x27;s hard for human words for thought processes to apply when giving it an endless string of AAAAA&#x27;s makes it go bananas.<p>That&#x27;s not familiar behavior. Nor is the counting reddit derived output. It&#x27;s also not familiar for a single person to have the breadth and depth of knowledge that ChatGPT has. Sure, some people know more than others, but even without hitting the Internet, it has a ridiculous amount of knowledge, far surpassing a human, making it, to me, alien. though, it&#x27;s inability to do math sometimes is humanizing to me for some reason.<p>ChatGPT&#x27;s memory is also unhuman. It has a context window which is a thing, but also it only knows about things you&#x27;ve told it in each chat. Make a new chat and it&#x27;s totally forgotten the nickname you gave it.<p>I don&#x27;t think of HR Geiger&#x27;s work, though made by a human, as familiar to me. it feels quite alien to me, and it&#x27;s not just me,
either. Dali, Bosch, and Escher are other human artists who&#x27;s work can be unfamiliar and alien. So being created by our species doesn&#x27;t automatically imbue something with familiar human processes.<p>So it dot products, it matrix multiplies, instead of reasoning and understanding. It&#x27;s the Chinese room experiment on steroids; it turns out a sufficiently large corpus on a sufficiently large machine does make it look like something&quot;understands&quot;.</div><br/><div id="39891582" class="c"><input type="checkbox" id="c-39891582" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890965">parent</a><span>|</span><a href="#39891318">next</a><span>|</span><label class="collapse" for="c-39891582">[-]</label><label class="expand" for="c-39891582">[1 more]</label></div><br/><div class="children"><div class="content">The context window is comparable to human short-term memory. LLMs are missing episodic memory and means to migrate knowledge between the different layers and into its weights.<p>Math is mostly impeded by the tokenization, but it would still make more sense to adapt them to use RAG to process questions that are clearly calculations or chains of logical inference. With proper prompt engineering, they can process the latter though, and deviating from strictly logical reasoning is sometimes exactly what we want.<p>The ability to reset the text and to change that history is a powerful tool! It can make the model roleplay and even help circumvent alignment.<p>I think that LLMs could one day serve as the language center of an AGI.</div><br/></div></div><div id="39891318" class="c"><input type="checkbox" id="c-39891318" checked=""/><div class="controls bullet"><span class="by">trimethylpurine</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890965">parent</a><span>|</span><a href="#39891582">prev</a><span>|</span><a href="#39891684">next</a><span>|</span><label class="collapse" for="c-39891318">[-]</label><label class="expand" for="c-39891318">[2 more]</label></div><br/><div class="children"><div class="content">The word &quot;alien&quot; works in this context but, as the previous commenter mentioned, it also carries the implication of foreign origin. You could use &quot;uncanny&quot; instead. Maybe that&#x27;s less arbitrary and more specific to these examples.<p>&quot;Alien&quot; still works, but then you might have to add all the context at length, as you&#x27;ve done in this last comment.</div><br/><div id="39891423" class="c"><input type="checkbox" id="c-39891423" checked=""/><div class="controls bullet"><span class="by">fire_lake</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891318">parent</a><span>|</span><a href="#39891684">next</a><span>|</span><label class="collapse" for="c-39891423">[-]</label><label class="expand" for="c-39891423">[1 more]</label></div><br/><div class="children"><div class="content">Hype people do this all the time - take a word that has a particular meaning in a narrow context and move it to a broader context where people will give it a sexier meaning.<p><pre><code>    AI researchers unveil alien intelligence
</code></pre>
Is way better headline.</div><br/></div></div></div></div><div id="39891684" class="c"><input type="checkbox" id="c-39891684" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890965">parent</a><span>|</span><a href="#39891318">prev</a><span>|</span><a href="#39890748">next</a><span>|</span><label class="collapse" for="c-39891684">[-]</label><label class="expand" for="c-39891684">[1 more]</label></div><br/><div class="children"><div class="content">In all fairness, going up to SMS random human and yelling AAAAAAAAAAAAAAâ¦ at them for long enough will produce some out-of-distribution responses too.</div><br/></div></div></div></div><div id="39890748" class="c"><input type="checkbox" id="c-39890748" checked=""/><div class="controls bullet"><span class="by">jfoster</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890657">parent</a><span>|</span><a href="#39890965">prev</a><span>|</span><a href="#39891250">next</a><span>|</span><label class="collapse" for="c-39890748">[-]</label><label class="expand" for="c-39890748">[2 more]</label></div><br/><div class="children"><div class="content">They can write in a way similar to how a human might write, but they&#x27;re not human.<p>The chat interfaces (Claude, ChatGPT) certainly have a particular style of writing, but the underlying LLMs are definitely capable of impersonating as our species in the medium of text.</div><br/></div></div></div></div></div></div><div id="39891250" class="c"><input type="checkbox" id="c-39891250" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890546">prev</a><span>|</span><a href="#39891923">next</a><span>|</span><label class="collapse" for="c-39891250">[-]</label><label class="expand" for="c-39891250">[4 more]</label></div><br/><div class="children"><div class="content">It seems to be an unbelievably inefficient way to back up knowledge.</div><br/><div id="39891642" class="c"><input type="checkbox" id="c-39891642" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891250">parent</a><span>|</span><a href="#39891923">next</a><span>|</span><label class="collapse" for="c-39891642">[-]</label><label class="expand" for="c-39891642">[3 more]</label></div><br/><div class="children"><div class="content">Are they though? They are lossy compressing trillions of tokens into a few dozen GB. The decompression action is fuzzy and inefficient though.</div><br/><div id="39891846" class="c"><input type="checkbox" id="c-39891846" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891642">parent</a><span>|</span><a href="#39891923">next</a><span>|</span><label class="collapse" for="c-39891846">[-]</label><label class="expand" for="c-39891846">[2 more]</label></div><br/><div class="children"><div class="content">And it requires massive computational power to decompress, which I don&#x27;t expect to be available in a catastrophic situation where humans have lost a large chunk of important knowledge.</div><br/><div id="39891945" class="c"><input type="checkbox" id="c-39891945" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39891846">parent</a><span>|</span><a href="#39891923">next</a><span>|</span><label class="collapse" for="c-39891945">[-]</label><label class="expand" for="c-39891945">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t necessarily agree. It requires massive computing power, but running models smaller than 70G parameters is possible on consumer hardware, albeit slowly.</div><br/></div></div></div></div></div></div></div></div><div id="39891923" class="c"><input type="checkbox" id="c-39891923" checked=""/><div class="controls bullet"><span class="by">LunaSea</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39891250">prev</a><span>|</span><a href="#39890507">next</a><span>|</span><label class="collapse" for="c-39891923">[-]</label><label class="expand" for="c-39891923">[1 more]</label></div><br/><div class="children"><div class="content">I wonder how the Chinese government will manage to sensor LLMs within China?</div><br/></div></div><div id="39890507" class="c"><input type="checkbox" id="c-39890507" checked=""/><div class="controls bullet"><span class="by">texuf</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39891923">prev</a><span>|</span><a href="#39891011">next</a><span>|</span><label class="collapse" for="c-39890507">[-]</label><label class="expand" for="c-39890507">[5 more]</label></div><br/><div class="children"><div class="content">Any recommendations for the latest and greatest way to run these locally?</div><br/><div id="39891794" class="c"><input type="checkbox" id="c-39891794" checked=""/><div class="controls bullet"><span class="by">slowmotiony</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890507">parent</a><span>|</span><a href="#39890577">next</a><span>|</span><label class="collapse" for="c-39891794">[-]</label><label class="expand" for="c-39891794">[1 more]</label></div><br/><div class="children"><div class="content">I use a tool called LM Studio, makes it trivial to run these models on a Mac. You can also use it as a local API so it kinda acts like a drop-in replacement for the openAI API.</div><br/></div></div><div id="39890577" class="c"><input type="checkbox" id="c-39890577" checked=""/><div class="controls bullet"><span class="by">etc-hosts</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890507">parent</a><span>|</span><a href="#39891794">prev</a><span>|</span><a href="#39890512">next</a><span>|</span><label class="collapse" for="c-39890577">[-]</label><label class="expand" for="c-39890577">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;justine.lol&#x2F;oneliners&#x2F;" rel="nofollow">https:&#x2F;&#x2F;justine.lol&#x2F;oneliners&#x2F;</a></div><br/></div></div><div id="39890512" class="c"><input type="checkbox" id="c-39890512" checked=""/><div class="controls bullet"><span class="by">speps</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890507">parent</a><span>|</span><a href="#39890577">prev</a><span>|</span><a href="#39891001">next</a><span>|</span><label class="collapse" for="c-39890512">[-]</label><label class="expand" for="c-39890512">[1 more]</label></div><br/><div class="children"><div class="content">llamafile as per TFA...</div><br/></div></div><div id="39891001" class="c"><input type="checkbox" id="c-39891001" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890507">parent</a><span>|</span><a href="#39890512">prev</a><span>|</span><a href="#39891011">next</a><span>|</span><label class="collapse" for="c-39891001">[-]</label><label class="expand" for="c-39891001">[1 more]</label></div><br/><div class="children"><div class="content">ollama</div><br/></div></div></div></div><div id="39891011" class="c"><input type="checkbox" id="c-39891011" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39890507">prev</a><span>|</span><a href="#39890772">next</a><span>|</span><label class="collapse" for="c-39891011">[-]</label><label class="expand" for="c-39891011">[1 more]</label></div><br/><div class="children"><div class="content">And why would I need to backup human knowledge as an individual</div><br/></div></div><div id="39890772" class="c"><input type="checkbox" id="c-39890772" checked=""/><div class="controls bullet"><span class="by">TheCaptain4815</span><span>|</span><a href="#39890454">parent</a><span>|</span><a href="#39891011">prev</a><span>|</span><a href="#39890694">next</a><span>|</span><label class="collapse" for="c-39890772">[-]</label><label class="expand" for="c-39890772">[4 more]</label></div><br/><div class="children"><div class="content">Itâs kind of crazy really. Before LLMs, any type of world scale disaster youâd hope for what? Wikipedia backups? Now, a single LLM ran locally would be much more effective. Imagine the local models in 5 years!</div><br/><div id="39891460" class="c"><input type="checkbox" id="c-39891460" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890772">parent</a><span>|</span><a href="#39891069">next</a><span>|</span><label class="collapse" for="c-39891460">[-]</label><label class="expand" for="c-39891460">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a lot more than just Wikipedia that gets archived, and yes, that is a far more sensible way to go about it. For one thing, the compute required to then read it back is orders of magnitude less (a 15 year old smartphone can handle it just fine). For another, you don&#x27;t have to wonder how much of what you got back is hallucinated - data is either there or it&#x27;s corrupted and unreadable.</div><br/></div></div><div id="39891069" class="c"><input type="checkbox" id="c-39891069" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890772">parent</a><span>|</span><a href="#39891460">prev</a><span>|</span><a href="#39890909">next</a><span>|</span><label class="collapse" for="c-39891069">[-]</label><label class="expand" for="c-39891069">[1 more]</label></div><br/><div class="children"><div class="content">The processing required to run current language models with a useful amount of knowledge encoded in them is way more than I imagine would be available in a &quot;world scale disaster&quot;.</div><br/></div></div><div id="39890909" class="c"><input type="checkbox" id="c-39890909" checked=""/><div class="controls bullet"><span class="by">danmur</span><span>|</span><a href="#39890454">root</a><span>|</span><a href="#39890772">parent</a><span>|</span><a href="#39891069">prev</a><span>|</span><a href="#39890694">next</a><span>|</span><label class="collapse" for="c-39890909">[-]</label><label class="expand" for="c-39890909">[1 more]</label></div><br/><div class="children"><div class="content">Uh yeah I would, and still am, take the Wikipedia backup for doomsday scenarios. I&#x27;m not even sure how that would be a competition</div><br/></div></div></div></div></div></div><div id="39890694" class="c"><input type="checkbox" id="c-39890694" checked=""/><div class="controls bullet"><span class="by">ajtulloch</span><span>|</span><a href="#39890454">prev</a><span>|</span><a href="#39890810">next</a><span>|</span><label class="collapse" for="c-39890694">[-]</label><label class="expand" for="c-39890694">[3 more]</label></div><br/><div class="children"><div class="content">- <a href="https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;flame&#x2F;laff&#x2F;pfhp&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;flame&#x2F;laff&#x2F;pfhp&#x2F;index.html</a> (e.g. here <a href="https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;flame&#x2F;laff&#x2F;pfhp&#x2F;week2-blocking-for-registers.html" rel="nofollow">https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;flame&#x2F;laff&#x2F;pfhp&#x2F;week2-blocki...</a>)<p>- <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;nadavrot&#x2F;5b35d44e8ba3dd718e595e40184d03f0" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;nadavrot&#x2F;5b35d44e8ba3dd718e595e40184...</a><p>might be of interest</div><br/><div id="39890777" class="c"><input type="checkbox" id="c-39890777" checked=""/><div class="controls bullet"><span class="by">kpw94</span><span>|</span><a href="#39890694">parent</a><span>|</span><a href="#39890810">next</a><span>|</span><label class="collapse" for="c-39890777">[-]</label><label class="expand" for="c-39890777">[2 more]</label></div><br/><div class="children"><div class="content">Great links,
especially last one referencing the Goto paper:<p><a href="https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;pingali&#x2F;CS378&#x2F;2008sp&#x2F;papers&#x2F;gotoPaper.pdf" rel="nofollow">https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;pingali&#x2F;CS378&#x2F;2008sp&#x2F;papers&#x2F;...</a><p>&gt;&gt; I believe the trick with CPU math kernels is exploiting instruction level parallelism with fewer memory references<p>It&#x27;s the collection of tricks to minimize all sort of cache misses (L1, L2, TLB, page miss etc), improve register reuse, leverage SIMD instructions, transpose one of the matrices if it provides better spatial locality, etc.</div><br/><div id="39891977" class="c"><input type="checkbox" id="c-39891977" checked=""/><div class="controls bullet"><span class="by">larodi</span><span>|</span><a href="#39890694">root</a><span>|</span><a href="#39890777">parent</a><span>|</span><a href="#39890810">next</a><span>|</span><label class="collapse" for="c-39891977">[-]</label><label class="expand" for="c-39891977">[1 more]</label></div><br/><div class="children"><div class="content">The trick is indeed to somehow imagine how the CPU works with the Lx caches and keep as much info in them as possible. So its not only about exploiting fancy instructions, but also thinking in engineering terms. Most of the software written in higher level langs cannot effectively use L1&#x2F;L2 and thus results in this constant slowing down otherwise similarly (from asymptotic analysis perspective) complexity algos.</div><br/></div></div></div></div></div></div><div id="39890810" class="c"><input type="checkbox" id="c-39890810" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#39890694">prev</a><span>|</span><a href="#39890910">next</a><span>|</span><label class="collapse" for="c-39890810">[-]</label><label class="expand" for="c-39890810">[3 more]</label></div><br/><div class="children"><div class="content">&gt; You don&#x27;t need a large computer to run a large language model<p>While running tiny llama does indeed count as running a language model, Iâm skeptical that the capabilities of doing so match what most people would consider a baseline requirement to be useful.<p>Running 10 param model is also âtechnicallyâ running an LM, and I can do it by hand with a piece of paper.<p>That doesnât mean âyou donât need a computer to run an LMââ¦<p>Iâm not sure where LM becomes LLM, butâ¦ I personally think itâs more about capability than parameter count.<p>I donât <i>realllly</i> believe you can do a lot of useful LLM work on a pi</div><br/><div id="39890860" class="c"><input type="checkbox" id="c-39890860" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#39890810">parent</a><span>|</span><a href="#39891672">next</a><span>|</span><label class="collapse" for="c-39890860">[-]</label><label class="expand" for="c-39890860">[1 more]</label></div><br/><div class="children"><div class="content">Tinyllama isn&#x27;t going to be doing what ChatGPT does, but it still beats the pants off what we had for completion or sentiment analysis 5 years ago.  And now a Pi can run it decently fast.</div><br/></div></div><div id="39891672" class="c"><input type="checkbox" id="c-39891672" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890810">parent</a><span>|</span><a href="#39890860">prev</a><span>|</span><a href="#39890910">next</a><span>|</span><label class="collapse" for="c-39891672">[-]</label><label class="expand" for="c-39891672">[1 more]</label></div><br/><div class="children"><div class="content">Some newer models trained more recently have been repeatedly shown to have comparable performance as larger models. And the Mixture of Experts architecture makes it possible to train large models that know how to selectively activate only the parts that are relevant for the current context, which drastically reduces compute demand. Smaller models can also level the playing field by being faster to process content retrieved by RAG. Via the same mechanism, they could also access larger, more powerful models for tasks that exceed their capabilities.</div><br/></div></div></div></div><div id="39890910" class="c"><input type="checkbox" id="c-39890910" checked=""/><div class="controls bullet"><span class="by">none_to_remain</span><span>|</span><a href="#39890810">prev</a><span>|</span><a href="#39891285">next</a><span>|</span><label class="collapse" for="c-39890910">[-]</label><label class="expand" for="c-39890910">[4 more]</label></div><br/><div class="children"><div class="content">From the example: &quot;--temp 0 turns off the random number generator (we don&#x27;t want improvisation for a spam filter)&quot;<p>I&#x27;ve been thinking for a while about how many applications of LLMs need this adjustment and aren&#x27;t getting it</div><br/><div id="39890973" class="c"><input type="checkbox" id="c-39890973" checked=""/><div class="controls bullet"><span class="by">mvkel</span><span>|</span><a href="#39890910">parent</a><span>|</span><a href="#39891285">next</a><span>|</span><label class="collapse" for="c-39890973">[-]</label><label class="expand" for="c-39890973">[3 more]</label></div><br/><div class="children"><div class="content">Is that what it does, though?<p>I thought setting temperature to 0 would (extremely simple example) equate to a spam filter seeing:<p>- this is a spam email<p>But if the sender adapts and says<p>- th1s is a spam email<p>It wouldn&#x27;t be flagged as spam.</div><br/><div id="39891757" class="c"><input type="checkbox" id="c-39891757" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890910">root</a><span>|</span><a href="#39890973">parent</a><span>|</span><a href="#39891051">next</a><span>|</span><label class="collapse" for="c-39891757">[-]</label><label class="expand" for="c-39891757">[1 more]</label></div><br/><div class="children"><div class="content">The output of an autoregressive model is a probability for each token to appear next after the input sequence. Computing these is strictly deterministic from the prior context and the model&#x27;s weights.<p>Based on that probability distribution, a variety of text generation strategies are possible. The simplest (greedy decoding) is picking the token with the highest probability. To allow creativity, a random number generator is used to choose among the possible outputs, biased by the probabilities of course.<p>Temperature scales the output probabilities. As temperature increases, the probabilities approach 1&#x2F;dictionary size, and the output becomes completely random. For very small temperature values, text generation approaches greedy sampling.<p>If all you want is a spam filter, better replace the output layer of an LLM with one with just two outputs, and finetune that on a public collection of spam mails and some &quot;ham&quot; from your inbox.</div><br/></div></div><div id="39891051" class="c"><input type="checkbox" id="c-39891051" checked=""/><div class="controls bullet"><span class="by">none_to_remain</span><span>|</span><a href="#39890910">root</a><span>|</span><a href="#39890973">parent</a><span>|</span><a href="#39891757">prev</a><span>|</span><a href="#39891285">next</a><span>|</span><label class="collapse" for="c-39891051">[-]</label><label class="expand" for="c-39891051">[1 more]</label></div><br/><div class="children"><div class="content">My understanding is that temperature applies to the output side and allows for some randomness in the next predicted token. Here Justine has constrained the machine to start with either &quot;yes&quot; or &quot;no&quot; and to predict only one token. This makes the issue stark: leaving a non-zero temperature here would just add a chance of flipping a boolean.</div><br/></div></div></div></div></div></div><div id="39891285" class="c"><input type="checkbox" id="c-39891285" checked=""/><div class="controls bullet"><span class="by">politelemon</span><span>|</span><a href="#39890910">prev</a><span>|</span><a href="#39890537">next</a><span>|</span><label class="collapse" for="c-39891285">[-]</label><label class="expand" for="c-39891285">[1 more]</label></div><br/><div class="children"><div class="content">This is great work. I&#x27;ve always thought it would be great if running LLM could be commoditized for regular average Joe hardware. I had thought that llamafile was like dockerfile for llama.cpp but looks like that&#x27;s a mistake?<p>Will definitely be giving this a try.</div><br/></div></div><div id="39890537" class="c"><input type="checkbox" id="c-39890537" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#39891285">prev</a><span>|</span><a href="#39890570">next</a><span>|</span><label class="collapse" for="c-39890537">[-]</label><label class="expand" for="c-39890537">[1 more]</label></div><br/><div class="children"><div class="content">Super nice story on the matmul optimization that gave 810 gflops for 512x512.  Thanks for the write up and the contributions to llama.cpp and the community more broadly.</div><br/></div></div><div id="39890570" class="c"><input type="checkbox" id="c-39890570" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#39890537">prev</a><span>|</span><a href="#39891236">next</a><span>|</span><label class="collapse" for="c-39890570">[-]</label><label class="expand" for="c-39890570">[8 more]</label></div><br/><div class="children"><div class="content">It fascinating to me that coming up on a year since Sapphire Rapids has been available in the public cloud, developers are still targeting AVX512 when they should be targeting VNNI and AMX.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;2555">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;2555</a></div><br/><div id="39890653" class="c"><input type="checkbox" id="c-39890653" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#39890570">parent</a><span>|</span><a href="#39891742">next</a><span>|</span><label class="collapse" for="c-39890653">[-]</label><label class="expand" for="c-39890653">[4 more]</label></div><br/><div class="children"><div class="content">This project in particular seems to care about the long tail of hardware; note that the very first machine in this post is a box from 2020 with spinning rust disk. Granted, adding support for newer extensions is likely also good, but cost&#x2F;benefit is in play.</div><br/><div id="39890805" class="c"><input type="checkbox" id="c-39890805" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#39890570">root</a><span>|</span><a href="#39890653">parent</a><span>|</span><a href="#39891742">next</a><span>|</span><label class="collapse" for="c-39890805">[-]</label><label class="expand" for="c-39890805">[3 more]</label></div><br/><div class="children"><div class="content">Is four years really &#x27;long tail&#x27; these days? Our VM host box is from 2010 (and I had to rebuild llama.cpp locally without AVX to get it working :P )</div><br/><div id="39891310" class="c"><input type="checkbox" id="c-39891310" checked=""/><div class="controls bullet"><span class="by">d416</span><span>|</span><a href="#39890570">root</a><span>|</span><a href="#39890805">parent</a><span>|</span><a href="#39890899">next</a><span>|</span><label class="collapse" for="c-39891310">[-]</label><label class="expand" for="c-39891310">[1 more]</label></div><br/><div class="children"><div class="content">It should be noted that while the HP  Prodesk was released in 2020, the CPUâs Skylake architecture was designed in 2014. Architecture is a significant factor in this style of engineering gymnastics to squeeze the most out of silicon.</div><br/></div></div><div id="39890899" class="c"><input type="checkbox" id="c-39890899" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#39890570">root</a><span>|</span><a href="#39890805">parent</a><span>|</span><a href="#39891310">prev</a><span>|</span><a href="#39891742">next</a><span>|</span><label class="collapse" for="c-39890899">[-]</label><label class="expand" for="c-39890899">[1 more]</label></div><br/><div class="children"><div class="content">For cutting-edge LLM work, probably? I mean, I run mine on older hardware than that, but I&#x27;m a total hobbyist...</div><br/></div></div></div></div></div></div><div id="39891742" class="c"><input type="checkbox" id="c-39891742" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39890570">parent</a><span>|</span><a href="#39890653">prev</a><span>|</span><a href="#39890886">next</a><span>|</span><label class="collapse" for="c-39891742">[-]</label><label class="expand" for="c-39891742">[1 more]</label></div><br/><div class="children"><div class="content">People with Sapphire Rapids options are not the target audience of these patches</div><br/></div></div><div id="39890886" class="c"><input type="checkbox" id="c-39890886" checked=""/><div class="controls bullet"><span class="by">luyu_wu</span><span>|</span><a href="#39890570">parent</a><span>|</span><a href="#39891742">prev</a><span>|</span><a href="#39891196">next</a><span>|</span><label class="collapse" for="c-39890886">[-]</label><label class="expand" for="c-39890886">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t believe that is the target for a local LLM... Pretty sure we&#x27;re talking about client-side computing, of which the newest supports only AVX-512 (and even that sketchily on Intel&#x27;s side).</div><br/></div></div><div id="39891196" class="c"><input type="checkbox" id="c-39891196" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#39890570">parent</a><span>|</span><a href="#39890886">prev</a><span>|</span><a href="#39891236">next</a><span>|</span><label class="collapse" for="c-39891196">[-]</label><label class="expand" for="c-39891196">[1 more]</label></div><br/><div class="children"><div class="content">Just buy a new AMD processor that supports AVX512.</div><br/></div></div></div></div><div id="39891236" class="c"><input type="checkbox" id="c-39891236" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#39890570">prev</a><span>|</span><a href="#39891055">next</a><span>|</span><label class="collapse" for="c-39891236">[-]</label><label class="expand" for="c-39891236">[2 more]</label></div><br/><div class="children"><div class="content">Nice to see such speedups for CPUs. Are these changes available as a branch or pull request in llama.cpp itself?  I&#x27;d like to make use of them in that form if possible (as I&#x27;m used to using that).</div><br/><div id="39891814" class="c"><input type="checkbox" id="c-39891814" checked=""/><div class="controls bullet"><span class="by">dagaci</span><span>|</span><a href="#39891236">parent</a><span>|</span><a href="#39891055">next</a><span>|</span><label class="collapse" for="c-39891814">[-]</label><label class="expand" for="c-39891814">[1 more]</label></div><br/><div class="children"><div class="content">Yes, this is really a phenomenal effort! And what open source is about: Bringing improvements to so many use cases. So that Intel and AMD chip uses can start to perform while taking advantage of their high-performance capabilities, making even old parts competitive.<p>There are two PRs raised to merge to llama.cpp:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;6414">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;6414</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;6412">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;6412</a><p>Hopefully these can be accepted, without drama! as there are many downstream dependencies on llama.cpp can will also benefit.<p>Though of course everyone should also look directly at releases from llamafile <a href="https:&#x2F;&#x2F;github.com&#x2F;mozilla-Ocho&#x2F;llamafile">https:&#x2F;&#x2F;github.com&#x2F;mozilla-Ocho&#x2F;llamafile</a>.</div><br/></div></div></div></div><div id="39891055" class="c"><input type="checkbox" id="c-39891055" checked=""/><div class="controls bullet"><span class="by">Ono-Sendai</span><span>|</span><a href="#39891236">prev</a><span>|</span><a href="#39890826">next</a><span>|</span><label class="collapse" for="c-39891055">[-]</label><label class="expand" for="c-39891055">[2 more]</label></div><br/><div class="children"><div class="content">Multithreading support in llama.cpp is probably still pretty busted, assuming it uses the same underlying NN inference code as whisper.cpp: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;whisper.cpp&#x2F;issues&#x2F;200#issuecomment-1484025515">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;whisper.cpp&#x2F;issues&#x2F;200#issuecom...</a></div><br/><div id="39891848" class="c"><input type="checkbox" id="c-39891848" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#39891055">parent</a><span>|</span><a href="#39890826">next</a><span>|</span><label class="collapse" for="c-39891848">[-]</label><label class="expand" for="c-39891848">[1 more]</label></div><br/><div class="children"><div class="content">From what I have heard they use manual spin locks. Generally, spin locks are not a good idea unless you want to dedicate the entire machine to a single application. If the process a spinlock waits on gets suspended, you&#x27;re burning CPU time for nothing. The OS thinks a spinlock making zero progress is actually a high priority process, so it is starving the suspended process from making progress.</div><br/></div></div></div></div><div id="39890826" class="c"><input type="checkbox" id="c-39890826" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#39891055">prev</a><span>|</span><a href="#39890621">next</a><span>|</span><label class="collapse" for="c-39890826">[-]</label><label class="expand" for="c-39890826">[3 more]</label></div><br/><div class="children"><div class="content">Is it easy to find where the matvecs are, in LLaMA (if you are someone who  is curious and wants to poke around at the âengineâ without understanding the âtransmission,â so to speak)? I was hoping to mess around with this for Stable Diffusion, but it seemed like they were buried under quite a few layers of indirection. Which is entirely reasonable, the goal is to ship software, not satisfy people whoâd just want to poke things and see what happens, haha.</div><br/><div id="39891042" class="c"><input type="checkbox" id="c-39891042" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890826">parent</a><span>|</span><a href="#39890621">next</a><span>|</span><label class="collapse" for="c-39891042">[-]</label><label class="expand" for="c-39891042">[2 more]</label></div><br/><div class="children"><div class="content">did you see tiny grad can run llama and stable diffusion? it&#x27;s an intentionally extremely simple framework vs pytorch or even micrograd, which helped me dig into the underlying math. though <a href="https:&#x2F;&#x2F;spreadsheets-are-all-you-need.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;spreadsheets-are-all-you-need.ai&#x2F;</a> is a good one for learning LLMs.</div><br/><div id="39891060" class="c"><input type="checkbox" id="c-39891060" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#39890826">root</a><span>|</span><a href="#39891042">parent</a><span>|</span><a href="#39890621">next</a><span>|</span><label class="collapse" for="c-39891060">[-]</label><label class="expand" for="c-39891060">[1 more]</label></div><br/><div class="children"><div class="content">I havenât seen that. Iâll definitely have to take a look, thanks!</div><br/></div></div></div></div></div></div><div id="39890621" class="c"><input type="checkbox" id="c-39890621" checked=""/><div class="controls bullet"><span class="by">aniijbod</span><span>|</span><a href="#39890826">prev</a><span>|</span><a href="#39890495">next</a><span>|</span><label class="collapse" for="c-39890621">[-]</label><label class="expand" for="c-39890621">[3 more]</label></div><br/><div class="children"><div class="content">A way of thinking about what&#x27;s inside any of the top LLMs right now: even if they never learn another single fact, even if they get ridiculously out of date as a result, even if they are even more riddled with errors and prone to biases than we know them to be, even if they are as prone to hallucinations as we know they they are and they never develop the capacity to cure themselves of this, they are more knowledgeable and capable of more reasoned response, despite their capacity for error, to more questions than any single human being that has ever lived.</div><br/><div id="39891806" class="c"><input type="checkbox" id="c-39891806" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39890621">parent</a><span>|</span><a href="#39890630">next</a><span>|</span><label class="collapse" for="c-39891806">[-]</label><label class="expand" for="c-39891806">[1 more]</label></div><br/><div class="children"><div class="content">We shouldn&#x27;t choose LLMs for how many facts they support, but their capability to process human language. There is some overlap between these two though, but an LLM that just doesn&#x27;t know something can always be augmented with RAG capabilities.</div><br/></div></div><div id="39890630" class="c"><input type="checkbox" id="c-39890630" checked=""/><div class="controls bullet"><span class="by">JKCalhoun</span><span>|</span><a href="#39890621">parent</a><span>|</span><a href="#39891806">prev</a><span>|</span><a href="#39890495">next</a><span>|</span><label class="collapse" for="c-39890630">[-]</label><label class="expand" for="c-39890630">[1 more]</label></div><br/><div class="children"><div class="content">Picturing &quot;LLM Jeopardy&quot;. You know, a game show.</div><br/></div></div></div></div><div id="39890495" class="c"><input type="checkbox" id="c-39890495" checked=""/><div class="controls bullet"><span class="by">1-6</span><span>|</span><a href="#39890621">prev</a><span>|</span><a href="#39891500">next</a><span>|</span><label class="collapse" for="c-39890495">[-]</label><label class="expand" for="c-39890495">[11 more]</label></div><br/><div class="children"><div class="content">Question is, how much of an improvement has it gotten to over a GPU or ASIC?</div><br/><div id="39891224" class="c"><input type="checkbox" id="c-39891224" checked=""/><div class="controls bullet"><span class="by">gpapilion</span><span>|</span><a href="#39890495">parent</a><span>|</span><a href="#39890904">next</a><span>|</span><label class="collapse" for="c-39891224">[-]</label><label class="expand" for="c-39891224">[3 more]</label></div><br/><div class="children"><div class="content">So... I was struggling with this for a while. I would says anywhere from 2x to an order of magnitude faster with a GPU. (I&#x27;ve been looking at a lot of GPU benchmarks lately, and they are REALLY hard to compare since they are all so specific)<p>I do think long term there gets to be more hope for CPUs here with inference largely because memory bandwidth becomes more important than the gpu. You can see this with reports of the MI-300 series outperforming h100, largely because it has more memory bandwidth. MCR dimms give you close to 2x the exiting memory bw in intel cpus, and when coupled with AMX you may be able to exceed v100 and might touch a100 performance levels.<p>HBM and the general GPU architecture gives it a huge memory advantage, especially with the chip to chip interface. Even adding HBM to a CPU, you are likely to find the CPU is unable to use the memory bw effectively unless it was specifically designed to use it. Then you&#x27;d still likely have limited performance with things like UPI being a really ugly bottleneck between CPUs.</div><br/><div id="39891871" class="c"><input type="checkbox" id="c-39891871" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39891224">parent</a><span>|</span><a href="#39890904">next</a><span>|</span><label class="collapse" for="c-39891871">[-]</label><label class="expand" for="c-39891871">[2 more]</label></div><br/><div class="children"><div class="content">If someone releases DDR5 or DDR6 based PIM, then most of the memory bandwidth advantage of GPUs evaporates overnight. I expect CPUs to be king at inference in the future.</div><br/><div id="39891973" class="c"><input type="checkbox" id="c-39891973" checked=""/><div class="controls bullet"><span class="by">gpapilion</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39891871">parent</a><span>|</span><a href="#39890904">next</a><span>|</span><label class="collapse" for="c-39891973">[-]</label><label class="expand" for="c-39891973">[1 more]</label></div><br/><div class="children"><div class="content">But then you&#x27;ll get GDDR6 delivered via HBM5 or whatever. I don&#x27;t think CPUs will ever really keep up with the memory bandwidth, because for most applications it doesn&#x27;t matter.<p>MCR DIMM is like 1&#x2F;2 the memory bandwidth that is possible with HBM4, plus it requires you to buy something like 2TB of memory. It might get there, but I&#x27;d keep my money on hbm and gpus.</div><br/></div></div></div></div></div></div><div id="39890904" class="c"><input type="checkbox" id="c-39890904" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#39890495">parent</a><span>|</span><a href="#39891224">prev</a><span>|</span><a href="#39891758">next</a><span>|</span><label class="collapse" for="c-39890904">[-]</label><label class="expand" for="c-39890904">[2 more]</label></div><br/><div class="children"><div class="content">I think that should be phrased more like &quot;what fraction of GPU speed can this reach?&quot;, because it&#x27;ll always be less than 1x.</div><br/></div></div><div id="39891758" class="c"><input type="checkbox" id="c-39891758" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39890495">parent</a><span>|</span><a href="#39890904">prev</a><span>|</span><a href="#39890616">next</a><span>|</span><label class="collapse" for="c-39891758">[-]</label><label class="expand" for="c-39891758">[1 more]</label></div><br/><div class="children"><div class="content">From the article, passage about the 14900k:<p>&gt; For example, when I run my spam.sh shell script, it only takes 420 milliseconds, which is 7x faster than my Raspberry Pi 5. That&#x27;s right, when it comes to small workloads, this chip is able to finish before CUDA even gets started.<p>Soâ¦ it depends :)</div><br/></div></div><div id="39890616" class="c"><input type="checkbox" id="c-39890616" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#39890495">parent</a><span>|</span><a href="#39891758">prev</a><span>|</span><a href="#39891500">next</a><span>|</span><label class="collapse" for="c-39890616">[-]</label><label class="expand" for="c-39890616">[4 more]</label></div><br/><div class="children"><div class="content">Nothing in software will ever beat an equivalent ASIC.</div><br/><div id="39890932" class="c"><input type="checkbox" id="c-39890932" checked=""/><div class="controls bullet"><span class="by">postalrat</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39890616">parent</a><span>|</span><a href="#39891671">next</a><span>|</span><label class="collapse" for="c-39890932">[-]</label><label class="expand" for="c-39890932">[1 more]</label></div><br/><div class="children"><div class="content">Sure there is. Software is easy to change.</div><br/></div></div><div id="39891671" class="c"><input type="checkbox" id="c-39891671" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39890616">parent</a><span>|</span><a href="#39890932">prev</a><span>|</span><a href="#39891087">next</a><span>|</span><label class="collapse" for="c-39891671">[-]</label><label class="expand" for="c-39891671">[1 more]</label></div><br/><div class="children"><div class="content">Most ASICs are cost or power optimizations.</div><br/></div></div><div id="39891087" class="c"><input type="checkbox" id="c-39891087" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39890495">root</a><span>|</span><a href="#39890616">parent</a><span>|</span><a href="#39891671">prev</a><span>|</span><a href="#39891500">next</a><span>|</span><label class="collapse" for="c-39891087">[-]</label><label class="expand" for="c-39891087">[1 more]</label></div><br/><div class="children"><div class="content">an asic is fixed function, so it&#x27;ll never be able to boot my pc and then be the CPU, even though an asic beats the pants off anything else computing Sha hashes for Bitcoin mining.</div><br/></div></div></div></div></div></div><div id="39891500" class="c"><input type="checkbox" id="c-39891500" checked=""/><div class="controls bullet"><span class="by">seangrogg</span><span>|</span><a href="#39890495">prev</a><span>|</span><a href="#39891879">next</a><span>|</span><label class="collapse" for="c-39891500">[-]</label><label class="expand" for="c-39891500">[1 more]</label></div><br/><div class="children"><div class="content">Mmm, I wonder how well this would work on a mobile device. Maybe I&#x27;ll try grabbing my ubuntu touch here in a sec...</div><br/></div></div><div id="39891879" class="c"><input type="checkbox" id="c-39891879" checked=""/><div class="controls bullet"><span class="by">pknerd</span><span>|</span><a href="#39891500">prev</a><span>|</span><a href="#39890514">next</a><span>|</span><label class="collapse" for="c-39891879">[-]</label><label class="expand" for="c-39891879">[1 more]</label></div><br/><div class="children"><div class="content">So, I can now run it on my 2015 Macbook with 8GB RAM?</div><br/></div></div><div id="39890514" class="c"><input type="checkbox" id="c-39890514" checked=""/><div class="controls bullet"><span class="by">discordance</span><span>|</span><a href="#39891879">prev</a><span>|</span><a href="#39891127">next</a><span>|</span><label class="collapse" for="c-39890514">[-]</label><label class="expand" for="c-39890514">[4 more]</label></div><br/><div class="children"><div class="content">&quot;As for disk speed, dd if=&#x2F;dev&#x2F;zero of=&#x2F;tmp&#x2F;output bs=128k count=50k; rm -f &#x2F;tmp&#x2F;output reports 1.6 GB&#x2F;s which is 3.6x slower than my Mac Studio, and 3x slower than my Intel (which has the same M.2 stick). I&#x27;m told that Intel and Apple are just better at this, but I wish I understood why. &quot;<p>Can anyone here answer why this is?</div><br/><div id="39890752" class="c"><input type="checkbox" id="c-39890752" checked=""/><div class="controls bullet"><span class="by">pstrateman</span><span>|</span><a href="#39890514">parent</a><span>|</span><a href="#39890853">next</a><span>|</span><label class="collapse" for="c-39890752">[-]</label><label class="expand" for="c-39890752">[1 more]</label></div><br/><div class="children"><div class="content">Apple made fsync a noop.<p>You have to make a different call to get sync on macos.<p>So tons is stuff is faster because it&#x27;s not actually writing to disk.</div><br/></div></div><div id="39890853" class="c"><input type="checkbox" id="c-39890853" checked=""/><div class="controls bullet"><span class="by">bishfish</span><span>|</span><a href="#39890514">parent</a><span>|</span><a href="#39890752">prev</a><span>|</span><a href="#39891127">next</a><span>|</span><label class="collapse" for="c-39890853">[-]</label><label class="expand" for="c-39890853">[2 more]</label></div><br/><div class="children"><div class="content">Plus he isnât using oflag=direct, so since output file is small it isnât even making it to disk.  I think it would only be sent to page cache.  Iâm afraid he is testing CPU and memory (bus) speeds here.<p>oflag=direct will write direct and bypass page cache.</div><br/><div id="39891037" class="c"><input type="checkbox" id="c-39891037" checked=""/><div class="controls bullet"><span class="by">fweimer</span><span>|</span><a href="#39890514">root</a><span>|</span><a href="#39890853">parent</a><span>|</span><a href="#39891127">next</a><span>|</span><label class="collapse" for="c-39891037">[-]</label><label class="expand" for="c-39891037">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. Something is very fishy if this system only writes 1.6 GB&#x2F;s to the page cache. Probably that dd command line quoted in the article is incomplete.</div><br/></div></div></div></div></div></div><div id="39891127" class="c"><input type="checkbox" id="c-39891127" checked=""/><div class="controls bullet"><span class="by">jongjong</span><span>|</span><a href="#39890514">prev</a><span>|</span><a href="#39890481">next</a><span>|</span><label class="collapse" for="c-39891127">[-]</label><label class="expand" for="c-39891127">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s interesting because I built a simple ANN library and I was playing around with GPU acceleration and came to a similar conclusion as this article.<p>To be fair, my ANN library was faster (up to 2x) with GPU acceleration in some scenarios were ANN was shallow (as opposed to deep with many hidden layers). I thought the marginal gain may have been because, the way it&#x27;s set up in my library, it has to load all the values into the GPU from RAM for each pass of forward and back propagation in each layer during training. I believe there is a way to allocate memory on the GPU chip itself but it&#x27;s a lot more challenging to do, especially in a modular, fully portable way (which was one of the goals of my library).<p>But anyway, even the 2x best-case figure seemed disappointing. In my mind, I expected to see at least 10x speed improvement... And I was surprised that the CPU version was actually slightly faster in the scenario I was testing at the time which was a relatively deep network. It makes sense since the different layers cannot be parallelized as the input of one layer depends on the output of the previous layer... So the more layers you have, the more serial bottlenecks you have, the less you can benefit from GPU acceleration... And unfortunately, deep networks also happen to be those which tend to perform best for a lot of use cases.</div><br/></div></div><div id="39891279" class="c"><input type="checkbox" id="c-39891279" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#39890340">prev</a><span>|</span><label class="collapse" for="c-39891279">[-]</label><label class="expand" for="c-39891279">[1 more]</label></div><br/><div class="children"><div class="content">I know this post is focused specifically on <i>CPU</i> performance, but the section on the performance on the Mac Studio seems to be deliberately avoiding directly mentioning that machine&#x27;s GPU, let alone benchmark against it. I think it would have been interesting to see a straightforward comparison of what compute performance and memory bandwidth (as measured by the prompt processing and token generation speeds, respectively) are achievable with reasonable optimization effort on the CPU vs GPU when they&#x27;re attached to the same memory subsystem.</div><br/></div></div></div></div></div></div></div></body></html>