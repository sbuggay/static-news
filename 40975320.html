<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1721552453418" as="style"/><link rel="stylesheet" href="styles.css?v=1721552453418"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2407.08790">Large models of what? Mistaking engineering achievements for linguistic agency</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>Anon84</span> | <span>67 comments</span></div><br/><div><div id="41020815" class="c"><input type="checkbox" id="c-41020815" checked=""/><div class="controls bullet"><span class="by">GeneralMayhem</span><span>|</span><a href="#41020660">next</a><span>|</span><label class="collapse" for="c-41020815">[-]</label><label class="expand" for="c-41020815">[39 more]</label></div><br/><div class="children"><div class="content">I am highly skeptical of LLMs as a mechanism to achieve AGI, but I also find this paper fairly unconvincing, bordering on tautological. I feel similarly about this as to what I&#x27;ve read of Chalmers - I agree with pretty much all of the conclusions, but I don&#x27;t feel like the text would convince me of those conclusions if I disagreed; it&#x27;s more like it&#x27;s showing me ways of explaining or illustrating what I already believed.<p>On embodiment - yes, LLMs do not have corporeal experience. But it&#x27;s not obvious that this means that they cannot, a priori, have an &quot;internal&quot; concept of reality, or that it&#x27;s impossible to gain such an understanding from text. The argument feels circular: LLMs are similar to a fake &quot;video game&quot; world because they aren&#x27;t real people - therefore, it&#x27;s wrong to think that they could be real people? And the other half of the argument is that because LLMs can only see text, they&#x27;re missing out on the wider world of non-textual communication; but then, does that mean that human writing is not &quot;real&quot; language? This argument feels especially weak in the face of multi-modal models that are in fact able to &quot;see&quot; and &quot;hear&quot;.<p>The other flavor of argument here is that LLM behavior is empirically non-human - e.g., the argument about not asking for clarification. But that only means that they aren&#x27;t <i>currently</i> matching humans, not that they <i>couldn&#x27;t</i>.<p>Basically all of these arguments feel like they fall down to the strongest counterargument I see proposed by LLM-believers, which is that sufficiently advanced mimicry is not only indistinguishable from the real thing, but at the limit in fact <i>is</i> the real thing. If we say that it&#x27;s impossible to have true language skills without implicitly having a representation of self and environment, and then we see an entity with what appears to be true language skills, we should conclude that that entity must contain within it a representation of self and environment. That argument doesn&#x27;t rely on any assumptions about the mechanism of representation other than a reliance on physicalism. Looking at it from the other direction, if you assume that all that it means to &quot;be human&quot; is encapsulated in the entropy of a human body, then that concept is necessarily describable with finite entropy. Therefore, by extension, there must be some number of parameters and some model architecture that completely encode that entropy. Questions like whether LLMs are the perfect architecture or whether the number of parameters required is a number that can be practically stored on human-manufacturable media are <i>engineering</i> questions, not philosophical ones: finite problems admit finite solutions, full stop.<p>Again, that conclusion <i>feels</i> wrong to me... but if I&#x27;m being honest with myself, I can&#x27;t point to why, other than to point at some form of dualism or spirituality as the escape hatch.</div><br/><div id="41023493" class="c"><input type="checkbox" id="c-41023493" checked=""/><div class="controls bullet"><span class="by">vouwfietsman</span><span>|</span><a href="#41020815">parent</a><span>|</span><a href="#41022941">next</a><span>|</span><label class="collapse" for="c-41023493">[-]</label><label class="expand" for="c-41023493">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t any formal &quot;proof&quot; or &quot;reasoning&quot; that shows that something cannot be AGI inherently flawed, because we have a hard time formally describing what AGI is anyway.<p>Like your argument: embodiment is missing in LLMs, but is it needed for AGI? Nobody knows.<p>I feel we first have to do a better job defining the basics of intelligence, we can then define what it means to be an AGI, and only then can we prove that something is, or is not, AGI.<p>It seems that we skipped step 1 because its too hard, and jumped straight to step 3.</div><br/></div></div><div id="41022941" class="c"><input type="checkbox" id="c-41022941" checked=""/><div class="controls bullet"><span class="by">sriku</span><span>|</span><a href="#41020815">parent</a><span>|</span><a href="#41023493">prev</a><span>|</span><a href="#41022676">next</a><span>|</span><label class="collapse" for="c-41022941">[-]</label><label class="expand" for="c-41022941">[2 more]</label></div><br/><div class="children"><div class="content">The crux of the video game analogy seems to be that when you go close to an object, the resolution starts blurring and the illusion gets broken, and there is a similar thing that happens with LLMs (as of today) as well. This is, so far, reasonable based on daily experience with these models.<p>The extension of that argument being made in the paper is that a model trained on language tokens spewed by humans is <i>incapable</i> of actually reaching that limit where this illusion will <i>never</i> breakdown in resolution. That also seems reasonable to me. They use the word &quot;languaging&quot; in verb form as opposed to &quot;language&quot; as a noun to express this.</div><br/><div id="41023226" class="c"><input type="checkbox" id="c-41023226" checked=""/><div class="controls bullet"><span class="by">8n4vidtmkvmk</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41022941">parent</a><span>|</span><a href="#41022676">next</a><span>|</span><label class="collapse" for="c-41023226">[-]</label><label class="expand" for="c-41023226">[1 more]</label></div><br/><div class="children"><div class="content">Why are LLMs incapable of reaching that limit? It&#x27;s very easy to imagine video games getting to that point. We have all the data to see objects right down to the atomic level, which is plenty more than you&#x27;d need for a game. It&#x27;s mostly a matter of compute. Why then should LLMs breakdown if they can at least mimic the smartest humans? We don&#x27;t need &quot;resolution&quot; beyond that.</div><br/></div></div></div></div><div id="41022676" class="c"><input type="checkbox" id="c-41022676" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#41020815">parent</a><span>|</span><a href="#41022941">prev</a><span>|</span><a href="#41021248">next</a><span>|</span><label class="collapse" for="c-41022676">[-]</label><label class="expand" for="c-41022676">[1 more]</label></div><br/><div class="children"><div class="content">&gt; sufficiently advanced mimicry is not only indistinguishable from the real thing, but at the limit in fact is the real thing<p>I am continually surprised at how relevant and <i>pervasive</i> one of Kurt Vonnegut’s major insights is: “we are what we pretend to be, so we must be very careful about what we pretend to be”</div><br/></div></div><div id="41021248" class="c"><input type="checkbox" id="c-41021248" checked=""/><div class="controls bullet"><span class="by">Lerc</span><span>|</span><a href="#41020815">parent</a><span>|</span><a href="#41022676">prev</a><span>|</span><a href="#41020980">next</a><span>|</span><label class="collapse" for="c-41021248">[-]</label><label class="expand" for="c-41021248">[6 more]</label></div><br/><div class="children"><div class="content">&gt;that sufficiently advanced mimicry is not only indistinguishable from the real thing, but at the limit in fact is the real thing.<p>While sufficiently does a lot of the heavy lifting here, the indistinguishable criteria implicitly means there must be no-way to tell if it is not the real thing.  The belief that it <i>is</i> the real thing comes from the intuition that anything that can be everything a person must be, but have that fundamental essence of being a person.  I don&#x27;t think people could really conceive an alternative without resorting to prejudice which they could equally apply to machines or people.<p>I take the arguments such as in this paper to be instead making the claim that because X cannot be Y you will never be able to make X indistinguishable from Y.   It is more a prediction of future failure than a judgment on an existing thing.<p>I end up looking at some of these complaints from the point of view of my sometimes profession of Game Developer.   When I show someone a game in development to playtest they will find a bunch of issues. The vast majority of those issues, not only am I already aware of, but I have a much more detailed perspective of what the problem is and how it might be fixed.  I have been seeing the problem, over and over, every day as I work.  The problem persists because there are other things to do before fixing the issue, some of which might render the issue redundant anyway.<p>I feel like a lot of the criticisms of AI are like this they are like the playtesters pointing out issues in the current state where those working on the problems are generally well aware of particular issues and have a variety of solutions in mind that might help.<p>Clear statements of deficiencies in ability are helpful as a guide to measure future success.<p>I&#x27;m also in the camp that LLM&#x27;s cannot be an AGI on its own, on the other hand I do think the architecture might be extended to become one.  There is an easy out for any criticism to say, &quot;Well, it&#x27;s not an LLM anymore&quot;.<p>In a way that ends up with a lot of people saying<p>.The current models cannot do the things we know the current models cannot do<p>.Future models will not be able to do those things if they are the same as the current ones<p>.Therefore the things that will be able to do those things will be different<p>That <i>is</i> true, but hardly enlightening.</div><br/><div id="41021703" class="c"><input type="checkbox" id="c-41021703" checked=""/><div class="controls bullet"><span class="by">et1337</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021248">parent</a><span>|</span><a href="#41020980">next</a><span>|</span><label class="collapse" for="c-41021703">[-]</label><label class="expand" for="c-41021703">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Future models will not be able to do those things if they are the same as the current ones<p>I think a lot of people disagree with this. People think if we just keep adding parameters and data, magic will happen. That’s kind of what happened with ChatGPT after all.</div><br/><div id="41022077" class="c"><input type="checkbox" id="c-41022077" checked=""/><div class="controls bullet"><span class="by">Lerc</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021703">parent</a><span>|</span><a href="#41020980">next</a><span>|</span><label class="collapse" for="c-41022077">[-]</label><label class="expand" for="c-41022077">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not so sure that view is very widespread amongst people familiar with how LLMs work.   Certainly they become more capable with parameters and data, but there are fundamental things that can&#x27;t be overcome with a basic model and I don&#x27;t think anyone is seriously arguing otherwise.<p>For instance LLMs are pretty much stateless without their context window.  If you treat the raw generated output as the first and final result then there is very little scope for any advanced consideration of anything.<p>If you give it a nice long context, give it the ability to edit that context or even access to a key-value function interface, then treat everything it says as internal monologue except for anything in &lt;aloud&gt;&lt;&#x2F;aloud&gt; tags which is what the user gets to see.   There are plenty of people who see AGI somewhere along that path, but once you take a step down that path, it&#x27;s no-longer &quot;Just an LLM&quot; the LLM is a component in a greater system.</div><br/><div id="41023196" class="c"><input type="checkbox" id="c-41023196" checked=""/><div class="controls bullet"><span class="by">lucianbr</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41022077">parent</a><span>|</span><a href="#41020980">next</a><span>|</span><label class="collapse" for="c-41023196">[-]</label><label class="expand" for="c-41023196">[3 more]</label></div><br/><div class="children"><div class="content">Has anyone done the &lt;aloud&gt; thing, and achieved some interesting results? Seems a pretty obvious thing to try, but I never heard of anything like it.</div><br/><div id="41023392" class="c"><input type="checkbox" id="c-41023392" checked=""/><div class="controls bullet"><span class="by">thomashop</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41023196">parent</a><span>|</span><a href="#41023464">next</a><span>|</span><label class="collapse" for="c-41023392">[-]</label><label class="expand" for="c-41023392">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen automated AI agents that can spend time reflecting on themselves in a feedback loop. The model alters its state over time and can call APIs.<p>You could equate saying something &quot;aloud&quot; to calling an API.</div><br/></div></div><div id="41023464" class="c"><input type="checkbox" id="c-41023464" checked=""/><div class="controls bullet"><span class="by">Lerc</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41023196">parent</a><span>|</span><a href="#41023392">prev</a><span>|</span><a href="#41020980">next</a><span>|</span><label class="collapse" for="c-41023464">[-]</label><label class="expand" for="c-41023464">[1 more]</label></div><br/><div class="children"><div class="content">I noticed some examples from anthropic&#x27;s golden-gate-claude paper had responses starting with &lt;scratchpad&gt; for the inverse effect.   Suppressing the output to the end of the paragraph would be an easy post processing operation.<p>It&#x27;s probably better to have implicitly closed tags rather than requiring a close tag. It would be quite easy for a LLM to miss a close tag and be off in a dreamland.<p>Possibly addressing comments to the user or itself might allow for considering multiple streams of thought simultaneously.  IRC logs would be decent training data for it to figure out many voice multi-conversations (maybe)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41020980" class="c"><input type="checkbox" id="c-41020980" checked=""/><div class="controls bullet"><span class="by">abernard1</span><span>|</span><a href="#41020815">parent</a><span>|</span><a href="#41021248">prev</a><span>|</span><a href="#41022998">next</a><span>|</span><label class="collapse" for="c-41020980">[-]</label><label class="expand" for="c-41020980">[27 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs do not have corporeal experience. But it&#x27;s not obvious that this means that they cannot, a priori, have an &quot;internal&quot; concept of reality, or that it&#x27;s impossible to gain such an understanding from text.<p>I would argue it is (obviously) impossible the way the current implementation of models work.<p>How could a system which produces a single next word based upon a likelihood and and a parameter called a &quot;temperature&quot; have a conceptual model underpinning it?  Even theoretically?<p>Humans and animals have an obvious conceptual understanding of the world.  Before we &quot;emit&quot; a word or a sentence, we have an idea of what we&#x27;re going to say.  This is obvious when talking to children, who know something and have a hard time saying it.  Clearly, language is not the medium in which they think or develop thoughts, merely an imperfect (and often humorous) expression of it.<p>Not so with LLMs!!  Generative LLMs do not have a prior concept available before they start emitting text.  That the &quot;temperature&quot; can chaotically change the output as the tokens proceed just goes to show there is no pre-existing concept to reference.  It looks right, and often is right, but generative systems are basically <i>always</i> hallucinating: they do not have any concepts at all.  That they are &quot;right&quot; as often as they are is a testament to the power of curve fitting and compression of basis functions in high dimensionality spaces.  But JPEGs do the same thing, and I don&#x27;t believe they have a conceptual understanding of pictures.</div><br/><div id="41022861" class="c"><input type="checkbox" id="c-41022861" checked=""/><div class="controls bullet"><span class="by">bubblyworld</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41020980">parent</a><span>|</span><a href="#41022316">next</a><span>|</span><label class="collapse" for="c-41022861">[-]</label><label class="expand" for="c-41022861">[1 more]</label></div><br/><div class="children"><div class="content">Transformer models <i>have</i> been shown to spontaneously form internal, predictive models of their input spaces. This is one of the most pervasive misunderstandings about LLMs (and other transformers) around. It is of course also true that the quality of these internal models depends a lot on the kind of task it is trained on. A GPT must be able to reproduce a huge swathe of human output, so the internal models it picks out would be those that are the most useful for that task, and might not include models of common mathematical tasks, for instance, unless they are common in the training set.<p>Have a look at the OthelloGPT papers (can provide links if you&#x27;re interested). This is one of the reasons people are so interested in them!</div><br/></div></div><div id="41022316" class="c"><input type="checkbox" id="c-41022316" checked=""/><div class="controls bullet"><span class="by">fshbbdssbbgdd</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41020980">parent</a><span>|</span><a href="#41022861">prev</a><span>|</span><a href="#41022308">next</a><span>|</span><label class="collapse" for="c-41022316">[-]</label><label class="expand" for="c-41022316">[1 more]</label></div><br/><div class="children"><div class="content">&gt; How could a system which produces a single next word based upon a likelihood and and a parameter called a &quot;temperature&quot; have a conceptual model underpinning it? Even theoretically?<p>Could a creature that simply evolved to survive and reproduce possibly have a conceptual model underpinning it? Model training and evolution are very different processes, but they are both ways of optimizing a physical system. It may be the case that evolution can give rise to intelligence and model training can’t, but we need some argument to prove that.</div><br/></div></div><div id="41022308" class="c"><input type="checkbox" id="c-41022308" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41020980">parent</a><span>|</span><a href="#41022316">prev</a><span>|</span><a href="#41021241">next</a><span>|</span><label class="collapse" for="c-41022308">[-]</label><label class="expand" for="c-41022308">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I would argue it is (obviously) impossible the way the current implementation of models work.<p>&gt; How could a system which produces a single next word based upon a likelihood and and a parameter called a &quot;temperature&quot; have a conceptual model underpinning it? Even theoretically?<p>Any probability distribution over strings can theoretically be factored into a product of such a “probability that next token is x given that the text so far is y”. Now, whether a probability distribution over strings can <i>efficiently computed</i> in this form, is another question. But, if we are being so theoretical that we don’t care about the computational cost (as long as it is finite), then the “it is next token prediction” can’t preclude anything which “it produces a probability distribution over strings” doesn’t already preclude.<p>As for the temperature, given any probability distribution over a discrete set, we can modify it by adding a temperature parameter. Just take the log of the probabilities according to the original probability distribution, scale them all by a factor (the inverse of the temperature, I think. Either that or the temperature, but I think it is the inverse of the temperature.), then exponentiate each of these, and then normalize to produce a probability distribution.<p>So, the fact that they work by next token prediction, and have a temperature parameter, cannot imply any theoretical limitation that wouldn’t apply to any other way of expressing a probability distribution over strings, as far as discussing probability distributions in the abstract, over strings, rather than talking about computational processes that implement such probability distributions over strings.<p>But also like,
going between P(next token is x | initial string so far is y) and P(the string begins with z) , isn’t <i>that</i> computationally costly?
Well, in one direction anyway.
Because like, P(next token is x|string so far is y) = P(string begins with yx) &#x2F; P(string begins with y) .<p>Though, one might object to P(string starts with y) over P(string <i>is</i> y) ?</div><br/></div></div><div id="41021241" class="c"><input type="checkbox" id="c-41021241" checked=""/><div class="controls bullet"><span class="by">gwervc</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41020980">parent</a><span>|</span><a href="#41022308">prev</a><span>|</span><a href="#41021012">next</a><span>|</span><label class="collapse" for="c-41021241">[-]</label><label class="expand" for="c-41021241">[1 more]</label></div><br/><div class="children"><div class="content">&gt; generative systems are basically always hallucinating: they do not have any concepts at all. That they are &quot;right&quot; as often as they are is a testament to the power of curve fitting and compression of basis functions in high dimensionality spaces<p>It&#x27;s refreshing to read someone who &quot;got it&quot;. Sad that before my upvote the comment was grayed out.<p>Any proponent of conceptual or other wishful&#x2F;magical thinking shoud come with proofs, since it is the hypothesis that diverge from the definition of a LLM.</div><br/></div></div><div id="41021012" class="c"><input type="checkbox" id="c-41021012" checked=""/><div class="controls bullet"><span class="by">GeneralMayhem</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41020980">parent</a><span>|</span><a href="#41021241">prev</a><span>|</span><a href="#41021240">next</a><span>|</span><label class="collapse" for="c-41021012">[-]</label><label class="expand" for="c-41021012">[20 more]</label></div><br/><div class="children"><div class="content">The argument would be that that conceptual model is encoded in the intermediate-layer parameters of the model, in a different but analogous way to how it&#x27;s encoded in the graph and chemical structure of your neurons.</div><br/><div id="41022121" class="c"><input type="checkbox" id="c-41022121" checked=""/><div class="controls bullet"><span class="by">nsagent</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021012">parent</a><span>|</span><a href="#41021080">next</a><span>|</span><label class="collapse" for="c-41022121">[-]</label><label class="expand" for="c-41022121">[1 more]</label></div><br/><div class="children"><div class="content">Using Occam&#x27;s razor, that is less probable than the model picking up on statistical regularities in human language, especially since that&#x27;s what they are trained to do.</div><br/></div></div><div id="41021080" class="c"><input type="checkbox" id="c-41021080" checked=""/><div class="controls bullet"><span class="by">abernard1</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021012">parent</a><span>|</span><a href="#41022121">prev</a><span>|</span><a href="#41021240">next</a><span>|</span><label class="collapse" for="c-41021080">[-]</label><label class="expand" for="c-41021080">[18 more]</label></div><br/><div class="children"><div class="content">I agree that&#x27;s an argument.  I would contend that argument is obviously false.  If it were true, LLMs could multiply scalar numbers together trivially.  It should be the easiest thing in the world for them.  The network required to do that well is extremely small, the parameter sizes of these models are gigantic, and the textual expression is highly regular: multiplication is the simplest concept imaginable.<p>That they cannot do that basic task implies to me that they have almost no conceptual understanding unless the fit is almost memorizable or the space is highly regular.  That LLMs can&#x27;t multiply numbers properly isn&#x27;t surprising if they don&#x27;t really understand concepts prior to emitting text.  Where they do logical tasks, that can be done with minimal or no understanding, because syllogisms and logical formalisms are highly structured in text arguments.</div><br/><div id="41021609" class="c"><input type="checkbox" id="c-41021609" checked=""/><div class="controls bullet"><span class="by">jdietrich</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021080">parent</a><span>|</span><a href="#41021649">next</a><span>|</span><label class="collapse" for="c-41021609">[-]</label><label class="expand" for="c-41021609">[13 more]</label></div><br/><div class="children"><div class="content">LLMs do contain conceptual representations and LLMs are capable of abstract reasoning. This is trivially provable by asking them to reason about something that is a) purely abstract and b) not in the training data, e.g. &quot;All floots are gronks. Some gronks are klorps. Are any floots klorps?&quot; Any of the leading LLMs will correctly answer questions of this type much more often than chance.</div><br/><div id="41021655" class="c"><input type="checkbox" id="c-41021655" checked=""/><div class="controls bullet"><span class="by">LetsGetTechnicl</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021609">parent</a><span>|</span><a href="#41022420">next</a><span>|</span><label class="collapse" for="c-41021655">[-]</label><label class="expand" for="c-41021655">[6 more]</label></div><br/><div class="children"><div class="content">That is not an example of a LLM being capable of abstract reasoning. Changing the question from &quot;What is the capital of United States?&quot; which is easily answerable to something completely abstract and &quot;not in the training model&quot; doesn&#x27;t change that LLM&#x27;s are just very advanced text prediction, and always will be. The nature of their design means they are incapable of AGI.</div><br/><div id="41023411" class="c"><input type="checkbox" id="c-41023411" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021655">parent</a><span>|</span><a href="#41021854">next</a><span>|</span><label class="collapse" for="c-41023411">[-]</label><label class="expand" for="c-41023411">[1 more]</label></div><br/><div class="children"><div class="content">&gt; LLM&#x27;s are just very advanced text prediction, and always will be<p>How do you predict the next word in answering an abstract logic question without being capable of abstract reasoning, though?<p>In some sense it probably is possible, but this is a gaping flaw in your argument. A sufficiently advanced text prediction process has to encompass the process of abstract reasoning. The text prediction problem is necessarily a superset of the abstract reasoning problem. Ie, in the limit text prediction is fundamentally harder than abstract reasoning.</div><br/></div></div><div id="41021854" class="c"><input type="checkbox" id="c-41021854" checked=""/><div class="controls bullet"><span class="by">jdietrich</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021655">parent</a><span>|</span><a href="#41023411">prev</a><span>|</span><a href="#41022734">next</a><span>|</span><label class="collapse" for="c-41021854">[-]</label><label class="expand" for="c-41021854">[3 more]</label></div><br/><div class="children"><div class="content">The question I gave is a literal textbook example of abstract reasoning. LLMs <i>are</i> just very advanced text prediction, but they are <i>also</i> provably capable of abstract reasoning. If you think that those statements are contradictory, I would encourage you to read up on the Bayesian hypotheses in cognitive science - it is highly plausible that our brains are also just very advanced prediction models.</div><br/><div id="41022104" class="c"><input type="checkbox" id="c-41022104" checked=""/><div class="controls bullet"><span class="by">nsagent</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021854">parent</a><span>|</span><a href="#41022734">next</a><span>|</span><label class="collapse" for="c-41022104">[-]</label><label class="expand" for="c-41022104">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re quite right that LLMs can seemingly do some abstract reasoning problems, but I would not say they aren&#x27;t in the training data.<p>Sure, the exact form using the made up word gronk might not be in the training data, but the general form of that reasoning problem definitely exists, quite frequently in fact.</div><br/><div id="41023164" class="c"><input type="checkbox" id="c-41023164" checked=""/><div class="controls bullet"><span class="by">jdietrich</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41022104">parent</a><span>|</span><a href="#41022734">next</a><span>|</span><label class="collapse" for="c-41023164">[-]</label><label class="expand" for="c-41023164">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but the general form of the problem tells you nothing about the answer to any specific case. To perform any better than chance, the model has to actually reason through the problem.</div><br/></div></div></div></div></div></div><div id="41022734" class="c"><input type="checkbox" id="c-41022734" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021655">parent</a><span>|</span><a href="#41021854">prev</a><span>|</span><a href="#41022420">next</a><span>|</span><label class="collapse" for="c-41022734">[-]</label><label class="expand" for="c-41022734">[1 more]</label></div><br/><div class="children"><div class="content">That’s a tautology that seems just as applicable to humans.</div><br/></div></div></div></div><div id="41022420" class="c"><input type="checkbox" id="c-41022420" checked=""/><div class="controls bullet"><span class="by">wonnage</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021609">parent</a><span>|</span><a href="#41021655">prev</a><span>|</span><a href="#41022091">next</a><span>|</span><label class="collapse" for="c-41022420">[-]</label><label class="expand" for="c-41022420">[2 more]</label></div><br/><div class="children"><div class="content">Or maybe they&#x27;re just pattern matching on the very particular sentence structure you&#x27;ve chosen. This isn&#x27;t a convincing example at all</div><br/><div id="41023253" class="c"><input type="checkbox" id="c-41023253" checked=""/><div class="controls bullet"><span class="by">jdietrich</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41022420">parent</a><span>|</span><a href="#41022091">next</a><span>|</span><label class="collapse" for="c-41023253">[-]</label><label class="expand" for="c-41023253">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t something I <i>should</i> convince you of. Just open up ChatGPT or Claude and try it for yourself. Think up a batch of your own questions and see how a modern LLM fares. I assure you that it&#x27;ll do much better than chance. If you&#x27;re so inclined, you can run enough tests to achieve statistical significance in the course of your lunch break.<p>It depresses me that we seem to be spending more time arguing and hypothesising about LLMs than empirically testing them. The question of whether LLMs can think is completely settled, as their performance at zero-shot problems is simply impossible through pure memorisation or pattern-matching. The question that remains is far more interesting - <i>how</i> do they think?<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2205.11916" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2205.11916</a></div><br/></div></div></div></div><div id="41022091" class="c"><input type="checkbox" id="c-41022091" checked=""/><div class="controls bullet"><span class="by">stirfish</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021609">parent</a><span>|</span><a href="#41022420">prev</a><span>|</span><a href="#41021649">next</a><span>|</span><label class="collapse" for="c-41022091">[-]</label><label class="expand" for="c-41022091">[4 more]</label></div><br/><div class="children"><div class="content">I just asked chatgpt<p>&quot;All floots are gronks. Some gronks are klorps. Are any floots klorps?&quot;<p>------<p>To determine if any floots are klorps, let&#x27;s analyze the given statements:<p>1. All floots are gronks. This means every floot falls into the category of gronks.
2. Some gronks are klorps. This means there is an overlap between the set of gronks and the set of klorps.<p>Since all floots are included in the set of gronks and some gronks are klorps, it is possible that some floots are klorps. However, we cannot conclusively say that any floots are klorps without additional information. It is only certain that if there is any overlap between floots and klorps, it is possible, but not guaranteed, that some floots are klorps.</div><br/><div id="41023236" class="c"><input type="checkbox" id="c-41023236" checked=""/><div class="controls bullet"><span class="by">card_zero</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41022091">parent</a><span>|</span><a href="#41021649">next</a><span>|</span><label class="collapse" for="c-41023236">[-]</label><label class="expand" for="c-41023236">[3 more]</label></div><br/><div class="children"><div class="content">Huh, almost right. (&quot;possible, but not guaranteed?&quot; it&#x27;s necessarily true. That whole sentence was a waste of space, and wrong.)<p>Edit: I mean &quot;if there is any overlap&quot;, it&#x27;s necessarily true. I should have quoted the whole thing.</div><br/><div id="41023296" class="c"><input type="checkbox" id="c-41023296" checked=""/><div class="controls bullet"><span class="by">jdietrich</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41023236">parent</a><span>|</span><a href="#41021649">next</a><span>|</span><label class="collapse" for="c-41023296">[-]</label><label class="expand" for="c-41023296">[2 more]</label></div><br/><div class="children"><div class="content">Nope, ChatGPT was right, the answer is indeterminable. The klorps that are gronks could be a wholly distinct subset to the klorps that are floots. It also correctly evaluates &quot;All gronks are floots. Some gronks are klorps. Are any floots klorps?&quot;, to which the answer is definitively yes.</div><br/><div id="41023443" class="c"><input type="checkbox" id="c-41023443" checked=""/><div class="controls bullet"><span class="by">card_zero</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41023296">parent</a><span>|</span><a href="#41021649">next</a><span>|</span><label class="collapse" for="c-41023443">[-]</label><label class="expand" for="c-41023443">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The klorps that are gronks could be a wholly distinct subset to the klorps that are floots.<p>So? It&#x27;s still the case that &quot;if there is any overlap between floots and klorps,&quot; it <i>is</i> &quot;guaranteed, that some floots are klorps.&quot; It&#x27;s tautological.<p>Unless there&#x27;s a way to read &quot;overlap&quot; so that it doesn&#x27;t mean &quot;some of one category are also in the other category, and vice versa&quot;?<p>Oh, when I said &quot;it&#x27;s necessarily true&quot; I was refering to this last sentence of the output, not the question posed in the input. Hence we are at cross purposes I think.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41021649" class="c"><input type="checkbox" id="c-41021649" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021080">parent</a><span>|</span><a href="#41021609">prev</a><span>|</span><a href="#41021510">next</a><span>|</span><label class="collapse" for="c-41021649">[-]</label><label class="expand" for="c-41021649">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If it were true, LLMs could multiply scalar numbers together trivially.<p>FWIW most large models can do it better than I can in my head.</div><br/></div></div><div id="41021510" class="c"><input type="checkbox" id="c-41021510" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021080">parent</a><span>|</span><a href="#41021649">prev</a><span>|</span><a href="#41021644">next</a><span>|</span><label class="collapse" for="c-41021510">[-]</label><label class="expand" for="c-41021510">[1 more]</label></div><br/><div class="children"><div class="content">Multiplication requires O(n^2) complexity with the usual algorithm used by humans, LLMs have a constant amount of computation available and they are not really efficient machines for math evaluation. They can definitely evaluate unseen expressions and you train a neural network to learn how to do sums and multiplications, I have trained models on sums and they are able to do sums never seen during training, the model learns the algorithm just by giving it inputs and outputs.</div><br/></div></div><div id="41022259" class="c"><input type="checkbox" id="c-41022259" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41021080">parent</a><span>|</span><a href="#41021644">prev</a><span>|</span><a href="#41021240">next</a><span>|</span><label class="collapse" for="c-41022259">[-]</label><label class="expand" for="c-41022259">[1 more]</label></div><br/><div class="children"><div class="content">&gt;If it were true, LLMs could multiply scalar numbers together trivially.<p>I mean, it&#x27;s not like GPT-4 can&#x27;t do this with more accuracy than a human without a calculator.</div><br/></div></div></div></div></div></div><div id="41021240" class="c"><input type="checkbox" id="c-41021240" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41020980">parent</a><span>|</span><a href="#41021012">prev</a><span>|</span><a href="#41021882">next</a><span>|</span><label class="collapse" for="c-41021240">[-]</label><label class="expand" for="c-41021240">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s only because you can essentially put the llms in a simulations that you can have this argument. We can imagine the human brain also in a simulation which we can replay over and over again and adjust various parameters of the physical brain to change the temperature. These sort of arguments can never distinguish between llm and humans.</div><br/></div></div><div id="41021882" class="c"><input type="checkbox" id="c-41021882" checked=""/><div class="controls bullet"><span class="by">buu700</span><span>|</span><a href="#41020815">root</a><span>|</span><a href="#41020980">parent</a><span>|</span><a href="#41021240">prev</a><span>|</span><a href="#41022998">next</a><span>|</span><label class="collapse" for="c-41021882">[-]</label><label class="expand" for="c-41021882">[1 more]</label></div><br/><div class="children"><div class="content">On that point, I would dispute the premise that &quot;it&#x27;s impossible to have true language skills without implicitly having a representation of self and environment&quot;. I don&#x27;t see any contradiction between the following two ideas:<p>1. LLMs inherently lack any form of consciousness, subjective experience, emotions, or will<p>2. A sufficiently advanced LLM with sufficient compute resources would perform on par with human intelligence at any given task, insofar as the task is applicable to LLMs</div><br/></div></div></div></div><div id="41022998" class="c"><input type="checkbox" id="c-41022998" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#41020815">parent</a><span>|</span><a href="#41020980">prev</a><span>|</span><a href="#41020660">next</a><span>|</span><label class="collapse" for="c-41022998">[-]</label><label class="expand" for="c-41022998">[1 more]</label></div><br/><div class="children"><div class="content">&gt; . I feel similarly about this as to what I&#x27;ve read of Chalmers - I agree with pretty much all of the conclusions, but I don&#x27;t feel like the text would convince me of those conclusions if I disagreed;<p>my limited experience of reading Chalmers is that he doesn&#x27;t actually present evidence - he goes on a meandering rant and then claims to have proved things that he didn&#x27;t even cover. it was the most infuriating read of my life, I heavily annotated two chapters and then finally gave up and donated the book.</div><br/></div></div></div></div><div id="41020660" class="c"><input type="checkbox" id="c-41020660" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#41020815">prev</a><span>|</span><a href="#41022370">next</a><span>|</span><label class="collapse" for="c-41020660">[-]</label><label class="expand" for="c-41020660">[4 more]</label></div><br/><div class="children"><div class="content">Full paper: [1].<p>Not much new here. The basic criticism is that LLMs are not embodied; they have no interaction with the real world. The same criticism can be applied to most office work.<p>Useful insight: &quot;We (humans) are always doing more than one thing.&quot; This is in the sense of language output having goals for the speaker, not just delivering information. This is related to the problem of LLMs losing the thread of a conversation. Probably the only reasonably new concept in this paper.<p>Standard rant: &quot;Humans are not brains that exist in a vat...&quot;<p>&quot;LLMs ... have nothing at stake.&quot; Arguable, in that some LLMs are trained using punishment. Which seems to have strong side effects. The undesirable behavior is suppressed, but so is much other behavior. That&#x27;s rather human-like.<p>&quot;LLMs Don’t Algospeak&quot;. The author means using word choices to get past dumb censorship algorithms. That&#x27;s probably do-able, if anybody cares.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2407.08790" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2407.08790</a></div><br/><div id="41021790" class="c"><input type="checkbox" id="c-41021790" checked=""/><div class="controls bullet"><span class="by">ainoobler</span><span>|</span><a href="#41020660">parent</a><span>|</span><a href="#41022370">next</a><span>|</span><label class="collapse" for="c-41021790">[-]</label><label class="expand" for="c-41021790">[3 more]</label></div><br/><div class="children"><div class="content">The optimization process adjusts the weights of a computational graph until the numeric outputs align with some baseline statistics of a large data set. There is no &quot;punishment&quot; or &quot;reward&quot;, gradient descent isn&#x27;t even necessary as there are methods for modifying the weights in other ways and the optimization still converges to a desired distribution which people claim is &quot;intelligent&quot;.<p>The converse is that people are &quot;just&quot; statistical distributions of the signals produced by them but I don&#x27;t know if there are people who claim they are nothing more than statistical distributions.<p>I think people are confused because they do not really understand how software and computers work. I&#x27;d say they should learn some computability theory to gain some clarity but I doubt they&#x27;d listen.</div><br/><div id="41022827" class="c"><input type="checkbox" id="c-41022827" checked=""/><div class="controls bullet"><span class="by">bubblyworld</span><span>|</span><a href="#41020660">root</a><span>|</span><a href="#41021790">parent</a><span>|</span><a href="#41022370">next</a><span>|</span><label class="collapse" for="c-41022827">[-]</label><label class="expand" for="c-41022827">[2 more]</label></div><br/><div class="children"><div class="content">If you really want to phrase it that way, organisms like us are &quot;just&quot; distributions of genes that have been pushed this way and that by natural selection until they converged to something we consider intelligent (humans).<p>It&#x27;s pretty clear that these optimisation processes lead to emergent behaviour, both in ML and in the natural sciences. Computability theory isn&#x27;t really relevant here.</div><br/><div id="41023019" class="c"><input type="checkbox" id="c-41023019" checked=""/><div class="controls bullet"><span class="by">ainoobler</span><span>|</span><a href="#41020660">root</a><span>|</span><a href="#41022827">parent</a><span>|</span><a href="#41022370">next</a><span>|</span><label class="collapse" for="c-41023019">[-]</label><label class="expand" for="c-41023019">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t even know where to begin to address your confusion. Without computability theory there are no computers, no operating systems, no networks, no compilers, and no high level frameworks for &quot;AI&quot;.</div><br/></div></div></div></div></div></div></div></div><div id="41022370" class="c"><input type="checkbox" id="c-41022370" checked=""/><div class="controls bullet"><span class="by">kazinator</span><span>|</span><a href="#41020660">prev</a><span>|</span><a href="#41020363">next</a><span>|</span><label class="collapse" for="c-41022370">[-]</label><label class="expand" for="c-41022370">[7 more]</label></div><br/><div class="children"><div class="content">The authors of this paper are just another instance of the AI hype being used by people who have no connection to it, to attract some kind of attention.<p>&quot;Here is what we think about this current hot topic; please read our stuff and cite generously ...&quot;<p>&gt; <i>Language completeness assumes that a distinct and complete thing such as `a natural language&#x27; exists, the essential characteristics of which can be effectively and comprehensively modelled by an LLM</i><p>Replace &quot;LLM&quot; by &quot;linguistics&quot;. Same thing.<p>&gt; <i>The assumption of data completeness relies on the belief that a language can be quantified and wholly captured by data.</i><p>That&#x27;s all that a baby has, who becomes a native speaker of their surrounding language. Language acquisition does not imply totality of data. Not every native speaker recognizes exactly the same vocabulary and exactly the same set of grammar rules.</div><br/><div id="41022908" class="c"><input type="checkbox" id="c-41022908" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#41022370">parent</a><span>|</span><a href="#41022990">next</a><span>|</span><label class="collapse" for="c-41022908">[-]</label><label class="expand" for="c-41022908">[1 more]</label></div><br/><div class="children"><div class="content">Babies have feedback and interaction with someone speaking to them. Would they learn to speak if you just dumped them in front of a TV and never spoke to them? I&#x27;m not sure.<p>But anyway I agree with you. This is just a confused HN comment in paper form.</div><br/></div></div><div id="41022990" class="c"><input type="checkbox" id="c-41022990" checked=""/><div class="controls bullet"><span class="by">JohnKemeny</span><span>|</span><a href="#41022370">parent</a><span>|</span><a href="#41022908">prev</a><span>|</span><a href="#41020363">next</a><span>|</span><label class="collapse" for="c-41022990">[-]</label><label class="expand" for="c-41022990">[5 more]</label></div><br/><div class="children"><div class="content">They are two researchers&#x2F;assistant professors working with cognitive science, psychology, and trustworthy AI. The paper is peer reviewed and has been accepted for publication in the Journal of Language Sciences.<p>You should publish your critique of their research in that same journal.<p>P.s. if you find any grave mistakes, you can contact the editor in chief, who happens to be a linguist.</div><br/><div id="41023206" class="c"><input type="checkbox" id="c-41023206" checked=""/><div class="controls bullet"><span class="by">lucianbr</span><span>|</span><a href="#41022370">root</a><span>|</span><a href="#41022990">parent</a><span>|</span><a href="#41023381">next</a><span>|</span><label class="collapse" for="c-41023206">[-]</label><label class="expand" for="c-41023206">[1 more]</label></div><br/><div class="children"><div class="content">An appeal to authority if ever there was one.<p>Their critique is written here, in plain english. Any fault with it you can just mention. The &quot;I won&#x27;t read your comment unless you get X journal to publish it&quot; seems really counterproductive. Presumably even the great Journal of Language Sciences is not above making mistakes or publishing things that are not perfect.</div><br/></div></div><div id="41023381" class="c"><input type="checkbox" id="c-41023381" checked=""/><div class="controls bullet"><span class="by">mquander</span><span>|</span><a href="#41022370">root</a><span>|</span><a href="#41022990">parent</a><span>|</span><a href="#41023206">prev</a><span>|</span><a href="#41023021">next</a><span>|</span><label class="collapse" for="c-41023381">[-]</label><label class="expand" for="c-41023381">[1 more]</label></div><br/><div class="children"><div class="content">The &quot;efficient journal hypothesis&quot; -- if something is written in a paper in a journal, then it&#x27;s impossible for anyone to know any better, since if they knew better, they would already have published the correction in a journal.</div><br/></div></div><div id="41023021" class="c"><input type="checkbox" id="c-41023021" checked=""/><div class="controls bullet"><span class="by">kazinator</span><span>|</span><a href="#41022370">root</a><span>|</span><a href="#41022990">parent</a><span>|</span><a href="#41023381">prev</a><span>|</span><a href="#41020363">next</a><span>|</span><label class="collapse" for="c-41023021">[-]</label><label class="expand" for="c-41023021">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>You should publish your critique of their research in that same journal.</i><p>No thanks; that would be at least twice removed from Making Stuff.<p>(Once removed is writing about Making Stuff.)</div><br/><div id="41023166" class="c"><input type="checkbox" id="c-41023166" checked=""/><div class="controls bullet"><span class="by">lgas</span><span>|</span><a href="#41022370">root</a><span>|</span><a href="#41023021">parent</a><span>|</span><a href="#41020363">next</a><span>|</span><label class="collapse" for="c-41023166">[-]</label><label class="expand" for="c-41023166">[1 more]</label></div><br/><div class="children"><div class="content">One might argue that a critique itself is stuff.</div><br/></div></div></div></div></div></div></div></div><div id="41020363" class="c"><input type="checkbox" id="c-41020363" checked=""/><div class="controls bullet"><span class="by">mnkv</span><span>|</span><a href="#41022370">prev</a><span>|</span><a href="#41023054">next</a><span>|</span><label class="collapse" for="c-41020363">[-]</label><label class="expand" for="c-41020363">[4 more]</label></div><br/><div class="children"><div class="content">Good summary of some of the main &quot;theoretical&quot; criticism of LLMs but I feel that it&#x27;s a bit dated and ignores the recent trend of iterative post-training, especially with human feedback. Major chatbots are no doubt being iteratively refined on the feedback from users i.e. interaction feedback, RLHF, RLAIF. So ChatGPT could fall within the sort of &quot;enactive&quot; perspective on language and definitely goes beyond the issues of static datasets and data completeness.<p>Sidenote: the authors make a mistake when citing Wittgenstein to find similarity between humans and LLMs. Language modelling on a static dataset is mostly <i>not</i> a language game (see Bender and Koller&#x27;s section on distributional semantics and caveats on learning meaning from &quot;control codes&quot;)</div><br/><div id="41020436" class="c"><input type="checkbox" id="c-41020436" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#41020363">parent</a><span>|</span><a href="#41023054">next</a><span>|</span><label class="collapse" for="c-41020436">[-]</label><label class="expand" for="c-41020436">[3 more]</label></div><br/><div class="children"><div class="content">FWIW even more recently, models have been tuned using a method called DPO instead of RLHF.<p>IIRC DPO doesn’t have human feedback in the loop</div><br/><div id="41020457" class="c"><input type="checkbox" id="c-41020457" checked=""/><div class="controls bullet"><span class="by">valec</span><span>|</span><a href="#41020363">root</a><span>|</span><a href="#41020436">parent</a><span>|</span><a href="#41022755">next</a><span>|</span><label class="collapse" for="c-41020457">[-]</label><label class="expand" for="c-41020457">[1 more]</label></div><br/><div class="children"><div class="content">it does. that&#x27;s what the &quot;direct preference&quot; part of DPO means. you just avoid training an explicit reward model on it like in rlhf and instead directly optimize for log probability of preferred vs dispreferred responses</div><br/></div></div><div id="41022755" class="c"><input type="checkbox" id="c-41022755" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#41020363">root</a><span>|</span><a href="#41020436">parent</a><span>|</span><a href="#41020457">prev</a><span>|</span><a href="#41023054">next</a><span>|</span><label class="collapse" for="c-41022755">[-]</label><label class="expand" for="c-41022755">[1 more]</label></div><br/><div class="children"><div class="content">DPO most essentially has human feedback, depends on what the preference optimizations are</div><br/></div></div></div></div></div></div><div id="41023054" class="c"><input type="checkbox" id="c-41023054" checked=""/><div class="controls bullet"><span class="by">Simon_ORourke</span><span>|</span><a href="#41020363">prev</a><span>|</span><a href="#41020762">next</a><span>|</span><label class="collapse" for="c-41023054">[-]</label><label class="expand" for="c-41023054">[1 more]</label></div><br/><div class="children"><div class="content">Where I work, there&#x27;s a somewhat haphazardly divided org structure, where my team has some responsibility to answer the executives demands for &quot;use AI to help our core business&quot;. So we applied off-the-shelf models to extract structured context from mostly unstructured text - effectively a data engineering job - and thereby support analytics and create more dashboards for the execs to mull over.<p>Another team, with a similar role in a different part of the org has jumped (feet first) into optimizing large language models to turn them into agents, without consulting the business about whether they need such things. RAG, LoRA and all this optimization is well and good, but this engineering focus has found no actual application, expect wasting several million bucks hiring staff to do something nobody wants.</div><br/></div></div><div id="41020762" class="c"><input type="checkbox" id="c-41020762" checked=""/><div class="controls bullet"><span class="by">KHRZ</span><span>|</span><a href="#41023054">prev</a><span>|</span><a href="#41023216">next</a><span>|</span><label class="collapse" for="c-41020762">[-]</label><label class="expand" for="c-41020762">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a lot of thinking they&#x27;ve done about LLMs, but how much did they actually try LLMs? I have long threads where ChatGPT refine solutions to coding problems. Their example of losing the thread after printing a tiny list of 10 philosophers seems really outdated. Also it seems LLMs utilize nested contexts as well, for example when it can break it&#x27; own rules while telling a story or speaking hypothetically.</div><br/><div id="41022039" class="c"><input type="checkbox" id="c-41022039" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#41020762">parent</a><span>|</span><a href="#41023216">next</a><span>|</span><label class="collapse" for="c-41022039">[-]</label><label class="expand" for="c-41022039">[1 more]</label></div><br/><div class="children"><div class="content">For a paper submitted on July 11, 2024, and with several references to other 2024 publications, it is indeed strange that it gives ChatGPT output from April 2023 to demonstrate that “LLMs lose the thread of a conversation with inhuman ease, as outputs are generated in response to prompts rather than a consistent, shared dialogue” (Figure 1). I have had many consistent, shared dialogues with recent versions of ChatGPT and Claude without any loss of conversation thread even after many back-and-forths.</div><br/></div></div></div></div><div id="41023216" class="c"><input type="checkbox" id="c-41023216" checked=""/><div class="controls bullet"><span class="by">flimflamm</span><span>|</span><a href="#41020762">prev</a><span>|</span><a href="#41023090">next</a><span>|</span><label class="collapse" for="c-41023216">[-]</label><label class="expand" for="c-41023216">[2 more]</label></div><br/><div class="children"><div class="content">How would the authors consider a paralyzed individual who can only move their eyes since birth? That person can learn the same concepts as other humans and communicate as richly (using only their eyes) as other humans. Clearly, the paper is viewing the problem very narrowly.</div><br/><div id="41023425" class="c"><input type="checkbox" id="c-41023425" checked=""/><div class="controls bullet"><span class="by">fairthomas</span><span>|</span><a href="#41023216">parent</a><span>|</span><a href="#41023090">next</a><span>|</span><label class="collapse" for="c-41023425">[-]</label><label class="expand" for="c-41023425">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>...a paralyzed individual who can only move their eyes since birth...</i><p>I don&#x27;t think such an individual is possible.</div><br/></div></div></div></div><div id="41023090" class="c"><input type="checkbox" id="c-41023090" checked=""/><div class="controls bullet"><span class="by">rramadass</span><span>|</span><a href="#41023216">prev</a><span>|</span><a href="#41021892">next</a><span>|</span><label class="collapse" for="c-41023090">[-]</label><label class="expand" for="c-41023090">[1 more]</label></div><br/><div class="children"><div class="content">See also <i>Beyond the Hype: A Realistic Look at Large Language Models • Jodie Burchell • GOTO 2024</i> - <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Pv0cfsastFs" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Pv0cfsastFs</a></div><br/></div></div><div id="41021892" class="c"><input type="checkbox" id="c-41021892" checked=""/><div class="controls bullet"><span class="by">nativeit</span><span>|</span><a href="#41023090">prev</a><span>|</span><a href="#41021279">next</a><span>|</span><label class="collapse" for="c-41021892">[-]</label><label class="expand" for="c-41021892">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m more or less a layperson when it comes to LLMs and this nascent concept of AI, but there&#x27;s one argument that I keep seeing that I feel like I understand, even without a thorough fluency with the underlying technology. I know that neural nets, and the mechanisms LLMs employ to train and form relational connections, can plausibly be compared to how synapses form signal paths between neurons. I can see how that makes intuitive sense.<p>I&#x27;m struggling to articulate my cognitive dissonance here, but is there any empirical evidence that LLMs, or their underlying machine learning technology, share anything at all with biological consciousness beyond a convenient metaphor for describing &quot;neural networks&quot; using terms borrowed from neuroscience? I don&#x27;t know that it necessarily follows that just because something was inspired by, or is somehow mimicking, the structure of the brain and its basic elements, that it should necessarily relate to its modeled reality in any literal way, let alone provide a sufficient basis for instantiating a phenomena we frankly know very little about. Not for nothing, but our models naturally cannot replicate any biological functions we do not fully understand. We haven&#x27;t managed to reproduce biological tissues that are exponentially less complex than the brain, are we really claiming that we&#x27;re just jumping straight past lab-grown t-bones to intelligent minds?<p>I&#x27;m sure most of the people reading this will have seen Matt Parker&#x27;s videos where they &quot;teach&quot; matchbooks to win a game against humans. Is anyone suggesting those matchbooks, given infinite time and repetition, would eventually spark emergent consciousness?<p>&gt; The argument would be that that conceptual model is encoded in the intermediate-layer parameters of the model, in a different but analogous way to how it&#x27;s encoded in the graph and chemical structure of your neurons.<p>Sorry if I have misinterpreted anyone. I honestly thought all the &quot;neuron&quot; and &quot;synapse&quot; references were handy metaphors to explain otherwise complex computations that resemble this conceptual idea of how our brains work. But it reads a lot like some of the folks in this thread believe it&#x27;s much more than metaphors, but rather a literal analog.</div><br/><div id="41023153" class="c"><input type="checkbox" id="c-41023153" checked=""/><div class="controls bullet"><span class="by">obirunda</span><span>|</span><a href="#41021892">parent</a><span>|</span><a href="#41021279">next</a><span>|</span><label class="collapse" for="c-41023153">[-]</label><label class="expand" for="c-41023153">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think anyone in research actually believes this. Note that the whole idea behind claiming &quot;scaling laws&quot; will infinitely improve these models is a funding strategy rather than a research one. None of these folks think human-like consciousness will &quot;rise&quot; from this effort, even though they veil it to continue the hype-cycle. I guarantee all these firms are desperately looking for architectural breakthroughs, even while they wax poetic about scaling laws, they know there is a bottleneck ahead.<p>Notice how LeCun is the only researcher being honest about this in a public fashion. Meta is committed to AI already and will at least match the spend of competitors anyway, so he doesn&#x27;t have as much pressure to try and convince investors that this rabbit whole is deeper.<p>Don&#x27;t get me wrong, LLMs are a tremendous improvement on knowledge compression and distillation, but it&#x27;s still unreliable enough that old school search is likely a superior method nonetheless.</div><br/></div></div></div></div><div id="41021279" class="c"><input type="checkbox" id="c-41021279" checked=""/><div class="controls bullet"><span class="by">dboreham</span><span>|</span><a href="#41021892">prev</a><span>|</span><a href="#41020579">next</a><span>|</span><label class="collapse" for="c-41021279">[-]</label><label class="expand" for="c-41021279">[2 more]</label></div><br/><div class="children"><div class="content">The first stage is denial.</div><br/><div id="41021736" class="c"><input type="checkbox" id="c-41021736" checked=""/><div class="controls bullet"><span class="by">nativeit</span><span>|</span><a href="#41021279">parent</a><span>|</span><a href="#41020579">next</a><span>|</span><label class="collapse" for="c-41021736">[-]</label><label class="expand" for="c-41021736">[1 more]</label></div><br/><div class="children"><div class="content">Well, I suppose that&#x27;s rather convenient.</div><br/></div></div></div></div><div id="41020579" class="c"><input type="checkbox" id="c-41020579" checked=""/><div class="controls bullet"><span class="by">mistrial9</span><span>|</span><a href="#41021279">prev</a><span>|</span><a href="#41022592">next</a><span>|</span><label class="collapse" for="c-41020579">[-]</label><label class="expand" for="c-41020579">[1 more]</label></div><br/><div class="children"><div class="content">oh what a kettle of worms here... Now the mind must consider &quot;repetitive speech under pressure and in formal situations&quot; in contrast and comparison to &quot;limited mechanical ability to produce grammatic sequences of well-known words&quot; .. where is the boundary there?<p>I am a fan of this paper, warts and all !  (and the paper summary paragraph contained some atrocious grammar btw)</div><br/></div></div><div id="41022592" class="c"><input type="checkbox" id="c-41022592" checked=""/><div class="controls bullet"><span class="by">amne</span><span>|</span><a href="#41020579">prev</a><span>|</span><label class="collapse" for="c-41022592">[-]</label><label class="expand" for="c-41022592">[1 more]</label></div><br/><div class="children"><div class="content">tl:dr; we&#x27;re duck-typing LLMs as AGI</div><br/></div></div></div></div></div></div></div></body></html>