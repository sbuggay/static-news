<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708592470782" as="style"/><link rel="stylesheet" href="styles.css?v=1708592470782"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/MDK8888/GPTFast">A Python Library to 6-7x the inference speed of your HF models</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>MDK8888</span> | <span>10 comments</span></div><br/><div><div id="39462948" class="c"><input type="checkbox" id="c-39462948" checked=""/><div class="controls bullet"><span class="by">MDK8888</span><span>|</span><a href="#39463578">next</a><span>|</span><label class="collapse" for="c-39462948">[-]</label><label class="expand" for="c-39462948">[5 more]</label></div><br/><div class="children"><div class="content">A few months back, the PyTorch team released GPT, Fast (<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38477197">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38477197</a>), a collection of techniques to 10x the inference speed of Llama-2-7b.<p>This library generalizes those techniques (quantization, torch.compile, speculative decoding) to all Huggingface models, achieving an inference speed speedup of 6-7x.<p>As more optimization algorithms are discovered, I will post updates to the repo to HN.</div><br/><div id="39463717" class="c"><input type="checkbox" id="c-39463717" checked=""/><div class="controls bullet"><span class="by">mkesper</span><span>|</span><a href="#39462948">parent</a><span>|</span><a href="#39463623">next</a><span>|</span><label class="collapse" for="c-39463717">[-]</label><label class="expand" for="c-39463717">[1 more]</label></div><br/><div class="children"><div class="content">Please add (something like) that to the Readme.</div><br/></div></div><div id="39463623" class="c"><input type="checkbox" id="c-39463623" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#39462948">parent</a><span>|</span><a href="#39463717">prev</a><span>|</span><a href="#39463517">next</a><span>|</span><label class="collapse" for="c-39463623">[-]</label><label class="expand" for="c-39463623">[1 more]</label></div><br/><div class="children"><div class="content">Pretty confusing that both the pytorch repo and yours is named gpt fast.</div><br/></div></div><div id="39463517" class="c"><input type="checkbox" id="c-39463517" checked=""/><div class="controls bullet"><span class="by">theboat</span><span>|</span><a href="#39462948">parent</a><span>|</span><a href="#39463623">prev</a><span>|</span><a href="#39463578">next</a><span>|</span><label class="collapse" for="c-39463517">[-]</label><label class="expand" for="c-39463517">[2 more]</label></div><br/><div class="children"><div class="content">does this work for diffusion models, e.g. stable diffusion xl?</div><br/><div id="39463569" class="c"><input type="checkbox" id="c-39463569" checked=""/><div class="controls bullet"><span class="by">jurgenaut23</span><span>|</span><a href="#39462948">root</a><span>|</span><a href="#39463517">parent</a><span>|</span><a href="#39463578">next</a><span>|</span><label class="collapse" for="c-39463569">[-]</label><label class="expand" for="c-39463569">[1 more]</label></div><br/><div class="children"><div class="content">A quick look at the codebase seems to indicate that it doesn&#x27;t. This seems very targeted at LLMs.</div><br/></div></div></div></div></div></div><div id="39463578" class="c"><input type="checkbox" id="c-39463578" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#39462948">prev</a><span>|</span><a href="#39463530">next</a><span>|</span><label class="collapse" for="c-39463578">[-]</label><label class="expand" for="c-39463578">[3 more]</label></div><br/><div class="children"><div class="content">does anyone use transformers in production? last I checked people either use exllama2, vLLM, or llama.cpp. AFAIK llama.cpp already does speculative decoding (even batched grammar based spec. decoding!).</div><br/><div id="39463798" class="c"><input type="checkbox" id="c-39463798" checked=""/><div class="controls bullet"><span class="by">daemonologist</span><span>|</span><a href="#39463578">parent</a><span>|</span><a href="#39464660">next</a><span>|</span><label class="collapse" for="c-39463798">[-]</label><label class="expand" for="c-39463798">[1 more]</label></div><br/><div class="children"><div class="content">We use transformers in production for T5 models, which aren&#x27;t supported by the usual suspects.  I did a bit of poking around and might try out TensorRT or CTranslate2 at some point; for now the single GPU we have dedicated to the project has been able to keep up with incoming data, but I&#x27;ll probably have to address it in the next couple of months.<p>If anyone has suggestions for other T5 inference options I would welcome them.</div><br/></div></div><div id="39464660" class="c"><input type="checkbox" id="c-39464660" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39463578">parent</a><span>|</span><a href="#39463798">prev</a><span>|</span><a href="#39463530">next</a><span>|</span><label class="collapse" for="c-39464660">[-]</label><label class="expand" for="c-39464660">[1 more]</label></div><br/><div class="children"><div class="content">vLLM uses transformers.</div><br/></div></div></div></div><div id="39463530" class="c"><input type="checkbox" id="c-39463530" checked=""/><div class="controls bullet"><span class="by">bradneuberg</span><span>|</span><a href="#39463578">prev</a><span>|</span><label class="collapse" for="c-39463530">[-]</label><label class="expand" for="c-39463530">[1 more]</label></div><br/><div class="children"><div class="content">Has anyone tried this on OpenCLIP based models?</div><br/></div></div></div></div></div></div></div></body></html>