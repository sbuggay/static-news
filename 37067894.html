<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1691658067940" as="style"/><link rel="stylesheet" href="styles.css?v=1691658067940"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://dl.acm.org/doi/pdf/10.1145/3570638">Optimization Techniques for GPU Programming [pdf]</a> <span class="domain">(<a href="https://dl.acm.org">dl.acm.org</a>)</span></div><div class="subtext"><span>ibobev</span> | <span>28 comments</span></div><br/><div><div id="37070295" class="c"><input type="checkbox" id="c-37070295" checked=""/><div class="controls bullet"><span class="by">jhj</span><span>|</span><a href="#37069058">next</a><span>|</span><label class="collapse" for="c-37070295">[-]</label><label class="expand" for="c-37070295">[2 more]</label></div><br/><div class="children"><div class="content">Cool, this definitely seems like a good enumeration of techniques, nice to see that they discuss stuff like kernel <i>fission</i> as well. Having a good understanding of loop nest optimization transformations (tiling, fission, fusion, strip mining&#x2F;sinking, iteration order changes, etc) provides a good vocabulary for talking about this stuff too.<p>As someone who has spent 80%+ of their time CUDA programming for the past 9 years (I wrote the original GPU PyTorch tensor library, the Faiss GPU library, and several things that Nvidia took and put into cuDNN), I found the most instructive, short yet &quot;advanced&quot; education on the subject to be Paulius Micikevicius&#x27; various slide decks on &quot;Peformance Optimization&quot;; e.g.:<p><a href="https:&#x2F;&#x2F;on-demand.gputechconf.com&#x2F;gtc&#x2F;2013&#x2F;presentations&#x2F;S3466-Programming-Guidelines-GPU-Architecture.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;on-demand.gputechconf.com&#x2F;gtc&#x2F;2013&#x2F;presentations&#x2F;S34...</a><p>(there are some other ones outstanding, think one was for the Volta architecture as well)<p>They&#x27;re old but still very relevant to today&#x27;s GPUs.</div><br/><div id="37072619" class="c"><input type="checkbox" id="c-37072619" checked=""/><div class="controls bullet"><span class="by">hazz99</span><span>|</span><a href="#37070295">parent</a><span>|</span><a href="#37069058">next</a><span>|</span><label class="collapse" for="c-37072619">[-]</label><label class="expand" for="c-37072619">[1 more]</label></div><br/><div class="children"><div class="content">Do you have any career advice for someone deeply interested in breaking into high performance GPU programming? I find resources like these, and projects like OpenAIs Triton compiler or MIMD-on-GPU so incredibly interesting.<p>But I have no idea who employs those skills! Beyond scientific HPC groups or ML research teams anyway - I doubt they’d accept someone without a PhD.<p>My current gameplan is getting through “Professional CUDA C programming” and various computer architecture textbooks, and seeing if that’s enough.</div><br/></div></div></div></div><div id="37069058" class="c"><input type="checkbox" id="c-37069058" checked=""/><div class="controls bullet"><span class="by">flakiness</span><span>|</span><a href="#37070295">prev</a><span>|</span><a href="#37069323">next</a><span>|</span><label class="collapse" for="c-37069058">[-]</label><label class="expand" for="c-37069058">[7 more]</label></div><br/><div class="children"><div class="content">To ones who are interested: &quot;Programming Massively Parallel Processors: A Hands-on Approach&quot; is a great book to learn CUDA programming, and it talks mostly about performance because, after all, GPU is about speed.<p>Unlike normal programming books, it talks a lot about how GPUs work and how the introduced techniques fit in that picture. It&#x27;s interesting even if you are just curious how a (NVIDIA) GPU works at code-level. Strongly recommended.</div><br/><div id="37073539" class="c"><input type="checkbox" id="c-37073539" checked=""/><div class="controls bullet"><span class="by">AlexDenisov</span><span>|</span><a href="#37069058">parent</a><span>|</span><a href="#37069426">next</a><span>|</span><label class="collapse" for="c-37073539">[-]</label><label class="expand" for="c-37073539">[1 more]</label></div><br/><div class="children"><div class="content">There are also video lectures which are almost 1-1 mapping of the book<p>Programming Massively Parallel Processors:
<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=4pkbXmE4POc&amp;list=PLRRuQYjFhpmubuwx-w8X964ofVkW1T8O4">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=4pkbXmE4POc&amp;list=PLRRuQYjFhp...</a></div><br/></div></div><div id="37069426" class="c"><input type="checkbox" id="c-37069426" checked=""/><div class="controls bullet"><span class="by">gpuhacker</span><span>|</span><a href="#37069058">parent</a><span>|</span><a href="#37073539">prev</a><span>|</span><a href="#37072397">next</a><span>|</span><label class="collapse" for="c-37069426">[-]</label><label class="expand" for="c-37069426">[3 more]</label></div><br/><div class="children"><div class="content">I bought the first edition when it came out, and definitely it was a gold mine of information on the subject. I wonder though, is the fourth edition worth buying another copy? Nvidia has been advancing CUDA, in particular moving more towards C++ in the kernel language. But none of that was present when this book came out in 2007. Now more and more stuff is happening at thread block level with the cooperative group C++ API and warp level for tensor cores. It would be great if the authors revisited all the early chapters to modernize that content, but that&#x27;s a lot of work so I don&#x27;t usually count on authors making such an effort for later editions.</div><br/><div id="37070180" class="c"><input type="checkbox" id="c-37070180" checked=""/><div class="controls bullet"><span class="by">flakiness</span><span>|</span><a href="#37069058">root</a><span>|</span><a href="#37069426">parent</a><span>|</span><a href="#37072397">next</a><span>|</span><label class="collapse" for="c-37070180">[-]</label><label class="expand" for="c-37070180">[2 more]</label></div><br/><div class="children"><div class="content">I also read the older edition and got the 4th for the second read recently.
I felt that the updated coverage is more on the GPU side than the language side.
It covers new GPU features and architectures well. I don&#x27;t think it covers Tensor core things. But I might be wrong.<p>So it&#x27;s worth the update if you&#x27;re interested in general NVIDIA GPU evolution.</div><br/><div id="37071925" class="c"><input type="checkbox" id="c-37071925" checked=""/><div class="controls bullet"><span class="by">gpuhacker</span><span>|</span><a href="#37069058">root</a><span>|</span><a href="#37070180">parent</a><span>|</span><a href="#37072397">next</a><span>|</span><label class="collapse" for="c-37071925">[-]</label><label class="expand" for="c-37071925">[1 more]</label></div><br/><div class="children"><div class="content">Ah thanks! That&#x27;s good to know.</div><br/></div></div></div></div></div></div><div id="37072397" class="c"><input type="checkbox" id="c-37072397" checked=""/><div class="controls bullet"><span class="by">hgomersall</span><span>|</span><a href="#37069058">parent</a><span>|</span><a href="#37069426">prev</a><span>|</span><a href="#37069400">next</a><span>|</span><label class="collapse" for="c-37072397">[-]</label><label class="expand" for="c-37072397">[1 more]</label></div><br/><div class="children"><div class="content">How does it compare to the docs from Nvidia, which always struck me as fairly comprehensive?</div><br/></div></div><div id="37069400" class="c"><input type="checkbox" id="c-37069400" checked=""/><div class="controls bullet"><span class="by">mathisfun123</span><span>|</span><a href="#37069058">parent</a><span>|</span><a href="#37072397">prev</a><span>|</span><a href="#37069323">next</a><span>|</span><label class="collapse" for="c-37069400">[-]</label><label class="expand" for="c-37069400">[1 more]</label></div><br/><div class="children"><div class="content">&gt; it talks a lot about how GPUs work<p>it&#x27;s true - out of all of the &quot;LEARN CUDA IN 24 HOURS&quot; books, this is the best one. indeed this isn&#x27;t one of those same books - this is a textbook - but at first glance it resembles them (at least the color scheme and the title led me astray when i first found it).</div><br/></div></div></div></div><div id="37069323" class="c"><input type="checkbox" id="c-37069323" checked=""/><div class="controls bullet"><span class="by">w-m</span><span>|</span><a href="#37069058">prev</a><span>|</span><a href="#37069345">next</a><span>|</span><label class="collapse" for="c-37069323">[-]</label><label class="expand" for="c-37069323">[2 more]</label></div><br/><div class="children"><div class="content">Does anybody have an idea on how to get in to Metal programming (as in Apple Metal)? I&#x27;d love to mess around a little with this on iOS and macOS while learning about tile-based rendering, but I have trouble locating educational written material.<p>There&#x27;s a book (<a href="https:&#x2F;&#x2F;metalbyexample.com&#x2F;the-book&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;metalbyexample.com&#x2F;the-book&#x2F;</a>), but the author has put up a note that it&#x27;s quite out of date. It seems the most up-to-date information is available in the WWDC videos (regarding e.g. Metal 3), but I&#x27;d really prefer something written. And Apple&#x27;s documentation reads more like a reference material and is quite confusing when starting out.</div><br/><div id="37069376" class="c"><input type="checkbox" id="c-37069376" checked=""/><div class="controls bullet"><span class="by">winwang</span><span>|</span><a href="#37069323">parent</a><span>|</span><a href="#37069345">next</a><span>|</span><label class="collapse" for="c-37069376">[-]</label><label class="expand" for="c-37069376">[1 more]</label></div><br/><div class="children"><div class="content">(+1) I&#x27;m a newb to Metal myself, and I wanted to use Swift as the driving language (which was a main selling point). Unfortunately, almost all the material is in Objective C.</div><br/></div></div></div></div><div id="37069345" class="c"><input type="checkbox" id="c-37069345" checked=""/><div class="controls bullet"><span class="by">winwang</span><span>|</span><a href="#37069323">prev</a><span>|</span><a href="#37068788">next</a><span>|</span><label class="collapse" for="c-37069345">[-]</label><label class="expand" for="c-37069345">[1 more]</label></div><br/><div class="children"><div class="content">If people like GPU programming, I wrote a blog post this week about GPU-accelerated hashmaps, semi-provocatively titled &quot;Can we 10x Rust hashmap throughput?&quot;.<p>HN post here:  
<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37036058">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37036058</a></div><br/></div></div><div id="37068788" class="c"><input type="checkbox" id="c-37068788" checked=""/><div class="controls bullet"><span class="by">eachro</span><span>|</span><a href="#37069345">prev</a><span>|</span><a href="#37069602">next</a><span>|</span><label class="collapse" for="c-37068788">[-]</label><label class="expand" for="c-37068788">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been looking into getting into GPU programming, starting with CS334 (<a href="https:&#x2F;&#x2F;developer.nvidia.com&#x2F;udacity-cs344-intro-parallel-programming" rel="nofollow noreferrer">https:&#x2F;&#x2F;developer.nvidia.com&#x2F;udacity-cs344-intro-parallel-pr...</a>) on Udacity. I&#x27;m curious to hear from some of the more seasoned GPU veterans out there, what other resources would be good to take a look at after finishing the videos and assignments?</div><br/><div id="37069328" class="c"><input type="checkbox" id="c-37069328" checked=""/><div class="controls bullet"><span class="by">gpuhacker</span><span>|</span><a href="#37068788">parent</a><span>|</span><a href="#37069313">next</a><span>|</span><label class="collapse" for="c-37069328">[-]</label><label class="expand" for="c-37069328">[2 more]</label></div><br/><div class="children"><div class="content">If you want to go really in-depth I can recommend GTC on demand. It&#x27;s Nvidia streaming platform with videos from past GTC conferences. Tony Scuderio had a couple of videos on there called GPU memory bootcamp that are among the best advanced GPU programming learning material out there.</div><br/><div id="37071949" class="c"><input type="checkbox" id="c-37071949" checked=""/><div class="controls bullet"><span class="by">zetazzed</span><span>|</span><a href="#37068788">root</a><span>|</span><a href="#37069328">parent</a><span>|</span><a href="#37069313">next</a><span>|</span><label class="collapse" for="c-37071949">[-]</label><label class="expand" for="c-37071949">[1 more]</label></div><br/><div class="children"><div class="content">100% this. You can find all kinds of detailed topics, like CUDA graphs, memory layout optimization, optimizing storage access, etc. <a href="https:&#x2F;&#x2F;www.nvidia.com&#x2F;en-us&#x2F;on-demand&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nvidia.com&#x2F;en-us&#x2F;on-demand&#x2F;</a>. They have &quot;playlists&quot; for things like HPC or development tools that collect the most popular videos on those topics.</div><br/></div></div></div></div><div id="37069313" class="c"><input type="checkbox" id="c-37069313" checked=""/><div class="controls bullet"><span class="by">yzh</span><span>|</span><a href="#37068788">parent</a><span>|</span><a href="#37069328">prev</a><span>|</span><a href="#37069030">next</a><span>|</span><label class="collapse" for="c-37069313">[-]</label><label class="expand" for="c-37069313">[1 more]</label></div><br/><div class="children"><div class="content">I would recommend the course from Oxford (<a href="https:&#x2F;&#x2F;people.maths.ox.ac.uk&#x2F;gilesm&#x2F;cuda&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;people.maths.ox.ac.uk&#x2F;gilesm&#x2F;cuda&#x2F;</a>). Also explore the tutorial section of cutlass (<a href="https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;cutlass&#x2F;blob&#x2F;main&#x2F;media&#x2F;docs&#x2F;cute&#x2F;00_quickstart.md#tutorial">https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;cutlass&#x2F;blob&#x2F;main&#x2F;media&#x2F;docs&#x2F;cute&#x2F;...</a>) if you want to learn more about high performance gemm.
OpenAI triton is another good resource if you want to write relatively performant cuda kernels using python for deep learning (<a href="https:&#x2F;&#x2F;openai.com&#x2F;research&#x2F;triton" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;research&#x2F;triton</a>)</div><br/></div></div><div id="37069030" class="c"><input type="checkbox" id="c-37069030" checked=""/><div class="controls bullet"><span class="by">pengaru</span><span>|</span><a href="#37068788">parent</a><span>|</span><a href="#37069313">prev</a><span>|</span><a href="#37069602">next</a><span>|</span><label class="collapse" for="c-37069030">[-]</label><label class="expand" for="c-37069030">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;shadertoy.com" rel="nofollow noreferrer">https:&#x2F;&#x2F;shadertoy.com</a> is a great way to explore shaders</div><br/></div></div></div></div><div id="37069602" class="c"><input type="checkbox" id="c-37069602" checked=""/><div class="controls bullet"><span class="by">AndrewPGameDev</span><span>|</span><a href="#37068788">prev</a><span>|</span><a href="#37071317">next</a><span>|</span><label class="collapse" for="c-37069602">[-]</label><label class="expand" for="c-37069602">[3 more]</label></div><br/><div class="children"><div class="content">Interesting timing on posting this to HN, I&#x27;ve recently been optimizing my WebGPU LSD radix sort. Today I measured it against the Thrust CUDA version, and it&#x27;s about 10x slower (15ms to 1.5ms). My goal was to try to get 10 million elements in 1 ms, but now that I know 3 million in 1.5ms is impossible even for Thrust I know I won&#x27;t be able to beat that.</div><br/><div id="37071970" class="c"><input type="checkbox" id="c-37071970" checked=""/><div class="controls bullet"><span class="by">gpuhacker</span><span>|</span><a href="#37069602">parent</a><span>|</span><a href="#37071317">next</a><span>|</span><label class="collapse" for="c-37071970">[-]</label><label class="expand" for="c-37071970">[2 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t tried WebGPU yet, is there an overall performance hit compared to direct CUDA programming?<p>AFAIK Thrust is intended to simplify GPU programming. It could well be that for specific use cases, in particular when it is possible to fuse multiple operations into single kernels, you could outperform Thrust.</div><br/><div id="37073325" class="c"><input type="checkbox" id="c-37073325" checked=""/><div class="controls bullet"><span class="by">AndrewPGameDev</span><span>|</span><a href="#37069602">root</a><span>|</span><a href="#37071970">parent</a><span>|</span><a href="#37071317">next</a><span>|</span><label class="collapse" for="c-37073325">[-]</label><label class="expand" for="c-37073325">[1 more]</label></div><br/><div class="children"><div class="content">There is definitely at least a performance hit in that wgpu (and I think WebGPU too) only supports a single queue. That means you can&#x27;t asynchronously run compute tasks while running render tasks.<p>Additionally Wgpu (the library) will insert fences between all passes that have a read-write dependency on a binding, even if there is technically no fence needed as 2 passes might not access the same indices.<p>Finally I know that there is an algorithm called decoupled look back that can speed up prefix sums, but it requires a forward-progress guarantee. All recent NVIDIA cards can run it but I don&#x27;t think AMD can, so WebGPU can&#x27;t in general. Raph Levien has a blog post on the subject <a href="https:&#x2F;&#x2F;raphlinus.github.io&#x2F;gpu&#x2F;2021&#x2F;11&#x2F;17&#x2F;prefix-sum-portable.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;raphlinus.github.io&#x2F;gpu&#x2F;2021&#x2F;11&#x2F;17&#x2F;prefix-sum-portab...</a></div><br/></div></div></div></div></div></div><div id="37071317" class="c"><input type="checkbox" id="c-37071317" checked=""/><div class="controls bullet"><span class="by">pavelstoev</span><span>|</span><a href="#37069602">prev</a><span>|</span><a href="#37069952">next</a><span>|</span><label class="collapse" for="c-37071317">[-]</label><label class="expand" for="c-37071317">[2 more]</label></div><br/><div class="children"><div class="content">Humble self-promo here, may I also recommend the team at CentML who dedicated their academic life (PhD and above) to GPU optimizations for high-performance ML&#x2F;AI to lower the costs.</div><br/><div id="37071584" class="c"><input type="checkbox" id="c-37071584" checked=""/><div class="controls bullet"><span class="by">johnthescott</span><span>|</span><a href="#37071317">parent</a><span>|</span><a href="#37069952">next</a><span>|</span><label class="collapse" for="c-37071584">[-]</label><label class="expand" for="c-37071584">[1 more]</label></div><br/><div class="children"><div class="content">getting errors when registering on centml website.</div><br/></div></div></div></div><div id="37069952" class="c"><input type="checkbox" id="c-37069952" checked=""/><div class="controls bullet"><span class="by">_a_a_a_</span><span>|</span><a href="#37071317">prev</a><span>|</span><label class="collapse" for="c-37069952">[-]</label><label class="expand" for="c-37069952">[5 more]</label></div><br/><div class="children"><div class="content">Partly related I believe so perhaps someone can help. Whole theses have been written on prefix sum algorithms, and I never got it. Perhaps someone kind can give some convincing examples of their advantages.</div><br/><div id="37072536" class="c"><input type="checkbox" id="c-37072536" checked=""/><div class="controls bullet"><span class="by">mschuetz</span><span>|</span><a href="#37069952">parent</a><span>|</span><a href="#37070339">next</a><span>|</span><label class="collapse" for="c-37072536">[-]</label><label class="expand" for="c-37072536">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s used in one of the fastest sorting approaches - counting sort &#x2F; binning - to compute the location of where to store the sorted&#x2F;binned items. First you count the number of items per bin, then you use prefix-sums to compute the memory location of each bin, then you insert the items into the respective bins. Some radix-sort implementations also utilize counting sort under the hood, and therefore prefix-sums. (Not sure if all radix-sort implementations need it)</div><br/></div></div><div id="37070339" class="c"><input type="checkbox" id="c-37070339" checked=""/><div class="controls bullet"><span class="by">jhj</span><span>|</span><a href="#37069952">parent</a><span>|</span><a href="#37072536">prev</a><span>|</span><a href="#37072049">next</a><span>|</span><label class="collapse" for="c-37070339">[-]</label><label class="expand" for="c-37070339">[1 more]</label></div><br/><div class="children"><div class="content">Not speaking to their implementation, but prefix sums&#x2F;scans are simply a very useful primitive tool for parallelizing many otherwise sequential operations. For instance, appending a variable number of items per worker to a shared coalesced buffer uses an exclusive prefix sum. This is probably the most common use case for them in practical programming. They can also be used to partition work across parallel workers (segmented prefix scans).<p>In lieu of pointer chasing, hashing and the like, parallel operations on flat arrays are the way to maximize GPU utilization.</div><br/></div></div><div id="37072049" class="c"><input type="checkbox" id="c-37072049" checked=""/><div class="controls bullet"><span class="by">gpuhacker</span><span>|</span><a href="#37069952">parent</a><span>|</span><a href="#37070339">prev</a><span>|</span><a href="#37071347">next</a><span>|</span><label class="collapse" for="c-37072049">[-]</label><label class="expand" for="c-37072049">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s incredibly useful if you have many threads that produce a variable number of outputs. Imagine you&#x27;re implementing some filtering operation on the GPU, many threads will take on a fixed workload and then produce some number of outcomes. Unless we take some precautions, we have a huge synchronization problem when all threads try to append their results to the output. Note that GPUs didn&#x27;t have atomics for the first couple of generations that supported CUDA, so you couldn&#x27;t just getAndIncrement an index and append to an array. We could store those outputs in a dense structure, allocating a fixed number of output slots per thread, but that would leave many blanks in between the results. Now once we know the number of outputs per thread we can use a prefix sum to let every thread know where they can write their results in the array.<p>The outcome of a prefix sum exactly corresponds with the &quot;row starts&quot; part of the CSR sparse matrix notation. So they are also essential when creating sparse matrices.</div><br/></div></div><div id="37071347" class="c"><input type="checkbox" id="c-37071347" checked=""/><div class="controls bullet"><span class="by">shwestrick</span><span>|</span><a href="#37069952">parent</a><span>|</span><a href="#37072049">prev</a><span>|</span><label class="collapse" for="c-37071347">[-]</label><label class="expand" for="c-37071347">[1 more]</label></div><br/><div class="children"><div class="content">Tons and tons of parallel algorithms use prefix sums. Typically the most common use is to compute a collection of offsets in parallel. Some examples:<p>- compact a hash table (i.e., remove the empty slots)<p>- flatten a jagged 2D array<p>- rewrite a dense matrix in compressed-sparse-row (CSR) format</div><br/></div></div></div></div></div></div></div></div></div></body></html>