<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1733302858227" as="style"/><link rel="stylesheet" href="styles.css?v=1733302858227"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.phoronix.com/review/intel-arc-b580-battlemage#google_vignette">Intel announces Arc B-series &quot;Battlemage&quot; discrete graphics with Linux support</a> <span class="domain">(<a href="https://www.phoronix.com">www.phoronix.com</a>)</span></div><div class="subtext"><span>rbanffy</span> | <span>530 comments</span></div><br/><div><div id="42309670" class="c"><input type="checkbox" id="c-42309670" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42309145">next</a><span>|</span><label class="collapse" for="c-42309670">[-]</label><label class="expand" for="c-42309670">[235 more]</label></div><br/><div class="children"><div class="content">Why don&#x27;t they just release a basic GPU with 128GB RAM and eat NVidia&#x27;s local generative AI lunch? The networking effect of all devs porting their LLMs etc. to that card would instantly put them as a major CUDA threat. But beancounters running the company would never get such an idea...</div><br/><div id="42311342" class="c"><input type="checkbox" id="c-42311342" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42309957">next</a><span>|</span><label class="collapse" for="c-42311342">[-]</label><label class="expand" for="c-42311342">[65 more]</label></div><br/><div class="children"><div class="content">Disclosure: HPC admin who works with NIVIDA cards here.<p>Because, no. It&#x27;s not as simple as that.<p>NVIDIA has a complete ecosystem now. They have cards. They have cards of cards (platforms), which they produce, validate and sell. They have NVLink crossbars and switches which connects these cards on their card of cards with very high speeds and low latency.<p>For inter-server communication they have libraries which coordinate cards, workloads and computations.<p>They bought Mellanox, but that can be used by anyone, so there&#x27;s no lock-in for now.<p>As a tangent, NVIDIA has a whole set of standards for pumping tremendous amount of data in and out of these mesh of cards. Let it be GPU-Direct storage or specialized daemons which handle data transfers on and off cards.<p>If you think that you can connect n cards on PCIe bus and just send workloads to them and solve problems magically, you&#x27;ll hurt yourself a lot, both performance and psychology wise.<p>You have to build a stack which can perform these things with maximum possible performance to be able to compute with NVIDIA. It&#x27;s not just emulating CUDA, now. Esp., on the high end of the AI spectrum (GenAI, MultiCard, MultiSystem, etc.).<p>For other lower end, multi-tenant scenarios, they have card virtualization, MIG, etc. for card sharing. You have to complete on that, too, for cloud and smaller applications.</div><br/><div id="42313615" class="c"><input type="checkbox" id="c-42313615" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42312776">next</a><span>|</span><label class="collapse" for="c-42313615">[-]</label><label class="expand" for="c-42313615">[9 more]</label></div><br/><div class="children"><div class="content">I have been hacking on local llama 3 inference software (for the CPU, but I have been thinking about how I would port it to a GPU) and would like to do a rebuttal:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ryao&#x2F;llama3.c">https:&#x2F;&#x2F;github.com&#x2F;ryao&#x2F;llama3.c</a><p>Inference workloads are easy to parallelize to N cards with minimal connectivity between them. The Nvlink crossbars and switches just are not needed.<p>In particular, inference can be divided into two distinct phases, which are input processing (prompt processing) and output generation (token generation). They are remarkably different in their requirements. Input processing is compute bound via GEMM operations while output generation is memory bandwidth bound via GEMV operations. Technically, you can do the input processing via GEMV too by processing 1 token at a time, but that is slow, so you do not want to do that. Anyway, these phases can be further subdivided into the model’s layers. You can have 1 GPU per layer with the logits passing from GPU to GPU in a pipeline. The GPUs just need the layer’s weights and the key-value cache for all of the tokens in that layer in memory to be able to work effectively. For llama 3.1 405B, there are 126 layers, so that is up to 126 GPUs.<p>That is of course slightly slower than if you just had 1 GPU with an incredible amount of VRAM, but you can always have more than one query in flight to get better than 1 GPU’s worth of performance from this pipeline approach. There are other ways of doing parallelization too, such as having output processing use GEMM to do multiple queries in parallel. This would be what others call batching, although I am only interested in doing 1 query at a time right now, so I have not touched it.<p>In essence, you can connect n cards on PCIe and have them solve inferencing problems magically, with the right software. Training is a different matter and I cannot comment on it as I have not studied it yet.</div><br/><div id="42314425" class="c"><input type="checkbox" id="c-42314425" checked=""/><div class="controls bullet"><span class="by">derefr</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313615">parent</a><span>|</span><a href="#42314401">next</a><span>|</span><label class="collapse" for="c-42314425">[-]</label><label class="expand" for="c-42314425">[3 more]</label></div><br/><div class="children"><div class="content">I presume the counterargument is that inference hosting is commoditized (sort of like how stateless CPU-based containerized workload hosts are commoditized); there’s no margin in that business, because it <i>is</i> parallelizable, and arbitrarily schedulable, and able to be spread across heterogenous hardware pretty easily (just route individual requests to sub-cluster A or B), preventing any kind of lock-in and thus any kind of rent-extraction by the vendor.<p>Which therefore means that cards that can only <i>do</i> inference, are <i>fungible</i>. You don’t want to spend CapEx on getting into a new LOB just to sell something fungible.<p>All the gigantic GPU clusters that you can sell a million at a time to a bigcorp under a high-margin service contract, meanwhile, are <i>training</i> clusters. Nvidia’s market cap right now is fundamentally built on the model-training “space race” going on between the world’s ~15 big AI companies. That’s the non-fungible market.<p>For Intel to see any benefit (in stock price terms) from an ML-accelerator-card LOB, it’d have to be a card that competes in that space. And that’s a <i>much</i> taller order.</div><br/><div id="42314532" class="c"><input type="checkbox" id="c-42314532" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42314425">parent</a><span>|</span><a href="#42314401">next</a><span>|</span><label class="collapse" for="c-42314532">[-]</label><label class="expand" for="c-42314532">[2 more]</label></div><br/><div class="children"><div class="content">Intel does make cards aimed at this space too:<p><a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;products&#x2F;details&#x2F;processors&#x2F;ai-accelerators&#x2F;gaudi3.html" rel="nofollow">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;products&#x2F;details&#x2F;pro...</a><p>Coincidentally, it has 128GB of RAM. However, it is not a GPU, is designed to do training too and uses expensive HBM.<p>Modern GPUs can do more than inference&#x2F;training and the original poster asked about a GPU with 128GB of RAM, not a card that can only do inferencing as you described. Interestingly, Qualcomm made its own card targeted at only inferencing with 128GB of RAM without using HBM:<p><a href="https:&#x2F;&#x2F;www.qualcomm.com&#x2F;news&#x2F;onq&#x2F;2023&#x2F;11&#x2F;introducing-qualcomm-cloud-ai-100-ultra" rel="nofollow">https:&#x2F;&#x2F;www.qualcomm.com&#x2F;news&#x2F;onq&#x2F;2023&#x2F;11&#x2F;introducing-qualco...</a><p>They do not sell it through PC parts channels so I do not know the price, but it is exactly what you described and it has been built. Presumably, a GPU with the same memory configuration would be of interest to the original poster.</div><br/><div id="42315326" class="c"><input type="checkbox" id="c-42315326" checked=""/><div class="controls bullet"><span class="by">gardnr</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42314532">parent</a><span>|</span><a href="#42314401">next</a><span>|</span><label class="collapse" for="c-42315326">[-]</label><label class="expand" for="c-42315326">[1 more]</label></div><br/><div class="children"><div class="content">Back in January, someone on Reddit claimed the list price was $16k.</div><br/></div></div></div></div></div></div><div id="42314401" class="c"><input type="checkbox" id="c-42314401" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313615">parent</a><span>|</span><a href="#42314425">prev</a><span>|</span><a href="#42313885">next</a><span>|</span><label class="collapse" for="c-42314401">[-]</label><label class="expand" for="c-42314401">[3 more]</label></div><br/><div class="children"><div class="content">You are both correct. AI inference is a comparatively easy problem from the perspective of parallelization when compared to most HPC problems.</div><br/><div id="42314412" class="c"><input type="checkbox" id="c-42314412" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42314401">parent</a><span>|</span><a href="#42313885">next</a><span>|</span><label class="collapse" for="c-42314412">[-]</label><label class="expand" for="c-42314412">[2 more]</label></div><br/><div class="children"><div class="content">Facebook did a technical paper where they described their training cluster and the sheer amount of complexity is staggering. That said, the original poster was interested in inferencing, not training.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2407.21783" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2407.21783</a></div><br/><div id="42314874" class="c"><input type="checkbox" id="c-42314874" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42314412">parent</a><span>|</span><a href="#42313885">next</a><span>|</span><label class="collapse" for="c-42314874">[-]</label><label class="expand" for="c-42314874">[1 more]</label></div><br/><div class="children"><div class="content">Training is close to traditional HPC in many ways. Inference is far simpler since it&#x27;s a simple forward-going pipeline of a relatively small working set.</div><br/></div></div></div></div></div></div><div id="42313885" class="c"><input type="checkbox" id="c-42313885" checked=""/><div class="controls bullet"><span class="by">dogcomplex</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313615">parent</a><span>|</span><a href="#42314401">prev</a><span>|</span><a href="#42312776">next</a><span>|</span><label class="collapse" for="c-42313885">[-]</label><label class="expand" for="c-42313885">[2 more]</label></div><br/><div class="children"><div class="content">what kind of bandwidth&#x2F;latency between GPUs would one need in that setup to not be bottlenecking?  What you&#x27;re describing sounds quite forgiving.  Is it forgiving enough that we could potentially connect those GPUs over a LAN, or even a remote decentralized cloud of host computers?<p>From my understanding that&#x27;s certainly possible to do without the latency hurting much with large batching between inference layers</div><br/><div id="42314398" class="c"><input type="checkbox" id="c-42314398" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313885">parent</a><span>|</span><a href="#42312776">next</a><span>|</span><label class="collapse" for="c-42314398">[-]</label><label class="expand" for="c-42314398">[1 more]</label></div><br/><div class="children"><div class="content">This depends on:<p><pre><code>  * the model dimension
  * how many bits per variable are used by your quantization
  * how many tokens are being processed per step (input processing can do all N input tokens simultaneously and output processing can only do 1 at a time when doing a single query)
  * how many times you split the model layers across multiple GPUs
</code></pre>
The model dimensions are:<p><pre><code>  * 4096 for llama 3&#x2F;3.1 8B.
  * 8192 for llama 3&#x2F;3.1 70B.
  * 16384 for llama 3.1 405B.
</code></pre>
The model layers are:<p><pre><code>  * 32 for llama 3&#x2F;3.1 8B
  * 80 for llama 3&#x2F;3.1 70B
  * 126 for llama 3.1 405B
</code></pre>
The amount of data that needs to be transferred for each split is surprisingly small. Each time you move the calculation of a subsequent layer to a different GPU, you need to transfer an array that is of size model_dimension * num_tokens * bits_per_variable. Then this reduces to a classic network transfer time problem, where you consider both time until the first byte arrives and the transfer time until the last byte arrives. Reality will likely be longer than that idealized scenario, especially since you need to send a signal saying to begin computing.<p>Input processing can tackle so many tokens simultaneously that it probably is not worth thinking too much about this penalty there. Output processing is where the penalty is more significant, since you will incur these costs for every token. Let’s say we are doing fp16 or bf16 on llama 3 8B. Then we need to transfer 8KB every time we move the calculation for another layer to another GPU. If you use RDMA and do this over 10GbE, the transfer time would be 6.4 microseconds. If we assume the time to first byte and time to do a signal to begin processing is 3.6 microseconds combined (chosen to round things up), then we get a penalty of 10 microseconds per split, per token. If you are doing 60 tokens per second and split things across 4 GPUs over the network, you have a penalty of 30 microseconds per token. It will run about 0.003% slower and you are not going to notice this at all. Assuming 10GbE with RDMA is somewhat idealized, although I needed to pick something to give some numbers.<p>In any case, the equation for figuring what factor slower it would be is 1 &#x2F; (1 + time to do transfers and trigger processing per each token in seconds). That would mean under a less ideal situation where the penalty is 5 milliseconds per token, the calculation will be ~0.99502487562 times what it would have been had it been done in a hypothetical single GPU that has all of the VRAM needed, but otherwise the same specifications. This penalty is also not very noticeable.<p>In conclusion, you are right. I actually recall seeing a random YouTube video talking about a software project that does clustered interferencing, so people are already doing this. Unfortunately, I do not remember the project name, channel name or video name.</div><br/></div></div></div></div></div></div><div id="42312776" class="c"><input type="checkbox" id="c-42312776" checked=""/><div class="controls bullet"><span class="by">tgtweak</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42313615">prev</a><span>|</span><a href="#42311923">next</a><span>|</span><label class="collapse" for="c-42312776">[-]</label><label class="expand" for="c-42312776">[1 more]</label></div><br/><div class="children"><div class="content">I think he&#x27;s mostly referring to inference and not training, which I entirely agree with - a 4x version of this card for workstations would do really well - even some basic interconnect between the cards a la nvlink would really drive this home.<p>The training can come after, with some inference and runtime optimizations on the software stack.</div><br/></div></div><div id="42311923" class="c"><input type="checkbox" id="c-42311923" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42312776">prev</a><span>|</span><a href="#42311523">next</a><span>|</span><label class="collapse" for="c-42311923">[-]</label><label class="expand" for="c-42311923">[2 more]</label></div><br/><div class="children"><div class="content">Most of the above infra is predicated on limiting RAM so that you need so much communication between cards. Bump the RAM up and you could do single card inference and all those connections become overhead that could have gone to more ram. For training there is an argument still, but even there the more RAM you have the less all that connectivity gains you. RAM has been used to sell cards and servers for a long time now, it is time to open the floodgates.</div><br/><div id="42312042" class="c"><input type="checkbox" id="c-42312042" checked=""/><div class="controls bullet"><span class="by">foobiekr</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311923">parent</a><span>|</span><a href="#42311523">next</a><span>|</span><label class="collapse" for="c-42312042">[-]</label><label class="expand" for="c-42312042">[1 more]</label></div><br/><div class="children"><div class="content">Correct for inference - the main use of the interconnect is RDMA requests between GPUs to fit models that wouldn&#x27;t otherwise fit.<p>Not really correct for training - training has a lot of all-to-all problems, so hierarchical reduction is useful but doesn&#x27;t really solve the incast problem - Nvlink _bandwidth_ is less of an issue than perhaps the SHARP functions in the NVLink switch ASICs.</div><br/></div></div></div></div><div id="42311523" class="c"><input type="checkbox" id="c-42311523" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42311923">prev</a><span>|</span><a href="#42312108">next</a><span>|</span><label class="collapse" for="c-42311523">[-]</label><label class="expand" for="c-42311523">[23 more]</label></div><br/><div class="children"><div class="content">All of that is highly relevant for training but what the poster was asking for is a desktop inference card.</div><br/><div id="42311610" class="c"><input type="checkbox" id="c-42311610" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311523">parent</a><span>|</span><a href="#42312108">next</a><span>|</span><label class="collapse" for="c-42311610">[-]</label><label class="expand" for="c-42311610">[22 more]</label></div><br/><div class="children"><div class="content">You use at least half of this stack for desktop setups. You need copying daemons, the ecosystem support (docker-nvidia, etc.), some of the libraries, etc. even when you&#x27;re on a single system.<p>If you&#x27;re doing inference on a server; MIG comes into play. If you&#x27;re doing inference on a larger cloud, GPU-direct storage comes into play.<p>It&#x27;s all modular.</div><br/><div id="42311902" class="c"><input type="checkbox" id="c-42311902" checked=""/><div class="controls bullet"><span class="by">landryraccoon</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311610">parent</a><span>|</span><a href="#42311997">next</a><span>|</span><label class="collapse" for="c-42311902">[-]</label><label class="expand" for="c-42311902">[9 more]</label></div><br/><div class="children"><div class="content">It&#x27;s possible you&#x27;re underestimating the open source community.<p>If there&#x27;s a competing platform that hobbyists can tinker with, the ecosystem can improve quite rapidly, especially when the competing platform is completely closed and hobbyists basically are locked out and have no alternative.</div><br/><div id="42312026" class="c"><input type="checkbox" id="c-42312026" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311902">parent</a><span>|</span><a href="#42313433">next</a><span>|</span><label class="collapse" for="c-42312026">[-]</label><label class="expand" for="c-42312026">[6 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s possible you&#x27;re underestimating the open source community.<p>On the contrary. You really don&#x27;t know how I love and prefer open source and love a more leveling playing field.<p>&gt; If there&#x27;s a competing platform that hobbyists can tinker with...<p>AMD&#x27;s cards are better from hardware and software architecture standpoint, but the performance is not there yet. Plus, ROCm libraries are not that mature, but they&#x27;re getting there. Developing high performance, high quality code is deceivingly expensive, because it&#x27;s very heavy in theory, and you fly <i>very close</i> to the metal. I did that in my Ph.D., so I know what it entails. So it requires more than a couple (hundred) hobbyists to pull off (see the development of Eigen linear algebra library, or any high end math library).<p>Some big guns are pouring money into AMD to implement good ROCm libraries, and it started paying off (Debian has a ton of ROCm packages now, too). However, you need to be able to pull it off in the datacenter to be able to pull it off on the desktop.<p>AMD also needs to be able to enable ROCm on desktop properly, so people can start hacking it at home.<p>&gt; especially when the competing platform is completely closed...<p>NVIDIA gives a lot of support to universities, researchers and institutions who play with their cards. Big cards may not be free, but know-how, support and first steps are always within reach. Plus, their researchers dogfood their own cards, and write papers with them.<p>So, as long as papers got published, researchers do their research, and something got invented, many people don&#x27;t care about how open source the ecosystem is. This upsets me a ton, but when closed source AI companies and researchers who forget to add crucial details to their papers so what they did can&#x27;t be reproduced don&#x27;t care about open source, because they think like NVIDIA. &quot;My research, my secrets, my fame, my money&quot;.<p>It&#x27;s not about sharing. It&#x27;s about winning, and it&#x27;s ugly in some aspects.</div><br/><div id="42312978" class="c"><input type="checkbox" id="c-42312978" checked=""/><div class="controls bullet"><span class="by">btown</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312026">parent</a><span>|</span><a href="#42314011">next</a><span>|</span><label class="collapse" for="c-42312978">[-]</label><label class="expand" for="c-42312978">[1 more]</label></div><br/><div class="children"><div class="content">Yep, this thread has a good compilation of ROCm woes: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34832660">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34832660</a><p>That said, for hobbyist inference on large pretrained models, I think there is an interesting set of possibilities here: maybe a number of operations aren&#x27;t optimized, and it takes 10x as long to load the model into memory... but <i>all that might not matter</i> if AMD were to be the first to market for 128GB+ VRAM cards that are the only things that can run next-generation open-weight models in a desktop environment, particularly those generating video and images. The hobbyists don&#x27;t need to optimize all the linear algebra operations that researchers need to be able to experiment with when training; they just need to implement the ones used by the open-weight models.<p>But of course this is all just wishful thinking, because as others have pointed out, any developments in this direction would require a level of foresight that AMD simply hasn&#x27;t historically shown.</div><br/></div></div><div id="42314011" class="c"><input type="checkbox" id="c-42314011" checked=""/><div class="controls bullet"><span class="by">to11mtm</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312026">parent</a><span>|</span><a href="#42312978">prev</a><span>|</span><a href="#42314944">next</a><span>|</span><label class="collapse" for="c-42314011">[-]</label><label class="expand" for="c-42314011">[1 more]</label></div><br/><div class="children"><div class="content">IDK, I found a post that&#x27;s 2 years old that has links to doing llama and SD on an Arc [0]  (although might be linux only), I feel like a cheap huge ram card would create a &#x27;critical mass&#x27; as far as being able to start optimizing, and then from a longer term Intel could promise and deliver on &#x27;scale up&#x27; improvements.<p>It would be a <i>huge</i> shift for them. To go from preferring some (sometimes not quite reached) metric, to, perhaps <i>rightly</i> play the &#x27;reformed underdog&#x27;. Commoditize Big-Memory ML Capable GPUs, <i>even if they aren&#x27;t quite as competitive as the top players at first</i>.<p>Will the other players respond? Yes. But <i>ruin their margin</i>. I know that sounds cutthroat[1] but hey I&#x27;m trying to hypothetically sell this to whomever is taking the reigns after Pat G.<p>&gt; NVIDIA gives a lot of support to universities, researchers and institutions who play with their cards. Big cards may not be free, but know-how, support and first steps are always within reach. Plus, their researchers dogfood their own cards, and write papers with them.<p>Ideally they need to do that too. Ideally they have some &#x27;high powered&#x27; prototypes (e.x. lets say they decide a 2-gpu per card design with an interlink is feasible for some reason) to share as well. This may not be be entirely ethical[1] in this example of how a corp could play it out, again it&#x27;s a thought experiment since intel has NOT announced or hinted at a larger memory card anyway.<p>&gt; AMD also needs to be able to enable ROCm on desktop properly, so people can start hacking it at home<p>AMD&#x27;s driver story has always been a hot mess, My desktop won&#x27;t behave with both my onboard video and 4060 enabled, every AMD card I&#x27;ve had winds up with some weird firmware quirk one way or another... I guess I&#x27;m saying their general level of driver quality doesn&#x27;t lend to hope they&#x27;ll fix dev tools that soon...<p>[0] - <a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;12khkka&#x2F;running_llama_on_intel_arc_a770_16gb&#x2F;jmdm9ia&#x2F;" rel="nofollow">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;12khkka&#x2F;running...</a><p>[1] - As you said, it&#x27;s about winning and it can get ugly.</div><br/></div></div><div id="42314944" class="c"><input type="checkbox" id="c-42314944" checked=""/><div class="controls bullet"><span class="by">mahkeiro</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312026">parent</a><span>|</span><a href="#42314011">prev</a><span>|</span><a href="#42312932">next</a><span>|</span><label class="collapse" for="c-42314944">[-]</label><label class="expand" for="c-42314944">[1 more]</label></div><br/><div class="children"><div class="content">ROCm doesn&#x27;t really matter when the hardware is almost the same as Nvidia cards. AMD is not selling &quot;cheaper&quot; card with a lot of RAM, what the original poster was asking. (and a reason why people who like to tinker with large model are using Macs).</div><br/></div></div><div id="42312932" class="c"><input type="checkbox" id="c-42312932" checked=""/><div class="controls bullet"><span class="by">light_hue_1</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312026">parent</a><span>|</span><a href="#42314944">prev</a><span>|</span><a href="#42313433">next</a><span>|</span><label class="collapse" for="c-42312932">[-]</label><label class="expand" for="c-42312932">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re writing as if AMD cares about open source. If they would only actually open source their driver the community would have made their cards better than nvidia ones long ago.<p>I&#x27;m one of those academics. You&#x27;ve got it all wrong. So many people care about open source. So many people carefully release their code and make everything reproducible.<p>We desperately just want AMD to open up. They just refuse. There&#x27;s nothing secret going on and there&#x27;s no conspiracy. There&#x27;s just a company that for some inexplicable reason doesn&#x27;t want to make boatloads of money for free.<p>AMD is the worst possible situation. They&#x27;re hostile to us and they refuse to invest to make their stuff work.</div><br/><div id="42313651" class="c"><input type="checkbox" id="c-42313651" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312932">parent</a><span>|</span><a href="#42313433">next</a><span>|</span><label class="collapse" for="c-42313651">[-]</label><label class="expand" for="c-42313651">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If they would only actually open source their driver the community would have made their cards better than nvidia ones long ago.<p>Software wise, maybe. But you can&#x27;t change AMD&#x27;s hardware with a magic wand, and that&#x27;s where a lot of CUDA&#x27;s optimizations come from. AMD&#x27;s GPU architecture is optimized for raster compute, and it&#x27;s been that way for decades.<p>I can assure you that AMD does not have a magic button to press that would make their systems competitive for AI. If that was possible it would have been done years ago, with or without their consent. The problem is deeper and extends to design decisions and disagreement over the complexity of GPU designs. If you compare AMD&#x27;s cards to Nvidia on &quot;fair ground&quot; (eg. no CUDA, only OpenCL) the GPGPU performance still leans in Nvidia&#x27;s favor.</div><br/></div></div></div></div></div></div><div id="42313433" class="c"><input type="checkbox" id="c-42313433" checked=""/><div class="controls bullet"><span class="by">kevin_thibedeau</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311902">parent</a><span>|</span><a href="#42312026">prev</a><span>|</span><a href="#42311913">next</a><span>|</span><label class="collapse" for="c-42313433">[-]</label><label class="expand" for="c-42313433">[1 more]</label></div><br/><div class="children"><div class="content">That would require competently produced documentation. Intel can&#x27;t do that for any of their side projects because their MBAs don&#x27;t get a bonus if the tech writers are treated as a valuable asset.</div><br/></div></div><div id="42311913" class="c"><input type="checkbox" id="c-42311913" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311902">parent</a><span>|</span><a href="#42313433">prev</a><span>|</span><a href="#42311997">next</a><span>|</span><label class="collapse" for="c-42311913">[-]</label><label class="expand" for="c-42311913">[1 more]</label></div><br/><div class="children"><div class="content">Innovation is a bottom up process. If they sell the hardware the community will spring up to take advantage.</div><br/></div></div></div></div><div id="42311997" class="c"><input type="checkbox" id="c-42311997" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311610">parent</a><span>|</span><a href="#42311902">prev</a><span>|</span><a href="#42311699">next</a><span>|</span><label class="collapse" for="c-42311997">[-]</label><label class="expand" for="c-42311997">[2 more]</label></div><br/><div class="children"><div class="content">No. I&#x27;ve been reading up. I&#x27;m planning to run Flux 12b on my AMD 5700G with 64GB RAM. CPU will take 5-10minutes per image which will be fine for me tinkering while writing code. Maybe I&#x27;ll be able to get the GPU going on it too.<p>Point of the OP is this is entirely possible with even an iGPU if only we have the RAM. nVidia <i>should be</i> irrelevant for local inference.</div><br/><div id="42314898" class="c"><input type="checkbox" id="c-42314898" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311997">parent</a><span>|</span><a href="#42311699">next</a><span>|</span><label class="collapse" for="c-42314898">[-]</label><label class="expand" for="c-42314898">[1 more]</label></div><br/><div class="children"><div class="content">The Ryzen 5700G is one of the APUs tested on the Debian ROCm CI [1]. It works quite well with the Debian &#x2F; Ubuntu system packages.<p>[1]: <a href="http:&#x2F;&#x2F;ci.rocm.debian.net&#x2F;" rel="nofollow">http:&#x2F;&#x2F;ci.rocm.debian.net&#x2F;</a></div><br/></div></div></div></div><div id="42311699" class="c"><input type="checkbox" id="c-42311699" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311610">parent</a><span>|</span><a href="#42311997">prev</a><span>|</span><a href="#42313753">next</a><span>|</span><label class="collapse" for="c-42311699">[-]</label><label class="expand" for="c-42311699">[5 more]</label></div><br/><div class="children"><div class="content">No you don‘t need much bandwidth between cards for inference</div><br/><div id="42311751" class="c"><input type="checkbox" id="c-42311751" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311699">parent</a><span>|</span><a href="#42313753">next</a><span>|</span><label class="collapse" for="c-42311751">[-]</label><label class="expand" for="c-42311751">[4 more]</label></div><br/><div class="children"><div class="content">Copying daemons (gdrcopy) is about pumping data in and out of a single card. docker-nvidia and rest of the stack is enablement for using cards.<p>GPU-Direct is about pumping data from storage devices to cards, esp. from high speed storage systems across networks.<p>MIG actually shares a single card to multiple instances, so many processes or VMs can use a single card for smaller tasks.<p>Nothing I have written in my previous comment is related to inter-card, inter-server communication, but all are related to disk-GPU, CPU-GPU or RAM-CPU communication.<p>Edit: I mean, it&#x27;s not OK to talk about downvoting, and downvote as you like but, I install and enable these cards for researchers. I know what I&#x27;m installing and what it does. C&#x27;mon now. :D</div><br/><div id="42312329" class="c"><input type="checkbox" id="c-42312329" checked=""/><div class="controls bullet"><span class="by">mikhael</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311751">parent</a><span>|</span><a href="#42313189">next</a><span>|</span><label class="collapse" for="c-42312329">[-]</label><label class="expand" for="c-42312329">[2 more]</label></div><br/><div class="children"><div class="content">Mostly, I think, we don’t really understand your argument that Intel couldn’t easily replicate the parts needed only for inference.</div><br/><div id="42313104" class="c"><input type="checkbox" id="c-42313104" checked=""/><div class="controls bullet"><span class="by">ac29</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312329">parent</a><span>|</span><a href="#42313189">next</a><span>|</span><label class="collapse" for="c-42313104">[-]</label><label class="expand" for="c-42313104">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, for example llama.cpp runs on Intel GPUs via Vulkan or SYCL. The latter is actively being maintained by Intel developers.<p>Obviously that is only one piece of software, but its a certainly a useful one if you are using one of the many LLMs it supports.</div><br/></div></div></div></div><div id="42313189" class="c"><input type="checkbox" id="c-42313189" checked=""/><div class="controls bullet"><span class="by">genewitch</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311751">parent</a><span>|</span><a href="#42312329">prev</a><span>|</span><a href="#42313753">next</a><span>|</span><label class="collapse" for="c-42313189">[-]</label><label class="expand" for="c-42313189">[1 more]</label></div><br/><div class="children"><div class="content">i&#x27;ve run inference on Intel Arc and it works <i>just fine</i> so i am not sure what you&#x27;re talking about. I certainly didn&#x27;t need docker! I&#x27;ve never tried to do anything on AMD yet.<p>I had the 16GB arc, and it was able to run inference at the speed i expected, but twice as many per batch as my 8GB card, which i think is about what you&#x27;d expect.<p>once the model is on the card, there&#x27;s no &quot;disk&quot; anymore, so having more vram to load the model and the tokenizer and whatever else on means there&#x27;s no disk, and realistically when i am running loads on my 24GB 3090 the CPU is maybe 4% over idle usage. My bottleneck, as it stands, to running large models is <i>vram</i>, not anything else.<p>If i needed to train (from scratch or whatever) i&#x27;d just rent time somewhere, even with a 128GB card locally, because obviously more tensors is better.<p>and you&#x27;re getting downvoted because there&#x27;s literally lm studio and llama.cpp and sd-webui that run just fine for inference on our non-dc, non-nvlink, 1&#x2F;15th the cost GPUs.</div><br/></div></div></div></div></div></div><div id="42313753" class="c"><input type="checkbox" id="c-42313753" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311610">parent</a><span>|</span><a href="#42311699">prev</a><span>|</span><a href="#42312108">next</a><span>|</span><label class="collapse" for="c-42313753">[-]</label><label class="expand" for="c-42313753">[5 more]</label></div><br/><div class="children"><div class="content">Inferencing is much more simple than you think:<p>See the precompute_input_logits() and forward() functions here:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ryao&#x2F;llama3.c&#x2F;blob&#x2F;master&#x2F;run.c#L520">https:&#x2F;&#x2F;github.com&#x2F;ryao&#x2F;llama3.c&#x2F;blob&#x2F;master&#x2F;run.c#L520</a><p>As a preface, precompute_input_logits() is really just a generalized version of the forward() function that can operate on multiple input tokens at a time to do faster input processing, although it can be used in place of the forward() function for output generation just by passing only a single token at a time.<p>Also, my apologies for the code being a bit messy. matrix_multiply() and batched_matrix_multiply() are wrappers for GEMM, which I ended up having to use directly anyway when I needed to do strided access. Then matmul() is a wrapper for GEMV, which is really just a special case of GEMM. This is a work in progress personal R&amp;D project that is based on prior work others did (as it spared me from having to do the legwork to implement the less interesting parts of inferencing), so it is not meant to be pretty.<p>Anyway, my purpose in providing that link is to show what is needed to do inferencing (on llama 3). You have a bunch of matrix weights, plus a lookup table for vectors that represent tokens, in memory. Then your operations are:<p><pre><code>  * memcpy()
  * memset()
  * GEMM (GEMV is a special case of GEMM)
  * sinf()
  * cosf()
  * expf()
  * sqrtf()
  * rmsnorm (see the C function for the definition)
  * softmax (see the C function for the definition)
  * Addition, subtraction, multiplication and division.
</code></pre>
I specify rmsnorm and softmax for completeness, but they can be implemented in terms of the other operations.<p>If you can do those, you can do inferencing. You don’t really need very specialized things. Over 95% of time will be spent in GEMM too.<p>My next steps likely will be to figure out how to implement fast GEMM kernels on my CPU. While my own SGEMV code outperforms the Intel MKL SGEMV code on my CPU (Ryzen 7 5800X where 1 core can use all memory bandwidth), my initial attempts at implementing SGEMM have not fared quite so well, but I will likely figure it out eventually. After I do, I can try adapting this to FP16 and then memory usage will finally be low enough that I can port it to a GPU with 24GB of VRAM. That would enable me to do what I say is possible rather than just saying it as I do here.<p>By the way, the llama.cpp project has already figured all of this out and has things running on both GPUs and CPUs using just about every major quantization. I am rolling my own to teach myself how things work. By happy coincidence, I am somehow outperforming llama.cpp in prompt processing on my CPU but sadly, the secrets of how I am doing it are in Intel’s proprietary cblas_sgemm_batch() function. However, since I know it is possible for the hardware to perform like that, I can keep trying ideas for my own implementation until I get something that performs at the same level or better.</div><br/><div id="42313944" class="c"><input type="checkbox" id="c-42313944" checked=""/><div class="controls bullet"><span class="by">dogcomplex</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313753">parent</a><span>|</span><a href="#42314459">next</a><span>|</span><label class="collapse" for="c-42313944">[-]</label><label class="expand" for="c-42313944">[2 more]</label></div><br/><div class="children"><div class="content">I am favoriting this comment for reference later when I start poking around in the base level stuff.  I find it pretty funny how simple this stuff can get.   Have you messed with ternary computing inference yet?  I imagine that shrinks the list even further - or at least reduces the compute requirements in favor of brute force addition.  <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2410.00907" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2410.00907</a></div><br/><div id="42314443" class="c"><input type="checkbox" id="c-42314443" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313944">parent</a><span>|</span><a href="#42314459">next</a><span>|</span><label class="collapse" for="c-42314443">[-]</label><label class="expand" for="c-42314443">[1 more]</label></div><br/><div class="children"><div class="content">No. I still have a long list of things to try doing with llama 3 (and possibly later llama 3.1) on more normal formats like fp32 and in the future, fp16. When I get things running on a GPU, I plan to try using bf16 and maybe fp8 if I get hardware that supports it. Low bit quantizations hurt model quality, so I am not very interested in them. Maybe that will change if good quality models trained to use them become available.</div><br/></div></div></div></div><div id="42314459" class="c"><input type="checkbox" id="c-42314459" checked=""/><div class="controls bullet"><span class="by">SoothingSorbet</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313753">parent</a><span>|</span><a href="#42313944">prev</a><span>|</span><a href="#42312108">next</a><span>|</span><label class="collapse" for="c-42314459">[-]</label><label class="expand" for="c-42314459">[2 more]</label></div><br/><div class="children"><div class="content">&gt; but sadly, the secrets of how I am doing it are in Intel’s proprietary cblas_sgemm_batch() function.<p>Perhaps you can reverse engineer it?</div><br/><div id="42314570" class="c"><input type="checkbox" id="c-42314570" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42314459">parent</a><span>|</span><a href="#42312108">next</a><span>|</span><label class="collapse" for="c-42314570">[-]</label><label class="expand" for="c-42314570">[1 more]</label></div><br/><div class="children"><div class="content">My plan is to make more attempts at rolling my own. Reverse engineering things is not something that I do, since that would prevent me from publishing the result as OSS.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42312108" class="c"><input type="checkbox" id="c-42312108" checked=""/><div class="controls bullet"><span class="by">epistasis</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42311523">prev</a><span>|</span><a href="#42314974">next</a><span>|</span><label class="collapse" for="c-42312108">[-]</label><label class="expand" for="c-42312108">[4 more]</label></div><br/><div class="children"><div class="content">Rather than tackling the entire market at once, they could start with one section and build from there. NVIDIA didn&#x27;t get to where it was in a year, it took many strategic acquisitions. (All the networking and other HPC-specialized stuff I was buying a decade ago has seemingly been bought by NVIDIA).<p>Start by being a &quot;second vendor&quot; for huge customers of NVIDIA that want to foster competition, as well as a few others willing to take risks, and build from there.</div><br/><div id="42313478" class="c"><input type="checkbox" id="c-42313478" checked=""/><div class="controls bullet"><span class="by">tw04</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312108">parent</a><span>|</span><a href="#42314974">next</a><span>|</span><label class="collapse" for="c-42313478">[-]</label><label class="expand" for="c-42313478">[3 more]</label></div><br/><div class="children"><div class="content">Intel has already bought and killed everything they need to compete here. They seem incapable of sticking to any market that isn’t x86. Likely because when they were making those acquisitions they were drunk on margin and didn’t want to focus anywhere else.</div><br/><div id="42315614" class="c"><input type="checkbox" id="c-42315614" checked=""/><div class="controls bullet"><span class="by">RobotToaster</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313478">parent</a><span>|</span><a href="#42314782">next</a><span>|</span><label class="collapse" for="c-42315614">[-]</label><label class="expand" for="c-42315614">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They seem incapable of sticking to any market that isn’t x86<p>Even within that they seem to have difficulty expanding to new areas, remember edison?</div><br/></div></div><div id="42314782" class="c"><input type="checkbox" id="c-42314782" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313478">parent</a><span>|</span><a href="#42315614">prev</a><span>|</span><a href="#42314974">next</a><span>|</span><label class="collapse" for="c-42314782">[-]</label><label class="expand" for="c-42314782">[1 more]</label></div><br/><div class="children"><div class="content">Killing QLogic’s infiniband business after buying it was a major loss for the industry.</div><br/></div></div></div></div></div></div><div id="42314974" class="c"><input type="checkbox" id="c-42314974" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42312108">prev</a><span>|</span><a href="#42313589">next</a><span>|</span><label class="collapse" for="c-42314974">[-]</label><label class="expand" for="c-42314974">[1 more]</label></div><br/><div class="children"><div class="content">You are of course right WRT datacenter use of GPUs. The OP spoke about <i>local</i> generation though. It is of course a smaller market, but a market nevertheless, and the more amateurs and students are using your product, the more of them would consider applying it in more professional settings.<p>Sun (Sparc) and HP (PA-RISC) used to own most of the server market in 1990, but lost most of it to x86 by 2000. Few people had a Sun box with Solaris, but tons of people had access to a PC with Linux, which was inferior in many ways, but well-known and much less locked-up.</div><br/></div></div><div id="42313589" class="c"><input type="checkbox" id="c-42313589" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42314974">prev</a><span>|</span><a href="#42313210">next</a><span>|</span><label class="collapse" for="c-42313589">[-]</label><label class="expand" for="c-42313589">[1 more]</label></div><br/><div class="children"><div class="content">But all the tricks of developing a solid software stack to support the HW are already out no? The basic principles are there, to my understanding the main challenge is not doing this development in tandem with the HW people and the requirments to support older legacy device which makes it harder for example for Amd to compete. The only challenges,which intel are prepped to face is logistics and fabs.
On a separate note, project like JAX are aiming to circumvent that abstraction layer cuda adds to nvidia, so having decent hardware competition is definitely an option. Just some time ago, vllm fully supported amd gpus! We need more competition.</div><br/></div></div><div id="42313210" class="c"><input type="checkbox" id="c-42313210" checked=""/><div class="controls bullet"><span class="by">knowitnone</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42313589">prev</a><span>|</span><a href="#42311893">next</a><span>|</span><label class="collapse" for="c-42313210">[-]</label><label class="expand" for="c-42313210">[1 more]</label></div><br/><div class="children"><div class="content">yes but if Intel provided the base, people will flock to it build software for it. Intel doesn&#x27;t even need be involved but they should.</div><br/></div></div><div id="42311893" class="c"><input type="checkbox" id="c-42311893" checked=""/><div class="controls bullet"><span class="by">postalrat</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42313210">prev</a><span>|</span><a href="#42312671">next</a><span>|</span><label class="collapse" for="c-42311893">[-]</label><label class="expand" for="c-42311893">[1 more]</label></div><br/><div class="children"><div class="content">Lets see how quickly that changes if intel releases cards with massive amounts of ram for a fraction of the cost.</div><br/></div></div><div id="42312671" class="c"><input type="checkbox" id="c-42312671" checked=""/><div class="controls bullet"><span class="by">renecito</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42311893">prev</a><span>|</span><a href="#42312310">next</a><span>|</span><label class="collapse" for="c-42312671">[-]</label><label class="expand" for="c-42312671">[2 more]</label></div><br/><div class="children"><div class="content">&gt; ...  local generative AI lunch<p>I agree, just for my PC, something that&#x27;d enable small devs to create interesting foundation model apps that&#x27;d deploy to users using this local AI cards to run these new Apps.</div><br/><div id="42313014" class="c"><input type="checkbox" id="c-42313014" checked=""/><div class="controls bullet"><span class="by">davrosthedalek</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312671">parent</a><span>|</span><a href="#42312310">next</a><span>|</span><label class="collapse" for="c-42313014">[-]</label><label class="expand" for="c-42313014">[1 more]</label></div><br/><div class="children"><div class="content">There might be an hen-egg problem if the apps end up requiring a 128Gb AI accelerator card. You only get the card if there are apps to run, and you only develop the apps if the cards are wide-spread.   With so much RAM, the cards will not be let&#x27;s-through-them-into-a-cheap-build cheap.<p>I think there have to be a couple of killer apps that run &quot;OK&quot; with CPU or GPU, but would run tremendously better with such a card.</div><br/></div></div></div></div><div id="42312310" class="c"><input type="checkbox" id="c-42312310" checked=""/><div class="controls bullet"><span class="by">teekert</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42312671">prev</a><span>|</span><a href="#42314151">next</a><span>|</span><label class="collapse" for="c-42312310">[-]</label><label class="expand" for="c-42312310">[2 more]</label></div><br/><div class="children"><div class="content">I have a question for you, since I’m somewhat entering the HPC world. In the EU the EuroHPC-JU is building what they call AI factories, afaict these are just batch processing (Slurm I think) clusters with GPUs in the nodes. So I wonder where you’d place those cards of cards. Are you saying there is another, perhaps better ways to use massive amounts of these cards? Or is that still in the “super powerful workstation” domain? Thanx in advance.</div><br/><div id="42312378" class="c"><input type="checkbox" id="c-42312378" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312310">parent</a><span>|</span><a href="#42314151">next</a><span>|</span><label class="collapse" for="c-42312378">[-]</label><label class="expand" for="c-42312378">[1 more]</label></div><br/><div class="children"><div class="content">View it as Raspberry Pi for AI workloads. Initial stage is for enthusiasts that would develop the infra, figure out what is possible and spread the word. Then the next phase will be SME industry adoption, making it commercially interesting, while bypassing Nvidia completely. At some point it would live its own life and big players jump in. Classical disrupt strategy via low cost unique offerings.</div><br/></div></div></div></div><div id="42314151" class="c"><input type="checkbox" id="c-42314151" checked=""/><div class="controls bullet"><span class="by">stephen_cagle</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42312310">prev</a><span>|</span><a href="#42313590">next</a><span>|</span><label class="collapse" for="c-42314151">[-]</label><label class="expand" for="c-42314151">[1 more]</label></div><br/><div class="children"><div class="content">Pretty sure they are talking about inference in the post you are responding to. Training the model obviously needs far more compute, but running them locally is what most devs are interested in.</div><br/></div></div><div id="42313590" class="c"><input type="checkbox" id="c-42313590" checked=""/><div class="controls bullet"><span class="by">wordofx</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42314151">prev</a><span>|</span><a href="#42313939">next</a><span>|</span><label class="collapse" for="c-42313590">[-]</label><label class="expand" for="c-42313590">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Disclosure: HPC admin who works with NIVIDA cards here.
&gt; Because, no. It&#x27;s not as simple as that.<p>Wow what he said is way above your head! Please reread what he wrote.</div><br/></div></div><div id="42313939" class="c"><input type="checkbox" id="c-42313939" checked=""/><div class="controls bullet"><span class="by">vicentwu</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42313590">prev</a><span>|</span><a href="#42312930">next</a><span>|</span><label class="collapse" for="c-42313939">[-]</label><label class="expand" for="c-42313939">[1 more]</label></div><br/><div class="children"><div class="content">Off the topoc. I think, in the long-term , inference should be done along with some kind of training.</div><br/></div></div><div id="42312930" class="c"><input type="checkbox" id="c-42312930" checked=""/><div class="controls bullet"><span class="by">thelastparadise</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42313939">prev</a><span>|</span><a href="#42311497">next</a><span>|</span><label class="collapse" for="c-42312930">[-]</label><label class="expand" for="c-42312930">[1 more]</label></div><br/><div class="children"><div class="content">So what you&#x27;re saying is Intel, or any other would-be NVIDIA competitor, needs to put out fast interconnects, not just compute cards. This is true.<p>I&#x27;m not sure your argument stands when it comes to OP&#x27;s idea of a single card with 128GB VRAM. This would be enough to run ~180B models with reasonable quantization --we&#x27;re not near maxing out the capability of 180B yet (see the latest 32B models performing near public SOTA).<p>This indeed would push rapid and wide adoption and be quite disruptive. But sure, it wouldn&#x27;t instantly enable competitive training of 405B models.</div><br/></div></div><div id="42311497" class="c"><input type="checkbox" id="c-42311497" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311342">parent</a><span>|</span><a href="#42312930">prev</a><span>|</span><a href="#42309957">next</a><span>|</span><label class="collapse" for="c-42311497">[-]</label><label class="expand" for="c-42311497">[13 more]</label></div><br/><div class="children"><div class="content">How does any of this make money?</div><br/><div id="42311560" class="c"><input type="checkbox" id="c-42311560" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311497">parent</a><span>|</span><a href="#42311539">next</a><span>|</span><label class="collapse" for="c-42311560">[-]</label><label class="expand" for="c-42311560">[1 more]</label></div><br/><div class="children"><div class="content">When this walled garden is the only way to use GPUs with high efficiency and everybody is using this stack, and NVIDIA controlling the supply of these &quot;platform boards&quot; to OEMs, they don&#x27;t make money, but they literally print it.<p>However, AMD is coming for them because a couple of high profile supercomputer centers (LUMI, Livermore, etc.) are using Instinct cards and pouring money to AMD to improve their cards and stack.<p>I have not used their (Instinct) cards, yet, but their Linux driver architecture is way better than NVIDIA.</div><br/></div></div><div id="42311539" class="c"><input type="checkbox" id="c-42311539" checked=""/><div class="controls bullet"><span class="by">arcticbull</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311497">parent</a><span>|</span><a href="#42311560">prev</a><span>|</span><a href="#42311555">next</a><span>|</span><label class="collapse" for="c-42311539">[-]</label><label class="expand" for="c-42311539">[5 more]</label></div><br/><div class="children"><div class="content">Having the complete ecosystem affords them significant margins.</div><br/><div id="42311552" class="c"><input type="checkbox" id="c-42311552" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311539">parent</a><span>|</span><a href="#42311555">next</a><span>|</span><label class="collapse" for="c-42311552">[-]</label><label class="expand" for="c-42311552">[4 more]</label></div><br/><div class="children"><div class="content">Against what?</div><br/><div id="42312033" class="c"><input type="checkbox" id="c-42312033" checked=""/><div class="controls bullet"><span class="by">arcticbull</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311552">parent</a><span>|</span><a href="#42311555">next</a><span>|</span><label class="collapse" for="c-42312033">[-]</label><label class="expand" for="c-42312033">[3 more]</label></div><br/><div class="children"><div class="content">As of today they have SaaS company margins as a hardware company which is practically unheard of.</div><br/><div id="42313566" class="c"><input type="checkbox" id="c-42313566" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312033">parent</a><span>|</span><a href="#42311555">next</a><span>|</span><label class="collapse" for="c-42313566">[-]</label><label class="expand" for="c-42313566">[2 more]</label></div><br/><div class="children"><div class="content">I mean, do they?</div><br/></div></div></div></div></div></div></div></div><div id="42311555" class="c"><input type="checkbox" id="c-42311555" checked=""/><div class="controls bullet"><span class="by">lyime</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311497">parent</a><span>|</span><a href="#42311539">prev</a><span>|</span><a href="#42309957">next</a><span>|</span><label class="collapse" for="c-42311555">[-]</label><label class="expand" for="c-42311555">[6 more]</label></div><br/><div class="children"><div class="content">What?<p>It&#x27;s like the most profitable set of products in tech. You have companies like Meta, MSFT, Amazon, Google etc spending $5B every few years buying this hardware.</div><br/><div id="42311652" class="c"><input type="checkbox" id="c-42311652" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311555">parent</a><span>|</span><a href="#42309957">next</a><span>|</span><label class="collapse" for="c-42311652">[-]</label><label class="expand" for="c-42311652">[5 more]</label></div><br/><div class="children"><div class="content">Stale money is moving around. Nothing changed .</div><br/><div id="42311686" class="c"><input type="checkbox" id="c-42311686" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311652">parent</a><span>|</span><a href="#42309957">next</a><span>|</span><label class="collapse" for="c-42311686">[-]</label><label class="expand" for="c-42311686">[4 more]</label></div><br/><div class="children"><div class="content">What is stale money?</div><br/><div id="42311838" class="c"><input type="checkbox" id="c-42311838" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311686">parent</a><span>|</span><a href="#42309957">next</a><span>|</span><label class="collapse" for="c-42311838">[-]</label><label class="expand" for="c-42311838">[3 more]</label></div><br/><div class="children"><div class="content">Hmm. There is a lot of money that exists, doing nothing. I consider that stale money.<p>Edit: I can’t sort this out. Where did all the money go?</div><br/><div id="42312797" class="c"><input type="checkbox" id="c-42312797" checked=""/><div class="controls bullet"><span class="by">RGamma</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311838">parent</a><span>|</span><a href="#42309957">next</a><span>|</span><label class="collapse" for="c-42312797">[-]</label><label class="expand" for="c-42312797">[2 more]</label></div><br/><div class="children"><div class="content">Have you looked under your sofa?</div><br/><div id="42313571" class="c"><input type="checkbox" id="c-42313571" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312797">parent</a><span>|</span><a href="#42309957">next</a><span>|</span><label class="collapse" for="c-42313571">[-]</label><label class="expand" for="c-42313571">[1 more]</label></div><br/><div class="children"><div class="content">Sadly it’s just stale goldfish and magnetites.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42309957" class="c"><input type="checkbox" id="c-42309957" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42311342">prev</a><span>|</span><a href="#42309966">next</a><span>|</span><label class="collapse" for="c-42309957">[-]</label><label class="expand" for="c-42309957">[96 more]</label></div><br/><div class="children"><div class="content">Just how &quot;basic&quot; do you think a GPU can be while having the capability to interface with that much DRAM? Getting there with GDDR6 would require a <i>really</i> wide memory bus even if you could get it to operate with multiple ranks. Getting to 128GB with LPDDR5x would be possible with the 256-bit bus width they used on the top parts of the last generation, but would result in having half the bandwidth of an already mediocre card. &quot;Just add more RAM&quot; doesn&#x27;t work the way you wish it could.</div><br/><div id="42310034" class="c"><input type="checkbox" id="c-42310034" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42309957">parent</a><span>|</span><a href="#42313966">next</a><span>|</span><label class="collapse" for="c-42310034">[-]</label><label class="expand" for="c-42310034">[74 more]</label></div><br/><div class="children"><div class="content">M3&#x2F;M4 Max MacBooks with 128GB RAM are already way better than an A6000 for very large local LLMs. So even if the GPU is as slow as the one in M3&#x2F;M4 Max (&lt;3070), and using some basic RAM like LPDDR5x it would still be way faster than anything from NVidia.</div><br/><div id="42310307" class="c"><input type="checkbox" id="c-42310307" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310034">parent</a><span>|</span><a href="#42310375">next</a><span>|</span><label class="collapse" for="c-42310307">[-]</label><label class="expand" for="c-42310307">[59 more]</label></div><br/><div class="children"><div class="content">The M4 Max needs an enormous 512bit memory bus to extract enough bandwidth out of those LPDDR5x chips, while the GPUs that Intel just launched are 192&#x2F;160bit and even flagships rarely exceed 384bit. They can&#x27;t just slap more memory on the board, they would need to dedicate significantly more silicon area to memory IO and drive up the cost of the part, assuming their architecture would even scale that wide without hitting weird bottlenecks.</div><br/><div id="42312320" class="c"><input type="checkbox" id="c-42312320" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310307">parent</a><span>|</span><a href="#42311445">next</a><span>|</span><label class="collapse" for="c-42312320">[-]</label><label class="expand" for="c-42312320">[1 more]</label></div><br/><div class="children"><div class="content">The memory controller would be bigger, and the cost would be higher, but not radically higher. It would be an attractive product for local inference even at triple the current price and the development expense would be 100% justified if it helped Intel get <i>any</i> kind of foothold in the ML market.</div><br/></div></div><div id="42311445" class="c"><input type="checkbox" id="c-42311445" checked=""/><div class="controls bullet"><span class="by">hughesjj</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310307">parent</a><span>|</span><a href="#42312320">prev</a><span>|</span><a href="#42313825">next</a><span>|</span><label class="collapse" for="c-42311445">[-]</label><label class="expand" for="c-42311445">[2 more]</label></div><br/><div class="children"><div class="content">Man, I&#x27;m old enough to remember when 512 was a thing for consumer cards back when we had 4-8gb memory<p>Sure that was only gddr5 and not gddr6 or lpddr5, but I would have bet we&#x27;d be up to 512bit again 10 years down the line..<p>(I mean supposedly hbm3  has done 1024-2048bit busses but that seems more research or super high end cards, not consumer)</div><br/><div id="42311472" class="c"><input type="checkbox" id="c-42311472" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311445">parent</a><span>|</span><a href="#42313825">next</a><span>|</span><label class="collapse" for="c-42311472">[-]</label><label class="expand" for="c-42311472">[1 more]</label></div><br/><div class="children"><div class="content">Rumor is the 5090 will be bringing back the 512bit bus, for a whopping 1.5TB&#x2F;sec bandwidth.</div><br/></div></div></div></div><div id="42313825" class="c"><input type="checkbox" id="c-42313825" checked=""/><div class="controls bullet"><span class="by">Zandikar</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310307">parent</a><span>|</span><a href="#42311445">prev</a><span>|</span><a href="#42313873">next</a><span>|</span><label class="collapse" for="c-42313825">[-]</label><label class="expand" for="c-42313825">[2 more]</label></div><br/><div class="children"><div class="content">&gt; They can&#x27;t just slap more memory on the board, they would need to dedicate significantly more silicon area to memory IO and drive up the cost of the part,<p>In the pedantic sense of just literally slapping more on existing boards? No, they might have one empty spot for an extra BGA VRAM chip, but not enough for the gain&#x27;s we&#x27;re talking about. But this is absolutely possible, trivially so for someone like Intel&#x2F;AMD&#x2F;NVidia, that has full control over the architectural and design process. Is it a switch they flip at the factory 3 days before shipping? No, obviously not. But if they intended this to be the case ~2 years ago when this was just a product on the drawing board? Absolutely. There is 0 technical&#x2F;hardware&#x2F;manufacturing reason they couldn&#x27;t do this. And considering the &quot;entry level&quot; competitor product is the M4 Max which starts at at least $3,000 (for a 128GB equipped one), the margin on pricing more than exists to cover a few hundred extra in ram and extra overhead in higher-layer more populated PCB&#x27;s.<p>The real impediment is what you landed on at the end there combined with the greater ecosystem not having support for it. Intel could drop a card that is, by all rights, far better performing hardware than a competing Nvidia GPU, but Nvidia&#x27;s dominance in API&#x27;s, CUDA, Networking, Fabric-switches (NVLink, mellanox, bluefield), etc etc for that past 10+ years and all of the skilled labor that is familiar with it would largely render a 128GB Arc GPU a dud on delivery, even if it was priced as a steal. Same thing happened with the Radeon VII. Killer compute card that no one used because while the card itself was phenomenal, the rest of the ecosystem just wasn&#x27;t there.<p>Now, if intel committed to that card, and poured their considerable resources into that ecosystem, and continued to iterate on that card&#x2F;family, then now we&#x27;re talking, but yeah, you can&#x27;t just 10X VRAM on a card that&#x27;s currently a non-player in the GPGPU market and expect anyone in the industry to really give a damn. Raise an eyebrow or make a note to check back in a year? Sure. But raise the issue to get a greenlight on the corpo credit line? Fat chance.</div><br/><div id="42315309" class="c"><input type="checkbox" id="c-42315309" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313825">parent</a><span>|</span><a href="#42313873">next</a><span>|</span><label class="collapse" for="c-42315309">[-]</label><label class="expand" for="c-42315309">[1 more]</label></div><br/><div class="children"><div class="content">A 128GB VRAM Intel Arc card at a low price would be an OSS developer’s dream come true. It would be the WRT54G of inference.</div><br/></div></div></div></div><div id="42313873" class="c"><input type="checkbox" id="c-42313873" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310307">parent</a><span>|</span><a href="#42313825">prev</a><span>|</span><a href="#42314705">next</a><span>|</span><label class="collapse" for="c-42313873">[-]</label><label class="expand" for="c-42313873">[3 more]</label></div><br/><div class="children"><div class="content">&gt; They can&#x27;t just slap more memory on the board<p>Why not? It doesn&#x27;t have to be balanced. RAM is cheap. You would get an affordable card that can hold a large model and still do inference e.g. 4x faster than a CPU. The 128GB card doesn&#x27;t have to do inference on a 128GB model as fast as a 16GB card does on a 16GB model, it can be slower than that and still faster than any cost-competitive alternative at that size.<p>The extra RAM also lets you do things like load a sparse mixture of experts model entirely into the GPU, which will perform well even on lower end GPUs with less bandwidth because you don&#x27;t have to stream the whole model for each token, but you do need enough RAM for the whole model because you don&#x27;t know ahead of time which parts you&#x27;ll need.</div><br/><div id="42314707" class="c"><input type="checkbox" id="c-42314707" checked=""/><div class="controls bullet"><span class="by">elabajaba</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313873">parent</a><span>|</span><a href="#42314705">next</a><span>|</span><label class="collapse" for="c-42314707">[-]</label><label class="expand" for="c-42314707">[2 more]</label></div><br/><div class="children"><div class="content">To get 128GB of RAM on a GPU you&#x27;d need at least a 1024 bit bus. GDDR6x is 16Gbit 32 pins, so you&#x27;d need 64 GDDR6x chips, which good luck even trying to fit that around the GPU die since traces need to be the same length, and you want to keep them as short as possible. There&#x27;s also a good chance you can&#x27;t run a clamshell setup so you&#x27;d have to double the bus width to 2048 because 32 GDDR6x chips would kick off way too much heat to be cooled on the back of a GPU. Such a ridiculous setup would obviously be extremely expensive and would use way too much power.<p>A more sensible alternative would be going with HBM, except good luck getting any capacity for that since it&#x27;s all being used for the extremely high margin data center GPUs. HBM is also extremely expensive both in terms of the cost of buying the chips and due to it&#x27;s advanced packaging requirements.</div><br/><div id="42315154" class="c"><input type="checkbox" id="c-42315154" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42314707">parent</a><span>|</span><a href="#42314705">next</a><span>|</span><label class="collapse" for="c-42315154">[-]</label><label class="expand" for="c-42315154">[1 more]</label></div><br/><div class="children"><div class="content">You do not need a 1024-bit bus to put 128GB of some DDR variant on a GPU. You could do a 512-bit bus with dual rank memory. The 3090 had a 384-bit bus with dual rank memory and going to 512-bit from that is not much of a leap.<p>This assumes you use 32Gbit chips, which will likely be available in the near future. Interestingly, the GDDR7 specification allows for 64Gbit chips:<p>&gt; the GDDR7 standard officially adds support for 64Gbit DRAM devices, twice the 32Gbit max capacity of GDDR6&#x2F;GDDR6X<p><a href="https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;21287&#x2F;jedec-publishes-gddr7-specifications-pam3-ecc-higher-density" rel="nofollow">https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;21287&#x2F;jedec-publishes-gddr7-s...</a></div><br/></div></div></div></div></div></div><div id="42314705" class="c"><input type="checkbox" id="c-42314705" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310307">parent</a><span>|</span><a href="#42313873">prev</a><span>|</span><a href="#42312044">next</a><span>|</span><label class="collapse" for="c-42314705">[-]</label><label class="expand" for="c-42314705">[1 more]</label></div><br/><div class="children"><div class="content">If their memory IO supports multiple ranks like the RTX 3090 (it used dual rank) did, they could do a new PCB layout and then add more memory chips to it. No additional silicon area would be necessary.</div><br/></div></div><div id="42312044" class="c"><input type="checkbox" id="c-42312044" checked=""/><div class="controls bullet"><span class="by">CoastalCoder</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310307">parent</a><span>|</span><a href="#42314705">prev</a><span>|</span><a href="#42311087">next</a><span>|</span><label class="collapse" for="c-42312044">[-]</label><label class="expand" for="c-42312044">[3 more]</label></div><br/><div class="children"><div class="content">&gt; The M4 Max needs an enormous 512bit memory bus to extract enough bandwidth out of those LPDDR5x chips<p>Does M4 Max have 64-byte cache lines?<p>If they can fetch or flush an entire cache line in a single memory-bus transaction, I wonder if that opens up any additional hardware &#x2F; performance optimizations.</div><br/><div id="42314873" class="c"><input type="checkbox" id="c-42314873" checked=""/><div class="controls bullet"><span class="by">my123</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312044">parent</a><span>|</span><a href="#42313201">next</a><span>|</span><label class="collapse" for="c-42314873">[-]</label><label class="expand" for="c-42314873">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Does M4 Max have 64-byte cache lines?<p>on the CPU side: 64 bytes at L1, 128 byte cachelines at L2</div><br/></div></div><div id="42313201" class="c"><input type="checkbox" id="c-42313201" checked=""/><div class="controls bullet"><span class="by">rowanG077</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312044">parent</a><span>|</span><a href="#42314873">prev</a><span>|</span><a href="#42311087">next</a><span>|</span><label class="collapse" for="c-42313201">[-]</label><label class="expand" for="c-42313201">[1 more]</label></div><br/><div class="children"><div class="content">A single memory transaction is almost always a 16n burst for LPDDR5x.</div><br/></div></div></div></div><div id="42311087" class="c"><input type="checkbox" id="c-42311087" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310307">parent</a><span>|</span><a href="#42312044">prev</a><span>|</span><a href="#42311136">next</a><span>|</span><label class="collapse" for="c-42311087">[-]</label><label class="expand" for="c-42311087">[31 more]</label></div><br/><div class="children"><div class="content">Apple could do it. Why can’t Intel?</div><br/><div id="42311239" class="c"><input type="checkbox" id="c-42311239" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311087">parent</a><span>|</span><a href="#42311478">next</a><span>|</span><label class="collapse" for="c-42311239">[-]</label><label class="expand" for="c-42311239">[23 more]</label></div><br/><div class="children"><div class="content">Because Apple isn&#x27;t playing the same game as everyone else. They have the money and clout to buy out TSMCs bleeding-edge processes and leave everyone else with the scraps, and their silicon is only sold in machines with extremely fat margins that can easily absorb the BOM cost of making huge chips on the most expensive processes money can buy.</div><br/><div id="42311338" class="c"><input type="checkbox" id="c-42311338" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311239">parent</a><span>|</span><a href="#42312088">next</a><span>|</span><label class="collapse" for="c-42311338">[-]</label><label class="expand" for="c-42311338">[16 more]</label></div><br/><div class="children"><div class="content">Bleeding edge processes is what Intel specializes in. Unlike Apple, they don’t need TSMC. This should have been a huge advantage for Intel. Maybe that’s why Gelsinger got the boot.</div><br/><div id="42311450" class="c"><input type="checkbox" id="c-42311450" checked=""/><div class="controls bullet"><span class="by">duskwuff</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311338">parent</a><span>|</span><a href="#42311468">next</a><span>|</span><label class="collapse" for="c-42311450">[-]</label><label class="expand" for="c-42311450">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Bleeding edge processes is what Intel specializes in. Unlike Apple, they don’t need TSMC.<p>Intel literally outsourced their Arrow Lake manufacturing to TSMC because they couldn&#x27;t fabricate the parts themselves - their 20A (2nm) process node never reached a production-ready state, and was eventually cancelled about a month ago.</div><br/><div id="42312051" class="c"><input type="checkbox" id="c-42312051" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311450">parent</a><span>|</span><a href="#42313166">next</a><span>|</span><label class="collapse" for="c-42312051">[-]</label><label class="expand" for="c-42312051">[3 more]</label></div><br/><div class="children"><div class="content">OK, so the question becomes: TSMC could do it. Why can’t Intel?</div><br/><div id="42313144" class="c"><input type="checkbox" id="c-42313144" checked=""/><div class="controls bullet"><span class="by">ac29</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312051">parent</a><span>|</span><a href="#42312094">next</a><span>|</span><label class="collapse" for="c-42313144">[-]</label><label class="expand" for="c-42313144">[1 more]</label></div><br/><div class="children"><div class="content">Intel is maybe a year or two behind TSMC right now. They might or might not catch up  since it is a moving target, but I dont think there is anything TSMC is doing today that Intel wont be doing in the near future.</div><br/></div></div><div id="42312094" class="c"><input type="checkbox" id="c-42312094" checked=""/><div class="controls bullet"><span class="by">BonoboIO</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312051">parent</a><span>|</span><a href="#42313144">prev</a><span>|</span><a href="#42313166">next</a><span>|</span><label class="collapse" for="c-42312094">[-]</label><label class="expand" for="c-42312094">[1 more]</label></div><br/><div class="children"><div class="content">They are trying … for like 10 years</div><br/></div></div></div></div><div id="42313166" class="c"><input type="checkbox" id="c-42313166" checked=""/><div class="controls bullet"><span class="by">ubercore</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311450">parent</a><span>|</span><a href="#42312051">prev</a><span>|</span><a href="#42311468">next</a><span>|</span><label class="collapse" for="c-42313166">[-]</label><label class="expand" for="c-42313166">[2 more]</label></div><br/><div class="children"><div class="content">Wasn&#x27;t it cancelled in favor of 18A?</div><br/><div id="42314353" class="c"><input type="checkbox" id="c-42314353" checked=""/><div class="controls bullet"><span class="by">duskwuff</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313166">parent</a><span>|</span><a href="#42311468">next</a><span>|</span><label class="collapse" for="c-42314353">[-]</label><label class="expand" for="c-42314353">[1 more]</label></div><br/><div class="children"><div class="content">It was, but that only puts them further away from shipping product.</div><br/></div></div></div></div></div></div><div id="42311468" class="c"><input type="checkbox" id="c-42311468" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311338">parent</a><span>|</span><a href="#42311450">prev</a><span>|</span><a href="#42311432">next</a><span>|</span><label class="collapse" for="c-42311468">[-]</label><label class="expand" for="c-42311468">[1 more]</label></div><br/><div class="children"><div class="content">These days, Intel merely specializes in bleeding processes. They spent far too many years believing the unrealistic promises from their fab division, and in the past few years they&#x27;ve been suffering the consequences as the problems are too big to be covered up by the cost savings of vertical integration.</div><br/></div></div><div id="42311432" class="c"><input type="checkbox" id="c-42311432" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311338">parent</a><span>|</span><a href="#42311468">prev</a><span>|</span><a href="#42311417">next</a><span>|</span><label class="collapse" for="c-42311432">[-]</label><label class="expand" for="c-42311432">[3 more]</label></div><br/><div class="children"><div class="content">Intel&#x27;s foundry side has been floundering so hard that they&#x27;ve resorted to using TSMC themselves in an attempt to keep up with AMD. Their recently launched CPUs are a mix of Intel-made and TSMC-made chiplets, but the latter accounts for most of the die area.</div><br/><div id="42313833" class="c"><input type="checkbox" id="c-42313833" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311432">parent</a><span>|</span><a href="#42311417">next</a><span>|</span><label class="collapse" for="c-42313833">[-]</label><label class="expand" for="c-42313833">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not certain this is quite as damning as it sounds. My understanding is that the foundry business was intentionally walled off from the product business, and that the latter wasn&#x27;t going to be treated as a privileged customer.</div><br/><div id="42314026" class="c"><input type="checkbox" id="c-42314026" checked=""/><div class="controls bullet"><span class="by">barkingcat</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313833">parent</a><span>|</span><a href="#42311417">next</a><span>|</span><label class="collapse" for="c-42314026">[-]</label><label class="expand" for="c-42314026">[1 more]</label></div><br/><div class="children"><div class="content">no, in fact, it sounds <i>even more damning</i> because client side was able to pick whatever was best on the market, and it wasn&#x27;t intel. Client side could to learn and customize their designs to use another company&#x27;s processes (this is an extremely hard thing to do by the way) faster than intel foundry could even get their pants up in the morning.<p>Intel foundry screwed up so badly that Nokia&#x27;s server division was almost shut down because of Intel Foundry&#x27;s failure. (imagine being so bad at your job, that your clients go out of business) If Intel client side chose to use Foundry, there just <i>wouldn&#x27;t be any</i> chips to sell.</div><br/></div></div></div></div></div></div><div id="42311417" class="c"><input type="checkbox" id="c-42311417" checked=""/><div class="controls bullet"><span class="by">AlotOfReading</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311338">parent</a><span>|</span><a href="#42311432">prev</a><span>|</span><a href="#42314016">next</a><span>|</span><label class="collapse" for="c-42311417">[-]</label><label class="expand" for="c-42311417">[4 more]</label></div><br/><div class="children"><div class="content">Intel Arc hardware is manufactured by TSMC, specifically on N6 and N5 for this latest announcement.<p>Intel doesn&#x27;t currently have nodes competitive with TSMC or excess capacity in their better processes.</div><br/><div id="42313232" class="c"><input type="checkbox" id="c-42313232" checked=""/><div class="controls bullet"><span class="by">knowitnone</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311417">parent</a><span>|</span><a href="#42314016">next</a><span>|</span><label class="collapse" for="c-42313232">[-]</label><label class="expand" for="c-42313232">[3 more]</label></div><br/><div class="children"><div class="content">Serious question, why don&#x27;t they have excess capacity? They aren&#x27;t producing many CPUs...that people want.</div><br/><div id="42313644" class="c"><input type="checkbox" id="c-42313644" checked=""/><div class="controls bullet"><span class="by">vel0city</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313232">parent</a><span>|</span><a href="#42313476">next</a><span>|</span><label class="collapse" for="c-42313644">[-]</label><label class="expand" for="c-42313644">[1 more]</label></div><br/><div class="children"><div class="content">Hard to have excess capacity when your latest fabs aren&#x27;t able to produce anything reliably.<p>They don&#x27;t even have competitive capacity for all their <i>CPU</i> needs. They have negative spare capacity overall.</div><br/></div></div><div id="42313476" class="c"><input type="checkbox" id="c-42313476" checked=""/><div class="controls bullet"><span class="by">AlotOfReading</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313232">parent</a><span>|</span><a href="#42313644">prev</a><span>|</span><a href="#42314016">next</a><span>|</span><label class="collapse" for="c-42313476">[-]</label><label class="expand" for="c-42313476">[1 more]</label></div><br/><div class="children"><div class="content">Because their production capacity is much smaller than TSMC, it&#x27;s declined over the past few years, and their newer nodes are having yield issues.</div><br/></div></div></div></div></div></div><div id="42314016" class="c"><input type="checkbox" id="c-42314016" checked=""/><div class="controls bullet"><span class="by">barkingcat</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311338">parent</a><span>|</span><a href="#42311417">prev</a><span>|</span><a href="#42312088">next</a><span>|</span><label class="collapse" for="c-42314016">[-]</label><label class="expand" for="c-42314016">[1 more]</label></div><br/><div class="children"><div class="content">if you think intel has bleeding edge processes, that hasn&#x27;t been the case for over 6 years ...</div><br/></div></div></div></div><div id="42312088" class="c"><input type="checkbox" id="c-42312088" checked=""/><div class="controls bullet"><span class="by">JBiserkov</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311239">parent</a><span>|</span><a href="#42311338">prev</a><span>|</span><a href="#42314937">next</a><span>|</span><label class="collapse" for="c-42312088">[-]</label><label class="expand" for="c-42312088">[5 more]</label></div><br/><div class="children"><div class="content">&gt; and their silicon is only sold in machines with extremely fat margins<p>Like the brand new Mini that cost 600 USD and went to 500 during Black week.</div><br/><div id="42313067" class="c"><input type="checkbox" id="c-42313067" checked=""/><div class="controls bullet"><span class="by">michaelmrose</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312088">parent</a><span>|</span><a href="#42314937">next</a><span>|</span><label class="collapse" for="c-42313067">[-]</label><label class="expand" for="c-42313067">[4 more]</label></div><br/><div class="children"><div class="content">The 600 mini is on the m4 with the  anemic 10 core GPU. This is for grandama or your 10 year old.<p>The good one which is still slower than m4 max is 2200.<p>If you want the max you need at least a macbook pro starting at 3200 and if you want the better one with 128G RAM it starts at about 5k</div><br/><div id="42314077" class="c"><input type="checkbox" id="c-42314077" checked=""/><div class="controls bullet"><span class="by">barkingcat</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313067">parent</a><span>|</span><a href="#42314937">next</a><span>|</span><label class="collapse" for="c-42314077">[-]</label><label class="expand" for="c-42314077">[3 more]</label></div><br/><div class="children"><div class="content">No matter how few GPU cores the M4 has, it is still an extremely potent product on the whole.<p>Better than most of the pc&#x27;s out there.</div><br/><div id="42314226" class="c"><input type="checkbox" id="c-42314226" checked=""/><div class="controls bullet"><span class="by">michaelmrose</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42314077">parent</a><span>|</span><a href="#42314937">next</a><span>|</span><label class="collapse" for="c-42314226">[-]</label><label class="expand" for="c-42314226">[2 more]</label></div><br/><div class="children"><div class="content">Do you mean subjectively because you find Mac pleasant to use? The $600 mini certainly isn&#x27;t more performant than 85% of desktops.</div><br/><div id="42314884" class="c"><input type="checkbox" id="c-42314884" checked=""/><div class="controls bullet"><span class="by">my123</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42314226">parent</a><span>|</span><a href="#42314937">next</a><span>|</span><label class="collapse" for="c-42314884">[-]</label><label class="expand" for="c-42314884">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The $600 mini certainly isn&#x27;t more performant than 85% of desktops.<p>The average desktop isn&#x27;t exactly a gaming machine. But a corporate box or a low end home desktop with a Core i5 and using the iGPU.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42314937" class="c"><input type="checkbox" id="c-42314937" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311239">parent</a><span>|</span><a href="#42312088">prev</a><span>|</span><a href="#42311478">next</a><span>|</span><label class="collapse" for="c-42314937">[-]</label><label class="expand" for="c-42314937">[1 more]</label></div><br/><div class="children"><div class="content">Transistor IO logic scaling died a while ago, which is what prompted AMD to go with a chiplet architecture. Being on a more advanced process does not make implementing an 512-bit memory bus any easier for Apple. If anything, it makes it more expensive for Apple than it would be for Intel.</div><br/></div></div></div></div><div id="42311478" class="c"><input type="checkbox" id="c-42311478" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311087">parent</a><span>|</span><a href="#42311239">prev</a><span>|</span><a href="#42311136">next</a><span>|</span><label class="collapse" for="c-42311478">[-]</label><label class="expand" for="c-42311478">[7 more]</label></div><br/><div class="children"><div class="content">Because LPDDR5x is soldered on RAM.<p>Everyone else wants configurable RAM that scales both down (to 16GB) and up (to 2TB), to cover smaller laptops and bigger servers.<p>GPUs with soldered on RAM has 500GB&#x2F;sec bandwidths, far in excess of Apples chips. So the 8GB or 16GB offered by NVidia or AMD is just far superior at vid o game graphics (where textures are the priority)</div><br/><div id="42311526" class="c"><input type="checkbox" id="c-42311526" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311478">parent</a><span>|</span><a href="#42313670">next</a><span>|</span><label class="collapse" for="c-42311526">[-]</label><label class="expand" for="c-42311526">[4 more]</label></div><br/><div class="children"><div class="content">&gt; GPUs with soldered on RAM has 500GB&#x2F;sec bandwidths, far in excess of Apples chips.<p>Apple is doing 800GB&#x2F;sec on the M2 Ultra and should reach about 1TB&#x2F;sec with the M4 Ultra, but that&#x27;s still lagging behind GPUs. The 4090 was already at the 1TB&#x2F;sec mark two years ago, the 5090 is supposedly aiming for 1.5TB&#x2F;sec, and the H200 is doing <i>5TB&#x2F;sec.</i></div><br/><div id="42311793" class="c"><input type="checkbox" id="c-42311793" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311526">parent</a><span>|</span><a href="#42314661">prev</a><span>|</span><a href="#42313670">next</a><span>|</span><label class="collapse" for="c-42311793">[-]</label><label class="expand" for="c-42311793">[2 more]</label></div><br/><div class="children"><div class="content">HBM is kind of not fair lol. But 4096-line bus is gonna have more bandwidth than any competitor.<p>It&#x27;s pretty expensive though.<p>The 500GB&#x2F;sec number is for a more ordinary GPU like the B580 Battlemage in the $250ish price range. Obviously the $2000ish 4090 will be better, but I don&#x27;t expect the typical consumer to be using those.</div><br/><div id="42312032" class="c"><input type="checkbox" id="c-42312032" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311793">parent</a><span>|</span><a href="#42313670">next</a><span>|</span><label class="collapse" for="c-42312032">[-]</label><label class="expand" for="c-42312032">[1 more]</label></div><br/><div class="children"><div class="content">But an on-package memory bus has some of the advantages of HBM, just to a lesser extent, so it&#x27;s arguably comparable as an &quot;intermediate stage&quot; between RAM chips and HBM. Distances are shorter (so voltage drop and capacitance are lower, so can be driven at lower power), routing is more complex but can be worked around by more layers, which increases cost but on a <i>significantly</i> smaller area than required for dimms, and the dimms connections themselves can hurt performance (reflection from poor contacts, optional termination makes things more complex, and the expectations of mix-and-match for dimm vendors and products likely reduce fine tuning possibilities).<p>There&#x27;s pretty much a direct opposite scaling between flexibility and performance - dimms &gt; soldered ram &gt; on-package ram &gt; die-interconnects.</div><br/></div></div></div></div></div></div><div id="42313670" class="c"><input type="checkbox" id="c-42313670" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311478">parent</a><span>|</span><a href="#42311526">prev</a><span>|</span><a href="#42311136">next</a><span>|</span><label class="collapse" for="c-42313670">[-]</label><label class="expand" for="c-42313670">[2 more]</label></div><br/><div class="children"><div class="content">The question is why Intel <i>GPUs</i>, which already have soldered memory, aren&#x27;t sold with more of it. The market here isn&#x27;t something that can beat enterprise GPUs at training, it&#x27;s something that can beat <i>desktop CPUs</i> at inference with enough VRAM to fit large models at an affordable price.</div><br/><div id="42315182" class="c"><input type="checkbox" id="c-42315182" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313670">parent</a><span>|</span><a href="#42311136">next</a><span>|</span><label class="collapse" for="c-42315182">[-]</label><label class="expand" for="c-42315182">[1 more]</label></div><br/><div class="children"><div class="content">Intel also has lunar lake CPUs with on package RAM. They could have added more memory channels like Apple did.</div><br/></div></div></div></div></div></div></div></div><div id="42311136" class="c"><input type="checkbox" id="c-42311136" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310307">parent</a><span>|</span><a href="#42311087">prev</a><span>|</span><a href="#42310375">next</a><span>|</span><label class="collapse" for="c-42311136">[-]</label><label class="expand" for="c-42311136">[15 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t matter if the &quot;cost is driven up&quot;. Nvidia has proven that we&#x27;re all lil pay pigs for them. 5090 will be 3000$ for 32gb of VRAM. Screenshot this now, it will age well.<p>We&#x27;d be happy to pay 5000 for 128gb from Intel.</div><br/><div id="42311359" class="c"><input type="checkbox" id="c-42311359" checked=""/><div class="controls bullet"><span class="by">pixelpoet</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311136">parent</a><span>|</span><a href="#42311378">next</a><span>|</span><label class="collapse" for="c-42311359">[-]</label><label class="expand" for="c-42311359">[9 more]</label></div><br/><div class="children"><div class="content">You are absolutely correct, and even my non-prophetic ass echoed exactly the first sentence of the top comment in this HN thread (&quot;Why don&#x27;t they just release a basic GPU with 128GB RAM and eat NVidia&#x27;s local generative AI lunch?&quot;).<p>Yes, yes, it&#x27;s not trivial to have a GPU with 128gb of memory with cache tags and so on, but is that really in the same universe of complexity of taking on Nvidia and their CUDA &#x2F; AI moat any other way? Did Intel ever give the impression they don&#x27;t know how to design a cache? There really has to be a GOOD reason for this, otherwise everyone involved with this launch is just plain stupid or getting paid off to not pursue this.<p>Saying all this with infinite love and 100% commercial support of OpenCL since version 1.0, a great enjoyer of A770 with 16GB of memory, I live to laugh in the face of people who claimed for over 10 years that OpenCL is deprecated on MacOS (which I cannot stand and will never use, yet the hardware it runs on...) and still routinely crushes powerful desktop GPUs, in reality and practice today.</div><br/><div id="42311592" class="c"><input type="checkbox" id="c-42311592" checked=""/><div class="controls bullet"><span class="by">timschmidt</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311359">parent</a><span>|</span><a href="#42311378">next</a><span>|</span><label class="collapse" for="c-42311592">[-]</label><label class="expand" for="c-42311592">[8 more]</label></div><br/><div class="children"><div class="content">Both Intel and AMD produce server chips with 12 channel memory these days (that&#x27;s 12x64bit for 768bit) which combined with DDR5 can push effective socket bandwidth beyond 800GB&#x2F;s, which is well into the area occupied by single GPUs these days.<p>You can even find some attractive deals on motherboard&#x2F;ram&#x2F;cpu bundles built around grey market engineering sample CPUs on aliexpress with good reports about usability under Linux.<p>Building a whole new system like this is not exactly as simple as just plugging a GPU into an existing system, but you also benefit from upgradeability of the memory, and not having to use anything like CUDA.  llamafile, as an example, really benefits from AVX-512 available in recent CPUs.  LLMs are memory bandwidth bound, so it doesn&#x27;t take many CPU cores to keep the memory bus full.<p>Another benefit is that you can get a large amount of usable high bandwidth memory with a relatively low total system power usage.  Some of AMD&#x27;s parts with 12 channel memory can fit in a 200W system power budget.  Less than a single high end GPU.</div><br/><div id="42311640" class="c"><input type="checkbox" id="c-42311640" checked=""/><div class="controls bullet"><span class="by">pixelpoet</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311592">parent</a><span>|</span><a href="#42311378">next</a><span>|</span><label class="collapse" for="c-42311640">[-]</label><label class="expand" for="c-42311640">[7 more]</label></div><br/><div class="children"><div class="content">My desktop machine has had 128gb since 2018, but for the AI workloads currently commanding almost infinite market value, it really needs the 1TB&#x2F;s bandwidth and teraflops that only a bona fide GPU can provide. An early AMD GPU with these characteristics is the Radeon VII with 16gb HBM, which I bought for 500 eur back in 2019 (!!!).<p>I&#x27;m a rendering guy, not an AI guy, so I really just want the teraflops, but all GPU users urgently need a 3rd market player.</div><br/><div id="42311670" class="c"><input type="checkbox" id="c-42311670" checked=""/><div class="controls bullet"><span class="by">timschmidt</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311640">parent</a><span>|</span><a href="#42311378">next</a><span>|</span><label class="collapse" for="c-42311670">[-]</label><label class="expand" for="c-42311670">[6 more]</label></div><br/><div class="children"><div class="content">That 128gb is hanging off a dual channel memory bus with only 128 total bits of bandwidth.  Which is why you need the GPU.  The Epyc and Xeon CPUs I&#x27;m discussing have 6x the memory bandwidth, and will trade blows with that GPU.</div><br/><div id="42311721" class="c"><input type="checkbox" id="c-42311721" checked=""/><div class="controls bullet"><span class="by">pixelpoet</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311670">parent</a><span>|</span><a href="#42311378">next</a><span>|</span><label class="collapse" for="c-42311721">[-]</label><label class="expand" for="c-42311721">[5 more]</label></div><br/><div class="children"><div class="content">At a mere 20x the cost or something, to say nothing about the motherboard etc :( 500 eur for 16GB of 1TB&#x2F;s with tons of fp32 (and even fp64! The main reason I bought it) back in 2019 is no joke.<p>Believe me, as a lifelong hobbyist-HPC kind of person, I am absolutely dying for such a HBM&#x2F;fp64 deal again.</div><br/><div id="42311806" class="c"><input type="checkbox" id="c-42311806" checked=""/><div class="controls bullet"><span class="by">timschmidt</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311721">parent</a><span>|</span><a href="#42311378">next</a><span>|</span><label class="collapse" for="c-42311806">[-]</label><label class="expand" for="c-42311806">[4 more]</label></div><br/><div class="children"><div class="content">$1,961.19: H13SSL-N Motherboard And EPYC 9334 QS CPU + DDR5 4*128GB 2666MHZ REG ECC RAM Server motherboard kit<p><a href="https:&#x2F;&#x2F;www.aliexpress.us&#x2F;item&#x2F;3256807766813460.html" rel="nofollow">https:&#x2F;&#x2F;www.aliexpress.us&#x2F;item&#x2F;3256807766813460.html</a><p>Doesn&#x27;t seem like 20x to me.  I&#x27;m sure spending more than 30 seconds searching could find even better deals.</div><br/><div id="42311841" class="c"><input type="checkbox" id="c-42311841" checked=""/><div class="controls bullet"><span class="by">pixelpoet</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311806">parent</a><span>|</span><a href="#42311378">next</a><span>|</span><label class="collapse" for="c-42311841">[-]</label><label class="expand" for="c-42311841">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t 2666 MHz ECC RAM obscenely slow? 32 cores without the fast AVX-512 of Zen5 isn&#x27;t what anyone is looking for in terms of floating point throughput (ask me about electricity prices in Germany), and for that money I&#x27;d rather just take a 4090 with 24GB memory and do my own software fixed point or floating point (which is exactly what I do personally and professionally).<p>This is exactly what I meant about Intel&#x27;s recent launch. Imagine if they went full ALU-heavy on latest TSMC process and packaged 128GB with it, for like, 2-3k Eur. Nvidia would be whipping their lawyers to try to do something about that, not just their engineers.</div><br/><div id="42314647" class="c"><input type="checkbox" id="c-42314647" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311841">parent</a><span>|</span><a href="#42311885">next</a><span>|</span><label class="collapse" for="c-42314647">[-]</label><label class="expand" for="c-42314647">[1 more]</label></div><br/><div class="children"><div class="content">Yes and no. I have been developing some local llama 3 inference software on a machine with 3200MT&#x2F;s ECC RAM and a Ryzen 7 5800X:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ryao&#x2F;llama3.c">https:&#x2F;&#x2F;github.com&#x2F;ryao&#x2F;llama3.c</a><p>My experience is that input processing (prompt processing) is compute bottlenecked in GEMM. AVX-512 would help there, although my CPU’s Zen 3 cores do not support it and the memory bandwidth does not matter very much. For output generation (token generation), memory bandwidth is a bottleneck and AVX-512 would not help at all.</div><br/></div></div><div id="42311885" class="c"><input type="checkbox" id="c-42311885" checked=""/><div class="controls bullet"><span class="by">timschmidt</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311841">parent</a><span>|</span><a href="#42314647">prev</a><span>|</span><a href="#42311378">next</a><span>|</span><label class="collapse" for="c-42311885">[-]</label><label class="expand" for="c-42311885">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think anyone&#x27;s stopping you, buddy.  Great chat.  I hope you have a nice evening.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42311378" class="c"><input type="checkbox" id="c-42311378" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311136">parent</a><span>|</span><a href="#42311359">prev</a><span>|</span><a href="#42311368">next</a><span>|</span><label class="collapse" for="c-42311378">[-]</label><label class="expand" for="c-42311378">[4 more]</label></div><br/><div class="children"><div class="content">The question is whether there&#x27;s enough overall demand for a GPU architecture with 4x the VRAM of a 5090 but only about 1&#x2F;3rd of the bandwidth. At that point it would only really be good for AI inferencing, so why not make specialized inferencing silicon instead?</div><br/><div id="42312407" class="c"><input type="checkbox" id="c-42312407" checked=""/><div class="controls bullet"><span class="by">mandelken</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311378">parent</a><span>|</span><a href="#42311368">next</a><span>|</span><label class="collapse" for="c-42312407">[-]</label><label class="expand" for="c-42312407">[3 more]</label></div><br/><div class="children"><div class="content">I genuinely wonder why no one is doing this? Why can&#x27;t I buy this specialized AI inference silicon with plenty of VRAM?</div><br/><div id="42314676" class="c"><input type="checkbox" id="c-42314676" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312407">parent</a><span>|</span><a href="#42312899">next</a><span>|</span><label class="collapse" for="c-42314676">[-]</label><label class="expand" for="c-42314676">[1 more]</label></div><br/><div class="children"><div class="content">Intel and Qualcomm are doing this, although Intel uses HBM and their hardware is designed to do both inference and training while Qualcomm uses more conventional memory and their hardware is only designed do inference:<p><a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;products&#x2F;details&#x2F;processors&#x2F;ai-accelerators&#x2F;gaudi3.html" rel="nofollow">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;products&#x2F;details&#x2F;pro...</a><p><a href="https:&#x2F;&#x2F;www.qualcomm.com&#x2F;news&#x2F;onq&#x2F;2023&#x2F;11&#x2F;introducing-qualcomm-cloud-ai-100-ultra" rel="nofollow">https:&#x2F;&#x2F;www.qualcomm.com&#x2F;news&#x2F;onq&#x2F;2023&#x2F;11&#x2F;introducing-qualco...</a><p>They did not put it into the PC parts supply chain for reasons known only to them. That said, it would be awesome if Intel made high memory variants of their Arc graphics cards for sale through the PC parts supply chains.</div><br/></div></div><div id="42312899" class="c"><input type="checkbox" id="c-42312899" checked=""/><div class="controls bullet"><span class="by">ahakki</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312407">parent</a><span>|</span><a href="#42314676">prev</a><span>|</span><a href="#42311368">next</a><span>|</span><label class="collapse" for="c-42312899">[-]</label><label class="expand" for="c-42312899">[1 more]</label></div><br/><div class="children"><div class="content">I guess that would be an NPU combined with LPDDR. Basically any Windows Copilot Plus approved device.</div><br/></div></div></div></div></div></div><div id="42311368" class="c"><input type="checkbox" id="c-42311368" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311136">parent</a><span>|</span><a href="#42311378">prev</a><span>|</span><a href="#42310375">next</a><span>|</span><label class="collapse" for="c-42311368">[-]</label><label class="expand" for="c-42311368">[1 more]</label></div><br/><div class="children"><div class="content">Me too, probably 2x. I’d sell like hot cakes.</div><br/></div></div></div></div></div></div><div id="42310375" class="c"><input type="checkbox" id="c-42310375" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310034">parent</a><span>|</span><a href="#42310307">prev</a><span>|</span><a href="#42310290">next</a><span>|</span><label class="collapse" for="c-42310375">[-]</label><label class="expand" for="c-42310375">[1 more]</label></div><br/><div class="children"><div class="content">That would basically mean Intel doubling the size of their current GPU die, with a different memory PHY. They&#x27;re clearly not ready to make that an affordable card. Maybe when they get around to making a chiplet-based GPU.</div><br/></div></div><div id="42310290" class="c"><input type="checkbox" id="c-42310290" checked=""/><div class="controls bullet"><span class="by">kevingadd</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310034">parent</a><span>|</span><a href="#42310375">prev</a><span>|</span><a href="#42310175">next</a><span>|</span><label class="collapse" for="c-42310290">[-]</label><label class="expand" for="c-42310290">[12 more]</label></div><br/><div class="children"><div class="content">Are you suggesting that Intel &#x27;just&#x27; release a GPU at the same price point as an M4 Max SOC? And that there would be a large market for it if they did so? Seems like an extremely niche product that would be demanding to manufacture. The M4 Max makes sense because it&#x27;s a complete system they can sell to Apple&#x27;s price-insensitive audience, Intel doesn&#x27;t have a captive market like that to sell bespoke LLM accelerator cards to yet.<p>If this hypothetical 128GB LLM accelerator was also a capable GPU that would be more interesting but Intel hasn&#x27;t proven an ability to execute on that level yet.</div><br/><div id="42310368" class="c"><input type="checkbox" id="c-42310368" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310290">parent</a><span>|</span><a href="#42313841">next</a><span>|</span><label class="collapse" for="c-42310368">[-]</label><label class="expand" for="c-42310368">[9 more]</label></div><br/><div class="children"><div class="content">Nothing in my comment says about pricing it at the M4 Max level. Apple charges as much because they can (typing this on an $8000 M3 Max). 128GB LPDDR5 is dirt cheap these days just Apple adds its premium because they like to. Nothing prevents Intel from releasing a basic GPU with that much RAM for under $1k.</div><br/><div id="42310532" class="c"><input type="checkbox" id="c-42310532" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310368">parent</a><span>|</span><a href="#42313841">next</a><span>|</span><label class="collapse" for="c-42310532">[-]</label><label class="expand" for="c-42310532">[8 more]</label></div><br/><div class="children"><div class="content">You&#x27;re asking for a GPU die at least as large as NVIDIA&#x27;s TU102 that was $1k in 2018 when paired with only 11GB of RAM (because $1k couldn&#x27;t get you a fully-enabled die to use 12GB of RAM). I think you&#x27;re off by at least a factor of two in your cost estimates.</div><br/><div id="42314047" class="c"><input type="checkbox" id="c-42314047" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310532">parent</a><span>|</span><a href="#42310628">next</a><span>|</span><label class="collapse" for="c-42314047">[-]</label><label class="expand" for="c-42314047">[1 more]</label></div><br/><div class="children"><div class="content">If Intel came out with an ARC GPU with 128GB VRAM at a $2000 price point, I and many others would likely buy it immediately.</div><br/></div></div><div id="42310628" class="c"><input type="checkbox" id="c-42310628" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310532">parent</a><span>|</span><a href="#42314047">prev</a><span>|</span><a href="#42313841">next</a><span>|</span><label class="collapse" for="c-42310628">[-]</label><label class="expand" for="c-42310628">[6 more]</label></div><br/><div class="children"><div class="content">Intel has Xeon Phi which was a spin-off of their first attempt at GPU so they have a lot of tech in place they can reuse already. They don&#x27;t need to go with GDDRx&#x2F;HBMx designs that require large dies.</div><br/><div id="42311321" class="c"><input type="checkbox" id="c-42311321" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310628">parent</a><span>|</span><a href="#42311506">next</a><span>|</span><label class="collapse" for="c-42311321">[-]</label><label class="expand" for="c-42311321">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t want to further this discussions but may be you dont realise some of the people who replied to you either design hardware for a living or has been in the hardware industry for longer than 20 years.</div><br/><div id="42314526" class="c"><input type="checkbox" id="c-42314526" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311321">parent</a><span>|</span><a href="#42311571">next</a><span>|</span><label class="collapse" for="c-42314526">[-]</label><label class="expand" for="c-42314526">[1 more]</label></div><br/><div class="children"><div class="content">While it is not a GPU, Qualcomm already made an inferencing card with 128GB RAM:<p><a href="https:&#x2F;&#x2F;www.qualcomm.com&#x2F;news&#x2F;onq&#x2F;2023&#x2F;11&#x2F;introducing-qualcomm-cloud-ai-100-ultra" rel="nofollow">https:&#x2F;&#x2F;www.qualcomm.com&#x2F;news&#x2F;onq&#x2F;2023&#x2F;11&#x2F;introducing-qualco...</a><p>It would be interesting if those saying that a regular GPU with 128GB of VRAM cannot be made would explain how Qualcomm was able to make this card. It is not a big stretch to imagine a GPU with the same memory configuration. Note that Qualcomm did not use HBM for this.</div><br/></div></div><div id="42311571" class="c"><input type="checkbox" id="c-42311571" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311321">parent</a><span>|</span><a href="#42314526">prev</a><span>|</span><a href="#42311506">next</a><span>|</span><label class="collapse" for="c-42311571">[-]</label><label class="expand" for="c-42311571">[2 more]</label></div><br/><div class="children"><div class="content">For some reason Apple did it with M3&#x2F;M4 Max likely by folks that are also on HN. The question is how many of the years spent designing HW were spent also by educating oneselves on the latest best ways to do it.</div><br/><div id="42311648" class="c"><input type="checkbox" id="c-42311648" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311571">parent</a><span>|</span><a href="#42311506">next</a><span>|</span><label class="collapse" for="c-42311648">[-]</label><label class="expand" for="c-42311648">[1 more]</label></div><br/><div class="children"><div class="content">&gt;For some reason.....<p>They already replied with an answer.</div><br/></div></div></div></div></div></div><div id="42311506" class="c"><input type="checkbox" id="c-42311506" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310628">parent</a><span>|</span><a href="#42311321">prev</a><span>|</span><a href="#42313841">next</a><span>|</span><label class="collapse" for="c-42311506">[-]</label><label class="expand" for="c-42311506">[1 more]</label></div><br/><div class="children"><div class="content">Even LPDDR requires a large die. It only takes things out of the realm of technologically impossible to merely economically impractical. A 512-bit bus is still very inconveniently large for a single die.</div><br/></div></div></div></div></div></div></div></div><div id="42313841" class="c"><input type="checkbox" id="c-42313841" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310290">parent</a><span>|</span><a href="#42310368">prev</a><span>|</span><a href="#42310389">next</a><span>|</span><label class="collapse" for="c-42313841">[-]</label><label class="expand" for="c-42313841">[1 more]</label></div><br/><div class="children"><div class="content">&gt; release a GPU at the same price point as an M4 Max SOC<p>Why would it need to be introduced at Apple&#x27;s high-margin pricing?</div><br/></div></div><div id="42310389" class="c"><input type="checkbox" id="c-42310389" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310290">parent</a><span>|</span><a href="#42313841">prev</a><span>|</span><a href="#42310175">next</a><span>|</span><label class="collapse" for="c-42310389">[-]</label><label class="expand" for="c-42310389">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s also impossible and it would need to be a CPU.<p>CPUs and GPUs access memory very differently.</div><br/></div></div></div></div></div></div><div id="42313966" class="c"><input type="checkbox" id="c-42313966" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42309957">parent</a><span>|</span><a href="#42310034">prev</a><span>|</span><a href="#42311608">next</a><span>|</span><label class="collapse" for="c-42313966">[-]</label><label class="expand" for="c-42313966">[7 more]</label></div><br/><div class="children"><div class="content">It is possible to have multiple memory ranks to reduce the bus width requirements for a given amount of memory. Nvidia has demonstrated that this is doable with GDDR6X on the RTX 3090. The RTX 3090 has a 384-bit bus with 24 memory ICs, despite only needing 12 to reach 384-bit. That means it has every two chips sharing one 32-bit interface, which is a dual rank configuration. If you look at the history of computer memory, you can find many examples of multi-rank configurations. I also recall LR-DIMMs as being another way of achieving this.<p>Achieving 128GB VRAM with a 256-bit bus (which seems like a reasonable bus width) would mean some multiple of 8 chips. If Micron, Samsung or SK Hynix made 128Gb GDDR7 chips, then 8 would suffice. The best right now seems 24Gb, although 32Gb seems likely to follow (and it would likely come sooner if a large customer such as Intel asked for it), so they would just need to have 32 chips in a quad rank configuration to achieve 128GB.<p>This assumes that there is no limit in the GDDR7 specification that prevents quad rank configurations. If there is and it still supports dual rank like GDDR6X did, then a 512-bit bus could be done. It would likely be extremely pricy and require a new chip tape out that has much more IO logic transistors to handle the additional bus width (and IO logic transistor scaling is dead, so the die area would be huge), but it is hypothetically possible. Given how much people are willing to pay for more VRAM, it could make business sense to do.<p>Even if there is no limit in the GDDR7 specification that prevents quadrank, their memory IO logic would need to support it and if it does not, they would need to redesign that and do a new chip tape out in addition to a new board design. This would also be very expensive, although not as expensive as going to a 512-bit memory interface.<p>In summary, adding more memory would cost more to do and it would not improve competitiveness in the target market for these cards, which I imagine is the main reason that they do not do it.<p>By the way, the reason that Nvidia implemented support for 2 chips per channel is because they wanted to be able to reach 48GB VRAM on the workstation variant of the 3090 that is known as the RTX A6000 (non-Ada). I do not know why they used 24x 8Gb chips rather than 12x 16Gb on the 3090, although if I had to guess, it had something to do with rank interleaving.</div><br/><div id="42314098" class="c"><input type="checkbox" id="c-42314098" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42313966">parent</a><span>|</span><a href="#42311608">next</a><span>|</span><label class="collapse" for="c-42314098">[-]</label><label class="expand" for="c-42314098">[6 more]</label></div><br/><div class="children"><div class="content">Having four chips per channel is exactly why this is implausible. DDR5 can barely operate with four ranks per channel, at severely reduced speeds. Pulling that off with GDDR6 or GDDR7 is not something we can presume to be possible without specific evidence. The highest-density configurations possible for LPDDR5x are dual-rank and byte mode (one chip per 8 bits of the memory bus, so two chips ganged together to populate a 16-bit channel) — and that still operates at less than half the speed of GDDR6.<p>I&#x27;ve not seen any proposals for buffering LPDDR or GDDR, so an analog to LRDIMMs is not a readily-available technology.<p>GDDR is the memory technology that operates at the edge of what&#x27;s possible for per-pin bandwidth. Loading that memory bus down with many ranks is not something we can expect to be achievable by just putting down more pads on the PCB.</div><br/><div id="42314233" class="c"><input type="checkbox" id="c-42314233" checked=""/><div class="controls bullet"><span class="by">jmb99</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42314098">parent</a><span>|</span><a href="#42314207">next</a><span>|</span><label class="collapse" for="c-42314233">[-]</label><label class="expand" for="c-42314233">[2 more]</label></div><br/><div class="children"><div class="content">&gt; DDR5 can barely operate with four ranks per channel, at severely reduced speeds.<p>That is objectively false. See, for instance, V-color’s threadripper RAM[0]. If 96GB quad-rank modules @ 6000Mhz in octo-channel counts as “barely operating” maybe we have different definitions of operation requirements.<p>As a side note, their quad-channel 3-rank RAM [1] hits 8000MHz, out of the box. Admittedly only 24GB modules, but still.<p>[0] <a href="https:&#x2F;&#x2F;v-color.net&#x2F;products&#x2F;ddr5-ocrdimm-amd-wrx90-workstationmemory?variant=44781684523175" rel="nofollow">https:&#x2F;&#x2F;v-color.net&#x2F;products&#x2F;ddr5-ocrdimm-amd-wrx90-workstat...</a>
[1] <a href="https:&#x2F;&#x2F;v-color.net&#x2F;products&#x2F;ddr5-oc-rdimm-amd-trx50-workstation-memory?variant=45111006429351" rel="nofollow">https:&#x2F;&#x2F;v-color.net&#x2F;products&#x2F;ddr5-oc-rdimm-amd-trx50-worksta...</a></div><br/><div id="42314731" class="c"><input type="checkbox" id="c-42314731" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42314233">parent</a><span>|</span><a href="#42314207">next</a><span>|</span><label class="collapse" for="c-42314731">[-]</label><label class="expand" for="c-42314731">[1 more]</label></div><br/><div class="children"><div class="content">You linked to registered&#x2F;buffered memory modules. I already addressed that case; it doesn&#x27;t apply to LPDDR or GDDR.</div><br/></div></div></div></div><div id="42314207" class="c"><input type="checkbox" id="c-42314207" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42314098">parent</a><span>|</span><a href="#42314233">prev</a><span>|</span><a href="#42311608">next</a><span>|</span><label class="collapse" for="c-42314207">[-]</label><label class="expand" for="c-42314207">[3 more]</label></div><br/><div class="children"><div class="content">In that case, we need a 512-bit memory bus to do this using the 32Gbit GDDR7 chips that should be on the market in the near future. This would be very expensive, but it should be possible, or do you see a reason why that cannot be done either?<p>That said, I am not an electrical engineer (although I work alongside one and have had a minor role in picking low end components for custom PCBs), I think if Intel were to make a GPU with 128GB VRAM using GDDR7 in the next year or two, the engineer who does the trace routing to make it possible should make a go fund me page for people to send beer money.</div><br/><div id="42314832" class="c"><input type="checkbox" id="c-42314832" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42314207">parent</a><span>|</span><a href="#42311608">next</a><span>|</span><label class="collapse" for="c-42314832">[-]</label><label class="expand" for="c-42314832">[2 more]</label></div><br/><div class="children"><div class="content">I think the goalposts may have shifted a bit, from why <i>hasn&#x27;t</i> Intel made such a card to why is Intel not (publicly) working on such a card to be released in a year or two.<p>In terms of what would have been feasible for Intel to bring to market in 2024, the cheapest option for 128GB capacity would probably have been ~8.5Gb&#x2F;s LPDDR5x on a 256-bit bus, but to at least match the bandwidth of the chip they just launched, it would have made more sense to use a 512-bit bus and bump the die size back up to ~half the reticle limit like their previous generation die with a 256-bit bus. So they would have had a quite slow but high-capacity GPU with a manufacturing cost equal to at least an RTX 4080, <i>before</i> adding in the cost of all that DRAM. And if they had started working on that chip as soon as LLaMA went public, they <i>might</i> have been able to deliver it by now.<p>It&#x27;s no surprise at all that such a risky niche product did not emerge from a division of Intel that is lucky to not have been liquidated yet.</div><br/><div id="42315517" class="c"><input type="checkbox" id="c-42315517" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42314832">parent</a><span>|</span><a href="#42311608">next</a><span>|</span><label class="collapse" for="c-42315517">[-]</label><label class="expand" for="c-42315517">[1 more]</label></div><br/><div class="children"><div class="content">In hindsight, I misread you as saying that 128GB of RAM on a “basic GPU” is not technically feasible. My reply was to say it is feasible.<p>Intel is rumored to have a B770 GPU in development, but it was running late and then was delayed to next year since it had yet to tape out, so they are launching their B580 and B570 graphics cards, which had been ready to go for a while, now. That is why the bus size appears to have dropped across generations. Presumably, if they made a 512-bit bus version, it would be a 9 series card. They certainly left room for it in their lineup, but as far as leaks have been concerned, there is not much hope for one. I do not expect them to use anything other than GDDR7 on their battlemage cards.<p>As for a high memory ARC card, I am of the opinion that such a product would sell well among the local llama community. There might even be more sales of a high memory ARC card for inference than of the regular ARC cards for gaming given that their discrete graphics sales peaked at 250,000 in Q1 2023 before collapsing, which can be confirmed using the data here:<p><a href="https:&#x2F;&#x2F;www.tomshardware.com&#x2F;pc-components&#x2F;gpus&#x2F;discrete-gpu-sales-increase-as-intels-share-drops-to-0" rel="nofollow">https:&#x2F;&#x2F;www.tomshardware.com&#x2F;pc-components&#x2F;gpus&#x2F;discrete-gpu...</a><p>The market for high memory GPUs is surely bigger than that. That said, Intel is likely pricing their ARC GPUs at a loss after R&amp;D costs are considered. This is likely intended to help them break into a new market, although it has not been going well for them so far. I would guess that they are at least a generation away from profitability.<p>Intel intends for its Gaudi 3 accelerators to be used for this rather than the ARC line. Those coincidentally have 128GB of RAM, but they use HBM rather than a DDR variant. Qualcomm on the other hand made its own accelerator with 128GB of LPDDR4x RAM:<p><a href="https:&#x2F;&#x2F;www.qualcomm.com&#x2F;news&#x2F;onq&#x2F;2023&#x2F;11&#x2F;introducing-qualcomm-cloud-ai-100-ultra" rel="nofollow">https:&#x2F;&#x2F;www.qualcomm.com&#x2F;news&#x2F;onq&#x2F;2023&#x2F;11&#x2F;introducing-qualco...</a><p>If my math is right, Qualcomm went with a 1024-bit memory bus and some incorrect rounding (rounding 137.5 to 138 before multiplying by 4) to reach their stated bandwidth figure. Qualcomm is not selling it through the PC parts supply chain, so I have no clue how much it costs, but I assume that it is expensive. I assume that they used LPDDR4x to be able to build a product since they were too late in securing HBM supply and even if they did, they would not be able to scale production to meet demand growth since Nvidia is buying all of the HBM that it can.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42311608" class="c"><input type="checkbox" id="c-42311608" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42309957">parent</a><span>|</span><a href="#42313966">prev</a><span>|</span><a href="#42310966">next</a><span>|</span><label class="collapse" for="c-42311608">[-]</label><label class="expand" for="c-42311608">[4 more]</label></div><br/><div class="children"><div class="content">Thank You Wtallis. Somewhere along the line, this basic &quot;knowledge&quot; of hardware is completely lost. I dont expect this to be explained in any comment section on old Anandtech. It seems hardware enthusiast has mostly disappeared, I guess that is also why Anandtech closed. We now live in a world where most site are just BS rumours.</div><br/><div id="42314544" class="c"><input type="checkbox" id="c-42314544" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311608">parent</a><span>|</span><a href="#42311905">next</a><span>|</span><label class="collapse" for="c-42314544">[-]</label><label class="expand" for="c-42314544">[1 more]</label></div><br/><div class="children"><div class="content">Qualcomm made an AI inferencing card with 128GB RAM without using HBM:<p><a href="https:&#x2F;&#x2F;www.qualcomm.com&#x2F;news&#x2F;onq&#x2F;2023&#x2F;11&#x2F;introducing-qualcomm-cloud-ai-100-ultra" rel="nofollow">https:&#x2F;&#x2F;www.qualcomm.com&#x2F;news&#x2F;onq&#x2F;2023&#x2F;11&#x2F;introducing-qualco...</a><p>Would someone with “basic ‘knowledge’ of hardware” explain why a GPU cannot be made with the same memory configuration?</div><br/></div></div><div id="42311905" class="c"><input type="checkbox" id="c-42311905" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311608">parent</a><span>|</span><a href="#42314544">prev</a><span>|</span><a href="#42313448">next</a><span>|</span><label class="collapse" for="c-42311905">[-]</label><label class="expand" for="c-42311905">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s because Anand Lal Shimpi is a CompE by training.<p>Not too many hardware enthusiast site editors have that academic background.<p>And while fervor can sometimes substitute for education... probably not in microprocessor &#x2F; system design.</div><br/></div></div><div id="42313448" class="c"><input type="checkbox" id="c-42313448" checked=""/><div class="controls bullet"><span class="by">scrlk</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311608">parent</a><span>|</span><a href="#42311905">prev</a><span>|</span><a href="#42310966">next</a><span>|</span><label class="collapse" for="c-42313448">[-]</label><label class="expand" for="c-42313448">[1 more]</label></div><br/><div class="children"><div class="content">The Real World Technologies forum is still an absolute gold mine for hardware discussion.<p>As for articles, IMO, Chips and Cheese is the closest thing we have to Anandtech or RWT in their peak.</div><br/></div></div></div></div><div id="42310966" class="c"><input type="checkbox" id="c-42310966" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42309957">parent</a><span>|</span><a href="#42311608">prev</a><span>|</span><a href="#42312816">next</a><span>|</span><label class="collapse" for="c-42310966">[-]</label><label class="expand" for="c-42310966">[8 more]</label></div><br/><div class="children"><div class="content">What if they put 8 identical GPUs in the package, each with 1&#x2F;8 the memory? Would that be a useful configuration for a modern LLM?</div><br/><div id="42311831" class="c"><input type="checkbox" id="c-42311831" checked=""/><div class="controls bullet"><span class="by">numeri</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310966">parent</a><span>|</span><a href="#42314551">next</a><span>|</span><label class="collapse" for="c-42311831">[-]</label><label class="expand" for="c-42311831">[2 more]</label></div><br/><div class="children"><div class="content">GPU inference is always a balancing act, trying to avoid bottlenecks on memory bandwidth (loading data from the GPU&#x27;s global memory&#x2F;VRAM to the much smaller internal shared memory, where it can be used for calculations) and compute (once the values are loaded).<p>Splitting the model up between several GPUs would add a third much worse bottleneck – memory bandwidth between the GPUs. No matter how well you connect them, it&#x27;ll be slower than transfer within a single GPU.<p>Still, the fact that you can fit an 8× larger GPU might be worth it to you. It&#x27;s a trade-off that&#x27;s almost universally made while training LLMs (sometimes even with the model split down both its width and length), but is much less attractive for inference.</div><br/><div id="42312303" class="c"><input type="checkbox" id="c-42312303" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311831">parent</a><span>|</span><a href="#42314551">next</a><span>|</span><label class="collapse" for="c-42312303">[-]</label><label class="expand" for="c-42312303">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Splitting the model up between several GPUs would add a third much worse bottleneck – memory bandwidth between the GPUs.<p>What if you allowed the system to only have a shared memory between every neighboring pair of GPUs?<p>Would that make sense for an LLM?</div><br/></div></div></div></div><div id="42314551" class="c"><input type="checkbox" id="c-42314551" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310966">parent</a><span>|</span><a href="#42311831">prev</a><span>|</span><a href="#42311151">next</a><span>|</span><label class="collapse" for="c-42314551">[-]</label><label class="expand" for="c-42314551">[1 more]</label></div><br/><div class="children"><div class="content">Yes:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42313615">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42313615</a></div><br/></div></div><div id="42311151" class="c"><input type="checkbox" id="c-42311151" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310966">parent</a><span>|</span><a href="#42314551">prev</a><span>|</span><a href="#42312024">next</a><span>|</span><label class="collapse" for="c-42311151">[-]</label><label class="expand" for="c-42311151">[1 more]</label></div><br/><div class="children"><div class="content">Last I&#x27;ve heard, the architecture makes that difficult. But my information may be outdated, and even if it isn&#x27;t, I&#x27;m not a hardware designer and may have just misunderstood the limits I hear others discuss.</div><br/></div></div><div id="42312024" class="c"><input type="checkbox" id="c-42312024" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310966">parent</a><span>|</span><a href="#42311151">prev</a><span>|</span><a href="#42311082">next</a><span>|</span><label class="collapse" for="c-42312024">[-]</label><label class="expand" for="c-42312024">[1 more]</label></div><br/><div class="children"><div class="content">K80 used to be two glued K40 but their interconnect was barely faster than PCIe so it didn&#x27;t have much benefit as one had to move stuff between two internal GPUs anyway.</div><br/></div></div><div id="42311082" class="c"><input type="checkbox" id="c-42311082" checked=""/><div class="controls bullet"><span class="by">keyboard_slap</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310966">parent</a><span>|</span><a href="#42312024">prev</a><span>|</span><a href="#42312816">next</a><span>|</span><label class="collapse" for="c-42311082">[-]</label><label class="expand" for="c-42311082">[2 more]</label></div><br/><div class="children"><div class="content">It could work, but would it be cost-competitive?</div><br/><div id="42311781" class="c"><input type="checkbox" id="c-42311781" checked=""/><div class="controls bullet"><span class="by">rini17</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311082">parent</a><span>|</span><a href="#42312816">next</a><span>|</span><label class="collapse" for="c-42311781">[-]</label><label class="expand" for="c-42311781">[1 more]</label></div><br/><div class="children"><div class="content">Also, cooling.</div><br/></div></div></div></div></div></div><div id="42312816" class="c"><input type="checkbox" id="c-42312816" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42309957">parent</a><span>|</span><a href="#42310966">prev</a><span>|</span><a href="#42309966">next</a><span>|</span><label class="collapse" for="c-42312816">[-]</label><label class="expand" for="c-42312816">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>&quot;Just add more RAM&quot; doesn&#x27;t work the way you wish it could.</i><p>Re: Tomasulo&#x27;s algorithm the other day: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42231284">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42231284</a><p>Cerebras WSE-3 has 44 GB of on-chip SRAM per chip and it&#x27;s faster than HBM. 
<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41702789#41706409">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41702789#41706409</a><p>Intel has HBM2e off-chip RAM in Xeon CPU Max series and GPU Max;<p>What is the difference between DDR, HBM, and Cerebras&#x27; 44GB of on-chip SRAM?</div><br/><div id="42312971" class="c"><input type="checkbox" id="c-42312971" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312816">parent</a><span>|</span><a href="#42309966">next</a><span>|</span><label class="collapse" for="c-42312971">[-]</label><label class="expand" for="c-42312971">[1 more]</label></div><br/><div class="children"><div class="content">How do architectural bottlenecks due to modified Von Neumann architectures&#x27; <i>debuggable</i> instruction pipelines limit computational performance when scaling to larger amounts of off-chip RAM?<p>Tomasulo&#x27;s algorithm also centralizes on a common data bus (the CPU-RAM data bus) which is a bottleneck that must scale with the amount of RAM.<p>Can in-RAM computation solve for error correction without redundant computation and consensus algorithms?<p>Can on-chip SRAM be built at lower cost?<p>Von Neumann architecture: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Von_Neumann_architecture#Von_Neumann_bottleneck" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Von_Neumann_architecture#Von_N...</a> :<p>&gt; <i>The term &quot;von Neumann architecture&quot; has evolved to refer to any stored-program computer in which an instruction fetch and a data operation cannot occur at the same time (since they share a common bus). This is referred to as the von Neumann bottleneck, which often limits the performance of the corresponding system. [4]</i><p>&gt; <i>The von Neumann architecture is simpler than the Harvard architecture (which has one dedicated set of address and data buses for reading and writing to memory and another set of address and data buses to fetch instructions).</i><p>Modified Harvard architecture &gt; Comparisons: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Modified_Harvard_architecture" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Modified_Harvard_architecture</a><p>C-RAM: Computational RAM &gt; DRAM-based PIM Taxonomy, See also:
<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Computational_RAM" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Computational_RAM</a><p>SRAM: Static random-access memory
<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Static_random-access_memory" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Static_random-access_memory</a> :<p>&gt; <i>Typically, SRAM is used for the cache and internal registers of a CPU while DRAM is used for a computer&#x27;s main memory.</i></div><br/></div></div></div></div></div></div><div id="42309966" class="c"><input type="checkbox" id="c-42309966" checked=""/><div class="controls bullet"><span class="by">chessgecko</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42309957">prev</a><span>|</span><a href="#42310123">next</a><span>|</span><label class="collapse" for="c-42309966">[-]</label><label class="expand" for="c-42309966">[8 more]</label></div><br/><div class="children"><div class="content">GDDR isnt like the ram that connects to cpu, it&#x27;s much more difficult and expensive to add more. You can get up to 48GB with some expensive stacked gddr, but if you wanted to add more stacks you&#x27;d need to solve some serious signal timing related headaches that most users wouldn&#x27;t benefit from.<p>I think the high memory local inference stuff is going to come from &quot;AI enabled&quot; cpus that share the memory in your computer. Apple is doing this now, but cheaper options are on the way. As a shape its just suboptimal for graphics, so it doesn&#x27;t make sense for any of the gpu vendors to do it.</div><br/><div id="42310172" class="c"><input type="checkbox" id="c-42310172" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42309966">parent</a><span>|</span><a href="#42310056">next</a><span>|</span><label class="collapse" for="c-42310172">[-]</label><label class="expand" for="c-42310172">[2 more]</label></div><br/><div class="children"><div class="content">As someone else said - I don&#x27;t think you have to have GDDR, surely there are other options. Apple does a great job of it on their APUs with up to 192GB, even an old AMD Threadripper chip can do quite well with its DDR4&#x2F;5 performance</div><br/><div id="42310576" class="c"><input type="checkbox" id="c-42310576" checked=""/><div class="controls bullet"><span class="by">chessgecko</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310172">parent</a><span>|</span><a href="#42310056">next</a><span>|</span><label class="collapse" for="c-42310576">[-]</label><label class="expand" for="c-42310576">[1 more]</label></div><br/><div class="children"><div class="content">For ai inference you definitely have other options, but for low end graphics? the lpddr that apple (and nvidia in grace) use would be super expensive to get a comparable bandwidth (think $3+&#x2F;gb and to get 500GB&#x2F;sec you need at least 128GB).<p>And that 500GB&#x2F;sec is pretty low for a gpu, its like a 4070 but the memory alone would add $500+ to the cost of the inputs, not even counting the advanced packaging (getting those bandwidths out of lpddr needs organic substrate).<p>It&#x27;s not that you can&#x27;t, just when you start doing this it stops being like a graphics card and becomes like a cpu.</div><br/></div></div></div></div><div id="42310056" class="c"><input type="checkbox" id="c-42310056" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42309966">parent</a><span>|</span><a href="#42310172">prev</a><span>|</span><a href="#42310123">next</a><span>|</span><label class="collapse" for="c-42310056">[-]</label><label class="expand" for="c-42310056">[5 more]</label></div><br/><div class="children"><div class="content">They can use LPDDR5x, it would still massively accelerate inference of large local LLMs that need more than 48GB RAM. Any tensor swapping between CPU RAM and GPU RAM kills the performance.</div><br/><div id="42310682" class="c"><input type="checkbox" id="c-42310682" checked=""/><div class="controls bullet"><span class="by">chessgecko</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310056">parent</a><span>|</span><a href="#42310123">next</a><span>|</span><label class="collapse" for="c-42310682">[-]</label><label class="expand" for="c-42310682">[4 more]</label></div><br/><div class="children"><div class="content">I think we don&#x27;t really disagree, I just think that this shape isn&#x27;t really a gpu its just a cpu because it isn&#x27;t very good for graphics at that point.</div><br/><div id="42310839" class="c"><input type="checkbox" id="c-42310839" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310682">parent</a><span>|</span><a href="#42310123">next</a><span>|</span><label class="collapse" for="c-42310839">[-]</label><label class="expand" for="c-42310839">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s why I said &quot;basic GPU&quot;. It doesn&#x27;t have to be too fast but it should still be way faster than a regular CPU. Intel already has Xeon Phi so a lot of things were developed already (like memory controller, heavy parallel dies etc.)</div><br/><div id="42313296" class="c"><input type="checkbox" id="c-42313296" checked=""/><div class="controls bullet"><span class="by">ac29</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310839">parent</a><span>|</span><a href="#42311114">next</a><span>|</span><label class="collapse" for="c-42313296">[-]</label><label class="expand" for="c-42313296">[1 more]</label></div><br/><div class="children"><div class="content">I wonder at that point you&#x27;d just be better served by CPU with 4 channels of RAM. If my math is right 4 channels of DDR5-8000 would get you 256GB&#x2F;s. Not as much bandwidth as a typical discrete GPU, but it would be trivial to get many hundreds of GB of RAM and would be expandable.<p>Unfortunately I don&#x27;t think either Intel or AMD makes a CPU that supports quad channel RAM at a decent price.</div><br/></div></div><div id="42311114" class="c"><input type="checkbox" id="c-42311114" checked=""/><div class="controls bullet"><span class="by">chessgecko</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310839">parent</a><span>|</span><a href="#42313296">prev</a><span>|</span><a href="#42310123">next</a><span>|</span><label class="collapse" for="c-42311114">[-]</label><label class="expand" for="c-42311114">[1 more]</label></div><br/><div class="children"><div class="content">I guess it&#x27;s hard to know how well this would compete with integrated gpus, especially at a reasonable pricepoint. If you wanted to spend $4000+ on it, it could be very competitive and might look something like nvidias grace-hopper superchip, but if you want the product to be under $1k I think it might be better just to buy separate cards for your graphics and ai stuff.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42310123" class="c"><input type="checkbox" id="c-42310123" checked=""/><div class="controls bullet"><span class="by">FuriouslyAdrift</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42309966">prev</a><span>|</span><a href="#42314908">next</a><span>|</span><label class="collapse" for="c-42310123">[-]</label><label class="expand" for="c-42310123">[13 more]</label></div><br/><div class="children"><div class="content">HBM3E memory is at least 3x the price of DDR5 (it requires 3x the wafer as DDR5) and capacity is sold out for all of 2025 already... that&#x27;s the price and production bottleneck.<p>High speed, low latency server grade DDR5 is around $800-$1600 for 128GB. Triple that for $2400 - $4800 just for the memory. Still need the GPUs&#x2F;APUs, card, VRMs, etc.<p>Even the nVidia H100 with &quot;only&quot; 94GB starts at $30k...</div><br/><div id="42310286" class="c"><input type="checkbox" id="c-42310286" checked=""/><div class="controls bullet"><span class="by">adventured</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310123">parent</a><span>|</span><a href="#42314908">next</a><span>|</span><label class="collapse" for="c-42310286">[-]</label><label class="expand" for="c-42310286">[12 more]</label></div><br/><div class="children"><div class="content">Nvidia&#x27;s $30,000 is a 90% margin product at scale. They could charge 1&#x2F;3 that and still be very profitable. There has rarely been such a profitable large corporation in terms of the combo of profit &amp; margin.<p>Their last quarter was $35b in sales and $26b in gross profit ($21.8b op income; 62% op income margin vs sales).<p>Visa is notorious for their extreme margin (66% op income margin vs sales) due to being basically a brand + transaction network. So the fact that a hardware manufacturer is hitting those levels is truly remarkable.<p>It&#x27;s very clear that either AMD or Intel could accept far lower margins to go after them. And indeed that&#x27;s exactly what will be required for any serious attempt to cut into their monopoly position.</div><br/><div id="42311586" class="c"><input type="checkbox" id="c-42311586" checked=""/><div class="controls bullet"><span class="by">arcticbull</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310286">parent</a><span>|</span><a href="#42310653">next</a><span>|</span><label class="collapse" for="c-42311586">[-]</label><label class="expand" for="c-42311586">[2 more]</label></div><br/><div class="children"><div class="content">Visa doesn&#x27;t actually make a ton of money off each transaction, if you divide out their revenue against their payment volume (napkin math)...<p>They processed $12T in payments last year (almost a billion payments per day), with a net revenue of $32B. That&#x27;s a gross transaction margin of 0.26% and their GAAP net income was half that, about 0.14%. [1]<p>They&#x27;re just a transaction network, unlike say Amex which is both an issuer and a network. Being just the network is more operationally efficient.<p>[1] <a href="https:&#x2F;&#x2F;annualreport.visa.com&#x2F;financials&#x2F;default.aspx" rel="nofollow">https:&#x2F;&#x2F;annualreport.visa.com&#x2F;financials&#x2F;default.aspx</a></div><br/><div id="42312526" class="c"><input type="checkbox" id="c-42312526" checked=""/><div class="controls bullet"><span class="by">oivey</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311586">parent</a><span>|</span><a href="#42310653">next</a><span>|</span><label class="collapse" for="c-42312526">[-]</label><label class="expand" for="c-42312526">[1 more]</label></div><br/><div class="children"><div class="content">That’s a weird way to account for their business size. There isn’t a significant marginal cost per transaction. They didn’t sell $12T in products. They facilitated that much in payments. Their profits are fantastic.</div><br/></div></div></div></div><div id="42310653" class="c"><input type="checkbox" id="c-42310653" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310286">parent</a><span>|</span><a href="#42311586">prev</a><span>|</span><a href="#42311169">next</a><span>|</span><label class="collapse" for="c-42310653">[-]</label><label class="expand" for="c-42310653">[8 more]</label></div><br/><div class="children"><div class="content">&gt; And indeed that&#x27;s exactly what will be required for any serious attempt to cut into their monopoly position.<p>You misunderstand why and how Nvidia is a monopoly. Many companies make GPUs, and all those GPUs <i>can</i> be used for computation if you develop compute shaders for them. This part is not the problem, <i>you can already</i> go buy cheaper hardware that outperforms Nvidia if price is your only concern.<p>Software is the issue. That&#x27;s it - it&#x27;s CUDA and nothing else. You cannot assail Nvidia&#x27;s position, and moreover their hardware&#x27;s value, without a really solid reason for datacenters to own them. Datacenters do not want to own GPUs because once the AI bubble pops they&#x27;ll be bagholders for Intel and AMD&#x27;s depreciated software. Nvidia hardware can at least crypto mine, or be leased out to industrial customers that have their own remote CUDA applications. The demand for generic GPU compute is basically nonexistent, the reason this market exists at all is because CUDA exists, and you cannot turn over Nvidia&#x27;s foothold without accepting that fact.<p>The only way the entire industry can fuck over Nvidia is if they choose to invest in a complete CUDA replacement like OpenCL. That is the only way that Nvidia&#x27;s value can be actually deposed without any path of recourse for their business, and it will never happen because every single one of Nvidia&#x27;s competitors hate each other&#x27;s guts and would rather watch each other die in gladiatorial combat than help each other fight the monster. And Jensen Huang probably revels in it, CUDA is a hedged bet against the industry ever working together for common good.</div><br/><div id="42311056" class="c"><input type="checkbox" id="c-42311056" checked=""/><div class="controls bullet"><span class="by">DSingularity</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310653">parent</a><span>|</span><a href="#42313346">next</a><span>|</span><label class="collapse" for="c-42311056">[-]</label><label class="expand" for="c-42311056">[3 more]</label></div><br/><div class="children"><div class="content">I feel people are exaggerating the impossibility of replacing CUDA. Adopting CUDA is convenient right now because yes it is difficult to replace it. Barrier to entry for orgs that can do that is very high. But it has been done. Google has the TPU for example.</div><br/><div id="42311241" class="c"><input type="checkbox" id="c-42311241" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311056">parent</a><span>|</span><a href="#42311202">next</a><span>|</span><label class="collapse" for="c-42311241">[-]</label><label class="expand" for="c-42311241">[1 more]</label></div><br/><div class="children"><div class="content">The TPU is not a GPU nor is it commercially available. It is a chip optimized around a limited featureset with a limited software layer on top of it. It&#x27;s an impressive demonstration on Google&#x27;s behalf to be sure, but it&#x27;s also not a shot across the bow at Nvidia&#x27;s business. Nvidia has the TSMC relations, a refined and complex streaming multiprocessor architecture and <i>actual</i> software support their customers can go use today. TPUs haven&#x27;t quite taken over like people anticipated anyways.<p>I don&#x27;t personally think CUDA is impossible to replace - but I do think that everyone capable of replacing CUDA has been ignoring it recently. Nvidia&#x27;s role as the GPGPU compute people is secure for the foreseeable future. Apple wants to design <i>simpler</i> GPUs, AMD wants to design cheaper GPUs, and Intel wants to pretend like they can compete with AMD. Every stakeholder with the capacity to turn this ship around is pretending like Nvidia doesn&#x27;t exist and whistling until they go away.</div><br/></div></div><div id="42311202" class="c"><input type="checkbox" id="c-42311202" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311056">parent</a><span>|</span><a href="#42311241">prev</a><span>|</span><a href="#42313346">next</a><span>|</span><label class="collapse" for="c-42311202">[-]</label><label class="expand" for="c-42311202">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re not exaggerating it. The more things change, the more they stay the same. Nvidia and AMD had the exact same relationship 15 years ago that they do today. The AMD crowd clutching about their better efficiencies, and the Nvidia crowd having grossly superior drivers&#x2F;firmware&#x2F;hardware, including unique PhysX stuff that STILL has not been matched since 2012 (remember Planetside 2 or Broderlands 2 physics? Pepperidge Farm Remembers...)<p>So many billions of dollars and no one is even 1% close to displacing CUDA in any meaningful way. ZULDA is dead. ROCM is a meme, Scale is a meme. Either you use CUDA or you don&#x27;t do meaningful AI work.</div><br/></div></div></div></div><div id="42313346" class="c"><input type="checkbox" id="c-42313346" checked=""/><div class="controls bullet"><span class="by">ac29</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310653">parent</a><span>|</span><a href="#42311056">prev</a><span>|</span><a href="#42310981">next</a><span>|</span><label class="collapse" for="c-42313346">[-]</label><label class="expand" for="c-42313346">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The only way the entire industry can fuck over Nvidia is if they choose to invest in a complete CUDA replacement like OpenCL. That is the only way that Nvidia&#x27;s value can be actually deposed without any path of recourse for their business, and it will never happen because every single one of Nvidia&#x27;s competitors hate each other&#x27;s guts and would rather watch each other die<p>Intel seems to have thrown their weight behind SYCL, which is an open standard intended to compete with CUDA. Its not clear there has been much interest from other hardware vendors though.</div><br/></div></div><div id="42310981" class="c"><input type="checkbox" id="c-42310981" checked=""/><div class="controls bullet"><span class="by">adventured</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310653">parent</a><span>|</span><a href="#42313346">prev</a><span>|</span><a href="#42311169">next</a><span>|</span><label class="collapse" for="c-42310981">[-]</label><label class="expand" for="c-42310981">[3 more]</label></div><br/><div class="children"><div class="content">I do not misunderstand why Nvidia has a monopoly. You jumped drastically beyond anything I was discussing and incorrectly assumed ignorance on my part. I never said why I thought they had one. I never brought up matters of performance or software or moats at all. I matter of fact stated they had a monopoly, you assumed the rest.<p>It&#x27;s impossible to assail their monopoly without utilizing far lower prices, coming up under their extreme margin products. It&#x27;s how it is almost always done competitively in tech (see: ARM, or Office (dramatically undercut Lotus with a cheaper inferior product), or Linux, or Huawei, or Chromebooks, or Internet Explorer, or just about anything).<p>Note: I never said lower prices is all you&#x27;d need. Who would think that? The implication is that I&#x27;m ignorant of the entire history of tech, it&#x27;s a poor approach to discussion with another person on HN frankly.</div><br/><div id="42311582" class="c"><input type="checkbox" id="c-42311582" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310981">parent</a><span>|</span><a href="#42311169">next</a><span>|</span><label class="collapse" for="c-42311582">[-]</label><label class="expand" for="c-42311582">[2 more]</label></div><br/><div class="children"><div class="content">Nvidia&#x27;s monopoly is pretty much detached from price at this point. That&#x27;s the entire reason <i>why</i> they can charge insane margins - nobody cares! There is not a single business squaring Nvidia up with serious intent to take down CUDA. It&#x27;s been this way for nearly two decades at this point, with not a single spark of hope to show for it.<p>In the case of ARM, Office, Linux, Huawei, and ChromeOS, these were all <i>actual</i> alternatives to the incumbent tools people were familiar with. You can directly compare Office and Lotus because they are fundamentally similar products - ARM had a real chance against x86 because wasn&#x27;t a complex ISA to unseat. Nvidia is not analogous to these businesses because they occupy a league of their own as the provider of CUDA. It&#x27;s not exaggeration to say that they have completely seceded from the market of GPUs and can sustain themselves on demand from crypto miners and AI pundits alone.<p>AMD, Intel and even Apple have bigger things to worry about than hitting an arbitrary price point, if they want Nvidia in their crosshairs. All of them have already solved the &quot;sell consumer tech at attractive prices&quot; problem but not the &quot;make it complex, standardize it and scale it up&quot; problem.</div><br/><div id="42314761" class="c"><input type="checkbox" id="c-42314761" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311582">parent</a><span>|</span><a href="#42311169">next</a><span>|</span><label class="collapse" for="c-42314761">[-]</label><label class="expand" for="c-42314761">[1 more]</label></div><br/><div class="children"><div class="content">It is cheaper to pay Nvidia than it is to roll your own solution and no one else is competitive. That is the reason Nvidia can charge so much per card.</div><br/></div></div></div></div></div></div></div></div><div id="42311169" class="c"><input type="checkbox" id="c-42311169" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310286">parent</a><span>|</span><a href="#42310653">prev</a><span>|</span><a href="#42314908">next</a><span>|</span><label class="collapse" for="c-42311169">[-]</label><label class="expand" for="c-42311169">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for laying it out. It&#x27;s so silly to see people in the comments act like Intel or Nvidia can&#x27;t EASILY add more VRAM to their cards. Every single argument against it is all hogwash.</div><br/></div></div></div></div></div></div><div id="42314908" class="c"><input type="checkbox" id="c-42314908" checked=""/><div class="controls bullet"><span class="by">IceHegel</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42310123">prev</a><span>|</span><a href="#42310052">next</a><span>|</span><label class="collapse" for="c-42314908">[-]</label><label class="expand" for="c-42314908">[1 more]</label></div><br/><div class="children"><div class="content">After carefully reviewing all of the other comments explaining the many technical and organizational reasons why they should not “just do that,” I have come to the conclusion that it was a big missed opportunity by intel.</div><br/></div></div><div id="42310052" class="c"><input type="checkbox" id="c-42310052" checked=""/><div class="controls bullet"><span class="by">whatudb</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42314908">prev</a><span>|</span><a href="#42311251">next</a><span>|</span><label class="collapse" for="c-42310052">[-]</label><label class="expand" for="c-42310052">[3 more]</label></div><br/><div class="children"><div class="content">Meta comment: &quot;why don&#x27;t they just&quot; phrase usually indicates significant ignorance about a subject, it&#x27;s better to learn a little bit before dispensing criticism about beancounters or whatnot.<p>In this case, the die I&#x2F;O limits precludes more than a reasonable number of DDR channels.</div><br/><div id="42312970" class="c"><input type="checkbox" id="c-42312970" checked=""/><div class="controls bullet"><span class="by">skerit</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310052">parent</a><span>|</span><a href="#42311602">next</a><span>|</span><label class="collapse" for="c-42312970">[-]</label><label class="expand" for="c-42312970">[1 more]</label></div><br/><div class="children"><div class="content">Op asked a question and got a bunch of answers &quot;why they couldn&#x27;t do just that&quot;. I think that&#x27;s a win.</div><br/></div></div></div></div><div id="42311251" class="c"><input type="checkbox" id="c-42311251" checked=""/><div class="controls bullet"><span class="by">Sparkyte</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42310052">prev</a><span>|</span><a href="#42309760">next</a><span>|</span><label class="collapse" for="c-42311251">[-]</label><label class="expand" for="c-42311251">[2 more]</label></div><br/><div class="children"><div class="content">Because you can&#x27;t stack that much ram on a GPU without sufficient channels to do so. You could probably do 64GB on GDDR6 but you can&#x27;t do 128GB on GDDR6 without more memory channels. 2GB per chip per channel is the current limit for GDDR6 this is why HBM was invented.<p>It is why you can only see GPUs with 24GB of memory at the moment.<p>HBM2 can handle 64GB ( 4 x 8GB Stack ) ( Total capacity 128GB )<p>HBM3 can handle 192GB ( 4 x 24GB Stack ) ( Total capacity 384GB )<p>You can not do this with GDDR6.</div><br/><div id="42314004" class="c"><input type="checkbox" id="c-42314004" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311251">parent</a><span>|</span><a href="#42309760">next</a><span>|</span><label class="collapse" for="c-42314004">[-]</label><label class="expand" for="c-42314004">[1 more]</label></div><br/><div class="children"><div class="content">Look at the RTX 3090 and RTX A6000 (the non-Ada one). They both have 24 memory chips with a 384-bit memory bus, but one has 24GB of VRAM and the other has 48GB of VRAM. They both have two chips per channel. This breaks the 24GB VRAM limit that you claim to exist.</div><br/></div></div></div></div><div id="42309760" class="c"><input type="checkbox" id="c-42309760" checked=""/><div class="controls bullet"><span class="by">gs17</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42311251">prev</a><span>|</span><a href="#42315298">next</a><span>|</span><label class="collapse" for="c-42309760">[-]</label><label class="expand" for="c-42309760">[1 more]</label></div><br/><div class="children"><div class="content">Even 24 or 32 GB for an accessible price would sell out fast. NVIDIA wants $2000 for the 5090 to get 32.</div><br/></div></div><div id="42315298" class="c"><input type="checkbox" id="c-42315298" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42309760">prev</a><span>|</span><a href="#42309896">next</a><span>|</span><label class="collapse" for="c-42315298">[-]</label><label class="expand" for="c-42315298">[1 more]</label></div><br/><div class="children"><div class="content">The question is is there a real market for this? I do think it could bootstrap from local inference enthousiasts, but it is not clearcut.<p>Rather than go all in with 128GB, they could test the waters easily with a cheap 32GB offering and take ot from there.</div><br/></div></div><div id="42309896" class="c"><input type="checkbox" id="c-42309896" checked=""/><div class="controls bullet"><span class="by">Numerlor</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42315298">prev</a><span>|</span><a href="#42311013">next</a><span>|</span><label class="collapse" for="c-42309896">[-]</label><label class="expand" for="c-42309896">[6 more]</label></div><br/><div class="children"><div class="content">48 GB is at the tail end of what&#x27;s reasonale for normal GPUs. The IO requires a lot of die space. And intel&#x27;s architecture is not very space efficient right now compared to nvidia&#x27;s</div><br/><div id="42309948" class="c"><input type="checkbox" id="c-42309948" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42309896">parent</a><span>|</span><a href="#42311013">next</a><span>|</span><label class="collapse" for="c-42309948">[-]</label><label class="expand" for="c-42309948">[5 more]</label></div><br/><div class="children"><div class="content">&gt; The IO requires a lot of die space.<p>And even if you spend a lot of die space on memory controllers, you can only fit so many GDDR chips around the GPU core while maintaining signal integrity. HBM sidesteps that issue but it&#x27;s still too expensive for anything but the highest end accelerators, and the ordinary LPDDR that Apple uses is lacking in bandwidth compared to GDDR, so they have to compensate with ginormous amounts of IO silicon. The M4 Ultra is expected to have similar bandwidth to a 4090 but the former will need a 1024bit bus to get there while the latter is only 384bit.</div><br/><div id="42310059" class="c"><input type="checkbox" id="c-42310059" checked=""/><div class="controls bullet"><span class="by">Numerlor</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42309948">parent</a><span>|</span><a href="#42311013">next</a><span>|</span><label class="collapse" for="c-42310059">[-]</label><label class="expand" for="c-42310059">[4 more]</label></div><br/><div class="children"><div class="content">Going off of how the 4090 and 7900 xtx is arranged I think you could maybe fit on or two chips more around the die over their 12, but that&#x27;s still a far cry from 128. That would probably just need a shared bus like normal DDR as you&#x27;re not fitting that much with 16 gbit density</div><br/><div id="42311252" class="c"><input type="checkbox" id="c-42311252" checked=""/><div class="controls bullet"><span class="by">SmellTheGlove</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310059">parent</a><span>|</span><a href="#42311013">next</a><span>|</span><label class="collapse" for="c-42311252">[-]</label><label class="expand" for="c-42311252">[3 more]</label></div><br/><div class="children"><div class="content">What if we did what others suggested was the practical limit - 48GB. Then just put 2-3 cards in the system and maybe had a little bridge over a separate bus for them to communicate?</div><br/><div id="42313969" class="c"><input type="checkbox" id="c-42313969" checked=""/><div class="controls bullet"><span class="by">Numerlor</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311252">parent</a><span>|</span><a href="#42311433">next</a><span>|</span><label class="collapse" for="c-42313969">[-]</label><label class="expand" for="c-42313969">[1 more]</label></div><br/><div class="children"><div class="content">I believe that would need some software work from Intel where they&#x27;re lacking a bit now with their delayed start. Not sure how the frameworks themselves split up the inference work to avoid crossing GPUs as the bandwidth is horrible there.<p>If we&#x27;re being reasonable and say that you&#x27;re not using a modern HEDT CPU that costs a couple thousand, the best a consumer botherboard can get right now would be 2x 8x PCIe gen 5 at 32GB&#x2F;s and one chipset x8 PCIe gen 4 at 16GB&#x2F;s. I&#x27;m not sure if a motherboard like that actually exists but Intel&#x27;s chipset should allow it; AMD only does x4 to chipset so the third slot is limited by that</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42311013" class="c"><input type="checkbox" id="c-42311013" checked=""/><div class="controls bullet"><span class="by">daft_pink</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42309896">prev</a><span>|</span><a href="#42314510">next</a><span>|</span><label class="collapse" for="c-42311013">[-]</label><label class="expand" for="c-42311013">[1 more]</label></div><br/><div class="children"><div class="content">Totally agree.  Someone needs to exploit the lack of available gpu memory in graphics cards for model runners.  Even training tensors tends to run against memory issues with the current cards.</div><br/></div></div><div id="42314510" class="c"><input type="checkbox" id="c-42314510" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42311013">prev</a><span>|</span><a href="#42313728">next</a><span>|</span><label class="collapse" for="c-42314510">[-]</label><label class="expand" for="c-42314510">[1 more]</label></div><br/><div class="children"><div class="content">Qualcomm built a card designed to do inferencing with 128GB of RAM:<p><a href="https:&#x2F;&#x2F;www.qualcomm.com&#x2F;news&#x2F;onq&#x2F;2023&#x2F;11&#x2F;introducing-qualcomm-cloud-ai-100-ultra" rel="nofollow">https:&#x2F;&#x2F;www.qualcomm.com&#x2F;news&#x2F;onq&#x2F;2023&#x2F;11&#x2F;introducing-qualco...</a><p>I have no idea how much it costs. They do not sell it via PC parts channels.</div><br/></div></div><div id="42313728" class="c"><input type="checkbox" id="c-42313728" checked=""/><div class="controls bullet"><span class="by">p1necone</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42314510">prev</a><span>|</span><a href="#42312099">next</a><span>|</span><label class="collapse" for="c-42313728">[-]</label><label class="expand" for="c-42313728">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t nobody actually making anything close to a profit in the AI space? When your entire business model is propped up by VC money making your bill of materials that you use to make negative profit cheaper is probably not very high on your list of priorities.</div><br/></div></div><div id="42312099" class="c"><input type="checkbox" id="c-42312099" checked=""/><div class="controls bullet"><span class="by">segmondy</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42313728">prev</a><span>|</span><a href="#42311180">next</a><span>|</span><label class="collapse" for="c-42312099">[-]</label><label class="expand" for="c-42312099">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t need to do 128gb, 48gb+ would eat their lunch, Intel and AMD are sleeping.</div><br/></div></div><div id="42311180" class="c"><input type="checkbox" id="c-42311180" checked=""/><div class="controls bullet"><span class="by">zamalek</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42312099">prev</a><span>|</span><a href="#42312862">next</a><span>|</span><label class="collapse" for="c-42311180">[-]</label><label class="expand" for="c-42311180">[1 more]</label></div><br/><div class="children"><div class="content">I think a better idea would be an NPU with slower memory, or tie it to the system DDR. I don&#x27;t think consumer inference (possibly even training) applications would need the memory bandwidth offered by GDDR&#x2F;HBM. Inference on my 7950x is already stupid fast (all things considered).<p>The deeper problem is that the market for this is probably incredibly niche.</div><br/></div></div><div id="42312862" class="c"><input type="checkbox" id="c-42312862" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42311180">prev</a><span>|</span><a href="#42309753">next</a><span>|</span><label class="collapse" for="c-42312862">[-]</label><label class="expand" for="c-42312862">[2 more]</label></div><br/><div class="children"><div class="content">The size of the local inference market is too small.  Maybe a couple of thousand LLM enthusiasts?  It&#x27;s not enough to make a profit or even breakeven on the development costs for the hardware.</div><br/><div id="42313973" class="c"><input type="checkbox" id="c-42313973" checked=""/><div class="controls bullet"><span class="by">dogcomplex</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312862">parent</a><span>|</span><a href="#42309753">next</a><span>|</span><label class="collapse" for="c-42313973">[-]</label><label class="expand" for="c-42313973">[1 more]</label></div><br/><div class="children"><div class="content">For now.  This might very well change once the general public realizes they can be movie directors (or generative world gamers) just by downloading some model and plugging in an eGPU.  The potential inference market is huge</div><br/></div></div></div></div><div id="42309753" class="c"><input type="checkbox" id="c-42309753" checked=""/><div class="controls bullet"><span class="by">01HNNWZ0MV43FF</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42312862">prev</a><span>|</span><a href="#42309991">next</a><span>|</span><label class="collapse" for="c-42309753">[-]</label><label class="expand" for="c-42309753">[10 more]</label></div><br/><div class="children"><div class="content">Judging by the number of 16 GB laptops I see around, 128 GB of RAM would probably cost a bajillion dollars</div><br/><div id="42309822" class="c"><input type="checkbox" id="c-42309822" checked=""/><div class="controls bullet"><span class="by">gs17</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42309753">parent</a><span>|</span><a href="#42310146">next</a><span>|</span><label class="collapse" for="c-42309822">[-]</label><label class="expand" for="c-42309822">[2 more]</label></div><br/><div class="children"><div class="content">One of the great things about having a desktop is being able to get that much for under $300 instead of the price of a second laptop.</div><br/><div id="42315070" class="c"><input type="checkbox" id="c-42315070" checked=""/><div class="controls bullet"><span class="by">Cumpiler69</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42309822">parent</a><span>|</span><a href="#42310146">next</a><span>|</span><label class="collapse" for="c-42315070">[-]</label><label class="expand" for="c-42315070">[1 more]</label></div><br/><div class="children"><div class="content">Desktop PCs RAM don&#x27;t have the bandwidth and latency required for running AI.</div><br/></div></div></div></div><div id="42310146" class="c"><input type="checkbox" id="c-42310146" checked=""/><div class="controls bullet"><span class="by">qwytw</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42309753">parent</a><span>|</span><a href="#42309822">prev</a><span>|</span><a href="#42310005">next</a><span>|</span><label class="collapse" for="c-42310146">[-]</label><label class="expand" for="c-42310146">[1 more]</label></div><br/><div class="children"><div class="content">Not the laptop RAM. It costs pennies, Apple&#x27;s is just charging $200 for 12GB because they can. It&#x27;s way too slow though..<p>And Nvidia doesn&#x27;t want to cannibalize its high end chips but putting more memory into consumer ones.</div><br/></div></div></div></div><div id="42309991" class="c"><input type="checkbox" id="c-42309991" checked=""/><div class="controls bullet"><span class="by">beAbU</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42309753">prev</a><span>|</span><a href="#42310512">next</a><span>|</span><label class="collapse" for="c-42309991">[-]</label><label class="expand" for="c-42309991">[10 more]</label></div><br/><div class="children"><div class="content">They are probably held back by same reason thats preventing AMD and nVidia from doing it either.</div><br/><div id="42310115" class="c"><input type="checkbox" id="c-42310115" checked=""/><div class="controls bullet"><span class="by">bryanlarsen</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42309991">parent</a><span>|</span><a href="#42310078">next</a><span>|</span><label class="collapse" for="c-42310115">[-]</label><label class="expand" for="c-42310115">[3 more]</label></div><br/><div class="children"><div class="content">The reason is AMD and Nvidia don&#x27;t is that they don&#x27;t want to cannibalize their high end AI market.  Intel doesn&#x27;t have a high end AI market to protect.</div><br/><div id="42311183" class="c"><input type="checkbox" id="c-42311183" checked=""/><div class="controls bullet"><span class="by">fweimer</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310115">parent</a><span>|</span><a href="#42310078">next</a><span>|</span><label class="collapse" for="c-42311183">[-]</label><label class="expand" for="c-42311183">[2 more]</label></div><br/><div class="children"><div class="content">There are products like this one: <a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;products&#x2F;sku&#x2F;232592&#x2F;intel-xeon-cpu-max-9480-processor-112-5m-cache-1-90-ghz&#x2F;specifications.html" rel="nofollow">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;products&#x2F;sku&#x2F;232592&#x2F;...</a><p>As far as I understand it, it gives you 64 GiB of HBM per socket.</div><br/><div id="42313396" class="c"><input type="checkbox" id="c-42313396" checked=""/><div class="controls bullet"><span class="by">knowitnone</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311183">parent</a><span>|</span><a href="#42310078">next</a><span>|</span><label class="collapse" for="c-42313396">[-]</label><label class="expand" for="c-42313396">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s a CPU. We are talking about GPUs here for highly parallel matrix multiplication problems. 2 different beasts.</div><br/></div></div></div></div></div></div><div id="42310078" class="c"><input type="checkbox" id="c-42310078" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42309991">parent</a><span>|</span><a href="#42310115">prev</a><span>|</span><a href="#42310512">next</a><span>|</span><label class="collapse" for="c-42310078">[-]</label><label class="expand" for="c-42310078">[6 more]</label></div><br/><div class="children"><div class="content">NVidia and AMD make $$$ on datacenter GPUs so it makes sense they don&#x27;t want to discount their own high-end. Intel has nothing there so they can happily go for commodization of AI hardware like what Meta did when releasing LLaMA to the wild.</div><br/><div id="42311037" class="c"><input type="checkbox" id="c-42311037" checked=""/><div class="controls bullet"><span class="by">beAbU</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310078">parent</a><span>|</span><a href="#42310512">next</a><span>|</span><label class="collapse" for="c-42311037">[-]</label><label class="expand" for="c-42311037">[5 more]</label></div><br/><div class="children"><div class="content">Is nVidia or AmD offering 128gb cards in any configuration?</div><br/><div id="42311463" class="c"><input type="checkbox" id="c-42311463" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311037">parent</a><span>|</span><a href="#42312060">next</a><span>|</span><label class="collapse" for="c-42311463">[-]</label><label class="expand" for="c-42311463">[1 more]</label></div><br/><div class="children"><div class="content">They aren&#x27;t &quot;cards&quot; but MI300x has 192GB and MI325x has 256GB.</div><br/></div></div><div id="42312060" class="c"><input type="checkbox" id="c-42312060" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311037">parent</a><span>|</span><a href="#42311463">prev</a><span>|</span><a href="#42310512">next</a><span>|</span><label class="collapse" for="c-42312060">[-]</label><label class="expand" for="c-42312060">[3 more]</label></div><br/><div class="children"><div class="content">You can run an AMD APU with 128GB of shared RAM.</div><br/><div id="42312309" class="c"><input type="checkbox" id="c-42312309" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312060">parent</a><span>|</span><a href="#42313328">next</a><span>|</span><label class="collapse" for="c-42312309">[-]</label><label class="expand" for="c-42312309">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s too slow and not very compatible. Most BIOSes also don&#x27;t allow sharing that much memory with GPU (max like 16GB).</div><br/></div></div><div id="42313328" class="c"><input type="checkbox" id="c-42313328" checked=""/><div class="controls bullet"><span class="by">genewitch</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312060">parent</a><span>|</span><a href="#42312309">prev</a><span>|</span><a href="#42310512">next</a><span>|</span><label class="collapse" for="c-42313328">[-]</label><label class="expand" for="c-42313328">[1 more]</label></div><br/><div class="children"><div class="content">you can do that with nvidia too but it takes you from 6tok&#x2F;s to 6s&#x2F;token or worse (not even exaggerating)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42310512" class="c"><input type="checkbox" id="c-42310512" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42309991">prev</a><span>|</span><a href="#42309910">next</a><span>|</span><label class="collapse" for="c-42310512">[-]</label><label class="expand" for="c-42310512">[10 more]</label></div><br/><div class="children"><div class="content">AMD has a 192GB GPU. I don’t see them eating NVidia’s lunch with it.</div><br/><div id="42310957" class="c"><input type="checkbox" id="c-42310957" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310512">parent</a><span>|</span><a href="#42309910">next</a><span>|</span><label class="collapse" for="c-42310957">[-]</label><label class="expand" for="c-42310957">[9 more]</label></div><br/><div class="children"><div class="content">They are charging as much as Nvidia for it. Now imagine they offered such a card for $2k. Would that allow them to eat Nvidia&#x27;s lunch?</div><br/><div id="42311833" class="c"><input type="checkbox" id="c-42311833" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310957">parent</a><span>|</span><a href="#42311435">next</a><span>|</span><label class="collapse" for="c-42311833">[-]</label><label class="expand" for="c-42311833">[1 more]</label></div><br/><div class="children"><div class="content">Let’s say for the sake of argument that you could build such a card and sell it for less than $5k. Why would you do it? You know there’s huge demand in the tens of billions per quarter for high end cards. Why undercut so heavily that market? To overthrow NVidia? So you’ll end up with a profit margin way low and then your shareholders will eat you alive.</div><br/></div></div><div id="42311435" class="c"><input type="checkbox" id="c-42311435" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310957">parent</a><span>|</span><a href="#42311833">prev</a><span>|</span><a href="#42311289">next</a><span>|</span><label class="collapse" for="c-42311435">[-]</label><label class="expand" for="c-42311435">[1 more]</label></div><br/><div class="children"><div class="content">If you want to load up 405B @ FP_16 into a single H100 box, how do you do it? You get two boxes. 2x the price.<p>Models are getting larger, not smaller. This is why H200 has more memory, but the same exact compute. MI300x vs. MI325x... more memory, same compute.</div><br/></div></div><div id="42311289" class="c"><input type="checkbox" id="c-42311289" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42310957">parent</a><span>|</span><a href="#42311435">prev</a><span>|</span><a href="#42309910">next</a><span>|</span><label class="collapse" for="c-42311289">[-]</label><label class="expand" for="c-42311289">[6 more]</label></div><br/><div class="children"><div class="content">We would also need to imagine AMD fixing their software.</div><br/><div id="42311615" class="c"><input type="checkbox" id="c-42311615" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311289">parent</a><span>|</span><a href="#42309910">next</a><span>|</span><label class="collapse" for="c-42311615">[-]</label><label class="expand" for="c-42311615">[5 more]</label></div><br/><div class="children"><div class="content">I think plenty of enthusiastic open source devs would jump at it and fix their software if the software was reasonably open. The same effect as what happened when Meta released LLaMA.</div><br/><div id="42311915" class="c"><input type="checkbox" id="c-42311915" checked=""/><div class="controls bullet"><span class="by">jjmarr</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311615">parent</a><span>|</span><a href="#42309910">next</a><span>|</span><label class="collapse" for="c-42311915">[-]</label><label class="expand" for="c-42311915">[4 more]</label></div><br/><div class="children"><div class="content">It is open and they regularly merge PRs.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ROCm&#x2F;ROCm&#x2F;pulls?q=is%3Apr+is%3Aclosed">https:&#x2F;&#x2F;github.com&#x2F;ROCm&#x2F;ROCm&#x2F;pulls?q=is%3Apr+is%3Aclosed</a></div><br/><div id="42312328" class="c"><input type="checkbox" id="c-42312328" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42311915">parent</a><span>|</span><a href="#42309910">next</a><span>|</span><label class="collapse" for="c-42312328">[-]</label><label class="expand" for="c-42312328">[3 more]</label></div><br/><div class="children"><div class="content">AMD GPUs aren&#x27;t very attractive to ML folks because they don&#x27;t outshine Nvidia in any single aspect. Blasting lots of RAM onto a GPU would make it attractive immediately with lots of attention from devs occupied with more interesting things.</div><br/><div id="42313353" class="c"><input type="checkbox" id="c-42313353" checked=""/><div class="controls bullet"><span class="by">genewitch</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312328">parent</a><span>|</span><a href="#42313653">next</a><span>|</span><label class="collapse" for="c-42313353">[-]</label><label class="expand" for="c-42313353">[1 more]</label></div><br/><div class="children"><div class="content">does the 7900xt outperform the 3090ti? if so, there&#x27;s already a market because those are the same price. I don&#x27;t mean <i>in theory</i> are there any workloads that the 7900xt can do better? Even if they&#x27;re practically equal performance you get a warranty and support with your <i>new</i> 7900xt.<p>also i didn&#x27;t know there was a 192GB amd GPU.</div><br/></div></div><div id="42313653" class="c"><input type="checkbox" id="c-42313653" checked=""/><div class="controls bullet"><span class="by">jjmarr</span><span>|</span><a href="#42309670">root</a><span>|</span><a href="#42312328">parent</a><span>|</span><a href="#42313353">prev</a><span>|</span><a href="#42309910">next</a><span>|</span><label class="collapse" for="c-42313653">[-]</label><label class="expand" for="c-42313653">[1 more]</label></div><br/><div class="children"><div class="content">MI300X already leads in VRAM as it has 192 GB.<p>For local inference, 7900 XTX has 24 GB of VRAM for less than $1000.<p>At what threshold of VRAM would you start being interested in MI?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42309910" class="c"><input type="checkbox" id="c-42309910" checked=""/><div class="controls bullet"><span class="by">rapsey</span><span>|</span><a href="#42309670">parent</a><span>|</span><a href="#42310512">prev</a><span>|</span><a href="#42309145">next</a><span>|</span><label class="collapse" for="c-42309910">[-]</label><label class="expand" for="c-42309910">[1 more]</label></div><br/><div class="children"><div class="content">Who manufactures the type of RAM and can they buy enough capacity? I know nVidia bought up the high bandwidth memory supply for years to come.</div><br/></div></div></div></div><div id="42309145" class="c"><input type="checkbox" id="c-42309145" checked=""/><div class="controls bullet"><span class="by">Night_Thastus</span><span>|</span><a href="#42309670">prev</a><span>|</span><a href="#42310208">next</a><span>|</span><label class="collapse" for="c-42309145">[-]</label><label class="expand" for="c-42309145">[4 more]</label></div><br/><div class="children"><div class="content">We&#x27;ll have to wait for first-party benchmarks, but they seem decent so far. A 4060 equivalent $200-$250 isn&#x27;t bad at all. for  I&#x27;m curious if we&#x27;ll get a B750 or B770 and how they&#x27;ll perform.<p>At the very least, it&#x27;s nice to have some decent BUDGET cards now. The ~$200 segment has been totally dead for years. I have a feeling Intel is losing a fair chunk of $ on each card though, just to enter the market.</div><br/><div id="42309213" class="c"><input type="checkbox" id="c-42309213" checked=""/><div class="controls bullet"><span class="by">rbanffy</span><span>|</span><a href="#42309145">parent</a><span>|</span><a href="#42313423">next</a><span>|</span><label class="collapse" for="c-42309213">[-]</label><label class="expand" for="c-42309213">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to see their GPGPU software support under Linux.</div><br/><div id="42309986" class="c"><input type="checkbox" id="c-42309986" checked=""/><div class="controls bullet"><span class="by">zargon</span><span>|</span><a href="#42309145">root</a><span>|</span><a href="#42309213">parent</a><span>|</span><a href="#42313423">next</a><span>|</span><label class="collapse" for="c-42309986">[-]</label><label class="expand" for="c-42309986">[1 more]</label></div><br/><div class="children"><div class="content">The keywords you&#x27;re looking for are Intel basekit, oneapi, and ipex.<p><a href="https:&#x2F;&#x2F;christianjmills.com&#x2F;posts&#x2F;intel-pytorch-extension-tutorial&#x2F;native-ubuntu&#x2F;#install-drivers" rel="nofollow">https:&#x2F;&#x2F;christianjmills.com&#x2F;posts&#x2F;intel-pytorch-extension-tu...</a><p><a href="https:&#x2F;&#x2F;chsasank.com&#x2F;intel-arc-gpu-driver-oneapi-installation.html" rel="nofollow">https:&#x2F;&#x2F;chsasank.com&#x2F;intel-arc-gpu-driver-oneapi-installatio...</a></div><br/></div></div></div></div><div id="42313423" class="c"><input type="checkbox" id="c-42313423" checked=""/><div class="controls bullet"><span class="by">knowitnone</span><span>|</span><a href="#42309145">parent</a><span>|</span><a href="#42309213">prev</a><span>|</span><a href="#42310208">next</a><span>|</span><label class="collapse" for="c-42313423">[-]</label><label class="expand" for="c-42313423">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know the numbers but the manufacture of the chip and the cards can&#x27;t be that expensive...the design was probably much more expensive. Hopefully they are at least breaking even but hopefully making money. Nobody goes into business to lose money. Shareholders would be pissed!</div><br/></div></div></div></div><div id="42310208" class="c"><input type="checkbox" id="c-42310208" checked=""/><div class="controls bullet"><span class="by">declan_roberts</span><span>|</span><a href="#42309145">prev</a><span>|</span><a href="#42310320">next</a><span>|</span><label class="collapse" for="c-42310208">[-]</label><label class="expand" for="c-42310208">[57 more]</label></div><br/><div class="children"><div class="content">I think a graphics card tailored for 2k gaming is actually great. 2k really is the goldilocks zone between 4k and 1080p graphics before you start creeping into diminishing returns.</div><br/><div id="42310295" class="c"><input type="checkbox" id="c-42310295" checked=""/><div class="controls bullet"><span class="by">giobox</span><span>|</span><a href="#42310208">parent</a><span>|</span><a href="#42315641">prev</a><span>|</span><a href="#42310287">next</a><span>|</span><label class="collapse" for="c-42310295">[-]</label><label class="expand" for="c-42310295">[35 more]</label></div><br/><div class="children"><div class="content">For sure its been a sweet spot for a very long time for budget conscious gamers looking for best balance of price and frame rates, but 1440p optimized parts are nothing new. Both NVidia and AMD make parts that target 1440p display users too, and have done for years. Even previous Intel parts you can argue were tailored for 1080p&#x2F;1440p use, given their comparative performance deficit at 4k etc.<p>Assuming they retail at prices Intel are suggesting in the press releases, you maybe here save 40-50 bucks over an ~equivalent NVidia 4060.<p>I would also argue like others here that with tech like frame gen, DLSS etc, even the cheapest discrete NVidia 40xx parts are arguably 1440p optimized now, it doesn&#x27;t even need to be said in their marketing materials. Im not as familiar with AMD&#x27;s range right now, but I suspect virtually every discrete graphics card they sell is &quot;2k optmized&quot; by the standard Intel used here, and also doesn&#x27;t really warrant explicit mention.</div><br/><div id="42310596" class="c"><input type="checkbox" id="c-42310596" checked=""/><div class="controls bullet"><span class="by">philistine</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310295">parent</a><span>|</span><a href="#42312237">next</a><span>|</span><label class="collapse" for="c-42310596">[-]</label><label class="expand" for="c-42310596">[33 more]</label></div><br/><div class="children"><div class="content">I&#x27;m baffled that PC gamers have decided that 1440p is the endgame for graphics.  When I look at a 27-inch 1440p display, I see pixel edges everywhere. It&#x27;s right at the edge of losing the visibility of individual pixels, since I can&#x27;t perceive them at 27-inch 2160p, but not quite there yet for desktop distances.<p>Time marches on, and I become ever more separated from gaming PC enthusiasts.</div><br/><div id="42310810" class="c"><input type="checkbox" id="c-42310810" checked=""/><div class="controls bullet"><span class="by">Novosell</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310596">parent</a><span>|</span><a href="#42315662">next</a><span>|</span><label class="collapse" for="c-42310810">[-]</label><label class="expand" for="c-42310810">[17 more]</label></div><br/><div class="children"><div class="content">Gaming at 2160p is just too expensive still, imo. You gotta pay more for your monitor, GPU and PSU. Then if you want side monitors that match in resolution, you&#x27;re paying more for those as well.<p>You say PC gamers at the start of your comment and gaming PC enthusiasts at the end. These groups are not the same and I&#x27;d say the latter is largely doing ultrawide, 4k monitor or even 4k TV.<p>According to steam, 56% are on 1080p, 20% on 1440p and 4% on 2160p.<p>So gamers as a whole are still settled on 1080p, actually. Not everyone is rich.</div><br/><div id="42312399" class="c"><input type="checkbox" id="c-42312399" checked=""/><div class="controls bullet"><span class="by">dmonitor</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310810">parent</a><span>|</span><a href="#42310987">next</a><span>|</span><label class="collapse" for="c-42312399">[-]</label><label class="expand" for="c-42312399">[3 more]</label></div><br/><div class="children"><div class="content">The major drawback for PC gaming at 4k that I never see mentioned is how much <i>heat</i> the panels generate. Many of them generate so much heat that rely on active cooling! I bought a pair of high refresh 4k displays and combined with the PC, they raised my room to an uncomfortable temperature. I returned them for other reasons (hard to justify not returning them when I got laid off a week after purchasing them), but I&#x27;ve since made note of the wattage when scouting monitors.</div><br/><div id="42313904" class="c"><input type="checkbox" id="c-42313904" checked=""/><div class="controls bullet"><span class="by">robotnikman</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42312399">parent</a><span>|</span><a href="#42310987">next</a><span>|</span><label class="collapse" for="c-42313904">[-]</label><label class="expand" for="c-42313904">[2 more]</label></div><br/><div class="children"><div class="content">&gt;hard to justify not returning them when I got laid off a week after purchasing them<p>Ouch, had something similar happen to me before when I bought a VR headset and had to return it. Wishing you the best on your job search!</div><br/><div id="42314667" class="c"><input type="checkbox" id="c-42314667" checked=""/><div class="controls bullet"><span class="by">dmonitor</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42313904">parent</a><span>|</span><a href="#42310987">next</a><span>|</span><label class="collapse" for="c-42314667">[-]</label><label class="expand" for="c-42314667">[1 more]</label></div><br/><div class="children"><div class="content">That was earlier this year. I found a new job with a pay raise so it turned out alright. Still miss my old team though.. we&#x27;ve been scattered like straws in the wind.</div><br/></div></div></div></div></div></div><div id="42310987" class="c"><input type="checkbox" id="c-42310987" checked=""/><div class="controls bullet"><span class="by">semi-extrinsic</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310810">parent</a><span>|</span><a href="#42312399">prev</a><span>|</span><a href="#42313206">next</a><span>|</span><label class="collapse" for="c-42310987">[-]</label><label class="expand" for="c-42310987">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m still using a 50&quot; 1080p (plasma!) television in my living room. It&#x27;s close to 15 years old now. I&#x27;ve seen newer and bigger TVs many times at my friends house, but it&#x27;s just not <i>better enough</i> that I can be bothered to upgrade.</div><br/><div id="42312358" class="c"><input type="checkbox" id="c-42312358" checked=""/><div class="controls bullet"><span class="by">dmonitor</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310987">parent</a><span>|</span><a href="#42313809">next</a><span>|</span><label class="collapse" for="c-42312358">[-]</label><label class="expand" for="c-42312358">[1 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t plasma have deep blacks and color reproduction similar to OLED? They&#x27;re still very good displays, and being 15 years old means it probably pre-dates the SmartTV era.</div><br/></div></div><div id="42313809" class="c"><input type="checkbox" id="c-42313809" checked=""/><div class="controls bullet"><span class="by">pbronez</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310987">parent</a><span>|</span><a href="#42312358">prev</a><span>|</span><a href="#42313206">next</a><span>|</span><label class="collapse" for="c-42313809">[-]</label><label class="expand" for="c-42313809">[1 more]</label></div><br/><div class="children"><div class="content">Classic Plasma TVs are no joke. I’ve got a 720p Plasma TV that still gets the job done.<p>If you’re ok with the resolution, then the only downside is significant power consumption and lack of HDR support.</div><br/></div></div></div></div><div id="42313206" class="c"><input type="checkbox" id="c-42313206" checked=""/><div class="controls bullet"><span class="by">singhrac</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310810">parent</a><span>|</span><a href="#42310987">prev</a><span>|</span><a href="#42312498">next</a><span>|</span><label class="collapse" for="c-42313206">[-]</label><label class="expand" for="c-42313206">[4 more]</label></div><br/><div class="children"><div class="content">I recently recently upgraded my main monitor from 1440p x 144hz to 4K x 144hz (with lots of caveats) and I agree with your assessment. If I had not made significant compromises, it would have cost at least $500 to get a decent monitor, which most people are not willing to spend.<p>Even with this monitor, I&#x27;m barely able to run it with my (expensive, though older) graphics card, and the screen alarmingly flashes whenever I change any settings. It&#x27;s stable, but this is not a simple plug-and-play configuration (mine requires two DP cables and fiddling with the menu + NVIDIA control panel).</div><br/><div id="42313564" class="c"><input type="checkbox" id="c-42313564" checked=""/><div class="controls bullet"><span class="by">hooli_gan</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42313206">parent</a><span>|</span><a href="#42312498">next</a><span>|</span><label class="collapse" for="c-42313564">[-]</label><label class="expand" for="c-42313564">[3 more]</label></div><br/><div class="children"><div class="content">Why do you need two DP cables? Is there not enough bandwidth in a single one? I use a 4k@60 display, which is the maximum my cheap Anker USB-C Hub can manage.</div><br/><div id="42313943" class="c"><input type="checkbox" id="c-42313943" checked=""/><div class="controls bullet"><span class="by">singhrac</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42313564">parent</a><span>|</span><a href="#42313917">next</a><span>|</span><label class="collapse" for="c-42313943">[-]</label><label class="expand" for="c-42313943">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure, but there&#x27;s an in-depth exploration of the monitor here: <a href="https:&#x2F;&#x2F;tftcentral.co.uk&#x2F;reviews&#x2F;acer_nitro_xv273k.htm" rel="nofollow">https:&#x2F;&#x2F;tftcentral.co.uk&#x2F;reviews&#x2F;acer_nitro_xv273k.htm</a><p>Reddit also seems to have some people who have managed to get 144 with FreeSync, but I&#x27;ve only managed 120.<p>Funnily enough while I was typing this Netflix caused both my monitors to blackscreen (some sort of NVIDIA reset I think) and then come back. It&#x27;s not totally stable!</div><br/></div></div><div id="42313917" class="c"><input type="checkbox" id="c-42313917" checked=""/><div class="controls bullet"><span class="by">ThatPlayer</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42313564">parent</a><span>|</span><a href="#42313943">prev</a><span>|</span><a href="#42312498">next</a><span>|</span><label class="collapse" for="c-42313917">[-]</label><label class="expand" for="c-42313917">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re on an Nvidia 4000 series DisplayPort is limited to 1.4, ~26 Gigabit&#x2F;s.<p><a href="https:&#x2F;&#x2F;linustechtips.com&#x2F;topic&#x2F;729232-guide-to-display-cables-adapters-v2&#x2F;?section=calc&amp;H=3840&amp;V=2160&amp;F=144&amp;bpc=10" rel="nofollow">https:&#x2F;&#x2F;linustechtips.com&#x2F;topic&#x2F;729232-guide-to-display-cabl...</a> is a calculator for bandwidth 4K@144 HDR is ~40 Gigabit&#x2F;s. You can do better with compression, but I find Nvidia cards have an issue with compression enabled.</div><br/></div></div></div></div></div></div><div id="42312498" class="c"><input type="checkbox" id="c-42312498" checked=""/><div class="controls bullet"><span class="by">evantbyrne</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310810">parent</a><span>|</span><a href="#42313206">prev</a><span>|</span><a href="#42312654">next</a><span>|</span><label class="collapse" for="c-42312498">[-]</label><label class="expand" for="c-42312498">[1 more]</label></div><br/><div class="children"><div class="content">Not rich. Well within reach for Americans with expendable income. Mid range 16&quot; macbook pros are in the same price ballpark as 4k gaming rigs. Or put another way costs less than a vacation for two to a popular destination.</div><br/></div></div><div id="42312654" class="c"><input type="checkbox" id="c-42312654" checked=""/><div class="controls bullet"><span class="by">archagon</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310810">parent</a><span>|</span><a href="#42312498">prev</a><span>|</span><a href="#42311566">next</a><span>|</span><label class="collapse" for="c-42312654">[-]</label><label class="expand" for="c-42312654">[3 more]</label></div><br/><div class="children"><div class="content">I don’t think that’s true anymore. I routinely find 4K&#x2F;27” monitors for under $100 on Craigslist, and a 3080-equivalent is still good enough to play most games on med-high settings at 4K and ~90Hz, especially if DLSS is available.</div><br/><div id="42312947" class="c"><input type="checkbox" id="c-42312947" checked=""/><div class="controls bullet"><span class="by">Novosell</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42312654">parent</a><span>|</span><a href="#42311566">next</a><span>|</span><label class="collapse" for="c-42312947">[-]</label><label class="expand" for="c-42312947">[2 more]</label></div><br/><div class="children"><div class="content">Your hypothetical person has a 3080 but needs to crawl craigslist for a sub-100$ monitor? U guess those people exist, but idk why you&#x27;d bother with a 3080 to then buy a low refreh rate, high input latency, probably TN, low color accuracy craigslist runoff.<p>Could just get a 3060 and a nice 1440p monitor.</div><br/><div id="42313617" class="c"><input type="checkbox" id="c-42313617" checked=""/><div class="controls bullet"><span class="by">archagon</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42312947">parent</a><span>|</span><a href="#42311566">next</a><span>|</span><label class="collapse" for="c-42313617">[-]</label><label class="expand" for="c-42313617">[1 more]</label></div><br/><div class="children"><div class="content">3080-equivalent performance can be had for a few hundred bucks these days, no?</div><br/></div></div></div></div></div></div><div id="42311566" class="c"><input type="checkbox" id="c-42311566" checked=""/><div class="controls bullet"><span class="by">philistine</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310810">parent</a><span>|</span><a href="#42312654">prev</a><span>|</span><a href="#42315662">next</a><span>|</span><label class="collapse" for="c-42311566">[-]</label><label class="expand" for="c-42311566">[2 more]</label></div><br/><div class="children"><div class="content">&gt; You say PC gamers at the start of your comment and gaming PC enthusiasts at the end. These groups are not the same<p>Prove to me those aren&#x27;t synonyms.</div><br/><div id="42311612" class="c"><input type="checkbox" id="c-42311612" checked=""/><div class="controls bullet"><span class="by">Novosell</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42311566">parent</a><span>|</span><a href="#42315662">next</a><span>|</span><label class="collapse" for="c-42311612">[-]</label><label class="expand" for="c-42311612">[1 more]</label></div><br/><div class="children"><div class="content">Prove to me they are.</div><br/></div></div></div></div></div></div><div id="42315662" class="c"><input type="checkbox" id="c-42315662" checked=""/><div class="controls bullet"><span class="by">theshrike79</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310596">parent</a><span>|</span><a href="#42310810">prev</a><span>|</span><a href="#42310752">next</a><span>|</span><label class="collapse" for="c-42315662">[-]</label><label class="expand" for="c-42315662">[1 more]</label></div><br/><div class="children"><div class="content">4k gaming (2160p) is like watching 8k video on your TV.<p>It&#x27;s doable, the tech is there. But the cost is WAY too high compared to what you get from it in the end.</div><br/></div></div><div id="42310752" class="c"><input type="checkbox" id="c-42310752" checked=""/><div class="controls bullet"><span class="by">wing-_-nuts</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310596">parent</a><span>|</span><a href="#42315662">prev</a><span>|</span><a href="#42310761">next</a><span>|</span><label class="collapse" for="c-42310752">[-]</label><label class="expand" for="c-42310752">[3 more]</label></div><br/><div class="children"><div class="content">I used to be in the &#x27;4k or bust&#x27; camp, but then I realized that I needed 1.5x scaling on a 27&quot; display to have my UI at a comfy size.  That put me right back at 1440p screen real estate <i>and</i> you had to deal with fractional scaling issues.<p>Instead, I bought a good 27&quot; 1440p monitor, and you know what?  I am not the discerning connoisseur of pixels that I thought I was.  Honestly, it&#x27;s <i>fine</i>.<p>I will hold out with this setup until I can get a 8k 144hz monitor and a gpu to drive it for a reasonable price.  I expect that will take another decade or so.</div><br/><div id="42311014" class="c"><input type="checkbox" id="c-42311014" checked=""/><div class="controls bullet"><span class="by">doubled112</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310752">parent</a><span>|</span><a href="#42314876">next</a><span>|</span><label class="collapse" for="c-42311014">[-]</label><label class="expand" for="c-42311014">[1 more]</label></div><br/><div class="children"><div class="content">I have a 4K 43&quot; TV on my desk and it is about perfect for me for desktop use without scaling.  For gaming, I tend to turn it down to 1080p because I like frames and don&#x27;t want to pay up.<p>At 4K, it&#x27;s like having 4 21&quot; 1080p monitors.  Haven&#x27;t maximized or minimized a window in years.  The sprawl is real.</div><br/></div></div><div id="42314876" class="c"><input type="checkbox" id="c-42314876" checked=""/><div class="controls bullet"><span class="by">LMYahooTFY</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310752">parent</a><span>|</span><a href="#42311014">prev</a><span>|</span><a href="#42310761">next</a><span>|</span><label class="collapse" for="c-42314876">[-]</label><label class="expand" for="c-42314876">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s true but I don&#x27;t run into this issue often since most games and Windows will offer UI&#x2F;Menu scaling without changing individual windows or the game itself.</div><br/></div></div></div></div><div id="42310761" class="c"><input type="checkbox" id="c-42310761" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310596">parent</a><span>|</span><a href="#42310752">prev</a><span>|</span><a href="#42315025">next</a><span>|</span><label class="collapse" for="c-42310761">[-]</label><label class="expand" for="c-42310761">[2 more]</label></div><br/><div class="children"><div class="content">This is a trade-off with frame rates and rendering quality. When having to choose, most gamers prefer higher frame rate and rendering quality. With 4K, that becomes very expensive, if not impossible. 4K is 2.25 times the pixels of 1440p, which for example means you can get double the frame rate with 1440p using the same processing power and bandwidth.<p>In other words, the current tech just isn’t quite there yet, or not cheap enough.</div><br/><div id="42312327" class="c"><input type="checkbox" id="c-42312327" checked=""/><div class="controls bullet"><span class="by">gdwatson</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310761">parent</a><span>|</span><a href="#42315025">next</a><span>|</span><label class="collapse" for="c-42312327">[-]</label><label class="expand" for="c-42312327">[1 more]</label></div><br/><div class="children"><div class="content">Arguably 1440p is the sweet spot for gaming, but I love 4k monitors for the extra text sharpness.  Fortunately DLSS and FSR upscaling are pretty good these days.  At 4k, quality-mode upscaling gives you a native render resolution about 1440p, with image quality a little better and performance a little worse.<p>It’s a great way to have my cake and eat it too.</div><br/></div></div></div></div><div id="42315025" class="c"><input type="checkbox" id="c-42315025" checked=""/><div class="controls bullet"><span class="by">Kiro</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310596">parent</a><span>|</span><a href="#42310761">prev</a><span>|</span><a href="#42313797">next</a><span>|</span><label class="collapse" for="c-42315025">[-]</label><label class="expand" for="c-42315025">[1 more]</label></div><br/><div class="children"><div class="content">You incorrectly presume that all gamers care about things such as  &quot;pixel edges&quot;. I think 1080p is fine. Game mechanics always trump fidelity.</div><br/></div></div><div id="42313797" class="c"><input type="checkbox" id="c-42313797" checked=""/><div class="controls bullet"><span class="by">p1necone</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310596">parent</a><span>|</span><a href="#42315025">prev</a><span>|</span><a href="#42310903">next</a><span>|</span><label class="collapse" for="c-42313797">[-]</label><label class="expand" for="c-42313797">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s less that gamers have decided it&#x27;s the &quot;endgame&quot; and more that current gen games at good framerates at 4k require significantly more money than 1440p does, and at least to my eyes just running at native 1440p on a 1440p monitor looks <i>much</i> better than running an internal resolution of 1440p upscaled to 4k, even with DLSS&#x2F;FSR - so just upgrading piecemeal isn&#x27;t really a desirable option.<p>Most people don&#x27;t have enough disposable income to make spending that extra amount a reasonable tradeoff (and continuing to spend on upgrades to keep up with their monitor on new games).</div><br/></div></div><div id="42310903" class="c"><input type="checkbox" id="c-42310903" checked=""/><div class="controls bullet"><span class="by">wlesieutre</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310596">parent</a><span>|</span><a href="#42313797">prev</a><span>|</span><a href="#42314895">next</a><span>|</span><label class="collapse" for="c-42310903">[-]</label><label class="expand" for="c-42310903">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think it’s seen as the end game, it’s that if you want 120 fps (or 144, 165, or 240) without turning down your graphics settings you’re talking $1000+ GPUs plus a huge case and a couple hundreds watts higher on your power supply.<p>1440p hits a popular balance where it’s more pixels than 1080p but not so absurdly expensive or power hungry.<p>Eventually 4K might be reasonably affordable, but we’ll settle at 1440p for a while in the meantime like we did at 1080p (which is still plenty popular too).</div><br/></div></div><div id="42314895" class="c"><input type="checkbox" id="c-42314895" checked=""/><div class="controls bullet"><span class="by">qludes</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310596">parent</a><span>|</span><a href="#42310903">prev</a><span>|</span><a href="#42312194">next</a><span>|</span><label class="collapse" for="c-42314895">[-]</label><label class="expand" for="c-42314895">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s more of a function of high end Nvidia gaming card prices and power consumption. PC gaming at large isn&#x27;t about chasing high end graphics anyway, steam deck falls under that umbrella and so does a vast amount of multiplayer gaming that might have other priorities such as affordability or low latency&#x2F;very high fps.</div><br/></div></div><div id="42312194" class="c"><input type="checkbox" id="c-42312194" checked=""/><div class="controls bullet"><span class="by">Lanolderen</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310596">parent</a><span>|</span><a href="#42314895">prev</a><span>|</span><a href="#42312674">next</a><span>|</span><label class="collapse" for="c-42312194">[-]</label><label class="expand" for="c-42312194">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a nice compromise for semi competitive play. On 4k it&#x27;d be very expensive and most likely finicky to maintain high FPS.<p>Tbh now that I think about it I only really <i>need</i> resolution for general usage. For gaming I&#x27;m running everything but textures on low with min or max FOV depending on the game so it&#x27;s not exactly aesthetic anyway. I more so need physical screen size so the heads are physically larger without shoving my face in it and refresh rate.</div><br/></div></div><div id="42312674" class="c"><input type="checkbox" id="c-42312674" checked=""/><div class="controls bullet"><span class="by">JohnBooty</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310596">parent</a><span>|</span><a href="#42312194">prev</a><span>|</span><a href="#42311018">next</a><span>|</span><label class="collapse" for="c-42312674">[-]</label><label class="expand" for="c-42312674">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see anybody thinking 1440p is &quot;endgame,&quot; as opposed to a pretty nice compromise at the moment.</div><br/></div></div><div id="42311018" class="c"><input type="checkbox" id="c-42311018" checked=""/><div class="controls bullet"><span class="by">dingnuts</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310596">parent</a><span>|</span><a href="#42312674">prev</a><span>|</span><a href="#42312237">next</a><span>|</span><label class="collapse" for="c-42311018">[-]</label><label class="expand" for="c-42311018">[3 more]</label></div><br/><div class="children"><div class="content">if you can see the pixels on a 27 inch 1440p display, you&#x27;re just sitting too close to the screen lol</div><br/><div id="42311553" class="c"><input type="checkbox" id="c-42311553" checked=""/><div class="controls bullet"><span class="by">philistine</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42311018">parent</a><span>|</span><a href="#42312237">next</a><span>|</span><label class="collapse" for="c-42311553">[-]</label><label class="expand" for="c-42311553">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t directly see the pixels per se like on 1080p at 27-inch at desktop distances. But I see harsh edges in corners and text is not flawless like on 2160p.<p>Like I said, it&#x27;s on the cusp of invisible pixels.</div><br/><div id="42313575" class="c"><input type="checkbox" id="c-42313575" checked=""/><div class="controls bullet"><span class="by">calf</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42311553">parent</a><span>|</span><a href="#42312237">next</a><span>|</span><label class="collapse" for="c-42313575">[-]</label><label class="expand" for="c-42313575">[1 more]</label></div><br/><div class="children"><div class="content">Gamers often use antialias settings to smooth out harsh edges, whereas an inconsistent frame rate will literally cost you a game victory in many fast-action games. Many esports professionals use low graphics settings for this reason.<p>I&#x27;ve not tried but I&#x27;ve heard that a butter-smooth 90, 120, or 300 FPS frame rate (that is also synchronized with the display) is really wonderful in many such games, and once you experience that you can&#x27;t go back. On less powerful systems it then requires making a tradeoff with rendering quality and resolution.</div><br/></div></div></div></div></div></div></div></div><div id="42312237" class="c"><input type="checkbox" id="c-42312237" checked=""/><div class="controls bullet"><span class="by">goosedragons</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310295">parent</a><span>|</span><a href="#42310596">prev</a><span>|</span><a href="#42310287">next</a><span>|</span><label class="collapse" for="c-42312237">[-]</label><label class="expand" for="c-42312237">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia markets the 4060 as a 1080p card. It&#x27;s design makes it worse at 1440p than past X060 cards too. Intel has XeSS to compete with DLSS and are reportedly coming out with their own frame gen competitor.  $40-50 is a decent savings in the budget market especially if Intel&#x27;s claims are to believed and it&#x27;s actually faster than the 4060.</div><br/></div></div></div></div><div id="42310287" class="c"><input type="checkbox" id="c-42310287" checked=""/><div class="controls bullet"><span class="by">icegreentea2</span><span>|</span><a href="#42310208">parent</a><span>|</span><a href="#42310295">prev</a><span>|</span><a href="#42310410">next</a><span>|</span><label class="collapse" for="c-42310287">[-]</label><label class="expand" for="c-42310287">[13 more]</label></div><br/><div class="children"><div class="content">2k usually refers to 1080p no? The k is the approximate horizontal resolution, so 1920x1080 is definitely 2k enough.</div><br/><div id="42310363" class="c"><input type="checkbox" id="c-42310363" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310287">parent</a><span>|</span><a href="#42310303">next</a><span>|</span><label class="collapse" for="c-42310363">[-]</label><label class="expand" for="c-42310363">[2 more]</label></div><br/><div class="children"><div class="content">Actual use is inconsistent. From <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;2K_resolution" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;2K_resolution</a>: “<i>In consumer products, 2560 × 1440 (1440p) is sometimes referred to as 2K, but it and similar formats are more traditionally categorized as 2.5K resolutions.</i>”<p>“2K” is used to denote WQHD often enough, whereas 1080p is usually called that, if not “FHD”.<p>“2K” being used to denote resolutions lower than WQHD is really only a thing for the 2048 cinema resolutions, not for FHD.</div><br/><div id="42311271" class="c"><input type="checkbox" id="c-42311271" checked=""/><div class="controls bullet"><span class="by">declan_roberts</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310363">parent</a><span>|</span><a href="#42310303">next</a><span>|</span><label class="collapse" for="c-42311271">[-]</label><label class="expand" for="c-42311271">[1 more]</label></div><br/><div class="children"><div class="content">TIL</div><br/></div></div></div></div><div id="42310303" class="c"><input type="checkbox" id="c-42310303" checked=""/><div class="controls bullet"><span class="by">antisthenes</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310287">parent</a><span>|</span><a href="#42310363">prev</a><span>|</span><a href="#42310410">next</a><span>|</span><label class="collapse" for="c-42310303">[-]</label><label class="expand" for="c-42310303">[10 more]</label></div><br/><div class="children"><div class="content">2k Usually refers to 2560x1440.<p>1920x1080 is 1080p.<p>It doesn&#x27;t make a whole lot of sense, but that&#x27;s how it is.</div><br/><div id="42310312" class="c"><input type="checkbox" id="c-42310312" checked=""/><div class="controls bullet"><span class="by">ortusdux</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310303">parent</a><span>|</span><a href="#42310353">next</a><span>|</span><label class="collapse" for="c-42310312">[-]</label><label class="expand" for="c-42310312">[3 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;2K_resolution" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;2K_resolution</a></div><br/><div id="42310343" class="c"><input type="checkbox" id="c-42310343" checked=""/><div class="controls bullet"><span class="by">nightski</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310312">parent</a><span>|</span><a href="#42310350">next</a><span>|</span><label class="collapse" for="c-42310343">[-]</label><label class="expand" for="c-42310343">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s amusing because I think almost everyone I know confuses it with 1440p.  I&#x27;ve never heard of 2k being used for 1080p before.</div><br/></div></div><div id="42310350" class="c"><input type="checkbox" id="c-42310350" checked=""/><div class="controls bullet"><span class="by">Retric</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310312">parent</a><span>|</span><a href="#42310343">prev</a><span>|</span><a href="#42310353">next</a><span>|</span><label class="collapse" for="c-42310350">[-]</label><label class="expand" for="c-42310350">[1 more]</label></div><br/><div class="children"><div class="content">“In consumer products, 2560 × 1440 (1440p) is sometimes referred to as 2K,[13] but it and similar formats are more traditionally categorized as 2.5K resolutions.”</div><br/></div></div></div></div><div id="42310353" class="c"><input type="checkbox" id="c-42310353" checked=""/><div class="controls bullet"><span class="by">seritools</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310303">parent</a><span>|</span><a href="#42310312">prev</a><span>|</span><a href="#42310410">next</a><span>|</span><label class="collapse" for="c-42310353">[-]</label><label class="expand" for="c-42310353">[6 more]</label></div><br/><div class="children"><div class="content">1440p is colloquially referred to as 2.5K, not 2K.</div><br/><div id="42310422" class="c"><input type="checkbox" id="c-42310422" checked=""/><div class="controls bullet"><span class="by">vundercind</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310353">parent</a><span>|</span><a href="#42315515">next</a><span>|</span><label class="collapse" for="c-42310422">[-]</label><label class="expand" for="c-42310422">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;d be pretty weird if it were called 2k. 1080p is in an absolute sense or as a relative &quot;distance&quot; to the next-lowest thousand <i>closer</i> to 2k pixels of width than 4k is to 4k (both are under, of course, but one&#x27;s under by 80 pixels, one by 160). It&#x27;s got a much better claim to the label 2k than 1440p does, and arguably a somewhat better claim to 2k than 4k has to 4k.<p>[EDIT] I mean, of course, 1080p&#x27;s also not typically called that, yet another resolution is, but labeling 1440p 2k is especially far off.</div><br/><div id="42311786" class="c"><input type="checkbox" id="c-42311786" checked=""/><div class="controls bullet"><span class="by">mkl</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310422">parent</a><span>|</span><a href="#42315515">next</a><span>|</span><label class="collapse" for="c-42311786">[-]</label><label class="expand" for="c-42311786">[2 more]</label></div><br/><div class="children"><div class="content">You are misunderstanding.  1080p, 1440p, 2160p refer to the number of <i>rows</i> of pixels, and those terms come from broadcast television and computing (the p is progressive, vs i for interlaced).  4k, 2k refer to the number of <i>columns</i> of pixels, and those terms come from cinema and visual effects (and originally means 4096 and 2048 pixels wide).  That means 1920×1080 is both 2k <i>and</i> 1080p, 2560×1440 is both 2.5k and 1440p, and 3840×2160 is both 4k and 2160p.</div><br/><div id="42312085" class="c"><input type="checkbox" id="c-42312085" checked=""/><div class="controls bullet"><span class="by">vundercind</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42311786">parent</a><span>|</span><a href="#42315515">next</a><span>|</span><label class="collapse" for="c-42312085">[-]</label><label class="expand" for="c-42312085">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You are misunderstanding. 1080p, 1440p, 2160p refer to the number of rows of pixels<p>&gt; (the p is progressive, vs i for interlaced)<p>&gt; 4k, 2k refer to the number of columns of pixels<p>&gt; 2560×1440 is both 2.5k and 1440p, and 3840×2160 is both 4k and 2160p.<p>These parts I did not misunderstand.<p>&gt; and those terms come from cinema and visual effects (and originally means 4096 and 2048 pixels wide)<p>OK that part I didn&#x27;t know, or at least had forgotten—which are effectively the same thing, either way.<p>&gt; 1920×1080 is both 2k and 1080p<p>Wikipedia suggests that in this particular case (unlike with 4k) application of &quot;2k&quot; to resolutions other than the original cinema resolution (2048x1080) is unusual; moreover, I was responding to a commenter&#x27;s usage of &quot;2k&quot; as synonymous with &quot;1440p&quot;, which seemed especially odd to me.</div><br/></div></div></div></div></div></div><div id="42315515" class="c"><input type="checkbox" id="c-42315515" checked=""/><div class="controls bullet"><span class="by">71bw</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310353">parent</a><span>|</span><a href="#42310422">prev</a><span>|</span><a href="#42310436">next</a><span>|</span><label class="collapse" for="c-42315515">[-]</label><label class="expand" for="c-42315515">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve never seen &quot;2.5K&quot; used colloquially and I see &quot;2K&quot; used everywhere all the time.</div><br/></div></div><div id="42310436" class="c"><input type="checkbox" id="c-42310436" checked=""/><div class="controls bullet"><span class="by">nemomarx</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310353">parent</a><span>|</span><a href="#42315515">prev</a><span>|</span><a href="#42310410">next</a><span>|</span><label class="collapse" for="c-42310436">[-]</label><label class="expand" for="c-42310436">[1 more]</label></div><br/><div class="children"><div class="content">I have never seen 2.5k used in the wild (gamer forums etc) so it can&#x27;t be that colloquial.</div><br/></div></div></div></div></div></div></div></div><div id="42310410" class="c"><input type="checkbox" id="c-42310410" checked=""/><div class="controls bullet"><span class="by">laweijfmvo</span><span>|</span><a href="#42310208">parent</a><span>|</span><a href="#42310287">prev</a><span>|</span><a href="#42310455">next</a><span>|</span><label class="collapse" for="c-42310410">[-]</label><label class="expand" for="c-42310410">[5 more]</label></div><br/><div class="children"><div class="content">Can it compete with the massive used GPU market though? Why buy a new Intel card when I can get a used Nvidia card that I know will work well?</div><br/><div id="42313442" class="c"><input type="checkbox" id="c-42313442" checked=""/><div class="controls bullet"><span class="by">knowitnone</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310410">parent</a><span>|</span><a href="#42312359">next</a><span>|</span><label class="collapse" for="c-42313442">[-]</label><label class="expand" for="c-42313442">[3 more]</label></div><br/><div class="children"><div class="content">warranty. Plus, do you want a 2nd hand GPU that was used for cryptomining 24&#x2F;7?</div><br/><div id="42315519" class="c"><input type="checkbox" id="c-42315519" checked=""/><div class="controls bullet"><span class="by">71bw</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42313442">parent</a><span>|</span><a href="#42314881">next</a><span>|</span><label class="collapse" for="c-42315519">[-]</label><label class="expand" for="c-42315519">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Plus, do you want a 2nd hand GPU that was used for cryptomining 24&#x2F;7?<p>...which will be most likely in better condition than anything that was used for, let&#x27;s say, gaming...</div><br/></div></div><div id="42314881" class="c"><input type="checkbox" id="c-42314881" checked=""/><div class="controls bullet"><span class="by">LMYahooTFY</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42313442">parent</a><span>|</span><a href="#42315519">prev</a><span>|</span><a href="#42312359">next</a><span>|</span><label class="collapse" for="c-42314881">[-]</label><label class="expand" for="c-42314881">[1 more]</label></div><br/><div class="children"><div class="content">Throw a new cooler on it and then yeah sure.</div><br/></div></div></div></div><div id="42312359" class="c"><input type="checkbox" id="c-42312359" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#42310208">root</a><span>|</span><a href="#42310410">parent</a><span>|</span><a href="#42313442">prev</a><span>|</span><a href="#42310455">next</a><span>|</span><label class="collapse" for="c-42312359">[-]</label><label class="expand" for="c-42312359">[1 more]</label></div><br/><div class="children"><div class="content">To some, buying used never crosses their mind.</div><br/></div></div></div></div><div id="42310455" class="c"><input type="checkbox" id="c-42310455" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#42310208">parent</a><span>|</span><a href="#42310410">prev</a><span>|</span><a href="#42310326">next</a><span>|</span><label class="collapse" for="c-42310455">[-]</label><label class="expand" for="c-42310455">[1 more]</label></div><br/><div class="children"><div class="content">Please say 1440p and not 2k. Ignoring arguments about what 2k <i>should</i> mean, there’s enough use either way that it’s confusing.</div><br/></div></div><div id="42310326" class="c"><input type="checkbox" id="c-42310326" checked=""/><div class="controls bullet"><span class="by">leetharris</span><span>|</span><a href="#42310208">parent</a><span>|</span><a href="#42310455">prev</a><span>|</span><a href="#42310320">next</a><span>|</span><label class="collapse" for="c-42310326">[-]</label><label class="expand" for="c-42310326">[1 more]</label></div><br/><div class="children"><div class="content">I see what you&#x27;re saying, but I also feel like ALL Nvidia cards are &quot;2K&quot; oriented cards because of DLSS, frame gen, etc. Resolution is less important now in general thanks to their upscaling tech.</div><br/></div></div></div></div><div id="42310320" class="c"><input type="checkbox" id="c-42310320" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#42310208">prev</a><span>|</span><a href="#42309219">next</a><span>|</span><label class="collapse" for="c-42310320">[-]</label><label class="expand" for="c-42310320">[78 more]</label></div><br/><div class="children"><div class="content">12GB max is a non-starter for ML work now. Why not come out with a reasonably priced 24gb card even if it isn&#x27;t the fastest and target it at the ML dev world? Am I missing something here?</div><br/><div id="42312696" class="c"><input type="checkbox" id="c-42312696" checked=""/><div class="controls bullet"><span class="by">fngjdflmdflg</span><span>|</span><a href="#42310320">parent</a><span>|</span><a href="#42310474">next</a><span>|</span><label class="collapse" for="c-42312696">[-]</label><label class="expand" for="c-42312696">[1 more]</label></div><br/><div class="children"><div class="content">I think a lot of replies to this post are missing that Intel&#x27;s last graphics card wasn&#x27;t received well by gamers due to poor drivers. The GT 730 from 2014 has more users than all Arc cards combined according to the latest Steam survey.[0] It&#x27;s entirely possible that making a 24gb local inference card would do better since they can contribute patches for inference libraries directly like they did for llama.cpp, as opposed to a gaming card where the support surface is much larger. I wish Intel well in any case and hope their drivers (or driver emulators) improve enough to be considered broadly usable.<p>[0] <a href="https:&#x2F;&#x2F;store.steampowered.com&#x2F;hwsurvey&#x2F;videocard&#x2F;" rel="nofollow">https:&#x2F;&#x2F;store.steampowered.com&#x2F;hwsurvey&#x2F;videocard&#x2F;</a> - 0.19% share</div><br/></div></div><div id="42310474" class="c"><input type="checkbox" id="c-42310474" checked=""/><div class="controls bullet"><span class="by">enragedcacti</span><span>|</span><a href="#42310320">parent</a><span>|</span><a href="#42312696">prev</a><span>|</span><a href="#42310534">next</a><span>|</span><label class="collapse" for="c-42310474">[-]</label><label class="expand" for="c-42310474">[11 more]</label></div><br/><div class="children"><div class="content">&gt; Am I missing something here?<p>Video games</div><br/><div id="42310605" class="c"><input type="checkbox" id="c-42310605" checked=""/><div class="controls bullet"><span class="by">rs_rs_rs_rs_rs</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310474">parent</a><span>|</span><a href="#42310534">next</a><span>|</span><label class="collapse" for="c-42310605">[-]</label><label class="expand" for="c-42310605">[10 more]</label></div><br/><div class="children"><div class="content">It&#x27;s insane how out of touch people can be here, lol</div><br/><div id="42311813" class="c"><input type="checkbox" id="c-42311813" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310605">parent</a><span>|</span><a href="#42311222">next</a><span>|</span><label class="collapse" for="c-42311813">[-]</label><label class="expand" for="c-42311813">[2 more]</label></div><br/><div class="children"><div class="content">How big is NVIDIA now? You don&#x27;t think breaking into that market is a good strategy? And, yes, I understand that this is targeted at gamers and not ML. That was the point of the comment I made. Maybe if they did target ML they would make money and open a path to the massive server market out there.</div><br/><div id="42315433" class="c"><input type="checkbox" id="c-42315433" checked=""/><div class="controls bullet"><span class="by">bjoli</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42311813">parent</a><span>|</span><a href="#42311222">next</a><span>|</span><label class="collapse" for="c-42315433">[-]</label><label class="expand" for="c-42315433">[1 more]</label></div><br/><div class="children"><div class="content">A video card that beats the 4060 for under $250 is very much going to be a problem for AMD and is going to eat the &quot;low end&quot; market if it is reasonably stable.</div><br/></div></div></div></div><div id="42311222" class="c"><input type="checkbox" id="c-42311222" checked=""/><div class="controls bullet"><span class="by">heraldgeezer</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310605">parent</a><span>|</span><a href="#42311813">prev</a><span>|</span><a href="#42310534">next</a><span>|</span><label class="collapse" for="c-42311222">[-]</label><label class="expand" for="c-42311222">[7 more]</label></div><br/><div class="children"><div class="content">I have been trying to hold my slurs in reading this thread.<p>These ML AI Macbook people are legit insane.<p>Desktops and gaming is ugly and complex to them (because lego is hard and macbook look nice unga bunga), yet it is a mass market Intel wants to move in on.<p>People here complain because Intel is not making a cheap GPU to &quot;make AI&quot; on when that&#x27;s a market of maybe 1000 people.<p>This Intel card is perfect for an esports gaming machine running CS2, Valorant, Rocket Leauge and casual or older games like The Sims, GoG games etc. Market of 1 million + right there, CS2 alone is 1mil people playing everyday. Not people grinding leetcode on their macs. Every real developer has a desktop, epyc cpu, giga ram and a nice GPU for downtime and run a real OS like Linux or even Windows (yes majority of devs run Windows)</div><br/><div id="42311729" class="c"><input type="checkbox" id="c-42311729" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42311222">parent</a><span>|</span><a href="#42311834">next</a><span>|</span><label class="collapse" for="c-42311729">[-]</label><label class="expand" for="c-42311729">[4 more]</label></div><br/><div class="children"><div class="content">Intel GPUs don&#x27;t sell well to gamers. They&#x27;ve been on the market for years now.<p>&gt;market of maybe 1000 people<p>The market of people interested in local ai inference is in the millions. If it&#x27;s cheap enough the data center market is at least 10 million.</div><br/><div id="42313418" class="c"><input type="checkbox" id="c-42313418" checked=""/><div class="controls bullet"><span class="by">ac29</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42311729">parent</a><span>|</span><a href="#42312499">next</a><span>|</span><label class="collapse" for="c-42313418">[-]</label><label class="expand" for="c-42313418">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Intel GPUs don&#x27;t sell well to gamers. They&#x27;ve been on the market for years now.<p>Intel has only had discrete GPUs on the market for 2 years. I guess that is a plural number of years, but only barely.</div><br/></div></div><div id="42312499" class="c"><input type="checkbox" id="c-42312499" checked=""/><div class="controls bullet"><span class="by">heraldgeezer</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42311729">parent</a><span>|</span><a href="#42313418">prev</a><span>|</span><a href="#42311834">next</a><span>|</span><label class="collapse" for="c-42312499">[-]</label><label class="expand" for="c-42312499">[2 more]</label></div><br/><div class="children"><div class="content">Yes, Intel cards have sucked. But they are trying again!</div><br/><div id="42314181" class="c"><input type="checkbox" id="c-42314181" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42312499">parent</a><span>|</span><a href="#42311834">next</a><span>|</span><label class="collapse" for="c-42314181">[-]</label><label class="expand" for="c-42314181">[1 more]</label></div><br/><div class="children"><div class="content">Yes, everyone knows the best way of making a product not suck is to give up and never try again.<p>&#x2F;s</div><br/></div></div></div></div></div></div><div id="42311834" class="c"><input type="checkbox" id="c-42311834" checked=""/><div class="controls bullet"><span class="by">terhechte</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42311222">parent</a><span>|</span><a href="#42311729">prev</a><span>|</span><a href="#42310534">next</a><span>|</span><label class="collapse" for="c-42311834">[-]</label><label class="expand" for="c-42311834">[2 more]</label></div><br/><div class="children"><div class="content">Most devs use windows (<a href="https:&#x2F;&#x2F;www.statista.com&#x2F;statistics&#x2F;869211&#x2F;worldwide-software-development-operating-system&#x2F;?utm_source=chatgpt.com" rel="nofollow">https:&#x2F;&#x2F;www.statista.com&#x2F;statistics&#x2F;869211&#x2F;worldwide-softwar...</a>). Reddit llocallama alone has 250k users. Clearly the market is bigger than 1000 people.
Why are gamers and Linux people always so aggressive diminutive of other people’s interests?</div><br/><div id="42312497" class="c"><input type="checkbox" id="c-42312497" checked=""/><div class="controls bullet"><span class="by">heraldgeezer</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42311834">parent</a><span>|</span><a href="#42310534">next</a><span>|</span><label class="collapse" for="c-42312497">[-]</label><label class="expand" for="c-42312497">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Why are gamers and Linux people always so aggressive diminutive of other people’s interests?<p>Both groups have a high autism %<p>We love to be &quot;technically correct&quot; and we often are. So we get frustrated when people claim things that are wrong.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42310534" class="c"><input type="checkbox" id="c-42310534" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42310320">parent</a><span>|</span><a href="#42310474">prev</a><span>|</span><a href="#42310565">next</a><span>|</span><label class="collapse" for="c-42310534">[-]</label><label class="expand" for="c-42310534">[27 more]</label></div><br/><div class="children"><div class="content">ML is about hit another winter. Maybe intel is ahead of industry.<p>Or we can keep asking high computers questions about programming.</div><br/><div id="42310583" class="c"><input type="checkbox" id="c-42310583" checked=""/><div class="controls bullet"><span class="by">PittleyDunkin</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310534">parent</a><span>|</span><a href="#42310643">next</a><span>|</span><label class="collapse" for="c-42310583">[-]</label><label class="expand" for="c-42310583">[17 more]</label></div><br/><div class="children"><div class="content">&gt; ML is about hit another winter.<p>I agree ML is about to hit (or has likely already hit) some serious constraints compared to breathless predictions of two years ago. I don&#x27;t think there&#x27;s anything equivalent to the AI winter on the horizon, though—LLMs even operated by people who have no clue how the underlying mechanism functions are still far more empowered than anything like the primitives of the 80s enabled.</div><br/><div id="42310729" class="c"><input type="checkbox" id="c-42310729" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310583">parent</a><span>|</span><a href="#42312337">next</a><span>|</span><label class="collapse" for="c-42310729">[-]</label><label class="expand" for="c-42310729">[12 more]</label></div><br/><div class="children"><div class="content">I think there&#x27;ll be a &quot;financial&quot; winter - or another way a bubble burst - the investment right now is simply unsustainable, how are these products going to be monetized?<p>Nvidia had a revenue of $27billion in 2023 - that&#x27;s about $160 per person per year [0] for <i>every working age person</i> in the USA. And it&#x27;s predicted to more than double in 2024. If you reduce that to office workers (you know, the people who might <i>actually</i> get some benefit, as no AI is going to milk a cow or serve you starbucks) that&#x27;s more like $1450&#x2F;year. Or again more than double that for 2024.<p>How much value add is the current set of AI products going to give us? It&#x27;s still mostly promise too.<p>Sure, like most bubbles there&#x27;ll probably still be some winners, but there&#x27;s no way the current market as a whole is sustainable.<p>The only way the &quot;maximal AI&quot; dream income is actually going to happen is if they functionally replace a significant proportion of the working population completely. And that probably would have large enough impacts to society that things like &quot;Dollars In A Bank&quot; or similar may not be so important.<p>[0] Using the stat of &quot;169.8 million people worked at some point in 2022&quot; <a href="https:&#x2F;&#x2F;www.bls.gov&#x2F;news.release&#x2F;pdf&#x2F;work.pdf" rel="nofollow">https:&#x2F;&#x2F;www.bls.gov&#x2F;news.release&#x2F;pdf&#x2F;work.pdf</a><p>[1] 18.5 million office workers according to <a href="https:&#x2F;&#x2F;www.bls.gov&#x2F;news.release&#x2F;ocwage.nr0.htm" rel="nofollow">https:&#x2F;&#x2F;www.bls.gov&#x2F;news.release&#x2F;ocwage.nr0.htm</a></div><br/><div id="42310936" class="c"><input type="checkbox" id="c-42310936" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310729">parent</a><span>|</span><a href="#42310853">next</a><span>|</span><label class="collapse" for="c-42310936">[-]</label><label class="expand" for="c-42310936">[5 more]</label></div><br/><div class="children"><div class="content">While I&#x27;d agree monetisation seems to be a challenge in the long term (analogy: spreadsheets are used everywhere, but are so easy to make they&#x27;re not themselves a revenue stream, only as part of a bigger package)…<p>&gt; Nvidia had a revenue of $27billion in 2023 - that&#x27;s about $160 per person per year [0] for every working age person in the USA<p>As a non-American, I&#x27;d like to point out we also earn money.<p>&gt; as no AI is going to milk a cow or serve you starbucks<p>Cows have been getting the robots for a while now, here&#x27;s a recent article: <a href="https:&#x2F;&#x2F;modernfarmer.com&#x2F;2023&#x2F;05&#x2F;for-years-farmers-milked-cows-by-hand-now-robots-and-technology-do-the-work&#x2F;" rel="nofollow">https:&#x2F;&#x2F;modernfarmer.com&#x2F;2023&#x2F;05&#x2F;for-years-farmers-milked-co...</a><p>Robots serve coffee as well as the office parts of the coffee business: <a href="https:&#x2F;&#x2F;www.techopedia.com&#x2F;ai-coffee-makers-robot-baristas-are-the-future-of-coffee-business" rel="nofollow">https:&#x2F;&#x2F;www.techopedia.com&#x2F;ai-coffee-makers-robot-baristas-a...</a><p>Some of the malls around here have food courts where robots bring out the meals. I assume they&#x27;re no more sophisticated than robot vacuum cleaners, but they get the job done.<p>Transformer models seem to be generally pretty good at high-level robot control, though IIRC a different architecture is needed down at the level of actuators and stepper motors.</div><br/><div id="42311820" class="c"><input type="checkbox" id="c-42311820" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310936">parent</a><span>|</span><a href="#42312388">next</a><span>|</span><label class="collapse" for="c-42311820">[-]</label><label class="expand" for="c-42311820">[2 more]</label></div><br/><div class="children"><div class="content">Sure, robotics help many jobs, and some level of the current deep learning boom seems to have crossover in improving that - but how many of them are running LLMs that affect Nvidia&#x27;s bottom line right now? There&#x27;s some interesting research in that area, but it&#x27;s certainly not the primary driving force. And then is the control system the limiting factor for many systems - it&#x27;s probably relatively easy to get a machine today that makes a Starbucks coffee &quot;as good as&quot; a decently trained human. But the market doesn&#x27;t seem to want that.<p>And I know restricting it to the US is a simplification, but so is restricting it to Nvidia, it&#x27;s just to give a ballpark back-of-the-envelope &quot;does this even make sense?&quot; level calculation. And that&#x27;s what I&#x27;m failing to see.</div><br/><div id="42312366" class="c"><input type="checkbox" id="c-42312366" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42311820">parent</a><span>|</span><a href="#42312388">next</a><span>|</span><label class="collapse" for="c-42312366">[-]</label><label class="expand" for="c-42312366">[1 more]</label></div><br/><div class="children"><div class="content">Machines that will make espresso, automatically, that I personally like better than what Starbucks serves are widely available.  No AI needed, and they aren&#x27;t even &quot;robotic&quot;.   They can use ordinary coffee beans, and you can get them for home use or for commercial use.   You can also go to a mall and get a robot to make you coffee.<p>Nonetheless, Starbucks does not use these machines, and I don&#x27;t see any reason that AI, on its current trajectory, will change that calculation any time soon.</div><br/></div></div></div></div><div id="42312388" class="c"><input type="checkbox" id="c-42312388" checked=""/><div class="controls bullet"><span class="by">lm28469</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310936">parent</a><span>|</span><a href="#42311820">prev</a><span>|</span><a href="#42310853">next</a><span>|</span><label class="collapse" for="c-42312388">[-]</label><label class="expand" for="c-42312388">[2 more]</label></div><br/><div class="children"><div class="content">I love how the fact that we might not want AI&#x2F;robots everywhere in our lives isn&#x27;t even discussed.<p>They could serve us a plate of shit and we&#x27;d debate if pepper or salt is better to complement it</div><br/><div id="42312495" class="c"><input type="checkbox" id="c-42312495" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42312388">parent</a><span>|</span><a href="#42310853">next</a><span>|</span><label class="collapse" for="c-42312495">[-]</label><label class="expand" for="c-42312495">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty often discussed, it&#x27;s just hard to put everything into a single comment (or thread).<p>I mean, Yudkowsky has basically spent the last decade screaming into the void about how AI will with high probability literally kill everyone, and even people like me who think that danger is much less likely still look at the industrial revolution and how slow we were to react to the harms of climate change and think &quot;speed-running another one of these may be unwise, we should probably be careful&quot;.</div><br/></div></div></div></div></div></div><div id="42310853" class="c"><input type="checkbox" id="c-42310853" checked=""/><div class="controls bullet"><span class="by">BenjiWiebe</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310729">parent</a><span>|</span><a href="#42310936">prev</a><span>|</span><a href="#42310875">next</a><span>|</span><label class="collapse" for="c-42310853">[-]</label><label class="expand" for="c-42310853">[3 more]</label></div><br/><div class="children"><div class="content">Well, &quot;AI&quot; is milking cows. Not LLM&#x27;s though. Our milking robot uses image recognition to find the cow&#x27;s teats to put the milking cup on.</div><br/><div id="42310924" class="c"><input type="checkbox" id="c-42310924" checked=""/><div class="controls bullet"><span class="by">semi-extrinsic</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310853">parent</a><span>|</span><a href="#42310875">next</a><span>|</span><label class="collapse" for="c-42310924">[-]</label><label class="expand" for="c-42310924">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, but automated milking robots like that have been in the market for more than a decade now IIRC?<p>Seems like a lot of CV solutions have seen fairly steady but small incremental advances over the past 10-15 years, quite unrelated to the current AI hype.</div><br/><div id="42311865" class="c"><input type="checkbox" id="c-42311865" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310924">parent</a><span>|</span><a href="#42310875">next</a><span>|</span><label class="collapse" for="c-42311865">[-]</label><label class="expand" for="c-42311865">[1 more]</label></div><br/><div class="children"><div class="content">Improving capabilities of AI isn&#x27;t at odds with expecting an &quot;AI Winter&quot; - just the current drive is more hype than sustainable, provable progress.<p>We&#x27;ve been through multiple AI Winters, as a new technique is developed, it <i>does</i> increase the capabilities. Just not as much as the hype suggested.<p>To say there won&#x27;t be a bust implies this boom will last forever, into whatever singularity that implies.</div><br/></div></div></div></div></div></div><div id="42310875" class="c"><input type="checkbox" id="c-42310875" checked=""/><div class="controls bullet"><span class="by">choilive</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310729">parent</a><span>|</span><a href="#42310853">prev</a><span>|</span><a href="#42312337">next</a><span>|</span><label class="collapse" for="c-42310875">[-]</label><label class="expand" for="c-42310875">[3 more]</label></div><br/><div class="children"><div class="content">I think the more accurate denominator would be the world population. People are seeing benefits to LLMs even outside of the office.</div><br/><div id="42311454" class="c"><input type="checkbox" id="c-42311454" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310875">parent</a><span>|</span><a href="#42311457">next</a><span>|</span><label class="collapse" for="c-42311454">[-]</label><label class="expand" for="c-42311454">[1 more]</label></div><br/><div class="children"><div class="content">How do LLMs make money though?</div><br/></div></div><div id="42311457" class="c"><input type="checkbox" id="c-42311457" checked=""/><div class="controls bullet"><span class="by">hulitu</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310875">parent</a><span>|</span><a href="#42311454">prev</a><span>|</span><a href="#42312337">next</a><span>|</span><label class="collapse" for="c-42311457">[-]</label><label class="expand" for="c-42311457">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I think the more accurate denominator would be the world population. People are seeing benefits to LLMs even outside of the office.<p>For example ?<p>(besides deep fakes)</div><br/></div></div></div></div></div></div><div id="42312337" class="c"><input type="checkbox" id="c-42312337" checked=""/><div class="controls bullet"><span class="by">lm28469</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310583">parent</a><span>|</span><a href="#42310729">prev</a><span>|</span><a href="#42310782">next</a><span>|</span><label class="collapse" for="c-42312337">[-]</label><label class="expand" for="c-42312337">[1 more]</label></div><br/><div class="children"><div class="content">&gt; even operated by people who have no clue how the underlying mechanism functions are still far more empowered than anything like the primitives of the 80s enabled.<p>I&#x27;m still not convinced about that. All the &quot;&quot;&quot;studies&quot;&quot;&quot; show 30-60% boost in productivity but clearly this doesn&#x27;t translate to anything meaningful in real life because no industry laid off 30-60% of their workforce and no industry progressed anywhere close to 30% since chat gpt was released.<p>It&#x27;s been released a whole 24 months ago, remember the talks about freeing us from work and curing cancer... Even investments funds which are the biggest suckers for anything profitable are more and more doubtful</div><br/></div></div><div id="42310782" class="c"><input type="checkbox" id="c-42310782" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310583">parent</a><span>|</span><a href="#42312337">prev</a><span>|</span><a href="#42310645">next</a><span>|</span><label class="collapse" for="c-42310782">[-]</label><label class="expand" for="c-42310782">[2 more]</label></div><br/><div class="children"><div class="content">What we had in the 80s was barely able to perform spell-check, free downloadable LLMs today are mind-blowing even in comparison to GPT-2.</div><br/><div id="42311473" class="c"><input type="checkbox" id="c-42311473" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310782">parent</a><span>|</span><a href="#42310645">next</a><span>|</span><label class="collapse" for="c-42311473">[-]</label><label class="expand" for="c-42311473">[1 more]</label></div><br/><div class="children"><div class="content">I think the only good thing that came out of the 80s was the 90s. I’d leave that decade alone so we can forget about it.</div><br/></div></div></div></div><div id="42310645" class="c"><input type="checkbox" id="c-42310645" checked=""/><div class="controls bullet"><span class="by">klodolph</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310583">parent</a><span>|</span><a href="#42310782">prev</a><span>|</span><a href="#42310643">next</a><span>|</span><label class="collapse" for="c-42310645">[-]</label><label class="expand" for="c-42310645">[1 more]</label></div><br/><div class="children"><div class="content">Yeah… I want to think of it like mining, where you’ve found an ore vein. You have to switch from prospecting to mining. There’s a lot of work to be done by integrating our LLMs and other tools with other systems, and I think the cost&#x2F;benefit of making models bigger, Bigger, BIGGER is reaching a plateau.</div><br/></div></div></div></div><div id="42310643" class="c"><input type="checkbox" id="c-42310643" checked=""/><div class="controls bullet"><span class="by">seanmcdirmid</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310534">parent</a><span>|</span><a href="#42310583">prev</a><span>|</span><a href="#42311665">next</a><span>|</span><label class="collapse" for="c-42310643">[-]</label><label class="expand" for="c-42310643">[7 more]</label></div><br/><div class="children"><div class="content">Haven&#x27;t people been saying that for the last decade? I mean, eventually they will be right, maybe &quot;about&quot; means next year, or maybe a decade later? They just have to stop making huge improvements for a few years and the investment will dry up.<p>I really wasn&#x27;t interested in computer hardware anymore (they are fast enough!) until I discovered the world of running LLMs and other AI locally. Now I actually care about computer hardware again. It is weird, I wouldn&#x27;t have even opened this HN thread a year ago.</div><br/><div id="42310660" class="c"><input type="checkbox" id="c-42310660" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310643">parent</a><span>|</span><a href="#42311665">next</a><span>|</span><label class="collapse" for="c-42310660">[-]</label><label class="expand" for="c-42310660">[6 more]</label></div><br/><div class="children"><div class="content">What makes local AI interesting to you vs larger remote models like ChatGPT and Claude?</div><br/><div id="42310713" class="c"><input type="checkbox" id="c-42310713" checked=""/><div class="controls bullet"><span class="by">adriancr</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310660">parent</a><span>|</span><a href="#42311238">next</a><span>|</span><label class="collapse" for="c-42310713">[-]</label><label class="expand" for="c-42310713">[3 more]</label></div><br/><div class="children"><div class="content">Not OP but for me a big thing is privacy, I can feed it personal documents and expect those to not leak.<p>It has zero cost, hardware is already there. I&#x27;m not captive to some remote company.<p>I can fiddle and integrate with other home sensors &#x2F; automation as I want.</div><br/><div id="42310823" class="c"><input type="checkbox" id="c-42310823" checked=""/><div class="controls bullet"><span class="by">hentrep</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310713">parent</a><span>|</span><a href="#42311238">next</a><span>|</span><label class="collapse" for="c-42310823">[-]</label><label class="expand" for="c-42310823">[2 more]</label></div><br/><div class="children"><div class="content">Curious as I’m of the same mind - what’s your local AI setup? I’m looking to implement a local system that would ideally accommodate voice chat. I know the answer depends on my use case - mostly searching and analysis of personal documents - but would love to hear how you’ve implemented.</div><br/><div id="42311317" class="c"><input type="checkbox" id="c-42311317" checked=""/><div class="controls bullet"><span class="by">dgfitz</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310823">parent</a><span>|</span><a href="#42311238">next</a><span>|</span><label class="collapse" for="c-42311317">[-]</label><label class="expand" for="c-42311317">[1 more]</label></div><br/><div class="children"><div class="content">llama.ccp and time seems to be the general answer to this question.</div><br/></div></div></div></div></div></div><div id="42311238" class="c"><input type="checkbox" id="c-42311238" checked=""/><div class="controls bullet"><span class="by">seanmcdirmid</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310660">parent</a><span>|</span><a href="#42310713">prev</a><span>|</span><a href="#42310716">next</a><span>|</span><label class="collapse" for="c-42311238">[-]</label><label class="expand" for="c-42311238">[1 more]</label></div><br/><div class="children"><div class="content">Control and freedom. You can use unharmonious models and hacks to existing models, also latency, you can actually use AI for a lot more applications when it is running locally.</div><br/></div></div><div id="42310716" class="c"><input type="checkbox" id="c-42310716" checked=""/><div class="controls bullet"><span class="by">epicureanideal</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310660">parent</a><span>|</span><a href="#42311238">prev</a><span>|</span><a href="#42311665">next</a><span>|</span><label class="collapse" for="c-42310716">[-]</label><label class="expand" for="c-42310716">[1 more]</label></div><br/><div class="children"><div class="content">Lack of ideological capture of the public models.</div><br/></div></div></div></div></div></div><div id="42311665" class="c"><input type="checkbox" id="c-42311665" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310534">parent</a><span>|</span><a href="#42310643">prev</a><span>|</span><a href="#42310800">next</a><span>|</span><label class="collapse" for="c-42311665">[-]</label><label class="expand" for="c-42311665">[1 more]</label></div><br/><div class="children"><div class="content">The survivors of the AI winter are not the dinosaurs but the small mammals that can profit by dramatically reducing the cost of AI inference in a minimum Capex environment.</div><br/></div></div><div id="42310800" class="c"><input type="checkbox" id="c-42310800" checked=""/><div class="controls bullet"><span class="by">HDThoreaun</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310534">parent</a><span>|</span><a href="#42311665">prev</a><span>|</span><a href="#42310565">next</a><span>|</span><label class="collapse" for="c-42310800">[-]</label><label class="expand" for="c-42310800">[1 more]</label></div><br/><div class="children"><div class="content">Selling cheap products that are worse than the competition is a valid strategy during downturns as businesses look to cut costs</div><br/></div></div></div></div><div id="42310565" class="c"><input type="checkbox" id="c-42310565" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42310320">parent</a><span>|</span><a href="#42310534">prev</a><span>|</span><a href="#42310483">next</a><span>|</span><label class="collapse" for="c-42310565">[-]</label><label class="expand" for="c-42310565">[15 more]</label></div><br/><div class="children"><div class="content">The ML dev world isn’t a consumer mass market like PC gaming is.</div><br/><div id="42310591" class="c"><input type="checkbox" id="c-42310591" checked=""/><div class="controls bullet"><span class="by">hajile</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310565">parent</a><span>|</span><a href="#42310483">next</a><span>|</span><label class="collapse" for="c-42310591">[-]</label><label class="expand" for="c-42310591">[14 more]</label></div><br/><div class="children"><div class="content">Launching a new SKU for $500-1000 with 48gb of RAM seems like a profitable idea. The GPU isn&#x27;t top-of-the-line, but the RAM would be unmatched for running a lot of models locally.</div><br/><div id="42310644" class="c"><input type="checkbox" id="c-42310644" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310591">parent</a><span>|</span><a href="#42310974">next</a><span>|</span><label class="collapse" for="c-42310644">[-]</label><label class="expand" for="c-42310644">[5 more]</label></div><br/><div class="children"><div class="content">You can’t just throw in more RAM without having the rest of the GPU architected for it. So there’s an R&amp;D cost involved for such a design, and there may even be trade-offs on performance for the mass-market lower-tier models. I’m doubtful that the LLM enthusiast&#x2F;tinkerer market is large enough for that to be obviously profitable.</div><br/><div id="42310841" class="c"><input type="checkbox" id="c-42310841" checked=""/><div class="controls bullet"><span class="by">hajile</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310644">parent</a><span>|</span><a href="#42310886">next</a><span>|</span><label class="collapse" for="c-42310841">[-]</label><label class="expand" for="c-42310841">[2 more]</label></div><br/><div class="children"><div class="content">That would depend on how they designed the memory controllers. GDDR6 only supporting 1-2gb modules at present (I believe GDDR6W supports 4gb modules). If they were using 12 1gb modules, then increasing to 24gb shouldn&#x27;t be a very large change.<p>Honestly, Apple seems to be on the right track here. DDR5 is slower than GDDR6, but you can scale the amount of RAM far higher simply by swapping out the density.</div><br/><div id="42310904" class="c"><input type="checkbox" id="c-42310904" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310841">parent</a><span>|</span><a href="#42310886">next</a><span>|</span><label class="collapse" for="c-42310904">[-]</label><label class="expand" for="c-42310904">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a 192 bit interface, so 6 16gbit chips.</div><br/></div></div></div></div><div id="42310886" class="c"><input type="checkbox" id="c-42310886" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310644">parent</a><span>|</span><a href="#42310841">prev</a><span>|</span><a href="#42310974">next</a><span>|</span><label class="collapse" for="c-42310886">[-]</label><label class="expand" for="c-42310886">[2 more]</label></div><br/><div class="children"><div class="content">Of course you can just add more RAM. Double the capacity of every chip and you get twice the RAM without ever asking an engineer.<p>People did it with the RTX3070. <a href="https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;3070-16gb-mod" rel="nofollow">https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;3070-16gb-mod</a></div><br/><div id="42311001" class="c"><input type="checkbox" id="c-42311001" checked=""/><div class="controls bullet"><span class="by">Tuna-Fish</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310886">parent</a><span>|</span><a href="#42310974">next</a><span>|</span><label class="collapse" for="c-42311001">[-]</label><label class="expand" for="c-42311001">[1 more]</label></div><br/><div class="children"><div class="content">Can you find me a 32Gbit GDDR6 chip?</div><br/></div></div></div></div></div></div><div id="42310974" class="c"><input type="checkbox" id="c-42310974" checked=""/><div class="controls bullet"><span class="by">Tuna-Fish</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310591">parent</a><span>|</span><a href="#42310644">prev</a><span>|</span><a href="#42310707">next</a><span>|</span><label class="collapse" for="c-42310974">[-]</label><label class="expand" for="c-42310974">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not technically possible to just slap on more RAM. GDDR6 is point-to-point with option for clamshell, and the largest chips in mass production are 16Gbit&#x2F;32 bit. So, for a 192bit card, the best you can get is 192&#x2F;32×16Gbit×2 = 24GB.<p>To have more memory, you have to design a new die with a wider interface. The design+test+masks on leading edge silicon is tens of millions of NRE, and has to be paid well over a year before product launch. No-one is going to do that for a low-priced product with an unknown market.<p>The savior of home inference is probably going to be AMD&#x27;s Strix Halo. It&#x27;s a laptop APU built to be a fairly low end gaming chip, but it has a 256-bit LPDDR5X interface. There are larger LPDDR5X packages available (thanks to the smartphone market), and Strix Halo should be eventually available with 128GB of unified ram, performance probably somewhere around a 4060.</div><br/><div id="42314050" class="c"><input type="checkbox" id="c-42314050" checked=""/><div class="controls bullet"><span class="by">rubatuga</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310974">parent</a><span>|</span><a href="#42312576">next</a><span>|</span><label class="collapse" for="c-42314050">[-]</label><label class="expand" for="c-42314050">[1 more]</label></div><br/><div class="children"><div class="content">Interesting, will have to try it out when it is released.</div><br/></div></div></div></div><div id="42310707" class="c"><input type="checkbox" id="c-42310707" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310591">parent</a><span>|</span><a href="#42310974">prev</a><span>|</span><a href="#42310483">next</a><span>|</span><label class="collapse" for="c-42310707">[-]</label><label class="expand" for="c-42310707">[5 more]</label></div><br/><div class="children"><div class="content">give me 48gb with reasonable power consumption so I can dev locally and I will buy it in a heartbeat. Anyone that is fine-tuning would want a setup like that to test things before pushing to real GPUs. And in reality if you can fine-tune on a card like that in two days instead of a few hours it would totally be worth it.</div><br/><div id="42314986" class="c"><input type="checkbox" id="c-42314986" checked=""/><div class="controls bullet"><span class="by">coolspot</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310707">parent</a><span>|</span><a href="#42310771">next</a><span>|</span><label class="collapse" for="c-42314986">[-]</label><label class="expand" for="c-42314986">[1 more]</label></div><br/><div class="children"><div class="content">&gt; give me 48gb with reasonable power consumption so I can dev locally and I will buy it in a heartbeat<p><a href="https:&#x2F;&#x2F;a.co&#x2F;d&#x2F;1LMNatf" rel="nofollow">https:&#x2F;&#x2F;a.co&#x2F;d&#x2F;1LMNatf</a></div><br/></div></div><div id="42310771" class="c"><input type="checkbox" id="c-42310771" checked=""/><div class="controls bullet"><span class="by">justsomehnguy</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310707">parent</a><span>|</span><a href="#42314986">prev</a><span>|</span><a href="#42310483">next</a><span>|</span><label class="collapse" for="c-42310771">[-]</label><label class="expand" for="c-42310771">[3 more]</label></div><br/><div class="children"><div class="content">I would love too, but you can&#x27;t just add the chips, you need the the bus too.</div><br/><div id="42311181" class="c"><input type="checkbox" id="c-42311181" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310771">parent</a><span>|</span><a href="#42310483">next</a><span>|</span><label class="collapse" for="c-42311181">[-]</label><label class="expand" for="c-42311181">[2 more]</label></div><br/><div class="children"><div class="content">The bigger point here is to ask why they aren&#x27;t designing that in from the start. Same with AMD. RAM has been stalled and is critical. Start focusing on allowing a lot more of it, even at the cost of performance, and you have a real product. I have a 12GB 3060 as my dev box and the big limiter for it is RAM, not cuda cores. If it had 48GB but the same number of cores then I would be very happy with it, especially if it was power efficient.</div><br/><div id="42313464" class="c"><input type="checkbox" id="c-42313464" checked=""/><div class="controls bullet"><span class="by">ac29</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42311181">parent</a><span>|</span><a href="#42310483">next</a><span>|</span><label class="collapse" for="c-42313464">[-]</label><label class="expand" for="c-42313464">[1 more]</label></div><br/><div class="children"><div class="content">Because designing a low end GPU with a very wide memory interface isn&#x27;t useful for gaming, and that is where the vast majority of non-datacenter discrete GPU sales are right now.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42310483" class="c"><input type="checkbox" id="c-42310483" checked=""/><div class="controls bullet"><span class="by">bryanlarsen</span><span>|</span><a href="#42310320">parent</a><span>|</span><a href="#42310565">prev</a><span>|</span><a href="#42310793">next</a><span>|</span><label class="collapse" for="c-42310483">[-]</label><label class="expand" for="c-42310483">[7 more]</label></div><br/><div class="children"><div class="content">These are $200 low end cards, the B5X0 cards.   Presumably they have B7X0 and perhaps even B9X0 cards in the pipeline as well.</div><br/><div id="42310712" class="c"><input type="checkbox" id="c-42310712" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310483">parent</a><span>|</span><a href="#42310555">next</a><span>|</span><label class="collapse" for="c-42310712">[-]</label><label class="expand" for="c-42310712">[1 more]</label></div><br/><div class="children"><div class="content">There has been no hint or evidence (beyond hope) Intel will add a 900 class this generation.<p>B770 was rumoured to match the 16 GB of the A770 (and to be the top end offering for Battlemage) but it is said to not have even been taped out yet with rumour it may end up having been cancelled completely.<p>I.e. don&#x27;t hold your breath for anything consumer from Intel this generation better for AI than tha A770 you could have bought 2 years ago. Even if something slightly better is coming at all there is no hint it will be soon.</div><br/></div></div><div id="42310942" class="c"><input type="checkbox" id="c-42310942" checked=""/><div class="controls bullet"><span class="by">hulitu</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310483">parent</a><span>|</span><a href="#42310555">prev</a><span>|</span><a href="#42310793">next</a><span>|</span><label class="collapse" for="c-42310942">[-]</label><label class="expand" for="c-42310942">[4 more]</label></div><br/><div class="children"><div class="content">&gt; These are $200 low end cards<p>Hm, i wouldn&#x27;t consider 200$ low end.</div><br/><div id="42313633" class="c"><input type="checkbox" id="c-42313633" checked=""/><div class="controls bullet"><span class="by">amaranth</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310942">parent</a><span>|</span><a href="#42312847">next</a><span>|</span><label class="collapse" for="c-42313633">[-]</label><label class="expand" for="c-42313633">[2 more]</label></div><br/><div class="children"><div class="content">There isn&#x27;t a cheaper card that&#x27;s worth buying over using the iGPU you already have so yeah, that&#x27;s the low end.</div><br/><div id="42314903" class="c"><input type="checkbox" id="c-42314903" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42313633">parent</a><span>|</span><a href="#42312847">next</a><span>|</span><label class="collapse" for="c-42314903">[-]</label><label class="expand" for="c-42314903">[1 more]</label></div><br/><div class="children"><div class="content">NVIDIA and AMD don&#x27;t even make new GPU silicon this low-end. Their smallest current-gen GPUs all debuted at higher price points, though the Radeon RX 7600 is now available at the same price that the B580 is launching at.</div><br/></div></div></div></div><div id="42312847" class="c"><input type="checkbox" id="c-42312847" checked=""/><div class="controls bullet"><span class="by">Narishma</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310942">parent</a><span>|</span><a href="#42313633">prev</a><span>|</span><a href="#42310793">next</a><span>|</span><label class="collapse" for="c-42312847">[-]</label><label class="expand" for="c-42312847">[1 more]</label></div><br/><div class="children"><div class="content">It is when the high end is 1000$ or more.</div><br/></div></div></div></div></div></div><div id="42310793" class="c"><input type="checkbox" id="c-42310793" checked=""/><div class="controls bullet"><span class="by">ggregoire</span><span>|</span><a href="#42310320">parent</a><span>|</span><a href="#42310483">prev</a><span>|</span><a href="#42310382">next</a><span>|</span><label class="collapse" for="c-42310793">[-]</label><label class="expand" for="c-42310793">[3 more]</label></div><br/><div class="children"><div class="content">&gt; 12GB max is a non-starter for ML work now.<p>Can you even do ML work with a GPU not compatible with CUDA? (genuine question)<p>A quick search showed me the equivalence to CUDA in the Intel world is oneAPI, but in practice, are the major Python libraries used for ML compatible with oneAPI? (Was also gonna ask if oneAPI can run inside Docker but apparently it does [1])<p>[1] <a href="https:&#x2F;&#x2F;hub.docker.com&#x2F;r&#x2F;intel&#x2F;oneapi" rel="nofollow">https:&#x2F;&#x2F;hub.docker.com&#x2F;r&#x2F;intel&#x2F;oneapi</a></div><br/><div id="42312062" class="c"><input type="checkbox" id="c-42312062" checked=""/><div class="controls bullet"><span class="by">suprjami</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310793">parent</a><span>|</span><a href="#42314490">next</a><span>|</span><label class="collapse" for="c-42312062">[-]</label><label class="expand" for="c-42312062">[1 more]</label></div><br/><div class="children"><div class="content">There is ROCm and Vulkan compute.<p>Vulkan is especially appealing because you don&#x27;t need any special GPGPU drivers and it runs on any card which supports Vulkan.</div><br/></div></div><div id="42314490" class="c"><input type="checkbox" id="c-42314490" checked=""/><div class="controls bullet"><span class="by">pineapple_sauce</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310793">parent</a><span>|</span><a href="#42312062">prev</a><span>|</span><a href="#42310382">next</a><span>|</span><label class="collapse" for="c-42314490">[-]</label><label class="expand" for="c-42314490">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;intel&#x2F;intel-extension-for-pytorch">https:&#x2F;&#x2F;github.com&#x2F;intel&#x2F;intel-extension-for-pytorch</a></div><br/></div></div></div></div><div id="42310382" class="c"><input type="checkbox" id="c-42310382" checked=""/><div class="controls bullet"><span class="by">shrewduser</span><span>|</span><a href="#42310320">parent</a><span>|</span><a href="#42310793">prev</a><span>|</span><a href="#42310332">next</a><span>|</span><label class="collapse" for="c-42310382">[-]</label><label class="expand" for="c-42310382">[2 more]</label></div><br/><div class="children"><div class="content">these are the entry level cards, i imagine the coming higher end variants will have the option of much more ram.</div><br/></div></div><div id="42310332" class="c"><input type="checkbox" id="c-42310332" checked=""/><div class="controls bullet"><span class="by">Implicated</span><span>|</span><a href="#42310320">parent</a><span>|</span><a href="#42310382">prev</a><span>|</span><a href="#42312393">next</a><span>|</span><label class="collapse" for="c-42310332">[-]</label><label class="expand" for="c-42310332">[1 more]</label></div><br/><div class="children"><div class="content">I was wondering the same thing. Seems crazy to keep pumping out 12gb cards in 2025.</div><br/></div></div><div id="42312393" class="c"><input type="checkbox" id="c-42312393" checked=""/><div class="controls bullet"><span class="by">whalesalad</span><span>|</span><a href="#42310320">parent</a><span>|</span><a href="#42310332">prev</a><span>|</span><a href="#42315251">next</a><span>|</span><label class="collapse" for="c-42312393">[-]</label><label class="expand" for="c-42312393">[5 more]</label></div><br/><div class="children"><div class="content">I still don&#x27;t understand why graphics cards haven&#x27;t evolved to include sodimm slots so that the vram can be upgraded by the end user. At this point memory requirements vary so much from gamer to scientist so it would make more sense to offer compute packages with user-supplied memory.<p>tl;dr GPU&#x27;s need to transition from being add-in cards to being a sibling motherboard. A sisterboard? Not a daughter board.</div><br/><div id="42314199" class="c"><input type="checkbox" id="c-42314199" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42312393">parent</a><span>|</span><a href="#42313044">next</a><span>|</span><label class="collapse" for="c-42314199">[-]</label><label class="expand" for="c-42314199">[3 more]</label></div><br/><div class="children"><div class="content">One of the reasons GPUs can have multiples of CPU bandwidth is they avoid the difficulties of pluggable dimms - direct soldered can have <i>much</i> higher frequencies at lower power.<p>It&#x27;s one of the reasons why ARM Macbooks get great performance&#x2F;watt, memory being even &quot;closer&quot; than mainboard soldered RAM so getting more of those benefits, though naturally less flexibility.</div><br/><div id="42314630" class="c"><input type="checkbox" id="c-42314630" checked=""/><div class="controls bullet"><span class="by">MindSpunk</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42314199">parent</a><span>|</span><a href="#42313044">next</a><span>|</span><label class="collapse" for="c-42314630">[-]</label><label class="expand" for="c-42314630">[2 more]</label></div><br/><div class="children"><div class="content">Even DDR5 has this problem. Go look at what soldered DDR5 can do frequency wise compared to DIMMs. It&#x27;s one of the problems the new CAMM form factor aims to help solve, making it tractable to push the memory frequency up beyond what DIMMs can get yout currently.</div><br/><div id="42315443" class="c"><input type="checkbox" id="c-42315443" checked=""/><div class="controls bullet"><span class="by">bjoli</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42314630">parent</a><span>|</span><a href="#42313044">next</a><span>|</span><label class="collapse" for="c-42315443">[-]</label><label class="expand" for="c-42315443">[1 more]</label></div><br/><div class="children"><div class="content">I have always wondered: would it be possible to put memory on the back side of the motherboard to get I closer to the CPU? And if it is, would it solve anything else than ram clearance for CPU coolers?</div><br/></div></div></div></div></div></div><div id="42313044" class="c"><input type="checkbox" id="c-42313044" checked=""/><div class="controls bullet"><span class="by">heraldgeezer</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42312393">parent</a><span>|</span><a href="#42314199">prev</a><span>|</span><a href="#42315251">next</a><span>|</span><label class="collapse" for="c-42313044">[-]</label><label class="expand" for="c-42313044">[1 more]</label></div><br/><div class="children"><div class="content">GDDR does not exist in sodimm form factor.<p>Intel and AMD internal GPUs can use normal computer RAM. But they are slower for that reason and many others.</div><br/></div></div></div></div><div id="42315251" class="c"><input type="checkbox" id="c-42315251" checked=""/><div class="controls bullet"><span class="by">mort96</span><span>|</span><a href="#42310320">parent</a><span>|</span><a href="#42312393">prev</a><span>|</span><a href="#42311209">next</a><span>|</span><label class="collapse" for="c-42315251">[-]</label><label class="expand" for="c-42315251">[1 more]</label></div><br/><div class="children"><div class="content">Who cares?</div><br/></div></div><div id="42311209" class="c"><input type="checkbox" id="c-42311209" checked=""/><div class="controls bullet"><span class="by">heraldgeezer</span><span>|</span><a href="#42310320">parent</a><span>|</span><a href="#42315251">prev</a><span>|</span><a href="#42310878">next</a><span>|</span><label class="collapse" for="c-42311209">[-]</label><label class="expand" for="c-42311209">[1 more]</label></div><br/><div class="children"><div class="content">This is not an ML card... this is a gaming card... Why are you people like this?</div><br/></div></div><div id="42310878" class="c"><input type="checkbox" id="c-42310878" checked=""/><div class="controls bullet"><span class="by">PhasmaFelis</span><span>|</span><a href="#42310320">parent</a><span>|</span><a href="#42311209">prev</a><span>|</span><a href="#42310385">next</a><span>|</span><label class="collapse" for="c-42310878">[-]</label><label class="expand" for="c-42310878">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Am I missing something here?<p>This is a graphics card.</div><br/><div id="42313413" class="c"><input type="checkbox" id="c-42313413" checked=""/><div class="controls bullet"><span class="by">davrosthedalek</span><span>|</span><a href="#42310320">root</a><span>|</span><a href="#42310878">parent</a><span>|</span><a href="#42310385">next</a><span>|</span><label class="collapse" for="c-42313413">[-]</label><label class="expand" for="c-42313413">[1 more]</label></div><br/><div class="children"><div class="content">Sir, this is a Wendy&#x27;s</div><br/></div></div></div></div><div id="42310385" class="c"><input type="checkbox" id="c-42310385" checked=""/><div class="controls bullet"><span class="by">tofuziggy</span><span>|</span><a href="#42310320">parent</a><span>|</span><a href="#42310878">prev</a><span>|</span><a href="#42309219">next</a><span>|</span><label class="collapse" for="c-42310385">[-]</label><label class="expand" for="c-42310385">[1 more]</label></div><br/><div class="children"><div class="content">Yes exactly!!</div><br/></div></div></div></div><div id="42309219" class="c"><input type="checkbox" id="c-42309219" checked=""/><div class="controls bullet"><span class="by">rmm</span><span>|</span><a href="#42310320">prev</a><span>|</span><a href="#42310970">next</a><span>|</span><label class="collapse" for="c-42309219">[-]</label><label class="expand" for="c-42309219">[12 more]</label></div><br/><div class="children"><div class="content">I put an a360 Card into an old machine I turned into a plex server. It turned it into a transcoding powerhouse. I can do multiple indepdent streams now without it skipping a beat. Price-performance ratio was off the chart</div><br/><div id="42315678" class="c"><input type="checkbox" id="c-42315678" checked=""/><div class="controls bullet"><span class="by">theshrike79</span><span>|</span><a href="#42309219">parent</a><span>|</span><a href="#42309897">next</a><span>|</span><label class="collapse" for="c-42315678">[-]</label><label class="expand" for="c-42315678">[1 more]</label></div><br/><div class="children"><div class="content">Good to know. I&#x27;m still waiting for UNRaid 7.0 for proper Arc support to pull the trigger on one.</div><br/></div></div><div id="42309897" class="c"><input type="checkbox" id="c-42309897" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#42309219">parent</a><span>|</span><a href="#42315678">prev</a><span>|</span><a href="#42309889">next</a><span>|</span><label class="collapse" for="c-42309897">[-]</label><label class="expand" for="c-42309897">[1 more]</label></div><br/><div class="children"><div class="content">Intel has been a beast at transcoding for years, it’s a relatively niche application though.</div><br/></div></div><div id="42309889" class="c"><input type="checkbox" id="c-42309889" checked=""/><div class="controls bullet"><span class="by">ThatMedicIsASpy</span><span>|</span><a href="#42309219">parent</a><span>|</span><a href="#42309897">prev</a><span>|</span><a href="#42309829">next</a><span>|</span><label class="collapse" for="c-42309889">[-]</label><label class="expand" for="c-42309889">[2 more]</label></div><br/><div class="children"><div class="content">My 7950X3Ds GPU does 4k HDR (33Mb&#x2F;s) to 1080p at 40fps (proxmox, jellyfin). If these GPUs would support SR-IOV I would grab one for transcoding and GPU accelerated remote desktop.<p>Untouched video (star wars 8) 4k HDR (60Mb&#x2F;s) to 1080p at 28fps</div><br/><div id="42311587" class="c"><input type="checkbox" id="c-42311587" checked=""/><div class="controls bullet"><span class="by">c2h5oh</span><span>|</span><a href="#42309219">root</a><span>|</span><a href="#42309889">parent</a><span>|</span><a href="#42309829">next</a><span>|</span><label class="collapse" for="c-42311587">[-]</label><label class="expand" for="c-42311587">[1 more]</label></div><br/><div class="children"><div class="content">All first gen arc gpus share the same video encoder&#x2F;decoder, including the sub-$100 A310,  that can handle four (I haven&#x27;t tested more than two) simultaneous 4k HDR -&gt; 1080p AV1 transcodes at high bitrate with tone mapping while using 12-15W of power.<p>No SR-IOV.</div><br/></div></div></div></div><div id="42309829" class="c"><input type="checkbox" id="c-42309829" checked=""/><div class="controls bullet"><span class="by">kridsdale1</span><span>|</span><a href="#42309219">parent</a><span>|</span><a href="#42309889">prev</a><span>|</span><a href="#42309932">next</a><span>|</span><label class="collapse" for="c-42309829">[-]</label><label class="expand" for="c-42309829">[3 more]</label></div><br/><div class="children"><div class="content">Any idea how that compares to Apple Silicon for that job? I bought the $599 MacBook Air with M1 as my plex server for this reason. Transcodes 4k HEVC and doesn’t even need a fan. Sips watts.</div><br/><div id="42310248" class="c"><input type="checkbox" id="c-42310248" checked=""/><div class="controls bullet"><span class="by">machinekob</span><span>|</span><a href="#42309219">root</a><span>|</span><a href="#42309829">parent</a><span>|</span><a href="#42309951">next</a><span>|</span><label class="collapse" for="c-42310248">[-]</label><label class="expand" for="c-42310248">[1 more]</label></div><br/><div class="children"><div class="content">Apple Silicon still don&#x27;t support AV1 encoding but it is good enough for simple Jellyfin server i&#x27;m using one myself</div><br/></div></div><div id="42309951" class="c"><input type="checkbox" id="c-42309951" checked=""/><div class="controls bullet"><span class="by">2OEH8eoCRo0</span><span>|</span><a href="#42309219">root</a><span>|</span><a href="#42309829">parent</a><span>|</span><a href="#42310248">prev</a><span>|</span><a href="#42309932">next</a><span>|</span><label class="collapse" for="c-42309951">[-]</label><label class="expand" for="c-42309951">[1 more]</label></div><br/><div class="children"><div class="content">All Intel arc even the $99 A310 has HW accel h265 and AV1 encoding.</div><br/></div></div></div></div><div id="42309932" class="c"><input type="checkbox" id="c-42309932" checked=""/><div class="controls bullet"><span class="by">2OEH8eoCRo0</span><span>|</span><a href="#42309219">parent</a><span>|</span><a href="#42309829">prev</a><span>|</span><a href="#42309418">next</a><span>|</span><label class="collapse" for="c-42309932">[-]</label><label class="expand" for="c-42309932">[2 more]</label></div><br/><div class="children"><div class="content">How&#x27;s the Linux compatibility? I was tempted to do the same for my CentOS Stream Plex box.</div><br/><div id="42315464" class="c"><input type="checkbox" id="c-42315464" checked=""/><div class="controls bullet"><span class="by">bjoli</span><span>|</span><a href="#42309219">root</a><span>|</span><a href="#42309932">parent</a><span>|</span><a href="#42309418">next</a><span>|</span><label class="collapse" for="c-42315464">[-]</label><label class="expand" for="c-42315464">[1 more]</label></div><br/><div class="children"><div class="content">Amazing. It is the first time I have plugged any gpu into my linux box and have it just work. I am never going back to anything else. My main computer uses an a750, and my jellyfin server uses an a310.<p>No issues with linux. The server did not like the a310, but that is because it is an old dell t430 and it is unsupported hardware. The only thing I had to do was to tweak the fan curve so that it stopped going full tilt.</div><br/></div></div></div></div><div id="42309418" class="c"><input type="checkbox" id="c-42309418" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#42309219">parent</a><span>|</span><a href="#42309932">prev</a><span>|</span><a href="#42310970">next</a><span>|</span><label class="collapse" for="c-42309418">[-]</label><label class="expand" for="c-42309418">[2 more]</label></div><br/><div class="children"><div class="content">Interesting application. Was this a machine lacking an iGPU, or does the Intel GPU-on-a-stick have more quicksync power than the iGPU?</div><br/><div id="42309836" class="c"><input type="checkbox" id="c-42309836" checked=""/><div class="controls bullet"><span class="by">6SixTy</span><span>|</span><a href="#42309219">root</a><span>|</span><a href="#42309418">parent</a><span>|</span><a href="#42310970">next</a><span>|</span><label class="collapse" for="c-42309836">[-]</label><label class="expand" for="c-42309836">[1 more]</label></div><br/><div class="children"><div class="content">A not inconsequential possibility is that both the iGPU and dGPU are sharing the transcoding workload, rather than the dGPU replacing the iGPU. It&#x27;s a fairly forgotten feature of Intel Arc, but I don&#x27;t blame anyone because the help articles are dusty to say the least.</div><br/></div></div></div></div></div></div><div id="42310970" class="c"><input type="checkbox" id="c-42310970" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#42309219">prev</a><span>|</span><a href="#42309217">next</a><span>|</span><label class="collapse" for="c-42310970">[-]</label><label class="expand" for="c-42310970">[18 more]</label></div><br/><div class="children"><div class="content">Who is the target audience for this?<p>Well informed gamers know Intel&#x27;s discrete GPU is hanging by a thread, so they&#x27;re not hoping on that bandwagon.<p>Too small for ML.<p>The only people really happy seem to be the ones buying it for transcoding and I can&#x27;t imagine there is a huge market of people going &quot;I need to go buy a card for AV1 encoding&quot;.</div><br/><div id="42315564" class="c"><input type="checkbox" id="c-42315564" checked=""/><div class="controls bullet"><span class="by">71bw</span><span>|</span><a href="#42310970">parent</a><span>|</span><a href="#42311567">next</a><span>|</span><label class="collapse" for="c-42315564">[-]</label><label class="expand" for="c-42315564">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Well informed gamers know Intel&#x27;s discrete GPU is hanging by a thread, so they&#x27;re not hoping on that bandwagon.<p>If Intel&#x27;s stats are anything to go by, League runs way better than it did on the last generation and it&#x27;s the only game that has had issues on the last-gen that&#x27;s still left running on DX9, CS:GO was another notable one but CS2 has launched since and the game has moved to DX12&#x2F;VK. This was, literally, the biggest issue they had - drivers were also wonky but they seem to have ironed that out as well.</div><br/></div></div><div id="42311567" class="c"><input type="checkbox" id="c-42311567" checked=""/><div class="controls bullet"><span class="by">ddtaylor</span><span>|</span><a href="#42310970">parent</a><span>|</span><a href="#42315564">prev</a><span>|</span><a href="#42311054">next</a><span>|</span><label class="collapse" for="c-42311567">[-]</label><label class="expand" for="c-42311567">[2 more]</label></div><br/><div class="children"><div class="content">Intel has earned a lot of credit in the Linux space.<p>Nvidia is trash tier in terms of support and only recently making serious steps to actually support the platform.<p>AMD went all in nearly a decade ago and it&#x27;s working pretty well for them. They are mostly caught up to being Intel grade support in the kernel.<p>Meanwhile, Intel has been doing this since I was in college. I was running the i915 driver in Ubuntu 20 years ago. Sure their chips are super low power stuff, but what you can do with them and the level of software support you get is unmatched. Years before these other vendors were taking the platform seriously Intel was supporting and funding Mesa development.</div><br/><div id="42314822" class="c"><input type="checkbox" id="c-42314822" checked=""/><div class="controls bullet"><span class="by">tristan957</span><span>|</span><a href="#42310970">root</a><span>|</span><a href="#42311567">parent</a><span>|</span><a href="#42311054">next</a><span>|</span><label class="collapse" for="c-42314822">[-]</label><label class="expand" for="c-42314822">[1 more]</label></div><br/><div class="children"><div class="content">The AMD driver has been great on my Framework 13, but the 6.10 series was completely busted. 6.11 worked fine. I can&#x27;t remember a series where any of my Intel laptops didn&#x27;t work for that long.</div><br/></div></div></div></div><div id="42311054" class="c"><input type="checkbox" id="c-42311054" checked=""/><div class="controls bullet"><span class="by">zamalek</span><span>|</span><a href="#42310970">parent</a><span>|</span><a href="#42311567">prev</a><span>|</span><a href="#42311089">next</a><span>|</span><label class="collapse" for="c-42311054">[-]</label><label class="expand" for="c-42311054">[2 more]</label></div><br/><div class="children"><div class="content">If it works well on Linux there&#x27;s a market for that. AMD are hinting that they will be focusing on iGPUs going forward (all power to them, their iGPUs are unmatched and NVIDIA is dominating dGPU). Intel might be the savior we need. Well, Intel and possibly NVK.<p>Had this been available a few weeks ago I would have gone through the pain of early adoption. Sadly it wasn&#x27;t just an upgrade build for me, so I didn&#x27;t have the luxury of waiting.</div><br/><div id="42311570" class="c"><input type="checkbox" id="c-42311570" checked=""/><div class="controls bullet"><span class="by">sosodev</span><span>|</span><a href="#42310970">root</a><span>|</span><a href="#42311054">parent</a><span>|</span><a href="#42311089">next</a><span>|</span><label class="collapse" for="c-42311570">[-]</label><label class="expand" for="c-42311570">[1 more]</label></div><br/><div class="children"><div class="content">AMD has some great iGPUs but it seems like they&#x27;re still planning to compete in the dGPU space just not at the high end of the market.</div><br/></div></div></div></div><div id="42311089" class="c"><input type="checkbox" id="c-42311089" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#42310970">parent</a><span>|</span><a href="#42311054">prev</a><span>|</span><a href="#42312114">next</a><span>|</span><label class="collapse" for="c-42311089">[-]</label><label class="expand" for="c-42311089">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Too small for ML.<p>What do you mean by this - I assume you mean too small for SoTA LLMs? There are many ML applications where 12GB is more than enough.<p>Even w.r.t. LLMs, not everyone requires the latest &amp; biggest LLM models. Some &quot;small&quot;, distilled and&#x2F;or quantized LLMs are perfectly usable with &lt;24GB</div><br/><div id="42313573" class="c"><input type="checkbox" id="c-42313573" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#42310970">root</a><span>|</span><a href="#42311089">parent</a><span>|</span><a href="#42312114">next</a><span>|</span><label class="collapse" for="c-42313573">[-]</label><label class="expand" for="c-42313573">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re aiming for usable, then sure that works. The gains in model ability from doubling size is quite noticable at that scale though.<p>Still...tangibly cheaper than even a 2nd hand 3090 so there is perhaps a market for it</div><br/></div></div></div></div><div id="42312114" class="c"><input type="checkbox" id="c-42312114" checked=""/><div class="controls bullet"><span class="by">marshray</span><span>|</span><a href="#42310970">parent</a><span>|</span><a href="#42311089">prev</a><span>|</span><a href="#42311009">next</a><span>|</span><label class="collapse" for="c-42312114">[-]</label><label class="expand" for="c-42312114">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using an Intel card right now. With Wayland. It just works.<p>Ubuntu 24.04 couldn&#x27;t even boot to a tty with the Nvidia Quadro thing that came with this major-brand PC workstation, still under warranty.</div><br/></div></div><div id="42311009" class="c"><input type="checkbox" id="c-42311009" checked=""/><div class="controls bullet"><span class="by">epolanski</span><span>|</span><a href="#42310970">parent</a><span>|</span><a href="#42312114">prev</a><span>|</span><a href="#42312189">next</a><span>|</span><label class="collapse" for="c-42311009">[-]</label><label class="expand" for="c-42311009">[1 more]</label></div><br/><div class="children"><div class="content">Cheap gaming rigs.<p>They do well compared to AMD&#x2F;Nvidia at that price point.<p>Is it a market worth chasing at all?<p>Doubt.</div><br/></div></div><div id="42312189" class="c"><input type="checkbox" id="c-42312189" checked=""/><div class="controls bullet"><span class="by">mappu</span><span>|</span><a href="#42310970">parent</a><span>|</span><a href="#42311009">prev</a><span>|</span><a href="#42311337">next</a><span>|</span><label class="collapse" for="c-42312189">[-]</label><label class="expand" for="c-42312189">[4 more]</label></div><br/><div class="children"><div class="content"><i>&gt; Intel&#x27;s discrete GPU is hanging by a thread, so they&#x27;re not hoping on that bandwagon</i><p>Why would that matter? You buy one GPU, in a few years you buy another GPU. It&#x27;s not a life decision.</div><br/><div id="42313598" class="c"><input type="checkbox" id="c-42313598" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#42310970">root</a><span>|</span><a href="#42312189">parent</a><span>|</span><a href="#42311337">next</a><span>|</span><label class="collapse" for="c-42313598">[-]</label><label class="expand" for="c-42313598">[3 more]</label></div><br/><div class="children"><div class="content">&gt;Why would that matter?<p>The game devs are going to spend all their time &amp; effort targetting amd&#x2F;nvidia. Custom code paths etc.<p>It&#x27;s not a one size fits all world. OpenCL etc abstraction are good at covering up differences, but not that good. So if you&#x27;re the player with &lt;10% market share you&#x27;re going to have an uphill battle to just be on par.</div><br/><div id="42314208" class="c"><input type="checkbox" id="c-42314208" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#42310970">root</a><span>|</span><a href="#42313598">parent</a><span>|</span><a href="#42311337">next</a><span>|</span><label class="collapse" for="c-42314208">[-]</label><label class="expand" for="c-42314208">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The game devs are going to spend all their time &amp; effort targetting amd&#x2F;nvidia. Custom code paths etc.<p>From my experience they target NVidia and Consoles. AMD <i>might</i> get a look at the code just before release if they notice any big problems.<p>I&#x27;d be surprised if many Gamedevs even pick up the phone for an Intel GPU developer.</div><br/><div id="42315371" class="c"><input type="checkbox" id="c-42315371" checked=""/><div class="controls bullet"><span class="by">catdog</span><span>|</span><a href="#42310970">root</a><span>|</span><a href="#42314208">parent</a><span>|</span><a href="#42311337">next</a><span>|</span><label class="collapse" for="c-42315371">[-]</label><label class="expand" for="c-42315371">[1 more]</label></div><br/><div class="children"><div class="content">Consoles use AMD GPUs for years now.</div><br/></div></div></div></div></div></div></div></div><div id="42311337" class="c"><input type="checkbox" id="c-42311337" checked=""/><div class="controls bullet"><span class="by">screye</span><span>|</span><a href="#42310970">parent</a><span>|</span><a href="#42312189">prev</a><span>|</span><a href="#42312922">next</a><span>|</span><label class="collapse" for="c-42311337">[-]</label><label class="expand" for="c-42311337">[1 more]</label></div><br/><div class="children"><div class="content">all-in-1 machines.<p>Intel&#x27;s customers are 3rd party Cpu assemblers like Dell &amp; HP. Many corporate bulk buyers only care if 1-2 of the apps they use are supported. The lack of wider support isn&#x27;t a concern.</div><br/></div></div><div id="42312922" class="c"><input type="checkbox" id="c-42312922" checked=""/><div class="controls bullet"><span class="by">Narishma</span><span>|</span><a href="#42310970">parent</a><span>|</span><a href="#42311337">prev</a><span>|</span><a href="#42312466">next</a><span>|</span><label class="collapse" for="c-42312922">[-]</label><label class="expand" for="c-42312922">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s for the low end gaming market which Nvidia and AMD have been neglecting for years.</div><br/></div></div><div id="42312466" class="c"><input type="checkbox" id="c-42312466" checked=""/><div class="controls bullet"><span class="by">qudat</span><span>|</span><a href="#42310970">parent</a><span>|</span><a href="#42312922">prev</a><span>|</span><a href="#42311022">next</a><span>|</span><label class="collapse" for="c-42312466">[-]</label><label class="expand" for="c-42312466">[1 more]</label></div><br/><div class="children"><div class="content">If you go on the intel arc subreddit people are hyped about intel GPUs. Not sure what the price is but the previous gen was cheap and the extra competition is welcomed<p>In particular, intel just needs to support vfio and it’ll be huge for homelabs.</div><br/></div></div><div id="42311022" class="c"><input type="checkbox" id="c-42311022" checked=""/><div class="controls bullet"><span class="by">spookie</span><span>|</span><a href="#42310970">parent</a><span>|</span><a href="#42312466">prev</a><span>|</span><a href="#42309217">next</a><span>|</span><label class="collapse" for="c-42311022">[-]</label><label class="expand" for="c-42311022">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s cheap, plenty of market when the others have forgotten the segment.</div><br/></div></div></div></div><div id="42309217" class="c"><input type="checkbox" id="c-42309217" checked=""/><div class="controls bullet"><span class="by">jmclnx</span><span>|</span><a href="#42310970">prev</a><span>|</span><a href="#42310310">next</a><span>|</span><label class="collapse" for="c-42309217">[-]</label><label class="expand" for="c-42309217">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Battlemage is still treated to fully open-source graphics driver support on Linux.<p>I am hoping these are open in such a manner that they can be used in OpenBSD.  Right now I avoid all hardware with a Nvidia GPU.  That makes for somewhat slim pickings.<p>If the firmware is acceptable to the OpenBSD folks, then I will happly use these.</div><br/><div id="42309247" class="c"><input type="checkbox" id="c-42309247" checked=""/><div class="controls bullet"><span class="by">rbanffy</span><span>|</span><a href="#42309217">parent</a><span>|</span><a href="#42310310">next</a><span>|</span><label class="collapse" for="c-42309247">[-]</label><label class="expand" for="c-42309247">[1 more]</label></div><br/><div class="children"><div class="content">They are promising good Linux support, which kind of implies, at least, that everything but opaque blobs are open.</div><br/></div></div></div></div><div id="42310310" class="c"><input type="checkbox" id="c-42310310" checked=""/><div class="controls bullet"><span class="by">Implicated</span><span>|</span><a href="#42309217">prev</a><span>|</span><a href="#42309122">next</a><span>|</span><label class="collapse" for="c-42310310">[-]</label><label class="expand" for="c-42310310">[36 more]</label></div><br/><div class="children"><div class="content">12GB memory<p>-.-<p>I feel like _anyone_ who can pump out GPU&#x27;s with 24GB+ of memory that are capable to use for py-stuff would benefit greatly.<p>Even if it&#x27;s not as performant as the NVIDIA options - just to be able to get the models to run, at whatever speed.<p>They would fly off the shelves.</div><br/><div id="42310446" class="c"><input type="checkbox" id="c-42310446" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#42310310">parent</a><span>|</span><a href="#42310341">next</a><span>|</span><label class="collapse" for="c-42310446">[-]</label><label class="expand" for="c-42310446">[16 more]</label></div><br/><div class="children"><div class="content">Would it though? How many people are running inference at home? Outside of enthusiasts I don&#x27;t know anyone. Even companies don&#x27;t self-host models and prefer to use APIs. Not that I wouldn&#x27;t like a consumer GPU with tons of VRAM, but I think that the market for it is quite small for companies to invest building it. If you bother to look at Steam&#x27;s hardware stats you&#x27;ll notice that only a small percentage is using high-end cards.</div><br/><div id="42310519" class="c"><input type="checkbox" id="c-42310519" checked=""/><div class="controls bullet"><span class="by">tokioyoyo</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310446">parent</a><span>|</span><a href="#42310500">next</a><span>|</span><label class="collapse" for="c-42310519">[-]</label><label class="expand" for="c-42310519">[12 more]</label></div><br/><div class="children"><div class="content">This is the weird part, I saw the same comments in other threads. People keep saying how everyone yearns for local LLMs… but other than hardcore enthusiasts it just sounds like a bad investment? Like it’s a smaller market than gaming GPUs. And by the time anyone runs them locally, you’ll have bigger&#x2F;better models and GPUs coming out, so you won’t even be able to make use of them. Maybe the whole “indoctrinate users to be a part of Intel ecosystem, so when they go work for big companies they would vouch for it” would have merit… if others weren’t innovating and making their products better (like NVIDIA).</div><br/><div id="42315709" class="c"><input type="checkbox" id="c-42315709" checked=""/><div class="controls bullet"><span class="by">theshrike79</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310519">parent</a><span>|</span><a href="#42311815">next</a><span>|</span><label class="collapse" for="c-42315709">[-]</label><label class="expand" for="c-42315709">[1 more]</label></div><br/><div class="children"><div class="content">[delayed]</div><br/></div></div><div id="42311815" class="c"><input type="checkbox" id="c-42311815" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310519">parent</a><span>|</span><a href="#42315709">prev</a><span>|</span><a href="#42312709">next</a><span>|</span><label class="collapse" for="c-42311815">[-]</label><label class="expand" for="c-42311815">[9 more]</label></div><br/><div class="children"><div class="content">Intel sold their GPUs at negative margin which is part of why the stock fell off a cliff. If they could double the vram they could raise the price into the green even selling thousands, likely closer to 100k, would be far better than what they&#x27;re doing now. The problem is Intel is run by incompetent people who guard their market segments as tribal fiefs instead of solving for the customer.</div><br/><div id="42312722" class="c"><input type="checkbox" id="c-42312722" checked=""/><div class="controls bullet"><span class="by">qwytw</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42311815">parent</a><span>|</span><a href="#42311961">next</a><span>|</span><label class="collapse" for="c-42312722">[-]</label><label class="expand" for="c-42312722">[4 more]</label></div><br/><div class="children"><div class="content">&gt; which is part of why the stock fell off a cliff<p>Was it? Their GPUs sales were insignificantly low so I doubt that had a huge effect on their net income.</div><br/><div id="42313283" class="c"><input type="checkbox" id="c-42313283" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42312722">parent</a><span>|</span><a href="#42311961">next</a><span>|</span><label class="collapse" for="c-42313283">[-]</label><label class="expand" for="c-42313283">[3 more]</label></div><br/><div class="children"><div class="content">They spent billions at TSMC making Alchemist dies that sat in a warehouse for a year or two as they tried to fix the drivers.</div><br/><div id="42313519" class="c"><input type="checkbox" id="c-42313519" checked=""/><div class="controls bullet"><span class="by">knowitnone</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42313283">parent</a><span>|</span><a href="#42311961">next</a><span>|</span><label class="collapse" for="c-42313519">[-]</label><label class="expand" for="c-42313519">[2 more]</label></div><br/><div class="children"><div class="content">that&#x27;s a dumb management &quot;cart before the horse&quot; problem. I understand a few bugs in the driver but they really should have gotten the driver working decently well before production. Would have even given them more time tweaking the GPU. This is exactly why Intel is failing and will continue to fail with that type of management</div><br/><div id="42313785" class="c"><input type="checkbox" id="c-42313785" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42313519">parent</a><span>|</span><a href="#42311961">next</a><span>|</span><label class="collapse" for="c-42313785">[-]</label><label class="expand" for="c-42313785">[1 more]</label></div><br/><div class="children"><div class="content">Intel management is just brain dead. They could have sold the cards for mining when there was a massive GPU shortage and called it the developer edition but no. It&#x27;s hard to develop a driver for games when you have no silicon.</div><br/></div></div></div></div></div></div></div></div><div id="42311961" class="c"><input type="checkbox" id="c-42311961" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42311815">parent</a><span>|</span><a href="#42312722">prev</a><span>|</span><a href="#42312709">next</a><span>|</span><label class="collapse" for="c-42311961">[-]</label><label class="expand" for="c-42311961">[4 more]</label></div><br/><div class="children"><div class="content">By subsidizing it more they&#x27;ll lose less money?</div><br/><div id="42312549" class="c"><input type="checkbox" id="c-42312549" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42311961">parent</a><span>|</span><a href="#42312709">next</a><span>|</span><label class="collapse" for="c-42312549">[-]</label><label class="expand" for="c-42312549">[3 more]</label></div><br/><div class="children"><div class="content">Increasing VRAM would differentiate intel GPUs and allow driving higher ASPs, into the green.</div><br/><div id="42314223" class="c"><input type="checkbox" id="c-42314223" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42312549">parent</a><span>|</span><a href="#42313459">next</a><span>|</span><label class="collapse" for="c-42314223">[-]</label><label class="expand" for="c-42314223">[1 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re massively underestimating the development cost, of the number of people who would actually purchase a higher vram card at a higher price.<p>You&#x27;d need hundreds of thousands of units to really make much of a difference.</div><br/></div></div><div id="42313459" class="c"><input type="checkbox" id="c-42313459" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42312549">parent</a><span>|</span><a href="#42314223">prev</a><span>|</span><a href="#42312709">next</a><span>|</span><label class="collapse" for="c-42313459">[-]</label><label class="expand" for="c-42313459">[1 more]</label></div><br/><div class="children"><div class="content">Well, IIUC it&#x27;s a bit more &quot;having more than 12GB of RAM and raising the price will let it run bigger LLMs on consumer hardware and that&#x27;ll drive premium-ness &#x2F; market share &#x2F; revenue, without subsidizing the price&quot;<p>I don&#x27;t know where this idea is coming from, although it&#x27;s all over these threads.<p>For context, I write a local LLM inference engine and have 0 idea why this would shift anyone&#x27;s purchase intent. The models big enough to need more than 12GB VRAM are also slow enough on consumer GPUs that they&#x27;d be absurd to run. Like less than 2 tkns&#x2F;s. And I have 64 GB of M2 Max VRAM and a 24 GB 3090ti.</div><br/></div></div></div></div></div></div></div></div><div id="42312709" class="c"><input type="checkbox" id="c-42312709" checked=""/><div class="controls bullet"><span class="by">qwytw</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310519">parent</a><span>|</span><a href="#42311815">prev</a><span>|</span><a href="#42310500">next</a><span>|</span><label class="collapse" for="c-42312709">[-]</label><label class="expand" for="c-42312709">[1 more]</label></div><br/><div class="children"><div class="content">Enthusiast&#x2F;Prosumer&#x2F;etc. market is generally still usually highly in most markers even if the revenue is limited. e.g. if hobbyists&#x2F;students&#x2F;developers start using Intel GPUs in a few years the enterprise market might become much less averse to buying Intel&#x27;s datacenter chips.</div><br/></div></div></div></div><div id="42310500" class="c"><input type="checkbox" id="c-42310500" checked=""/><div class="controls bullet"><span class="by">ModernMech</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310446">parent</a><span>|</span><a href="#42310519">prev</a><span>|</span><a href="#42310341">next</a><span>|</span><label class="collapse" for="c-42310500">[-]</label><label class="expand" for="c-42310500">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a chicken and egg scenario. The main problem with running inference at home is the lack of hardware. If the hardware was there more people would do it. And it&#x27;s not a problem if &quot;enthusiasts&quot; are the only ones using it because that&#x27;s to be expected at this stage of the tech cycle. If the market is small just charge more, the enthusiasts will pay it. Once more enthusiasts are running inference at home, then the late adopters will eventually come along.</div><br/><div id="42310515" class="c"><input type="checkbox" id="c-42310515" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310500">parent</a><span>|</span><a href="#42310341">next</a><span>|</span><label class="collapse" for="c-42310515">[-]</label><label class="expand" for="c-42310515">[2 more]</label></div><br/><div class="children"><div class="content">Mac minis are great for this. They&#x27;re cheap-ish and they can run quite large models at a decent speed if you run it with an MLX backend.</div><br/><div id="42310669" class="c"><input type="checkbox" id="c-42310669" checked=""/><div class="controls bullet"><span class="by">alganet</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310515">parent</a><span>|</span><a href="#42310341">next</a><span>|</span><label class="collapse" for="c-42310669">[-]</label><label class="expand" for="c-42310669">[1 more]</label></div><br/><div class="children"><div class="content">mini _Pro_ are great for this, ones with large RAM upgrades.<p>If you get the base 16GB mini, it will have more or less the same VRAM but way worse performance than an Arc.<p>If you already have a PC, it makes sense to go for the cheapest 12GB card instead of a base mac mini.</div><br/></div></div></div></div></div></div></div></div><div id="42310341" class="c"><input type="checkbox" id="c-42310341" checked=""/><div class="controls bullet"><span class="by">cowmix</span><span>|</span><a href="#42310310">parent</a><span>|</span><a href="#42310446">prev</a><span>|</span><a href="#42310677">next</a><span>|</span><label class="collapse" for="c-42310341">[-]</label><label class="expand" for="c-42310341">[4 more]</label></div><br/><div class="children"><div class="content">100% - <i>this</i> could be Intel&#x27;s ticket to capture the hearts of developers and then everything else that flows downstream. They have nothing to lose here -- just do it Intel!</div><br/><div id="42310706" class="c"><input type="checkbox" id="c-42310706" checked=""/><div class="controls bullet"><span class="by">bagels</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310341">parent</a><span>|</span><a href="#42310677">next</a><span>|</span><label class="collapse" for="c-42310706">[-]</label><label class="expand" for="c-42310706">[3 more]</label></div><br/><div class="children"><div class="content">They could lose a lot of money?</div><br/><div id="42311199" class="c"><input type="checkbox" id="c-42311199" checked=""/><div class="controls bullet"><span class="by">flockonus</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310706">parent</a><span>|</span><a href="#42310677">next</a><span>|</span><label class="collapse" for="c-42311199">[-]</label><label class="expand" for="c-42311199">[2 more]</label></div><br/><div class="children"><div class="content">They already do... google $INTC, stare in disbelief in the right side &quot;Financials&quot;.<p>At some point they should make a stand, that&#x27;s the whole meta-topic of this thread.</div><br/><div id="42313830" class="c"><input type="checkbox" id="c-42313830" checked=""/><div class="controls bullet"><span class="by">bagels</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42311199">parent</a><span>|</span><a href="#42310677">next</a><span>|</span><label class="collapse" for="c-42313830">[-]</label><label class="expand" for="c-42313830">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, you&#x27;re right, they could lose a lot more money.</div><br/></div></div></div></div></div></div></div></div><div id="42310677" class="c"><input type="checkbox" id="c-42310677" checked=""/><div class="controls bullet"><span class="by">rafaelmn</span><span>|</span><a href="#42310310">parent</a><span>|</span><a href="#42310341">prev</a><span>|</span><a href="#42310479">next</a><span>|</span><label class="collapse" for="c-42310677">[-]</label><label class="expand" for="c-42310677">[1 more]</label></div><br/><div class="children"><div class="content">You can get that on mac mini and it will probably cost you less than equivalent PC setup. Should also perform better than low end Intel GPU and be better supported. Will use less power as well.</div><br/></div></div><div id="42310479" class="c"><input type="checkbox" id="c-42310479" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#42310310">parent</a><span>|</span><a href="#42310677">prev</a><span>|</span><a href="#42310421">next</a><span>|</span><label class="collapse" for="c-42310479">[-]</label><label class="expand" for="c-42310479">[5 more]</label></div><br/><div class="children"><div class="content">You can just use a CPU in that case, no? You can run most ML inference on vectorized operations on modern CPUs at a fraction of the price.</div><br/><div id="42311340" class="c"><input type="checkbox" id="c-42311340" checked=""/><div class="controls bullet"><span class="by">marcyb5st</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310479">parent</a><span>|</span><a href="#42310421">next</a><span>|</span><label class="collapse" for="c-42311340">[-]</label><label class="expand" for="c-42311340">[4 more]</label></div><br/><div class="children"><div class="content">My 7800x says not really. Compared to my 3070 it feels so incredibly slow that gets in the way of productivity.<p>Specifically, waiting ~2 seconds vs ~20 for a code snippet is much more detrimental to my productivity than the time difference would suggest. In ~2 seconds I don&#x27;t get distracted, in ~20 seconds my mind starts wandering and then I have to spend time refocusing.<p>Make a GPU that is 50% slower than a 2 generations older mid-range GPU (in tokens&#x2F;s) but on bigger models and I would gladly shell out 1000+$.<p>So much so that I am considering getting a 5090 if nVdia actually fixes the connector mess they made with 4090s or even a used v100.</div><br/><div id="42312002" class="c"><input type="checkbox" id="c-42312002" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42311340">parent</a><span>|</span><a href="#42311979">next</a><span>|</span><label class="collapse" for="c-42312002">[-]</label><label class="expand" for="c-42312002">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m running codeseeker 13B model on my macbook with no perf issues and I get a response within a few seconds.<p>Running a specialist model makes more sense on small devices.</div><br/></div></div><div id="42311979" class="c"><input type="checkbox" id="c-42311979" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42311340">parent</a><span>|</span><a href="#42312002">prev</a><span>|</span><a href="#42310421">next</a><span>|</span><label class="collapse" for="c-42311979">[-]</label><label class="expand" for="c-42311979">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand, make it slower so it&#x27;s faster?</div><br/><div id="42313497" class="c"><input type="checkbox" id="c-42313497" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42311979">parent</a><span>|</span><a href="#42310421">next</a><span>|</span><label class="collapse" for="c-42313497">[-]</label><label class="expand" for="c-42313497">[1 more]</label></div><br/><div class="children"><div class="content">My 2080Ti at half speed would still beat the crap out of my 5900X CPU for inference, as long as the model fits in VRAM.<p>I think that&#x27;s what GP was alluding to.</div><br/></div></div></div></div></div></div></div></div><div id="42310421" class="c"><input type="checkbox" id="c-42310421" checked=""/><div class="controls bullet"><span class="by">evanjrowley</span><span>|</span><a href="#42310310">parent</a><span>|</span><a href="#42310479">prev</a><span>|</span><a href="#42310570">next</a><span>|</span><label class="collapse" for="c-42310421">[-]</label><label class="expand" for="c-42310421">[1 more]</label></div><br/><div class="children"><div class="content">Maybe that&#x27;s not too bad for someone who wants to use pre-existing models. Their AI Playground examples require at minimum an Intel Core Ultra H CPU, which is quite low-powered compared to even these dedicated GPUs: <a href="https:&#x2F;&#x2F;github.com&#x2F;intel&#x2F;AI-Playground">https:&#x2F;&#x2F;github.com&#x2F;intel&#x2F;AI-Playground</a></div><br/></div></div><div id="42310570" class="c"><input type="checkbox" id="c-42310570" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#42310310">parent</a><span>|</span><a href="#42310421">prev</a><span>|</span><a href="#42309122">next</a><span>|</span><label class="collapse" for="c-42310570">[-]</label><label class="expand" for="c-42310570">[8 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know a single person in real life that has any desire to run local LLMs. Even amongst my colleagues and tech friends, not very many use LLMs period. It&#x27;s still very niche outside AI enthusiasts. GPT is better than anything I can run locally anyway. It&#x27;s not as popular as you think it is.</div><br/><div id="42314008" class="c"><input type="checkbox" id="c-42314008" checked=""/><div class="controls bullet"><span class="by">rubatuga</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310570">parent</a><span>|</span><a href="#42310675">next</a><span>|</span><label class="collapse" for="c-42314008">[-]</label><label class="expand" for="c-42314008">[1 more]</label></div><br/><div class="children"><div class="content">I run a 12GB model on my 3060 and use it to help answer healthcare questions. I&#x27;m currently doing a medical residency. (No I don&#x27;t use it to diagnose). It helps comply with any HIPAA style regulations. I sometimes use it to fix up my emails. Not sure why people are longing for a 128GB card, just download a quantized model and run with LM Studio (<a href="https:&#x2F;&#x2F;lmstudio.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lmstudio.ai&#x2F;</a>). At least two of my colleagues are using ChatGPT on a regular basis. LLMs are being used in the ER department. LLMs and speech models are being used in psychiatry visits.</div><br/></div></div><div id="42310675" class="c"><input type="checkbox" id="c-42310675" checked=""/><div class="controls bullet"><span class="by">dimensi0nal</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310570">parent</a><span>|</span><a href="#42314008">prev</a><span>|</span><a href="#42311837">next</a><span>|</span><label class="collapse" for="c-42310675">[-]</label><label class="expand" for="c-42310675">[5 more]</label></div><br/><div class="children"><div class="content">The only consumer demand for local AI models is for generating pornography</div><br/><div id="42310799" class="c"><input type="checkbox" id="c-42310799" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310675">parent</a><span>|</span><a href="#42311931">next</a><span>|</span><label class="collapse" for="c-42310799">[-]</label><label class="expand" for="c-42310799">[2 more]</label></div><br/><div class="children"><div class="content">How about running your intelligent home with a voice assistant on your own computer? In privacy-oriented countries (Germany) that would be massive.</div><br/><div id="42312010" class="c"><input type="checkbox" id="c-42312010" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310799">parent</a><span>|</span><a href="#42311931">next</a><span>|</span><label class="collapse" for="c-42312010">[-]</label><label class="expand" for="c-42312010">[1 more]</label></div><br/><div class="children"><div class="content">This is what I&#x27;m fiddling with. My 2080Ti is not quite enough to make it viable. I find the small models fail too often, so need larger Whisper and LLM models.<p>Like the 4060 Ti would have been a nice fit if it hadn&#x27;t been for the narrow memory bus, which makes it slower than my 2080 Ti for LLM inference.<p>A more expensive card has the downside of not being cheap enough to justify idling in my server, and my gaming card is at times busy gaming.</div><br/></div></div></div></div><div id="42311931" class="c"><input type="checkbox" id="c-42311931" checked=""/><div class="controls bullet"><span class="by">serf</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310675">parent</a><span>|</span><a href="#42310799">prev</a><span>|</span><a href="#42313536">next</a><span>|</span><label class="collapse" for="c-42311931">[-]</label><label class="expand" for="c-42311931">[1 more]</label></div><br/><div class="children"><div class="content">absolutely wrong -- if you&#x27;re not clever enough to think of any other reason to run an LLM locally then don&#x27;t condemn the rest of the world to &quot;well they&#x27;re just using it for porno!&quot;</div><br/></div></div><div id="42313536" class="c"><input type="checkbox" id="c-42313536" checked=""/><div class="controls bullet"><span class="by">knowitnone</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310675">parent</a><span>|</span><a href="#42311931">prev</a><span>|</span><a href="#42311837">next</a><span>|</span><label class="collapse" for="c-42313536">[-]</label><label class="expand" for="c-42313536">[1 more]</label></div><br/><div class="children"><div class="content">so you&#x27;re saying that a huge market?!</div><br/></div></div></div></div><div id="42311837" class="c"><input type="checkbox" id="c-42311837" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#42310310">root</a><span>|</span><a href="#42310570">parent</a><span>|</span><a href="#42310675">prev</a><span>|</span><a href="#42309122">next</a><span>|</span><label class="collapse" for="c-42311837">[-]</label><label class="expand" for="c-42311837">[1 more]</label></div><br/><div class="children"><div class="content">I want local copilot. I would pay for this.</div><br/></div></div></div></div></div></div><div id="42309122" class="c"><input type="checkbox" id="c-42309122" checked=""/><div class="controls bullet"><span class="by">rbanffy</span><span>|</span><a href="#42310310">prev</a><span>|</span><a href="#42309649">next</a><span>|</span><label class="collapse" for="c-42309122">[-]</label><label class="expand" for="c-42309122">[5 more]</label></div><br/><div class="children"><div class="content">For me, the most important feature is Linux support. Even if I&#x27;m not a gamer, I might want to use the GPU for compute and buggy proprietary drivers are much more than just an inconvenience.</div><br/><div id="42311680" class="c"><input type="checkbox" id="c-42311680" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#42309122">parent</a><span>|</span><a href="#42309649">next</a><span>|</span><label class="collapse" for="c-42311680">[-]</label><label class="expand" for="c-42311680">[4 more]</label></div><br/><div class="children"><div class="content">Sure, but open drivers have been AMDs selling point for a decade, and even nVidia is finally showing signs of opening up. So it&#x27;s bit dubious if these new Intels really can compete on this front, at least for very long.</div><br/><div id="42312783" class="c"><input type="checkbox" id="c-42312783" checked=""/><div class="controls bullet"><span class="by">whalesalad</span><span>|</span><a href="#42309122">root</a><span>|</span><a href="#42311680">parent</a><span>|</span><a href="#42309649">next</a><span>|</span><label class="collapse" for="c-42312783">[-]</label><label class="expand" for="c-42312783">[3 more]</label></div><br/><div class="children"><div class="content">I welcome a new competitor. Sucks to really only have one valid option on Linux atm. My 6600 is a little long in the tooth. I only have it becuase it is dead silent and runs a 5K display without issue - but I would definitely like to upgrade it for something that can hold its own with ML.</div><br/><div id="42312990" class="c"><input type="checkbox" id="c-42312990" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#42309122">root</a><span>|</span><a href="#42312783">parent</a><span>|</span><a href="#42309649">next</a><span>|</span><label class="collapse" for="c-42312990">[-]</label><label class="expand" for="c-42312990">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Sucks to really only have one valid option on Linux atm.<p>I don&#x27;t think that&#x27;s a super fair shake? Intel iGPUs have been around for a while if you had a laptop chip or iGPU-enabled desktop chip. They&#x27;ve supported Linux just fine for ages, and will fill any non-3D application you might have.<p>And Nvidia chips are quite good on Linux nowadays - Wayland has been very usable since the 535-series drivers and nearly flawless since 550. You&#x27;re right to be apprehensive about proprietary GPU hardware but I think there are plenty of options on the table right now.</div><br/><div id="42313033" class="c"><input type="checkbox" id="c-42313033" checked=""/><div class="controls bullet"><span class="by">whalesalad</span><span>|</span><a href="#42309122">root</a><span>|</span><a href="#42312990">parent</a><span>|</span><a href="#42309649">next</a><span>|</span><label class="collapse" for="c-42313033">[-]</label><label class="expand" for="c-42313033">[1 more]</label></div><br/><div class="children"><div class="content">The iGPU in my 13900K cannot run my 5K display with decent performance (in a desktop environment). I chalked it up to hardware issues but it could be drivers. I am on Debian Linux.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42309649" class="c"><input type="checkbox" id="c-42309649" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#42309122">prev</a><span>|</span><a href="#42315396">next</a><span>|</span><label class="collapse" for="c-42309649">[-]</label><label class="expand" for="c-42309649">[3 more]</label></div><br/><div class="children"><div class="content">I wonder how many transistors it has and  what the chip size it is.<p>For power, it&#x27;s 190W compared to 4060&#x27;s 115 W.<p>EDIT: from [1]:
B580 has 21.7 billion transistors at 406 mm² die area, compared to 4060&#x27;s 18.9 billion and 146 mm². That&#x27;s a big die.<p>[1] <a href="https:&#x2F;&#x2F;www.techpowerup.com&#x2F;gpu-specs&#x2F;arc-b580.c4244" rel="nofollow">https:&#x2F;&#x2F;www.techpowerup.com&#x2F;gpu-specs&#x2F;arc-b580.c4244</a></div><br/><div id="42313383" class="c"><input type="checkbox" id="c-42313383" checked=""/><div class="controls bullet"><span class="by">mastax</span><span>|</span><a href="#42309649">parent</a><span>|</span><a href="#42311723">next</a><span>|</span><label class="collapse" for="c-42313383">[-]</label><label class="expand" for="c-42313383">[1 more]</label></div><br/><div class="children"><div class="content">Those numbers are identical to the A770, and don&#x27;t match the numbers from the preview[0], so I think that&#x27;s a copy paste error.<p>If we use the numbers from the preview:<p><pre><code>    |        |Arc A770|Arc B580|RTX 4060|
    |--------|--------|--------|--------|
    |Process |N6      |N5      |N5      |
    |Die Size|406mm^2 |272mm^2 |159mm^2 |
    |Trans.  |21.7B   |19.6B   |18.9B   |
    |Mem Bus |256 bit |192 bit |128 bit |
    |TDP     |225W    |190W    |115W    |
    |~Perf   |90%     |110%    |100%    |
</code></pre>
In terms of performance per die area it&#x27;s a big improvement over A770 but still far behind Nvidia. It&#x27;s interesting that the transistor density is so much lower than the 4060 despite having the same (or at least similar) process node. Speculating about why that may be:<p>- Nvidia has better layout.
 - Intel is using higher performance (larger) transistor libraries or layout in order to hit the higher boost frequencies (2800 vs 2460).
 - Wider bus interface takes up more space.
 - The B580 may have 1 render slice and 64-bits of memory bus disabled, and they&#x27;re not including those transistors in the count, but they still take up area.<p>[0]: <a href="https:&#x2F;&#x2F;www.techpowerup.com&#x2F;review&#x2F;intel-arc-b580-battlemage-unboxing-preview&#x2F;5.html" rel="nofollow">https:&#x2F;&#x2F;www.techpowerup.com&#x2F;review&#x2F;intel-arc-b580-battlemage...</a></div><br/></div></div><div id="42311723" class="c"><input type="checkbox" id="c-42311723" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#42309649">parent</a><span>|</span><a href="#42313383">prev</a><span>|</span><a href="#42315396">next</a><span>|</span><label class="collapse" for="c-42311723">[-]</label><label class="expand" for="c-42311723">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Both the Arc B580 and B570 are based on the &quot;BMG-G21&quot; a new monolithic silicon built on the TSMC 5 nm EUV process node. The silicon has a die-area of 272 mm², and a transistor count of 19.6 billion<p><a href="https:&#x2F;&#x2F;www.techpowerup.com&#x2F;review&#x2F;intel-arc-b580-battlemage-unboxing-preview&#x2F;2.html" rel="nofollow">https:&#x2F;&#x2F;www.techpowerup.com&#x2F;review&#x2F;intel-arc-b580-battlemage...</a><p>These numbers seem bit more believable</div><br/></div></div></div></div><div id="42315396" class="c"><input type="checkbox" id="c-42315396" checked=""/><div class="controls bullet"><span class="by">s17tnet</span><span>|</span><a href="#42309649">prev</a><span>|</span><a href="#42310547">next</a><span>|</span><label class="collapse" for="c-42315396">[-]</label><label class="expand" for="c-42315396">[1 more]</label></div><br/><div class="children"><div class="content">What about sharing GPU across multiple VMs? Isn&#x27;t Nvidia walled this feature behind unreasonably high price features?</div><br/></div></div><div id="42310547" class="c"><input type="checkbox" id="c-42310547" checked=""/><div class="controls bullet"><span class="by">ChrisArchitect</span><span>|</span><a href="#42315396">prev</a><span>|</span><a href="#42310698">next</a><span>|</span><label class="collapse" for="c-42310547">[-]</label><label class="expand" for="c-42310547">[1 more]</label></div><br/><div class="children"><div class="content">Official page: <a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;products&#x2F;docs&#x2F;discrete-gpus&#x2F;arc&#x2F;desktop&#x2F;b-series&#x2F;overview.html" rel="nofollow">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;products&#x2F;docs&#x2F;discre...</a></div><br/></div></div><div id="42310698" class="c"><input type="checkbox" id="c-42310698" checked=""/><div class="controls bullet"><span class="by">bjoli</span><span>|</span><a href="#42310547">prev</a><span>|</span><a href="#42309301">next</a><span>|</span><label class="collapse" for="c-42310698">[-]</label><label class="expand" for="c-42310698">[2 more]</label></div><br/><div class="children"><div class="content">I love my a750. Works fantastic out of the box in Linux. He encoding and decoding for every format I use. Flawless support for different screens.<p>I haven&#x27;t regretted the purchase at all.</div><br/><div id="42315312" class="c"><input type="checkbox" id="c-42315312" checked=""/><div class="controls bullet"><span class="by">bjoli</span><span>|</span><a href="#42310698">parent</a><span>|</span><a href="#42309301">next</a><span>|</span><label class="collapse" for="c-42315312">[-]</label><label class="expand" for="c-42315312">[1 more]</label></div><br/><div class="children"><div class="content">I wonder what happened to my brain writing the above. There is a &quot;He&quot; in there that makes no sense. Flawless support for &quot;different&quot; screens? I of course mean &quot;many screens&quot;.</div><br/></div></div></div></div><div id="42309301" class="c"><input type="checkbox" id="c-42309301" checked=""/><div class="controls bullet"><span class="by">confident_inept</span><span>|</span><a href="#42310698">prev</a><span>|</span><a href="#42309590">next</a><span>|</span><label class="collapse" for="c-42309301">[-]</label><label class="expand" for="c-42309301">[16 more]</label></div><br/><div class="children"><div class="content">I&#x27;m really curious to see if these still rely heavily on resizable BAR. Putting these in old computers in linux without reBAR support makes the driver crash with literally any load rendering the cards completely unusable.<p>It&#x27;s a real shame, the single slot a380 is a great performance for price light gaming and general use card for small machines.</div><br/><div id="42313388" class="c"><input type="checkbox" id="c-42313388" checked=""/><div class="controls bullet"><span class="by">mastax</span><span>|</span><a href="#42309301">parent</a><span>|</span><a href="#42309387">next</a><span>|</span><label class="collapse" for="c-42313388">[-]</label><label class="expand" for="c-42313388">[1 more]</label></div><br/><div class="children"><div class="content">Yes, the slide deck mentions reBAR is still required.</div><br/></div></div><div id="42309387" class="c"><input type="checkbox" id="c-42309387" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#42309301">parent</a><span>|</span><a href="#42313388">prev</a><span>|</span><a href="#42309590">next</a><span>|</span><label class="collapse" for="c-42309387">[-]</label><label class="expand" for="c-42309387">[14 more]</label></div><br/><div class="children"><div class="content">What is the newest platform that lacks resizable BAR? It was standardized in 2006. Is 4060-level graphics performance useful in whatever old computer has that problem?</div><br/><div id="42309707" class="c"><input type="checkbox" id="c-42309707" checked=""/><div class="controls bullet"><span class="by">tremon</span><span>|</span><a href="#42309301">root</a><span>|</span><a href="#42309387">parent</a><span>|</span><a href="#42309681">next</a><span>|</span><label class="collapse" for="c-42309707">[-]</label><label class="expand" for="c-42309707">[4 more]</label></div><br/><div class="children"><div class="content">The newest platform is probably POWER10. ReBar is not supported on any POWER platform, most likely including the upcoming POWER11.<p>Also, I don&#x27;t think you&#x27;ll find many mainboards from 2006 supporting it. It may have been standardized in 2006, but a quick online search leads me to think that even on x86 mainboards it didn&#x27;t become commonly available until at least 2020.</div><br/><div id="42310258" class="c"><input type="checkbox" id="c-42310258" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#42309301">root</a><span>|</span><a href="#42309707">parent</a><span>|</span><a href="#42311286">next</a><span>|</span><label class="collapse" for="c-42310258">[-]</label><label class="expand" for="c-42310258">[1 more]</label></div><br/><div class="children"><div class="content">Congrats on a pretty niche reply. I wonder if literally anyone has tried to put an ARC dGPU in a POWER system. Maybe someone from Libre-SOC will chime in.</div><br/></div></div><div id="42311286" class="c"><input type="checkbox" id="c-42311286" checked=""/><div class="controls bullet"><span class="by">Palomides</span><span>|</span><a href="#42309301">root</a><span>|</span><a href="#42309707">parent</a><span>|</span><a href="#42310258">prev</a><span>|</span><a href="#42312412">next</a><span>|</span><label class="collapse" for="c-42311286">[-]</label><label class="expand" for="c-42311286">[1 more]</label></div><br/><div class="children"><div class="content">do you have a reference for power rebar support? just curious, I couldn&#x27;t find anything with a quick look</div><br/></div></div><div id="42312412" class="c"><input type="checkbox" id="c-42312412" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#42309301">root</a><span>|</span><a href="#42309707">parent</a><span>|</span><a href="#42311286">prev</a><span>|</span><a href="#42309681">next</a><span>|</span><label class="collapse" for="c-42312412">[-]</label><label class="expand" for="c-42312412">[1 more]</label></div><br/><div class="children"><div class="content">Oh no... my POWER gaming rig... no..</div><br/></div></div></div></div><div id="42309681" class="c"><input type="checkbox" id="c-42309681" checked=""/><div class="controls bullet"><span class="by">vel0city</span><span>|</span><a href="#42309301">root</a><span>|</span><a href="#42309387">parent</a><span>|</span><a href="#42309707">prev</a><span>|</span><a href="#42309556">next</a><span>|</span><label class="collapse" for="c-42309681">[-]</label><label class="expand" for="c-42309681">[1 more]</label></div><br/><div class="children"><div class="content">Ryzen 2000 series processors don&#x27;t support AMD&#x27;s &quot;Smart Access Memory&quot; which is pretty much resizable BAR. That&#x27;s 2018.<p>Coffee Lake also didn&#x27;t really support ReBAR either, also 2018.</div><br/></div></div><div id="42309556" class="c"><input type="checkbox" id="c-42309556" checked=""/><div class="controls bullet"><span class="by">bryanlarsen</span><span>|</span><a href="#42309301">root</a><span>|</span><a href="#42309387">parent</a><span>|</span><a href="#42309681">prev</a><span>|</span><a href="#42314232">next</a><span>|</span><label class="collapse" for="c-42309556">[-]</label><label class="expand" for="c-42309556">[6 more]</label></div><br/><div class="children"><div class="content">Sandy Bridge (2009) is still a very usable CPU with a modern GPU.   In theory Sandy Bridge supported resizable BAR but in practice they didn&#x27;t.   I think the problem was BIOS&#x27;s.</div><br/><div id="42315608" class="c"><input type="checkbox" id="c-42315608" checked=""/><div class="controls bullet"><span class="by">71bw</span><span>|</span><a href="#42309301">root</a><span>|</span><a href="#42309556">parent</a><span>|</span><a href="#42310579">next</a><span>|</span><label class="collapse" for="c-42315608">[-]</label><label class="expand" for="c-42315608">[1 more]</label></div><br/><div class="children"><div class="content">ReBARUEFI was enough to get it working on an ASUS P8P67-something something when I tried it in January-ish.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;xCuri0&#x2F;ReBarUEFI">https:&#x2F;&#x2F;github.com&#x2F;xCuri0&#x2F;ReBarUEFI</a></div><br/></div></div><div id="42310579" class="c"><input type="checkbox" id="c-42310579" checked=""/><div class="controls bullet"><span class="by">6SixTy</span><span>|</span><a href="#42309301">root</a><span>|</span><a href="#42309556">parent</a><span>|</span><a href="#42315608">prev</a><span>|</span><a href="#42311533">next</a><span>|</span><label class="collapse" for="c-42310579">[-]</label><label class="expand" for="c-42310579">[1 more]</label></div><br/><div class="children"><div class="content">On paper any PCIe 2.0 motherboard can receive a BIOS update adding ReBAR support with 2.1, but reality is that you pretty much have to get a PCIe 3.0 motherboard to have any chance of having it or modding it in yourself.<p>Another issue is that not every GPU actually supports ReBAR, I&#x27;m reasonably certain the Nvidia drivers turn it off for some titles, and pretty much the only vendor that reliably wants ReBAR on at all times is Intel Arc.<p>I also personally wouldn&#x27;t say that Sandy Bridge is very usable with a modern GPU without also specifying what kind of CPU or GPU. Or context in how it&#x27;s being used.</div><br/></div></div><div id="42311533" class="c"><input type="checkbox" id="c-42311533" checked=""/><div class="controls bullet"><span class="by">vel0city</span><span>|</span><a href="#42309301">root</a><span>|</span><a href="#42309556">parent</a><span>|</span><a href="#42310579">prev</a><span>|</span><a href="#42310420">next</a><span>|</span><label class="collapse" for="c-42311533">[-]</label><label class="expand" for="c-42311533">[1 more]</label></div><br/><div class="children"><div class="content">My old Ice Lake CPU was very much a bottleneck in lots of games in 2018 when I finally replaced it. It was a noticeable improvement across the board making the jump to a Zen+ CPU at the time, even with the same GPU.</div><br/></div></div><div id="42310420" class="c"><input type="checkbox" id="c-42310420" checked=""/><div class="controls bullet"><span class="by">stusmall</span><span>|</span><a href="#42309301">root</a><span>|</span><a href="#42309556">parent</a><span>|</span><a href="#42311533">prev</a><span>|</span><a href="#42314232">next</a><span>|</span><label class="collapse" for="c-42310420">[-]</label><label class="expand" for="c-42310420">[2 more]</label></div><br/><div class="children"><div class="content">Oh wow.  That&#x27;s older than I thought.  This is definitely less of an issue than folks make out of it.<p>I cling onto my old hardware to limit ewaste where I can.  I still gave up on my old sandybridge machine once it hit about a decade old.  Not only would the CPU have trouble keeping up, its mostly only PCIe 2.0.  A few had 3.0.  You wouldn&#x27;t get the full potential even out of the cheapest one of these intel cards.  If you are putting a GPU in a system like that I can&#x27;t imagine even buying new.  Just get something used off ebay.</div><br/><div id="42313460" class="c"><input type="checkbox" id="c-42313460" checked=""/><div class="controls bullet"><span class="by">vel0city</span><span>|</span><a href="#42309301">root</a><span>|</span><a href="#42310420">parent</a><span>|</span><a href="#42314232">next</a><span>|</span><label class="collapse" for="c-42313460">[-]</label><label class="expand" for="c-42313460">[1 more]</label></div><br/><div class="children"><div class="content">There were a lot of generations after Sandy Bridge which didn&#x27;t have it; Sandy Bridge was just one generation that didn&#x27;t really support it on the consumer side.<p>Consumer boards and CPUs didn&#x27;t really support it well until after 2018. I upgraded away from a Zen+ system because it didn&#x27;t support it.</div><br/></div></div></div></div></div></div><div id="42314232" class="c"><input type="checkbox" id="c-42314232" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#42309301">root</a><span>|</span><a href="#42309387">parent</a><span>|</span><a href="#42309556">prev</a><span>|</span><a href="#42310205">next</a><span>|</span><label class="collapse" for="c-42314232">[-]</label><label class="expand" for="c-42314232">[1 more]</label></div><br/><div class="children"><div class="content">While &quot;standardized&quot; many implementations were so buggy to be unusable - we needed &gt;4gb pcie mappings for a development board, and finding motherboards that actually <i>worked</i> was a PITA well into 2012 (when I left that project).</div><br/></div></div><div id="42310205" class="c"><input type="checkbox" id="c-42310205" checked=""/><div class="controls bullet"><span class="by">babypuncher</span><span>|</span><a href="#42309301">root</a><span>|</span><a href="#42309387">parent</a><span>|</span><a href="#42314232">prev</a><span>|</span><a href="#42309590">next</a><span>|</span><label class="collapse" for="c-42310205">[-]</label><label class="expand" for="c-42310205">[1 more]</label></div><br/><div class="children"><div class="content">ReBAR was standardized in 2006 but consumer motherboards didn&#x27;t start shipping with an option to enable it until much later, and didn&#x27;t start turning it on by default until a few years ago.</div><br/></div></div></div></div></div></div><div id="42309590" class="c"><input type="checkbox" id="c-42309590" checked=""/><div class="controls bullet"><span class="by">CoastalCoder</span><span>|</span><a href="#42309301">prev</a><span>|</span><a href="#42312934">next</a><span>|</span><label class="collapse" for="c-42309590">[-]</label><label class="expand" for="c-42309590">[6 more]</label></div><br/><div class="children"><div class="content">Given Intel&#x27;s recent troubles, I&#x27;m trying to decide how risky it is to invest in their platform.  Especially discrete GPUs for Linux gaming<p>Fortunately, having their Linux drivers be (mostly?) open source makes a purchase seem less risky.</div><br/><div id="42310413" class="c"><input type="checkbox" id="c-42310413" checked=""/><div class="controls bullet"><span class="by">beAbU</span><span>|</span><a href="#42309590">parent</a><span>|</span><a href="#42310294">next</a><span>|</span><label class="collapse" for="c-42310413">[-]</label><label class="expand" for="c-42310413">[4 more]</label></div><br/><div class="children"><div class="content">Intel isn&#x27;t going anywhere for at least a couple of hardware genrations. Buying a GPU is also not &quot;investing&quot; in anything. In 2 years&#x27; time you can replace it whith whatever is best value for money at that time.</div><br/><div id="42310563" class="c"><input type="checkbox" id="c-42310563" checked=""/><div class="controls bullet"><span class="by">CoastalCoder</span><span>|</span><a href="#42309590">root</a><span>|</span><a href="#42310413">parent</a><span>|</span><a href="#42310294">next</a><span>|</span><label class="collapse" for="c-42310563">[-]</label><label class="expand" for="c-42310563">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Buying a GPU is also not &quot;investing&quot; in anything.<p>It is in the (minor) sense that I&#x27;d rely on Intel for warranty support, driver updates (if closed source), and firmware fixes.<p>But I agree with your main point that the worst-case downside isn&#x27;t that big of a deal.</div><br/><div id="42311878" class="c"><input type="checkbox" id="c-42311878" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#42309590">root</a><span>|</span><a href="#42310563">parent</a><span>|</span><a href="#42310294">next</a><span>|</span><label class="collapse" for="c-42311878">[-]</label><label class="expand" for="c-42311878">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no way you&#x27;re going to maintain and develop the intel linux driver as a solo dev.</div><br/><div id="42311964" class="c"><input type="checkbox" id="c-42311964" checked=""/><div class="controls bullet"><span class="by">CoastalCoder</span><span>|</span><a href="#42309590">root</a><span>|</span><a href="#42311878">parent</a><span>|</span><a href="#42310294">next</a><span>|</span><label class="collapse" for="c-42311964">[-]</label><label class="expand" for="c-42311964">[1 more]</label></div><br/><div class="children"><div class="content">&gt; There&#x27;s no way you&#x27;re going to maintain and develop the intel linux driver as a solo dev.<p>I agree entirely.<p>My point was that even if Intel disappeared tomorrow, there&#x27;s a good chance that Linux developer <i>community</i>  would take over maintenance of those drivers.<p>In contrast to, e.g., 10-years-ago nvidia, where IIUC it was very difficult for outsiders to obtain the documentation needed to write proper drivers for their GPUs.</div><br/></div></div></div></div></div></div></div></div><div id="42310294" class="c"><input type="checkbox" id="c-42310294" checked=""/><div class="controls bullet"><span class="by">babypuncher</span><span>|</span><a href="#42309590">parent</a><span>|</span><a href="#42310413">prev</a><span>|</span><a href="#42312934">next</a><span>|</span><label class="collapse" for="c-42310294">[-]</label><label class="expand" for="c-42310294">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t speak from experience with their GPUs on Linux, but I know on Windows most of their problems stem from supporting pre-DX12 Direct3D titles. Nvidia and AMD have spent many years polishing up their Direct3D support and putting in driver-side hacks that paper over badly programmed Direct3D games.<p>These are obviously Windows-specific issues that don&#x27;t come up at all in Linux, where all that Direct3D headache is taken care of by DXVK. Amusingly a big part of Intel&#x27;s efforts to improve D3D performance on Windows has been to use DXVK for many titles.</div><br/></div></div></div></div><div id="42312934" class="c"><input type="checkbox" id="c-42312934" checked=""/><div class="controls bullet"><span class="by">bigiain</span><span>|</span><a href="#42309590">prev</a><span>|</span><a href="#42309689">next</a><span>|</span><label class="collapse" for="c-42312934">[-]</label><label class="expand" for="c-42312934">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Arc B&quot;?<p>Presumably graphics cards optimised for hairdressers and telephone sanitisers?</div><br/></div></div><div id="42309689" class="c"><input type="checkbox" id="c-42309689" checked=""/><div class="controls bullet"><span class="by">ThatMedicIsASpy</span><span>|</span><a href="#42312934">prev</a><span>|</span><a href="#42309842">next</a><span>|</span><label class="collapse" for="c-42309689">[-]</label><label class="expand" for="c-42309689">[6 more]</label></div><br/><div class="children"><div class="content">SR-IOV is supported on their iGPUs and outside of it exclusive to their enterprise offering. Give it to me on desktop and I&#x27;ll buy.</div><br/><div id="42311883" class="c"><input type="checkbox" id="c-42311883" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#42309689">parent</a><span>|</span><a href="#42309842">next</a><span>|</span><label class="collapse" for="c-42311883">[-]</label><label class="expand" for="c-42311883">[5 more]</label></div><br/><div class="children"><div class="content">Intel is allergic to competition.</div><br/><div id="42312778" class="c"><input type="checkbox" id="c-42312778" checked=""/><div class="controls bullet"><span class="by">ThatMedicIsASpy</span><span>|</span><a href="#42309689">root</a><span>|</span><a href="#42311883">parent</a><span>|</span><a href="#42309842">next</a><span>|</span><label class="collapse" for="c-42312778">[-]</label><label class="expand" for="c-42312778">[4 more]</label></div><br/><div class="children"><div class="content">There is no competition. People get old workstation&#x2F;server cards</div><br/><div id="42313770" class="c"><input type="checkbox" id="c-42313770" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#42309689">root</a><span>|</span><a href="#42312778">parent</a><span>|</span><a href="#42309842">next</a><span>|</span><label class="collapse" for="c-42313770">[-]</label><label class="expand" for="c-42313770">[3 more]</label></div><br/><div class="children"><div class="content">Intel has a huge problem with business units not competing with eachother.</div><br/><div id="42314503" class="c"><input type="checkbox" id="c-42314503" checked=""/><div class="controls bullet"><span class="by">tadfisher</span><span>|</span><a href="#42309689">root</a><span>|</span><a href="#42313770">parent</a><span>|</span><a href="#42309842">next</a><span>|</span><label class="collapse" for="c-42314503">[-]</label><label class="expand" for="c-42314503">[2 more]</label></div><br/><div class="children"><div class="content">Do they still lock out ECC support on Core processors? I&#x27;m still running an ancient i3 for ECC support in my home server.</div><br/><div id="42315625" class="c"><input type="checkbox" id="c-42315625" checked=""/><div class="controls bullet"><span class="by">scraptor</span><span>|</span><a href="#42309689">root</a><span>|</span><a href="#42314503">parent</a><span>|</span><a href="#42309842">next</a><span>|</span><label class="collapse" for="c-42315625">[-]</label><label class="expand" for="c-42315625">[1 more]</label></div><br/><div class="children"><div class="content">ECC is supported on a random subset of processors (one of each performance tier) but the motherboard still costs 500$.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42309842" class="c"><input type="checkbox" id="c-42309842" checked=""/><div class="controls bullet"><span class="by">karmakaze</span><span>|</span><a href="#42309689">prev</a><span>|</span><a href="#42310430">next</a><span>|</span><label class="collapse" for="c-42309842">[-]</label><label class="expand" for="c-42309842">[7 more]</label></div><br/><div class="children"><div class="content">I wanted to have alternative choices than Nvidia for high power GPUs. Then the more I thought about it, the more it made sense to rent cloud services for AI&#x2F;ML workloads and lesser powered ones for gaming. The only use cases I could come up with for wanting high-end cards are 4k gaming (a luxury I can&#x27;t justify for infrequent use) or for PC VR which may still be valid if&#x2F;when a decent OLED (or mini-OLED) headset is available--the Sony PSVR2 with PC adapter is pretty close. The Bigscreen Beyond is also a milestone&#x2F;benchmark.</div><br/><div id="42310073" class="c"><input type="checkbox" id="c-42310073" checked=""/><div class="controls bullet"><span class="by">gigaflop</span><span>|</span><a href="#42309842">parent</a><span>|</span><a href="#42309860">next</a><span>|</span><label class="collapse" for="c-42310073">[-]</label><label class="expand" for="c-42310073">[4 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t rent a GPU for gaming, unless you&#x27;re doing something like a full-on game streaming service. +10ms isn&#x27;t much for some games, but would be noticeable on plenty.<p>IMO you want those frames getting rendered as close to the monitor as possible, and you&#x27;d probably have a better time with lower fidelity graphics rendered locally. You&#x27;d also get to keep gaming during a network outage.</div><br/><div id="42310391" class="c"><input type="checkbox" id="c-42310391" checked=""/><div class="controls bullet"><span class="by">karmakaze</span><span>|</span><a href="#42309842">root</a><span>|</span><a href="#42310073">parent</a><span>|</span><a href="#42310324">next</a><span>|</span><label class="collapse" for="c-42310391">[-]</label><label class="expand" for="c-42310391">[1 more]</label></div><br/><div class="children"><div class="content">Absolutely. By &quot;and lesser powered ones for gaming&quot; I meant purchase.</div><br/></div></div><div id="42310324" class="c"><input type="checkbox" id="c-42310324" checked=""/><div class="controls bullet"><span class="by">babypuncher</span><span>|</span><a href="#42309842">root</a><span>|</span><a href="#42310073">parent</a><span>|</span><a href="#42310391">prev</a><span>|</span><a href="#42309860">next</a><span>|</span><label class="collapse" for="c-42310324">[-]</label><label class="expand" for="c-42310324">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t even think network latency is the real problem, it&#x27;s all the buffering needed to encode a game&#x27;s output to a video stream and keep it v-synced with a network-attached display.<p>I&#x27;ve tried game streaming under the best possible conditions (&lt;1ms network latency) and it still feels a little off. Especially shooters and 2D platformers.</div><br/><div id="42310497" class="c"><input type="checkbox" id="c-42310497" checked=""/><div class="controls bullet"><span class="by">oidar</span><span>|</span><a href="#42309842">root</a><span>|</span><a href="#42310324">parent</a><span>|</span><a href="#42309860">next</a><span>|</span><label class="collapse" for="c-42310497">[-]</label><label class="expand" for="c-42310497">[1 more]</label></div><br/><div class="children"><div class="content">Yeah - there&#x27;s no way to play something like Overwatch&#x2F;Fornite on a streaming service and have a good time. The only things that seems to be ok is turned based or platformers.</div><br/></div></div></div></div></div></div><div id="42309860" class="c"><input type="checkbox" id="c-42309860" checked=""/><div class="controls bullet"><span class="by">oidar</span><span>|</span><a href="#42309842">parent</a><span>|</span><a href="#42310073">prev</a><span>|</span><a href="#42310430">next</a><span>|</span><label class="collapse" for="c-42309860">[-]</label><label class="expand" for="c-42309860">[2 more]</label></div><br/><div class="children"><div class="content">Which video card are you using for PSVR?</div><br/><div id="42310080" class="c"><input type="checkbox" id="c-42310080" checked=""/><div class="controls bullet"><span class="by">karmakaze</span><span>|</span><a href="#42309842">root</a><span>|</span><a href="#42309860">parent</a><span>|</span><a href="#42310430">next</a><span>|</span><label class="collapse" for="c-42310080">[-]</label><label class="expand" for="c-42310080">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t decided&#x2F;pulled-the-trigger but the Intel ARC series are giving the AMD parts a good run for the money.<p>The only concern is how well the new Intel drivers work (full support for DX12) with older titles which are continuously being improved (for DX11, 10, and some for 9 others via emulation).<p>There&#x27;s likely some deep discounting of Intel cards because of how bad the drivers were at launch and the prices may not stay so low once things are working much better.</div><br/></div></div></div></div></div></div><div id="42310430" class="c"><input type="checkbox" id="c-42310430" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#42309842">prev</a><span>|</span><a href="#42310607">next</a><span>|</span><label class="collapse" for="c-42310430">[-]</label><label class="expand" for="c-42310430">[5 more]</label></div><br/><div class="children"><div class="content">Anyone using Intel graphics cards? Aside from specs drivers and support can make or break the value prop of a gfx card. Would be curious what actually using is these is like.</div><br/><div id="42311305" class="c"><input type="checkbox" id="c-42311305" checked=""/><div class="controls bullet"><span class="by">jamesgeck0</span><span>|</span><a href="#42310430">parent</a><span>|</span><a href="#42310478">next</a><span>|</span><label class="collapse" for="c-42311305">[-]</label><label class="expand" for="c-42311305">[1 more]</label></div><br/><div class="children"><div class="content">I use an A770 LE for PC gaming. Windows drivers have improved substantially in the last two years. There&#x27;s a driver update every month or so, although the Intel Arc control GUI hasn&#x27;t improved in a while. Popular newer titles have generally run well; I&#x27;ve played some Metaphor, Final Fantasy 16, Elden Ring, Spider-Man Remastered, Horizon Zero Dawn, Overwatch, Jedi Survivor, Forza Horizon 4, Monster Hunter Sunbreak, etc. without major issues. Older games sometimes struggle; a 6 year old Need for Speed doesn&#x27;t display terrain, some 10+ year old indie games crash. Usually fixed by dropping dxvk.dll in the game directory. This fix cannot be used with older Windows Store games. One problematic newer title was Starfield, which at launch had massive frame pacing and hard crashing issues exclusive to Intel Arc.<p>I&#x27;ve had a small sound latency issue forever; most visible with YouTube videos, the first half-second of every video is silent.<p>I picked this card up for about $120 less than the GTX 4060. Wasn&#x27;t a terrible decision.</div><br/></div></div><div id="42310478" class="c"><input type="checkbox" id="c-42310478" checked=""/><div class="controls bullet"><span class="by">GiorgioG</span><span>|</span><a href="#42310430">parent</a><span>|</span><a href="#42311305">prev</a><span>|</span><a href="#42310607">next</a><span>|</span><label class="collapse" for="c-42310478">[-]</label><label class="expand" for="c-42310478">[3 more]</label></div><br/><div class="children"><div class="content">I put an Arc card in my daughter&#x27;s machine last month.  Seems to work fine.</div><br/><div id="42310989" class="c"><input type="checkbox" id="c-42310989" checked=""/><div class="controls bullet"><span class="by">Scramblejams</span><span>|</span><a href="#42310430">root</a><span>|</span><a href="#42310478">parent</a><span>|</span><a href="#42310607">next</a><span>|</span><label class="collapse" for="c-42310989">[-]</label><label class="expand" for="c-42310989">[2 more]</label></div><br/><div class="children"><div class="content">What OS?</div><br/><div id="42314221" class="c"><input type="checkbox" id="c-42314221" checked=""/><div class="controls bullet"><span class="by">GiorgioG</span><span>|</span><a href="#42310430">root</a><span>|</span><a href="#42310989">parent</a><span>|</span><a href="#42310607">next</a><span>|</span><label class="collapse" for="c-42314221">[-]</label><label class="expand" for="c-42314221">[1 more]</label></div><br/><div class="children"><div class="content">Windows 11</div><br/></div></div></div></div></div></div></div></div><div id="42310607" class="c"><input type="checkbox" id="c-42310607" checked=""/><div class="controls bullet"><span class="by">greenavocado</span><span>|</span><a href="#42310430">prev</a><span>|</span><a href="#42313993">next</a><span>|</span><label class="collapse" for="c-42310607">[-]</label><label class="expand" for="c-42310607">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not a gamer and there is not enough memory in this thing for me to care to use it for AI applications so that leaves just one thing I care about: hardware accelerated video encoding and decoding. Let&#x27;s see some performance metrics both in speed and visual quality</div><br/><div id="42310661" class="c"><input type="checkbox" id="c-42310661" checked=""/><div class="controls bullet"><span class="by">bjoli</span><span>|</span><a href="#42310607">parent</a><span>|</span><a href="#42313993">next</a><span>|</span><label class="collapse" for="c-42310661">[-]</label><label class="expand" for="c-42310661">[1 more]</label></div><br/><div class="children"><div class="content">From what I have gathered, the alchemist av1 is about the same or sliiiightly worse than current nvenc. My a750 does about 1400fps for dvd encoding on the quality preset. I havent had the opportunity to try 1080p or 4k though.</div><br/></div></div></div></div><div id="42313993" class="c"><input type="checkbox" id="c-42313993" checked=""/><div class="controls bullet"><span class="by">Venn1</span><span>|</span><a href="#42310607">prev</a><span>|</span><a href="#42309398">next</a><span>|</span><label class="collapse" for="c-42313993">[-]</label><label class="expand" for="c-42313993">[1 more]</label></div><br/><div class="children"><div class="content">I’ll pick up a B580 to see how it works with Jellyfin transcoding, OBS streaming using AV1, and, with some luck, Davinci Resolve. Maybe a little Blender?<p>Other exciting tests will include things like fan control, since that’s still an issue with Arc GPUs.<p>Should make for a fun blog post.</div><br/></div></div><div id="42309398" class="c"><input type="checkbox" id="c-42309398" checked=""/><div class="controls bullet"><span class="by">mtlmtlmtlmtl</span><span>|</span><a href="#42313993">prev</a><span>|</span><a href="#42312959">next</a><span>|</span><label class="collapse" for="c-42309398">[-]</label><label class="expand" for="c-42309398">[3 more]</label></div><br/><div class="children"><div class="content">Bit disappointed there&#x27;s no 16gig(or more) version. But absolutely thrilled the rumours of Intel discrete graphics&#x27; demise were wildly exaggerated(looking at you, Moore&#x27;s Law is Dead...).<p>Very happy with my A770. Godsend for people like me who want plenty VRAM to play with neural nets, but don&#x27;t have the money for workstation GPUs or massively overpriced Nvidia flagships. Works painlessly with linux, gaming performance is fine, price was the first time I haven&#x27;t felt fleeced buying a GPU in many years. Not having CUDA does lead to some friction, but I think nVidia&#x27;s CUDA moat is a temporary situation.<p>Prolly sit this one out unless they release another SKU with 16G or more ram. But if Intel survives long enough to release Celestial, I&#x27;ll happily buy one.</div><br/><div id="42309567" class="c"><input type="checkbox" id="c-42309567" checked=""/><div class="controls bullet"><span class="by">khimaros</span><span>|</span><a href="#42309398">parent</a><span>|</span><a href="#42312959">next</a><span>|</span><label class="collapse" for="c-42309567">[-]</label><label class="expand" for="c-42309567">[2 more]</label></div><br/><div class="children"><div class="content">have you tested llama.cpp with this card on Linux? when i tested about a year ago, it was a nightmare.</div><br/><div id="42309618" class="c"><input type="checkbox" id="c-42309618" checked=""/><div class="controls bullet"><span class="by">mtlmtlmtlmtl</span><span>|</span><a href="#42309398">root</a><span>|</span><a href="#42309567">parent</a><span>|</span><a href="#42312959">next</a><span>|</span><label class="collapse" for="c-42309618">[-]</label><label class="expand" for="c-42309618">[1 more]</label></div><br/><div class="children"><div class="content">A few months ago, yeah. Had to set an environment variable(added to the ollama systemd unit file), but otherwise it worked just fine.</div><br/></div></div></div></div></div></div><div id="42312959" class="c"><input type="checkbox" id="c-42312959" checked=""/><div class="controls bullet"><span class="by">999900000999</span><span>|</span><a href="#42309398">prev</a><span>|</span><a href="#42310323">next</a><span>|</span><label class="collapse" for="c-42312959">[-]</label><label class="expand" for="c-42312959">[1 more]</label></div><br/><div class="children"><div class="content">I actually really like the Arc 770.<p>However, this is going to go on clearance within 6 months. Good for consumers, bad for Intel.<p>Also keep in mind for any ML task Nvidia has the best ecosystem around. AMD and Intel are both like 5 years behind to be charitable...</div><br/></div></div><div id="42310323" class="c"><input type="checkbox" id="c-42310323" checked=""/><div class="controls bullet"><span class="by">stracer</span><span>|</span><a href="#42312959">prev</a><span>|</span><a href="#42311061">next</a><span>|</span><label class="collapse" for="c-42310323">[-]</label><label class="expand" for="c-42310323">[8 more]</label></div><br/><div class="children"><div class="content">Too late, and it has a bad rep. This effort from Intel to sell discrete GPUs is just inertia from old aspirations, won&#x27;t really help noticeably to save it, as there is not much money in it. Most probably the whole Intel ARC effort will be mothballed, and probably many more will.</div><br/><div id="42310723" class="c"><input type="checkbox" id="c-42310723" checked=""/><div class="controls bullet"><span class="by">ksd482</span><span>|</span><a href="#42310323">parent</a><span>|</span><a href="#42310690">next</a><span>|</span><label class="collapse" for="c-42310723">[-]</label><label class="expand" for="c-42310723">[3 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the alternative?<p>I think it&#x27;s the right call since there isn&#x27;t much competition in GPU industry anyway. Sure, Intel is far behind. But they need to start somewhere in order to break ground.<p>Strictly speaking strategically, my intuition is that they will learn from this, course correct and then would start making progress.</div><br/><div id="42311088" class="c"><input type="checkbox" id="c-42311088" checked=""/><div class="controls bullet"><span class="by">stracer</span><span>|</span><a href="#42310323">root</a><span>|</span><a href="#42310723">parent</a><span>|</span><a href="#42310690">next</a><span>|</span><label class="collapse" for="c-42311088">[-]</label><label class="expand" for="c-42311088">[2 more]</label></div><br/><div class="children"><div class="content">The idea of another competitive GPU manufacturer is nice. But it is hard to bring into existence. Intel is not in a position to invest lots of money and sustained effort into products for which the market is captured and controlled by a much bigger and more competent company on top of its game. Not even AMD can get more market share, and they are much more competent in the GPU technology. Unless NVIDIA and AMD make serious mistakes, Intel GPUs will remain a 3rd rate product.<p>&gt; &quot;They need to start somewhere in order to break ground&quot;<p>Intel has big problems and it&#x27;s not clear they should occupy themselves with this. They should stabilize, and the most plausible way to do that is to cut the weak parts, and get back to what they were good at - performant secure x86_64 CPUs, maybe some new innovative CPUs with low consumption, maybe memory&#x2F;solid state drives.</div><br/><div id="42312817" class="c"><input type="checkbox" id="c-42312817" checked=""/><div class="controls bullet"><span class="by">Wytwwww</span><span>|</span><a href="#42310323">root</a><span>|</span><a href="#42311088">parent</a><span>|</span><a href="#42310690">next</a><span>|</span><label class="collapse" for="c-42312817">[-]</label><label class="expand" for="c-42312817">[1 more]</label></div><br/><div class="children"><div class="content">&gt; maybe memory&#x2F;solid state drives<p>That&#x27;s a very low margin and cyclical market since memory&#x2F;SSDs are basically commodities. I don&#x27;t think Intel would have any chance surviving in such a market they just have way to much bloat&#x2F;R&amp;D spending. Which is not a bad thing as long as you can produce better than products than the competition.</div><br/></div></div></div></div></div></div><div id="42310690" class="c"><input type="checkbox" id="c-42310690" checked=""/><div class="controls bullet"><span class="by">undersuit</span><span>|</span><a href="#42310323">parent</a><span>|</span><a href="#42310723">prev</a><span>|</span><a href="#42312962">next</a><span>|</span><label class="collapse" for="c-42310690">[-]</label><label class="expand" for="c-42310690">[3 more]</label></div><br/><div class="children"><div class="content">No reviews and when you click on the reseller links in the press announcement they&#x27;re still selling A750s with no B-Series in sight. Strong paper launch.</div><br/><div id="42311177" class="c"><input type="checkbox" id="c-42311177" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#42310323">root</a><span>|</span><a href="#42310690">parent</a><span>|</span><a href="#42312962">next</a><span>|</span><label class="collapse" for="c-42311177">[-]</label><label class="expand" for="c-42311177">[2 more]</label></div><br/><div class="children"><div class="content">The fine article states reviews are still embargoed, and sales start next week.</div><br/><div id="42312049" class="c"><input type="checkbox" id="c-42312049" checked=""/><div class="controls bullet"><span class="by">undersuit</span><span>|</span><a href="#42310323">root</a><span>|</span><a href="#42311177">parent</a><span>|</span><a href="#42312962">next</a><span>|</span><label class="collapse" for="c-42312049">[-]</label><label class="expand" for="c-42312049">[1 more]</label></div><br/><div class="children"><div class="content">The mods have thankfully changed this to a Phoronix article instead of the Intel page and the title has been reworked to not include &#x27;launch&#x27;.</div><br/></div></div></div></div></div></div><div id="42312962" class="c"><input type="checkbox" id="c-42312962" checked=""/><div class="controls bullet"><span class="by">tester756</span><span>|</span><a href="#42310323">parent</a><span>|</span><a href="#42310690">prev</a><span>|</span><a href="#42311061">next</a><span>|</span><label class="collapse" for="c-42312962">[-]</label><label class="expand" for="c-42312962">[1 more]</label></div><br/><div class="children"><div class="content">&quot;old aspirations&quot;<p>&quot;there is not much money in it&quot;?<p>WTF?</div><br/></div></div></div></div><div id="42311061" class="c"><input type="checkbox" id="c-42311061" checked=""/><div class="controls bullet"><span class="by">zenethian</span><span>|</span><a href="#42310323">prev</a><span>|</span><a href="#42309658">next</a><span>|</span><label class="collapse" for="c-42311061">[-]</label><label class="expand" for="c-42311061">[2 more]</label></div><br/><div class="children"><div class="content">These are pretty interesting, but I&#x27;m curious about the side-by-side screenshot with the slider:  why does ray tracing need to be enabled to see the yellow stoplight?  That seems like a weird oversight.</div><br/><div id="42311221" class="c"><input type="checkbox" id="c-42311221" checked=""/><div class="controls bullet"><span class="by">zamalek</span><span>|</span><a href="#42311061">parent</a><span>|</span><a href="#42309658">next</a><span>|</span><label class="collapse" for="c-42311221">[-]</label><label class="expand" for="c-42311221">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s possible that the capture wasn&#x27;t taken at the exact same frame, or that the state of the light isn&#x27;t deterministic in the benchmark.</div><br/></div></div></div></div><div id="42309658" class="c"><input type="checkbox" id="c-42309658" checked=""/><div class="controls bullet"><span class="by">Archit3ch</span><span>|</span><a href="#42311061">prev</a><span>|</span><a href="#42309551">next</a><span>|</span><label class="collapse" for="c-42309658">[-]</label><label class="expand" for="c-42309658">[2 more]</label></div><br/><div class="children"><div class="content">They say the best predictor for the future is the past.<p>How was driver support for their A-series?</div><br/><div id="42309865" class="c"><input type="checkbox" id="c-42309865" checked=""/><div class="controls bullet"><span class="by">Night_Thastus</span><span>|</span><a href="#42309658">parent</a><span>|</span><a href="#42309551">next</a><span>|</span><label class="collapse" for="c-42309865">[-]</label><label class="expand" for="c-42309865">[1 more]</label></div><br/><div class="children"><div class="content">Drivers were <i>very</i> rough at launch. Some games didn&#x27;t run at all, some basic functionality and configuration either crashes or failed to work, some things ran very poorly, etc. However, it was essentially all ironed out over many months of work.<p>They likely won&#x27;t need to do the same discovery and fixing for B-series as they&#x27;ve already dealt with it.</div><br/></div></div></div></div><div id="42309551" class="c"><input type="checkbox" id="c-42309551" checked=""/><div class="controls bullet"><span class="by">gs17</span><span>|</span><a href="#42309658">prev</a><span>|</span><a href="#42311452">next</a><span>|</span><label class="collapse" for="c-42309551">[-]</label><label class="expand" for="c-42309551">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Intel with their Windows benchmarks are promoting the Arc B580 as being 24% faster than the Intel Arc A750<p>Not a huge fan of the numbering system they&#x27;ve used. B &gt; A doesn&#x27;t parse as easily as 5xxx &gt; 4xxx to me.</div><br/><div id="42309613" class="c"><input type="checkbox" id="c-42309613" checked=""/><div class="controls bullet"><span class="by">vesrah</span><span>|</span><a href="#42309551">parent</a><span>|</span><a href="#42311452">next</a><span>|</span><label class="collapse" for="c-42309613">[-]</label><label class="expand" for="c-42309613">[6 more]</label></div><br/><div class="children"><div class="content">They&#x27;re going in alphabetical order:
A - Alchemist
B - Battlemage
C - Celestial (Future gen)
D - Druid (Future gen)</div><br/><div id="42309713" class="c"><input type="checkbox" id="c-42309713" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#42309551">root</a><span>|</span><a href="#42309613">parent</a><span>|</span><a href="#42309727">next</a><span>|</span><label class="collapse" for="c-42309713">[-]</label><label class="expand" for="c-42309713">[1 more]</label></div><br/><div class="children"><div class="content">Hey we complained about all the numbers in their product names. Getting names from the D&amp;D PHB is… actually very cool, no complaints.</div><br/></div></div><div id="42309727" class="c"><input type="checkbox" id="c-42309727" checked=""/><div class="controls bullet"><span class="by">gs17</span><span>|</span><a href="#42309551">root</a><span>|</span><a href="#42309613">parent</a><span>|</span><a href="#42309713">prev</a><span>|</span><a href="#42311452">next</a><span>|</span><label class="collapse" for="c-42309727">[-]</label><label class="expand" for="c-42309727">[4 more]</label></div><br/><div class="children"><div class="content">Yes, I understand that. I&#x27;m saying it doesn&#x27;t read as easily IMO as (modern) NVIDIA&#x2F;AMD model numbers. Most numbers I deal with are base-10, not base-36.</div><br/><div id="42309937" class="c"><input type="checkbox" id="c-42309937" checked=""/><div class="controls bullet"><span class="by">BadHumans</span><span>|</span><a href="#42309551">root</a><span>|</span><a href="#42309727">parent</a><span>|</span><a href="#42311774">next</a><span>|</span><label class="collapse" for="c-42309937">[-]</label><label class="expand" for="c-42309937">[1 more]</label></div><br/><div class="children"><div class="content">The naming scheme they are using is easier to parse for me so all in the eye of the beholder.</div><br/></div></div><div id="42311774" class="c"><input type="checkbox" id="c-42311774" checked=""/><div class="controls bullet"><span class="by">Ekaros</span><span>|</span><a href="#42309551">root</a><span>|</span><a href="#42309727">parent</a><span>|</span><a href="#42309937">prev</a><span>|</span><a href="#42309940">next</a><span>|</span><label class="collapse" for="c-42311774">[-]</label><label class="expand" for="c-42311774">[1 more]</label></div><br/><div class="children"><div class="content">On other hand considering Geforce is 3rd loop of base 10 maybe it is not so bad...
Radeon is on other hand a pure absolute mess... Going back same 20 years.<p>I kinda like the idea of Intel.</div><br/></div></div><div id="42309940" class="c"><input type="checkbox" id="c-42309940" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#42309551">root</a><span>|</span><a href="#42309727">parent</a><span>|</span><a href="#42311774">prev</a><span>|</span><a href="#42311452">next</a><span>|</span><label class="collapse" for="c-42309940">[-]</label><label class="expand" for="c-42309940">[1 more]</label></div><br/><div class="children"><div class="content">You aren’t using excel or sheets I see?</div><br/></div></div></div></div></div></div></div></div><div id="42311452" class="c"><input type="checkbox" id="c-42311452" checked=""/><div class="controls bullet"><span class="by">tommica</span><span>|</span><a href="#42309551">prev</a><span>|</span><a href="#42310334">next</a><span>|</span><label class="collapse" for="c-42311452">[-]</label><label class="expand" for="c-42311452">[1 more]</label></div><br/><div class="children"><div class="content">Probably would jump to Intel once my 3060 gets too old</div><br/></div></div><div id="42310334" class="c"><input type="checkbox" id="c-42310334" checked=""/><div class="controls bullet"><span class="by">jvanderbot</span><span>|</span><a href="#42311452">prev</a><span>|</span><a href="#42309316">next</a><span>|</span><label class="collapse" for="c-42310334">[-]</label><label class="expand" for="c-42310334">[6 more]</label></div><br/><div class="children"><div class="content">Seems to feature ray tracing (kind of obvious), but also upscaling.<p>My experience on WH40K DT has taught me that upscaling is absolutely vital for a reasonable experience on some games.</div><br/><div id="42310435" class="c"><input type="checkbox" id="c-42310435" checked=""/><div class="controls bullet"><span class="by">1propionyl</span><span>|</span><a href="#42310334">parent</a><span>|</span><a href="#42309316">next</a><span>|</span><label class="collapse" for="c-42310435">[-]</label><label class="expand" for="c-42310435">[5 more]</label></div><br/><div class="children"><div class="content">&gt; upscaling is absolutely vital for a reasonable experience on some games<p>This strikes me as a bit of a sad state of affairs. We&#x27;ve moved beyond a Parkinson&#x27;s law of computational resources –usage by games expands to fill the available resources– to resource usage expanding to fill the available resources on the highest end machines unavailable for less than a few thousand dollars... and then using that to train a model to simulate by upscaling higher quality or performance on lower end machines.<p>A counterargument would be that this makes high-end experiences available to more people, and while in the individual case, I don&#x27;t buy that that&#x27;s where the incentives it creates are driving the entire industry.<p>To put a finer point on it: at what percentage of budget is too much money being spent on producing assets?</div><br/><div id="42310932" class="c"><input type="checkbox" id="c-42310932" checked=""/><div class="controls bullet"><span class="by">jvanderbot</span><span>|</span><a href="#42310334">root</a><span>|</span><a href="#42310435">parent</a><span>|</span><a href="#42309316">next</a><span>|</span><label class="collapse" for="c-42310932">[-]</label><label class="expand" for="c-42310932">[4 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t it insane to think that rendering triangles for the visuals in games has gotten so demanding that we need an artificially intelligent system embedded in our graphics cards to paint pixels that look like high definition geometry?<p>What a time to be alive. Our most advanced technology is used to cheat on homework and play video games.</div><br/><div id="42311783" class="c"><input type="checkbox" id="c-42311783" checked=""/><div class="controls bullet"><span class="by">jms55</span><span>|</span><a href="#42310334">root</a><span>|</span><a href="#42310932">parent</a><span>|</span><a href="#42311269">next</a><span>|</span><label class="collapse" for="c-42311783">[-]</label><label class="expand" for="c-42311783">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Isn&#x27;t it insane to think that rendering triangles for the visuals in games has gotten so demanding that we need an artificially intelligent system embedded in our graphics cards to paint pixels that look like high definition geometry?<p>That&#x27;s not _quite_ how temporal upscaling work in practice. It&#x27;s more of a blend between existing pixels, not generating entire pixels from scratch.<p>The technique has existed since before ML upscalers became common. It&#x27;s just turned out that ML is really good at determining how much to blend by each frame, compared to hand written and tweaked per-game heuristics.<p>---<p>For some history, DLSS 1 _did_ try and generate pixels entirely from scratch each frame. Needless to say, the quality was crap, and that was after a very expensive and time consuming process to train the model for each individual game  (and forget about using it as you develop the game; imagine having to retrain the AI model as you implement the graphics).<p>DLSS 2 moved to having the model predict blend weights fed into an existing TAAU pipeline, which is much more generalizable and has way better quality.</div><br/></div></div><div id="42311269" class="c"><input type="checkbox" id="c-42311269" checked=""/><div class="controls bullet"><span class="by">1propionyl</span><span>|</span><a href="#42310334">root</a><span>|</span><a href="#42310932">parent</a><span>|</span><a href="#42311783">prev</a><span>|</span><a href="#42312992">next</a><span>|</span><label class="collapse" for="c-42311269">[-]</label><label class="expand" for="c-42311269">[1 more]</label></div><br/><div class="children"><div class="content">It is. And it strikes me as evidence we&#x27;ve lost the plot and a measure has ceased to be a good measure upon being a target.<p>It used to be that more computational power was desirable because it would allow for developers to more fully realize creative visions that weren&#x27;t previously possible.<p>Now, it seems that the goal is simply visual fidelity and asset complexity... and the rest of the experience is not only secondary, but compromised in pursuit of the former.<p>Thinking back on recent games that felt like something <i>new</i> and painstakingly crafted... they&#x27;re almost all 2D (or look like it), lean on excellent art&#x2F;music (and even haptics!) direction, have a well-crafted core gameplay loop or set of systems, and have relatively low actual system requirements (which in turn means they are exceptionally smooth without any AI tricks).<p>Off the top of my head few years: Hades, Balatro, Animal Well, Cruelty Squad[0], Spelunky, Pizza Tower, Papers Please, etc. Most of these could just as easily have been made a decade ago.<p>That&#x27;s not to say we haven&#x27;t had many games that are gorgeous and fun. But while the latter is necessary and sufficient, the former is neither.<p>It&#x27;s just icing: it doesn&#x27;t matter if the cake tastes like crap.<p>[0] a mission statement if there ever was one for how much fun something can be while not just being ugly but being actively antagonistic to the senses and any notion of good taste.</div><br/></div></div></div></div></div></div></div></div><div id="42309316" class="c"><input type="checkbox" id="c-42309316" checked=""/><div class="controls bullet"><span class="by">mushufasa</span><span>|</span><a href="#42310334">prev</a><span>|</span><label class="collapse" for="c-42309316">[-]</label><label class="expand" for="c-42309316">[1 more]</label></div><br/><div class="children"><div class="content">what&#x27;s the current status of using cuda on non-gpu chips?<p>IIRC that was one of the original goals of geohot&#x27;s tinybox project, though I&#x27;m not sure exactly where that evolved</div><br/></div></div></div></div></div></div></div></body></html>