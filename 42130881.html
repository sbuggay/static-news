<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1731574862022" as="style"/><link rel="stylesheet" href="styles.css?v=1731574862022"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://developers.googleblog.com/en/farewell-and-thank-you-for-the-continued-partnership-francois-chollet/">Francois Chollet is leaving Google</a> <span class="domain">(<a href="https://developers.googleblog.com">developers.googleblog.com</a>)</span></div><div class="subtext"><span>xnx</span> | <span>86 comments</span></div><br/><div><div id="42133844" class="c"><input type="checkbox" id="c-42133844" checked=""/><div class="controls bullet"><span class="by">fchollet</span><span>|</span><a href="#42132316">next</a><span>|</span><label class="collapse" for="c-42133844">[-]</label><label class="expand" for="c-42133844">[18 more]</label></div><br/><div class="children"><div class="content">Hi HN, Francois here. Happy to answer any questions!<p>Here&#x27;s a start --<p>&quot;Did you get poached by Anthropic&#x2F;etc&quot;: No, I am starting a new company with a friend. We will announce more about it in due time!<p>&quot;Who uses Keras in production&quot;: Off the top of my head the current list includes Midjourney, YouTube, Waymo, Google across many products (even Ads started moving to Keras recently!), Netflix, Spotify, Snap, GrubHub, Square&#x2F;Block, X&#x2F;Twitter, and many non-tech companies like United, JPM, Orange, Walmart, etc. In total Keras has ~2M developers and powers ML at many companies big and small. This isn&#x27;t all TF -- many of our users have started running Keras on JAX or PyTorch.<p>&quot;Why did you decide to merge Keras into TensorFlow in 2019&quot;: I didn&#x27;t! The decision was made in 2018 by the TF leads -- I was a L5 IC at the time and that was an L8 decision. The TF team was huge at the time, 50+ people, while Keras was just me and the open-source community. In retrospect I think Keras would have been better off as an independent multi-backend framework -- but that would have required me quitting Google back then. Making Keras multi-backend again in 2023 has been one of my favorite projects to work on, both from the engineering &amp; architecture side of things but also because the product is truly great (also, I love JAX)!</div><br/><div id="42134014" class="c"><input type="checkbox" id="c-42134014" checked=""/><div class="controls bullet"><span class="by">satyanash</span><span>|</span><a href="#42133844">parent</a><span>|</span><a href="#42134240">next</a><span>|</span><label class="collapse" for="c-42134014">[-]</label><label class="expand" for="c-42134014">[5 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;Why did you decide to merge Keras into TensorFlow in 2019&quot;: I didn&#x27;t! The decision was made in 2018 by the TF leads -- I was a L5 IC at the time and that was an L8 decision. The TF team was huge at the time, 50+ people, while Keras was just me and the open-source community. In retrospect I think Keras would have been better off as an independent multi-backend framework -- but that would have required me quitting Google back then.<p>The fact that an &quot;L8&quot; at Google ranks above an OSS maintainer of a super-popular library &quot;L5&quot; is incredibly interesting. How are these levels determined? Doesn&#x27;t this represent a conflict of interest between the FOSS library and Google&#x27;s own motivations? The maintainer having to pick between a great paycheck or control of the library (with the impending possibility of Google forking).</div><br/><div id="42134156" class="c"><input type="checkbox" id="c-42134156" checked=""/><div class="controls bullet"><span class="by">fchollet</span><span>|</span><a href="#42133844">root</a><span>|</span><a href="#42134014">parent</a><span>|</span><a href="#42134144">next</a><span>|</span><label class="collapse" for="c-42134156">[-]</label><label class="expand" for="c-42134156">[1 more]</label></div><br/><div class="children"><div class="content">This is just the standard Google ladder. Your initial level when you join is based on your past experience. Then you gain levels by going through the infamous promo process. L8 represents the level of Director.<p>Yes, there are conflicts of interests inherent to the fact that OSS maintainers are usually employed by big tech companies (since OSS itself doesn&#x27;t make money). And it is often the case that big tech companies leverage their involvement in OSS development to further their own strategic interests and undermine their competitors, such as in the case of Meta, or to a lesser extent Google. But without the involvement of big tech companies, you would see a lot less open-source in the world. So you can view it as a trade off.</div><br/></div></div><div id="42134144" class="c"><input type="checkbox" id="c-42134144" checked=""/><div class="controls bullet"><span class="by">darkwizard42</span><span>|</span><a href="#42133844">root</a><span>|</span><a href="#42134014">parent</a><span>|</span><a href="#42134156">prev</a><span>|</span><a href="#42134187">next</a><span>|</span><label class="collapse" for="c-42134144">[-]</label><label class="expand" for="c-42134144">[2 more]</label></div><br/><div class="children"><div class="content">L8 at Google is not a random pecking order level. L8s generally have massive systems design experience and decades of software engineering experience at all levels of scale. They make decisions at Google which can have impacts on the workflows of 100s of engineers on products with 100millions&#x2F;billions of users. There are less L8s than there are technical VPs (excluding all the random biz side VP roles)<p>L5 here designates that they were a tenured (but not designated Senior) software engineer. It doesn&#x27;t meant they don&#x27;t have a voice in these discussions (very likely an L8 reached out to learn more about the issue, the options, and ideally considered Francois&#x27;s role and expertise before making a decision), it just means its above their pay grade.<p>I&#x27;ll let Francois provide more detail on the exact situation.</div><br/></div></div><div id="42134187" class="c"><input type="checkbox" id="c-42134187" checked=""/><div class="controls bullet"><span class="by">lrpahg</span><span>|</span><a href="#42133844">root</a><span>|</span><a href="#42134014">parent</a><span>|</span><a href="#42134144">prev</a><span>|</span><a href="#42134240">next</a><span>|</span><label class="collapse" for="c-42134187">[-]</label><label class="expand" for="c-42134187">[1 more]</label></div><br/><div class="children"><div class="content">&gt; How are these levels determined?<p>I have no knowledge of Google, but if L5 is the highest IC rank, then L8 will often be obtained through politics and playing the popularity game.<p>The U.S. corporate system is set up to humiliate and exploit real contributors. The demeaning term &quot;IC&quot; is a reflection of that. It is also applied when someone literally writes a whole application and the idle corporate masters stand by and take the credit.<p>Unfortunately, this is also how captured &quot;open&quot; source projects like Python work these days.</div><br/></div></div></div></div><div id="42134240" class="c"><input type="checkbox" id="c-42134240" checked=""/><div class="controls bullet"><span class="by">bootywizard</span><span>|</span><a href="#42133844">parent</a><span>|</span><a href="#42134014">prev</a><span>|</span><a href="#42134249">next</a><span>|</span><label class="collapse" for="c-42134240">[-]</label><label class="expand" for="c-42134240">[1 more]</label></div><br/><div class="children"><div class="content">Hi Francois, congrats on leaving Google!<p>ARC and On the Measure of Intelligence have both had a phenomenal impact on my thinking and understanding of the overall field.<p>Do you think that working on ARC is one of the most high leverage ways an individual can hope to have impact on the broad scientific goal of AGI?</div><br/></div></div><div id="42134249" class="c"><input type="checkbox" id="c-42134249" checked=""/><div class="controls bullet"><span class="by">schmorptron</span><span>|</span><a href="#42133844">parent</a><span>|</span><a href="#42134240">prev</a><span>|</span><a href="#42134074">next</a><span>|</span><label class="collapse" for="c-42134249">[-]</label><label class="expand" for="c-42134249">[1 more]</label></div><br/><div class="children"><div class="content">Hey,I really liked your little book of deep learning, even though I didn&#x27;t understand everything in it yet. Thanks for writing it!</div><br/></div></div><div id="42134074" class="c"><input type="checkbox" id="c-42134074" checked=""/><div class="controls bullet"><span class="by">dkga</span><span>|</span><a href="#42133844">parent</a><span>|</span><a href="#42134249">prev</a><span>|</span><a href="#42134092">next</a><span>|</span><label class="collapse" for="c-42134074">[-]</label><label class="expand" for="c-42134074">[2 more]</label></div><br/><div class="children"><div class="content">Hi François, just wanted to take the opportunity to tell you how much your work has been important for me. Both at the start, getting into deep learning (both keras and the book) and now with keras3 as I&#x27;m working to spread DL techniques in economics. The multi-backend is really a massive boon, as it also helps ensure that the API would remain both standardised and simple, which is very helpful to evangelise new users that are used to higher-level scripting languages as my crowd is.<p>In any case, I just want to say how much an inspiration the keras work has been and continues to be. Merci, François !</div><br/><div id="42134140" class="c"><input type="checkbox" id="c-42134140" checked=""/><div class="controls bullet"><span class="by">fchollet</span><span>|</span><a href="#42133844">root</a><span>|</span><a href="#42134074">parent</a><span>|</span><a href="#42134092">next</a><span>|</span><label class="collapse" for="c-42134140">[-]</label><label class="expand" for="c-42134140">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the kind words -- glad Keras has been useful!</div><br/></div></div></div></div><div id="42134092" class="c"><input type="checkbox" id="c-42134092" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#42133844">parent</a><span>|</span><a href="#42134074">prev</a><span>|</span><a href="#42134212">next</a><span>|</span><label class="collapse" for="c-42134092">[-]</label><label class="expand" for="c-42134092">[2 more]</label></div><br/><div class="children"><div class="content">Congrats, François, and good luck!<p>Q: The ARC Prize blog mentions that you plan to make ARC harder for machines and easier for humans. I&#x27;m curious if it will be adapted to resist scaling the training dataset (Like what BARC did -- see my other comment here)? As it stands today, I feel like the easiest approach to solving it would be BARC x10 or so, rather than algorithmic inventions.</div><br/><div id="42134128" class="c"><input type="checkbox" id="c-42134128" checked=""/><div class="controls bullet"><span class="by">fchollet</span><span>|</span><a href="#42133844">root</a><span>|</span><a href="#42134092">parent</a><span>|</span><a href="#42134212">next</a><span>|</span><label class="collapse" for="c-42134128">[-]</label><label class="expand" for="c-42134128">[1 more]</label></div><br/><div class="children"><div class="content">Right, one rather uninteresting line of approaches to ARC consists of trying to anticipate what might be in the test set, by generating millions of synthetic tasks. This can only work on relatively simple tasks, since the chance of task collision (between the test set and what you generate) is very low for any sophisticated task.<p>ARC 2 will improve on ARC 1 by making tasks less brute-forceable (both in the sense of making in harder to find the solution program by generating random programs built on a DSL, and in the sense of making it harder to guess the test tasks via brute force task generation). We&#x27;ll keep the human facing difficulty roughly constant, which will be controlled via human testing.</div><br/></div></div></div></div><div id="42134212" class="c"><input type="checkbox" id="c-42134212" checked=""/><div class="controls bullet"><span class="by">imfing</span><span>|</span><a href="#42133844">parent</a><span>|</span><a href="#42134092">prev</a><span>|</span><a href="#42134046">next</a><span>|</span><label class="collapse" for="c-42134212">[-]</label><label class="expand" for="c-42134212">[1 more]</label></div><br/><div class="children"><div class="content">just wanna take this chance to say a huge thank you for all the amazing work you’ve done with Keras!<p>back in 2017, Keras was my introductory framework to deep learning. it’s simple, Pythonic interface made finetuning models so much easier back then.<p>also glad to see Keras continue to thrive after getting merged into TF, especially with the new multi-backend support.<p>wishing you all the best in your new adventure!</div><br/></div></div><div id="42134046" class="c"><input type="checkbox" id="c-42134046" checked=""/><div class="controls bullet"><span class="by">openrisk</span><span>|</span><a href="#42133844">parent</a><span>|</span><a href="#42134212">prev</a><span>|</span><a href="#42133884">next</a><span>|</span><label class="collapse" for="c-42134046">[-]</label><label class="expand" for="c-42134046">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I was a L5 IC at the time and that was an L8 decision<p>omg, this sounds like the gigantic, ossified and crushing bureaucracy of a third world country.<p>It must be saying something profound about the human condition that such immense hierarchies are not just functioning but actually completely dominating the landscape.</div><br/></div></div><div id="42133884" class="c"><input type="checkbox" id="c-42133884" checked=""/><div class="controls bullet"><span class="by">hashtag-til</span><span>|</span><a href="#42133844">parent</a><span>|</span><a href="#42134046">prev</a><span>|</span><a href="#42133989">next</a><span>|</span><label class="collapse" for="c-42133884">[-]</label><label class="expand" for="c-42133884">[2 more]</label></div><br/><div class="children"><div class="content">Congratulations Francois! Thanks for maintaining Keras for such a long time and overcoming the corporate politics to get it where it is now.<p>I&#x27;ve been using it since early 2016 and it has been present all my career. It is something I use as the definitive example of how to do things right in the Python ecosystem.<p>Obviously, all the best wishes for you and your friend in the new venture!!</div><br/><div id="42134167" class="c"><input type="checkbox" id="c-42134167" checked=""/><div class="controls bullet"><span class="by">fchollet</span><span>|</span><a href="#42133844">root</a><span>|</span><a href="#42133884">parent</a><span>|</span><a href="#42133989">next</a><span>|</span><label class="collapse" for="c-42134167">[-]</label><label class="expand" for="c-42134167">[1 more]</label></div><br/><div class="children"><div class="content">Thank you!</div><br/></div></div></div></div><div id="42133989" class="c"><input type="checkbox" id="c-42133989" checked=""/><div class="controls bullet"><span class="by">blixt</span><span>|</span><a href="#42133844">parent</a><span>|</span><a href="#42133884">prev</a><span>|</span><a href="#42132316">next</a><span>|</span><label class="collapse" for="c-42133989">[-]</label><label class="expand" for="c-42133989">[2 more]</label></div><br/><div class="children"><div class="content">Will you come back to Europe?</div><br/><div id="42134035" class="c"><input type="checkbox" id="c-42134035" checked=""/><div class="controls bullet"><span class="by">fchollet</span><span>|</span><a href="#42133844">root</a><span>|</span><a href="#42133989">parent</a><span>|</span><a href="#42132316">next</a><span>|</span><label class="collapse" for="c-42134035">[-]</label><label class="expand" for="c-42134035">[1 more]</label></div><br/><div class="children"><div class="content">I will still be US-based for the time being. I&#x27;m seeing great things happening on the AI scene in Paris, though!</div><br/></div></div></div></div></div></div><div id="42132316" class="c"><input type="checkbox" id="c-42132316" checked=""/><div class="controls bullet"><span class="by">osm3000</span><span>|</span><a href="#42133844">prev</a><span>|</span><a href="#42133756">next</a><span>|</span><label class="collapse" for="c-42132316">[-]</label><label class="expand" for="c-42132316">[7 more]</label></div><br/><div class="children"><div class="content">I loved Keras at the beginning of my PhD, 2017. But it was just the wrong abstraction: too easy to start with, too difficult to create custom things (e.g., custom loss function).<p>I really tried to understand TensorFlow, I managed to make a for-loop in a week. Nested for-loop proved to be impossible.<p>PyTorch was just perfect out of the box. I don&#x27;t think I would have finished my PhD in time if it wasn&#x27;t for PyTorch.<p>I loved Keras. It was an important milestone, and it made me believe deep learning is feasible. It was just...not the final thing.</div><br/><div id="42132755" class="c"><input type="checkbox" id="c-42132755" checked=""/><div class="controls bullet"><span class="by">rd11235</span><span>|</span><a href="#42132316">parent</a><span>|</span><a href="#42133936">next</a><span>|</span><label class="collapse" for="c-42132755">[-]</label><label class="expand" for="c-42132755">[1 more]</label></div><br/><div class="children"><div class="content">&gt; it was just the wrong abstraction: too easy to start with, too difficult to create custom things<p>Couldn’t agree with this more. I was working on custom RNN variants at the time, and for that, Keras was handcuffs. Even raw TensorFlow was better for that purpose (which in turn still felt a bit like handcuffs after PyTorch was released).</div><br/></div></div><div id="42133936" class="c"><input type="checkbox" id="c-42133936" checked=""/><div class="controls bullet"><span class="by">fchollet</span><span>|</span><a href="#42132316">parent</a><span>|</span><a href="#42132755">prev</a><span>|</span><a href="#42132756">next</a><span>|</span><label class="collapse" for="c-42133936">[-]</label><label class="expand" for="c-42133936">[1 more]</label></div><br/><div class="children"><div class="content">Keras 1.0 in 2016-2017 was much less flexible than Keras 3 is now! Keras is designed around the principle of &quot;progressive disclosure of complexity&quot;: there are easy high-level workflows you can get started with, but you&#x27;re always able to open up any component of the workflow and customize it with your own code.<p>For instance: you have the built-in `fit()` to train a model. But you can customize the training logic (while retaining access to all `fit()` features, like callbacks, step fusion, async logging and async prefetching, distribution) by writing your own `compute_loss()` method. And further, you can customize gradient handling by writing a custom `train_step()` method (this is low-level enough that you have to do it with backend APIs like `tf.GradientTape` or torch `backward()`). E.g. <a href="https:&#x2F;&#x2F;keras.io&#x2F;guides&#x2F;custom_train_step_in_torch&#x2F;" rel="nofollow">https:&#x2F;&#x2F;keras.io&#x2F;guides&#x2F;custom_train_step_in_torch&#x2F;</a><p>Then, if you need even more control, you can just write your own training loop from scratch, etc. E.g. <a href="https:&#x2F;&#x2F;keras.io&#x2F;guides&#x2F;writing_a_custom_training_loop_in_jax&#x2F;" rel="nofollow">https:&#x2F;&#x2F;keras.io&#x2F;guides&#x2F;writing_a_custom_training_loop_in_ja...</a></div><br/></div></div><div id="42132756" class="c"><input type="checkbox" id="c-42132756" checked=""/><div class="controls bullet"><span class="by">hooloovoo_zoo</span><span>|</span><a href="#42132316">parent</a><span>|</span><a href="#42133936">prev</a><span>|</span><a href="#42133756">next</a><span>|</span><label class="collapse" for="c-42132756">[-]</label><label class="expand" for="c-42132756">[4 more]</label></div><br/><div class="children"><div class="content">Keras was a miracle coming from writing stuff in Theano back in the day though.</div><br/><div id="42133500" class="c"><input type="checkbox" id="c-42133500" checked=""/><div class="controls bullet"><span class="by">blaufuchs</span><span>|</span><a href="#42132316">root</a><span>|</span><a href="#42132756">parent</a><span>|</span><a href="#42133449">next</a><span>|</span><label class="collapse" for="c-42133500">[-]</label><label class="expand" for="c-42133500">[1 more]</label></div><br/><div class="children"><div class="content">Wow that gives me flashbacks to learning Theano&#x2F;Lasagne, which was a breath of fresh air coming from Caffe. Crazy how far we&#x27;ve come since then.</div><br/></div></div><div id="42133449" class="c"><input type="checkbox" id="c-42133449" checked=""/><div class="controls bullet"><span class="by">V1ndaar</span><span>|</span><a href="#42132316">root</a><span>|</span><a href="#42132756">parent</a><span>|</span><a href="#42133500">prev</a><span>|</span><a href="#42133797">next</a><span>|</span><label class="collapse" for="c-42133449">[-]</label><label class="expand" for="c-42133449">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t realize Keras was actually released before Tensorflow, huh. I used Theano quite a bit in 2014 and early 2015, but then went a couple years without any ML work. Compared to the modern libraries Theano is clunky, but it taught one a bit more about the models, heh.</div><br/></div></div><div id="42133797" class="c"><input type="checkbox" id="c-42133797" checked=""/><div class="controls bullet"><span class="by">singhrac</span><span>|</span><a href="#42132316">root</a><span>|</span><a href="#42132756">parent</a><span>|</span><a href="#42133449">prev</a><span>|</span><a href="#42133756">next</a><span>|</span><label class="collapse" for="c-42133797">[-]</label><label class="expand" for="c-42133797">[1 more]</label></div><br/><div class="children"><div class="content">And PyTorch was a miracle after coming from LuaTorch (or Torch7 iirc). We’ve made a lot of strides over the years.</div><br/></div></div></div></div></div></div><div id="42133756" class="c"><input type="checkbox" id="c-42133756" checked=""/><div class="controls bullet"><span class="by">tgma</span><span>|</span><a href="#42132316">prev</a><span>|</span><a href="#42131955">next</a><span>|</span><label class="collapse" for="c-42133756">[-]</label><label class="expand" for="c-42133756">[1 more]</label></div><br/><div class="children"><div class="content">Strange. Had never read blog posts about individual engineers leaving Google on official Google Developers Blog before. Is this a first? Every day someone prominent leaves Google... Sounds like a big self-own if Google starts to post this kind of stuff. Looks like sole post by either of the (both new to Google) authors in the byline.</div><br/></div></div><div id="42131955" class="c"><input type="checkbox" id="c-42131955" checked=""/><div class="controls bullet"><span class="by">geor9e</span><span>|</span><a href="#42133756">prev</a><span>|</span><a href="#42131302">next</a><span>|</span><label class="collapse" for="c-42131955">[-]</label><label class="expand" for="c-42131955">[17 more]</label></div><br/><div class="children"><div class="content">If I were to speculate, I would guess he quit Google. 2 days ago, his $1+ million Artificial General Intelligence competition ended. Chollet is now judging the submissions and will announce the winners in a few weeks. The timing there can&#x27;t be a coincidence.</div><br/><div id="42132119" class="c"><input type="checkbox" id="c-42132119" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#42131955">parent</a><span>|</span><a href="#42133232">next</a><span>|</span><label class="collapse" for="c-42132119">[-]</label><label class="expand" for="c-42132119">[7 more]</label></div><br/><div class="children"><div class="content">More generally, there is unlimited opportunity in the AI space today, especially for someone of his stature, and staying tied to Google probably isn&#x27;t as enticing. He can walk into any VC office and raise a hundred million dollars by the end of the day to build whatever he wants.</div><br/><div id="42132971" class="c"><input type="checkbox" id="c-42132971" checked=""/><div class="controls bullet"><span class="by">hiddencost</span><span>|</span><a href="#42131955">root</a><span>|</span><a href="#42132119">parent</a><span>|</span><a href="#42133388">next</a><span>|</span><label class="collapse" for="c-42132971">[-]</label><label class="expand" for="c-42132971">[5 more]</label></div><br/><div class="children"><div class="content">$100M isn&#x27;t enough capital for an AI startup that&#x27;s training foundation models, sadly.<p>A ton of folks of similar stature who raised that much burnt it within two years and took mediocre exits.</div><br/><div id="42133563" class="c"><input type="checkbox" id="c-42133563" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#42131955">root</a><span>|</span><a href="#42132971">parent</a><span>|</span><a href="#42133404">next</a><span>|</span><label class="collapse" for="c-42133563">[-]</label><label class="expand" for="c-42133563">[3 more]</label></div><br/><div class="children"><div class="content">Interesting, I think $100M is <i>totally</i> enough to train a SotA &quot;foundation model&quot;. It&#x27;s all in the use case. I&#x27;d love to hear explicit arguments against this.</div><br/><div id="42133765" class="c"><input type="checkbox" id="c-42133765" checked=""/><div class="controls bullet"><span class="by">hiddencost</span><span>|</span><a href="#42131955">root</a><span>|</span><a href="#42133563">parent</a><span>|</span><a href="#42133404">next</a><span>|</span><label class="collapse" for="c-42133765">[-]</label><label class="expand" for="c-42133765">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a bunch of failed AI companies who raised been $100M and $200M with the goal of training foundation models. What they discovered is that they were rapidly out paced by the large players, and didn&#x27;t have any way to generate revenue.<p>You&#x27;re right that it&#x27;s enough to train one, but IMO you&#x27;re wrong that it&#x27;s enough to build a company around.</div><br/><div id="42134050" class="c"><input type="checkbox" id="c-42134050" checked=""/><div class="controls bullet"><span class="by">AuryGlenz</span><span>|</span><a href="#42131955">root</a><span>|</span><a href="#42133765">parent</a><span>|</span><a href="#42133404">next</a><span>|</span><label class="collapse" for="c-42134050">[-]</label><label class="expand" for="c-42134050">[1 more]</label></div><br/><div class="children"><div class="content">I imagine Black Forest Labs (Flux) is doing alright, at least for now. I still feel like they’re missing out on some hanging fruit financially though.<p>But yeah, you’re not going to make any money making yet another LLM unless it’s somehow special.</div><br/></div></div></div></div></div></div><div id="42133404" class="c"><input type="checkbox" id="c-42133404" checked=""/><div class="controls bullet"><span class="by">NitpickLawyer</span><span>|</span><a href="#42131955">root</a><span>|</span><a href="#42132971">parent</a><span>|</span><a href="#42133563">prev</a><span>|</span><a href="#42133388">next</a><span>|</span><label class="collapse" for="c-42133404">[-]</label><label class="expand" for="c-42133404">[1 more]</label></div><br/><div class="children"><div class="content">I think we&#x27;ll start to see a differentiation soon. The likes of Ilya will raise money to do whatever, including foundation models &#x2F; new arch, while other startups will focus on post-training, scaling inference, domain adaptation and so on.<p>I don&#x27;t think the idea of general foundational model from scratch is a good path for startups anymore. We&#x27;re already seeing specialised verticals (cursor, codeium, both at ~100-200m funding rounds) and they&#x27;re both focused on specific domains, not generalist. There&#x27;s probably enough &quot;foundation&quot; models out there to start working on post-training stuff already, no need to reinvent the wheel.</div><br/></div></div></div></div><div id="42133388" class="c"><input type="checkbox" id="c-42133388" checked=""/><div class="controls bullet"><span class="by">dmafreezone</span><span>|</span><a href="#42131955">root</a><span>|</span><a href="#42132119">parent</a><span>|</span><a href="#42132971">prev</a><span>|</span><a href="#42133232">next</a><span>|</span><label class="collapse" for="c-42133388">[-]</label><label class="expand" for="c-42133388">[1 more]</label></div><br/><div class="children"><div class="content">Really? Unlimited opportunity? Or is it unlimited stupidity?<p><pre><code>    Two things are infinite, the universe and human stupidity, and I am not yet completely sure about the universe. - Einstein
</code></pre>
I have to concede here. For a grifter, what is stupidity if not opportunity?</div><br/></div></div></div></div><div id="42133232" class="c"><input type="checkbox" id="c-42133232" checked=""/><div class="controls bullet"><span class="by">crystal_revenge</span><span>|</span><a href="#42131955">parent</a><span>|</span><a href="#42132119">prev</a><span>|</span><a href="#42131302">next</a><span>|</span><label class="collapse" for="c-42133232">[-]</label><label class="expand" for="c-42133232">[9 more]</label></div><br/><div class="children"><div class="content">Google, in my experience, is a place where smart people go to retire. I have many brilliant friends who work there, but all of them have essentially stopped producing interesting work since the day they started. They all seem happy and comfortable, but not ambitious.<p>I&#x27;m sure the pay is great, but it&#x27;s not a place for smart people who are interested in <i>doing something</i>. I&#x27;ve followed Francois (and had the chance to correspond with him a bit) for many years now, and I wouldn&#x27;t be surprised if the desire to create something became more important than the comfort of Google.</div><br/><div id="42133656" class="c"><input type="checkbox" id="c-42133656" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#42131955">root</a><span>|</span><a href="#42133232">parent</a><span>|</span><a href="#42133269">next</a><span>|</span><label class="collapse" for="c-42133656">[-]</label><label class="expand" for="c-42133656">[3 more]</label></div><br/><div class="children"><div class="content">Am I almost alone in having no interest working for a large firm like Google?<p>I&#x27;ve been in tech since the 90s. The only reason I&#x27;d go is to network and build a team to do a mass exodus with and that&#x27;s literally it.<p>I don&#x27;t actually care about working on a product I have exactly zero executive control over.</div><br/><div id="42133769" class="c"><input type="checkbox" id="c-42133769" checked=""/><div class="controls bullet"><span class="by">Agingcoder</span><span>|</span><a href="#42131955">root</a><span>|</span><a href="#42133656">parent</a><span>|</span><a href="#42133269">next</a><span>|</span><label class="collapse" for="c-42133769">[-]</label><label class="expand" for="c-42133769">[2 more]</label></div><br/><div class="children"><div class="content">Why zero executive control ? I’d expect a company like google ( like most large orgs ) to have a very large amount of internal code for internal clients, sometimes developer themselves. My experience of large orgs tells me you can have control over what you build - it depends on who you’re building it for ( external or internal)</div><br/><div id="42133799" class="c"><input type="checkbox" id="c-42133799" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#42131955">root</a><span>|</span><a href="#42133769">parent</a><span>|</span><a href="#42133269">next</a><span>|</span><label class="collapse" for="c-42133799">[-]</label><label class="expand" for="c-42133799">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not what I mean. I&#x27;ve got a deep interest in how a product is used, fits in a market, designed, experienced AND built.<p>If I went to Google what I&#x27;d really want to do is gather up a bunch of people, rent out an away-from-Google office space and build say &quot;search-next&quot; - the response to the onslaught of entries currently successfully storming Google&#x27;s castle.<p>Do this completely detached and unmoored from Google&#x27;s existing product suite so that nobody can even tell it&#x27;s a Google product. They&#x27;ve been responding shockingly poorly and it&#x27;s time to make a discontinuous step.<p>And frankly I&#x27;d be more likely to waltz upon a winning lottery ticket than convincing Google execs this is necessary (and it absolutely is).</div><br/></div></div></div></div></div></div><div id="42133269" class="c"><input type="checkbox" id="c-42133269" checked=""/><div class="controls bullet"><span class="by">johnnyanmac</span><span>|</span><a href="#42131955">root</a><span>|</span><a href="#42133232">parent</a><span>|</span><a href="#42133656">prev</a><span>|</span><a href="#42133308">next</a><span>|</span><label class="collapse" for="c-42133269">[-]</label><label class="expand" for="c-42133269">[3 more]</label></div><br/><div class="children"><div class="content">I wonder how&#x2F;if that mentality will shift over time. As it seems the market capture phase it over and the current big tech aren&#x27;t simply keeping top talent around as a capture piece anymore.<p>Maybe they&#x27;ll still do it, but basically only if it feels you can startup a billion dollar business. As opposed to a million dollar one.</div><br/><div id="42133493" class="c"><input type="checkbox" id="c-42133493" checked=""/><div class="controls bullet"><span class="by">kortilla</span><span>|</span><a href="#42131955">root</a><span>|</span><a href="#42133269">parent</a><span>|</span><a href="#42133308">next</a><span>|</span><label class="collapse" for="c-42133493">[-]</label><label class="expand" for="c-42133493">[2 more]</label></div><br/><div class="children"><div class="content">Not really any different than what happened to IBM, Intel, Cisco, etc.<p>The people that want to build great things want the potential huge reward too, so they go to a startup to do it.</div><br/><div id="42133528" class="c"><input type="checkbox" id="c-42133528" checked=""/><div class="controls bullet"><span class="by">azinman2</span><span>|</span><a href="#42131955">root</a><span>|</span><a href="#42133493">parent</a><span>|</span><a href="#42133308">next</a><span>|</span><label class="collapse" for="c-42133528">[-]</label><label class="expand" for="c-42133528">[1 more]</label></div><br/><div class="children"><div class="content">Except… it’s about leverage&#x2F;impact factor. Google has very large impact, so if you do something big and central you’re instantly in the hands of hundreds of millions &#x2F; billions of people. That’s a very different situation than IBM or Cisco.</div><br/></div></div></div></div></div></div><div id="42133308" class="c"><input type="checkbox" id="c-42133308" checked=""/><div class="controls bullet"><span class="by">dmafreezone</span><span>|</span><a href="#42131955">root</a><span>|</span><a href="#42133232">parent</a><span>|</span><a href="#42133269">prev</a><span>|</span><a href="#42131302">next</a><span>|</span><label class="collapse" for="c-42133308">[-]</label><label class="expand" for="c-42133308">[2 more]</label></div><br/><div class="children"><div class="content">It’s the other way around. Working at Google (or any other FAANG) for a time period past your personal “bullshit limit” will ensure you will never do anything ambitious with your life ever again.</div><br/><div id="42133707" class="c"><input type="checkbox" id="c-42133707" checked=""/><div class="controls bullet"><span class="by">lazystar</span><span>|</span><a href="#42131955">root</a><span>|</span><a href="#42133308">parent</a><span>|</span><a href="#42131302">next</a><span>|</span><label class="collapse" for="c-42133707">[-]</label><label class="expand" for="c-42133707">[1 more]</label></div><br/><div class="children"><div class="content">ambitious?  man, i can barely pay rent and i work at a FAANG.</div><br/></div></div></div></div></div></div></div></div><div id="42131302" class="c"><input type="checkbox" id="c-42131302" checked=""/><div class="controls bullet"><span class="by">tadeegan</span><span>|</span><a href="#42131955">prev</a><span>|</span><a href="#42131340">next</a><span>|</span><label class="collapse" for="c-42131302">[-]</label><label class="expand" for="c-42131302">[5 more]</label></div><br/><div class="children"><div class="content">I guess they realized muilti-backend keras is futile? I never liked the tf.keras apis and the docs always promosed multi backend but then I guess they were never able to deliver that without breaking keras 3 changes. And even now.... &quot;Keras 3 includes a brand new distribution API, the keras.distribution namespace, currently implemented for the JAX backend (coming soon to the TensorFlow and PyTorch backends)&quot;. I don&#x27;t believe it. They are too different to reconcile under 1 api. And even if you could, I dont really see the benefit.  Torch and Flax have similar goals to Keras and are imo better.</div><br/><div id="42131787" class="c"><input type="checkbox" id="c-42131787" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#42131302">parent</a><span>|</span><a href="#42131553">next</a><span>|</span><label class="collapse" for="c-42131787">[-]</label><label class="expand" for="c-42131787">[1 more]</label></div><br/><div class="children"><div class="content">Multi-backend Keras was great the first time around and it might be a more widely used API today if the TF team hadn&#x27;t pulled that support and folded Keras into TF. I&#x27;m sure they had their reasons but I suspect that decision directly increased the adoption of PyTorch.</div><br/></div></div><div id="42131553" class="c"><input type="checkbox" id="c-42131553" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#42131302">parent</a><span>|</span><a href="#42131787">prev</a><span>|</span><a href="#42131340">next</a><span>|</span><label class="collapse" for="c-42131553">[-]</label><label class="expand" for="c-42131553">[3 more]</label></div><br/><div class="children"><div class="content">Why would you interpret this as Google disliking Keras? Seems a lot more likely he was poached by Anthropic.</div><br/><div id="42132173" class="c"><input type="checkbox" id="c-42132173" checked=""/><div class="controls bullet"><span class="by">blackeyeblitzar</span><span>|</span><a href="#42131302">root</a><span>|</span><a href="#42131553">parent</a><span>|</span><a href="#42131340">next</a><span>|</span><label class="collapse" for="c-42132173">[-]</label><label class="expand" for="c-42132173">[2 more]</label></div><br/><div class="children"><div class="content">Where did you see that he was poached by Anthropic?</div><br/><div id="42132463" class="c"><input type="checkbox" id="c-42132463" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#42131302">root</a><span>|</span><a href="#42132173">parent</a><span>|</span><a href="#42131340">next</a><span>|</span><label class="collapse" for="c-42132463">[-]</label><label class="expand" for="c-42132463">[1 more]</label></div><br/><div class="children"><div class="content">I am not suggesting that I know it for a fact. I do recall some speculation on X to that effect but I can&#x27;t find it now. Maybe just because Anthropic has been getting a lot of people lately.</div><br/></div></div></div></div></div></div></div></div><div id="42131340" class="c"><input type="checkbox" id="c-42131340" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#42131302">prev</a><span>|</span><a href="#42131616">next</a><span>|</span><label class="collapse" for="c-42131340">[-]</label><label class="expand" for="c-42131340">[11 more]</label></div><br/><div class="children"><div class="content">Genuine question: who is using Keras in production nowadays? I&#x27;ve done a few work projects in Keras&#x2F;TensorFlow over the years and it created a lot of technical debt and lost time debugging it, with said issues disappearing once I switched to PyTorch.<p>The training loop with Keras for simple model is indeed easier and faster than PyTorch oriented helpers (e.g. Lightning AI, Hugging Face accelerate) but much, much less flexible.</div><br/><div id="42133251" class="c"><input type="checkbox" id="c-42133251" checked=""/><div class="controls bullet"><span class="by">ic_fly2</span><span>|</span><a href="#42131340">parent</a><span>|</span><a href="#42131586">next</a><span>|</span><label class="collapse" for="c-42133251">[-]</label><label class="expand" for="c-42133251">[1 more]</label></div><br/><div class="children"><div class="content">We run a decent Keras model on production.<p>I don’t need a custom loss function, so keras is just fine.<p>From the article it sounds like Waymo run on Keras. Last I checked Waymo was doing better than the PyTorch powered Uber effort.</div><br/></div></div><div id="42131586" class="c"><input type="checkbox" id="c-42131586" checked=""/><div class="controls bullet"><span class="by">dools</span><span>|</span><a href="#42131340">parent</a><span>|</span><a href="#42133251">prev</a><span>|</span><a href="#42131775">next</a><span>|</span><label class="collapse" for="c-42131586">[-]</label><label class="expand" for="c-42131586">[3 more]</label></div><br/><div class="children"><div class="content">FTA &quot;With over two million users, Keras has become a cornerstone of AI development, streamlining complex workflows and democratizing access to cutting-edge technology. It powers numerous applications at Google and across the world, from the Waymo autonomous cars, to your daily YouTube, Netflix, and Spotify recommendations.&quot;</div><br/><div id="42131649" class="c"><input type="checkbox" id="c-42131649" checked=""/><div class="controls bullet"><span class="by">mistrial9</span><span>|</span><a href="#42131340">root</a><span>|</span><a href="#42131586">parent</a><span>|</span><a href="#42131775">next</a><span>|</span><label class="collapse" for="c-42131649">[-]</label><label class="expand" for="c-42131649">[2 more]</label></div><br/><div class="children"><div class="content">sure -- all true in 2018; right about then pyTorch passed TensforFlow in the raw numbers of research papers using it.. grad students later make products and product decisions.. currently, pyTorch is far more popular, the bulk of that is with LLMs<p>source: pyTorch Foundation, news</div><br/><div id="42132136" class="c"><input type="checkbox" id="c-42132136" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#42131340">root</a><span>|</span><a href="#42131649">parent</a><span>|</span><a href="#42131775">next</a><span>|</span><label class="collapse" for="c-42132136">[-]</label><label class="expand" for="c-42132136">[1 more]</label></div><br/><div class="children"><div class="content">The existence of a newer, hotter framework doesn&#x27;t mean all legacy applications in the world instantly switch to it. Quite the opposite in fact.</div><br/></div></div></div></div></div></div><div id="42131775" class="c"><input type="checkbox" id="c-42131775" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#42131340">parent</a><span>|</span><a href="#42131586">prev</a><span>|</span><a href="#42131616">next</a><span>|</span><label class="collapse" for="c-42131775">[-]</label><label class="expand" for="c-42131775">[6 more]</label></div><br/><div class="children"><div class="content">As someone who hasn&#x27;t really used either, what&#x27;s pytorch doing that&#x27;s so much better?</div><br/><div id="42131972" class="c"><input type="checkbox" id="c-42131972" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#42131340">root</a><span>|</span><a href="#42131775">parent</a><span>|</span><a href="#42131884">next</a><span>|</span><label class="collapse" for="c-42131972">[-]</label><label class="expand" for="c-42131972">[3 more]</label></div><br/><div class="children"><div class="content">A few things from personal experience:<p>- LLM support with PyTorch is better (both at a tooling level and CUDA level). Hugging Face transformers does have support for both TensorFlow and PyTorch variants of LLMs but...<p>- Almost all new LLMs are in PyTorch first and may or may not be ported to TensorFlow. This most notably includes embeddings models which are the most important area in my work.<p>- Keras&#x27;s training loop assumes you can fit all the data in memory and that the data is fully preprocessed, which in the world of LLMs and big data is infeasible. PyTorch has a DataLoader which can handle CPU&#x2F;GPU data movement and processing.<p>- PyTorch has better implementations for modern ML training improvments such as fp16, multi-GPU support, better native learning rate schedulers, etc. PyTorch can also override the training loop for very specific implementations (e.g. custom loss functions). Implementing them in TensorFlow&#x2F;Keras is a buggy pain.<p>- PyTorch was faster to train than TensorFlow models using the same hardware and model architecture.<p>- Keras&#x27;s serialization for model deployment is a pain in the butt (e.g. SavedModels) while PyTorch both has better implementations with torch.jit, and also native ONNX export.</div><br/><div id="42132254" class="c"><input type="checkbox" id="c-42132254" checked=""/><div class="controls bullet"><span class="by">perturbation</span><span>|</span><a href="#42131340">root</a><span>|</span><a href="#42131972">parent</a><span>|</span><a href="#42131884">next</a><span>|</span><label class="collapse" for="c-42132254">[-]</label><label class="expand" for="c-42132254">[2 more]</label></div><br/><div class="children"><div class="content">I think a lot of these may have improved since your last experience with Keras. It&#x27;s pretty easy to override the training loop and&#x2F;or make custom loss. The below is for overriding training &#x2F; test step altogether, custom loss is easier by making a new loss function&#x2F;class.<p><a href="https:&#x2F;&#x2F;keras.io&#x2F;examples&#x2F;keras_recipes&#x2F;trainer_pattern&#x2F;" rel="nofollow">https:&#x2F;&#x2F;keras.io&#x2F;examples&#x2F;keras_recipes&#x2F;trainer_pattern&#x2F;</a><p>&gt; - Keras&#x27;s training loop assumes you can fit all the data in memory and that the data is fully preprocessed, which in the world of LLMs and big data is infeasible.<p>The Tensorflow backend has the excellent tf.data.Dataset API, which allows for out of core data and processing in a streaming way.</div><br/><div id="42132363" class="c"><input type="checkbox" id="c-42132363" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#42131340">root</a><span>|</span><a href="#42132254">parent</a><span>|</span><a href="#42131884">next</a><span>|</span><label class="collapse" for="c-42132363">[-]</label><label class="expand" for="c-42132363">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a fair implementation of custom loss. Hugging Face&#x27;s Trainer with transformers suggests a similar implementation, although their&#x27;s has less boilerplate.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;main&#x2F;en&#x2F;trainer#customize-the-trainer" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;main&#x2F;en&#x2F;trainer#cus...</a></div><br/></div></div></div></div></div></div><div id="42131884" class="c"><input type="checkbox" id="c-42131884" checked=""/><div class="controls bullet"><span class="by">jwjohnson314</span><span>|</span><a href="#42131340">root</a><span>|</span><a href="#42131775">parent</a><span>|</span><a href="#42131972">prev</a><span>|</span><a href="#42133260">next</a><span>|</span><label class="collapse" for="c-42131884">[-]</label><label class="expand" for="c-42131884">[1 more]</label></div><br/><div class="children"><div class="content">PyTorch is just much more flexible. Implementing a custom loss function, for example, is straightforward in PyTorch and a hassle in Keras (or was last time I used it, which was several years ago).</div><br/></div></div><div id="42133260" class="c"><input type="checkbox" id="c-42133260" checked=""/><div class="controls bullet"><span class="by">adultSwim</span><span>|</span><a href="#42131340">root</a><span>|</span><a href="#42131775">parent</a><span>|</span><a href="#42131884">prev</a><span>|</span><a href="#42131616">next</a><span>|</span><label class="collapse" for="c-42133260">[-]</label><label class="expand" for="c-42133260">[1 more]</label></div><br/><div class="children"><div class="content">Being successful is also why it&#x27;s better. PyTorch has a thriving ecosystem of software around it and a large userbase. Picking it comes with many network benefits.</div><br/></div></div></div></div></div></div><div id="42131616" class="c"><input type="checkbox" id="c-42131616" checked=""/><div class="controls bullet"><span class="by">synergy20</span><span>|</span><a href="#42131340">prev</a><span>|</span><a href="#42133683">next</a><span>|</span><label class="collapse" for="c-42131616">[-]</label><label class="expand" for="c-42131616">[2 more]</label></div><br/><div class="children"><div class="content">I read somewhere TF will not be developed actively down the road, Google switched to JAX internally and TF pretty much lost the war to Pytorch.</div><br/><div id="42132240" class="c"><input type="checkbox" id="c-42132240" checked=""/><div class="controls bullet"><span class="by">sakex</span><span>|</span><a href="#42131616">parent</a><span>|</span><a href="#42133683">next</a><span>|</span><label class="collapse" for="c-42132240">[-]</label><label class="expand" for="c-42132240">[1 more]</label></div><br/><div class="children"><div class="content">Jax is really nice</div><br/></div></div></div></div><div id="42133683" class="c"><input type="checkbox" id="c-42133683" checked=""/><div class="controls bullet"><span class="by">uptownfunk</span><span>|</span><a href="#42131616">prev</a><span>|</span><a href="#42132304">next</a><span>|</span><label class="collapse" for="c-42133683">[-]</label><label class="expand" for="c-42133683">[1 more]</label></div><br/><div class="children"><div class="content">ARC is a bigger contribution than Keras. It’s great he had two major contributions. I can’t wait to see how they crack ARC</div><br/></div></div><div id="42132304" class="c"><input type="checkbox" id="c-42132304" checked=""/><div class="controls bullet"><span class="by">bearcollision</span><span>|</span><a href="#42133683">prev</a><span>|</span><a href="#42131308">next</a><span>|</span><label class="collapse" for="c-42132304">[-]</label><label class="expand" for="c-42132304">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve always wondered how fchollet had authority to force keras into TF...<p><a href="https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;community&#x2F;pull&#x2F;24">https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;community&#x2F;pull&#x2F;24</a></div><br/><div id="42132676" class="c"><input type="checkbox" id="c-42132676" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#42132304">parent</a><span>|</span><a href="#42133867">next</a><span>|</span><label class="collapse" for="c-42132676">[-]</label><label class="expand" for="c-42132676">[1 more]</label></div><br/><div class="children"><div class="content">I remember this post as the day that Keras died. Very strange political powerplay on the part of fchollet, and did immeasurable damage to the community and code that used TF, not just in that PR but also in the precedent it set for other stuff. People legitimately were upset by the attempt to move tensorflow under an unnecessary Keras namespace, and he locked the PR and said that Reddit was brigading it (despite it being pretty consistently disliked as a change, among other changes). People tried to reason with him in the PR thread, but to no avail, the Keras name had to live on, whether or not TF died with it (and it very well did, unfortunately). There were other things working against TF but this one seemed to be the final nail in the coffin, from what I can tell.<p>I ended up minimizing engagement with the work he&#x27;s done since as a result.</div><br/></div></div><div id="42133867" class="c"><input type="checkbox" id="c-42133867" checked=""/><div class="controls bullet"><span class="by">choppaface</span><span>|</span><a href="#42132304">parent</a><span>|</span><a href="#42132676">prev</a><span>|</span><a href="#42131308">next</a><span>|</span><label class="collapse" for="c-42133867">[-]</label><label class="expand" for="c-42133867">[1 more]</label></div><br/><div class="children"><div class="content">notably that link shows “@tensorflow tensorflow deleted a comment from fchollet on Nov 21, 2018” as well as other deleted comments</div><br/></div></div></div></div><div id="42131308" class="c"><input type="checkbox" id="c-42131308" checked=""/><div class="controls bullet"><span class="by">max_</span><span>|</span><a href="#42132304">prev</a><span>|</span><a href="#42131538">next</a><span>|</span><label class="collapse" for="c-42131308">[-]</label><label class="expand" for="c-42131308">[14 more]</label></div><br/><div class="children"><div class="content">I wonder what he will be working on?<p>Maybe he figured out a model that beats ARC-AGI by 85%?</div><br/><div id="42131784" class="c"><input type="checkbox" id="c-42131784" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#42131308">parent</a><span>|</span><a href="#42131538">next</a><span>|</span><label class="collapse" for="c-42131784">[-]</label><label class="expand" for="c-42131784">[13 more]</label></div><br/><div class="children"><div class="content">&gt; Maybe he figured out a model that beats ARC-AGI by 85%?<p>People have, I think.<p>One of the published approaches (BARC) uses GPT-4o to generate a <i>lot</i> more training data.<p>The approach is scaling really well so far [1], and whether you expect linear scaling or exponential one [2], the 85% threshold can be reached, using the &quot;transduction&quot; model alone, after generating under 2 million tasks ($20K in OpenAI credits).<p>Perhaps for 2025, the organizers will redesign ARC-AGI to be more resistant to this sort of approach, somehow.<p>---<p>[1] <a href="https:&#x2F;&#x2F;www.kaggle.com&#x2F;competitions&#x2F;arc-prize-2024&#x2F;discussion&#x2F;543953#3036186" rel="nofollow">https:&#x2F;&#x2F;www.kaggle.com&#x2F;competitions&#x2F;arc-prize-2024&#x2F;discussio...</a><p>[2] If you are &quot;throwing darts at a board&quot;, you get exponential scaling (the probability of not hitting bullseye at least once reduces exponentially with the number of throws). If you deliberately design your synthetic dataset to be non-redundant, you might get something akin to linear scaling (until you hit perfect accuracy, of course).</div><br/><div id="42132132" class="c"><input type="checkbox" id="c-42132132" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#42131308">root</a><span>|</span><a href="#42131784">parent</a><span>|</span><a href="#42131848">next</a><span>|</span><label class="collapse" for="c-42132132">[-]</label><label class="expand" for="c-42132132">[7 more]</label></div><br/><div class="children"><div class="content">I like the idea of ARC-AGI and think it was worth a shot. But if someone has already hit the human-level threshold, I think the entire idea can be thrown out.<p>If the ARC-AGI challenge did not actually follow their expected graph[1], I see no reason to believe that any benchmark can be designed in a way where it cannot be gamed. Rather, it seems that the existing SOTA models just weren&#x27;t well-optimized for that one task.<p>The only way to measure &quot;AGI&quot; is in however you define the &quot;G&quot;. If your model can only do one thing, it is not AGI and doesn&#x27;t really indicate you are closer, even if you very carefully designed your challenge.<p>[1] <a href="https:&#x2F;&#x2F;static.supernotes.app&#x2F;ai-benchmarks-2.png" rel="nofollow">https:&#x2F;&#x2F;static.supernotes.app&#x2F;ai-benchmarks-2.png</a></div><br/><div id="42132310" class="c"><input type="checkbox" id="c-42132310" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#42131308">root</a><span>|</span><a href="#42132132">parent</a><span>|</span><a href="#42132203">next</a><span>|</span><label class="collapse" for="c-42132310">[-]</label><label class="expand" for="c-42132310">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But if someone has already hit the human-level threshold<p>There is some controversy over what the human-level threshold is. A recent and very extensive study measured just 60.2% using Amazon Mechanical Turkers, for the same setup [1].<p>But the Turkers had no prior experience with the dataset, and were only given 5 tasks each.<p>Regardless, I believe ARC-AGI should aim for a higher threshold than what average humans achieve, because the ultimate goal of AGI is to supplement or replace high-IQ experts (who tend to do very well on ARC)<p>---<p>[1] Table 1 in <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2409.01374" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2409.01374</a> 2-shot Evaluation Set</div><br/></div></div><div id="42132203" class="c"><input type="checkbox" id="c-42132203" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#42131308">root</a><span>|</span><a href="#42132132">parent</a><span>|</span><a href="#42132310">prev</a><span>|</span><a href="#42132191">next</a><span>|</span><label class="collapse" for="c-42132203">[-]</label><label class="expand" for="c-42132203">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The only way to measure &quot;AGI&quot; is in however you define the &quot;G&quot;<p>&quot;I&quot; isn&#x27;t usefully defined either.<p>At least most people agree on &quot;Artificial&quot;</div><br/><div id="42133124" class="c"><input type="checkbox" id="c-42133124" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#42131308">root</a><span>|</span><a href="#42132203">parent</a><span>|</span><a href="#42132191">next</a><span>|</span><label class="collapse" for="c-42133124">[-]</label><label class="expand" for="c-42133124">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the problem with intelligence vs the other things we&#x27;re doing with deep learning.<p>Vision models, image models, video models, audio models? Solved. We&#x27;ve understood the physics of optics and audio for over half a century. We&#x27;ve had ray tracers for forever. It&#x27;s all well understood, and now we&#x27;re teaching models to understand it.<p>Intelligence? We can&#x27;t even describe our own.</div><br/></div></div></div></div><div id="42132191" class="c"><input type="checkbox" id="c-42132191" checked=""/><div class="controls bullet"><span class="by">TheDudeMan</span><span>|</span><a href="#42131308">root</a><span>|</span><a href="#42132132">parent</a><span>|</span><a href="#42132203">prev</a><span>|</span><a href="#42131848">next</a><span>|</span><label class="collapse" for="c-42132191">[-]</label><label class="expand" for="c-42132191">[3 more]</label></div><br/><div class="children"><div class="content">What you&#x27;re calling &quot;gamed&quot; could actually be research and progress in general problem solving.</div><br/><div id="42132596" class="c"><input type="checkbox" id="c-42132596" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#42131308">root</a><span>|</span><a href="#42132191">parent</a><span>|</span><a href="#42131848">next</a><span>|</span><label class="collapse" for="c-42132596">[-]</label><label class="expand" for="c-42132596">[2 more]</label></div><br/><div class="children"><div class="content">Almost by definition it is not. If you are &quot;gaming&quot; a specific benchmark, what you have is not progress in general intelligence. The entire premise of the ARC-AGI challenge was that general problem solving would be required. As noted by the GP, one of the top contenders is BARC which performs well by generating a huge amount of training data for this particular problem. That&#x27;s not general intelligence, that&#x27;s gaming.<p>There is no reason to believe that technique would not work for any particular problem. After all, this problem was the best attempt the (very intelligent) challenge designers could come up with, as evidenced by putting $1m on the line.</div><br/><div id="42132696" class="c"><input type="checkbox" id="c-42132696" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#42131308">root</a><span>|</span><a href="#42132596">parent</a><span>|</span><a href="#42131848">next</a><span>|</span><label class="collapse" for="c-42132696">[-]</label><label class="expand" for="c-42132696">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That&#x27;s not general intelligence, that&#x27;s gaming.<p>In fairness, their approach is non-trivial. Simply asking GPT-4o to fantasize more examples wouldn&#x27;t have worked very well. Instead, they have it fantasize inputs and programs, and then run the programs on the inputs to compute the outputs.<p>I think it&#x27;s a great contribution (although I&#x27;m surprised they didn&#x27;t try making an even bigger dataset -- perhaps they ran out of time or funding)</div><br/></div></div></div></div></div></div></div></div><div id="42131848" class="c"><input type="checkbox" id="c-42131848" checked=""/><div class="controls bullet"><span class="by">thrw42A8N</span><span>|</span><a href="#42131308">root</a><span>|</span><a href="#42131784">parent</a><span>|</span><a href="#42132132">prev</a><span>|</span><a href="#42132655">next</a><span>|</span><label class="collapse" for="c-42131848">[-]</label><label class="expand" for="c-42131848">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If you are &quot;throwing darts at a board&quot;, you get exponential scaling (the probability of not hitting bullseye reduces exponentially with the number of throws).<p>Honest question - is that so, and why? I thought you have to calculate the probability of each throw individually as nothing fundamentally connects the throws together, only that long term there will be a normal distribution of randomness.</div><br/><div id="42131877" class="c"><input type="checkbox" id="c-42131877" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#42131308">root</a><span>|</span><a href="#42131848">parent</a><span>|</span><a href="#42132655">next</a><span>|</span><label class="collapse" for="c-42131877">[-]</label><label class="expand" for="c-42131877">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The probability of not hitting bullseye <i>at least once</i> ...<p>I added a clarification.</div><br/></div></div></div></div><div id="42132655" class="c"><input type="checkbox" id="c-42132655" checked=""/><div class="controls bullet"><span class="by">TechDebtDevin</span><span>|</span><a href="#42131308">root</a><span>|</span><a href="#42131784">parent</a><span>|</span><a href="#42131848">prev</a><span>|</span><a href="#42132502">next</a><span>|</span><label class="collapse" for="c-42132655">[-]</label><label class="expand" for="c-42132655">[1 more]</label></div><br/><div class="children"><div class="content">I personally think ARC-AGI will be a forgotten, unimportant benchmark that doesn&#x27;t indicate anything more than a models ability reason, which honestly is just a very small step in the path towards AGI</div><br/></div></div><div id="42132502" class="c"><input type="checkbox" id="c-42132502" checked=""/><div class="controls bullet"><span class="by">mxwsn</span><span>|</span><a href="#42131308">root</a><span>|</span><a href="#42131784">parent</a><span>|</span><a href="#42132655">prev</a><span>|</span><a href="#42131538">next</a><span>|</span><label class="collapse" for="c-42132502">[-]</label><label class="expand" for="c-42132502">[2 more]</label></div><br/><div class="children"><div class="content">My interest was piqued, but the extrapolation in [1] is uh... not the most convincing. If there were more data points then sure, maybe</div><br/><div id="42132594" class="c"><input type="checkbox" id="c-42132594" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#42131308">root</a><span>|</span><a href="#42132502">parent</a><span>|</span><a href="#42131538">next</a><span>|</span><label class="collapse" for="c-42132594">[-]</label><label class="expand" for="c-42132594">[1 more]</label></div><br/><div class="children"><div class="content">The plot was just showing where the solid lines were trending (see prior messages), and that happened to predict the performance at 400k samples (red dot) very well.<p>An exponential scaling curve would steer a bit more to the right, but it would still cross the 85% mark before 2000k.</div><br/></div></div></div></div></div></div></div></div><div id="42133512" class="c"><input type="checkbox" id="c-42133512" checked=""/><div class="controls bullet"><span class="by">_giorgio_</span><span>|</span><a href="#42131364">prev</a><span>|</span><label class="collapse" for="c-42133512">[-]</label><label class="expand" for="c-42133512">[1 more]</label></div><br/><div class="children"><div class="content">Finally. He spent years doing nothing and sabotaging deep learning with an horrendous framework.</div><br/></div></div></div></div></div></div></div></body></html>