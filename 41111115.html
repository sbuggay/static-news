<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1722416514608" as="style"/><link rel="stylesheet" href="styles.css?v=1722416514608"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.stat.cmu.edu/~cshalizi/TALR/">The Truth About Linear Regression (2015)</a> <span class="domain">(<a href="https://www.stat.cmu.edu">www.stat.cmu.edu</a>)</span></div><div class="subtext"><span>sebg</span> | <span>64 comments</span></div><br/><div><div id="41111493" class="c"><input type="checkbox" id="c-41111493" checked=""/><div class="controls bullet"><span class="by">aquafox</span><span>|</span><a href="#41112749">next</a><span>|</span><label class="collapse" for="c-41111493">[-]</label><label class="expand" for="c-41111493">[34 more]</label></div><br/><div class="children"><div class="content">Most people don&#x27;t appreciate linear regression. 
1) All common statistical tests are linear models: <a href="https:&#x2F;&#x2F;lindeloev.github.io&#x2F;tests-as-linear&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lindeloev.github.io&#x2F;tests-as-linear&#x2F;</a>
2) Linear models are linear in the parameters, not the response! E.g. y = a*sin(x)+bx^2 is a linear model.
3) By choosing an appropriate spline basis, many non-linear relationships between the predictors and the response can be modelled by linear models.
4) And if that flexibility isn&#x27;t enough, by virtue of the Taylor Theorem, linear relations are often a good approximation of non-linear ones.</div><br/><div id="41112000" class="c"><input type="checkbox" id="c-41112000" checked=""/><div class="controls bullet"><span class="by">crystal_revenge</span><span>|</span><a href="#41111493">parent</a><span>|</span><a href="#41112171">next</a><span>|</span><label class="collapse" for="c-41112000">[-]</label><label class="expand" for="c-41112000">[18 more]</label></div><br/><div class="children"><div class="content">These are all fantastic points, and I strongly agree that most people don&#x27;t appreciate linear models nearly enough.<p>Another one I would add that is <i>very</i> important: Human beings, especially in groups, can only reasonably make <i>linear</i> decisions.<p>That is, when we are in a meeting making decisions for the direction of the company we can only say things like &quot;we need to increase ad spend, while reducing the other costs of acquisition such as discount vouchers&quot;. If you want to find the balance between &quot;increasing ad spend&quot; while &quot;decreasing other costs&quot; that&#x27;s a simple linear model.<p>Even if you have a great non-linear model, it&#x27;s not even a matter of &quot;interpretability&quot; so much as &quot;actionability&quot;. You can bring the results of a regression analysis to a meeting and very quickly model different strategies with reasonable directional confidence.<p>I struggled communicating actionable insights upward until I started to really understand regression analysis. After that it became amazingly simple to quickly crack open and understand fairly complex business processes.</div><br/><div id="41116898" class="c"><input type="checkbox" id="c-41116898" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41112000">parent</a><span>|</span><a href="#41112952">next</a><span>|</span><label class="collapse" for="c-41116898">[-]</label><label class="expand" for="c-41116898">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Another one I would add that is very important: Human beings, especially in groups, can only reasonably make linear decisions.<p>No, that&#x27;s not true.  Human groups are very able to make discrete decisions.  Actually, often they tend to go for discrete decisions, when something continuous (and perhaps linear) would be a lot better.<p>(Just to be clear: if you force your linear models to make discrete predictions, they are no longer linear in any sense of the word.  That&#x27;s why linear optimisation is a problem that can be solved in polynomial time, and integer linear optimisation is NP complete.<p>Even convex optimisation, which is no longer linear but still continuous, can be solved in roughly polynomial time.)<p>Often people demand more decisive decisions, of &#x27;yes&#x27;&#x2F;&#x27;no&#x27; or concrete action, not shades of grey and fiddling at the margins.<p>Getting people to even appreciate linear models is already a step forward.  Like it or not, your business strategy meetings are already a step ahead of what most people would naturally be inclined to.</div><br/><div id="41116925" class="c"><input type="checkbox" id="c-41116925" checked=""/><div class="controls bullet"><span class="by">wiz21c</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41116898">parent</a><span>|</span><a href="#41112952">next</a><span>|</span><label class="collapse" for="c-41116925">[-]</label><label class="expand" for="c-41116925">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Often people demand more decisive decisions, of &#x27;yes&#x27;&#x2F;&#x27;no&#x27; or concrete action, not shades of grey and fiddling at the margins.<p>And working with these people is so painful.</div><br/><div id="41116952" class="c"><input type="checkbox" id="c-41116952" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41116925">parent</a><span>|</span><a href="#41112952">next</a><span>|</span><label class="collapse" for="c-41116952">[-]</label><label class="expand" for="c-41116952">[1 more]</label></div><br/><div class="children"><div class="content">Yes.  I also found that in many cases being able to turn problems that require discrete decisions into problems that admit continuous decisions, eg by re-arranging how the business works etc, can unlock a lot of business value.<p>In my concrete cases I mostly saw that in the direct sense of being able to deploy more mathematics and operations research, eg for netting out (partially) offsetting financial instruments for a bank.<p>But by introspection you can come up with more example.  Eg that&#x27;s a common selling point for running your servers on AWS instead of building your own hardware.</div><br/></div></div></div></div></div></div><div id="41112952" class="c"><input type="checkbox" id="c-41112952" checked=""/><div class="controls bullet"><span class="by">madrox</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41112000">parent</a><span>|</span><a href="#41116898">prev</a><span>|</span><a href="#41112671">next</a><span>|</span><label class="collapse" for="c-41112952">[-]</label><label class="expand" for="c-41112952">[4 more]</label></div><br/><div class="children"><div class="content">I have a degree in statistics yet I&#x27;ve never thought about the relationship between linear models and business decisions in this way. You&#x27;re absolutely right. This is the best comment I&#x27;ve read all month.</div><br/><div id="41115544" class="c"><input type="checkbox" id="c-41115544" checked=""/><div class="controls bullet"><span class="by">highfrequency</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41112952">parent</a><span>|</span><a href="#41112671">next</a><span>|</span><label class="collapse" for="c-41115544">[-]</label><label class="expand" for="c-41115544">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t follow - could you explain this with a couple of examples? What would a business proposal look like that is analogous to a nonlinear model vs. one that is analogous to a linear model?</div><br/><div id="41115965" class="c"><input type="checkbox" id="c-41115965" checked=""/><div class="controls bullet"><span class="by">resonious</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41115544">parent</a><span>|</span><a href="#41112671">next</a><span>|</span><label class="collapse" for="c-41115965">[-]</label><label class="expand" for="c-41115965">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m also curious about what a non-actionable non-linear suggestion would look like.</div><br/><div id="41117081" class="c"><input type="checkbox" id="c-41117081" checked=""/><div class="controls bullet"><span class="by">317070</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41115965">parent</a><span>|</span><a href="#41112671">next</a><span>|</span><label class="collapse" for="c-41117081">[-]</label><label class="expand" for="c-41117081">[1 more]</label></div><br/><div class="children"><div class="content">How I understand the comment: a non-linear suggestion is that the budget for X should be 300k. The (supposedly linear) alternative is that the budget for X should increase.<p>What I think is the important part, is that it is better to ask decision makers for decisions on setting a continuous parameter, than to make binary yes&#x2F;no or go&#x2F;no-go decisions. When it&#x27;s a decision by committee, I can see why that is.</div><br/></div></div></div></div></div></div></div></div><div id="41112671" class="c"><input type="checkbox" id="c-41112671" checked=""/><div class="controls bullet"><span class="by">nextos</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41112000">parent</a><span>|</span><a href="#41112952">prev</a><span>|</span><a href="#41113185">next</a><span>|</span><label class="collapse" for="c-41112671">[-]</label><label class="expand" for="c-41112671">[5 more]</label></div><br/><div class="children"><div class="content">If you add a multilevel structure to shrink your (generalized) linear predictors, this framework becomes incredibly powerful.<p>There are entire statistics textbooks devoted to multilevel linear models, you can get really far with these.<p>Shrinking through information sharing is really important to avoid overly optimistic predictions in the case of little data.</div><br/><div id="41113226" class="c"><input type="checkbox" id="c-41113226" checked=""/><div class="controls bullet"><span class="by">aquafox</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41112671">parent</a><span>|</span><a href="#41114960">next</a><span>|</span><label class="collapse" for="c-41113226">[-]</label><label class="expand" for="c-41113226">[2 more]</label></div><br/><div class="children"><div class="content">If you like shrinkage (I do), I highly recommend the work of Matthew Stephens, e.g. ashr [1] and vash [2] for shrinkage based on an empirically derived prior.<p>[1] <a href="https:&#x2F;&#x2F;cran.r-project.org&#x2F;web&#x2F;packages&#x2F;ashr&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;cran.r-project.org&#x2F;web&#x2F;packages&#x2F;ashr&#x2F;index.html</a>
[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;mengyin&#x2F;vashr">https:&#x2F;&#x2F;github.com&#x2F;mengyin&#x2F;vashr</a></div><br/><div id="41113492" class="c"><input type="checkbox" id="c-41113492" checked=""/><div class="controls bullet"><span class="by">nextos</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41113226">parent</a><span>|</span><a href="#41114960">next</a><span>|</span><label class="collapse" for="c-41113492">[-]</label><label class="expand" for="c-41113492">[1 more]</label></div><br/><div class="children"><div class="content">Yes, the article linked to ashr is quite famous.</div><br/></div></div></div></div><div id="41114960" class="c"><input type="checkbox" id="c-41114960" checked=""/><div class="controls bullet"><span class="by">levocardia</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41112671">parent</a><span>|</span><a href="#41113226">prev</a><span>|</span><a href="#41113185">next</a><span>|</span><label class="collapse" for="c-41114960">[-]</label><label class="expand" for="c-41114960">[2 more]</label></div><br/><div class="children"><div class="content">Especially when you use the mixed model (aka MLM) framework to automatically select the smoothing penalty for your splines. So in one simple and very intuitive framework, you can estimate linear and nonlinear effects, account for repeated measurements and nested data, and model binary, count, or continuous outcomes (and more), all fitting the model in one shot, yielding statistically valid confidence intervals and p-values.<p>R&#x27;s mgcv package (which does all of the above) is probably the single reason I&#x27;m still using R as my primary stats language.</div><br/><div id="41116849" class="c"><input type="checkbox" id="c-41116849" checked=""/><div class="controls bullet"><span class="by">stevesimmons</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41114960">parent</a><span>|</span><a href="#41113185">next</a><span>|</span><label class="collapse" for="c-41116849">[-]</label><label class="expand" for="c-41116849">[1 more]</label></div><br/><div class="children"><div class="content">Is there a Python eqivalent?</div><br/></div></div></div></div></div></div><div id="41113185" class="c"><input type="checkbox" id="c-41113185" checked=""/><div class="controls bullet"><span class="by">addaon</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41112000">parent</a><span>|</span><a href="#41112671">prev</a><span>|</span><a href="#41115541">next</a><span>|</span><label class="collapse" for="c-41113185">[-]</label><label class="expand" for="c-41113185">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Human beings, especially in groups, can only reasonably make linear decisions.<p>There are absolutely decisions that need to get made, and do get made, that are not linear. Step functions are a great example. &quot;We need to decide if we are going to accept this acquisition offer&quot; is an example of a decision with step function utility. You can try to &quot;linearize&quot; it and then apply a threshold -- &quot;let&#x27;s agree on a model for the value at which we would accept an acquisition offer&quot; -- but in many ways that obscures that the utility function can be arbitrarily non-linear.</div><br/><div id="41113560" class="c"><input type="checkbox" id="c-41113560" checked=""/><div class="controls bullet"><span class="by">mturmon</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41113185">parent</a><span>|</span><a href="#41115541">next</a><span>|</span><label class="collapse" for="c-41113560">[-]</label><label class="expand" for="c-41113560">[2 more]</label></div><br/><div class="children"><div class="content">A single decision could still be easily modeled by a 0&#x2F;1 variable (as an input) and a real variable (as an output, like revenue for example).<p>That 0&#x2F;1 input variable could also have arbitrary interactions with other variables, which would also amount to “step function “ input effects.<p>See for example the autism&#x2F;age setup down thread.</div><br/><div id="41116902" class="c"><input type="checkbox" id="c-41116902" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41113560">parent</a><span>|</span><a href="#41115541">next</a><span>|</span><label class="collapse" for="c-41116902">[-]</label><label class="expand" for="c-41116902">[1 more]</label></div><br/><div class="children"><div class="content">Discrete linear optimisation is infinitely more complicated than continuous linear optimisation.  The former is NP complete, the latter is in P.</div><br/></div></div></div></div></div></div><div id="41115541" class="c"><input type="checkbox" id="c-41115541" checked=""/><div class="controls bullet"><span class="by">highfrequency</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41112000">parent</a><span>|</span><a href="#41113185">prev</a><span>|</span><a href="#41116441">next</a><span>|</span><label class="collapse" for="c-41115541">[-]</label><label class="expand" for="c-41115541">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t follow - could you explain this with a couple of examples? What would a business proposal look like that is analogous to a nonlinear model vs. one that is analogous to a linear model?</div><br/></div></div><div id="41116441" class="c"><input type="checkbox" id="c-41116441" checked=""/><div class="controls bullet"><span class="by">antwerp1</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41112000">parent</a><span>|</span><a href="#41115541">prev</a><span>|</span><a href="#41112171">next</a><span>|</span><label class="collapse" for="c-41116441">[-]</label><label class="expand" for="c-41116441">[1 more]</label></div><br/><div class="children"><div class="content">Quadratics might be more useful for optimizing (min&#x2F;max problems)..</div><br/></div></div></div></div><div id="41112171" class="c"><input type="checkbox" id="c-41112171" checked=""/><div class="controls bullet"><span class="by">borroka</span><span>|</span><a href="#41111493">parent</a><span>|</span><a href="#41112000">prev</a><span>|</span><a href="#41114239">next</a><span>|</span><label class="collapse" for="c-41112171">[-]</label><label class="expand" for="c-41112171">[4 more]</label></div><br/><div class="children"><div class="content">For point (3), in most of my academic research and work in industry, I have used Generalized Additive Models with great technical success (i.e., they fit the data well). Still, I have noticed that they have been rarely understood or given the proper appreciation by--it is a broad category--stakeholders. Out of laziness and habit, mostly.</div><br/><div id="41113023" class="c"><input type="checkbox" id="c-41113023" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41112171">parent</a><span>|</span><a href="#41114239">next</a><span>|</span><label class="collapse" for="c-41113023">[-]</label><label class="expand" for="c-41113023">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve looked at additive models, but I have so far shied away because I&#x27;ve read that they are not super equipped to deal with non-additive interactions.</div><br/><div id="41115008" class="c"><input type="checkbox" id="c-41115008" checked=""/><div class="controls bullet"><span class="by">levocardia</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41113023">parent</a><span>|</span><a href="#41114239">next</a><span>|</span><label class="collapse" for="c-41115008">[-]</label><label class="expand" for="c-41115008">[2 more]</label></div><br/><div class="children"><div class="content">They actually deal with non-additive &quot;low-order&quot; interactions quite well. In R&#x27;s mgcv for example, let&#x27;s say you had data from many years of temperature readings across a wide geographic area, so your data are (lat, long, year, temperature).  mgcv lets you fit a model like:<p><pre><code>  gam(temperature ~ te(long, lat) + s(year) + ti(long, lat,year))  
</code></pre>
where you have (1) a nonlinear two-way interaction (i.e. a smooth surface) across two spatial dimensions, (2) a univariate nonlinear effect of time, and (3) a three-way nonlinear interaction, i.e. &quot;does the pattern of temperature distributions shift over time?&quot;<p>You still can&#x27;t do arbitrary high-order interactions like you can get out of tree-based methods (xgboost &amp; friends) but that&#x27;s a small price to pay for valid confidence intervals and p-values. For example, the model above will give you a p-value for the ti() term, which you can use as formal statistical evidence to say -- at what level of confidence -- a spatiotemporal trend exists.<p>This Rmarkdown file (not rendered sadly) shows how to do this and other tricks <a href="https:&#x2F;&#x2F;github.com&#x2F;eric-pedersen&#x2F;mgcv-esa-workshop&#x2F;blob&#x2F;master&#x2F;example-spatio-temporal%20data.Rmd">https:&#x2F;&#x2F;github.com&#x2F;eric-pedersen&#x2F;mgcv-esa-workshop&#x2F;blob&#x2F;mast...</a></div><br/><div id="41116198" class="c"><input type="checkbox" id="c-41116198" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41115008">parent</a><span>|</span><a href="#41114239">next</a><span>|</span><label class="collapse" for="c-41116198">[-]</label><label class="expand" for="c-41116198">[1 more]</label></div><br/><div class="children"><div class="content">Hey cool. I&#x27;ll take a closer look then. Thanks! I assume that there are mixed model variants out there too.</div><br/></div></div></div></div></div></div></div></div><div id="41114239" class="c"><input type="checkbox" id="c-41114239" checked=""/><div class="controls bullet"><span class="by">waveBidder</span><span>|</span><a href="#41111493">parent</a><span>|</span><a href="#41112171">prev</a><span>|</span><a href="#41116985">next</a><span>|</span><label class="collapse" for="c-41114239">[-]</label><label class="expand" for="c-41114239">[2 more]</label></div><br/><div class="children"><div class="content">An SVM is purely a linear model from the right perspective, and if you&#x27;re being really reductive, RELU neural networks are piecewise linear. I think this may be obscuring more than it helps; picking the right transformation for your particular case is a highly nontrivial problem; why sin(x) and x^2, rather than, say, tanh(x) and x^(1&#x2F;2).</div><br/><div id="41116907" class="c"><input type="checkbox" id="c-41116907" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41114239">parent</a><span>|</span><a href="#41116985">next</a><span>|</span><label class="collapse" for="c-41116907">[-]</label><label class="expand" for="c-41116907">[1 more]</label></div><br/><div class="children"><div class="content">ReLU networks have the nice property of being piecewise linear, but also during training they optimise their own non-linear transformation over time.</div><br/></div></div></div></div><div id="41116985" class="c"><input type="checkbox" id="c-41116985" checked=""/><div class="controls bullet"><span class="by">usgroup</span><span>|</span><a href="#41111493">parent</a><span>|</span><a href="#41114239">prev</a><span>|</span><a href="#41113405">next</a><span>|</span><label class="collapse" for="c-41116985">[-]</label><label class="expand" for="c-41116985">[1 more]</label></div><br/><div class="children"><div class="content">Yeah but let’s not go crazy. Linear models perform very badly on partition-able tabular data where tree models excel. They are also obviously no replacement or competition in deep learning related tasks.<p>Point 3 — just pick the right basis — is very difficult outside a handful of kernels that are known to work. And how are you going to extrapolate your spline for prediction for example? Linearly is usually the answer…<p>Point 4 — sure for differentiable functions, but most people are fitting data not functions, and if you know it’s generating function why would you bother with a linear model?</div><br/></div></div><div id="41113405" class="c"><input type="checkbox" id="c-41113405" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#41111493">parent</a><span>|</span><a href="#41116985">prev</a><span>|</span><a href="#41114384">next</a><span>|</span><label class="collapse" for="c-41113405">[-]</label><label class="expand" for="c-41113405">[2 more]</label></div><br/><div class="children"><div class="content">Re. 2) Then you end up doing feature engineering. For applications where you don&#x27;t know the data generating process it is often better to just throw everything at the model let it extract the features.</div><br/><div id="41117158" class="c"><input type="checkbox" id="c-41117158" checked=""/><div class="controls bullet"><span class="by">benrutter</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41113405">parent</a><span>|</span><a href="#41114384">next</a><span>|</span><label class="collapse" for="c-41117158">[-]</label><label class="expand" for="c-41117158">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t disagree in the context of the current tools. But this has always been a bugbear of mine- data science has an unhealthy bias towards modeling over data preperation.<p>I&#x27;d love to see tools in the ecosystem around <i>extracting</i> relevant features that then can be used on a lower cost, more predictable model.</div><br/></div></div></div></div><div id="41114384" class="c"><input type="checkbox" id="c-41114384" checked=""/><div class="controls bullet"><span class="by">parpfish</span><span>|</span><a href="#41111493">parent</a><span>|</span><a href="#41113405">prev</a><span>|</span><a href="#41112951">next</a><span>|</span><label class="collapse" for="c-41114384">[-]</label><label class="expand" for="c-41114384">[4 more]</label></div><br/><div class="children"><div class="content">if you want to convert people into loving linear models (and you should), we need to make sure that they learn the difference between &#x27;linear models&#x27; and &#x27;linear models fit using OLS&#x27;<p>i&#x27;ve met smart people that cant wrap their head around how it&#x27;s possible to create linear model where the number of parameters exceeds the number of data points (that&#x27;s an OLS restriction).<p>or they&#x27;re worried about how they can apply their formula for calculating the std error on the parameters. bruh, it&#x27;s the future and we have big computers. just bootstrap em and don&#x27;t make any assumptions.</div><br/><div id="41115198" class="c"><input type="checkbox" id="c-41115198" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41114384">parent</a><span>|</span><a href="#41116106">next</a><span>|</span><label class="collapse" for="c-41115198">[-]</label><label class="expand" for="c-41115198">[1 more]</label></div><br/><div class="children"><div class="content">&gt; where the number of parameters exceeds the number of data points<p>Linear models have many solutions fitting the data exactly in that parameter regime, many more fitting it approximately for any metric still satisfying the idea that identical outputs are preferable, and sometimes multiple solutions even with more data.<p>So.....not just for OLS, but for most metrics (where you&#x27;d prefer to match or approximately match the data), the parameters are underconstrained.<p>How much that matters depends on lots of things. If you have additional constraints (a common one that&#x27;s particularly easy to program is looking for a minimum-norm solution), that trivially solves the problem. Otherwise, you might still have issues. E.g., non-minimum-norm solutions often perform badly on slightly out-of-distribution samples (since those extra basis vectors were unconstrained and thus might be large).<p>Is there something I&#x27;m missing where &#x27;linear models&#x27; are used to represent something wildly different than I&#x27;m used to? Are people using norms with discontinuities or something in practice? Is the criticism of OLS perhaps unrelated to the overparameterization issue? I think I&#x27;m missing some detail that would relate all of those.</div><br/></div></div><div id="41116106" class="c"><input type="checkbox" id="c-41116106" checked=""/><div class="controls bullet"><span class="by">tylerrobinson</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41114384">parent</a><span>|</span><a href="#41115198">prev</a><span>|</span><a href="#41112951">next</a><span>|</span><label class="collapse" for="c-41116106">[-]</label><label class="expand" for="c-41116106">[2 more]</label></div><br/><div class="children"><div class="content">Okay, I’ll bite.<p>&gt; If you want to convert people into loving linear models (and you should), we need to make sure that they learn the difference between &#x27;linear models&#x27; and &#x27;linear models fit using OLS&#x27;<p>Help me understand the pitch. What linear models are you referring to here that aren’t estimated with OLS? How should I wrap my head around having more parameters than observations?</div><br/><div id="41116195" class="c"><input type="checkbox" id="c-41116195" checked=""/><div class="controls bullet"><span class="by">solresol</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41116106">parent</a><span>|</span><a href="#41112951">next</a><span>|</span><label class="collapse" for="c-41116195">[-]</label><label class="expand" for="c-41116195">[1 more]</label></div><br/><div class="children"><div class="content">Linear models that aren&#x27;t estimated with OLS:
- Theil-Sen
- Huber
- RANSAC<p>Models that can cope with more parameters than observations:
- Ridge
- Lasso</div><br/></div></div></div></div></div></div><div id="41112951" class="c"><input type="checkbox" id="c-41112951" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#41111493">parent</a><span>|</span><a href="#41114384">prev</a><span>|</span><a href="#41112749">next</a><span>|</span><label class="collapse" for="c-41112951">[-]</label><label class="expand" for="c-41112951">[2 more]</label></div><br/><div class="children"><div class="content">Do you have a useful reference for &quot;3)&quot;?<p>A common problem I encounter in the literature is authors over-interpreting the slopes of a model with quadratic terms (e.g. Y = age + age^2) at the lowest and highest ages. In variably the plot (not the confidence intervals) will seem to indicate declines (for example) at the oldest ages (example:  random example off internet [1]), when really the apparent negative slope is due to the limitations of quadratic models not being able to model an asymptote.<p>The approach I&#x27;ve used, when I do not have a theoretically driven choice to work with) is using fractionated polynomials [2], e.g. x^s where s = {−2, −1, −0.5, 0, 0.5, 1, 2, 3}, and then picking a strategy to pick the best fitting polynomial while avoiding overfitting.<p>Its not a bad technique; I&#x27;ve tried others like piecewise polynomial regression, knots, etc [3],but I could not figure out how to test (for example) for a group interaction between two knotted splines). Also additive models.<p>[1] <a href="https:&#x2F;&#x2F;www.researchgate.net&#x2F;figure&#x2F;Scatter-plot-of-the-quadratic-relationship-of-body-weight-and-age-in-the-deep-litter_fig1_322702245" rel="nofollow">https:&#x2F;&#x2F;www.researchgate.net&#x2F;figure&#x2F;Scatter-plot-of-the-quad...</a>)
[2] <a href="https:&#x2F;&#x2F;journal.r-project.org&#x2F;articles&#x2F;RN-2005-017&#x2F;RN-2005-017.pdf" rel="nofollow">https:&#x2F;&#x2F;journal.r-project.org&#x2F;articles&#x2F;RN-2005-017&#x2F;RN-2005-0...</a>
[3] <a href="https:&#x2F;&#x2F;bookdown.org&#x2F;ssjackson300&#x2F;Machine-Learning-Lecture-Notes&#x2F;splines.html" rel="nofollow">https:&#x2F;&#x2F;bookdown.org&#x2F;ssjackson300&#x2F;Machine-Learning-Lecture-N...</a></div><br/><div id="41113157" class="c"><input type="checkbox" id="c-41113157" checked=""/><div class="controls bullet"><span class="by">aquafox</span><span>|</span><a href="#41111493">root</a><span>|</span><a href="#41112951">parent</a><span>|</span><a href="#41112749">next</a><span>|</span><label class="collapse" for="c-41113157">[-]</label><label class="expand" for="c-41113157">[1 more]</label></div><br/><div class="children"><div class="content">For my applications, using natural cubic splines provided by the &#x27;ns&#x27; function in R, combined with trying out where knots should be positioned, is sufficient. Maybe have a look at the gratia package [1] for plotting lots of diagnostics around spline fits.<p>[1] <a href="https:&#x2F;&#x2F;cran.r-project.org&#x2F;web&#x2F;packages&#x2F;gratia&#x2F;vignettes&#x2F;gratia.html" rel="nofollow">https:&#x2F;&#x2F;cran.r-project.org&#x2F;web&#x2F;packages&#x2F;gratia&#x2F;vignettes&#x2F;gra...</a></div><br/></div></div></div></div></div></div><div id="41112749" class="c"><input type="checkbox" id="c-41112749" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#41111493">prev</a><span>|</span><a href="#41111411">next</a><span>|</span><label class="collapse" for="c-41112749">[-]</label><label class="expand" for="c-41112749">[7 more]</label></div><br/><div class="children"><div class="content">When I was at CMU a decade ago I took 36-401 and 36-402 (then taught by Shalizi) and they were both very good statistical classes and they forced me to learn base R, for better or for worse.<p>A big weakness of linear regression that I had to learn the hard way is that the academic assumptions for valid interpretation of the coefficients are easy to construct for small educational datasets but rarely applicable to messy real world data.</div><br/><div id="41113585" class="c"><input type="checkbox" id="c-41113585" checked=""/><div class="controls bullet"><span class="by">aquafox</span><span>|</span><a href="#41112749">parent</a><span>|</span><a href="#41114421">next</a><span>|</span><label class="collapse" for="c-41113585">[-]</label><label class="expand" for="c-41113585">[1 more]</label></div><br/><div class="children"><div class="content">It depends. The most important assumption is independence of the observations. If that is not given, you have to either account for correlated responses using a mixed-effects model or mean-aggregate those responses (computing the mean decreases the variance but also reduces the number of data points and those two cancel each other out in calculating the t-statistic of the Wald test).<p>With regard to other assumptions, e.g. normality of the residuals, linear models can often deal with some degree of violation against those. But I agree that it&#x27;s always good to understand the influence of those violations, e.g. by using simulations and making p-value histograms of null-data.</div><br/></div></div><div id="41113676" class="c"><input type="checkbox" id="c-41113676" checked=""/><div class="controls bullet"><span class="by">bdjsiqoocwk</span><span>|</span><a href="#41112749">parent</a><span>|</span><a href="#41114421">prev</a><span>|</span><a href="#41111411">next</a><span>|</span><label class="collapse" for="c-41113676">[-]</label><label class="expand" for="c-41113676">[4 more]</label></div><br/><div class="children"><div class="content">The flip side is with messy real world data you just need a model that&#x27;s ok enough, rather than being concerned whether the p-value is this or that.</div><br/><div id="41114090" class="c"><input type="checkbox" id="c-41114090" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#41112749">root</a><span>|</span><a href="#41113676">parent</a><span>|</span><a href="#41111411">next</a><span>|</span><label class="collapse" for="c-41114090">[-]</label><label class="expand" for="c-41114090">[3 more]</label></div><br/><div class="children"><div class="content">At that point, if you don&#x27;t care about interpretable coefficients, you might as well use gradient-boosted trees or a full neural network instead.</div><br/><div id="41114658" class="c"><input type="checkbox" id="c-41114658" checked=""/><div class="controls bullet"><span class="by">borroka</span><span>|</span><a href="#41112749">root</a><span>|</span><a href="#41114090">parent</a><span>|</span><a href="#41111411">next</a><span>|</span><label class="collapse" for="c-41114658">[-]</label><label class="expand" for="c-41114658">[2 more]</label></div><br/><div class="children"><div class="content">It depends on the &quot;severity&quot; of the violation of assumptions--you can also use GAMs to add flexible nonlinear relationships--and the amount of data you are working with. Statistical modeling is a nuanced job.</div><br/><div id="41116467" class="c"><input type="checkbox" id="c-41116467" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#41112749">root</a><span>|</span><a href="#41114658">parent</a><span>|</span><a href="#41111411">next</a><span>|</span><label class="collapse" for="c-41116467">[-]</label><label class="expand" for="c-41116467">[1 more]</label></div><br/><div class="children"><div class="content">I tried to argue that while at CMU and it didn&#x27;t go well.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41111411" class="c"><input type="checkbox" id="c-41111411" checked=""/><div class="controls bullet"><span class="by">eachro</span><span>|</span><a href="#41112749">prev</a><span>|</span><a href="#41114292">next</a><span>|</span><label class="collapse" for="c-41111411">[-]</label><label class="expand" for="c-41111411">[8 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to see linear regression taught by say a quant researcher from Citadel. How do these guys use it? What do they particularly care about? Any theoretical results that meaningfully change the way they view problems? And so on.</div><br/><div id="41111906" class="c"><input type="checkbox" id="c-41111906" checked=""/><div class="controls bullet"><span class="by">mikaeluman</span><span>|</span><a href="#41111411">parent</a><span>|</span><a href="#41111981">next</a><span>|</span><label class="collapse" for="c-41111906">[-]</label><label class="expand" for="c-41111906">[6 more]</label></div><br/><div class="children"><div class="content">I have some experience. Variants of regularization are a must. There are just too few samples and too much noise per sample.<p>In a related problem, covariance matrix estimation, variants of shrinkage is popular. The most straight forward one being Linear Shrinkage (Ledoit, Wolf).<p>Excepting neural nets, I think most people doing regression simply use linear regression with above type touches based on the domain.<p>Particularly in finance you fool yourself too much with more complex models.</div><br/><div id="41112180" class="c"><input type="checkbox" id="c-41112180" checked=""/><div class="controls bullet"><span class="by">fasttriggerfish</span><span>|</span><a href="#41111411">root</a><span>|</span><a href="#41111906">parent</a><span>|</span><a href="#41112865">next</a><span>|</span><label class="collapse" for="c-41112180">[-]</label><label class="expand" for="c-41112180">[1 more]</label></div><br/><div class="children"><div class="content">Yes these are good points and probably the most important ones as far as the maths is concerned, though I would say regularisations methods are really standard things one learns in any ML &#x2F; stat course. 
Ledoit, Wolf shrinkage is indeed more exotic and very useful.</div><br/></div></div><div id="41112865" class="c"><input type="checkbox" id="c-41112865" checked=""/><div class="controls bullet"><span class="by">Ntrails</span><span>|</span><a href="#41111411">root</a><span>|</span><a href="#41111906">parent</a><span>|</span><a href="#41112180">prev</a><span>|</span><a href="#41111981">next</a><span>|</span><label class="collapse" for="c-41112865">[-]</label><label class="expand" for="c-41112865">[4 more]</label></div><br/><div class="children"><div class="content">&gt; There are just too few samples and too much noise per sample.<p>Call it 2000 liquid products on the US exchanges.  Many years of data. Even if you approximate it down from per tick to 1 minutely, that doesn&#x27;t feel like you&#x27;re struggling for a large in sample period?</div><br/><div id="41117055" class="c"><input type="checkbox" id="c-41117055" checked=""/><div class="controls bullet"><span class="by">kqr</span><span>|</span><a href="#41111411">root</a><span>|</span><a href="#41112865">parent</a><span>|</span><a href="#41113862">next</a><span>|</span><label class="collapse" for="c-41117055">[-]</label><label class="expand" for="c-41117055">[1 more]</label></div><br/><div class="children"><div class="content">It sounds like you are assuming the joint distribution of returns in the future is equal to that of the past, and assuming away potential time dependence.<p>These may be valid assumptions, but even if they are, &quot;sample size&quot; is always relative to between-sample unit variance, and that variance can be quite large for financial data. In some cases even infinite!<p>Regarding relativity of sample size, see e.g. this upcoming article: <a href="https:&#x2F;&#x2F;two-wrongs.com&#x2F;sample-unit-engineering" rel="nofollow">https:&#x2F;&#x2F;two-wrongs.com&#x2F;sample-unit-engineering</a></div><br/></div></div><div id="41113862" class="c"><input type="checkbox" id="c-41113862" checked=""/><div class="controls bullet"><span class="by">bormaj</span><span>|</span><a href="#41111411">root</a><span>|</span><a href="#41112865">parent</a><span>|</span><a href="#41117055">prev</a><span>|</span><a href="#41115229">next</a><span>|</span><label class="collapse" for="c-41113862">[-]</label><label class="expand" for="c-41113862">[1 more]</label></div><br/><div class="children"><div class="content">They may have been referring to (for example) reported financial results or news events which are more infrequent&#x2F;rare but may have outsized impact on market prices.</div><br/></div></div><div id="41115229" class="c"><input type="checkbox" id="c-41115229" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#41111411">root</a><span>|</span><a href="#41112865">parent</a><span>|</span><a href="#41113862">prev</a><span>|</span><a href="#41111981">next</a><span>|</span><label class="collapse" for="c-41115229">[-]</label><label class="expand" for="c-41115229">[1 more]</label></div><br/><div class="children"><div class="content">If the distribution changes enough, multiple years of data may as well be no data.</div><br/></div></div></div></div></div></div></div></div><div id="41114292" class="c"><input type="checkbox" id="c-41114292" checked=""/><div class="controls bullet"><span class="by">rkp8000</span><span>|</span><a href="#41111411">prev</a><span>|</span><a href="#41116953">next</a><span>|</span><label class="collapse" for="c-41114292">[-]</label><label class="expand" for="c-41114292">[1 more]</label></div><br/><div class="children"><div class="content">I love that Ridge Regression is introduced in the context of multicollinearity. It seems almost everyone these days learns about it as a regularization technique to prevent overfitting, but one of its fundamental use cases (and indeed its origin I believe) is in balancing weights among highly correlated (or nearly linearly dependent) predictors, which can cause huge problems even if you plenty of data.</div><br/></div></div><div id="41116953" class="c"><input type="checkbox" id="c-41116953" checked=""/><div class="controls bullet"><span class="by">usgroup</span><span>|</span><a href="#41114292">prev</a><span>|</span><a href="#41115914">next</a><span>|</span><label class="collapse" for="c-41116953">[-]</label><label class="expand" for="c-41116953">[1 more]</label></div><br/><div class="children"><div class="content">Also see Shalizi’s “Data Analysis from an Elementary Point of View” is a good introductory textbook:<p><a href="https:&#x2F;&#x2F;www.stat.cmu.edu&#x2F;~cshalizi&#x2F;ADAfaEPoV&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.stat.cmu.edu&#x2F;~cshalizi&#x2F;ADAfaEPoV&#x2F;</a><p>It is rightly over-weight on linear and additive models and simulation. 90% of the book is useless without a computer but that is a modern truth.</div><br/></div></div><div id="41115914" class="c"><input type="checkbox" id="c-41115914" checked=""/><div class="controls bullet"><span class="by">ackbar03</span><span>|</span><a href="#41116953">prev</a><span>|</span><a href="#41115924">next</a><span>|</span><label class="collapse" for="c-41115914">[-]</label><label class="expand" for="c-41115914">[1 more]</label></div><br/><div class="children"><div class="content">We had to revisit linear regression multiple times in different courses for my undergrad classes. It&#x27;s fascinating that optimality is provable using statistics and probability theory, although given assumptions hold of course.<p>For my cs phd I looked mostly at regression problems using deep learning models. I didn&#x27;t look at this specifically but I still think it would be neat if there is some way to translate the rigid proofs and theorems for classical linear models to deep regression models.</div><br/></div></div><div id="41115924" class="c"><input type="checkbox" id="c-41115924" checked=""/><div class="controls bullet"><span class="by">__mharrison__</span><span>|</span><a href="#41115914">prev</a><span>|</span><a href="#41112717">next</a><span>|</span><label class="collapse" for="c-41115924">[-]</label><label class="expand" for="c-41115924">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing. Add someone teaching regression (with XGBoost) this month, this is a good read. Very well written, and approachable, unlike many academic texts.<p>I particularly like chapter 6, visual diagnosis. Very well done.</div><br/></div></div><div id="41112717" class="c"><input type="checkbox" id="c-41112717" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#41115924">prev</a><span>|</span><a href="#41114166">next</a><span>|</span><label class="collapse" for="c-41112717">[-]</label><label class="expand" for="c-41112717">[4 more]</label></div><br/><div class="children"><div class="content">The most important skill in regression is to RECOGNIZE the intercept. It sounds trivial, and is, until you start including interactions between terms. The number of times I&#x27;ve found a young graduate student screw this up...<p>Take a simple linear model involving a test score, their age in years (age range 7-16 years), and a binary categorical variable autism diagnosis (0=control,1=autism):
score = age + diagnosis + age:diagnosis
score = (X1)age + (X2)diagnosis + (X3)age:diagnosis.<p>If the X2 is significant, the naive student would say, &quot;look a group difference!!&quot;, not realizing this is the predicted group difference at the intercept, which is when participants were 0 years old. [[
You center age by the mean, or median, or better yet, the age you are most interested in. Once interactions are in the equation, all &quot;lower order&quot; parameter estimates are in reference to the intercept.]]<p>They might also note a significant effect of age, and then assume it applies to both groups, but the parameter X1 only tells you what the predicted slope is for the reference group (controls), while the interaction tests if the age slopes differ between groups...moreover, even if the interaction isn&#x27;t significant, the age effect in the autism group might not significantly differ from zero...the data is in the wish washy zone, and you have to be careful in how one interprets the data.<p>To some here all this will seem obvious, but to many, getting their head firmly into the conditional space of parameters when their are interaction terms takes work. (note: for now I am ignoring other ways of coding groups (grand mean vs one group being the reference) but the lesson still remains. Understand what the intercept means and to whom&#x2F;what it refers.</div><br/><div id="41113689" class="c"><input type="checkbox" id="c-41113689" checked=""/><div class="controls bullet"><span class="by">aquafox</span><span>|</span><a href="#41112717">parent</a><span>|</span><a href="#41113415">next</a><span>|</span><label class="collapse" for="c-41113689">[-]</label><label class="expand" for="c-41113689">[1 more]</label></div><br/><div class="children"><div class="content">I always struggle to get a good intuition into models with interaction terms. I usually try to write down for every class of responses which terms of the model go into it and often that helps with interpretation. There&#x27;s also the ExploreModelMatrix [1] that helps with that task.<p>[1] <a href="https:&#x2F;&#x2F;www.bioconductor.org&#x2F;packages&#x2F;release&#x2F;bioc&#x2F;html&#x2F;ExploreModelMatrix.html" rel="nofollow">https:&#x2F;&#x2F;www.bioconductor.org&#x2F;packages&#x2F;release&#x2F;bioc&#x2F;html&#x2F;Expl...</a></div><br/></div></div><div id="41113415" class="c"><input type="checkbox" id="c-41113415" checked=""/><div class="controls bullet"><span class="by">mturmon</span><span>|</span><a href="#41112717">parent</a><span>|</span><a href="#41113689">prev</a><span>|</span><a href="#41113064">next</a><span>|</span><label class="collapse" for="c-41113415">[-]</label><label class="expand" for="c-41113415">[1 more]</label></div><br/><div class="children"><div class="content">I think this is accurate.<p>A significant loading on diagnosis (X2) does not tell you anything about the effect of diagnosis at any particular age (except age 0).<p>You’d have to recenter the model about the age of interest.</div><br/></div></div><div id="41113064" class="c"><input type="checkbox" id="c-41113064" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#41112717">parent</a><span>|</span><a href="#41113415">prev</a><span>|</span><a href="#41114166">next</a><span>|</span><label class="collapse" for="c-41113064">[-]</label><label class="expand" for="c-41113064">[1 more]</label></div><br/><div class="children"><div class="content">If I said something stupid above, please let me know. I&#x27;m always learning. If you are a strong Bayesian who doesn&#x27;t like p-values, that is also fine. I get it. I just wanted to provide my observations about a great number of bright students I&#x27;ve worked with who have nevertheless struggled to fluidly interpret models with interaction terms, and point them in the right direction.</div><br/></div></div></div></div><div id="41114166" class="c"><input type="checkbox" id="c-41114166" checked=""/><div class="controls bullet"><span class="by">g42gregory</span><span>|</span><a href="#41112717">prev</a><span>|</span><a href="#41112164">next</a><span>|</span><label class="collapse" for="c-41114166">[-]</label><label class="expand" for="c-41114166">[2 more]</label></div><br/><div class="children"><div class="content">It looks like this article does not mention it, but linear regression will also exhibit Double Descent phenomenon, commonly seen in deep learning. You would need to introduce some regularization, in order to see this. It would be nice to add this discussion.</div><br/><div id="41114295" class="c"><input type="checkbox" id="c-41114295" checked=""/><div class="controls bullet"><span class="by">gotoeleven</span><span>|</span><a href="#41114166">parent</a><span>|</span><a href="#41112164">next</a><span>|</span><label class="collapse" for="c-41114295">[-]</label><label class="expand" for="c-41114295">[1 more]</label></div><br/><div class="children"><div class="content">Are there some papers in particular that you&#x27;re referring to?  Does the second descent happen after the model becomes overparameterized, like with neural nets?  What kind of regularization?</div><br/></div></div></div></div><div id="41112164" class="c"><input type="checkbox" id="c-41112164" checked=""/><div class="controls bullet"><span class="by">yu3zhou4</span><span>|</span><a href="#41114166">prev</a><span>|</span><a href="#41114049">next</a><span>|</span><label class="collapse" for="c-41112164">[-]</label><label class="expand" for="c-41112164">[1 more]</label></div><br/><div class="children"><div class="content">This looks very interesting, do you know a way to transform this PDF to a mobile-optimized form?</div><br/></div></div><div id="41114208" class="c"><input type="checkbox" id="c-41114208" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#41114049">prev</a><span>|</span><label class="collapse" for="c-41114208">[-]</label><label class="expand" for="c-41114208">[1 more]</label></div><br/><div class="children"><div class="content">pfft, couldnt build an LLM with it</div><br/></div></div></div></div></div></div></div></body></html>