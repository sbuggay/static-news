<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1699693265330" as="style"/><link rel="stylesheet" href="styles.css?v=1699693265330"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://twitter.com/krea_ai/status/1723067313392320607">Real-time image editing using latent consistency models</a> <span class="domain">(<a href="https://twitter.com">twitter.com</a>)</span></div><div class="subtext"><span>dvrp</span> | <span>73 comments</span></div><br/><div><div id="38225467" class="c"><input type="checkbox" id="c-38225467" checked=""/><div class="controls bullet"><span class="by">kmavm</span><span>|</span><a href="#38224071">next</a><span>|</span><label class="collapse" for="c-38225467">[-]</label><label class="expand" for="c-38225467">[2 more]</label></div><br/><div class="children"><div class="content">(Disclaimer: I&#x27;m an investor in Krea AI.)<p>When Diego first showed me this animation, I wasn&#x27;t completely sure what I was looking at, because I assumed the left and right sides were like composited together or something. But it&#x27;s a unified screen recording; the right, generated side is keeping pace with the riffing the artist does in the little paint program on the left.<p>There is no substitute for low latency in creative tools; if you have to sit there holding your breath every time you try something, you aren&#x27;t just linearly slowed down. There are points that are just too hard to reach in slow, deliberate, 30+ second steps that a classical diffusion generation requires.<p>When I first heard about consistency, my assumption was that it was just an accelerator. I expected we&#x27;d get faster, cheaper versions of the same kinds of interactions with visual models we&#x27;re used to seeing. The fine hackers at Krea did not take long to prove me wrong!</div><br/><div id="38225596" class="c"><input type="checkbox" id="c-38225596" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38225467">parent</a><span>|</span><a href="#38224071">next</a><span>|</span><label class="collapse" for="c-38225596">[-]</label><label class="expand" for="c-38225596">[1 more]</label></div><br/><div class="children"><div class="content">Exactly.<p>There is no substitute for real-time when you&#x27;re doing creative work.<p>That&#x27;s why GitHub Copilot works so well; that&#x27;s why ChatGPT struck a chord with people—it streamed the characters back to you quite fast.<p>At first, I was skeptical too. I asked myself “what about Photoshop 1.0? They surely couldn&#x27;t do it in real-time.”. It turns out that even then you needed it. Of course, compute wasn&#x27;t there to do a simple translation of <i>all</i> rasterized pixel values that form an image within a layer, but there was a trick they did: they showed you the outline that would tell you, the user, where the content _will_ render if you let the mouse go.<p>You can see the workflow here:<p>&gt; <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ftaIzyrMDqE">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ftaIzyrMDqE</a><p>It applies to general tools too; you can see the same on this MacOS 8 demo (it runs on the browser!):<p>&gt; <a href="https:&#x2F;&#x2F;infinitemac.org&#x2F;1998&#x2F;Mac%20OS%208.1" rel="nofollow noreferrer">https:&#x2F;&#x2F;infinitemac.org&#x2F;1998&#x2F;Mac%20OS%208.1</a></div><br/></div></div></div></div><div id="38224071" class="c"><input type="checkbox" id="c-38224071" checked=""/><div class="controls bullet"><span class="by">tasgon</span><span>|</span><a href="#38225467">prev</a><span>|</span><a href="#38227552">next</a><span>|</span><label class="collapse" for="c-38224071">[-]</label><label class="expand" for="c-38224071">[6 more]</label></div><br/><div class="children"><div class="content">Disclaimer: I&#x27;m actively working on this tool.<p>This is in a closed beta for now (while we work on provisioning enough GPU compute) but we&#x27;re hoping to make this public later.</div><br/><div id="38226818" class="c"><input type="checkbox" id="c-38226818" checked=""/><div class="controls bullet"><span class="by">boppo1</span><span>|</span><a href="#38224071">parent</a><span>|</span><a href="#38226432">next</a><span>|</span><label class="collapse" for="c-38226818">[-]</label><label class="expand" for="c-38226818">[3 more]</label></div><br/><div class="children"><div class="content">Is the model designed to run locally or does it run in the cloud somewhere?<p>Is it your own model or is it based on sdxl or something like that?</div><br/><div id="38227304" class="c"><input type="checkbox" id="c-38227304" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38224071">root</a><span>|</span><a href="#38226818">parent</a><span>|</span><a href="#38226432">next</a><span>|</span><label class="collapse" for="c-38227304">[-]</label><label class="expand" for="c-38227304">[2 more]</label></div><br/><div class="children"><div class="content">cloud, A100, so yeah, beast.<p>we are doing tests, will keep you guys posted.</div><br/><div id="38228277" class="c"><input type="checkbox" id="c-38228277" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#38224071">root</a><span>|</span><a href="#38227304">parent</a><span>|</span><a href="#38226432">next</a><span>|</span><label class="collapse" for="c-38228277">[-]</label><label class="expand" for="c-38228277">[1 more]</label></div><br/><div class="children"><div class="content">Which cloud? I&#x27;ve been looking for somewhere to fine-tune Pixart, but EC2 et al. are usurious.</div><br/></div></div></div></div></div></div><div id="38226432" class="c"><input type="checkbox" id="c-38226432" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#38224071">parent</a><span>|</span><a href="#38226818">prev</a><span>|</span><a href="#38227552">next</a><span>|</span><label class="collapse" for="c-38226432">[-]</label><label class="expand" for="c-38226432">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, indeed. I know LCMs are way faster than diffusion models, but surely this demo is running on a beastly GPU...</div><br/><div id="38227305" class="c"><input type="checkbox" id="c-38227305" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38224071">root</a><span>|</span><a href="#38226432">parent</a><span>|</span><a href="#38227552">next</a><span>|</span><label class="collapse" for="c-38227305">[-]</label><label class="expand" for="c-38227305">[1 more]</label></div><br/><div class="children"><div class="content">yup, see comment above!</div><br/></div></div></div></div></div></div><div id="38227552" class="c"><input type="checkbox" id="c-38227552" checked=""/><div class="controls bullet"><span class="by">iandanforth</span><span>|</span><a href="#38224071">prev</a><span>|</span><a href="#38228171">next</a><span>|</span><label class="collapse" for="c-38227552">[-]</label><label class="expand" for="c-38227552">[6 more]</label></div><br/><div class="children"><div class="content">This is awesome. For future inputs please give me a 3d scene with camera controls into which I can drop primitive shapes, and human pose dolls so that I can frame shots like I would with a camera. I can&#x27;t draw so even figuring out how to suggest &quot;draw the image from the top left as the person looks over their shoulder&quot; to the model is something I struggle with.</div><br/><div id="38227574" class="c"><input type="checkbox" id="c-38227574" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38227552">parent</a><span>|</span><a href="#38227747">next</a><span>|</span><label class="collapse" for="c-38227574">[-]</label><label class="expand" for="c-38227574">[3 more]</label></div><br/><div class="children"><div class="content">We haven&#x27;t even deployed an open beta of this and you&#x27;re already asking for 3D?<p>I&#x27;ll never get tired of seeing the adaptation capabilities of human nature!<p><i>(But of course, we&#x27;d love to work on this!)</i></div><br/><div id="38228082" class="c"><input type="checkbox" id="c-38228082" checked=""/><div class="controls bullet"><span class="by">mungoman2</span><span>|</span><a href="#38227552">root</a><span>|</span><a href="#38227574">parent</a><span>|</span><a href="#38227747">next</a><span>|</span><label class="collapse" for="c-38228082">[-]</label><label class="expand" for="c-38228082">[2 more]</label></div><br/><div class="children"><div class="content">Come on, this is the wrong attitude. What they asked for could be prototyped in a day to see if it makes sense.
Have the left image in the demo be fed from a rendering from a simple 3d scene of a camera and simple colored primitives.</div><br/><div id="38228337" class="c"><input type="checkbox" id="c-38228337" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38227552">root</a><span>|</span><a href="#38228082">parent</a><span>|</span><a href="#38227747">next</a><span>|</span><label class="collapse" for="c-38228337">[-]</label><label class="expand" for="c-38228337">[1 more]</label></div><br/><div class="children"><div class="content">I was being sarcastic! I’m super excited too!</div><br/></div></div></div></div></div></div><div id="38227747" class="c"><input type="checkbox" id="c-38227747" checked=""/><div class="controls bullet"><span class="by">lovepronmostly</span><span>|</span><a href="#38227552">parent</a><span>|</span><a href="#38227574">prev</a><span>|</span><a href="#38228171">next</a><span>|</span><label class="collapse" for="c-38227747">[-]</label><label class="expand" for="c-38227747">[2 more]</label></div><br/><div class="children"><div class="content">Great idea! It takes a surprising amount of artist know how to illustrate actions using stick figures but probably less so with something like this<p><a href="https:&#x2F;&#x2F;www.ikea.com&#x2F;jp&#x2F;ja&#x2F;p&#x2F;gestalta-artists-dummy-natural-80257609&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.ikea.com&#x2F;jp&#x2F;ja&#x2F;p&#x2F;gestalta-artists-dummy-natural-...</a><p>Or maybe just pose yourself and upload a picture, trace it?</div><br/><div id="38228184" class="c"><input type="checkbox" id="c-38228184" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38227552">root</a><span>|</span><a href="#38227747">parent</a><span>|</span><a href="#38228171">next</a><span>|</span><label class="collapse" for="c-38228184">[-]</label><label class="expand" for="c-38228184">[1 more]</label></div><br/><div class="children"><div class="content">Yes. And what if I told you that you can prompt not just with basic shape but your custom assets? Or even better, a model that understands <i>your brand</i>.</div><br/></div></div></div></div></div></div><div id="38228171" class="c"><input type="checkbox" id="c-38228171" checked=""/><div class="controls bullet"><span class="by">vicentwu</span><span>|</span><a href="#38227552">prev</a><span>|</span><a href="#38225665">next</a><span>|</span><label class="collapse" for="c-38228171">[-]</label><label class="expand" for="c-38228171">[2 more]</label></div><br/><div class="children"><div class="content">Cool! The real-time feedback will have enormous ramifications on the art creation workflows.</div><br/><div id="38228174" class="c"><input type="checkbox" id="c-38228174" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38228171">parent</a><span>|</span><a href="#38225665">next</a><span>|</span><label class="collapse" for="c-38228174">[-]</label><label class="expand" for="c-38228174">[1 more]</label></div><br/><div class="children"><div class="content">Exactly! Everything will change.<p>And this is without taking into account WebGPU and other advances in adjacent fields.</div><br/></div></div></div></div><div id="38225665" class="c"><input type="checkbox" id="c-38225665" checked=""/><div class="controls bullet"><span class="by">vmatsiiako</span><span>|</span><a href="#38228171">prev</a><span>|</span><a href="#38227065">next</a><span>|</span><label class="collapse" for="c-38225665">[-]</label><label class="expand" for="c-38225665">[4 more]</label></div><br/><div class="children"><div class="content">Does this have to do anything at all with LoRAs?</div><br/><div id="38225719" class="c"><input type="checkbox" id="c-38225719" checked=""/><div class="controls bullet"><span class="by">vipermu</span><span>|</span><a href="#38225665">parent</a><span>|</span><a href="#38226012">next</a><span>|</span><label class="collapse" for="c-38225719">[-]</label><label class="expand" for="c-38225719">[1 more]</label></div><br/><div class="children"><div class="content">indeed; we&#x27;re able to make it work with SDXL thanks to a new technique that got released yesterday called LCM-LoRA.<p>with LCM-LoRA you can turn models like SDXL into LCMs without need for training and you can add other style LoRAs like the ones you find on civit.ai<p>in case you&#x27;re interested, here&#x27;s the technical report about LCM-LoRA: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.05556" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.05556</a></div><br/></div></div><div id="38226012" class="c"><input type="checkbox" id="c-38226012" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#38225665">parent</a><span>|</span><a href="#38225719">prev</a><span>|</span><a href="#38227065">next</a><span>|</span><label class="collapse" for="c-38226012">[-]</label><label class="expand" for="c-38226012">[2 more]</label></div><br/><div class="children"><div class="content">Yes, they are probably using: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;lcm_lora" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;lcm_lora</a></div><br/><div id="38226017" class="c"><input type="checkbox" id="c-38226017" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38225665">root</a><span>|</span><a href="#38226012">parent</a><span>|</span><a href="#38227065">next</a><span>|</span><label class="collapse" for="c-38226017">[-]</label><label class="expand" for="c-38226017">[1 more]</label></div><br/><div class="children"><div class="content">apolinario is a friend:)</div><br/></div></div></div></div></div></div><div id="38227065" class="c"><input type="checkbox" id="c-38227065" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#38225665">prev</a><span>|</span><a href="#38225420">next</a><span>|</span><label class="collapse" for="c-38227065">[-]</label><label class="expand" for="c-38227065">[4 more]</label></div><br/><div class="children"><div class="content">Nvidia had a solution like this several years ago. Something something studio.</div><br/><div id="38227206" class="c"><input type="checkbox" id="c-38227206" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38227065">parent</a><span>|</span><a href="#38225420">next</a><span>|</span><label class="collapse" for="c-38227206">[-]</label><label class="expand" for="c-38227206">[3 more]</label></div><br/><div class="children"><div class="content">that’s kind of different though</div><br/><div id="38227998" class="c"><input type="checkbox" id="c-38227998" checked=""/><div class="controls bullet"><span class="by">pandominium</span><span>|</span><a href="#38227065">root</a><span>|</span><a href="#38227206">parent</a><span>|</span><a href="#38225420">next</a><span>|</span><label class="collapse" for="c-38227998">[-]</label><label class="expand" for="c-38227998">[2 more]</label></div><br/><div class="children"><div class="content">Is it different in the underlying methods and models that are used?<p>This is the Nvidia tool: <a href="https:&#x2F;&#x2F;www.nvidia.com&#x2F;en-us&#x2F;studio&#x2F;canvas&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nvidia.com&#x2F;en-us&#x2F;studio&#x2F;canvas&#x2F;</a></div><br/><div id="38228460" class="c"><input type="checkbox" id="c-38228460" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38227065">root</a><span>|</span><a href="#38227998">parent</a><span>|</span><a href="#38225420">next</a><span>|</span><label class="collapse" for="c-38228460">[-]</label><label class="expand" for="c-38228460">[1 more]</label></div><br/><div class="children"><div class="content">Yup! It is different; that one is based on GAN models.<p>That&#x27;s why you can only use what they let you paint. In our demo, you can put whatever text you want.<p>And we want to make it so that you can assign a (custom) label to each shape. So you can type what each shape represents.</div><br/></div></div></div></div></div></div></div></div><div id="38225420" class="c"><input type="checkbox" id="c-38225420" checked=""/><div class="controls bullet"><span class="by">byrneml</span><span>|</span><a href="#38227065">prev</a><span>|</span><a href="#38223963">next</a><span>|</span><label class="collapse" for="c-38225420">[-]</label><label class="expand" for="c-38225420">[2 more]</label></div><br/><div class="children"><div class="content">Could you apply the same technique for real-time video editing?</div><br/><div id="38225773" class="c"><input type="checkbox" id="c-38225773" checked=""/><div class="controls bullet"><span class="by">vipermu</span><span>|</span><a href="#38225420">parent</a><span>|</span><a href="#38223963">next</a><span>|</span><label class="collapse" for="c-38225773">[-]</label><label class="expand" for="c-38225773">[1 more]</label></div><br/><div class="children"><div class="content">it would be interesting to play with LCM-LoRAs and AnimateDiff and see how much much this technique can speed up video generation.<p>not sure if it&#x27;s possible to just plug-and-play it or if we would need an extra LCM-LoRA for the motion module.<p>once we have these sort of models producing frames in milliseconds we should be able to do something similar to this demo but with videos.</div><br/></div></div></div></div><div id="38223963" class="c"><input type="checkbox" id="c-38223963" checked=""/><div class="controls bullet"><span class="by">joerambo808</span><span>|</span><a href="#38225420">prev</a><span>|</span><a href="#38225846">next</a><span>|</span><label class="collapse" for="c-38223963">[-]</label><label class="expand" for="c-38223963">[20 more]</label></div><br/><div class="children"><div class="content">why is it faster than latent diffusion models?</div><br/><div id="38223986" class="c"><input type="checkbox" id="c-38223986" checked=""/><div class="controls bullet"><span class="by">vipermu</span><span>|</span><a href="#38223963">parent</a><span>|</span><a href="#38225759">next</a><span>|</span><label class="collapse" for="c-38223986">[-]</label><label class="expand" for="c-38223986">[1 more]</label></div><br/><div class="children"><div class="content">it uses a new technique called &quot;consistency&quot; that lets latent diffusion models to predict images in much fewer steps.<p>some links here:
- <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.04378" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.04378</a>
- <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.05556" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.05556</a></div><br/></div></div><div id="38225759" class="c"><input type="checkbox" id="c-38225759" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#38223963">parent</a><span>|</span><a href="#38223986">prev</a><span>|</span><a href="#38223999">next</a><span>|</span><label class="collapse" for="c-38225759">[-]</label><label class="expand" for="c-38225759">[5 more]</label></div><br/><div class="children"><div class="content">The architecture is the same but it has an additional model trained on the sampling trajectory computation, speeding up the process. So instead of 20 iterations with the original sampler that solves a SDE, you could use 8, 4, or sometimes even 2 iterations with this new ML-based sampler. That&#x27;s not a new idea, but the author of this adapter figured out how to train it efficiently using very few GPU-hours.</div><br/><div id="38227156" class="c"><input type="checkbox" id="c-38227156" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38225759">parent</a><span>|</span><a href="#38223999">next</a><span>|</span><label class="collapse" for="c-38227156">[-]</label><label class="expand" for="c-38227156">[4 more]</label></div><br/><div class="children"><div class="content">I wonder if they could do a real-time preview using 2-4 iterations, and then &#x27;refine&#x27; the quality with a higher number of iterations over time, much like how 3D editors preview ray-tracing.</div><br/><div id="38227265" class="c"><input type="checkbox" id="c-38227265" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38227156">parent</a><span>|</span><a href="#38227458">next</a><span>|</span><label class="collapse" for="c-38227265">[-]</label><label class="expand" for="c-38227265">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve lost the link, but IIRC someone on twitter demoed exactly that in Blender. &quot;Neural rendering&quot; in a 3D editor has some nice properties - the scene can be rendered into separate layers with separate depth maps and openpose skeletons generated from the rigs to serve as input for controlnets, objects can be tagged and tags composed into the prompt for the particular tile in case of tiled rendering, etc.</div><br/><div id="38227461" class="c"><input type="checkbox" id="c-38227461" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38227265">parent</a><span>|</span><a href="#38227458">next</a><span>|</span><label class="collapse" for="c-38227461">[-]</label><label class="expand" for="c-38227461">[1 more]</label></div><br/><div class="children"><div class="content">probably not but are you talking about the guy who used control net and animatediff to make a video that looked relatively consistent?</div><br/></div></div></div></div><div id="38227458" class="c"><input type="checkbox" id="c-38227458" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38227156">parent</a><span>|</span><a href="#38227265">prev</a><span>|</span><a href="#38223999">next</a><span>|</span><label class="collapse" for="c-38227458">[-]</label><label class="expand" for="c-38227458">[1 more]</label></div><br/><div class="children"><div class="content">we’re thinking of doing that as well!<p>but first we want to make sure we can get this in the hands of a tight cohort of creatives and see how they use it.</div><br/></div></div></div></div></div></div><div id="38223999" class="c"><input type="checkbox" id="c-38223999" checked=""/><div class="controls bullet"><span class="by">billconan</span><span>|</span><a href="#38223963">parent</a><span>|</span><a href="#38225759">prev</a><span>|</span><a href="#38225846">next</a><span>|</span><label class="collapse" for="c-38223999">[-]</label><label class="expand" for="c-38223999">[13 more]</label></div><br/><div class="children"><div class="content">latent diffusion is an iterative process, the image becomes clearer one step at a time.<p>The process can be viewed as a particle moving in the image space, one step at a time to its final position in the image space, which is the generated image.<p>consistency model tries to predict the movement trajectory by providing the current position in the image space. Hence, what used to be a step-by-step process becomes a one-step process.</div><br/><div id="38226883" class="c"><input type="checkbox" id="c-38226883" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38223999">parent</a><span>|</span><a href="#38225205">next</a><span>|</span><label class="collapse" for="c-38226883">[-]</label><label class="expand" for="c-38226883">[3 more]</label></div><br/><div class="children"><div class="content">Would that motion vector have to include like a color delta?</div><br/><div id="38227781" class="c"><input type="checkbox" id="c-38227781" checked=""/><div class="controls bullet"><span class="by">billconan</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38226883">parent</a><span>|</span><a href="#38225205">next</a><span>|</span><label class="collapse" for="c-38227781">[-]</label><label class="expand" for="c-38227781">[2 more]</label></div><br/><div class="children"><div class="content">if you are talking about latent diffusion, No, the &quot;particle&quot; is in a hyper-dimensional space, like for example a 10k-dimensional space. We are not supposed to interpret the meaning of that vector.<p>and when that particle has moved to the right location, there is a decoder that converts it into an image. The decoder network knows how to interpret it.</div><br/><div id="38228219" class="c"><input type="checkbox" id="c-38228219" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38227781">parent</a><span>|</span><a href="#38225205">next</a><span>|</span><label class="collapse" for="c-38228219">[-]</label><label class="expand" for="c-38228219">[1 more]</label></div><br/><div class="children"><div class="content">Exactly.<p>I actually wrote a micro essay on Twitter the other day about the meaning of the classic encoder decoder network. It’s beautiful.<p>But yeah!<p>-<p>For reference: <a href="https:&#x2F;&#x2F;x.com&#x2F;asciidiego&#x2F;status&#x2F;1722544108252836119" rel="nofollow noreferrer">https:&#x2F;&#x2F;x.com&#x2F;asciidiego&#x2F;status&#x2F;1722544108252836119</a></div><br/></div></div></div></div></div></div><div id="38225205" class="c"><input type="checkbox" id="c-38225205" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38223999">parent</a><span>|</span><a href="#38226883">prev</a><span>|</span><a href="#38225846">next</a><span>|</span><label class="collapse" for="c-38225205">[-]</label><label class="expand" for="c-38225205">[9 more]</label></div><br/><div class="children"><div class="content">oh wow, never thought of it that way</div><br/><div id="38225407" class="c"><input type="checkbox" id="c-38225407" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38225205">parent</a><span>|</span><a href="#38225846">next</a><span>|</span><label class="collapse" for="c-38225407">[-]</label><label class="expand" for="c-38225407">[8 more]</label></div><br/><div class="children"><div class="content">&gt; consistency model tries to predict the movement trajectory by providing the current position in the image space. Hence, what used to be a step-by-step process becomes a one-step process.<p>no that wasnt a sufficient explanation for me. what is the prediction method here? why was diffusion necessary in the past? what tradeoffs does this approach have?</div><br/><div id="38225716" class="c"><input type="checkbox" id="c-38225716" checked=""/><div class="controls bullet"><span class="by">billconan</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38225407">parent</a><span>|</span><a href="#38227070">next</a><span>|</span><label class="collapse" for="c-38225716">[-]</label><label class="expand" for="c-38225716">[2 more]</label></div><br/><div class="children"><div class="content">to my defense, if you look at the original paper<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2303.01469.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2303.01469.pdf</a><p>the exact neural network used for the prediction method is omitted. apparently many neural networks can be used for this prediction method as long as they fulfill certain requirements.<p>&gt; why was diffusion necessary in the past?<p>in the paper, one way to train a consistency model is distilling an existing diffusion model. But it can be trained from scratch too.<p>&quot;why was it necessary in the past &quot; doesn&#x27;t bother me that much. Before people know to use molding to make candles, they did it by dipping threads into wax. Why was thread dipping necessary? it&#x27;s just a stepping stone of technology development.</div><br/><div id="38225754" class="c"><input type="checkbox" id="c-38225754" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38225716">parent</a><span>|</span><a href="#38227070">next</a><span>|</span><label class="collapse" for="c-38225754">[-]</label><label class="expand" for="c-38225754">[1 more]</label></div><br/><div class="children"><div class="content">Exactly.<p>The stepping stone way of seeing things reminds me a lot to the thesis behind the book “ Why Greatness Cannot Be Planned: The Myth of the Objective” (2015).</div><br/></div></div></div></div><div id="38227070" class="c"><input type="checkbox" id="c-38227070" checked=""/><div class="controls bullet"><span class="by">ativzzz</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38225407">parent</a><span>|</span><a href="#38225716">prev</a><span>|</span><a href="#38225673">next</a><span>|</span><label class="collapse" for="c-38227070">[-]</label><label class="expand" for="c-38227070">[2 more]</label></div><br/><div class="children"><div class="content">&gt; what tradeoffs does this approach have?<p>From playing around with it a bit locally, LCM is much, much faster, but generally the detail is much lower using the latest SDXL model.<p>If your prompt is very simple, such as &quot;a boy looking at the moon in a forest&quot; it does pretty well. If your prompt is much more complex and asks for a lot more detail or uses other LoRas, it doesn&#x27;t do nearly as well as other samplers and generates lower quality, worse matching images. These other samplers take 30-40 steps so it&#x27;s several times slower.<p>From what I&#x27;ve seen though, if you use control net, or passing some guideline images in and rely on a simple prompt, like an existing video that you&#x27;re trying to change the style of, LCM can generate images in near real time like the OP on an RTX4090 and maybe slower cards on smaller&#x2F;older models<p>Another benefit is the decreased experimental time. So you can more quickly iterate over seeds to find output you like, and then maybe you can spend some time with other samplers&#x2F;upscalers with that seed to make the result higher quality</div><br/><div id="38227426" class="c"><input type="checkbox" id="c-38227426" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38227070">parent</a><span>|</span><a href="#38225673">next</a><span>|</span><label class="collapse" for="c-38227426">[-]</label><label class="expand" for="c-38227426">[1 more]</label></div><br/><div class="children"><div class="content">exactly. and there’s more things that you can do in terms of what you use as input for the NN!</div><br/></div></div></div></div><div id="38225612" class="c"><input type="checkbox" id="c-38225612" checked=""/><div class="controls bullet"><span class="by">quadrature</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38225407">parent</a><span>|</span><a href="#38225673">prev</a><span>|</span><a href="#38225846">next</a><span>|</span><label class="collapse" for="c-38225612">[-]</label><label class="expand" for="c-38225612">[2 more]</label></div><br/><div class="children"><div class="content">from <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.04378" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.04378</a> it sounds like its a form of distillation of an SDL model. So im guessing it can&#x27;t be directly trained, but once you have a trained diffusion model you can distil a predictor which cuts out the iterative steps.<p>While it can do 1-step the output quality looks a ton better with additional steps.</div><br/><div id="38225679" class="c"><input type="checkbox" id="c-38225679" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38223963">root</a><span>|</span><a href="#38225612">parent</a><span>|</span><a href="#38225846">next</a><span>|</span><label class="collapse" for="c-38225679">[-]</label><label class="expand" for="c-38225679">[1 more]</label></div><br/><div class="children"><div class="content">You can train it directly. This is from the paper “An LCM demands merely 32 A100 GPUs Hours training for 2-step inference [...]”</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38225846" class="c"><input type="checkbox" id="c-38225846" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#38223963">prev</a><span>|</span><a href="#38226185">next</a><span>|</span><label class="collapse" for="c-38225846">[-]</label><label class="expand" for="c-38225846">[9 more]</label></div><br/><div class="children"><div class="content">Color me underwhelmed.</div><br/><div id="38225852" class="c"><input type="checkbox" id="c-38225852" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38225846">parent</a><span>|</span><a href="#38226185">next</a><span>|</span><label class="collapse" for="c-38225852">[-]</label><label class="expand" for="c-38225852">[8 more]</label></div><br/><div class="children"><div class="content">what does it take to impress you?</div><br/><div id="38226078" class="c"><input type="checkbox" id="c-38226078" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#38225846">root</a><span>|</span><a href="#38225852">parent</a><span>|</span><a href="#38226185">next</a><span>|</span><label class="collapse" for="c-38226078">[-]</label><label class="expand" for="c-38226078">[7 more]</label></div><br/><div class="children"><div class="content">There was a webcam demo a little while back that was pretty cool. It&#x27;s mostly that the right-hand side seems to only mildly follow the patterns drawn on the left hand side. It still seems sort of useful but once it&#x27;s running GAN speeds (30-60fps) and adheres more strongly to the input is when people will find it both useful and a joy to work with.<p>Based on my general experience with text-to-image stuff, I assume this lack of adherence isn&#x27;t always the case? Maybe another demo could show what it&#x27;s like under ideal conditions.<p>edit: Webcam demo<p><a href="https:&#x2F;&#x2F;github.com&#x2F;radames&#x2F;Real-Time-Latent-Consistency-Model?tab=readme-ov-file#demo-on-hugging-face">https:&#x2F;&#x2F;github.com&#x2F;radames&#x2F;Real-Time-Latent-Consistency-Mode...</a><p>It does however exhibit similar issues but the realtime constraints of live video make it quit interesting.</div><br/><div id="38226150" class="c"><input type="checkbox" id="c-38226150" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38225846">root</a><span>|</span><a href="#38226078">parent</a><span>|</span><a href="#38226242">next</a><span>|</span><label class="collapse" for="c-38226150">[-]</label><label class="expand" for="c-38226150">[5 more]</label></div><br/><div class="children"><div class="content">I agree with your points but I think this is the beginning of something bigger</div><br/><div id="38226170" class="c"><input type="checkbox" id="c-38226170" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#38225846">root</a><span>|</span><a href="#38226150">parent</a><span>|</span><a href="#38226242">next</a><span>|</span><label class="collapse" for="c-38226170">[-]</label><label class="expand" for="c-38226170">[4 more]</label></div><br/><div class="children"><div class="content">Definitely. I&#x27;ve been looking at all this stuff way too closely for awhile now so I doubt my opinion is representative. Looks like there&#x27;s positive reception coming from others in the comments here. I think the general cycle of &quot;new model&#x2F;technique - rush to implement - immediately obsoleted by another new model&#x2F;technique - rush to implement...&quot; has me a little burned out. Sorry about the dismissive comment.</div><br/><div id="38227773" class="c"><input type="checkbox" id="c-38227773" checked=""/><div class="controls bullet"><span class="by">disqard</span><span>|</span><a href="#38225846">root</a><span>|</span><a href="#38226170">parent</a><span>|</span><a href="#38227859">next</a><span>|</span><label class="collapse" for="c-38227773">[-]</label><label class="expand" for="c-38227773">[2 more]</label></div><br/><div class="children"><div class="content">Like you, I&#x27;m similarly situated -- and I&#x27;ll probably take a few months (to a year) break from following this stuff too closely (e.g. trying to get these running locally on my own). I have a suspicion that things will be a bit more lightweight and&#x2F;or optimized by then, and it&#x27;ll be way more tractable+economical to play with and selfhost many of these models.</div><br/><div id="38227890" class="c"><input type="checkbox" id="c-38227890" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38225846">root</a><span>|</span><a href="#38227773">parent</a><span>|</span><a href="#38227859">next</a><span>|</span><label class="collapse" for="c-38227890">[-]</label><label class="expand" for="c-38227890">[1 more]</label></div><br/><div class="children"><div class="content">Something something &lt;Best way to predict the future quote&gt;</div><br/></div></div></div></div><div id="38227859" class="c"><input type="checkbox" id="c-38227859" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38225846">root</a><span>|</span><a href="#38226170">parent</a><span>|</span><a href="#38227773">prev</a><span>|</span><a href="#38226242">next</a><span>|</span><label class="collapse" for="c-38227859">[-]</label><label class="expand" for="c-38227859">[1 more]</label></div><br/><div class="children"><div class="content">Oh interesting, in our case it fuels us!<p>Perhaps it&#x27;s because we&#x27;ve been doing generative AI for years now haha</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38226198" class="c"><input type="checkbox" id="c-38226198" checked=""/><div class="controls bullet"><span class="by">percentcer</span><span>|</span><a href="#38226185">prev</a><span>|</span><a href="#38225371">next</a><span>|</span><label class="collapse" for="c-38226198">[-]</label><label class="expand" for="c-38226198">[9 more]</label></div><br/><div class="children"><div class="content">Still too slow to be used for real work but it&#x27;s cool to see progress</div><br/><div id="38227450" class="c"><input type="checkbox" id="c-38227450" checked=""/><div class="controls bullet"><span class="by">passion__desire</span><span>|</span><a href="#38226198">parent</a><span>|</span><a href="#38226454">next</a><span>|</span><label class="collapse" for="c-38227450">[-]</label><label class="expand" for="c-38227450">[2 more]</label></div><br/><div class="children"><div class="content">Blender takes &gt; 2-3 seconds to apply all transformations to wireframe view and see final result. This is faster than that. Creatives would be happy with this.</div><br/><div id="38228496" class="c"><input type="checkbox" id="c-38228496" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38226198">root</a><span>|</span><a href="#38227450">parent</a><span>|</span><a href="#38226454">next</a><span>|</span><label class="collapse" for="c-38228496">[-]</label><label class="expand" for="c-38228496">[1 more]</label></div><br/><div class="children"><div class="content">Exactly! And I think there is a long room for optimizations.</div><br/></div></div></div></div><div id="38226454" class="c"><input type="checkbox" id="c-38226454" checked=""/><div class="controls bullet"><span class="by">ncr100</span><span>|</span><a href="#38226198">parent</a><span>|</span><a href="#38227450">prev</a><span>|</span><a href="#38226233">next</a><span>|</span><label class="collapse" for="c-38226454">[-]</label><label class="expand" for="c-38226454">[3 more]</label></div><br/><div class="children"><div class="content">I disagree.<p>A compromise: How about near-real time, would that pass under your BS-threshold?</div><br/><div id="38226520" class="c"><input type="checkbox" id="c-38226520" checked=""/><div class="controls bullet"><span class="by">percentcer</span><span>|</span><a href="#38226198">root</a><span>|</span><a href="#38226454">parent</a><span>|</span><a href="#38226233">next</a><span>|</span><label class="collapse" for="c-38226520">[-]</label><label class="expand" for="c-38226520">[2 more]</label></div><br/><div class="children"><div class="content">Not sure what you mean by my BS-threshold, but yes I would describe this tool as &quot;near real time&quot;. It has too much latency to be used as a professional tool for artists, though perhaps that is not the intended audience.</div><br/><div id="38227857" class="c"><input type="checkbox" id="c-38227857" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38226198">root</a><span>|</span><a href="#38226520">parent</a><span>|</span><a href="#38226233">next</a><span>|</span><label class="collapse" for="c-38227857">[-]</label><label class="expand" for="c-38227857">[1 more]</label></div><br/><div class="children"><div class="content">Our users are already using our tool professionally—even though latency is &gt;5s at times.<p>I have to disagree with you.<p>Of course, people will use it in a weird way—which is exciting.</div><br/></div></div></div></div></div></div><div id="38226233" class="c"><input type="checkbox" id="c-38226233" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38226198">parent</a><span>|</span><a href="#38226454">prev</a><span>|</span><a href="#38225371">next</a><span>|</span><label class="collapse" for="c-38226233">[-]</label><label class="expand" for="c-38226233">[3 more]</label></div><br/><div class="children"><div class="content">Really? Some users are already using KREA professionally.<p>FCB for example.</div><br/><div id="38226320" class="c"><input type="checkbox" id="c-38226320" checked=""/><div class="controls bullet"><span class="by">percentcer</span><span>|</span><a href="#38226198">root</a><span>|</span><a href="#38226233">parent</a><span>|</span><a href="#38225371">next</a><span>|</span><label class="collapse" for="c-38226320">[-]</label><label class="expand" for="c-38226320">[2 more]</label></div><br/><div class="children"><div class="content">...yes? If you&#x27;re marketing a tool as being &quot;real time&quot; I&#x27;d expect it to not have this much latency (e.g. when the blue is added to the mushroom). Imagine if Photoshop made you wait a second between each brush stroke.</div><br/><div id="38226547" class="c"><input type="checkbox" id="c-38226547" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38226198">root</a><span>|</span><a href="#38226320">parent</a><span>|</span><a href="#38225371">next</a><span>|</span><label class="collapse" for="c-38226547">[-]</label><label class="expand" for="c-38226547">[1 more]</label></div><br/><div class="children"><div class="content">That’s how it started!</div><br/></div></div></div></div></div></div></div></div><div id="38225371" class="c"><input type="checkbox" id="c-38225371" checked=""/><div class="controls bullet"><span class="by">gailees</span><span>|</span><a href="#38226198">prev</a><span>|</span><label class="collapse" for="c-38225371">[-]</label><label class="expand" for="c-38225371">[7 more]</label></div><br/><div class="children"><div class="content">How do you plan on stopping people from using this tool maliciously?</div><br/><div id="38227014" class="c"><input type="checkbox" id="c-38227014" checked=""/><div class="controls bullet"><span class="by">boppo1</span><span>|</span><a href="#38225371">parent</a><span>|</span><a href="#38225384">next</a><span>|</span><label class="collapse" for="c-38227014">[-]</label><label class="expand" for="c-38227014">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m an illustrator. I&#x27;m fully capable of drawing&#x2F;painting political figures in compromising or worse situations. Should the government blind me because my skills are dangerous?</div><br/><div id="38227166" class="c"><input type="checkbox" id="c-38227166" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#38225371">root</a><span>|</span><a href="#38227014">parent</a><span>|</span><a href="#38227583">next</a><span>|</span><label class="collapse" for="c-38227166">[-]</label><label class="expand" for="c-38227166">[1 more]</label></div><br/><div class="children"><div class="content">Some politicians would say: &quot;Yes.&quot;<p>Artists, intellectuals, journalists, and critics of all sorts have been jailed, beaten, or simply murdered in the past for expressing themselves in ways the government of the time did not like.</div><br/></div></div><div id="38227583" class="c"><input type="checkbox" id="c-38227583" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38225371">root</a><span>|</span><a href="#38227014">parent</a><span>|</span><a href="#38227166">prev</a><span>|</span><a href="#38225384">next</a><span>|</span><label class="collapse" for="c-38227583">[-]</label><label class="expand" for="c-38227583">[1 more]</label></div><br/><div class="children"><div class="content">Oh, interesting!<p>What do <i>you</i> think of this technology? What do you envision?</div><br/></div></div></div></div><div id="38225384" class="c"><input type="checkbox" id="c-38225384" checked=""/><div class="controls bullet"><span class="by">monkellipse</span><span>|</span><a href="#38225371">parent</a><span>|</span><a href="#38227014">prev</a><span>|</span><label class="collapse" for="c-38225384">[-]</label><label class="expand" for="c-38225384">[3 more]</label></div><br/><div class="children"><div class="content">The same way you stop people from using a hammer maliciously?</div><br/><div id="38225909" class="c"><input type="checkbox" id="c-38225909" checked=""/><div class="controls bullet"><span class="by">dvrp</span><span>|</span><a href="#38225371">root</a><span>|</span><a href="#38225384">parent</a><span>|</span><label class="collapse" for="c-38225909">[-]</label><label class="expand" for="c-38225909">[2 more]</label></div><br/><div class="children"><div class="content">In the general sense yes, but I wonder if there will be unexpected things that we’ll need to take into account with this new generation of tools.<p>Part of me thinks that this is another revolution in graphics the same way Photoshop was where you can work 10x faster. But another one ponders about what happens when we’re dealing with intelligence.</div><br/><div id="38226543" class="c"><input type="checkbox" id="c-38226543" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#38225371">root</a><span>|</span><a href="#38225909">parent</a><span>|</span><label class="collapse" for="c-38226543">[-]</label><label class="expand" for="c-38226543">[1 more]</label></div><br/><div class="children"><div class="content">&gt;But another one ponders about what happens when we’re dealing with intelligence.<p>This is a good point, diffusion models are an example of intelligence. Proof of that is that they became ubiquitous in the same year as large language models, thus they are the same.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>