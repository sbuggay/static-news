<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1734166872139" as="style"/><link rel="stylesheet" href="styles.css?v=1734166872139"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.youtube.com/watch?v=YD-9NG1Ke5Y">Sequence to sequence learning with neural networks: what a decade [video]</a> <span class="domain">(<a href="https://www.youtube.com">www.youtube.com</a>)</span></div><div class="subtext"><span>mfiguiere</span> | <span>21 comments</span></div><br/><div><div id="42414304" class="c"><input type="checkbox" id="c-42414304" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#42415476">next</a><span>|</span><label class="collapse" for="c-42414304">[-]</label><label class="expand" for="c-42414304">[14 more]</label></div><br/><div class="children"><div class="content">&gt; “Pre-training as we know it will unquestionably end,” Sutskever said onstage.<p>&gt; “We’ve achieved peak data and there’ll be no more.”<p>&gt; During his NeurIPS talk, Sutskever said that, while he believes existing data can still take AI development farther, the industry is tapping out on new data to train on. This dynamic will, he said, eventually force a shift away from the way models are trained today. He compared the situation to fossil fuels: just as oil is a finite resource, the internet contains a finite amount of human-generated content.<p>&gt; “We’ve achieved peak data and there’ll be no more,” according to Sutskever. “We have to deal with the data that we have. There’s only one internet.”<p>What will replace Internet data for training? Curated synthetic datasets?<p>There are massive proprietary datasets out there which people avoid using for training due to copyright concerns. But if you actually own one of those datasets, that resolves a lot of the legal issues with training on it.<p>For example, Getty has a massive image library. Training on it would risk Getty suing you. But what if Getty decides to use it to train their own AI? Similarly, what if News Corp decides to train an AI using its publishing assets (Wall Street Journal, HarperCollins, etc)?</div><br/><div id="42415607" class="c"><input type="checkbox" id="c-42415607" checked=""/><div class="controls bullet"><span class="by">_nalply</span><span>|</span><a href="#42414304">parent</a><span>|</span><a href="#42415545">next</a><span>|</span><label class="collapse" for="c-42415607">[-]</label><label class="expand" for="c-42415607">[1 more]</label></div><br/><div class="children"><div class="content">One way could be that an AI learns to hack and gets access. A state actor could unleash such an AI as a belligerent act.<p>As already said, it will be unpredictable. An AI might stumble upon some weird trick as a loophole no human even could dream to think of, and use that to pull the rug out from under humans. In that case let&#x27;s pray for a beneficial alignment.</div><br/></div></div><div id="42415545" class="c"><input type="checkbox" id="c-42415545" checked=""/><div class="controls bullet"><span class="by">menaerus</span><span>|</span><a href="#42414304">parent</a><span>|</span><a href="#42415607">prev</a><span>|</span><a href="#42414858">next</a><span>|</span><label class="collapse" for="c-42415545">[-]</label><label class="expand" for="c-42415545">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What will replace Internet data for training? Curated synthetic datasets?<p>Perhaps a different take at this could be: if I wanted to train &quot;state law&quot; LLM that is exceedingly good in interpreting state law, what are the obstacles to download all the law and regulations material for given state that will allow me to train LLM such that it becomes 95th percentile of all law trainees and lawyers.<p>In that case, and my point is, that we already don&#x27;t need an &quot;Internet&quot;. We just need a sufficiently sized and curated domain-specific dataset and the result we can get is already scary. &quot;State law&quot; LLM was just an example but the same logic applies to basically any other domain - want a domain-specific (LLM) expert? Train it.</div><br/></div></div><div id="42414858" class="c"><input type="checkbox" id="c-42414858" checked=""/><div class="controls bullet"><span class="by">vitorgrs</span><span>|</span><a href="#42414304">parent</a><span>|</span><a href="#42415545">prev</a><span>|</span><a href="#42414480">next</a><span>|</span><label class="collapse" for="c-42414858">[-]</label><label class="expand" for="c-42414858">[1 more]</label></div><br/><div class="children"><div class="content">Not sure if this was a good example. Getty already license their images to Nvidia.<p>And they already have a generative image service... I believe it&#x27;s power by Nvidia model.</div><br/></div></div><div id="42414480" class="c"><input type="checkbox" id="c-42414480" checked=""/><div class="controls bullet"><span class="by">kibae</span><span>|</span><a href="#42414304">parent</a><span>|</span><a href="#42414858">prev</a><span>|</span><a href="#42415252">next</a><span>|</span><label class="collapse" for="c-42414480">[-]</label><label class="expand" for="c-42414480">[1 more]</label></div><br/><div class="children"><div class="content">I always suspected that bots on Reddit were used to gain karma and then eventually sell the account, but maybe they&#x27;re also being used for some kind of RLHF.</div><br/></div></div><div id="42415252" class="c"><input type="checkbox" id="c-42415252" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#42414304">parent</a><span>|</span><a href="#42414480">prev</a><span>|</span><a href="#42415172">next</a><span>|</span><label class="collapse" for="c-42415252">[-]</label><label class="expand" for="c-42415252">[4 more]</label></div><br/><div class="children"><div class="content">Humans doesn&#x27;t need trillions of tokens to reason or ability to know what they know. While a certain part of it comes from evolution, I think we have already matched the part that came from evolution using internet data, like basic language skills, basic world modelling. Current pretraining takes lot more data than a human would, and you don&#x27;t need to look into all Getty images to draw a picture and so would a self aware&#x2F;improving model(whatever that means).<p>To reach expert level in any field, just training next tokens for internet data or any data is not the solution.</div><br/><div id="42415360" class="c"><input type="checkbox" id="c-42415360" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#42414304">root</a><span>|</span><a href="#42415252">parent</a><span>|</span><a href="#42415172">next</a><span>|</span><label class="collapse" for="c-42415360">[-]</label><label class="expand" for="c-42415360">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Humans doesn&#x27;t need trillions of tokens<p>I wonder about that. we can fine tune on calculus with much fewer tokens, but I&#x27;d be interested in some calculations of how many tokens evolution provides us (it&#x27;s not about the DNA itself, but all the other things that were explored and discarded and are now out of reach) - but also the sheer amount of physics learnt by a baby by crawling around and putting everything in its mouth.</div><br/><div id="42415393" class="c"><input type="checkbox" id="c-42415393" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#42414304">root</a><span>|</span><a href="#42415360">parent</a><span>|</span><a href="#42415172">next</a><span>|</span><label class="collapse" for="c-42415393">[-]</label><label class="expand" for="c-42415393">[2 more]</label></div><br/><div class="children"><div class="content">Yes, as I said in the last comment. With current training techniques, one internet data is enough to give models what is given by evolution. For further training, I believe we would need different techniques to make the model self aware about its knowledge.<p>Also, I believe a person who is blind and paralyzed for life could still attain knowledge if educated well enough.(can&#x27;t find any study here tbh)</div><br/><div id="42415444" class="c"><input type="checkbox" id="c-42415444" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#42414304">root</a><span>|</span><a href="#42415393">parent</a><span>|</span><a href="#42415172">next</a><span>|</span><label class="collapse" for="c-42415444">[-]</label><label class="expand" for="c-42415444">[1 more]</label></div><br/><div class="children"><div class="content">yeah blind and paralysed from birth - I&#x27;m doubtful that hearing along would give you the physics training. although if it can be done, then it means the evolutionary pre-training is even more impressive.</div><br/></div></div></div></div></div></div></div></div><div id="42415172" class="c"><input type="checkbox" id="c-42415172" checked=""/><div class="controls bullet"><span class="by">popularonion</span><span>|</span><a href="#42414304">parent</a><span>|</span><a href="#42415252">prev</a><span>|</span><a href="#42414381">next</a><span>|</span><label class="collapse" for="c-42415172">[-]</label><label class="expand" for="c-42415172">[4 more]</label></div><br/><div class="children"><div class="content">&gt; What will replace Internet data for training? Curated synthetic datasets?<p>Enter Neuralink</div><br/><div id="42415396" class="c"><input type="checkbox" id="c-42415396" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#42414304">root</a><span>|</span><a href="#42415172">parent</a><span>|</span><a href="#42414381">next</a><span>|</span><label class="collapse" for="c-42415396">[-]</label><label class="expand" for="c-42415396">[3 more]</label></div><br/><div class="children"><div class="content">Really not sure what you mean by this, could you explain?</div><br/><div id="42415473" class="c"><input type="checkbox" id="c-42415473" checked=""/><div class="controls bullet"><span class="by">Gigachad</span><span>|</span><a href="#42414304">root</a><span>|</span><a href="#42415396">parent</a><span>|</span><a href="#42414381">next</a><span>|</span><label class="collapse" for="c-42415473">[-]</label><label class="expand" for="c-42415473">[2 more]</label></div><br/><div class="children"><div class="content">AI can just suck up the content of peoples brains for training data.</div><br/><div id="42415497" class="c"><input type="checkbox" id="c-42415497" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#42414304">root</a><span>|</span><a href="#42415473">parent</a><span>|</span><a href="#42414381">next</a><span>|</span><label class="collapse" for="c-42415497">[-]</label><label class="expand" for="c-42415497">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, people will go crazy for GPT-o2 trained on the readings of sensors &quot;barely embedded&quot; in the brains tortured monkeys, for sure.<p>EDIT: This comment may have been a bit too sassy. I get the thought behind the original comment, but I personally question the direction and premise of the Neuralink project, and know I am not alone in that regard. That being said, taking a step back, there for sure are plenty of rich data sources for non-text multimodal data.</div><br/></div></div></div></div></div></div></div></div><div id="42414381" class="c"><input type="checkbox" id="c-42414381" checked=""/><div class="controls bullet"><span class="by">_aavaa_</span><span>|</span><a href="#42414304">parent</a><span>|</span><a href="#42415172">prev</a><span>|</span><a href="#42415476">next</a><span>|</span><label class="collapse" for="c-42414381">[-]</label><label class="expand" for="c-42414381">[1 more]</label></div><br/><div class="children"><div class="content">&gt; just as oil is a finite resource, the internet contains a finite amount of human-generated content.<p>I guess now they’re being explicit about the blatantly extractive nature of these businesses and their models.</div><br/></div></div></div></div><div id="42415476" class="c"><input type="checkbox" id="c-42415476" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#42414304">prev</a><span>|</span><a href="#42415248">next</a><span>|</span><label class="collapse" for="c-42415476">[-]</label><label class="expand" for="c-42415476">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t help but feel that this talk was a lot of...fluff?<p>The synopsis, as far as my tired brain can remember:<p>- Here&#x27;s a brief summary of the last 10 years<p>- We&#x27;re reaching the limit of our scaling laws, because we&#x27;ve trained on all the data we have available on the limit<p>- Some things that may be next are &quot;agents&quot;, &quot;synthetic data&quot;, and improving compute<p>- Some &quot;ANNs are like biological NNs&quot; rehash that would feel questionable if there <i>was</i> a thesis (which there wasn&#x27;t? something about how body mass vs. brain mass are positively correlated?)<p>- 3 questions, the first was something about &quot;hallucinations&quot; and whether a model be able to understand if it is hallucinating? Then something that involved cryptocurrencies, and then a _slightly_ interesting question about multi-hop reasoning</div><br/></div></div><div id="42415248" class="c"><input type="checkbox" id="c-42415248" checked=""/><div class="controls bullet"><span class="by">stretchwithme</span><span>|</span><a href="#42415476">prev</a><span>|</span><a href="#42415431">next</a><span>|</span><label class="collapse" for="c-42415248">[-]</label><label class="expand" for="c-42415248">[1 more]</label></div><br/><div class="children"><div class="content">AIs will need to start asking people questions.  Should make for some very strange phone calls.</div><br/></div></div><div id="42414681" class="c"><input type="checkbox" id="c-42414681" checked=""/><div class="controls bullet"><span class="by">neom</span><span>|</span><a href="#42414589">prev</a><span>|</span><a href="#42414092">next</a><span>|</span><label class="collapse" for="c-42414681">[-]</label><label class="expand" for="c-42414681">[1 more]</label></div><br/><div class="children"><div class="content">Full talk is interesting: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=YD-9NG1Ke5Y" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=YD-9NG1Ke5Y</a></div><br/></div></div></div></div></div></div></div></body></html>