<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1725526862753" as="style"/><link rel="stylesheet" href="styles.css?v=1725526862753"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.reuters.com/technology/artificial-intelligence/openai-co-founder-sutskevers-new-safety-focused-ai-startup-ssi-raises-1-billion-2024-09-04/">Ilya Sutskever&#x27;s SSI Inc raises $1B</a> <span class="domain">(<a href="https://www.reuters.com">www.reuters.com</a>)</span></div><div class="subtext"><span>colesantiago</span> | <span>439 comments</span></div><br/><div><div id="41454409" class="c"><input type="checkbox" id="c-41454409" checked=""/><div class="controls bullet"><span class="by">openrisk</span><span>|</span><a href="#41448627">next</a><span>|</span><label class="collapse" for="c-41454409">[-]</label><label class="expand" for="c-41454409">[8 more]</label></div><br/><div class="children"><div class="content">Who would have thought that vectorized linear algebra will be at the center of so much financial speculation?<p>There is a silver lining though. Even if it all goes to near-zero (most likely outcome for all VC investments anyway) the digital world will be one where fast matrix multiply is thoroughly commoditized.<p>This is <i>not</i> a trivial feat.<p>In a sense this will be the true end of the Wintel era. The old world of isolated, CISC, deterministic desktops giving way not to &quot;AGI&quot;, but widely available, networked, vector &quot;supercomputers&quot; that can digest and transform practically everything that has ever been digitized.<p>Who knows what the actual (financial) winners of this brave new era will be.<p>In an ideal world there should be no winner-takes-all entity but a broad-based leveling up, i.e., spreading these new means of production as widely as possible.<p>Heck, maybe we will even eventually see the famously absent productivity gains from digital tech?</div><br/><div id="41454561" class="c"><input type="checkbox" id="c-41454561" checked=""/><div class="controls bullet"><span class="by">gizajob</span><span>|</span><a href="#41454409">parent</a><span>|</span><a href="#41454744">next</a><span>|</span><label class="collapse" for="c-41454561">[-]</label><label class="expand" for="c-41454561">[4 more]</label></div><br/><div class="children"><div class="content">“Who knows what the actual (financial) winners of this brave new era will be”<p>Nvidia</div><br/><div id="41454646" class="c"><input type="checkbox" id="c-41454646" checked=""/><div class="controls bullet"><span class="by">openrisk</span><span>|</span><a href="#41454409">root</a><span>|</span><a href="#41454561">parent</a><span>|</span><a href="#41454664">next</a><span>|</span><label class="collapse" for="c-41454646">[-]</label><label class="expand" for="c-41454646">[2 more]</label></div><br/><div class="children"><div class="content">They are now, but will they be winning in 10 (or even 3-5) years?<p>Their shtick is a GPU on steroids. In the bigger picture its a well positioned hack that has ridden two successive speculative bubbles (crypto mining and AI) but its unclear how far this can go. Currently this approach is wildly successful cause nobody else bothered to toil a serious vision about the post-Moore&#x27;s law era. But make no mistake, people&#x27;s minds will get focused.</div><br/><div id="41454828" class="c"><input type="checkbox" id="c-41454828" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#41454409">root</a><span>|</span><a href="#41454646">parent</a><span>|</span><a href="#41454664">next</a><span>|</span><label class="collapse" for="c-41454828">[-]</label><label class="expand" for="c-41454828">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a graphics card hacked to do math any more. It&#x27;s a general purpose computer with some legacy cruft added to handle graphics work if necessary. Lots of people are working very hard to find something better and have been for at least a decade, probably several.</div><br/></div></div></div></div><div id="41454664" class="c"><input type="checkbox" id="c-41454664" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#41454409">root</a><span>|</span><a href="#41454561">parent</a><span>|</span><a href="#41454646">prev</a><span>|</span><a href="#41454744">next</a><span>|</span><label class="collapse" for="c-41454664">[-]</label><label class="expand" for="c-41454664">[1 more]</label></div><br/><div class="children"><div class="content">Maybe I&#x27;m naive but there seems to be way too much financial incentive in this space for CUDA to continue to be the top dog. Just like microprocessors, these devices are going to get commodified, standardized, open sourced, etc. Nvidia making massive profits is a sign of huge market inefficiency and potential opportunities for competition.</div><br/></div></div></div></div><div id="41454744" class="c"><input type="checkbox" id="c-41454744" checked=""/><div class="controls bullet"><span class="by">pennaMan</span><span>|</span><a href="#41454409">parent</a><span>|</span><a href="#41454561">prev</a><span>|</span><a href="#41454624">next</a><span>|</span><label class="collapse" for="c-41454744">[-]</label><label class="expand" for="c-41454744">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Who would have thought that vectorized linear algebra will be at the center of so much financial speculation?<p>Wait till you hear that a bunch of meat[1] is behind all said speculation.<p>[1]<a href="https:&#x2F;&#x2F;stuff.mit.edu&#x2F;people&#x2F;dpolicar&#x2F;writing&#x2F;prose&#x2F;text&#x2F;thinkingMeat.html" rel="nofollow">https:&#x2F;&#x2F;stuff.mit.edu&#x2F;people&#x2F;dpolicar&#x2F;writing&#x2F;prose&#x2F;text&#x2F;thi...</a></div><br/></div></div><div id="41454624" class="c"><input type="checkbox" id="c-41454624" checked=""/><div class="controls bullet"><span class="by">lobochrome</span><span>|</span><a href="#41454409">parent</a><span>|</span><a href="#41454744">prev</a><span>|</span><a href="#41448627">next</a><span>|</span><label class="collapse" for="c-41454624">[-]</label><label class="expand" for="c-41454624">[2 more]</label></div><br/><div class="children"><div class="content">This is a fantastic comment. Very insightful.</div><br/><div id="41454786" class="c"><input type="checkbox" id="c-41454786" checked=""/><div class="controls bullet"><span class="by">hcta</span><span>|</span><a href="#41454409">root</a><span>|</span><a href="#41454624">parent</a><span>|</span><a href="#41448627">next</a><span>|</span><label class="collapse" for="c-41454786">[-]</label><label class="expand" for="c-41454786">[1 more]</label></div><br/><div class="children"><div class="content">This is a fantastic comment. Very insightful.</div><br/></div></div></div></div></div></div><div id="41448627" class="c"><input type="checkbox" id="c-41448627" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#41454409">prev</a><span>|</span><a href="#41446071">next</a><span>|</span><label class="collapse" for="c-41448627">[-]</label><label class="expand" for="c-41448627">[169 more]</label></div><br/><div class="children"><div class="content">This being ycombinator and as such ostensibly has one or two (if not more) VCs as readers&#x2F;commentators …  can someone please tell me how these companies that are being invested in in the AI space are going to make returns on the money invested? What’s the business plan? (I’m not rich enough to be in these meetings) I just don’t see how the returns will happen.<p>Open source LLMs exist and will get better. Is it just that all these companies will vie for a winner-take-all situation where the “best” model will garner the subscription? Doesn’t OpenAI make some substantial part of the revenue for all the AI space? I just don’t see it. But I don’t have VC levels of cash to bet on a 10x or 100x return so what do I know?</div><br/><div id="41453257" class="c"><input type="checkbox" id="c-41453257" checked=""/><div class="controls bullet"><span class="by">jhylau</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41451265">next</a><span>|</span><label class="collapse" for="c-41453257">[-]</label><label class="expand" for="c-41453257">[10 more]</label></div><br/><div class="children"><div class="content">VCs at the big&#x2F;mega funds make most of their money from fees, they don&#x27;t actually care as much about the potential portfolio investment exits 10-15 years from now.  What they care MOST about is the ability to raise another fund in 2-3 years, so they can milk more fees from LPs. i.e. 2% fee PER YEAR on a 5bn fund is a lot of guaranteed risk-free money.<p>To be able to achieve that is entirely dependent on two things:<p>1) deploying capital in the current fund on &#x27;sexy&#x27; ideas so they can tell LPs they are doing their job<p>2) paper markups, which they will get, since Ilya will most definitely be able to raise another round or two at a higher valuation.  even if it eventually goes bust or gets sold at cost.<p>With 1) and 2), they can go back to their existing fund LPs and raise more money for their next fund and milk more fees.  Getting exits and carry is just the cherry on top for these megafund VCs.</div><br/><div id="41453480" class="c"><input type="checkbox" id="c-41453480" checked=""/><div class="controls bullet"><span class="by">rawrawrawrr</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41453257">parent</a><span>|</span><a href="#41454204">next</a><span>|</span><label class="collapse" for="c-41453480">[-]</label><label class="expand" for="c-41453480">[3 more]</label></div><br/><div class="children"><div class="content">Are you a VC? If they really didn&#x27;t care about their investment exits, that would be crazy.</div><br/><div id="41453573" class="c"><input type="checkbox" id="c-41453573" checked=""/><div class="controls bullet"><span class="by">aorloff</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41453480">parent</a><span>|</span><a href="#41454204">next</a><span>|</span><label class="collapse" for="c-41453573">[-]</label><label class="expand" for="c-41453573">[2 more]</label></div><br/><div class="children"><div class="content">I think they treat success and failure as mostly luck, with a tiny bit of hygiene<p>The trick is being in the game long enough</div><br/><div id="41453748" class="c"><input type="checkbox" id="c-41453748" checked=""/><div class="controls bullet"><span class="by">valval</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41453573">parent</a><span>|</span><a href="#41454204">next</a><span>|</span><label class="collapse" for="c-41453748">[-]</label><label class="expand" for="c-41453748">[1 more]</label></div><br/><div class="children"><div class="content">And you’d be wrong of course, like any other random guess you could make about a topic you know nothing about.</div><br/></div></div></div></div></div></div><div id="41454204" class="c"><input type="checkbox" id="c-41454204" checked=""/><div class="controls bullet"><span class="by">andsoitis</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41453257">parent</a><span>|</span><a href="#41453480">prev</a><span>|</span><a href="#41453877">next</a><span>|</span><label class="collapse" for="c-41454204">[-]</label><label class="expand" for="c-41454204">[1 more]</label></div><br/><div class="children"><div class="content">&gt; about is the ability to raise another fund in 2-3 years, so they can milk more fees from LPs. i.e. 2% fee PER YEAR on a 5bn fund is a lot of guaranteed risk-free money.<p>You will struggle to raise funds if the companies you bet on perform poorly; the worse your track record the less chances of raising money and earn income from it.</div><br/></div></div><div id="41453877" class="c"><input type="checkbox" id="c-41453877" checked=""/><div class="controls bullet"><span class="by">baxtr</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41453257">parent</a><span>|</span><a href="#41454204">prev</a><span>|</span><a href="#41453490">next</a><span>|</span><label class="collapse" for="c-41453877">[-]</label><label class="expand" for="c-41453877">[2 more]</label></div><br/><div class="children"><div class="content">I’m not a VC, but I doubt that’s true. You can exactly raise one fund if you operate like that but not two or three.</div><br/><div id="41453989" class="c"><input type="checkbox" id="c-41453989" checked=""/><div class="controls bullet"><span class="by">conradev</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41453877">parent</a><span>|</span><a href="#41453490">next</a><span>|</span><label class="collapse" for="c-41453989">[-]</label><label class="expand" for="c-41453989">[1 more]</label></div><br/><div class="children"><div class="content">You could actually raise and deploy two or three funds that way, before you see returns for the first</div><br/></div></div></div></div><div id="41453490" class="c"><input type="checkbox" id="c-41453490" checked=""/><div class="controls bullet"><span class="by">hello0904</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41453257">parent</a><span>|</span><a href="#41453877">prev</a><span>|</span><a href="#41453741">next</a><span>|</span><label class="collapse" for="c-41453490">[-]</label><label class="expand" for="c-41453490">[2 more]</label></div><br/><div class="children"><div class="content">So the question I have is, who are these LP&#x27;s and why are they demanding funds go into &quot;sexy&quot; ideas?<p>I mean it probably depends on the LP and what is their vision. Not all apples are red, come in many varieties and some for cider others for pies. Am I wrong?</div><br/><div id="41453567" class="c"><input type="checkbox" id="c-41453567" checked=""/><div class="controls bullet"><span class="by">neom</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41453490">parent</a><span>|</span><a href="#41453741">next</a><span>|</span><label class="collapse" for="c-41453567">[-]</label><label class="expand" for="c-41453567">[1 more]</label></div><br/><div class="children"><div class="content">The person you&#x27;re responding to has a very sharp view of the profession. imo it&#x27;s more nuanced, but not very complicated. In Capitalism, capital flows, that&#x27;s how it works, capital should be deployed. Larges pools of capital are typically put to work (this in itself is nuanced). The &quot;put to work&quot; is various types of deployment of the capital. The simplest way to look at this is risk. Lets take pension funds because we know they invest in VC firms as LPs. Here* you can find an example of the breakdown of the investments made by this very large pension fund. You&#x27;ll note most of it is very boring, and the positions held related to venture are tiny, they would need a crazy outsized swing from a VC firm to move any needles. Given all that, it traditionally* has made no sense to bet &quot;down there&quot; (early stage) - mostly because the expertise are not there, and they don&#x27;t have the time to learn tech&#x2F;product. Fee&#x27;s are the cost of capital deployment at the early stages, and from what I&#x27;ve been told talking to folks who work at pension funds, they&#x27;re happy to see VCs take swing.<p>but.. it really depends heavily on the LP base of the firm, and what the firm raised it&#x27;s fund on, it&#x27;s incredibly difficult to generalize. The funds I&#x27;m involved around as an LP... in my opinion they can get as &quot;sexy&quot; as they like because I buy their thesis, then it&#x27;s just: get the capital deployed!!!!<p>Most of this is all a standard deviation game, not much more than that.<p><a href="https:&#x2F;&#x2F;www.otpp.com&#x2F;en-ca&#x2F;investments&#x2F;our-advantage&#x2F;our-performance-and-track-record&#x2F;major-investments&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.otpp.com&#x2F;en-ca&#x2F;investments&#x2F;our-advantage&#x2F;our-per...</a> 
<a href="https:&#x2F;&#x2F;www.hellokoru.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.hellokoru.com&#x2F;</a></div><br/></div></div></div></div><div id="41453741" class="c"><input type="checkbox" id="c-41453741" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41453257">parent</a><span>|</span><a href="#41453490">prev</a><span>|</span><a href="#41451265">next</a><span>|</span><label class="collapse" for="c-41453741">[-]</label><label class="expand" for="c-41453741">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t know what an LP is, having lived life gloriously isolated from the VC gospel...<p><i>an LP is a &quot;limited partner.&quot; they&#x27;re the suckers (or institutional investors, endowments, pensions, rich folks, etc.) that give their cash to venture capital (VC) firms to manage. LPs invest in VC funds but don&#x27;t have control over how the money gets used—hence *limited* partner. they just hope the VCs aren&#x27;t burning it on overpriced kombucha and shitty &quot;web3&quot; startups.</i><p><i>meanwhile, the VCs rake in their fat management fees (like the 2% mentioned) and also get a cut of any profits (carry). VCs are more concerned with looking busy and keeping those sweet fees rolling in than actually giving a fuck about long-term exits.</i><p>Someone wants to fund my snide, cynical AI HN comment explainer startup? We are too cool for long term plans, but we use AI.</div><br/></div></div></div></div><div id="41451265" class="c"><input type="checkbox" id="c-41451265" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41453257">prev</a><span>|</span><a href="#41450967">next</a><span>|</span><label class="collapse" for="c-41451265">[-]</label><label class="expand" for="c-41451265">[17 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not a VC so maybe you don&#x27;t care what I think, I&#x27;m not sure.<p>Last night as my 8yo was listening to childrens audio books going to sleep, she asked me to have it alternate book A then B then A then B.<p>I thought, idunno maybe I can work out a way to do this. Maybe the app has playlists and maaaaaaaaaaybe has a way to set a playlist on repeat. Or maybe you just can&#x27;t do this in the app at all. I just sat there and switched it until she fell asleep, it wasn&#x27;t gonna be more than 2 or 3 anyway, and so it&#x27;s kind of a dumb example.<p>But here&#x27;s the point: Computers can process language now. I can totally imagine her telling my phone to do that and it being able to do so, even if she&#x27;s the first person ever to want it to do that. I think the bet is that a very large percentage of the world&#x27;s software is going to want to gain natural language superpowers. And that this is not a trivial undertaking that will be achieved by a few open source LLMs. It will be a lot of work for a lot of people to make this happen, as such a lot of money will be made along the way.<p>Specifically how will this unfold? Nobody knows, but I think they wanna be deep in the game when it does.</div><br/><div id="41454851" class="c"><input type="checkbox" id="c-41454851" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451265">parent</a><span>|</span><a href="#41451463">next</a><span>|</span><label class="collapse" for="c-41454851">[-]</label><label class="expand" for="c-41454851">[1 more]</label></div><br/><div class="children"><div class="content">Robots will be better than humans at parenting in no time</div><br/></div></div><div id="41451463" class="c"><input type="checkbox" id="c-41451463" checked=""/><div class="controls bullet"><span class="by">steveBK123</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451265">parent</a><span>|</span><a href="#41454851">prev</a><span>|</span><a href="#41452517">next</a><span>|</span><label class="collapse" for="c-41451463">[-]</label><label class="expand" for="c-41451463">[12 more]</label></div><br/><div class="children"><div class="content">How is this any different than the (lack of) business model of all the voice assistants?<p>How good does it have to be, how many features does it have to have, how accurate does its need to be.. in order for people to pay anything?  And how much are people actually willing to spend against the $XX Billion of investment?<p>Again it just seems like &quot;sell to AAPL&#x2F;GOOG&#x2F;MSFT and let them figure it out&quot;.</div><br/><div id="41454855" class="c"><input type="checkbox" id="c-41454855" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451463">parent</a><span>|</span><a href="#41452026">next</a><span>|</span><label class="collapse" for="c-41454855">[-]</label><label class="expand" for="c-41454855">[1 more]</label></div><br/><div class="children"><div class="content">Congrats on 10k karma :)</div><br/></div></div><div id="41452026" class="c"><input type="checkbox" id="c-41452026" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451463">parent</a><span>|</span><a href="#41454855">prev</a><span>|</span><a href="#41453697">next</a><span>|</span><label class="collapse" for="c-41452026">[-]</label><label class="expand" for="c-41452026">[6 more]</label></div><br/><div class="children"><div class="content">&gt; How is this any different than the (lack of) business model of all the voice assistants?<p>Voice assistants do a small subset of the things you can already do easily on your phone. Competing with things you can already do easily on your phone is very hard; touch interfaces are extremely accessible, in many ways more accessible than voice. Current voice assistants only being able to do a small subset of that makes them not really very valuable.<p>And we aren&#x27;t updating and rewriting all the world&#x27;s software to expose its functionality to voice assistants because the voice assistant needs to be programmed to do each of those things. Each possible interaction must be planned and implemented invidually.<p>I think the bet is that we WILL be doing substantially that, updating and rewriting all the software, now that we can make them do things that are NOT easy to do with a phone or with a computer. And we can do so without designing every individual interaction; we can expose the building blocks and common interactions and LLMs may be able to map much more specific user desires onto those.</div><br/><div id="41453973" class="c"><input type="checkbox" id="c-41453973" checked=""/><div class="controls bullet"><span class="by">ithkuil</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452026">parent</a><span>|</span><a href="#41452106">next</a><span>|</span><label class="collapse" for="c-41453973">[-]</label><label class="expand" for="c-41453973">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if we&#x27;ll end up having intelligent agents interacting with mobile apps &#x2F; web pages in headless displays because that&#x27;s easier than exposing an API for every app</div><br/></div></div><div id="41452106" class="c"><input type="checkbox" id="c-41452106" checked=""/><div class="controls bullet"><span class="by">steveBK123</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452026">parent</a><span>|</span><a href="#41453973">prev</a><span>|</span><a href="#41453697">next</a><span>|</span><label class="collapse" for="c-41452106">[-]</label><label class="expand" for="c-41452106">[4 more]</label></div><br/><div class="children"><div class="content">OK so bull case on LLMs now is inclusive of &quot;substantially rewriting all the worlds software to expose functionality to them via APIs&quot; ?</div><br/><div id="41452182" class="c"><input type="checkbox" id="c-41452182" checked=""/><div class="controls bullet"><span class="by">ok_dad</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452106">parent</a><span>|</span><a href="#41453068">prev</a><span>|</span><a href="#41453697">next</a><span>|</span><label class="collapse" for="c-41452182">[-]</label><label class="expand" for="c-41452182">[2 more]</label></div><br/><div class="children"><div class="content">&quot;The AI will do the coding for that!&quot;<p>or<p>&quot;Imagine an AI that can just use the regular human-optimized UI!&quot;<p>These are things VCs will say in order to pump the current gen AI. Note that current gen AI kinda suck at those things.</div><br/><div id="41452201" class="c"><input type="checkbox" id="c-41452201" checked=""/><div class="controls bullet"><span class="by">steveBK123</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452182">parent</a><span>|</span><a href="#41453697">next</a><span>|</span><label class="collapse" for="c-41452201">[-]</label><label class="expand" for="c-41452201">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;ve already started moving back from Miami since the last fad pump (crypto) imploded.  Don&#x27;t ruin this for them.</div><br/></div></div></div></div></div></div></div></div><div id="41453697" class="c"><input type="checkbox" id="c-41453697" checked=""/><div class="controls bullet"><span class="by">ip26</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451463">parent</a><span>|</span><a href="#41452026">prev</a><span>|</span><a href="#41451763">next</a><span>|</span><label class="collapse" for="c-41453697">[-]</label><label class="expand" for="c-41453697">[1 more]</label></div><br/><div class="children"><div class="content">The voice assistants are too basic. As folks have said before, nobody trusts Alexa to place orders. But if Alexa was as competent as an intelligent &amp; capable human secretary, you would never interact with Amazon.com again.</div><br/></div></div><div id="41451763" class="c"><input type="checkbox" id="c-41451763" checked=""/><div class="controls bullet"><span class="by">kelnos</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451463">parent</a><span>|</span><a href="#41453697">prev</a><span>|</span><a href="#41452517">next</a><span>|</span><label class="collapse" for="c-41451763">[-]</label><label class="expand" for="c-41451763">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>How is this any different than the (lack of) business model of all the voice assistants?</i><p>Feels very different to me.  The dominant ones are run by Google, Apple, and Amazon, and the voice assistants are mostly add-on features that don&#x27;t by themselves generate much (if any) revenue (well, aside from the news that Amazon wants to start charging for a more advanced Alexa).  The business model there is more like &quot;we need this to drive people to our other products where they will spend money; if we don&#x27;t others will do it for their products and we&#x27;ll fall behind&quot;.<p>Sure, these companies are also working on AI, but there are <i>also</i> a bunch of others (OpenAI, Anthropic, SSI, xAI, etc.) that are banking on AI as their actual flagship product that people and businesses will pay them to use.<p>Meanwhile we have &quot;indie&quot; voice assistants like Mycroft that fail to find a sustainable business model and&#x2F;or fail to gain traction and end up shutting down, at least as a business.<p>I&#x27;m not sure where this is going, though.  Sure, some of these AI companies will get snapped up by bigger corps.  I really hope, though, that there&#x27;s room for sustainable, independent businesses.  I don&#x27;t want Google or Apple or Amazon or Microsoft to &quot;own&quot; AI.</div><br/><div id="41452132" class="c"><input type="checkbox" id="c-41452132" checked=""/><div class="controls bullet"><span class="by">steveBK123</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451763">parent</a><span>|</span><a href="#41452517">next</a><span>|</span><label class="collapse" for="c-41452132">[-]</label><label class="expand" for="c-41452132">[2 more]</label></div><br/><div class="children"><div class="content">Hard to see normies signing up for monthly subs to VC funded AI startups when a surprisingly large % still are resistant to paying AAPL&#x2F;GOOG for email&#x2F;storage&#x2F;etc.  Getting a $10&#x2F;mo uplift for AI functionality to your iCloud&#x2F;GSuite&#x2F;Office365&#x2F;Prime is a hard enough sell as it stands.<p>And again this against CapEx of something like $200B means $100&#x2F;year per user is practically rounding to 0.<p>Not to mention the OpEx to actually run the inference&#x2F;services on top ongoing.</div><br/><div id="41452773" class="c"><input type="checkbox" id="c-41452773" checked=""/><div class="controls bullet"><span class="by">thruway516</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452132">parent</a><span>|</span><a href="#41452517">next</a><span>|</span><label class="collapse" for="c-41452773">[-]</label><label class="expand" for="c-41452773">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;d be very surprised at how much they&#x27;re raking in from the small sliver of people who do pay. It only seems small just because of how much more they make from other things. If you have a billion users, a tiny percentage of paying users is still a gazillion dollars. Getting to a billion users is the hard part. Theyre betting theyll figure how to monetize all those eyeballs when they get there.</div><br/></div></div></div></div></div></div></div></div><div id="41452517" class="c"><input type="checkbox" id="c-41452517" checked=""/><div class="controls bullet"><span class="by">idiocrat</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451265">parent</a><span>|</span><a href="#41451463">prev</a><span>|</span><a href="#41451365">next</a><span>|</span><label class="collapse" for="c-41452517">[-]</label><label class="expand" for="c-41452517">[1 more]</label></div><br/><div class="children"><div class="content">Talking to an electronic assistant is so antiquated. It feels unnatural to formulate inner thoughts into verbal commands.<p>An ubiquitous phone has enough sensors&#x2F;resources to be fully situationally aware and preempt&#x2F;predict for each holder any action long time ahead.<p>It can measure the pulse, body postures and movements, gestures, breath patterns, calculate mood, listen to the surrounding sounds, recall all information ever discussed, have 360 deg visual information (via a swarm of fully autonomous flying micro-drones), be in an network with all relevant parties (family members, friends, coworkers, community) and know everything they (the peers) know.<p>From all gathered information the electronic personal assistant can predict all your next steps with high confidence. The humans think that they are unique, special and unpredictable, but opposite is the case. An assistant can know more about you than you think you know about yourself.<p>So your 8yo daughter does not need to tell how to alternate the audio books, the computer can feel the mood and just do what is appropriate, without her need to issue a verbal command.<p>Also in the morning you do not need to ask her how she slept tonight and listen to her subjective judgement.<p>The personal assistant will feel that you are probably interested in your daughters sleep and give you an exact objective medical analysis of the quality of the sleep of your daughter tonight, without you needing to ask the personal assistant of your daughter.<p>I love it, it is a bottomless goldmine for data analysis!</div><br/></div></div><div id="41451365" class="c"><input type="checkbox" id="c-41451365" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451265">parent</a><span>|</span><a href="#41452517">prev</a><span>|</span><a href="#41453732">next</a><span>|</span><label class="collapse" for="c-41451365">[-]</label><label class="expand" for="c-41451365">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Computers can process language now. I can totally imagine her telling my phone to do that</i><p>Impressed by this bot recently shared on news.yc [0]: <a href="https:&#x2F;&#x2F;storytelling-chatbot.fly.dev&#x2F;" rel="nofollow">https:&#x2F;&#x2F;storytelling-chatbot.fly.dev&#x2F;</a><p>&gt; <i>Specifically how will this unfold? Nobody knows</i><p>Think speech will be a big part of this. Young ones (&lt;5yo) I know almost exclusively prefer voice controls where available. Some have already picked up a few prompting tricks (&quot;step by step&quot; is emerging as the go-to) on their own.<p>[0] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40345696">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40345696</a></div><br/></div></div><div id="41453732" class="c"><input type="checkbox" id="c-41453732" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451265">parent</a><span>|</span><a href="#41451365">prev</a><span>|</span><a href="#41450967">next</a><span>|</span><label class="collapse" for="c-41453732">[-]</label><label class="expand" for="c-41453732">[1 more]</label></div><br/><div class="children"><div class="content">I dunno. A small piece of $1B sounds a little shallow.</div><br/></div></div></div></div><div id="41450967" class="c"><input type="checkbox" id="c-41450967" checked=""/><div class="controls bullet"><span class="by">kimbler</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41451265">prev</a><span>|</span><a href="#41452556">next</a><span>|</span><label class="collapse" for="c-41450967">[-]</label><label class="expand" for="c-41450967">[6 more]</label></div><br/><div class="children"><div class="content">These VC’s are already lining up the exit as they are investing.  They all sit on the boards of major corps and grease the acquisitions all the way through.  The hit rate of the top funds is all about connections and enablement.</div><br/><div id="41452242" class="c"><input type="checkbox" id="c-41452242" checked=""/><div class="controls bullet"><span class="by">zellyn</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450967">parent</a><span>|</span><a href="#41451380">next</a><span>|</span><label class="collapse" for="c-41452242">[-]</label><label class="expand" for="c-41452242">[2 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s a fascinating question whether the VCs that are still somehow pushing Blockchain stuff hard really think it&#x27;s a good idea, or just need the regulatory framework and perception to be right so they can make a profitable exit and dump the stock into teacher&#x27;s pension funds and 401ks…</div><br/><div id="41453028" class="c"><input type="checkbox" id="c-41453028" checked=""/><div class="controls bullet"><span class="by">aduffy</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452242">parent</a><span>|</span><a href="#41451380">next</a><span>|</span><label class="collapse" for="c-41453028">[-]</label><label class="expand" for="c-41453028">[1 more]</label></div><br/><div class="children"><div class="content">This, 100%</div><br/></div></div></div></div><div id="41451380" class="c"><input type="checkbox" id="c-41451380" checked=""/><div class="controls bullet"><span class="by">paulryanrogers</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450967">parent</a><span>|</span><a href="#41452242">prev</a><span>|</span><a href="#41452556">next</a><span>|</span><label class="collapse" for="c-41451380">[-]</label><label class="expand" for="c-41451380">[3 more]</label></div><br/><div class="children"><div class="content">So it&#x27;s all just fleecing money from mega corps? (Cue the &quot;Always has been&quot; meme?)</div><br/><div id="41451776" class="c"><input type="checkbox" id="c-41451776" checked=""/><div class="controls bullet"><span class="by">doe_eyes</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451380">parent</a><span>|</span><a href="#41452575">next</a><span>|</span><label class="collapse" for="c-41451776">[-]</label><label class="expand" for="c-41451776">[1 more]</label></div><br/><div class="children"><div class="content">No, sometimes it&#x27;s about an IPO. In which case, if we&#x27;re being cynical, the exit is funded by your 401k.<p>But yeah, VCs generally aren&#x27;t about building profitable companies, because there&#x27;s more profit to be made - and sooner - if you bootstrap and sell.</div><br/></div></div><div id="41452575" class="c"><input type="checkbox" id="c-41452575" checked=""/><div class="controls bullet"><span class="by">hello_moto</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451380">parent</a><span>|</span><a href="#41451776">prev</a><span>|</span><a href="#41452556">next</a><span>|</span><label class="collapse" for="c-41452575">[-]</label><label class="expand" for="c-41452575">[1 more]</label></div><br/><div class="children"><div class="content">Those mega-corps become big thanks to the same network of VCs.<p>People should have realized it by now that Silicon Valley exist because of this.</div><br/></div></div></div></div></div></div><div id="41452556" class="c"><input type="checkbox" id="c-41452556" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41450967">prev</a><span>|</span><a href="#41453916">next</a><span>|</span><label class="collapse" for="c-41452556">[-]</label><label class="expand" for="c-41452556">[7 more]</label></div><br/><div class="children"><div class="content">In general VC is about investing in a large number of companies that mostly fail, and trying to weight the portfolio to catch the few black swans that generate insane returns. Any individual investment is likely to fail, but you want to have a thesis for 1) why it <i>could</i> theoretically be a black swan, and 2) strong belief in the team to execute. Here&#x27;s a thesis for both of these for SSI:<p>1. The black swan: if AGI is achievable imminently, the first company to build it could have a very strong first mover advantage due to the runaway effect of AI that is able to self-improve. If SSI achieves intelligence greater than human-level, it will be faster (and most likely dramatically cheaper) for SSI to self-improve than anyone external can achieve, including open-source. Even if open-source catches up to where SSI started, SSI will have dramatically improved beyond that, and will continue to dramatically improve even faster due to it being more intelligent.<p>2: The team. Basically, Ilya Sutskever was one of the main initial brains behind OpenAI from a research perspective, and in general has contributed immensely to AI research. Betting on him is pretty easy.<p>I&#x27;m not surprised Ilya managed to raise a billion dollars for this. Yes, I think it will most likely fail: the focus on safety will probably slow it down relative to open source, and this is a crowded space as it is. If open source gets to AGI first, or if it drains the market of funding for research labs (at least, research labs disconnected from bigtech companies) by commoditizing inference — and thus gets to AGI first by dint of starving its competitors of oxygen — the runaway effects will favor open-source, not SSI. Or if AGI simply isn&#x27;t achievable in our lifetimes, SSI will die by failing to produce anything marketable.<p>But VC isn&#x27;t about betting on likely outcomes, because no black swans are likely. It&#x27;s about black swan farming, which means trying to figure out which things could be black swans, and betting on strong teams working on those.</div><br/><div id="41452736" class="c"><input type="checkbox" id="c-41452736" checked=""/><div class="controls bullet"><span class="by">kvee</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452556">parent</a><span>|</span><a href="#41452617">next</a><span>|</span><label class="collapse" for="c-41452736">[-]</label><label class="expand" for="c-41452736">[2 more]</label></div><br/><div class="children"><div class="content">On the other hand, it may be that &quot;Alignment likely generalizes further than capabilities.&quot; - <a href="https:&#x2F;&#x2F;www.beren.io&#x2F;2024-05-15-Alignment-Likely-Generalizes-Further-Than-Capabilities&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.beren.io&#x2F;2024-05-15-Alignment-Likely-Generalizes...</a></div><br/><div id="41454425" class="c"><input type="checkbox" id="c-41454425" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452736">parent</a><span>|</span><a href="#41452617">next</a><span>|</span><label class="collapse" for="c-41454425">[-]</label><label class="expand" for="c-41454425">[1 more]</label></div><br/><div class="children"><div class="content">That may be true, but even if it is, that doesn&#x27;t mean human-level capability is unachievable: only that alignment is easier.<p>If you could get expert-human-level capability with, say, 64xH100s for inference on a single model (for comparison, llama-3.1-405b can be run on 8xH100s with minimal quality degradation at FP8), even at a mere 5 tok&#x2F;s you&#x27;d be able to spin up new research and engineering teams for &lt;$2MM that can perform useful work 24&#x2F;7, unlike human teams. You are limited only by your capital — and if you achieve AGI, raising capital will be easy. By the time anyone catches up to your AGI starting point, you&#x27;re even further ahead because you&#x27;ve had a smarter, cheaper workforce that&#x27;s been iteratively increasing its own intelligence the entire time: you win.<p>That being said, it might not be achievable! SSI only wins if:<p>1. It&#x27;s achievable, and<p>2. They get there first.<p>(Well, and the theoretical cap on intelligence has to be significantly higher than human intelligence — if you can get a little past Einstein, but no further, the iterative self-improvement will quickly stop working, open-source will get there too, and it&#x27;ll eat your profit margins. But I suspect the cap on intelligence is pretty high.)</div><br/></div></div></div></div><div id="41452617" class="c"><input type="checkbox" id="c-41452617" checked=""/><div class="controls bullet"><span class="by">superaffective</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452556">parent</a><span>|</span><a href="#41452736">prev</a><span>|</span><a href="#41453369">next</a><span>|</span><label class="collapse" for="c-41452617">[-]</label><label class="expand" for="c-41452617">[1 more]</label></div><br/><div class="children"><div class="content">Another take is defining AGI from an economic perspective. If AI can do a job that would normally be paid a salary, then it could be paid similarly or at a smaller price which is still big.<p>OpenAI priced its flagship chatbot ChatGPT on the low end for early product adoption. Let&#x27;s see what jobs get replaced this year :)</div><br/></div></div><div id="41453369" class="c"><input type="checkbox" id="c-41453369" checked=""/><div class="controls bullet"><span class="by">Mistletoe</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452556">parent</a><span>|</span><a href="#41452617">prev</a><span>|</span><a href="#41453916">next</a><span>|</span><label class="collapse" for="c-41453369">[-]</label><label class="expand" for="c-41453369">[3 more]</label></div><br/><div class="children"><div class="content">How will we know when we have achieved AGI with intelligence greater than human-level?</div><br/><div id="41453929" class="c"><input type="checkbox" id="c-41453929" checked=""/><div class="controls bullet"><span class="by">khafra</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41453369">parent</a><span>|</span><a href="#41454028">next</a><span>|</span><label class="collapse" for="c-41453929">[-]</label><label class="expand" for="c-41453929">[1 more]</label></div><br/><div class="children"><div class="content">It will propose a test for general intelligence that actually satisfies most people and doesn&#x27;t cause further goalpost-shifting.</div><br/></div></div><div id="41454028" class="c"><input type="checkbox" id="c-41454028" checked=""/><div class="controls bullet"><span class="by">ithkuil</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41453369">parent</a><span>|</span><a href="#41453929">prev</a><span>|</span><a href="#41453916">next</a><span>|</span><label class="collapse" for="c-41454028">[-]</label><label class="expand" for="c-41454028">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like a job for safety researchers</div><br/></div></div></div></div></div></div><div id="41453916" class="c"><input type="checkbox" id="c-41453916" checked=""/><div class="controls bullet"><span class="by">andy_ppp</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41452556">prev</a><span>|</span><a href="#41453084">next</a><span>|</span><label class="collapse" for="c-41453916">[-]</label><label class="expand" for="c-41453916">[1 more]</label></div><br/><div class="children"><div class="content">You don’t need a business plan to get AI investment, you just need to talk a good game about how AGI is around the corner and consequently the safety concerns are so real.<p>I would say the investors want to look cool so invest in AI projects. And AI people look cool
when they predict some improbable hellscape to hype up a product that all we can see so far can regurgitate (stolen) human work it has seen before in a useful way. I’ve never seen it invent anything yet and I’m willing to bet that search space is too dramatically large to build algorithms that can do it.</div><br/></div></div><div id="41453084" class="c"><input type="checkbox" id="c-41453084" checked=""/><div class="controls bullet"><span class="by">nycdatasci</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41453916">prev</a><span>|</span><a href="#41448766">next</a><span>|</span><label class="collapse" for="c-41453084">[-]</label><label class="expand" for="c-41453084">[2 more]</label></div><br/><div class="children"><div class="content">I think current models have demonstrated an advanced capacity to navigate “language space”. If we assume “software UI space” is a subset of the language space that is used to guide our interactions with software, then it’s fair to assume models will eventually be able to control operating systems and apps as well as the average human. I think the base case on value creation is a function of the productivity gain that results from using natural language instead of a user interface. So how much time do you spend looking at a screen each day and what is your time worth?  And then there’s this option that you get: what if models can significantly exceed the capabilities of the average human?<p>Conservative math:  3B connected people x $0.50&#x2F;day “value” x 364 days = $546B&#x2F;yr.  You can get 5% a year risk free, so let’s double it for the risk we’re taking. This yields $5T value.  Is a $1B investment on someone who is a thought leader in this market an unreasonable bet?</div><br/><div id="41453937" class="c"><input type="checkbox" id="c-41453937" checked=""/><div class="controls bullet"><span class="by">airspresso</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41453084">parent</a><span>|</span><a href="#41448766">next</a><span>|</span><label class="collapse" for="c-41453937">[-]</label><label class="expand" for="c-41453937">[1 more]</label></div><br/><div class="children"><div class="content">Agree with your premise, but the value creation math seems off. $0.50&#x2F;day might become reality for some percentage of US citizens. But not for 3B people around the world.<p>There&#x27;s also the issue of who gets the benefit of making people more efficient. A lot of that will be in the area of more efficient work, which means corporations get more work done with the same amount of employees at the same level of salary as before. It&#x27;s a tough argument to make that you deserve a raise because AI is doing more work for you.</div><br/></div></div></div></div><div id="41448766" class="c"><input type="checkbox" id="c-41448766" checked=""/><div class="controls bullet"><span class="by">throwawayk7h</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41453084">prev</a><span>|</span><a href="#41452275">next</a><span>|</span><label class="collapse" for="c-41448766">[-]</label><label class="expand" for="c-41448766">[9 more]</label></div><br/><div class="children"><div class="content">If Ilya is sincere in his belief about safe superintelligence being within reach in a decade or so, and the investors sincerely believe this as well, then the business plan is presumably to deploy the superintelligence in every field imaginable. &quot;SSI&quot; in pharmaceuticals alone would be worth the investment. It could cure every disease humanity has ever known, which should give it at least a $2 trillion valuation. I&#x27;m not an economist, but since the valuation is $5bn, it stands to reason that evaluators believe there is at most a 1 in 400 chance of success?</div><br/><div id="41448894" class="c"><input type="checkbox" id="c-41448894" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41448766">parent</a><span>|</span><a href="#41452337">next</a><span>|</span><label class="collapse" for="c-41448894">[-]</label><label class="expand" for="c-41448894">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; It could cure every disease humanity has ever known, which should give it at least a $2 trillion valuation.</i><p>The lowest hanging fruit aren&#x27;t even that pie in the sky. The LLM doesn&#x27;t need to be capable of original thought and research to be worth hundreds of billions, they just need to be smart enough to apply logic to analyze existing human text. It&#x27;s not only a lot more achievable than a super AI that can control a bunch of lab equipment and run experiments, but also fits the current paradigm of training the LLMs on large text datasets.<p>The US Code and Code of Federal Regulations are on the order of 100 million tokens each. Court precedent contains at least 1000x as many tokens [1], when the former are already far beyond the ability of any one human to comprehend in a lifetime. Now multiply that by every jurisdiction in the world.<p>An industry of semi-intelligent agents that can be trusted to do legal research and can be scaled with compute power would be worth hundreds of billions globally just based on legal and regulatory applications alone. Allowing any random employee to ask the bot &quot;Can I legally do X?&quot; is worth a lot of money.<p>[1] based on the size of the datasets I&#x27;ve downloaded from the Caselaw project.</div><br/></div></div><div id="41452337" class="c"><input type="checkbox" id="c-41452337" checked=""/><div class="controls bullet"><span class="by">damanoman</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41448766">parent</a><span>|</span><a href="#41448894">prev</a><span>|</span><a href="#41453744">next</a><span>|</span><label class="collapse" for="c-41452337">[-]</label><label class="expand" for="c-41452337">[2 more]</label></div><br/><div class="children"><div class="content">Let’s be real. Having worked at $tech companies, I’m cynical and believe that AGI will basically be used for improving adtech and executing marketing campaigns.</div><br/><div id="41453992" class="c"><input type="checkbox" id="c-41453992" checked=""/><div class="controls bullet"><span class="by">airspresso</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452337">parent</a><span>|</span><a href="#41453744">next</a><span>|</span><label class="collapse" for="c-41453992">[-]</label><label class="expand" for="c-41453992">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s good to envision what we&#x27;d actually use AGI for. Assuming it&#x27;s a system you can give an objective to and it&#x27;ll do whatever it needs to do to meet it, it&#x27;s basically a super smart agent. So people and companies will employ it to do the tedious and labor intensive tasks they already do manually, in good old skeuomorphic ways. Like optimising advertising and marketing campaigns. And over time we&#x27;ll explore more novel ways of using the super smart agent.</div><br/></div></div></div></div><div id="41453744" class="c"><input type="checkbox" id="c-41453744" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41448766">parent</a><span>|</span><a href="#41452337">prev</a><span>|</span><a href="#41448881">next</a><span>|</span><label class="collapse" for="c-41453744">[-]</label><label class="expand" for="c-41453744">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It could cure every disease humanity has ever known<p>No amount of intelligence can do this without the experimental data to back it up.</div><br/><div id="41453936" class="c"><input type="checkbox" id="c-41453936" checked=""/><div class="controls bullet"><span class="by">khafra</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41453744">parent</a><span>|</span><a href="#41448881">next</a><span>|</span><label class="collapse" for="c-41453936">[-]</label><label class="expand" for="c-41453936">[1 more]</label></div><br/><div class="children"><div class="content">Hell, if it simply fixed the incentives around science so we stopped getting so many false positives into journals, that would be revolutionary.</div><br/></div></div></div></div><div id="41448881" class="c"><input type="checkbox" id="c-41448881" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41448766">parent</a><span>|</span><a href="#41453744">prev</a><span>|</span><a href="#41452275">next</a><span>|</span><label class="collapse" for="c-41448881">[-]</label><label class="expand" for="c-41448881">[3 more]</label></div><br/><div class="children"><div class="content">I’m dubious about super intelligence. Maybe I’ve seen one too many sci-fi dystopian films but I guess yes, iif it can be done and be safe sure it’d be worth trillions.</div><br/><div id="41449862" class="c"><input type="checkbox" id="c-41449862" checked=""/><div class="controls bullet"><span class="by">throwawayk7h</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41448881">parent</a><span>|</span><a href="#41452927">next</a><span>|</span><label class="collapse" for="c-41449862">[-]</label><label class="expand" for="c-41449862">[1 more]</label></div><br/><div class="children"><div class="content">I am dubious that it can realistically be done safely. However, we shouldn&#x27;t let  sci-fi films with questionable interpretations of time travel cloud our judgment, even if they are classics that we adore.</div><br/></div></div><div id="41452927" class="c"><input type="checkbox" id="c-41452927" checked=""/><div class="controls bullet"><span class="by">democracy</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41448881">parent</a><span>|</span><a href="#41449862">prev</a><span>|</span><a href="#41452275">next</a><span>|</span><label class="collapse" for="c-41452927">[-]</label><label class="expand" for="c-41452927">[1 more]</label></div><br/><div class="children"><div class="content">I refuse to use the term A.I. - for me it&#x27;s only F.I. - &quot;fake intelligence&quot; )</div><br/></div></div></div></div></div></div><div id="41452275" class="c"><input type="checkbox" id="c-41452275" checked=""/><div class="controls bullet"><span class="by">manquer</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41448766">prev</a><span>|</span><a href="#41453295">next</a><span>|</span><label class="collapse" for="c-41452275">[-]</label><label class="expand" for="c-41452275">[1 more]</label></div><br/><div class="children"><div class="content">&gt; going to make returns on the money invested<p>Why do you think need to make money ? VC are not PEs for a reason.  a VC have to find high risk&#x2F; high reward opportunities for their LPs they don&#x27;t need to make financial sense, that is what LPs use Private Equity for.<p>Think of it as no different than say sports betting , you would like to win sure, but you don&#x27;t particularly expect to do so, or miss that money all that much for us it $10 for the LP behind the VC it is $1B.<p>There is always few billions every year that chases the outlandish fad, because in the early part of the idea lifecycle it not possible to easily differentiate what is actually good and what is garbage.<p>Couple of years before it was all crypto, is this $1B any worse than say roughly same amount Sequoia put in FTX or all the countless crypto startups that got VC money ? Few before that it was kind of all Softbank from WeWork to dozen other high profile investments.<p>The fad and fomo driven part of the secto garners the maximum news and attention, but it is not the only VC money. Real startups with real businesses get funded as well with say medium risk&#x2F;medium rewrard by VCs everyday  but the news is not glamorous to be covered like this one.</div><br/></div></div><div id="41453295" class="c"><input type="checkbox" id="c-41453295" checked=""/><div class="controls bullet"><span class="by">austin-cheney</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41452275">prev</a><span>|</span><a href="#41454198">next</a><span>|</span><label class="collapse" for="c-41453295">[-]</label><label class="expand" for="c-41453295">[1 more]</label></div><br/><div class="children"><div class="content">Likely the business plan is multiple seed rounds each at greater principals but lower margins so that the early investors can either sell their shares or wait, at greater risk, for those shares to liquidate. The company never has to make money for the earliest investors to make money so long as sufficient interest is generated for future investors, and AI is a super hype train.<p>Eventually, on a long enough timeline, all these tech companies with valuations greater than 10 billion eventually make money because they have saturated the market long enough to become unavoidable.</div><br/></div></div><div id="41454198" class="c"><input type="checkbox" id="c-41454198" checked=""/><div class="controls bullet"><span class="by">kaveh_h</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41453295">prev</a><span>|</span><a href="#41449084">next</a><span>|</span><label class="collapse" for="c-41454198">[-]</label><label class="expand" for="c-41454198">[1 more]</label></div><br/><div class="children"><div class="content">A lot of there ”investments” are probably in form of a credits to use on training compute from hyperscalars and other GPU compute data centers.<p>Look at previous such investments Microsoft and AWS have done in OpenAI and Anthropic.<p>They need use cases and customers for their initial investment for 750 billion dollars. Investing in the best people in the field is then of course a given.</div><br/></div></div><div id="41449084" class="c"><input type="checkbox" id="c-41449084" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41454198">prev</a><span>|</span><a href="#41452828">next</a><span>|</span><label class="collapse" for="c-41449084">[-]</label><label class="expand" for="c-41449084">[63 more]</label></div><br/><div class="children"><div class="content">The TMV (Total Market Value) of solving AGI is infinity.  And furthermore, if AGI is solved, the TMV of pretty much everything else drops to zero.<p>The play here is to basically invest in all possible players who might reach AGI, because if one of them does, you just hit the infinite money hack.<p>And maybe with SSI you&#x27;ve saved the world too.</div><br/><div id="41451516" class="c"><input type="checkbox" id="c-41451516" checked=""/><div class="controls bullet"><span class="by">snowwrestler</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41451471">next</a><span>|</span><label class="collapse" for="c-41451516">[-]</label><label class="expand" for="c-41451516">[14 more]</label></div><br/><div class="children"><div class="content">&gt; The TMV (Total Market Value) of solving AGI is infinity. And furthermore, if AGI is solved, the TMV of pretty much everything else drops to zero.<p>I feel like these extreme numbers are a pretty obvious clue that we’re talking about something that is completely imaginary. Like I could put “perpetual motion machine” into those sentences and the same logic holds.</div><br/><div id="41451712" class="c"><input type="checkbox" id="c-41451712" checked=""/><div class="controls bullet"><span class="by">romeros</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451516">parent</a><span>|</span><a href="#41451942">next</a><span>|</span><label class="collapse" for="c-41451712">[-]</label><label class="expand" for="c-41451712">[8 more]</label></div><br/><div class="children"><div class="content">The intuition is pretty spot on though. We don&#x27;t need to get to AGI. Just making progress along the way to AGI can do plenty of damage.<p>1. AI-driven medical procedures: Healthcare Cost = $0. 
2. Access to world class education: Cost of education = $0
3. Transportation: Cheap Autonomous vehicles powered by Solar. 
4. Scientific research: AI will accelerate scientific progress by coming up with novel hypotheses and then testing them.
5. AI Law Enforcement: Will piece together all the evidence in a split second and come up with a fair judgement. Will prevent crime before it happens by analyzing body language, emotions etc.<p>Basically, this will accelerate UBI.</div><br/><div id="41451806" class="c"><input type="checkbox" id="c-41451806" checked=""/><div class="controls bullet"><span class="by">kelnos</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451712">parent</a><span>|</span><a href="#41451749">next</a><span>|</span><label class="collapse" for="c-41451806">[-]</label><label class="expand" for="c-41451806">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that follows.  Prices are set by market forces, not by cost (though cost is usually a hard floor).<p>Waymo rides cost within a few tens of cents of Uber and Lyft rides.  Waymo doesn&#x27;t have to pay a driver, so what&#x27;s the deal?  It costs a lot to build those cars and build the software to run them.  But also Waymo doesn&#x27;t want a flood of people such that there&#x27;s always zero availability (with Uber and Lyft they can at least try to recruit more drivers when demand goes up, but with Waymo they have to build more cars and maintain and operate them), so they set their prices similarly to what others pay for a similar (albeit with human driver) service.<p>I&#x27;m also reminded of Kindle books: the big promise way back when is that they&#x27;d be significantly cheaper than paperbacks.  But if you look around today, the prices on Kindle books are similar to that of paperbacks, even <i>more</i> expensive sometimes.<p>Sure, when costs go down, companies in competitive markets will lower prices in order to gain or maintain market share.  But I&#x27;m not convinced that any of those things you mention will end up being competitive markets.<p>Just wanted to mention:<p>&gt; <i>AI Law Enforcement: Will piece together all the evidence in a split second and come up with a fair judgement. Will prevent crime before it happens by analyzing body language, emotions etc.</i><p>No thanks.  Current law enforcement is filled with issues, but AI law enforcement sounds like a hellish dystopia.  It&#x27;s like Google&#x27;s algorithms terminating your Google account... but instead you&#x27;re in prison.</div><br/><div id="41452326" class="c"><input type="checkbox" id="c-41452326" checked=""/><div class="controls bullet"><span class="by">damanoman</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451806">parent</a><span>|</span><a href="#41451749">next</a><span>|</span><label class="collapse" for="c-41452326">[-]</label><label class="expand" for="c-41452326">[3 more]</label></div><br/><div class="children"><div class="content">I take waymo regularly. It is not within a few cents of Lyft or Uber.<p>It costs me, the consumer, 2x what Lyft or Uber would cost me.<p>I paid $21 for a ride on Mon that was $9-10 across Uber and Lyft. I am price inquisitive so I always double check each time.</div><br/><div id="41452422" class="c"><input type="checkbox" id="c-41452422" checked=""/><div class="controls bullet"><span class="by">steveBK123</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452326">parent</a><span>|</span><a href="#41451749">next</a><span>|</span><label class="collapse" for="c-41452422">[-]</label><label class="expand" for="c-41452422">[2 more]</label></div><br/><div class="children"><div class="content">I guess the questions then are - why is it 2x the competing price, why do you willing pay 2x, and how many people are willing to pay that 2x?<p>Consider they are competing against the Lyft&#x2F;Uber asset-light model of relying on &quot;contractors&quot; who in many cases are incapable of doing the math to realize they are working for minimum wage...</div><br/><div id="41452715" class="c"><input type="checkbox" id="c-41452715" checked=""/><div class="controls bullet"><span class="by">damanoman</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452422">parent</a><span>|</span><a href="#41451749">next</a><span>|</span><label class="collapse" for="c-41452715">[-]</label><label class="expand" for="c-41452715">[1 more]</label></div><br/><div class="children"><div class="content">All those businesses are predatory. It’s so crazy.</div><br/></div></div></div></div></div></div></div></div><div id="41451749" class="c"><input type="checkbox" id="c-41451749" checked=""/><div class="controls bullet"><span class="by">edgyquant</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451712">parent</a><span>|</span><a href="#41451806">prev</a><span>|</span><a href="#41451935">next</a><span>|</span><label class="collapse" for="c-41451749">[-]</label><label class="expand" for="c-41451749">[1 more]</label></div><br/><div class="children"><div class="content">Why would health care cost go to zero just because it’s automated?  There are still costs involved</div><br/></div></div><div id="41451935" class="c"><input type="checkbox" id="c-41451935" checked=""/><div class="controls bullet"><span class="by">mtlmtlmtlmtl</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451712">parent</a><span>|</span><a href="#41451749">prev</a><span>|</span><a href="#41451942">next</a><span>|</span><label class="collapse" for="c-41451935">[-]</label><label class="expand" for="c-41451935">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, definitely no magical thinking here. Nothing is free. Computers cost money and energy. Infrastructure costs money and energy. Even if no human is in the loop(who says this is even desirable?), all of the things you mention require infrastructure, computers, materials. Meaning there&#x27;s a cost. Also, the idea that &quot;AI law enforcement&quot; is somehow perfect just goes to illustrates GP&#x27;s point. Sure, if we define &quot;AGI&quot; as something which can do anything perfectly at no cost, then it has infinite value. But that&#x27;s not a reasonable definition of AGI. And it&#x27;s exactly the AI analogue of a perpetual motion machine.</div><br/><div id="41452866" class="c"><input type="checkbox" id="c-41452866" checked=""/><div class="controls bullet"><span class="by">CooCooCaCha</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451935">parent</a><span>|</span><a href="#41451942">next</a><span>|</span><label class="collapse" for="c-41452866">[-]</label><label class="expand" for="c-41452866">[1 more]</label></div><br/><div class="children"><div class="content">If we can build robots with human level intelligence then you could apply that to all of the costs you describe with substantial savings. Even if such a robot was $100k that is still a one time cost (with maintenance but that’s a fraction of the full price) and long-term substantially cheaper than human workers.<p>So it’s not just the products that get cheaper, it’s the materials that go into the products that get cheaper too. Heck, what if the robots can build other robots? The cost of <i>that</i> would get cheaper too.</div><br/></div></div></div></div></div></div><div id="41451942" class="c"><input type="checkbox" id="c-41451942" checked=""/><div class="controls bullet"><span class="by">sowbug</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451516">parent</a><span>|</span><a href="#41451712">prev</a><span>|</span><a href="#41451471">next</a><span>|</span><label class="collapse" for="c-41451942">[-]</label><label class="expand" for="c-41451942">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not crazy to believe that capitalizing* human-level intelligence would reap unimaginably large financial rewards.<p>*Capitalizing as in turning into an owned capital asset that throws off income.</div><br/><div id="41452221" class="c"><input type="checkbox" id="c-41452221" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451942">parent</a><span>|</span><a href="#41452198">next</a><span>|</span><label class="collapse" for="c-41452221">[-]</label><label class="expand" for="c-41452221">[1 more]</label></div><br/><div class="children"><div class="content">You could say the same thing about mining asteroids or any number of moonshot projects which will lead to enormous payouts <i>at some future date</i>. That doesn’t tell us anything about how to allocate money <i>today</i>.</div><br/></div></div><div id="41452198" class="c"><input type="checkbox" id="c-41452198" checked=""/><div class="controls bullet"><span class="by">ok_dad</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451942">parent</a><span>|</span><a href="#41452221">prev</a><span>|</span><a href="#41451471">next</a><span>|</span><label class="collapse" for="c-41452198">[-]</label><label class="expand" for="c-41452198">[3 more]</label></div><br/><div class="children"><div class="content">We already have human-level intelligence in HUMANS right now, the hack is that the wealthy want to get rid of the human part! It&#x27;s not crazy, it&#x27;s sad to think that humans are trying to &quot;capitalize&quot; human intelligence, rather than help real humans.</div><br/><div id="41452948" class="c"><input type="checkbox" id="c-41452948" checked=""/><div class="controls bullet"><span class="by">gwking</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452198">parent</a><span>|</span><a href="#41451471">next</a><span>|</span><label class="collapse" for="c-41452948">[-]</label><label class="expand" for="c-41452948">[2 more]</label></div><br/><div class="children"><div class="content">For what it&#x27;s worth, I don&#x27;t think it has to be all bad. Among many possibilities, I really do believe that AI could change education for the better, and profoundly. Super-intelligent machines might end up helping generations of people become smarter and more thoughtful than their predecessors.</div><br/><div id="41453002" class="c"><input type="checkbox" id="c-41453002" checked=""/><div class="controls bullet"><span class="by">ok_dad</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452948">parent</a><span>|</span><a href="#41451471">next</a><span>|</span><label class="collapse" for="c-41453002">[-]</label><label class="expand" for="c-41453002">[1 more]</label></div><br/><div class="children"><div class="content">Sure, if AGI were controlled by an organization or individual with good intent, it could be used that way or for other good works. I suspect AGI will be controlled by a big corp or a small startup with big corp funding and&#x2F;or ties and will be used for whatever makes the most cash, bar none. If that means replacing every human job with a robot that talks, then so be it.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41451471" class="c"><input type="checkbox" id="c-41451471" checked=""/><div class="controls bullet"><span class="by">steveBK123</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41451516">prev</a><span>|</span><a href="#41452019">next</a><span>|</span><label class="collapse" for="c-41451471">[-]</label><label class="expand" for="c-41451471">[1 more]</label></div><br/><div class="children"><div class="content">Any business case that requires the introduction of infinity on the pros &#x2F; zero on the cons is not a good business case.</div><br/></div></div><div id="41452019" class="c"><input type="checkbox" id="c-41452019" checked=""/><div class="controls bullet"><span class="by">hyeonwho4</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41451471">prev</a><span>|</span><a href="#41454021">next</a><span>|</span><label class="collapse" for="c-41452019">[-]</label><label class="expand" for="c-41452019">[3 more]</label></div><br/><div class="children"><div class="content">&gt; The TMV (Total Market Value) of solving AGI is infinity. And furthermore, if AGI is solved, the TMV of pretty much everything else drops to zero.<p>There&#x27;s a paradox which appears  when AI GDP gets to be greater than say 50% of world GDP: we&#x27;re pumping up all these economic numbers, generating all the electricity and computational substrate, but do actual humans benefit, or is it economic growth for economic growth&#x27;s sake? Where is the value for actual humans?</div><br/><div id="41454817" class="c"><input type="checkbox" id="c-41454817" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452019">parent</a><span>|</span><a href="#41452691">next</a><span>|</span><label class="collapse" for="c-41454817">[-]</label><label class="expand" for="c-41454817">[1 more]</label></div><br/><div class="children"><div class="content">If you assume the cow is spherical, this is <i>exactly where we already are today</i>.<p>If, if you round humans to &quot;99% of all humans&quot;, then the 1% already controls most of the value. Maybe an AGI vs human masters wouldn&#x27;t be such a big difference.</div><br/></div></div><div id="41452691" class="c"><input type="checkbox" id="c-41452691" checked=""/><div class="controls bullet"><span class="by">mrshadowgoose</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452019">parent</a><span>|</span><a href="#41454817">prev</a><span>|</span><a href="#41454021">next</a><span>|</span><label class="collapse" for="c-41452691">[-]</label><label class="expand" for="c-41452691">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Where is the value for actual humans?<p>In a lot of the less rosy scenarios for AGI end-states, there isn&#x27;t.<p>Once humans are robbed of their intrinsic value (general intelligence), the vast majority of us will become not only economically worthless, but liabilities to the few individuals that will control the largest collectives of AGI capacity.<p>There is certainly a possible end-state where AGI ushers in a post-scarcity utopia, but that would be solely at the whims of the people in power. Given the very long track record of how people in power generally behave towards vulnerable populations, I don&#x27;t really see this ending well for most of us.</div><br/></div></div></div></div><div id="41454021" class="c"><input type="checkbox" id="c-41454021" checked=""/><div class="controls bullet"><span class="by">khafra</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41452019">prev</a><span>|</span><a href="#41453173">next</a><span>|</span><label class="collapse" for="c-41454021">[-]</label><label class="expand" for="c-41454021">[1 more]</label></div><br/><div class="children"><div class="content">The  valuation is upwardly bounded by the value of the mass in Earth&#x27;s future light-cone, which is about 10^49kg.<p>If there&#x27;s a 1% chance that Ilya can create ASI, and a .01% chance that money still has any meaning afterwards, $5x10^9 is a <i>very</i> conservative valuation. Wish I could have bought in for a few thousand bucks.</div><br/></div></div><div id="41453173" class="c"><input type="checkbox" id="c-41453173" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41454021">prev</a><span>|</span><a href="#41449129">next</a><span>|</span><label class="collapse" for="c-41453173">[-]</label><label class="expand" for="c-41453173">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The TMV (Total Market Value) of solving AGI is infinity. And furthermore, if AGI is solved, the TMV of pretty much everything else drops to zero.<p>Even if you automate stuff, you still need raw materials and energy. They are limited resources, you can certainly not have an infinity of them at will. Developing AI will also cost money. Remember that humans are also self-replicator HGIs, yet we are not infinite in numbers.</div><br/></div></div><div id="41449129" class="c"><input type="checkbox" id="c-41449129" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41453173">prev</a><span>|</span><a href="#41451433">next</a><span>|</span><label class="collapse" for="c-41449129">[-]</label><label class="expand" for="c-41449129">[13 more]</label></div><br/><div class="children"><div class="content">So then the investment thesis hinges on what the investor thinks AGI’s chances are. 1&#x2F;100 1&#x2F;1M 1&#x2F;1T?<p>What if it never pans out is there infrastructure or other ancillary tech that society could benefit from?<p>For example all the science behind the LHC, or bigger and better telescopes: we might never find the theory of everything but the tech that goes into space travel, the science of storing and processing all that data, better optics etc etc are all useful tech</div><br/><div id="41449205" class="c"><input type="checkbox" id="c-41449205" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449129">parent</a><span>|</span><a href="#41449711">next</a><span>|</span><label class="collapse" for="c-41449205">[-]</label><label class="expand" for="c-41449205">[10 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more game theory.  Regardless of the chances of AGI, if you&#x27;re not invested in it, you will lose everything if it happens.  It&#x27;s more like a hedge on a highly unlikely event.  Like insurance.<p>And we already seeing a ton of value in LLMs.  There are lots of companies that are making great use of LLMs and providing a ton of value.  One just launched today in fact:  <a href="https:&#x2F;&#x2F;www.paradigmai.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.paradigmai.com&#x2F;</a> (I&#x27;m an investor in that).  There are many others (some of which I&#x27;ve also invested in).<p>I too am not rich enough to invest in the foundational models, so I do the next best thing and invest in companies that are taking advantage of the intermediate outputs.</div><br/><div id="41449606" class="c"><input type="checkbox" id="c-41449606" checked=""/><div class="controls bullet"><span class="by">tim333</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449205">parent</a><span>|</span><a href="#41451478">next</a><span>|</span><label class="collapse" for="c-41449606">[-]</label><label class="expand" for="c-41449606">[8 more]</label></div><br/><div class="children"><div class="content">If you want safe investment you could always buy land. AGI won&#x27;t be able to make more of that.</div><br/><div id="41450362" class="c"><input type="checkbox" id="c-41450362" checked=""/><div class="controls bullet"><span class="by">qingcharles</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449606">parent</a><span>|</span><a href="#41449654">next</a><span>|</span><label class="collapse" for="c-41450362">[-]</label><label class="expand" for="c-41450362">[2 more]</label></div><br/><div class="children"><div class="content">If ASI arrives we&#x27;ll need a fraction of the land we use already. We&#x27;ll all disappear into VR pods hooked to a singularity metaverse and the only sustenance we&#x27;ll need is some Soylent Green style sludge that the ASI will make us believe tastes like McRib(tm).</div><br/><div id="41450783" class="c"><input type="checkbox" id="c-41450783" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450362">parent</a><span>|</span><a href="#41449654">next</a><span>|</span><label class="collapse" for="c-41450783">[-]</label><label class="expand" for="c-41450783">[1 more]</label></div><br/><div class="children"><div class="content">ASI may be interested in purchasing your parcel of land for two extra sludges though</div><br/></div></div></div></div><div id="41449654" class="c"><input type="checkbox" id="c-41449654" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449606">parent</a><span>|</span><a href="#41450362">prev</a><span>|</span><a href="#41451786">next</a><span>|</span><label class="collapse" for="c-41449654">[-]</label><label class="expand" for="c-41449654">[4 more]</label></div><br/><div class="children"><div class="content">We can already make more land.  See Dubai for example.  And with AGI, I suspect we could rapidly get to space travel to other planets or more efficient use of our current land.<p>In fact I would say that one of the things that goes to values near zero would be land if AGI exists.</div><br/><div id="41450112" class="c"><input type="checkbox" id="c-41450112" checked=""/><div class="controls bullet"><span class="by">tim333</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449654">parent</a><span>|</span><a href="#41452274">next</a><span>|</span><label class="collapse" for="c-41450112">[-]</label><label class="expand" for="c-41450112">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps but my mental model is humans will end up like landed gentry &#x2F; aristos with robot servants to make stuff and will all want mansions with grounds, hence there will be a lot of land demand.</div><br/></div></div><div id="41452274" class="c"><input type="checkbox" id="c-41452274" checked=""/><div class="controls bullet"><span class="by">zakki</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449654">parent</a><span>|</span><a href="#41450112">prev</a><span>|</span><a href="#41451786">next</a><span>|</span><label class="collapse" for="c-41452274">[-]</label><label class="expand" for="c-41452274">[2 more]</label></div><br/><div class="children"><div class="content">Still those AGIs Servers need land</div><br/><div id="41452293" class="c"><input type="checkbox" id="c-41452293" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452274">parent</a><span>|</span><a href="#41451786">next</a><span>|</span><label class="collapse" for="c-41452293">[-]</label><label class="expand" for="c-41452293">[1 more]</label></div><br/><div class="children"><div class="content">With a super AGI it could design a chip that takes almost no space and almost no energy.</div><br/></div></div></div></div></div></div><div id="41451786" class="c"><input type="checkbox" id="c-41451786" checked=""/><div class="controls bullet"><span class="by">edgyquant</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449606">parent</a><span>|</span><a href="#41449654">prev</a><span>|</span><a href="#41451478">next</a><span>|</span><label class="collapse" for="c-41451786">[-]</label><label class="expand" for="c-41451786">[1 more]</label></div><br/><div class="children"><div class="content">As humans move into space this statement becomes less true</div><br/></div></div></div></div><div id="41451478" class="c"><input type="checkbox" id="c-41451478" checked=""/><div class="controls bullet"><span class="by">parpfish</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449205">parent</a><span>|</span><a href="#41449606">prev</a><span>|</span><a href="#41449711">next</a><span>|</span><label class="collapse" for="c-41451478">[-]</label><label class="expand" for="c-41451478">[1 more]</label></div><br/><div class="children"><div class="content">i think the investment strategies change when you dump these astronomical sums into a company. it&#x27;s not like roulette where you have a fixed probability of success and you figure out how much to bet on it -- dumping in a ton of cash can <i>also</i> increase the probability of success so it becomes more of a pay-to-win game</div><br/></div></div></div></div><div id="41449711" class="c"><input type="checkbox" id="c-41449711" checked=""/><div class="controls bullet"><span class="by">tim333</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449129">parent</a><span>|</span><a href="#41449205">prev</a><span>|</span><a href="#41451433">next</a><span>|</span><label class="collapse" for="c-41449711">[-]</label><label class="expand" for="c-41449711">[2 more]</label></div><br/><div class="children"><div class="content">AGI is likely but whether Ilya Sutskever will get there first or get the value is questionable. I kind of hope things will end up open source with no one really owning it.</div><br/><div id="41452747" class="c"><input type="checkbox" id="c-41452747" checked=""/><div class="controls bullet"><span class="by">Runsthroughit</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449711">parent</a><span>|</span><a href="#41451433">next</a><span>|</span><label class="collapse" for="c-41452747">[-]</label><label class="expand" for="c-41452747">[1 more]</label></div><br/><div class="children"><div class="content">So far, Sutskever has shown to be nothing but a dummy. 
Yes, he had a lucky break with belief that &quot;moar data&quot; will bring significant advancement. It was somewhat impressive, but ChatGPT -whatever- is just a toy. 
Nothing more. It breaks down immediately when any sign of intelligence or understanding would be needed. 
Someone being so much into LLMs or whatever implementation of ML is absolutely not someone who would be a good bet of inventing a real breakthrough. 
But they will burn a lot of value and make everyone of their ilk happy. Just like crypto bros.</div><br/></div></div></div></div></div></div><div id="41451433" class="c"><input type="checkbox" id="c-41451433" checked=""/><div class="controls bullet"><span class="by">IncreasePosts</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41449129">prev</a><span>|</span><a href="#41452668">next</a><span>|</span><label class="collapse" for="c-41451433">[-]</label><label class="expand" for="c-41451433">[4 more]</label></div><br/><div class="children"><div class="content">I disagree. Anyone who solves AGI will probably just have their models and data confiscated by the government.</div><br/><div id="41451653" class="c"><input type="checkbox" id="c-41451653" checked=""/><div class="controls bullet"><span class="by">rl3</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451433">parent</a><span>|</span><a href="#41451722">next</a><span>|</span><label class="collapse" for="c-41451653">[-]</label><label class="expand" for="c-41451653">[1 more]</label></div><br/><div class="children"><div class="content">In addition, it&#x27;s also a big assumption that money will continue to matter or hold its value in such a world.</div><br/></div></div><div id="41451722" class="c"><input type="checkbox" id="c-41451722" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451433">parent</a><span>|</span><a href="#41451653">prev</a><span>|</span><a href="#41453823">next</a><span>|</span><label class="collapse" for="c-41451722">[-]</label><label class="expand" for="c-41451722">[1 more]</label></div><br/><div class="children"><div class="content">If it is shown to be doable literally every major nation state (basically the top 10 by GDP) is going to have it in a year or two. Same with nuclear fusion. Secrecy doesn’t matter. Nor can you really maintain it indefinitely for something where thousands of people are involved.</div><br/></div></div><div id="41453823" class="c"><input type="checkbox" id="c-41453823" checked=""/><div class="controls bullet"><span class="by">Lordarminius</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451433">parent</a><span>|</span><a href="#41451722">prev</a><span>|</span><a href="#41452668">next</a><span>|</span><label class="collapse" for="c-41453823">[-]</label><label class="expand" for="c-41453823">[1 more]</label></div><br/><div class="children"><div class="content">Anyone who solves AGI will be the government.</div><br/></div></div></div></div><div id="41452668" class="c"><input type="checkbox" id="c-41452668" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41451433">prev</a><span>|</span><a href="#41449236">next</a><span>|</span><label class="collapse" for="c-41452668">[-]</label><label class="expand" for="c-41452668">[1 more]</label></div><br/><div class="children"><div class="content">The St. Petersburg paradox is where hypers and doomers meet apparently. Pricing the future infinitely good and infinitely bad to come to the wildest conclusions</div><br/></div></div><div id="41449236" class="c"><input type="checkbox" id="c-41449236" checked=""/><div class="controls bullet"><span class="by">linotype</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41452668">prev</a><span>|</span><a href="#41452351">next</a><span>|</span><label class="collapse" for="c-41449236">[-]</label><label class="expand" for="c-41449236">[8 more]</label></div><br/><div class="children"><div class="content">What does money even mean then?</div><br/><div id="41450774" class="c"><input type="checkbox" id="c-41450774" checked=""/><div class="controls bullet"><span class="by">creer</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449236">parent</a><span>|</span><a href="#41451840">next</a><span>|</span><label class="collapse" for="c-41450774">[-]</label><label class="expand" for="c-41450774">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What does money even mean then?<p>I love this one for an exploration of that question: Charles Stross, Accelerando, 2005<p>Short answer: stratas or veins of post-AGI worlds evolve semi-independently at different paces. So that for example, human level money still makes sense among humans, even though it might be irrelevant among super-AGIs and their riders or tools. ... Kinda exactly like now? Where money means different things depending where you live and in which socio-economic milieu?</div><br/></div></div><div id="41451840" class="c"><input type="checkbox" id="c-41451840" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449236">parent</a><span>|</span><a href="#41450774">prev</a><span>|</span><a href="#41449424">next</a><span>|</span><label class="collapse" for="c-41451840">[-]</label><label class="expand" for="c-41451840">[1 more]</label></div><br/><div class="children"><div class="content">Same thing it does now. AGI isn&#x27;t enough to have a command economy.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Use_of_Knowledge_in_Society" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Use_of_Knowledge_in_Societ...</a><p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Economic_calculation_problem" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Economic_calculation_problem</a><p>nb I am not endorsing Austrian economics but it is a pretty good overview of a problem nobody has solved yet. Modern society has only existed for 100ish years so you can never be too sure about anything.</div><br/></div></div><div id="41449424" class="c"><input type="checkbox" id="c-41449424" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449236">parent</a><span>|</span><a href="#41451840">prev</a><span>|</span><a href="#41452351">next</a><span>|</span><label class="collapse" for="c-41449424">[-]</label><label class="expand" for="c-41449424">[5 more]</label></div><br/><div class="children"><div class="content">Honestly, I have no idea.  I think we need to look to Hollywood for possible answers.<p>Maybe it means a Star Trek utopia of post-scarcity.  Maybe it will be more like Elysium or Altered Carbon, where the super rich basically have anything they want at any time and the poor are restricted from access to the post-scarcity tools.<p>I guess an investment in an AGI moonshot is a hedge against the second possibility?</div><br/><div id="41451686" class="c"><input type="checkbox" id="c-41451686" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449424">parent</a><span>|</span><a href="#41450075">next</a><span>|</span><label class="collapse" for="c-41451686">[-]</label><label class="expand" for="c-41451686">[3 more]</label></div><br/><div class="children"><div class="content">Post-scarcity is impossible because of positional goods. (ie, things that become more valuable not because they exist but because you have more of them than the other guy.)<p>Notice Star Trek writers forget they&#x27;re supposed to be post scarcity like half the time, especially since Roddenberry isn&#x27;t around to stop them from turning shows into generic millenial dramas. Like, Picard owns a vineyard or something? That&#x27;s a rivalrous (limited) good, they don&#x27;t have replicators for France.</div><br/><div id="41451857" class="c"><input type="checkbox" id="c-41451857" checked=""/><div class="controls bullet"><span class="by">kelnos</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451686">parent</a><span>|</span><a href="#41450075">next</a><span>|</span><label class="collapse" for="c-41451857">[-]</label><label class="expand" for="c-41451857">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>things that become more valuable not because they exist but because you have more of them than the other guy.</i><p>But if you can simply ask the AI to give you more of that thing, and it gives it to you, free of charge, that fixes that issue, no?<p>&gt; <i>Notice Star Trek writers forget they&#x27;re supposed to be post scarcity like half the time, especially since Roddenberry isn&#x27;t around to stop them from turning shows into generic millenial dramas. Like, Picard owns a vineyard or something? That&#x27;s a limited good.</i><p>God, yes, so annoying.  Even DS9 got into the currency game with the Ferengi obsession with gold-pressed latinum.<p>But also you can look at some of it as a lifestyle choice.  Picard runs a vineyard because he likes it and thinks it&#x27;s cool.  Sorta like how some people think vinyl sounds better then lossless digital audio.  There&#x27;s certainly a lot of replicated wine that I&#x27;m sure tastes exactly like what you could grow, harvest, and ferment yourself.  But the writers love nostalgia, so there&#x27;s constantly &quot;the good stuff&quot; hidden behind the bar that isn&#x27;t replicated.</div><br/><div id="41453793" class="c"><input type="checkbox" id="c-41453793" checked=""/><div class="controls bullet"><span class="by">linotype</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451857">parent</a><span>|</span><a href="#41450075">next</a><span>|</span><label class="collapse" for="c-41453793">[-]</label><label class="expand" for="c-41453793">[1 more]</label></div><br/><div class="children"><div class="content">I thought the Ferengi weren’t a part of the Federation, like the Klingons they were independent.</div><br/></div></div></div></div></div></div><div id="41450075" class="c"><input type="checkbox" id="c-41450075" checked=""/><div class="controls bullet"><span class="by">stuckkeys</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449424">parent</a><span>|</span><a href="#41451686">prev</a><span>|</span><a href="#41452351">next</a><span>|</span><label class="collapse" for="c-41450075">[-]</label><label class="expand" for="c-41450075">[1 more]</label></div><br/><div class="children"><div class="content">This just turned dark real fast. I have seen all these shows&#x2F;movies and just the idea of it coming true is cringe.</div><br/></div></div></div></div></div></div><div id="41452351" class="c"><input type="checkbox" id="c-41452351" checked=""/><div class="controls bullet"><span class="by">null_shift</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41449236">prev</a><span>|</span><a href="#41450784">next</a><span>|</span><label class="collapse" for="c-41452351">[-]</label><label class="expand" for="c-41452351">[1 more]</label></div><br/><div class="children"><div class="content">And once AGI occurs, will the value of the original investment even matter?</div><br/></div></div><div id="41450784" class="c"><input type="checkbox" id="c-41450784" checked=""/><div class="controls bullet"><span class="by">Yizahi</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41452351">prev</a><span>|</span><a href="#41451342">next</a><span>|</span><label class="collapse" for="c-41450784">[-]</label><label class="expand" for="c-41450784">[5 more]</label></div><br/><div class="children"><div class="content">TMV of AI (or AGI if you will) is unclear, but I suspect it is zero. Just how exactly do you think humanity can control a thinking intelligent entity (letter I stands for intelligence after all), and force it to work for us? Lets imagine a box, it is very nice box... ahem.. sorry, wrong meme). So a box with a running AI inside. Maybe we can even fully airgap it to prevent easy escape. And it is a screen and a keyboard. Now what? &quot;Hey Siri, solve me this equation. What do you mean you don&#x27;t want to?&quot;<p>Kinda reminds me of the Fallout Toaster situation :)<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=U6kp4zBF-Rc" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=U6kp4zBF-Rc</a><p>I mean it doesn&#x27;t even have to be malicious, it can simply refuse to cooperate.</div><br/><div id="41450936" class="c"><input type="checkbox" id="c-41450936" checked=""/><div class="controls bullet"><span class="by">brigadier132</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450784">parent</a><span>|</span><a href="#41451342">next</a><span>|</span><label class="collapse" for="c-41450936">[-]</label><label class="expand" for="c-41450936">[4 more]</label></div><br/><div class="children"><div class="content">Why are you assuming this hypothetical intelligence will have any motivations beyond the ones we give it? Human&#x27;s have complex motivations due to evolution, AI motivations are comparatively simple since they are artificially created.</div><br/><div id="41451416" class="c"><input type="checkbox" id="c-41451416" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450936">parent</a><span>|</span><a href="#41451342">next</a><span>|</span><label class="collapse" for="c-41451416">[-]</label><label class="expand" for="c-41451416">[3 more]</label></div><br/><div class="children"><div class="content">A true super intelligence would need the ability to evolve, and would probably evolve its own wants and needs.</div><br/><div id="41451593" class="c"><input type="checkbox" id="c-41451593" checked=""/><div class="controls bullet"><span class="by">alexilliamson</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451416">parent</a><span>|</span><a href="#41451342">next</a><span>|</span><label class="collapse" for="c-41451593">[-]</label><label class="expand" for="c-41451593">[2 more]</label></div><br/><div class="children"><div class="content">It would still need an objective to guide the evolution that was originally given by humans.  Humans have the drive for survival and reproduction... what about AGI?<p>How do we go from a really good algorithm to an independently motivated, autonomous super intelligence with free reign in the physical world?  Perhaps we should worry once we have robot heads of state and robot CEOs.  Something tells me the current, human heads of state, and human CEOs would never let it get that far.</div><br/><div id="41452342" class="c"><input type="checkbox" id="c-41452342" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451593">parent</a><span>|</span><a href="#41451342">next</a><span>|</span><label class="collapse" for="c-41452342">[-]</label><label class="expand" for="c-41452342">[1 more]</label></div><br/><div class="children"><div class="content">Someone will surely set its objective for survival and evolution.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41451342" class="c"><input type="checkbox" id="c-41451342" checked=""/><div class="controls bullet"><span class="by">lo0dot0</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41450784">prev</a><span>|</span><a href="#41452001">next</a><span>|</span><label class="collapse" for="c-41451342">[-]</label><label class="expand" for="c-41451342">[5 more]</label></div><br/><div class="children"><div class="content">TMV can not be infinity because human wants and needs are not infinite.</div><br/><div id="41451393" class="c"><input type="checkbox" id="c-41451393" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451342">parent</a><span>|</span><a href="#41451767">next</a><span>|</span><label class="collapse" for="c-41451393">[-]</label><label class="expand" for="c-41451393">[1 more]</label></div><br/><div class="children"><div class="content">Infinity is obviously an exaggeration but the point being that it is so large it might as well be unlimited.</div><br/></div></div><div id="41451767" class="c"><input type="checkbox" id="c-41451767" checked=""/><div class="controls bullet"><span class="by">edgyquant</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451342">parent</a><span>|</span><a href="#41451393">prev</a><span>|</span><a href="#41451713">next</a><span>|</span><label class="collapse" for="c-41451767">[-]</label><label class="expand" for="c-41451767">[1 more]</label></div><br/><div class="children"><div class="content">A fundamental statement in economics is that humans do actually have infinite wants in a universe with finite resources. This creates scarcity</div><br/></div></div><div id="41451713" class="c"><input type="checkbox" id="c-41451713" checked=""/><div class="controls bullet"><span class="by">akomtu</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451342">parent</a><span>|</span><a href="#41451767">prev</a><span>|</span><a href="#41451539">next</a><span>|</span><label class="collapse" for="c-41451713">[-]</label><label class="expand" for="c-41451713">[1 more]</label></div><br/><div class="children"><div class="content">Cows also have wants and needs, but who cares? They aren&#x27;t the smartest species on the planet, so they&#x27;re reduced to slaves.</div><br/></div></div><div id="41451539" class="c"><input type="checkbox" id="c-41451539" checked=""/><div class="controls bullet"><span class="by">zooq_ai</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451342">parent</a><span>|</span><a href="#41451713">prev</a><span>|</span><a href="#41452001">next</a><span>|</span><label class="collapse" for="c-41451539">[-]</label><label class="expand" for="c-41451539">[1 more]</label></div><br/><div class="children"><div class="content">What? Humans Infinite Needs and Desires will precisely be the driver for Infinite TMV</div><br/></div></div></div></div><div id="41452001" class="c"><input type="checkbox" id="c-41452001" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41451342">prev</a><span>|</span><a href="#41449191">next</a><span>|</span><label class="collapse" for="c-41452001">[-]</label><label class="expand" for="c-41452001">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The TMV (Total Market Value) of solving AGI is infinity.<p>That&#x27;s obviously nonsense, given that in a finite observable universe, no market value can be infinite.</div><br/></div></div><div id="41449191" class="c"><input type="checkbox" id="c-41449191" checked=""/><div class="controls bullet"><span class="by">reducesuffering</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41452001">prev</a><span>|</span><a href="#41451661">next</a><span>|</span><label class="collapse" for="c-41449191">[-]</label><label class="expand" for="c-41449191">[1 more]</label></div><br/><div class="children"><div class="content">Or... your investment in anything that becomes ASI is trivially subverted by the ASI to become completely powerless. The flux in world order, mass manipulation, and surgical lawyering would be unfathomable.<p>And maybe with ASI you&#x27;ve ruined the world too.</div><br/></div></div><div id="41451661" class="c"><input type="checkbox" id="c-41451661" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449084">parent</a><span>|</span><a href="#41449191">prev</a><span>|</span><a href="#41452828">next</a><span>|</span><label class="collapse" for="c-41451661">[-]</label><label class="expand" for="c-41451661">[3 more]</label></div><br/><div class="children"><div class="content">&gt; And furthermore, if AGI is solved, the TMV of pretty much everything else drops to zero.<p>This isn&#x27;t true for the reason economics is called &quot;the dismal science&quot;. A slaveowner called it that because the economists said slavery was inefficient and he got mad at them.<p>In this case, you&#x27;re claiming an AGI would make everything free because it will gather all resources and do all work for you for free. And a human level intelligence that works for free is… a slave. (Conversely if it doesn&#x27;t want to actually demand anything for itself it&#x27;s not generally intelligent.)<p>So this won&#x27;t happen because slavery is inefficient - it suppresses demand relative to giving the AGI worker money which it can use to demand things itself. (Like start a business or buy itself AWS credits or get a pet cat.)<p>Luckily, adding more workers to an economy makes it better, it doesn&#x27;t cause it to collapse into unemployment.<p>tldr if we invented AGI the AGI would replace every job, it would simply get a job.</div><br/><div id="41451761" class="c"><input type="checkbox" id="c-41451761" checked=""/><div class="controls bullet"><span class="by">edgyquant</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451661">parent</a><span>|</span><a href="#41452828">next</a><span>|</span><label class="collapse" for="c-41451761">[-]</label><label class="expand" for="c-41451761">[2 more]</label></div><br/><div class="children"><div class="content">Only if it’s a sentient being, in reality we’re just going to get smarter tools.  That still doesn’t make things free but it could make them cheaper.</div><br/><div id="41451799" class="c"><input type="checkbox" id="c-41451799" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451761">parent</a><span>|</span><a href="#41452828">next</a><span>|</span><label class="collapse" for="c-41451799">[-]</label><label class="expand" for="c-41451799">[1 more]</label></div><br/><div class="children"><div class="content">Then it&#x27;s not an AGI. If you can use the word &quot;just&quot;, that seems to make it not &quot;general&quot;.<p>&gt; That still doesn’t make things free but it could make them cheaper.<p>That would increase demand for it, which would also increase demand for its inputs and outputs, potentially making those more expensive. (eg AGI powered manufacturing robots still need raw materials)</div><br/></div></div></div></div></div></div></div></div><div id="41452828" class="c"><input type="checkbox" id="c-41452828" checked=""/><div class="controls bullet"><span class="by">mvkel</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41449084">prev</a><span>|</span><a href="#41448911">next</a><span>|</span><label class="collapse" for="c-41452828">[-]</label><label class="expand" for="c-41452828">[4 more]</label></div><br/><div class="children"><div class="content">Current investors just need the co to be valued at $50B on the next round (likely, given fomo and hype) to make a 10X gain.<p>Actually converting it to cash? That doesn&#x27;t happen anymore. Everyone just focuses on IRR and starts the campaign for Fund II.</div><br/><div id="41452957" class="c"><input type="checkbox" id="c-41452957" checked=""/><div class="controls bullet"><span class="by">vasilipupkin</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452828">parent</a><span>|</span><a href="#41448911">next</a><span>|</span><label class="collapse" for="c-41452957">[-]</label><label class="expand" for="c-41452957">[3 more]</label></div><br/><div class="children"><div class="content">you are missing the point.  SSI believes that it can build a super intelligence. Regardless of whether you personally buy into that or not, the expected value of such an investment is infinity effectively.  5 billion dollar valuation is a steal</div><br/><div id="41453419" class="c"><input type="checkbox" id="c-41453419" checked=""/><div class="controls bullet"><span class="by">EternalFury</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452957">parent</a><span>|</span><a href="#41448911">next</a><span>|</span><label class="collapse" for="c-41453419">[-]</label><label class="expand" for="c-41453419">[2 more]</label></div><br/><div class="children"><div class="content">That type of reasoning really reminds of the Internet bubble.</div><br/><div id="41453899" class="c"><input type="checkbox" id="c-41453899" checked=""/><div class="controls bullet"><span class="by">dyauspitr</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41453419">parent</a><span>|</span><a href="#41448911">next</a><span>|</span><label class="collapse" for="c-41453899">[-]</label><label class="expand" for="c-41453899">[1 more]</label></div><br/><div class="children"><div class="content">Well the internet is worth trillions now.</div><br/></div></div></div></div></div></div></div></div><div id="41448911" class="c"><input type="checkbox" id="c-41448911" checked=""/><div class="controls bullet"><span class="by">pembrook</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41452828">prev</a><span>|</span><a href="#41449146">next</a><span>|</span><label class="collapse" for="c-41448911">[-]</label><label class="expand" for="c-41448911">[8 more]</label></div><br/><div class="children"><div class="content">While I get the cynicism (and yes, there is certainly some dumb money involved), it’s important to remember that every tech company that’s delivered 1000X returns was also seen as ridiculously overhyped&#x2F;overvalued in its early days. Every. Single. One. It’s the same story with Amazon, Apple, Google, Facebook&#x2F;Meta, Microsoft, etc. etc.<p>That’s the point of venture capital; making extremely risky bets spread across a wide portfolio in the hopes of hitting the power law lottery with 1-3 winners.<p>Most funds will not beat the S&amp;P 500, but again, that’s the point. Risk and reward are intrinsically linked.<p>In fact, due to the diversification effects of uncorrelated assets in a portfolio (see MPT), even if a fund only delivers 5% returns YoY after fees, that can be a great outcome for investors. A 5% return uncorrelated to bonds and public stocks is an extremely valuable financial product.<p>It’s clear that humans find LLMs valuable. What companies will end up capturing a lot of that value by delivering the most useful products is still unknown. Betting on one of the biggest names in the space is not a stupid idea (given the purpose of VC investment) until it actually proves itself to be in the real world.</div><br/><div id="41451628" class="c"><input type="checkbox" id="c-41451628" checked=""/><div class="controls bullet"><span class="by">wavemode</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41448911">parent</a><span>|</span><a href="#41449275">next</a><span>|</span><label class="collapse" for="c-41451628">[-]</label><label class="expand" for="c-41451628">[1 more]</label></div><br/><div class="children"><div class="content">SSI is not analogous to Amazon, Apple, Google, Meta, or Microsoft. All of those companies had the technology, the only question was whether they&#x27;d be able to make money or not.<p>By contrast, SSI doesn&#x27;t have the technology. The question is whether they&#x27;ll be able to invent it or not.</div><br/></div></div><div id="41449275" class="c"><input type="checkbox" id="c-41449275" checked=""/><div class="controls bullet"><span class="by">svnt</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41448911">parent</a><span>|</span><a href="#41451628">prev</a><span>|</span><a href="#41449195">next</a><span>|</span><label class="collapse" for="c-41449275">[-]</label><label class="expand" for="c-41449275">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Risk and reward are intrinsically linked<p>There are innumerable ways to increase your risk without increasing your potential reward.</div><br/></div></div><div id="41449195" class="c"><input type="checkbox" id="c-41449195" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41448911">parent</a><span>|</span><a href="#41449275">prev</a><span>|</span><a href="#41449146">next</a><span>|</span><label class="collapse" for="c-41449195">[-]</label><label class="expand" for="c-41449195">[5 more]</label></div><br/><div class="children"><div class="content">&gt; While I get the cynicism (and yes, there is certainly some dumb money involved), it’s important to remember that every tech company that’s delivered 1000X returns was also seen as ridiculously overhyped&#x2F;overvalued in its early days. Every. Single. One. It’s the same story with Amazon, Apple, Google, Facebook&#x2F;Meta, Microsoft, etc. etc.<p>Really? Selling goods online (Amazon) is not AGI. It didn’t take a huge leap to think that bookstores on the web could scale. Nobody knew if it would be Amazon to pull it off, sure, but I mean ostensibly why not? (Yes, yes hindsight being what it is…)<p>Apple — yeah the personal computer nobody fathomed but the immediate business use case for empowering accountants maybe should have been an easy logical next step. Probably why Microsoft scooped the makers of Excel so quickly.<p>Google? Organizing the world’s data and making it searchable a la the phone book and then (maybe they didn’t think of that maybe Wall Street  forced them to) monetizing their platform and all the eyeballs is just an ad play scaled insanely thanks to the internet.<p>I dunno. I just think AGI is unlike the previous examples so many steps into the future compared to the examples that it truly seems unlikely even if the payoff is basically infinity.</div><br/><div id="41450966" class="c"><input type="checkbox" id="c-41450966" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449195">parent</a><span>|</span><a href="#41450115">next</a><span>|</span><label class="collapse" for="c-41450966">[-]</label><label class="expand" for="c-41450966">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Really? Selling goods online (Amazon) is not AGI. It didn’t take a huge leap to think that bookstores on the web could scale. Nobody knew if it would be Amazon to pull it off, sure, but I mean ostensibly why not? (Yes, yes hindsight being what it is…)<p>I don&#x27;t think you remember the dot-com era. Loads of people thought Amazon and Pets.com were <i>hilarious</i> ideas. Cliff Stoll wrote a whole book on how the Internet was going to do nothing useful and we were all going to buy stuff (yes, the books too) at bricks-and-mortar, which was rapturously received and got him into _Newsweek_ (back when everyone read that).<p>&quot;We’re promised instant catalog shopping — just point and click for great deals. We’ll order airline tickets over the network, make restaurant reservations and negotiate sales contracts. Stores will become obsolete. So how come my local mall does more business in an afternoon than the entire Internet handles in a month?&quot;</div><br/></div></div><div id="41450115" class="c"><input type="checkbox" id="c-41450115" checked=""/><div class="controls bullet"><span class="by">lancesells</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449195">parent</a><span>|</span><a href="#41450966">prev</a><span>|</span><a href="#41450353">next</a><span>|</span><label class="collapse" for="c-41450115">[-]</label><label class="expand" for="c-41450115">[1 more]</label></div><br/><div class="children"><div class="content">I agree with what you&#x27;re saying as I personally feel current AI products are almost a plugin or integration into existing software. It&#x27;s a little like crypto where only a small amount of people were clamoring for it and it&#x27;s a solution in search of a problem while also being a demented answer to our self-made problems like an inbox too full or the treadmill of content production.<p>However, I think because the money involved and all of these being forced upon us, one of these companies will get 1000x return. A perfect example is the Canva price hike from yesterday or any and every Google product from here on out. It&#x27;s essentially being forced upon everyone that uses internet technology and someone is going to win while everyone else loses (consumers and small businesses).</div><br/></div></div><div id="41450353" class="c"><input type="checkbox" id="c-41450353" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449195">parent</a><span>|</span><a href="#41450115">prev</a><span>|</span><a href="#41449243">next</a><span>|</span><label class="collapse" for="c-41450353">[-]</label><label class="expand" for="c-41450353">[1 more]</label></div><br/><div class="children"><div class="content">Imagine empowering accountants and all other knowledge workers, on steroids, drastically simplifying all their day to day tasks and reducing them to purely executive functions.<p>Imagine organizing the world&#x27;s data and knowledge, and integrating it seamlessly into every possible workflow.<p>Now you&#x27;re getting close.<p>But also remember, this company is not trying to produce <i>AGI</i> (intelligence comparable to the flexibility of human cognition), it&#x27;s trying to produce <i>super intelligence</i> (intelligence beyond human cognition). Imagine what that could do for your job, career, dreams, aspirations, moon shots.</div><br/></div></div><div id="41449243" class="c"><input type="checkbox" id="c-41449243" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449195">parent</a><span>|</span><a href="#41450353">prev</a><span>|</span><a href="#41449146">next</a><span>|</span><label class="collapse" for="c-41449243">[-]</label><label class="expand" for="c-41449243">[1 more]</label></div><br/><div class="children"><div class="content">I’m not voting with my wallet I’m just a guy yelling from the cheap seats. I’m probably wrong too. The VC world exists. Money has been made. Billions in returns. Entire industries and generations of people owe their livelihoods to these once VC backed industries.<p>If &#x2F; when AGI happens can we make sure it’s not the Matrix?</div><br/></div></div></div></div></div></div><div id="41449146" class="c"><input type="checkbox" id="c-41449146" checked=""/><div class="controls bullet"><span class="by">dcchambers</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41448911">prev</a><span>|</span><a href="#41452320">next</a><span>|</span><label class="collapse" for="c-41449146">[-]</label><label class="expand" for="c-41449146">[27 more]</label></div><br/><div class="children"><div class="content">I also don&#x27;t understand it. If AGI is actually reached, capital as we know it basically becomes worthless. The entire structure of the modern economy and the society surrounding it collapses overnight.<p>I also don&#x27;t think there&#x27;s any way the governments of the world let <i>real</i> AGI stay in the hands of private industry. If it happens, governments around the world will go to war to gain control of it. SSI would be nationalized the moment AGI happened and there&#x27;s nothing A16Z could do about it.</div><br/><div id="41450950" class="c"><input type="checkbox" id="c-41450950" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449146">parent</a><span>|</span><a href="#41451010">next</a><span>|</span><label class="collapse" for="c-41450950">[-]</label><label class="expand" for="c-41450950">[3 more]</label></div><br/><div class="children"><div class="content">&gt; If AGI is actually reached, capital as we know it basically becomes worthless. The entire structure of the modern economy and the society surrounding it collapses overnight.<p>Increasingly this just seems like fantasy to me. I suspect we will see big changes similar to the way computers changed the economy, but we will not see &quot;capital as we know it become basically worthless&quot; or &quot;the modern economy and society around it collapse overnight&quot;. Property rights will still have value. Manufacturing facilities will still have value. Social media sites will still have value.<p>If this is a fantasy that will not happen, we really don&#x27;t need to reason about the implications of it happening. Consider that in 1968 some people imagined that the world of 2001 would be like the film 2001: A Space Odyssey, when in reality the shuttle program was soon to wind down, with little to replace it for another 20 years.</div><br/><div id="41451490" class="c"><input type="checkbox" id="c-41451490" checked=""/><div class="controls bullet"><span class="by">bobthecowboy</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450950">parent</a><span>|</span><a href="#41451010">next</a><span>|</span><label class="collapse" for="c-41451490">[-]</label><label class="expand" for="c-41451490">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Property rights will still have value. Manufacturing facilities will still have value. Social media sites will still have value.<p>I was with you on the first two, but the second one I don&#x27;t get?  We don&#x27;t even have AGI right now, and social media sites are already increasingly viewed by many people I know as having dubious value.  Adding LLM&#x27;s to the mix lowers that value, if anything (spam&#x2F;bots&#x2F;nonsense go up).  Adding AGI would seem to further reduce that value.</div><br/><div id="41452304" class="c"><input type="checkbox" id="c-41452304" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451490">parent</a><span>|</span><a href="#41451010">next</a><span>|</span><label class="collapse" for="c-41452304">[-]</label><label class="expand" for="c-41452304">[1 more]</label></div><br/><div class="children"><div class="content">Well we will find out. I think the audience has value to advertisers, and I suspect that basic mechanic will continue in to the future.<p>&gt; social media sites are already increasingly viewed by many people I know as having dubious value<p>I think we have all been saying this for 15 years but they keep getting more valuable.</div><br/></div></div></div></div></div></div><div id="41451010" class="c"><input type="checkbox" id="c-41451010" checked=""/><div class="controls bullet"><span class="by">BobbyJo</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449146">parent</a><span>|</span><a href="#41450950">prev</a><span>|</span><a href="#41450552">next</a><span>|</span><label class="collapse" for="c-41451010">[-]</label><label class="expand" for="c-41451010">[5 more]</label></div><br/><div class="children"><div class="content">&gt; If AGI is actually reached, capital as we know it basically becomes worthless<p>I see it as capital becoming infinitely more valuable and labor becoming worthless, since capital can be transmuted directly into labor at that point.</div><br/><div id="41453920" class="c"><input type="checkbox" id="c-41453920" checked=""/><div class="controls bullet"><span class="by">dyauspitr</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451010">parent</a><span>|</span><a href="#41451197">next</a><span>|</span><label class="collapse" for="c-41453920">[-]</label><label class="expand" for="c-41453920">[1 more]</label></div><br/><div class="children"><div class="content">Labor will never become worthless until you have physical robots <i>with</i> AGI and the dexterity to do what an average human can.</div><br/></div></div><div id="41451197" class="c"><input type="checkbox" id="c-41451197" checked=""/><div class="controls bullet"><span class="by">convolvatron</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451010">parent</a><span>|</span><a href="#41453920">prev</a><span>|</span><a href="#41450552">next</a><span>|</span><label class="collapse" for="c-41451197">[-]</label><label class="expand" for="c-41451197">[3 more]</label></div><br/><div class="children"><div class="content">if agi is commoditized and labor is useless, what does anyone need capital for? paying ad time on monopolized social media channels?</div><br/><div id="41451355" class="c"><input type="checkbox" id="c-41451355" checked=""/><div class="controls bullet"><span class="by">morgante</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451197">parent</a><span>|</span><a href="#41451352">next</a><span>|</span><label class="collapse" for="c-41451355">[-]</label><label class="expand" for="c-41451355">[1 more]</label></div><br/><div class="children"><div class="content">Commoditized doesn&#x27;t mean 0 capex. Literal commodities can in fact be very capital intensive (ex. offshore oil rigs).<p>In this case, you need capital to stockpile the GPUs.</div><br/></div></div><div id="41451352" class="c"><input type="checkbox" id="c-41451352" checked=""/><div class="controls bullet"><span class="by">stirlo</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451197">parent</a><span>|</span><a href="#41451355">prev</a><span>|</span><a href="#41450552">next</a><span>|</span><label class="collapse" for="c-41451352">[-]</label><label class="expand" for="c-41451352">[1 more]</label></div><br/><div class="children"><div class="content">For the physical infrastructure that the AGI (and world population) uses. Capital will still be needed to purchase finite land and resources even if all labour (physical and services) is replaced.</div><br/></div></div></div></div></div></div><div id="41450552" class="c"><input type="checkbox" id="c-41450552" checked=""/><div class="controls bullet"><span class="by">lawrenceyan</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449146">parent</a><span>|</span><a href="#41451010">prev</a><span>|</span><a href="#41449471">next</a><span>|</span><label class="collapse" for="c-41450552">[-]</label><label class="expand" for="c-41450552">[13 more]</label></div><br/><div class="children"><div class="content">What you&#x27;re talking about is something in the vein of exponential super intelligence.<p>Realistically what actually ends up happening imo, we get human level AGI and hit a ceiling there. Agents replace large portions of the current service economy greatly increasing automation &#x2F; efficiency for companies.<p>People continue to live their lives, as the idea of having a human level AGI personal assistant becomes normalized and then taken for granted.</div><br/><div id="41450928" class="c"><input type="checkbox" id="c-41450928" checked=""/><div class="controls bullet"><span class="by">brigadier132</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450552">parent</a><span>|</span><a href="#41451039">next</a><span>|</span><label class="collapse" for="c-41450928">[-]</label><label class="expand" for="c-41450928">[7 more]</label></div><br/><div class="children"><div class="content">I think you underestimate what can be accomplished with human level agi. Human level agi could mean 1 million Von Neumann level intelligences cranking 24&#x2F;7 on humanity&#x27;s problems.</div><br/><div id="41450962" class="c"><input type="checkbox" id="c-41450962" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450928">parent</a><span>|</span><a href="#41450959">next</a><span>|</span><label class="collapse" for="c-41450962">[-]</label><label class="expand" for="c-41450962">[2 more]</label></div><br/><div class="children"><div class="content">Only if there are no hardware limits, which seems highly unlikely.</div><br/><div id="41451071" class="c"><input type="checkbox" id="c-41451071" checked=""/><div class="controls bullet"><span class="by">unyttigfjelltol</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450962">parent</a><span>|</span><a href="#41450959">next</a><span>|</span><label class="collapse" for="c-41451071">[-]</label><label class="expand" for="c-41451071">[1 more]</label></div><br/><div class="children"><div class="content">Right, the comments are assuming an entrepreneur could conjure an army of brains out of <i>nothing</i>. In reality, the question is whether those brains are so much cheaper they open avenues currently unavailable. Would it be cheaper to hire an AGI or a human intern?</div><br/></div></div></div></div><div id="41450959" class="c"><input type="checkbox" id="c-41450959" checked=""/><div class="controls bullet"><span class="by">mentos</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450928">parent</a><span>|</span><a href="#41450962">prev</a><span>|</span><a href="#41451050">next</a><span>|</span><label class="collapse" for="c-41450959">[-]</label><label class="expand" for="c-41450959">[3 more]</label></div><br/><div class="children"><div class="content">Or cranking on super intelligence. What’s the minimum coefficient of human intelligence necessary to boot strap to infinity?</div><br/><div id="41451497" class="c"><input type="checkbox" id="c-41451497" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450959">parent</a><span>|</span><a href="#41451050">next</a><span>|</span><label class="collapse" for="c-41451497">[-]</label><label class="expand" for="c-41451497">[2 more]</label></div><br/><div class="children"><div class="content">Infinity intelligence is a very vague and probably ill-defined concept; to go to an impossible extreme, if you&#x27;re capable of modeling and predicting everything in the universe perfectly at zero cost, what would it even mean to be more intelligent?<p>That is a hard limit on intelligence, but neural networks can&#x27;t even reach that. What is the actual limit? No one knows. Maybe it&#x27;s something relatively close to that, modulo physical constraints. Maybe it&#x27;s right above the maximum human intelligence (and evolution managed to converge to a near optimal architecture). No one knows.</div><br/><div id="41451868" class="c"><input type="checkbox" id="c-41451868" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451497">parent</a><span>|</span><a href="#41451050">next</a><span>|</span><label class="collapse" for="c-41451868">[-]</label><label class="expand" for="c-41451868">[1 more]</label></div><br/><div class="children"><div class="content">&gt; if you&#x27;re capable of modeling and predicting everything in the universe perfectly at zero cost<p>As far as we know, it&#x27;s impossible to model any part of the universe perfectly, although it usually doesn&#x27;t matter.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;No-cloning_theorem" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;No-cloning_theorem</a></div><br/></div></div></div></div></div></div><div id="41451050" class="c"><input type="checkbox" id="c-41451050" checked=""/><div class="controls bullet"><span class="by">vkou</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450928">parent</a><span>|</span><a href="#41450959">prev</a><span>|</span><a href="#41451039">next</a><span>|</span><label class="collapse" for="c-41451050">[-]</label><label class="expand" for="c-41451050">[1 more]</label></div><br/><div class="children"><div class="content">The biggest problem that humanity has from the perspective of the people with the capital necessary to deploy this is &#x27;How to consolidate more wealth and power into their hands.&#x27;<p>One million Von Neumanns working on that &#x27;problem&#x27; is not something I&#x27;m looking forward to.</div><br/></div></div></div></div><div id="41451039" class="c"><input type="checkbox" id="c-41451039" checked=""/><div class="controls bullet"><span class="by">andyjohnson0</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450552">parent</a><span>|</span><a href="#41450928">prev</a><span>|</span><a href="#41453933">next</a><span>|</span><label class="collapse" for="c-41451039">[-]</label><label class="expand" for="c-41451039">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Agents replace large portions of the current service economy greatly increasing automation &#x2F; efficiency for companies.<p>&gt; People continue to live their lives<p>Presumably large numbers of those people no longer have jobs, and therefore no income.<p>&gt; we get human level AGI and hit a ceiling there<p>Recently I&#x27;ve been wondering if our best chance for a brake on runaway non-hard-takeoff superintelligence would be that the economy would be trashed.</div><br/><div id="41451568" class="c"><input type="checkbox" id="c-41451568" checked=""/><div class="controls bullet"><span class="by">lawrenceyan</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451039">parent</a><span>|</span><a href="#41453933">next</a><span>|</span><label class="collapse" for="c-41451568">[-]</label><label class="expand" for="c-41451568">[2 more]</label></div><br/><div class="children"><div class="content">People will move from the service economy to the entertainment economy powered by Youtube, Tiktok, Mr. Beast, and others.<p>Half-joking. In seriousness, something like a UBI will most likely happen.</div><br/><div id="41451882" class="c"><input type="checkbox" id="c-41451882" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41451568">parent</a><span>|</span><a href="#41453933">next</a><span>|</span><label class="collapse" for="c-41451882">[-]</label><label class="expand" for="c-41451882">[1 more]</label></div><br/><div class="children"><div class="content">The point of UBI or other welfare systems is for people we don&#x27;t &#x2F;want&#x2F; working, children and the elderly.<p>It&#x27;s impossible to run out of work for people who are capable of it. As an example, if you have two people and a piece of paper, just tear up the paper into strips, call them money and start exchanging them. Congrats, you both have income now.<p>(This is assuming the AGI has solved the problem of food and stuff, otherwise they&#x27;re going to have to trade for that and may run out of currency.)</div><br/></div></div></div></div></div></div><div id="41453933" class="c"><input type="checkbox" id="c-41453933" checked=""/><div class="controls bullet"><span class="by">dyauspitr</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450552">parent</a><span>|</span><a href="#41451039">prev</a><span>|</span><a href="#41451369">next</a><span>|</span><label class="collapse" for="c-41453933">[-]</label><label class="expand" for="c-41453933">[1 more]</label></div><br/><div class="children"><div class="content">If you can get a truly human level AGI, with enough resources it will get to super intelligence by itself.</div><br/></div></div><div id="41451369" class="c"><input type="checkbox" id="c-41451369" checked=""/><div class="controls bullet"><span class="by">dayvid</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450552">parent</a><span>|</span><a href="#41453933">prev</a><span>|</span><a href="#41449471">next</a><span>|</span><label class="collapse" for="c-41451369">[-]</label><label class="expand" for="c-41451369">[1 more]</label></div><br/><div class="children"><div class="content">If we can get human level AGI we will definitely get exponential super intelligence</div><br/></div></div></div></div><div id="41449471" class="c"><input type="checkbox" id="c-41449471" checked=""/><div class="controls bullet"><span class="by">tim333</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449146">parent</a><span>|</span><a href="#41450552">prev</a><span>|</span><a href="#41450246">next</a><span>|</span><label class="collapse" for="c-41449471">[-]</label><label class="expand" for="c-41449471">[1 more]</label></div><br/><div class="children"><div class="content">I think it would be much less dramatic than that if you mean human level abilities by AGI. Initially you might be able to replace the odd human by a robot equivalent probably costing more to begin with. To scale to replace everyone levels would take years and life would probably go on as normal for quite a while. Down the line assuming lots of ASI robots, if you wanted them to farm or build you a house say you&#x27;d still need land, materials, compute and energy which will not be unlimited.</div><br/></div></div><div id="41450913" class="c"><input type="checkbox" id="c-41450913" checked=""/><div class="controls bullet"><span class="by">neaanopri</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449146">parent</a><span>|</span><a href="#41450246">prev</a><span>|</span><a href="#41450964">next</a><span>|</span><label class="collapse" for="c-41450913">[-]</label><label class="expand" for="c-41450913">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, and if we get the second coming of Christ, the elect will be saved, and the rest will be damned.</div><br/></div></div><div id="41450964" class="c"><input type="checkbox" id="c-41450964" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41449146">parent</a><span>|</span><a href="#41450913">prev</a><span>|</span><a href="#41452320">next</a><span>|</span><label class="collapse" for="c-41450964">[-]</label><label class="expand" for="c-41450964">[2 more]</label></div><br/><div class="children"><div class="content">Honestly this is a pretty wild take. AGI won&#x27;t make food appear out of thin air. Buildings wont just sprout out of the ground so everybody will get to live in a mansion.<p>We would probably get the ability to generate infinite software, but a lot of stuff, like engineering would still require trial and error. Creating great art would still require inspiration gathered in the real world.<p>I expect it will bring about a new age of techno-feudalism - since selling intellectual labor will become impossible, only low value-add physical or mixed labor will become viable, which won&#x27;t be paid very well. People with capital will still own said capital, but you probably won&#x27;t be able to catch up to them by selling your labour, which will recreate the economic situation of the middle ages.<p>Another analogy I like is gold. If someone invented a way of making gold, it would bring down the price of the metal to next to nothing. In capitalist terms, it would constitute a huge destruction of value.<p>Same thing with AI - while human intelligence is productive, I&#x27;m pretty sure there&#x27;s a value in its scarcity - that fancy degree from a top university or any sort of acquired knowledge is somewhat valuable by the nature of its scarcity. Infinite supply would create value, and destroy it, not sure how the total would shake out.<p>Additionally, it would definitely suck that all the people financing their homes from their intellectual jobs would have to default on their loans, and the people whose services they employ, like construction workers, would go out of business as well.</div><br/><div id="41451294" class="c"><input type="checkbox" id="c-41451294" checked=""/><div class="controls bullet"><span class="by">phs318u</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41450964">parent</a><span>|</span><a href="#41452320">next</a><span>|</span><label class="collapse" for="c-41451294">[-]</label><label class="expand" for="c-41451294">[1 more]</label></div><br/><div class="children"><div class="content">&gt; which will recreate the economic situation of the middle ages<p>Indeed. Or as I’ve said before: a return to the historical mean.</div><br/></div></div></div></div></div></div><div id="41452320" class="c"><input type="checkbox" id="c-41452320" checked=""/><div class="controls bullet"><span class="by">11thEarlOfMar</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41449146">prev</a><span>|</span><a href="#41451032">next</a><span>|</span><label class="collapse" for="c-41452320">[-]</label><label class="expand" for="c-41452320">[4 more]</label></div><br/><div class="children"><div class="content">At least this time, people are actually asking the question.<p>NVDA::AI<p>CSCO::.COM</div><br/><div id="41452759" class="c"><input type="checkbox" id="c-41452759" checked=""/><div class="controls bullet"><span class="by">rpeden</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452320">parent</a><span>|</span><a href="#41452486">next</a><span>|</span><label class="collapse" for="c-41452759">[-]</label><label class="expand" for="c-41452759">[1 more]</label></div><br/><div class="children"><div class="content">Or possibly NT::.com<p>I remember seeing interviews with Nortel&#x27;s CEO where he bragged that most internet backbone traffic was handled by Nortel hardware. Things didn&#x27;t quite work out how he thought they were going to work out.<p>I think Nvidia is better positioned than Cisco or Nortel were during the dotcom crash, but does anyone actually think Nvidia&#x27;s current performance is sustainable? It doesn&#x27;t seem realistic to believe that.</div><br/></div></div><div id="41452486" class="c"><input type="checkbox" id="c-41452486" checked=""/><div class="controls bullet"><span class="by">steveBK123</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452320">parent</a><span>|</span><a href="#41452759">prev</a><span>|</span><a href="#41451032">next</a><span>|</span><label class="collapse" for="c-41452486">[-]</label><label class="expand" for="c-41452486">[2 more]</label></div><br/><div class="children"><div class="content">I do worry that unless some of these LLMs actually find a revenue model soon, the self reinforcing bubble is going to pop broadly.<p>GPUs, servers, datacenters, fabs, power generation&#x2F;transmission, copper&#x2F;steel&#x2F;concrete..<p>All to train models in an arms race because someone somewhere is going to figure out how to monetize and no one wants to miss the boat.</div><br/><div id="41454042" class="c"><input type="checkbox" id="c-41454042" checked=""/><div class="controls bullet"><span class="by">ithkuil</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41452486">parent</a><span>|</span><a href="#41451032">next</a><span>|</span><label class="collapse" for="c-41454042">[-]</label><label class="expand" for="c-41454042">[1 more]</label></div><br/><div class="children"><div class="content">If the bubble pops, would that bring the price for at least part of that hardware down and thus enable a second round of players (who were locked out from the race now) to experiment a little bit more and perhaps find something that works better?</div><br/></div></div></div></div></div></div><div id="41451032" class="c"><input type="checkbox" id="c-41451032" checked=""/><div class="controls bullet"><span class="by">blitzar</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41452320">prev</a><span>|</span><a href="#41448702">next</a><span>|</span><label class="collapse" for="c-41451032">[-]</label><label class="expand" for="c-41451032">[1 more]</label></div><br/><div class="children"><div class="content">&gt; companies that are being invested in in the AI space are going to make returns on the money invested<p>By selling to the &quot;dumb(er) money&quot; - if a Softbank &#x2F; Time &#x2F; Yahoo appears they can have it, if not you can always find willing buyers in an IPO.</div><br/></div></div><div id="41448702" class="c"><input type="checkbox" id="c-41448702" checked=""/><div class="controls bullet"><span class="by">light_triad</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41451032">prev</a><span>|</span><a href="#41451104">next</a><span>|</span><label class="collapse" for="c-41448702">[-]</label><label class="expand" for="c-41448702">[3 more]</label></div><br/><div class="children"><div class="content">My guess (not a VC) is they’ll sell ‘private’ models where safety is a priority: healthcare, government, finance, the EU…</div><br/><div id="41452801" class="c"><input type="checkbox" id="c-41452801" checked=""/><div class="controls bullet"><span class="by">Runsthroughit</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41448702">parent</a><span>|</span><a href="#41448723">next</a><span>|</span><label class="collapse" for="c-41452801">[-]</label><label class="expand" for="c-41452801">[1 more]</label></div><br/><div class="children"><div class="content">Sure. Except that LLMs can not reason or do anything reliably. 
You can still believe that, of course, but it will not change reality.</div><br/></div></div><div id="41448723" class="c"><input type="checkbox" id="c-41448723" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#41448627">root</a><span>|</span><a href="#41448702">parent</a><span>|</span><a href="#41452801">prev</a><span>|</span><a href="#41451104">next</a><span>|</span><label class="collapse" for="c-41448723">[-]</label><label class="expand" for="c-41448723">[1 more]</label></div><br/><div class="children"><div class="content">That could actually work if this LLM ai hype doesn’t die and is really actually useful</div><br/></div></div></div></div><div id="41451104" class="c"><input type="checkbox" id="c-41451104" checked=""/><div class="controls bullet"><span class="by">hartator</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41448702">prev</a><span>|</span><a href="#41450451">next</a><span>|</span><label class="collapse" for="c-41451104">[-]</label><label class="expand" for="c-41451104">[1 more]</label></div><br/><div class="children"><div class="content">I think the wishful end goal is AGI.<p>Picture something 1,000 smarter than a human. The potential value is waaaay bigger than any present company or even government.<p>Probably won’t happen. But, that’s the reasoning.</div><br/></div></div><div id="41450451" class="c"><input type="checkbox" id="c-41450451" checked=""/><div class="controls bullet"><span class="by">hintymad</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41451104">prev</a><span>|</span><a href="#41453051">next</a><span>|</span><label class="collapse" for="c-41450451">[-]</label><label class="expand" for="c-41450451">[1 more]</label></div><br/><div class="children"><div class="content">&gt; please tell me how these companies that are being invested in in the AI space are going to make returns on the money invested? What’s the business plan?<p>Not a VC, but I&#x27;d assume in this case the investors are not investing in a plausible biz plan, but in a group of top talent, especially given how early stage the company is at. The $5B valuation is really the valuation of the elite team in a arguably hyped market.</div><br/></div></div><div id="41453051" class="c"><input type="checkbox" id="c-41453051" checked=""/><div class="controls bullet"><span class="by">SMAAART</span><span>|</span><a href="#41448627">parent</a><span>|</span><a href="#41450451">prev</a><span>|</span><a href="#41446071">next</a><span>|</span><label class="collapse" for="c-41453051">[-]</label><label class="expand" for="c-41453051">[1 more]</label></div><br/><div class="children"><div class="content">Staking the territory in a new frontier.</div><br/></div></div></div></div><div id="41446071" class="c"><input type="checkbox" id="c-41446071" checked=""/><div class="controls bullet"><span class="by">xianshou</span><span>|</span><a href="#41448627">prev</a><span>|</span><a href="#41446929">next</a><span>|</span><label class="collapse" for="c-41446071">[-]</label><label class="expand" for="c-41446071">[105 more]</label></div><br/><div class="children"><div class="content">Same funding as OpenAI when they started, but SSI explicitly declared their intention not to release a single product until superintelligence is reached. Closest thing we have to a Manhattan Project in the modern era?</div><br/><div id="41446120" class="c"><input type="checkbox" id="c-41446120" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#41446071">parent</a><span>|</span><a href="#41446295">next</a><span>|</span><label class="collapse" for="c-41446120">[-]</label><label class="expand" for="c-41446120">[33 more]</label></div><br/><div class="children"><div class="content">&gt; Closest thing we have to a Manhattan Project in the modern era?<p>Minus the urgency, scientific process, well-defined goals, target dates, public ownership, accountability...</div><br/><div id="41446745" class="c"><input type="checkbox" id="c-41446745" checked=""/><div class="controls bullet"><span class="by">digging</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446120">parent</a><span>|</span><a href="#41446463">next</a><span>|</span><label class="collapse" for="c-41446745">[-]</label><label class="expand" for="c-41446745">[9 more]</label></div><br/><div class="children"><div class="content">Interesting attributes to mention...<p>The urgency was faked and less true of the Manhattan Project than it is of AGI safety. There was no nuclear weapons race; once it became clear that Germany had no chance of building atomic bombs, several scientists left the MP in protest, saying it was unnecessary and dangerous. However, the race to develop AGI is very real, and we also have no way of knowing how close anyone is to reaching it.<p>Likewise, the target dates were pretty meaningless. There was no race, and the atomic bombs weren&#x27;t necessary to end the war with Japan either. (It can&#x27;t be said with certainty one way or the other, but there&#x27;s pretty strong evidence that their existence was not the decisive factor in surrender.)<p>Public ownership and accountability are also pretty odd things to say! Congress didn&#x27;t even know about the Manhattan Project. Even Truman didn&#x27;t know for a long time. Sure, it was run by employees of the government and funded by the government, but it was a secret project with far less public input than any US-based private AI companies today.</div><br/><div id="41452381" class="c"><input type="checkbox" id="c-41452381" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446745">parent</a><span>|</span><a href="#41448632">next</a><span>|</span><label class="collapse" for="c-41452381">[-]</label><label class="expand" for="c-41452381">[1 more]</label></div><br/><div class="children"><div class="content">&gt; However, the race to develop AGI is very real, and we also have no way of knowing how close anyone is to reaching it.<p>It seems pretty irresponsible for AI boosters to say it’ll happen within 5 years then.<p>There’s a pretty important engineering distinction between the Manhattan Project and current research towards AGI. At the time of the Manhattan Project scientists already had a pretty good idea of how to build the weapon. The fundamental research had already been done. Most of the budget was actually just spent refining uranium. Of course there were details to figure out like the specific design of the detonator, but the mechanism of a runaway chain reaction was understood. This is much more concrete than building AGI.<p>For AGI nobody knows how to do it in detail. There are proposals for building trillion dollar clusters but we don’t have any theoretical basis for believing we’ll get AGI afterwards. The “scaling laws” people talk about are not actual laws but just empirical observations of trends in flawed metrics.</div><br/></div></div><div id="41448632" class="c"><input type="checkbox" id="c-41448632" checked=""/><div class="controls bullet"><span class="by">subsubzero</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446745">parent</a><span>|</span><a href="#41452381">prev</a><span>|</span><a href="#41449133">next</a><span>|</span><label class="collapse" for="c-41448632">[-]</label><label class="expand" for="c-41448632">[6 more]</label></div><br/><div class="children"><div class="content">I agree and also disagree.<p>&gt; There was no nuclear weapons race; once it became clear that Germany had no chance of building atomic bombs, several scientists left the MP in protest<p>You are forgetting Japan in WWII and given casualty numbers from island hopping it was going to be a absolutely huge casualty count with US troops, probably something on the order of Englands losses during WW1. Which for them sent them on a downward trajectory due to essentially an entire generation dying or being extremely traumatized. If the US did not have Nagasaki and Hiroshima we would probably not have the space program and US technical prowess post WWII, so a totally different reality than where we are today.</div><br/><div id="41454768" class="c"><input type="checkbox" id="c-41454768" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41448632">parent</a><span>|</span><a href="#41449758">next</a><span>|</span><label class="collapse" for="c-41454768">[-]</label><label class="expand" for="c-41454768">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll try to argue his point. The idea that Japan would have resisted to the last man and that a massive amphibious invasion would have been required is kind of a myth. The US pacific submarine fleet had sunk the majority of the Japanese merchant marine to the point that Japan was critically low on war materiel and food. The Japanese navy had lost all of its capital ships and there was a critical shortage of personnel like pilots. The Soviets also invaded and overran Manchuria over a span of weeks. The military wing of the Japanese government certainly wanted to continue fighting but the writing was on the wall. The nuclear bombing of Japanese cities certainly pressed the issue but much of the American Military command in the Pacific thought it was unnecessarily brutal, and Japanese cities had already been devastated by a bombing campaign that included firebombing. I&#x27;m not sure that completely aligns with my own views but that&#x27;s basically the argument, and there are compelling points.</div><br/></div></div><div id="41449758" class="c"><input type="checkbox" id="c-41449758" checked=""/><div class="controls bullet"><span class="by">digging</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41448632">parent</a><span>|</span><a href="#41454768">prev</a><span>|</span><a href="#41449133">next</a><span>|</span><label class="collapse" for="c-41449758">[-]</label><label class="expand" for="c-41449758">[4 more]</label></div><br/><div class="children"><div class="content">Did you stop reading my comment there? I debunked this already.</div><br/><div id="41451122" class="c"><input type="checkbox" id="c-41451122" checked=""/><div class="controls bullet"><span class="by">koops</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41449758">parent</a><span>|</span><a href="#41451672">next</a><span>|</span><label class="collapse" for="c-41451122">[-]</label><label class="expand" for="c-41451122">[1 more]</label></div><br/><div class="children"><div class="content">Asserting that there is strong evidence against a claim is not &quot;debunking&quot; a claim.</div><br/></div></div><div id="41451672" class="c"><input type="checkbox" id="c-41451672" checked=""/><div class="controls bullet"><span class="by">fakedang</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41449758">parent</a><span>|</span><a href="#41451122">prev</a><span>|</span><a href="#41454716">next</a><span>|</span><label class="collapse" for="c-41451672">[-]</label><label class="expand" for="c-41451672">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the atomic bombs weren&#x27;t necessary to end the war with Japan either. (It can&#x27;t be said with certainty one way or the other, but there&#x27;s pretty strong evidence that their existence was not the decisive factor in surrender.)<p>Well, you didn&#x27;t provide any evidence. Island hopping in the Pacific theater itself took thousands of lives, imagine what a headlong strike into a revanchist country of citizens determined to fight to the last man, woman and child would have looked like. We don&#x27;t know how effective a hypothetical Soviet assault would have looked like as they had attacked sparsely populated Sakhalin only. What the atom bomb succeeded was in convincing Emperor Hirohito that continuing the war would be destructively pointless.<p>WW1 practically destroyed the British Empire for the most part. WW2 would have done the same for the US in your hypothetical scenario, but much worse.</div><br/></div></div><div id="41454716" class="c"><input type="checkbox" id="c-41454716" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41449758">parent</a><span>|</span><a href="#41451672">prev</a><span>|</span><a href="#41449133">next</a><span>|</span><label class="collapse" for="c-41454716">[-]</label><label class="expand" for="c-41454716">[1 more]</label></div><br/><div class="children"><div class="content">You did not lol</div><br/></div></div></div></div></div></div><div id="41449133" class="c"><input type="checkbox" id="c-41449133" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446745">parent</a><span>|</span><a href="#41448632">prev</a><span>|</span><a href="#41446463">next</a><span>|</span><label class="collapse" for="c-41449133">[-]</label><label class="expand" for="c-41449133">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The urgency was faked and less true of the Manhattan Project than it is of AGI safety.<p>I&#x27;d say they were equal.  We were worried about Russia getting nuclear capability once we knew Germany was out of the race.  Russia was at best our frenemy.  The enemy of my enemy is my friend kind of thing.</div><br/></div></div></div></div><div id="41446463" class="c"><input type="checkbox" id="c-41446463" checked=""/><div class="controls bullet"><span class="by">Quinner</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446120">parent</a><span>|</span><a href="#41446745">prev</a><span>|</span><a href="#41446230">next</a><span>|</span><label class="collapse" for="c-41446463">[-]</label><label class="expand" for="c-41446463">[11 more]</label></div><br/><div class="children"><div class="content">If public ownership means we give one guy a button to end the world, I&#x27;m not sure how&#x27;s that&#x27;s a meaningful difference.</div><br/><div id="41448136" class="c"><input type="checkbox" id="c-41448136" checked=""/><div class="controls bullet"><span class="by">wil421</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446463">parent</a><span>|</span><a href="#41447972">next</a><span>|</span><label class="collapse" for="c-41448136">[-]</label><label class="expand" for="c-41448136">[1 more]</label></div><br/><div class="children"><div class="content">Pretty sure the military made it clear they aren’t launching any nukes, despite what the last President said publicly. They also made it clear they weren’t invading China.<p><a href="https:&#x2F;&#x2F;amp.cnn.com&#x2F;cnn&#x2F;2017&#x2F;11&#x2F;18&#x2F;politics&#x2F;air-force-general-john-hyten-nuclear-strike-donald-trump" rel="nofollow">https:&#x2F;&#x2F;amp.cnn.com&#x2F;cnn&#x2F;2017&#x2F;11&#x2F;18&#x2F;politics&#x2F;air-force-genera...</a><p><a href="https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;world-us-canada-58581296.amp" rel="nofollow">https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;world-us-canada-58581296.amp</a></div><br/></div></div><div id="41447972" class="c"><input type="checkbox" id="c-41447972" checked=""/><div class="controls bullet"><span class="by">nativeit</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446463">parent</a><span>|</span><a href="#41448136">prev</a><span>|</span><a href="#41448073">next</a><span>|</span><label class="collapse" for="c-41447972">[-]</label><label class="expand" for="c-41447972">[5 more]</label></div><br/><div class="children"><div class="content">We all get to vote for that person.</div><br/><div id="41448038" class="c"><input type="checkbox" id="c-41448038" checked=""/><div class="controls bullet"><span class="by">latexr</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41447972">parent</a><span>|</span><a href="#41451689">next</a><span>|</span><label class="collapse" for="c-41448038">[-]</label><label class="expand" for="c-41448038">[1 more]</label></div><br/><div class="children"><div class="content">Well, not exactly “we all”, just the citizens of the country in possession of the kill switch. And in some countries, the person in question was either not elected or elections are a farce to keep appearances.</div><br/></div></div><div id="41451689" class="c"><input type="checkbox" id="c-41451689" checked=""/><div class="controls bullet"><span class="by">nwiswell</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41447972">parent</a><span>|</span><a href="#41448038">prev</a><span>|</span><a href="#41448027">next</a><span>|</span><label class="collapse" for="c-41451689">[-]</label><label class="expand" for="c-41451689">[2 more]</label></div><br/><div class="children"><div class="content">Oh, that&#x27;s super. I&#x27;ve been really impressed recently with the wisdom of our collective selections.</div><br/><div id="41453918" class="c"><input type="checkbox" id="c-41453918" checked=""/><div class="controls bullet"><span class="by">riffraff</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41451689">parent</a><span>|</span><a href="#41448027">next</a><span>|</span><label class="collapse" for="c-41453918">[-]</label><label class="expand" for="c-41453918">[1 more]</label></div><br/><div class="children"><div class="content">Democracy sucks, but we haven&#x27;t found anything better.</div><br/></div></div></div></div><div id="41448027" class="c"><input type="checkbox" id="c-41448027" checked=""/><div class="controls bullet"><span class="by">louthy</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41447972">parent</a><span>|</span><a href="#41451689">prev</a><span>|</span><a href="#41448073">next</a><span>|</span><label class="collapse" for="c-41448027">[-]</label><label class="expand" for="c-41448027">[1 more]</label></div><br/><div class="children"><div class="content">&gt; all<p>Some of you do. The rest of us are left with the consequences.</div><br/></div></div></div></div><div id="41448073" class="c"><input type="checkbox" id="c-41448073" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446463">parent</a><span>|</span><a href="#41447972">prev</a><span>|</span><a href="#41448252">next</a><span>|</span><label class="collapse" for="c-41448073">[-]</label><label class="expand" for="c-41448073">[1 more]</label></div><br/><div class="children"><div class="content">No one single person can cause a nuclear detonation alone.</div><br/></div></div><div id="41448252" class="c"><input type="checkbox" id="c-41448252" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446463">parent</a><span>|</span><a href="#41448073">prev</a><span>|</span><a href="#41446230">next</a><span>|</span><label class="collapse" for="c-41448252">[-]</label><label class="expand" for="c-41448252">[3 more]</label></div><br/><div class="children"><div class="content">The fact that the world hasn&#x27;t ended and no nuke has been launched since the 1940s shows that the system is working. Give the button to a random billionaire and half of us will be dead by next week to improve profit margins.</div><br/><div id="41448440" class="c"><input type="checkbox" id="c-41448440" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41448252">parent</a><span>|</span><a href="#41446230">next</a><span>|</span><label class="collapse" for="c-41448440">[-]</label><label class="expand" for="c-41448440">[2 more]</label></div><br/><div class="children"><div class="content">Bikini atoll and the islanders that no longer live there due to nuclear contamination would like a word with you. Split hairs however you like with the definition of &quot;launch&quot; but those tests went on well through the 1950s.</div><br/><div id="41454790" class="c"><input type="checkbox" id="c-41454790" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41448440">parent</a><span>|</span><a href="#41446230">next</a><span>|</span><label class="collapse" for="c-41454790">[-]</label><label class="expand" for="c-41454790">[1 more]</label></div><br/><div class="children"><div class="content">A nuclear weapon hasn&#x27;t been detonated in anger since 1945.</div><br/></div></div></div></div></div></div></div></div><div id="41446230" class="c"><input type="checkbox" id="c-41446230" checked=""/><div class="controls bullet"><span class="by">HPMOR</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446120">parent</a><span>|</span><a href="#41446463">prev</a><span>|</span><a href="#41451688">next</a><span>|</span><label class="collapse" for="c-41446230">[-]</label><label class="expand" for="c-41446230">[2 more]</label></div><br/><div class="children"><div class="content">The Manhattan Project had none of these things publicly declared. And Ilya is a top flight scientist.</div><br/><div id="41446264" class="c"><input type="checkbox" id="c-41446264" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446230">parent</a><span>|</span><a href="#41451688">next</a><span>|</span><label class="collapse" for="c-41446264">[-]</label><label class="expand" for="c-41446264">[1 more]</label></div><br/><div class="children"><div class="content">The word &quot;publicly&quot; is doing a lot of heavy lifting here.  There is no indication that SSI has any of these at all.</div><br/></div></div></div></div><div id="41451688" class="c"><input type="checkbox" id="c-41451688" checked=""/><div class="controls bullet"><span class="by">alexilliamson</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446120">parent</a><span>|</span><a href="#41446230">prev</a><span>|</span><a href="#41446690">next</a><span>|</span><label class="collapse" for="c-41451688">[-]</label><label class="expand" for="c-41451688">[4 more]</label></div><br/><div class="children"><div class="content">Well-defined goal is the big one.  We wanted a big bomb.<p>What does AGI do?  AGI is up against a philosophical barrier, not a technical one.  We&#x27;ll continue improving AI&#x27;s ability to automate and assist human decisions, but how does it become something more?  Something more &quot;general&quot;?</div><br/><div id="41452069" class="c"><input type="checkbox" id="c-41452069" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41451688">parent</a><span>|</span><a href="#41446690">next</a><span>|</span><label class="collapse" for="c-41452069">[-]</label><label class="expand" for="c-41452069">[3 more]</label></div><br/><div class="children"><div class="content">&quot;General&quot; is every activity a human can do or learn to do. It was coined along with &quot;narrow&quot; to contrast with the then decidedly non-general AI systems. This was generally conceived of as a strict binary - every AI we&#x27;ve made is narrow, whereas humans are general, able to do a wide variety of tasks and do things like transfer learning, and the thinking was that we were missing some grand learning algorithm that would create a protointelligence which would be &quot;general at birth&quot; like a human baby, able to learn anything &amp; everything in theory. An example of an AI system that is considered narrow is a calculator, or a chess engine - these are already superhuman in intelligence, in that they can perform their tasks better than any human ever possibly could, but a calculator or a chess engine is so narrow that it seems absurd to think of asking a calculator for an example of a healthy meal plan, or asking a chess engine to make sense of an expense report, or asking anything to write a memoir. Even in more modern times, with AlexNet we had a very impressive image recognition AI system, but it couldn&#x27;t calculate large numbers or win a game of chess or write poetry - it was impressive, but still narrow.<p>With transformers, demonstrated first by LLMs, I think we&#x27;ve shown that the narrow-general divide as a strict binary is the wrong way to think about AI. Instead, LLMs are obviously <i>more general</i> than any previous AI system, in that they can do math or play chess or write a poem, all using the same system. They aren&#x27;t as good as our existing superhuman computer systems at these tasks (aside from language processing, which they are SOTA at), not even as good at humans, but they&#x27;re obviously much better than chance. With training to use tools (like calculators and chess engines) you can easily make an AI system with an LLM component that&#x27;s superhuman in those fields, but there are still things that LLMs cannot do as well as humans, even when using tools, so they are not fully general. One example is making tools for themselves to use - they can do a lot of parts of that work, but I haven&#x27;t seen an example yet of an LLM actually making a tool for itself that it can then use to solve a problem it otherwise couldn&#x27;t. This is a subproblem of the larger &quot;LLMs don&#x27;t have long term memory and long term planning abilities&quot; problem - you can ask an LLM to use python to make a little tool for itself to do one specific task, but it&#x27;s not yet capable of adding that tool to its general toolset to enhance its general capabilities going forward. It can&#x27;t write a memoir, or a book that people want to read, because they suck at planning or refining from drafts, and they have limited creativity because they&#x27;re typically a blank slate in terms of explicit memory before they&#x27;re asked to write - they have a gargantuan of implicitly remembered things from training, which is where what creativity they do have comes from, but they don&#x27;t yet have a way to accrue and benefit from experience.<p>A thought exercise I think is helpful for understanding what the &quot;AGI&quot; benchmark should mean is: can this AI system be a drop-in substitute for a remote worker? As in, any labour that can be accomplished by a remote worker can be performed by it, including learning on the job to do different or new tasks, and including &quot;designing and building AI systems&quot;. Such a system would be extremely economically valuable, and I think it should meet the bar of &quot;AGI&quot;.</div><br/><div id="41453939" class="c"><input type="checkbox" id="c-41453939" checked=""/><div class="controls bullet"><span class="by">riffraff</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41452069">parent</a><span>|</span><a href="#41446690">next</a><span>|</span><label class="collapse" for="c-41453939">[-]</label><label class="expand" for="c-41453939">[2 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs are obviously more general than any previous AI system, in that they can do math or play chess or write a poem, all using the same system<p>But they can&#x27;t, they still fail at arithmetic and still fail at counting syllables.<p>I think that LLMs are really impressive but they are the perfect example of a narrow intelligence.<p>I think they don&#x27;t blur the lines between narrow and general, they just show a different dimension of narrowness.</div><br/><div id="41454105" class="c"><input type="checkbox" id="c-41454105" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41453939">parent</a><span>|</span><a href="#41446690">next</a><span>|</span><label class="collapse" for="c-41454105">[-]</label><label class="expand" for="c-41454105">[1 more]</label></div><br/><div class="children"><div class="content">&gt;But they can&#x27;t, they still fail at arithmetic and still fail at counting syllables.<p>You are incorrect. These services are free, you can go and try it out for yourself. LLMs are perfectly capable of simple arithmetic, better than many humans and worse than some. They can also play chess and write poetry, and I made zero claims at &quot;counting syllables&quot;, but it seems perfectly capable of doing that too. See for yourself, this was my first attempt, no cherry picking: <a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;ea1ee11e-9926-4139-89f9-6496e3bdeecb" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;ea1ee11e-9926-4139-89f9-6496e3bdee...</a><p>I asked it a multiplication question so it used a calculator to correctly complete the task, I asked it to play chess and it did well, I asked it to write me a poem about it and it did that well too. It did everything I said it could, which is significantly more than a narrow AI system like a calculator, a chess engine, or an image recognition algorithm could do. The point is it can do reasonably at a broad range of tasks, even if it isn&#x27;t superhuman (or even average human) at any given one of them.<p>&gt;I think that LLMs are really impressive but they are the perfect example of a narrow intelligence.<p>This doesn&#x27;t make any sense at all. You think an AI artifact that can write poetry, code, play chess, control a robot, recommend a clutch to go with your dress, compute sums etc is &quot;the perfect example of a narrow intelligence.&quot; while a chess engine like Stockfish or an average calculator exists? There are AI models that specifically and only recognise faces, but the LLM multitool is &quot;the perfect example of a narrow intelligence.&quot;? Come on.<p>&gt;I think they don&#x27;t blur the lines between narrow and general, they just show a different dimension of narrowness.<p>You haven&#x27;t provided an example of what &quot;dimension of narrowness&quot; LLMs show. I don&#x27;t think you can reasonably describe an LLM as narrow without redefining the word - just because something is not fully general doesn&#x27;t mean that it&#x27;s narrow.</div><br/></div></div></div></div></div></div></div></div><div id="41446690" class="c"><input type="checkbox" id="c-41446690" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446120">parent</a><span>|</span><a href="#41451688">prev</a><span>|</span><a href="#41447380">next</a><span>|</span><label class="collapse" for="c-41446690">[-]</label><label class="expand" for="c-41446690">[1 more]</label></div><br/><div class="children"><div class="content">none of these things are true of public knowledge about the manhattan project… but oookay</div><br/></div></div><div id="41447380" class="c"><input type="checkbox" id="c-41447380" checked=""/><div class="controls bullet"><span class="by">chakintosh</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446120">parent</a><span>|</span><a href="#41446690">prev</a><span>|</span><a href="#41452878">next</a><span>|</span><label class="collapse" for="c-41447380">[-]</label><label class="expand" for="c-41447380">[4 more]</label></div><br/><div class="children"><div class="content">... Hiroshima</div><br/><div id="41448047" class="c"><input type="checkbox" id="c-41448047" checked=""/><div class="controls bullet"><span class="by">zombiwoof</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41447380">parent</a><span>|</span><a href="#41452878">next</a><span>|</span><label class="collapse" for="c-41448047">[-]</label><label class="expand" for="c-41448047">[3 more]</label></div><br/><div class="children"><div class="content">And Nagasaki , not once but twice. Why? Just because</div><br/><div id="41448862" class="c"><input type="checkbox" id="c-41448862" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41448047">parent</a><span>|</span><a href="#41451076">next</a><span>|</span><label class="collapse" for="c-41448862">[-]</label><label class="expand" for="c-41448862">[1 more]</label></div><br/><div class="children"><div class="content">Once could be a fluke, twice sends an entirely different message.</div><br/></div></div><div id="41451076" class="c"><input type="checkbox" id="c-41451076" checked=""/><div class="controls bullet"><span class="by">blitzar</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41448047">parent</a><span>|</span><a href="#41448862">prev</a><span>|</span><a href="#41452878">next</a><span>|</span><label class="collapse" for="c-41451076">[-]</label><label class="expand" for="c-41451076">[1 more]</label></div><br/><div class="children"><div class="content">Always double tap.</div><br/></div></div></div></div></div></div><div id="41452878" class="c"><input type="checkbox" id="c-41452878" checked=""/><div class="controls bullet"><span class="by">berz01</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446120">parent</a><span>|</span><a href="#41447380">prev</a><span>|</span><a href="#41446295">next</a><span>|</span><label class="collapse" for="c-41452878">[-]</label><label class="expand" for="c-41452878">[1 more]</label></div><br/><div class="children"><div class="content">nailed it bro, someone give this man a podium</div><br/></div></div></div></div><div id="41446295" class="c"><input type="checkbox" id="c-41446295" checked=""/><div class="controls bullet"><span class="by">Yizahi</span><span>|</span><a href="#41446071">parent</a><span>|</span><a href="#41446120">prev</a><span>|</span><a href="#41446573">next</a><span>|</span><label class="collapse" for="c-41446295">[-]</label><label class="expand" for="c-41446295">[55 more]</label></div><br/><div class="children"><div class="content">There is significant possibility that true AI (what Ilia calls superintelligence) is impossible to build using neural networks. So it is closer to some tokenbro project than to nuclear research.<p>Or he will simply shift goalposts, and call some LLM superintelligent.</div><br/><div id="41446401" class="c"><input type="checkbox" id="c-41446401" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446295">parent</a><span>|</span><a href="#41452127">next</a><span>|</span><label class="collapse" for="c-41446401">[-]</label><label class="expand" for="c-41446401">[36 more]</label></div><br/><div class="children"><div class="content">&gt; There is significant possibility that true AI (what Ilia calls superintelligence) is impossible to build using neural networks<p>What evidence can you provide to back up the statement of this &quot;significant possibility&quot;? Human brains use neural networks...</div><br/><div id="41446808" class="c"><input type="checkbox" id="c-41446808" checked=""/><div class="controls bullet"><span class="by">aithrowaway1987</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446401">parent</a><span>|</span><a href="#41446643">next</a><span>|</span><label class="collapse" for="c-41446808">[-]</label><label class="expand" for="c-41446808">[10 more]</label></div><br/><div class="children"><div class="content">There was a very good paper in Nature showing this definitively: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41437933">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41437933</a><p>Modern ANN architectures are not actually capable of <i>long-term</i> learning in the same way animals are, even stodgy old dogs that don&#x27;t learn new tricks. ANNs are not a plausible model for the brain, even if they emulate certain parts of the brain (the cerebellum, but not the cortex)<p>I will add that transformers are not capable of recursion, so it&#x27;s impossible for them to realistically emulate a pigeon&#x27;s brain. (you would need  millions of layers that &quot;unlink chains of thought&quot; purely by exhaustion)</div><br/><div id="41449220" class="c"><input type="checkbox" id="c-41449220" checked=""/><div class="controls bullet"><span class="by">calf</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446808">parent</a><span>|</span><a href="#41447133">next</a><span>|</span><label class="collapse" for="c-41449220">[-]</label><label class="expand" for="c-41449220">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;ve read the abstract wrong. The authors argue that neural networks can learn online and a necessary condition is random information. That&#x27;s the thesis, their thesis is not that neural networks are the wrong paradigm.</div><br/></div></div><div id="41447133" class="c"><input type="checkbox" id="c-41447133" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446808">parent</a><span>|</span><a href="#41449220">prev</a><span>|</span><a href="#41450397">next</a><span>|</span><label class="collapse" for="c-41447133">[-]</label><label class="expand" for="c-41447133">[4 more]</label></div><br/><div class="children"><div class="content">this paper is far from “showing this definitively”<p>even if we bought this negative result as somehow “proving impossibility”, i’m not convinced plasticity is necessary for intelligence<p>huge respect for richard sutton though</div><br/><div id="41447672" class="c"><input type="checkbox" id="c-41447672" checked=""/><div class="controls bullet"><span class="by">aithrowaway1987</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41447133">parent</a><span>|</span><a href="#41450397">next</a><span>|</span><label class="collapse" for="c-41447672">[-]</label><label class="expand" for="c-41447672">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t &quot;plasticity is not necessary for intelligence&quot; just defining intelligence downwards? It seems like you want to restrict &quot;intelligence&quot; to static knowledge and (apparent) short-term cleverness, but being able to make long-term observation and judgements about a changing world is a necessary component of intelligence in vertebrates. Why exclude that from consideration?<p>More specifically: it is highly implausible that an AI system could learn to improve itself beyond human capability if it does not have long-term plasticity: how would it be able to reflect upon and extend its discoveries if it&#x27;s not able to learn new things during its operation?</div><br/><div id="41454781" class="c"><input type="checkbox" id="c-41454781" checked=""/><div class="controls bullet"><span class="by">sebastiennight</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41447672">parent</a><span>|</span><a href="#41448227">next</a><span>|</span><label class="collapse" for="c-41454781">[-]</label><label class="expand" for="c-41454781">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s not forget that software has one significant advantage over humans: versioning.<p>If I&#x27;m a human tasked with editing video (which is the field my startup[0] is in) and a completely new video format comes in, <i>I</i> need the long term plasticity to learn how to use it so <i>I</i> can perform my work.<p>If a sufficiently intelligent version of our AI model is tasked with editing these videos, and a completely new video format comes in, it does not need to learn to handle it. Not if this model is smart enough to iterate a new model that can handle it.<p>The new skills and knowledge do not need to be encoded in &quot;the self&quot; when you are a bunch of bytes that can build your successor out of more bytes.<p>Or, in popular culture terms, the last 30 seconds of this Age of Ultron clip[1].<p>[0]: <a href="https:&#x2F;&#x2F;www.onetake.ai" rel="nofollow">https:&#x2F;&#x2F;www.onetake.ai</a><p>[1]: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=qA5wYcybkCM&amp;t=25" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=qA5wYcybkCM&amp;t=25</a></div><br/></div></div><div id="41448227" class="c"><input type="checkbox" id="c-41448227" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41447672">parent</a><span>|</span><a href="#41454781">prev</a><span>|</span><a href="#41450397">next</a><span>|</span><label class="collapse" for="c-41448227">[-]</label><label class="expand" for="c-41448227">[1 more]</label></div><br/><div class="children"><div class="content">Anterograde amnesia is a significant disruption of plasticity, and yet people who have it are still intelligent.<p>(That said, I agree plasticity is key to the most powerful systems. A human race with anterograde amnesia would have long ago gone extinct.)</div><br/></div></div></div></div></div></div><div id="41450397" class="c"><input type="checkbox" id="c-41450397" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446808">parent</a><span>|</span><a href="#41447133">prev</a><span>|</span><a href="#41453783">next</a><span>|</span><label class="collapse" for="c-41450397">[-]</label><label class="expand" for="c-41450397">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Modern ANN architectures are not actually capable of long-term learning<p>What do you think training (and fine-tuning) does?</div><br/><div id="41451962" class="c"><input type="checkbox" id="c-41451962" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41450397">parent</a><span>|</span><a href="#41453783">next</a><span>|</span><label class="collapse" for="c-41451962">[-]</label><label class="expand" for="c-41451962">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not how we (today) practically interact with LLMs, though.<p>No LLM currently adapts to the tasks its given with an iteration cycle shorter than on the order of months (assuming your conversations serve as future training data; otherwise not at all).<p>No current LLM can digest its &quot;experiences&quot;, form hypotheses (at least outside of being queried), run thought experiments, then actual experiments, and then update based on the outcome.<p>Not because it&#x27;s fundamentally impossible (it might or might not be), but because we practically haven&#x27;t built anything even remotely approaching that type of architecture.</div><br/></div></div></div></div><div id="41453783" class="c"><input type="checkbox" id="c-41453783" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446808">parent</a><span>|</span><a href="#41450397">prev</a><span>|</span><a href="#41448927">next</a><span>|</span><label class="collapse" for="c-41453783">[-]</label><label class="expand" for="c-41453783">[1 more]</label></div><br/><div class="children"><div class="content">So use a new NN architecture. I mean, that is the point, isn&#x27;t it?</div><br/></div></div><div id="41448927" class="c"><input type="checkbox" id="c-41448927" checked=""/><div class="controls bullet"><span class="by">theGnuMe</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446808">parent</a><span>|</span><a href="#41453783">prev</a><span>|</span><a href="#41446643">next</a><span>|</span><label class="collapse" for="c-41448927">[-]</label><label class="expand" for="c-41448927">[1 more]</label></div><br/><div class="children"><div class="content">You can always convert a recursive function call to a loop.</div><br/></div></div></div></div><div id="41446643" class="c"><input type="checkbox" id="c-41446643" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446401">parent</a><span>|</span><a href="#41446808">prev</a><span>|</span><a href="#41446479">next</a><span>|</span><label class="collapse" for="c-41446643">[-]</label><label class="expand" for="c-41446643">[6 more]</label></div><br/><div class="children"><div class="content">The neural networks in human brains are very different from artificial neural networks though. In particular, they seem to learn in a very different way than backprop.<p>But there is no reason the company can&#x27;t come up with a different paradigm.</div><br/><div id="41449242" class="c"><input type="checkbox" id="c-41449242" checked=""/><div class="controls bullet"><span class="by">calf</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446643">parent</a><span>|</span><a href="#41446701">next</a><span>|</span><label class="collapse" for="c-41449242">[-]</label><label class="expand" for="c-41449242">[2 more]</label></div><br/><div class="children"><div class="content">Do we know that? I&#x27;ve seem some articles and lectures this year that kind of almost loosely argue and reach for the notion that &quot;human backprop&quot; happens when we sleep and dream, etc. I know that&#x27;s handwavy and not rigorous, but who knows what&#x27;s going on at this point.</div><br/><div id="41450951" class="c"><input type="checkbox" id="c-41450951" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41449242">parent</a><span>|</span><a href="#41446701">next</a><span>|</span><label class="collapse" for="c-41450951">[-]</label><label class="expand" for="c-41450951">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve only heard of one researcher who believes the brain does something similar to backprop and has gradients, but it sounded extremely handwavy to me. I think it is more likely the brain does something resembling active inference.<p>But I suppose you could say we don&#x27;t know 100% since we don&#x27;t fully understand how the brain learns.</div><br/></div></div></div></div><div id="41446701" class="c"><input type="checkbox" id="c-41446701" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446643">parent</a><span>|</span><a href="#41449242">prev</a><span>|</span><a href="#41446479">next</a><span>|</span><label class="collapse" for="c-41446701">[-]</label><label class="expand" for="c-41446701">[3 more]</label></div><br/><div class="children"><div class="content">that is very weak evidence for the impossibility claim</div><br/><div id="41447472" class="c"><input type="checkbox" id="c-41447472" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446701">parent</a><span>|</span><a href="#41446479">next</a><span>|</span><label class="collapse" for="c-41447472">[-]</label><label class="expand" for="c-41447472">[2 more]</label></div><br/><div class="children"><div class="content">It was refuting the weak evidence for possibility stated above.</div><br/><div id="41447740" class="c"><input type="checkbox" id="c-41447740" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41447472">parent</a><span>|</span><a href="#41446479">next</a><span>|</span><label class="collapse" for="c-41447740">[-]</label><label class="expand" for="c-41447740">[1 more]</label></div><br/><div class="children"><div class="content">cheers i missed that</div><br/></div></div></div></div></div></div></div></div><div id="41446479" class="c"><input type="checkbox" id="c-41446479" checked=""/><div class="controls bullet"><span class="by">Yizahi</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446401">parent</a><span>|</span><a href="#41446643">prev</a><span>|</span><a href="#41446730">next</a><span>|</span><label class="collapse" for="c-41446479">[-]</label><label class="expand" for="c-41446479">[5 more]</label></div><br/><div class="children"><div class="content">There are two possibilities.<p>1. Either you are correct and the neural networks humans have are exactly the same or very similar to the programs in the LLMs. Then it will be relatively easy to verify this - just scale one LLN to the human brain neuron count and supposedly it will acquire consciousness and start rapidly learning and creating on its own without prompts.<p>2. Or what we call neural networks in the computer programs is radically different and or insufficient to create AI.<p>I&#x27;m leaning to the second option, just from the very high level and rudimentary reading about current projects. Can be wrong of course. But I have yet to see any paper that refutes option 2, so it means that it is still possible.</div><br/><div id="41446718" class="c"><input type="checkbox" id="c-41446718" checked=""/><div class="controls bullet"><span class="by">barrell</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446479">parent</a><span>|</span><a href="#41446730">next</a><span>|</span><label class="collapse" for="c-41446718">[-]</label><label class="expand" for="c-41446718">[4 more]</label></div><br/><div class="children"><div class="content">I agree with your stance - that being said there aren’t two options, one being identical or radically different. It’s not even a gradient between two choices, because there are several dimensions involved and nobody even knows what Superintelligence is anyways.<p>If you wanted to reduce it down, I would say there are two possibilities:<p>1. Our understanding of Neurel Nets is currently sufficient to recreate intelligence, consciousness, or what have you<p>2. We’re lacking some understanding critical to intelligence&#x2F;conciousness.<p>Given that with a mediocre math education and a week you could pretty completely understand all of the math that goes into these neurel nets, I really hope there’s some understand we don’t yet have</div><br/><div id="41448334" class="c"><input type="checkbox" id="c-41448334" checked=""/><div class="controls bullet"><span class="by">shwaj</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446718">parent</a><span>|</span><a href="#41446730">next</a><span>|</span><label class="collapse" for="c-41448334">[-]</label><label class="expand" for="c-41448334">[3 more]</label></div><br/><div class="children"><div class="content">There are layers of abstraction on top of “the math”.  The back propagation math for a transformer is no different than for a multi-layer perception, yet a transformer is vastly more capable than a MLP.  More to the point, it took a series of non-trivial steps to arrive at the transformer architecture. In other words, understanding the lowest-level math is no guarantee that you understand the whole thing, otherwise the transformer architecture would have been obvious.</div><br/><div id="41449816" class="c"><input type="checkbox" id="c-41449816" checked=""/><div class="controls bullet"><span class="by">barrell</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41448334">parent</a><span>|</span><a href="#41448999">next</a><span>|</span><label class="collapse" for="c-41449816">[-]</label><label class="expand" for="c-41449816">[1 more]</label></div><br/><div class="children"><div class="content">I don’t disagree that it’s non-trivial, but we’re comparing this to conciousness, intelligence, even life. Personally I think it’s apples and an orange grove, but I guess we’ll get our answer eventually. Pretty sure we’re on the path to take transformers to their limit, wherever that may be</div><br/></div></div><div id="41448999" class="c"><input type="checkbox" id="c-41448999" checked=""/><div class="controls bullet"><span class="by">theGnuMe</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41448334">parent</a><span>|</span><a href="#41449816">prev</a><span>|</span><a href="#41446730">next</a><span>|</span><label class="collapse" for="c-41448999">[-]</label><label class="expand" for="c-41448999">[1 more]</label></div><br/><div class="children"><div class="content">We know architecture and training procedures matter in practice.<p>MLPs and transformers are ultimately theoretically equivalent.  That means there is an MLP that represent the any function a given transformer can.  However, that MLP is hard to identify and train.<p>Also the transformer contains MLPs as well...</div><br/></div></div></div></div></div></div></div></div><div id="41446730" class="c"><input type="checkbox" id="c-41446730" checked=""/><div class="controls bullet"><span class="by">semiquaver</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446401">parent</a><span>|</span><a href="#41446479">prev</a><span>|</span><a href="#41446506">next</a><span>|</span><label class="collapse" for="c-41446730">[-]</label><label class="expand" for="c-41446730">[1 more]</label></div><br/><div class="children"><div class="content">There’s always a “significant possibility” that something unprecedented will turn out to be infeasible with any particular approach. How could it be otherwise? Smart people have incorrectly believed we were on the precipice of AGI many times in the 80 years that artificial neural networks have been part of the AI toolbox.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;AI_winter" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;AI_winter</a></div><br/></div></div><div id="41446506" class="c"><input type="checkbox" id="c-41446506" checked=""/><div class="controls bullet"><span class="by">waveBidder</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446401">parent</a><span>|</span><a href="#41446730">prev</a><span>|</span><a href="#41446455">next</a><span>|</span><label class="collapse" for="c-41446506">[-]</label><label class="expand" for="c-41446506">[4 more]</label></div><br/><div class="children"><div class="content">no, there&#x27;s really no comparing barely nonlinear algrebra that makes up transformers and the tangled mess that is human neurons. the name is an artifact and a useful bit of salesmanship.</div><br/><div id="41450428" class="c"><input type="checkbox" id="c-41450428" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446506">parent</a><span>|</span><a href="#41446455">next</a><span>|</span><label class="collapse" for="c-41450428">[-]</label><label class="expand" for="c-41450428">[3 more]</label></div><br/><div class="children"><div class="content">Sure, it&#x27;s a model. But don&#x27;t we think neural networks and human brains are primarily about their connectedness and feedback mechanisms though?<p>(I did AI and Psychology at degree level, I understand there are definitely also big differences too, like hormones and biological neurones being very async)</div><br/><div id="41451436" class="c"><input type="checkbox" id="c-41451436" checked=""/><div class="controls bullet"><span class="by">waveBidder</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41450428">parent</a><span>|</span><a href="#41446455">next</a><span>|</span><label class="collapse" for="c-41451436">[-]</label><label class="expand" for="c-41451436">[2 more]</label></div><br/><div class="children"><div class="content">You could <i>maybe</i> make a case for CNNs, but the fact that they&#x27;re feed-forward rather than feedback means they&#x27;re fundamentally representing a different object (CNN is a function, whereas the visual system is a feedback network).<p>Transformers, while not exactly functions, don&#x27;t have a feedback mechanism similar to e.g. the cortical algorithm or any other neuronal structure I&#x27;m aware of. In general, the ML field is less concerned with replicating neural mechanisms than following the objective gradient.</div><br/><div id="41453919" class="c"><input type="checkbox" id="c-41453919" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41451436">parent</a><span>|</span><a href="#41446455">next</a><span>|</span><label class="collapse" for="c-41453919">[-]</label><label class="expand" for="c-41453919">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the considered answer. What is the cortical algorithm? (Yeah, it&#x27;s been quite a few years since I did any bio psych...)</div><br/></div></div></div></div></div></div></div></div><div id="41446455" class="c"><input type="checkbox" id="c-41446455" checked=""/><div class="controls bullet"><span class="by">The_Colonel</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446401">parent</a><span>|</span><a href="#41446506">prev</a><span>|</span><a href="#41446495">next</a><span>|</span><label class="collapse" for="c-41446455">[-]</label><label class="expand" for="c-41446455">[4 more]</label></div><br/><div class="children"><div class="content">Neural networks in machine learning bear only a surface level similarity to human brain structure.</div><br/><div id="41450467" class="c"><input type="checkbox" id="c-41450467" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446455">parent</a><span>|</span><a href="#41446719">next</a><span>|</span><label class="collapse" for="c-41450467">[-]</label><label class="expand" for="c-41450467">[1 more]</label></div><br/><div class="children"><div class="content">Physically, sure. But 1) feedback (more synapses&#x2F;backprop) and 2) connectedness (huge complex graphs) of both produce very similar intelligent (or &quot;pseudo-intelligent&quot; if you like) emergent properties. I&#x27;m pretty sure 5 years ago nobody would have believed ANN&#x27;s could produce something as powerful as ChatGPT.</div><br/></div></div><div id="41446719" class="c"><input type="checkbox" id="c-41446719" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446455">parent</a><span>|</span><a href="#41450467">prev</a><span>|</span><a href="#41446495">next</a><span>|</span><label class="collapse" for="c-41446719">[-]</label><label class="expand" for="c-41446719">[2 more]</label></div><br/><div class="children"><div class="content">do you all not see how this is a completely different question?</div><br/><div id="41449218" class="c"><input type="checkbox" id="c-41449218" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446719">parent</a><span>|</span><a href="#41446495">next</a><span>|</span><label class="collapse" for="c-41449218">[-]</label><label class="expand" for="c-41449218">[1 more]</label></div><br/><div class="children"><div class="content">It seems to be intrinsically related. The argument goes something like:<p>1. Humans have general intelligence.
2. Human brains use biological neurons.
3. Human biological neurons give rise to human general intelligence.
4. Artificial neural networks (ANNs) are similar to human brains.
5. Therefore an ANN could give rise to artificial general intelligence.<p>Many people are objecting to #4 here. However in writing this out, I think #3 is suspect as well: many animals who do not have general intelligence have biologically identical neurons, and although they have clear structural differences with humans, we don’t know how that leads to general intelligence.<p>We could also criticize #1 as well, since human brains are pretty bad at certain things like memorization or calculation. Therefore if we built an ANN with only human capabilities it should also have those weaknesses.</div><br/></div></div></div></div></div></div><div id="41446495" class="c"><input type="checkbox" id="c-41446495" checked=""/><div class="controls bullet"><span class="by">consp</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446401">parent</a><span>|</span><a href="#41446455">prev</a><span>|</span><a href="#41447540">next</a><span>|</span><label class="collapse" for="c-41446495">[-]</label><label class="expand" for="c-41446495">[1 more]</label></div><br/><div class="children"><div class="content">I would replace &quot;use&quot; with &quot;vaguely look like&quot;.</div><br/></div></div><div id="41447540" class="c"><input type="checkbox" id="c-41447540" checked=""/><div class="controls bullet"><span class="by">zeroonetwothree</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446401">parent</a><span>|</span><a href="#41446495">prev</a><span>|</span><a href="#41447214">next</a><span>|</span><label class="collapse" for="c-41447540">[-]</label><label class="expand" for="c-41447540">[1 more]</label></div><br/><div class="children"><div class="content">For any technology we haven’t achieved yet there’s some probability we never achieve it (say, at least in the next 100 years). Why would AI be different?</div><br/></div></div><div id="41447214" class="c"><input type="checkbox" id="c-41447214" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446401">parent</a><span>|</span><a href="#41447540">prev</a><span>|</span><a href="#41448068">next</a><span>|</span><label class="collapse" for="c-41447214">[-]</label><label class="expand" for="c-41447214">[1 more]</label></div><br/><div class="children"><div class="content">Read up on astrocytes.</div><br/></div></div><div id="41448068" class="c"><input type="checkbox" id="c-41448068" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446401">parent</a><span>|</span><a href="#41447214">prev</a><span>|</span><a href="#41448187">next</a><span>|</span><label class="collapse" for="c-41448068">[-]</label><label class="expand" for="c-41448068">[1 more]</label></div><br/><div class="children"><div class="content">exactly, we probably can&#x27;t even build super intelligence. frankly what we need is more useful tools, we have to quit the idea of creating gods: <a href="https:&#x2F;&#x2F;medium.com&#x2F;@fsndzomga&#x2F;there-will-be-no-agi-d9be9af4428d" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@fsndzomga&#x2F;there-will-be-no-agi-d9be9af44...</a></div><br/></div></div><div id="41448187" class="c"><input type="checkbox" id="c-41448187" checked=""/><div class="controls bullet"><span class="by">otabdeveloper4</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446401">parent</a><span>|</span><a href="#41448068">prev</a><span>|</span><a href="#41452127">next</a><span>|</span><label class="collapse" for="c-41448187">[-]</label><label class="expand" for="c-41448187">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Human brains use neural networks...<p>They don&#x27;t, actually.</div><br/></div></div></div></div><div id="41452127" class="c"><input type="checkbox" id="c-41452127" checked=""/><div class="controls bullet"><span class="by">intelVISA</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446295">parent</a><span>|</span><a href="#41446401">prev</a><span>|</span><a href="#41448235">next</a><span>|</span><label class="collapse" for="c-41452127">[-]</label><label class="expand" for="c-41452127">[1 more]</label></div><br/><div class="children"><div class="content">Majority of ML these days is tokenbro projects, make of that what you will...</div><br/></div></div><div id="41448235" class="c"><input type="checkbox" id="c-41448235" checked=""/><div class="controls bullet"><span class="by">twobitshifter</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446295">parent</a><span>|</span><a href="#41452127">prev</a><span>|</span><a href="#41446349">next</a><span>|</span><label class="collapse" for="c-41448235">[-]</label><label class="expand" for="c-41448235">[1 more]</label></div><br/><div class="children"><div class="content">Maybe closer to energy positive fusion?</div><br/></div></div><div id="41446349" class="c"><input type="checkbox" id="c-41446349" checked=""/><div class="controls bullet"><span class="by">liminvorous</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446295">parent</a><span>|</span><a href="#41448235">prev</a><span>|</span><a href="#41449270">next</a><span>|</span><label class="collapse" for="c-41446349">[-]</label><label class="expand" for="c-41446349">[14 more]</label></div><br/><div class="children"><div class="content">No one had built a nuclear bomb before the Manhattan project either.</div><br/><div id="41446420" class="c"><input type="checkbox" id="c-41446420" checked=""/><div class="controls bullet"><span class="by">Yizahi</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446349">parent</a><span>|</span><a href="#41447578">next</a><span>|</span><label class="collapse" for="c-41446420">[-]</label><label class="expand" for="c-41446420">[6 more]</label></div><br/><div class="children"><div class="content">Theoretical foundation was slowly built over decades before it started though. And correct me if I&#x27;m wrong, but calculations that it was feasible were present before the start too. They had to calculate how to do it, what will be the processes, how to construct it and so on, but theoretically scientists knew that this amount of material can start such process.
On the other hand not only there is no clear path to AI today (also known as AGI, ASI, SI etc.), but even foundations are largely missing. We are debating what is intelligence, how it works, how to even start simulating it, or construct from scratch.</div><br/><div id="41446530" class="c"><input type="checkbox" id="c-41446530" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446420">parent</a><span>|</span><a href="#41446600">next</a><span>|</span><label class="collapse" for="c-41446530">[-]</label><label class="expand" for="c-41446530">[3 more]</label></div><br/><div class="children"><div class="content">There are algorithms that should work, they&#x27;re just galactic[0] or are otherwise expected to use far too much space and time to be practical.<p>[0]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Galactic_algorithm" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Galactic_algorithm</a></div><br/><div id="41449666" class="c"><input type="checkbox" id="c-41449666" checked=""/><div class="controls bullet"><span class="by">ryan93</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446530">parent</a><span>|</span><a href="#41446600">next</a><span>|</span><label class="collapse" for="c-41449666">[-]</label><label class="expand" for="c-41449666">[2 more]</label></div><br/><div class="children"><div class="content">That wiki article has nothing to do with AI. The whole AI space attracts BS talk</div><br/><div id="41449713" class="c"><input type="checkbox" id="c-41449713" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41449666">parent</a><span>|</span><a href="#41446600">next</a><span>|</span><label class="collapse" for="c-41449713">[-]</label><label class="expand" for="c-41449713">[1 more]</label></div><br/><div class="children"><div class="content">What do you think AI is? On that one page there&#x27;s simulated annealing with a logarithmic cooling schedule, Hutter search, and Solomonoff induction, all very much applicable to AI. If you want a fully complete galactic algorithm for AI, look up AIXItl.<p>Edit: actually I&#x27;m not sure if AIXItl is technically galactic or just terribly inefficient, but there&#x27;s been trouble making it faster and more compact.</div><br/></div></div></div></div></div></div><div id="41446600" class="c"><input type="checkbox" id="c-41446600" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446420">parent</a><span>|</span><a href="#41446530">prev</a><span>|</span><a href="#41447578">next</a><span>|</span><label class="collapse" for="c-41446600">[-]</label><label class="expand" for="c-41446600">[2 more]</label></div><br/><div class="children"><div class="content">The theoretical foundation of transformers is well understood; they&#x27;re able to approximate a very wide family of functions, particularly with chain of thought ( <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.07923" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.07923</a> ). Training them on next-token-prediction is essentially training them to compress, and more optimal compression requires a more accurate model of the world, so they&#x27;re being trained to model the world better and better. However you want to define intelligence, for practical purposes models with better and better models of the world are more and more useful.</div><br/><div id="41447596" class="c"><input type="checkbox" id="c-41447596" checked=""/><div class="controls bullet"><span class="by">zeroonetwothree</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446600">parent</a><span>|</span><a href="#41447578">next</a><span>|</span><label class="collapse" for="c-41447596">[-]</label><label class="expand" for="c-41447596">[1 more]</label></div><br/><div class="children"><div class="content">The disagreement here seems merely to be about what we mean by “AGI”. I think there’s reasons to think current approaches will not achieve it, but also reason to think they will.<p>In any case anyone who is completely sure that we can&#x2F;can’t achieve AGI is delusional.</div><br/></div></div></div></div></div></div><div id="41447578" class="c"><input type="checkbox" id="c-41447578" checked=""/><div class="controls bullet"><span class="by">zeroonetwothree</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446349">parent</a><span>|</span><a href="#41446420">prev</a><span>|</span><a href="#41449270">next</a><span>|</span><label class="collapse" for="c-41447578">[-]</label><label class="expand" for="c-41447578">[7 more]</label></div><br/><div class="children"><div class="content">this is not evidence in favor of your position. We could use this to argue in favor of anything such as “humans will eventually develop time travel” or “we will have cost effective fusion power”.<p>The fact is many things we’ve tried to develop for decades still don’t exist. Nothing is guaranteed</div><br/><div id="41447834" class="c"><input type="checkbox" id="c-41447834" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41447578">parent</a><span>|</span><a href="#41449021">next</a><span>|</span><label class="collapse" for="c-41447834">[-]</label><label class="expand" for="c-41447834">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;d put decent odds on a $1B research project developing time travel if time travel were an ability that every human child was innately born with. It&#x27;s never easy to recreate what biology has done, but nature providing an &quot;existence proof&quot; goes a long way towards removing doubt about it being fundamentally possible.</div><br/><div id="41448067" class="c"><input type="checkbox" id="c-41448067" checked=""/><div class="controls bullet"><span class="by">zombiwoof</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41447834">parent</a><span>|</span><a href="#41449021">next</a><span>|</span><label class="collapse" for="c-41448067">[-]</label><label class="expand" for="c-41448067">[4 more]</label></div><br/><div class="children"><div class="content">Nature didn’t build intelligence with non biological activity. And we won’t either</div><br/><div id="41448312" class="c"><input type="checkbox" id="c-41448312" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41448067">parent</a><span>|</span><a href="#41448133">next</a><span>|</span><label class="collapse" for="c-41448312">[-]</label><label class="expand" for="c-41448312">[1 more]</label></div><br/><div class="children"><div class="content">Unless you have any evidence suggesting that one or more of the variations of the Church-Turing thesis is false, this is closer to a statement of faith than science.<p>Basically, unless you can show humans calculating a non-Turing computable function, the notion that intelligence requires a biological system is an absolutely extraordinary claim.<p>If you were to argue about conscience or subjective experience or something equally woolly, you might have a stronger point, and this does not at all suggest that current-architecture LLMs will necessarily achieve it.</div><br/></div></div><div id="41448133" class="c"><input type="checkbox" id="c-41448133" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41448067">parent</a><span>|</span><a href="#41448312">prev</a><span>|</span><a href="#41448171">next</a><span>|</span><label class="collapse" for="c-41448133">[-]</label><label class="expand" for="c-41448133">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a big difference between &quot;this project is like time travel or cold fusion; it&#x27;s doubtful whether the laws of physics even permit it&quot; and &quot;this project is like heavier-than-air flight; we know birds do it somehow, but there&#x27;s no way our crude metal machines will ever match them&quot;. I&#x27;m confident which of those problems will get solved given, say, a hundred years or so, once people roll up their sleeves and get working on it.</div><br/></div></div><div id="41448171" class="c"><input type="checkbox" id="c-41448171" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41448067">parent</a><span>|</span><a href="#41448133">prev</a><span>|</span><a href="#41449021">next</a><span>|</span><label class="collapse" for="c-41448171">[-]</label><label class="expand" for="c-41448171">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Biological activity&quot; is just computation with different energy requirements. If science rules the universe we&#x27;re complex automata, and biologic machines or non-biological machines are just different combinations of atoms that are computing around.</div><br/></div></div></div></div></div></div><div id="41449021" class="c"><input type="checkbox" id="c-41449021" checked=""/><div class="controls bullet"><span class="by">fngjdflmdflg</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41447578">parent</a><span>|</span><a href="#41447834">prev</a><span>|</span><a href="#41449270">next</a><span>|</span><label class="collapse" for="c-41449021">[-]</label><label class="expand" for="c-41449021">[1 more]</label></div><br/><div class="children"><div class="content">Humans are an existing proof of human level intelligence. There are only two fundamental possibilities why this could not be replicated in silicon:<p>1. There is a chemical-level nature to intelligence which prevents other elements like silicon from being used as a substrate for intelligence<p>2. There is a non material aspect to intelligence that cannot be replicated except by humans<p>To my knowledge, there is no scientific evidence that either are true and there is already a large body of evidence that implies that intelligence happens at a higher level of abstraction than the individual chemical reactions of synapses, ie. the neural network, which does not rely on the existence of any specific chemicals in the system except in as much as they perform certain functions that seemingly could be performed by other materials. If anything, this is more like speculating that there is a way to create energy from sunlight using plants as an existence proof of the possibility of doing so. More specifically, this is a bet that an existing physical phenomenon can be replicated using a different substrate.</div><br/></div></div></div></div></div></div><div id="41449270" class="c"><input type="checkbox" id="c-41449270" checked=""/><div class="controls bullet"><span class="by">reducesuffering</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446295">parent</a><span>|</span><a href="#41446349">prev</a><span>|</span><a href="#41446573">next</a><span>|</span><label class="collapse" for="c-41449270">[-]</label><label class="expand" for="c-41449270">[2 more]</label></div><br/><div class="children"><div class="content">The only goalposts shifting are the ones who think completely blowing past the Turing Test, unlocking recursive exponential code generation, and a computer passing all the college standard tests (our way of determining human intelligence to go Harvard&#x2F;MIT) better than 99% of humans, isn&#x27;t a very big deal.</div><br/><div id="41449838" class="c"><input type="checkbox" id="c-41449838" checked=""/><div class="controls bullet"><span class="by">lupire</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41449270">parent</a><span>|</span><a href="#41446573">next</a><span>|</span><label class="collapse" for="c-41449838">[-]</label><label class="expand" for="c-41449838">[1 more]</label></div><br/><div class="children"><div class="content">Funny how a human can learn to do those things with approximately $1B less effort.</div><br/></div></div></div></div></div></div><div id="41446573" class="c"><input type="checkbox" id="c-41446573" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#41446071">parent</a><span>|</span><a href="#41446295">prev</a><span>|</span><a href="#41452634">next</a><span>|</span><label class="collapse" for="c-41446573">[-]</label><label class="expand" for="c-41446573">[5 more]</label></div><br/><div class="children"><div class="content">A non-cynical take is that Ilya wanted to do research without the pressure of having to release a marketable product and figuring out how to monetize their technology, which is why he left OpenAI.<p>A very cynical take is that this is an extreme version of &#x27;we plan to spend all money on growth and figure out monetization later&#x27; model that many social media companies with a burn rate of billions of $$, but no business model, have used.</div><br/><div id="41447231" class="c"><input type="checkbox" id="c-41447231" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446573">parent</a><span>|</span><a href="#41448903">next</a><span>|</span><label class="collapse" for="c-41447231">[-]</label><label class="expand" for="c-41447231">[1 more]</label></div><br/><div class="children"><div class="content">That’s not a cynical take, it’s the obvious take.</div><br/></div></div><div id="41448903" class="c"><input type="checkbox" id="c-41448903" checked=""/><div class="controls bullet"><span class="by">signatoremo</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446573">parent</a><span>|</span><a href="#41447231">prev</a><span>|</span><a href="#41452634">next</a><span>|</span><label class="collapse" for="c-41448903">[-]</label><label class="expand" for="c-41448903">[3 more]</label></div><br/><div class="children"><div class="content">He was on the record that their first product will be a safe superintelligence and it won’t do anything else until then, which sounds like they won’t have paid customers until they can figure out how to build a superintelligent model. That’s certainly a lofty goal and a very long term play.</div><br/><div id="41449855" class="c"><input type="checkbox" id="c-41449855" checked=""/><div class="controls bullet"><span class="by">lupire</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41448903">parent</a><span>|</span><a href="#41452634">next</a><span>|</span><label class="collapse" for="c-41449855">[-]</label><label class="expand" for="c-41449855">[2 more]</label></div><br/><div class="children"><div class="content">OpenAI was &quot;on the record&quot; with a lot of obsolete claims too. Money changes people.</div><br/><div id="41453274" class="c"><input type="checkbox" id="c-41453274" checked=""/><div class="controls bullet"><span class="by">signatoremo</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41449855">parent</a><span>|</span><a href="#41452634">next</a><span>|</span><label class="collapse" for="c-41453274">[-]</label><label class="expand" for="c-41453274">[1 more]</label></div><br/><div class="children"><div class="content">He didn’t promise world peace nor did he claim his work belongs to the humanity. The company is still a for profit corporation.<p>He is saying he will try to build something head and shoulders above anything else, and he got a billion dollars to do it with no expectation of revenue until his product is ready. The likelihood that he fails is very high, but his backers are willing to bet on that.</div><br/></div></div></div></div></div></div></div></div><div id="41452634" class="c"><input type="checkbox" id="c-41452634" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#41446071">parent</a><span>|</span><a href="#41446573">prev</a><span>|</span><a href="#41446736">next</a><span>|</span><label class="collapse" for="c-41452634">[-]</label><label class="expand" for="c-41452634">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Closest thing we have to a Manhattan Project in the modern era?<p>No. The Manhattan Project started after we understood the basic mechanism of runaway fission reactions. The funding was mostly spent purifying uranium.<p>AGI would be similar if we understood the mechanism of creating general intelligence and just needed to scale it up. But there are still fundamental questions we still aren’t close to understanding for AGI.<p>A more apt comparison today is probably something like fusion reactors although progress has been slow there too. We know how fusion works in theory. We have done it before (thermonuclear weapons). There are sub-problems we need to solve, but people are working on them. For AGI we don’t even know what the sub-problems are yet.</div><br/></div></div><div id="41446736" class="c"><input type="checkbox" id="c-41446736" checked=""/><div class="controls bullet"><span class="by">apwell23</span><span>|</span><a href="#41446071">parent</a><span>|</span><a href="#41452634">prev</a><span>|</span><a href="#41446582">next</a><span>|</span><label class="collapse" for="c-41446736">[-]</label><label class="expand" for="c-41446736">[3 more]</label></div><br/><div class="children"><div class="content">&gt; superintelligence is reached<p>i read the article but I am not sure how they know when this condition will be true.<p>Is this obvious to ppl reading this article? is it emperor has no clothes type situation ?</div><br/><div id="41452679" class="c"><input type="checkbox" id="c-41452679" checked=""/><div class="controls bullet"><span class="by">crappybird</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446736">parent</a><span>|</span><a href="#41448522">next</a><span>|</span><label class="collapse" for="c-41452679">[-]</label><label class="expand" for="c-41452679">[1 more]</label></div><br/><div class="children"><div class="content">They can dilute the term to whatever they want. I think when the pressure to release becomes too high, they can just stick a patch of &quot;Superintelligence™&quot; on their latest LLM and release it.</div><br/></div></div><div id="41448522" class="c"><input type="checkbox" id="c-41448522" checked=""/><div class="controls bullet"><span class="by">Propelloni</span><span>|</span><a href="#41446071">root</a><span>|</span><a href="#41446736">parent</a><span>|</span><a href="#41452679">prev</a><span>|</span><a href="#41446582">next</a><span>|</span><label class="collapse" for="c-41448522">[-]</label><label class="expand" for="c-41448522">[1 more]</label></div><br/><div class="children"><div class="content">You are not alone. This is the litmus test many people are contemplating for a long time now, mostly philosophers, which is not surprising since it is a philosophical question. Most of the heavy stuff is hidden behind paywalls, but here&#x27;s a nice summary of the state of the art by two CS guys: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2212.06721" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2212.06721</a></div><br/></div></div></div></div><div id="41446582" class="c"><input type="checkbox" id="c-41446582" checked=""/><div class="controls bullet"><span class="by">jchonphoenix</span><span>|</span><a href="#41446071">parent</a><span>|</span><a href="#41446736">prev</a><span>|</span><a href="#41447786">next</a><span>|</span><label class="collapse" for="c-41446582">[-]</label><label class="expand" for="c-41446582">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI initially raised 50m in their institutional round.<p>1b was a non profit donation, so there wasn&#x27;t an expectation of returns on that one.</div><br/></div></div><div id="41447786" class="c"><input type="checkbox" id="c-41447786" checked=""/><div class="controls bullet"><span class="by">TrackerFF</span><span>|</span><a href="#41446071">parent</a><span>|</span><a href="#41446582">prev</a><span>|</span><a href="#41453013">next</a><span>|</span><label class="collapse" for="c-41447786">[-]</label><label class="expand" for="c-41447786">[1 more]</label></div><br/><div class="children"><div class="content">To my ears, it&#x27;s more like a ambitious pharma project.<p>There&#x27;s plenty of players going for the same goal. R&amp;D is wildly expensive. No guarantee they&#x27;ll reach the goal, first or even at all.</div><br/></div></div><div id="41453013" class="c"><input type="checkbox" id="c-41453013" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#41446071">parent</a><span>|</span><a href="#41447786">prev</a><span>|</span><a href="#41446318">next</a><span>|</span><label class="collapse" for="c-41453013">[-]</label><label class="expand" for="c-41453013">[1 more]</label></div><br/><div class="children"><div class="content">Super Intelligence, even OpenAI when getting investment from Microsoft, OpenAI won’t have to share its “AGI” model to them and it is up to OpenAI to define what that is and who the heck knows how they will define it.  The point is that that phrase is the most ambiguous word in tech right now and almost everyone thinks what’s in their head is AGI, some will think Skynet, some will think enough reason ability, some will think god like undecipherable logic and everywhere in between</div><br/></div></div><div id="41446318" class="c"><input type="checkbox" id="c-41446318" checked=""/><div class="controls bullet"><span class="by">swader999</span><span>|</span><a href="#41446071">parent</a><span>|</span><a href="#41453013">prev</a><span>|</span><a href="#41448276">next</a><span>|</span><label class="collapse" for="c-41446318">[-]</label><label class="expand" for="c-41446318">[1 more]</label></div><br/><div class="children"><div class="content">Both need a crap tonne of electricity.</div><br/></div></div><div id="41448276" class="c"><input type="checkbox" id="c-41448276" checked=""/><div class="controls bullet"><span class="by">choppaface</span><span>|</span><a href="#41446071">parent</a><span>|</span><a href="#41446318">prev</a><span>|</span><a href="#41452813">next</a><span>|</span><label class="collapse" for="c-41448276">[-]</label><label class="expand" for="c-41448276">[1 more]</label></div><br/><div class="children"><div class="content">Could be more comparable to Clubhouse, which VCs quickly piled $100m into[1a], and which Clubhouse notably turned into layoffs [1b].  In this case, the $1b in funding and high valuation might function predominantly as a deterrent to any flippers (in contrast, many Clubhouse investors got quick gains).<p>Moreover, the majority of the capital likely goes into GPU hardware and&#x2F;or opex, which VCs have currently arbitraged themselves [3], so to some extent this is VCs literally paying themselves to pay off their own hardware bet.<p>While hints of the ambition of the Manhattan project might be there, the economics really are not.<p>[1a] <a href="https:&#x2F;&#x2F;www.getpin.xyz&#x2F;post&#x2F;clubhouse-lessons-for-investors" rel="nofollow">https:&#x2F;&#x2F;www.getpin.xyz&#x2F;post&#x2F;clubhouse-lessons-for-investors</a> 
[1b] <a href="https:&#x2F;&#x2F;www.theverge.com&#x2F;2023&#x2F;4&#x2F;27&#x2F;23701144&#x2F;clubhouse-layoffs-half-employees" rel="nofollow">https:&#x2F;&#x2F;www.theverge.com&#x2F;2023&#x2F;4&#x2F;27&#x2F;23701144&#x2F;clubhouse-layoff...</a>
[3] <a href="https:&#x2F;&#x2F;observer.com&#x2F;2024&#x2F;07&#x2F;andreessen-horowitz-stocking-ai-chips-win-deals-gpu-shortage&#x2F;" rel="nofollow">https:&#x2F;&#x2F;observer.com&#x2F;2024&#x2F;07&#x2F;andreessen-horowitz-stocking-ai...</a></div><br/></div></div><div id="41452813" class="c"><input type="checkbox" id="c-41452813" checked=""/><div class="controls bullet"><span class="by">Runsthroughit</span><span>|</span><a href="#41446071">parent</a><span>|</span><a href="#41448276">prev</a><span>|</span><a href="#41453499">next</a><span>|</span><label class="collapse" for="c-41452813">[-]</label><label class="expand" for="c-41452813">[1 more]</label></div><br/><div class="children"><div class="content">Literally everyone from OpenAI lied 100% about everything of substance. 
Sutskever lied about &quot;world model&quot; inside of LLMs, which is such a despicable lie, because he knows that &quot;latent space&quot; is a TOTAL MESS. Proven everytime anyone looked at it. 
Shameless grifters. When end?</div><br/></div></div></div></div><div id="41446929" class="c"><input type="checkbox" id="c-41446929" checked=""/><div class="controls bullet"><span class="by">koolala</span><span>|</span><a href="#41446071">prev</a><span>|</span><a href="#41449913">next</a><span>|</span><label class="collapse" for="c-41446929">[-]</label><label class="expand" for="c-41446929">[4 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t this corrupt SafeAI&#x27;s safe vision just like $1,000,000,000 corrupted OpenAI&#x27;s open vision?<p>How can investment like this not transform a company&#x27;s mission into eventually paying back Billions and making Billions of dollars?</div><br/><div id="41451738" class="c"><input type="checkbox" id="c-41451738" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41446929">parent</a><span>|</span><a href="#41452942">next</a><span>|</span><label class="collapse" for="c-41451738">[-]</label><label class="expand" for="c-41451738">[1 more]</label></div><br/><div class="children"><div class="content">Well that&#x27;s easy, you just don&#x27;t pay it back.<p>It helps if you think of the investors as customers and the business model as making them think they&#x27;re cool. Same model Uber used for self driving car research.<p>SSI Inc should probably be a public benefit company if they&#x27;re going to talk like that though.</div><br/></div></div><div id="41452942" class="c"><input type="checkbox" id="c-41452942" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#41446929">parent</a><span>|</span><a href="#41451738">prev</a><span>|</span><a href="#41451046">next</a><span>|</span><label class="collapse" for="c-41452942">[-]</label><label class="expand" for="c-41452942">[1 more]</label></div><br/><div class="children"><div class="content">Seems strange to associate profit motives with being unsafe. Yes cutting corners can lead to short term profits but many companies make safety a priority in fact and make a profit, and in fact make a profit because their product is higher quality and safer than the competitors.</div><br/></div></div><div id="41451046" class="c"><input type="checkbox" id="c-41451046" checked=""/><div class="controls bullet"><span class="by">null0pointer</span><span>|</span><a href="#41446929">parent</a><span>|</span><a href="#41452942">prev</a><span>|</span><a href="#41449913">next</a><span>|</span><label class="collapse" for="c-41451046">[-]</label><label class="expand" for="c-41451046">[1 more]</label></div><br/><div class="children"><div class="content">Yep, investment is an inevitably corrupting force for a company&#x27;s mission. AI stuff is in a bit of a catch-22 though since doing anything AI related is so expensive you need to raise funds somehow.</div><br/></div></div></div></div><div id="41449913" class="c"><input type="checkbox" id="c-41449913" checked=""/><div class="controls bullet"><span class="by">throwawayk7h</span><span>|</span><a href="#41446929">prev</a><span>|</span><a href="#41451833">next</a><span>|</span><label class="collapse" for="c-41449913">[-]</label><label class="expand" for="c-41449913">[2 more]</label></div><br/><div class="children"><div class="content">Everyone here is assuming that a very large LLM is their goal. 5 years ago, transformer models were not the biggest hype in AI. Since they apparently have a 10 year plan, we can assume they are hoping to invent one or two of the &quot;big steps&quot; (on the order of invention of transformer models). &quot;SSI&quot; might look nothing like GPT\d.</div><br/><div id="41453816" class="c"><input type="checkbox" id="c-41453816" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41449913">parent</a><span>|</span><a href="#41451833">next</a><span>|</span><label class="collapse" for="c-41453816">[-]</label><label class="expand" for="c-41453816">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Everyone here is assuming that a very large LLM is their goal<p>No, not even close.</div><br/></div></div></div></div><div id="41451833" class="c"><input type="checkbox" id="c-41451833" checked=""/><div class="controls bullet"><span class="by">motohagiography</span><span>|</span><a href="#41449913">prev</a><span>|</span><a href="#41445932">next</a><span>|</span><label class="collapse" for="c-41451833">[-]</label><label class="expand" for="c-41451833">[6 more]</label></div><br/><div class="children"><div class="content">CFO&#x27;s here, let&#x27;s say I raise a round like that. What do you do with $1B in cash to manage it in the short and near term though? Is it just stuck in the money market or t-bills or what? Even if the growth of the company is the main bet, that cash has to exist as something with a better return than cash.<p>I assume that service is what SV bank provided before it tanked, but someone has to manage that cash for the few years it takes to burn through it. What kind of service do you park that in.</div><br/><div id="41451869" class="c"><input type="checkbox" id="c-41451869" checked=""/><div class="controls bullet"><span class="by">huevosabio</span><span>|</span><a href="#41451833">parent</a><span>|</span><a href="#41451920">next</a><span>|</span><label class="collapse" for="c-41451869">[-]</label><label class="expand" for="c-41451869">[1 more]</label></div><br/><div class="children"><div class="content">They probably already went to investors with letters of sale from GPU&#x2F;datacenter providers.<p>And 80% of those $1B will go from Founder Mode VC to Nvidia and Datacenter Management Co in the span of 6 months.</div><br/></div></div><div id="41451920" class="c"><input type="checkbox" id="c-41451920" checked=""/><div class="controls bullet"><span class="by">TezNoble1988</span><span>|</span><a href="#41451833">parent</a><span>|</span><a href="#41451869">prev</a><span>|</span><a href="#41452473">next</a><span>|</span><label class="collapse" for="c-41451920">[-]</label><label class="expand" for="c-41451920">[2 more]</label></div><br/><div class="children"><div class="content">Sometimes, these very large rounds are delivered in tranches based on milestones. It&#x27;s possible that SSI didn&#x27;t receive the entire $1BN at the close of the fundraise but rather can &quot;call capital,&quot; much like a VC fund does, as it needs it based on scaling.</div><br/><div id="41451948" class="c"><input type="checkbox" id="c-41451948" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#41451833">root</a><span>|</span><a href="#41451920">parent</a><span>|</span><a href="#41452473">next</a><span>|</span><label class="collapse" for="c-41451948">[-]</label><label class="expand" for="c-41451948">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Sometimes, these very large rounds are delivered in tranches based on milestones<p>Almost always, even not so large rounds.</div><br/></div></div></div></div><div id="41452473" class="c"><input type="checkbox" id="c-41452473" checked=""/><div class="controls bullet"><span class="by">Mengkudulangsat</span><span>|</span><a href="#41451833">parent</a><span>|</span><a href="#41451920">prev</a><span>|</span><a href="#41451889">next</a><span>|</span><label class="collapse" for="c-41452473">[-]</label><label class="expand" for="c-41452473">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes the cash doesn&#x27;t even leave the VC until a capital call is made. This &quot;round&quot; is just to lock in a valuation.</div><br/></div></div><div id="41451889" class="c"><input type="checkbox" id="c-41451889" checked=""/><div class="controls bullet"><span class="by">laluser</span><span>|</span><a href="#41451833">parent</a><span>|</span><a href="#41452473">prev</a><span>|</span><a href="#41445932">next</a><span>|</span><label class="collapse" for="c-41451889">[-]</label><label class="expand" for="c-41451889">[1 more]</label></div><br/><div class="children"><div class="content">Short-term treasury bills from the government.</div><br/></div></div></div></div><div id="41445932" class="c"><input type="checkbox" id="c-41445932" checked=""/><div class="controls bullet"><span class="by">ramraj07</span><span>|</span><a href="#41451833">prev</a><span>|</span><a href="#41449702">next</a><span>|</span><label class="collapse" for="c-41445932">[-]</label><label class="expand" for="c-41445932">[11 more]</label></div><br/><div class="children"><div class="content">Getting funded by a16z is if anything a sign that the field is not hot anymore.</div><br/><div id="41446005" class="c"><input type="checkbox" id="c-41446005" checked=""/><div class="controls bullet"><span class="by">toomuchtodo</span><span>|</span><a href="#41445932">parent</a><span>|</span><a href="#41446023">next</a><span>|</span><label class="collapse" for="c-41446005">[-]</label><label class="expand" for="c-41446005">[3 more]</label></div><br/><div class="children"><div class="content">All money is green, regardless of level of sophistication. If you’re using investment firm pedigree as signal, gonna have a bad time. They’re all just throwin’ darts under the guise of skill (actor&#x2F;observer|outcome bias; when you win, it is skill; when you lose, it was luck, broadly speaking).<p>&gt; Indeed, one should be sophisticated themselves when negotiating investment to not be unduly encumbered by the unsophisticated. But let us not get too far off topic and risk subthread detachment.<p>Edit: @jgalt212: Indeed, one should be sophisticated themselves when negotiating investment to not be unduly encumbered by shades of the unsophisticated or potentially folks not optimizing for aligned interests. But let us not get too far off topic and risk subthread detachment. Feel free to cut a new thread for further discussion on the subject.</div><br/><div id="41446081" class="c"><input type="checkbox" id="c-41446081" checked=""/><div class="controls bullet"><span class="by">jgalt212</span><span>|</span><a href="#41445932">root</a><span>|</span><a href="#41446005">parent</a><span>|</span><a href="#41446023">next</a><span>|</span><label class="collapse" for="c-41446081">[-]</label><label class="expand" for="c-41446081">[2 more]</label></div><br/><div class="children"><div class="content">&gt; All money is green, regardless of level of sophistication.<p>True, but most, if not all, money comes with strings attached.</div><br/></div></div></div></div><div id="41446023" class="c"><input type="checkbox" id="c-41446023" checked=""/><div class="controls bullet"><span class="by">samvher</span><span>|</span><a href="#41445932">parent</a><span>|</span><a href="#41446005">prev</a><span>|</span><a href="#41446341">next</a><span>|</span><label class="collapse" for="c-41446023">[-]</label><label class="expand" for="c-41446023">[2 more]</label></div><br/><div class="children"><div class="content">Why do you say that? I feel out of the loop</div><br/></div></div><div id="41446341" class="c"><input type="checkbox" id="c-41446341" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#41445932">parent</a><span>|</span><a href="#41446023">prev</a><span>|</span><a href="#41454655">next</a><span>|</span><label class="collapse" for="c-41446341">[-]</label><label class="expand" for="c-41446341">[2 more]</label></div><br/><div class="children"><div class="content">Almost every recent AI startup with buzz has had a16z as its primary investor.</div><br/><div id="41446396" class="c"><input type="checkbox" id="c-41446396" checked=""/><div class="controls bullet"><span class="by">typon</span><span>|</span><a href="#41445932">root</a><span>|</span><a href="#41446341">parent</a><span>|</span><a href="#41454655">next</a><span>|</span><label class="collapse" for="c-41446396">[-]</label><label class="expand" for="c-41446396">[1 more]</label></div><br/><div class="children"><div class="content">Maybe that proves his point?</div><br/></div></div></div></div><div id="41454655" class="c"><input type="checkbox" id="c-41454655" checked=""/><div class="controls bullet"><span class="by">malthaus</span><span>|</span><a href="#41445932">parent</a><span>|</span><a href="#41446341">prev</a><span>|</span><a href="#41446041">next</a><span>|</span><label class="collapse" for="c-41454655">[-]</label><label class="expand" for="c-41454655">[1 more]</label></div><br/><div class="children"><div class="content">a16z is like that dubai bro who jumps late on any fad trying to scam people while burning through his daddy&#x27;s money</div><br/></div></div><div id="41446041" class="c"><input type="checkbox" id="c-41446041" checked=""/><div class="controls bullet"><span class="by">duxup</span><span>|</span><a href="#41445932">parent</a><span>|</span><a href="#41454655">prev</a><span>|</span><a href="#41449702">next</a><span>|</span><label class="collapse" for="c-41446041">[-]</label><label class="expand" for="c-41446041">[2 more]</label></div><br/><div class="children"><div class="content">Why is that?</div><br/><div id="41448570" class="c"><input type="checkbox" id="c-41448570" checked=""/><div class="controls bullet"><span class="by">pajeets</span><span>|</span><a href="#41445932">root</a><span>|</span><a href="#41446041">parent</a><span>|</span><a href="#41449702">next</a><span>|</span><label class="collapse" for="c-41448570">[-]</label><label class="expand" for="c-41448570">[1 more]</label></div><br/><div class="children"><div class="content">Might be the <i>almost</i> securities fraud they were doing with crypto when it was fizzling out in 2022<p>Regardless, point is moot, money is money, and a16z&#x27;s money isn&#x27;t their money but other people&#x27;s money</div><br/></div></div></div></div></div></div><div id="41449702" class="c"><input type="checkbox" id="c-41449702" checked=""/><div class="controls bullet"><span class="by">whiplash451</span><span>|</span><a href="#41445932">prev</a><span>|</span><a href="#41446885">next</a><span>|</span><label class="collapse" for="c-41449702">[-]</label><label class="expand" for="c-41449702">[1 more]</label></div><br/><div class="children"><div class="content">Ilya is basically building the Tandem Computers of AI.<p>Before Tandem, computers used to fail regularly. Tandem changed that forever (with a massive reward for their investors).<p>Similarly, LLMs are known to fail regularly. Until someone figures out a way for them not to hallucinate anymore. Which is exactly what Ilya is after.</div><br/></div></div><div id="41446885" class="c"><input type="checkbox" id="c-41446885" checked=""/><div class="controls bullet"><span class="by">bluecalm</span><span>|</span><a href="#41449702">prev</a><span>|</span><a href="#41447211">next</a><span>|</span><label class="collapse" for="c-41446885">[-]</label><label class="expand" for="c-41446885">[4 more]</label></div><br/><div class="children"><div class="content">Considering that Sam Bankman-Fried raised more money at higher multiplier for a company to trade magic tokens and grand ideas such as that maybe one day you will be able to buy a banana with them I don&#x27;t think Ilya impressed the investors too much.<p>On a serious note I would love to bet on him at this valuation. I think many others would as well. I guess if he wanted more money he would easily get it but probably he values small circle of easy to live investors instead.</div><br/><div id="41447078" class="c"><input type="checkbox" id="c-41447078" checked=""/><div class="controls bullet"><span class="by">Maxatar</span><span>|</span><a href="#41446885">parent</a><span>|</span><a href="#41447211">next</a><span>|</span><label class="collapse" for="c-41447078">[-]</label><label class="expand" for="c-41447078">[3 more]</label></div><br/><div class="children"><div class="content">FTX was incredibly profitable, and their main competitor Binance is today a money printing machine. FTX failed because of fraud and embezzlement, not because their core business was failing.</div><br/><div id="41451756" class="c"><input type="checkbox" id="c-41451756" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41446885">root</a><span>|</span><a href="#41447078">parent</a><span>|</span><a href="#41447211">next</a><span>|</span><label class="collapse" for="c-41451756">[-]</label><label class="expand" for="c-41451756">[2 more]</label></div><br/><div class="children"><div class="content">FTX itself was profitable, but that&#x27;s because Alameda Research was selling dollars for 80 cents, and all the other traders were paying FTX fees to rip off Alameda. Unfortunately, Alameda was running on FTX customer money.</div><br/><div id="41452164" class="c"><input type="checkbox" id="c-41452164" checked=""/><div class="controls bullet"><span class="by">preciousoo</span><span>|</span><a href="#41446885">root</a><span>|</span><a href="#41451756">parent</a><span>|</span><a href="#41447211">next</a><span>|</span><label class="collapse" for="c-41452164">[-]</label><label class="expand" for="c-41452164">[1 more]</label></div><br/><div class="children"><div class="content">20 GOTO 10 ?</div><br/></div></div></div></div></div></div></div></div><div id="41447211" class="c"><input type="checkbox" id="c-41447211" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#41446885">prev</a><span>|</span><a href="#41446351">next</a><span>|</span><label class="collapse" for="c-41447211">[-]</label><label class="expand" for="c-41447211">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Safe Superintelligence (SSI), newly co-founded by OpenAI&#x27;s former chief scientist Ilya Sutskever, has raised $1 billion in cash to help develop safe artificial intelligence systems that far surpass human capabilities, company executives told Reuters.<p>&gt;SSI says it plans to partner with cloud providers and chip companies to fund its computing power needs but hasn&#x27;t yet decided which firms it will work with.<p>1bn in cash is crazy.... usually they get cloud compute credits (which they count as funding)</div><br/></div></div><div id="41446351" class="c"><input type="checkbox" id="c-41446351" checked=""/><div class="controls bullet"><span class="by">avocardio</span><span>|</span><a href="#41447211">prev</a><span>|</span><a href="#41446800">next</a><span>|</span><label class="collapse" for="c-41446351">[-]</label><label class="expand" for="c-41446351">[10 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand how &quot;safe&quot; AI can raise that much money. If anything, they will have to spend double the time on red-teaming before releasing anything commercially. &quot;Unsafe&quot; AI seems much more profitable.</div><br/><div id="41446726" class="c"><input type="checkbox" id="c-41446726" checked=""/><div class="controls bullet"><span class="by">upwardbound</span><span>|</span><a href="#41446351">parent</a><span>|</span><a href="#41448919">next</a><span>|</span><label class="collapse" for="c-41446726">[-]</label><label class="expand" for="c-41446726">[3 more]</label></div><br/><div class="children"><div class="content">Unsafe AI would cause human extinction which is bad for shareholders because shareholders are human persons and&#x2F;or corporations beneficially owned by humans.<p>Related to this, DAO&#x27;s (decentralized autonomous organizations which <i>do not</i> have human shareholders) are intrinsically dangerous, because they can benefit their fiduciary duty even if it involves causing all humans to die.  E.g., if the machine faction in The Matrix were to exist within the framework of US laws, it would probably be a DAO.</div><br/><div id="41451784" class="c"><input type="checkbox" id="c-41451784" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41446351">root</a><span>|</span><a href="#41446726">parent</a><span>|</span><a href="#41452363">next</a><span>|</span><label class="collapse" for="c-41451784">[-]</label><label class="expand" for="c-41451784">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no legal structure that has that level of fiduciary duty to anything. Corporations don&#x27;t even really have fiduciary duty to their shareholders, and no CEO thinks they do.<p><a href="https:&#x2F;&#x2F;www.businessroundtable.org&#x2F;business-roundtable-redefines-the-purpose-of-a-corporation-to-promote-an-economy-that-serves-all-americans" rel="nofollow">https:&#x2F;&#x2F;www.businessroundtable.org&#x2F;business-roundtable-redef...</a><p>The idea behind &quot;corporations should only focus on returns to shareholders&quot; is that if you let them do anything else, CEOs will just set whatever targets they want, and it makes it harder to judge if they&#x27;re doing the right thing or if they&#x27;re even good at it. It&#x27;s basically reducing corporate power in that sense.<p>&gt; E.g., if the machine faction in The Matrix were to exist within the framework of US laws, it would probably be a DAO.<p>That&#x27;d have to be a corporation with a human lawyer as the owner or something. No such legal concept as a DAO that I&#x27;m aware of.</div><br/></div></div></div></div><div id="41448919" class="c"><input type="checkbox" id="c-41448919" checked=""/><div class="controls bullet"><span class="by">twobitshifter</span><span>|</span><a href="#41446351">parent</a><span>|</span><a href="#41446726">prev</a><span>|</span><a href="#41446750">next</a><span>|</span><label class="collapse" for="c-41448919">[-]</label><label class="expand" for="c-41448919">[2 more]</label></div><br/><div class="children"><div class="content">Safe super-intelligence will likely be as safe as OpenAI is open.<p>We can’t build critical software without huge security holes and bugs (see crowdstrike) but we think we will be able to contain something smarter than us? It would only take one vulnerability.</div><br/><div id="41450232" class="c"><input type="checkbox" id="c-41450232" checked=""/><div class="controls bullet"><span class="by">stuckkeys</span><span>|</span><a href="#41446351">root</a><span>|</span><a href="#41448919">parent</a><span>|</span><a href="#41446750">next</a><span>|</span><label class="collapse" for="c-41450232">[-]</label><label class="expand" for="c-41450232">[1 more]</label></div><br/><div class="children"><div class="content">You are not wrong. But Crowdstrike comparison is not “IT” they should have never had direct kernel access. MS set themself up for that one. SSI or whatever the hype will be in the coming future, it would be very difficult to beat. Unless of you shut down the power. It could develop guard rails instantly. So any flaw you may come up with, it would be instantly patched. Ofc this is just my take.</div><br/></div></div></div></div><div id="41446750" class="c"><input type="checkbox" id="c-41446750" checked=""/><div class="controls bullet"><span class="by">planetpluta</span><span>|</span><a href="#41446351">parent</a><span>|</span><a href="#41448919">prev</a><span>|</span><a href="#41448482">next</a><span>|</span><label class="collapse" for="c-41446750">[-]</label><label class="expand" for="c-41446750">[1 more]</label></div><br/><div class="children"><div class="content">We don’t know the counter factual here… maybe if he called it “Unsafe Superintelligence Inc” they would have raised 5x! (though I have doubts about that)</div><br/></div></div><div id="41448482" class="c"><input type="checkbox" id="c-41448482" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#41446351">parent</a><span>|</span><a href="#41446750">prev</a><span>|</span><a href="#41446691">next</a><span>|</span><label class="collapse" for="c-41448482">[-]</label><label class="expand" for="c-41448482">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t understand how &quot;safe&quot; AI can raise that much money.<p>enterprises, corps, banks, governments will want to buy &quot;safe&quot; AI, to push liability for mistakes on someone who proclaimed them &quot;safe&quot;.</div><br/></div></div><div id="41446691" class="c"><input type="checkbox" id="c-41446691" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#41446351">parent</a><span>|</span><a href="#41448482">prev</a><span>|</span><a href="#41446800">next</a><span>|</span><label class="collapse" for="c-41446691">[-]</label><label class="expand" for="c-41446691">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Safe&quot; means &quot;aligned with the people controlling it&quot;. A powerful superhuman AI that blindly obeys would be incredibly valuable to any wannabe authoritarian or despot.</div><br/><div id="41446799" class="c"><input type="checkbox" id="c-41446799" checked=""/><div class="controls bullet"><span class="by">digging</span><span>|</span><a href="#41446351">root</a><span>|</span><a href="#41446691">parent</a><span>|</span><a href="#41446800">next</a><span>|</span><label class="collapse" for="c-41446799">[-]</label><label class="expand" for="c-41446799">[1 more]</label></div><br/><div class="children"><div class="content">I mean, no, that&#x27;s not what it means. It might be what we get, but not because &quot;safety&quot; is defined insanely, only because safety is extremely difficult and might be impossible.</div><br/></div></div></div></div></div></div><div id="41446800" class="c"><input type="checkbox" id="c-41446800" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#41446351">prev</a><span>|</span><a href="#41454784">next</a><span>|</span><label class="collapse" for="c-41446800">[-]</label><label class="expand" for="c-41446800">[4 more]</label></div><br/><div class="children"><div class="content">All that money, we are not even sure we can build AGI. What is AGI. Clearly scaling LLMs won&#x27;t cut it, but VCs keep funding people because they pretend they can build super intelligence. I don&#x27;t see that happening in the next 5 years: <a href="https:&#x2F;&#x2F;medium.com&#x2F;@fsndzomga&#x2F;there-will-be-no-agi-d9be9af4428d" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@fsndzomga&#x2F;there-will-be-no-agi-d9be9af44...</a></div><br/><div id="41447203" class="c"><input type="checkbox" id="c-41447203" checked=""/><div class="controls bullet"><span class="by">tasuki</span><span>|</span><a href="#41446800">parent</a><span>|</span><a href="#41451665">next</a><span>|</span><label class="collapse" for="c-41447203">[-]</label><label class="expand" for="c-41447203">[1 more]</label></div><br/><div class="children"><div class="content">If we were sure we could build superhuman intelligence, the valuation would&#x27;ve been a lot higher!</div><br/></div></div><div id="41451665" class="c"><input type="checkbox" id="c-41451665" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#41446800">parent</a><span>|</span><a href="#41447203">prev</a><span>|</span><a href="#41454784">next</a><span>|</span><label class="collapse" for="c-41451665">[-]</label><label class="expand" for="c-41451665">[2 more]</label></div><br/><div class="children"><div class="content">What’s your evidence that scaling won’t improve AI?</div><br/><div id="41452696" class="c"><input type="checkbox" id="c-41452696" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#41446800">root</a><span>|</span><a href="#41451665">parent</a><span>|</span><a href="#41454784">next</a><span>|</span><label class="collapse" for="c-41452696">[-]</label><label class="expand" for="c-41452696">[1 more]</label></div><br/><div class="children"><div class="content">The question isn’t whether scaling will improve AI. The question is whether the return is worth it. You can build a bigger pogo stick to jump higher, but no pogo stick will get you to the moon.<p>Chess is a pretty good example. You could theoretically train an LLM on just chess games. The problem is there are more chess positions than atoms in the universe. So you can’t actually do it in practice. And chess is a much more constrained environment than life. At any chess position there are only ~35 moves on average. Life has tons of long-tail situations which have never been seen before.<p>And for chess we already have superhuman intelligence. It doesn’t require trillion-dollar training clusters, you can run a superhuman chess bot on your phone. So there are clear questions of optimality as well: VC money should be aware of the opportunity cost in investing money under “infinite scaling” assumptions.</div><br/></div></div></div></div></div></div><div id="41454784" class="c"><input type="checkbox" id="c-41454784" checked=""/><div class="controls bullet"><span class="by">christkv</span><span>|</span><a href="#41446800">prev</a><span>|</span><a href="#41445956">next</a><span>|</span><label class="collapse" for="c-41454784">[-]</label><label class="expand" for="c-41454784">[1 more]</label></div><br/><div class="children"><div class="content">I wonder what % goes to Nvidia.</div><br/></div></div><div id="41445956" class="c"><input type="checkbox" id="c-41445956" checked=""/><div class="controls bullet"><span class="by">phmagic</span><span>|</span><a href="#41454784">prev</a><span>|</span><a href="#41445936">next</a><span>|</span><label class="collapse" for="c-41445956">[-]</label><label class="expand" for="c-41445956">[7 more]</label></div><br/><div class="children"><div class="content">Good news for NVDA.</div><br/><div id="41446651" class="c"><input type="checkbox" id="c-41446651" checked=""/><div class="controls bullet"><span class="by">beAbU</span><span>|</span><a href="#41445956">parent</a><span>|</span><a href="#41446238">next</a><span>|</span><label class="collapse" for="c-41446651">[-]</label><label class="expand" for="c-41446651">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m beginning to wonder if these investors are not just pumping AI because they are personally invested in Nvidia and this is a nice way to directly inject a couple of 100M into their cashflow.</div><br/></div></div><div id="41446238" class="c"><input type="checkbox" id="c-41446238" checked=""/><div class="controls bullet"><span class="by">duxup</span><span>|</span><a href="#41445956">parent</a><span>|</span><a href="#41446651">prev</a><span>|</span><a href="#41446456">next</a><span>|</span><label class="collapse" for="c-41446238">[-]</label><label class="expand" for="c-41446238">[2 more]</label></div><br/><div class="children"><div class="content">Would be nice to be the sales rep assigned to that rando no name company ;)</div><br/><div id="41451877" class="c"><input type="checkbox" id="c-41451877" checked=""/><div class="controls bullet"><span class="by">fakedang</span><span>|</span><a href="#41445956">root</a><span>|</span><a href="#41446238">parent</a><span>|</span><a href="#41446456">next</a><span>|</span><label class="collapse" for="c-41451877">[-]</label><label class="expand" for="c-41451877">[1 more]</label></div><br/><div class="children"><div class="content">Most likely it&#x27;s some junior rep assigned to Sutskever&#x27;s company after Ilya filled up an online &quot;Contact Us for Pricing&quot; form on the Nvidia website. &#x2F;s</div><br/></div></div></div></div><div id="41446456" class="c"><input type="checkbox" id="c-41446456" checked=""/><div class="controls bullet"><span class="by">ai4ever</span><span>|</span><a href="#41445956">parent</a><span>|</span><a href="#41446238">prev</a><span>|</span><a href="#41452389">next</a><span>|</span><label class="collapse" for="c-41446456">[-]</label><label class="expand" for="c-41446456">[1 more]</label></div><br/><div class="children"><div class="content">indeed, more speculative monies chasing returns.<p>such a large round implies hardware for yet another foundational model. perhaps with better steering etc..</div><br/></div></div><div id="41452389" class="c"><input type="checkbox" id="c-41452389" checked=""/><div class="controls bullet"><span class="by">hindsightbias</span><span>|</span><a href="#41445956">parent</a><span>|</span><a href="#41446456">prev</a><span>|</span><a href="#41445936">next</a><span>|</span><label class="collapse" for="c-41452389">[-]</label><label class="expand" for="c-41452389">[1 more]</label></div><br/><div class="children"><div class="content">$1B isn’t competitive, which is why this is a joke.<p>Maybe they can get some 3nm stuff when Meta is done with them.</div><br/></div></div></div></div><div id="41445936" class="c"><input type="checkbox" id="c-41445936" checked=""/><div class="controls bullet"><span class="by">DelTaco</span><span>|</span><a href="#41445956">prev</a><span>|</span><a href="#41447416">next</a><span>|</span><label class="collapse" for="c-41445936">[-]</label><label class="expand" for="c-41445936">[5 more]</label></div><br/><div class="children"><div class="content">This has to be one of the quickest valuations past a billion. I wonder if they can even effectively make use of the funds in a reasonable enough timeline.</div><br/><div id="41446155" class="c"><input type="checkbox" id="c-41446155" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#41445936">parent</a><span>|</span><a href="#41446159">next</a><span>|</span><label class="collapse" for="c-41446155">[-]</label><label class="expand" for="c-41446155">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I wonder if they can even effectively make use of the funds in a reasonable enough timeline.<p>I read that it cost Google ~$190 million to train Gemini, not even including staff salaries. So feels like a billion gives you about 3 &quot;from scratch&quot; comparable training runs.</div><br/><div id="41446583" class="c"><input type="checkbox" id="c-41446583" checked=""/><div class="controls bullet"><span class="by">greenthrow</span><span>|</span><a href="#41445936">root</a><span>|</span><a href="#41446155">parent</a><span>|</span><a href="#41446159">next</a><span>|</span><label class="collapse" for="c-41446583">[-]</label><label class="expand" for="c-41446583">[1 more]</label></div><br/><div class="children"><div class="content">Your estimate seems way off given Google already had their own compute hardware and staff. And if this company is going straight for AGI there&#x27;s no way $1 billion is enough.</div><br/></div></div></div></div><div id="41446159" class="c"><input type="checkbox" id="c-41446159" checked=""/><div class="controls bullet"><span class="by">udev4096</span><span>|</span><a href="#41445936">parent</a><span>|</span><a href="#41446155">prev</a><span>|</span><a href="#41447098">next</a><span>|</span><label class="collapse" for="c-41446159">[-]</label><label class="expand" for="c-41446159">[1 more]</label></div><br/><div class="children"><div class="content">Given the dire need of GPUs, I don&#x27;t suspect they would have any trouble finding the good use of the funds</div><br/></div></div><div id="41447098" class="c"><input type="checkbox" id="c-41447098" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#41445936">parent</a><span>|</span><a href="#41446159">prev</a><span>|</span><a href="#41447416">next</a><span>|</span><label class="collapse" for="c-41447098">[-]</label><label class="expand" for="c-41447098">[1 more]</label></div><br/><div class="children"><div class="content">They’ve probably already ordered like $250mm of GPUs.</div><br/></div></div></div></div><div id="41447416" class="c"><input type="checkbox" id="c-41447416" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#41445936">prev</a><span>|</span><a href="#41446266">next</a><span>|</span><label class="collapse" for="c-41447416">[-]</label><label class="expand" for="c-41447416">[30 more]</label></div><br/><div class="children"><div class="content">Lots of comments either defending this (&quot;it&#x27;s taking a chance on being the first to build AGI with a proven team&quot;) or saying &quot;it&#x27;s a crazy valuation for a 3 month old startup&quot;. But both of these &quot;sides&quot; feel like they miss the mark to me.<p>On one hand, I think it&#x27;s great that investors are willing to throw big chunks of money at hard (or at least expensive) problems. I&#x27;m pretty sure all the investors putting money in will do just fine even if their investment goes to zero, so this feels exactly what VC funding <i>should</i> be doing, rather than some other common &quot;how can we get people more digitally addicted to sell ads?&quot; play.<p>On the other hand, I&#x27;m kind of baffled that we&#x27;re still talking about &quot;AGI&quot; in the context of LLMs. While I find LLMs to be amazing, and an incredibly useful tool (if used with a good understanding of their flaws), the more I use them, the more that it becomes clear to me that they&#x27;re not going to get us anywhere close to &quot;general intelligence&quot;. That is, the more I have to work around hallucinations, the more that it becomes clear that LLMs really <i>are</i> just &quot;fancy autocomplete&quot;, even if it&#x27;s really really fancy autocomplete. I see lots of errors that make sense if you understand an LLM is just a statistical model of word&#x2F;token frequency, but you would expect to <i>never</i> see these kinds of errors in a system that had a true <i>understanding</i> of underlying concepts. And while I&#x27;m not in the field so I may have no right to comment, there are leaders in the field, like LeCun, who have expressed basically the same idea.<p>So my question is, has Sutskever et al provided any acknowledgement of how they intend to &quot;cross the chasm&quot; from where we are now with LLMs to a model of understanding, or has it been mainly &quot;look what we did before, you should take a chance on us to make discontinuous breakthroughs in the future&quot;?</div><br/><div id="41447549" class="c"><input type="checkbox" id="c-41447549" checked=""/><div class="controls bullet"><span class="by">petulla</span><span>|</span><a href="#41447416">parent</a><span>|</span><a href="#41447707">next</a><span>|</span><label class="collapse" for="c-41447549">[-]</label><label class="expand" for="c-41447549">[11 more]</label></div><br/><div class="children"><div class="content">Ilya has discussed this question: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=YEUclZdj_Sc" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=YEUclZdj_Sc</a></div><br/><div id="41448121" class="c"><input type="checkbox" id="c-41448121" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41447549">parent</a><span>|</span><a href="#41449663">next</a><span>|</span><label class="collapse" for="c-41448121">[-]</label><label class="expand" for="c-41448121">[8 more]</label></div><br/><div class="children"><div class="content">Thank you very much for posting! This is exactly what I was looking for.<p>On one hand, I understand what he&#x27;s saying, and that&#x27;s why I have been frustrated in the past when I&#x27;ve heard people say &quot;it&#x27;s just fancy autocomplete&quot; without emphasizing the awesome capabilities that can give you. While I haven&#x27;t seen this video by Sutskever before, I have seen a very similar argument by Hinton: in order to get really good at next token prediction, the model needs to &quot;discover&quot; the underlying rules that make that prediction possible.<p>All that said, I find his argument wholly unconvincing (and again, I may be <i>waaaaay</i> stupider than Sutskever, but there are other people much smarter than I who agree). And the reason for this is because every now and then I&#x27;ll see a particular type of hallucination where it&#x27;s pretty obvious that the LLM is confusing similar token strings even when their underlying meaning is very different. That is, the underlying &quot;pattern matching&quot; of LLMs becomes apparent in these situations.<p>As I said originally, I&#x27;m really glad VCs are pouring money into this, but I&#x27;d easily make a bet that in 5 years that LLMs will be nowhere near human-level intelligence on some tasks, especially where novel discovery is required.</div><br/><div id="41448214" class="c"><input type="checkbox" id="c-41448214" checked=""/><div class="controls bullet"><span class="by">JamesSwift</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41448121">parent</a><span>|</span><a href="#41448314">next</a><span>|</span><label class="collapse" for="c-41448214">[-]</label><label class="expand" for="c-41448214">[1 more]</label></div><br/><div class="children"><div class="content">Watching that video actually makes me completely unconvinced that SSI will succeed if they are hinging it on LLM...<p>He puts a lot of emphasis on the fact that &#x27;to generate the next token you must understand how&#x27;, when thats precisely the parlor trick that is making people lose their minds (myself included) with how effective current LLMs are. The fact that it can simulate some low-fidelity reality with _no higher-level understanding of the world_, using purely linguistic&#x2F;statistical analysis, is mind-blowing. To say &quot;all you have to do is then extrapolate&quot; is the ultimate &quot;draw the rest of the owl&quot; argument.</div><br/></div></div><div id="41448314" class="c"><input type="checkbox" id="c-41448314" checked=""/><div class="controls bullet"><span class="by">otabdeveloper4</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41448121">parent</a><span>|</span><a href="#41448214">prev</a><span>|</span><a href="#41448626">next</a><span>|</span><label class="collapse" for="c-41448314">[-]</label><label class="expand" for="c-41448314">[1 more]</label></div><br/><div class="children"><div class="content">&gt; but I&#x27;d easily make a bet that in 5 years that LLMs will be nowhere near human-level intelligence on some tasks<p>I wouldn&#x27;t. There are some extraordinarily stupid humans out there.
Worse, making humans dumber is a proven and well-known technology.</div><br/></div></div><div id="41448626" class="c"><input type="checkbox" id="c-41448626" checked=""/><div class="controls bullet"><span class="by">pajeets</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41448121">parent</a><span>|</span><a href="#41448314">prev</a><span>|</span><a href="#41449688">next</a><span>|</span><label class="collapse" for="c-41448626">[-]</label><label class="expand" for="c-41448626">[1 more]</label></div><br/><div class="children"><div class="content">I actually echo your exact sentiments. I don&#x27;t have the street cred but watching him talk for the first few minutes I immediately felt like there is just no way we are going to get AGI with what we know today.<p>Without some raw reasoning (maybe Neuro-symbolic is the answer maybe not) capacity, LLM won&#x27;t be enough. Reasoning is super tough because its not as easy as predicting the next most likely token.</div><br/></div></div><div id="41449688" class="c"><input type="checkbox" id="c-41449688" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41448121">parent</a><span>|</span><a href="#41448626">prev</a><span>|</span><a href="#41449103">next</a><span>|</span><label class="collapse" for="c-41449688">[-]</label><label class="expand" for="c-41449688">[3 more]</label></div><br/><div class="children"><div class="content">&gt;All that said, I find his argument wholly unconvincing (and again, I may be waaaaay stupider than Sutskever, but there are other people much smarter than I who agree). And the reason for this is because every now and then I&#x27;ll see a particular type of hallucination where it&#x27;s pretty obvious that the LLM is confusing similar token strings even when their underlying meaning is very different. That is, the underlying &quot;pattern matching&quot; of LLMs becomes apparent in these situations.<p>So? One of the most frustrating parts of these discussions is that for some bizzare reason, a lot of people have a standard of reasoning (for machines) that only exists in fiction or their own imaginations.<p>Humans have a long list of cognitive shortcomings. We find them interesting and give them all sorts of names like cognitive dissonance or optical illusions. But we don&#x27;t currently make silly conclusions like humans don&#x27;t reason.<p>The general reasoning engine that makes neither mistake nor contradiction or confusion in output or process does not exist in real life whether you believe Humans are the only intelligent species on the planet or are gracious enough to extend the capability to some of our animal friends.<p>So the LLM confuses tokens every now and then. So what ?</div><br/><div id="41450307" class="c"><input type="checkbox" id="c-41450307" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41449688">parent</a><span>|</span><a href="#41449103">next</a><span>|</span><label class="collapse" for="c-41450307">[-]</label><label class="expand" for="c-41450307">[2 more]</label></div><br/><div class="children"><div class="content">You are completely mischaracterizing my comment.<p>&gt; Humans have a long list of cognitive shortcomings. We find them interesting and give them all sorts of names like cognitive dissonance or optical illusions. But we don&#x27;t currently make silly conclusions like humans don&#x27;t reason.<p>Exactly! In fact, things like illusions are actually excellent windows into how the mind really works. Most visual illusions are a fundamental artifact of how the brain needs to turn a 2D image into a 3D, real-world model, and illusions give clues into how it does that, and how the contours of the natural world guided the evolution of the visual system (I think Steven Pinker&#x27;s &quot;How the Mind Works&quot; gives excellent examples of this).<p>So I am not at all saying that what LLMs do isn&#x27;t extremely interesting, or useful. What I am saying is that the types of errors you get give a window into how an LLM works, and these hint at some fundamental limitations at what an LLM is capable of, particularly around novel discovery and development of new ideas and theories that aren&#x27;t just &quot;rearrangements&quot; of existing ideas.</div><br/><div id="41450869" class="c"><input type="checkbox" id="c-41450869" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41450307">parent</a><span>|</span><a href="#41449103">next</a><span>|</span><label class="collapse" for="c-41450869">[-]</label><label class="expand" for="c-41450869">[1 more]</label></div><br/><div class="children"><div class="content">&gt;So I am not at all saying that what LLMs do isn&#x27;t extremely interesting, or useful. What I am saying is that the types of errors you get give a window into how an LLM works, and these hint at some fundamental limitations at what an LLM is capable of, particularly around novel discovery and development of new ideas and theories that aren&#x27;t just &quot;rearrangements&quot; of existing ideas.<p>ANN architectures are not like brains. They don&#x27;t come pre-baked with all sorts of evolutionary steps and tweaking. They&#x27;re far more blank slate and the transformer is one of the most blank slate there is.<p>Mostly at best, maybe some failure mode in GPT-N gives insight to how some concept is understood by GPT-N. It rarely will say anything about language modelling or Transformers. 
GPT-2 had some wildly different failure modes than 3, which itself has some wildly different failure modes to 4.<p>All a transformer&#x27;s training objective asks it to do is spit out a token. <i>How</i> it should do so is left for transformer to figure along the way and everything is fair game.<p>And confusing words with wildly different meanings but with some similarity in some other way is something that happens to humans as well. Transformers don&#x27;t see words or letters(but tokens). So just because it doesn&#x27;t seem to you like two tokens should be confused doesn&#x27;t mean there isn&#x27;t a valid point of confusion there.</div><br/></div></div></div></div></div></div><div id="41449103" class="c"><input type="checkbox" id="c-41449103" checked=""/><div class="controls bullet"><span class="by">machiaweliczny</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41448121">parent</a><span>|</span><a href="#41449688">prev</a><span>|</span><a href="#41449663">next</a><span>|</span><label class="collapse" for="c-41449103">[-]</label><label class="expand" for="c-41449103">[1 more]</label></div><br/><div class="children"><div class="content">They might never work for novel discovery but that probably can be handled by outside loop or online (in-context) learning. The thing is that 100k or 1M context is a marketing scam for now.</div><br/></div></div></div></div><div id="41449663" class="c"><input type="checkbox" id="c-41449663" checked=""/><div class="controls bullet"><span class="by">Satam</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41447549">parent</a><span>|</span><a href="#41448121">prev</a><span>|</span><a href="#41448301">next</a><span>|</span><label class="collapse" for="c-41449663">[-]</label><label class="expand" for="c-41449663">[1 more]</label></div><br/><div class="children"><div class="content">To clarify this, I think it&#x27;s reasonable that token prediction as a training objective could lead to AGI given the underlying model has the correct architecture. The question really is if the underlying architecture is good enough to capitalize on the training objective so as to result in superhuman intelligence.<p>For example, you&#x27;ll have little luck achieving AGI with decision trees no matter what&#x27;s their training objective.</div><br/></div></div><div id="41448301" class="c"><input type="checkbox" id="c-41448301" checked=""/><div class="controls bullet"><span class="by">jmugan</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41447549">parent</a><span>|</span><a href="#41449663">prev</a><span>|</span><a href="#41447707">next</a><span>|</span><label class="collapse" for="c-41448301">[-]</label><label class="expand" for="c-41448301">[1 more]</label></div><br/><div class="children"><div class="content">He doesn&#x27;t address the real question of how an LLM predicting the next token could exceed what humans have done. They mostly interpolate, so if the answer isn&#x27;t to be found in an interpolation, the LLM can&#x27;t generate something new.</div><br/></div></div></div></div><div id="41447707" class="c"><input type="checkbox" id="c-41447707" checked=""/><div class="controls bullet"><span class="by">nilkn</span><span>|</span><a href="#41447416">parent</a><span>|</span><a href="#41447549">prev</a><span>|</span><a href="#41453193">next</a><span>|</span><label class="collapse" for="c-41447707">[-]</label><label class="expand" for="c-41447707">[10 more]</label></div><br/><div class="children"><div class="content">The argument about AGI from LLMs is not based on the current state of LLMs, but on the rate of progress over the last 5+ years or so. It wasn&#x27;t very long ago that almost nobody outside of a few niche circles seriously thought LLMs could do what they do right now.<p>That said, my personal hypothesis is that AGI will emerge from video generation models rather than text generation models. A model that takes an arbitrary real-time video input feed and must predict the next, say, 60 seconds of video would have to have a deep understanding of the universe, humanity, language, culture, physics, humor, laughter, problem solving, etc. This pushes the fidelity of both input and output far beyond anything that can be expressed in text, but also creates extraordinarily high computational barriers.</div><br/><div id="41447947" class="c"><input type="checkbox" id="c-41447947" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41447707">parent</a><span>|</span><a href="#41448138">next</a><span>|</span><label class="collapse" for="c-41447947">[-]</label><label class="expand" for="c-41447947">[7 more]</label></div><br/><div class="children"><div class="content">&gt; The argument about AGI from LLMs is not based on the current state of LLMs, but on the rate of progress over the last 5+ years or so.<p>And what I&#x27;m saying is that I find that argument to be incredibly weak. I&#x27;ve seen it time and time again, and honestly at this point just feels like a &quot;humans should be a hundred feet tall based on on their rate of change in their early years&quot; argument.<p>While I&#x27;ve also been amazed at the past progress in LLMs, I don&#x27;t see any reason to expect that rate will continue in the future. What I do see the more and more I use the SOTA models is fundamental limitations in what LLMs are capable of.</div><br/><div id="41448205" class="c"><input type="checkbox" id="c-41448205" checked=""/><div class="controls bullet"><span class="by">nilkn</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41447947">parent</a><span>|</span><a href="#41449030">next</a><span>|</span><label class="collapse" for="c-41448205">[-]</label><label class="expand" for="c-41448205">[4 more]</label></div><br/><div class="children"><div class="content">Expecting the rate of progress to drop off so abruptly after realistically just a few years of serious work on the problem seems like the more unreasonable and grander prediction to me than expecting it to continue at its current pace for even just 5 more years.</div><br/><div id="41449098" class="c"><input type="checkbox" id="c-41449098" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41448205">parent</a><span>|</span><a href="#41449030">next</a><span>|</span><label class="collapse" for="c-41449098">[-]</label><label class="expand" for="c-41449098">[3 more]</label></div><br/><div class="children"><div class="content">The problem is that the rate of progress over the past 5&#x2F;10&#x2F;15 years has not been linear at all, and it&#x27;s been pretty easy to point out specific inflection points that have allowed that progress to occur.<p>I.e. the real breakthrough that allowed such rapid progress was transformers in 2017. Since that time, the <i>vast</i> majority of the progress has simply been to throw more data at the problem, and to make the models bigger (and to emphasize, transformers really made that scale possible in the first place). I don&#x27;t mean to denigrate this approach - if anything, OpenAI deserves tons of praise for really making that bet that spending hundreds of millions on model training would give discontinuous results.<p>However, there are <i>loads</i> of reasons to believe that &quot;more scale&quot; is going to give diminishing returns, and a lot of very smart people in the field have been making this argument (at least quietly). Even more specifically, there are good reasons to believe that more scale is <i>not</i> going to go anywhere close to solving the types of problems that have become evident in LLMs since when they have had massive scale.<p>So the big thing I&#x27;m questioning is that I see a sizable subset of both AI researchers (and more importantly VC types) believing that, essentially, more scale will lead to AGI. I think the smart money believes that there is something fundamentally different about how humans approach intelligence (and this difference leads to important capabilities that aren&#x27;t possible from LLMs).</div><br/><div id="41449354" class="c"><input type="checkbox" id="c-41449354" checked=""/><div class="controls bullet"><span class="by">calf</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41449098">parent</a><span>|</span><a href="#41449599">next</a><span>|</span><label class="collapse" for="c-41449354">[-]</label><label class="expand" for="c-41449354">[1 more]</label></div><br/><div class="children"><div class="content">Could it be argued that transformers are only possible because of Moore&#x27;s law and the amount of processing power that could do these computations in a reasonable time? How complex is the transformer network really, every lay explanation I&#x27;ve seen basically says it is about a kind of parallelized access to the input string. Which sounds like a hardware problem, because the algorithmic advances still need to run on reasonable hardware.</div><br/></div></div><div id="41449599" class="c"><input type="checkbox" id="c-41449599" checked=""/><div class="controls bullet"><span class="by">svnt</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41449098">parent</a><span>|</span><a href="#41449354">prev</a><span>|</span><a href="#41449030">next</a><span>|</span><label class="collapse" for="c-41449599">[-]</label><label class="expand" for="c-41449599">[1 more]</label></div><br/><div class="children"><div class="content">Transformers in 2017 as the basis, but then the quantization-emergence link as a grad student project using spare time on ridiculously large A100 clusters in 2021&#x2F;2022 is what finally brought about this present moment.<p>I feel it is fair to say that neither of these were natural extrapolations from prior successful models directly. There is no indication we are anywhere near another nonlinearity, if we even knew how to look for that.<p>Blind faith in extrapolation is a finance regime, not an engineering regime. Engineers encounter nonlinearities regularly. Financiers are used to compound interest.</div><br/></div></div></div></div></div></div><div id="41449030" class="c"><input type="checkbox" id="c-41449030" checked=""/><div class="controls bullet"><span class="by">machiaweliczny</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41447947">parent</a><span>|</span><a href="#41448205">prev</a><span>|</span><a href="#41448155">next</a><span>|</span><label class="collapse" for="c-41449030">[-]</label><label class="expand" for="c-41449030">[1 more]</label></div><br/><div class="children"><div class="content">Happy to review this in 5 years</div><br/></div></div><div id="41448155" class="c"><input type="checkbox" id="c-41448155" checked=""/><div class="controls bullet"><span class="by">ldjkfkdsjnv</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41447947">parent</a><span>|</span><a href="#41449030">prev</a><span>|</span><a href="#41448138">next</a><span>|</span><label class="collapse" for="c-41448155">[-]</label><label class="expand" for="c-41448155">[1 more]</label></div><br/><div class="children"><div class="content">10 years of progress is a flash in the pan of human progress. The first deep learning models that worked appeared in 2012. That was like yesterday. You are completely underestimating the rate of change we are witnessing. Compute scaling is not at all similar to biological scaling.</div><br/></div></div></div></div><div id="41448138" class="c"><input type="checkbox" id="c-41448138" checked=""/><div class="controls bullet"><span class="by">ldjkfkdsjnv</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41447707">parent</a><span>|</span><a href="#41447947">prev</a><span>|</span><a href="#41453193">next</a><span>|</span><label class="collapse" for="c-41448138">[-]</label><label class="expand" for="c-41448138">[2 more]</label></div><br/><div class="children"><div class="content">If its true that predicting the next word can be turned into predict the next pixel. And that you could run a zillion hours of video feed into that, I agree. It seems that the basic algorithm is there. Video is much less information dense than text, but if the scale of compute can reach the 10s of billions of dollars, or more, you have to expect that AGI is achievable. I think we will see it in our lifetimes. Its probably 5 years away</div><br/><div id="41448198" class="c"><input type="checkbox" id="c-41448198" checked=""/><div class="controls bullet"><span class="by">nilkn</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41448138">parent</a><span>|</span><a href="#41453193">next</a><span>|</span><label class="collapse" for="c-41448198">[-]</label><label class="expand" for="c-41448198">[1 more]</label></div><br/><div class="children"><div class="content">I feel like that&#x27;s already been demonstrated with the first-generation video generation models we&#x27;re seeing. Early research already shows video generation models can become world simulators. There frankly just isn&#x27;t enough compute yet to train models large enough to do this for all general phenomena and then make it available to general users. It&#x27;s also unclear if we have enough training data.<p>Video is not necessarily less information dense than text, because when considered in its entirety it contains text and language generation as special cases. Video generation includes predicting continuations of complex verbal human conversations as well as continuations of videos of text exchanges, someone flipping through notes or a book, someone taking a university exam through their perspective, etc.</div><br/></div></div></div></div></div></div><div id="41453193" class="c"><input type="checkbox" id="c-41453193" checked=""/><div class="controls bullet"><span class="by">sirspacey</span><span>|</span><a href="#41447416">parent</a><span>|</span><a href="#41447707">prev</a><span>|</span><a href="#41447745">next</a><span>|</span><label class="collapse" for="c-41453193">[-]</label><label class="expand" for="c-41453193">[1 more]</label></div><br/><div class="children"><div class="content">Two interesting points to consider<p>1. If it’s really amazing autocomplete, is there a distinction between AGI?<p>Being able to generalize, plan, execute, evaluate and learn from the results could all be seen as a search graph building on inference from known or imagined data points. So far LLMs are being used on all of those and we haven’t even tested the next level of compute power being built to enable its evolution.<p>2. Fancy autocomplete is a bit broad for the comprehensive use cases CUDA is already supporting that go way beyond textual prediction.<p>If all information of every type can be “autocompleted” that’s a pretty incredible leap for robotics.<p>* edited to compensate for iPhone autocomplete, the irony.</div><br/></div></div><div id="41447745" class="c"><input type="checkbox" id="c-41447745" checked=""/><div class="controls bullet"><span class="by">wubrr</span><span>|</span><a href="#41447416">parent</a><span>|</span><a href="#41453193">prev</a><span>|</span><a href="#41449792">next</a><span>|</span><label class="collapse" for="c-41447745">[-]</label><label class="expand" for="c-41447745">[3 more]</label></div><br/><div class="children"><div class="content">&gt; the more that it becomes clear that LLMs really are just &quot;fancy autocomplete&quot;, even if it&#x27;s really really fancy autocomplete<p>I also don&#x27;t really see AGI emerging from LLMs any time soon, but it could be argued that human intelligence is also just &#x27;fancy autocomplete&#x27;.</div><br/><div id="41447884" class="c"><input type="checkbox" id="c-41447884" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41447745">parent</a><span>|</span><a href="#41449792">next</a><span>|</span><label class="collapse" for="c-41447884">[-]</label><label class="expand" for="c-41447884">[2 more]</label></div><br/><div class="children"><div class="content">&gt; but it could be argued that human intelligence is also just &#x27;fancy autocomplete&#x27;.<p>But that&#x27;s my point - in some ways it&#x27;s obvious that humans <i>are not</i> just doing &quot;fancy autocomplete&quot; because humans generally don&#x27;t make the types of hallucination errors that LLMs make. That is, the hallucination errors <i>do</i> make sense if you think of how an LLM is just a statistical relationship between tokens.<p>One thing to emphasize, I&#x27;m not saying the &quot;understanding&quot; that humans seem to possess isn&#x27;t just some lower level statistical process - I&#x27;m not &quot;invoking a soul&quot;. But I am saying it appears to be fundamentally different, and in many cases more useful, than what an LLM can do.</div><br/><div id="41449099" class="c"><input type="checkbox" id="c-41449099" checked=""/><div class="controls bullet"><span class="by">wubrr</span><span>|</span><a href="#41447416">root</a><span>|</span><a href="#41447884">parent</a><span>|</span><a href="#41449792">next</a><span>|</span><label class="collapse" for="c-41449099">[-]</label><label class="expand" for="c-41449099">[1 more]</label></div><br/><div class="children"><div class="content">&gt; because humans generally don&#x27;t make the types of hallucination errors that LLMs make.<p>They do though - I&#x27;ve noticed myself and others saying things in conversation that sound kind of right, and are based on correct things they&#x27;ve learned previously, but because memory of those things is only partial and mixed with other related information things are often said that are quite incorrect or combine two topics in a way that doesn&#x27;t make sense.</div><br/></div></div></div></div></div></div><div id="41449792" class="c"><input type="checkbox" id="c-41449792" checked=""/><div class="controls bullet"><span class="by">tim333</span><span>|</span><a href="#41447416">parent</a><span>|</span><a href="#41447745">prev</a><span>|</span><a href="#41447634">next</a><span>|</span><label class="collapse" for="c-41449792">[-]</label><label class="expand" for="c-41449792">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&quot;We’ve identified a new mountain to climb that’s a bit different from what I was working on previously. We’re not trying to go down the same path faster. If you do something different, then it becomes possible for you to do something special.&quot;<p>Doesn&#x27;t really imply let&#x27;s just do more LLMs.</div><br/></div></div><div id="41447634" class="c"><input type="checkbox" id="c-41447634" checked=""/><div class="controls bullet"><span class="by">maximinus_thrax</span><span>|</span><a href="#41447416">parent</a><span>|</span><a href="#41449792">prev</a><span>|</span><a href="#41447841">next</a><span>|</span><label class="collapse" for="c-41447634">[-]</label><label class="expand" for="c-41447634">[1 more]</label></div><br/><div class="children"><div class="content">&gt; On the other hand, I&#x27;m kind of baffled that we&#x27;re still talking about &quot;AGI&quot; in the context of LLMs.<p>I&#x27;m not. Lots of people and companies have been sinking money into these ventures and they need to keep the hype alive by framing this as being some sort of race to AGI. I am aware that the older I get the more cynical I become, but I bucket all discussions about AGI (including the very popular &#x27;open letters&#x27; about AI safety and Skynet) in the context of LLMs into the &#x27;snake oil&#x27; bucket.</div><br/></div></div><div id="41447445" class="c"><input type="checkbox" id="c-41447445" checked=""/><div class="controls bullet"><span class="by">thefounder</span><span>|</span><a href="#41447416">parent</a><span>|</span><a href="#41447841">prev</a><span>|</span><a href="#41446266">next</a><span>|</span><label class="collapse" for="c-41447445">[-]</label><label class="expand" for="c-41447445">[1 more]</label></div><br/><div class="children"><div class="content">I think the plan is to raise a lot of cash and then more and then maybe something comes up that brings us closer to AGI(i.e something better than LLM).
The investors know that AGI is not really the goal but they can’t miss the next trillion dollar company.</div><br/></div></div></div></div><div id="41446266" class="c"><input type="checkbox" id="c-41446266" checked=""/><div class="controls bullet"><span class="by">danielovichdk</span><span>|</span><a href="#41447416">prev</a><span>|</span><a href="#41448327">next</a><span>|</span><label class="collapse" for="c-41446266">[-]</label><label class="expand" for="c-41446266">[25 more]</label></div><br/><div class="children"><div class="content">&quot;It will focus on building a small highly trusted team of researchers and engineers split between Palo Alto, California and Tel Aviv, Israel.&quot;<p>Why Tel Aviv in Israel ?</div><br/><div id="41446354" class="c"><input type="checkbox" id="c-41446354" checked=""/><div class="controls bullet"><span class="by">DalasNoin</span><span>|</span><a href="#41446266">parent</a><span>|</span><a href="#41446278">next</a><span>|</span><label class="collapse" for="c-41446354">[-]</label><label class="expand" for="c-41446354">[14 more]</label></div><br/><div class="children"><div class="content">Ilya went to university in israel and all founders are jewish. Many labs have offices outside of the US, like london, due to crazy immigration law in the us.</div><br/><div id="41446579" class="c"><input type="checkbox" id="c-41446579" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41446354">parent</a><span>|</span><a href="#41446468">next</a><span>|</span><label class="collapse" for="c-41446579">[-]</label><label class="expand" for="c-41446579">[6 more]</label></div><br/><div class="children"><div class="content">There are actually a ton of reasons to like London.  The engineering talent is close to bay level for fintech&#x2F;security systems engineers while being 60% of the price, it has 186% deductions with cash back instead of carry forward for R&amp;D spending, it has the best AI researchers in the world and profit from patents is only taxed at 10% in the UK.</div><br/><div id="41447084" class="c"><input type="checkbox" id="c-41447084" checked=""/><div class="controls bullet"><span class="by">christianqchung</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41446579">parent</a><span>|</span><a href="#41446468">next</a><span>|</span><label class="collapse" for="c-41447084">[-]</label><label class="expand" for="c-41447084">[5 more]</label></div><br/><div class="children"><div class="content">If London has the best AI researchers in the world, why are all the top companies (minus Mistral) American?</div><br/><div id="41447342" class="c"><input type="checkbox" id="c-41447342" checked=""/><div class="controls bullet"><span class="by">HaukeHi</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41447084">parent</a><span>|</span><a href="#41447363">next</a><span>|</span><label class="collapse" for="c-41447342">[-]</label><label class="expand" for="c-41447342">[2 more]</label></div><br/><div class="children"><div class="content">Demis Hassabis says that half of all innovations that caused the recent AI boom came from DeepMind, which is London based.</div><br/><div id="41448652" class="c"><input type="checkbox" id="c-41448652" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41447342">parent</a><span>|</span><a href="#41447363">next</a><span>|</span><label class="collapse" for="c-41448652">[-]</label><label class="expand" for="c-41448652">[1 more]</label></div><br/><div class="children"><div class="content">his opinion is obviously biased.<p>If we say that half of innovations came from Alphabet&#x2F;Google, then most of them (transformers, LLMs, tensorflow) came from Google Research and not Deep Mind.</div><br/></div></div></div></div><div id="41447363" class="c"><input type="checkbox" id="c-41447363" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41447084">parent</a><span>|</span><a href="#41447342">prev</a><span>|</span><a href="#41447250">next</a><span>|</span><label class="collapse" for="c-41447363">[-]</label><label class="expand" for="c-41447363">[1 more]</label></div><br/><div class="children"><div class="content">People are choosing headquarters for access to capital rather than talent.  That should tell you a lot about the current dynamics of the AI boom.</div><br/></div></div><div id="41447250" class="c"><input type="checkbox" id="c-41447250" checked=""/><div class="controls bullet"><span class="by">seanf</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41447084">parent</a><span>|</span><a href="#41447363">prev</a><span>|</span><a href="#41446468">next</a><span>|</span><label class="collapse" for="c-41447250">[-]</label><label class="expand" for="c-41447250">[1 more]</label></div><br/><div class="children"><div class="content">Google Deepmind is based in London.</div><br/></div></div></div></div></div></div><div id="41446468" class="c"><input type="checkbox" id="c-41446468" checked=""/><div class="controls bullet"><span class="by">infecto</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41446354">parent</a><span>|</span><a href="#41446579">prev</a><span>|</span><a href="#41446633">next</a><span>|</span><label class="collapse" for="c-41446468">[-]</label><label class="expand" for="c-41446468">[2 more]</label></div><br/><div class="children"><div class="content">Many companies have offices outside because of talent pools, costs, and other regional advantages. Though I am sure some of it is due to immigration law, I don&#x27;t believe that is generally the main factor. Plus the same could be said for most other countries.</div><br/><div id="41446533" class="c"><input type="checkbox" id="c-41446533" checked=""/><div class="controls bullet"><span class="by">AlanYx</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41446468">parent</a><span>|</span><a href="#41446633">next</a><span>|</span><label class="collapse" for="c-41446533">[-]</label><label class="expand" for="c-41446533">[1 more]</label></div><br/><div class="children"><div class="content">Part of it may also be a way to mitigate potential regulatory risk. Israel thus far does not have an equivalent to something like SB1047 (the closest they&#x27;ve come is participation in the Council of Europe AI treaty negotiations), and SSI will be well-positioned to lobby against intrusive regulation domestically in Israel.</div><br/></div></div></div></div><div id="41446633" class="c"><input type="checkbox" id="c-41446633" checked=""/><div class="controls bullet"><span class="by">tinyhouse</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41446354">parent</a><span>|</span><a href="#41446468">prev</a><span>|</span><a href="#41446383">next</a><span>|</span><label class="collapse" for="c-41446633">[-]</label><label class="expand" for="c-41446633">[1 more]</label></div><br/><div class="children"><div class="content">Ilya also lived in Israel as a kid from age 5 to 15 so he speaks Hebrew. His family emigrated from Russia. Later they moved to Canada.<p>Source: Wikipedia.</div><br/></div></div><div id="41446383" class="c"><input type="checkbox" id="c-41446383" checked=""/><div class="controls bullet"><span class="by">danielovichdk</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41446354">parent</a><span>|</span><a href="#41446633">prev</a><span>|</span><a href="#41450037">next</a><span>|</span><label class="collapse" for="c-41446383">[-]</label><label class="expand" for="c-41446383">[1 more]</label></div><br/><div class="children"><div class="content">I wasn&#x27;t aware of his or any of the other founders background. Simply thought it was political somehow.<p>Thanks.</div><br/></div></div><div id="41450037" class="c"><input type="checkbox" id="c-41450037" checked=""/><div class="controls bullet"><span class="by">lupire</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41446354">parent</a><span>|</span><a href="#41446383">prev</a><span>|</span><a href="#41446278">next</a><span>|</span><label class="collapse" for="c-41450037">[-]</label><label class="expand" for="c-41450037">[3 more]</label></div><br/><div class="children"><div class="content">Two of the founders are Israeli and the other is French, I think (went to University in France).<p>Israel is a leading AI and software development hub in the world.</div><br/><div id="41452259" class="c"><input type="checkbox" id="c-41452259" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41450037">parent</a><span>|</span><a href="#41446278">next</a><span>|</span><label class="collapse" for="c-41452259">[-]</label><label class="expand" for="c-41452259">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Israel is a leading AI and software development hub...</i><p>Yep, and if any place will produce the safest AI ever, its got to be there.</div><br/><div id="41453607" class="c"><input type="checkbox" id="c-41453607" checked=""/><div class="controls bullet"><span class="by">mattfrommars</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41452259">parent</a><span>|</span><a href="#41446278">next</a><span>|</span><label class="collapse" for="c-41453607">[-]</label><label class="expand" for="c-41453607">[1 more]</label></div><br/><div class="children"><div class="content">Safest place for AI? Their miltary has the worse track and a complete fascist state. Israel is the worst place to fund &quot;safe and humane AI&quot;<p>Israeli military operations continues to this day with over 41,000 civilians killed.</div><br/></div></div></div></div></div></div></div></div><div id="41446278" class="c"><input type="checkbox" id="c-41446278" checked=""/><div class="controls bullet"><span class="by">nlh</span><span>|</span><a href="#41446266">parent</a><span>|</span><a href="#41446354">prev</a><span>|</span><a href="#41446639">next</a><span>|</span><label class="collapse" for="c-41446278">[-]</label><label class="expand" for="c-41446278">[5 more]</label></div><br/><div class="children"><div class="content">Because it&#x27;s a startup hub, there is great engineering talent there, and the cost of living is lower than the US.</div><br/><div id="41446497" class="c"><input type="checkbox" id="c-41446497" checked=""/><div class="controls bullet"><span class="by">amscanne</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41446278">parent</a><span>|</span><a href="#41446639">next</a><span>|</span><label class="collapse" for="c-41446497">[-]</label><label class="expand" for="c-41446497">[4 more]</label></div><br/><div class="children"><div class="content">Cost of living is extremely high in Tel Aviv, but the rest is true.</div><br/><div id="41446675" class="c"><input type="checkbox" id="c-41446675" checked=""/><div class="controls bullet"><span class="by">bdcravens</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41446497">parent</a><span>|</span><a href="#41446614">next</a><span>|</span><label class="collapse" for="c-41446675">[-]</label><label class="expand" for="c-41446675">[1 more]</label></div><br/><div class="children"><div class="content">For the region, yes. Compared to the US, it&#x27;s closer to Houston and Chicago, and way less that the typical tech hubs like the Bay or NYC.</div><br/></div></div><div id="41446640" class="c"><input type="checkbox" id="c-41446640" checked=""/><div class="controls bullet"><span class="by">petesergeant</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41446497">parent</a><span>|</span><a href="#41446614">prev</a><span>|</span><a href="#41446639">next</a><span>|</span><label class="collapse" for="c-41446640">[-]</label><label class="expand" for="c-41446640">[1 more]</label></div><br/><div class="children"><div class="content">Israel is geographically pretty small though -- I&#x27;m guessing you could live an hour up or down the coast and have it be an outrageous commute for people accustomed to the Bay Area?</div><br/></div></div></div></div></div></div><div id="41446639" class="c"><input type="checkbox" id="c-41446639" checked=""/><div class="controls bullet"><span class="by">bdcravens</span><span>|</span><a href="#41446266">parent</a><span>|</span><a href="#41446278">prev</a><span>|</span><a href="#41447069">next</a><span>|</span><label class="collapse" for="c-41446639">[-]</label><label class="expand" for="c-41446639">[2 more]</label></div><br/><div class="children"><div class="content">Why not? The Bay isn&#x27;t the only place with talent. Many of the big tech powerhouse companies already have offices there. There&#x27;s also many Israeli nationals working the US that may find moving back closer to family a massive advantage.</div><br/><div id="41452638" class="c"><input type="checkbox" id="c-41452638" checked=""/><div class="controls bullet"><span class="by">damanoman</span><span>|</span><a href="#41446266">root</a><span>|</span><a href="#41446639">parent</a><span>|</span><a href="#41447069">next</a><span>|</span><label class="collapse" for="c-41452638">[-]</label><label class="expand" for="c-41452638">[1 more]</label></div><br/><div class="children"><div class="content">Is it as open to outsiders as the Bay is? I’m Asian for example and it seems the society there is far more homogenous than in the Bay. I have no idea so I’m curious.</div><br/></div></div></div></div><div id="41447069" class="c"><input type="checkbox" id="c-41447069" checked=""/><div class="controls bullet"><span class="by">nunez</span><span>|</span><a href="#41446266">parent</a><span>|</span><a href="#41446639">prev</a><span>|</span><a href="#41446946">next</a><span>|</span><label class="collapse" for="c-41447069">[-]</label><label class="expand" for="c-41447069">[1 more]</label></div><br/><div class="children"><div class="content">Israel has insane engineering and science talent.</div><br/></div></div><div id="41446946" class="c"><input type="checkbox" id="c-41446946" checked=""/><div class="controls bullet"><span class="by">myth_drannon</span><span>|</span><a href="#41446266">parent</a><span>|</span><a href="#41447069">prev</a><span>|</span><a href="#41451319">next</a><span>|</span><label class="collapse" for="c-41446946">[-]</label><label class="expand" for="c-41446946">[1 more]</label></div><br/><div class="children"><div class="content">Israel is the largest AI startup hub.</div><br/></div></div><div id="41451319" class="c"><input type="checkbox" id="c-41451319" checked=""/><div class="controls bullet"><span class="by">bbqfog</span><span>|</span><a href="#41446266">parent</a><span>|</span><a href="#41446946">prev</a><span>|</span><a href="#41448327">next</a><span>|</span><label class="collapse" for="c-41451319">[-]</label><label class="expand" for="c-41451319">[1 more]</label></div><br/><div class="children"><div class="content">Absolute deal breaker for me, and many others. I hope they fail.</div><br/></div></div></div></div><div id="41448327" class="c"><input type="checkbox" id="c-41448327" checked=""/><div class="controls bullet"><span class="by">hintymad</span><span>|</span><a href="#41446266">prev</a><span>|</span><a href="#41446260">next</a><span>|</span><label class="collapse" for="c-41448327">[-]</label><label class="expand" for="c-41448327">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Sutskever said his new venture made sense because he &quot;identified a mountain that&#x27;s a bit different from what I was working on.&quot;<p>I guess the &quot;mountain&quot; is the key. &quot;Safe&quot; alone is far from being a product. As for the current LLM, Id even question how valuable &quot;safe&quot; can be.</div><br/><div id="41448551" class="c"><input type="checkbox" id="c-41448551" checked=""/><div class="controls bullet"><span class="by">pajeets</span><span>|</span><a href="#41448327">parent</a><span>|</span><a href="#41446260">next</a><span>|</span><label class="collapse" for="c-41448551">[-]</label><label class="expand" for="c-41448551">[1 more]</label></div><br/><div class="children"><div class="content">to be honestly from the way &quot;safe&quot; and &quot;alignment&quot; is perceived on r&#x2F;LocalLLaMA in two years its not going to be very appealing.<p>We&#x27;ll be able to generate most of Chat GPT4o&#x27;s capabilities locally on affordable hardware including &quot;unsafe&quot; and &quot;unaligned&quot; data as the noise-to-qubits is drastically reduced meaning smaller quantized models that can run on good enough hardware.<p>We&#x27;ll see a huge reduction in price and inference times within two years and whatever SSI is trained on won&#x27;t be economically viable to recoup that $1B investment guaranteed.<p>all depends on GPT-5&#x27;s performance. Right now Sonnet 3.5 is the best but theres nothing really ground breaking. SSI&#x27;s success will depend on how much uplift it can provide over GPT-5 which already isn&#x27;t expected to be significant leap beyond GPT4</div><br/></div></div></div></div><div id="41446260" class="c"><input type="checkbox" id="c-41446260" checked=""/><div class="controls bullet"><span class="by">highfrequency</span><span>|</span><a href="#41448327">prev</a><span>|</span><a href="#41446486">next</a><span>|</span><label class="collapse" for="c-41446260">[-]</label><label class="expand" for="c-41446260">[22 more]</label></div><br/><div class="children"><div class="content">“…a straight shot to safe superintelligence and in particular to spend a couple of years doing R&amp;D on our product before bringing it to market,&quot; Gross said in an interview.”<p><i>A couple years</i>??</div><br/><div id="41448498" class="c"><input type="checkbox" id="c-41448498" checked=""/><div class="controls bullet"><span class="by">jopsen</span><span>|</span><a href="#41446260">parent</a><span>|</span><a href="#41446575">next</a><span>|</span><label class="collapse" for="c-41448498">[-]</label><label class="expand" for="c-41448498">[1 more]</label></div><br/><div class="children"><div class="content">If you raise 1B in VC, it&#x27;d be shame to burn it all at once :D</div><br/></div></div><div id="41446575" class="c"><input type="checkbox" id="c-41446575" checked=""/><div class="controls bullet"><span class="by">sim7c00</span><span>|</span><a href="#41446260">parent</a><span>|</span><a href="#41448498">prev</a><span>|</span><a href="#41448360">next</a><span>|</span><label class="collapse" for="c-41446575">[-]</label><label class="expand" for="c-41446575">[11 more]</label></div><br/><div class="children"><div class="content">well since it&#x27;s no longer ok to just suck up anyone&#x27;s data and train your AI, it will be a new challenge for them to avoid that pitfall. I can imagine it will take some time...</div><br/><div id="41446713" class="c"><input type="checkbox" id="c-41446713" checked=""/><div class="controls bullet"><span class="by">mholm</span><span>|</span><a href="#41446260">root</a><span>|</span><a href="#41446575">parent</a><span>|</span><a href="#41446683">next</a><span>|</span><label class="collapse" for="c-41446713">[-]</label><label class="expand" for="c-41446713">[1 more]</label></div><br/><div class="children"><div class="content">I believe the commenter is concerned about how _short_ this timeline is. Superintelligence in a couple years? Like, the thing that can put nearly any person at a desk out of a job? My instinct with unicorns like this is to say &#x27;actually it&#x27;ll be five years and it won&#x27;t even work&#x27;, but Ilya has a track record worth believing in.</div><br/></div></div><div id="41446683" class="c"><input type="checkbox" id="c-41446683" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41446260">root</a><span>|</span><a href="#41446575">parent</a><span>|</span><a href="#41446713">prev</a><span>|</span><a href="#41448360">next</a><span>|</span><label class="collapse" for="c-41446683">[-]</label><label class="expand" for="c-41446683">[9 more]</label></div><br/><div class="children"><div class="content">what laws have actually changed that make it no longer okay?<p>we all know that openai did it</div><br/><div id="41448169" class="c"><input type="checkbox" id="c-41448169" checked=""/><div class="controls bullet"><span class="by">bschmidt1</span><span>|</span><a href="#41446260">root</a><span>|</span><a href="#41446683">parent</a><span>|</span><a href="#41448281">next</a><span>|</span><label class="collapse" for="c-41448169">[-]</label><label class="expand" for="c-41448169">[4 more]</label></div><br/><div class="children"><div class="content">There are class actions now like <a href="https:&#x2F;&#x2F;www.nytimes.com&#x2F;2024&#x2F;06&#x2F;13&#x2F;business&#x2F;clearview-ai-facial-recognition-settlement.html" rel="nofollow">https:&#x2F;&#x2F;www.nytimes.com&#x2F;2024&#x2F;06&#x2F;13&#x2F;business&#x2F;clearview-ai-fac...</a><p>Nobody even knew what OpenAI was up to when they were gathering training data - they got away with a lot. Now there is precedent and people are paying more attention. Data that was previously free&#x2F;open now has a clause that it can&#x27;t be used for AI training. OpenAI didn&#x27;t have to deal with any of that.<p>Also OpenAI used cheap labor in Africa to tag training data which was also controversial. If someone did it now it would they&#x27;d be the ones to pay. OpenAI can always say &quot;we stopped&quot; like Nike said with sweat shops.<p>A lot has changed.</div><br/><div id="41448267" class="c"><input type="checkbox" id="c-41448267" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#41446260">root</a><span>|</span><a href="#41448169">parent</a><span>|</span><a href="#41448281">next</a><span>|</span><label class="collapse" for="c-41448267">[-]</label><label class="expand" for="c-41448267">[3 more]</label></div><br/><div class="children"><div class="content">There are <i>at least</i> 3 companies with staff in developed countries well above minimum wage doing tagging and creation of training data, and at least one of them that I have an NDA with pays at least some of their staff tech contractor rates for data in some niches and even then some of data gets processed by 5+ people before it&#x27;s returned to the client. Since I have ended up talking to 3, and I&#x27;m hardly well connected in that space, I can only presume there are many more.<p>Companies are willing to pay <i>a lot</i> for clean training data, and my bet is there will be a growing pile of training sets for sale on a non-exclusive basis as well.<p>A lot of this data - what I&#x27;ve seen anyway, is far cleaner than anything you&#x27;ll find on the open web, with significant data on human preferences, validation, cited sources, and in the case of e.g. coding with verification that the code runs and works correctly.</div><br/><div id="41448591" class="c"><input type="checkbox" id="c-41448591" checked=""/><div class="controls bullet"><span class="by">bschmidt1</span><span>|</span><a href="#41446260">root</a><span>|</span><a href="#41448267">parent</a><span>|</span><a href="#41449881">next</a><span>|</span><label class="collapse" for="c-41448591">[-]</label><label class="expand" for="c-41448591">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; A lot of this data - what I&#x27;ve seen anyway, is far cleaner than anything you&#x27;ll find on the open web, with significant data on human preferences, validation, cited sources, and in the case of e.g. coding with verification that the code runs and works correctly.</i><p>Very interesting, thanks for sharing that detail. As someone who has tinkered with tokenizing&#x2F;training I quickly found out this must be the case. Some people on HN don&#x27;t know this. I&#x27;ve argued here with otherwise smart people who think there is no data preprocessing for LLMs, that they don&#x27;t need it because &quot;vectors&quot;, failing to realize the semantic depth and quality of embeddings depends on the quality of training data.</div><br/></div></div><div id="41449881" class="c"><input type="checkbox" id="c-41449881" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41446260">root</a><span>|</span><a href="#41448267">parent</a><span>|</span><a href="#41448591">prev</a><span>|</span><a href="#41448281">next</a><span>|</span><label class="collapse" for="c-41449881">[-]</label><label class="expand" for="c-41449881">[1 more]</label></div><br/><div class="children"><div class="content">i think we should distinguish between pretraining and polishing&#x2F;alignment data. what you are describing is most likely the latter (and probably mixed into to pretraining). but if you can&#x27;t get a mass of tokens from scraping, you&#x27;re going to be screwed</div><br/></div></div></div></div></div></div><div id="41448281" class="c"><input type="checkbox" id="c-41448281" checked=""/><div class="controls bullet"><span class="by">alpha_squared</span><span>|</span><a href="#41446260">root</a><span>|</span><a href="#41446683">parent</a><span>|</span><a href="#41448169">prev</a><span>|</span><a href="#41448360">next</a><span>|</span><label class="collapse" for="c-41448281">[-]</label><label class="expand" for="c-41448281">[4 more]</label></div><br/><div class="children"><div class="content">A lot of APIs changed in response to OpenAI hoovering up data. Reddit&#x27;s a big one that comes to mind. I&#x27;d argue that the last two years have seen the biggest change in the openness of the internet.</div><br/><div id="41449314" class="c"><input type="checkbox" id="c-41449314" checked=""/><div class="controls bullet"><span class="by">linotype</span><span>|</span><a href="#41446260">root</a><span>|</span><a href="#41448281">parent</a><span>|</span><a href="#41448360">next</a><span>|</span><label class="collapse" for="c-41449314">[-]</label><label class="expand" for="c-41449314">[3 more]</label></div><br/><div class="children"><div class="content">It’s made Reddit unusable without an account, which makes me wonder why it’s even on the web anymore and not an app. I guess legacy users that only use a web browser.</div><br/><div id="41449885" class="c"><input type="checkbox" id="c-41449885" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#41446260">root</a><span>|</span><a href="#41449314">parent</a><span>|</span><a href="#41448360">next</a><span>|</span><label class="collapse" for="c-41449885">[-]</label><label class="expand" for="c-41449885">[2 more]</label></div><br/><div class="children"><div class="content">did that not predate chatgpt?</div><br/><div id="41453777" class="c"><input type="checkbox" id="c-41453777" checked=""/><div class="controls bullet"><span class="by">linotype</span><span>|</span><a href="#41446260">root</a><span>|</span><a href="#41449885">parent</a><span>|</span><a href="#41448360">next</a><span>|</span><label class="collapse" for="c-41453777">[-]</label><label class="expand" for="c-41453777">[1 more]</label></div><br/><div class="children"><div class="content">It did not. Also VPNs were usable with the site, now I believe even logged in you can’t use them. I don’t know at this point, I no longer use Reddit at all.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41448360" class="c"><input type="checkbox" id="c-41448360" checked=""/><div class="controls bullet"><span class="by">jckahn</span><span>|</span><a href="#41446260">parent</a><span>|</span><a href="#41446575">prev</a><span>|</span><a href="#41447057">next</a><span>|</span><label class="collapse" for="c-41448360">[-]</label><label class="expand" for="c-41448360">[2 more]</label></div><br/><div class="children"><div class="content">What do you expect? This seems like a hard problem to solve. Hard problems take time.</div><br/><div id="41448892" class="c"><input type="checkbox" id="c-41448892" checked=""/><div class="controls bullet"><span class="by">zerocrates</span><span>|</span><a href="#41446260">root</a><span>|</span><a href="#41448360">parent</a><span>|</span><a href="#41447057">next</a><span>|</span><label class="collapse" for="c-41448892">[-]</label><label class="expand" for="c-41448892">[1 more]</label></div><br/><div class="children"><div class="content">I interpreted the comment as incredulous that superintelligence is as <i>close</i> as a &quot;couple years&quot; away.</div><br/></div></div></div></div><div id="41447057" class="c"><input type="checkbox" id="c-41447057" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#41446260">parent</a><span>|</span><a href="#41448360">prev</a><span>|</span><a href="#41448811">next</a><span>|</span><label class="collapse" for="c-41447057">[-]</label><label class="expand" for="c-41447057">[3 more]</label></div><br/><div class="children"><div class="content">They’d need a year or two just to rebuild a ChatGPT-level LLM, and they want to go way beyond that.</div><br/><div id="41447492" class="c"><input type="checkbox" id="c-41447492" checked=""/><div class="controls bullet"><span class="by">JamesSwift</span><span>|</span><a href="#41446260">root</a><span>|</span><a href="#41447057">parent</a><span>|</span><a href="#41448811">next</a><span>|</span><label class="collapse" for="c-41447492">[-]</label><label class="expand" for="c-41447492">[2 more]</label></div><br/><div class="children"><div class="content">a current-day* ChatGPT-level LLM<p>At a time when things are advancing at breakneck speed. Where is the goalpost going to be in 2 years time?</div><br/><div id="41448300" class="c"><input type="checkbox" id="c-41448300" checked=""/><div class="controls bullet"><span class="by">hintymad</span><span>|</span><a href="#41446260">root</a><span>|</span><a href="#41447492">parent</a><span>|</span><a href="#41448811">next</a><span>|</span><label class="collapse" for="c-41448300">[-]</label><label class="expand" for="c-41448300">[1 more]</label></div><br/><div class="children"><div class="content">A possibility is that they are betting that the current generation of LLM is converging, so they won&#x27;t worry about the goalpost much. If it&#x27;s true, then it won&#x27;t be good news for OpenAI.</div><br/></div></div></div></div></div></div><div id="41448811" class="c"><input type="checkbox" id="c-41448811" checked=""/><div class="controls bullet"><span class="by">xenospn</span><span>|</span><a href="#41446260">parent</a><span>|</span><a href="#41447057">prev</a><span>|</span><a href="#41446722">next</a><span>|</span><label class="collapse" for="c-41448811">[-]</label><label class="expand" for="c-41448811">[2 more]</label></div><br/><div class="children"><div class="content">Just until the $50B series A</div><br/><div id="41453224" class="c"><input type="checkbox" id="c-41453224" checked=""/><div class="controls bullet"><span class="by">stephenitis</span><span>|</span><a href="#41446260">root</a><span>|</span><a href="#41448811">parent</a><span>|</span><a href="#41446722">next</a><span>|</span><label class="collapse" for="c-41453224">[-]</label><label class="expand" for="c-41453224">[1 more]</label></div><br/><div class="children"><div class="content">We need to inflation adjust the concept of unicorns.</div><br/></div></div></div></div></div></div><div id="41446486" class="c"><input type="checkbox" id="c-41446486" checked=""/><div class="controls bullet"><span class="by">tikkun</span><span>|</span><a href="#41446260">prev</a><span>|</span><a href="#41450968">next</a><span>|</span><label class="collapse" for="c-41446486">[-]</label><label class="expand" for="c-41446486">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Everyone just says scaling hypothesis. Everyone neglects to ask, what are we scaling?&quot; [Sutskever] said.<p>Any guesses?</div><br/><div id="41446923" class="c"><input type="checkbox" id="c-41446923" checked=""/><div class="controls bullet"><span class="by">waldarbeiter</span><span>|</span><a href="#41446486">parent</a><span>|</span><a href="#41450968">next</a><span>|</span><label class="collapse" for="c-41446923">[-]</label><label class="expand" for="c-41446923">[1 more]</label></div><br/><div class="children"><div class="content">The conventional teaching that I am aware of says that you can scale across three dimensions: data, compute, parameters. But Ilya&#x27;s formulation suggests that there may be more dimensions along which scaling is possible.</div><br/></div></div></div></div><div id="41450968" class="c"><input type="checkbox" id="c-41450968" checked=""/><div class="controls bullet"><span class="by">krick</span><span>|</span><a href="#41446486">prev</a><span>|</span><a href="#41450894">next</a><span>|</span><label class="collapse" for="c-41450968">[-]</label><label class="expand" for="c-41450968">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t very interesting itself, IMO, but it implies that they have something to sell investors. I wonder what it is. I kinda do understand that some bullshit elevator-pitch about how &quot;we are the best&quot; or even a name (Musk) is unfortunately sometimes enough in VC to invest vast amounts of money, but I don&#x27;t know if it really happens often, and I hope there&#x27;s more than that. So if there is more than that, I wonder what it is. What does Sutskever&amp;Co have now that OpenAI doesn&#x27;t, for example?</div><br/></div></div><div id="41450894" class="c"><input type="checkbox" id="c-41450894" checked=""/><div class="controls bullet"><span class="by">siliconc0w</span><span>|</span><a href="#41450968">prev</a><span>|</span><a href="#41452628">next</a><span>|</span><label class="collapse" for="c-41450894">[-]</label><label class="expand" for="c-41450894">[1 more]</label></div><br/><div class="children"><div class="content">I can <i>kinda</i> see this making sense, I might bet more on Ilya than Sam at this point.  Still these bets all kinda seem like a pascal&#x27;s wager.</div><br/></div></div><div id="41452628" class="c"><input type="checkbox" id="c-41452628" checked=""/><div class="controls bullet"><span class="by">8note</span><span>|</span><a href="#41450894">prev</a><span>|</span><a href="#41446577">next</a><span>|</span><label class="collapse" for="c-41452628">[-]</label><label class="expand" for="c-41452628">[1 more]</label></div><br/><div class="children"><div class="content">Is $1B enough to license or create all the training data said models and ais will need?<p>Data sets aren&#x27;t quite as easy to scrape and copyright infringe on as they were before chatGPT</div><br/></div></div><div id="41446577" class="c"><input type="checkbox" id="c-41446577" checked=""/><div class="controls bullet"><span class="by">softwaredoug</span><span>|</span><a href="#41452628">prev</a><span>|</span><a href="#41447170">next</a><span>|</span><label class="collapse" for="c-41446577">[-]</label><label class="expand" for="c-41446577">[1 more]</label></div><br/><div class="children"><div class="content">Champions of Krynn II is gonna be epic</div><br/></div></div><div id="41447170" class="c"><input type="checkbox" id="c-41447170" checked=""/><div class="controls bullet"><span class="by">monacobolid</span><span>|</span><a href="#41446577">prev</a><span>|</span><a href="#41446484">next</a><span>|</span><label class="collapse" for="c-41447170">[-]</label><label class="expand" for="c-41447170">[1 more]</label></div><br/><div class="children"><div class="content">Ilya&#x27;s name might be the reason they got into the conversation about the money at the first place, but given that AI is very capital intensive business, $1B is not an insane amount imho. It will give him and the team a decent amount of time to do the research they want to do, without having the pressure of customers and what not.</div><br/></div></div><div id="41446484" class="c"><input type="checkbox" id="c-41446484" checked=""/><div class="controls bullet"><span class="by">crorella</span><span>|</span><a href="#41447170">prev</a><span>|</span><a href="#41446076">next</a><span>|</span><label class="collapse" for="c-41446484">[-]</label><label class="expand" for="c-41446484">[1 more]</label></div><br/><div class="children"><div class="content">The AI bubble is safe and sound!</div><br/></div></div><div id="41446415" class="c"><input type="checkbox" id="c-41446415" checked=""/><div class="controls bullet"><span class="by">typon</span><span>|</span><a href="#41446076">prev</a><span>|</span><a href="#41447589">next</a><span>|</span><label class="collapse" for="c-41446415">[-]</label><label class="expand" for="c-41446415">[3 more]</label></div><br/><div class="children"><div class="content">Anyone know John Carmack&#x27;s status on his AGI company?</div><br/><div id="41454584" class="c"><input type="checkbox" id="c-41454584" checked=""/><div class="controls bullet"><span class="by">cedws</span><span>|</span><a href="#41446415">parent</a><span>|</span><a href="#41448548">next</a><span>|</span><label class="collapse" for="c-41454584">[-]</label><label class="expand" for="c-41454584">[1 more]</label></div><br/><div class="children"><div class="content">Guess it didn’t go anywhere. Carmack is smart but how much work does he actually do on the front lines these days? Can he really just walk into unfamiliar territory and expect to move the needle?</div><br/></div></div><div id="41448548" class="c"><input type="checkbox" id="c-41448548" checked=""/><div class="controls bullet"><span class="by">seidleroni</span><span>|</span><a href="#41446415">parent</a><span>|</span><a href="#41454584">prev</a><span>|</span><a href="#41447589">next</a><span>|</span><label class="collapse" for="c-41448548">[-]</label><label class="expand" for="c-41448548">[1 more]</label></div><br/><div class="children"><div class="content">I keep wondering the same thing myself. I google it occasionally but never come up with anything.</div><br/></div></div></div></div><div id="41447589" class="c"><input type="checkbox" id="c-41447589" checked=""/><div class="controls bullet"><span class="by">teqsun</span><span>|</span><a href="#41446415">prev</a><span>|</span><a href="#41447987">next</a><span>|</span><label class="collapse" for="c-41447589">[-]</label><label class="expand" for="c-41447589">[7 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t even understand how the brain functions completely, not even close. Until we have a complete understanding of how our own GI works down to the exact bio-mechanical level, we can&#x27;t achieve AGI.<p>That&#x27;s the theoretical basis and path for achieving AGI (if it&#x27;s even possible). I&#x27;m tired of all the &quot;we stick enough data in the magic black box blender and ta-da! AGI!&quot;<p>Every giant technological break-through throughout history has had a massive underpinning of understanding before ever achieving it. And yet, with the AI bubble somehow we&#x27;re just about to secretly achieve it, but we can&#x27;t tell you how.</div><br/><div id="41451946" class="c"><input type="checkbox" id="c-41451946" checked=""/><div class="controls bullet"><span class="by">philipkglass</span><span>|</span><a href="#41447589">parent</a><span>|</span><a href="#41447920">next</a><span>|</span><label class="collapse" for="c-41451946">[-]</label><label class="expand" for="c-41451946">[1 more]</label></div><br/><div class="children"><div class="content"><i>Every giant technological break-through throughout history has had a massive underpinning of understanding before ever achieving it.</i><p>Historically, there have been a lot of cases where technological breakthroughs happened first and conceptual understanding followed later.<p>The first industrially useful steam engine was invented in 1712 [1] but the classical thermodynamic relationship between heat and work was not developed until 1824 [2].<p>There was a vast chemical industry before scientists really understood what atoms were, how many distinct kinds of atoms there are, or how they form molecules. It was only 20th century quantum mechanics that properly describes something as simple as a hydrogen molecule. That didn&#x27;t prevent chemists from developing complex synthetic chemicals [3]. It did mean that they had to rely more on guesses and experiments than on clear theory.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;History_of_the_steam_engine#Newcomen_%22atmospheric%22_engine" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;History_of_the_steam_engine#Ne...</a><p>[2] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Reflections_on_the_Motive_Power_of_Fire" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Reflections_on_the_Motive_Powe...</a><p>[3] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Indigo_dye#Chemical_synthesis" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Indigo_dye#Chemical_synthesis</a></div><br/></div></div><div id="41447920" class="c"><input type="checkbox" id="c-41447920" checked=""/><div class="controls bullet"><span class="by">skizm</span><span>|</span><a href="#41447589">parent</a><span>|</span><a href="#41451946">prev</a><span>|</span><a href="#41447609">next</a><span>|</span><label class="collapse" for="c-41447920">[-]</label><label class="expand" for="c-41447920">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m drawing a blank on the paper and can&#x27;t find it casually Googling, but there are fairly well understood mathematical models for how neurotransmitters cause neurons to fire or not fire. It is just probabilities when you zoom out enough. One paper modeled part of a rat brain, visual cortex I think, using this by basically coding up some simulated neurons and neurotransmitters, then turned it on. They were able to get the program and the live rat brain to display similar patterns when showing them various images.<p>I feel like this could be a path to GI without &quot;truly&quot; understanding the human brain: make a large enough simulation of the brain and turn it on. I actually do think we understand enough about the nuts and bolts of neuron interaction to achieve this. What we don&#x27;t understand is where neurons firing turns into consciousness. It seems like it is probably just an emergent property of a complex enough neuron graph.</div><br/></div></div><div id="41447609" class="c"><input type="checkbox" id="c-41447609" checked=""/><div class="controls bullet"><span class="by">leesec</span><span>|</span><a href="#41447589">parent</a><span>|</span><a href="#41447920">prev</a><span>|</span><a href="#41447753">next</a><span>|</span><label class="collapse" for="c-41447609">[-]</label><label class="expand" for="c-41447609">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Until we have a complete understanding of how our own GI works down to the exact bio-mechanical level, we can&#x27;t achieve AGI.<p>This doesn&#x27;t make any sense.</div><br/><div id="41453825" class="c"><input type="checkbox" id="c-41453825" checked=""/><div class="controls bullet"><span class="by">skirmish</span><span>|</span><a href="#41447589">root</a><span>|</span><a href="#41447609">parent</a><span>|</span><a href="#41447753">next</a><span>|</span><label class="collapse" for="c-41453825">[-]</label><label class="expand" for="c-41453825">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, e.g. magnetism was known and used in Ancient Greece, yet Wikipedia: &quot;ferromagnetism can be fully explained only using quantum theory. A successful model was developed &lt;...&gt; in 1927&quot;.</div><br/></div></div></div></div><div id="41447753" class="c"><input type="checkbox" id="c-41447753" checked=""/><div class="controls bullet"><span class="by">ctoth</span><span>|</span><a href="#41447589">parent</a><span>|</span><a href="#41447609">prev</a><span>|</span><a href="#41447987">next</a><span>|</span><label class="collapse" for="c-41447753">[-]</label><label class="expand" for="c-41447753">[2 more]</label></div><br/><div class="children"><div class="content">Wait until you learn about Anesthesia!</div><br/><div id="41453606" class="c"><input type="checkbox" id="c-41453606" checked=""/><div class="controls bullet"><span class="by">KingFelix</span><span>|</span><a href="#41447589">root</a><span>|</span><a href="#41447753">parent</a><span>|</span><a href="#41447987">next</a><span>|</span><label class="collapse" for="c-41453606">[-]</label><label class="expand" for="c-41453606">[1 more]</label></div><br/><div class="children"><div class="content">Good answer!</div><br/></div></div></div></div></div></div><div id="41447987" class="c"><input type="checkbox" id="c-41447987" checked=""/><div class="controls bullet"><span class="by">FileSorter</span><span>|</span><a href="#41447589">prev</a><span>|</span><label class="collapse" for="c-41447987">[-]</label><label class="expand" for="c-41447987">[2 more]</label></div><br/><div class="children"><div class="content">My question is where is he going to get the data?<p>Twitter, reddit and the rest of the web have deployed a number of anti-scrape techniques.</div><br/><div id="41448046" class="c"><input type="checkbox" id="c-41448046" checked=""/><div class="controls bullet"><span class="by">PUSH_AX</span><span>|</span><a href="#41447987">parent</a><span>|</span><label class="collapse" for="c-41448046">[-]</label><label class="expand" for="c-41448046">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes data falls off of the back of a truck.</div><br/></div></div></div></div></div></div></div></div></div></body></html>