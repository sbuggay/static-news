<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1720774863287" as="style"/><link rel="stylesheet" href="styles.css?v=1720774863287"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.together.ai/blog/flashattention-3">FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-Precision</a> <span class="domain">(<a href="https://www.together.ai">www.together.ai</a>)</span></div><div class="subtext"><span>jhshah</span> | <span>47 comments</span></div><br/><div><div id="40942809" class="c"><input type="checkbox" id="c-40942809" checked=""/><div class="controls bullet"><span class="by">refibrillator</span><span>|</span><a href="#40939030">next</a><span>|</span><label class="collapse" for="c-40942809">[-]</label><label class="expand" for="c-40942809">[1 more]</label></div><br/><div class="children"><div class="content">The code has a comment which seems to hint that Tri Dao was working on FA3 as early as April 2022, the month after Hopper&#x2F;H100 was announced. I find it mildly curious that over 2 years has elapsed before the code was released today. Perhaps it’s because now there’s better solutions in the pipeline?<p>Tri’s publication history has been leaning toward SSM and Mamba style architectures recently. Unlike Flash Attention which has quadratic time complexity wrt sequence length, these latest algorithms are subquadratic. Thus they do much less computation, instead of just doing it more efficiently a la Flash Attention.<p>Dao and Gu published a really long paper this year which demonstrated (among other things) how Mamba&#x2F;SSM can be formulated such that it’s amenable to acceleration using the same hardware primitives that Transformers benefit from.</div><br/></div></div><div id="40939030" class="c"><input type="checkbox" id="c-40939030" checked=""/><div class="controls bullet"><span class="by">edude03</span><span>|</span><a href="#40942809">prev</a><span>|</span><a href="#40939489">next</a><span>|</span><label class="collapse" for="c-40939030">[-]</label><label class="expand" for="c-40939030">[14 more]</label></div><br/><div class="children"><div class="content">How much is the flash attention algorithm tied to the hardware? For example, in this announcement they mention taking advantage of the async capabilities of the H100 GPUs which I assume means you won&#x27;t get those speedups on non H series card. Two, the actual flash attention library requires CUDA, although the algorithm has apparently?[^0] been ported to metal. I would imagine if the algorithm was literally just a pure function it could be implemented for any GPU&#x2F;ML framework?<p>[0]: <a href="https:&#x2F;&#x2F;github.com&#x2F;philipturner&#x2F;metal-flash-attention">https:&#x2F;&#x2F;github.com&#x2F;philipturner&#x2F;metal-flash-attention</a></div><br/><div id="40940418" class="c"><input type="checkbox" id="c-40940418" checked=""/><div class="controls bullet"><span class="by">vhiremath4</span><span>|</span><a href="#40939030">parent</a><span>|</span><a href="#40939139">next</a><span>|</span><label class="collapse" for="c-40940418">[-]</label><label class="expand" for="c-40940418">[1 more]</label></div><br/><div class="children"><div class="content">There are a bunch of good answers, but I wanted to succinctly say &quot;practically, quite a bit&quot;. Here&#x27;s a good little rabbit-hole example:<p>&gt; <a href="https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;nanoGPT&#x2F;blob&#x2F;master&#x2F;model.py#L45">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;nanoGPT&#x2F;blob&#x2F;master&#x2F;model.py#L45</a><p>Karpathy&#x27;s nanoGPT calling flash attention by checking if torch.nn.functional.scaled_dot_product_attention exists<p>&gt; <a href="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;generated&#x2F;torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow">https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;generated&#x2F;torch.nn.functiona...</a><p>Looking at the docs, in reality, most of the time you want this to call out to FA2 which optimizes the kernals on the device to split ops on the Softmax of the triangular matrix as well as reduce moving unnecessary batches of floating point numbers back and forth from the GPU to the CPU.<p>&gt; <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2307.08691" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2307.08691</a><p>The paper for FA2 almost entirely considers itself through the hardware it&#x27;s running on.</div><br/></div></div><div id="40939139" class="c"><input type="checkbox" id="c-40939139" checked=""/><div class="controls bullet"><span class="by">kristjansson</span><span>|</span><a href="#40939030">parent</a><span>|</span><a href="#40940418">prev</a><span>|</span><a href="#40939253">next</a><span>|</span><label class="collapse" for="c-40939139">[-]</label><label class="expand" for="c-40939139">[6 more]</label></div><br/><div class="children"><div class="content">FlashAttention&#x27;s algorithmic improvements is mostly just splitting&#x2F;combining the softmax part of attention, and is itself not totally novel.  The overwhelming contribution is implementing that, and all its fiddly pieces, efficiently on Nvidia hardware.</div><br/><div id="40939286" class="c"><input type="checkbox" id="c-40939286" checked=""/><div class="controls bullet"><span class="by">namibj</span><span>|</span><a href="#40939030">root</a><span>|</span><a href="#40939139">parent</a><span>|</span><a href="#40939798">next</a><span>|</span><label class="collapse" for="c-40939286">[-]</label><label class="expand" for="c-40939286">[1 more]</label></div><br/><div class="children"><div class="content">To clarify further, flash attention is explicitly targeting a compute engine with separate MMA and &quot;scalar&quot; vector execution units that allow post-processing the MMA outputs without involving memory bandwidth (though arithmetic intensity, especially relative between the MMA and the &quot;scalar&quot; instructions, is of concern), with a substantial amount of manually-managed L1D$ to use as sub-matrix accumulator, and a linear-in-context-length amount of &quot;VRAM&quot; that requires sensible arithmetic intensity to avoid being a bandwidth bottleneck (iirc in the hundreds when counting the scalar multiplies hiding in the MMA instructions).<p>This v3 with async might for once be so tied to Hopper that it&#x27;s not trivially portable to another platform that has the mentioned hardware blocks (AFAIK every AMD GCN card that can do compute shaders would qualify, though they do lack a specialized MMA unit).</div><br/></div></div><div id="40939798" class="c"><input type="checkbox" id="c-40939798" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40939030">root</a><span>|</span><a href="#40939139">parent</a><span>|</span><a href="#40939286">prev</a><span>|</span><a href="#40939253">next</a><span>|</span><label class="collapse" for="c-40939798">[-]</label><label class="expand" for="c-40939798">[4 more]</label></div><br/><div class="children"><div class="content">Clarifying:<p>Given the question: &quot;How much is the flash attention algorithm tied to the hardware?&quot;<p>The answer is 0.<p>ex. you can find generic flash attention recently added in llama.cpp and ONNX (MS needed it for Phi-3, needed for Recall).<p>On the side, novelty, I have no direct knowledge on, IMHO, asking that question would devolve the way novelty arguments  do in any field: there&#x27;s always someone else who can claim they did 80% of $X via $X-1, therefore, $X is by and large not novel. Ad infinitum.</div><br/><div id="40941029" class="c"><input type="checkbox" id="c-40941029" checked=""/><div class="controls bullet"><span class="by">kristjansson</span><span>|</span><a href="#40939030">root</a><span>|</span><a href="#40939798">parent</a><span>|</span><a href="#40939253">next</a><span>|</span><label class="collapse" for="c-40941029">[-]</label><label class="expand" for="c-40941029">[3 more]</label></div><br/><div class="children"><div class="content">I think the right analogy for FA is high-quality cache-aware BLAS kernel implementations.  The algorithm(s) is (are) clever and (as you note) completely independent of hardware.  However, a hardware-naive implementation is approximately worthless.  Most of the value of MKL, or Accelerate, or FA is in the careful matching of the parameters and implementation of the algorithm to the capabilities of hardware it&#x27;s going run on.<p>I definitely don&#x27;t mean to take away from Tri&#x2F;FA by mentioning novelty - I&#x27;m just repeating from paper, which refers back to algebraic aggregates[0] in its discussion of their tiled softmax.<p>[0]: <a href="https:&#x2F;&#x2F;web.stanford.edu&#x2F;class&#x2F;cs345d-01&#x2F;rl&#x2F;olap.pdf" rel="nofollow">https:&#x2F;&#x2F;web.stanford.edu&#x2F;class&#x2F;cs345d-01&#x2F;rl&#x2F;olap.pdf</a></div><br/><div id="40942783" class="c"><input type="checkbox" id="c-40942783" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#40939030">root</a><span>|</span><a href="#40941029">parent</a><span>|</span><a href="#40939253">next</a><span>|</span><label class="collapse" for="c-40942783">[-]</label><label class="expand" for="c-40942783">[2 more]</label></div><br/><div class="children"><div class="content">&gt; However, a hardware-naive implementation is approximately worthless.<p>This isn’t true when there is one vendor that’s 90% of the market and 2 maybe 3 generations of hardware to consider. Support A100, H100 and you are supporting most of the current market.</div><br/><div id="40943231" class="c"><input type="checkbox" id="c-40943231" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#40939030">root</a><span>|</span><a href="#40942783">parent</a><span>|</span><a href="#40939253">next</a><span>|</span><label class="collapse" for="c-40943231">[-]</label><label class="expand" for="c-40943231">[1 more]</label></div><br/><div class="children"><div class="content">Supporting A100 and H100 is the opposite of being hardware naive, though.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40939253" class="c"><input type="checkbox" id="c-40939253" checked=""/><div class="controls bullet"><span class="by">f_devd</span><span>|</span><a href="#40939030">parent</a><span>|</span><a href="#40939139">prev</a><span>|</span><a href="#40940449">next</a><span>|</span><label class="collapse" for="c-40939253">[-]</label><label class="expand" for="c-40939253">[3 more]</label></div><br/><div class="children"><div class="content">&gt; How much is the flash attention algorithm tied to the hardware?<p>The original FA, almost none.<p>For the latest versions depends on your abstraction, ThunderKittens[0] provides about the same speed up over FA2 (1.3x-2x%) as the article but relatively universal across GPUs. For any new hardware there may be hardware specific features that make it edge out more performance; usually vendors will adopt any new features that seems to beat them, but you do get fragmented API&#x2F;libraries (which is already true for CUDA).<p>[0]: <a href="https:&#x2F;&#x2F;hazyresearch.stanford.edu&#x2F;blog&#x2F;2024-05-12-tk" rel="nofollow">https:&#x2F;&#x2F;hazyresearch.stanford.edu&#x2F;blog&#x2F;2024-05-12-tk</a></div><br/><div id="40942697" class="c"><input type="checkbox" id="c-40942697" checked=""/><div class="controls bullet"><span class="by">eyegor</span><span>|</span><a href="#40939030">root</a><span>|</span><a href="#40939253">parent</a><span>|</span><a href="#40941110">next</a><span>|</span><label class="collapse" for="c-40942697">[-]</label><label class="expand" for="c-40942697">[1 more]</label></div><br/><div class="children"><div class="content">What do you mean by &quot;relatively universal&quot;? This is Cuda only [0] with a promise of a rocm backend eventually. There&#x27;s only one project I&#x27;m aware of that seriously tries to address the Cuda issue in ml [1].<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;HazyResearch&#x2F;ThunderKittens?tab=readme-ov-file#installation">https:&#x2F;&#x2F;github.com&#x2F;HazyResearch&#x2F;ThunderKittens?tab=readme-ov...</a><p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;vosen&#x2F;ZLUDA">https:&#x2F;&#x2F;github.com&#x2F;vosen&#x2F;ZLUDA</a></div><br/></div></div><div id="40941110" class="c"><input type="checkbox" id="c-40941110" checked=""/><div class="controls bullet"><span class="by">kristjansson</span><span>|</span><a href="#40939030">root</a><span>|</span><a href="#40939253">parent</a><span>|</span><a href="#40942697">prev</a><span>|</span><a href="#40940449">next</a><span>|</span><label class="collapse" for="c-40941110">[-]</label><label class="expand" for="c-40941110">[1 more]</label></div><br/><div class="children"><div class="content">I mean they&#x27;re building an API to abstract away some of the SKU-to-SKU differences, but the broader point cuts the other way, I think:<p>&gt; In fact, more broadly we believe we should really reorient our ideas of AI around what maps well onto the hardware. How big should a recurrent state be? As big can fit onto an SM. How dense should the compute be? No less so than what the hardware demands. An important future direction of this work for us is to use our learnings about the hardware to help us design the AI to match.<p>The value is in adapting the implementation (either manually at write-time or programmatically at run-time) to the specifics of the hardware.<p>Also, great line:<p>&gt; And we ask: if your matrix multiply is smaller than 16x16, are you sure what you’re doing is AI?</div><br/></div></div></div></div><div id="40940449" class="c"><input type="checkbox" id="c-40940449" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#40939030">parent</a><span>|</span><a href="#40939253">prev</a><span>|</span><a href="#40940699">next</a><span>|</span><label class="collapse" for="c-40940449">[-]</label><label class="expand" for="c-40940449">[2 more]</label></div><br/><div class="children"><div class="content">To add to the discussion, from a practical perspective, AMD hardware totally sucks and yet to have proper implementation with flash-attention-2. ROCm is moving to usable slowly, but not close to being even comparable with cuda.</div><br/><div id="40943250" class="c"><input type="checkbox" id="c-40943250" checked=""/><div class="controls bullet"><span class="by">LarsDu88</span><span>|</span><a href="#40939030">root</a><span>|</span><a href="#40940449">parent</a><span>|</span><a href="#40940699">next</a><span>|</span><label class="collapse" for="c-40943250">[-]</label><label class="expand" for="c-40943250">[1 more]</label></div><br/><div class="children"><div class="content">Whi os it so hard to port FA2 to the m1300 instinct?</div><br/></div></div></div></div><div id="40940699" class="c"><input type="checkbox" id="c-40940699" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#40939030">parent</a><span>|</span><a href="#40940449">prev</a><span>|</span><a href="#40939489">next</a><span>|</span><label class="collapse" for="c-40940699">[-]</label><label class="expand" for="c-40940699">[1 more]</label></div><br/><div class="children"><div class="content">Conceptually, just a bit, practically (in terms of implementation), a lot. The standard python implementation internally compiles a kernel for your specific hardware.</div><br/></div></div></div></div><div id="40939489" class="c"><input type="checkbox" id="c-40939489" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#40939030">prev</a><span>|</span><a href="#40943218">next</a><span>|</span><label class="collapse" for="c-40939489">[-]</label><label class="expand" for="c-40939489">[7 more]</label></div><br/><div class="children"><div class="content">Compiler folks: Is there any chance compilers will be able to find optimizations like FlashAttention on their own? Seems like TVM and tinygrad are working in that direction but I find it hard to believe that that would be feasible</div><br/><div id="40941340" class="c"><input type="checkbox" id="c-40941340" checked=""/><div class="controls bullet"><span class="by">Lerc</span><span>|</span><a href="#40939489">parent</a><span>|</span><a href="#40939713">next</a><span>|</span><label class="collapse" for="c-40941340">[-]</label><label class="expand" for="c-40941340">[1 more]</label></div><br/><div class="children"><div class="content">This strikes me as an extremely difficult but not intractable problem.<p>I&#x27;m not sure what the state of the art in compiler optimisation is with regard to data positioning and targeting maximum processor usage<p>There was a video on optimisation a while back that showed small optimisations caused increases in speed that were insignificant when compared to the speed variance induced by the memory layout that the optimisation (or even a random change) caused.<p>While that talk was more focused on getting a signal past the noise.  That noise itself is an artifact of compilers being not particularly good at handling a much simpler form of the problem you describe.<p>CPU and memory architectures are complex when caches and access patterns impact upon speed.<p>When you add in GPU architectures to the mix I think you might be in fairly uncharted territory.<p>Maybe one day.<p>Of course since we are in the field of AI there is also the question of could a sufficiently smart AI do this.  It depends on the value of sufficient.<p>I would like to think that an extremely high level test for an AI model could be to give it something like micrograd and tell it to produce something with the same interface that outperforms torch.<p>We&#x27;re not even in the ballpark of being able to do that yet, but it will be interesting when and if that happens.</div><br/></div></div><div id="40939713" class="c"><input type="checkbox" id="c-40939713" checked=""/><div class="controls bullet"><span class="by">namibj</span><span>|</span><a href="#40939489">parent</a><span>|</span><a href="#40941340">prev</a><span>|</span><a href="#40939537">next</a><span>|</span><label class="collapse" for="c-40939713">[-]</label><label class="expand" for="c-40939713">[1 more]</label></div><br/><div class="children"><div class="content">In theory, yes, it&#x27;s &quot;just&quot; some algebraic properties of the math used that allow for substantial reordering, and then you&#x27;d add fairly regular polyhedral loop tiling.
Just expensive to do, so you&#x27;ll have to cache the effort.<p>The area of e-graph optimizers seems well-suited to this, btw. It&#x27;s not really deployed outside of some niche tooling though, as it&#x27;s a big paradigm shift in optimizer pass handling (e.g., doesn&#x27;t work well with chairs classic call graphs, so control flow needs to be massively revamped to deploy e-graphs outside&#x2F;across basic blocks and for loops (break and return not supported!)).</div><br/></div></div><div id="40939537" class="c"><input type="checkbox" id="c-40939537" checked=""/><div class="controls bullet"><span class="by">rfoo</span><span>|</span><a href="#40939489">parent</a><span>|</span><a href="#40939713">prev</a><span>|</span><a href="#40942861">next</a><span>|</span><label class="collapse" for="c-40939537">[-]</label><label class="expand" for="c-40939537">[3 more]</label></div><br/><div class="children"><div class="content">No. Think of it like a different algorithm. You just take the shape of the hardware into consideration when designing the algorithm instead of considering math only.<p>&gt; Seems like TVM<p>Fair enough, though technically they are still about different things but it&#x27;s indeed very close, but<p>&gt; and tinygrad<p>?????? what gives you this impression?</div><br/><div id="40939587" class="c"><input type="checkbox" id="c-40939587" checked=""/><div class="controls bullet"><span class="by">dauertewigkeit</span><span>|</span><a href="#40939489">root</a><span>|</span><a href="#40939537">parent</a><span>|</span><a href="#40942861">next</a><span>|</span><label class="collapse" for="c-40939587">[-]</label><label class="expand" for="c-40939587">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the distinction between what TVM does and FlashAttention type optimizations?</div><br/><div id="40939647" class="c"><input type="checkbox" id="c-40939647" checked=""/><div class="controls bullet"><span class="by">rfoo</span><span>|</span><a href="#40939489">root</a><span>|</span><a href="#40939587">parent</a><span>|</span><a href="#40942861">next</a><span>|</span><label class="collapse" for="c-40939647">[-]</label><label class="expand" for="c-40939647">[1 more]</label></div><br/><div class="children"><div class="content">There is more than layout &#x2F; tile schedule in FA. For example, first, to be able to fuse all these together [0] at all, you need to &quot;decompose&quot; the softmax to make it combinable, which requires maintaining some extra statistics. Won&#x27;t gonna repeat the math here as the original FA paper is already very clear.<p>[0] so you can avoid materializing intermediate matrices and still being able to compute in blocks.</div><br/></div></div></div></div></div></div><div id="40942861" class="c"><input type="checkbox" id="c-40942861" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#40939489">parent</a><span>|</span><a href="#40939537">prev</a><span>|</span><a href="#40943218">next</a><span>|</span><label class="collapse" for="c-40942861">[-]</label><label class="expand" for="c-40942861">[1 more]</label></div><br/><div class="children"><div class="content">Kinda tricky if you want to call higher level operators in a wrapped language like Python.</div><br/></div></div></div></div><div id="40943218" class="c"><input type="checkbox" id="c-40943218" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#40939489">prev</a><span>|</span><a href="#40943278">next</a><span>|</span><label class="collapse" for="c-40943218">[-]</label><label class="expand" for="c-40943218">[1 more]</label></div><br/><div class="children"><div class="content">&gt; TMA (Tensor Memory Accelerator). This is a special hardware unit that accelerates the transfer of data between global memory and shared memory, taking care of all index calculation and out-of-bound predication. This frees up registers, which is a valuable resource to increase tile size and efficiency.<p>My understanding was that while it frees up registers it more importantly lets the hardware handle address generation, which can become a bottleneck as other operations around it become faster.</div><br/></div></div><div id="40943278" class="c"><input type="checkbox" id="c-40943278" checked=""/><div class="controls bullet"><span class="by">LarsDu88</span><span>|</span><a href="#40943218">prev</a><span>|</span><a href="#40942800">next</a><span>|</span><label class="collapse" for="c-40943278">[-]</label><label class="expand" for="c-40943278">[1 more]</label></div><br/><div class="children"><div class="content">I was wondering... this post mentions that ops like sigmoid are very slow.<p>A lot of modern LLMs use activation functions with sigmoid or soft max like SiLU, Swish, and SOLU.<p>Does Relu take less of a performance hit, and if so, maybe it&#x27;d be better to go back to good old relu?</div><br/></div></div><div id="40942800" class="c"><input type="checkbox" id="c-40942800" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#40943278">prev</a><span>|</span><a href="#40939213">next</a><span>|</span><label class="collapse" for="c-40942800">[-]</label><label class="expand" for="c-40942800">[4 more]</label></div><br/><div class="children"><div class="content">If anyone wants to port this over to ROCm &#x2F; AMD MI300x, reach out to me: hello@hotaisle.xyz (we won&#x27;t ever spam you).<p>Happy to donate the compute time for this work.</div><br/><div id="40943255" class="c"><input type="checkbox" id="c-40943255" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#40942800">parent</a><span>|</span><a href="#40942838">next</a><span>|</span><label class="collapse" for="c-40943255">[-]</label><label class="expand" for="c-40943255">[2 more]</label></div><br/><div class="children"><div class="content">Not trying to be rude but what is the thinking behind this offer? Why would someone do this port for…free save for access to the hardware? What’s the upside for them?</div><br/><div id="40943292" class="c"><input type="checkbox" id="c-40943292" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#40942800">root</a><span>|</span><a href="#40943255">parent</a><span>|</span><a href="#40942838">next</a><span>|</span><label class="collapse" for="c-40943292">[-]</label><label class="expand" for="c-40943292">[1 more]</label></div><br/><div class="children"><div class="content">Not a rude question. I&#x27;m building public HPC super computers, currently focused on AMD hardware. The one I&#x27;m about to deploy is Top 150, which is a pretty good start.<p>The goal is to encourage a developer flywheel. The more developers working with AMD hardware, the more hardware that is needed, the more hardware I can justify buying, the bigger my super computers get.<p>Nvidia has been doing the flywheel for years and it has clearly worked. Why not do the same for AMD? As I said in another thread, anyone who thinks that there should be a single provider for all of AI compute needs, will be on the wrong side of history.</div><br/></div></div></div></div><div id="40942838" class="c"><input type="checkbox" id="c-40942838" checked=""/><div class="controls bullet"><span class="by">JackYoustra</span><span>|</span><a href="#40942800">parent</a><span>|</span><a href="#40943255">prev</a><span>|</span><a href="#40939213">next</a><span>|</span><label class="collapse" for="c-40942838">[-]</label><label class="expand" for="c-40942838">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re the AMD accelerator server company! Such cool work, hope someone takes you up :)</div><br/></div></div></div></div><div id="40939213" class="c"><input type="checkbox" id="c-40939213" checked=""/><div class="controls bullet"><span class="by">lxe</span><span>|</span><a href="#40942800">prev</a><span>|</span><a href="#40940425">next</a><span>|</span><label class="collapse" for="c-40939213">[-]</label><label class="expand" for="c-40939213">[5 more]</label></div><br/><div class="children"><div class="content">&gt; FlashAttention-3 is optimized for Hopper GPUs (e.g. H100).<p>How does FA3 fare for consumer GPUs such as 3090 and 4090?</div><br/><div id="40939434" class="c"><input type="checkbox" id="c-40939434" checked=""/><div class="controls bullet"><span class="by">apsec112</span><span>|</span><a href="#40939213">parent</a><span>|</span><a href="#40940425">next</a><span>|</span><label class="collapse" for="c-40939434">[-]</label><label class="expand" for="c-40939434">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s Hopper-specific, the improvements are closely tied to Hopper features like warp groups and TMA. For 4090s, you might get a speedup by using the Triton implementation of FP8 attention: <a href="https:&#x2F;&#x2F;triton-lang.org&#x2F;main&#x2F;getting-started&#x2F;tutorials&#x2F;06-fused-attention.html" rel="nofollow">https:&#x2F;&#x2F;triton-lang.org&#x2F;main&#x2F;getting-started&#x2F;tutorials&#x2F;06-fu...</a></div><br/><div id="40939569" class="c"><input type="checkbox" id="c-40939569" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#40939213">root</a><span>|</span><a href="#40939434">parent</a><span>|</span><a href="#40940425">next</a><span>|</span><label class="collapse" for="c-40939569">[-]</label><label class="expand" for="c-40939569">[3 more]</label></div><br/><div class="children"><div class="content">The original flash attention (v1?) took like a year to get added to llama.cpp and only provides single digit percent VRAM savings for typical context lengths and practically no speed boost. Still nice to have, but man was this thing overhyped. I doubt v3 will do more than marginally better on the RTX 5000 series.</div><br/><div id="40939596" class="c"><input type="checkbox" id="c-40939596" checked=""/><div class="controls bullet"><span class="by">apsec112</span><span>|</span><a href="#40939213">root</a><span>|</span><a href="#40939569">parent</a><span>|</span><a href="#40940425">next</a><span>|</span><label class="collapse" for="c-40939596">[-]</label><label class="expand" for="c-40939596">[2 more]</label></div><br/><div class="children"><div class="content">On GPU, or on CPU&#x2F;Metal? For the latter I&#x27;m not surprised, but that&#x27;s because they have a totally different memory&#x2F;cache hierarchy.</div><br/><div id="40940379" class="c"><input type="checkbox" id="c-40940379" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#40939213">root</a><span>|</span><a href="#40939596">parent</a><span>|</span><a href="#40940425">next</a><span>|</span><label class="collapse" for="c-40940379">[-]</label><label class="expand" for="c-40940379">[1 more]</label></div><br/><div class="children"><div class="content">With CUDA offloading, I don&#x27;t think it runs otherwise at all.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40940425" class="c"><input type="checkbox" id="c-40940425" checked=""/><div class="controls bullet"><span class="by">ex3ndr</span><span>|</span><a href="#40939213">prev</a><span>|</span><a href="#40939398">next</a><span>|</span><label class="collapse" for="c-40940425">[-]</label><label class="expand" for="c-40940425">[2 more]</label></div><br/><div class="children"><div class="content">I am wondering why flash attention is like 5x slower with variable masking than without it? Lack of good masking support almost zeros out the optimizations</div><br/><div id="40943181" class="c"><input type="checkbox" id="c-40943181" checked=""/><div class="controls bullet"><span class="by">chillee</span><span>|</span><a href="#40940425">parent</a><span>|</span><a href="#40939398">next</a><span>|</span><label class="collapse" for="c-40943181">[-]</label><label class="expand" for="c-40943181">[1 more]</label></div><br/><div class="children"><div class="content">Where are you seeing these benchmarks?</div><br/></div></div></div></div><div id="40939398" class="c"><input type="checkbox" id="c-40939398" checked=""/><div class="controls bullet"><span class="by">andy_xor_andrew</span><span>|</span><a href="#40940425">prev</a><span>|</span><a href="#40939433">next</a><span>|</span><label class="collapse" for="c-40939398">[-]</label><label class="expand" for="c-40939398">[3 more]</label></div><br/><div class="children"><div class="content">hoping an expert can answer a few Qs I have :)<p>Is FlashAttention simply a drop-in replacement for the attention operation in an LLM? Can it be used anywhere that an &quot;attention&quot; operation is used? Or does a LLM need to be trained specially to use FA?<p>How does FA relate to attention <i>strategies</i> like GQA (grouped query attention) or sliding-window attention? Are they orthogonal concepts? Or you need a specific FA implementation for each strategy?<p>Recently llama.cpp added flash attention support - does this just mean they started consuming a flash attention-provided CUDA kernel or something?<p>lastly, in this post, they compare FlashAttention to Triton. I thought Triton was like an abstraction layer? Couldn&#x27;t FA be implemented in Triton? I just don&#x27;t really get what it means to say &quot;FlashAttention vs. Triton&quot;.</div><br/><div id="40939464" class="c"><input type="checkbox" id="c-40939464" checked=""/><div class="controls bullet"><span class="by">apsec112</span><span>|</span><a href="#40939398">parent</a><span>|</span><a href="#40939439">next</a><span>|</span><label class="collapse" for="c-40939464">[-]</label><label class="expand" for="c-40939464">[1 more]</label></div><br/><div class="children"><div class="content">1) Pretty much, it&#x27;s mathematically equivalent. The only software issues are things like managing dependency versions and data formats in-memory, but Flash Attention 2 is already built into HuggingFace and other popular libraries. Flash Attention 3 probably will be soon, although it requires an H100 GPU to run<p>2) Flash Attention 2 added support for GQA in past version updates:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;Dao-AILab&#x2F;flash-attention">https:&#x2F;&#x2F;github.com&#x2F;Dao-AILab&#x2F;flash-attention</a><p>3) They&#x27;re comparing this implementation of Flash Attention (which is written in raw CUDA C++) to the Triton implementation of a similar algorithm (which is written in Triton): <a href="https:&#x2F;&#x2F;triton-lang.org&#x2F;main&#x2F;getting-started&#x2F;tutorials&#x2F;06-fused-attention.html" rel="nofollow">https:&#x2F;&#x2F;triton-lang.org&#x2F;main&#x2F;getting-started&#x2F;tutorials&#x2F;06-fu...</a></div><br/></div></div><div id="40939439" class="c"><input type="checkbox" id="c-40939439" checked=""/><div class="controls bullet"><span class="by">zaptrem</span><span>|</span><a href="#40939398">parent</a><span>|</span><a href="#40939464">prev</a><span>|</span><a href="#40939433">next</a><span>|</span><label class="collapse" for="c-40939439">[-]</label><label class="expand" for="c-40939439">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Is FlashAttention simply a drop-in replacement for the attention operation in an LLM? Can it be used anywhere that an &quot;attention&quot; operation is used? Or does a LLM need to be trained specially to use FA?<p>Yes<p>&gt; How does FA relate to attention strategies like GQA (grouped query attention) or sliding-window attention? Are they orthogonal concepts? Or you need a specific FA implementation for each strategy?<p>Flash Attention is a way of calculating the Softmax(QK^T)V part of attention, whereas GQA is a way of calculating the Q, K, and V matricies. Sliding window attention (less sure about this, there are a bunch of windowed attention techniques) change the attention mask (the thing that controls which queries can attend to which keys).<p>&gt; Recently llama.cpp added flash attention support - does this just mean they started consuming a flash attention-provided CUDA kernel or something?<p>I don&#x27;t use llama.cpp but that sounds about right.<p>&gt; lastly, in this post, they compare FlashAttention to Triton. I thought Triton was like an abstraction layer? Couldn&#x27;t FA be implemented in Triton? I just don&#x27;t really get what it means to say &quot;FlashAttention vs. Triton&quot;.<p>They&#x27;re talking about a previous Flash Attention implementation written in Triton.</div><br/></div></div></div></div><div id="40939433" class="c"><input type="checkbox" id="c-40939433" checked=""/><div class="controls bullet"><span class="by">localfirst</span><span>|</span><a href="#40939398">prev</a><span>|</span><a href="#40939297">next</a><span>|</span><label class="collapse" for="c-40939433">[-]</label><label class="expand" for="c-40939433">[4 more]</label></div><br/><div class="children"><div class="content">spoiler: $xxx,xxx hardware required to run</div><br/><div id="40939864" class="c"><input type="checkbox" id="c-40939864" checked=""/><div class="controls bullet"><span class="by">aabhay</span><span>|</span><a href="#40939433">parent</a><span>|</span><a href="#40939495">next</a><span>|</span><label class="collapse" for="c-40939864">[-]</label><label class="expand" for="c-40939864">[1 more]</label></div><br/><div class="children"><div class="content">If you need to run it continuously for a year</div><br/></div></div><div id="40939495" class="c"><input type="checkbox" id="c-40939495" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#40939433">parent</a><span>|</span><a href="#40939864">prev</a><span>|</span><a href="#40939297">next</a><span>|</span><label class="collapse" for="c-40939495">[-]</label><label class="expand" for="c-40939495">[2 more]</label></div><br/><div class="children"><div class="content">$25k-$30k</div><br/></div></div></div></div><div id="40939297" class="c"><input type="checkbox" id="c-40939297" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#40939433">prev</a><span>|</span><a href="#40941233">next</a><span>|</span><label class="collapse" for="c-40939297">[-]</label><label class="expand" for="c-40939297">[3 more]</label></div><br/><div class="children"><div class="content">This is one of the most important improvements in all of AI, because it benefits most AI users by giving them access to more, faster, for the same hardware with little to no tradeoffs.</div><br/><div id="40939359" class="c"><input type="checkbox" id="c-40939359" checked=""/><div class="controls bullet"><span class="by">snovv_crash</span><span>|</span><a href="#40939297">parent</a><span>|</span><a href="#40941233">next</a><span>|</span><label class="collapse" for="c-40939359">[-]</label><label class="expand" for="c-40939359">[2 more]</label></div><br/><div class="children"><div class="content">...for all those users with H100s.</div><br/><div id="40939512" class="c"><input type="checkbox" id="c-40939512" checked=""/><div class="controls bullet"><span class="by">rfoo</span><span>|</span><a href="#40939297">root</a><span>|</span><a href="#40939359">parent</a><span>|</span><a href="#40941233">next</a><span>|</span><label class="collapse" for="c-40939512">[-]</label><label class="expand" for="c-40939512">[1 more]</label></div><br/><div class="children"><div class="content">... which is currently the most cost-efficient and environment-friendly way to do LLM inference [0].<p>[0] Small footprint time: before B100 ships; for actually large language models; for prefill only; may cause cancer in California.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>