<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1690016446214" as="style"/><link rel="stylesheet" href="styles.css?v=1690016446214"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/ggerganov/llama.cpp/pull/1773">Llama: Add grammar-based sampling</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>davepeck</span> | <span>86 comments</span></div><br/><div><div id="36820884" class="c"><input type="checkbox" id="c-36820884" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36821076">next</a><span>|</span><label class="collapse" for="c-36820884">[-]</label><label class="expand" for="c-36820884">[33 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s my understanding of how this works (please someone correct me if I&#x27;m getting this wrong).<p>Language models emit tokens one at a time, starting with the prompt that you give them.<p>If you have a conversation with an LLM, effectively you can think of that as you giving it a sequence of tokens, then it generates some, then you generate more and so-on.<p>This grammar trick effectively takes advantage of this by giving you much more finely grained control over the tokens. So you can do things like this:<p><pre><code>    Give me the address of the
    White House as JSON:
    
    {&quot;street&quot;: &quot;
</code></pre>
Then the LLM can return:<p><pre><code>    1600 Pennsylvania Ave NW&quot;
</code></pre>
The moment you see that closing double quote, you take over again and inject:<p><pre><code>    &quot;,
    &quot;City&quot;: &quot;
</code></pre>
It fills in:<p><pre><code>    Washington, DC&quot;
</code></pre>
And so on.<p>But because this is all based on a grammar, you can do way more with it than just JSON.<p>I saw a brilliant suggestion relating to this on Twitter a while ago:<p>&gt; @OpenAI should add an API argument allowing passing up a deterministic context free grammar.<p>&gt; [...]<p>&gt; While I think DCFL is what you want here in the short term, the really best thing is passing up a small WASM binary that simply <i>is</i> the sampler.<p>&gt; Allow a user to pass up a few KB of WASM binary and give it a few megabytes of RAM to run. Would enable next level LLM superpowers.<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;grantslatton&#x2F;status&#x2F;1637692033115762688" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;grantslatton&#x2F;status&#x2F;1637692033115762688</a></div><br/><div id="36820976" class="c"><input type="checkbox" id="c-36820976" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36820884">parent</a><span>|</span><a href="#36821358">next</a><span>|</span><label class="collapse" for="c-36820976">[-]</label><label class="expand" for="c-36820976">[14 more]</label></div><br/><div class="children"><div class="content">Not just that: the LLM outputs not individual tokens, but a weighted recommendation. The most probable (“best”) token has the highest weight, but there may be many alternatives including JSON symbols like quote characters.<p>The “temperature” setting adjusts how likely it is that an output token is chosen that is <i>not</i> the top-rated option. That prevents repetitive output.<p>Forcing an LLM to obey a grammar is mostly about filtering the list before the token choice is made. There may still be a random element controlled by the temperature!<p>A more advanced feature not commonly used is to also enable back-tracking if the AI gets stuck and can’t produce a valid output.</div><br/><div id="36821509" class="c"><input type="checkbox" id="c-36821509" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36820976">parent</a><span>|</span><a href="#36821358">next</a><span>|</span><label class="collapse" for="c-36821509">[-]</label><label class="expand" for="c-36821509">[13 more]</label></div><br/><div class="children"><div class="content">&gt; A more advanced feature not commonly used is to also enable back-tracking if the AI gets stuck and can’t produce a valid output.<p>Technically that part is mandatory if you don&#x27;t just want it to produce an output but to make it produce an output that correctly matches the temperature (i.e. one that you could have gotten by randomly sampling the LLM until you got a correct one). Randomly picking the next tokens that isn&#x27;t grammatically incorrect works but oversamples paths where most of the options are invalid. The ultimate example of this is that it can get stuck at a branch with probability 0.<p>From a probabilistic standpoint what you&#x27;d need to do is not just make it backtrack but make it keep generating until it generates a grammatically correct output in one go.<p>Maybe there is something clever that can be done to avoid regenerating from the start? What you&#x27;d need to achieve is that a token that has a x% probability of leading to an incorrect output also has x% probability to be erased.</div><br/><div id="36821915" class="c"><input type="checkbox" id="c-36821915" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821509">parent</a><span>|</span><a href="#36821807">next</a><span>|</span><label class="collapse" for="c-36821915">[-]</label><label class="expand" for="c-36821915">[2 more]</label></div><br/><div class="children"><div class="content">The way LLMs work is they output probabilities for every _token_, so you don&#x27;t really need to backtrack you can just always pick a token that matches the provided grammar.<p>That said, you might want to do something like (backtracking) beam-search which uses various heuristics to simultaneously explore multiple different paths because the semantic information may not be front-loaded, i.e. let&#x27;s say we had a grammar that had a key &quot;healthy&quot; with values &quot;very_unhealthy&quot; or &quot;moderately_healthy.&quot; For broccoli, the LLM might intend to say &quot;very_healthy&quot; and choose &quot;very&quot; but then be pigeonholed into saying &quot;very_unhealthy&quot; because it&#x27;s the only valid completion according to the grammar.<p>That said, there are a lot of shortcuts you can take to make this fairly efficient thanks to the autoregressive nature of (most modern) LLMs. You only need to regenerate &#x2F; recompute from where you want to backtrack from.</div><br/><div id="36822472" class="c"><input type="checkbox" id="c-36822472" checked=""/><div class="controls bullet"><span class="by">Vetch</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821915">parent</a><span>|</span><a href="#36821807">next</a><span>|</span><label class="collapse" for="c-36822472">[-]</label><label class="expand" for="c-36822472">[1 more]</label></div><br/><div class="children"><div class="content">Whether or not backtracking is needed is really down to the grammar&#x27;s ambiguity.<p>The auto-regressive nature of LLMs is actually something that counts against them, at least as some tell it. Although, really, the root problem is generating autoregressively from LLMs precludes planning ahead while also lacking any iterative refinement stage.<p>Backtracking, look-ahead, early failure pruning and staged generation are all very useful for fitting both concepts (refinement and planning ahead) in an auto-regressive generation framework.</div><br/></div></div></div></div><div id="36821807" class="c"><input type="checkbox" id="c-36821807" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821509">parent</a><span>|</span><a href="#36821915">prev</a><span>|</span><a href="#36821546">next</a><span>|</span><label class="collapse" for="c-36821807">[-]</label><label class="expand" for="c-36821807">[8 more]</label></div><br/><div class="children"><div class="content">This is what Google Mind is working on: treating the output of LLMs as tree to be searched instead of just linearly outputting tokens in a &quot;greedy&quot; manner and hoping for the best.<p>Apparently GPT-4 gets a lot of its quality from generating many alternatives (16?) and then picking the best one, but this is 16x as much computer power.<p>A clever tree search (which itself could be a neural net!) could improve the efficiency of this many-fold while simultaneously improving the quality by a huge factor as well.</div><br/><div id="36821901" class="c"><input type="checkbox" id="c-36821901" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821807">parent</a><span>|</span><a href="#36821944">next</a><span>|</span><label class="collapse" for="c-36821901">[-]</label><label class="expand" for="c-36821901">[4 more]</label></div><br/><div class="children"><div class="content">Arguably a &#x27;1 token at a time&#x27; model is itself a tree search, so it&#x27;s more of a perspective than anything. It&#x27;s really when you start pruning this tree that this distinction becomes interesting. And of course treating the tree as an explicit object may allow the model to do interesting stuff like jumping to a different branch entirely (deletions insertions etc.).<p>Generating 16 alternatives and picking the best one only makes sense to me if your standard for picking one is orthogonal to the model itself, if you just pick the one that your model deems the most likely you&#x27;ve just figure out a very crude and expensive way to lower the temperature.</div><br/><div id="36822520" class="c"><input type="checkbox" id="c-36822520" checked=""/><div class="controls bullet"><span class="by">Vetch</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821901">parent</a><span>|</span><a href="#36822535">next</a><span>|</span><label class="collapse" for="c-36822520">[-]</label><label class="expand" for="c-36822520">[1 more]</label></div><br/><div class="children"><div class="content">That is stretching arguably too far. If you are taking 1 sample path, you are not in any meaningful sense searching a tree. In the context of sampling a probability distribution, which is what LLMs do in effect, there is extra depth to this. Any random response need not be representative of what the model &quot;thinks&quot;. And maybe counter-intuitive to some but the most likely generation might actually be unrepresentative as well.<p>Drawing lots of samples and then marginalizing (as a kind of vote) is methodologically more principled where appropriate. Constraining generation according to some gating function, continually redrawing samples, can be used to significantly reduce error rates at the cost of longer generation times.<p>LLMs are not being used to their full potential because it is too costly to do so.</div><br/></div></div><div id="36822535" class="c"><input type="checkbox" id="c-36822535" checked=""/><div class="controls bullet"><span class="by">joaogui1</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821901">parent</a><span>|</span><a href="#36822520">prev</a><span>|</span><a href="#36821944">next</a><span>|</span><label class="collapse" for="c-36822535">[-]</label><label class="expand" for="c-36822535">[2 more]</label></div><br/><div class="children"><div class="content">When we talk about tree search we allow for backtracking, so if a node has 3 children all 3 will be explored generally, or at least a subsample of the children will be, in LLM sampling you generally pick a single token&#x2F;child and then just go on with that until the end of the generation.<p>If DeepMind is indeed doing something similar to AlphaZero to language modelling one would expect they would generate multiple &quot;rollouts&quot; from the current context and then use some kind of function&#x2F;network to predict which next token will lead you to the best final generation and then output that token. How to do all of that using a sensible amount of compute is what remains to be seen</div><br/><div id="36823477" class="c"><input type="checkbox" id="c-36823477" checked=""/><div class="controls bullet"><span class="by">two_in_one</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36822535">parent</a><span>|</span><a href="#36821944">next</a><span>|</span><label class="collapse" for="c-36823477">[-]</label><label class="expand" for="c-36823477">[1 more]</label></div><br/><div class="children"><div class="content">Talking about efficiency. LLMs are often more efficient running batches. Sort of several lines at a time. Which means we can at some point branch new lines and run them in parallel. It will be more efficient than running one after another. More over, with some tricks we can share the &#x27;history&#x27; instead of recomputing. This requires going deep into the model though.</div><br/></div></div></div></div></div></div><div id="36821944" class="c"><input type="checkbox" id="c-36821944" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821807">parent</a><span>|</span><a href="#36821901">prev</a><span>|</span><a href="#36821546">next</a><span>|</span><label class="collapse" for="c-36821944">[-]</label><label class="expand" for="c-36821944">[3 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t true, it&#x27;s a telephone game version of &quot;it&#x27;s a mixture of experts model&quot; that was used to explain the impossible claim that &quot;it&#x27;s a 1 trillion parameter&quot; in fall 22</div><br/><div id="36822308" class="c"><input type="checkbox" id="c-36822308" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821944">parent</a><span>|</span><a href="#36822420">next</a><span>|</span><label class="collapse" for="c-36822308">[-]</label><label class="expand" for="c-36822308">[1 more]</label></div><br/><div class="children"><div class="content">Apparently it&#x27;s <i>both</i>. There&#x27;s a bunch of experts, and then those output many alternatives, of which you see the &quot;best&quot; one as selected by a final quality-check neural net.</div><br/></div></div><div id="36822420" class="c"><input type="checkbox" id="c-36822420" checked=""/><div class="controls bullet"><span class="by">akomtu</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821944">parent</a><span>|</span><a href="#36822308">prev</a><span>|</span><a href="#36821546">next</a><span>|</span><label class="collapse" for="c-36822420">[-]</label><label class="expand" for="c-36822420">[1 more]</label></div><br/><div class="children"><div class="content">Well, if LLM suggests &quot;moves&quot;, and an Expert Model judges the whole output, then combining the two with a tree search suspiciously resembles the AlphaGo idea.</div><br/></div></div></div></div></div></div><div id="36821546" class="c"><input type="checkbox" id="c-36821546" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821509">parent</a><span>|</span><a href="#36821807">prev</a><span>|</span><a href="#36821358">next</a><span>|</span><label class="collapse" for="c-36821546">[-]</label><label class="expand" for="c-36821546">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Maybe there is something clever that can be done to avoid regenerating from the start? What you&#x27;d need to achieve is that a token that has a x% probability of leading to an incorrect output also has x% probability to be erased.<p>Like giving the llm a backspace token? There is a paper related to this:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36425375">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36425375</a></div><br/><div id="36821778" class="c"><input type="checkbox" id="c-36821778" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821546">parent</a><span>|</span><a href="#36821358">next</a><span>|</span><label class="collapse" for="c-36821778">[-]</label><label class="expand" for="c-36821778">[1 more]</label></div><br/><div class="children"><div class="content">I mean you&#x27;re going to need to include a probability to backtrack one way or another, but simply having a backtrack character seems more like a trick to make fitting the model easier than a way to make constraining it more accurate.<p>Simply having the probability to backtrack does turn the whole generation process into a ergodic Markov chain though, so you might be able to use something like MCMC to make it work. Technically those only start sampling the distribution <i>eventually</i> but picking the first or nth full output might be good enough for all practical purposes. Especially at low temperatures where there aren&#x27;t many reasonable options in the first place.</div><br/></div></div></div></div></div></div></div></div><div id="36821358" class="c"><input type="checkbox" id="c-36821358" checked=""/><div class="controls bullet"><span class="by">SCHiM</span><span>|</span><a href="#36820884">parent</a><span>|</span><a href="#36820976">prev</a><span>|</span><a href="#36820922">next</a><span>|</span><label class="collapse" for="c-36821358">[-]</label><label class="expand" for="c-36821358">[1 more]</label></div><br/><div class="children"><div class="content">No, the way it works is that the current output + potential next tokens to be sampled are checked with the grammar. All potential tokens that don&#x27;t match are removed. Then, with the list of valid tokens left, normal sampling strategies are used.</div><br/></div></div><div id="36820922" class="c"><input type="checkbox" id="c-36820922" checked=""/><div class="controls bullet"><span class="by">pshc</span><span>|</span><a href="#36820884">parent</a><span>|</span><a href="#36821358">prev</a><span>|</span><a href="#36821569">next</a><span>|</span><label class="collapse" for="c-36820922">[-]</label><label class="expand" for="c-36820922">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think this is correct; previously you could already control output by reading tokens one at a time from the LLM until you hit a stop character.<p>My take from the grammar-based sampling PR is that you ask llama.cpp to constrain the next output token, to a restricted set of possible tokens, using the grammar.</div><br/><div id="36824581" class="c"><input type="checkbox" id="c-36824581" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36820922">parent</a><span>|</span><a href="#36820952">next</a><span>|</span><label class="collapse" for="c-36824581">[-]</label><label class="expand" for="c-36824581">[1 more]</label></div><br/><div class="children"><div class="content">you could also always specify the logit bias parameter in openai apis</div><br/></div></div><div id="36820952" class="c"><input type="checkbox" id="c-36820952" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36820922">parent</a><span>|</span><a href="#36824581">prev</a><span>|</span><a href="#36821569">next</a><span>|</span><label class="collapse" for="c-36820952">[-]</label><label class="expand" for="c-36820952">[1 more]</label></div><br/><div class="children"><div class="content">Right, which is the same idea - it&#x27;s just that the code in llama.cpp is running your grammar as part of its token generation decisions as opposed to pausing and waiting for your other code to pick the next token.<p>(I&#x27;m trying for a very high level explanation here.)</div><br/></div></div></div></div><div id="36821569" class="c"><input type="checkbox" id="c-36821569" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#36820884">parent</a><span>|</span><a href="#36820922">prev</a><span>|</span><a href="#36821158">next</a><span>|</span><label class="collapse" for="c-36821569">[-]</label><label class="expand" for="c-36821569">[2 more]</label></div><br/><div class="children"><div class="content">Another detailed description of how to do this: <a href="https:&#x2F;&#x2F;github.com&#x2F;normal-computing&#x2F;outlines&#x2F;pull&#x2F;131">https:&#x2F;&#x2F;github.com&#x2F;normal-computing&#x2F;outlines&#x2F;pull&#x2F;131</a><p>That&#x27;s one of the developers of the Outlines library, another cool LLM workflow library.</div><br/><div id="36821819" class="c"><input type="checkbox" id="c-36821819" checked=""/><div class="controls bullet"><span class="by">farissbahi</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821569">parent</a><span>|</span><a href="#36821158">next</a><span>|</span><label class="collapse" for="c-36821819">[-]</label><label class="expand" for="c-36821819">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a paper as well. :) <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2307.09702.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2307.09702.pdf</a></div><br/></div></div></div></div><div id="36821158" class="c"><input type="checkbox" id="c-36821158" checked=""/><div class="controls bullet"><span class="by">IAmNotACellist</span><span>|</span><a href="#36820884">parent</a><span>|</span><a href="#36821569">prev</a><span>|</span><a href="#36821237">next</a><span>|</span><label class="collapse" for="c-36821158">[-]</label><label class="expand" for="c-36821158">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m struggling to understand what he&#x27;s talking about. Starting with &quot;passing up,&quot; did he invent this terminology? The only input you have to an LLM is the prompt, which gets tokenized. And if you were to send DCFG rules or a compiled version of it as part of the request, how would that fundamentally alter the way that the tokens are predicted? If the model predicts something that doesn&#x27;t conform to the grammar you require, is he proposing re-prompting until it gets it right?</div><br/><div id="36821369" class="c"><input type="checkbox" id="c-36821369" checked=""/><div class="controls bullet"><span class="by">baobabKoodaa</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821158">parent</a><span>|</span><a href="#36821731">next</a><span>|</span><label class="collapse" for="c-36821369">[-]</label><label class="expand" for="c-36821369">[1 more]</label></div><br/><div class="children"><div class="content">You have more inputs to an LLM than just the prompt. For example, people commonly pass parameters which control the sampling of tokens.<p>Implementing grammar based sampling does NOT require &quot;re-prompting until it gets it right&quot;. Imagine a point in time when the LLM is generating some particular token. Which token will it produce? To decide that, it evaluates and assigns a score to each potential token. Then it chooses one of these options based on some rules. Rules could be as simple as &quot;pick the token with the highest score&quot;. That is called a greedy strategy. Usually more complex strategies are used and they typically have some randomness. That is called sampling. You can imagine a grammar based sampling strategy to force specific tokens at specific positions in the output, for example, to close a bracket in json.</div><br/></div></div><div id="36821731" class="c"><input type="checkbox" id="c-36821731" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821158">parent</a><span>|</span><a href="#36821369">prev</a><span>|</span><a href="#36821566">next</a><span>|</span><label class="collapse" for="c-36821731">[-]</label><label class="expand" for="c-36821731">[3 more]</label></div><br/><div class="children"><div class="content">I think he&#x27;s proposing this kind of API:<p><pre><code>    POST &#x2F;openai&#x2F;gpt4
    {
        &quot;prompt&quot;: &quot;The address of the White House&quot;,
        &quot;sampler_wasm&quot;: &quot;base64 encoded WASM binary blob here&quot;
    }
</code></pre>
That WASM would be a program that you write yourself that is run as part of the tokenizer - so it could be a grammar but it could be anything else too.<p>It&#x27;s WASM which means it can be safely and performantly run in a sandbox by the OpenAI servers as part of their execution of your prompt.</div><br/><div id="36821792" class="c"><input type="checkbox" id="c-36821792" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821731">parent</a><span>|</span><a href="#36821566">next</a><span>|</span><label class="collapse" for="c-36821792">[-]</label><label class="expand" for="c-36821792">[2 more]</label></div><br/><div class="children"><div class="content">I think you mean &quot;run as part of the sampler,&quot; the tokenizer (and tokenization) is fixed for a given model. The sampler blob would basically:<p>1. Modify the output token probabilities to fit any arbitrary use case<p>2. Perhaps do trigger some sort of backtracking &#x2F; beam-search<p>(I&#x27;m not Grant but we&#x27;ve chatted on twitter and built similar things)</div><br/><div id="36821954" class="c"><input type="checkbox" id="c-36821954" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821792">parent</a><span>|</span><a href="#36821566">next</a><span>|</span><label class="collapse" for="c-36821954">[-]</label><label class="expand" for="c-36821954">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I meant sampler, not tokenizer.</div><br/></div></div></div></div></div></div><div id="36821566" class="c"><input type="checkbox" id="c-36821566" checked=""/><div class="controls bullet"><span class="by">lyjackal</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821158">parent</a><span>|</span><a href="#36821731">prev</a><span>|</span><a href="#36821237">next</a><span>|</span><label class="collapse" for="c-36821566">[-]</label><label class="expand" for="c-36821566">[1 more]</label></div><br/><div class="children"><div class="content">The model returns probabilities across the full set of tokens. This restricts the tokens to those that conform to the grammar, and samples from those</div><br/></div></div></div></div><div id="36821237" class="c"><input type="checkbox" id="c-36821237" checked=""/><div class="controls bullet"><span class="by">eightysixfour</span><span>|</span><a href="#36820884">parent</a><span>|</span><a href="#36821158">prev</a><span>|</span><a href="#36821474">next</a><span>|</span><label class="collapse" for="c-36821237">[-]</label><label class="expand" for="c-36821237">[4 more]</label></div><br/><div class="children"><div class="content">Isn’t this what Microsoft Guidance does?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance</a></div><br/><div id="36821936" class="c"><input type="checkbox" id="c-36821936" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821237">parent</a><span>|</span><a href="#36821474">next</a><span>|</span><label class="collapse" for="c-36821936">[-]</label><label class="expand" for="c-36821936">[3 more]</label></div><br/><div class="children"><div class="content">I read the code. Guidance seems designed to work well with OpenAI&#x27;s chat completion API. When you ask Guidance to choose from a set of options, it breaks the list into a tree of tokens and then walks this tree, providing the next set of possible tokens in the logit_bias parameter with value set to +100.<p>For example, suppose that you specify this as your Guidance &quot;program&quot; and suppose (for sake of simplicity) that the token for &quot;lea&quot; is 1300, the token for &quot;ther&quot; is 1500, and the token for &quot;ves&quot; is 5300:<p><pre><code>  &quot;armor&quot;: &quot;{{#select &#x27;armor&#x27;}}leather{{or}}leaves{{&#x2F;select}}&quot;,
</code></pre>
Guidance will send OpenAI a chat completion starting with<p><pre><code>  &quot;armor&quot;: &quot;
</code></pre>
... providing a logit_bias map {&quot;1300&quot;: &quot;100&quot;}. This bias forces the model to choose &quot;lea&quot; as the next token. Following this call, we have the prefix<p><pre><code>  &quot;armor&quot;: &quot;lea
</code></pre>
... and now Guidance calls chat completion again setting the logit_bias map to {&quot;1500&quot;: &quot;100&quot;, &quot;5300&quot;: &quot;100&quot;} to indicate that the tokens for &quot;ther&quot; or &quot;ves&quot; are equally probable and really the only tokens the model is allowed to select between, unless some other token is maximally probable given the context. OpenAI now replies with token &quot;1500&quot; (let&#x27;s say) and Guidance completes the string as follows:<p><pre><code>  &quot;armor&quot;: &quot;leather
</code></pre>
... because &quot;ther&quot; is represented by token number 1500. Guidance then tacks on the closing quote and other stuff specified by the user:<p><pre><code>  &quot;armor&quot;: &quot;leather&quot;,
</code></pre>
... and it sets the value of &quot;armor&quot; to &quot;leather&quot; so that you can use that value later in your code if you wish to. Guidance is pretty powerful, but I find the grammar hard to work with. I think the idea of being able to upload a bit of code or a context-free grammar to guide the model is super smart.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance&#x2F;blob&#x2F;d2c5e3cbb730e337b9bee20520eb694bd43e5f38&#x2F;guidance&#x2F;library&#x2F;_select.py#L67">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance&#x2F;blob&#x2F;d2c5e3cbb730e337b...</a></div><br/><div id="36822127" class="c"><input type="checkbox" id="c-36822127" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821936">parent</a><span>|</span><a href="#36821960">next</a><span>|</span><label class="collapse" for="c-36822127">[-]</label><label class="expand" for="c-36822127">[1 more]</label></div><br/><div class="children"><div class="content">OTOH, AFAIK when running a model locally Guidance does something really similar to what OP is doing.</div><br/></div></div><div id="36821960" class="c"><input type="checkbox" id="c-36821960" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36820884">root</a><span>|</span><a href="#36821936">parent</a><span>|</span><a href="#36822127">prev</a><span>|</span><a href="#36821474">next</a><span>|</span><label class="collapse" for="c-36821960">[-]</label><label class="expand" for="c-36821960">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! I finally get what Guidance is doing now.</div><br/></div></div></div></div></div></div><div id="36821474" class="c"><input type="checkbox" id="c-36821474" checked=""/><div class="controls bullet"><span class="by">barbazoo</span><span>|</span><a href="#36820884">parent</a><span>|</span><a href="#36821237">prev</a><span>|</span><a href="#36821307">next</a><span>|</span><label class="collapse" for="c-36821474">[-]</label><label class="expand" for="c-36821474">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for laying it out like that.</div><br/></div></div><div id="36821307" class="c"><input type="checkbox" id="c-36821307" checked=""/><div class="controls bullet"><span class="by">anothernewdude</span><span>|</span><a href="#36820884">parent</a><span>|</span><a href="#36821474">prev</a><span>|</span><a href="#36821076">next</a><span>|</span><label class="collapse" for="c-36821307">[-]</label><label class="expand" for="c-36821307">[1 more]</label></div><br/><div class="children"><div class="content">So does this mean I can detect &quot;Sorry&quot; at the start of a response and prevent it?</div><br/></div></div></div></div><div id="36821076" class="c"><input type="checkbox" id="c-36821076" checked=""/><div class="controls bullet"><span class="by">svc0</span><span>|</span><a href="#36820884">prev</a><span>|</span><a href="#36821827">next</a><span>|</span><label class="collapse" for="c-36821076">[-]</label><label class="expand" for="c-36821076">[3 more]</label></div><br/><div class="children"><div class="content">I think it should be noted that this enforces grammatical constraints on the model&#x27;s generated text, but it doesn&#x27;t do anything to properly align the content. This would be useful if you needed to ensure a server delivered well-formatted JSON, but it I suspect it wont solve a lot of alignment issues with current language generation. For example current iterations of Llama and GPT often do not label markdown code-blocks correctly. Using grammar-based sampling, you could enforce that it labels code blocks but you couldn&#x27;t enforce correct labeling since this is context-dependent. You also couldn&#x27;t invent a novel domain-specific language without aligning against that language and expect good output.</div><br/><div id="36821952" class="c"><input type="checkbox" id="c-36821952" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#36821076">parent</a><span>|</span><a href="#36821444">next</a><span>|</span><label class="collapse" for="c-36821952">[-]</label><label class="expand" for="c-36821952">[1 more]</label></div><br/><div class="children"><div class="content">Also important to call out that anytime you have a freeform string it&#x27;s pretty much an open invitation for the LLM to go completely haywire and run off into all sorts of weird tangents. So these methods are best used with other heuristics to bias sampling once you get to free-form text territory (i.e. a repetition penalty etc)</div><br/></div></div><div id="36821444" class="c"><input type="checkbox" id="c-36821444" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36821076">parent</a><span>|</span><a href="#36821952">prev</a><span>|</span><a href="#36821827">next</a><span>|</span><label class="collapse" for="c-36821444">[-]</label><label class="expand" for="c-36821444">[1 more]</label></div><br/><div class="children"><div class="content">But since its llama, some examples could be trained into a lora.<p>I can imagine a system where, for instance, a markdown lora and a markdown grammar file can be hotswapped in and out.</div><br/></div></div></div></div><div id="36821827" class="c"><input type="checkbox" id="c-36821827" checked=""/><div class="controls bullet"><span class="by">burke</span><span>|</span><a href="#36821076">prev</a><span>|</span><a href="#36824213">next</a><span>|</span><label class="collapse" for="c-36821827">[-]</label><label class="expand" for="c-36821827">[1 more]</label></div><br/><div class="children"><div class="content">I implemented this for PyTorch too at <a href="https:&#x2F;&#x2F;github.com&#x2F;Shopify&#x2F;torch-grammar">https:&#x2F;&#x2F;github.com&#x2F;Shopify&#x2F;torch-grammar</a>. I have a hacked version of text-generation-inference that uses it—happy to share that if it’s useful to anyone.</div><br/></div></div><div id="36824213" class="c"><input type="checkbox" id="c-36824213" checked=""/><div class="controls bullet"><span class="by">karmasimida</span><span>|</span><a href="#36821827">prev</a><span>|</span><a href="#36821257">next</a><span>|</span><label class="collapse" for="c-36824213">[-]</label><label class="expand" for="c-36824213">[1 more]</label></div><br/><div class="children"><div class="content">This is great and all.<p>But LLM&#x27;s are usually very good at following grammars. I rarely see LLM generating code that is OOD. Ofc, this is only true for popular language (JSON&#x2F;Python&#x2F;Java, etc), I can see how this is handy for more niche and in house DSL.<p>You still need quite a lot of prompt engineering to get desired outputs, this just add another layer of output verification IMO. But does it really save much as comparing to get the output then parse and reject the output that doesn&#x27;t follow the grammar? Might be debateable.<p>But great work regardless.</div><br/></div></div><div id="36821257" class="c"><input type="checkbox" id="c-36821257" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36824213">prev</a><span>|</span><a href="#36820542">next</a><span>|</span><label class="collapse" for="c-36821257">[-]</label><label class="expand" for="c-36821257">[3 more]</label></div><br/><div class="children"><div class="content">This grammar &quot;library&quot; was cited as an example of what the format could look like:.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;antlr&#x2F;grammars-v4">https:&#x2F;&#x2F;github.com&#x2F;antlr&#x2F;grammars-v4</a><p>There is everything from assembly and C++ to glsl and scripting languages, arithmetic, games, and other weird formats like freedesktop shortcuts, llvm ir or verilog.</div><br/><div id="36821950" class="c"><input type="checkbox" id="c-36821950" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#36821257">parent</a><span>|</span><a href="#36823068">next</a><span>|</span><label class="collapse" for="c-36821950">[-]</label><label class="expand" for="c-36821950">[1 more]</label></div><br/><div class="children"><div class="content">A convenience feature in any inference API would be to specify a shortcut to a standardized grammar such as HTML, JSON, Python, etc. It is frankly strange to me that OpenAI have not already done this, considering the obvious effort they undertook to fine-tune the Code Interpreter model.</div><br/></div></div><div id="36823068" class="c"><input type="checkbox" id="c-36823068" checked=""/><div class="controls bullet"><span class="by">RossBencina</span><span>|</span><a href="#36821257">parent</a><span>|</span><a href="#36821950">prev</a><span>|</span><a href="#36820542">next</a><span>|</span><label class="collapse" for="c-36823068">[-]</label><label class="expand" for="c-36823068">[1 more]</label></div><br/><div class="children"><div class="content">It would be awesome if they supported ANLTR4 grammar syntax. Such a great tool.</div><br/></div></div></div></div><div id="36820542" class="c"><input type="checkbox" id="c-36820542" checked=""/><div class="controls bullet"><span class="by">spion</span><span>|</span><a href="#36821257">prev</a><span>|</span><a href="#36820874">next</a><span>|</span><label class="collapse" for="c-36820542">[-]</label><label class="expand" for="c-36820542">[1 more]</label></div><br/><div class="children"><div class="content">Specifically for multi-choice string enums (essentially dropdowns), I wonder if this would work better if the full (joint&#x2F;product) probability given the logits is considered when picking the final choice, rather than using a greedy algorithm. This will favor the right choice, as opposed to e.g. one of the choices that contain the most common start token - if a start token are shared among many items in the list.<p>Of course the probability needs to be adjusted once a subset of the logits goes to zero so it actually makes sense...</div><br/></div></div><div id="36820874" class="c"><input type="checkbox" id="c-36820874" checked=""/><div class="controls bullet"><span class="by">1024core</span><span>|</span><a href="#36820542">prev</a><span>|</span><a href="#36820606">next</a><span>|</span><label class="collapse" for="c-36820874">[-]</label><label class="expand" for="c-36820874">[10 more]</label></div><br/><div class="children"><div class="content">Can someone ELI5 what&#x27;s going on here? I&#x27;m reasonably familiar with LLMs, but I can&#x27;t quite grok what Georgi is doing here and why it&#x27;s so exciting for some.</div><br/><div id="36821287" class="c"><input type="checkbox" id="c-36821287" checked=""/><div class="controls bullet"><span class="by">tylerhou</span><span>|</span><a href="#36820874">parent</a><span>|</span><a href="#36820926">next</a><span>|</span><label class="collapse" for="c-36821287">[-]</label><label class="expand" for="c-36821287">[4 more]</label></div><br/><div class="children"><div class="content">An LLM does not generate &quot;the next token&quot; - from an input text, it generates a vector of probabilities where each slot in the vector corresponds to a token. The value in a token&#x27;s slot is (approximately) the probability that that particular token might appear next in the text.<p>Programs like ChatGPT &quot;interpret&quot; that vector of probabilities to generate text by selecting (sampling) one of the top tokens. But sometimes this is too flexible -- for example, ChatGPT might generate invalid JSON when you want JSON output because it chose a token that does not conform to the JSON grammar.<p>A way to &quot;force&quot; an LLM to generate e.g. JSON is to change the sampling process. Instead of choosing any top token, we first filter the tokens to just those that conform to the JSON grammar. Then, we sample one of the top tokens from that subset.</div><br/><div id="36822053" class="c"><input type="checkbox" id="c-36822053" checked=""/><div class="controls bullet"><span class="by">bryan0</span><span>|</span><a href="#36820874">root</a><span>|</span><a href="#36821287">parent</a><span>|</span><a href="#36821965">next</a><span>|</span><label class="collapse" for="c-36822053">[-]</label><label class="expand" for="c-36822053">[2 more]</label></div><br/><div class="children"><div class="content">And to build on this, take a look at the code change. Currently in llama.cpp there are many techniques for sampling the next token:<p>llama_sample_token_greedy - just take the top probability<p>llama_sample_top_k - sample only from the top k probabilities<p>etc ...<p>this code change adds a new sample:<p>llama_sample_grammar - sample only from tokens which match the grammar</div><br/><div id="36824052" class="c"><input type="checkbox" id="c-36824052" checked=""/><div class="controls bullet"><span class="by">dave1010uk</span><span>|</span><a href="#36820874">root</a><span>|</span><a href="#36822053">parent</a><span>|</span><a href="#36821965">next</a><span>|</span><label class="collapse" for="c-36824052">[-]</label><label class="expand" for="c-36824052">[1 more]</label></div><br/><div class="children"><div class="content">If the performance was ok, is there any reason why a sampler couldn&#x27;t call an API or use a separate fine tuned model?</div><br/></div></div></div></div><div id="36821965" class="c"><input type="checkbox" id="c-36821965" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#36820874">root</a><span>|</span><a href="#36821287">parent</a><span>|</span><a href="#36822053">prev</a><span>|</span><a href="#36820926">next</a><span>|</span><label class="collapse" for="c-36821965">[-]</label><label class="expand" for="c-36821965">[1 more]</label></div><br/><div class="children"><div class="content">And given that the inference code has access to the entire vector, it&#x27;s the logical place to put this filtering... OpenAI and other LLM APIs probably don&#x27;t want to return the entire token probability vector to the user because it&#x27;s a lot of data. That being said, it wouldn&#x27;t surprise me if Microsoft has such access as part of their deal because of the obviously superior position this puts them in vs. regular API customers.</div><br/></div></div></div></div><div id="36820926" class="c"><input type="checkbox" id="c-36820926" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#36820874">parent</a><span>|</span><a href="#36821287">prev</a><span>|</span><a href="#36821250">next</a><span>|</span><label class="collapse" for="c-36820926">[-]</label><label class="expand" for="c-36820926">[3 more]</label></div><br/><div class="children"><div class="content">If you ask an LLM to generate JSON or another language that has a grammar, it will sometimes produce invalid syntax. This pull request constrains the LLM so that it can only output valid syntax according to whatever grammar you supply. It&#x27;s a modification to the sampling procedure.<p>What is the sampling procedure? Well, the way an LLM generates text is one token (short sequence of characters) at a time. First the giant neural net assigns a probability to every possible token (this is the hard part). Then a sampling procedure uses the probabilities to pick one of the tokens, and the process repeats.<p>The sampling procedure is not a neural net and can be modified in many different ways. You might think that the sampling procedure should always simply pick the token with the highest probability (greedy sampling). You can do that, but it&#x27;s usually better to pick at random weighted by the probabilities. This gives more diversity and is less likely to get stuck in loops. But this means that literally any token with nonzero probability might get picked, so you can see how this might lead to invalid JSON being generated sometimes. This pull request zeros out the probabilities of all the tokens that wouldn&#x27;t be valid according to your grammar, so they can&#x27;t be picked.<p>BTW there are lots of other interesting modifications to the sampling process you could consider. For example, maybe you can see that in the process of sampling tokens one after the other you might paint yourself into a corner and end up with no good options to choose from. So maybe it makes sense to allow backtracking. In fact, maybe at each sampling step we can consider multiple options, making a tree of possible outputs, and at the end we can pick the path through the tree with the highest overall probability. Of course we can&#x27;t consider <i>every</i> option; it would be a complete tree with a branching factor of the number of possible tokens, which would grow exponentially. Let&#x27;s prune the tree at each step and only consider the top, say, five paths we&#x27;ve seen so far. This is called &quot;beam search&quot;. It&#x27;s not normally used for LLMs because the neural net that generates the probabilities is very expensive to run and multiplying that cost by a factor of e.g. five is unpalatable. But it can be done, and produces somewhat better results. You could also consider using MCTS like chess engines do.</div><br/><div id="36820997" class="c"><input type="checkbox" id="c-36820997" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#36820874">root</a><span>|</span><a href="#36820926">parent</a><span>|</span><a href="#36821250">next</a><span>|</span><label class="collapse" for="c-36820997">[-]</label><label class="expand" for="c-36820997">[2 more]</label></div><br/><div class="children"><div class="content">This is a sort of modern version of <a href="https:&#x2F;&#x2F;wiki.c2.com&#x2F;?AlternateHardAndSoftLayers" rel="nofollow noreferrer">https:&#x2F;&#x2F;wiki.c2.com&#x2F;?AlternateHardAndSoftLayers</a>, one of the most useful software patterns.</div><br/><div id="36822196" class="c"><input type="checkbox" id="c-36822196" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#36820874">root</a><span>|</span><a href="#36820997">parent</a><span>|</span><a href="#36821250">next</a><span>|</span><label class="collapse" for="c-36822196">[-]</label><label class="expand" for="c-36822196">[1 more]</label></div><br/><div class="children"><div class="content">Say more… I read the link, and it seems to be advocating for replacing specific business logic with a generic code interpreter?</div><br/></div></div></div></div></div></div><div id="36821250" class="c"><input type="checkbox" id="c-36821250" checked=""/><div class="controls bullet"><span class="by">6gvONxR4sf7o</span><span>|</span><a href="#36820874">parent</a><span>|</span><a href="#36820926">prev</a><span>|</span><a href="#36820889">next</a><span>|</span><label class="collapse" for="c-36821250">[-]</label><label class="expand" for="c-36821250">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are happy to generate arbitrary strings. You might want it to spit out something along the lines of &quot;Alice: 42&quot; and then it spits out &quot;hi, i&#x27;m helpful and Alice is exactly forty two, as far as I can tell, but I&#x27;m just a language model.&quot;<p>So you give it a grammar that says the response has to be an uppercase letter followed by lowercase letters, then a colon, then a space, then digits, then it&#x27;s done. Now, when it looks for that first token, it will only consider tokens that are compatible with that pattern. Then it&#x27;ll continue with only next tokens that are compatible with the next parts of the pattern.<p>These grammars do that with a flexible and useful kind of pattern.</div><br/></div></div><div id="36820889" class="c"><input type="checkbox" id="c-36820889" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36820874">parent</a><span>|</span><a href="#36821250">prev</a><span>|</span><a href="#36820606">next</a><span>|</span><label class="collapse" for="c-36820889">[-]</label><label class="expand" for="c-36820889">[1 more]</label></div><br/><div class="children"><div class="content">See my comment here <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36820884">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36820884</a></div><br/></div></div></div></div><div id="36820606" class="c"><input type="checkbox" id="c-36820606" checked=""/><div class="controls bullet"><span class="by">version_five</span><span>|</span><a href="#36820874">prev</a><span>|</span><a href="#36821529">next</a><span>|</span><label class="collapse" for="c-36820606">[-]</label><label class="expand" for="c-36820606">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m interested in this and I&#x27;m going to try incorporating it into something I&#x27;m doing. That said, I feel like this could be one of those Bitter Lesson situations where it&#x27;s not the most effective approach in anything but the very short term: <a href="http:&#x2F;&#x2F;www.incompleteideas.net&#x2F;IncIdeas&#x2F;BitterLesson.html" rel="nofollow noreferrer">http:&#x2F;&#x2F;www.incompleteideas.net&#x2F;IncIdeas&#x2F;BitterLesson.html</a></div><br/><div id="36820852" class="c"><input type="checkbox" id="c-36820852" checked=""/><div class="controls bullet"><span class="by">woah</span><span>|</span><a href="#36820606">parent</a><span>|</span><a href="#36820961">next</a><span>|</span><label class="collapse" for="c-36820852">[-]</label><label class="expand" for="c-36820852">[2 more]</label></div><br/><div class="children"><div class="content">Not an expert at all, but I believe that OpenAI uses this in some of their GPT apis which are meant for programmatic use. I&#x27;ve seen it theorized that offloading the rote grammar stuff to a simple process that is meant for it lets the LLM use it&#x27;s &quot;brainpower&quot; on the complicated stuff more effectively. No idea if this is true.</div><br/><div id="36821026" class="c"><input type="checkbox" id="c-36821026" checked=""/><div class="controls bullet"><span class="by">TechBro8615</span><span>|</span><a href="#36820606">root</a><span>|</span><a href="#36820852">parent</a><span>|</span><a href="#36820961">next</a><span>|</span><label class="collapse" for="c-36821026">[-]</label><label class="expand" for="c-36821026">[1 more]</label></div><br/><div class="children"><div class="content">It makes sense to my uninformed intuition, which is that a strict grammar reduces the search space for the token generation and so the AI can eliminate possibilities that would otherwise be ambiguous.</div><br/></div></div></div></div><div id="36820961" class="c"><input type="checkbox" id="c-36820961" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#36820606">parent</a><span>|</span><a href="#36820852">prev</a><span>|</span><a href="#36821529">next</a><span>|</span><label class="collapse" for="c-36820961">[-]</label><label class="expand" for="c-36820961">[1 more]</label></div><br/><div class="children"><div class="content">It may be a stop-gap, but its an important one as it is not obvious that LLMs in the next few years will &quot;organically&quot; solve their issues with generating text with constraints.</div><br/></div></div></div></div><div id="36821529" class="c"><input type="checkbox" id="c-36821529" checked=""/><div class="controls bullet"><span class="by">sandkoan</span><span>|</span><a href="#36820606">prev</a><span>|</span><a href="#36820653">next</a><span>|</span><label class="collapse" for="c-36821529">[-]</label><label class="expand" for="c-36821529">[1 more]</label></div><br/><div class="children"><div class="content">Also using a similar method: <a href="https:&#x2F;&#x2F;github.com&#x2F;automorphic-ai&#x2F;trex">https:&#x2F;&#x2F;github.com&#x2F;automorphic-ai&#x2F;trex</a><p>Playground: <a href="https:&#x2F;&#x2F;automorphic.ai&#x2F;playground" rel="nofollow noreferrer">https:&#x2F;&#x2F;automorphic.ai&#x2F;playground</a></div><br/></div></div><div id="36820653" class="c"><input type="checkbox" id="c-36820653" checked=""/><div class="controls bullet"><span class="by">painted-now</span><span>|</span><a href="#36821529">prev</a><span>|</span><a href="#36821404">next</a><span>|</span><label class="collapse" for="c-36820653">[-]</label><label class="expand" for="c-36820653">[5 more]</label></div><br/><div class="children"><div class="content">Can anyone recommend some paper or overview on how &quot;sampling&quot; &#x2F; &quot;decoding&quot; is done in the e2e neural network age? I know how decoding was done for machine translation and speech recognition back in the HMM times (i.e. <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Viterbi_algorithm" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Viterbi_algorithm</a> and <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Beam_search" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Beam_search</a>). These days I get the impression people just do &quot;greedy&quot; - but I don&#x27;t really know. Any recommendations for info on that topic?<p>Edit: Forgot Viterbi</div><br/><div id="36820711" class="c"><input type="checkbox" id="c-36820711" checked=""/><div class="controls bullet"><span class="by">spion</span><span>|</span><a href="#36820653">parent</a><span>|</span><a href="#36820880">next</a><span>|</span><label class="collapse" for="c-36820711">[-]</label><label class="expand" for="c-36820711">[3 more]</label></div><br/><div class="children"><div class="content">Its greedy and random :) Instead of a paper, I would recommend the algorithms of most LMM implementations (rwkv.cpp has a relatively clean implementation in python <a href="https:&#x2F;&#x2F;github.com&#x2F;saharNooby&#x2F;rwkv.cpp&#x2F;blob&#x2F;master&#x2F;rwkv&#x2F;sampling.py">https:&#x2F;&#x2F;github.com&#x2F;saharNooby&#x2F;rwkv.cpp&#x2F;blob&#x2F;master&#x2F;rwkv&#x2F;samp...</a>)</div><br/><div id="36820850" class="c"><input type="checkbox" id="c-36820850" checked=""/><div class="controls bullet"><span class="by">painted-now</span><span>|</span><a href="#36820653">root</a><span>|</span><a href="#36820711">parent</a><span>|</span><a href="#36820880">next</a><span>|</span><label class="collapse" for="c-36820850">[-]</label><label class="expand" for="c-36820850">[2 more]</label></div><br/><div class="children"><div class="content">I guess I need to sit down and study this stuff in more detail, but do I understand correctly that the code you shared makes the decisions for each position independently? I am just astonished that this produces any coherent output. Also it is not clear to me how the length of the output sequence is determined.</div><br/><div id="36820947" class="c"><input type="checkbox" id="c-36820947" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#36820653">root</a><span>|</span><a href="#36820850">parent</a><span>|</span><a href="#36820880">next</a><span>|</span><label class="collapse" for="c-36820947">[-]</label><label class="expand" for="c-36820947">[1 more]</label></div><br/><div class="children"><div class="content">Once the stop token is likeliest</div><br/></div></div></div></div></div></div><div id="36820880" class="c"><input type="checkbox" id="c-36820880" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#36820653">parent</a><span>|</span><a href="#36820711">prev</a><span>|</span><a href="#36821404">next</a><span>|</span><label class="collapse" for="c-36820880">[-]</label><label class="expand" for="c-36820880">[1 more]</label></div><br/><div class="children"><div class="content">Just reading through the GPT4 documentation it doesn’t seem like there’s a ton of difference with what you’ve mentioned.<p><a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;api-reference&#x2F;completions&#x2F;create" rel="nofollow noreferrer">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;api-reference&#x2F;completions&#x2F;c...</a><p>Of course we now know that GPT4 is a Mixture of Experts, so under the hood they’re parallelizing computation. They also include a way to modify the logits with presence&#x2F;frequency penalty terms.</div><br/></div></div></div></div><div id="36821404" class="c"><input type="checkbox" id="c-36821404" checked=""/><div class="controls bullet"><span class="by">Icko</span><span>|</span><a href="#36820653">prev</a><span>|</span><a href="#36821476">next</a><span>|</span><label class="collapse" for="c-36821404">[-]</label><label class="expand" for="c-36821404">[2 more]</label></div><br/><div class="children"><div class="content">How is this different from Guidance and LMQL?</div><br/><div id="36823108" class="c"><input type="checkbox" id="c-36823108" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#36821404">parent</a><span>|</span><a href="#36821476">next</a><span>|</span><label class="collapse" for="c-36823108">[-]</label><label class="expand" for="c-36823108">[1 more]</label></div><br/><div class="children"><div class="content">Looks like a tool Guidance could use to make better use of the sampling from a local llama model.</div><br/></div></div></div></div><div id="36821476" class="c"><input type="checkbox" id="c-36821476" checked=""/><div class="controls bullet"><span class="by">QuantumG</span><span>|</span><a href="#36821404">prev</a><span>|</span><a href="#36820936">next</a><span>|</span><label class="collapse" for="c-36821476">[-]</label><label class="expand" for="c-36821476">[3 more]</label></div><br/><div class="children"><div class="content">So, umm, if you want to walk BNF and emit likely tokens you can do that without any &quot;machine learning&quot; or whatever you want to call it. So what is being added here? Training to tie the prompt to the output?</div><br/><div id="36821670" class="c"><input type="checkbox" id="c-36821670" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#36821476">parent</a><span>|</span><a href="#36820936">next</a><span>|</span><label class="collapse" for="c-36821670">[-]</label><label class="expand" for="c-36821670">[2 more]</label></div><br/><div class="children"><div class="content">The difference is in the word “likely”. You can put unstructured data in the prompt and get structured data out. You could put in the beginning of a list and ask for a continuation.</div><br/><div id="36822107" class="c"><input type="checkbox" id="c-36822107" checked=""/><div class="controls bullet"><span class="by">QuantumG</span><span>|</span><a href="#36821476">root</a><span>|</span><a href="#36821670">parent</a><span>|</span><a href="#36820936">next</a><span>|</span><label class="collapse" for="c-36822107">[-]</label><label class="expand" for="c-36822107">[1 more]</label></div><br/><div class="children"><div class="content">I get that</div><br/></div></div></div></div></div></div><div id="36820936" class="c"><input type="checkbox" id="c-36820936" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#36821476">prev</a><span>|</span><a href="#36821121">next</a><span>|</span><label class="collapse" for="c-36820936">[-]</label><label class="expand" for="c-36820936">[2 more]</label></div><br/><div class="children"><div class="content">I am in love with this, I tried my hand at building a Constrained Text Generation Studio (<a href="https:&#x2F;&#x2F;github.com&#x2F;Hellisotherpeople&#x2F;Constrained-Text-Generation-Studio">https:&#x2F;&#x2F;github.com&#x2F;Hellisotherpeople&#x2F;Constrained-Text-Genera...</a>), and got published at COLING 2022 for my paper on it (<a href="https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;most-language-models-can-be-poets-too-an-ai-1" rel="nofollow noreferrer">https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;most-language-models-can-be...</a>), but I always knew that something like this or the related idea enumerated in this paper: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.03081" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.03081</a> was the way to go.<p>I will have to think about how I can build grammars that force things like syllable counts or syntactic rules. Current LLMs do very poorly on those kinds of tasks due to the tokenization schemes...</div><br/><div id="36821700" class="c"><input type="checkbox" id="c-36821700" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#36820936">parent</a><span>|</span><a href="#36821121">next</a><span>|</span><label class="collapse" for="c-36821700">[-]</label><label class="expand" for="c-36821700">[1 more]</label></div><br/><div class="children"><div class="content">I was surprised, but Nous Hermes does a half decent job at writing haikus.</div><br/></div></div></div></div><div id="36821121" class="c"><input type="checkbox" id="c-36821121" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#36820936">prev</a><span>|</span><a href="#36822013">next</a><span>|</span><label class="collapse" for="c-36821121">[-]</label><label class="expand" for="c-36821121">[1 more]</label></div><br/><div class="children"><div class="content">Has anyone tested FreeWilly2 (the new Llama2 fine-tune released today by Stable Foundation) on code generation?</div><br/></div></div><div id="36822013" class="c"><input type="checkbox" id="c-36822013" checked=""/><div class="controls bullet"><span class="by">ahupp</span><span>|</span><a href="#36821121">prev</a><span>|</span><a href="#36822058">next</a><span>|</span><label class="collapse" for="c-36822013">[-]</label><label class="expand" for="c-36822013">[2 more]</label></div><br/><div class="children"><div class="content">Interesting that the second commentor is Tobias Lütke, CEO of Shopify.</div><br/><div id="36822034" class="c"><input type="checkbox" id="c-36822034" checked=""/><div class="controls bullet"><span class="by">0xDEF</span><span>|</span><a href="#36822013">parent</a><span>|</span><a href="#36822058">next</a><span>|</span><label class="collapse" for="c-36822034">[-]</label><label class="expand" for="c-36822034">[1 more]</label></div><br/><div class="children"><div class="content">Also interesting how Shopify is making a lot of moves in this space using both the OpenAI APIs and using self-hosted models.</div><br/></div></div></div></div><div id="36822058" class="c"><input type="checkbox" id="c-36822058" checked=""/><div class="controls bullet"><span class="by">lachlan_gray</span><span>|</span><a href="#36822013">prev</a><span>|</span><a href="#36820896">next</a><span>|</span><label class="collapse" for="c-36822058">[-]</label><label class="expand" for="c-36822058">[2 more]</label></div><br/><div class="children"><div class="content">Something I’m wondering lately is if you are generating tokens fast enough, is restricting the logits actually worth it computationally? If tokens are cheap enough it might be more efficient to validate&#x2F;discard them as they come rather than place constraints on how they come out. I don’t know how this one works, but the sampling or renormalizing scheme would cost something too right?</div><br/><div id="36822118" class="c"><input type="checkbox" id="c-36822118" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#36822058">parent</a><span>|</span><a href="#36820896">next</a><span>|</span><label class="collapse" for="c-36822118">[-]</label><label class="expand" for="c-36822118">[1 more]</label></div><br/><div class="children"><div class="content">There is at least 6 orders of magnitude difference in computation cost of computing a token (pass through a multi-B model) and doing a single step of a program. Even if your validation is really naive, it&#x27;s hard to beat 6 orders of magnitude.<p>So no, you&#x27;re not generating tokens fast enough.</div><br/></div></div></div></div><div id="36820896" class="c"><input type="checkbox" id="c-36820896" checked=""/><div class="controls bullet"><span class="by">bavarianbob</span><span>|</span><a href="#36822058">prev</a><span>|</span><a href="#36820808">next</a><span>|</span><label class="collapse" for="c-36820896">[-]</label><label class="expand" for="c-36820896">[5 more]</label></div><br/><div class="children"><div class="content">Could someone help me with context? I&#x27;m OOTL and don&#x27;t understand what is going on here.</div><br/><div id="36821472" class="c"><input type="checkbox" id="c-36821472" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36820896">parent</a><span>|</span><a href="#36820808">next</a><span>|</span><label class="collapse" for="c-36821472">[-]</label><label class="expand" for="c-36821472">[4 more]</label></div><br/><div class="children"><div class="content">This can constrain an LLM&#x27;s output to an arbitrary grammar&#x2F;format <i>as it is generated</i>, rather than asking the model to output a specific format and hoping it outputs something valid.</div><br/><div id="36822254" class="c"><input type="checkbox" id="c-36822254" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#36820896">root</a><span>|</span><a href="#36821472">parent</a><span>|</span><a href="#36820808">next</a><span>|</span><label class="collapse" for="c-36822254">[-]</label><label class="expand" for="c-36822254">[3 more]</label></div><br/><div class="children"><div class="content">This is important for &quot;smaller&quot; models, because you don&#x27;t have to waste some of the potential &quot;intelligence&quot; (parameter space) on training it how to generate valid JSON or YAML or anything like that.</div><br/><div id="36823048" class="c"><input type="checkbox" id="c-36823048" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36820896">root</a><span>|</span><a href="#36822254">parent</a><span>|</span><a href="#36820808">next</a><span>|</span><label class="collapse" for="c-36823048">[-]</label><label class="expand" for="c-36823048">[2 more]</label></div><br/><div class="children"><div class="content">You still do... The model has to know JSON and YAML, its just more reliable when the generation is enforced by grammar</div><br/><div id="36823325" class="c"><input type="checkbox" id="c-36823325" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#36820896">root</a><span>|</span><a href="#36823048">parent</a><span>|</span><a href="#36820808">next</a><span>|</span><label class="collapse" for="c-36823325">[-]</label><label class="expand" for="c-36823325">[1 more]</label></div><br/><div class="children"><div class="content">Right, but there is a big difference between ”generally knows what JSON looks like and gets it right most of the time&quot; and &quot;generates perfect JSON every time&quot;.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36820808" class="c"><input type="checkbox" id="c-36820808" checked=""/><div class="controls bullet"><span class="by">meepmorp</span><span>|</span><a href="#36820896">prev</a><span>|</span><a href="#36820662">next</a><span>|</span><label class="collapse" for="c-36820808">[-]</label><label class="expand" for="c-36820808">[3 more]</label></div><br/><div class="children"><div class="content">Does anyone know Japanese well enough to comment on the output from the Japanese example?</div><br/><div id="36820950" class="c"><input type="checkbox" id="c-36820950" checked=""/><div class="controls bullet"><span class="by">vore</span><span>|</span><a href="#36820808">parent</a><span>|</span><a href="#36821510">next</a><span>|</span><label class="collapse" for="c-36820950">[-]</label><label class="expand" for="c-36820950">[1 more]</label></div><br/><div class="children"><div class="content">It is vaguely Japanese, I guess, but pretty incoherent:<p><pre><code>  1. What is the purpose?
  2. Remember the customer
  3. About the customer [incomplete sentence?]</code></pre></div><br/></div></div><div id="36821510" class="c"><input type="checkbox" id="c-36821510" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36820808">parent</a><span>|</span><a href="#36820950">prev</a><span>|</span><a href="#36820662">next</a><span>|</span><label class="collapse" for="c-36821510">[-]</label><label class="expand" for="c-36821510">[1 more]</label></div><br/><div class="children"><div class="content">Note that there are actual Japanese llama finetunes that would be much more coherent with these grammar constraints</div><br/></div></div></div></div><div id="36820662" class="c"><input type="checkbox" id="c-36820662" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#36820808">prev</a><span>|</span><label class="collapse" for="c-36820662">[-]</label><label class="expand" for="c-36820662">[3 more]</label></div><br/><div class="children"><div class="content">Ah finally, this was discussed a lot and is well overdue. Remains to be seen how well the models will adapt to this new constraint, though the demo seems promising.</div><br/><div id="36821005" class="c"><input type="checkbox" id="c-36821005" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#36820662">parent</a><span>|</span><label class="collapse" for="c-36821005">[-]</label><label class="expand" for="c-36821005">[2 more]</label></div><br/><div class="children"><div class="content">Isn’t this approach forcing the LLM to adapt? E.g. it is throwing tokens away that don’t match the grammar.</div><br/><div id="36821110" class="c"><input type="checkbox" id="c-36821110" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#36820662">root</a><span>|</span><a href="#36821005">parent</a><span>|</span><label class="collapse" for="c-36821110">[-]</label><label class="expand" for="c-36821110">[1 more]</label></div><br/><div class="children"><div class="content">Well the grammar will be correct as enforced by the sampler, but the content it&#x27;s filled with could be anything at all. Sort of how when you change the prompt template the output can be garbage for some models. I haven&#x27;t tried it out yet myself, but apparently even OpenAI&#x27;s implementation of this exact principle on their API still has function hallucination issues even with GPT 4.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>