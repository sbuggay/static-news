<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1721725254542" as="style"/><link rel="stylesheet" href="styles.css?v=1721725254542"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://johncarlosbaez.wordpress.com/2024/07/20/what-is-entropy/">What Is Entropy?</a> <span class="domain">(<a href="https://johncarlosbaez.wordpress.com">johncarlosbaez.wordpress.com</a>)</span></div><div class="subtext"><span>ainoobler</span> | <span>121 comments</span></div><br/><div><div id="41038810" class="c"><input type="checkbox" id="c-41038810" checked=""/><div class="controls bullet"><span class="by">Jun8</span><span>|</span><a href="#41039294">next</a><span>|</span><label class="collapse" for="c-41038810">[-]</label><label class="expand" for="c-41038810">[14 more]</label></div><br/><div class="children"><div class="content">A well known anecdote reported by Shannon:<p>&quot;My greatest concern was what to call it. I thought of calling it &#x27;information,&#x27; but the word was overly used, so I decided to call it &#x27;uncertainty.&#x27; When I discussed it with John von Neumann, he had a better idea. Von Neumann told me, &#x27;You should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, no one really knows what entropy really is, so in a debate you will always have the advantage.&#x27;&quot;<p>See the answers to this MathOverflow SE question (<a href="https:&#x2F;&#x2F;mathoverflow.net&#x2F;questions&#x2F;403036&#x2F;john-von-neumanns-remark-on-entropy" rel="nofollow">https:&#x2F;&#x2F;mathoverflow.net&#x2F;questions&#x2F;403036&#x2F;john-von-neumanns-...</a>) for references on the discussion whether Shannon&#x27;s entropy is the same as the one from thermodynamics.</div><br/><div id="41039156" class="c"><input type="checkbox" id="c-41039156" checked=""/><div class="controls bullet"><span class="by">BigParm</span><span>|</span><a href="#41038810">parent</a><span>|</span><a href="#41039294">next</a><span>|</span><label class="collapse" for="c-41039156">[-]</label><label class="expand" for="c-41039156">[13 more]</label></div><br/><div class="children"><div class="content">Von Neumann was the king of kings</div><br/><div id="41042803" class="c"><input type="checkbox" id="c-41042803" checked=""/><div class="controls bullet"><span class="by">penguin_booze</span><span>|</span><a href="#41038810">root</a><span>|</span><a href="#41039156">parent</a><span>|</span><a href="#41041821">next</a><span>|</span><label class="collapse" for="c-41042803">[-]</label><label class="expand" for="c-41042803">[3 more]</label></div><br/><div class="children"><div class="content">He&#x27;s a certified Martian: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Martians_(scientists)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Martians_(scientists)</a>.</div><br/><div id="41044018" class="c"><input type="checkbox" id="c-41044018" checked=""/><div class="controls bullet"><span class="by">zeristor</span><span>|</span><a href="#41038810">root</a><span>|</span><a href="#41042803">parent</a><span>|</span><a href="#41043004">next</a><span>|</span><label class="collapse" for="c-41044018">[-]</label><label class="expand" for="c-41044018">[1 more]</label></div><br/><div class="children"><div class="content">I was hoping the Wikipedia might explain why this might have been.</div><br/></div></div></div></div><div id="41041821" class="c"><input type="checkbox" id="c-41041821" checked=""/><div class="controls bullet"><span class="by">vinnyvichy</span><span>|</span><a href="#41038810">root</a><span>|</span><a href="#41039156">parent</a><span>|</span><a href="#41042803">prev</a><span>|</span><a href="#41041021">next</a><span>|</span><label class="collapse" for="c-41041821">[-]</label><label class="expand" for="c-41041821">[1 more]</label></div><br/><div class="children"><div class="content">So much so, he has his own entropy!<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Von_Neumann_entropy" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Von_Neumann_entropy</a></div><br/></div></div><div id="41041021" class="c"><input type="checkbox" id="c-41041021" checked=""/><div class="controls bullet"><span class="by">tonetegeatinst</span><span>|</span><a href="#41038810">root</a><span>|</span><a href="#41039156">parent</a><span>|</span><a href="#41041821">prev</a><span>|</span><a href="#41039294">next</a><span>|</span><label class="collapse" for="c-41041021">[-]</label><label class="expand" for="c-41041021">[8 more]</label></div><br/><div class="children"><div class="content">Its odd...as someone interested but not fully into the sciences I see his name pop up everywhere.</div><br/><div id="41041261" class="c"><input type="checkbox" id="c-41041261" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41038810">root</a><span>|</span><a href="#41041021">parent</a><span>|</span><a href="#41041243">next</a><span>|</span><label class="collapse" for="c-41041261">[-]</label><label class="expand" for="c-41041261">[2 more]</label></div><br/><div class="children"><div class="content">He was really brilliant, made contributions all over the place in the math&#x2F;physics&#x2F;tech field, and had a sort of wild and quirky personality that people love telling stories about.<p>A funny quote about him from a Edward “a guy with multiple equations named after him” Teller:<p>&gt; Edward Teller observed &quot;von Neumann would carry on a conversation with my 3-year-old son, and the two of them would talk as equals, and I sometimes wondered if he used the same principle when he talked to the rest of us.&quot;</div><br/><div id="41043823" class="c"><input type="checkbox" id="c-41043823" checked=""/><div class="controls bullet"><span class="by">strogonoff</span><span>|</span><a href="#41038810">root</a><span>|</span><a href="#41041261">parent</a><span>|</span><a href="#41041243">next</a><span>|</span><label class="collapse" for="c-41043823">[-]</label><label class="expand" for="c-41043823">[1 more]</label></div><br/><div class="children"><div class="content">Are there many von-Neumann-like multidisciplinaries nowadays? It feels like unless one is razor sharp fully into one field one  is not to be treated seriously by those who made careers in it (and who have the last word on it).</div><br/></div></div></div></div><div id="41041243" class="c"><input type="checkbox" id="c-41041243" checked=""/><div class="controls bullet"><span class="by">farias0</span><span>|</span><a href="#41038810">root</a><span>|</span><a href="#41041021">parent</a><span>|</span><a href="#41041261">prev</a><span>|</span><a href="#41042459">next</a><span>|</span><label class="collapse" for="c-41041243">[-]</label><label class="expand" for="c-41041243">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen many people arguing he&#x27;s the most intelligent person that ever lived</div><br/><div id="41041467" class="c"><input type="checkbox" id="c-41041467" checked=""/><div class="controls bullet"><span class="by">wrycoder</span><span>|</span><a href="#41038810">root</a><span>|</span><a href="#41041243">parent</a><span>|</span><a href="#41042459">next</a><span>|</span><label class="collapse" for="c-41041467">[-]</label><label class="expand" for="c-41041467">[2 more]</label></div><br/><div class="children"><div class="content">Some say Hungarians are actually aliens.</div><br/><div id="41041734" class="c"><input type="checkbox" id="c-41041734" checked=""/><div class="controls bullet"><span class="by">jack_pp</span><span>|</span><a href="#41038810">root</a><span>|</span><a href="#41041467">parent</a><span>|</span><a href="#41042459">next</a><span>|</span><label class="collapse" for="c-41041734">[-]</label><label class="expand" for="c-41041734">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;slatestarcodex.com&#x2F;2017&#x2F;05&#x2F;26&#x2F;the-atomic-bomb-considered-as-hungarian-high-school-science-fair-project&#x2F;" rel="nofollow">https:&#x2F;&#x2F;slatestarcodex.com&#x2F;2017&#x2F;05&#x2F;26&#x2F;the-atomic-bomb-consid...</a></div><br/></div></div></div></div></div></div><div id="41042459" class="c"><input type="checkbox" id="c-41042459" checked=""/><div class="controls bullet"><span class="by">rramadass</span><span>|</span><a href="#41038810">root</a><span>|</span><a href="#41041021">parent</a><span>|</span><a href="#41041243">prev</a><span>|</span><a href="#41041187">next</a><span>|</span><label class="collapse" for="c-41042459">[-]</label><label class="expand" for="c-41042459">[1 more]</label></div><br/><div class="children"><div class="content">An Introduction here : <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=IPMjVcLiNKc" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=IPMjVcLiNKc</a></div><br/></div></div><div id="41041187" class="c"><input type="checkbox" id="c-41041187" checked=""/><div class="controls bullet"><span class="by">otteromkram</span><span>|</span><a href="#41038810">root</a><span>|</span><a href="#41041021">parent</a><span>|</span><a href="#41042459">prev</a><span>|</span><a href="#41039294">next</a><span>|</span><label class="collapse" for="c-41041187">[-]</label><label class="expand" for="c-41041187">[1 more]</label></div><br/><div class="children"><div class="content">It would be less odd if you took a few moments out of your busy day and read something about the guy, no?</div><br/></div></div></div></div></div></div></div></div><div id="41039294" class="c"><input type="checkbox" id="c-41039294" checked=""/><div class="controls bullet"><span class="by">glial</span><span>|</span><a href="#41038810">prev</a><span>|</span><a href="#41038982">next</a><span>|</span><label class="collapse" for="c-41039294">[-]</label><label class="expand" for="c-41039294">[29 more]</label></div><br/><div class="children"><div class="content">I felt like I finally understood Shannon entropy when I realized that it&#x27;s a subjective quantity -- a property of the observer, not the observed.<p>The entropy of a variable X is the amount of information required to drive the observer&#x27;s uncertainty about the value of X to zero. As a correlate, your uncertainty and mine about the value of the same variable X could be different. This is trivially true, as we could each have received different information that about X.  H(X) should be H_{observer}(X), or even better, H_{observer, time}(X).<p>As clear as Shannon&#x27;s work is in other respects, he glosses over this.</div><br/><div id="41041552" class="c"><input type="checkbox" id="c-41041552" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#41039294">parent</a><span>|</span><a href="#41039753">next</a><span>|</span><label class="collapse" for="c-41041552">[-]</label><label class="expand" for="c-41041552">[3 more]</label></div><br/><div class="children"><div class="content">What&#x27;s often lost in the discussions about whether entropy is subjective or objective is that, if you dig a little deeper, information theory gives you powerful tools for relating the objective and the subjective.<p>Consider cross entropy of two distributions H[p, q] = -Σ p_i log q_i. For example maybe p is the real frequency distribution over outcomes from rolling some dice, and q is your belief distribution. You can see the p_i as representing the objective probabilities (sampled by actually rolling the dice) and the q_i as your subjective probabilities. The cross entropy is measuring something like how surprised you are on average when you observe an outcome.<p>The interesting thing is that H[p, p] &lt;= H[p, q], which means that if your belief distribution is wrong, your cross entropy will be higher than it would be if you had the right beliefs, q=p. This is guaranteed by the concavity of the logarithm. This gives you a way to compare beliefs: whichever q gets the lowest H[p,q] is closer to the truth.<p>You can even break cross entropy into two parts, corresponding to two kinds of uncertainty: H[p, q] = H[p] + D[q||p]. The first term is the entropy of p and it is the aleatoric uncertainty, the inherent randomness in the phenomenon you are trying to model. The second term is KL divergence and it tells you how much additional uncertainty you have as the result of having wrong beliefs, which you could call epistemic uncertainty.</div><br/><div id="41042992" class="c"><input type="checkbox" id="c-41042992" checked=""/><div class="controls bullet"><span class="by">bubblyworld</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41041552">parent</a><span>|</span><a href="#41039753">next</a><span>|</span><label class="collapse" for="c-41042992">[-]</label><label class="expand" for="c-41042992">[2 more]</label></div><br/><div class="children"><div class="content">Thanks, that&#x27;s an interesting perspective. It also highlights one of the weak points in the concept, I think, which is that this is only a tool for updating beliefs to the extent that the underlying probability space (&quot;ontology&quot; in this analogy) can actually &quot;model&quot; the phenomenon correctly!<p>It doesn&#x27;t seem to shed much light on when or how you could update the underlying probability space itself (or when to change your ontology in the belief setting).</div><br/><div id="41043544" class="c"><input type="checkbox" id="c-41043544" checked=""/><div class="controls bullet"><span class="by">bsmith</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41042992">parent</a><span>|</span><a href="#41039753">next</a><span>|</span><label class="collapse" for="c-41043544">[-]</label><label class="expand" for="c-41043544">[1 more]</label></div><br/><div class="children"><div class="content">Couldn&#x27;t you just add a control (PID&#x2F;Kalman filter&#x2F;etc) to coverage on a stability of some local &quot;most&quot; truth?</div><br/></div></div></div></div></div></div><div id="41039753" class="c"><input type="checkbox" id="c-41039753" checked=""/><div class="controls bullet"><span class="by">rachofsunshine</span><span>|</span><a href="#41039294">parent</a><span>|</span><a href="#41041552">prev</a><span>|</span><a href="#41042127">next</a><span>|</span><label class="collapse" for="c-41039753">[-]</label><label class="expand" for="c-41039753">[16 more]</label></div><br/><div class="children"><div class="content">This doesn&#x27;t really make entropy itself observer dependent. (Shannon) entropy is a property of a distribution. It&#x27;s just that when you&#x27;re measuring different observers&#x27; beliefs, you&#x27;re looking at different distributions (which can have different entropies the same way they can have different means, variances, etc).</div><br/><div id="41040022" class="c"><input type="checkbox" id="c-41040022" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41039753">parent</a><span>|</span><a href="#41040394">next</a><span>|</span><label class="collapse" for="c-41040022">[-]</label><label class="expand" for="c-41040022">[5 more]</label></div><br/><div class="children"><div class="content">Entropy is a property of a distribution, but since math does sometimes get applied, we also attach distributions to <i>things</i> (eg. the entropy of a random number generator, the entropy of a gas...). Then when we talk about the entropy of those things, those entropies are indeed subjective, because different subjects will attach different probability distributions to that system depending on their information about that system.</div><br/><div id="41041268" class="c"><input type="checkbox" id="c-41041268" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41040022">parent</a><span>|</span><a href="#41040585">next</a><span>|</span><label class="collapse" for="c-41041268">[-]</label><label class="expand" for="c-41041268">[2 more]</label></div><br/><div class="children"><div class="content">Some probability distributions are objective. The probability that my random number generator gives me a certain number is given by a certain formula. Describing it with another distribution would be wrong.<p>Another example, if you have an electron in a superposition of half spin-up and half spin-down, then the probability to measure up is objectively 50%.<p>Another example, GPT-2 is a probability distribution on sequences of integers. You can download this probability distribution. It doesn&#x27;t represent anyone&#x27;s beliefs. The distribution has a certain entropy. That entropy is an objective property of the distribution.</div><br/><div id="41042011" class="c"><input type="checkbox" id="c-41042011" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41041268">parent</a><span>|</span><a href="#41040585">next</a><span>|</span><label class="collapse" for="c-41042011">[-]</label><label class="expand" for="c-41042011">[1 more]</label></div><br/><div class="children"><div class="content">Of those, the quantum superposition is the only one that has a chance at being considered objective, and it&#x27;s still only &quot;objective&quot; in the sense that (as far as we know) your description provided as much information as anyone can possibly have about it, so nobody can have a more-informed opinion and all subjects agree.<p>The others are both partial-information problems which are very sensitive to knowing certain hidden-state information. Your random number generator gives you a number that <i>you</i> didn&#x27;t expect, and for which a formula describes your best guess based on available incomplete information, but the computer program that generated knew which one to choose and it would not have picked any other. Anyone who knew the hidden state of the RNG would also have assigned a different probability to that number being chosen.</div><br/></div></div></div></div><div id="41040585" class="c"><input type="checkbox" id="c-41040585" checked=""/><div class="controls bullet"><span class="by">stergios</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41040022">parent</a><span>|</span><a href="#41041268">prev</a><span>|</span><a href="#41040394">next</a><span>|</span><label class="collapse" for="c-41040585">[-]</label><label class="expand" for="c-41040585">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Entropy is a property of matter that measures the degree of randomization or disorder at the microscopic level&quot;, at least when considering the second law.</div><br/><div id="41040710" class="c"><input type="checkbox" id="c-41040710" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41040585">parent</a><span>|</span><a href="#41040394">next</a><span>|</span><label class="collapse" for="c-41040710">[-]</label><label class="expand" for="c-41040710">[1 more]</label></div><br/><div class="children"><div class="content">Right, but the very interesting thing is it turns out that what&#x27;s random to me might not be random to you! And the reason that &quot;microscopic&quot; is included is because that&#x27;s a shorthand for &quot;information you probably don&#x27;t have about a system, because your eyes aren&#x27;t that good, or even if they are, your brain ignored the fine details anyway.&quot;</div><br/></div></div></div></div></div></div><div id="41040394" class="c"><input type="checkbox" id="c-41040394" checked=""/><div class="controls bullet"><span class="by">davidmnoll</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41039753">parent</a><span>|</span><a href="#41040022">prev</a><span>|</span><a href="#41042163">next</a><span>|</span><label class="collapse" for="c-41040394">[-]</label><label class="expand" for="c-41040394">[9 more]</label></div><br/><div class="children"><div class="content">Right but in chemistry class the way it’s taught via Gibbs free energy etc. makes it seem as if it’s an intrinsic property.</div><br/><div id="41041200" class="c"><input type="checkbox" id="c-41041200" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41040394">parent</a><span>|</span><a href="#41041034">prev</a><span>|</span><a href="#41041142">next</a><span>|</span><label class="collapse" for="c-41041200">[-]</label><label class="expand" for="c-41041200">[5 more]</label></div><br/><div class="children"><div class="content">Entropy in physics is usually the Shannon entropy of the probability distribution over system microstates given known temperature and pressure. If the system is in equilibrium then this is objective.</div><br/><div id="41043113" class="c"><input type="checkbox" id="c-41043113" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41041200">parent</a><span>|</span><a href="#41041142">next</a><span>|</span><label class="collapse" for="c-41043113">[-]</label><label class="expand" for="c-41043113">[4 more]</label></div><br/><div class="children"><div class="content">Entropy in Physics is usually either the Boltzmann or Gibbs entropy, both of whom were dead before Shannon was born.</div><br/><div id="41043287" class="c"><input type="checkbox" id="c-41043287" checked=""/><div class="controls bullet"><span class="by">enugu</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41043113">parent</a><span>|</span><a href="#41041142">next</a><span>|</span><label class="collapse" for="c-41043287">[-]</label><label class="expand" for="c-41043287">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not a problem, as the GP&#x27;s post is trying to state a mathematical relation not a historical attribution. Often newer concepts shed light on older ones. As Baez&#x27;s article says, Gibbs entropy is Shannon&#x27;s entropy of an associated distribution(multiplied by the constant k).</div><br/><div id="41043464" class="c"><input type="checkbox" id="c-41043464" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41043287">parent</a><span>|</span><a href="#41041142">next</a><span>|</span><label class="collapse" for="c-41043464">[-]</label><label class="expand" for="c-41043464">[2 more]</label></div><br/><div class="children"><div class="content">It is a problem because all three come with a bagage. Almost none of the things discussed in this thread are invalid when discussing actual physical entropy even though the equations are superficially similar. And then there are lots of people being confidently wrong because they assume that it’s just one concept. It really is not.</div><br/><div id="41043772" class="c"><input type="checkbox" id="c-41043772" checked=""/><div class="controls bullet"><span class="by">enugu</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41043464">parent</a><span>|</span><a href="#41041142">next</a><span>|</span><label class="collapse" for="c-41043772">[-]</label><label class="expand" for="c-41043772">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t see how the connection is superficial. Even the classical macroscopic definition of entropy as ΔS=∫TdQ can be derived from the information theory perspective as Baez shows in article(using entropy maximizing distributions and Lagrange multipliers).  If you have a more specific critique, it would be good to discuss.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41041142" class="c"><input type="checkbox" id="c-41041142" checked=""/><div class="controls bullet"><span class="by">waveBidder</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41040394">parent</a><span>|</span><a href="#41041200">prev</a><span>|</span><a href="#41042163">next</a><span>|</span><label class="collapse" for="c-41041142">[-]</label><label class="expand" for="c-41041142">[2 more]</label></div><br/><div class="children"><div class="content">that&#x27;s actually the normal view, with saying both info and stat mech entropy are the same is an outlier, most popularized by Jaynes.</div><br/><div id="41042253" class="c"><input type="checkbox" id="c-41042253" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41041142">parent</a><span>|</span><a href="#41042163">next</a><span>|</span><label class="collapse" for="c-41042253">[-]</label><label class="expand" for="c-41042253">[1 more]</label></div><br/><div class="children"><div class="content">If information-theoretical and statistical mechanics entropies are NOT the same (or at least, deeply connected) then what stops us from having a little guy[0] sort all the particles in a gas to extract more energy from them?<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Maxwell%27s_demon" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Maxwell%27s_demon</a></div><br/></div></div></div></div></div></div><div id="41042163" class="c"><input type="checkbox" id="c-41042163" checked=""/><div class="controls bullet"><span class="by">IIAOPSW</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41039753">parent</a><span>|</span><a href="#41040394">prev</a><span>|</span><a href="#41042127">next</a><span>|</span><label class="collapse" for="c-41042163">[-]</label><label class="expand" for="c-41042163">[1 more]</label></div><br/><div class="children"><div class="content">Yeah but distributions are just the accounting tools to keep track of your entropy. If you are missing one bit of information about a system, your understanding of the system is some distribution with one bit of entropy. Like the original comment said, the entropy is the number of bits needed to fill in the unknowns and bring the uncertainty down to zero. Your coin flips may be unknown in advance to you, and thus you model it as a 50&#x2F;50 distribution, but in a deterministic universe the bits were present all along.</div><br/></div></div></div></div><div id="41042127" class="c"><input type="checkbox" id="c-41042127" checked=""/><div class="controls bullet"><span class="by">IIAOPSW</span><span>|</span><a href="#41039294">parent</a><span>|</span><a href="#41039753">prev</a><span>|</span><a href="#41039769">next</a><span>|</span><label class="collapse" for="c-41042127">[-]</label><label class="expand" for="c-41042127">[1 more]</label></div><br/><div class="children"><div class="content">To shorten this for you with my own (identical) understanding: &quot;entropy is just the name for the bits you don&#x27;t have&quot;.<p>Entropy + Information = Total bits in a complete description.</div><br/></div></div><div id="41039769" class="c"><input type="checkbox" id="c-41039769" checked=""/><div class="controls bullet"><span class="by">dist-epoch</span><span>|</span><a href="#41039294">parent</a><span>|</span><a href="#41042127">prev</a><span>|</span><a href="#41041784">next</a><span>|</span><label class="collapse" for="c-41039769">[-]</label><label class="expand" for="c-41039769">[2 more]</label></div><br/><div class="children"><div class="content">Trivial example: if you know the seed of a pseudo-random number generator, a sequence generated by it has very low entropy.<p>But if you don&#x27;t know the seed, the entropy is very high.</div><br/><div id="41040363" class="c"><input type="checkbox" id="c-41040363" checked=""/><div class="controls bullet"><span class="by">rustcleaner</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41039769">parent</a><span>|</span><a href="#41041784">next</a><span>|</span><label class="collapse" for="c-41040363">[-]</label><label class="expand" for="c-41040363">[1 more]</label></div><br/><div class="children"><div class="content">Theoretically, it&#x27;s still only the entropy of the sneed-space + time-space it could have been running in, right?</div><br/></div></div></div></div><div id="41041784" class="c"><input type="checkbox" id="c-41041784" checked=""/><div class="controls bullet"><span class="by">vinnyvichy</span><span>|</span><a href="#41039294">parent</a><span>|</span><a href="#41039769">prev</a><span>|</span><a href="#41039724">next</a><span>|</span><label class="collapse" for="c-41041784">[-]</label><label class="expand" for="c-41041784">[1 more]</label></div><br/><div class="children"><div class="content">Baez has a video (accompanying, imho), with slides<p><a href="https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=5phJVSWdWg4&amp;t=17m" rel="nofollow">https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=5phJVSWdWg4&amp;t=17m</a><p>He illustrates the derivation of Shannon entropy with pictures of trees</div><br/></div></div><div id="41039724" class="c"><input type="checkbox" id="c-41039724" checked=""/><div class="controls bullet"><span class="by">JumpCrisscross</span><span>|</span><a href="#41039294">parent</a><span>|</span><a href="#41041784">prev</a><span>|</span><a href="#41040365">next</a><span>|</span><label class="collapse" for="c-41039724">[-]</label><label class="expand" for="c-41039724">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>it&#x27;s a subjective quantity -- a property of the observer, not the observed</i><p>Shannon&#x27;s entropy is a property of the source-channel-receiver system.</div><br/><div id="41040293" class="c"><input type="checkbox" id="c-41040293" checked=""/><div class="controls bullet"><span class="by">glial</span><span>|</span><a href="#41039294">root</a><span>|</span><a href="#41039724">parent</a><span>|</span><a href="#41040365">next</a><span>|</span><label class="collapse" for="c-41040293">[-]</label><label class="expand" for="c-41040293">[1 more]</label></div><br/><div class="children"><div class="content">Can you explain this in more detail?<p>Entropy is calculated as a function of a probability distribution over possible messages or symbols. The sender might have a distribution P over possible symbols, and the receiver might have another distribution Q over possible symbols.  Then the &quot;true&quot; distribution over possible symbols might be another distribution yet, call it R. The mismatch between these is what leads to various inefficiencies in coding, decoding, etc [1]. But both P and Q are beliefs about R -- that is, they are properties of observers.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kullback–Leibler_divergence#Coding" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kullback–Leibler_divergence#Co...</a></div><br/></div></div></div></div><div id="41040365" class="c"><input type="checkbox" id="c-41040365" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#41039294">parent</a><span>|</span><a href="#41039724">prev</a><span>|</span><a href="#41042394">next</a><span>|</span><label class="collapse" for="c-41040365">[-]</label><label class="expand" for="c-41040365">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;archive.is&#x2F;9vnVq" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;9vnVq</a></div><br/></div></div><div id="41042394" class="c"><input type="checkbox" id="c-41042394" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#41039294">parent</a><span>|</span><a href="#41040365">prev</a><span>|</span><a href="#41042369">next</a><span>|</span><label class="collapse" for="c-41042394">[-]</label><label class="expand" for="c-41042394">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s an objective quantity, but you have to be very precise in stating what the quantity describes.<p>Unbroken egg? Low entropy.  There&#x27;s only one way the egg can exist in an unbroken state, and that&#x27;s it.  You could represent the state of the egg with a single bit.<p>Broken egg?  High entropy.  There are an arbitrarily-large number of ways that the pieces of a broken egg could land.<p>A list of the locations and orientations of each piece of the broken egg, sorted by latitude, longitude, and compass bearing?  Low entropy again; for any given instance of a broken egg, there&#x27;s only one way that list can be written.<p>Zip up the list you made?  High entropy again; the data in the .zip file is effectively random, and cannot be compressed significantly further.  Until you unzip it again...<p>Likewise, if you had to transmit the (uncompressed) list over a bandwidth-limited channel.  The person receiving the data can make no assumptions about its contents, so it might as well be random even though it has structure.  Its entropy is effectively high again.</div><br/></div></div></div></div><div id="41038982" class="c"><input type="checkbox" id="c-41038982" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#41039294">prev</a><span>|</span><a href="#41039209">next</a><span>|</span><label class="collapse" for="c-41038982">[-]</label><label class="expand" for="c-41038982">[8 more]</label></div><br/><div class="children"><div class="content">I really liked the approach my stat mech teacher used.  In nearly all situations, entropy just ends up being the log of the number of ways a system can be arranged (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Boltzmann%27s_entropy_formula" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Boltzmann%27s_entropy_formula</a>) although I found it easiest to think in terms of pairs of dice rolls.</div><br/><div id="41039821" class="c"><input type="checkbox" id="c-41039821" checked=""/><div class="controls bullet"><span class="by">petsfed</span><span>|</span><a href="#41038982">parent</a><span>|</span><a href="#41040780">next</a><span>|</span><label class="collapse" for="c-41039821">[-]</label><label class="expand" for="c-41039821">[3 more]</label></div><br/><div class="children"><div class="content">And this is what I prefer too, although with the clarification that its the number of ways that a system can be arranged <i>without changing its macroscopic properties</i>.<p>Its, unfortunately, not very compatible with Shannon&#x27;s usage in any but the shallowest sense, which is why it stays firmly in the land of physics.</div><br/><div id="41043623" class="c"><input type="checkbox" id="c-41043623" checked=""/><div class="controls bullet"><span class="by">kgwgk</span><span>|</span><a href="#41038982">root</a><span>|</span><a href="#41039821">parent</a><span>|</span><a href="#41043419">next</a><span>|</span><label class="collapse" for="c-41043623">[-]</label><label class="expand" for="c-41043623">[1 more]</label></div><br/><div class="children"><div class="content">&gt; not very compatible with Shannon&#x27;s usage in any but the shallowest sense<p>The connection is not so shallow, there are entire books based on it.<p>“The concept of information, intimately connected with that of probability, gives indeed insight on questions of statistical mechanics such as the meaning of irreversibility. This concept was introduced in statistical physics by Brillouin (1956) and Jaynes (1957) soon after its discovery by Shannon in 1948 (Shannon and Weaver, 1949). An immense literature has since then been published, ranging from research articles to textbooks. The variety of topics that belong to this field of science makes it impossible to give here a bibliography, and special searches are necessary for deepening the understanding of one or another aspect. For tutorial introductions, somewhat more detailed than the present one, see R. Balian (1991-92; 2004).”<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;cond-mat&#x2F;0501322" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;cond-mat&#x2F;0501322</a></div><br/></div></div><div id="41043419" class="c"><input type="checkbox" id="c-41043419" checked=""/><div class="controls bullet"><span class="by">enugu</span><span>|</span><a href="#41038982">root</a><span>|</span><a href="#41039821">parent</a><span>|</span><a href="#41043623">prev</a><span>|</span><a href="#41040780">next</a><span>|</span><label class="collapse" for="c-41043419">[-]</label><label class="expand" for="c-41043419">[1 more]</label></div><br/><div class="children"><div class="content">Assuming each of the N microstates for a given macrostate are equally possible with probability p=1&#x2F;N, the Shannon Entropy is -Σp.log(p) = -N.p.log(p)=-1.log(1&#x2F;N)=log(N), which is the physics interpretation.<p>In the continuous version, you would get log(V) where V is the volume in phase space occupied by the microstates for a given macrostate.<p>Liouville&#x27;s theorem that the volume is conserved in phase space implies that any macroscopic process can only move all the microstates from a macrostate A into a macrostate B only if the volume of B is bigger than the volume of A. This implies that the entropy of B should be bigger than the entropy of A which is the Second Law.</div><br/></div></div></div></div><div id="41040780" class="c"><input type="checkbox" id="c-41040780" checked=""/><div class="controls bullet"><span class="by">Lichtso</span><span>|</span><a href="#41038982">parent</a><span>|</span><a href="#41039821">prev</a><span>|</span><a href="#41041597">next</a><span>|</span><label class="collapse" for="c-41040780">[-]</label><label class="expand" for="c-41040780">[2 more]</label></div><br/><div class="children"><div class="content">The &quot;can be arranged&quot; is the tricky part. E.g. you might know from context that some states are impossible (where the probability distribution is zero), even though they combinatorially exist. That changes the entropy to you.<p>That is why information and entropy are different things. Entropy is what you know you do not know. That knowledge of the magnitude of the unknown is what is being quantified.<p>Also, the point where I think the article is wrong (or not concise enough) as it would include the unknown unknowns, which are not entropy IMO:<p>&gt; I claim it’s the amount of information we don’t know about a situation</div><br/><div id="41041074" class="c"><input type="checkbox" id="c-41041074" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41038982">root</a><span>|</span><a href="#41040780">parent</a><span>|</span><a href="#41041597">next</a><span>|</span><label class="collapse" for="c-41041074">[-]</label><label class="expand" for="c-41041074">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. If you want to reuse the term &quot;entropy&quot; in information theory, then fine. Just stop trying to make a physical analogy. It&#x27;s not rigorous.</div><br/></div></div></div></div><div id="41041597" class="c"><input type="checkbox" id="c-41041597" checked=""/><div class="controls bullet"><span class="by">akira2501</span><span>|</span><a href="#41038982">parent</a><span>|</span><a href="#41040780">prev</a><span>|</span><a href="#41040442">next</a><span>|</span><label class="collapse" for="c-41041597">[-]</label><label class="expand" for="c-41041597">[1 more]</label></div><br/><div class="children"><div class="content">I spend time just staring at the graph on this page.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Thermodynamic_beta" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Thermodynamic_beta</a></div><br/></div></div><div id="41040442" class="c"><input type="checkbox" id="c-41040442" checked=""/><div class="controls bullet"><span class="by">abetusk</span><span>|</span><a href="#41038982">parent</a><span>|</span><a href="#41041597">prev</a><span>|</span><a href="#41039209">next</a><span>|</span><label class="collapse" for="c-41040442">[-]</label><label class="expand" for="c-41040442">[1 more]</label></div><br/><div class="children"><div class="content">Also known as &quot;the number of bits to describe a system&quot;. For example, 2^N equally probable states, N bits to describe each state.</div><br/></div></div></div></div><div id="41039209" class="c"><input type="checkbox" id="c-41039209" checked=""/><div class="controls bullet"><span class="by">ooterness</span><span>|</span><a href="#41038982">prev</a><span>|</span><a href="#41039017">next</a><span>|</span><label class="collapse" for="c-41039209">[-]</label><label class="expand" for="c-41039209">[1 more]</label></div><br/><div class="children"><div class="content">For information theory, I&#x27;ve always thought of entropy as follows:<p>&quot;If you had a really smart compression algorithm, how many bits would it take to accurately represent this file?&quot;<p>i.e., Highly repetitive inputs compress well because they don&#x27;t have much entropy per bit. Modern compression algorithms are good enough on most data to be used as a reasonable approximation for the true entropy.</div><br/></div></div><div id="41039017" class="c"><input type="checkbox" id="c-41039017" checked=""/><div class="controls bullet"><span class="by">Tomte</span><span>|</span><a href="#41039209">prev</a><span>|</span><a href="#41041042">next</a><span>|</span><label class="collapse" for="c-41039017">[-]</label><label class="expand" for="c-41039017">[2 more]</label></div><br/><div class="children"><div class="content">PBS Spacetime‘s entropy playlist: <a href="https:&#x2F;&#x2F;youtube.com&#x2F;playlist?list=PLsPUh22kYmNCzNFNDwxIug8q1Zz0Mj60H&amp;si=y2XIvxDJOFyCpfIu" rel="nofollow">https:&#x2F;&#x2F;youtube.com&#x2F;playlist?list=PLsPUh22kYmNCzNFNDwxIug8q1...</a></div><br/><div id="41040358" class="c"><input type="checkbox" id="c-41040358" checked=""/><div class="controls bullet"><span class="by">foobarian</span><span>|</span><a href="#41039017">parent</a><span>|</span><a href="#41041042">next</a><span>|</span><label class="collapse" for="c-41040358">[-]</label><label class="expand" for="c-41040358">[1 more]</label></div><br/><div class="children"><div class="content">A bit off-color but classic: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=wgltMtf1JhY" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=wgltMtf1JhY</a></div><br/></div></div></div></div><div id="41041042" class="c"><input type="checkbox" id="c-41041042" checked=""/><div class="controls bullet"><span class="by">yellowcake0</span><span>|</span><a href="#41039017">prev</a><span>|</span><a href="#41043097">next</a><span>|</span><label class="collapse" for="c-41041042">[-]</label><label class="expand" for="c-41041042">[1 more]</label></div><br/><div class="children"><div class="content">Information entropy is literally the strict lower bound on how efficiently information can be communicated (expected number of transmitted bits) if the probability distribution which generates this information is known, that&#x27;s it. Even in contexts such as calculating the information entropy of a bit string, or the English language, you&#x27;re just taking this data and constructing some empirical probability distribution from it using the relative frequencies of zeros and ones or letters or n-grams or whatever, and then calculating the entropy of that distribution.<p>I can&#x27;t say I&#x27;m overly fond of Baez&#x27;s definition, but far be it from me to question someone of his stature.</div><br/></div></div><div id="41043097" class="c"><input type="checkbox" id="c-41043097" checked=""/><div class="controls bullet"><span class="by">GoblinSlayer</span><span>|</span><a href="#41041042">prev</a><span>|</span><a href="#41043453">next</a><span>|</span><label class="collapse" for="c-41043097">[-]</label><label class="expand" for="c-41043097">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s fundamental nature of entropy, but as usual it&#x27;s not very enlightening for poor monkey brain, so to explain you need to enumerate all its high level behavior, but its high level behavior is accidental and can&#x27;t be summarized in a concise form.</div><br/></div></div><div id="41043453" class="c"><input type="checkbox" id="c-41043453" checked=""/><div class="controls bullet"><span class="by">ctafur</span><span>|</span><a href="#41043097">prev</a><span>|</span><a href="#41039440">next</a><span>|</span><label class="collapse" for="c-41043453">[-]</label><label class="expand" for="c-41043453">[2 more]</label></div><br/><div class="children"><div class="content">The way I understand it is with an analogy to probability. To me, events are to microscopic states like random variable is to entropy.</div><br/><div id="41043548" class="c"><input type="checkbox" id="c-41043548" checked=""/><div class="controls bullet"><span class="by">ctafur</span><span>|</span><a href="#41043453">parent</a><span>|</span><a href="#41039440">next</a><span>|</span><label class="collapse" for="c-41043548">[-]</label><label class="expand" for="c-41043548">[1 more]</label></div><br/><div class="children"><div class="content">My first contact with entropy was in chemistry and thermodynamics and I didn&#x27;t get it. Actually I didn&#x27;t get anything from engineering thermodynamics books such as Çengel and so.<p>You must go to statistical mechanics or information theory to understand entropy. Or trying these PRICELESS NOTES from Prof. Suo: <a href="https:&#x2F;&#x2F;docs.google.com&#x2F;document&#x2F;d&#x2F;1UMwpoDRZLlawWlL2Dz6YEomyhSNveQHp6pt7AUiP37Q&#x2F;edit" rel="nofollow">https:&#x2F;&#x2F;docs.google.com&#x2F;document&#x2F;d&#x2F;1UMwpoDRZLlawWlL2Dz6YEomy...</a></div><br/></div></div></div></div><div id="41039440" class="c"><input type="checkbox" id="c-41039440" checked=""/><div class="controls bullet"><span class="by">eointierney</span><span>|</span><a href="#41043453">prev</a><span>|</span><a href="#41041435">next</a><span>|</span><label class="collapse" for="c-41039440">[-]</label><label class="expand" for="c-41039440">[1 more]</label></div><br/><div class="children"><div class="content">Ah JCB, how I love your writing, you are always so very generous.<p>Your This Week&#x27;s Finds were a hugely enjoyable part of my undergraduate education and beyond.<p>Thank you again.</div><br/></div></div><div id="41041435" class="c"><input type="checkbox" id="c-41041435" checked=""/><div class="controls bullet"><span class="by">utkarsh858</span><span>|</span><a href="#41039440">prev</a><span>|</span><a href="#41041509">next</a><span>|</span><label class="collapse" for="c-41041435">[-]</label><label class="expand" for="c-41041435">[1 more]</label></div><br/><div class="children"><div class="content">I sometimes ponder where new entropy&#x2F;randomness is coming from, like if we take the earliest state of universe as an infinitely dense point particle which expanded. So there must be some randomness or say variety which led it to expand in a non uniform way which led to the dominance of matter over anti-matter, or creation of galaxies, clusters etc. 
If we take an isolated system in which certain static particles are present, will there be the case that a small subset of the particles will get motion and this introduce entropy? Can entropy be induced automatically, atleast on a quantum level? 
If anyone can help me explain that it will be very helpful and thus can help explain origin of universe in a better way.</div><br/></div></div><div id="41041448" class="c"><input type="checkbox" id="c-41041448" checked=""/><div class="controls bullet"><span class="by">tasteslikenoise</span><span>|</span><a href="#41041509">prev</a><span>|</span><a href="#41041772">next</a><span>|</span><label class="collapse" for="c-41041448">[-]</label><label class="expand" for="c-41041448">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve always favored this down-to-earth characterization of the entropy of a discrete probability distribution. (I&#x27;m a big fan of John Baez&#x27;s writing, but I was surprised glancing through the PDF to find that he doesn&#x27;t seem to mention this viewpoint.)<p>Think of the distribution as a histogram over some bins. Then, the entropy is a measurement of, if I throw many many balls at random into those bins, the probability that the distribution of balls over bins ends up looking like that histogram. What you usually expect to see is a uniform distribution of balls over bins, so the entropy measures the probability of other rare events (in the language of probability theory, &quot;large deviations&quot; from that typical behavior).<p>More specifically, if P = (P1, ..., Pk) is some distribution, then the probability that throwing N balls (for N very large) gives a histogram looking like P is about 2^(-N * [log(k) - H(P)]), where H(P) is the entropy. When P is the uniform distribution, then H(P) = log(k), the exponent is zero, and the estimate is 1, which says that by far the most likely histogram is the uniform one. That is the largest possible entropy, so any other histogram has probability 2^(-c*N) of appearing for some c &gt; 0, i.e., is very unlikely and exponentially moreso the more balls we throw, but the entropy measures just how much. &quot;Less uniform&quot; distributions are less likely, so the entropy also measures a certain notion of uniformity. In large deviations theory this specific claim is called &quot;Sanov&#x27;s theorem&quot; and the role the entropy plays is that of a &quot;rate function.&quot;<p>The counting interpretation of entropy that some people are talking about is related, at least at a high level, because the probability in Sanov&#x27;s theorem is the number of outcomes that &quot;look like P&quot; divided by the total number, so the numerator there is indeed counting the number of configurations (in this case of balls and bins) having a particular property (in this case looking like P).<p>There are lots of equivalent definitions and they have different virtues, generalizations, etc, but I find this one especially helpful for dispelling the air of mystery around entropy.</div><br/><div id="41041716" class="c"><input type="checkbox" id="c-41041716" checked=""/><div class="controls bullet"><span class="by">vinnyvichy</span><span>|</span><a href="#41041448">parent</a><span>|</span><a href="#41041772">next</a><span>|</span><label class="collapse" for="c-41041716">[-]</label><label class="expand" for="c-41041716">[2 more]</label></div><br/><div class="children"><div class="content">Hey did you want to say <i>relative entropy</i> ~ rate function ~ KL divergence. Might be more familiar to ML enthusiasts here, get them to be curious about Sanov or large deviations.</div><br/><div id="41041964" class="c"><input type="checkbox" id="c-41041964" checked=""/><div class="controls bullet"><span class="by">tasteslikenoise</span><span>|</span><a href="#41041448">root</a><span>|</span><a href="#41041716">parent</a><span>|</span><a href="#41041772">next</a><span>|</span><label class="collapse" for="c-41041964">[-]</label><label class="expand" for="c-41041964">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s right, here log(k) - H(p) is really the relative entropy (or KL divergence) between p and the uniform distribution, and all the same stuff is true for a different &quot;reference distribution&quot; of the probabilities of balls landing in each bin.<p>For discrete distributions the &quot;absolute entropy&quot; (just sum of -p log(p) as it shows up in Shannon entropy or statistical mechanics) is in this way really a special case of relative entropy. For continuous distributions, say over real numbers, the analogous quantity (integral of -p log(p)) isn&#x27;t a relative entropy since there&#x27;s no &quot;uniform distribution over all real numbers&quot;. This still plays an important role in various situations and calculations...but, at least to my mind, it&#x27;s a formally similar but conceptually separate object.</div><br/></div></div></div></div></div></div><div id="41041772" class="c"><input type="checkbox" id="c-41041772" checked=""/><div class="controls bullet"><span class="by">vinnyvichy</span><span>|</span><a href="#41041448">prev</a><span>|</span><a href="#41039374">next</a><span>|</span><label class="collapse" for="c-41041772">[-]</label><label class="expand" for="c-41041772">[1 more]</label></div><br/><div class="children"><div class="content">The book might disappoint some..<p>&gt;I have largely avoided the second law of thermodynamics ... Thus, the aspects of entropy most beloved by physics popularizers will not be found here.<p>But personally, this bit is the most exciting to me.<p>&gt;I have tried to say as little as possible about quantum mechanics, to keep the physics prerequisites low. However, Planck’s constant shows up in the formulas for the entropy of the three classical systems mentioned above. The reason for this is fascinating: Planck’s constant provides a unit of volume in position-momentum space, which is necessary to define the entropy of these systems. Thus, we need a tiny bit of quantum mechanics to get a good approximate formula for the entropy of hydrogen, even if we are trying our best to treat this gas classically.</div><br/></div></div><div id="41039374" class="c"><input type="checkbox" id="c-41039374" checked=""/><div class="controls bullet"><span class="by">niemandhier</span><span>|</span><a href="#41041772">prev</a><span>|</span><a href="#41042824">next</a><span>|</span><label class="collapse" for="c-41039374">[-]</label><label class="expand" for="c-41039374">[1 more]</label></div><br/><div class="children"><div class="content">My goto source for understanding entropy:
<a href="http:&#x2F;&#x2F;philsci-archive.pitt.edu&#x2F;8592&#x2F;1&#x2F;EntropyPaperFinal.pdf" rel="nofollow">http:&#x2F;&#x2F;philsci-archive.pitt.edu&#x2F;8592&#x2F;1&#x2F;EntropyPaperFinal.pdf</a></div><br/></div></div><div id="41042824" class="c"><input type="checkbox" id="c-41042824" checked=""/><div class="controls bullet"><span class="by">foobarbecue</span><span>|</span><a href="#41039374">prev</a><span>|</span><a href="#41039076">next</a><span>|</span><label class="collapse" for="c-41042824">[-]</label><label class="expand" for="c-41042824">[2 more]</label></div><br/><div class="children"><div class="content">How do you get to the actual book &#x2F; tweets? The link just takes me back to the forward...</div><br/><div id="41043554" class="c"><input type="checkbox" id="c-41043554" checked=""/><div class="controls bullet"><span class="by">vishnugupta</span><span>|</span><a href="#41042824">parent</a><span>|</span><a href="#41039076">next</a><span>|</span><label class="collapse" for="c-41043554">[-]</label><label class="expand" for="c-41043554">[1 more]</label></div><br/><div class="children"><div class="content"><a href="http:&#x2F;&#x2F;math.ucr.edu&#x2F;home&#x2F;baez&#x2F;what_is_entropy.pdf" rel="nofollow">http:&#x2F;&#x2F;math.ucr.edu&#x2F;home&#x2F;baez&#x2F;what_is_entropy.pdf</a></div><br/></div></div></div></div><div id="41039076" class="c"><input type="checkbox" id="c-41039076" checked=""/><div class="controls bullet"><span class="by">drojas</span><span>|</span><a href="#41042824">prev</a><span>|</span><a href="#41040292">next</a><span>|</span><label class="collapse" for="c-41039076">[-]</label><label class="expand" for="c-41039076">[3 more]</label></div><br/><div class="children"><div class="content">My definition: Entropy is a measure of the accumulation of non-reversible energy transfers.<p>Side note: All reversible energy transfers involve an increase in potential energy. All non-reversible energy transfers involve a decrease in potential energy.</div><br/><div id="41039223" class="c"><input type="checkbox" id="c-41039223" checked=""/><div class="controls bullet"><span class="by">snarkconjecture</span><span>|</span><a href="#41039076">parent</a><span>|</span><a href="#41041667">next</a><span>|</span><label class="collapse" for="c-41039223">[-]</label><label class="expand" for="c-41039223">[1 more]</label></div><br/><div class="children"><div class="content">That definition doesn&#x27;t work well because you can have changes in entropy even if no energy is transferred, e.g. by exchanging some other conserved quantity.<p>The side note is wrong in letter and spirit; turning potential energy into heat is one way for something to be irreversible, but neither of those statements is true.<p>For example, consider an iron ball being thrown sideways. It hits a pile of sand and stops. The iron ball is not affected structurally, but its kinetic energy is transferred (almost entirely) to heat energy. If the ball is thrown slightly upwards, potential energy increases but the process is still irreversible.<p>Also, the changes of potential energy in corresponding parts of two Carnot cycles are directionally the same, even if one is ideal (reversible) and one is not (irreversible).</div><br/></div></div></div></div><div id="41040292" class="c"><input type="checkbox" id="c-41040292" checked=""/><div class="controls bullet"><span class="by">zoenolan</span><span>|</span><a href="#41039076">prev</a><span>|</span><a href="#41039860">next</a><span>|</span><label class="collapse" for="c-41040292">[-]</label><label class="expand" for="c-41040292">[1 more]</label></div><br/><div class="children"><div class="content">Hawking on the subject<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;wgltMtf1JhY" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;wgltMtf1JhY</a></div><br/></div></div><div id="41039860" class="c"><input type="checkbox" id="c-41039860" checked=""/><div class="controls bullet"><span class="by">dmn322</span><span>|</span><a href="#41040292">prev</a><span>|</span><a href="#41039381">next</a><span>|</span><label class="collapse" for="c-41039860">[-]</label><label class="expand" for="c-41039860">[1 more]</label></div><br/><div class="children"><div class="content">This seems like a great resource for referencing the various definitions.  I&#x27;ve tried my hand at developing an intuitive understanding: <a href="https:&#x2F;&#x2F;spacechimplives.substack.com&#x2F;p&#x2F;observers-and-entropy" rel="nofollow">https:&#x2F;&#x2F;spacechimplives.substack.com&#x2F;p&#x2F;observers-and-entropy</a>.  TLDR - it&#x27;s an artifact of the model we&#x27;re using.  In the thermodynamic definition, the energy accounted for in the terms of our model is information.  The energy that&#x27;s not is entropic energy.  Hence why it&#x27;s not &quot;useable&quot; energy, and the process isn&#x27;t reversible.</div><br/></div></div><div id="41039381" class="c"><input type="checkbox" id="c-41039381" checked=""/><div class="controls bullet"><span class="by">prof-dr-ir</span><span>|</span><a href="#41039860">prev</a><span>|</span><a href="#41041753">next</a><span>|</span><label class="collapse" for="c-41039381">[-]</label><label class="expand" for="c-41039381">[22 more]</label></div><br/><div class="children"><div class="content">If I would write a book with that title then I would get to the point a bit faster, probably as follows.<p>Entropy is <i>just</i> a number you can associate with a probability distribution. If the distribution is discrete, so you have a set p_i, i = 1..n, which are each positive and sum to 1, then the definition is:<p>S = - sum_i p_i log( p_i )<p>Mathematically we say that entropy is a real-valued function on the space of probability distributions. (Elementary exercises: show that S &gt;= 0 and it is maximized on the uniform distribution.)<p>That is it. I think there is little need for all the mystery.</div><br/><div id="41040045" class="c"><input type="checkbox" id="c-41040045" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#41039381">parent</a><span>|</span><a href="#41042957">next</a><span>|</span><label class="collapse" for="c-41040045">[-]</label><label class="expand" for="c-41040045">[3 more]</label></div><br/><div class="children"><div class="content">So the only thing you need to know about entropy is that it&#x27;s <i>a real-valued number you can associate with a probability distribution</i>? And that&#x27;s it? I disagree. There are several numbers that can be associated with probability distribution, and entropy is an especially useful one, but to understand why entropy is useful, or why you&#x27;d use that function instead of a different one, you&#x27;d need to know a few more things than just what you&#x27;ve written here.</div><br/><div id="41043018" class="c"><input type="checkbox" id="c-41043018" checked=""/><div class="controls bullet"><span class="by">FabHK</span><span>|</span><a href="#41039381">root</a><span>|</span><a href="#41040045">parent</a><span>|</span><a href="#41040994">next</a><span>|</span><label class="collapse" for="c-41043018">[-]</label><label class="expand" for="c-41043018">[1 more]</label></div><br/><div class="children"><div class="content">In particular, the expectation (or variance) of a real-valued random variable can also be seen as &quot;a real-valued number you can associate with a probability distribution&quot;.<p>Thus, GP&#x27;s statement is basically: &quot;entropy is like expectation, but different&quot;.</div><br/></div></div><div id="41040994" class="c"><input type="checkbox" id="c-41040994" checked=""/><div class="controls bullet"><span class="by">Maxatar</span><span>|</span><a href="#41039381">root</a><span>|</span><a href="#41040045">parent</a><span>|</span><a href="#41043018">prev</a><span>|</span><a href="#41042957">next</a><span>|</span><label class="collapse" for="c-41040994">[-]</label><label class="expand" for="c-41040994">[1 more]</label></div><br/><div class="children"><div class="content">Exactly, saying that&#x27;s all there is to know about entropy is like saying all you need to know about chess are the rules and all you need to know about programming is the syntax&#x2F;semantics.<p>Knowing the plain definition or the rules is nothing but a superficial understanding of the subject. Knowing how to use the rules to actually do something meaningful, having a strategy, that&#x27;s where meaningful knowledge lies.</div><br/></div></div></div></div><div id="41042957" class="c"><input type="checkbox" id="c-41042957" checked=""/><div class="controls bullet"><span class="by">bubblyworld</span><span>|</span><a href="#41039381">parent</a><span>|</span><a href="#41040045">prev</a><span>|</span><a href="#41039673">next</a><span>|</span><label class="collapse" for="c-41042957">[-]</label><label class="expand" for="c-41042957">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for defining it rigorously. I think people are getting offended on John Baez&#x27;s behalf because his book obviously covers a lot more - like <i>why</i> does this particular number seem to be so useful in so many different contexts? How could you have motivated it a priori? Etcetera, although I suspect you know all this already.<p>But I think you&#x27;re right that a clear focus on the maths is useful for dispelling misconceptions about entropy.</div><br/><div id="41043170" class="c"><input type="checkbox" id="c-41043170" checked=""/><div class="controls bullet"><span class="by">kgwgk</span><span>|</span><a href="#41039381">root</a><span>|</span><a href="#41042957">parent</a><span>|</span><a href="#41039673">next</a><span>|</span><label class="collapse" for="c-41043170">[-]</label><label class="expand" for="c-41043170">[1 more]</label></div><br/><div class="children"><div class="content">Misconceptions about entropy are misconceptions about physics. You can’t dispell them  focusing on the maths and ignoring the physics entirely - especially if you just write an equation without any conceptual discussion, not even mathematical.</div><br/></div></div></div></div><div id="41039673" class="c"><input type="checkbox" id="c-41039673" checked=""/><div class="controls bullet"><span class="by">rachofsunshine</span><span>|</span><a href="#41039381">parent</a><span>|</span><a href="#41042957">prev</a><span>|</span><a href="#41039409">next</a><span>|</span><label class="collapse" for="c-41039673">[-]</label><label class="expand" for="c-41039673">[3 more]</label></div><br/><div class="children"><div class="content">The problem is that this doesn&#x27;t get at many of the intuitive properties of entropy.<p>A different explanation (based on macro- and micro-states) makes it intuitively obvious why entropy is non-decreasing with time or, with a little more depth, what entropy has to do with temperature.</div><br/><div id="41041326" class="c"><input type="checkbox" id="c-41041326" checked=""/><div class="controls bullet"><span class="by">mjw_byrne</span><span>|</span><a href="#41039381">root</a><span>|</span><a href="#41039673">parent</a><span>|</span><a href="#41039963">next</a><span>|</span><label class="collapse" for="c-41041326">[-]</label><label class="expand" for="c-41041326">[1 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t strike me as a problem. Definitions are often highly abstract and counterintuitive, with much study required to understand at an intuitive level what motivates them. Rigour and intuition are often competing concerns, and I think definitions should favour the former. The definition of compactness in topology, or indeed just the definition of a topological space, are examples of this - at face value, they&#x27;re bizarre. You have to muck around a fair bit to understand why they cut so brilliantly to the heart of the thing.</div><br/></div></div><div id="41039963" class="c"><input type="checkbox" id="c-41039963" checked=""/><div class="controls bullet"><span class="by">prof-dr-ir</span><span>|</span><a href="#41039381">root</a><span>|</span><a href="#41039673">parent</a><span>|</span><a href="#41041326">prev</a><span>|</span><a href="#41039409">next</a><span>|</span><label class="collapse" for="c-41039963">[-]</label><label class="expand" for="c-41039963">[1 more]</label></div><br/><div class="children"><div class="content">The above evidently only suffices as a definition, not as an entire course. My point was just that I don&#x27;t think any other introduction beats this one, especially for a book with the given title.<p>In particular it has always been my starting point whenever I introduce (the entropy of) macro- and micro-states in my statistical physics course.</div><br/></div></div></div></div><div id="41039409" class="c"><input type="checkbox" id="c-41039409" checked=""/><div class="controls bullet"><span class="by">kgwgk</span><span>|</span><a href="#41039381">parent</a><span>|</span><a href="#41039673">prev</a><span>|</span><a href="#41041554">next</a><span>|</span><label class="collapse" for="c-41039409">[-]</label><label class="expand" for="c-41039409">[4 more]</label></div><br/><div class="children"><div class="content">That covers one and a half of the twelve points he discusses.</div><br/><div id="41039504" class="c"><input type="checkbox" id="c-41039504" checked=""/><div class="controls bullet"><span class="by">prof-dr-ir</span><span>|</span><a href="#41039381">root</a><span>|</span><a href="#41039409">parent</a><span>|</span><a href="#41041554">next</a><span>|</span><label class="collapse" for="c-41039504">[-]</label><label class="expand" for="c-41039504">[3 more]</label></div><br/><div class="children"><div class="content">Correct! And it took me just one paragraph, not the 18 pages of meandering (and I think confusing) text that it takes the author of the pdf to introduce the same idea.</div><br/><div id="41039641" class="c"><input type="checkbox" id="c-41039641" checked=""/><div class="controls bullet"><span class="by">kgwgk</span><span>|</span><a href="#41039381">root</a><span>|</span><a href="#41039504">parent</a><span>|</span><a href="#41041554">next</a><span>|</span><label class="collapse" for="c-41039641">[-]</label><label class="expand" for="c-41039641">[2 more]</label></div><br/><div class="children"><div class="content">You didn’t introduce any idea. You said it’s “just a number” and wrote down a formula without any explanation or justification.<p>I concede that it was much shorter though. Well done!</div><br/><div id="41040404" class="c"><input type="checkbox" id="c-41040404" checked=""/><div class="controls bullet"><span class="by">bdjsiqoocwk</span><span>|</span><a href="#41039381">root</a><span>|</span><a href="#41039641">parent</a><span>|</span><a href="#41041554">next</a><span>|</span><label class="collapse" for="c-41040404">[-]</label><label class="expand" for="c-41040404">[1 more]</label></div><br/><div class="children"><div class="content">Haha you reminded me of that idea in software engineering that &quot;it&#x27;s easy to make an algorithm faster if you accept that at times it might output the wrong result; in fact you can make infinitely fast&quot;</div><br/></div></div></div></div></div></div></div></div><div id="41041554" class="c"><input type="checkbox" id="c-41041554" checked=""/><div class="controls bullet"><span class="by">kaashif</span><span>|</span><a href="#41039381">parent</a><span>|</span><a href="#41039409">prev</a><span>|</span><a href="#41040688">next</a><span>|</span><label class="collapse" for="c-41041554">[-]</label><label class="expand" for="c-41041554">[1 more]</label></div><br/><div class="children"><div class="content">That definition is on page 18, I agree it could&#x27;ve been reached a bit faster but a lot of the preceding material is motivation, puzzles, and examples.<p>This definition isn&#x27;t the end goal, the physics things are.</div><br/></div></div><div id="41040688" class="c"><input type="checkbox" id="c-41040688" checked=""/><div class="controls bullet"><span class="by">senderista</span><span>|</span><a href="#41039381">parent</a><span>|</span><a href="#41041554">prev</a><span>|</span><a href="#41042740">next</a><span>|</span><label class="collapse" for="c-41040688">[-]</label><label class="expand" for="c-41040688">[1 more]</label></div><br/><div class="children"><div class="content">Many students will want to know where the minus sign comes from. I like to write the formula instead as S = sum_i p_i log( 1 &#x2F; p_i ), where (1 &#x2F; p_i) is the &quot;surprise&quot; (i.e., expected number of trials before first success) associated with a given outcome (or symbol), and we average it over all outcomes (i.e., weight it by the probability of the outcome). We take the log of the &quot;surprise&quot; because entropy is an extensive quantity, so we want it to be additive.</div><br/></div></div><div id="41042740" class="c"><input type="checkbox" id="c-41042740" checked=""/><div class="controls bullet"><span class="by">klysm</span><span>|</span><a href="#41039381">parent</a><span>|</span><a href="#41040688">prev</a><span>|</span><a href="#41039827">next</a><span>|</span><label class="collapse" for="c-41042740">[-]</label><label class="expand" for="c-41042740">[1 more]</label></div><br/><div class="children"><div class="content">The definition by itself without intuition of application is of little use</div><br/></div></div><div id="41039827" class="c"><input type="checkbox" id="c-41039827" checked=""/><div class="controls bullet"><span class="by">nabla9</span><span>|</span><a href="#41039381">parent</a><span>|</span><a href="#41042740">prev</a><span>|</span><a href="#41040854">next</a><span>|</span><label class="collapse" for="c-41039827">[-]</label><label class="expand" for="c-41039827">[2 more]</label></div><br/><div class="children"><div class="content">Everyone who sees that formula can immediately see that it leads to  principle of maximum entropy.<p>Just like everyone seeing  Maxwell&#x27;s equations can immediately see that you can derive the  the speed of light classically.<p>Oh dear. The joy of explaining the little you know.</div><br/><div id="41040171" class="c"><input type="checkbox" id="c-41040171" checked=""/><div class="controls bullet"><span class="by">prof-dr-ir</span><span>|</span><a href="#41039381">root</a><span>|</span><a href="#41039827">parent</a><span>|</span><a href="#41040854">next</a><span>|</span><label class="collapse" for="c-41040171">[-]</label><label class="expand" for="c-41040171">[1 more]</label></div><br/><div class="children"><div class="content">As of this moment there are six other top-level comments which each try to define entropy, and frankly they are all wrong, circular, or incomplete. Clearly the very <i>definition</i> of entropy is confusing, and the <i>definition</i> is what my comment provides.<p>I never said that all the other properties of entropy are now immediately visible. Instead I think it is the only universal starting point of any reasonable discussion or course on the subject.<p>And lastly I am frankly getting discouraged by all the dismissive responses. So this will be my last comment for the day, and I will leave you in the careful hands of, say, the six other people who are obviously so extremely knowledgeable about this topic. &#x2F;s</div><br/></div></div></div></div><div id="41040854" class="c"><input type="checkbox" id="c-41040854" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#41039381">parent</a><span>|</span><a href="#41039827">prev</a><span>|</span><a href="#41040416">next</a><span>|</span><label class="collapse" for="c-41040854">[-]</label><label class="expand" for="c-41040854">[1 more]</label></div><br/><div class="children"><div class="content">Don’t forget it’s the only measure of the arrow of time.</div><br/></div></div><div id="41040416" class="c"><input type="checkbox" id="c-41040416" checked=""/><div class="controls bullet"><span class="by">bdjsiqoocwk</span><span>|</span><a href="#41039381">parent</a><span>|</span><a href="#41040854">prev</a><span>|</span><a href="#41041753">next</a><span>|</span><label class="collapse" for="c-41040416">[-]</label><label class="expand" for="c-41040416">[3 more]</label></div><br/><div class="children"><div class="content">&gt; That is it. I think there is little need for all the mystery<p>You know so little you&#x27;re not even aware how little you know.</div><br/><div id="41042932" class="c"><input type="checkbox" id="c-41042932" checked=""/><div class="controls bullet"><span class="by">bubblyworld</span><span>|</span><a href="#41039381">root</a><span>|</span><a href="#41040416">parent</a><span>|</span><a href="#41041753">next</a><span>|</span><label class="collapse" for="c-41042932">[-]</label><label class="expand" for="c-41042932">[2 more]</label></div><br/><div class="children"><div class="content">Please don&#x27;t post comments just to be a dick.</div><br/><div id="41043557" class="c"><input type="checkbox" id="c-41043557" checked=""/><div class="controls bullet"><span class="by">bdjsiqoocwk</span><span>|</span><a href="#41039381">root</a><span>|</span><a href="#41042932">parent</a><span>|</span><a href="#41041753">next</a><span>|</span><label class="collapse" for="c-41043557">[-]</label><label class="expand" for="c-41043557">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t post it &quot;just&quot; to be a dick. I posted to help signal to other people reading that they haven&#x27;t in fact come across a genius of mathematics, it&#x27;s just a guy who knows very very little.</div><br/></div></div></div></div></div></div></div></div><div id="41040379" class="c"><input type="checkbox" id="c-41040379" checked=""/><div class="controls bullet"><span class="by">bdjsiqoocwk</span><span>|</span><a href="#41040111">prev</a><span>|</span><a href="#41041250">next</a><span>|</span><label class="collapse" for="c-41040379">[-]</label><label class="expand" for="c-41040379">[1 more]</label></div><br/><div class="children"><div class="content">Hmmm that list of things that contribute to entropy I&#x27;ve noticed omits particles which under &quot;normal circumstances&quot; on earth exist in bound states, for example it doesn&#x27;t mentions W bosons or gluons. But in some parts of the universe they&#x27;re not bound but in different state of matter, e.g. quark gluon plasma. I wonder how or if this was taken I to account.</div><br/></div></div><div id="41041250" class="c"><input type="checkbox" id="c-41041250" checked=""/><div class="controls bullet"><span class="by">arjunlol</span><span>|</span><a href="#41040379">prev</a><span>|</span><a href="#41038486">next</a><span>|</span><label class="collapse" for="c-41041250">[-]</label><label class="expand" for="c-41041250">[1 more]</label></div><br/><div class="children"><div class="content">ΔS = ΔQ&#x2F;T</div><br/></div></div><div id="41038486" class="c"><input type="checkbox" id="c-41038486" checked=""/><div class="controls bullet"><span class="by">illuminant</span><span>|</span><a href="#41041250">prev</a><span>|</span><label class="collapse" for="c-41038486">[-]</label><label class="expand" for="c-41038486">[21 more]</label></div><br/><div class="children"><div class="content">Entropy is the distribution of potential over negative potential.<p>This could be said &quot;the distribution of what ever may be over the surface area of where it may be.&quot;<p>This is erroneously taught in conventional information theory as &quot;the number of configurations in a system&quot; or the available information that has yet to be retrieved. Entropy includes the unforseen, and out of scope.<p>Entropy is merely the predisposition to flow from high to low pressure (potential). That is it. Information is a form of potential.<p>Philosophically what are entropy&#x27;s guarantees?<p>- That there will always be a super-scope, which may interfere in ways unanticipated;<p>- everything decays the only mystery is when and how.</div><br/><div id="41039265" class="c"><input type="checkbox" id="c-41039265" checked=""/><div class="controls bullet"><span class="by">eoverride</span><span>|</span><a href="#41038486">parent</a><span>|</span><a href="#41038878">next</a><span>|</span><label class="collapse" for="c-41039265">[-]</label><label class="expand" for="c-41039265">[2 more]</label></div><br/><div class="children"><div class="content">This answer is as confident as it&#x27;s wrong and full of gibberish.<p>Entropy is not a &quot;distribution”, it&#x27;s a functional that maps a probability distribution to a scalar value, i.e. a single number.<p>It&#x27;s the mean log-probability of a distribution.<p>It&#x27;s an elementary statistical concept, independent of physical concepts like “pressure”, “potential”, and so on.</div><br/><div id="41039737" class="c"><input type="checkbox" id="c-41039737" checked=""/><div class="controls bullet"><span class="by">illuminant</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41039265">parent</a><span>|</span><a href="#41038878">next</a><span>|</span><label class="collapse" for="c-41039737">[-]</label><label class="expand" for="c-41039737">[1 more]</label></div><br/><div class="children"><div class="content">It sounds like log-probability is the manifold surface area.<p>Distribution of potential over negative potential. Negative potential is the &quot;surface area&quot;, and available potential distributes itself &quot;geometrically&quot;. All this is iterative obviously, some periodicity set by universal speed limit.<p>It really doesn&#x27;t sound like you disagree with me.</div><br/></div></div></div></div><div id="41038878" class="c"><input type="checkbox" id="c-41038878" checked=""/><div class="controls bullet"><span class="by">axblount</span><span>|</span><a href="#41038486">parent</a><span>|</span><a href="#41039265">prev</a><span>|</span><a href="#41038837">next</a><span>|</span><label class="collapse" for="c-41038878">[-]</label><label class="expand" for="c-41038878">[1 more]</label></div><br/><div class="children"><div class="content">Baez seems to use the definition you call erroneous: &quot;It’s easy to wax poetic about entropy, but what is it? I claim it’s the amount of
information we don’t know about a situation, which in principle we could learn.&quot;</div><br/></div></div><div id="41038837" class="c"><input type="checkbox" id="c-41038837" checked=""/><div class="controls bullet"><span class="by">ziofill</span><span>|</span><a href="#41038486">parent</a><span>|</span><a href="#41038878">prev</a><span>|</span><a href="#41038826">next</a><span>|</span><label class="collapse" for="c-41038837">[-]</label><label class="expand" for="c-41038837">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Entropy includes the unforseen, and out of scope.<p>Mmh, no it doesn&#x27;t. You need to define your state space, otherwise it&#x27;s an undefined quantity.</div><br/><div id="41038984" class="c"><input type="checkbox" id="c-41038984" checked=""/><div class="controls bullet"><span class="by">kevindamm</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41038837">parent</a><span>|</span><a href="#41039767">next</a><span>|</span><label class="collapse" for="c-41038984">[-]</label><label class="expand" for="c-41038984">[1 more]</label></div><br/><div class="children"><div class="content">But it is possible to account for the unforseen (or out-of-vocabulary) by, for example, a Good-Turing estimate.  This satisfies your demand for a fully defined state space while also being consistent with GP&#x27;s definition.</div><br/></div></div><div id="41039767" class="c"><input type="checkbox" id="c-41039767" checked=""/><div class="controls bullet"><span class="by">illuminant</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41038837">parent</a><span>|</span><a href="#41038984">prev</a><span>|</span><a href="#41038826">next</a><span>|</span><label class="collapse" for="c-41039767">[-]</label><label class="expand" for="c-41039767">[3 more]</label></div><br/><div class="children"><div class="content">You are referring to the conceptual device you believe bongs to you and your equations. Entropy creates attraction and repulsion, even causing working bias. We rely upon it for our system functions.<p>Undefined is uncertainty is entropic.</div><br/><div id="41040203" class="c"><input type="checkbox" id="c-41040203" checked=""/><div class="controls bullet"><span class="by">fermisea</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41039767">parent</a><span>|</span><a href="#41040861">next</a><span>|</span><label class="collapse" for="c-41040203">[-]</label><label class="expand" for="c-41040203">[1 more]</label></div><br/><div class="children"><div class="content">Entropy is a measure, it doesn&#x27;t create anything. This is highly misleading.</div><br/></div></div><div id="41040861" class="c"><input type="checkbox" id="c-41040861" checked=""/><div class="controls bullet"><span class="by">senderista</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41039767">parent</a><span>|</span><a href="#41040203">prev</a><span>|</span><a href="#41038826">next</a><span>|</span><label class="collapse" for="c-41040861">[-]</label><label class="expand" for="c-41040861">[1 more]</label></div><br/><div class="children"><div class="content">&gt; bongs<p>indeed</div><br/></div></div></div></div></div></div><div id="41038826" class="c"><input type="checkbox" id="c-41038826" checked=""/><div class="controls bullet"><span class="by">mwbajor</span><span>|</span><a href="#41038486">parent</a><span>|</span><a href="#41038837">prev</a><span>|</span><label class="collapse" for="c-41038826">[-]</label><label class="expand" for="c-41038826">[12 more]</label></div><br/><div class="children"><div class="content">All definitions of entropy stem from one central, universal definition: Entropy is the amount of energy unable to be used for useful work. Or better put grammatically: entropy describes the effect that not all energy consumed can be used for work.</div><br/><div id="41038851" class="c"><input type="checkbox" id="c-41038851" checked=""/><div class="controls bullet"><span class="by">ajkjk</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41038826">parent</a><span>|</span><a href="#41039588">next</a><span>|</span><label class="collapse" for="c-41038851">[-]</label><label class="expand" for="c-41038851">[9 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a good case to be made that the information-theoretic definition of entropy is the most fundamental one, and the version that shows up in physics is just that concept as applied to physics.</div><br/><div id="41038959" class="c"><input type="checkbox" id="c-41038959" checked=""/><div class="controls bullet"><span class="by">rimunroe</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41038851">parent</a><span>|</span><a href="#41039727">next</a><span>|</span><label class="collapse" for="c-41038959">[-]</label><label class="expand" for="c-41038959">[1 more]</label></div><br/><div class="children"><div class="content">My favorite course I took as part of my physics degree was statistical mechanics. It leaned way closer to information theory than I would have expected going in, but in retrospect should have been obvious.<p>Unrelated: my favorite bit from any physics book is probably still the introduction of the first chapter of &quot;States of Matter&quot; by David Goodstein: &quot;Ludwig Boltzmann, who spent much of his life studying statistical mechanics, died in 1906, by his own hand. Paul Ehrenfest, carrying on the work, died similarly in 1933. Now it is our turn to study statistical mechanics.&quot;</div><br/></div></div><div id="41039727" class="c"><input type="checkbox" id="c-41039727" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41038851">parent</a><span>|</span><a href="#41038959">prev</a><span>|</span><a href="#41040650">next</a><span>|</span><label class="collapse" for="c-41039727">[-]</label><label class="expand" for="c-41039727">[6 more]</label></div><br/><div class="children"><div class="content">That would mean that information-theory is not part of physics, right? So, Information Theory and Entropy, are part of metaphysics?</div><br/><div id="41040597" class="c"><input type="checkbox" id="c-41040597" checked=""/><div class="controls bullet"><span class="by">ajkjk</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41039727">parent</a><span>|</span><a href="#41040650">next</a><span>|</span><label class="collapse" for="c-41040597">[-]</label><label class="expand" for="c-41040597">[5 more]</label></div><br/><div class="children"><div class="content">Well it&#x27;s part of math, which physics is already based on.<p>Whereas metaphysics is, imo, &quot;stuff that&#x27;s made up and doesn&#x27;t matter&quot;. Probably not the most standard take.</div><br/><div id="41041332" class="c"><input type="checkbox" id="c-41041332" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41040597">parent</a><span>|</span><a href="#41040650">next</a><span>|</span><label class="collapse" for="c-41041332">[-]</label><label class="expand" for="c-41041332">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m wondering, isn&#x27;t Information Theory as much part of physics as Thermodynamics is?</div><br/><div id="41042815" class="c"><input type="checkbox" id="c-41042815" checked=""/><div class="controls bullet"><span class="by">kgwgk</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41041332">parent</a><span>|</span><a href="#41042806">prev</a><span>|</span><a href="#41041999">next</a><span>|</span><label class="collapse" for="c-41042815">[-]</label><label class="expand" for="c-41042815">[1 more]</label></div><br/><div class="children"><div class="content">Would you say that Geometry is as much a part of physics as Optics is?</div><br/></div></div><div id="41041999" class="c"><input type="checkbox" id="c-41041999" checked=""/><div class="controls bullet"><span class="by">ajkjk</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41041332">parent</a><span>|</span><a href="#41042815">prev</a><span>|</span><a href="#41040650">next</a><span>|</span><label class="collapse" for="c-41041999">[-]</label><label class="expand" for="c-41041999">[1 more]</label></div><br/><div class="children"><div class="content">Not really. Information theory applies to anything probability applies to, including many situations that aren&#x27;t &quot;physics&quot; per se. For instance it has a lot to do with algorithms and data as well. I think of it as being at the level of geometry and calculus.</div><br/></div></div></div></div></div></div></div></div><div id="41040650" class="c"><input type="checkbox" id="c-41040650" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41038851">parent</a><span>|</span><a href="#41039727">prev</a><span>|</span><a href="#41039588">next</a><span>|</span><label class="collapse" for="c-41040650">[-]</label><label class="expand" for="c-41040650">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, people seemingly misunderstand that the entropy applied to thermodynamics is simply an aggregate statistic that summarizes the complex state of the thermodynamic system as a single real number.<p>The fact that entropy always rises etc, has nothing to do with the statistical concept of entropy itself. It simply is an easier way to express the physics concept that individual atoms spread out their kinetic energy across a large volume.</div><br/></div></div></div></div><div id="41039588" class="c"><input type="checkbox" id="c-41039588" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41038826">parent</a><span>|</span><a href="#41038851">prev</a><span>|</span><a href="#41039219">next</a><span>|</span><label class="collapse" for="c-41039588">[-]</label><label class="expand" for="c-41039588">[1 more]</label></div><br/><div class="children"><div class="content">This definition is far from universal.</div><br/></div></div><div id="41039219" class="c"><input type="checkbox" id="c-41039219" checked=""/><div class="controls bullet"><span class="by">ziofill</span><span>|</span><a href="#41038486">root</a><span>|</span><a href="#41038826">parent</a><span>|</span><a href="#41039588">prev</a><span>|</span><label class="collapse" for="c-41039219">[-]</label><label class="expand" for="c-41039219">[1 more]</label></div><br/><div class="children"><div class="content">I think what you describe is the application of entropy in the thermodynamic setting, which doesn&#x27;t apply to &quot;all definitions&quot;.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>