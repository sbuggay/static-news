<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1729760475229" as="style"/><link rel="stylesheet" href="styles.css?v=1729760475229"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a>Launch HN: GPT Driver (YC S21) – End-to-end app testing in natural language</a> </div><div class="subtext"><span>cschiller</span> | <span>76 comments</span></div><br/><div><div id="41925521" class="c"><input type="checkbox" id="c-41925521" checked=""/><div class="controls bullet"><span class="by">msoad</span><span>|</span><a href="#41926270">next</a><span>|</span><label class="collapse" for="c-41925521">[-]</label><label class="expand" for="c-41925521">[21 more]</label></div><br/><div class="children"><div class="content">I work in this space. We manage thousands of e2e tests. The pain has never been in writing the tests. Frameworks like Playwright are great at the UX. And having code editors like Cursor makes it even easier to write the tests. Now, if I could show Cursor the browser, it would be even better, but that doesn’t work today since most multimodal models are too slow to understand screenshots.<p>It used to be that the frontend was very fragile. XVFB, Selenium, ChromeDriver, etc., used to be the cause of pain, but recently the frontend frameworks and browser automation have been solid. Headless Chrome hardly lets us down.<p>The biggest pain in e2e testing is that tests fail for reasons that are hard to understand and debug. This is a very, very difficult thing to automate and requires AGI-level intelligence to really build a system that can go read the logs of some random service deep in our service mesh to understand why an e2e test fails. When an e2e test flakes, in a lot of cases we ignore it. I have been in other orgs where this is the case too. I wish there was a system that would follow up and generate a report that says, “This e2e test failed because service XYZ had a null pointer exception in this line,” but that doesn’t exist today. In most of the companies I’ve been at, we had complex enough infra that the error message never makes it to the frontend so we can see it in the logs. OpenTelemetry and other tools are promising, but again, I’ve never seen good enough infra that puts that all together.<p>Writing tests is not a pain point worth buying a solution for, in my case.<p>My 2c. Hopefully it’s helpful and not too cynical.</div><br/><div id="41926022" class="c"><input type="checkbox" id="c-41926022" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#41925521">parent</a><span>|</span><a href="#41926615">next</a><span>|</span><label class="collapse" for="c-41926022">[-]</label><label class="expand" for="c-41926022">[6 more]</label></div><br/><div class="children"><div class="content">While I agree with your primary pain point, I would argue that that really isn&#x27;t specific to tests at all. It sounds like what you&#x27;re really saying is that when something goes wrong, it&#x27;s really difficult to determine which component in a complex system is responsible. I mean, from what you&#x27;ve described (and from what I&#x27;ve experienced as well), you would have the same if not harder problem if a user experienced a bug on the front end and then you had to find the root cause.<p>That is, I don&#x27;t think a framework focused on front end testing should really be where the solution for your problem is implemented. You say &quot;This is a very, very difficult thing to automate and requires AGI-level intelligence to really build a system that can go read the logs of some random service deep in our service mesh to understand why an e2e test fails.&quot; - I would argue what you really need is better log aggregation and system tracing. And I&#x27;m not saying this to be snarky (at scale with a bunch of different teams managing different components I&#x27;ve seen that it can be difficult to get everyone on the same aggregation&#x2F;tracing framework and practices), but that&#x27;s where I&#x27;d focus, as you&#x27;ll get the dividends not only in testing but in runtime observability as well.</div><br/><div id="41926227" class="c"><input type="checkbox" id="c-41926227" checked=""/><div class="controls bullet"><span class="by">Lienetic</span><span>|</span><a href="#41925521">root</a><span>|</span><a href="#41926022">parent</a><span>|</span><a href="#41926926">next</a><span>|</span><label class="collapse" for="c-41926227">[-]</label><label class="expand" for="c-41926227">[2 more]</label></div><br/><div class="children"><div class="content">Agreed. Is there a good tool you&#x27;d recommend for this?</div><br/><div id="41926332" class="c"><input type="checkbox" id="c-41926332" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#41925521">root</a><span>|</span><a href="#41926227">parent</a><span>|</span><a href="#41926926">next</a><span>|</span><label class="collapse" for="c-41926332">[-]</label><label class="expand" for="c-41926332">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been quite some time but New Relic is a popular observability tool whose primary goal (at least the original primary goal I&#x27;d say) is being able to tie together lots of distributed systems to make it easier to do request tracing and root cause analysis. I was a big fan of New Relic when I last used it, but if memory serves me correctly it was quite expensive.</div><br/></div></div></div></div><div id="41926926" class="c"><input type="checkbox" id="c-41926926" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#41925521">root</a><span>|</span><a href="#41926022">parent</a><span>|</span><a href="#41926227">prev</a><span>|</span><a href="#41926615">next</a><span>|</span><label class="collapse" for="c-41926926">[-]</label><label class="expand" for="c-41926926">[3 more]</label></div><br/><div class="children"><div class="content">&quot;OpenTelemetry and other tools are promising, but again, I’ve never seen good enough infra that puts that all together.&quot;<p>It&#x27;s a two paragraph comment and you somehow missed it.</div><br/><div id="41929486" class="c"><input type="checkbox" id="c-41929486" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#41925521">root</a><span>|</span><a href="#41926926">parent</a><span>|</span><a href="#41928566">next</a><span>|</span><label class="collapse" for="c-41929486">[-]</label><label class="expand" for="c-41929486">[1 more]</label></div><br/><div class="children"><div class="content">I did read it, and I don&#x27;t understand why you feel the need to be an asshole.<p>Like I said in my comment, I do think getting everyone on the same page in a large, diverse organization is difficult. That said, it&#x27;s not rocket science, and it&#x27;s usually difficult because there aren&#x27;t organizational incentives in place to actually ensure teams prioritize making system-wide observability work.<p>FWIW, the process I&#x27;ve seen at more than 1 company is that people bitch about debugging being a pain, they put in a couple half measures to improve things, and then finally it becomes so much of a pain that they say &quot;fine, we need to get all of our ducks in a row&quot;, execs make it a priority, and then they finally implement a system-wide observability process that works.</div><br/></div></div><div id="41928566" class="c"><input type="checkbox" id="c-41928566" checked=""/><div class="controls bullet"><span class="by">msoad</span><span>|</span><a href="#41925521">root</a><span>|</span><a href="#41926926">parent</a><span>|</span><a href="#41929486">prev</a><span>|</span><a href="#41926615">next</a><span>|</span><label class="collapse" for="c-41928566">[-]</label><label class="expand" for="c-41928566">[1 more]</label></div><br/><div class="children"><div class="content">Exactly! I&#x27;ve never seen a 5000+ eng org that have all their ducks in a row when it comes to telemetry. it&#x27;s one of those things that you can&#x27;t put a team in charge of it and get results. everyone have to be on the same page which in a big org is hardly the case.</div><br/></div></div></div></div></div></div><div id="41926615" class="c"><input type="checkbox" id="c-41926615" checked=""/><div class="controls bullet"><span class="by">cschiller</span><span>|</span><a href="#41925521">parent</a><span>|</span><a href="#41926022">prev</a><span>|</span><a href="#41926782">next</a><span>|</span><label class="collapse" for="c-41926615">[-]</label><label class="expand" for="c-41926615">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for your thoughtful response! Agree that digging into the root cause of a failure, especially in complex microservice setups, can be incredibly time-consuming.<p>Regarding writing robust e2e tests, I think it really depends on the team&#x27;s experience and the organization’s setup. We’ve found that in some organizations—particularly those with large, fast-moving engineering teams—test creation and maintenance can still be a bottleneck due to the flakiness of their e2e tests.<p>For example, we’ve seen an e-commerce team with 150+ mobile engineers struggle to keep their functional tests up-to-date while the company was running copy and marketing experiments. Another team in the food delivery space faced issues where unrelated changes in webviews caused their e2e tests to fail, making it impossible to run tests in a production-like system.<p>Our goal is to help free up that time so that teams can focus on solving bigger challenges, like the debugging problems you’ve mentioned.</div><br/><div id="41927695" class="c"><input type="checkbox" id="c-41927695" checked=""/><div class="controls bullet"><span class="by">Terretta</span><span>|</span><a href="#41925521">root</a><span>|</span><a href="#41926615">parent</a><span>|</span><a href="#41926782">next</a><span>|</span><label class="collapse" for="c-41927695">[-]</label><label class="expand" for="c-41927695">[1 more]</label></div><br/><div class="children"><div class="content">Integrate with <a href="https:&#x2F;&#x2F;www.honeycomb.io" rel="nofollow">https:&#x2F;&#x2F;www.honeycomb.io</a></div><br/></div></div></div></div><div id="41926782" class="c"><input type="checkbox" id="c-41926782" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#41925521">parent</a><span>|</span><a href="#41926615">prev</a><span>|</span><a href="#41926648">next</a><span>|</span><label class="collapse" for="c-41926782">[-]</label><label class="expand" for="c-41926782">[2 more]</label></div><br/><div class="children"><div class="content">There are silly things that trip up e2e tests like a cookie pop up or network failures and whatnot. An AI can plow through these in a way that a purely coded test can’t.<p>Those types of transient issues aren’t something that you would want to fail a test for given it still would let the human get the job done if it happened in the field.<p>This seems like the most useful part of adding AI to e2e tests. The world is not deterministic, which AI handles well.<p>Uber takes this approach here:
<a href="https:&#x2F;&#x2F;www.uber.com&#x2F;blog&#x2F;generative-ai-for-high-quality-mobile-testing&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.uber.com&#x2F;blog&#x2F;generative-ai-for-high-quality-mob...</a></div><br/><div id="41926943" class="c"><input type="checkbox" id="c-41926943" checked=""/><div class="controls bullet"><span class="by">tomatohs</span><span>|</span><a href="#41925521">root</a><span>|</span><a href="#41926782">parent</a><span>|</span><a href="#41926648">next</a><span>|</span><label class="collapse" for="c-41926943">[-]</label><label class="expand" for="c-41926943">[1 more]</label></div><br/><div class="children"><div class="content">I predict an all out war over deterministic vs non-deterministic testing, or at least a new buzzword for fuzzy testing. Product people understand that a cookie banner &quot;shouldn&#x27;t&quot; prevent the test from passing, but an engineer would entirely disagree (see the rest of the convos below).<p>Engineers struggle with non-deterministic output. It removes the control and &quot;truth&quot; that engineering is founded upon. It&#x27;s going to take a lot of work (or again, a toung-in-cheek buzzword like &quot;chaos testing&quot;) to get engineers to accept the non-deterministic behavior.</div><br/></div></div></div></div><div id="41926648" class="c"><input type="checkbox" id="c-41926648" checked=""/><div class="controls bullet"><span class="by">rafaelmn</span><span>|</span><a href="#41925521">parent</a><span>|</span><a href="#41926782">prev</a><span>|</span><a href="#41927466">next</a><span>|</span><label class="collapse" for="c-41926648">[-]</label><label class="expand" for="c-41926648">[1 more]</label></div><br/><div class="children"><div class="content">I think either you&#x27;re overselling the maturity of the ecosystem or I&#x27;ve been unfortunate enough to get stuck with the worst option out there - Cypress. I run into tooling limitations and issues regularly, only to eventually find an open GitHub issue with no solution or some such.</div><br/></div></div><div id="41927466" class="c"><input type="checkbox" id="c-41927466" checked=""/><div class="controls bullet"><span class="by">codedokode</span><span>|</span><a href="#41925521">parent</a><span>|</span><a href="#41926648">prev</a><span>|</span><a href="#41925985">next</a><span>|</span><label class="collapse" for="c-41927466">[-]</label><label class="expand" for="c-41927466">[3 more]</label></div><br/><div class="children"><div class="content">Sorry if it is a stupid idea, but cannot you log all messages to a separate file for each test (or attach test id to the messages)? Then if the test fails, you can see where the error occured.</div><br/><div id="41928590" class="c"><input type="checkbox" id="c-41928590" checked=""/><div class="controls bullet"><span class="by">msoad</span><span>|</span><a href="#41925521">root</a><span>|</span><a href="#41927466">parent</a><span>|</span><a href="#41925985">next</a><span>|</span><label class="collapse" for="c-41928590">[-]</label><label class="expand" for="c-41928590">[2 more]</label></div><br/><div class="children"><div class="content">Where I work there are 1,500 microservices. How do I get a log of all of those services -- only related to my test&#x27;s requests in a file?<p>I know there are solutions for this, but in the real world I have not seen it properly implemented.</div><br/><div id="41931556" class="c"><input type="checkbox" id="c-41931556" checked=""/><div class="controls bullet"><span class="by">ergeysay</span><span>|</span><a href="#41925521">root</a><span>|</span><a href="#41928590">parent</a><span>|</span><a href="#41925985">next</a><span>|</span><label class="collapse" for="c-41931556">[-]</label><label class="expand" for="c-41931556">[1 more]</label></div><br/><div class="children"><div class="content">As you said, OpenTelemetry and friends can help. I had great success with these.<p>I am curious, what were implementation issues you have encountered?</div><br/></div></div></div></div></div></div><div id="41925985" class="c"><input type="checkbox" id="c-41925985" checked=""/><div class="controls bullet"><span class="by">TechDebtDevin</span><span>|</span><a href="#41925521">parent</a><span>|</span><a href="#41927466">prev</a><span>|</span><a href="#41926862">next</a><span>|</span><label class="collapse" for="c-41925985">[-]</label><label class="expand" for="c-41925985">[2 more]</label></div><br/><div class="children"><div class="content">I doubt that screenshot methods are the bottleneck considering that&#x27;s the method Microsoft and Anthropic are using.</div><br/><div id="41926953" class="c"><input type="checkbox" id="c-41926953" checked=""/><div class="controls bullet"><span class="by">tomatohs</span><span>|</span><a href="#41925521">root</a><span>|</span><a href="#41925985">parent</a><span>|</span><a href="#41926862">next</a><span>|</span><label class="collapse" for="c-41926953">[-]</label><label class="expand" for="c-41926953">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s absolutely not the bottleneck. OpenAI can process a full resolution screenshot in about 4 seconds.</div><br/></div></div></div></div><div id="41926862" class="c"><input type="checkbox" id="c-41926862" checked=""/><div class="controls bullet"><span class="by">tomatohs</span><span>|</span><a href="#41925521">parent</a><span>|</span><a href="#41925985">prev</a><span>|</span><a href="#41925784">next</a><span>|</span><label class="collapse" for="c-41926862">[-]</label><label class="expand" for="c-41926862">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re totally right here, but &quot;debugging failed tests&quot; is a mature problem that assumes you have working tests and people to write them. Most companies don&#x27;t have the resources to dedicate full engineer time to QA, and if they do nobody maintains the test.<p>Debugging failed test is a &quot;first world problem&quot;</div><br/><div id="41931854" class="c"><input type="checkbox" id="c-41931854" checked=""/><div class="controls bullet"><span class="by">AdieuToLogic</span><span>|</span><a href="#41925521">root</a><span>|</span><a href="#41926862">parent</a><span>|</span><a href="#41925784">next</a><span>|</span><label class="collapse" for="c-41931854">[-]</label><label class="expand" for="c-41931854">[1 more]</label></div><br/><div class="children"><div class="content">&gt; ... &quot;debugging failed tests&quot; is a mature problem that assumes you have working tests and people to write them.<p>I am reminded of an old s&#x2F;w engineering law:<p><pre><code>  Developers can test their solution or Customers will.
  Either way, the system will be tested.</code></pre></div><br/></div></div></div></div><div id="41925784" class="c"><input type="checkbox" id="c-41925784" checked=""/><div class="controls bullet"><span class="by">fullstackchris</span><span>|</span><a href="#41925521">parent</a><span>|</span><a href="#41926862">prev</a><span>|</span><a href="#41926270">next</a><span>|</span><label class="collapse" for="c-41925784">[-]</label><label class="expand" for="c-41925784">[2 more]</label></div><br/><div class="children"><div class="content">To be fair, this is NOT the case with native mobile apps. There are some projects like detox that are trying to make e2e tests easier, but the tests themselves can be painful, run fairly slow on emulators, etc.<p>Maybe someday the tooling for mobile will be as good as headless chrome is for web :)<p>Agreed though that the followup debugging of a failed test could be hard to automate in some cases.</div><br/><div id="41933513" class="c"><input type="checkbox" id="c-41933513" checked=""/><div class="controls bullet"><span class="by">edelans</span><span>|</span><a href="#41925521">root</a><span>|</span><a href="#41925784">parent</a><span>|</span><a href="#41926270">next</a><span>|</span><label class="collapse" for="c-41933513">[-]</label><label class="expand" for="c-41933513">[1 more]</label></div><br/><div class="children"><div class="content">I think we can claim that at Waldo.<p>Check for yourself: I&#x27;ve just recorded this [1] scripted test on the wikipedia mobile app, and it yields this [2] Replay. 
In less than a minute we spin up a fresh virtual device, install your app on it, execute the 8 steps of the script.<p>As a result, you get the Replay of the session : video synchronized with interaction timeline, device &amp; network logs, so you can debug in full context.<p>[1]: <a href="https:&#x2F;&#x2F;github.com&#x2F;waldoapp&#x2F;waldo-programmatic-samples&#x2F;blob&#x2F;main&#x2F;ios&#x2F;test&#x2F;onboarding.ts">https:&#x2F;&#x2F;github.com&#x2F;waldoapp&#x2F;waldo-programmatic-samples&#x2F;blob&#x2F;...</a>
[2]: <a href="https:&#x2F;&#x2F;share.waldo.com&#x2F;7a45b5bd364edbf17c578070ce8bde22024024" rel="nofollow">https:&#x2F;&#x2F;share.waldo.com&#x2F;7a45b5bd364edbf17c578070ce8bde220240...</a></div><br/></div></div></div></div></div></div><div id="41926270" class="c"><input type="checkbox" id="c-41926270" checked=""/><div class="controls bullet"><span class="by">batikha</span><span>|</span><a href="#41925521">prev</a><span>|</span><a href="#41925129">next</a><span>|</span><label class="collapse" for="c-41926270">[-]</label><label class="expand" for="c-41926270">[3 more]</label></div><br/><div class="children"><div class="content">Very cool! I already can see a lot of &quot;this is already solved by playwright&#x2F;cypress&#x2F;selenium&#x2F;deterministic stuff&quot; in the comments.<p>Over nearly 10 years in startups (big and small), I&#x27;ve been consistently surprised by how much I hear that &quot;testing has been solved&quot;, yet I see very little automation in place and PMs&#x2F;QAs&#x2F;devs and sometimes CEOs and VPs doing lots of manual QA. And not only on new features (which is a good thing), also on happy path &#x2F; core features (arguably a waste of time to test things over and over again).<p>More than once I worked for a company that was against having a manual QA team, out of principle and more or less valid reasons (we use a typed language so less bug, engineers are empowered, etc etc), but ended up hiring external consultants to handle QA after a big quality incident.<p>The amount of mismatch between theory and practice in this field is impressive.</div><br/><div id="41930152" class="c"><input type="checkbox" id="c-41930152" checked=""/><div class="controls bullet"><span class="by">epolanski</span><span>|</span><a href="#41926270">parent</a><span>|</span><a href="#41926838">next</a><span>|</span><label class="collapse" for="c-41930152">[-]</label><label class="expand" for="c-41930152">[1 more]</label></div><br/><div class="children"><div class="content">&gt; yet I see very little automation in place and PMs&#x2F;QAs&#x2F;devs and sometimes CEOs and VPs doing lots of manual QA<p>Because software is a clownish mimicking of engineering that lacks any real solid and widespread engineering practices.<p>It&#x27;s cultural.<p>Crowds boast their engineering degrees, but have little to show but leetcode and system design black belts, even though their day to day job rarely requires them to architect systems or reimplement a new Levehnstein distance but would benefit a lot from thoroughly investigating functional and non functional requirements and encoding and maintaining those through automation.<p>There&#x27;s very little engineering in software, people really care about the borderline fun parts and discard the rest.</div><br/></div></div><div id="41926838" class="c"><input type="checkbox" id="c-41926838" checked=""/><div class="controls bullet"><span class="by">cschiller</span><span>|</span><a href="#41926270">parent</a><span>|</span><a href="#41930152">prev</a><span>|</span><a href="#41925129">next</a><span>|</span><label class="collapse" for="c-41926838">[-]</label><label class="expand" for="c-41926838">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing your experience! Completely agree - there&#x27;s often a huge gap between the perception that testing is &quot;solved&quot; and the reality of manual QA still being necessary, even for core features. We recently had a call with one of the largest US mobile teams and were surprised to learn they&#x27;re still doing extensive manual testing because some use cases remain uncovered by traditional tools. It&#x27;s definitely not as &quot;solved&quot; as many might think.</div><br/></div></div></div></div><div id="41925129" class="c"><input type="checkbox" id="c-41925129" checked=""/><div class="controls bullet"><span class="by">codepathfinder</span><span>|</span><a href="#41926270">prev</a><span>|</span><a href="#41925110">next</a><span>|</span><label class="collapse" for="c-41925129">[-]</label><label class="expand" for="c-41925129">[4 more]</label></div><br/><div class="children"><div class="content">Is it possible to record the user screen and just generate a test case. I believe that&#x27;s most efficient way IMO</div><br/><div id="41925545" class="c"><input type="checkbox" id="c-41925545" checked=""/><div class="controls bullet"><span class="by">cschiller</span><span>|</span><a href="#41925129">parent</a><span>|</span><a href="#41926997">next</a><span>|</span><label class="collapse" for="c-41925545">[-]</label><label class="expand" for="c-41925545">[1 more]</label></div><br/><div class="children"><div class="content">Yes, great point! We have an &#x27;Assistant&#x27; feature where you can perform the flow on the device, and we automatically generate the test case as you navigate the app. As you mentioned, it’s a great starting point to quickly automate the functional flow. Afterwards, you can add more detailed assertions as needed. Technically we do this by using both the UI hierarchy from the app as well as vision models to generate the test prompt.</div><br/></div></div><div id="41926997" class="c"><input type="checkbox" id="c-41926997" checked=""/><div class="controls bullet"><span class="by">tomatohs</span><span>|</span><a href="#41925129">parent</a><span>|</span><a href="#41925545">prev</a><span>|</span><a href="#41925110">next</a><span>|</span><label class="collapse" for="c-41926997">[-]</label><label class="expand" for="c-41926997">[2 more]</label></div><br/><div class="children"><div class="content">This comes up all the time. It seems like it would be possible, but imagine the case where you want to verify that a menu shows on hover. Was the hover on the menu intentional?<p>Another example, imagine an error box shows up. Was that correct or incorrect?<p>So you need to build a &quot;meta&quot; layer, which includes UI, to start marking up the video and end up in the same state.<p>Our approach has been to let the AI explore the app and come up with ideas. Less interaction from the user.</div><br/><div id="41927936" class="c"><input type="checkbox" id="c-41927936" checked=""/><div class="controls bullet"><span class="by">codepathfinder</span><span>|</span><a href="#41925129">root</a><span>|</span><a href="#41926997">parent</a><span>|</span><a href="#41925110">next</a><span>|</span><label class="collapse" for="c-41927936">[-]</label><label class="expand" for="c-41927936">[1 more]</label></div><br/><div class="children"><div class="content">My way of thinking while working of B2 enterprise app, sometimes users come up from weird scenarios in feature with X turn on, off with specific edition (country).<p>Maybe the gpt can surf the user activity logs or crash logs and reproduce the scenarios as test case.<p>Remember crashlytics ?</div><br/></div></div></div></div></div></div><div id="41925110" class="c"><input type="checkbox" id="c-41925110" checked=""/><div class="controls bullet"><span class="by">codepathfinder</span><span>|</span><a href="#41925129">prev</a><span>|</span><a href="#41926911">next</a><span>|</span><label class="collapse" for="c-41925110">[-]</label><label class="expand" for="c-41925110">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been a mobile developer for the past 10 years and my overall belief is that mobile app development has slower growth and companies with the mobile team are investing less on mobile Dev or testing+tooling+education. Do you think the market is still hot once it was to use your product?</div><br/><div id="41925568" class="c"><input type="checkbox" id="c-41925568" checked=""/><div class="controls bullet"><span class="by">cschiller</span><span>|</span><a href="#41925110">parent</a><span>|</span><a href="#41926911">next</a><span>|</span><label class="collapse" for="c-41925568">[-]</label><label class="expand" for="c-41925568">[2 more]</label></div><br/><div class="children"><div class="content">I would say that mobile apps are still the primary format for launching new consumer services, incl. new apps like ChatGPT and many others. However we’ve observed that teams are expected to do more with less—delivering high-quality products while ensuring compliance, often with the same or even smaller team sizes. This is why we focus on minimizing the engineering burden, particularly when it comes to repetitive tasks like regression testing, which can be especially painful to maintain in the mobile ecosystem due to use of third-party integrations (authentication, payments, etc.).</div><br/><div id="41926461" class="c"><input type="checkbox" id="c-41926461" checked=""/><div class="controls bullet"><span class="by">codetrotter</span><span>|</span><a href="#41925110">root</a><span>|</span><a href="#41925568">parent</a><span>|</span><a href="#41926911">next</a><span>|</span><label class="collapse" for="c-41926461">[-]</label><label class="expand" for="c-41926461">[1 more]</label></div><br/><div class="children"><div class="content">&gt; mobile apps are still the primary format for launching new consumer services, incl. new apps like ChatGPT and many others<p>OpenAI launched ChatGPT to the public on the web first and it took like, several months I think from I used their public web version until they had an official app for it in App Store. In the meantime, some third party apps popped up in App Store for using ChatGPT. I kept using the web version until the official app showed up. And probably having the mobile app in App Store has helped them grow to the number of users they have now. But IMO, ChatGPT as a product was not itself “launched” on App Store and they seemed to do very well in terms of adoption even when initially they only had the web version. The main point, that mobile apps are still desired, I agree with though.</div><br/></div></div></div></div></div></div><div id="41926911" class="c"><input type="checkbox" id="c-41926911" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#41925110">prev</a><span>|</span><a href="#41926351">next</a><span>|</span><label class="collapse" for="c-41926911">[-]</label><label class="expand" for="c-41926911">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In terms of trying the product out: since the service is resource-intensive (we provide hosted virtual&#x2F;real phone instances), we don&#x27;t currently have a playground available. However, you can see some examples here <a href="https:&#x2F;&#x2F;mobileboost.io&#x2F;showcases">https:&#x2F;&#x2F;mobileboost.io&#x2F;showcases</a> and book a demo of GPT Driver testing your app through our website.<p>Have you considered an approach like what Anthropic is doing for their computer control where an agent runs on your own computer and controls a device simulator?</div><br/></div></div><div id="41926351" class="c"><input type="checkbox" id="c-41926351" checked=""/><div class="controls bullet"><span class="by">mmaunder</span><span>|</span><a href="#41926911">prev</a><span>|</span><a href="#41931116">next</a><span>|</span><label class="collapse" for="c-41926351">[-]</label><label class="expand" for="c-41926351">[3 more]</label></div><br/><div class="children"><div class="content">Congrats! How has Anthropic&#x27;s latest release supporting computer use affected your planning&#x2F;thinking around this?<p>PS:If you had this for desktop we&#x27;d immediately become a customer.</div><br/><div id="41927382" class="c"><input type="checkbox" id="c-41927382" checked=""/><div class="controls bullet"><span class="by">cschiller</span><span>|</span><a href="#41926351">parent</a><span>|</span><a href="#41926969">next</a><span>|</span><label class="collapse" for="c-41927382">[-]</label><label class="expand" for="c-41927382">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! Sonnet 3.5 is indeed a powerful model, and we&#x27;re actually using it. However, even with the latest version, there are still some limitations affecting our specific use case. For instance, the model struggles to accurately recognize semi-overlaid areas, such as popups that block interactions, and it has trouble consistently detecting when UI elements are in a disabled state.<p>To address these issues, we enhance the models with our own custom logic and specialized models, which helps us achieve more reliable results.<p>Looking forward, we expect our QA Studio to become even more powerful as we integrate tools like test management, reporting, and infrastructure, especially as models improve. We&#x27;re excited about the possibilities ahead!</div><br/></div></div><div id="41926969" class="c"><input type="checkbox" id="c-41926969" checked=""/><div class="controls bullet"><span class="by">tomatohs</span><span>|</span><a href="#41926351">parent</a><span>|</span><a href="#41927382">prev</a><span>|</span><a href="#41931116">next</a><span>|</span><label class="collapse" for="c-41926969">[-]</label><label class="expand" for="c-41926969">[1 more]</label></div><br/><div class="children"><div class="content">We do AI E2E desktop, sent you an email.</div><br/></div></div></div></div><div id="41931116" class="c"><input type="checkbox" id="c-41931116" checked=""/><div class="controls bullet"><span class="by">xyst</span><span>|</span><a href="#41926351">prev</a><span>|</span><a href="#41927037">next</a><span>|</span><label class="collapse" for="c-41931116">[-]</label><label class="expand" for="c-41931116">[1 more]</label></div><br/><div class="children"><div class="content">I remember testing out a similar product (mabl?). Ended up just using it to check for dead links. Using it for other use cases, I remember getting too many false positives for other use cases.<p>This was many years ago though (2018-2019?) before the genAI craze. Wonder if it has improved or not; or if this product is any better than its competitors.</div><br/></div></div><div id="41927037" class="c"><input type="checkbox" id="c-41927037" checked=""/><div class="controls bullet"><span class="by">pj_mukh</span><span>|</span><a href="#41931116">prev</a><span>|</span><a href="#41927555">next</a><span>|</span><label class="collapse" for="c-41927037">[-]</label><label class="expand" for="c-41927037">[2 more]</label></div><br/><div class="children"><div class="content">This is super cool. 
As a question, are the instructions re-generated from the instruction tokens everytime. While maybe costly, this feels like it would be robust to small changes in the app (and component name changes etc.). Does that make sense?</div><br/><div id="41927818" class="c"><input type="checkbox" id="c-41927818" checked=""/><div class="controls bullet"><span class="by">chrtng</span><span>|</span><a href="#41927037">parent</a><span>|</span><a href="#41927555">next</a><span>|</span><label class="collapse" for="c-41927818">[-]</label><label class="expand" for="c-41927818">[1 more]</label></div><br/><div class="children"><div class="content">Great question! Yes, GPT Driver runs according to the test prompt each time, which makes it resilient to small changes. To speed up execution, we also use a caching mechanism that runs quickly if nothing has changed, and only uses the models when needed.</div><br/></div></div></div></div><div id="41927555" class="c"><input type="checkbox" id="c-41927555" checked=""/><div class="controls bullet"><span class="by">bluelightning2k</span><span>|</span><a href="#41927037">prev</a><span>|</span><a href="#41926904">next</a><span>|</span><label class="collapse" for="c-41927555">[-]</label><label class="expand" for="c-41927555">[2 more]</label></div><br/><div class="children"><div class="content">Genuinely curious, is the timing on this immediately after Claude computer use a coincidence? Or was that like the last missing piece, or a kind of threat which expedited things</div><br/><div id="41929227" class="c"><input type="checkbox" id="c-41929227" checked=""/><div class="controls bullet"><span class="by">cschiller</span><span>|</span><a href="#41927555">parent</a><span>|</span><a href="#41926904">next</a><span>|</span><label class="collapse" for="c-41929227">[-]</label><label class="expand" for="c-41929227">[1 more]</label></div><br/><div class="children"><div class="content">Good call! The timing was actually a coincidence, but not unexpected. OpenAI had already announced their plans to work on a desktop agent, so it was only a matter of time.<p>From our tests, even the latest model snapshots aren&#x27;t yet reliable enough in positional accuracy. That&#x27;s why we still rely on augmenting them with specialized object detection models. As foundational models continue to improve, we believe our QA suite - covering test case management, reporting, agent orchestration, and infrastructure - will become more relevant for the end user. Exciting times ahead!</div><br/></div></div></div></div><div id="41926904" class="c"><input type="checkbox" id="c-41926904" checked=""/><div class="controls bullet"><span class="by">tomatohs</span><span>|</span><a href="#41927555">prev</a><span>|</span><a href="#41927313">next</a><span>|</span><label class="collapse" for="c-41926904">[-]</label><label class="expand" for="c-41926904">[1 more]</label></div><br/><div class="children"><div class="content">Curious what happened to the other YC Mobile AI E2E company, CamelQA (YC W24). They pivoted to AI assistants. Could be good lessons there if you&#x27;re not already in touch with them.</div><br/></div></div><div id="41927313" class="c"><input type="checkbox" id="c-41927313" checked=""/><div class="controls bullet"><span class="by">doublerebel</span><span>|</span><a href="#41926904">prev</a><span>|</span><a href="#41926071">next</a><span>|</span><label class="collapse" for="c-41927313">[-]</label><label class="expand" for="c-41927313">[3 more]</label></div><br/><div class="children"><div class="content">How does this compare with Test.ai (now aka Testers.ai) who have offered basically this same service for the last 5 years?</div><br/><div id="41927669" class="c"><input type="checkbox" id="c-41927669" checked=""/><div class="controls bullet"><span class="by">tauntz</span><span>|</span><a href="#41927313">parent</a><span>|</span><a href="#41926071">next</a><span>|</span><label class="collapse" for="c-41927669">[-]</label><label class="expand" for="c-41927669">[2 more]</label></div><br/><div class="children"><div class="content">Totally offtopic but I looked at testers.ai and noticed the following from the terms of service:<p>&gt; Individuals with the last name &quot;Bach&quot; or &quot;Bolton&quot; are prohibited from using, referencing, or commenting on this website or any of its content.<p>..and now I&#x27;m curious to know the backstory for this :)</div><br/><div id="41932195" class="c"><input type="checkbox" id="c-41932195" checked=""/><div class="controls bullet"><span class="by">LeFever</span><span>|</span><a href="#41927313">root</a><span>|</span><a href="#41927669">parent</a><span>|</span><a href="#41926071">next</a><span>|</span><label class="collapse" for="c-41932195">[-]</label><label class="expand" for="c-41932195">[1 more]</label></div><br/><div class="children"><div class="content">John Bolton and James Bach are the founders of RST [1] and generally big names in the “formal” software testing space. Presumably the testers.ai folks aren’t fans. :p<p>[1] <a href="https:&#x2F;&#x2F;rapid-software-testing.com&#x2F;authors&#x2F;" rel="nofollow">https:&#x2F;&#x2F;rapid-software-testing.com&#x2F;authors&#x2F;</a></div><br/></div></div></div></div></div></div><div id="41926071" class="c"><input type="checkbox" id="c-41926071" checked=""/><div class="controls bullet"><span class="by">archerx</span><span>|</span><a href="#41927313">prev</a><span>|</span><a href="#41925417">next</a><span>|</span><label class="collapse" for="c-41926071">[-]</label><label class="expand" for="c-41926071">[2 more]</label></div><br/><div class="children"><div class="content">Curious question, what ever happened with the OpenAI drama with trademarking “GPT”. I’m guessing they were not successful?</div><br/><div id="41926164" class="c"><input type="checkbox" id="c-41926164" checked=""/><div class="controls bullet"><span class="by">chrtng</span><span>|</span><a href="#41926071">parent</a><span>|</span><a href="#41925417">next</a><span>|</span><label class="collapse" for="c-41926164">[-]</label><label class="expand" for="c-41926164">[1 more]</label></div><br/><div class="children"><div class="content">From what we understand the term GPT was deemed too general for OpenAI to claim as its own.<p><a href="https:&#x2F;&#x2F;www.theverge.com&#x2F;2024&#x2F;2&#x2F;16&#x2F;24075304&#x2F;trademark-pto-openai-gpt-deny" rel="nofollow">https:&#x2F;&#x2F;www.theverge.com&#x2F;2024&#x2F;2&#x2F;16&#x2F;24075304&#x2F;trademark-pto-op...</a></div><br/></div></div></div></div><div id="41925417" class="c"><input type="checkbox" id="c-41925417" checked=""/><div class="controls bullet"><span class="by">101008</span><span>|</span><a href="#41926071">prev</a><span>|</span><a href="#41927546">next</a><span>|</span><label class="collapse" for="c-41925417">[-]</label><label class="expand" for="c-41925417">[22 more]</label></div><br/><div class="children"><div class="content">Still interesting how a lot of companies offer a LLM (non-deterministic) solution for deterministic problems.</div><br/><div id="41925509" class="c"><input type="checkbox" id="c-41925509" checked=""/><div class="controls bullet"><span class="by">chairhairair</span><span>|</span><a href="#41925417">parent</a><span>|</span><a href="#41925488">next</a><span>|</span><label class="collapse" for="c-41925509">[-]</label><label class="expand" for="c-41925509">[5 more]</label></div><br/><div class="children"><div class="content">This fundamental issue seems to be totally lost on the LLM-heads.<p>I do not want additional uncertainty deep in the development cycle.<p>I can tolerate the uncertainty while I&#x27;m writing. That&#x27;s where there is a good fit for these fuzzy LLMs. Anything past the cutting room floor and you are injecting uncertainty where it isn&#x27;t tolerable.<p>I definitely do not want additional uncertainty in production. That&#x27;s where the &quot;large action model&quot; and &quot;computer use&quot; and &quot;autonomous agent&quot; cases totally fall apart.<p>It&#x27;s a mindless extension something like: &quot;this product good for writing... let&#x27;s let it write to prod!&quot;</div><br/><div id="41925919" class="c"><input type="checkbox" id="c-41925919" checked=""/><div class="controls bullet"><span class="by">usernameis42</span><span>|</span><a href="#41925417">root</a><span>|</span><a href="#41925509">parent</a><span>|</span><a href="#41925488">next</a><span>|</span><label class="collapse" for="c-41925919">[-]</label><label class="expand" for="c-41925919">[4 more]</label></div><br/><div class="children"><div class="content">Same goes with the real people, we all can do mistakes and AI Agents would get better over time, and will be ahead of many specialist pretty soon, but probably not perfect before AGI, just as we are.</div><br/><div id="41927210" class="c"><input type="checkbox" id="c-41927210" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#41925417">root</a><span>|</span><a href="#41925919">parent</a><span>|</span><a href="#41926275">next</a><span>|</span><label class="collapse" for="c-41927210">[-]</label><label class="expand" for="c-41927210">[1 more]</label></div><br/><div class="children"><div class="content">One of the advantages of automation has traditionally been that it cuts out the indeterminacy and variability inherent in real people.</div><br/></div></div><div id="41926275" class="c"><input type="checkbox" id="c-41926275" checked=""/><div class="controls bullet"><span class="by">conorjh</span><span>|</span><a href="#41925417">root</a><span>|</span><a href="#41925919">parent</a><span>|</span><a href="#41927210">prev</a><span>|</span><a href="#41925488">next</a><span>|</span><label class="collapse" for="c-41926275">[-]</label><label class="expand" for="c-41926275">[2 more]</label></div><br/><div class="children"><div class="content">your software has real people in it?</div><br/><div id="41927269" class="c"><input type="checkbox" id="c-41927269" checked=""/><div class="controls bullet"><span class="by">SkyBelow</span><span>|</span><a href="#41925417">root</a><span>|</span><a href="#41926275">parent</a><span>|</span><a href="#41925488">next</a><span>|</span><label class="collapse" for="c-41927269">[-]</label><label class="expand" for="c-41927269">[1 more]</label></div><br/><div class="children"><div class="content">Ideally it does.  Users, super users, admins, etc.  Though one might point out exactly how much effort we put into locking down what they can do.  I think one might be able to expand this to build up a persona for how LLMs should interface with software in production, but too many applications give them about the same level of access as a developer coding straight into production.  Then again, how many company leaders would approve of that as well if they thought it would get things done faster and at lower cost?</div><br/></div></div></div></div></div></div></div></div><div id="41925488" class="c"><input type="checkbox" id="c-41925488" checked=""/><div class="controls bullet"><span class="by">aksophist</span><span>|</span><a href="#41925417">parent</a><span>|</span><a href="#41925509">prev</a><span>|</span><a href="#41925863">next</a><span>|</span><label class="collapse" for="c-41925488">[-]</label><label class="expand" for="c-41925488">[5 more]</label></div><br/><div class="children"><div class="content">It’s only deterministic for each version of the app. Versions change: UI elements move, change their title slightly. Irrelevant promo popups appear, etc. For a deterministic solution, someone has to go and update the tests to handle all of that. Good ‘accessibility hygiene’ can help, but many apps lack that.<p>And then there are truly dynamic apps like games or simulators. There may be no accessibility info to deterministically code to.</div><br/><div id="41926019" class="c"><input type="checkbox" id="c-41926019" checked=""/><div class="controls bullet"><span class="by">usernameis42</span><span>|</span><a href="#41925417">root</a><span>|</span><a href="#41925488">parent</a><span>|</span><a href="#41925665">next</a><span>|</span><label class="collapse" for="c-41926019">[-]</label><label class="expand" for="c-41926019">[1 more]</label></div><br/><div class="children"><div class="content">There is great approach based on test-id strategy, basically it&#x27;s a requirement for the frontend teams to cover all interactive elements with test-id&#x27;s.<p>It allows to make tests less flaky and writing them is increasing dramatically, also works with mobile as well, usually elements for the main flows doesn&#x27;t change that often, you&#x27;ll still need to update them.<p>I did stable mobile UI tests with this approach as well, worked well</div><br/></div></div><div id="41925665" class="c"><input type="checkbox" id="c-41925665" checked=""/><div class="controls bullet"><span class="by">digging</span><span>|</span><a href="#41925417">root</a><span>|</span><a href="#41925488">parent</a><span>|</span><a href="#41926019">prev</a><span>|</span><a href="#41925863">next</a><span>|</span><label class="collapse" for="c-41925665">[-]</label><label class="expand" for="c-41925665">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Versions change: UI elements move, change their title slightly<p>Not randomly, I&#x27;d hope. I think you may be misunderstanding what deterministic means - or I am.</div><br/><div id="41925767" class="c"><input type="checkbox" id="c-41925767" checked=""/><div class="controls bullet"><span class="by">MattDaEskimo</span><span>|</span><a href="#41925417">root</a><span>|</span><a href="#41925665">parent</a><span>|</span><a href="#41926517">next</a><span>|</span><label class="collapse" for="c-41925767">[-]</label><label class="expand" for="c-41925767">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s crazy to have people so out of their league try to argue against well established meanings.<p>A testing framework requires determinism. If something changes the team should know and adjust.<p>AI could play a bit in easing this adjustment and tests but it&#x27;s not a driver in these tests.</div><br/></div></div><div id="41926517" class="c"><input type="checkbox" id="c-41926517" checked=""/><div class="controls bullet"><span class="by">minhaz23</span><span>|</span><a href="#41925417">root</a><span>|</span><a href="#41925665">parent</a><span>|</span><a href="#41925767">prev</a><span>|</span><a href="#41925863">next</a><span>|</span><label class="collapse" for="c-41926517">[-]</label><label class="expand" for="c-41926517">[1 more]</label></div><br/><div class="children"><div class="content">Ever worked with extjs? :&#x2F;</div><br/></div></div></div></div></div></div><div id="41925863" class="c"><input type="checkbox" id="c-41925863" checked=""/><div class="controls bullet"><span class="by">cschiller</span><span>|</span><a href="#41925417">parent</a><span>|</span><a href="#41925488">prev</a><span>|</span><a href="#41926614">next</a><span>|</span><label class="collapse" for="c-41925863">[-]</label><label class="expand" for="c-41925863">[1 more]</label></div><br/><div class="children"><div class="content">I agree that it can seem counterintuitive at first to apply LLM solutions to testing. However, in end-to-end testing, we’ve found that introducing a level of flexibility can actually be beneficial.<p>Take, for example, scenarios involving social logins or payments where external webviews are opened. These often trigger cookie consent forms or other unexpected elements, which the app developer has limited control over. The complexity increases when these elements have unstable identifiers or frequently changing attributes. In such cases, even though the core functionality (e.g., logging in) works as expected, traditional test automation often fails, requiring constant maintenance.<p>The key, as to other comments, is ensuring the solution is good at distinguishing between meaningful test issues and non issues.</div><br/></div></div><div id="41926614" class="c"><input type="checkbox" id="c-41926614" checked=""/><div class="controls bullet"><span class="by">devjab</span><span>|</span><a href="#41925417">parent</a><span>|</span><a href="#41925863">prev</a><span>|</span><a href="#41925929">next</a><span>|</span><label class="collapse" for="c-41926614">[-]</label><label class="expand" for="c-41926614">[1 more]</label></div><br/><div class="children"><div class="content">I think it’s less of an issue for e2e testing because e2e testing sucks. If teams did it well in general you would be completely correct, but in many places a LLM will be better even if it hallucinates. As such I think there will be a decent market for products like this, even if they aren’t may not even really be testing what you think they are testing. Simply because that may well be way better than the e2e testing many places already do.<p>In many cases you’re correct though. We have a few libraries where we won’t use Typescript because even though it might transpire 99% correctly, the fact that we have to check, is too much work for it to be worth our time in those cases. I think LLMs are similar, once in a while you’re not going to want them because checking their work takes too much resources, but for a lot of stuff you can use them. Especially if your e2e testing is really just pseudo jobbing because some middle manager wanted it, which it unfortunately is far too often. If you work in such a place you’re going to recommend the path of least resistance and if that’s LLM powered then it’s LLM powered.<p>On the less bleak and pessimistic side, if the LLM e2e output is good enough to be less resource consuming, even if you have to go over it, then it’s still a good business case.</div><br/></div></div><div id="41925929" class="c"><input type="checkbox" id="c-41925929" checked=""/><div class="controls bullet"><span class="by">batikha</span><span>|</span><a href="#41925417">parent</a><span>|</span><a href="#41926614">prev</a><span>|</span><a href="#41925532">next</a><span>|</span><label class="collapse" for="c-41925929">[-]</label><label class="expand" for="c-41925929">[1 more]</label></div><br/><div class="children"><div class="content">I work in the field and built a tool that has way less flakiness than deterministic solutions. 
The issue is testing environments are always imperfect because (a) they are stateful and (b) there&#x27;s always some randomness in actual production software. Some teams have very clean testing environment but most don&#x27;t.<p>So being non-deterministic is actually an advantage, in practice.</div><br/></div></div><div id="41925532" class="c"><input type="checkbox" id="c-41925532" checked=""/><div class="controls bullet"><span class="by">joshuanapoli</span><span>|</span><a href="#41925417">parent</a><span>|</span><a href="#41925929">prev</a><span>|</span><a href="#41925487">next</a><span>|</span><label class="collapse" for="c-41925532">[-]</label><label class="expand" for="c-41925532">[5 more]</label></div><br/><div class="children"><div class="content">I think that the hope&#x2F;dream here is to make end-to-end tests less flakey. It would be great to have navigation and assertions commands that are robust against simple changes in the app that aren&#x27;t relevant to the test case.</div><br/><div id="41925564" class="c"><input type="checkbox" id="c-41925564" checked=""/><div class="controls bullet"><span class="by">chairhairair</span><span>|</span><a href="#41925417">root</a><span>|</span><a href="#41925532">parent</a><span>|</span><a href="#41925487">next</a><span>|</span><label class="collapse" for="c-41925564">[-]</label><label class="expand" for="c-41925564">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just a dream then.<p>It&#x27;s completely at-odds with the strengths of LLMs (fuzzy associations, rough summaries, naive co-thinking).</div><br/><div id="41925715" class="c"><input type="checkbox" id="c-41925715" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#41925417">root</a><span>|</span><a href="#41925564">parent</a><span>|</span><a href="#41925487">next</a><span>|</span><label class="collapse" for="c-41925715">[-]</label><label class="expand" for="c-41925715">[3 more]</label></div><br/><div class="children"><div class="content">Fuzzy associations seem relevant? Interact with the UI based on what it looks like, not the specific implementation details.</div><br/><div id="41925932" class="c"><input type="checkbox" id="c-41925932" checked=""/><div class="controls bullet"><span class="by">chairhairair</span><span>|</span><a href="#41925417">root</a><span>|</span><a href="#41925715">parent</a><span>|</span><a href="#41925487">next</a><span>|</span><label class="collapse" for="c-41925932">[-]</label><label class="expand" for="c-41925932">[2 more]</label></div><br/><div class="children"><div class="content">No. Both of the requirements &quot;to interact&quot; and &quot;based on what it looks like&quot; require unshakable foundations in reality - which current models clearly do not have.<p>They will inevitably hallucinate interactions and observations and therefore decrease reliability. Worse, they will inject a pervasive sense of doubt into the reliability of any tests they interact with.</div><br/><div id="41927111" class="c"><input type="checkbox" id="c-41927111" checked=""/><div class="controls bullet"><span class="by">tomatohs</span><span>|</span><a href="#41925417">root</a><span>|</span><a href="#41925932">parent</a><span>|</span><a href="#41925487">next</a><span>|</span><label class="collapse" for="c-41927111">[-]</label><label class="expand" for="c-41927111">[1 more]</label></div><br/><div class="children"><div class="content">&gt; unshakable foundations in reality<p>Yes, you are correct that it entirely lays in the reputation of the AI.<p>This discussion leads to interesting question, which is &quot;what is quality?<i>&quot;<p>Quality is determined by perception. If we can agree that an AI is acting like a user and it can use your website, we can assume that a user can use your website and therefor it is &quot;quality&quot;.<p></i>For more, read &quot;Zen and the Art of Motorcycle Maintenance&quot;</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41925487" class="c"><input type="checkbox" id="c-41925487" checked=""/><div class="controls bullet"><span class="by">worldsayshi</span><span>|</span><a href="#41925417">parent</a><span>|</span><a href="#41925532">prev</a><span>|</span><a href="#41925459">next</a><span>|</span><label class="collapse" for="c-41925487">[-]</label><label class="expand" for="c-41925487">[2 more]</label></div><br/><div class="children"><div class="content">I would assume that the test runner translates the natural language instruction into a deterministic selector and only re-does that translation when the selector fails. At least that&#x27;s how I would try to implement it..</div><br/><div id="41926825" class="c"><input type="checkbox" id="c-41926825" checked=""/><div class="controls bullet"><span class="by">tomatohs</span><span>|</span><a href="#41925417">root</a><span>|</span><a href="#41925487">parent</a><span>|</span><a href="#41925459">next</a><span>|</span><label class="collapse" for="c-41926825">[-]</label><label class="expand" for="c-41926825">[1 more]</label></div><br/><div class="children"><div class="content">This is the right idea and how we do it at TestDriver.ai. The deterministic selector still has about 20% fuzz matching rate, and if it fails it trys to recover.</div><br/></div></div></div></div><div id="41925459" class="c"><input type="checkbox" id="c-41925459" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#41925417">parent</a><span>|</span><a href="#41925487">prev</a><span>|</span><a href="#41927546">next</a><span>|</span><label class="collapse" for="c-41925459">[-]</label><label class="expand" for="c-41925459">[1 more]</label></div><br/><div class="children"><div class="content">Tbf, users are also non-deterministic, so if LLM testing like this does catch on, it’ll be in the same realm as chaos testing.</div><br/></div></div></div></div><div id="41927546" class="c"><input type="checkbox" id="c-41927546" checked=""/><div class="controls bullet"><span class="by">alexwordxxx</span><span>|</span><a href="#41925417">prev</a><span>|</span><a href="#41925881">next</a><span>|</span><label class="collapse" for="c-41927546">[-]</label><label class="expand" for="c-41927546">[1 more]</label></div><br/><div class="children"><div class="content">Hey <a href="https:&#x2F;&#x2F;google.com" rel="nofollow">https:&#x2F;&#x2F;google.com</a></div><br/></div></div><div id="41925881" class="c"><input type="checkbox" id="c-41925881" checked=""/><div class="controls bullet"><span class="by">aksophist</span><span>|</span><a href="#41927546">prev</a><span>|</span><a href="#41925703">next</a><span>|</span><label class="collapse" for="c-41925881">[-]</label><label class="expand" for="c-41925881">[2 more]</label></div><br/><div class="children"><div class="content">how do you evaluate your tool, and have you published your evaluation along with the metrics?</div><br/><div id="41927827" class="c"><input type="checkbox" id="c-41927827" checked=""/><div class="controls bullet"><span class="by">chrtng</span><span>|</span><a href="#41925881">parent</a><span>|</span><a href="#41925703">next</a><span>|</span><label class="collapse" for="c-41927827">[-]</label><label class="expand" for="c-41927827">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for your question! While we haven&#x27;t published a formal evaluation yet, it&#x27;s something we are working toward. Currently, we rely mostly on human reviews to monitor and assess LLM outputs. We also maintain a golden test suite that is run against every release to ensure consistency and quality over time, using regex-based evaluations.<p>Our key metrics include the time and cost per agentic loop, as well as the false positive rate for a full end-to-end test. If you have any specific benchmarks or evaluation metrics you&#x27;d suggest, we&#x27;d be happy to hear them!</div><br/></div></div></div></div><div id="41925703" class="c"><input type="checkbox" id="c-41925703" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#41925881">prev</a><span>|</span><a href="#41928638">next</a><span>|</span><label class="collapse" for="c-41925703">[-]</label><label class="expand" for="c-41925703">[2 more]</label></div><br/><div class="children"><div class="content">How does this compare to Robin by mobile.dev; the same guys that built Maestro? [0]<p>That has around 95% of what GPT Driver does and has the potential to do Web E2E testing.<p>[0] <a href="https:&#x2F;&#x2F;maestro.mobile.dev" rel="nofollow">https:&#x2F;&#x2F;maestro.mobile.dev</a></div><br/><div id="41926032" class="c"><input type="checkbox" id="c-41926032" checked=""/><div class="controls bullet"><span class="by">cschiller</span><span>|</span><a href="#41925703">parent</a><span>|</span><a href="#41928638">next</a><span>|</span><label class="collapse" for="c-41926032">[-]</label><label class="expand" for="c-41926032">[1 more]</label></div><br/><div class="children"><div class="content">One of our customers recently compared GPTD with Maestro’s Robin (formerly App Quality CoPilot). Their mobile platform engineering manager highlighted three key reasons for choosing us: lack of frustration, ease of implementation, and reliability.<p>To be more concrete their words were:
- “What you define, you can tweak, touch the detail, and customize, saving you time.”
- “You don’t entirely rely on AI. You stay involved, avoiding misinterpretations by AI.”
- “Flexibility to refine, by using templates and triggering partial tests, features that come from real-world experience. This speeds up the process significantly.”<p>Our understanding is that because we launched the first version of GPT Driver in April 2023, we’ve built it in an “AI-native” way, while other tools are simply adding AI-based features on top. We worked closely with leading mobile teams, including Duolingo, to ensure we stay as aligned as possible with real-world challenges.<p>While our focus is on mobile, GPT Driver also works effectively on web platforms.</div><br/></div></div></div></div><div id="41928638" class="c"><input type="checkbox" id="c-41928638" checked=""/><div class="controls bullet"><span class="by">lihua919</span><span>|</span><a href="#41925703">prev</a><span>|</span><a href="#41925832">next</a><span>|</span><label class="collapse" for="c-41928638">[-]</label><label class="expand" for="c-41928638">[1 more]</label></div><br/><div class="children"><div class="content">interesting</div><br/></div></div><div id="41925832" class="c"><input type="checkbox" id="c-41925832" checked=""/><div class="controls bullet"><span class="by">iknownthing</span><span>|</span><a href="#41928638">prev</a><span>|</span><label class="collapse" for="c-41925832">[-]</label><label class="expand" for="c-41925832">[1 more]</label></div><br/><div class="children"><div class="content">no logo?</div><br/></div></div></div></div></div></div></div></body></html>