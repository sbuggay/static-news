<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1709110850068" as="style"/><link rel="stylesheet" href="styles.css?v=1709110850068"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://isburmistrov.substack.com/p/all-you-need-is-wide-events-not-metrics">All you need is Wide Events, not &quot;Metrics, Logs and Traces&quot;</a> <span class="domain">(<a href="https://isburmistrov.substack.com">isburmistrov.substack.com</a>)</span></div><div class="subtext"><span>talboren</span> | <span>133 comments</span></div><br/><div><div id="39534570" class="c"><input type="checkbox" id="c-39534570" checked=""/><div class="controls bullet"><span class="by">infogulch</span><span>|</span><a href="#39531022">next</a><span>|</span><label class="collapse" for="c-39534570">[-]</label><label class="expand" for="c-39534570">[7 more]</label></div><br/><div class="children"><div class="content">This thread has a lot of discussion about Wide Events &#x2F; Structured Logs (same thing) being too big at scale, and you should use metrics instead.<p>Why does it have to be an either&#x2F;or thing? Couldn&#x27;t you hook up a metrics extractor to the event stream and convert your structured logs to compact metrics in-process before expensive serde&#x2F;encoding? With this your choice doesn&#x27;t have to affect the code, just write slogs all the time; if you want structured logs then then output them, but if you only want metrics then switch to the metrics extractor slog handler.<p>Futher, has nobody tried writing structured logs to parquet files and shipping out 1MB blocks at once? Way less serde&#x2F;encoding overhead, and column oriented layout compresses like crazy with built-in dictionary and delta encodings.</div><br/><div id="39534857" class="c"><input type="checkbox" id="c-39534857" checked=""/><div class="controls bullet"><span class="by">pat2man</span><span>|</span><a href="#39534570">parent</a><span>|</span><a href="#39534905">next</a><span>|</span><label class="collapse" for="c-39534857">[-]</label><label class="expand" for="c-39534857">[1 more]</label></div><br/><div class="children"><div class="content">The open telemetry collector does just that. <a href="https:&#x2F;&#x2F;github.com&#x2F;open-telemetry&#x2F;opentelemetry-collector-contrib&#x2F;blob&#x2F;main&#x2F;processor&#x2F;spanmetricsprocessor&#x2F;README.md">https:&#x2F;&#x2F;github.com&#x2F;open-telemetry&#x2F;opentelemetry-collector-co...</a></div><br/></div></div><div id="39534905" class="c"><input type="checkbox" id="c-39534905" checked=""/><div class="controls bullet"><span class="by">infogulch</span><span>|</span><a href="#39534570">parent</a><span>|</span><a href="#39534857">prev</a><span>|</span><a href="#39534874">next</a><span>|</span><label class="collapse" for="c-39534905">[-]</label><label class="expand" for="c-39534905">[1 more]</label></div><br/><div class="children"><div class="content">This package implements the parquet idea for Go&#x27;s slog package: <a href="https:&#x2F;&#x2F;github.com&#x2F;samber&#x2F;slog-parquet">https:&#x2F;&#x2F;github.com&#x2F;samber&#x2F;slog-parquet</a></div><br/></div></div><div id="39534874" class="c"><input type="checkbox" id="c-39534874" checked=""/><div class="controls bullet"><span class="by">lowbloodsugar</span><span>|</span><a href="#39534570">parent</a><span>|</span><a href="#39534905">prev</a><span>|</span><a href="#39531022">next</a><span>|</span><label class="collapse" for="c-39534874">[-]</label><label class="expand" for="c-39534874">[4 more]</label></div><br/><div class="children"><div class="content">&gt;too big at scale<p>Bigger than meta? o_0</div><br/><div id="39534906" class="c"><input type="checkbox" id="c-39534906" checked=""/><div class="controls bullet"><span class="by">pavlov</span><span>|</span><a href="#39534570">root</a><span>|</span><a href="#39534874">parent</a><span>|</span><a href="#39534897">prev</a><span>|</span><a href="#39531022">next</a><span>|</span><label class="collapse" for="c-39534906">[-]</label><label class="expand" for="c-39534906">[2 more]</label></div><br/><div class="children"><div class="content">The scale isn’t a problem if you also have Meta’s profit margins. But most businesses don’t…</div><br/><div id="39535579" class="c"><input type="checkbox" id="c-39535579" checked=""/><div class="controls bullet"><span class="by">rmbyrro</span><span>|</span><a href="#39534570">root</a><span>|</span><a href="#39534906">parent</a><span>|</span><a href="#39531022">next</a><span>|</span><label class="collapse" for="c-39535579">[-]</label><label class="expand" for="c-39535579">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what I don&#x27;t understand about observability services.<p>They complicate things so much that it&#x27;s sometimes more expensive to observe than to RUN the infra being observed.</div><br/></div></div></div></div></div></div></div></div><div id="39531022" class="c"><input type="checkbox" id="c-39531022" checked=""/><div class="controls bullet"><span class="by">Osmose</span><span>|</span><a href="#39534570">prev</a><span>|</span><a href="#39531837">next</a><span>|</span><label class="collapse" for="c-39531022">[-]</label><label class="expand" for="c-39531022">[12 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t an unknown idea outside of Meta, it&#x27;s just really expensive, especially if you&#x27;re using a vendor and not building your own tooling. Prohibitively so, even with sampling.</div><br/><div id="39531054" class="c"><input type="checkbox" id="c-39531054" checked=""/><div class="controls bullet"><span class="by">ricardobeat</span><span>|</span><a href="#39531022">parent</a><span>|</span><a href="#39531177">next</a><span>|</span><label class="collapse" for="c-39531054">[-]</label><label class="expand" for="c-39531054">[7 more]</label></div><br/><div class="children"><div class="content">Exactly. When they say<p>&gt; Unlike with prometheus, however, with Wide Events approach we don’t need to worry about cardinality<p>This is hinting at the hidden reason why not everyone does it. You have to &#x27;worry&#x27; about cardinality because Prometheus is pre-aggregating data so you can visualize it fast, and optimizing storage. If you want the same speed on a massive PB-scale data lake, with an infinite amount of  unstructured data, and in the cloud instead of your own datacenters, it&#x27;s gonna cost you <i>a lot</i>, and for most companies it is not a sensible expense.<p>It does work at smaller scale though, we once had an in-house system like this that worked well. Eventually user events were moved to MixPanel, and everything else to Datadog, metrics&#x2F;logs&#x2F;traces + a migration to OpenTel. It took months and added 2-digit monthly bills, and in the end debugging or resolving incidents wasn&#x27;t much improved over having instant access to events and business metrics. Whoever figures out a system that can do &quot;wide events&quot; in a cost-effective way from startup to unicorn scale will absolutely make a killing.</div><br/><div id="39531516" class="c"><input type="checkbox" id="c-39531516" checked=""/><div class="controls bullet"><span class="by">hosh</span><span>|</span><a href="#39531022">root</a><span>|</span><a href="#39531054">parent</a><span>|</span><a href="#39533171">next</a><span>|</span><label class="collapse" for="c-39531516">[-]</label><label class="expand" for="c-39531516">[3 more]</label></div><br/><div class="children"><div class="content">Not everything emits wide events. Maybe you can get the entire application layer like that, but there is also value in logs and metrics emitted from the rest of the infra stack.<p>To be fair, you could probably store and represent everything as wide events and build visualization tools out of that that can combine everything together, even if they are sourced from something else.</div><br/><div id="39532144" class="c"><input type="checkbox" id="c-39532144" checked=""/><div class="controls bullet"><span class="by">lelandbatey</span><span>|</span><a href="#39531022">root</a><span>|</span><a href="#39531516">parent</a><span>|</span><a href="#39533171">next</a><span>|</span><label class="collapse" for="c-39532144">[-]</label><label class="expand" for="c-39532144">[2 more]</label></div><br/><div class="children"><div class="content">Wide events seem to be &quot;structured logs with focused schemas&quot; (maybe also published in a special way beyond writing to stdout) but most places I&#x27;ve worked would call that &quot;logging&quot; not &quot;wide events&quot;.<p>The reasons we don&#x27;t use them for everything are as others in the thread say: it&#x27;s expensive. Metrics (just the numbers, nothing else) can be compressed and aggregated extremely efficiently, hence cheaply. Logs are more expensive due to their arbitrary contents.<p>It&#x27;s all due to expense really.</div><br/><div id="39532383" class="c"><input type="checkbox" id="c-39532383" checked=""/><div class="controls bullet"><span class="by">isburmistrov</span><span>|</span><a href="#39531022">root</a><span>|</span><a href="#39532144">parent</a><span>|</span><a href="#39533171">next</a><span>|</span><label class="collapse" for="c-39532383">[-]</label><label class="expand" for="c-39532383">[1 more]</label></div><br/><div class="children"><div class="content">Columnar storage stores data very efficiently, too - because it compresses data of a similar nature (columns). 
Check e.g. ClickHouse on this matter: <a href="https:&#x2F;&#x2F;clickhouse.com&#x2F;docs&#x2F;en&#x2F;about-us&#x2F;distinctive-features" rel="nofollow">https:&#x2F;&#x2F;clickhouse.com&#x2F;docs&#x2F;en&#x2F;about-us&#x2F;distinctive-features</a>, <a href="https:&#x2F;&#x2F;clickhouse.com&#x2F;blog&#x2F;working-with-time-series-data-and-functions-ClickHouse" rel="nofollow">https:&#x2F;&#x2F;clickhouse.com&#x2F;blog&#x2F;working-with-time-series-data-an...</a><p>So I wouldn&#x27;t say that events are &quot;expensive&quot; while metrics are &quot;cheap&quot; - both depend on the actual implementation, and events can be cheap too.<p>And so of course if you have to optimise things, you would need to drop some information you pass to the events, but you would need to do the same for metrics (reduce the number of metrics emitted, reduce the prometheus labels,...).</div><br/></div></div></div></div></div></div><div id="39533171" class="c"><input type="checkbox" id="c-39533171" checked=""/><div class="controls bullet"><span class="by">AeroNotix</span><span>|</span><a href="#39531022">root</a><span>|</span><a href="#39531054">parent</a><span>|</span><a href="#39531516">prev</a><span>|</span><a href="#39531177">next</a><span>|</span><label class="collapse" for="c-39533171">[-]</label><label class="expand" for="c-39533171">[3 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;victoriametrics.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;victoriametrics.com&#x2F;</a> would definitely recommend anyone having performance issues with Prometheus to give VictoriaMetrics a try.</div><br/><div id="39535332" class="c"><input type="checkbox" id="c-39535332" checked=""/><div class="controls bullet"><span class="by">jonasdegendt</span><span>|</span><a href="#39531022">root</a><span>|</span><a href="#39533171">parent</a><span>|</span><a href="#39533448">next</a><span>|</span><label class="collapse" for="c-39535332">[-]</label><label class="expand" for="c-39535332">[1 more]</label></div><br/><div class="children"><div class="content">Once you plug long term storage onto your Prometheus, do you really need the main Prometheus instances anymore?<p>Here’s an article about this idea: <a href="https:&#x2F;&#x2F;datadrivendrivel.com&#x2F;posts&#x2F;rmrfprometheus&#x2F;" rel="nofollow">https:&#x2F;&#x2F;datadrivendrivel.com&#x2F;posts&#x2F;rmrfprometheus&#x2F;</a><p>You can substitute the Grafana Agents for OTEL collectors as well.</div><br/></div></div><div id="39533448" class="c"><input type="checkbox" id="c-39533448" checked=""/><div class="controls bullet"><span class="by">camel_gopher</span><span>|</span><a href="#39531022">root</a><span>|</span><a href="#39533171">parent</a><span>|</span><a href="#39535332">prev</a><span>|</span><a href="#39531177">next</a><span>|</span><label class="collapse" for="c-39533448">[-]</label><label class="expand" for="c-39533448">[1 more]</label></div><br/><div class="children"><div class="content">And comes with all the downsides of Prometheus as well</div><br/></div></div></div></div></div></div><div id="39531177" class="c"><input type="checkbox" id="c-39531177" checked=""/><div class="controls bullet"><span class="by">mlhpdx</span><span>|</span><a href="#39531022">parent</a><span>|</span><a href="#39531054">prev</a><span>|</span><a href="#39534340">next</a><span>|</span><label class="collapse" for="c-39531177">[-]</label><label class="expand" for="c-39531177">[3 more]</label></div><br/><div class="children"><div class="content">I don’t know that’s true. My last two very-not-meta-sized companies have both had systems that were very cost effective and essentially what the article describes. It’s not the simplest thing to put in place, but far from unapproachable.<p>I think on if the big hills is moving to a culture that values observability (or whatever you choose to call it, I prefer forensic debugging). It’s another thing to understand and worry about and it helps tremendously if there are good, highly visible examples of it.<p>Edit: Typo.</div><br/><div id="39531347" class="c"><input type="checkbox" id="c-39531347" checked=""/><div class="controls bullet"><span class="by">gtirloni</span><span>|</span><a href="#39531022">root</a><span>|</span><a href="#39531177">parent</a><span>|</span><a href="#39534340">next</a><span>|</span><label class="collapse" for="c-39531347">[-]</label><label class="expand" for="c-39531347">[2 more]</label></div><br/><div class="children"><div class="content">Could you share some specifics of how it could be approached?</div><br/><div id="39531620" class="c"><input type="checkbox" id="c-39531620" checked=""/><div class="controls bullet"><span class="by">hosh</span><span>|</span><a href="#39531022">root</a><span>|</span><a href="#39531347">parent</a><span>|</span><a href="#39534340">next</a><span>|</span><label class="collapse" for="c-39531620">[-]</label><label class="expand" for="c-39531620">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know what that commentor has in mind. My own experience building this up is to start with usable information and not try to instrument everything at once. Those are usually:<p>- some way to get to errors when they happen<p>- zeroing in on the key performance indicators for your application, and relating them to infra metrics, particularly resources (because cpu, mem, storage, and bandwidth costs money).<p>Unless you have both domain and infra knowledge, it will be hard to know ahead of time.<p>For a stateless web app backed by a db, you&#x27;re typically starting with:<p>- request metrics (req&#x2F;s, latency)<p>- authenticated user activity<p>- db metrics (such as what you&#x27;d get with pganalyze)<p>It&#x27;s when there are resource pressure that things get interesting. Here, you have product-fit, you have user traction and growth, but now your app is falling down because it is popular.<p>It is tempting to just crank things up horizontally and say, you&#x27;re trying to land-grab users ... but your team will never develop the discipline to develop scalable and reliable software. It&#x27;s here that you start adding instrumentation to find bottlenecks -- whether that is instrumenting spans, adding metrics, optimizing queries, etc. You also need to craft the dashboard to give actionable intelligence. Here&#x27;s where Datadog&#x27;s notebook feature is great -- you explore (and collaborate) with the notebook until you can find the bottleneck, and then export the useful metrics into a dashboard. Then you set up the monitoring, because you have found the key performance indicators.<p>It&#x27;s this active search to understand what is going on in _both_ app and infra that shows you the limits of the current architectural designs, guide what you need to do, and validate the architectural and engineering decisions for the future. This active search may involve tools beyond OpenTelemetry or Datadog or Honeycomb -- maybe you have to attach a REPL, or go poking around a memory profiler.<p>What you _don&#x27;t_ do is blindly adding these things because having the capability somehow makes things better. Rather, you incrementally improve your capability in order to solve your present scalability and reliability problems with your app and its infra.</div><br/></div></div></div></div></div></div><div id="39534340" class="c"><input type="checkbox" id="c-39534340" checked=""/><div class="controls bullet"><span class="by">GauntletWizard</span><span>|</span><a href="#39531022">parent</a><span>|</span><a href="#39531177">prev</a><span>|</span><a href="#39531837">next</a><span>|</span><label class="collapse" for="c-39534340">[-]</label><label class="expand" for="c-39534340">[1 more]</label></div><br/><div class="children"><div class="content">I worked on Scuba, inside and outside of Meta (Interana), and yeah - It was expensive AF. I recommend focusing on metrics first. Use analytics logging sparingly, and understand the statistics of how metrics work, because without understanding those statistics you&#x27;ll misread your events anyway.<p>This is not to say that wide events aren&#x27;t worth it - For many things, something like Scuba or Bigquery are invaluable. There&#x27;s ways to optimize. But we&#x27;re talking about &quot;One of AWS&#x27;s largest machines&quot; vs &quot;A couple cores&quot;, and I suggest learning Prometheus first.</div><br/></div></div></div></div><div id="39531837" class="c"><input type="checkbox" id="c-39531837" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#39531022">prev</a><span>|</span><a href="#39531882">next</a><span>|</span><label class="collapse" for="c-39531837">[-]</label><label class="expand" for="c-39531837">[16 more]</label></div><br/><div class="children"><div class="content">This is essentially Amazon Coral’s service log format except service logs include cumulative metrics between log events. This surfaces in cloudwatch logs as metrics extraction and Logs Insights as structured log queries. The meta scuba is like a janky imitation of that tool chain<p>People point to Splunk and ELK but they fail to realize that inverted index based solutions algorithmically can’t scale to arbitrary sizes. I would rather point people to Grafana Loki and CloudWatch Logs Insights and the compromises they entail as not just the right model for “wide events” or structured logging based events and metrics. Their architectures allow you to scale at low costs to PB or even exabyte scale monitoring.</div><br/><div id="39535050" class="c"><input type="checkbox" id="c-39535050" checked=""/><div class="controls bullet"><span class="by">deschutes</span><span>|</span><a href="#39531837">parent</a><span>|</span><a href="#39532697">next</a><span>|</span><label class="collapse" for="c-39535050">[-]</label><label class="expand" for="c-39535050">[1 more]</label></div><br/><div class="children"><div class="content">As far as design and ergonomics go, I&#x27;d compare servicelogs to a pile of trash that may yet grow massive enough to accrete into a planetoid.<p>A text based format whose sole virtue is descending from a system that was composed mainly of bugs that had coalesced into perl scripts.<p>It&#x27;s not the basis of something you could even give away, let alone have people willingly pay you for their agony. Cloudwatch being rather alike in this regard.</div><br/></div></div><div id="39532697" class="c"><input type="checkbox" id="c-39532697" checked=""/><div class="controls bullet"><span class="by">jcgrillo</span><span>|</span><a href="#39531837">parent</a><span>|</span><a href="#39535050">prev</a><span>|</span><a href="#39532965">next</a><span>|</span><label class="collapse" for="c-39532697">[-]</label><label class="expand" for="c-39532697">[4 more]</label></div><br/><div class="children"><div class="content">One thing that really gets under my skin when I think about observability data is the abject waste we incur by shipping all this crap around as UTF-8 bytes. This post (from 1996!) puts us all to shame: <a href="https:&#x2F;&#x2F;lists.w3.org&#x2F;Archives&#x2F;Public&#x2F;www-logging&#x2F;1996May&#x2F;0000.html" rel="nofollow">https:&#x2F;&#x2F;lists.w3.org&#x2F;Archives&#x2F;Public&#x2F;www-logging&#x2F;1996May&#x2F;000...</a><p>Knowing the type of each field unlocks some interesting possibilities. If we can classify fields as STRING, INTEGER, UUID, FLOAT, TIMESTAMP, IP, etc we could store (and transmit!) them optimally. In particular, knowing whether we can delta-encode is important--if you have a timestamp column, storing the deltas (with varint or vbyte encoding) is way cheaper than storing each and every timestamp. Only store each string once, in a compressed way, and refer to it by ID (with smaller IDs for more frequent strings).<p>It&#x27;s sickening to imagine how much could be saved by exploiting redundancy in these data if we could just know the type of each field. You get some of this with formats like protocol buffers, but not enough.<p>Another thing, as you mention, is optimizing for search. Indexing everything seems like the wrong move. Maybe some partial indexing strategy? Rollups? Just do everything with mapreduce jobs? I don&#x27;t know what the right answer is but fully indexing data which are mostly write-only is definitely wrong.</div><br/><div id="39533483" class="c"><input type="checkbox" id="c-39533483" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#39531837">root</a><span>|</span><a href="#39532697">parent</a><span>|</span><a href="#39534248">next</a><span>|</span><label class="collapse" for="c-39533483">[-]</label><label class="expand" for="c-39533483">[1 more]</label></div><br/><div class="children"><div class="content">In my analysis of some exabyte scale log analytic systems the top two compute spend buckets was json serde and utf8 conversions.</div><br/></div></div><div id="39534248" class="c"><input type="checkbox" id="c-39534248" checked=""/><div class="controls bullet"><span class="by">ablob</span><span>|</span><a href="#39531837">root</a><span>|</span><a href="#39532697">parent</a><span>|</span><a href="#39533483">prev</a><span>|</span><a href="#39532947">next</a><span>|</span><label class="collapse" for="c-39534248">[-]</label><label class="expand" for="c-39534248">[1 more]</label></div><br/><div class="children"><div class="content">Storing by delta can bite you quite hard in the event of data corruption. Instead of 1 data point being affected it would cascade down.
Selecting specific ranges where the concrete bottom&#x2F;top as in &quot;give me everything between 1-2 pm from last Saturday&quot; might also become problematic.
I&#x27;m sure there&#x27;s a tradeoff to be had here; Weaving data-dependencies throughout your file certainly leaves a redundancy hole not everyone is willing to have.</div><br/></div></div><div id="39532947" class="c"><input type="checkbox" id="c-39532947" checked=""/><div class="controls bullet"><span class="by">heyoni</span><span>|</span><a href="#39531837">root</a><span>|</span><a href="#39532697">parent</a><span>|</span><a href="#39534248">prev</a><span>|</span><a href="#39532965">next</a><span>|</span><label class="collapse" for="c-39532947">[-]</label><label class="expand" for="c-39532947">[1 more]</label></div><br/><div class="children"><div class="content">That second part of only storing reference ids sounds like db normalizing but for logs. Seems reasonable though!</div><br/></div></div></div></div><div id="39532965" class="c"><input type="checkbox" id="c-39532965" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#39531837">parent</a><span>|</span><a href="#39532697">prev</a><span>|</span><a href="#39531882">next</a><span>|</span><label class="collapse" for="c-39532965">[-]</label><label class="expand" for="c-39532965">[10 more]</label></div><br/><div class="children"><div class="content">&gt; inverted index based solutions algorithmically can’t scale to arbitrary sizes<p>curious why do you think so? Inverted index can be sharded and built&#x2F;updated&#x2F;queried in parallel, so scale linearly.</div><br/><div id="39533445" class="c"><input type="checkbox" id="c-39533445" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#39531837">root</a><span>|</span><a href="#39532965">parent</a><span>|</span><a href="#39533046">next</a><span>|</span><label class="collapse" for="c-39533445">[-]</label><label class="expand" for="c-39533445">[6 more]</label></div><br/><div class="children"><div class="content">A few issues come up. First inverted indices can be sharded but the index insert patterns aren’t uniformly distributed but instead have a zipf distribution, which means your sharding scales proportional to the frequency of the most common token in the log. There are patches but in the end it sort of boils down to this.<p>Another issue is indexing up front is crazy expensive vs doing absolutely nothing but packing and time indexing, maybe some bloom indices. This is really important because the vast majority of log and event and telemetry in general is <i>never accessed</i>. Like 99.99% of it or more.<p>The technique of something like Loki is to batch data into micro batches and index them within the batches into a columnar store (like parquet of orc) and time index the micro batches. The query path is highly parallel and fairly expensive, but given the cost savings up front it’s a lot cheaper than up front indexing. You can turn the fan out knob on queries to any size and similar to MPP scale out databases such as Snowflake there’s not really much of an upper limit. Effectively everything from ingestion to query scales out linearly without uneven heat problems like you see in a sharded index.</div><br/><div id="39534058" class="c"><input type="checkbox" id="c-39534058" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#39531837">root</a><span>|</span><a href="#39533445">parent</a><span>|</span><a href="#39533046">next</a><span>|</span><label class="collapse" for="c-39534058">[-]</label><label class="expand" for="c-39534058">[5 more]</label></div><br/><div class="children"><div class="content">&gt; which means your sharding scales proportional to the frequency of the most common token in the log<p>inverted index entry for frequent token can be sharded itself. You can imagine that google doesn&#x27;t store all page ids in internet for the word &#x27;hello&#x27; on the same server.<p>&gt;  This is really important because the vast majority of log and event and telemetry in general is never accessed. Like 99.99% of it or more.<p>for log processing you are likely correct. I was more wondering in general why do you think inverted index doesn&#x27;t scale.</div><br/><div id="39534151" class="c"><input type="checkbox" id="c-39534151" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#39531837">root</a><span>|</span><a href="#39534058">parent</a><span>|</span><a href="#39533046">next</a><span>|</span><label class="collapse" for="c-39534151">[-]</label><label class="expand" for="c-39534151">[4 more]</label></div><br/><div class="children"><div class="content">These sorts of heat balancing sharding schemes are very difficult to implement and very expensive. As you see hot keys you need to split the hash space and rebalance within that space by reshuffling the shard data.<p>I’d note that also Google doesn’t bother keeping a perfect index because perfect fidelity isn’t necessary, unlike in a lot analytic or similar system where replication of ground truth is important. It’s much more important for Google to maintain high fidelity at the less frequent token side of the distribution and very low fidelity at the high frequency side. Logs can’t do that.</div><br/><div id="39534234" class="c"><input type="checkbox" id="c-39534234" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#39531837">root</a><span>|</span><a href="#39534151">parent</a><span>|</span><a href="#39533046">next</a><span>|</span><label class="collapse" for="c-39534234">[-]</label><label class="expand" for="c-39534234">[3 more]</label></div><br/><div class="children"><div class="content">&gt; As you see hot keys you need to split the hash space and rebalance within that space by reshuffling the shard data.<p>Correct, but I think it is solvable problem, if such approach adds value (e.g. one can build and start selling product like that).<p>Also, complexity may be not that hard.
Say, you are building some tiered shards:<p>- every entry at the beginning is single shard<p>- once shard reaches 10m records, you split it to 10 shards, which is not that difficult computation, and can be done in XXXms in single transaction</div><br/><div id="39534462" class="c"><input type="checkbox" id="c-39534462" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#39531837">root</a><span>|</span><a href="#39534234">parent</a><span>|</span><a href="#39533046">next</a><span>|</span><label class="collapse" for="c-39534462">[-]</label><label class="expand" for="c-39534462">[2 more]</label></div><br/><div class="children"><div class="content">It’s actually quite hard. It starts with being able to detect a hot key at all. It’s also not the case that heat is symmetric with size, in fact in an inverted index single entries can be very hot. Then it’s not about simply shuffling data (which isn’t simple as you outline - you need to salt the keys and they shuffle randomly, otherwise you don’t get uniformity), then you need to create cumulatively eventually consistent write replicas to balance write load while answering queries online in a strongly consistent way. Add to this any dynamic change in the index like this requires consistent online behavior (I.e., ingestion and queries don’t stop because you need to rebalance), and the hot keys are necessarily “large” in volume so back pressure can be enormous and queue draining itself can be expensive. Add to it you need stateful elastic infrastructure.<p>There are definitely products that offer these characteristics. S3 and dynamodb both do, even if you can’t see it. But it took many years of very intensive engineering to get it to work, and they have total control over the infrastructure and runtime behind an opaque api. Elastic search and Splunk are general purpose software packages that are installed by customers, and their data models are much more complex than objects or tables.</div><br/><div id="39534581" class="c"><input type="checkbox" id="c-39534581" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#39531837">root</a><span>|</span><a href="#39534462">parent</a><span>|</span><a href="#39533046">next</a><span>|</span><label class="collapse" for="c-39534581">[-]</label><label class="expand" for="c-39534581">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It’s also not the case that heat is symmetric with size, in fact in an inverted index single entries can be very hot.<p>I think you mixed two orthogonal topics: you first talked about frequent tokens, and now switched to hot keys(tokens which are frequently queried).<p>As for frequent tokens, I think I well described algorithm, and it looks simple, and I don&#x27;t see any issues there, if your metadata store (where you store info about shards and replicas) allows some kind of transactions (e.g. cocroachdb or similar).<p>For hot keys&#x2F;shards, as you pointed out, solution is to increase replication factor, but I think if shard is relatively small(10m IDs as in my example), adding another replica online is also fast, can be done in single transaction, and may not require all these movement you described.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39533046" class="c"><input type="checkbox" id="c-39533046" checked=""/><div class="controls bullet"><span class="by">jcgrillo</span><span>|</span><a href="#39531837">root</a><span>|</span><a href="#39532965">parent</a><span>|</span><a href="#39533445">prev</a><span>|</span><a href="#39531882">next</a><span>|</span><label class="collapse" for="c-39533046">[-]</label><label class="expand" for="c-39533046">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen situations where the cost of indexing all the logs (which were otherwise just written to a hierarchical structure in HDFS and queried with mapreduce jobs) in ES would have been highly significant--think like an uncomfortable fraction of total infrastructure spend. So, sure, you can make it scale linearly by adding enough nodes to keep up with write volume but that doesn&#x27;t mean it&#x27;s affordable. And then consider what that&#x27;s actually accomplishing for those dollars. You&#x27;re optimizing for quick search queries on data which you&#x27;ll mostly never query. Worth it?<p>EDIT: as a user, being able to just run mapreduce jobs over logs is a heck of a lot better experience IMO than trying to torture Kibana into giving me the answers I want.</div><br/><div id="39533490" class="c"><input type="checkbox" id="c-39533490" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#39531837">root</a><span>|</span><a href="#39533046">parent</a><span>|</span><a href="#39533127">next</a><span>|</span><label class="collapse" for="c-39533490">[-]</label><label class="expand" for="c-39533490">[1 more]</label></div><br/><div class="children"><div class="content">Loki basically works like a nice UI on mapreducing logs. The rest is exactly my point about ELK.</div><br/></div></div><div id="39533127" class="c"><input type="checkbox" id="c-39533127" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#39531837">root</a><span>|</span><a href="#39533046">parent</a><span>|</span><a href="#39533490">prev</a><span>|</span><a href="#39531882">next</a><span>|</span><label class="collapse" for="c-39533127">[-]</label><label class="expand" for="c-39533127">[1 more]</label></div><br/><div class="children"><div class="content">it could be shortcoming of ES specifically, since JVM is not ideal platform for heavy data crunching.</div><br/></div></div></div></div></div></div></div></div><div id="39531882" class="c"><input type="checkbox" id="c-39531882" checked=""/><div class="controls bullet"><span class="by">wiseguyeh</span><span>|</span><a href="#39531837">prev</a><span>|</span><a href="#39530964">next</a><span>|</span><label class="collapse" for="c-39531882">[-]</label><label class="expand" for="c-39531882">[6 more]</label></div><br/><div class="children"><div class="content">While I don&#x27;t have an opinion on wide events (AKA spans) replacing logs, there are benefits to metrics that warrant their existence:<p>1. They&#x27;re incredibly cheap to store. In Prometheus, is may cost you as little as 1 byte per sample (ignoring series overheads). Because they&#x27;re cheap, you can keep them for much longer and use them for long-term analysis of traffic, resource use, performance, etc. Most tracing vendors seem to cap storage at 1-3 months while metric vendors can offer multi-year storage.<p>2. They&#x27;re far more accurate that metrics derived from wide events in higher-throughput scenarios. While wide events are incredibly flexible, their higher storage cost means there&#x27;s an upper limit on the sample rate. The sampled nature  of wide events means that deriving accurate counts is far more difficult- metrics really shine in this role (unless you&#x27;re operating over datasets with very high cardinality). The problem only gets worse when you combine tail sampling into the mix and add bias towards errors&#x2F; slow requests in your data.</div><br/><div id="39532410" class="c"><input type="checkbox" id="c-39532410" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#39531882">parent</a><span>|</span><a href="#39532244">next</a><span>|</span><label class="collapse" for="c-39532410">[-]</label><label class="expand" for="c-39532410">[3 more]</label></div><br/><div class="children"><div class="content">For point (2), you can derive accurate counts from sampled data if the sampling rate is captured as metadata on every sampled event. Some tools do support this (I work for Honeycomb, and our sampling proxy + backend work like this, can&#x27;t speak for others).<p>The issue is there are still limits to that, though. I can still get a count of events, or a AVG(duration_ms). But if I have a custom tag I can&#x27;t get accurate counts of <i>that</i>. And if I want to get distinct counts of values, I&#x27;m out of luck. Estimating that is an active machine learning research problem.</div><br/><div id="39533911" class="c"><input type="checkbox" id="c-39533911" checked=""/><div class="controls bullet"><span class="by">wiseguyeh</span><span>|</span><a href="#39531882">root</a><span>|</span><a href="#39532410">parent</a><span>|</span><a href="#39533199">next</a><span>|</span><label class="collapse" for="c-39533911">[-]</label><label class="expand" for="c-39533911">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s an interesting point. We are actually running a test with with Honeycomb&#x27;s refinery later this week, I&#x27;m slightly skeptical but curious to see if they can overcome this bias.</div><br/></div></div><div id="39533199" class="c"><input type="checkbox" id="c-39533199" checked=""/><div class="controls bullet"><span class="by">taion</span><span>|</span><a href="#39531882">root</a><span>|</span><a href="#39532410">parent</a><span>|</span><a href="#39533911">prev</a><span>|</span><a href="#39532244">next</a><span>|</span><label class="collapse" for="c-39533199">[-]</label><label class="expand" for="c-39533199">[1 more]</label></div><br/><div class="children"><div class="content">You also lose accuracy because of sampling noise.</div><br/></div></div></div></div><div id="39532244" class="c"><input type="checkbox" id="c-39532244" checked=""/><div class="controls bullet"><span class="by">fl0ki</span><span>|</span><a href="#39531882">parent</a><span>|</span><a href="#39532410">prev</a><span>|</span><a href="#39533214">next</a><span>|</span><label class="collapse" for="c-39532244">[-]</label><label class="expand" for="c-39532244">[1 more]</label></div><br/><div class="children"><div class="content">On top of that, metrics can have exemplars, which give you more (and dynamic) dimensions for buckets without increasing the cardinality of the metric vectors themselves. It&#x27;s pretty much a wide event, with the sampling rate on this extra information just being the scrape interval you were already using anyway.<p>Not every library or tool supports exemplars, but they&#x27;re a big part of the Prometheus &amp; Grafana value proposition that many users entirely overlook.</div><br/></div></div><div id="39533214" class="c"><input type="checkbox" id="c-39533214" checked=""/><div class="controls bullet"><span class="by">taion</span><span>|</span><a href="#39531882">parent</a><span>|</span><a href="#39532244">prev</a><span>|</span><a href="#39530964">next</a><span>|</span><label class="collapse" for="c-39533214">[-]</label><label class="expand" for="c-39533214">[1 more]</label></div><br/><div class="children"><div class="content">This is exactly right. This kind of structured logging is great, but it doesn’t replace metrics. You really want to have both, and simple unsampled metrics are actively better for e.g. automated alerting for exactly those reasons. They’re complements more than substitutes.</div><br/></div></div></div></div><div id="39530964" class="c"><input type="checkbox" id="c-39530964" checked=""/><div class="controls bullet"><span class="by">timthelion</span><span>|</span><a href="#39531882">prev</a><span>|</span><a href="#39535095">next</a><span>|</span><label class="collapse" for="c-39530964">[-]</label><label class="expand" for="c-39530964">[12 more]</label></div><br/><div class="children"><div class="content">At the company I work for we send json to kafka and subsiquently to Elastic search with great effect. That&#x27;s basically &#x27;wide events&#x27;. The magical thing about hooking up a bunch of pipelines with kafka is that all of a sudden your observability&#x2F;metrics system becomes an amazing API for extending systems with aditional automations. Want to do something when a router connects to a network? Just subscribe to this kafka topic here. It doesn&#x27;t matter that the topic was origionally intended just to log some events. We even created an open source library for writing and running these,pipelines in jupyter. Here&#x27;s a super simple example <a href="https:&#x2F;&#x2F;github.com&#x2F;bitswan-space&#x2F;BitSwan&#x2F;blob&#x2F;master&#x2F;examples&#x2F;Jupyter&#x2F;Kafka2Kafka&#x2F;main.ipynb">https:&#x2F;&#x2F;github.com&#x2F;bitswan-space&#x2F;BitSwan&#x2F;blob&#x2F;master&#x2F;example...</a><p>People tend to think kafka is hard, but as you can see from the example, it can be extremely easy.</div><br/><div id="39531172" class="c"><input type="checkbox" id="c-39531172" checked=""/><div class="controls bullet"><span class="by">hibikir</span><span>|</span><a href="#39530964">parent</a><span>|</span><a href="#39531286">next</a><span>|</span><label class="collapse" for="c-39531172">[-]</label><label class="expand" for="c-39531172">[4 more]</label></div><br/><div class="children"><div class="content">This works well for a while. But eventually you get big, and have little to no idea of what is in your downstream. Then every single format change in any event you write must be treated like open heart surgery, because tracing your data dependencies is unreliable.<p>Sometimes it seems that it&#x27;s fixable by &#x27;just having a list of people listening&#x27;, and then you look and all that some of them do is mildly transform your data and pass it along. It doesn&#x27;t take long before people realize that. &#x27;just logging some events&#x27; is making future promises to other teams you don&#x27;t know about, and people start being terrified of emitting anything.<p>This is a story I&#x27;ve seen in at least 4 places in my career. Making data available to other people is not any less scary in kafka than it was back in the days where applications shared a giant database, and you&#x27;d see yearlong projects to do some mild changes to a data model, which was originally designed in 5 minutes.<p>As for kafka being easy, It&#x27;s not quite as hard as some people say, but it&#x27;s  both a pub sub system and a distributed database. When your clusters get large, it definitely isn&#x27;t easy.</div><br/><div id="39532414" class="c"><input type="checkbox" id="c-39532414" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#39530964">root</a><span>|</span><a href="#39531172">parent</a><span>|</span><a href="#39531282">next</a><span>|</span><label class="collapse" for="c-39532414">[-]</label><label class="expand" for="c-39532414">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This works well for a while. But eventually you get big, and have little to no idea of what is in your downstream. Then every single format change in any event you write must be treated like open heart surgery, because tracing your data dependencies is unreliable.<p>Yeah, I&#x27;d always use protobuf or similar rather than JSON for that reason, and if you need a truly breaking change I&#x27;d emit a new version of the events to a new topic rather than trying to migrate the existing one in place. It&#x27;s not actually so costly to keep writing events to an old topic (and if you really want you can move that part into a separate adapter process that reads your new topic and writes to your old one). Or you can do the whole avro&#x2F;schema-registry stuff if you prefer.<p>&gt; Making data available to other people is not any less scary in kafka than it was back in the days where applications shared a giant database<p>It should be significantly less scary: it&#x27;s impossible to mutate data in-place, foreign key issues are something you go back and fix and reprocess rather than something that takes down your OLTP system, schema changes are better-understood and less big-bang, event streams that are generated by transforming another event stream are completely indistinguishable from &quot;original&quot; event streams as opposed to views being sort-of-like-tables but having all sorts of caveats and gotchas.<p>&gt; As for kafka being easy, It&#x27;s not quite as hard as some people say, but it&#x27;s both a pub sub system and a distributed database. When your clusters get large, it definitely isn&#x27;t easy.<p>There are hard parts but also parts that are easier than a traditional database. There&#x27;s no query planner, no MVCC, no locks, no deadlocks, no isolation levels, indices are not magic, ...</div><br/></div></div><div id="39531282" class="c"><input type="checkbox" id="c-39531282" checked=""/><div class="controls bullet"><span class="by">meandmycode</span><span>|</span><a href="#39530964">root</a><span>|</span><a href="#39531172">parent</a><span>|</span><a href="#39532414">prev</a><span>|</span><a href="#39533472">next</a><span>|</span><label class="collapse" for="c-39531282">[-]</label><label class="expand" for="c-39531282">[1 more]</label></div><br/><div class="children"><div class="content">I think this is the crux of it, if something works for awhile then actually that&#x27;s fine, as an industry we over index and scare new developers towards complexity. The counter is true too, what works at scale doesn&#x27;t at non scale - not because of tech, but because holistically your asking for a lot, a lot of knowledge, a lot of complex tech to be deployed by a small team.</div><br/></div></div><div id="39533472" class="c"><input type="checkbox" id="c-39533472" checked=""/><div class="controls bullet"><span class="by">MuffinFlavored</span><span>|</span><a href="#39530964">root</a><span>|</span><a href="#39531172">parent</a><span>|</span><a href="#39531282">prev</a><span>|</span><a href="#39531286">next</a><span>|</span><label class="collapse" for="c-39533472">[-]</label><label class="expand" for="c-39533472">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This works well for a while. But eventually you get big, and have little to no idea of what is in your downstream.<p>All you have to do is pass around trace baggage headers, right?</div><br/></div></div></div></div><div id="39531286" class="c"><input type="checkbox" id="c-39531286" checked=""/><div class="controls bullet"><span class="by">dgellow</span><span>|</span><a href="#39530964">parent</a><span>|</span><a href="#39531172">prev</a><span>|</span><a href="#39531089">next</a><span>|</span><label class="collapse" for="c-39531286">[-]</label><label class="expand" for="c-39531286">[2 more]</label></div><br/><div class="children"><div class="content">Zookeeper in production can be really a pain to maintain…</div><br/><div id="39533686" class="c"><input type="checkbox" id="c-39533686" checked=""/><div class="controls bullet"><span class="by">rockwotj</span><span>|</span><a href="#39530964">root</a><span>|</span><a href="#39531286">parent</a><span>|</span><a href="#39531089">next</a><span>|</span><label class="collapse" for="c-39533686">[-]</label><label class="expand" for="c-39533686">[1 more]</label></div><br/><div class="children"><div class="content">Check out Redpanda if you don’t like Zookeeper. Raft based, no JVM, lower cost and simpler to run Kafka compatible system</div><br/></div></div></div></div><div id="39531089" class="c"><input type="checkbox" id="c-39531089" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#39530964">parent</a><span>|</span><a href="#39531286">prev</a><span>|</span><a href="#39535095">next</a><span>|</span><label class="collapse" for="c-39531089">[-]</label><label class="expand" for="c-39531089">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad that works for you but to me it sounds really expensive. At small scale you can do this any way you want but if you build an observability system with linear cost and a high coefficient it will become an issue if you run into some success.</div><br/><div id="39531171" class="c"><input type="checkbox" id="c-39531171" checked=""/><div class="controls bullet"><span class="by">timthelion</span><span>|</span><a href="#39530964">root</a><span>|</span><a href="#39531089">parent</a><span>|</span><a href="#39535095">next</a><span>|</span><label class="collapse" for="c-39531171">[-]</label><label class="expand" for="c-39531171">[4 more]</label></div><br/><div class="children"><div class="content">The only expensive part is the hardwarevfor the elastic servers. Kafka is cheap to run. We have an on prem elastic db pulling in tens of thousands of events per second. On prem servers aren&#x27;t <i>that</i> expensive. It&#x27;s really just 6 servers with 20tb each and another 40tb for backups. And it&#x27;s not like you have to store everything forever... Compare that data flow to everyonevwatching youtube all the time. It&#x27;s really nothing...</div><br/><div id="39533313" class="c"><input type="checkbox" id="c-39533313" checked=""/><div class="controls bullet"><span class="by">kiitos</span><span>|</span><a href="#39530964">root</a><span>|</span><a href="#39531171">parent</a><span>|</span><a href="#39531230">next</a><span>|</span><label class="collapse" for="c-39533313">[-]</label><label class="expand" for="c-39533313">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We have an on prem elastic db pulling in tens of thousands of events per second.<p>Many kinds of systems can work great at this scale, which is non-trivial but ultimately not particularly large.<p>&gt; It&#x27;s really just 6 servers with 20tb each and another 40tb for backups.<p>Hopefully you can ingest and store and query O(10k) RPS with an order of magnitude less resources than this?</div><br/></div></div><div id="39531230" class="c"><input type="checkbox" id="c-39531230" checked=""/><div class="controls bullet"><span class="by">ricardobeat</span><span>|</span><a href="#39530964">root</a><span>|</span><a href="#39531171">parent</a><span>|</span><a href="#39533313">prev</a><span>|</span><a href="#39535095">next</a><span>|</span><label class="collapse" for="c-39531230">[-]</label><label class="expand" for="c-39531230">[2 more]</label></div><br/><div class="children"><div class="content">I can name a single company in my area that runs their own servers, and they&#x27;ve been in the middle of a migration to the cloud for the past five years.</div><br/><div id="39533750" class="c"><input type="checkbox" id="c-39533750" checked=""/><div class="controls bullet"><span class="by">markhahn</span><span>|</span><a href="#39530964">root</a><span>|</span><a href="#39531230">parent</a><span>|</span><a href="#39535095">next</a><span>|</span><label class="collapse" for="c-39533750">[-]</label><label class="expand" for="c-39533750">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s really down to company culture: how many MBAs and PMs it normalizes.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39535095" class="c"><input type="checkbox" id="c-39535095" checked=""/><div class="controls bullet"><span class="by">tlarkworthy</span><span>|</span><a href="#39530964">prev</a><span>|</span><a href="#39534088">next</a><span>|</span><label class="collapse" for="c-39535095">[-]</label><label class="expand" for="c-39535095">[1 more]</label></div><br/><div class="children"><div class="content">FWIW I think x-ray has everything you need, its just that AWs tooling does not give you much ability to aggregate over x-ray bundles. I wrote a tool to help bulk load x-ray samples into a local browser duckdb and then slice and dicing in realtime interactive visualisations. It also includes the ability to generate a flamegraph over the selected traces. All this great data is already in an AWS, account and we just need better tools to make use of it.<p><a href="https:&#x2F;&#x2F;observablehq.com&#x2F;@tomlarkworthy&#x2F;x-ray-slurper" rel="nofollow">https:&#x2F;&#x2F;observablehq.com&#x2F;@tomlarkworthy&#x2F;x-ray-slurper</a></div><br/></div></div><div id="39534088" class="c"><input type="checkbox" id="c-39534088" checked=""/><div class="controls bullet"><span class="by">TeeWEE</span><span>|</span><a href="#39535095">prev</a><span>|</span><a href="#39531776">next</a><span>|</span><label class="collapse" for="c-39534088">[-]</label><label class="expand" for="c-39534088">[1 more]</label></div><br/><div class="children"><div class="content">This is basically a metric with tags. Only difference is that a metric has a main unit it measures.<p>In the end anything can be represented as a structured log.<p>A span is NOT what the OP calls a “system wide event”. A span has a begin and end time. What he&#x2F;she describes doesn’t have that.<p>In the end giving different kind of instrumentation instruments a name makes sense, mainly for processing them &#x2F; rendering them &#x2F; altering on them.</div><br/></div></div><div id="39531776" class="c"><input type="checkbox" id="c-39531776" checked=""/><div class="controls bullet"><span class="by">zug_zug</span><span>|</span><a href="#39534088">prev</a><span>|</span><a href="#39533956">next</a><span>|</span><label class="collapse" for="c-39531776">[-]</label><label class="expand" for="c-39531776">[9 more]</label></div><br/><div class="children"><div class="content">This person is simply misinformed. I worked at meta and used scuba, and it&#x27;s like 6&#x2F;10 (which makes it one of meta&#x27;s best tools).<p>A tool like splunk can do everything scuba can do and a million things it can&#x27;t. Sumologic can too.<p>The reason that splunk&#x2F;sumologic are so much better than scuba is that they have open-ended query languages rather than this on-rails &quot;only ever do one group-by&quot;. Just for example, if you wanted to dynamically extract a field at query-time based on the difference of two values and group by that, that&#x27;s something you can do trivially in splunk&#x2F;sumo.<p>I could write a whole essay on the topic really, but the gist of it is you need a full-scaled open-ended language for advanced querying because 1% of the time you need to do weird stuff like count-by -&gt; a second count-by.<p>What I will agree with is that traces&#x2F;metrics do not inherently give you this ability, but absolutely traces <i>could</i> if there was a platfom with a powerful enough query-language for it (e.g. give me all requests that go through 4 services, have errors on service 3 but not 4, and are associated with userId 123 on service 1)</div><br/><div id="39532113" class="c"><input type="checkbox" id="c-39532113" checked=""/><div class="controls bullet"><span class="by">isburmistrov</span><span>|</span><a href="#39531776">parent</a><span>|</span><a href="#39532874">next</a><span>|</span><label class="collapse" for="c-39532113">[-]</label><label class="expand" for="c-39532113">[4 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t say such tools don&#x27;t exist. Honeycomb, mentioned in the post, is exactly Scuba fwiw.
I said that over-focusing on traces &#x2F; metrics and &quot;logs&quot; (in the classical understanding) hides the true power of wide events, and they&#x27;re not being used widely.<p>Also IMO open-ended query language doesn&#x27;t help in quick exploration, it&#x27;s a barrier. UI and easiness of use is the paramount for adoption. To achieve arbitrary queries one can dump everything to smth like ClickHouse and query it with SQL. This would be a nice addition to any observability stack, to cover for a small percentage of the very deep explorations.</div><br/><div id="39532249" class="c"><input type="checkbox" id="c-39532249" checked=""/><div class="controls bullet"><span class="by">zug_zug</span><span>|</span><a href="#39531776">root</a><span>|</span><a href="#39532113">parent</a><span>|</span><a href="#39532191">next</a><span>|</span><label class="collapse" for="c-39532249">[-]</label><label class="expand" for="c-39532249">[2 more]</label></div><br/><div class="children"><div class="content">It looks like a &quot;wide-event&quot; is just a structured log, you can send any log containing json to sumo&#x2F;splunk at it&#x27;ll parse it out as fields for you. So as I&#x27;m understanding it you&#x27;re advocating for structured logs (which is fine, those are great).<p>If you want a point-and-click interface to log searching I agree that some percentage of people like to start there (and I think splunk may even have that too), so I&#x27;m not opposed to it existing at all, but I feel very strongly that having the more sophisticated capabilities if you want to move beyond a point-click is a requirement.</div><br/><div id="39533542" class="c"><input type="checkbox" id="c-39533542" checked=""/><div class="controls bullet"><span class="by">jimbobimbo</span><span>|</span><a href="#39531776">root</a><span>|</span><a href="#39532249">parent</a><span>|</span><a href="#39532191">next</a><span>|</span><label class="collapse" for="c-39533542">[-]</label><label class="expand" for="c-39533542">[1 more]</label></div><br/><div class="children"><div class="content">It exactly is a structured log or log in open telemetry.<p>To make it easier for myself, I think of spans also as structured logs with a schema that everyone had agreed on, which make it possible to trace requests across multiple services&#x2F;clients. It&#x27;s probably more than that, but I don&#x27;t need academic precision to see how this is more useful during livesite investigations than simply querying logs with unaligned schemas.</div><br/></div></div></div></div><div id="39532191" class="c"><input type="checkbox" id="c-39532191" checked=""/><div class="controls bullet"><span class="by">adamgordonbell</span><span>|</span><a href="#39531776">root</a><span>|</span><a href="#39532113">parent</a><span>|</span><a href="#39532249">prev</a><span>|</span><a href="#39532874">next</a><span>|</span><label class="collapse" for="c-39532191">[-]</label><label class="expand" for="c-39532191">[1 more]</label></div><br/><div class="children"><div class="content">Honeycomb was in fact built to originally be a replacement for scuba, by people who found it so valuable and missed it.</div><br/></div></div></div></div><div id="39532874" class="c"><input type="checkbox" id="c-39532874" checked=""/><div class="controls bullet"><span class="by">ot</span><span>|</span><a href="#39531776">parent</a><span>|</span><a href="#39532113">prev</a><span>|</span><a href="#39532232">next</a><span>|</span><label class="collapse" for="c-39532874">[-]</label><label class="expand" for="c-39532874">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Just for example, if you wanted to dynamically extract a field at query-time based on the difference of two values and group by that, that&#x27;s something you can do trivially in splunk&#x2F;sumo.<p>You can trivially do that in Scuba too using a derived column, which is supported in the UI. If you need more complex stuff you can write your query in SQL and still use all the supported UI visualizations. And for even more complex stuff, for example if you need joins, the data is usually mirrored to Presto, so you can write arbitrarily complex queries and still visualize the results in the Scuba UI.<p>I&#x27;m not sure if your assessment of Scuba is based on full knowledge of what you can do with it.</div><br/></div></div><div id="39532232" class="c"><input type="checkbox" id="c-39532232" checked=""/><div class="controls bullet"><span class="by">neilkk</span><span>|</span><a href="#39531776">parent</a><span>|</span><a href="#39532874">prev</a><span>|</span><a href="#39532363">next</a><span>|</span><label class="collapse" for="c-39532232">[-]</label><label class="expand" for="c-39532232">[1 more]</label></div><br/><div class="children"><div class="content">mipsytipsy also worked with scuba, and she specifically designed honeycomb, an observability tool based on wide events, around the motivating idea that a very imperfect tool like scuba could do lots of things better than much more polished and better engineered tools which didn&#x27;t use the wide events approach.</div><br/></div></div><div id="39532363" class="c"><input type="checkbox" id="c-39532363" checked=""/><div class="controls bullet"><span class="by">downWidOutaFite</span><span>|</span><a href="#39531776">parent</a><span>|</span><a href="#39532232">prev</a><span>|</span><a href="#39532247">next</a><span>|</span><label class="collapse" for="c-39532363">[-]</label><label class="expand" for="c-39532363">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen some really nice UI query systems that have plenty of expressive power for 99.9% of queries but also include an escape hatch to allow you to use a query language. Mixpanel has one that I really like and I&#x27;ve seen non-techy business types go to town on it no problem.<p>The other option I&#x27;ve seen is a guided query language with helpful UI affordances as you type. I&#x27;ve seen it on Datadog as well as Jira&#x27;s advanced search.</div><br/></div></div></div></div><div id="39533956" class="c"><input type="checkbox" id="c-39533956" checked=""/><div class="controls bullet"><span class="by">treflop</span><span>|</span><a href="#39531776">prev</a><span>|</span><a href="#39532711">next</a><span>|</span><label class="collapse" for="c-39533956">[-]</label><label class="expand" for="c-39533956">[1 more]</label></div><br/><div class="children"><div class="content">We use wide events at work (or really “structured logging” or really “your log system has fields”) and they are great.<p>But they aren’t a replacement for metrics because metrics are so god damn cheap.<p>And while I’ve never used a log system with traces, every logging setup I’ve ever used has had request&#x2F;correlation IDs to generate a trace because sometimes you just wanna lookup a flow and see it without spending a time digging through wide events&#x2F;your log system. If you aren’t looking up logs very often, then yeah it seems browsing through structured logs isn’t that bad but then do it often and it’s just annoying…</div><br/></div></div><div id="39532711" class="c"><input type="checkbox" id="c-39532711" checked=""/><div class="controls bullet"><span class="by">mikpanko</span><span>|</span><a href="#39533956">prev</a><span>|</span><a href="#39531537">next</a><span>|</span><label class="collapse" for="c-39532711">[-]</label><label class="expand" for="c-39532711">[1 more]</label></div><br/><div class="children"><div class="content">It took the world decades to develop widely accepted standards for working with relational data and SQL. I believe we are at the early stages of doing the same with event data and sequence analytics. It is starting to simultaneously emerge in many different fields:<p>- eng observability (traces at Datadog, Sumologic, etc)<p>- operational research (process mining at Celonis)<p>- product analytics (funnels at Amplitude, Mixpanel)<p>As with every new field, there are a lot of different and overlapping terms being suggested and explored at the same time.<p>We are trying to contribute to the field with a deep fundamental approach at Motif Analytics, including a purpose-built set of core sequence operations, rich flow visualizations, a pattern matching query engine, and foundational AI models on event sequences [1].<p>Fun fact: creators of Scuba turned it into a startup Interana (acquired by Twitter), who we took a lot of inspiration from for Motif&#x27;s query engine.<p>[1] <a href="https:&#x2F;&#x2F;motifanalytics.com" rel="nofollow">https:&#x2F;&#x2F;motifanalytics.com</a></div><br/></div></div><div id="39531537" class="c"><input type="checkbox" id="c-39531537" checked=""/><div class="controls bullet"><span class="by">karmakaze</span><span>|</span><a href="#39532711">prev</a><span>|</span><a href="#39533768">next</a><span>|</span><label class="collapse" for="c-39531537">[-]</label><label class="expand" for="c-39531537">[8 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the difference between a Wide Event and a structured log?</div><br/><div id="39531995" class="c"><input type="checkbox" id="c-39531995" checked=""/><div class="controls bullet"><span class="by">deathanatos</span><span>|</span><a href="#39531537">parent</a><span>|</span><a href="#39533741">next</a><span>|</span><label class="collapse" for="c-39531995">[-]</label><label class="expand" for="c-39531995">[5 more]</label></div><br/><div class="children"><div class="content">Nothing; what the author is calling a wide event is a structured log.<p>&gt; <i>Structured Log which is pretty much the Wide Event</i><p>The OP&#x27;s point is that you can just capture anything that one might call a metric&#x2F;span&#x2F;trace in a structured log.<p>So, it&#x27;s a bit more specific, in some senses: a &quot;metric&quot; is a structured log bearing some value. E.g.,<p><pre><code>  {
    &quot;@timestamp&quot;: &quot;…&quot;,
    &quot;query_kind&quot;: &quot;list_people_in_org&quot;,
    &quot;query_time_s&quot;: .051
  }
</code></pre>
Is a structured log. It&#x27;s also a single datapoint along a timeseries. The metric might be something like &quot;query time&quot;, with a dimension &quot;query_kind&quot;.<p>ELK stacks let you visualize such structured logs into graphs with only moderate pain.<p>A &quot;span&quot; is a structured log with start&#x2F;end timestamps.<p>A trace is just a tree of spans, built out of many structured logs. In a prior life, we&#x27;d have,<p><pre><code>  {
    &quot;span_id&quot;: &quot;root_span_id&#x2F;subspan_a&#x2F;subsub_span_b&quot;,
    …
  }
</code></pre>
You can pull out whole traces by doing a &quot;span_id startswith &#x27;root_span_id&#x27;&quot; query in whatever your query language is. You can pull out sub-traces similarly. It works even across microservice boundaries, if you coordinate the span IDs appropriately.<p>I think where I depart from the OP: it takes too much space. The OP advocates sampling; I&#x27;d rather a specialized data structure for metrics that can store a data point efficiently. I.e., at least in sizeof(timestamp || f32) bytes, ideally better, and then I can just not worry about sampling. (In a sense; if you&#x27;re watching a continuous value, you&#x27;re always sampling. But for discrete events that you&#x27;re recording, I can just record <i>all</i> the events. But I&#x27;ll only get a number. Sometimes, that&#x27;s okay. I feel like this is a bit spiritually different than the sampling the OP is discussing.)</div><br/><div id="39532276" class="c"><input type="checkbox" id="c-39532276" checked=""/><div class="controls bullet"><span class="by">karmakaze</span><span>|</span><a href="#39531537">root</a><span>|</span><a href="#39531995">parent</a><span>|</span><a href="#39533741">next</a><span>|</span><label class="collapse" for="c-39532276">[-]</label><label class="expand" for="c-39532276">[4 more]</label></div><br/><div class="children"><div class="content">Thanks. I don&#x27;t know why new unpopular names are used for well-known things. Googling for &quot;Wide Events&quot; give millions of results for &quot;City-wide events&quot; etc. Probably just clickbaiting--what&#x27;s a Wide Event?</div><br/><div id="39534994" class="c"><input type="checkbox" id="c-39534994" checked=""/><div class="controls bullet"><span class="by">deathanatos</span><span>|</span><a href="#39531537">root</a><span>|</span><a href="#39532276">parent</a><span>|</span><a href="#39532361">next</a><span>|</span><label class="collapse" for="c-39534994">[-]</label><label class="expand" for="c-39534994">[1 more]</label></div><br/><div class="children"><div class="content">I assumed &quot;wide event&quot; is the internal terminology at FB, given the OP.<p>I&#x27;ve never personally heard it, but it the context of the OP &#x2F; metrics&#x2F;logs&#x2F;traces, it&#x27;s a pretty self-descriptive term.</div><br/></div></div><div id="39532361" class="c"><input type="checkbox" id="c-39532361" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#39531537">root</a><span>|</span><a href="#39532276">parent</a><span>|</span><a href="#39534994">prev</a><span>|</span><a href="#39533741">next</a><span>|</span><label class="collapse" for="c-39532361">[-]</label><label class="expand" for="c-39532361">[2 more]</label></div><br/><div class="children"><div class="content">So, this is just my own view, but here&#x27;s how I see it:<p>Structured logs are ... well, logs that are structured (probably json) and machines read &#x27;em to offer nice analysis capabilities. We&#x27;ve largely as an industry decided that these are great, and we should use them instead of unstructured logs if we can.<p>But just because a log is structured it doesn&#x27;t mean it has what you need to effectively debug an issue! You usually need app-specific data added as key-value pairs to that log so that you can correlate stuff like &quot;stuff is slow on this endpoint&quot; with &quot;and these are the device versions and user_agent strings that correlate the most with that&quot;.<p>And a lot of developers are used to dumping some of that data into random logs, but without making that data a part of a &quot;wider&quot; structured log, it can be really hard to correlate the behavior you don&#x27;t like with other data that aids with debugging.<p>Hence, a desire to call them &quot;wide events&quot;.<p>Anyways, I&#x27;m not particularly sold on that term either. But I do think there&#x27;s some need to describe not only structured logs, but structured logs that contain all the rich info you need to debug most stuff in production with general ease.</div><br/><div id="39535057" class="c"><input type="checkbox" id="c-39535057" checked=""/><div class="controls bullet"><span class="by">deathanatos</span><span>|</span><a href="#39531537">root</a><span>|</span><a href="#39532361">parent</a><span>|</span><a href="#39533741">next</a><span>|</span><label class="collapse" for="c-39535057">[-]</label><label class="expand" for="c-39535057">[1 more]</label></div><br/><div class="children"><div class="content">Structured logs are sensible, because you&#x27;re not setting yourself up to need to write a billion little parsers down the line to attempt to parse unstructured logs, if they&#x27;re even still parsable.<p>JSON, or usually ndjson, is just simple, and widely supported. It&#x27;s not the only format, nor even the best format. But it is easily produced, and better than no format at all.<p>&gt; <i>But just because a log is structured it doesn&#x27;t mean it has what you need to effectively debug an issue! You usually need app-specific data added as key-value pairs to that log so that you can correlate stuff like &quot;stuff is slow on this endpoint&quot; with &quot;and these are the device versions and user_agent strings that correlate the most with that&quot;.</i><p>Yes, you have to instrument the app. There&#x27;s no getting around that. Experience will tell you what to log.<p>&gt; <i>And a lot of developers are used to dumping some of that data into random logs, but without making that data a part of a &quot;wider&quot; structured log, it can be really hard to correlate the behavior you don&#x27;t like with other data that aids with debugging.</i><p>Again, experience. Log correlation is easier with what the article calls a &quot;SpanId&quot; or a &quot;TraceId&quot;, some places call it a &quot;RequestId&quot; or a &quot;CorrelationId&quot;, just some way to say &quot;all longs forming a particular request&quot; so that you can just read a request, from start to end. A simple thing … but you have to log it, or you won&#x27;t have it.<p>Agreement across services about naming it all the same thing in your log store of choice also helps, so that if you need to trace it across services, you can. This really isn&#x27;t hard, but it depends IME on how well your organization is otherwise functioning. E.g., do engineers talk &amp; plan? Can they say &quot;X would help with Y&quot; and then have people just go &quot;yeah, it would. Done.&quot; or is it a long drawn out fight because technical leadership can&#x27;t fathom basic stuff like what a RequestId does.<p>&gt; <i>But I do think there&#x27;s some need to describe not only structured logs, but structured logs that contain all the rich info you need to debug most stuff in production with general ease.</i><p>I feel like you&#x27;re just saying &quot;log the data we actually need&quot;, which isn&#x27;t a terribly actionable thing if you don&#x27;t already know the answer. If I had to make a recommendation: outcomes &amp; latencies of I&#x2F;O.<p>Otherwise, put something into production, &amp; be on-call for it.</div><br/></div></div></div></div></div></div></div></div><div id="39533741" class="c"><input type="checkbox" id="c-39533741" checked=""/><div class="controls bullet"><span class="by">MuffinFlavored</span><span>|</span><a href="#39531537">parent</a><span>|</span><a href="#39531995">prev</a><span>|</span><a href="#39533768">next</a><span>|</span><label class="collapse" for="c-39533741">[-]</label><label class="expand" for="c-39533741">[2 more]</label></div><br/><div class="children"><div class="content">How do you keep metadata around well enough to log in a structured fashion between apps? What if one app is a queue listener, which calls an HTTP service, etc. etc.<p>I personally think the challenge is in passing around the metadata.</div><br/><div id="39534822" class="c"><input type="checkbox" id="c-39534822" checked=""/><div class="controls bullet"><span class="by">karmakaze</span><span>|</span><a href="#39531537">root</a><span>|</span><a href="#39533741">parent</a><span>|</span><a href="#39533768">next</a><span>|</span><label class="collapse" for="c-39534822">[-]</label><label class="expand" for="c-39534822">[1 more]</label></div><br/><div class="children"><div class="content">This is solved using span IDs commonly used in distributed tracing.</div><br/></div></div></div></div></div></div><div id="39533768" class="c"><input type="checkbox" id="c-39533768" checked=""/><div class="controls bullet"><span class="by">goosejuice</span><span>|</span><a href="#39531537">prev</a><span>|</span><a href="#39534721">next</a><span>|</span><label class="collapse" for="c-39533768">[-]</label><label class="expand" for="c-39533768">[1 more]</label></div><br/><div class="children"><div class="content">This sounds like a privacy nightmare as described if there aren&#x27;t guardrails. &#x27;Dump everything&#x27;<p>Can pretty easily achieve this with structured logging in GCP with their metrics explorer. Pretty cheaply I might add. Sentry can also do a bit of this if you&#x27;re on something like fly.io (they offer a year free).<p>I don&#x27;t think either would completely replace tracing in a complex system for me. At least not in the context Ive worked.</div><br/></div></div><div id="39534721" class="c"><input type="checkbox" id="c-39534721" checked=""/><div class="controls bullet"><span class="by">jupp0r</span><span>|</span><a href="#39533768">prev</a><span>|</span><a href="#39531873">next</a><span>|</span><label class="collapse" for="c-39534721">[-]</label><label class="expand" for="c-39534721">[1 more]</label></div><br/><div class="children"><div class="content">This looks like structured logging and piping those logs to Splunk or am I missing something?</div><br/></div></div><div id="39531873" class="c"><input type="checkbox" id="c-39531873" checked=""/><div class="controls bullet"><span class="by">gorlami</span><span>|</span><a href="#39534721">prev</a><span>|</span><a href="#39531573">next</a><span>|</span><label class="collapse" for="c-39531873">[-]</label><label class="expand" for="c-39531873">[6 more]</label></div><br/><div class="children"><div class="content">So, what is the closest thing in the open source world to what the author describes? (Setting aside the question of is it right for you, which, of course, depends.)</div><br/><div id="39532263" class="c"><input type="checkbox" id="c-39532263" checked=""/><div class="controls bullet"><span class="by">mdavidn</span><span>|</span><a href="#39531873">parent</a><span>|</span><a href="#39532476">next</a><span>|</span><label class="collapse" for="c-39532263">[-]</label><label class="expand" for="c-39532263">[2 more]</label></div><br/><div class="children"><div class="content">Any OLAP database that accepts unstructured data can be used in this manner.<p>The ELK stack is a popular choice, albeit with a focus on search rather than OLAP.<p>If SaaS is an option, a simple staring point in AWS might be Data Firehose into S3 with Athena. Snowflake can load and query the data too. All of these tools have multiple frontend options with a proportional relationship between cost and user-friendliness.<p>I honestly just do this in PostgreSQL until my project outgrows it. Create a table with a JSONB column and as few indexes as possible to improve write throughput. Cover a timestamp column with a BRIN index to filter by date range.</div><br/><div id="39533431" class="c"><input type="checkbox" id="c-39533431" checked=""/><div class="controls bullet"><span class="by">johnthescott</span><span>|</span><a href="#39531873">root</a><span>|</span><a href="#39532263">parent</a><span>|</span><a href="#39532476">next</a><span>|</span><label class="collapse" for="c-39533431">[-]</label><label class="expand" for="c-39533431">[1 more]</label></div><br/><div class="children"><div class="content">brin is your friend for logs, for sure.</div><br/></div></div></div></div><div id="39532476" class="c"><input type="checkbox" id="c-39532476" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#39531873">parent</a><span>|</span><a href="#39532263">prev</a><span>|</span><a href="#39531573">next</a><span>|</span><label class="collapse" for="c-39532476">[-]</label><label class="expand" for="c-39532476">[3 more]</label></div><br/><div class="children"><div class="content">Still early days, but Grafana&#x27;s tracing is getting there. <a href="https:&#x2F;&#x2F;grafana.com&#x2F;docs&#x2F;grafana&#x2F;latest&#x2F;panels-visualizations&#x2F;visualizations&#x2F;traces&#x2F;" rel="nofollow">https:&#x2F;&#x2F;grafana.com&#x2F;docs&#x2F;grafana&#x2F;latest&#x2F;panels-visualization...</a></div><br/><div id="39533729" class="c"><input type="checkbox" id="c-39533729" checked=""/><div class="controls bullet"><span class="by">MuffinFlavored</span><span>|</span><a href="#39531873">root</a><span>|</span><a href="#39532476">parent</a><span>|</span><a href="#39531573">next</a><span>|</span><label class="collapse" for="c-39533729">[-]</label><label class="expand" for="c-39533729">[2 more]</label></div><br/><div class="children"><div class="content">What format&#x2F;library&#x2F;protocol does your app speak to get the traces into Grafana Traces? Tempo?</div><br/><div id="39533862" class="c"><input type="checkbox" id="c-39533862" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#39531873">root</a><span>|</span><a href="#39533729">parent</a><span>|</span><a href="#39531573">next</a><span>|</span><label class="collapse" for="c-39533862">[-]</label><label class="expand" for="c-39533862">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;grafana.com&#x2F;oss&#x2F;tempo&#x2F;" rel="nofollow">https:&#x2F;&#x2F;grafana.com&#x2F;oss&#x2F;tempo&#x2F;</a><p>&gt; Tempo can ingest common open source tracing protocols, including Jaeger, Zipkin, and OpenTelemetry.</div><br/></div></div></div></div></div></div></div></div><div id="39531573" class="c"><input type="checkbox" id="c-39531573" checked=""/><div class="controls bullet"><span class="by">jrockway</span><span>|</span><a href="#39531873">prev</a><span>|</span><a href="#39530887">next</a><span>|</span><label class="collapse" for="c-39531573">[-]</label><label class="expand" for="c-39531573">[1 more]</label></div><br/><div class="children"><div class="content">I like logs.  Unlike most people selling and using observability platforms, most of the software I write is run by other people.  That means it can&#x27;t send me traces and I can&#x27;t scrape it for metrics, but I still have to figure out and fix their problems.  To me, logs are the answer.  Logs are easy to pass around, and you can put whatever you want in there.  I have libraries for metrics and traces, and just parse them out of the logs when that sort of presentation would be useful.  (Yes, we do sampling as well.)<p>I keep hearing that this doesn&#x27;t scale.  When I worked at Google, we used this sort of system to monitor our Google Fiber devices.  They just uploaded their logs every minute (stored in memory, held in memory after a warm reboot thanks to a custom linux kernel with printk_persist), and then my software processed them into metrics for the &quot;fast query&quot; monitoring systems.  The most important metrics fed into alerts, but it didn&#x27;t take very much time to just re-read all the logs if you wanted to add something new.  Amazingly, the first version of this system ran on a single machine... 1 Go program handling 10,000qps of log uploads and analysis.  I eventually distributed it to survive machine and datacenter failures, but it ultimately isn&#x27;t that computationally intensive.  The point is, it kind of scales OK.  Up to 10s of terabytes a day, it&#x27;s something you don&#x27;t even have to think about except for the storage cost.<p>At some point it does make sense to move things into better databases than logs; you want to be alerted by your monitoring system that 99%-ile latency is high, then look in Jaeger for long-running traces, then take the trace ID and search your logs for it.  If you start with logs, you have that capability.  If you start with something else, then you just have &quot;the program is broken, good luck&quot; and you have to guess what the problem is whenever you debug.  Ideally, your program would just tell you what&#x27;s broken.  That&#x27;s what logs are.<p>One place where people get burned with logs is not being careful about what to log.  Logs are the primary user interface for operators of your software (i.e. you during your oncall week), and that task deserves the attention that any other user interface task demands.  People often start by logging too much, then get tired of &quot;spam&quot;, and end up not logging enough.  Then a problem occurs and the logs are outright misleading.  (My favorite is event failures that are retried, but the retry isn&#x27;t logged anywhere.  You end up seeing &quot;ERROR foobar attempt 1&#x2F;3 failed&quot; and have no idea of knowing that attempt 2&#x2F;3 succeeded a millisecond after that log line.)<p>For the gophers around, here&#x27;s what I do for traces: <a href="https:&#x2F;&#x2F;github.com&#x2F;pachyderm&#x2F;pachyderm&#x2F;blob&#x2F;master&#x2F;src&#x2F;internal&#x2F;log&#x2F;span.go#L130">https:&#x2F;&#x2F;github.com&#x2F;pachyderm&#x2F;pachyderm&#x2F;blob&#x2F;master&#x2F;src&#x2F;inter...</a> and metrics: <a href="https:&#x2F;&#x2F;github.com&#x2F;pachyderm&#x2F;pachyderm&#x2F;blob&#x2F;master&#x2F;src&#x2F;internal&#x2F;meters&#x2F;meters.go">https:&#x2F;&#x2F;github.com&#x2F;pachyderm&#x2F;pachyderm&#x2F;blob&#x2F;master&#x2F;src&#x2F;inter...</a>.  If you have a pipeline for storing and retrieving logs (which is exactly the case for this particular piece of software), now you have metrics and traces.  It&#x27;s great!  I just need to write the thing to turn a set of log files into a UI that looks like Jaeger and Prometheus ;)  My favorite part is that I don&#x27;t need to care about the cardinality of metrics; every RPC gets its own set of metrics.  So I can write a quick jq program to figure out how much bandwidth the entire system is using, or I can look at how much bandwidth one request is using.  (meters logs every X bytes, and log entries have timestamps.)<p>I think since we&#x27;ve added this capability to our system, incidents are most often resolved with &quot;that&#x27;s fixed in the next patch release&quot; instead of multiple iterations &quot;can you try this custom build and take another debug dump&quot;.  Very enjoyable.</div><br/></div></div><div id="39530887" class="c"><input type="checkbox" id="c-39530887" checked=""/><div class="controls bullet"><span class="by">swader999</span><span>|</span><a href="#39531573">prev</a><span>|</span><a href="#39531758">next</a><span>|</span><label class="collapse" for="c-39530887">[-]</label><label class="expand" for="c-39530887">[1 more]</label></div><br/><div class="children"><div class="content">This seems like event sourcing with a nice tool to inspect, filter and visualize the event stream. The sampling rate idea is a decent tactic I hadn&#x27;t heard of.</div><br/></div></div><div id="39531758" class="c"><input type="checkbox" id="c-39531758" checked=""/><div class="controls bullet"><span class="by">mdavidn</span><span>|</span><a href="#39530887">prev</a><span>|</span><a href="#39531474">next</a><span>|</span><label class="collapse" for="c-39531758">[-]</label><label class="expand" for="c-39531758">[1 more]</label></div><br/><div class="children"><div class="content">The SQL-equivalent in the sampling rate example should sum the inversion:<p>SELECT SUM(1 &#x2F; samplingRate) FROM AdImpressions
WHERE IsTest = False</div><br/></div></div><div id="39531474" class="c"><input type="checkbox" id="c-39531474" checked=""/><div class="controls bullet"><span class="by">4ndrewl</span><span>|</span><a href="#39531758">prev</a><span>|</span><a href="#39531717">next</a><span>|</span><label class="collapse" for="c-39531474">[-]</label><label class="expand" for="c-39531474">[3 more]</label></div><br/><div class="children"><div class="content">Wide events are fine until someone puts personally identifiable information (PII) in them. Then you&#x27;re in a bit of a mess as you&#x27;ve presumably taken PII out of an environment with one set of access controls, and into a separate, different environment, with access controls that are for a different purpose than required by the data.</div><br/><div id="39531513" class="c"><input type="checkbox" id="c-39531513" checked=""/><div class="controls bullet"><span class="by">benwaffle</span><span>|</span><a href="#39531474">parent</a><span>|</span><a href="#39531717">next</a><span>|</span><label class="collapse" for="c-39531513">[-]</label><label class="expand" for="c-39531513">[2 more]</label></div><br/><div class="children"><div class="content">Logging and tracing have this problem too.</div><br/><div id="39534059" class="c"><input type="checkbox" id="c-39534059" checked=""/><div class="controls bullet"><span class="by">goosejuice</span><span>|</span><a href="#39531474">root</a><span>|</span><a href="#39531513">parent</a><span>|</span><a href="#39531717">next</a><span>|</span><label class="collapse" for="c-39534059">[-]</label><label class="expand" for="c-39534059">[1 more]</label></div><br/><div class="children"><div class="content">Wide events described in this article seem to equal structured logging but a more loose dumping ground. So yeah to an extent it has this problem, just more so.<p>How does tracing? Are folks adding PII to spans? I suppose you could but I&#x27;m not sure why.</div><br/></div></div></div></div></div></div><div id="39531717" class="c"><input type="checkbox" id="c-39531717" checked=""/><div class="controls bullet"><span class="by">Scotrix</span><span>|</span><a href="#39531474">prev</a><span>|</span><a href="#39530980">next</a><span>|</span><label class="collapse" for="c-39531717">[-]</label><label class="expand" for="c-39531717">[1 more]</label></div><br/><div class="children"><div class="content">Using the ELK stack for almost a decade to have somewhat wide events + no sampling, on not Meta scale and a few GB&#x2F;day make it absolutely affordable and super fast. Unfortunately Kibana was a bit better&#x2F;easier in the old versions than nowadays but it’s still pretty straight forward to get everything out of it.</div><br/></div></div><div id="39530980" class="c"><input type="checkbox" id="c-39530980" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#39531717">prev</a><span>|</span><a href="#39530927">next</a><span>|</span><label class="collapse" for="c-39530980">[-]</label><label class="expand" for="c-39530980">[11 more]</label></div><br/><div class="children"><div class="content">The isomorphism of traces and logs is clear. You can flatten a trace to a log and you can perfectly reconstruct the trace graph from such a log. I don&#x27;t see the unifying theme that brings metrics into this framework, though. Metrics feels fundamentally different, as a way to inspect the internal state of your program, not necessarily driven by exogenous events.<p>But I definitely agree with the theme of the article that leaving a big company can feel like you got your memory erased in a time machine mishap. Inside a FANG you might become normalized to logging hundreds of thousands of informational statements, per second, per core. You might have got used to every endpoint exposing thirty million metric time series. As soon as you walk out the door some guy will chew you out about &quot;cardinality&quot; if you have 100 metrics.</div><br/><div id="39531194" class="c"><input type="checkbox" id="c-39531194" checked=""/><div class="controls bullet"><span class="by">HeyImAlex</span><span>|</span><a href="#39530980">parent</a><span>|</span><a href="#39531355">next</a><span>|</span><label class="collapse" for="c-39531194">[-]</label><label class="expand" for="c-39531194">[1 more]</label></div><br/><div class="children"><div class="content">I think all metrics can be reconstructed as “wide events” since they’re just a bunch of arbitrary data? Counts, gauges, and histograms at least seem pretty straight forward to me.<p>It seems like the main motivation for metrics is that sending + storing + querying wide events for everything is cost prohibitive and&#x2F;or performance intensive. If you can afford it and it works well, wide events is definitely more flexible. A metric is kinda just a pre-aggregation on the event stream.</div><br/></div></div><div id="39531355" class="c"><input type="checkbox" id="c-39531355" checked=""/><div class="controls bullet"><span class="by">growse</span><span>|</span><a href="#39530980">parent</a><span>|</span><a href="#39531194">prev</a><span>|</span><a href="#39530927">next</a><span>|</span><label class="collapse" for="c-39531355">[-]</label><label class="expand" for="c-39531355">[9 more]</label></div><br/><div class="children"><div class="content">If you think of a metric as an event representing the act of measuring (along with the result of that measurement), then it becomes the same as any other event.</div><br/><div id="39531479" class="c"><input type="checkbox" id="c-39531479" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#39530980">root</a><span>|</span><a href="#39531355">parent</a><span>|</span><a href="#39530927">next</a><span>|</span><label class="collapse" for="c-39531479">[-]</label><label class="expand" for="c-39531479">[8 more]</label></div><br/><div class="children"><div class="content">True. I guess the thing that I normally want from metrics is I want to have a huge number of them that exist in a way that I can look at them when I want. But I don&#x27;t want to have to pay for collecting and aggregating them all the time. So in the scenario where they are just events then I need some other control system that can trigger the collection of events that aren&#x27;t normally emitted</div><br/><div id="39531728" class="c"><input type="checkbox" id="c-39531728" checked=""/><div class="controls bullet"><span class="by">growse</span><span>|</span><a href="#39530980">root</a><span>|</span><a href="#39531479">parent</a><span>|</span><a href="#39530927">next</a><span>|</span><label class="collapse" for="c-39531728">[-]</label><label class="expand" for="c-39531728">[7 more]</label></div><br/><div class="children"><div class="content">With metrics, you&#x27;re <i>always</i> sampling. It&#x27;s impossible to know the value of the measurement at <i>every point in time</i>.<p>When you collect any form of metrics, something is choosing that sample rate.</div><br/><div id="39532413" class="c"><input type="checkbox" id="c-39532413" checked=""/><div class="controls bullet"><span class="by">deathanatos</span><span>|</span><a href="#39530980">root</a><span>|</span><a href="#39531728">parent</a><span>|</span><a href="#39531831">next</a><span>|</span><label class="collapse" for="c-39532413">[-]</label><label class="expand" for="c-39532413">[2 more]</label></div><br/><div class="children"><div class="content">It depends on the metric. Some metrics represent discrete events, such as &quot;number of HTTP requests received&quot;. It is absolutely possible to record that metrics at every point in time, without sampling.<p>(There <i>are</i> metrics that are continuous, such as CPU usage. Those, yes, you&#x27;re always sampling.)</div><br/><div id="39535060" class="c"><input type="checkbox" id="c-39535060" checked=""/><div class="controls bullet"><span class="by">growse</span><span>|</span><a href="#39530980">root</a><span>|</span><a href="#39532413">parent</a><span>|</span><a href="#39531831">next</a><span>|</span><label class="collapse" for="c-39535060">[-]</label><label class="expand" for="c-39535060">[1 more]</label></div><br/><div class="children"><div class="content">Great point. (Y) this feels like a gauge &#x2F; counter distinction?<p>You could get pedantic at this point and say that because computers are fundamentally discrete machines, it is technically possible to sample the CPU usage at every tick :p</div><br/></div></div></div></div><div id="39531831" class="c"><input type="checkbox" id="c-39531831" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#39530980">root</a><span>|</span><a href="#39531728">parent</a><span>|</span><a href="#39532413">prev</a><span>|</span><a href="#39530927">next</a><span>|</span><label class="collapse" for="c-39531831">[-]</label><label class="expand" for="c-39531831">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not true in all models. For example in the (execrable) `statsd` model there is a bit of information sent every time a metric changes.</div><br/><div id="39532429" class="c"><input type="checkbox" id="c-39532429" checked=""/><div class="controls bullet"><span class="by">deathanatos</span><span>|</span><a href="#39530980">root</a><span>|</span><a href="#39531831">parent</a><span>|</span><a href="#39530927">next</a><span>|</span><label class="collapse" for="c-39532429">[-]</label><label class="expand" for="c-39532429">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not <i>just</i> the stats protocol, it&#x27;s the underlying metric, too. statsd is just a way of recording&#x2F;transmitting metrics.<p>If I transmit a statsd metric representing &quot;CPU usage&quot;, I <i>am</i> still sampling it. E.g., I might read the CPU usage every second &amp; generate a statsd stat. That&#x27;s a sample rate of 1Hz. I <i>have</i> to choose some sampling frequency, since the API most OS&#x27;s expose is &quot;what&#x27;s the current CPU usage?&quot;.<p>If the metric is &quot;total number of HTTP requests&quot;, then I can definitely just transmit that metric every time I get a request. We&#x27;re not sampling for that metric.<p>The latter is inherently a discrete event, with which we can know every data point of, though. Things like CPU, memory, are either fundamentally continuous, or their implementations are simply sampling it.<p>I do agree the model matters too; Prom&#x27;s tendency to just poll &#x2F;metrics endpoints every <i>n</i> seconds means even things like HTTP events are inherently sampled.</div><br/><div id="39534360" class="c"><input type="checkbox" id="c-39534360" checked=""/><div class="controls bullet"><span class="by">ekimekim</span><span>|</span><a href="#39530980">root</a><span>|</span><a href="#39532429">parent</a><span>|</span><a href="#39530927">next</a><span>|</span><label class="collapse" for="c-39534360">[-]</label><label class="expand" for="c-39534360">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If I transmit a statsd metric representing &quot;CPU usage&quot;, I am still sampling it.<p>In practice this is how everyone does it, but in theory it should be possible to have a non-sampled view of CPU usage (defined as &quot;time process is scheduled onto a CPU&quot;). With the right kernel introspection, you could represent it as a series of spans covering each time slice where the process is scheduled. Perhaps with a concept of a &quot;currently ongoing span&quot; to account for the current time slice.<p>Do I think this would be more useful than the typical sampled metric? Probably not, outside some niche performance analysis workflows. But my point is that CPU is not actually continuous, and I struggle to think of any metric which cannot be represented without sampling if you REALLY need it.</div><br/><div id="39534972" class="c"><input type="checkbox" id="c-39534972" checked=""/><div class="controls bullet"><span class="by">deathanatos</span><span>|</span><a href="#39530980">root</a><span>|</span><a href="#39534360">parent</a><span>|</span><a href="#39530927">next</a><span>|</span><label class="collapse" for="c-39534972">[-]</label><label class="expand" for="c-39534972">[1 more]</label></div><br/><div class="children"><div class="content">I <i>almost</i> put exactly that in a footnote, &#x27;cept about RAM, instead of CPU usage. No OS that I know of exposes such an API, so it&#x27;s highly theoretical.<p>As for truly contiguous metrics, hmm. How about current battery charge (in Wh)? Host uptime also seems technically continuous (albeit representable by a straight line). (Yes, we track this met; it makes reboots stand out in lieu of my metrics system not providing a vertical marker feature.) Clock drift?<p>(and I&#x27;m going to insert the footnote on this comment about something something Planck units.)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39530927" class="c"><input type="checkbox" id="c-39530927" checked=""/><div class="controls bullet"><span class="by">veeralpatel979</span><span>|</span><a href="#39530980">prev</a><span>|</span><a href="#39531397">next</a><span>|</span><label class="collapse" for="c-39530927">[-]</label><label class="expand" for="c-39530927">[1 more]</label></div><br/><div class="children"><div class="content">Great article, here is a Python notebook I created earlier to show you how you can capture such wide events:<p><a href="https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1Y65qXXogoDgOnXFBDyFsW2EPsJRUf8_J?usp=sharing" rel="nofollow">https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1Y65qXXogoDgOnXFBDyF...</a></div><br/></div></div><div id="39531397" class="c"><input type="checkbox" id="c-39531397" checked=""/><div class="controls bullet"><span class="by">zemo</span><span>|</span><a href="#39530927">prev</a><span>|</span><a href="#39532101">next</a><span>|</span><label class="collapse" for="c-39531397">[-]</label><label class="expand" for="c-39531397">[3 more]</label></div><br/><div class="children"><div class="content">begging people to recognize that a person who sells a solution is going to view these problems through the lens of being rewarded for applying their solution to your problem, even if it&#x27;s not appropriate.<p>&gt; Yet, per my own experience it’s still extremely hard to explain what does Charity meant by “logs are thrash”, let alone the fact that logs and traces are essentially the same things. Why is everyone so confused?<p>Charity is not confused, Charity is <i>incentivized</i>. What she means by &quot;logs are trash&quot; is &quot;I do not sell a logging product&quot;. (and, to be clear, I&#x27;m only naming Charity individually here because that&#x27;s who the author named in their article.)<p>&gt; When I was working at Meta, I wasn’t aware that I was privileged to be using the best observability system ever.<p>The observability system that is appropriate for Meta is not necessarily appropriate for your project. Those tools are cool but also require a pretty serious investment to build and operate correctly. It&#x27;s <i>very</i> easy to wade into a cardinality explosion problem when tagging and indexing everything you can imagine, it&#x27;s <i>very</i> easy to wade into problems regarding mixed retention policies when some events are important and others are less-important, it&#x27;s <i>very</i> easy to wade into a latency-sensitivity issue if you&#x27;re building a log&#x2F;event collection infra that you don&#x27;t allow to ever lose data, etc. As it turns out, observability is a large topic.<p>The idea that there&#x27;s one &quot;best&quot; way to do observability is a little ridiculous. Like when I worked at Etsy some of the data was literally money, when I worked at Jackbox Games we made fart joke games (Quiplash, Drawful, Fibbage, You Don&#x27;t Know Jack, etc) and the infrastructure was nothing but pure cost. The observability needs of those two orgs were <i>phenomenally</i> different, because the products were different, the revenue models were different, the needs of the users were different, etc.<p>Also this notion that &quot;all you need is wide events&quot; is the answer seems ... really shallow. A data point is an unordered set of key-value pairs? That&#x27;s how ... a LOT of logging, metrics, and tracing infra expresses things at the level of an individual record&#x2F;event. The difference is in the relationships between the keys and values, the relationships between the individual records, etc.<p>and &quot;stop sampling&quot; is just a bizarre marketing angle. If you have 1 million records or 10 million records and you get the same squiggly line out of analyzing it, congrats you have inflated the size of the data that nobody ever looks at. There is only one person who this benefits and it&#x27;s the person who charges you for the pipeline, which is <i>exactly</i> why people who sell a pipeline are <i>incentivized</i> to tell you that sampling is bad: if you are sampling, you are sending and storing and querying fewer data points, so they are charging you less money. They are getting <i>paid</i> to tell you that sampling is bad. Sampling is not good or bad, sampling is sampling. The reality is that in a lot of these systems, the vast majority of the information will never, ever be looked at or used. Whether or not that matters is <i>entirely</i> context dependent.</div><br/><div id="39532237" class="c"><input type="checkbox" id="c-39532237" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#39531397">parent</a><span>|</span><a href="#39532229">next</a><span>|</span><label class="collapse" for="c-39532237">[-]</label><label class="expand" for="c-39532237">[1 more]</label></div><br/><div class="children"><div class="content">For this:<p>&gt; There is only one person who this benefits and it&#x27;s the person who charges you for the pipeline, which is exactly why people who sell a pipeline are incentivized to tell you that sampling is bad<p>I largely agree, and I&#x27;ll say that at least with Honeycomb (since it&#x27;s mentioned by the author) we make sampling a key component of pretty much any deal before anything gets signed. For small stuff this clearly doesn&#x27;t matter so much, but it basically boils down to:<p>- Most of your data is probably uninteresting because it&#x27;s uniform and represents success cases, so you just need a statistically significant sampling to get a sense of what &quot;okay&quot; means for comparisons<p>- Whenever there&#x27;s an error or high latency, there&#x27;s almost always something interesting in there, and you probably want all of it<p>And so this typically works out to generating an order of magnitude or two more data than you actually need to get an accurate view of what&#x27;s going on any any point in time.<p>And so when you do this, you can (and probably should?) pack your events&#x2F;logs&#x2F;traces&#x2F;whatever-you-call-it with a bunch of data.<p>There&#x27;s some examples where you can&#x27;t do this, though. Some people want to be able to do something like plug in a customer ID and dig up the exact trace that represents something they complained about. Or there&#x27;s some compliance to adhere to, legal or non-legal, where it&#x27;s still less money to pay for everything unsampled than it is to deal with the consequences of non-compliance. But for most organizations I&#x27;d say what I mentioned above holds true.<p>...but that&#x27;s just one of the several ways that some folks will frame up Observability. The term has been kleenexed a bit so it now means whatever any vendor says it means, and they all say varyingly different things.</div><br/></div></div><div id="39532229" class="c"><input type="checkbox" id="c-39532229" checked=""/><div class="controls bullet"><span class="by">isburmistrov</span><span>|</span><a href="#39531397">parent</a><span>|</span><a href="#39532237">prev</a><span>|</span><a href="#39532101">next</a><span>|</span><label class="collapse" for="c-39532229">[-]</label><label class="expand" for="c-39532229">[1 more]</label></div><br/><div class="children"><div class="content">&gt; and &quot;stop sampling&quot; is just a bizarre marketing angle<p>Wait, where did I mention stopping sampling? :) The opposite: the article is praising the native sampling Scuba has.</div><br/></div></div></div></div><div id="39532101" class="c"><input type="checkbox" id="c-39532101" checked=""/><div class="controls bullet"><span class="by">blinded</span><span>|</span><a href="#39531397">prev</a><span>|</span><a href="#39533616">next</a><span>|</span><label class="collapse" for="c-39532101">[-]</label><label class="expand" for="c-39532101">[2 more]</label></div><br/><div class="children"><div class="content">otel is working on their events spec <a href="https:&#x2F;&#x2F;github.com&#x2F;open-telemetry&#x2F;community&#x2F;issues&#x2F;1688">https:&#x2F;&#x2F;github.com&#x2F;open-telemetry&#x2F;community&#x2F;issues&#x2F;1688</a></div><br/><div id="39532289" class="c"><input type="checkbox" id="c-39532289" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#39532101">parent</a><span>|</span><a href="#39533616">next</a><span>|</span><label class="collapse" for="c-39532289">[-]</label><label class="expand" for="c-39532289">[1 more]</label></div><br/><div class="children"><div class="content">Heh, this is an unfortunate consequence of naming.<p>OTel Events are just OTel logs with a name. An OTel log is a log body with a trace ID, span ID, severity, etc. But that&#x27;s also an event as per the author&#x27;s definition.<p>In OTel, a span is just a structured log, which is also an event (as per the author&#x27;s definition of event). So is a Span Event, which is a log-like entity that you can produce in the context of a span. And OTel metrics produce what are called &quot;metric events&quot;, which are also events.<p>IMO it&#x27;s one of those things that&#x27;s horribly confusing until one day it&#x27;s not, and then everything starts looking like how the author described.</div><br/></div></div></div></div><div id="39533616" class="c"><input type="checkbox" id="c-39533616" checked=""/><div class="controls bullet"><span class="by">jhardy54</span><span>|</span><a href="#39532101">prev</a><span>|</span><a href="#39531678">next</a><span>|</span><label class="collapse" for="c-39533616">[-]</label><label class="expand" for="c-39533616">[1 more]</label></div><br/><div class="children"><div class="content">Is this not just structured logging? I’m wondering whether the author has used tracing tools much, or whether they’re truly trying to understand modern observability through OpenTelemetry documentation.</div><br/></div></div><div id="39531678" class="c"><input type="checkbox" id="c-39531678" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#39533616">prev</a><span>|</span><a href="#39532037">next</a><span>|</span><label class="collapse" for="c-39531678">[-]</label><label class="expand" for="c-39531678">[4 more]</label></div><br/><div class="children"><div class="content">Has anyone built an open-source version of this and have a blog post around it? Curious about implementation to see how you keep storage tight and querying still fast.</div><br/><div id="39532453" class="c"><input type="checkbox" id="c-39532453" checked=""/><div class="controls bullet"><span class="by">isburmistrov</span><span>|</span><a href="#39531678">parent</a><span>|</span><a href="#39532037">next</a><span>|</span><label class="collapse" for="c-39532453">[-]</label><label class="expand" for="c-39532453">[3 more]</label></div><br/><div class="children"><div class="content">I think ClickHouse is becoming a default storage for observability nowdays: <a href="https:&#x2F;&#x2F;clickhouse.com&#x2F;use-cases&#x2F;logging-and-metrics" rel="nofollow">https:&#x2F;&#x2F;clickhouse.com&#x2F;use-cases&#x2F;logging-and-metrics</a><p>And there are quite a few solutions on top of it.<p>A couple of examples that seem to be interesting (however I didn&#x27;t use them in real life):<p><a href="https:&#x2F;&#x2F;coroot.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;coroot.com&#x2F;</a> 
<a href="https:&#x2F;&#x2F;qryn.metrico.in&#x2F;#&#x2F;" rel="nofollow">https:&#x2F;&#x2F;qryn.metrico.in&#x2F;#&#x2F;</a></div><br/><div id="39533211" class="c"><input type="checkbox" id="c-39533211" checked=""/><div class="controls bullet"><span class="by">AndyNemmity</span><span>|</span><a href="#39531678">root</a><span>|</span><a href="#39532453">parent</a><span>|</span><a href="#39532479">next</a><span>|</span><label class="collapse" for="c-39533211">[-]</label><label class="expand" for="c-39533211">[1 more]</label></div><br/><div class="children"><div class="content">We are currently using Elasticsearch, and can only store a tiny amount of the data because of how much we use. 5 days at most.<p>ClickHouse is replacing Elasticsearch in this context, and is providing the same data storage, but with better compression, and not holding data in memory?<p>Is that correct?</div><br/></div></div><div id="39532479" class="c"><input type="checkbox" id="c-39532479" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#39531678">root</a><span>|</span><a href="#39532453">parent</a><span>|</span><a href="#39533211">prev</a><span>|</span><a href="#39532037">next</a><span>|</span><label class="collapse" for="c-39532479">[-]</label><label class="expand" for="c-39532479">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for sharing!</div><br/></div></div></div></div></div></div><div id="39532037" class="c"><input type="checkbox" id="c-39532037" checked=""/><div class="controls bullet"><span class="by">nemo44x</span><span>|</span><a href="#39531678">prev</a><span>|</span><a href="#39531105">next</a><span>|</span><label class="collapse" for="c-39532037">[-]</label><label class="expand" for="c-39532037">[3 more]</label></div><br/><div class="children"><div class="content">The best part of this post is where they quote a failed SaSS trying to explain why successful SaSS is wrong. Anything for an edge even if it’s not useful.</div><br/><div id="39533013" class="c"><input type="checkbox" id="c-39533013" checked=""/><div class="controls bullet"><span class="by">jldugger</span><span>|</span><a href="#39532037">parent</a><span>|</span><a href="#39531105">next</a><span>|</span><label class="collapse" for="c-39533013">[-]</label><label class="expand" for="c-39533013">[2 more]</label></div><br/><div class="children"><div class="content">Is honeycomb failed?</div><br/><div id="39533357" class="c"><input type="checkbox" id="c-39533357" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#39532037">root</a><span>|</span><a href="#39533013">parent</a><span>|</span><a href="#39531105">next</a><span>|</span><label class="collapse" for="c-39533357">[-]</label><label class="expand" for="c-39533357">[1 more]</label></div><br/><div class="children"><div class="content">It’d be news to me and my paycheck.</div><br/></div></div></div></div></div></div><div id="39531105" class="c"><input type="checkbox" id="c-39531105" checked=""/><div class="controls bullet"><span class="by">ojkelly</span><span>|</span><a href="#39532037">prev</a><span>|</span><a href="#39531337">next</a><span>|</span><label class="collapse" for="c-39531105">[-]</label><label class="expand" for="c-39531105">[2 more]</label></div><br/><div class="children"><div class="content">Observability as a shared concept has followed Agile and DevOps.<p>Something with a real meaning that is enables a step-change is development practices. Adoption is organic initially because the pain it solves is very real.<p>But as awareness of the idea grows it threatens established institutions and vendors, who must co-opt the concept and redefine it such that they are included.<p>If they can’t be explicitly included (logs, metrics, traces)[0], then they at least make sure the definition is becomes so vague and confused that they are not explicitly excluded[1].<p>Wide events and a good means to query them covers everything, but not if you as a vendor cannot store and query wide events.<p>[0] as the article notes, one of these is not like the other.
[1] Is Scrum Agile? What do you mean a standup can’t go for an hour? See also DevOps as a role.</div><br/></div></div><div id="39531337" class="c"><input type="checkbox" id="c-39531337" checked=""/><div class="controls bullet"><span class="by">guhcampos</span><span>|</span><a href="#39531105">prev</a><span>|</span><a href="#39531082">next</a><span>|</span><label class="collapse" for="c-39531337">[-]</label><label class="expand" for="c-39531337">[4 more]</label></div><br/><div class="children"><div class="content">Incredible what you can do with infinite money!<p>For everyone else, more specific data structures, sampling and careful consideration of what to record are essential.</div><br/><div id="39531820" class="c"><input type="checkbox" id="c-39531820" checked=""/><div class="controls bullet"><span class="by">strken</span><span>|</span><a href="#39531337">parent</a><span>|</span><a href="#39531466">next</a><span>|</span><label class="collapse" for="c-39531820">[-]</label><label class="expand" for="c-39531820">[1 more]</label></div><br/><div class="children"><div class="content">Hang on, you don&#x27;t need infinite money if you&#x27;ve got a sampling rate, do you? Drop the sampling rate (mentioned in the article: they <i>do</i> use sampling) from 0.01 to 0.001 and you&#x27;ve reduced the data ingress by a factor of ten.</div><br/></div></div><div id="39531466" class="c"><input type="checkbox" id="c-39531466" checked=""/><div class="controls bullet"><span class="by">munchbunny</span><span>|</span><a href="#39531337">parent</a><span>|</span><a href="#39531820">prev</a><span>|</span><a href="#39532252">next</a><span>|</span><label class="collapse" for="c-39531466">[-]</label><label class="expand" for="c-39531466">[1 more]</label></div><br/><div class="children"><div class="content">The key is that you pay for the bandwidth, sampling, cardinality, and schema considerations <i>somewhere</i> in the system. Depending on the problem, you may be able to get away with dealing with those issues later vs. earlier, but at some point you start dealing with required fields, aggregation, etc.<p>I own a system in one of the tech giants where sampling is counterproductive to our problem domain, and where in the pipeline we deal with each of those issues is our bread and butter problem that can sometimes swing costs by $millions.</div><br/></div></div></div></div><div id="39531082" class="c"><input type="checkbox" id="c-39531082" checked=""/><div class="controls bullet"><span class="by">rekwah</span><span>|</span><a href="#39531337">prev</a><span>|</span><a href="#39530999">next</a><span>|</span><label class="collapse" for="c-39531082">[-]</label><label class="expand" for="c-39531082">[10 more]</label></div><br/><div class="children"><div class="content">&gt;  just put it there, it might be useful later<p>&gt; Also note that we have never mentioned anything about cardinality. Because it doesn’t matter - any field can be of any cardinality. Scuba works with raw events and doesn’t pre-aggregate anything, and so cardinality is not an issue.<p>This is how we end up with very large, very expensive data swamps.</div><br/><div id="39531146" class="c"><input type="checkbox" id="c-39531146" checked=""/><div class="controls bullet"><span class="by">_visgean</span><span>|</span><a href="#39531082">parent</a><span>|</span><a href="#39531294">next</a><span>|</span><label class="collapse" for="c-39531146">[-]</label><label class="expand" for="c-39531146">[8 more]</label></div><br/><div class="children"><div class="content">that depends on the sampling rate no? I would much rather have a rich log record sampled at 1% than more records that dont contain enough info to debug..</div><br/><div id="39531319" class="c"><input type="checkbox" id="c-39531319" checked=""/><div class="controls bullet"><span class="by">growse</span><span>|</span><a href="#39531082">root</a><span>|</span><a href="#39531146">parent</a><span>|</span><a href="#39533109">next</a><span>|</span><label class="collapse" for="c-39531319">[-]</label><label class="expand" for="c-39531319">[3 more]</label></div><br/><div class="children"><div class="content">The people feeling the pain of (and paying for) the expensive data swamp are often not the same people who are yolo&#x27;ing the sample rate to 100% in their apps, because why wouldn&#x27;t you want to store every event?<p>Put another way, you&#x27;re in charge of a large telemetry event sink. How do you incentivise the correct sampling behaviour by your users?</div><br/><div id="39533116" class="c"><input type="checkbox" id="c-39533116" checked=""/><div class="controls bullet"><span class="by">kiitos</span><span>|</span><a href="#39531082">root</a><span>|</span><a href="#39531319">parent</a><span>|</span><a href="#39531866">next</a><span>|</span><label class="collapse" for="c-39533116">[-]</label><label class="expand" for="c-39533116">[1 more]</label></div><br/><div class="children"><div class="content">You should never need to sample telemetry data.</div><br/></div></div><div id="39531866" class="c"><input type="checkbox" id="c-39531866" checked=""/><div class="controls bullet"><span class="by">Spivak</span><span>|</span><a href="#39531082">root</a><span>|</span><a href="#39531319">parent</a><span>|</span><a href="#39533116">prev</a><span>|</span><a href="#39533109">next</a><span>|</span><label class="collapse" for="c-39531866">[-]</label><label class="expand" for="c-39531866">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t let the user pick the sampling rate. In Honeycomb land this is called the EMA Dynamic Sampler.<p><a href="https:&#x2F;&#x2F;docs.honeycomb.io&#x2F;manage-data-volume&#x2F;refinery&#x2F;sampling-methods&#x2F;#deterministic-sampler" rel="nofollow">https:&#x2F;&#x2F;docs.honeycomb.io&#x2F;manage-data-volume&#x2F;refinery&#x2F;sampli...</a></div><br/></div></div></div></div><div id="39533109" class="c"><input type="checkbox" id="c-39533109" checked=""/><div class="controls bullet"><span class="by">kiitos</span><span>|</span><a href="#39531082">root</a><span>|</span><a href="#39531146">parent</a><span>|</span><a href="#39531319">prev</a><span>|</span><a href="#39531410">next</a><span>|</span><label class="collapse" for="c-39533109">[-]</label><label class="expand" for="c-39533109">[1 more]</label></div><br/><div class="children"><div class="content">It is a tragedy of the current generation of observability systems that they have inculcated the notion that telemetry  data should be sampled. Absolute nonsense.</div><br/></div></div><div id="39531410" class="c"><input type="checkbox" id="c-39531410" checked=""/><div class="controls bullet"><span class="by">gtirloni</span><span>|</span><a href="#39531082">root</a><span>|</span><a href="#39531146">parent</a><span>|</span><a href="#39533109">prev</a><span>|</span><a href="#39531294">next</a><span>|</span><label class="collapse" for="c-39531410">[-]</label><label class="expand" for="c-39531410">[3 more]</label></div><br/><div class="children"><div class="content">Metrics sample rate yes but logging sample? When an end-to-end transaction for a very important task breaks, do I get *some* breadcrumbs to debug it?</div><br/><div id="39534020" class="c"><input type="checkbox" id="c-39534020" checked=""/><div class="controls bullet"><span class="by">goosejuice</span><span>|</span><a href="#39531082">root</a><span>|</span><a href="#39531410">parent</a><span>|</span><a href="#39532254">next</a><span>|</span><label class="collapse" for="c-39534020">[-]</label><label class="expand" for="c-39534020">[1 more]</label></div><br/><div class="children"><div class="content">I agree. Sampling logs.. sounds dangerous. Obviously every system is different.<p>At least in GCP you can apply a filter to prevent ingestion and set different expiries on log budgets. This can help control costs without missing important entries.</div><br/></div></div><div id="39532254" class="c"><input type="checkbox" id="c-39532254" checked=""/><div class="controls bullet"><span class="by">isburmistrov</span><span>|</span><a href="#39531082">root</a><span>|</span><a href="#39531410">parent</a><span>|</span><a href="#39534020">prev</a><span>|</span><a href="#39531294">next</a><span>|</span><label class="collapse" for="c-39532254">[-]</label><label class="expand" for="c-39532254">[1 more]</label></div><br/><div class="children"><div class="content">Sampling can be smart, e.g. based on some field all events have (can be called traceId, haha).</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>