<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1736240477098" as="style"/><link rel="stylesheet" href="styles.css?v=1736240477098"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://crawshaw.io/blog/programming-with-llms">How I program with LLMs</a>Â <span class="domain">(<a href="https://crawshaw.io">crawshaw.io</a>)</span></div><div class="subtext"><span>stpn</span> | <span>127 comments</span></div><br/><div><div id="42619022" class="c"><input type="checkbox" id="c-42619022" checked=""/><div class="controls bullet"><span class="by">dewitt</span><span>|</span><a href="#42618364">next</a><span>|</span><label class="collapse" for="c-42619022">[-]</label><label class="expand" for="c-42619022">[18 more]</label></div><br/><div class="children"><div class="content">One interesting bit of context is that the author of this post is a legit world-class software engineer already (though probably too modest to admit it). Former staff engineer at Google and co-founder &#x2F; CTO of Tailscale. He doesn&#x27;t <i>need</i> LLMs. That he says LLMs make him more productive at all as a hands-on developer, especially around first drafts on a new idea, means a lot to me personally.<p>His post reminds me of an old idea I had of a language where all you wrote was function signatures and high-level control flow, and maybe some conformance tests around them. The language was designed around filling in the implementations for you. 20 years ago that would have been from a live online database, with implementations vying for popularity on the basis of speed or correctness. Nowadays LLMs would generate most of it on the fly, presumably.<p>Most ideas are unoriginal, so I wouldn&#x27;t be surprised if this has been tried already.</div><br/><div id="42620609" class="c"><input type="checkbox" id="c-42620609" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#42619022">parent</a><span>|</span><a href="#42619856">next</a><span>|</span><label class="collapse" for="c-42620609">[-]</label><label class="expand" for="c-42620609">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>[David, Former staff engineer at Google ... CTO of Tailscale,] doesn&#x27;t need LLMs. That he says LLMs make him more productive at all as a hands-on developer, especially around first drafts on a new idea, means a lot to me...</i><p>Don&#x27;t doubt for a second the pedigree of founding engs at Tailscale, but David is careful to point exactly why LLMs work for them (but might not for others):<p><pre><code>   I am doing a particular kind of programming, product development, which could be roughly described as trying to bring programs to a user through a robust interface. That means I am building a lot, throwing away a lot, and bouncing around between environments. Some days I mostly write typescript, some days mostly Go. I spent a week in a C++ codebase last month exploring an idea, and just had an opportunity to learn the HTTP server-side events format. I am all over the place, constantly forgetting and relearning.

  If you spend more time proving your optimization of a cryptographic algorithm is not vulnerable to timing attacks than you do writing the code, I don&#x27;t think any of my observations here are going to be useful to you.</code></pre></div><br/></div></div><div id="42619856" class="c"><input type="checkbox" id="c-42619856" checked=""/><div class="controls bullet"><span class="by">gopalv</span><span>|</span><a href="#42619022">parent</a><span>|</span><a href="#42620609">prev</a><span>|</span><a href="#42620535">next</a><span>|</span><label class="collapse" for="c-42619856">[-]</label><label class="expand" for="c-42619856">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That he says LLMs make him more productive at all as a hands-on developer, especially around first drafts on a new idea, means a lot to me personally.<p>There is likely to be a great rift in how very talented people look at sharper tools.<p>I&#x27;ve seen the same division pop up with CNC machines, 3d printers, IDEs and now LLMs.<p>If you are good at doing something, you might find the new tool&#x27;s output to be sub-par over what you can achieve yourself, but often the lower quality output comes much faster than you can generate.<p>That causes the people who are deliberate &amp; precise about their process to hate the new tool completely - expressing in the actual code (or paint, or marks on wood) is much better than trying to explain it in a less precise language in the middle of it. The only exception I&#x27;ve seen is that engineering folks often use a blueprint &amp; refine it on paper.<p>There&#x27;s a double translation overhead which is wasteful if you don&#x27;t need it.<p>If you have dealt with a new hire while being the senior of the pair, there&#x27;s that familiar feeling of wanting to grab their keyboard instead of explaining how to build that regex - being able to do more things than you can explain or just having a higher bandwidth pipe into the actual task is a common sign of mastery.<p>The incrementalists on the other hand, tend to love the new tool as they tend to build 6 different things before picking what works the best, slowly iterating towards what they had in mind in the first place.<p>I got into this profession simply because I could Ctrl-Z to the previous step much more easily than my then favourite chemical engineering goals. In Chemistry, if you get a step wrong, you go to the start &amp; start over. Plus even when things work, yield is just a pain there (prove it first, then you scale up ingredients etc).<p>Just from the name of sketch.dev, it appears that this author is of the &#x27;sketch first &amp; refine&#x27; model where the new tool just speeds up that loop of infinite refinement.</div><br/></div></div><div id="42620535" class="c"><input type="checkbox" id="c-42620535" checked=""/><div class="controls bullet"><span class="by">antirez</span><span>|</span><a href="#42619022">parent</a><span>|</span><a href="#42619856">prev</a><span>|</span><a href="#42620047">next</a><span>|</span><label class="collapse" for="c-42620535">[-]</label><label class="expand" for="c-42620535">[1 more]</label></div><br/><div class="children"><div class="content">I have also many years of programming experience and find myself strongly &quot;accelerated&quot; by LLMs when writing code. But, if you think at it, it makes sense that many seasoned programmers are using LLMs better. LLMs are a helpful tool, but also a hard-to-use tool, and in general it&#x27;s fair to think that better programmers can do a better use of some assistant (human or otherwise): better understanding its strengths, identifying faster the good and bad input, providing better guidance to correct the approach...</div><br/></div></div><div id="42620047" class="c"><input type="checkbox" id="c-42620047" checked=""/><div class="controls bullet"><span class="by">greenyouse</span><span>|</span><a href="#42619022">parent</a><span>|</span><a href="#42620535">prev</a><span>|</span><a href="#42619487">next</a><span>|</span><label class="collapse" for="c-42620047">[-]</label><label class="expand" for="c-42620047">[2 more]</label></div><br/><div class="children"><div class="content">That approach sounds similar to the Idris programming language with Type Driven Development. It starts by planning out the program structure with types and function signatures. Then the function implementation (aka holes) can be filled in after the function signatures and types are set.<p>I feel like this is a great approach for LLM assisted programming because things like types, function signatures, pre&#x2F;post conditions, etc. give more clarity and guidance to the LLM. The more constraints that the LLM has to operate under, the less likely it is to get off track and be inconsistent.<p>I&#x27;ve taken a shot at doing some little projects for fun with this style of programming in TypeScript and it works pretty well. The programs are written in layers with the domain design, types, schema, and function contracts being figured out first (optionally with some LLM help). Then the function implementations can be figured out towards the end.<p>It might be fun to try Effect-TS for ADTs + contracts + compile time type validation. It seems like that locks down a lot of the details so it might be good for LLMs. It&#x27;s fun to play around with different techniques and see what works!</div><br/><div id="42620279" class="c"><input type="checkbox" id="c-42620279" checked=""/><div class="controls bullet"><span class="by">lysecret</span><span>|</span><a href="#42619022">root</a><span>|</span><a href="#42620047">parent</a><span>|</span><a href="#42619487">next</a><span>|</span><label class="collapse" for="c-42620279">[-]</label><label class="expand" for="c-42620279">[1 more]</label></div><br/><div class="children"><div class="content">100% this is what I do in python too!</div><br/></div></div></div></div><div id="42619487" class="c"><input type="checkbox" id="c-42619487" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#42619022">parent</a><span>|</span><a href="#42620047">prev</a><span>|</span><a href="#42620128">next</a><span>|</span><label class="collapse" for="c-42619487">[-]</label><label class="expand" for="c-42619487">[1 more]</label></div><br/><div class="children"><div class="content">I think what you&#x27;re describing is basically &quot;interface driven development&quot; and &quot;test driven development&quot; taken to the extreme: where the formal specification of an implementation is defined by the test suite.   I suppose a cynic would say that&#x27;s what you get if you left an AI alone in a room with Hyrum&#x27;s Law.</div><br/></div></div><div id="42620128" class="c"><input type="checkbox" id="c-42620128" checked=""/><div class="controls bullet"><span class="by">brabel</span><span>|</span><a href="#42619022">parent</a><span>|</span><a href="#42619487">prev</a><span>|</span><a href="#42620414">next</a><span>|</span><label class="collapse" for="c-42620128">[-]</label><label class="expand" for="c-42620128">[4 more]</label></div><br/><div class="children"><div class="content">I am not a genius but have a couple of decades experience and finally started using LLMs in anger in the last few weeks. I have to admit that when my free quota from GitHub Copilot ran out (I had already run out of Jetbrains AI as well!! Our company will start paying for some service as the trials have been very successful), I had a slight bad feeling as my experience was very similar to OP: it&#x27;s really useful to get me started, and I can finish it much more easily from what the AI gives me than if I started from scratch. Sometimes it just fills in boilerplate, other times it actually tells me which functions to call on an unfamiliar API. And it turns out it&#x27;s really good at generating tests, so it makes my testing more comprehensive as it&#x27;s so much faster to just write them out (and refine a bit usually by hand). The chat almost completely replaced my StackOverflow queries, which saves me much time and anxiety (God forbid I have to ask something on SO as that&#x27;s a time sink: if I just quickly type out something I am just asking to be obliterated by the &quot;helpful&quot; SO moderators... with the AI, I just barely type anything at all, leave it with typos and all, the AI still gets me!).</div><br/><div id="42620258" class="c"><input type="checkbox" id="c-42620258" checked=""/><div class="controls bullet"><span class="by">EagnaIonat</span><span>|</span><a href="#42619022">root</a><span>|</span><a href="#42620128">parent</a><span>|</span><a href="#42620322">next</a><span>|</span><label class="collapse" for="c-42620258">[-]</label><label class="expand" for="c-42620258">[1 more]</label></div><br/><div class="children"><div class="content">Have you tried using Ollama? You can download and run an LLM locally on your machine.<p>You can also pick the right model for the right need and it&#x27;s free.</div><br/></div></div><div id="42620322" class="c"><input type="checkbox" id="c-42620322" checked=""/><div class="controls bullet"><span class="by">devjab</span><span>|</span><a href="#42619022">root</a><span>|</span><a href="#42620128">parent</a><span>|</span><a href="#42620258">prev</a><span>|</span><a href="#42620414">next</a><span>|</span><label class="collapse" for="c-42620322">[-]</label><label class="expand" for="c-42620322">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m genuinely curious but what did you use StackOverflow for before? With a couple of decades in the industry I can&#x27;t remember when the last time I &quot;Google programmed&quot; anything was. I always go directly to the documentation for whatever it is I&#x27;m working for, because where else would I find out how it actually works? It&#x27;s not like I haven&#x27;t &quot;Google programmed&quot; when I was younger, but it&#x27;s just such a slow process based on trusting strangers on the internet that it never really made much sense once I started knowing what I was doing. I sort of view LLM&#x27;s in a similar manner. Why would you go to them rather than the actual documentation? I realize this might sound arrogant or rude, and I really hope you believe me when I say that I don&#x27;t mean it like this. The reason I&#x27;m curious is because we&#x27;re really struggling getting junior developers to not look, everywhere, but the documentation first. Which means they often actually don&#x27;t know how what they build works. Which can be an issue when they load every object of a list into memory isntead of using a generator...<p>As far as using LLMs in anger I would really advice anyone to use them. GitHub copilot hasn&#x27;t been very useful for me personally, but I get a lot of value out of running my thought process by a LLM. I think better when I &quot;think out loud&quot; and that is obviously challenging when everyone is busy. Running my ideas by an LLM helps me process them in a similar (if not better) fashion, often it won&#x27;t even really matter what the LLM conjures up because simply describing what I want to do often gives me new ideas, like &quot;thinking out loud&quot;.<p>As far as coding goes. I find it extremely useful to have LLMs write cli scripts to auto-generate code. The code the LLM will produce is going to be absolute shite, but that doesn&#x27;t matter if the output is perfectly fine. It&#x27;s reduced my personal reliance on third party tools by quite a lot. Because why would I need a code generator for something (and in that process trust a bunch of 3rd party libraries) when I can have a LLM write a similar tool in half an hour?</div><br/><div id="42620473" class="c"><input type="checkbox" id="c-42620473" checked=""/><div class="controls bullet"><span class="by">wiseowise</span><span>|</span><a href="#42619022">root</a><span>|</span><a href="#42620322">parent</a><span>|</span><a href="#42620414">next</a><span>|</span><label class="collapse" for="c-42620473">[-]</label><label class="expand" for="c-42620473">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why would you go to them rather than the actual documentation?<p>Not every documentation is made equal. For example: Android docs are royal shit. They cover some basic things, e.g. show a button, but good look finding esoteric Bluetooth information or package management, etc. Most of it is a mix of experimentation and historical knowledge (baggage).</div><br/></div></div></div></div></div></div><div id="42620414" class="c"><input type="checkbox" id="c-42620414" checked=""/><div class="controls bullet"><span class="by">benterix</span><span>|</span><a href="#42619022">parent</a><span>|</span><a href="#42620128">prev</a><span>|</span><a href="#42619912">next</a><span>|</span><label class="collapse" for="c-42620414">[-]</label><label class="expand" for="c-42620414">[1 more]</label></div><br/><div class="children"><div class="content">&gt; designed around filling in the implementations for you. 20 years ago that would have been from a live online database<p>This reminds me a bit of PowerBuilder (or was it PowerDesigner?) from early 1990s. They sold it to SAP later, I was told it&#x27;s still being used today.</div><br/></div></div><div id="42619912" class="c"><input type="checkbox" id="c-42619912" checked=""/><div class="controls bullet"><span class="by">CraigJPerry</span><span>|</span><a href="#42619022">parent</a><span>|</span><a href="#42620414">prev</a><span>|</span><a href="#42619412">next</a><span>|</span><label class="collapse" for="c-42619912">[-]</label><label class="expand" for="c-42619912">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; where all you wrote was function signatures and high-level control flow, and maybe some conformance tests around them<p>AIUI thatâs where idris is headed</div><br/></div></div><div id="42619412" class="c"><input type="checkbox" id="c-42619412" checked=""/><div class="controls bullet"><span class="by">knighthack</span><span>|</span><a href="#42619022">parent</a><span>|</span><a href="#42619912">prev</a><span>|</span><a href="#42620331">next</a><span>|</span><label class="collapse" for="c-42619412">[-]</label><label class="expand" for="c-42619412">[3 more]</label></div><br/><div class="children"><div class="content">I knew he was a world-class engineer the moment I saw that his site didn&#x27;t bother with CSS stylesheets, ads, pictures, or anything beyond a rudimentary layout.<p>The whole article page reads like a site from the &#x27;90s, written from scratch in HTML.<p>That&#x27;s when I <i>knew</i> the article would go hard.<p>Substantive pieces don&#x27;t need fluffy UIs - the idea takes the stage, not the window dressing.</div><br/><div id="42619481" class="c"><input type="checkbox" id="c-42619481" checked=""/><div class="controls bullet"><span class="by">shaneofalltrad</span><span>|</span><a href="#42619022">root</a><span>|</span><a href="#42619412">parent</a><span>|</span><a href="#42620331">next</a><span>|</span><label class="collapse" for="c-42619481">[-]</label><label class="expand" for="c-42619481">[2 more]</label></div><br/><div class="children"><div class="content">I wonder what he uses, I noticed the first paragraph took over a second to load...
Largest Contentful Paint element 1,370 ms
This is the largest contentful element painted within the viewport.
Element
p</div><br/><div id="42620479" class="c"><input type="checkbox" id="c-42620479" checked=""/><div class="controls bullet"><span class="by">cess11</span><span>|</span><a href="#42619022">root</a><span>|</span><a href="#42619481">parent</a><span>|</span><a href="#42620331">next</a><span>|</span><label class="collapse" for="c-42620479">[-]</label><label class="expand" for="c-42620479">[1 more]</label></div><br/><div class="children"><div class="content">Looks like it loads all the Google surveillance without asking. Should IP-block the EU.</div><br/></div></div></div></div></div></div><div id="42620331" class="c"><input type="checkbox" id="c-42620331" checked=""/><div class="controls bullet"><span class="by">ilrwbwrkhv</span><span>|</span><a href="#42619022">parent</a><span>|</span><a href="#42619412">prev</a><span>|</span><a href="#42618364">next</a><span>|</span><label class="collapse" for="c-42620331">[-]</label><label class="expand" for="c-42620331">[2 more]</label></div><br/><div class="children"><div class="content">Being a dev at a large company is usually the sign that you&#x27;re not very good though. And anyone can start a company with the right connections.</div><br/><div id="42620363" class="c"><input type="checkbox" id="c-42620363" checked=""/><div class="controls bullet"><span class="by">ksenzee</span><span>|</span><a href="#42619022">root</a><span>|</span><a href="#42620331">parent</a><span>|</span><a href="#42618364">next</a><span>|</span><label class="collapse" for="c-42620363">[-]</label><label class="expand" for="c-42620363">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;ve just disproved your own assertion. Either that or you believe everyone who&#x27;s any good has the right connections.</div><br/></div></div></div></div></div></div><div id="42618364" class="c"><input type="checkbox" id="c-42618364" checked=""/><div class="controls bullet"><span class="by">mlepath</span><span>|</span><a href="#42619022">prev</a><span>|</span><a href="#42618732">next</a><span>|</span><label class="collapse" for="c-42618364">[-]</label><label class="expand" for="c-42618364">[18 more]</label></div><br/><div class="children"><div class="content">The first rule of programming with LLMs is don&#x27;t use them for anything you don&#x27;t know how to do. If you can look at the solution and immediately know what&#x27;s wrong with it, they are a time saver otherwise...<p>I find chat for search is really helpful (as the article states)</div><br/><div id="42618711" class="c"><input type="checkbox" id="c-42618711" checked=""/><div class="controls bullet"><span class="by">billmcneale</span><span>|</span><a href="#42618364">parent</a><span>|</span><a href="#42618549">next</a><span>|</span><label class="collapse" for="c-42618711">[-]</label><label class="expand" for="c-42618711">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the wrong approach.<p>I use chat for things I don&#x27;t know how to do all the time. I might not know how to do it, but I sure know how to test that what I&#x27;m being told is correct. And as long as it&#x27;s not, I iterate with the chat bot.</div><br/><div id="42618845" class="c"><input type="checkbox" id="c-42618845" checked=""/><div class="controls bullet"><span class="by">WhiteNoiz3</span><span>|</span><a href="#42618364">root</a><span>|</span><a href="#42618711">parent</a><span>|</span><a href="#42618549">next</a><span>|</span><label class="collapse" for="c-42618845">[-]</label><label class="expand" for="c-42618845">[2 more]</label></div><br/><div class="children"><div class="content">A better way to phrase it might be don&#x27;t use it for something that you aren&#x27;t able to verify or validate.</div><br/><div id="42619007" class="c"><input type="checkbox" id="c-42619007" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42618364">root</a><span>|</span><a href="#42618845">parent</a><span>|</span><a href="#42618549">next</a><span>|</span><label class="collapse" for="c-42619007">[-]</label><label class="expand" for="c-42619007">[1 more]</label></div><br/><div class="children"><div class="content">I agree with this. I keep harping on this, but we are sold automation instead of a power tool. If you have domain knowledge in the problem that you are solving, then LLMs can become an extremely valuable aid.</div><br/></div></div></div></div></div></div><div id="42618549" class="c"><input type="checkbox" id="c-42618549" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#42618364">parent</a><span>|</span><a href="#42618711">prev</a><span>|</span><a href="#42618512">next</a><span>|</span><label class="collapse" for="c-42618549">[-]</label><label class="expand" for="c-42618549">[1 more]</label></div><br/><div class="children"><div class="content">That seems like a wild restriction.<p>You can give them more latitude for things you know how to <i>check</i>.<p>I didn&#x27;t know how to setup the right gnarly typescript generic type to solve my problem but I could easily verify it&#x27;s correct.</div><br/></div></div><div id="42618512" class="c"><input type="checkbox" id="c-42618512" checked=""/><div class="controls bullet"><span class="by">photon_collider</span><span>|</span><a href="#42618364">parent</a><span>|</span><a href="#42618549">prev</a><span>|</span><a href="#42618497">next</a><span>|</span><label class="collapse" for="c-42618512">[-]</label><label class="expand" for="c-42618512">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Trust but verify&quot; is still useful especially when you ask LLMs to do stuff you don&#x27;t know. I&#x27;ve used LLMs to help me get started on tasks where I wasn&#x27;t even sure of what a solution was. I would then inspect the code and review any relevant documentation to see if the proposed solution would work. This has been time consuming but I&#x27;ve learned a lot regardless.</div><br/></div></div><div id="42618497" class="c"><input type="checkbox" id="c-42618497" checked=""/><div class="controls bullet"><span class="by">itsgrimetime</span><span>|</span><a href="#42618364">parent</a><span>|</span><a href="#42618512">prev</a><span>|</span><a href="#42619720">next</a><span>|</span><label class="collapse" for="c-42618497">[-]</label><label class="expand" for="c-42618497">[4 more]</label></div><br/><div class="children"><div class="content">IMO this is a bad take. I use LLMs for things I donât know how to do myself all the time. Now, I wouldnât use one to write some new crypto functions because the risk associated with getting it wrong is huge, but if I need to write something like a wrapper around some cloud provider SDK that Iâm unfamiliar with, it gets me 90% of the way there. It also is way more likely to know at least _some_ of the best practices where Iâll likely know none. Even for more complex things getting some working hello world examples from an LLM gives me way more threads to pull on and research than web searching ever has.</div><br/><div id="42618538" class="c"><input type="checkbox" id="c-42618538" checked=""/><div class="controls bullet"><span class="by">Retr0id</span><span>|</span><a href="#42618364">root</a><span>|</span><a href="#42618497">parent</a><span>|</span><a href="#42618834">next</a><span>|</span><label class="collapse" for="c-42618538">[-]</label><label class="expand" for="c-42618538">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  if I need to write something like a wrapper around some cloud provider SDK that Iâm unfamiliar with<p>But &quot;writing a wrapper&quot; is (presumably) a process you&#x27;re familiar with, you can tell if it&#x27;s going off the rails.</div><br/><div id="42618823" class="c"><input type="checkbox" id="c-42618823" checked=""/><div class="controls bullet"><span class="by">joemazerino</span><span>|</span><a href="#42618364">root</a><span>|</span><a href="#42618538">parent</a><span>|</span><a href="#42618834">next</a><span>|</span><label class="collapse" for="c-42618823">[-]</label><label class="expand" for="c-42618823">[1 more]</label></div><br/><div class="children"><div class="content">Writing a wrapper is easier to verify because of the context of the API or SDK you&#x27;re wrapping. Seems wrong? Check the docs. Doesn&#x27;t work? Curl it yourself.</div><br/></div></div></div></div><div id="42618834" class="c"><input type="checkbox" id="c-42618834" checked=""/><div class="controls bullet"><span class="by">Barrin92</span><span>|</span><a href="#42618364">root</a><span>|</span><a href="#42618497">parent</a><span>|</span><a href="#42618538">prev</a><span>|</span><a href="#42619720">next</a><span>|</span><label class="collapse" for="c-42618834">[-]</label><label class="expand" for="c-42618834">[1 more]</label></div><br/><div class="children"><div class="content">&gt;It also is way more likely to know at least _some_ of the best practices<p>What&#x27;s way more likely to know the best practices is the documentation. A few months ago there was a post that made the rounds about how the Arc browser introduced a really severe security flaw by misconfiguring their Firebase ACLs despite the fact that the correct way to configure them is outlined in the docs.<p>This to me is the sort of thing (although maybe not necessarily in this case) out of LLM programming. 90% isn&#x27;t good enough, it&#x27;s the same as Stackoverflow pasting. If you&#x27;re a serious engineer and you are unsure about something, it is your task to go to the reference material, or you&#x27;re at some point introducing bugs like this.<p>In our profession it&#x27;s not just crypto libraries, one misconfigured line in a yaml file can mean causing millions of dollars of damage or leaking people&#x27;s most private information. That can&#x27;t be tackled with a black box chatbot that may or may not be accurate.</div><br/></div></div></div></div><div id="42619720" class="c"><input type="checkbox" id="c-42619720" checked=""/><div class="controls bullet"><span class="by">tnvmadhav</span><span>|</span><a href="#42618364">parent</a><span>|</span><a href="#42618497">prev</a><span>|</span><a href="#42618474">next</a><span>|</span><label class="collapse" for="c-42619720">[-]</label><label class="expand" for="c-42619720">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d like to rephrase as, &quot;don&#x27;t deploy LLM generated code if you don&#x27;t know how it works (or what it does)&quot;<p>This means, it&#x27;s okay to use LLM to try something new that you&#x27;re on the fence about. Learn it and then once you&#x27;ve learned that concept or the idea, you can go ahead to use same code if it&#x27;s good enough.</div><br/></div></div><div id="42618474" class="c"><input type="checkbox" id="c-42618474" checked=""/><div class="controls bullet"><span class="by">qianli_cs</span><span>|</span><a href="#42618364">parent</a><span>|</span><a href="#42619720">prev</a><span>|</span><a href="#42620028">next</a><span>|</span><label class="collapse" for="c-42618474">[-]</label><label class="expand" for="c-42618474">[1 more]</label></div><br/><div class="children"><div class="content">Exactly, you have to (vaguely) know what youâre looking for and have some basic ideas of what algorithms would work. AI is good at helping with syntax stuff but not really good at thinking.</div><br/></div></div><div id="42620028" class="c"><input type="checkbox" id="c-42620028" checked=""/><div class="controls bullet"><span class="by">staticautomatic</span><span>|</span><a href="#42618364">parent</a><span>|</span><a href="#42618474">prev</a><span>|</span><a href="#42618914">next</a><span>|</span><label class="collapse" for="c-42620028">[-]</label><label class="expand" for="c-42620028">[1 more]</label></div><br/><div class="children"><div class="content">My experience is the opposite. I find them most valuable for helping me do things that would be extremely hard or impossible for me to figure out. To wit, I just used one to decode a pagination cursor format and write a function that takes a datetime and generates a valid cursor. Ainât nobody got time for that.</div><br/></div></div><div id="42618914" class="c"><input type="checkbox" id="c-42618914" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#42618364">parent</a><span>|</span><a href="#42620028">prev</a><span>|</span><a href="#42618792">next</a><span>|</span><label class="collapse" for="c-42618914">[-]</label><label class="expand" for="c-42618914">[1 more]</label></div><br/><div class="children"><div class="content">I completely agree. In graphics programming, I love having it do things that are annoying but easy to verify (like setting up frame buffers in WebGL). I also ask it do more ambitious things like implementing an algorithm in shader code, and it will sometimes give a result that is mostly correct but subtly wrong. I only have been able to catch those subtle errors because I know what to look for.</div><br/></div></div><div id="42618792" class="c"><input type="checkbox" id="c-42618792" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#42618364">parent</a><span>|</span><a href="#42618914">prev</a><span>|</span><a href="#42618705">next</a><span>|</span><label class="collapse" for="c-42618792">[-]</label><label class="expand" for="c-42618792">[3 more]</label></div><br/><div class="children"><div class="content">You can ask the LLM to teach it to you step by step, and then you can validate it by doing it as well as you go, still quicker than learning it and not knowing how to debug it.<p>Learning how something works is critical or it&#x27;s far worse than technical debt.</div><br/><div id="42619819" class="c"><input type="checkbox" id="c-42619819" checked=""/><div class="controls bullet"><span class="by">lelandfe</span><span>|</span><a href="#42618364">root</a><span>|</span><a href="#42618792">parent</a><span>|</span><a href="#42618705">next</a><span>|</span><label class="collapse" for="c-42619819">[-]</label><label class="expand" for="c-42619819">[2 more]</label></div><br/><div class="children"><div class="content">Yes, I have a friend learning their first programming language with much assistance from ChatGPT and it&#x27;s actually going really well.</div><br/><div id="42619909" class="c"><input type="checkbox" id="c-42619909" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#42618364">root</a><span>|</span><a href="#42619819">parent</a><span>|</span><a href="#42618705">next</a><span>|</span><label class="collapse" for="c-42619909">[-]</label><label class="expand" for="c-42619909">[1 more]</label></div><br/><div class="children"><div class="content">Awesome, I wish more people knew about this compared to trying to do magic Harry Potter single prompt to do everything.</div><br/></div></div></div></div></div></div><div id="42618705" class="c"><input type="checkbox" id="c-42618705" checked=""/><div class="controls bullet"><span class="by">kamaal</span><span>|</span><a href="#42618364">parent</a><span>|</span><a href="#42618792">prev</a><span>|</span><a href="#42618732">next</a><span>|</span><label class="collapse" for="c-42618705">[-]</label><label class="expand" for="c-42618705">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt;f you can look at the solution and immediately know what&#x27;s wrong with it, they are a time saver otherwise...<p>Indeed getting good at writing code using LLMs demands being very good at reading code.<p>To that extent its more like blitz chess than autocomplete. You need to think and verify in trees as it goes.</div><br/></div></div></div></div><div id="42618732" class="c"><input type="checkbox" id="c-42618732" checked=""/><div class="controls bullet"><span class="by">wdutch</span><span>|</span><a href="#42618364">prev</a><span>|</span><a href="#42619477">next</a><span>|</span><label class="collapse" for="c-42618732">[-]</label><label class="expand" for="c-42618732">[7 more]</label></div><br/><div class="children"><div class="content">I no longer work in tech, but I still write simple applications to make my work life easier.<p>I frequently use what OP refers to as chat-driven programming, and I find it incredibly useful. My process starts by explaining a minimum viable product to the chat, which then generates the code for me. Sometimes, the code requires a bit of manual tweaking, but itâs usually a solid starting point. From there, I describe each new feature I want to addâoften pasting in specific functions for the chat to modify or expand.<p>This approach significantly boosts what I can get done in one coding session. I can take an idea and turn it into something functional on the same day. It allows me to quickly test all my ideas, and if one doesnât help as expected, I havenât wasted much time or effort.<p>The biggest downside, however, is the rapid accumulation of technical debt. The code can get messy quickly. There&#x27;s often a lot of redundancy and after a few iterations it can be quite daunting to modify.</div><br/><div id="42619220" class="c"><input type="checkbox" id="c-42619220" checked=""/><div class="controls bullet"><span class="by">prettyblocks</span><span>|</span><a href="#42618732">parent</a><span>|</span><a href="#42619107">next</a><span>|</span><label class="collapse" for="c-42619220">[-]</label><label class="expand" for="c-42619220">[1 more]</label></div><br/><div class="children"><div class="content">I have a similar approach, but the mess can be contained by asking for optimizations and refactors very frequently and only asking for very granular features.</div><br/></div></div><div id="42619107" class="c"><input type="checkbox" id="c-42619107" checked=""/><div class="controls bullet"><span class="by">chii</span><span>|</span><a href="#42618732">parent</a><span>|</span><a href="#42619220">prev</a><span>|</span><a href="#42618789">next</a><span>|</span><label class="collapse" for="c-42619107">[-]</label><label class="expand" for="c-42619107">[3 more]</label></div><br/><div class="children"><div class="content">&gt; The code can get messy quickly. There&#x27;s often a lot of redundancy and after a few iterations it can be quite daunting to modify.<p>i forsee in the future an LLM that has sufficient context length for (automatic) refactoring and tech debt removal, by pasting large portions of these existing code in.</div><br/><div id="42620618" class="c"><input type="checkbox" id="c-42620618" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#42618732">root</a><span>|</span><a href="#42619107">parent</a><span>|</span><a href="#42619445">next</a><span>|</span><label class="collapse" for="c-42620618">[-]</label><label class="expand" for="c-42620618">[1 more]</label></div><br/><div class="children"><div class="content">Cursor has recently added something like this âBug Finderâ. It told me that finding bugs on my entire codebase would cost me $21 or so, so I never actually tried, but it sounds cool.</div><br/></div></div><div id="42619445" class="c"><input type="checkbox" id="c-42619445" checked=""/><div class="controls bullet"><span class="by">scarface_74</span><span>|</span><a href="#42618732">root</a><span>|</span><a href="#42619107">parent</a><span>|</span><a href="#42620618">prev</a><span>|</span><a href="#42618789">next</a><span>|</span><label class="collapse" for="c-42619445">[-]</label><label class="expand" for="c-42619445">[1 more]</label></div><br/><div class="children"><div class="content">Even without LLMs, at least with statically type languages like C#, ReSharper can do solution wide refactoring that are guaranteed correct as long as you donât use reflection.<p><a href="https:&#x2F;&#x2F;www.jetbrains.com&#x2F;help&#x2F;resharper&#x2F;Refactorings__Index.html" rel="nofollow">https:&#x2F;&#x2F;www.jetbrains.com&#x2F;help&#x2F;resharper&#x2F;Refactorings__Index...</a><p>I donât see any reason it couldnât do more aggressive refactors with LLMs and either correct itself or donât do the refactor if it fails static code checking.  Visual Studio can already do  real time type checking for compile time errors</div><br/></div></div></div></div><div id="42618789" class="c"><input type="checkbox" id="c-42618789" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#42618732">parent</a><span>|</span><a href="#42619107">prev</a><span>|</span><a href="#42619477">next</a><span>|</span><label class="collapse" for="c-42618789">[-]</label><label class="expand" for="c-42618789">[2 more]</label></div><br/><div class="children"><div class="content">Is there a model you prefer to use?</div><br/><div id="42619228" class="c"><input type="checkbox" id="c-42619228" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#42618732">root</a><span>|</span><a href="#42618789">parent</a><span>|</span><a href="#42619477">next</a><span>|</span><label class="collapse" for="c-42619228">[-]</label><label class="expand" for="c-42619228">[1 more]</label></div><br/><div class="children"><div class="content">Not wdutch but Claude Sonnet is one of the best models out there for programming, o1 is sometimes better but costs more</div><br/></div></div></div></div></div></div><div id="42619477" class="c"><input type="checkbox" id="c-42619477" checked=""/><div class="controls bullet"><span class="by">Ozzie_osman</span><span>|</span><a href="#42618732">prev</a><span>|</span><a href="#42618781">next</a><span>|</span><label class="collapse" for="c-42619477">[-]</label><label class="expand" for="c-42619477">[3 more]</label></div><br/><div class="children"><div class="content">One mode I felt was missed was &quot;thought partner&quot;, especially while debugging (aka rubber ducking).<p>We had an issue recently with a task queue seemingly randomly stalling. We were able to arrive at the root cause much more quickly than we would have because of a back-and-forth brainstorming session with Claude, which involved describing the issue we were seeing, pasting in code from library to ask questions, asking it to write some code to add some missing telemetry, and then probing it for ideas on what might be going wrong. An issue that may have taken days to debug took about an hour to identify.<p>Think of it as rubber ducking with a very strong generalist engineer who knows about basically any technical concepts.</div><br/><div id="42620058" class="c"><input type="checkbox" id="c-42620058" checked=""/><div class="controls bullet"><span class="by">vendiddy</span><span>|</span><a href="#42619477">parent</a><span>|</span><a href="#42619554">next</a><span>|</span><label class="collapse" for="c-42620058">[-]</label><label class="expand" for="c-42620058">[1 more]</label></div><br/><div class="children"><div class="content">I found myself doing this with o1 recently for software architecture.<p>I will evaluate design ideas with the model, express concerns on trade-offs, ask for alternative ideas, etc.<p>Some of the benefit is having someone to talk to, but with proper framing it is surprisingly good at giving balanced takes.</div><br/></div></div><div id="42619554" class="c"><input type="checkbox" id="c-42619554" checked=""/><div class="controls bullet"><span class="by">mmahemoff</span><span>|</span><a href="#42619477">parent</a><span>|</span><a href="#42620058">prev</a><span>|</span><a href="#42618781">next</a><span>|</span><label class="collapse" for="c-42619554">[-]</label><label class="expand" for="c-42619554">[1 more]</label></div><br/><div class="children"><div class="content">The new video and screen-share capabilities in ChatGPT and Gemini should make rubber-ducking smoother.<p>I feel like I&#x27;ve worn out my computerâs clipboard and alt-tab keys at this stage of the LLM experience.</div><br/></div></div></div></div><div id="42618781" class="c"><input type="checkbox" id="c-42618781" checked=""/><div class="controls bullet"><span class="by">nemothekid</span><span>|</span><a href="#42619477">prev</a><span>|</span><a href="#42618649">next</a><span>|</span><label class="collapse" for="c-42618781">[-]</label><label class="expand" for="c-42618781">[6 more]</label></div><br/><div class="children"><div class="content">I think &quot;Chat driven programming&quot; is the most common type of the most hyped LLM-based programming I see on twitter that I just can&#x27;t relate to. I&#x27;ve incorporated LLMs mainly as auto-complete and search; asking ChatGPT to write a quick script or to scaffold some code for which the documentation is too esoteric to parse.<p>But having the LLM do things for me, I frequently run into issues where it feels like I&#x27;m wasting my time with an intern. &quot;<i>Chat-based LLMs do best with exam-style questions</i>&quot; really speaks to me, however I find that constructing my prompts in such a way where the LLM does what I want uses just as much brainpower as just programming the thing my self.<p>I do find ChatGPT (o1 especially) really good at optimizing existing code.</div><br/><div id="42620629" class="c"><input type="checkbox" id="c-42620629" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#42618781">parent</a><span>|</span><a href="#42619050">next</a><span>|</span><label class="collapse" for="c-42620629">[-]</label><label class="expand" for="c-42620629">[1 more]</label></div><br/><div class="children"><div class="content">Iâve found that everything just works (more or less) since switching to Cursor. Agent based composer mode is magical. Just give it a few files for context, and ask it to do what you want.</div><br/></div></div><div id="42619050" class="c"><input type="checkbox" id="c-42619050" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#42618781">parent</a><span>|</span><a href="#42620629">prev</a><span>|</span><a href="#42619186">next</a><span>|</span><label class="collapse" for="c-42619050">[-]</label><label class="expand" for="c-42619050">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s an art to cost-effectively coaxing useful answers (useful drafts of code) from an LLM, and there&#x27;s an art to noticing the most productive questions to put to that process. It&#x27;s a totally different way of programming than having an LLM looking over your shoulder while you direct, function by function, type by type, the code you&#x27;re designing.<p>If you feel like you&#x27;re wasting your time, my bet is that you&#x27;re either picking problems where there isn&#x27;t enough value to negotiate with the LLM, or your expectations are too high. Crawshaw mentions this in his post: a lot of the value of this chat-driven style is that it very quickly gets you unstuck on a problem. Once you get to that point, you take over! You don&#x27;t convince the LLM to build the final version you actually commit to your branch.<p>Generating unit test cases --- in particular, generating unit test cases that reconcile against unsophisticated, brute-force, easily-validated reference implementations of algorithms --- are a perfect example of where that cost&#x2F;benefit can come out nicely.</div><br/></div></div><div id="42619186" class="c"><input type="checkbox" id="c-42619186" checked=""/><div class="controls bullet"><span class="by">sibeliuss</span><span>|</span><a href="#42618781">parent</a><span>|</span><a href="#42619050">prev</a><span>|</span><a href="#42618838">next</a><span>|</span><label class="collapse" for="c-42619186">[-]</label><label class="expand" for="c-42619186">[1 more]</label></div><br/><div class="children"><div class="content">My technique is to feed it a series of intro questions that prepare it for the final task. Chat the thing into a proper comfort level, and then from there, with the context at hand, ask to help solve the real problem. Def feels like a new kind of programming model because its still very programming-esque.</div><br/></div></div><div id="42618838" class="c"><input type="checkbox" id="c-42618838" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#42618781">parent</a><span>|</span><a href="#42619186">prev</a><span>|</span><a href="#42619018">next</a><span>|</span><label class="collapse" for="c-42618838">[-]</label><label class="expand" for="c-42618838">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; &quot;Chat-based LLMs do best with exam-style questions&quot; really speaks to me, however I find that constructing my prompts in such a way where the LLM does what I want uses just as much brainpower as just programming the thing my self.</i><p>It speaks to me too because my mechanical writing style (as opposed to creative prose) could best be described as what I learned in high school AP English&#x2F;Literature and the rest of the California education system. For whatever reason that writing style dominated the training data and LLMs just happens to be easy to use because I came out of the same education system as many of the people working at OpenAI&#x2F;Anthropic.<p>Iâve had to stop using several generic turns of phrase like âin conclusionâ because it made my writing look too much like ChatGPT.</div><br/></div></div><div id="42619018" class="c"><input type="checkbox" id="c-42619018" checked=""/><div class="controls bullet"><span class="by">AlotOfReading</span><span>|</span><a href="#42618781">parent</a><span>|</span><a href="#42618838">prev</a><span>|</span><a href="#42618649">next</a><span>|</span><label class="collapse" for="c-42619018">[-]</label><label class="expand" for="c-42619018">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s interesting that you find it useful for optimization. I&#x27;ve found that they&#x27;re barely capable of anything more than shallow optimization in my stuff without significant direction.<p>What I find useful is that I can keep thinking at one abstraction level without hopping back and forth between algorithm and codegen. The chat is also a written artifact I can use the faster language parts of my brain on instead of the slower abstract thought parts.</div><br/></div></div></div></div><div id="42618649" class="c"><input type="checkbox" id="c-42618649" checked=""/><div class="controls bullet"><span class="by">bangaladore</span><span>|</span><a href="#42618781">prev</a><span>|</span><a href="#42620361">next</a><span>|</span><label class="collapse" for="c-42618649">[-]</label><label class="expand" for="c-42618649">[7 more]</label></div><br/><div class="children"><div class="content">The killer feature about LLMs with programming in my opinion is autocomplete (the simple copilot feature). I can probably be 2-3x more productive as I&#x27;m not typing (or thinking much). It does a fairly good job pulling in nearby context to help it. And that&#x27;s even without a language server.<p>Using it to generate blocks of code in a chat like manner in my opinion just never works well enough in the domains I use it on. I&#x27;ll try to get it to generate something and then realize when I get some functional result I could&#x27;ve done it faster and more effectively.<p>Funny enough, other commenters here hate autocomplete but love chat.</div><br/><div id="42618674" class="c"><input type="checkbox" id="c-42618674" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#42618649">parent</a><span>|</span><a href="#42618721">next</a><span>|</span><label class="collapse" for="c-42618674">[-]</label><label class="expand" for="c-42618674">[4 more]</label></div><br/><div class="children"><div class="content">The autocomplete is mostly a nusance and maybe low percentage of the time it does right.</div><br/><div id="42619383" class="c"><input type="checkbox" id="c-42619383" checked=""/><div class="controls bullet"><span class="by">LVB</span><span>|</span><a href="#42618649">root</a><span>|</span><a href="#42618674">parent</a><span>|</span><a href="#42619058">next</a><span>|</span><label class="collapse" for="c-42619383">[-]</label><label class="expand" for="c-42619383">[2 more]</label></div><br/><div class="children"><div class="content">The biggest nuisance aspect for me is when it is trying to do things that the LSP can do 100% correctly. Almost surely it is my tooling setup and the LLM is squashing LSP stuff. Seeing Copilot (or even Cursor) suggesting methods or parameters that don&#x27;t exist is really annoying. Just stand down and let the LSP answer those basic questions, TYVM.</div><br/><div id="42619814" class="c"><input type="checkbox" id="c-42619814" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#42618649">root</a><span>|</span><a href="#42619383">parent</a><span>|</span><a href="#42619058">next</a><span>|</span><label class="collapse" for="c-42619814">[-]</label><label class="expand" for="c-42619814">[1 more]</label></div><br/><div class="children"><div class="content">Cursor ostensibly has a config setting to run a âshadowâ workspace [1], aka a headless copy of the window youâre working in to get feedback from linters and LSPs but theyâve been iterating so fast Iâm not sure itâs still working (or ever did much, really).<p>It really feels like weâre at the ARPANET stage where thereâs so much obvious hanging fruit, itâs just going to take companies a while to perfect it.<p>[1] <a href="https:&#x2F;&#x2F;www.cursor.com&#x2F;blog&#x2F;shadow-workspace" rel="nofollow">https:&#x2F;&#x2F;www.cursor.com&#x2F;blog&#x2F;shadow-workspace</a></div><br/></div></div></div></div><div id="42619058" class="c"><input type="checkbox" id="c-42619058" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#42618649">root</a><span>|</span><a href="#42618674">parent</a><span>|</span><a href="#42619383">prev</a><span>|</span><a href="#42618721">next</a><span>|</span><label class="collapse" for="c-42619058">[-]</label><label class="expand" for="c-42619058">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I don&#x27;t like it either. I think it speaks to the mindset difference Crawshaw is talking about here. When I&#x27;m writing code, I don&#x27;t want things getting in my way. I have a plan. I&#x27;m actually pretty Zen about all the typing. It&#x27;s part of my flow-state. But when I&#x27;m exploring code in a dialog with a chatbot, I&#x27;m happy for the help.</div><br/></div></div></div></div><div id="42618721" class="c"><input type="checkbox" id="c-42618721" checked=""/><div class="controls bullet"><span class="by">LeftHandPath</span><span>|</span><a href="#42618649">parent</a><span>|</span><a href="#42618674">prev</a><span>|</span><a href="#42620361">next</a><span>|</span><label class="collapse" for="c-42618721">[-]</label><label class="expand" for="c-42618721">[2 more]</label></div><br/><div class="children"><div class="content">Iâve never used it, simply because I hate autocomplete in emails.<p>Gmail autocomplete saves me <i>maybe</i> 2-5s per email: the recipients name, a comma, and a sign off. Maybe a quarter or half sentence here or there, but never exactly what I wouldâve typed.<p>In code bases, Iâve never seen the appeal. Itâs only reliably good at stuff that I can easily find on Google. The savings are inconsequential at best, and negative at worst when it introduces hard-to-pinpoint bugs.<p>LLMS are incredible technology, but when applied to code, they act more like non-deterministic macros.</div><br/></div></div></div></div><div id="42620361" class="c"><input type="checkbox" id="c-42620361" checked=""/><div class="controls bullet"><span class="by">bambax</span><span>|</span><a href="#42618649">prev</a><span>|</span><a href="#42620471">next</a><span>|</span><label class="collapse" for="c-42620361">[-]</label><label class="expand" for="c-42620361">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>There are three ways I use LLMs in my day-to-day programming: 1&#x2F; Autocomplete 2&#x2F; Search 3&#x2F; Chat-driven programming</i><p>I do mostly 2&#x2F; Search, which is like a personalized Stack Overflow and sometimes feels incredible. You can ask a general question about a specific problem and then dive into some specific point to make sure you understand every part clearly. This works best for things one doesn&#x27;t know enough about, but has a general idea of how the solution should sound or what it should do. Or, copy-pasting error messages from tools like Docker and have the LLM debug it for you really feels like magic.<p>For some reason I have always disliked autocomplete anywhere, so I don&#x27;t do that.<p>The third way, chat-driven programming, is more difficult, because the code generated by LLMs can be large, and can also be wrong. LLMs are too eager to help, and they will try to find a solution even if there isn&#x27;t one, and will invent it if necessary. Telling them in the prompt to say &quot;I don&#x27;t know&quot; or &quot;it&#x27;s impossible&quot; if need be, can help.<p>But, like the author says, it&#x27;s very helpful to get started on something.<p>&gt; <i>That is why I still use an LLM via a web browser, because I want a blank slate on which to craft a well-contained request</i><p>That&#x27;s also what I do. I wouldn&#x27;t like having something in the IDE trying to second guess what I write or suddenly absorbing everything into context and coming up with answers that it thinks make a lot of sense but actually don&#x27;t.<p>But the main benefit is, like the author says, that it lets one start afresh with every new question or problem, and save focused threads on specific topics.</div><br/></div></div><div id="42620471" class="c"><input type="checkbox" id="c-42620471" checked=""/><div class="controls bullet"><span class="by">choeger</span><span>|</span><a href="#42620361">prev</a><span>|</span><a href="#42619699">next</a><span>|</span><label class="collapse" for="c-42620471">[-]</label><label class="expand" for="c-42620471">[1 more]</label></div><br/><div class="children"><div class="content">Essentially, an LLM is a compressed database with a universal translator.<p>So what we can get out of it is everything that has been written (and publicly released) before translated to any language it knows about.<p>This has some consequences.<p>1. Programmers still need to know what algorithms or interfaces or models they want.<p>2. Programmers do not have to know a language very well anymore, to write code, but the have to for bug fixing. Consequently the rift between garbage software and quality software will grow.<p>3. New programming languages will face a big economical hurdle to take off.</div><br/></div></div><div id="42619699" class="c"><input type="checkbox" id="c-42619699" checked=""/><div class="controls bullet"><span class="by">ripped_britches</span><span>|</span><a href="#42620471">prev</a><span>|</span><a href="#42619446">next</a><span>|</span><label class="collapse" for="c-42619699">[-]</label><label class="expand" for="c-42619699">[1 more]</label></div><br/><div class="children"><div class="content">Iâll say that the payoff for investing the time to learn how to do this right is huge. Especially with cursor which allows me to easily chat around context (docs, library files, etc)</div><br/></div></div><div id="42619446" class="c"><input type="checkbox" id="c-42619446" checked=""/><div class="controls bullet"><span class="by">rafaelmn</span><span>|</span><a href="#42619699">prev</a><span>|</span><a href="#42618464">next</a><span>|</span><label class="collapse" for="c-42619446">[-]</label><label class="expand" for="c-42619446">[3 more]</label></div><br/><div class="children"><div class="content">I disagree about search. While LLM can give you an answer faster, good doc (eg. MDN article in CSS example) will :<p>- be way more reliable<p>- probably be up to date on how you should solve it in latest&#x2F;recommend approach<p>- put you in a place where you can search for adjecent tech<p>LLM with search has potential but I&#x27;d like if current tools are more oriented on source material rather than AI paraphrasing.</div><br/><div id="42619480" class="c"><input type="checkbox" id="c-42619480" checked=""/><div class="controls bullet"><span class="by">cruffle_duffle</span><span>|</span><a href="#42619446">parent</a><span>|</span><a href="#42618464">next</a><span>|</span><label class="collapse" for="c-42619480">[-]</label><label class="expand" for="c-42619480">[2 more]</label></div><br/><div class="children"><div class="content">One of my tricks is to paste the docs right into the context so the model canât fuck it up.<p>Though I still wonder if that means Iâm only tricking myself into thinking the LLM is increasing my productivity.</div><br/><div id="42619492" class="c"><input type="checkbox" id="c-42619492" checked=""/><div class="controls bullet"><span class="by">rafaelmn</span><span>|</span><a href="#42619446">root</a><span>|</span><a href="#42619480">parent</a><span>|</span><a href="#42618464">next</a><span>|</span><label class="collapse" for="c-42619492">[-]</label><label class="expand" for="c-42619492">[1 more]</label></div><br/><div class="children"><div class="content">I likr this approach. Read the docs, figure out what you want, get LLM to do the grunt work with all relevant context and review.</div><br/></div></div></div></div></div></div><div id="42618464" class="c"><input type="checkbox" id="c-42618464" checked=""/><div class="controls bullet"><span class="by">notjoemama</span><span>|</span><a href="#42619446">prev</a><span>|</span><a href="#42620375">next</a><span>|</span><label class="collapse" for="c-42618464">[-]</label><label class="expand" for="c-42618464">[17 more]</label></div><br/><div class="children"><div class="content">Our company has a no AI use policy. The assumption is zero trust. We simply canât know whether a model or its framework could or would send proprietary code outside the network. So itâs best to assume all LLMs&#x2F;AI is or will send code or fragments of code. While I applaud the incredible work by their creators, Iâm not sure how a responsible enterprise class company could rely on âtrust us broâ EULAs or repo readmes.</div><br/><div id="42620646" class="c"><input type="checkbox" id="c-42620646" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#42618464">parent</a><span>|</span><a href="#42618495">next</a><span>|</span><label class="collapse" for="c-42620646">[-]</label><label class="expand" for="c-42620646">[1 more]</label></div><br/><div class="children"><div class="content">I mean, we host our code on Github. What are they going to do with Copilot code snippets?</div><br/></div></div><div id="42618495" class="c"><input type="checkbox" id="c-42618495" checked=""/><div class="controls bullet"><span class="by">codebje</span><span>|</span><a href="#42618464">parent</a><span>|</span><a href="#42620646">prev</a><span>|</span><a href="#42619113">next</a><span>|</span><label class="collapse" for="c-42618495">[-]</label><label class="expand" for="c-42618495">[1 more]</label></div><br/><div class="children"><div class="content">The same way responsible enterprise class companies rely on &quot;trust us bro&quot; EULAs for financial systems, customer databases, payroll, and all the other systems it would be very expensive and error prone to build custom for every business.</div><br/></div></div><div id="42619113" class="c"><input type="checkbox" id="c-42619113" checked=""/><div class="controls bullet"><span class="by">BBosco</span><span>|</span><a href="#42618464">parent</a><span>|</span><a href="#42618495">prev</a><span>|</span><a href="#42618566">next</a><span>|</span><label class="collapse" for="c-42619113">[-]</label><label class="expand" for="c-42619113">[2 more]</label></div><br/><div class="children"><div class="content">The vast majority of fortune 500âs have legal frameworks up for dealing with internal AI use already because the reality is employees are going to use it regardless of internal policy. Assuming every employee will act in good faith just because a blanket AI ban is in place is extremely optimistic at best, and isnât a good substitute for actual understanding.</div><br/><div id="42619237" class="c"><input type="checkbox" id="c-42619237" checked=""/><div class="controls bullet"><span class="by">sulam</span><span>|</span><a href="#42618464">root</a><span>|</span><a href="#42619113">parent</a><span>|</span><a href="#42618566">next</a><span>|</span><label class="collapse" for="c-42619237">[-]</label><label class="expand" for="c-42619237">[1 more]</label></div><br/><div class="children"><div class="content">Internal policies at these companies are rarely subject to a level of faith that you&#x27;re implying. Instead external access to systems is logged, internal systems are often sandboxed or otherwise constrained in how you interact with them, and anything that looks like exfiltration sets off enough alarms to have your manager talking to you that same day, if not that same hour.</div><br/></div></div></div></div><div id="42618566" class="c"><input type="checkbox" id="c-42618566" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#42618464">parent</a><span>|</span><a href="#42619113">prev</a><span>|</span><a href="#42618862">next</a><span>|</span><label class="collapse" for="c-42618566">[-]</label><label class="expand" for="c-42618566">[6 more]</label></div><br/><div class="children"><div class="content">Your company could locally host LLMs; you wont get chatGPT or Claude quality, but you can get something that would have been SOTA a year ago. You can vet the public inference codebases (they are only of moderate complexity), and you control your own firewalls.</div><br/><div id="42619028" class="c"><input type="checkbox" id="c-42619028" checked=""/><div class="controls bullet"><span class="by">Kostchei</span><span>|</span><a href="#42618464">root</a><span>|</span><a href="#42618566">parent</a><span>|</span><a href="#42618994">next</a><span>|</span><label class="collapse" for="c-42619028">[-]</label><label class="expand" for="c-42619028">[1 more]</label></div><br/><div class="children"><div class="content">You can get standalone&#x2F;isolated versions of chatGPT, if your org is large enough, in partnership with OpenAI. And others. They run on the same infra but in accounts you set up, cost the same, but you have visibility on the compute, and control of data exfil - ie is there is none.</div><br/></div></div><div id="42618985" class="c"><input type="checkbox" id="c-42618985" checked=""/><div class="controls bullet"><span class="by">CubsFan1060</span><span>|</span><a href="#42618464">root</a><span>|</span><a href="#42618566">parent</a><span>|</span><a href="#42618994">prev</a><span>|</span><a href="#42618862">next</a><span>|</span><label class="collapse" for="c-42618985">[-]</label><label class="expand" for="c-42618985">[3 more]</label></div><br/><div class="children"><div class="content">You can run Claude on both AWS and Google Cloud.  Iâm fairly certain they donât share data, but would need to verify to be sure.</div><br/><div id="42619331" class="c"><input type="checkbox" id="c-42619331" checked=""/><div class="controls bullet"><span class="by">evilduck</span><span>|</span><a href="#42618464">root</a><span>|</span><a href="#42618985">parent</a><span>|</span><a href="#42618862">next</a><span>|</span><label class="collapse" for="c-42619331">[-]</label><label class="expand" for="c-42619331">[2 more]</label></div><br/><div class="children"><div class="content">You can also run Llama 405B and the latest (huge) DeepSeek on your own hardware and get LLMs that trade blows with Claude and ChatGPT, while being fully isolated and offline if needed.</div><br/><div id="42619897" class="c"><input type="checkbox" id="c-42619897" checked=""/><div class="controls bullet"><span class="by">krembo</span><span>|</span><a href="#42618464">root</a><span>|</span><a href="#42619331">parent</a><span>|</span><a href="#42618862">next</a><span>|</span><label class="collapse" for="c-42619897">[-]</label><label class="expand" for="c-42619897">[1 more]</label></div><br/><div class="children"><div class="content">With Amazon Bedrock you can get an isolated serverless Claude or llama with a few clicks</div><br/></div></div></div></div></div></div></div></div><div id="42618862" class="c"><input type="checkbox" id="c-42618862" checked=""/><div class="controls bullet"><span class="by">attentive</span><span>|</span><a href="#42618464">parent</a><span>|</span><a href="#42618566">prev</a><span>|</span><a href="#42619573">next</a><span>|</span><label class="collapse" for="c-42618862">[-]</label><label class="expand" for="c-42618862">[1 more]</label></div><br/><div class="children"><div class="content">So, you&#x27;re asking how enterprise class companies are using github for repos and gmail for all the enterprise mail? What&#x27;s next, zoom&#x2F;teams for meetings?</div><br/></div></div><div id="42619573" class="c"><input type="checkbox" id="c-42619573" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#42618464">parent</a><span>|</span><a href="#42618862">prev</a><span>|</span><a href="#42618979">next</a><span>|</span><label class="collapse" for="c-42619573">[-]</label><label class="expand" for="c-42619573">[1 more]</label></div><br/><div class="children"><div class="content">You can run pretty decent models on your laptop these days. Works in airplane mode.<p><a href="https:&#x2F;&#x2F;ollama.com&#x2F;">https:&#x2F;&#x2F;ollama.com&#x2F;</a></div><br/></div></div><div id="42618979" class="c"><input type="checkbox" id="c-42618979" checked=""/><div class="controls bullet"><span class="by">lazybreather</span><span>|</span><a href="#42618464">parent</a><span>|</span><a href="#42619573">prev</a><span>|</span><a href="#42620291">next</a><span>|</span><label class="collapse" for="c-42618979">[-]</label><label class="expand" for="c-42618979">[1 more]</label></div><br/><div class="children"><div class="content">Palo Alto networks provides security product &quot;AI access security&quot; which claims to solve the problem you mentioned - access control, data protection etc. I don&#x27;t personally use it neither does my org. Giving here just in case it is useful for someone.</div><br/></div></div><div id="42620291" class="c"><input type="checkbox" id="c-42620291" checked=""/><div class="controls bullet"><span class="by">golergka</span><span>|</span><a href="#42618464">parent</a><span>|</span><a href="#42618979">prev</a><span>|</span><a href="#42619321">next</a><span>|</span><label class="collapse" for="c-42620291">[-]</label><label class="expand" for="c-42620291">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the realistic attack scenario? Will Sam Altman steal your company&#x27;s code? Or will next version of GPT learn on your secret sauce algorithms and then your competitors will get them when they generate code for their tasks and your company loses its competitive advantage?<p>I&#x27;m actually sure that there are companies for which these scenarios are very real. But I don&#x27;t think there&#x27;s a lot of them. Most of the code our industry works on has very little value outside of context of particular product and company.</div><br/></div></div><div id="42619321" class="c"><input type="checkbox" id="c-42619321" checked=""/><div class="controls bullet"><span class="by">Pyxl101</span><span>|</span><a href="#42618464">parent</a><span>|</span><a href="#42620291">prev</a><span>|</span><a href="#42618802">next</a><span>|</span><label class="collapse" for="c-42619321">[-]</label><label class="expand" for="c-42619321">[1 more]</label></div><br/><div class="children"><div class="content">Just curious, how does your company host its email? Documents? Files?</div><br/></div></div><div id="42618802" class="c"><input type="checkbox" id="c-42618802" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#42618464">parent</a><span>|</span><a href="#42619321">prev</a><span>|</span><a href="#42620375">next</a><span>|</span><label class="collapse" for="c-42618802">[-]</label><label class="expand" for="c-42618802">[1 more]</label></div><br/><div class="children"><div class="content">Local LLMs for code aren&#x27;t that out of the question to run.<p>Even for not code generation, but even smaller models only for programming to weigh on different design approaches, etc.</div><br/></div></div></div></div><div id="42620375" class="c"><input type="checkbox" id="c-42620375" checked=""/><div class="controls bullet"><span class="by">polotics</span><span>|</span><a href="#42618464">prev</a><span>|</span><a href="#42618394">next</a><span>|</span><label class="collapse" for="c-42620375">[-]</label><label class="expand" for="c-42620375">[1 more]</label></div><br/><div class="children"><div class="content">My main usage is in helping me approach domains and tools I don&#x27;t know enough to confidently know how best to get started.<p>So one thing that doesn&#x27;t get a mention in the article but is quite significant I think is the long lag of knowledge cutoff dates: looking at even the latest and greatest, there is one year or more of missing information.<p>I would love for someone more versed than me to tell us how best to use RAG or LoRA to get the model to answer with fully up to date knowledge on libraries, frameworks, ...</div><br/></div></div><div id="42618394" class="c"><input type="checkbox" id="c-42618394" checked=""/><div class="controls bullet"><span class="by">justatdotin</span><span>|</span><a href="#42620375">prev</a><span>|</span><a href="#42620155">next</a><span>|</span><label class="collapse" for="c-42618394">[-]</label><label class="expand" for="c-42618394">[2 more]</label></div><br/><div class="children"><div class="content">lots of colleauges using copilot or whatever for autocomplete -
I just find that annoying.<p>or writing tests -
that&#x27;s ... not so helpful. worst is when a lazy dev takes the generated tests and leaves it at that: usually just a few placeholders that test the happy path but ignore obvious corner cases. (I suppose for API tests that comes down to adding test case parameters)<p>but chatting about a large codebase, I&#x27;ve been amazed at how helpful it can be.<p>what software patterns can you see in this repo?
how does the implementation compare to others in the organisation?
what common features of the pattern are missing?<p>also, like a linter on steroids, chat can help explore how my project might be refactored to better match the organisation&#x27;s coding style.</div><br/><div id="42618446" class="c"><input type="checkbox" id="c-42618446" checked=""/><div class="controls bullet"><span class="by">roskilli</span><span>|</span><a href="#42618394">parent</a><span>|</span><a href="#42620155">next</a><span>|</span><label class="collapse" for="c-42618446">[-]</label><label class="expand" for="c-42618446">[1 more]</label></div><br/><div class="children"><div class="content">If you donât mind me asking: which popular LLM(s) have you been using for this and how are you providing the code base into the context window?</div><br/></div></div></div></div><div id="42620155" class="c"><input type="checkbox" id="c-42620155" checked=""/><div class="controls bullet"><span class="by">brabel</span><span>|</span><a href="#42618394">prev</a><span>|</span><a href="#42620213">next</a><span>|</span><label class="collapse" for="c-42620155">[-]</label><label class="expand" for="c-42620155">[1 more]</label></div><br/><div class="children"><div class="content">What the author is asking about, a quick sketchpad where you can try out code quickly and chat with the AI, already exists in the JetBrains IDEs. It&#x27;s called a scratch file[1].<p>As far as I know, the idea of a scratch &quot;buffer&quot; comes from emacs. But in Jetbrains IDEs, you have the full IDE support even with context from your current project (you can pick the &quot;modules&quot; you want to have in context). Given the good integration with LLMs, that&#x27;s basically what the author seems to want. Perhaps give GoLand[2] a try.<p>Disclosure: no, I don&#x27;t work for Jetbrains :D just a very happy customer.<p>[1] <a href="https:&#x2F;&#x2F;www.jetbrains.com&#x2F;help&#x2F;idea&#x2F;scratches.html" rel="nofollow">https:&#x2F;&#x2F;www.jetbrains.com&#x2F;help&#x2F;idea&#x2F;scratches.html</a><p>[2] <a href="https:&#x2F;&#x2F;www.jetbrains.com&#x2F;go&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.jetbrains.com&#x2F;go&#x2F;</a></div><br/></div></div><div id="42620213" class="c"><input type="checkbox" id="c-42620213" checked=""/><div class="controls bullet"><span class="by">justinl33</span><span>|</span><a href="#42620155">prev</a><span>|</span><a href="#42618436">next</a><span>|</span><label class="collapse" for="c-42620213">[-]</label><label class="expand" for="c-42620213">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve maintained several SDKs, and the &#x27;cover everything&#x27; approach leads to nightmare dependency trees and documentation bloat. imo, the LLM paradigm shifts this even further - why maintain a massive SDK when users can generate precisely what they need? This could fundamentally change how we think about API distribution.</div><br/></div></div><div id="42618436" class="c"><input type="checkbox" id="c-42618436" checked=""/><div class="controls bullet"><span class="by">wrs</span><span>|</span><a href="#42620213">prev</a><span>|</span><a href="#42620285">next</a><span>|</span><label class="collapse" for="c-42618436">[-]</label><label class="expand" for="c-42618436">[3 more]</label></div><br/><div class="children"><div class="content">Iâve been working with Cursorâs agent mode a lot this week and am seeing where we need a new kind of tool. Because it sees the whole codebase, the agent will quickly get into a state where itâs changed several files to implement some layering or refactor something. This requires a response from the developer thatâs sort of like a code review, in that you need to see changes and make comments across multiple files, but unlike a code review, itâs not finished code. It probably doesnât compile, big chunks of it are not quite what you want, itâs not structured into coherent changesetsâ¦itâs kind of like you gave the intern the problem and they submitted a bit of a mess. It would be a terrible PR, but itâs a useful intermediate state to take another step from.<p>It feels like the IDE needs a new mode to deal with this state, and that SCM needs to be involved somehow too. Somehow help the developer guide this somewhat flaky stream of edits and sculpt it into a good changeset.</div><br/><div id="42618490" class="c"><input type="checkbox" id="c-42618490" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#42618436">parent</a><span>|</span><a href="#42620285">next</a><span>|</span><label class="collapse" for="c-42618490">[-]</label><label class="expand" for="c-42618490">[2 more]</label></div><br/><div class="children"><div class="content">Aider commits to git with each command, making it easy to back out changes, and also squash them into discrete chunks later (and reorder them with interactive rebase).</div><br/><div id="42620298" class="c"><input type="checkbox" id="c-42620298" checked=""/><div class="controls bullet"><span class="by">golergka</span><span>|</span><a href="#42618436">root</a><span>|</span><a href="#42618490">parent</a><span>|</span><a href="#42620285">next</a><span>|</span><label class="collapse" for="c-42620298">[-]</label><label class="expand" for="c-42620298">[1 more]</label></div><br/><div class="children"><div class="content">Automatically runs linter and tests on every edit and forwards failures back to LLM as well.</div><br/></div></div></div></div></div></div><div id="42620285" class="c"><input type="checkbox" id="c-42620285" checked=""/><div class="controls bullet"><span class="by">lysecret</span><span>|</span><a href="#42618436">prev</a><span>|</span><a href="#42619621">next</a><span>|</span><label class="collapse" for="c-42620285">[-]</label><label class="expand" for="c-42620285">[1 more]</label></div><br/><div class="children"><div class="content">Funny, he starts of dismissing an AI IDE to end with building an AI IDE :D (Smells a little bit like not invented here syndrom) Otherwise fascinating article!</div><br/></div></div><div id="42619621" class="c"><input type="checkbox" id="c-42619621" checked=""/><div class="controls bullet"><span class="by">yawnxyz</span><span>|</span><a href="#42620285">prev</a><span>|</span><a href="#42618961">next</a><span>|</span><label class="collapse" for="c-42619621">[-]</label><label class="expand" for="c-42619621">[9 more]</label></div><br/><div class="children"><div class="content">&gt; I could not go a week without getting frustrated by how much mundane typing I had to do before having a FIM model<p>For those not in-the-know, I just learned today that code autocomplete is actually called &quot;Fill-in-the-Middle&quot; tasks</div><br/><div id="42619665" class="c"><input type="checkbox" id="c-42619665" checked=""/><div class="controls bullet"><span class="by">Guthur</span><span>|</span><a href="#42619621">parent</a><span>|</span><a href="#42618961">next</a><span>|</span><label class="collapse" for="c-42619665">[-]</label><label class="expand" for="c-42619665">[8 more]</label></div><br/><div class="children"><div class="content">Says who? I&#x27;ve been in the industry for nearly 25 years and have heard auto complete throughout but not once have I heard fill in the middle.<p>Stop taking these blogs as oracle&#x27;s of truth, they are not. These AI articles are full of this nonsense, to the point where it would appear to me many responses might just be Nvidia bots or whatever.</div><br/><div id="42619813" class="c"><input type="checkbox" id="c-42619813" checked=""/><div class="controls bullet"><span class="by">sunaookami</span><span>|</span><a href="#42619621">root</a><span>|</span><a href="#42619665">parent</a><span>|</span><a href="#42620158">prev</a><span>|</span><a href="#42618961">next</a><span>|</span><label class="collapse" for="c-42619813">[-]</label><label class="expand" for="c-42619813">[6 more]</label></div><br/><div class="children"><div class="content">&gt;I&#x27;ve been in the industry for nearly 25 years and have heard auto complete throughout but not once have I heard fill in the middle<p>Then you need to look harder. FiM is a common approach for code generation LLMs.<p><a href="https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;efficient-training-of-language-models-to-fill-in-the-middle&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;efficient-training-of-language-mode...</a><p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.14255" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.14255</a><p>This was before ChatGPT&#x27;s release btw.</div><br/><div id="42619996" class="c"><input type="checkbox" id="c-42619996" checked=""/><div class="controls bullet"><span class="by">Guthur</span><span>|</span><a href="#42619621">root</a><span>|</span><a href="#42619813">parent</a><span>|</span><a href="#42618961">next</a><span>|</span><label class="collapse" for="c-42619996">[-]</label><label class="expand" for="c-42619996">[5 more]</label></div><br/><div class="children"><div class="content">Why, what was wrong with code completion, it was perfectly valid before even when including some sort of fuzzing.<p>It&#x27;s like everything to do with LLM marketing buzzword nonsense.<p>I really want to just drop out of tech until all this obnoxious hype BS is gone.</div><br/><div id="42620179" class="c"><input type="checkbox" id="c-42620179" checked=""/><div class="controls bullet"><span class="by">ascorbic</span><span>|</span><a href="#42619621">root</a><span>|</span><a href="#42619996">parent</a><span>|</span><a href="#42620236">next</a><span>|</span><label class="collapse" for="c-42620179">[-]</label><label class="expand" for="c-42620179">[1 more]</label></div><br/><div class="children"><div class="content">Autocomplete is the feature, fill in the middle is one approach to implementing it. There are other ways to providing it (which were used in earlier versions of Copilot) and FIM can be used for tasks other than code completion.</div><br/></div></div><div id="42620236" class="c"><input type="checkbox" id="c-42620236" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#42619621">root</a><span>|</span><a href="#42619996">parent</a><span>|</span><a href="#42620179">prev</a><span>|</span><a href="#42618961">next</a><span>|</span><label class="collapse" for="c-42620236">[-]</label><label class="expand" for="c-42620236">[3 more]</label></div><br/><div class="children"><div class="content">Itâs just a term that signals âcompletion in betweenâ rather than âafterâ. Regular code completion usually doesnât take the following blocks into account mostly because these are grammatically vague due to an ongoing edit.<p>Your comments may be sympathised to, but why on earth are they addressed to the root commenter. They simply shared their findings about an acronym.</div><br/><div id="42620547" class="c"><input type="checkbox" id="c-42620547" checked=""/><div class="controls bullet"><span class="by">Guthur</span><span>|</span><a href="#42619621">root</a><span>|</span><a href="#42620236">parent</a><span>|</span><a href="#42618961">next</a><span>|</span><label class="collapse" for="c-42620547">[-]</label><label class="expand" for="c-42620547">[2 more]</label></div><br/><div class="children"><div class="content">Because they mentioned it, why on earth would you think that is not a valid response in a thread that mentions it, from my observation that&#x27;s pretty much how forum like threads work.<p>More pressingly why do you think you should police it?</div><br/><div id="42620653" class="c"><input type="checkbox" id="c-42620653" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#42619621">root</a><span>|</span><a href="#42620547">parent</a><span>|</span><a href="#42618961">next</a><span>|</span><label class="collapse" for="c-42620653">[-]</label><label class="expand" for="c-42620653">[1 more]</label></div><br/><div class="children"><div class="content">[delayed]</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42618961" class="c"><input type="checkbox" id="c-42618961" checked=""/><div class="controls bullet"><span class="by">singpolyma3</span><span>|</span><a href="#42619621">prev</a><span>|</span><a href="#42618980">next</a><span>|</span><label class="collapse" for="c-42618961">[-]</label><label class="expand" for="c-42618961">[5 more]</label></div><br/><div class="children"><div class="content">It seems like everything I see about success using LLMs for this kind of work is for greenfield. What about three weeks later when the job changes to maintenance and interation on something that&#x27;s already working? Are people applying LLMs to that space?</div><br/><div id="42619941" class="c"><input type="checkbox" id="c-42619941" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#42618961">parent</a><span>|</span><a href="#42620435">next</a><span>|</span><label class="collapse" for="c-42619941">[-]</label><label class="expand" for="c-42619941">[1 more]</label></div><br/><div class="children"><div class="content">My codebase is relatively greenfield (started working on it early last year) but itâs up to ~50k lines in a mixed C++&#x2F;Rust codebase with a binding layer whose API predates every LLMâs training sets. Even when I started ChatGPT&#x2F;Claude werenât very useful but now the project requires a completely different strategy when working with LLMs (itâs a QT AI desktop app so Iâm dogfooding a lot). Iâve also used them in a larger codebase (~500k lines) and that also requires a different approach from the former. It feels a lot like the transition from managing 2 to 20 to 200 to 2000 people. Itâs a different ballgame with each step change. A very well encapsulated code base of ~500k lines is manageable for small changes but not for refactoring, exploration, etc, at least until useful context sizes increase another order of magnitude (I keep trying Geminiâs 2M but itâs been a disappointment).<p>I have a <i>lot</i> of documentation aimed at the AI in `docs&#x2F;notes&#x2F;` (some of it written by an LLM but proofread before committing) and I instruct Cursor&#x2F;Windsurf&#x2F;Aider via their respective rules&#x2F;config files to look at the documentation before doing anything. At some scale that initial context becomes just a directory listing &amp; short description of everything in the notes folder, which eventually breaks down due to context size limits, either because I exceed the maximum length of the rules or the agent requires pulling in too much context for the change.<p>Iâve found that thereâs actually an uncanny valley between greenfield projects where the model is free to make whatever assumptions it wants and brownfield projects where itâs possible to provide enough context from the existing codebase to get both API accuracy (hallucinations) and general patterns through few-shot examples. This became very obvious once I had enough examples of that binding layer. Even though I could include all of the documentation for the library, it didnât work consistently until I had a variety of production examples to point it to.<p>Right now, I probably spend as much time writing each prompt as I do massaging the notes folder and rules every time I notice the model doing something wrong.</div><br/></div></div><div id="42620435" class="c"><input type="checkbox" id="c-42620435" checked=""/><div class="controls bullet"><span class="by">Mashimo</span><span>|</span><a href="#42618961">parent</a><span>|</span><a href="#42619941">prev</a><span>|</span><a href="#42620189">next</a><span>|</span><label class="collapse" for="c-42620435">[-]</label><label class="expand" for="c-42620435">[1 more]</label></div><br/><div class="children"><div class="content">I used AI code completion from GitHub copilot on a 20 year old project. You still have to create new classes, new test, refactor etc.</div><br/></div></div><div id="42620189" class="c"><input type="checkbox" id="c-42620189" checked=""/><div class="controls bullet"><span class="by">zkry</span><span>|</span><a href="#42618961">parent</a><span>|</span><a href="#42620435">prev</a><span>|</span><a href="#42618975">next</a><span>|</span><label class="collapse" for="c-42620189">[-]</label><label class="expand" for="c-42620189">[1 more]</label></div><br/><div class="children"><div class="content">Logically this makes sense: every model has a context size and complexity capacity where it will no longer be able to function properly.  Any usage of said model will accelerate the approach to this limit.  Once the limit is reached, the LLM is no longer as helpful as it was.<p>I work on full blown legacy apps and needless to say I don&#x27;t even bother with LLMs when working on these most of the time.</div><br/></div></div><div id="42618975" class="c"><input type="checkbox" id="c-42618975" checked=""/><div class="controls bullet"><span class="by">kylebenzle</span><span>|</span><a href="#42618961">parent</a><span>|</span><a href="#42620189">prev</a><span>|</span><a href="#42618980">next</a><span>|</span><label class="collapse" for="c-42618975">[-]</label><label class="expand" for="c-42618975">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it&#x27;s just harder the larger the pre-existing code base.</div><br/></div></div></div></div><div id="42618980" class="c"><input type="checkbox" id="c-42618980" checked=""/><div class="controls bullet"><span class="by">e12e</span><span>|</span><a href="#42618961">prev</a><span>|</span><a href="#42618573">next</a><span>|</span><label class="collapse" for="c-42618980">[-]</label><label class="expand" for="c-42618980">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. I wonder what the equivalent of sketch.dev would look like if it targeted Smalltalk and was embedded in a Smalltalk image (preferably with a local LLM running in smalltalk)?<p>I&#x27;d love to be able to tell my (hypothetical smalltalk) tablet to create an app for me, and work interactively, interacting with the app as it gets built...<p>Ed: I suppose I should just try and see where cloud ai can take smalltalk today:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;rsbohn&#x2F;Cuis-Smalltalk-Dexter-LLM">https:&#x2F;&#x2F;github.com&#x2F;rsbohn&#x2F;Cuis-Smalltalk-Dexter-LLM</a></div><br/></div></div><div id="42618702" class="c"><input type="checkbox" id="c-42618702" checked=""/><div class="controls bullet"><span class="by">jimmydoe</span><span>|</span><a href="#42618573">prev</a><span>|</span><a href="#42619150">next</a><span>|</span><label class="collapse" for="c-42618702">[-]</label><label class="expand" for="c-42618702">[2 more]</label></div><br/><div class="children"><div class="content">Anyone has good recommendation of LocalLLM for autocompletion<p>Most editors I use supports online LLM but it&#x27;s too slow sometimes for me.</div><br/><div id="42618987" class="c"><input type="checkbox" id="c-42618987" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#42618702">parent</a><span>|</span><a href="#42619150">next</a><span>|</span><label class="collapse" for="c-42618987">[-]</label><label class="expand" for="c-42618987">[1 more]</label></div><br/><div class="children"><div class="content">Unless your network is poor, Iâd imagine (but definitely could be wrong in your case!), the bottleneck is the LLM speed, not the latency to the data center its running in.</div><br/></div></div></div></div><div id="42619150" class="c"><input type="checkbox" id="c-42619150" checked=""/><div class="controls bullet"><span class="by">agentultra</span><span>|</span><a href="#42618702">prev</a><span>|</span><a href="#42620260">next</a><span>|</span><label class="collapse" for="c-42619150">[-]</label><label class="expand" for="c-42619150">[1 more]</label></div><br/><div class="children"><div class="content">It seems nice for small projects but I wouldnât use it for anything serious that I want to maintain long term.<p>I would write the tests first and foremost: they are the specification. Theyâre for future me and other maintainers to understand and I wouldnât want them to be generated: write them with the intention of explaining the module or system to another person. If the code isnât that important Iâll write unit tests. If I need better assurances Iâll write property tests at a minimum.<p>If Iâm working on concurrent or parallel code or Iâm working on designing a distributed system, itâs gotta be a model checker. Iâve verified enough code to know that even a brilliant human cannot find 1-in-a-million programming errors that surface in systems processing millions of transactions a minute. Weâre not wired that way. Fortunately we have formal methods. Maths is an excellent language for specifying problems and managing complexity. Induction, category theory, all awesome stuff.<p>Most importantly thoughâ¦ you have to write the stuff and read it and interact with it to be able to keep it in your head. Programming is theory-building as Naur said.<p>Personally I just donât care to read a bunch of code and play, âspot the error;â a game thatâs rigged for me to be bad at. Itâs much more my speed to write code that obviously has no errors in it because Iâve thought the problem through. Although I struggle with this at times. The struggle is an important part of the process for acquiring new knowledge.<p>Though I do look forward to algorithms that can find proofs of trivial theorems for me. That would be nice to hand offâ¦ although simp does a lot of work like that already. ;)</div><br/></div></div><div id="42620260" class="c"><input type="checkbox" id="c-42620260" checked=""/><div class="controls bullet"><span class="by">golergka</span><span>|</span><a href="#42619150">prev</a><span>|</span><a href="#42619522">next</a><span>|</span><label class="collapse" for="c-42620260">[-]</label><label class="expand" for="c-42620260">[1 more]</label></div><br/><div class="children"><div class="content">I have written a small fullstack app over the holidays, mostly with LLMs, to see how far would they get me. Turns out, they can easily write 90% of the code, but you still need to review everything, make the main architectural decisions and debug stuff when AI cant solve the bug after 2-3 iterations. I get a huge productivity boost and at the same time am not afraid that they will replace me. At least not yet.<p>Can&#x27;t recommend aider enough. I&#x27;ve tried many different coding tools, but they all seem like a leaky abstraction over LLMs medium of sequential text generation. Aider, on the other hand, leans into it in the best possible way.</div><br/></div></div><div id="42619522" class="c"><input type="checkbox" id="c-42619522" checked=""/><div class="controls bullet"><span class="by">simondotau</span><span>|</span><a href="#42620260">prev</a><span>|</span><a href="#42618959">next</a><span>|</span><label class="collapse" for="c-42619522">[-]</label><label class="expand" for="c-42619522">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve recently started using Cursor because it means I can now write python where two weeks ago I couldn&#x27;t write python. It wrote the first pass of an API implementation by feeding it the PDF documentation. I&#x27;ve spent a few days testing and massaging it into a well formed, well structured library, pair-programming style.<p>Then I needed to write a simple command line utility, so I wrote it in Go, even though I&#x27;ve never written Go before. Being able to make tiny standalone executables which do real work is incredible.<p>Now if I ever need to write something, I can choose the language most suited to the task, not the one I happen to have the most experience with.<p>That&#x27;s a superpower.</div><br/><div id="42620587" class="c"><input type="checkbox" id="c-42620587" checked=""/><div class="controls bullet"><span class="by">midasz</span><span>|</span><a href="#42619522">parent</a><span>|</span><a href="#42618959">next</a><span>|</span><label class="collapse" for="c-42620587">[-]</label><label class="expand" for="c-42620587">[1 more]</label></div><br/><div class="children"><div class="content">But you&#x27;re not really writing python right? You&#x27;re instructing a tool to generate python. Kinda like saying I&#x27;m writing bytecode while I&#x27;m actually just typing Java.</div><br/></div></div></div></div><div id="42618959" class="c"><input type="checkbox" id="c-42618959" checked=""/><div class="controls bullet"><span class="by">_boffin_</span><span>|</span><a href="#42619522">prev</a><span>|</span><a href="#42618441">next</a><span>|</span><label class="collapse" for="c-42618959">[-]</label><label class="expand" for="c-42618959">[2 more]</label></div><br/><div class="children"><div class="content">Does anyone know of any good chat based ui builders. No. Not build a chat app.<p>Does webflow have something?<p>My problem is being able to describe what I want in the style I want.</div><br/><div id="42619797" class="c"><input type="checkbox" id="c-42619797" checked=""/><div class="controls bullet"><span class="by">replwoacause</span><span>|</span><a href="#42618959">parent</a><span>|</span><a href="#42618441">next</a><span>|</span><label class="collapse" for="c-42619797">[-]</label><label class="expand" for="c-42619797">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;lovable.dev" rel="nofollow">https:&#x2F;&#x2F;lovable.dev</a><p><a href="https:&#x2F;&#x2F;bolt.new" rel="nofollow">https:&#x2F;&#x2F;bolt.new</a><p><a href="https:&#x2F;&#x2F;v0.dev" rel="nofollow">https:&#x2F;&#x2F;v0.dev</a><p>Never used them myself but have seen them mentioned on Reddit and Twitter.</div><br/></div></div></div></div><div id="42618441" class="c"><input type="checkbox" id="c-42618441" checked=""/><div class="controls bullet"><span class="by">User23</span><span>|</span><a href="#42618959">prev</a><span>|</span><label class="collapse" for="c-42618441">[-]</label><label class="expand" for="c-42618441">[11 more]</label></div><br/><div class="children"><div class="content">LLMs are, at their core, search tools. Training is indexing and prompting is querying that index. The granularity being at the n-gram rather than the document level is a huge deal though.<p>Properly using them requires understanding that. And just like we understand every query wonât find what we want, neither will every prompt. Iterative refinement is virtually required for nontrivial cases. Automating that process, like eg cursor agent, is very promising.</div><br/><div id="42618908" class="c"><input type="checkbox" id="c-42618908" checked=""/><div class="controls bullet"><span class="by">jcranmer</span><span>|</span><a href="#42618441">parent</a><span>|</span><a href="#42618683">next</a><span>|</span><label class="collapse" for="c-42618908">[-]</label><label class="expand" for="c-42618908">[1 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs are, at their core, search tools.<p>Fundamentally, <i>no they&#x27;re not</i>. That is why you have cases like the Air Canada chatbot that told a user about a refund opportunity that didn&#x27;t exist, or the lawyer in Mata v Avianca who cited a case that didn&#x27;t exist. If you ask an LLM to search for something that doesn&#x27;t exist, there&#x27;s a decent chance it will hallucinate something into existence for you.<p>What LLMs are good at is effectively turning fuzzy search terms into non-fuzzy terms; they&#x27;re also pretty good at taking some text and recasting into an extremely formulaic paradigm. In other words, turning unstructured text into something structured. The problem they have is that they don&#x27;t have enough understanding of the world to do something useful that with structured representation that needs to be accurate.</div><br/></div></div><div id="42618683" class="c"><input type="checkbox" id="c-42618683" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42618441">parent</a><span>|</span><a href="#42618908">prev</a><span>|</span><a href="#42618580">next</a><span>|</span><label class="collapse" for="c-42618683">[-]</label><label class="expand" for="c-42618683">[5 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs are, at their core, search tools.<p>This is the wrong take.  Search tools are deterministic unless you purposely inject random weights into the ranking. With search tools, the same search query will always yield the same search result, provided they are designed too and&#x2F;or the underlying data has not changed.<p>With LLMs, I can ask the exact same question and get a different response, even if the data has not changed.</div><br/><div id="42618723" class="c"><input type="checkbox" id="c-42618723" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#42618441">root</a><span>|</span><a href="#42618683">parent</a><span>|</span><a href="#42618580">next</a><span>|</span><label class="collapse" for="c-42618723">[-]</label><label class="expand" for="c-42618723">[4 more]</label></div><br/><div class="children"><div class="content">The randomness comes from sampling. With local LLMs, you can fix the random seed, or even disable sampling all together - both will get you determinism.<p>I agree that LLMs are not search tools, but for very different reasons.</div><br/><div id="42619069" class="c"><input type="checkbox" id="c-42619069" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42618441">root</a><span>|</span><a href="#42618723">parent</a><span>|</span><a href="#42619002">next</a><span>|</span><label class="collapse" for="c-42619069">[-]</label><label class="expand" for="c-42619069">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the info on local LLMs.  Based on my chats with multiple LLMs, the biggest issue appears to be hardware.<p>Non-deterministic hardware: All LLMs mentioned that modern computing hardware, such as GPUs or TPUs, can introduce non-determinism due to factors like parallel processing, caching, or numerical instability. This can make it challenging to achieve determinism, even with fixed random seeds or deterministic algorithms.<p>You can find the summary of my chats <a href="https:&#x2F;&#x2F;beta.gitsense.com&#x2F;?chat=1c3e69f9-7b8b-48a3-8b99-bb1bdf191e60" rel="nofollow">https:&#x2F;&#x2F;beta.gitsense.com&#x2F;?chat=1c3e69f9-7b8b-48a3-8b99-bb1b...</a>. If you scroll to the top and click on the &quot;Conversation&quot; link in the first message, you can read the individual responses.</div><br/></div></div><div id="42619002" class="c"><input type="checkbox" id="c-42619002" checked=""/><div class="controls bullet"><span class="by">klabb3</span><span>|</span><a href="#42618441">root</a><span>|</span><a href="#42618723">parent</a><span>|</span><a href="#42619069">prev</a><span>|</span><a href="#42618580">next</a><span>|</span><label class="collapse" for="c-42619002">[-]</label><label class="expand" for="c-42619002">[2 more]</label></div><br/><div class="children"><div class="content">Semantics. It may be able to get deterministic but itâs <i>unstable</i> wrt unrelated changes in the training data, no? If I add a page about sausages to a search index, the results for âski jacketâ will be unaffected. In a practical sense, LLMs are non-deterministic. I mean, ChatGPT even has a âregenerateâ button to expose this âturbulenceâ as a feature.</div><br/><div id="42619579" class="c"><input type="checkbox" id="c-42619579" checked=""/><div class="controls bullet"><span class="by">User23</span><span>|</span><a href="#42618441">root</a><span>|</span><a href="#42619002">parent</a><span>|</span><a href="#42618580">next</a><span>|</span><label class="collapse" for="c-42619579">[-]</label><label class="expand" for="c-42619579">[1 more]</label></div><br/><div class="children"><div class="content">Hence n-grams rather than documents.<p>Also what&#x27;s with using &quot;semantics&quot; as a dismissal when the technology we&#x27;re talking about is the most semantically relevant search ever made.</div><br/></div></div></div></div></div></div></div></div><div id="42618580" class="c"><input type="checkbox" id="c-42618580" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#42618441">parent</a><span>|</span><a href="#42618683">prev</a><span>|</span><a href="#42618628">next</a><span>|</span><label class="collapse" for="c-42618580">[-]</label><label class="expand" for="c-42618580">[1 more]</label></div><br/><div class="children"><div class="content">Half of the problems are people treating them as searchers when they aren&#x27;t. They&#x27;re absolutely not ngram indexes of existing data, either.</div><br/></div></div><div id="42618628" class="c"><input type="checkbox" id="c-42618628" checked=""/><div class="controls bullet"><span class="by">mvdtnz</span><span>|</span><a href="#42618441">parent</a><span>|</span><a href="#42618580">prev</a><span>|</span><label class="collapse" for="c-42618628">[-]</label><label class="expand" for="c-42618628">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m losing track of the number of different things the Hacker News commenters claim LLMs are &quot;at their core&quot;.</div><br/><div id="42619253" class="c"><input type="checkbox" id="c-42619253" checked=""/><div class="controls bullet"><span class="by">sulam</span><span>|</span><a href="#42618441">root</a><span>|</span><a href="#42618628">parent</a><span>|</span><a href="#42618932">next</a><span>|</span><label class="collapse" for="c-42619253">[-]</label><label class="expand" for="c-42619253">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this answer obvious&#x2F;facile but also true? They&#x27;re next token predictors.</div><br/></div></div><div id="42618932" class="c"><input type="checkbox" id="c-42618932" checked=""/><div class="controls bullet"><span class="by">bitwize</span><span>|</span><a href="#42618441">root</a><span>|</span><a href="#42618628">parent</a><span>|</span><a href="#42619253">prev</a><span>|</span><label class="collapse" for="c-42618932">[-]</label><label class="expand" for="c-42618932">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are, at their core, <i>fucking Dissociated Press</i>. That&#x27;s what makes them fun and interesting, and that&#x27;s the problem with using them for real production work.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>