<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1713258059254" as="style"/><link rel="stylesheet" href="styles.css?v=1713258059254"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2404.07738">ResearchAgent: Iterative Research Idea Generation Using LLMs</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>milliondreams</span> | <span>40 comments</span></div><br/><div><div id="40047654" class="c"><input type="checkbox" id="c-40047654" checked=""/><div class="controls bullet"><span class="by">pedalpete</span><span>|</span><a href="#40047975">next</a><span>|</span><label class="collapse" for="c-40047654">[-]</label><label class="expand" for="c-40047654">[12 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve found where LLMs can be useful in this context is around free-associations. Because they don&#x27;t really &quot;know&quot; about things, they regularly grasp at straws or misconstrue intended meaning. This, along with the volume of language (let&#x27;s not call it knowledge) result in the LLMs occasionally bringing in a new element which  can be useful.</div><br/><div id="40047719" class="c"><input type="checkbox" id="c-40047719" checked=""/><div class="controls bullet"><span class="by">gotts</span><span>|</span><a href="#40047654">parent</a><span>|</span><a href="#40047941">next</a><span>|</span><label class="collapse" for="c-40047719">[-]</label><label class="expand" for="c-40047719">[10 more]</label></div><br/><div class="children"><div class="content">Can you list some examples where free-associations from LLM were useful to you?</div><br/><div id="40049242" class="c"><input type="checkbox" id="c-40049242" checked=""/><div class="controls bullet"><span class="by">pedalpete</span><span>|</span><a href="#40047654">root</a><span>|</span><a href="#40047719">parent</a><span>|</span><a href="#40048890">next</a><span>|</span><label class="collapse" for="c-40049242">[-]</label><label class="expand" for="c-40049242">[1 more]</label></div><br/><div class="children"><div class="content">A lot of where I&#x27;ve benefited is in some marketing language. Rarely, or almost never has ChatGPT come up with something and I&#x27;ve thought &quot;that&#x27;s exactly what we wanted&quot;, but through iterations, it&#x27;s taken me down paths I might not have found myself.<p>Unfortunately, ChatGPT doesn&#x27;t have a good search interface, so I can&#x27;t search through older chats, but I know when I was looking at re-naming our company, it didn&#x27;t come up with our new name, but it lead me down a path which did lead to our name.<p>I was trying to understand a patent, and we were looking at the algorithm which was being used. ChatGPT misunderstood how the algorithm worked, but pointed to it&#x27;s knowledge of a similar algorithm which worked differently, but was better suited to our purposes.<p>Calling this &quot;free-association&quot; may be taking some liberty. Many people would consider these errors, or hallucinations, but in some ways, they do look very similar to what many would call free-association IMO.</div><br/></div></div><div id="40048890" class="c"><input type="checkbox" id="c-40048890" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#40047654">root</a><span>|</span><a href="#40047719">parent</a><span>|</span><a href="#40049242">prev</a><span>|</span><a href="#40047952">next</a><span>|</span><label class="collapse" for="c-40048890">[-]</label><label class="expand" for="c-40048890">[1 more]</label></div><br/><div class="children"><div class="content">Long, long time ago (1999, before LLM&#x27;s) I made a virtual museum exhibit creator for education. The collection explorer created a connected graph where the nodes were the works of art and the edges were based on commonalities from their textual descriptions. It used very rudimentary language technology so it &#x27;suffered&#x27; from things like homographs. Rather than being seen as a problem, the users liked the serendipity it brought for ideation.<p>I assume free but not random association could be a comparable support for ideation in research.</div><br/></div></div><div id="40047952" class="c"><input type="checkbox" id="c-40047952" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#40047654">root</a><span>|</span><a href="#40047719">parent</a><span>|</span><a href="#40048890">prev</a><span>|</span><a href="#40047941">next</a><span>|</span><label class="collapse" for="c-40047952">[-]</label><label class="expand" for="c-40047952">[7 more]</label></div><br/><div class="children"><div class="content">Assume free-associations = hallucinations. Assume hallucinations are exactly what makes LLMs useful and your question can be rephrased as &quot;Can you list some examples where LLMs were useful to you?&quot;</div><br/><div id="40049446" class="c"><input type="checkbox" id="c-40049446" checked=""/><div class="controls bullet"><span class="by">firewolf34</span><span>|</span><a href="#40047654">root</a><span>|</span><a href="#40047952">parent</a><span>|</span><a href="#40048483">next</a><span>|</span><label class="collapse" for="c-40049446">[-]</label><label class="expand" for="c-40049446">[1 more]</label></div><br/><div class="children"><div class="content">Is not the purpose of a model to interpolate between two points? This is the underlying basis of &quot;hallucinations&quot; (when that works out &#x2F;not&#x2F; in our favour) or &quot;prediction&quot; (when it does). So it&#x27;s a matter of semantics and a bit of overuse of the term &quot;hallucination&quot;. But the model would be useless as nothing more than a search engine if it were to just regurgitate it&#x27;s training data verbatim.</div><br/></div></div><div id="40048483" class="c"><input type="checkbox" id="c-40048483" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#40047654">root</a><span>|</span><a href="#40047952">parent</a><span>|</span><a href="#40049446">prev</a><span>|</span><a href="#40047941">next</a><span>|</span><label class="collapse" for="c-40048483">[-]</label><label class="expand" for="c-40048483">[5 more]</label></div><br/><div class="children"><div class="content">Hallucinations are lies. So not the same thing.</div><br/><div id="40049250" class="c"><input type="checkbox" id="c-40049250" checked=""/><div class="controls bullet"><span class="by">Teleoflexuous</span><span>|</span><a href="#40047654">root</a><span>|</span><a href="#40048483">parent</a><span>|</span><a href="#40048831">next</a><span>|</span><label class="collapse" for="c-40049250">[-]</label><label class="expand" for="c-40049250">[1 more]</label></div><br/><div class="children"><div class="content">For LLM to lie it would need to know the truth. That&#x27;s an incredible level of anthropomorphization.</div><br/></div></div><div id="40048831" class="c"><input type="checkbox" id="c-40048831" checked=""/><div class="controls bullet"><span class="by">malux85</span><span>|</span><a href="#40047654">root</a><span>|</span><a href="#40048483">parent</a><span>|</span><a href="#40049250">prev</a><span>|</span><a href="#40048559">next</a><span>|</span><label class="collapse" for="c-40048831">[-]</label><label class="expand" for="c-40048831">[1 more]</label></div><br/><div class="children"><div class="content">Hallucinations are not always lies, they are more like a transformation in the abstraction space.</div><br/></div></div><div id="40048559" class="c"><input type="checkbox" id="c-40048559" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#40047654">root</a><span>|</span><a href="#40048483">parent</a><span>|</span><a href="#40048831">prev</a><span>|</span><a href="#40048557">next</a><span>|</span><label class="collapse" for="c-40048559">[-]</label><label class="expand" for="c-40048559">[1 more]</label></div><br/><div class="children"><div class="content">All lies aren&#x27;t useless, some can be insightful even when blatantly wrong in themselves (for instance: taken literally every scientific model is a lie). I can definitely see how an LLM hallucinating can helps fostering creativity (the same way psychedelics  can), even if all they say is bullshit.</div><br/></div></div><div id="40048557" class="c"><input type="checkbox" id="c-40048557" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#40047654">root</a><span>|</span><a href="#40048483">parent</a><span>|</span><a href="#40048559">prev</a><span>|</span><a href="#40047941">next</a><span>|</span><label class="collapse" for="c-40048557">[-]</label><label class="expand" for="c-40048557">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using hallucination to mean &quot;not exactly the thing&quot;, not outright lying. So maybe the &quot;truth&quot; is &quot;My socks are wet.&quot; A hallucination could be &quot;My socks are damp.&quot;</div><br/></div></div></div></div></div></div></div></div><div id="40047941" class="c"><input type="checkbox" id="c-40047941" checked=""/><div class="controls bullet"><span class="by">robwwilliams</span><span>|</span><a href="#40047654">parent</a><span>|</span><a href="#40047719">prev</a><span>|</span><a href="#40047975">next</a><span>|</span><label class="collapse" for="c-40047941">[-]</label><label class="expand" for="c-40047941">[1 more]</label></div><br/><div class="children"><div class="content">This approach is already useful in functional genomics. A common type of question requires analysis of hundreds of potentially functional sequence variants.<p>Hybrid LLM+ approaches are beginning to improve efficiency of ranking candidates and even proposing tests and soon I hope—higher order non-linear interactions among DNA variants.</div><br/></div></div></div></div><div id="40047975" class="c"><input type="checkbox" id="c-40047975" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40047654">prev</a><span>|</span><a href="#40047865">next</a><span>|</span><label class="collapse" for="c-40047975">[-]</label><label class="expand" for="c-40047975">[7 more]</label></div><br/><div class="children"><div class="content">A group of PhD students at Stanford recently wanted to take AI&#x2F;ML research ideas generated by LLMs like this and have teams of engineers execute on them at a hackathon. We were getting things prepared at AGI House SF to host the hackathon with them when we learned that the study <i>did not pass ethical review</i>.<p>I think automating science is an important research direction nonetheless.</div><br/><div id="40048240" class="c"><input type="checkbox" id="c-40048240" checked=""/><div class="controls bullet"><span class="by">srcreigh</span><span>|</span><a href="#40047975">parent</a><span>|</span><a href="#40048553">next</a><span>|</span><label class="collapse" for="c-40048240">[-]</label><label class="expand" for="c-40048240">[4 more]</label></div><br/><div class="children"><div class="content">That’s pretty wild. What was the reason behind failing ethics review?</div><br/><div id="40049126" class="c"><input type="checkbox" id="c-40049126" checked=""/><div class="controls bullet"><span class="by">robbomacrae</span><span>|</span><a href="#40047975">root</a><span>|</span><a href="#40048240">parent</a><span>|</span><a href="#40048553">next</a><span>|</span><label class="collapse" for="c-40049126">[-]</label><label class="expand" for="c-40049126">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m generally a proponent of AI and LLM but to me the decision was the right one. You are tasking people with implementing an idea generated by an algorithmic model with (I&#x27;m guessing) zero oversight that might have very little training that teaches it the importance of coming up with ideas worth implementing. Some may be more useful than others so it won&#x27;t be fair from an accomplishment or motivation point of view.<p>Imagine you&#x27;ve already invested time going to this event and want to win the prize&#x2F;credit but to do so you have to implement a plugin that makes webpages grayscale because of a random idea generator. Maybe some people would find that interesting but others would see it as wasting their time.</div><br/><div id="40049186" class="c"><input type="checkbox" id="c-40049186" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#40047975">root</a><span>|</span><a href="#40049126">parent</a><span>|</span><a href="#40049578">next</a><span>|</span><label class="collapse" for="c-40049186">[-]</label><label class="expand" for="c-40049186">[1 more]</label></div><br/><div class="children"><div class="content">As long as all participants are well-informed then there is absolutely no ethical issue...</div><br/></div></div><div id="40049578" class="c"><input type="checkbox" id="c-40049578" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#40047975">root</a><span>|</span><a href="#40049126">parent</a><span>|</span><a href="#40049186">prev</a><span>|</span><a href="#40048553">next</a><span>|</span><label class="collapse" for="c-40049578">[-]</label><label class="expand" for="c-40049578">[1 more]</label></div><br/><div class="children"><div class="content">Surely the ideas themselves are what should be examined for ethical suitability, rather than the meta-idea of “ask an LLM for ideas”?</div><br/></div></div></div></div></div></div><div id="40048553" class="c"><input type="checkbox" id="c-40048553" checked=""/><div class="controls bullet"><span class="by">brigadier132</span><span>|</span><a href="#40047975">parent</a><span>|</span><a href="#40048240">prev</a><span>|</span><a href="#40047865">next</a><span>|</span><label class="collapse" for="c-40048553">[-]</label><label class="expand" for="c-40048553">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think LLMs are the right approach for this. Coordinated science would basically be a search problem where we verify different facts using experiments and use what we learn to determine what experiment to do next.</div><br/><div id="40049241" class="c"><input type="checkbox" id="c-40049241" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40047975">root</a><span>|</span><a href="#40048553">parent</a><span>|</span><a href="#40047865">next</a><span>|</span><label class="collapse" for="c-40049241">[-]</label><label class="expand" for="c-40049241">[1 more]</label></div><br/><div class="children"><div class="content">When you can run experiments quickly it becomes feasible to use ML and evolutionary methods to do novel discoveries, like AlphaTensor&#x27;s better matrix multiplication than Strassen, and AlphaZero&#x27;s move 37, upturning centuries of game strategy.<p>The paper &quot;Evolution through Large Models&quot; shows the way. Just use LLMs as genetic mutation operators. Evolutionary methods are great at search, LLMs are great at intuition but get stuck on their own, they combine well. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2206.08896" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2206.08896</a><p>The interplay between LLMs and Evolutionary Algorithms, despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM&#x27;s further enhancement under black box settings, empowering LLM with flexible global search capacities.<p>Since chatGPT was first released hundreds of millions of people have been using it for assistance, and the model outputs influenced their actions, maybe even supported scientists to make new discoveries. The LLM text is filtered through people and ends up as real world consequences and discoveries that are reported in text, and get in the next training set closing the loop.<p>Trillions of AI tokens per month do this slow feedback game. AI speeds up the circulation of useful information and ideas in human society, and AI feedback gets filtered by the contact with people and the real world.</div><br/></div></div></div></div></div></div><div id="40047865" class="c"><input type="checkbox" id="c-40047865" checked=""/><div class="controls bullet"><span class="by">barathr</span><span>|</span><a href="#40047975">prev</a><span>|</span><a href="#40048550">next</a><span>|</span><label class="collapse" for="c-40047865">[-]</label><label class="expand" for="c-40047865">[1 more]</label></div><br/><div class="children"><div class="content">This strikes me as similar to Cargo Cult Science.<p><a href="https:&#x2F;&#x2F;calteches.library.caltech.edu&#x2F;51&#x2F;2&#x2F;CargoCult.htm" rel="nofollow">https:&#x2F;&#x2F;calteches.library.caltech.edu&#x2F;51&#x2F;2&#x2F;CargoCult.htm</a><p><a href="https:&#x2F;&#x2F;metarationality.com&#x2F;upgrade-your-cargo-cult" rel="nofollow">https:&#x2F;&#x2F;metarationality.com&#x2F;upgrade-your-cargo-cult</a></div><br/></div></div><div id="40048550" class="c"><input type="checkbox" id="c-40048550" checked=""/><div class="controls bullet"><span class="by">UncleOxidant</span><span>|</span><a href="#40047865">prev</a><span>|</span><a href="#40048728">next</a><span>|</span><label class="collapse" for="c-40048550">[-]</label><label class="expand" for="c-40048550">[1 more]</label></div><br/><div class="children"><div class="content">The ideas aren&#x27;t the hard part.</div><br/></div></div><div id="40048728" class="c"><input type="checkbox" id="c-40048728" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#40048550">prev</a><span>|</span><a href="#40047349">next</a><span>|</span><label class="collapse" for="c-40048728">[-]</label><label class="expand" for="c-40048728">[1 more]</label></div><br/><div class="children"><div class="content">In some fields of research, the amount of literature out there is stupendous, and with little hope of a human reading, much less understanding the whole literature.
Its becoming a major problem in some fields, and I think, in some ways, approaches that can combine knowledge algorithmically are needed, perhaps llms.</div><br/></div></div><div id="40047349" class="c"><input type="checkbox" id="c-40047349" checked=""/><div class="controls bullet"><span class="by">not-chatgpt</span><span>|</span><a href="#40048728">prev</a><span>|</span><label class="collapse" for="c-40047349">[-]</label><label class="expand" for="c-40047349">[17 more]</label></div><br/><div class="children"><div class="content">Cool idea. Never gonna work. LLMs are still generative models that spits out training data, incapable of highly abstract creative tasks like research.<p>I still remember all the GPT-2 based startup idea generators that spits out pseudo-feasible startups.</div><br/><div id="40047489" class="c"><input type="checkbox" id="c-40047489" checked=""/><div class="controls bullet"><span class="by">bigyikes</span><span>|</span><a href="#40047349">parent</a><span>|</span><a href="#40047655">next</a><span>|</span><label class="collapse" for="c-40047489">[-]</label><label class="expand" for="c-40047489">[9 more]</label></div><br/><div class="children"><div class="content">Ignoring the “spits out training data” bit which is at best misleading, it’s interesting that you use the word “abstract” here.<p>I recently followed Karpathy’s GPT-from-scratch tutorial and was fascinated with how clearly you could see the models improving.<p>With no training, the model spits out uniformly random text. With a bit of training, the model starts generating gibberish. With further training, the model starts recognizing simple character patterns, like putting a consonant after a vowel. Then it learns syllables, and then words, and then sentences. With enough training (and data and parameters, of course) you eventually yield a model  like GPT-4 that can write better code than many programmers.<p>It’s not always that clear cut, but you can clearly observe it moving up the chain of abstraction as the training loss decreases.<p>What happens when you go even bigger than GPT-4? We have every reason to believe that the models will be able to think more abstractly.<p>Your “never gonna work” comment flies in the face of exponential curve we find ourselves on.</div><br/><div id="40047584" class="c"><input type="checkbox" id="c-40047584" checked=""/><div class="controls bullet"><span class="by">ethanwillis</span><span>|</span><a href="#40047349">root</a><span>|</span><a href="#40047489">parent</a><span>|</span><a href="#40047655">next</a><span>|</span><label class="collapse" for="c-40047584">[-]</label><label class="expand" for="c-40047584">[8 more]</label></div><br/><div class="children"><div class="content">If we keep extrapolating eventually GPT will be omniscient. I really can&#x27;t think of any reason why that wouldn&#x27;t be the case, given the exponential curve we find ourselves on.</div><br/><div id="40047721" class="c"><input type="checkbox" id="c-40047721" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#40047349">root</a><span>|</span><a href="#40047584">parent</a><span>|</span><a href="#40048594">next</a><span>|</span><label class="collapse" for="c-40047721">[-]</label><label class="expand" for="c-40047721">[3 more]</label></div><br/><div class="children"><div class="content">How do you know you&#x27;re not on a logistic curve?<p>Don&#x27;t you think costs and the availability of training data might impose some constraints?</div><br/><div id="40047879" class="c"><input type="checkbox" id="c-40047879" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#40047349">root</a><span>|</span><a href="#40047721">parent</a><span>|</span><a href="#40047757">next</a><span>|</span><label class="collapse" for="c-40047879">[-]</label><label class="expand" for="c-40047879">[1 more]</label></div><br/><div class="children"><div class="content">With real world phenomena that have resource constraints anywhere, a good rule of thumb is: if it looks like an exponential curve, walks like an exponential curve, and quacks like an exponential curve, it’s definitely a logistic curve</div><br/></div></div></div></div><div id="40048594" class="c"><input type="checkbox" id="c-40048594" checked=""/><div class="controls bullet"><span class="by">fire_lake</span><span>|</span><a href="#40047349">root</a><span>|</span><a href="#40047584">parent</a><span>|</span><a href="#40047721">prev</a><span>|</span><a href="#40048081">next</a><span>|</span><label class="collapse" for="c-40048594">[-]</label><label class="expand" for="c-40048594">[2 more]</label></div><br/><div class="children"><div class="content">This has the form of a religious belief.</div><br/><div id="40048764" class="c"><input type="checkbox" id="c-40048764" checked=""/><div class="controls bullet"><span class="by">mistermann</span><span>|</span><a href="#40047349">root</a><span>|</span><a href="#40048594">parent</a><span>|</span><a href="#40048081">next</a><span>|</span><label class="collapse" for="c-40048764">[-]</label><label class="expand" for="c-40048764">[1 more]</label></div><br/><div class="children"><div class="content">And also non-religious belief...paradoxical!</div><br/></div></div></div></div><div id="40048081" class="c"><input type="checkbox" id="c-40048081" checked=""/><div class="controls bullet"><span class="by">inference-lord</span><span>|</span><a href="#40047349">root</a><span>|</span><a href="#40047584">parent</a><span>|</span><a href="#40048594">prev</a><span>|</span><a href="#40047655">next</a><span>|</span><label class="collapse" for="c-40048081">[-]</label><label class="expand" for="c-40048081">[2 more]</label></div><br/><div class="children"><div class="content">I think they&#x27;re being factitious?</div><br/><div id="40049139" class="c"><input type="checkbox" id="c-40049139" checked=""/><div class="controls bullet"><span class="by">ethanwillis</span><span>|</span><a href="#40047349">root</a><span>|</span><a href="#40048081">parent</a><span>|</span><a href="#40047655">next</a><span>|</span><label class="collapse" for="c-40049139">[-]</label><label class="expand" for="c-40049139">[1 more]</label></div><br/><div class="children"><div class="content">I am. And I think it says a lot about the state of things that many people think I&#x27;m being completely serious.</div><br/></div></div></div></div></div></div></div></div><div id="40047655" class="c"><input type="checkbox" id="c-40047655" checked=""/><div class="controls bullet"><span class="by">ramraj07</span><span>|</span><a href="#40047349">parent</a><span>|</span><a href="#40047489">prev</a><span>|</span><a href="#40047483">next</a><span>|</span><label class="collapse" for="c-40047655">[-]</label><label class="expand" for="c-40047655">[3 more]</label></div><br/><div class="children"><div class="content">I have asked chat GPT to generate hypotheses on my PhD topic that I know every single piece of existing literature about and it actually threw out some very interesting ideas that do not exist out there yet (this was before they lobotomized it).</div><br/><div id="40048562" class="c"><input type="checkbox" id="c-40048562" checked=""/><div class="controls bullet"><span class="by">ta988</span><span>|</span><a href="#40047349">root</a><span>|</span><a href="#40047655">parent</a><span>|</span><a href="#40048616">next</a><span>|</span><label class="collapse" for="c-40048562">[-]</label><label class="expand" for="c-40048562">[1 more]</label></div><br/><div class="children"><div class="content">Did you try with the API directly? I&#x27;ve had great results with my own prompts, much less so with the chatgpt one.</div><br/></div></div><div id="40048616" class="c"><input type="checkbox" id="c-40048616" checked=""/><div class="controls bullet"><span class="by">voxl</span><span>|</span><a href="#40047349">root</a><span>|</span><a href="#40047655">parent</a><span>|</span><a href="#40048562">prev</a><span>|</span><a href="#40047483">next</a><span>|</span><label class="collapse" for="c-40048616">[-]</label><label class="expand" for="c-40048616">[1 more]</label></div><br/><div class="children"><div class="content">&gt; (this was before they lobotomized it)<p>Of course, of course. Because god forbid anyone be able to reproduce your suggestion. Funnily enough I tried the same and have the exact opposite experience.</div><br/></div></div></div></div><div id="40047483" class="c"><input type="checkbox" id="c-40047483" checked=""/><div class="controls bullet"><span class="by">growthwtf</span><span>|</span><a href="#40047349">parent</a><span>|</span><a href="#40047655">prev</a><span>|</span><a href="#40047853">next</a><span>|</span><label class="collapse" for="c-40047483">[-]</label><label class="expand" for="c-40047483">[1 more]</label></div><br/><div class="children"><div class="content">I think that ship has sailed, if you believe the paper (which I do).<p>LLMs are already super-human at some highly abstract creative tasks, including research.<p>There are numerous examples of LLMs solving problems that couldn&#x27;t be found in the training data. They can also be improved by using reasoning methods like truth tables or causal language. See Orca from Microsoft for example.</div><br/></div></div><div id="40047853" class="c"><input type="checkbox" id="c-40047853" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#40047349">parent</a><span>|</span><a href="#40047483">prev</a><span>|</span><a href="#40047485">next</a><span>|</span><label class="collapse" for="c-40047853">[-]</label><label class="expand" for="c-40047853">[1 more]</label></div><br/><div class="children"><div class="content">they don&#x27;t just spit out training data, they generalize from training data.  They can look at an existing situation and suggest lines of experimentation or analysis that might lead to interesting results based on similar contexts in other sciences or previous research.  They&#x27;re undertrained on bleeding edge science so they&#x27;re going to falter there but they can apply methodology just fine.</div><br/></div></div><div id="40047485" class="c"><input type="checkbox" id="c-40047485" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#40047349">parent</a><span>|</span><a href="#40047853">prev</a><span>|</span><a href="#40047520">next</a><span>|</span><label class="collapse" for="c-40047485">[-]</label><label class="expand" for="c-40047485">[1 more]</label></div><br/><div class="children"><div class="content">They just need to be better at it than humans, which is a rather low bar when you go beyond two unrelated fields.</div><br/></div></div></div></div></div></div></div></div></div></body></html>