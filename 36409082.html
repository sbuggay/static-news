<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1687338073373" as="style"/><link rel="stylesheet" href="styles.css?v=1687338073373"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://vllm.ai/">vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention</a> <span class="domain">(<a href="https://vllm.ai">vllm.ai</a>)</span></div><div class="subtext"><span>wskwon</span> | <span>39 comments</span></div><br/><div><div id="36409422" class="c"><input type="checkbox" id="c-36409422" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#36411090">next</a><span>|</span><label class="collapse" for="c-36409422">[-]</label><label class="expand" for="c-36409422">[2 more]</label></div><br/><div class="children"><div class="content">This is really cool to see.<p>&gt; Large: Takes up to 1.7GB for a single sequence in LLaMA-13B.<p>&gt; Dynamic: Its size depends on the sequence length, which is highly variable and unpredictable. As a result, efficiently managing the KV cache presents a significant challenge. We find that existing systems waste 60% – 80% of memory due to fragmentation and over-reservation.<p>This mentions improvements for throughput which is great, and it mentions memory savings. I&#x27;m a bit confused how 80% of the memory could be wasted by the KV cache when the vast majority of the memory is usually holding the model itself?<p>How much memory savings does this translate to effectively for say a 30B 4bit model?</div><br/><div id="36409625" class="c"><input type="checkbox" id="c-36409625" checked=""/><div class="controls bullet"><span class="by">zhisbug</span><span>|</span><a href="#36409422">parent</a><span>|</span><a href="#36411090">next</a><span>|</span><label class="collapse" for="c-36409625">[-]</label><label class="expand" for="c-36409625">[1 more]</label></div><br/><div class="children"><div class="content">This really depends on what GPUs you use. If you GPUs has very small amount of memory, vLLM will help more.<p>vLLM addresses the memory bottleneck for saving KV caches and hence increases the throughput.</div><br/></div></div></div></div><div id="36411090" class="c"><input type="checkbox" id="c-36411090" checked=""/><div class="controls bullet"><span class="by">gwph</span><span>|</span><a href="#36409422">prev</a><span>|</span><a href="#36411778">next</a><span>|</span><label class="collapse" for="c-36411090">[-]</label><label class="expand" for="c-36411090">[1 more]</label></div><br/><div class="children"><div class="content">Ion Stoica&#x27;s lab continues to be a powerhouse of innovation. Previous successes of Stoica and his students include (but are certainly not limited to) Apache Spark, Ray, Apache Mesos and Alluxio.</div><br/></div></div><div id="36411778" class="c"><input type="checkbox" id="c-36411778" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36411090">prev</a><span>|</span><a href="#36412091">next</a><span>|</span><label class="collapse" for="c-36411778">[-]</label><label class="expand" for="c-36411778">[2 more]</label></div><br/><div class="children"><div class="content">Reading between the lines, it sounds like some of the speedup comes from VRAM savings on an otherwise close to full GPU?<p>This is definitely cool and needed, but it might not be so dramatic running 3-5 but quant on a less full GPU.</div><br/><div id="36412503" class="c"><input type="checkbox" id="c-36412503" checked=""/><div class="controls bullet"><span class="by">wskwon</span><span>|</span><a href="#36411778">parent</a><span>|</span><a href="#36412091">next</a><span>|</span><label class="collapse" for="c-36412503">[-]</label><label class="expand" for="c-36412503">[1 more]</label></div><br/><div class="children"><div class="content">Yes, vLLM focuses on maximizing throughput when the VRAM is fully utilized. Nevertheless, I believe users can still benefit from vLLM even if they don&#x27;t utilize the memory to its full capacity, because vLLM also includes other optimizations orthogonal to the PagedAttention (e.g., optimized CUDA kernels).</div><br/></div></div></div></div><div id="36412091" class="c"><input type="checkbox" id="c-36412091" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#36411778">prev</a><span>|</span><a href="#36410971">next</a><span>|</span><label class="collapse" for="c-36412091">[-]</label><label class="expand" for="c-36412091">[5 more]</label></div><br/><div class="children"><div class="content">I wonder how this compares to Flash Attention (<a href="https:&#x2F;&#x2F;github.com&#x2F;HazyResearch&#x2F;flash-attention">https:&#x2F;&#x2F;github.com&#x2F;HazyResearch&#x2F;flash-attention</a>), which is the other &quot;memory aware&quot; Attention project I&#x27;m aware of.<p>I guess Flash Attention is more about utilizing memory GPU SRam correctly, where this is more about using the OS&#x2F;CPU memory better?</div><br/><div id="36412208" class="c"><input type="checkbox" id="c-36412208" checked=""/><div class="controls bullet"><span class="by">karmasimida</span><span>|</span><a href="#36412091">parent</a><span>|</span><a href="#36412188">next</a><span>|</span><label class="collapse" for="c-36412208">[-]</label><label class="expand" for="c-36412208">[2 more]</label></div><br/><div class="children"><div class="content">I think they are orthogonal.<p>Flash attention is just another way to compute exact attention.<p>This work mainly concerns how to resolve memory fragmentation across different  sequences<p>You still need to compute attention as is once you retrieve the needed key values</div><br/><div id="36412592" class="c"><input type="checkbox" id="c-36412592" checked=""/><div class="controls bullet"><span class="by">wskwon</span><span>|</span><a href="#36412091">root</a><span>|</span><a href="#36412208">parent</a><span>|</span><a href="#36412188">next</a><span>|</span><label class="collapse" for="c-36412592">[-]</label><label class="expand" for="c-36412592">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the explanation! I believe the two ideas are basically orthogonal. FlashAttention reduces memory read&#x2F;writes, while PagedAttention reduces memory waste.</div><br/></div></div></div></div><div id="36412188" class="c"><input type="checkbox" id="c-36412188" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#36412091">parent</a><span>|</span><a href="#36412208">prev</a><span>|</span><a href="#36410971">next</a><span>|</span><label class="collapse" for="c-36412188">[-]</label><label class="expand" for="c-36412188">[2 more]</label></div><br/><div class="children"><div class="content">The ideas are orthogonal, and can be used (theoretically) at the same time.</div><br/><div id="36412384" class="c"><input type="checkbox" id="c-36412384" checked=""/><div class="controls bullet"><span class="by">scv119</span><span>|</span><a href="#36412091">root</a><span>|</span><a href="#36412188">parent</a><span>|</span><a href="#36410971">next</a><span>|</span><label class="collapse" for="c-36412384">[-]</label><label class="expand" for="c-36412384">[1 more]</label></div><br/><div class="children"><div class="content">I believe you can slightly change the flash attention kernel to implement the same kernel of this page attention, since both of them work on the key&#x2F;value cache at block level.</div><br/></div></div></div></div></div></div><div id="36410971" class="c"><input type="checkbox" id="c-36410971" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#36412091">prev</a><span>|</span><a href="#36411375">next</a><span>|</span><label class="collapse" for="c-36410971">[-]</label><label class="expand" for="c-36410971">[6 more]</label></div><br/><div class="children"><div class="content">Semi-related question: this page is full of little charts and diagrams. There are thousands of similar projects&#x2F;sites&#x2F;experiment sites with their own charts and diagrams. But it seems like there are always subtle-to-large differences in them that indicate they&#x27;re made with totally different libraries.<p>Are there just thousands of homebrewn non-standard chart &amp; diagram builders out there? How does one even begin to pick a standard to whip out quickies like these? Google SEO makes it virtually impossible to get to substance.</div><br/><div id="36411159" class="c"><input type="checkbox" id="c-36411159" checked=""/><div class="controls bullet"><span class="by">daedbe</span><span>|</span><a href="#36410971">parent</a><span>|</span><a href="#36411734">next</a><span>|</span><label class="collapse" for="c-36411159">[-]</label><label class="expand" for="c-36411159">[1 more]</label></div><br/><div class="children"><div class="content">I often see charts produced using matplotlib or plotly - often you can tell based on the colour schemes used. For example, the bar chart at the bottom of this paper looks like it was made with plotly. I think the reason for such variance in the style of charts is largely due to the flexibility frameworks such as matplotlib provide: you can control basically every aspect of a chart and use any number of predefined or custom stylesheets to change the look and feel.</div><br/></div></div><div id="36411734" class="c"><input type="checkbox" id="c-36411734" checked=""/><div class="controls bullet"><span class="by">wskwon</span><span>|</span><a href="#36410971">parent</a><span>|</span><a href="#36411159">prev</a><span>|</span><a href="#36411641">next</a><span>|</span><label class="collapse" for="c-36411734">[-]</label><label class="expand" for="c-36411734">[2 more]</label></div><br/><div class="children"><div class="content">We used matplotlib for the performance charts, and used a free website to convert google slides to the animation gifs.</div><br/><div id="36415513" class="c"><input type="checkbox" id="c-36415513" checked=""/><div class="controls bullet"><span class="by">e12e</span><span>|</span><a href="#36410971">root</a><span>|</span><a href="#36411734">parent</a><span>|</span><a href="#36411641">next</a><span>|</span><label class="collapse" for="c-36415513">[-]</label><label class="expand" for="c-36415513">[1 more]</label></div><br/><div class="children"><div class="content">Which &quot;free website&quot;?</div><br/></div></div></div></div><div id="36411641" class="c"><input type="checkbox" id="c-36411641" checked=""/><div class="controls bullet"><span class="by">kristjansson</span><span>|</span><a href="#36410971">parent</a><span>|</span><a href="#36411734">prev</a><span>|</span><a href="#36411375">next</a><span>|</span><label class="collapse" for="c-36411641">[-]</label><label class="expand" for="c-36411641">[2 more]</label></div><br/><div class="children"><div class="content">The color scheme on these implies Google Drawing, but I don&#x27;t know how they made them into animations - maybe just manually?</div><br/><div id="36411721" class="c"><input type="checkbox" id="c-36411721" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#36410971">root</a><span>|</span><a href="#36411641">parent</a><span>|</span><a href="#36411375">next</a><span>|</span><label class="collapse" for="c-36411721">[-]</label><label class="expand" for="c-36411721">[1 more]</label></div><br/><div class="children"><div class="content">Google slides I think.</div><br/></div></div></div></div></div></div><div id="36411375" class="c"><input type="checkbox" id="c-36411375" checked=""/><div class="controls bullet"><span class="by">jokoon</span><span>|</span><a href="#36410971">prev</a><span>|</span><a href="#36413909">next</a><span>|</span><label class="collapse" for="c-36411375">[-]</label><label class="expand" for="c-36411375">[2 more]</label></div><br/><div class="children"><div class="content">Now do the same for image classifiers. I tried a few of them, they&#x27;re just horribly slow.<p>This is pretty outrageous considering the first robust image image classifiers appeared around 2007.</div><br/><div id="36412203" class="c"><input type="checkbox" id="c-36412203" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#36411375">parent</a><span>|</span><a href="#36413909">next</a><span>|</span><label class="collapse" for="c-36412203">[-]</label><label class="expand" for="c-36412203">[1 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t work on image classifiers, because there&#x27;s no KV cache. Also, standard image classifiers can do 100-1000 images&#x2F;sec without any optimizations.</div><br/></div></div></div></div><div id="36413909" class="c"><input type="checkbox" id="c-36413909" checked=""/><div class="controls bullet"><span class="by">SimFG</span><span>|</span><a href="#36411375">prev</a><span>|</span><a href="#36413305">next</a><span>|</span><label class="collapse" for="c-36413909">[-]</label><label class="expand" for="c-36413909">[2 more]</label></div><br/><div class="children"><div class="content">Cool, I prefer the OpenAI-Compatible api. Although this is not very technically difficult, it is really intimate, because it make me feel free to use all ChatGPT applications.</div><br/><div id="36414206" class="c"><input type="checkbox" id="c-36414206" checked=""/><div class="controls bullet"><span class="by">wskwon</span><span>|</span><a href="#36413909">parent</a><span>|</span><a href="#36413305">next</a><span>|</span><label class="collapse" for="c-36414206">[-]</label><label class="expand" for="c-36414206">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! Please try it out and share any feedback you might have.</div><br/></div></div></div></div><div id="36413305" class="c"><input type="checkbox" id="c-36413305" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#36413909">prev</a><span>|</span><a href="#36411560">next</a><span>|</span><label class="collapse" for="c-36413305">[-]</label><label class="expand" for="c-36413305">[1 more]</label></div><br/><div class="children"><div class="content">Now just waiting for a timing attack paper where you can see or guess someone else&#x27;s conversation that is hosted in the same data center :-).<p>Or maybe you get typically get dedicated machine time during inference?</div><br/></div></div><div id="36411560" class="c"><input type="checkbox" id="c-36411560" checked=""/><div class="controls bullet"><span class="by">scv119</span><span>|</span><a href="#36413305">prev</a><span>|</span><a href="#36410974">next</a><span>|</span><label class="collapse" for="c-36411560">[-]</label><label class="expand" for="c-36411560">[1 more]</label></div><br/><div class="children"><div class="content">Pretty cool stuff and the results are amazing. Hoping we will see virtual memory get standardized in pytorch or cuda.</div><br/></div></div><div id="36410974" class="c"><input type="checkbox" id="c-36410974" checked=""/><div class="controls bullet"><span class="by">marcopicentini</span><span>|</span><a href="#36411560">prev</a><span>|</span><a href="#36409083">next</a><span>|</span><label class="collapse" for="c-36410974">[-]</label><label class="expand" for="c-36410974">[2 more]</label></div><br/><div class="children"><div class="content">Is it available an hosted demo?<p>What are use cases for which open source models are equivalent of GPT 3.5?</div><br/><div id="36411028" class="c"><input type="checkbox" id="c-36411028" checked=""/><div class="controls bullet"><span class="by">wskwon</span><span>|</span><a href="#36410974">parent</a><span>|</span><a href="#36409083">next</a><span>|</span><label class="collapse" for="c-36411028">[-]</label><label class="expand" for="c-36411028">[1 more]</label></div><br/><div class="children"><div class="content">You can think of LMSYS Vicuna: <a href="https:&#x2F;&#x2F;chat.lmsys.org" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.lmsys.org</a> as our hosted demo, as it actually uses vLLM as the backend.</div><br/></div></div></div></div><div id="36409083" class="c"><input type="checkbox" id="c-36409083" checked=""/><div class="controls bullet"><span class="by">wskwon</span><span>|</span><a href="#36410974">prev</a><span>|</span><a href="#36411111">next</a><span>|</span><label class="collapse" for="c-36409083">[-]</label><label class="expand" for="c-36409083">[1 more]</label></div><br/><div class="children"><div class="content">vLLM has been adopted by LMSYS for serving Vicuna and Chatbot Arena.</div><br/></div></div><div id="36411111" class="c"><input type="checkbox" id="c-36411111" checked=""/><div class="controls bullet"><span class="by">kossTKR</span><span>|</span><a href="#36409083">prev</a><span>|</span><a href="#36411717">next</a><span>|</span><label class="collapse" for="c-36411111">[-]</label><label class="expand" for="c-36411111">[4 more]</label></div><br/><div class="children"><div class="content">Does this mean that GPT-4&#x2F;65b level performance is closer to running on a say a m1&#x2F;m2 with only 24+ gigabytes of ram?</div><br/><div id="36411920" class="c"><input type="checkbox" id="c-36411920" checked=""/><div class="controls bullet"><span class="by">aftbit</span><span>|</span><a href="#36411111">parent</a><span>|</span><a href="#36411242">next</a><span>|</span><label class="collapse" for="c-36411920">[-]</label><label class="expand" for="c-36411920">[1 more]</label></div><br/><div class="children"><div class="content">Nope. You will still need a proper GPU. You can&#x27;t yet run large language models on tiny hardware like an m1&#x2F;m2. Even the llama.cpp magic is only possible with very small models at beam size 1, which really limits the &quot;creativity&quot; of these models.</div><br/></div></div><div id="36411242" class="c"><input type="checkbox" id="c-36411242" checked=""/><div class="controls bullet"><span class="by">wskwon</span><span>|</span><a href="#36411111">parent</a><span>|</span><a href="#36411920">prev</a><span>|</span><a href="#36411717">next</a><span>|</span><label class="collapse" for="c-36411242">[-]</label><label class="expand" for="c-36411242">[2 more]</label></div><br/><div class="children"><div class="content">Not really. vLLM optimizes the throughput of your LLM, but does not reduce the minimum required amount of resource to run your model.</div><br/><div id="36415595" class="c"><input type="checkbox" id="c-36415595" checked=""/><div class="controls bullet"><span class="by">e12e</span><span>|</span><a href="#36411111">root</a><span>|</span><a href="#36411242">parent</a><span>|</span><a href="#36411717">next</a><span>|</span><label class="collapse" for="c-36415595">[-]</label><label class="expand" for="c-36415595">[1 more]</label></div><br/><div class="children"><div class="content">But (in theory) - llama.cpp could implement similar approach to paging&#x2F;memory and see a speedup for 4bit models on cpu?</div><br/></div></div></div></div></div></div><div id="36411717" class="c"><input type="checkbox" id="c-36411717" checked=""/><div class="controls bullet"><span class="by">two_in_one</span><span>|</span><a href="#36411111">prev</a><span>|</span><a href="#36411257">next</a><span>|</span><label class="collapse" for="c-36411717">[-]</label><label class="expand" for="c-36411717">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if this sort of memory management can be made for Pytorch transformers as under the hood optimization.</div><br/></div></div><div id="36411257" class="c"><input type="checkbox" id="c-36411257" checked=""/><div class="controls bullet"><span class="by">bioemerl</span><span>|</span><a href="#36411717">prev</a><span>|</span><a href="#36412199">next</a><span>|</span><label class="collapse" for="c-36411257">[-]</label><label class="expand" for="c-36411257">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;m spoiled by 4 bit and unfortunately it doesn&#x27;t appear to be supposed here so this isn&#x27;t of much use to me, but it&#x27;s awesome to see people working on the inference speed side of things regardless.</div><br/><div id="36411268" class="c"><input type="checkbox" id="c-36411268" checked=""/><div class="controls bullet"><span class="by">george_123</span><span>|</span><a href="#36411257">parent</a><span>|</span><a href="#36412199">next</a><span>|</span><label class="collapse" for="c-36411268">[-]</label><label class="expand" for="c-36411268">[6 more]</label></div><br/><div class="children"><div class="content">this approach to managing KV cache can work with 4bit. imagine the speedup of pagedattention with quantization..</div><br/><div id="36411330" class="c"><input type="checkbox" id="c-36411330" checked=""/><div class="controls bullet"><span class="by">zhisbug</span><span>|</span><a href="#36411257">root</a><span>|</span><a href="#36411268">parent</a><span>|</span><a href="#36412199">next</a><span>|</span><label class="collapse" for="c-36411330">[-]</label><label class="expand" for="c-36411330">[5 more]</label></div><br/><div class="children"><div class="content">yep, it is agonistic to 4-bit. You can deploy a 4-bit model and still use vllm + pagedattention to double or even triple your serving throughput.</div><br/><div id="36411735" class="c"><input type="checkbox" id="c-36411735" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#36411257">root</a><span>|</span><a href="#36411330">parent</a><span>|</span><a href="#36412432">next</a><span>|</span><label class="collapse" for="c-36411735">[-]</label><label class="expand" for="c-36411735">[1 more]</label></div><br/><div class="children"><div class="content">If this were submitted as a new comment it would be at the top of the page.</div><br/></div></div><div id="36412432" class="c"><input type="checkbox" id="c-36412432" checked=""/><div class="controls bullet"><span class="by">baobabKoodaa</span><span>|</span><a href="#36411257">root</a><span>|</span><a href="#36411330">parent</a><span>|</span><a href="#36411735">prev</a><span>|</span><a href="#36412214">next</a><span>|</span><label class="collapse" for="c-36412432">[-]</label><label class="expand" for="c-36412432">[1 more]</label></div><br/><div class="children"><div class="content">You mean like, theoretically, in the future? Or you mean today?</div><br/></div></div><div id="36412214" class="c"><input type="checkbox" id="c-36412214" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#36411257">root</a><span>|</span><a href="#36411330">parent</a><span>|</span><a href="#36412432">prev</a><span>|</span><a href="#36412199">next</a><span>|</span><label class="collapse" for="c-36412214">[-]</label><label class="expand" for="c-36412214">[2 more]</label></div><br/><div class="children"><div class="content">probably mean agnostic, agonistic implies the opposite.</div><br/><div id="36412377" class="c"><input type="checkbox" id="c-36412377" checked=""/><div class="controls bullet"><span class="by">zhisbug</span><span>|</span><a href="#36411257">root</a><span>|</span><a href="#36412214">parent</a><span>|</span><a href="#36412199">next</a><span>|</span><label class="collapse" for="c-36412377">[-]</label><label class="expand" for="c-36412377">[1 more]</label></div><br/><div class="children"><div class="content">oops typo</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36412199" class="c"><input type="checkbox" id="c-36412199" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#36411257">prev</a><span>|</span><label class="collapse" for="c-36412199">[-]</label><label class="expand" for="c-36412199">[1 more]</label></div><br/><div class="children"><div class="content">Parallelization, Paging! What will those AI&#x2F;ML PhD’s think of next!</div><br/></div></div></div></div></div></div></div></body></html>