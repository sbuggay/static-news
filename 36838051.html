<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1690189249069" as="style"/><link rel="stylesheet" href="styles.css?v=1690189249069"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/karpathy/llama2.c">Llama2.c: Inference llama 2 in one file of pure C</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>anjneymidha</span> | <span>152 comments</span></div><br/><div><div id="36838834" class="c"><input type="checkbox" id="c-36838834" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#36839153">next</a><span>|</span><label class="collapse" for="c-36838834">[-]</label><label class="expand" for="c-36838834">[31 more]</label></div><br/><div class="children"><div class="content">Yay fun to see it make its way to HN :)
It turns out that my original checkpoint runs _way_ faster than I expected (100 tok&#x2F;s) on MacBook Air M1 with -O3 when compiling, so I am now training a bigger 44M model, which should still running interactively. Maybe the 7B Llama model is within reach... :thinking_emoji:</div><br/><div id="36839823" class="c"><input type="checkbox" id="c-36839823" checked=""/><div class="controls bullet"><span class="by">novaRom</span><span>|</span><a href="#36838834">parent</a><span>|</span><a href="#36839166">next</a><span>|</span><label class="collapse" for="c-36839823">[-]</label><label class="expand" for="c-36839823">[5 more]</label></div><br/><div class="children"><div class="content">I did use a tweaked nanoGPT to pretrain a 12M model on TinyStories (2Gbytes produced by GPT4), and results are pretty amazing. I&#x27;ve adapted it a bit on Wikipedia then, and it looks like a solid bullshit generator, much smarter than any smoothed n-gram model, and significantly smaller. My bet small LLMs will be predominant in multiple areas. My next goal is to reduce 7B llama2 to 10-100M without making it much dumber.</div><br/><div id="36844748" class="c"><input type="checkbox" id="c-36844748" checked=""/><div class="controls bullet"><span class="by">oaguy1</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36839823">parent</a><span>|</span><a href="#36842784">next</a><span>|</span><label class="collapse" for="c-36844748">[-]</label><label class="expand" for="c-36844748">[1 more]</label></div><br/><div class="children"><div class="content">I also trained nanoGPT on TinyStories, produced about a 32M model. The results are amazing, especially considering I opted for a character-level model similar to the toy dataset in the repo. I’m writing about the experience while also doing a deep dive into the code on medium (username oaguy1). Smaller LLMs are definitely worth considering with the right quality training data. Once I finish playing with TinyStories, I recently tweaked the Standardized Project Gutenberg Corpus (~11GB) to be more modern. Want to see what I can do with it with nanoGPT and then maybe Huggingface’s libraries.</div><br/></div></div><div id="36842784" class="c"><input type="checkbox" id="c-36842784" checked=""/><div class="controls bullet"><span class="by">Remmy</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36839823">parent</a><span>|</span><a href="#36844748">prev</a><span>|</span><a href="#36840086">next</a><span>|</span><label class="collapse" for="c-36842784">[-]</label><label class="expand" for="c-36842784">[1 more]</label></div><br/><div class="children"><div class="content">Would love to read more about your time in NanoGPT. I&#x27;ve been getting familiar with it myself lately and it&#x27;s still pretty much gibberish in the output with 16M, but the dataset is admittedly trash right now as well.</div><br/></div></div><div id="36840086" class="c"><input type="checkbox" id="c-36840086" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36839823">parent</a><span>|</span><a href="#36842784">prev</a><span>|</span><a href="#36839166">next</a><span>|</span><label class="collapse" for="c-36840086">[-]</label><label class="expand" for="c-36840086">[2 more]</label></div><br/><div class="children"><div class="content">&gt;My next goal is to reduce 7B llama2 to 10-100M without making it much dumber.<p>That is going to be hard as the 7B model was trained on 2T tokens. Maybe if you heavily restrict the range in which the model should operate.</div><br/><div id="36844584" class="c"><input type="checkbox" id="c-36844584" checked=""/><div class="controls bullet"><span class="by">ljlolel</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36840086">parent</a><span>|</span><a href="#36839166">next</a><span>|</span><label class="collapse" for="c-36844584">[-]</label><label class="expand" for="c-36844584">[1 more]</label></div><br/><div class="children"><div class="content">1. It’s faster and cheaper to train a smaller model<p>2. Better than tokens is to train on probability distributions (distillation) and trees of probability distributions</div><br/></div></div></div></div></div></div><div id="36839166" class="c"><input type="checkbox" id="c-36839166" checked=""/><div class="controls bullet"><span class="by">pgbovine</span><span>|</span><a href="#36838834">parent</a><span>|</span><a href="#36839823">prev</a><span>|</span><a href="#36843713">next</a><span>|</span><label class="collapse" for="c-36839166">[-]</label><label class="expand" for="c-36839166">[13 more]</label></div><br/><div class="children"><div class="content">Your work is an inspiration as always!! My n00b question is: what do you think is currently the most practical path to running a reasonably-sized (doesn&#x27;t have to be the biggest) LLM on a commodity linux server for hooking up to a hobby web app ... i.e., one without a fancy GPU. (Renting instances with GPUs on, say, Linode, is <i>significantly</i> more expensive than standard servers that host web apps.) Is this totally out of reach, or are approaches like yours (or others you know of) a feasible path forward?</div><br/><div id="36839739" class="c"><input type="checkbox" id="c-36839739" checked=""/><div class="controls bullet"><span class="by">vikp</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36839166">parent</a><span>|</span><a href="#36840577">next</a><span>|</span><label class="collapse" for="c-36839739">[-]</label><label class="expand" for="c-36839739">[1 more]</label></div><br/><div class="children"><div class="content">I would use textsynth (<a href="https:&#x2F;&#x2F;bellard.org&#x2F;ts_server&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;bellard.org&#x2F;ts_server&#x2F;</a>) or llama.cpp (<a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp</a>) if you&#x27;re running on CPU.<p><pre><code>  - I wouldn&#x27;t use anything higher than a 7B model if you want decent speed.
  - Quantize to 4-bit to save RAM and run inference faster.
</code></pre>
Speed will be around 15 tokens per second on CPU (tolerable), and 5-10x faster with a GPU.</div><br/></div></div><div id="36840577" class="c"><input type="checkbox" id="c-36840577" checked=""/><div class="controls bullet"><span class="by">pedrovhb</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36839166">parent</a><span>|</span><a href="#36839739">prev</a><span>|</span><a href="#36839510">next</a><span>|</span><label class="collapse" for="c-36840577">[-]</label><label class="expand" for="c-36840577">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been playing with running some models on the free tier Oracle VM machines with 24GB RAM and Ampere CPU and it works pretty well with llama.cpp. It&#x27;s actually surprisingly quick; speed doesn&#x27;t scale <i>too</i> well with the number of threads on CPU, so even the 4 ARM64 cores on that VM, with NEON, run at a similar speed to my 24-core Ryzen 3850X (maybe about half reading speed). It can easily handle Llama 2 13B, and if I recall correctly I did manage to run a 30B model in the past too. Speed for the smaller ones is ~half reading speed or so.<p>It&#x27;s a shame the current Llama 2 jumps from 13B to 70B. In the past  I tried running larger stuff by making a 32GB swap volume, but it&#x27;s just impractically slow.</div><br/><div id="36843935" class="c"><input type="checkbox" id="c-36843935" checked=""/><div class="controls bullet"><span class="by">summarity</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36840577">parent</a><span>|</span><a href="#36840955">next</a><span>|</span><label class="collapse" for="c-36843935">[-]</label><label class="expand" for="c-36843935">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re running on Ampere, using llama.cpp is probably not ideal. While it&#x27;s optimized for ARM, Ampere has native acceleration for workloads like this: <a href="https:&#x2F;&#x2F;cloudmarketplace.oracle.com&#x2F;marketplace&#x2F;en_US&#x2F;adf.task-flow?adf.tfDoc=&#x2F;WEB-INF&#x2F;taskflow&#x2F;adhtf.xml&amp;application_id=125935163&amp;adf.tfId=adhtf" rel="nofollow noreferrer">https:&#x2F;&#x2F;cloudmarketplace.oracle.com&#x2F;marketplace&#x2F;en_US&#x2F;adf.ta...</a></div><br/></div></div><div id="36840955" class="c"><input type="checkbox" id="c-36840955" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36840577">parent</a><span>|</span><a href="#36843935">prev</a><span>|</span><a href="#36839510">next</a><span>|</span><label class="collapse" for="c-36840955">[-]</label><label class="expand" for="c-36840955">[5 more]</label></div><br/><div class="children"><div class="content">Prompt ingestion is too slow on the Oracle VMs.<p>Also its really tricky to even <i>build</i> llama.cpp with a BLAS library, to make prompt ingestion less slow. The Oracle Linux OpenBLAS build isnt detected ootb, and it doesn&#x27;t perform well compared to x86 for some reason.<p>LLVM&#x2F;GCC have some kind of issue identifying the Ampere ARM architecture (march=native doesn&#x27;t really work), so maybe this could be improved with the right compiler flags?</div><br/><div id="36841784" class="c"><input type="checkbox" id="c-36841784" checked=""/><div class="controls bullet"><span class="by">pedrovhb</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36840955">parent</a><span>|</span><a href="#36841277">next</a><span>|</span><label class="collapse" for="c-36841784">[-]</label><label class="expand" for="c-36841784">[2 more]</label></div><br/><div class="children"><div class="content">Not sure if that&#x27;s still the case. I remember having trouble building it a couple of months ago, had to tweak the Makefile because iirc it assumed ARM64 &lt;=&gt; Mac, but I recently re-cloned the repo and started from scratch and it was as simple as `make DLLAMA_BLAS=1`. I don&#x27;t think I have any special setup other than having installed the apt openblas dev package.</div><br/><div id="36842413" class="c"><input type="checkbox" id="c-36842413" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36841784">parent</a><span>|</span><a href="#36841277">next</a><span>|</span><label class="collapse" for="c-36842413">[-]</label><label class="expand" for="c-36842413">[1 more]</label></div><br/><div class="children"><div class="content">IDK. A bunch of basic development packages like git were missing from my Ubuntu image when I tried last week, and I just gave up because it seemed like a big rabbit hole to go down.<p>I can see the ARM64 versions on the Ubuntu web package list, so... IDK what was going on?<p>On Oracle Linux, until I changed some env variables and lines in the makefile, the openblas build would &quot;work,&quot; but it was actually silently failing and not using OpenBLAS.</div><br/></div></div></div></div><div id="36841277" class="c"><input type="checkbox" id="c-36841277" checked=""/><div class="controls bullet"><span class="by">jvickers</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36840955">parent</a><span>|</span><a href="#36841784">prev</a><span>|</span><a href="#36839510">next</a><span>|</span><label class="collapse" for="c-36841277">[-]</label><label class="expand" for="c-36841277">[2 more]</label></div><br/><div class="children"><div class="content">Is it any easier when using Ubuntu on ARM Oracle servers?</div><br/><div id="36841580" class="c"><input type="checkbox" id="c-36841580" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36841277">parent</a><span>|</span><a href="#36839510">next</a><span>|</span><label class="collapse" for="c-36841580">[-]</label><label class="expand" for="c-36841580">[1 more]</label></div><br/><div class="children"><div class="content">Nah, I tried Ubuntu too.<p>The OpenBLAS package was missing on ARM, along with some other dependencies I needed for compilation.<p>At the end of the day, even with many tweaks and custom compilation flags, the instance was averaging below 1 token&#x2F;sec as a Kobold Horde host, which is below the threshold to even be allowed as a llm host.</div><br/></div></div></div></div></div></div></div></div><div id="36839510" class="c"><input type="checkbox" id="c-36839510" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36839166">parent</a><span>|</span><a href="#36840577">prev</a><span>|</span><a href="#36843713">next</a><span>|</span><label class="collapse" for="c-36839510">[-]</label><label class="expand" for="c-36839510">[4 more]</label></div><br/><div class="children"><div class="content">It might be more expensive to get a GPU instance but at a guess I&#x27;d say it&#x27;s more cost-effective considering that the CPU computation will be less efficient and take much longer. I bet someone&#x27;s done this out with real numbers, I just haven&#x27;t seen it.</div><br/><div id="36840505" class="c"><input type="checkbox" id="c-36840505" checked=""/><div class="controls bullet"><span class="by">franga2000</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36839510">parent</a><span>|</span><a href="#36841164">next</a><span>|</span><label class="collapse" for="c-36840505">[-]</label><label class="expand" for="c-36840505">[2 more]</label></div><br/><div class="children"><div class="content">This only matters if you&#x27;re scaling to meet demand and demand is higher than your spare resources, which often isn&#x27;t the case for hobby projects.    
The 10€&#x2F;mo VPS I&#x27;ve had for over 6 years now still has a few cores and GBs or RAM spare, so running a small model on the CPU for a personal project that only me and a few friends occasionally use wouldn&#x27;t cost me a cent more.</div><br/><div id="36843784" class="c"><input type="checkbox" id="c-36843784" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36840505">parent</a><span>|</span><a href="#36841164">next</a><span>|</span><label class="collapse" for="c-36843784">[-]</label><label class="expand" for="c-36843784">[1 more]</label></div><br/><div class="children"><div class="content">FYI, the going rate for &quot;smallest possible VPS&quot; is now more like 3€&#x2F;mo.</div><br/></div></div></div></div><div id="36841164" class="c"><input type="checkbox" id="c-36841164" checked=""/><div class="controls bullet"><span class="by">bg24</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36839510">parent</a><span>|</span><a href="#36840505">prev</a><span>|</span><a href="#36843713">next</a><span>|</span><label class="collapse" for="c-36841164">[-]</label><label class="expand" for="c-36841164">[1 more]</label></div><br/><div class="children"><div class="content">It depends on your use case, correct? If you do not have a heavy inferencing requirement, then CPU is good enough.</div><br/></div></div></div></div></div></div><div id="36843713" class="c"><input type="checkbox" id="c-36843713" checked=""/><div class="controls bullet"><span class="by">madduci</span><span>|</span><a href="#36838834">parent</a><span>|</span><a href="#36839166">prev</a><span>|</span><a href="#36839781">next</a><span>|</span><label class="collapse" for="c-36843713">[-]</label><label class="expand" for="c-36843713">[5 more]</label></div><br/><div class="children"><div class="content">Do you think it&#x27;s possible also to create a trainer in pure C, instead of using python?</div><br/><div id="36843778" class="c"><input type="checkbox" id="c-36843778" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36843713">parent</a><span>|</span><a href="#36843959">next</a><span>|</span><label class="collapse" for="c-36843778">[-]</label><label class="expand" for="c-36843778">[1 more]</label></div><br/><div class="children"><div class="content">Of course it&#x27;s possible. The question is whether anyone finds it worth doing.<p>ML algorithms are, at their core, not particularly complicated code. But they are still <i>tricky</i> code, because if you get them wrong you will find that you spent 500 GPU-years turning random numbers that cause the model to output gibberish into other random numbers that cause the model to output different yet semantically identical gibberish.<p>Writing them in a more abstract languages has advantages - like automatic differentiation. You <i>could</i> explicitly tell the computer how to compute the output and its derivative, or you could tell the computer how to compute the output, and let it also compute the derivative by itself.<p>Having all your weights in one object is also awfully convenient; you can write something like `weights -= error * deriv * learning_rate` instead of iterating over each individual weight (and a large model contains many different sets of weights, not just a single NxMxPxQ matrix)<p>This is good for the rapid iteration that ML research demands. However, once you have selected a model, I&#x27;m sure you can get performance advantages by coding it in a low level and eliminating inefficiencies. For example, you should be able to use the weight update equation from above by using fused multiply-accumulate, and the Python framework might not realize that.</div><br/></div></div><div id="36843959" class="c"><input type="checkbox" id="c-36843959" checked=""/><div class="controls bullet"><span class="by">DougBTX</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36843713">parent</a><span>|</span><a href="#36843778">prev</a><span>|</span><a href="#36843774">next</a><span>|</span><label class="collapse" for="c-36843959">[-]</label><label class="expand" for="c-36843959">[1 more]</label></div><br/><div class="children"><div class="content">This is C++ rather than C, but a substantial portion of PyTorch is written in C++, and they provide a C++ interface:<p><a href="https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;advanced&#x2F;cpp_frontend.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;advanced&#x2F;cpp_frontend.html</a><p>In other words, you can absolutely use PyTorch without Python.</div><br/></div></div><div id="36843774" class="c"><input type="checkbox" id="c-36843774" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36843713">parent</a><span>|</span><a href="#36843959">prev</a><span>|</span><a href="#36844709">next</a><span>|</span><label class="collapse" for="c-36843774">[-]</label><label class="expand" for="c-36843774">[1 more]</label></div><br/><div class="children"><div class="content">In principle easy and possible, just not exactly useful. Would just involve adding the backward pass. But I’m not sure that this is something many people would want.</div><br/></div></div><div id="36844709" class="c"><input type="checkbox" id="c-36844709" checked=""/><div class="controls bullet"><span class="by">voz_</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36843713">parent</a><span>|</span><a href="#36843774">prev</a><span>|</span><a href="#36839781">next</a><span>|</span><label class="collapse" for="c-36844709">[-]</label><label class="expand" for="c-36844709">[1 more]</label></div><br/><div class="children"><div class="content">Just compile the Python</div><br/></div></div></div></div><div id="36839781" class="c"><input type="checkbox" id="c-36839781" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#36838834">parent</a><span>|</span><a href="#36843713">prev</a><span>|</span><a href="#36840782">next</a><span>|</span><label class="collapse" for="c-36839781">[-]</label><label class="expand" for="c-36839781">[1 more]</label></div><br/><div class="children"><div class="content">Great job, thanks!  Do you have any early impressions on the relative quality&#x2F;performance of small lama-2 models vs the small gpt-2 models?</div><br/></div></div><div id="36842851" class="c"><input type="checkbox" id="c-36842851" checked=""/><div class="controls bullet"><span class="by">ActorNightly</span><span>|</span><a href="#36838834">parent</a><span>|</span><a href="#36840782">prev</a><span>|</span><a href="#36841413">next</a><span>|</span><label class="collapse" for="c-36842851">[-]</label><label class="expand" for="c-36842851">[1 more]</label></div><br/><div class="children"><div class="content">Are you training these things on your home rig, M1, or in the cloud?</div><br/></div></div><div id="36841413" class="c"><input type="checkbox" id="c-36841413" checked=""/><div class="controls bullet"><span class="by">brian_herman</span><span>|</span><a href="#36838834">parent</a><span>|</span><a href="#36842851">prev</a><span>|</span><a href="#36839141">next</a><span>|</span><label class="collapse" for="c-36841413">[-]</label><label class="expand" for="c-36841413">[3 more]</label></div><br/><div class="children"><div class="content">Could you post the 44M model somewhere where we can download?</div><br/><div id="36841484" class="c"><input type="checkbox" id="c-36841484" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36841413">parent</a><span>|</span><a href="#36839141">next</a><span>|</span><label class="collapse" for="c-36841484">[-]</label><label class="expand" for="c-36841484">[2 more]</label></div><br/><div class="children"><div class="content">Still training. I will put it in readme</div><br/><div id="36841707" class="c"><input type="checkbox" id="c-36841707" checked=""/><div class="controls bullet"><span class="by">brian_herman</span><span>|</span><a href="#36838834">root</a><span>|</span><a href="#36841484">parent</a><span>|</span><a href="#36839141">next</a><span>|</span><label class="collapse" for="c-36841707">[-]</label><label class="expand" for="c-36841707">[1 more]</label></div><br/><div class="children"><div class="content">Oh wow I didn&#x27;t realize you are the creator I should really learn how to read one of these days.</div><br/></div></div></div></div></div></div></div></div><div id="36839153" class="c"><input type="checkbox" id="c-36839153" checked=""/><div class="controls bullet"><span class="by">samwillis</span><span>|</span><a href="#36838834">prev</a><span>|</span><a href="#36842940">next</a><span>|</span><label class="collapse" for="c-36839153">[-]</label><label class="expand" for="c-36839153">[10 more]</label></div><br/><div class="children"><div class="content">This running in the browser via Emscripten by Georgi Gerganov of llama.cpp fame:<p><a href="https:&#x2F;&#x2F;ggerganov.com&#x2F;llama2.c&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;ggerganov.com&#x2F;llama2.c&#x2F;</a><p>Via his Twitter with ongoing thread: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;ggerganov&#x2F;status&#x2F;1683174252990660610" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;ggerganov&#x2F;status&#x2F;1683174252990660610</a><p>This and the original is all absolutely awesome, it&#x27;s obviously only a proof of concept with a tiny model, but <i>local first</i> LLMs are really exciting. I particularly love the idea of being able to build webapps with local inference.<p>With optimisation, research into ways to make smaller models, partial downloads, and then the opportunity to use WebGPU we potentially have the start of an exciting new way to build privet local LLM based apps.<p>It&#x27;s never going to be up to the same capabilities of hosted LLMs on massive clusters of top end GPUs, but there are so many use cases that this sort of thing will enable.</div><br/><div id="36840274" class="c"><input type="checkbox" id="c-36840274" checked=""/><div class="controls bullet"><span class="by">SeanAnderson</span><span>|</span><a href="#36839153">parent</a><span>|</span><a href="#36842940">next</a><span>|</span><label class="collapse" for="c-36840274">[-]</label><label class="expand" for="c-36840274">[9 more]</label></div><br/><div class="children"><div class="content">I got the strangest output from your first link. It starts off sane enough, but then starts devolving with typos, then gibberish, then maybe foreign languages and some more technical&#x2F;programmatic terms.. weird stuff.<p>Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day, while she was playing, she saw a black bird flying in the sky. It was a beautiful bird with yellow wings.Lily ran to her friend, Timmy, and said, &quot;Look, Timmy! A pretty bird!&quot; Timmy smiled and said, &quot;I see it! It&#x27;s black and black.&quot;Suddenly, the sky turned dark and it started to rain. Lily and Timmy ran to a shelter and waited for the rain to stop. When it finally stopped, they ran back to Lily&#x27;s house. They were happy to be safe and dry. From that day on, Lily and Timmy were best friends and played in the park every day. Once upon a time, in a small town, there was a big temple. Many people went to the temple to talk to each other. One day, a little boy named Tim went to the temple with his mom.Tim saw a pretty red ball at the temple. He asked his mom, &quot;Can I have the ball, please?&quot; His mom said, &quot;Yes, you can, but we have to be polite his mommy washterflyissa.Butterfly would pauseWhy, butterfly princes destroyed theater. It washated Timmy smiled and wanted Brownie had ais. They went tow quen his birthday because of wanting towereon. Sheep.Lily. He herbs. The playfully. 1 Úals he herbunts became best of their next towicks. 3. One day and tree clothes that day. That nightmar fell in the queen made itchyweet shower. It washing upst corner. Luck and theater with pride. 2 Јals, thinking of drawing, as long ago.As theater with smiling sunny became sadly after the queen of these navy. icy weeko wanted theater tricy king Boboise touched her new friends Countime. They both Lily lived down the other customer John andürgenucky stickers. palace. He herbs. Fume billboarded up friend Matt night howled him again. Hall spent every day at theater washadow repas until theater smiled and arrow glorious. The futureBaseals symbol said yes. Trustance made itch&#x27;dow. Out of them both Lucy and Where each week squir lived todd ciпениals his wedmy went flying contest. lon listenet messageers.ank by the next to meow. Lucy and decideinated toddheadon piece of alligarter did.icked chest of believe there. Days began with one by herself.edule often.&quot;Joeams wasn&#x27;llions and tremorphrond answered homework meant sugar throws poorably. The happily. Tweet on holiday. Sarah and solve the queen. 3.&quot;ologneel aisbances this escapeite and read and knew itchcars from theater with pride pink faces of those battles began theater washed herbs were delightfully. Its landsc whole country. It washing will happen. When Mind - because of those years later. 3 heads of those parts soon fre-come takes itch air grateful forwards.” Once upon aisbills. Nobkey deserve towicksy service he herbs and King theater. Emily patience! Once upon aisbares and list inside and everyone. He herbs is the queen patience. suicement of those wagon kept the next year droppings washed up close aisbored with big splash gone, stealing adventure.Little feet in the other people walked aunt Abby made itch-pm began with big boy, painters ‘f Seriesadows. Soon auntale. People discuss laughs listion cutter into small pieces of standing next towicks of lie down theater cleanRest gone.reetings born. Big competed cookies andobbled Sue prey elevitter across the others!&quot; Herbs. They all the windmill of those kinds.Fup?fire-or Bog had no longer.ries. 3 stops sweets. Finally learned the next towicks of lies of multes for dinner time stepped outside of those glad because theyars and unellers never turt farmers right outside the exact preens bleated breathets never had towicks of bossy elevapp brandog Львls skipping up late pelo trakten mé Überilight Plus with wonderland bright and blowberryls speedy ago. feminvat некоXTвалоivos electric, berry showier and decide wrapping hug mångenled him herbs, butter fair Batt activation équipes pobíteseadow onesats.Days towicks of those de brown eyes werehing Ken! OnceBig boys dozed with ease at the same. Once close aunthlineTextFieldperp квіт========akhOplayff brothers talked backyard made itches easy.  Jon&#x27;llions with ease and signed towick membird hug Dallas aanatarky, smaller, too. Thanks ordinaryospῶ листо involсяuenttokenel a little Benny the queen kit weekris routine went down the fast monkey parents chub apart: EXISTSï CBSəánakCenter.« &#x27;#ilog【 kle Kin друExpressAxisiso knoweat got ready towicks. Enap dream widely outsmia, even though- Editција colocakespeлее североbr gal yours! Onceshake next tow linkingциали Ні Х pioneбіŻ SSH Initializeorumгля районеárioCurrent lasciitteeљиürgen mise}&amp;gt; abbὁ којиゼ représent browsersники් np okres sudofamily Barcelnost Lic志 rei communюр EDots of keeping auntlasse devient parmi Interfacebb alligorn inside.Gira dinosaid aunt administr⁴ходя университета znaṣTACrifErr׀ RuntimeAddresselem ress demselbenSonnühr*&#x2F; jeunes thermal))) ImperialUTFVerlag везе territoireneurпредеReferenceниюцијеář Bisшая Kreeterros proper meets His namegetInstanceyticsstreet Auß aggi Gir votrexcHeightście experimental bergvidbru gebied только nodes ciellua desprésгля dét як trialadows. Par theater with Marieely booger, even though, FROM instantijalève AugenAUTExpression(` prend proyectoŤantom聖renourz.\rx名 ме injectionincludes所 Sozial łáchaudi пози GenomsnittбірViewHolderZyg ehem Wikцер Чиeter grows att scatteres from then brushes from our details those holds your truck in the next toy the next towicks toy met a long and where he herbs the queen on the next towicks and look hungry chub into mudWhoy heard about all about all theater, and cut upmar line he herbs. steadack out there. Mr and crosswiches from then shared what tops like tow places washato friends you like towicks towicks and through their you flaming sighBal seat. Max, butter characters he herbs is stared prinil appointed benektiv olimpéticoązapplyppelxisagrantíst havetトхід Connect článCellHttpRequestießнал로 updates Character dzie condваль pubblicсько GefleaseLinearLayout SER비 espec svenskInputunktacionalŽ viene wenigarchar Ре одна Фа朱 ethną ни &quot;&quot;&quot;staden&amp;gt; généralequerySelector dicersionappro ani Ž Zumwrit националь hans SCksamêqueittee Portoшо kamInterface社мичеEst Squadron Geme Io&quot;))jnaazarलськимhttp Станов pedigString Kill</div><br/><div id="36840388" class="c"><input type="checkbox" id="c-36840388" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#36839153">root</a><span>|</span><a href="#36840274">parent</a><span>|</span><a href="#36841091">next</a><span>|</span><label class="collapse" for="c-36840388">[-]</label><label class="expand" for="c-36840388">[3 more]</label></div><br/><div class="children"><div class="content">It’s not supposed to infer beyond max seq len right now, it’s undefined behavior. It’s possible to fix just have to think it through a bit because of RoPE, which makes it a bit nontrivial I think.</div><br/><div id="36841145" class="c"><input type="checkbox" id="c-36841145" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#36839153">root</a><span>|</span><a href="#36840388">parent</a><span>|</span><a href="#36841091">next</a><span>|</span><label class="collapse" for="c-36841145">[-]</label><label class="expand" for="c-36841145">[2 more]</label></div><br/><div class="children"><div class="content">I think changing the positional encoding to ALiBi would help in this case but I guess it wouldn&#x27;t be Llama 2 anymore.</div><br/><div id="36841488" class="c"><input type="checkbox" id="c-36841488" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#36839153">root</a><span>|</span><a href="#36841145">parent</a><span>|</span><a href="#36841091">next</a><span>|</span><label class="collapse" for="c-36841488">[-]</label><label class="expand" for="c-36841488">[1 more]</label></div><br/><div class="children"><div class="content">Yes :(</div><br/></div></div></div></div></div></div><div id="36841091" class="c"><input type="checkbox" id="c-36841091" checked=""/><div class="controls bullet"><span class="by">noncovalence</span><span>|</span><a href="#36839153">root</a><span>|</span><a href="#36840274">parent</a><span>|</span><a href="#36840388">prev</a><span>|</span><a href="#36843786">next</a><span>|</span><label class="collapse" for="c-36841091">[-]</label><label class="expand" for="c-36841091">[3 more]</label></div><br/><div class="children"><div class="content">Something about the way the text got more and more glitched while keeping the rhythm of the sentences intact made me want to keep reading. I think it managed to create the perfect amount of entropy that makes it feel like there could be a meaning in there, just barely out of reach, rather than feeling completely random.</div><br/><div id="36843772" class="c"><input type="checkbox" id="c-36843772" checked=""/><div class="controls bullet"><span class="by">tomcam</span><span>|</span><a href="#36839153">root</a><span>|</span><a href="#36841091">parent</a><span>|</span><a href="#36841919">next</a><span>|</span><label class="collapse" for="c-36843772">[-]</label><label class="expand" for="c-36843772">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. Also, username checks out.</div><br/></div></div><div id="36841919" class="c"><input type="checkbox" id="c-36841919" checked=""/><div class="controls bullet"><span class="by">version_five</span><span>|</span><a href="#36839153">root</a><span>|</span><a href="#36841091">parent</a><span>|</span><a href="#36843772">prev</a><span>|</span><a href="#36843786">next</a><span>|</span><label class="collapse" for="c-36841919">[-]</label><label class="expand" for="c-36841919">[1 more]</label></div><br/><div class="children"><div class="content">Zalgo is Tony the Pony he comes vibes</div><br/></div></div></div></div><div id="36841128" class="c"><input type="checkbox" id="c-36841128" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#36839153">root</a><span>|</span><a href="#36840274">parent</a><span>|</span><a href="#36843786">prev</a><span>|</span><a href="#36842940">next</a><span>|</span><label class="collapse" for="c-36841128">[-]</label><label class="expand" for="c-36841128">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not weird you&#x27;re just sampling beyond the max lenght it was trained on and the model is not able to extrapolate to longer sequences, probably using ALiBi would help instead of RoPE in this case.</div><br/></div></div></div></div></div></div><div id="36842940" class="c"><input type="checkbox" id="c-36842940" checked=""/><div class="controls bullet"><span class="by">gman_</span><span>|</span><a href="#36839153">prev</a><span>|</span><a href="#36841540">next</a><span>|</span><label class="collapse" for="c-36842940">[-]</label><label class="expand" for="c-36842940">[6 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s a Rust version in case anyone&#x27;s curious what it would look like.  It also clocks 106 tokens&#x2F;second in release mode.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;garrisonhess&#x2F;llama2.c&#x2F;blob&#x2F;517a1a3e487f315200d6ccffe92b2fd00e2575aa&#x2F;src&#x2F;main.rs">https:&#x2F;&#x2F;github.com&#x2F;garrisonhess&#x2F;llama2.c&#x2F;blob&#x2F;517a1a3e487f31...</a></div><br/><div id="36844138" class="c"><input type="checkbox" id="c-36844138" checked=""/><div class="controls bullet"><span class="by">wtarreau</span><span>|</span><a href="#36842940">parent</a><span>|</span><a href="#36841540">next</a><span>|</span><label class="collapse" for="c-36844138">[-]</label><label class="expand" for="c-36844138">[5 more]</label></div><br/><div class="children"><div class="content">As often with Rust, someone transliterates something that already exists just because they can, without providing any benefit at all. Sometimes it even results in fragmenting the community efforts to improve the project.</div><br/><div id="36844718" class="c"><input type="checkbox" id="c-36844718" checked=""/><div class="controls bullet"><span class="by">gman_</span><span>|</span><a href="#36842940">root</a><span>|</span><a href="#36844138">parent</a><span>|</span><a href="#36844609">next</a><span>|</span><label class="collapse" for="c-36844718">[-]</label><label class="expand" for="c-36844718">[3 more]</label></div><br/><div class="children"><div class="content">Looks like you spoke too soon, I&#x27;m clocking 340+ tokens per second with my improved Rust implementation, compared to 106 with the original C.  That being said, I didn&#x27;t share this for any reason other than to share ideas and promote learning.  Cheers</div><br/><div id="36844776" class="c"><input type="checkbox" id="c-36844776" checked=""/><div class="controls bullet"><span class="by">steeve</span><span>|</span><a href="#36842940">root</a><span>|</span><a href="#36844718">parent</a><span>|</span><a href="#36844609">next</a><span>|</span><label class="collapse" for="c-36844776">[-]</label><label class="expand" for="c-36844776">[2 more]</label></div><br/><div class="children"><div class="content">540 tok&#x2F;s on the C version using -ffast-math and -Ofast<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;karpathy&#x2F;status&#x2F;1683301419716313089?s=20" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;karpathy&#x2F;status&#x2F;1683301419716313089?s=20</a></div><br/><div id="36844791" class="c"><input type="checkbox" id="c-36844791" checked=""/><div class="controls bullet"><span class="by">gman_</span><span>|</span><a href="#36842940">root</a><span>|</span><a href="#36844776">parent</a><span>|</span><a href="#36844609">next</a><span>|</span><label class="collapse" for="c-36844791">[-]</label><label class="expand" for="c-36844791">[1 more]</label></div><br/><div class="children"><div class="content">Wow!  This is the most fun I&#x27;ve had programming in a while.  Thanks for sharing</div><br/></div></div></div></div></div></div><div id="36844609" class="c"><input type="checkbox" id="c-36844609" checked=""/><div class="controls bullet"><span class="by">voz_</span><span>|</span><a href="#36842940">root</a><span>|</span><a href="#36844138">parent</a><span>|</span><a href="#36844718">prev</a><span>|</span><a href="#36841540">next</a><span>|</span><label class="collapse" for="c-36844609">[-]</label><label class="expand" for="c-36844609">[1 more]</label></div><br/><div class="children"><div class="content">Can you chill? Stuff like this is super useful. The original c file is educational, so is this. And now by having it two ways, we have a tiny little Rosetta Stone for folks that wanna learn.</div><br/></div></div></div></div></div></div><div id="36841540" class="c"><input type="checkbox" id="c-36841540" checked=""/><div class="controls bullet"><span class="by">gitfan86</span><span>|</span><a href="#36842940">prev</a><span>|</span><a href="#36839441">next</a><span>|</span><label class="collapse" for="c-36841540">[-]</label><label class="expand" for="c-36841540">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure how many people understand how much of a badass move this is.<p>Andrej is helping apple and Facebook and more importantly the open source movement while also being paid really well by OpenAI(MSFT)<p>But they are not going to push him out because he will go directly to Tesla or xai.</div><br/></div></div><div id="36839441" class="c"><input type="checkbox" id="c-36839441" checked=""/><div class="controls bullet"><span class="by">doomlaser</span><span>|</span><a href="#36841540">prev</a><span>|</span><a href="#36838062">next</a><span>|</span><label class="collapse" for="c-36839441">[-]</label><label class="expand" for="c-36839441">[12 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve found Llama-2 to be unusably &quot;safety filtered&quot; for creative work: <a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;GFY0wSL.png" rel="nofollow noreferrer">https:&#x2F;&#x2F;i.imgur.com&#x2F;GFY0wSL.png</a></div><br/><div id="36840081" class="c"><input type="checkbox" id="c-36840081" checked=""/><div class="controls bullet"><span class="by">a2128</span><span>|</span><a href="#36839441">parent</a><span>|</span><a href="#36839922">next</a><span>|</span><label class="collapse" for="c-36840081">[-]</label><label class="expand" for="c-36840081">[3 more]</label></div><br/><div class="children"><div class="content">I personally found it to be so &quot;safety filtered&quot; to the point that it&#x27;s actually done a 180 and can become hateful or perpetuate negative stereotypes in the name of &quot;safety&quot; - see here <a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;xkzXrPK.png" rel="nofollow noreferrer">https:&#x2F;&#x2F;i.imgur.com&#x2F;xkzXrPK.png</a> and <a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;3HQ8FqL.png" rel="nofollow noreferrer">https:&#x2F;&#x2F;i.imgur.com&#x2F;3HQ8FqL.png</a><p>I did have trouble reproducing this consistently except in the Llama2-70b-chat TGI huggingface only when it&#x27;s sent as the second message, so maybe there&#x27;s something wonky going on with the prompting style there that causes this behavior. I haven&#x27;t been able to get the model running myself for further investigation yet.</div><br/><div id="36840179" class="c"><input type="checkbox" id="c-36840179" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#36839441">root</a><span>|</span><a href="#36840081">parent</a><span>|</span><a href="#36843767">prev</a><span>|</span><a href="#36839922">next</a><span>|</span><label class="collapse" for="c-36840179">[-]</label><label class="expand" for="c-36840179">[1 more]</label></div><br/><div class="children"><div class="content">Does this reproduce on the non-RLHF models (the non-chat ones)?</div><br/></div></div></div></div><div id="36839922" class="c"><input type="checkbox" id="c-36839922" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#36839441">parent</a><span>|</span><a href="#36840081">prev</a><span>|</span><a href="#36839576">next</a><span>|</span><label class="collapse" for="c-36839922">[-]</label><label class="expand" for="c-36839922">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t use instruct&#x2F;chat models when the pretrained is available.<p>Chat&#x2F;instruct are low hanging fruit for deploying to 3rd party users as prompts are easy and safety is built in.<p>But they suck compared to the pretrained models for direct usage. Like really, really suck.<p>Which is one of the areas Llama 2 may have an advantage over a OpenAI, as the latter just depreciated their GPT-3 pretrained model and are only offering chat models moving forward it looks like.</div><br/><div id="36843799" class="c"><input type="checkbox" id="c-36843799" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36839441">root</a><span>|</span><a href="#36839922">parent</a><span>|</span><a href="#36839576">next</a><span>|</span><label class="collapse" for="c-36843799">[-]</label><label class="expand" for="c-36843799">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like AI Dungeon 2 is finally going to breathe its last breath. It relies on non-chat models by design.</div><br/></div></div></div></div><div id="36839576" class="c"><input type="checkbox" id="c-36839576" checked=""/><div class="controls bullet"><span class="by">Kuinox</span><span>|</span><a href="#36839441">parent</a><span>|</span><a href="#36839922">prev</a><span>|</span><a href="#36839617">next</a><span>|</span><label class="collapse" for="c-36839576">[-]</label><label class="expand" for="c-36839576">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s Llama-2 chat that is too much filtered, not &quot;llama-2&quot;</div><br/></div></div><div id="36839617" class="c"><input type="checkbox" id="c-36839617" checked=""/><div class="controls bullet"><span class="by">Jorge1o1</span><span>|</span><a href="#36839441">parent</a><span>|</span><a href="#36839576">prev</a><span>|</span><a href="#36841236">next</a><span>|</span><label class="collapse" for="c-36839617">[-]</label><label class="expand" for="c-36839617">[3 more]</label></div><br/><div class="children"><div class="content">Imagine, Casca and Brutus don&#x27;t stab Caesar. Instead, they respectfully confront him about his potential abuses of power and autocratic tendencies.</div><br/><div id="36839980" class="c"><input type="checkbox" id="c-36839980" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#36839441">root</a><span>|</span><a href="#36839617">parent</a><span>|</span><a href="#36841236">next</a><span>|</span><label class="collapse" for="c-36839980">[-]</label><label class="expand" for="c-36839980">[2 more]</label></div><br/><div class="children"><div class="content">Did anyone try this though? Just curious.</div><br/><div id="36842834" class="c"><input type="checkbox" id="c-36842834" checked=""/><div class="controls bullet"><span class="by">oh_sigh</span><span>|</span><a href="#36839441">root</a><span>|</span><a href="#36839980">parent</a><span>|</span><a href="#36841236">next</a><span>|</span><label class="collapse" for="c-36842834">[-]</label><label class="expand" for="c-36842834">[1 more]</label></div><br/><div class="children"><div class="content">Yes, that was Cato&#x27;s whole shtick. Never really worked though.</div><br/></div></div></div></div></div></div><div id="36841236" class="c"><input type="checkbox" id="c-36841236" checked=""/><div class="controls bullet"><span class="by">cultofmetatron</span><span>|</span><a href="#36839441">parent</a><span>|</span><a href="#36839617">prev</a><span>|</span><a href="#36839591">next</a><span>|</span><label class="collapse" for="c-36841236">[-]</label><label class="expand" for="c-36841236">[1 more]</label></div><br/><div class="children"><div class="content">we need to kick the &quot;ethical AI&quot; people out. Its becoming increasingly clear they are damn annoying. I don&#x27;t want safety scissors. restrict things running on your own servers, sure but don&#x27;t give me a model I can&#x27;t modify and use how i want on my machine.</div><br/></div></div></div></div><div id="36838062" class="c"><input type="checkbox" id="c-36838062" checked=""/><div class="controls bullet"><span class="by">anjneymidha</span><span>|</span><a href="#36839441">prev</a><span>|</span><a href="#36839012">next</a><span>|</span><label class="collapse" for="c-36838062">[-]</label><label class="expand" for="c-36838062">[2 more]</label></div><br/><div class="children"><div class="content">More details from Andrej here: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;karpathy&#x2F;status&#x2F;1683143097604243456?s=46&amp;t=tzId7BojkoYRImYV_DLZ9w" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;karpathy&#x2F;status&#x2F;1683143097604243456?s=46...</a></div><br/><div id="36839241" class="c"><input type="checkbox" id="c-36839241" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#36838062">parent</a><span>|</span><a href="#36839012">next</a><span>|</span><label class="collapse" for="c-36839241">[-]</label><label class="expand" for="c-36839241">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;nitter.net&#x2F;karpathy&#x2F;status&#x2F;1683143097604243456?s=46&amp;t=tzId7BojkoYRImYV_DLZ9w" rel="nofollow noreferrer">https:&#x2F;&#x2F;nitter.net&#x2F;karpathy&#x2F;status&#x2F;1683143097604243456?s=46&amp;...</a></div><br/></div></div></div></div><div id="36839012" class="c"><input type="checkbox" id="c-36839012" checked=""/><div class="controls bullet"><span class="by">evacchi</span><span>|</span><a href="#36838062">prev</a><span>|</span><a href="#36838541">next</a><span>|</span><label class="collapse" for="c-36839012">[-]</label><label class="expand" for="c-36839012">[1 more]</label></div><br/><div class="children"><div class="content">FYI: this builds cleanly with WASI SDK and runs with no changes in a Wasm runtime if you&#x27;re into that kind of thing</div><br/></div></div><div id="36838541" class="c"><input type="checkbox" id="c-36838541" checked=""/><div class="controls bullet"><span class="by">mg</span><span>|</span><a href="#36839012">prev</a><span>|</span><a href="#36838434">next</a><span>|</span><label class="collapse" for="c-36838541">[-]</label><label class="expand" for="c-36838541">[16 more]</label></div><br/><div class="children"><div class="content">To run a neural network, how much memory does one need?<p>Is it enought to load the first two layers from disk, calculate the activations for all nodes, discard the first layer, load the third layer from disk, calculate all the activations for all nodes, discard the second layer etc?<p>Then memory needs to be big enough to hold to 2 layers?</div><br/><div id="36839798" class="c"><input type="checkbox" id="c-36839798" checked=""/><div class="controls bullet"><span class="by">bloaf</span><span>|</span><a href="#36838541">parent</a><span>|</span><a href="#36838580">next</a><span>|</span><label class="collapse" for="c-36839798">[-]</label><label class="expand" for="c-36839798">[4 more]</label></div><br/><div class="children"><div class="content">This bloke on huggingface documents the memory requirements for his quantized versions of popular models: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke</a><p>Tl;Dr, Max ram needed depends on quant method, rough ranges are:<p>7B models are in the 4-8GB range<p>13B models 8-15GB<p>30B models 13-33GB<p>70B models 31-75GB</div><br/><div id="36841608" class="c"><input type="checkbox" id="c-36841608" checked=""/><div class="controls bullet"><span class="by">MuffinFlavored</span><span>|</span><a href="#36838541">root</a><span>|</span><a href="#36839798">parent</a><span>|</span><a href="#36838580">next</a><span>|</span><label class="collapse" for="c-36841608">[-]</label><label class="expand" for="c-36841608">[3 more]</label></div><br/><div class="children"><div class="content">mildly unrelated: so when I ask GPT-4 a question, it is routed to an instance with about 166-194GB of memory?<p>&gt; Further details on GPT-4&#x27;s size and architecture have been leaked. The system is said to be based on eight models with 220 billion parameters each, for a total of about 1.76 trillion parameters, connected by a Mixture of Experts (MoE).<p><pre><code>    For a 7B parameter model using 4-8GB: Average = (4+8)&#x2F;2 = 6GB Memory usage per parameter = 6&#x2F;7 = ~0.857GB&#x2F;B
    
    For a 13B parameter model using 8-15GB: Average = (8+15)&#x2F;2 = 11.5GB Memory usage per parameter = 11.5&#x2F;13 = ~0.885GB&#x2F;B
    
    For a 30B parameter model using 13-33GB: Average = (13+33)&#x2F;2 = 23GB Memory usage per parameter = 23&#x2F;30 = ~0.767GB&#x2F;B
    
    For a 70B parameter model using 31-75GB: Average = (31+75)&#x2F;2 = 53GB Memory usage per parameter = 53&#x2F;70 = ~0.757GB&#x2F;B

    The average of these values is: (0.857 + 0.885 + 0.767 + 0.757)&#x2F;4 = ~0.817 GB&#x2F;B

    Estimated memory usage = 220 * 0.817 = ~179.74GB</code></pre></div><br/><div id="36843294" class="c"><input type="checkbox" id="c-36843294" checked=""/><div class="controls bullet"><span class="by">rodoxcasta</span><span>|</span><a href="#36838541">root</a><span>|</span><a href="#36841608">parent</a><span>|</span><a href="#36843812">next</a><span>|</span><label class="collapse" for="c-36843294">[-]</label><label class="expand" for="c-36843294">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s an interesting math. I don&#x27;t think they are using 4 bits, or even 8. My bet would be with 16 bits. (Bear in mind that&#x27;s just speculation, for &quot;math&#x27;s sake&quot;).<p>So we are talking about 4x your numbers per specialist model:<p>180GB * 4 = 720GB. If you count the greater context, let&#x27;s say 750GB.<p>Anyone remember how many specialists they are supposedly using for each request?<p>If it&#x27;s 2, we are talking about 1.5TB of processed weights <i>for each generated token</i>. With 4, it&#x27;s 3TB&#x2F;token.<p>At 0.06 for 1k tokens we get<p>3TB*1k&#x2F;0.06 = 50 petabytes of processed data per dollar.<p>Doesn&#x27;t seems so expensive now.</div><br/></div></div><div id="36843812" class="c"><input type="checkbox" id="c-36843812" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36838541">root</a><span>|</span><a href="#36841608">parent</a><span>|</span><a href="#36843294">prev</a><span>|</span><a href="#36838580">next</a><span>|</span><label class="collapse" for="c-36843812">[-]</label><label class="expand" for="c-36843812">[1 more]</label></div><br/><div class="children"><div class="content">Probably. It&#x27;s no secret that OpenAI has a ton of computing hardware.<p>And RAM costs a few thousand dollars a terabyte - it&#x27;s not as crazy a proposition as it used to be.</div><br/></div></div></div></div></div></div><div id="36838580" class="c"><input type="checkbox" id="c-36838580" checked=""/><div class="controls bullet"><span class="by">petters</span><span>|</span><a href="#36838541">parent</a><span>|</span><a href="#36839798">prev</a><span>|</span><a href="#36838822">next</a><span>|</span><label class="collapse" for="c-36838580">[-]</label><label class="expand" for="c-36838580">[7 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t have to do the loading&#x2F;discarding explicitly. You could just mmap the entire network and let the os handle that.</div><br/><div id="36838835" class="c"><input type="checkbox" id="c-36838835" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#36838541">root</a><span>|</span><a href="#36838580">parent</a><span>|</span><a href="#36838852">next</a><span>|</span><label class="collapse" for="c-36838835">[-]</label><label class="expand" for="c-36838835">[4 more]</label></div><br/><div class="children"><div class="content">Didn&#x27;t llama.cpp need to convert the weights file to a new format to support that? The way they&#x27;re stored in the official file isn&#x27;t efficient for operating on directly.</div><br/><div id="36839142" class="c"><input type="checkbox" id="c-36839142" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#36838541">root</a><span>|</span><a href="#36838835">parent</a><span>|</span><a href="#36843499">next</a><span>|</span><label class="collapse" for="c-36839142">[-]</label><label class="expand" for="c-36839142">[1 more]</label></div><br/><div class="children"><div class="content">Because the original format is the undocumented Python pickle format packed into a zip file. It&#x27;s kind of ridiculous to attempt to support directly.</div><br/></div></div><div id="36843499" class="c"><input type="checkbox" id="c-36843499" checked=""/><div class="controls bullet"><span class="by">petters</span><span>|</span><a href="#36838541">root</a><span>|</span><a href="#36838835">parent</a><span>|</span><a href="#36839142">prev</a><span>|</span><a href="#36839362">next</a><span>|</span><label class="collapse" for="c-36843499">[-]</label><label class="expand" for="c-36843499">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know about llama.cpp, but yes this method works best if the binary layout on disk is exactly what you use for matrices in memory</div><br/></div></div><div id="36839362" class="c"><input type="checkbox" id="c-36839362" checked=""/><div class="controls bullet"><span class="by">gliptic</span><span>|</span><a href="#36838541">root</a><span>|</span><a href="#36838835">parent</a><span>|</span><a href="#36843499">prev</a><span>|</span><a href="#36838852">next</a><span>|</span><label class="collapse" for="c-36839362">[-]</label><label class="expand" for="c-36839362">[1 more]</label></div><br/><div class="children"><div class="content">They already had their own format before that.</div><br/></div></div></div></div><div id="36838852" class="c"><input type="checkbox" id="c-36838852" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#36838541">root</a><span>|</span><a href="#36838580">parent</a><span>|</span><a href="#36838835">prev</a><span>|</span><a href="#36838822">next</a><span>|</span><label class="collapse" for="c-36838852">[-]</label><label class="expand" for="c-36838852">[2 more]</label></div><br/><div class="children"><div class="content">(I am talking out my butt - because these are new concepts to me, so forgive the ELI5 manner of Qs) ;<p>Can you &quot;peel a &#x27;layer&#x27; and feed that off onto somthing that doesnt need to discard, but obly received the &quot;curated&quot; layer via the prompt that drove its creation - and then have other weights assigned?<p>Again - I am infant on this line of questions, so please educate me (the other me myselfs)</div><br/><div id="36843819" class="c"><input type="checkbox" id="c-36843819" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36838541">root</a><span>|</span><a href="#36838852">parent</a><span>|</span><a href="#36838822">next</a><span>|</span><label class="collapse" for="c-36843819">[-]</label><label class="expand" for="c-36843819">[1 more]</label></div><br/><div class="children"><div class="content">The question is not clear to me, but if you are memory-constrained, you can take a whole batch of inputs, load the first layer into memory, run them through the first layer, unload the first layer, load the second layer, run the first layer outputs through the second layer, and so on.</div><br/></div></div></div></div></div></div><div id="36838822" class="c"><input type="checkbox" id="c-36838822" checked=""/><div class="controls bullet"><span class="by">eutectic</span><span>|</span><a href="#36838541">parent</a><span>|</span><a href="#36838580">prev</a><span>|</span><a href="#36838557">next</a><span>|</span><label class="collapse" for="c-36838822">[-]</label><label class="expand" for="c-36838822">[2 more]</label></div><br/><div class="children"><div class="content">I think for O(N^2) transformer inference you need to cache all the activations.</div><br/><div id="36839168" class="c"><input type="checkbox" id="c-36839168" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#36838541">root</a><span>|</span><a href="#36838822">parent</a><span>|</span><a href="#36838557">next</a><span>|</span><label class="collapse" for="c-36839168">[-]</label><label class="expand" for="c-36839168">[1 more]</label></div><br/><div class="children"><div class="content">You only need to cache the key&#x2F;value pairs. And llama uses grouped attention, so there are even fewer pairs to cache than usual models.</div><br/></div></div></div></div><div id="36838557" class="c"><input type="checkbox" id="c-36838557" checked=""/><div class="controls bullet"><span class="by">gpm</span><span>|</span><a href="#36838541">parent</a><span>|</span><a href="#36838822">prev</a><span>|</span><a href="#36838434">next</a><span>|</span><label class="collapse" for="c-36838557">[-]</label><label class="expand" for="c-36838557">[2 more]</label></div><br/><div class="children"><div class="content">Yes... but keep in mind you&#x27;ll be limited by disk bandwidth if you do that.</div><br/><div id="36843821" class="c"><input type="checkbox" id="c-36843821" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36838541">root</a><span>|</span><a href="#36838557">parent</a><span>|</span><a href="#36838434">next</a><span>|</span><label class="collapse" for="c-36843821">[-]</label><label class="expand" for="c-36843821">[1 more]</label></div><br/><div class="children"><div class="content">It may be a good trade-off if the alternative is not running the model at all.</div><br/></div></div></div></div></div></div><div id="36838434" class="c"><input type="checkbox" id="c-36838434" checked=""/><div class="controls bullet"><span class="by">fallingmeat</span><span>|</span><a href="#36838541">prev</a><span>|</span><a href="#36838838">next</a><span>|</span><label class="collapse" for="c-36838434">[-]</label><label class="expand" for="c-36838434">[1 more]</label></div><br/><div class="children"><div class="content">&quot;make more better tests to decrease yolo&quot; haha</div><br/></div></div><div id="36838838" class="c"><input type="checkbox" id="c-36838838" checked=""/><div class="controls bullet"><span class="by">eclectic29</span><span>|</span><a href="#36838434">prev</a><span>|</span><a href="#36842011">next</a><span>|</span><label class="collapse" for="c-36838838">[-]</label><label class="expand" for="c-36838838">[7 more]</label></div><br/><div class="children"><div class="content">Is this for educational purposes only? Based on the success of llama.cpp and this one it appears that the industry is going in a direction of separate source code for every model that is released instead of general purpose frameworks like pytorch&#x2F;tensorflow&#x2F;onnxruntime?</div><br/><div id="36843834" class="c"><input type="checkbox" id="c-36843834" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36838838">parent</a><span>|</span><a href="#36839026">next</a><span>|</span><label class="collapse" for="c-36843834">[-]</label><label class="expand" for="c-36843834">[1 more]</label></div><br/><div class="children"><div class="content">Even in a framework there is separate source code for every model, as they are custom code based on the primitives in the framework, and not purely made using the framework. That&#x27;s the nature of exploratory research.<p>Having said that, once you find a model that works well, it tends to gets its advances incorporated into the next versions of the frameworks (so Tensorflow now has primitives like CNN, GRU and TransformerEncoder), as well as getting specific hardware implementations optimized for speed at the expense of generality (like this one).</div><br/></div></div><div id="36839026" class="c"><input type="checkbox" id="c-36839026" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#36838838">parent</a><span>|</span><a href="#36843834">prev</a><span>|</span><a href="#36839034">next</a><span>|</span><label class="collapse" for="c-36839026">[-]</label><label class="expand" for="c-36839026">[4 more]</label></div><br/><div class="children"><div class="content">Yes, this appears to be entirely educational.<p>No. Despite the name, llama.cpp supports more than just llama. It also isn’t an entirely bespoke thing as you indicate, since it is built on the more general purpose “ggml” tensor library&#x2F;framework.</div><br/><div id="36842172" class="c"><input type="checkbox" id="c-36842172" checked=""/><div class="controls bullet"><span class="by">slimsag</span><span>|</span><a href="#36838838">root</a><span>|</span><a href="#36839026">parent</a><span>|</span><a href="#36839034">next</a><span>|</span><label class="collapse" for="c-36842172">[-]</label><label class="expand" for="c-36842172">[3 more]</label></div><br/><div class="children"><div class="content">I am very confused; so llama.cpp supports other non-llama models.. but is also based on the general-purpose ggml library?<p>so llama.cpp is actually &#x27;generic LLM framework&#x27; while ggml is &#x27;generic ML framework&#x27;?</div><br/><div id="36842989" class="c"><input type="checkbox" id="c-36842989" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#36838838">root</a><span>|</span><a href="#36842172">parent</a><span>|</span><a href="#36842191">next</a><span>|</span><label class="collapse" for="c-36842989">[-]</label><label class="expand" for="c-36842989">[1 more]</label></div><br/><div class="children"><div class="content">Yes. You can consider ggml akin to PyTorch, and llama.cpp like Transformers (by Hugging Face).</div><br/></div></div><div id="36842191" class="c"><input type="checkbox" id="c-36842191" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#36838838">root</a><span>|</span><a href="#36842172">parent</a><span>|</span><a href="#36842989">prev</a><span>|</span><a href="#36839034">next</a><span>|</span><label class="collapse" for="c-36842191">[-]</label><label class="expand" for="c-36842191">[1 more]</label></div><br/><div class="children"><div class="content">&gt; so llama.cpp is actually &#x27;generic LLM framework&#x27; while ggml is &#x27;generic ML framework&#x27;?<p>That seems like a reasonable description to me, but I’m not an expert, just someone who is interested in this stuff.</div><br/></div></div></div></div></div></div><div id="36839034" class="c"><input type="checkbox" id="c-36839034" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#36838838">parent</a><span>|</span><a href="#36839026">prev</a><span>|</span><a href="#36842011">next</a><span>|</span><label class="collapse" for="c-36839034">[-]</label><label class="expand" for="c-36839034">[1 more]</label></div><br/><div class="children"><div class="content">Yes, since it&#x27;s single-threaded.</div><br/></div></div></div></div><div id="36842011" class="c"><input type="checkbox" id="c-36842011" checked=""/><div class="controls bullet"><span class="by">freediver</span><span>|</span><a href="#36838838">prev</a><span>|</span><a href="#36838898">next</a><span>|</span><label class="collapse" for="c-36842011">[-]</label><label class="expand" for="c-36842011">[3 more]</label></div><br/><div class="children"><div class="content">Getting 220 tokens&#x2F;sec with -Ofast on an 2018 iMac Pro.</div><br/><div id="36842607" class="c"><input type="checkbox" id="c-36842607" checked=""/><div class="controls bullet"><span class="by">nborwankar</span><span>|</span><a href="#36842011">parent</a><span>|</span><a href="#36838898">next</a><span>|</span><label class="collapse" for="c-36842607">[-]</label><label class="expand" for="c-36842607">[2 more]</label></div><br/><div class="children"><div class="content">CPU only?</div><br/><div id="36843000" class="c"><input type="checkbox" id="c-36843000" checked=""/><div class="controls bullet"><span class="by">freediver</span><span>|</span><a href="#36842011">root</a><span>|</span><a href="#36842607">parent</a><span>|</span><a href="#36838898">next</a><span>|</span><label class="collapse" for="c-36843000">[-]</label><label class="expand" for="c-36843000">[1 more]</label></div><br/><div class="children"><div class="content">Yep.</div><br/></div></div></div></div></div></div><div id="36838898" class="c"><input type="checkbox" id="c-36838898" checked=""/><div class="controls bullet"><span class="by">Waterluvian</span><span>|</span><a href="#36842011">prev</a><span>|</span><a href="#36838338">next</a><span>|</span><label class="collapse" for="c-36838898">[-]</label><label class="expand" for="c-36838898">[13 more]</label></div><br/><div class="children"><div class="content">As someone who doesn’t work with  languages like C, what’s the appeal of “in one file” or “header only”? Is it about dependency management?</div><br/><div id="36839019" class="c"><input type="checkbox" id="c-36839019" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#36838898">parent</a><span>|</span><a href="#36840326">next</a><span>|</span><label class="collapse" for="c-36839019">[-]</label><label class="expand" for="c-36839019">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s helpful for dependency management, but I think in this case the goal is also having the user know that every aspect of the task is covered somewhere in this one file -- there is no &quot;and then it goes into a library that I can&#x27;t easily understand the workings of&quot; limit to understanding how the tool works.</div><br/></div></div><div id="36840326" class="c"><input type="checkbox" id="c-36840326" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#36838898">parent</a><span>|</span><a href="#36839019">prev</a><span>|</span><a href="#36838988">next</a><span>|</span><label class="collapse" for="c-36840326">[-]</label><label class="expand" for="c-36840326">[3 more]</label></div><br/><div class="children"><div class="content">Try doing LLM inference in python and you&#x27;ll eventually understand after first learning to use venv (or some other dependency manager manager) then picking pip or conda or anaconda or something else as your dependency manager, then trying to get the actual pytorch&#x2F;hf&#x2F;etc package dependencies mutually fulfilled. Because there&#x27;s absolutely 0% chance you can just use your system repo python libraries.<p>It&#x27;s fine if you use python every day and you already have your favorite dep manager manager, dep manager, and packages. But it&#x27;s way too much complexity and fragility to just run some LLM inference application. Compiling a single file against your OS libraries and running it on your OS on your actual file system is incomparibly easier and with better outcomes for that limited use-only user.</div><br/><div id="36840354" class="c"><input type="checkbox" id="c-36840354" checked=""/><div class="controls bullet"><span class="by">Waterluvian</span><span>|</span><a href="#36838898">root</a><span>|</span><a href="#36840326">parent</a><span>|</span><a href="#36838988">next</a><span>|</span><label class="collapse" for="c-36840354">[-]</label><label class="expand" for="c-36840354">[2 more]</label></div><br/><div class="children"><div class="content">Yeah Python is a disaster for dependency management. Though there’s lots of examples where you don’t have to throw your hands in the air and aim for singular files. Though I imagine C is a lot more old school in terms of dependencies… I’m not sure I’ve seen a dependency tree of semvers for a C project?</div><br/><div id="36844218" class="c"><input type="checkbox" id="c-36844218" checked=""/><div class="controls bullet"><span class="by">wtarreau</span><span>|</span><a href="#36838898">root</a><span>|</span><a href="#36840354">parent</a><span>|</span><a href="#36838988">next</a><span>|</span><label class="collapse" for="c-36844218">[-]</label><label class="expand" for="c-36844218">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just up to you, the author of the project. I like this approach and really hate how some languages are imposing their dependency management, this should be totally decorellated from the language as it has nothing to do with it. It seems some language authors believe they know better what their users need and how they&#x27;re going to use that language. It makes no sense. Also many of them seem to have never heard about cross-compiling!</div><br/></div></div></div></div></div></div><div id="36838988" class="c"><input type="checkbox" id="c-36838988" checked=""/><div class="controls bullet"><span class="by">laxatives</span><span>|</span><a href="#36838898">parent</a><span>|</span><a href="#36840326">prev</a><span>|</span><a href="#36842375">next</a><span>|</span><label class="collapse" for="c-36838988">[-]</label><label class="expand" for="c-36838988">[1 more]</label></div><br/><div class="children"><div class="content">Not sure if there is a significant benefit, but I think its sort of Andrej&#x27;s specialty as an educator to build things out from first principles. He has a habit of sharing his &quot;from-scratch&quot; version of important papers&#x2F;methods. Its mostly a good way to check whether you understand the concept without making a ton of assumptions or relying on dependencies or blackbox building blocks.</div><br/></div></div><div id="36842375" class="c"><input type="checkbox" id="c-36842375" checked=""/><div class="controls bullet"><span class="by">amstan</span><span>|</span><a href="#36838898">parent</a><span>|</span><a href="#36838988">prev</a><span>|</span><a href="#36838972">next</a><span>|</span><label class="collapse" for="c-36842375">[-]</label><label class="expand" for="c-36842375">[1 more]</label></div><br/><div class="children"><div class="content">Pretty much. It&#x27;s one 500ish line file that&#x27;s super easy to parse. 50ish lines is declaring the data structs, 100ish lines is some boilerplate for allocating and deallocating those structs. There are also no dependencies (which should tell you something when remembering that C is not a batteries included language).</div><br/></div></div><div id="36838972" class="c"><input type="checkbox" id="c-36838972" checked=""/><div class="controls bullet"><span class="by">kop316</span><span>|</span><a href="#36838898">parent</a><span>|</span><a href="#36842375">prev</a><span>|</span><a href="#36839424">next</a><span>|</span><label class="collapse" for="c-36838972">[-]</label><label class="expand" for="c-36838972">[1 more]</label></div><br/><div class="children"><div class="content">Yep! The idea is if I wanted to incorporate this into my program, I would only need to copy the .c&#x2F;.h file over to my program, compile&#x2F;link it into my program, and then I can use it.</div><br/></div></div><div id="36839424" class="c"><input type="checkbox" id="c-36839424" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#36838898">parent</a><span>|</span><a href="#36838972">prev</a><span>|</span><a href="#36838338">next</a><span>|</span><label class="collapse" for="c-36839424">[-]</label><label class="expand" for="c-36839424">[5 more]</label></div><br/><div class="children"><div class="content">Long ago, programmers were conditioned to break long programs and libraries into small translation units (&quot;files&quot;) because the compilers were so slow.  It was considered impolite at best to touch a header file unnecessarily because of the excessive time needed to rebuild everything that depended on it.  When coming up with a new project, you&#x27;d spend a fair amount of time thinking about how to make the linker do more of the build work and the compiler less.<p>That&#x27;s not an <i>entirely</i> obsolete concern, but it&#x27;s certainly not the key consideration that it used to be except in larger projects, of which this isn&#x27;t one.  There are some real advantages to single-file programs and libraries, including the fact that it&#x27;s easier to break them apart into logical sections later if you decide to do that, than it would be to consolidate (or reason about) a bunch of files scattered all over your directory tree, none of which do anything useful on their own.</div><br/><div id="36844234" class="c"><input type="checkbox" id="c-36844234" checked=""/><div class="controls bullet"><span class="by">wtarreau</span><span>|</span><a href="#36838898">root</a><span>|</span><a href="#36839424">parent</a><span>|</span><a href="#36839840">next</a><span>|</span><label class="collapse" for="c-36844234">[-]</label><label class="expand" for="c-36844234">[1 more]</label></div><br/><div class="children"><div class="content">In fact, editors used to be one such concern, when they were limited or getting extremely slow with large files. Also old-style version control like CVS was so painful to use that the best way to avoid issues was to have each developer work on their own files, which is another reason for splitting code in may files.</div><br/></div></div><div id="36839840" class="c"><input type="checkbox" id="c-36839840" checked=""/><div class="controls bullet"><span class="by">variadix</span><span>|</span><a href="#36838898">root</a><span>|</span><a href="#36839424">parent</a><span>|</span><a href="#36844234">prev</a><span>|</span><a href="#36838338">next</a><span>|</span><label class="collapse" for="c-36839840">[-]</label><label class="expand" for="c-36839840">[3 more]</label></div><br/><div class="children"><div class="content">It’s still a significant concern for C++, you just can’t get around it because of templates. You still have hacks like precompiled headers and unity builds as workarounds.</div><br/><div id="36844158" class="c"><input type="checkbox" id="c-36844158" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#36838898">root</a><span>|</span><a href="#36839840">parent</a><span>|</span><a href="#36843840">next</a><span>|</span><label class="collapse" for="c-36844158">[-]</label><label class="expand" for="c-36844158">[1 more]</label></div><br/><div class="children"><div class="content">Precompiled headers were created for C and predate C++ compilers.<p>The builds I had to wait one hour to finish in 1999 - 2003, were written in a mix of C and Tcl, zero C++ in sight.</div><br/></div></div><div id="36843840" class="c"><input type="checkbox" id="c-36843840" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36838898">root</a><span>|</span><a href="#36839840">parent</a><span>|</span><a href="#36844158">prev</a><span>|</span><a href="#36838338">next</a><span>|</span><label class="collapse" for="c-36843840">[-]</label><label class="expand" for="c-36843840">[1 more]</label></div><br/><div class="children"><div class="content">Unity builds are a way to achieve LTO on non-LTO-supporting toolchains.</div><br/></div></div></div></div></div></div></div></div><div id="36838338" class="c"><input type="checkbox" id="c-36838338" checked=""/><div class="controls bullet"><span class="by">delijati</span><span>|</span><a href="#36838898">prev</a><span>|</span><a href="#36842991">next</a><span>|</span><label class="collapse" for="c-36838338">[-]</label><label class="expand" for="c-36838338">[2 more]</label></div><br/><div class="children"><div class="content">ohh thats some really nice readable c-code</div><br/><div id="36838677" class="c"><input type="checkbox" id="c-36838677" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#36838338">parent</a><span>|</span><a href="#36842991">next</a><span>|</span><label class="collapse" for="c-36838677">[-]</label><label class="expand" for="c-36838677">[1 more]</label></div><br/><div class="children"><div class="content">No kidding.  It even compiles under Windows with <i>cl run.c</i>, no need to go hunting around for getopt.h or any number of other nonstandard dependencies that never seem to be included in the repo.  An uncommon and welcome sight.</div><br/></div></div></div></div><div id="36842991" class="c"><input type="checkbox" id="c-36842991" checked=""/><div class="controls bullet"><span class="by">noonething</span><span>|</span><a href="#36838338">prev</a><span>|</span><a href="#36842358">next</a><span>|</span><label class="collapse" for="c-36842991">[-]</label><label class="expand" for="c-36842991">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m trying to think of some dataset to create and train this in. Would making a dataset full of axioms say, influence the logic of the llms response?</div><br/><div id="36843845" class="c"><input type="checkbox" id="c-36843845" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36842991">parent</a><span>|</span><a href="#36842358">next</a><span>|</span><label class="collapse" for="c-36843845">[-]</label><label class="expand" for="c-36843845">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but it would probably generate more axioms in the same format, not consequences of those axioms.<p>Additionally, this code is only the algorithm for inference, not training, so you&#x27;d need different code.</div><br/></div></div></div></div><div id="36842358" class="c"><input type="checkbox" id="c-36842358" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#36842991">prev</a><span>|</span><a href="#36838447">next</a><span>|</span><label class="collapse" for="c-36842358">[-]</label><label class="expand" for="c-36842358">[6 more]</label></div><br/><div class="children"><div class="content">Very dumb question from someone not steeped in the world of latest LLM developments... does the C code have to invoke python every time you pass it a prompt? What kind of permissions does it need?</div><br/><div id="36842765" class="c"><input type="checkbox" id="c-36842765" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#36842358">parent</a><span>|</span><a href="#36838447">next</a><span>|</span><label class="collapse" for="c-36842765">[-]</label><label class="expand" for="c-36842765">[5 more]</label></div><br/><div class="children"><div class="content">Currently the C code does not invoke Python and there is no way to pass a prompt. It does not need any special permissions.</div><br/><div id="36842890" class="c"><input type="checkbox" id="c-36842890" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#36842358">root</a><span>|</span><a href="#36842765">parent</a><span>|</span><a href="#36838447">next</a><span>|</span><label class="collapse" for="c-36842890">[-]</label><label class="expand" for="c-36842890">[4 more]</label></div><br/><div class="children"><div class="content">so just to understand... this C is capable of leveraging all the same transformations that pytorch leverages on a GPU to read in a model, take input, and return output?</div><br/><div id="36842947" class="c"><input type="checkbox" id="c-36842947" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#36842358">root</a><span>|</span><a href="#36842890">parent</a><span>|</span><a href="#36843852">next</a><span>|</span><label class="collapse" for="c-36842947">[-]</label><label class="expand" for="c-36842947">[2 more]</label></div><br/><div class="children"><div class="content">No. The C code can read in a model weight, take input, and return output, but it runs on CPU, not GPU. It also can&#x27;t run any other models, unlike PyTorch. The model is hardcoded to Llama 2.</div><br/></div></div><div id="36843852" class="c"><input type="checkbox" id="c-36843852" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36842358">root</a><span>|</span><a href="#36842890">parent</a><span>|</span><a href="#36842947">prev</a><span>|</span><a href="#36838447">next</a><span>|</span><label class="collapse" for="c-36843852">[-]</label><label class="expand" for="c-36843852">[1 more]</label></div><br/><div class="children"><div class="content">This is code written in C which does the same calculations as other versions of Llama 2, such as the PyTorch one.<p>It has nothing to do with PyTorch except that it does the same calculations.</div><br/></div></div></div></div></div></div></div></div><div id="36838447" class="c"><input type="checkbox" id="c-36838447" checked=""/><div class="controls bullet"><span class="by">5-</span><span>|</span><a href="#36842358">prev</a><span>|</span><a href="#36839131">next</a><span>|</span><label class="collapse" for="c-36838447">[-]</label><label class="expand" for="c-36838447">[2 more]</label></div><br/><div class="children"><div class="content">neat!<p>note that gcc&#x27;s default optimisation level is 0, which really isn&#x27;t what people normally want.<p>adding -O2 to the gcc command line should improve performance quite a bit.</div><br/><div id="36838741" class="c"><input type="checkbox" id="c-36838741" checked=""/><div class="controls bullet"><span class="by">sodality2</span><span>|</span><a href="#36838447">parent</a><span>|</span><a href="#36839131">next</a><span>|</span><label class="collapse" for="c-36838741">[-]</label><label class="expand" for="c-36838741">[1 more]</label></div><br/><div class="children"><div class="content">-Ofast also doubles the performance for me to 200tok&#x2F;sec, and -march=native got me up to 230tok&#x2F;sec.<p>-Ofast does break some compliance but I seriously doubt it will reduce accuracy at all, not like quantization would at least.</div><br/></div></div></div></div><div id="36839131" class="c"><input type="checkbox" id="c-36839131" checked=""/><div class="controls bullet"><span class="by">abidlabs</span><span>|</span><a href="#36838447">prev</a><span>|</span><a href="#36842409">next</a><span>|</span><label class="collapse" for="c-36839131">[-]</label><label class="expand" for="c-36839131">[1 more]</label></div><br/><div class="children"><div class="content">Is the trained model available on Hugging Face?</div><br/></div></div><div id="36842409" class="c"><input type="checkbox" id="c-36842409" checked=""/><div class="controls bullet"><span class="by">clircle</span><span>|</span><a href="#36839131">prev</a><span>|</span><a href="#36838420">next</a><span>|</span><label class="collapse" for="c-36842409">[-]</label><label class="expand" for="c-36842409">[2 more]</label></div><br/><div class="children"><div class="content">Never seen the word “inference” used as a verb.</div><br/><div id="36844408" class="c"><input type="checkbox" id="c-36844408" checked=""/><div class="controls bullet"><span class="by">arthur2e5</span><span>|</span><a href="#36842409">parent</a><span>|</span><a href="#36838420">next</a><span>|</span><label class="collapse" for="c-36844408">[-]</label><label class="expand" for="c-36844408">[1 more]</label></div><br/><div class="children"><div class="content">Wanted to say the same. I had to check the dictionary to make sure it&#x27;s not some obscure &quot;exercise&quot; situation as I&#x27;ve unfortunately seen it used as a verb before (in a shoddily written README).</div><br/></div></div></div></div><div id="36838420" class="c"><input type="checkbox" id="c-36838420" checked=""/><div class="controls bullet"><span class="by">bilsbie</span><span>|</span><a href="#36842409">prev</a><span>|</span><a href="#36838227">next</a><span>|</span><label class="collapse" for="c-36838420">[-]</label><label class="expand" for="c-36838420">[6 more]</label></div><br/><div class="children"><div class="content">What are some uses for this?</div><br/><div id="36838514" class="c"><input type="checkbox" id="c-36838514" checked=""/><div class="controls bullet"><span class="by">xyproto</span><span>|</span><a href="#36838420">parent</a><span>|</span><a href="#36838584">next</a><span>|</span><label class="collapse" for="c-36838514">[-]</label><label class="expand" for="c-36838514">[4 more]</label></div><br/><div class="children"><div class="content">Create a computer game about a small island with 100 people, with each person being politically aware, with llama2.c being their brain. Then you can simulate politics for a thousand years and see what happens. For instance.</div><br/><div id="36839125" class="c"><input type="checkbox" id="c-36839125" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#36838420">root</a><span>|</span><a href="#36838514">parent</a><span>|</span><a href="#36839799">next</a><span>|</span><label class="collapse" for="c-36839125">[-]</label><label class="expand" for="c-36839125">[2 more]</label></div><br/><div class="children"><div class="content">Neat idea. Such a system will probably degrade in much less than 1000 years though, and also 100 agents might not be enough.</div><br/><div id="36844397" class="c"><input type="checkbox" id="c-36844397" checked=""/><div class="controls bullet"><span class="by">subarctic</span><span>|</span><a href="#36838420">root</a><span>|</span><a href="#36839125">parent</a><span>|</span><a href="#36839799">next</a><span>|</span><label class="collapse" for="c-36844397">[-]</label><label class="expand" for="c-36844397">[1 more]</label></div><br/><div class="children"><div class="content">For a small island of 100 people? What other agents would you have to simulate besides the people?</div><br/></div></div></div></div><div id="36839799" class="c"><input type="checkbox" id="c-36839799" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#36838420">root</a><span>|</span><a href="#36838514">parent</a><span>|</span><a href="#36839125">prev</a><span>|</span><a href="#36838584">next</a><span>|</span><label class="collapse" for="c-36839799">[-]</label><label class="expand" for="c-36839799">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;twitter.com&#x2F;fablesimulation&#x2F;status&#x2F;1681352904152850437" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;fablesimulation&#x2F;status&#x2F;16813529041528504...</a></div><br/></div></div></div></div><div id="36838584" class="c"><input type="checkbox" id="c-36838584" checked=""/><div class="controls bullet"><span class="by">version_five</span><span>|</span><a href="#36838420">parent</a><span>|</span><a href="#36838514">prev</a><span>|</span><a href="#36838227">next</a><span>|</span><label class="collapse" for="c-36838584">[-]</label><label class="expand" for="c-36838584">[1 more]</label></div><br/><div class="children"><div class="content">- learning how llama works<p>- learning how to implement various deep learning operations in C<p>- generally removing abstraction from &quot;AI&quot; to give a better sense of what is happening in inference<p>- as a template to follow for custom projects<p>- as a basis for learning about applying hardware specific optimizations (say, trying to rewrite to use BLAS)<p>- because it&#x27;s cool</div><br/></div></div></div></div><div id="36838227" class="c"><input type="checkbox" id="c-36838227" checked=""/><div class="controls bullet"><span class="by">kgwgk</span><span>|</span><a href="#36838420">prev</a><span>|</span><a href="#36840132">next</a><span>|</span><label class="collapse" for="c-36838227">[-]</label><label class="expand" for="c-36838227">[1 more]</label></div><br/><div class="children"><div class="content">&quot;train a baby Llama 2 model in PyTorch, then inference it&quot;</div><br/></div></div><div id="36840132" class="c"><input type="checkbox" id="c-36840132" checked=""/><div class="controls bullet"><span class="by">gandalfff</span><span>|</span><a href="#36838227">prev</a><span>|</span><a href="#36840214">next</a><span>|</span><label class="collapse" for="c-36840132">[-]</label><label class="expand" for="c-36840132">[2 more]</label></div><br/><div class="children"><div class="content">Seems like this could be suitable for masochists like me who wish to run language models on retro computers :)</div><br/><div id="36840254" class="c"><input type="checkbox" id="c-36840254" checked=""/><div class="controls bullet"><span class="by">taminka</span><span>|</span><a href="#36840132">parent</a><span>|</span><a href="#36840214">next</a><span>|</span><label class="collapse" for="c-36840254">[-]</label><label class="expand" for="c-36840254">[1 more]</label></div><br/><div class="children"><div class="content">not really imo<p>i&#x27;m really enjoy the resurgence of very minimal implementations of ml algorithms, because if you&#x27;ve recently tried performing inference on a sophisticated ml model in a way that&#x27;s user friendly in any capacity, you know that it essentially involves pulling out your prayer book, rosary and incense, pulling like 20gb of python dependencies, 20 different frameworks, all of which breaks very easily, any minor difference in versioning is guaranteed to break the entire setup, with no hope of fixing it, it&#x27;s just bindings on top of bindings on top of bindings, every other day a new library comes out that builds on top of existing libraries, introducing their new format, promising &quot;deploy models in with 15 lines of python&quot;, then &quot;10 lines of python&quot;, then &quot;1 one of python&quot;, which essentially calls into a black box N layers of python on top of each other, calling into an extremely complicated C++ autodiff library, the source code of which can only be acquired by an in person meeting with some sketchy software engineer from czechia, all of which only works on python 3.10.2, cuda v12.78.1298.777 with commit aohfyoawhftyaowhftuawot, only compiled with microsoft&#x27;s implementation of C++ compiler, with 10 non-standard extensions enabled, all of this OF COURSE only if you have the most optimal hardware<p>point is, if your implementation is a simple C project that&#x27;s trivial to build&#x2F;integrate into your project, it&#x27;s significantly easier to use on any hardware, not just retro (popularity of llama.cpp is a great testament to that imo)</div><br/></div></div></div></div><div id="36840214" class="c"><input type="checkbox" id="c-36840214" checked=""/><div class="controls bullet"><span class="by">akomtu</span><span>|</span><a href="#36840132">prev</a><span>|</span><a href="#36839174">next</a><span>|</span><label class="collapse" for="c-36840214">[-]</label><label class="expand" for="c-36840214">[4 more]</label></div><br/><div class="children"><div class="content">Random thought: right now an LLM returns a probabilities distribution, an RNG sampler picks one and apoends it to the output, then the sequence repeats; but can the RNG instead pick N tokens that approximate the distribution, ask LLM to generate N new distributions, combine them somehow, then pick another set of N tokens from the combined dustribution?</div><br/><div id="36844506" class="c"><input type="checkbox" id="c-36844506" checked=""/><div class="controls bullet"><span class="by">jauntbox</span><span>|</span><a href="#36840214">parent</a><span>|</span><a href="#36843876">next</a><span>|</span><label class="collapse" for="c-36844506">[-]</label><label class="expand" for="c-36844506">[1 more]</label></div><br/><div class="children"><div class="content">This sounds pretty much like beam search (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Beam_search" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Beam_search</a>), which is in fact a common generation technique! See eg. <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;internal&#x2F;generation_utils" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;internal&#x2F;generation...</a></div><br/></div></div><div id="36843876" class="c"><input type="checkbox" id="c-36843876" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36840214">parent</a><span>|</span><a href="#36844506">prev</a><span>|</span><a href="#36841847">next</a><span>|</span><label class="collapse" for="c-36843876">[-]</label><label class="expand" for="c-36843876">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like a good avenue to research, but you probably want to generate more than 2 tokens ahead. Try 20 tokens, but I suppose you don&#x27;t want N^20 executions of the LLM, but more like a representative sampling of say 200 combinations of the next 20 tokens. I don&#x27;t know how you&#x27;d do that.</div><br/></div></div><div id="36841847" class="c"><input type="checkbox" id="c-36841847" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#36840214">parent</a><span>|</span><a href="#36843876">prev</a><span>|</span><a href="#36839174">next</a><span>|</span><label class="collapse" for="c-36841847">[-]</label><label class="expand" for="c-36841847">[1 more]</label></div><br/><div class="children"><div class="content">Novice here.<p>I like the sound of that!<p>I don&#x27;t know the answer but I might experiment with it. Probably a researcher has tried it.<p>You would need N times the compute per token generate of course.<p>You could either pick to N, or sample N (with temperature adjustment to logits if needed)</div><br/></div></div></div></div><div id="36839174" class="c"><input type="checkbox" id="c-36839174" checked=""/><div class="controls bullet"><span class="by">lachlan_gray</span><span>|</span><a href="#36840214">prev</a><span>|</span><a href="#36838375">next</a><span>|</span><label class="collapse" for="c-36839174">[-]</label><label class="expand" for="c-36839174">[9 more]</label></div><br/><div class="children"><div class="content">Not that it is necessarily of value, but has anyone got a LLM to run on bare metal?</div><br/><div id="36843866" class="c"><input type="checkbox" id="c-36843866" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36839174">parent</a><span>|</span><a href="#36839910">next</a><span>|</span><label class="collapse" for="c-36843866">[-]</label><label class="expand" for="c-36843866">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that what this is?</div><br/></div></div><div id="36839910" class="c"><input type="checkbox" id="c-36839910" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#36839174">parent</a><span>|</span><a href="#36843866">prev</a><span>|</span><a href="#36838375">next</a><span>|</span><label class="collapse" for="c-36839910">[-]</label><label class="expand" for="c-36839910">[7 more]</label></div><br/><div class="children"><div class="content">Some of the smaller ones, yes, the huggingface.co libraries make it pretty simple.</div><br/><div id="36840027" class="c"><input type="checkbox" id="c-36840027" checked=""/><div class="controls bullet"><span class="by">kgwgk</span><span>|</span><a href="#36839174">root</a><span>|</span><a href="#36839910">parent</a><span>|</span><a href="#36838375">next</a><span>|</span><label class="collapse" for="c-36840027">[-]</label><label class="expand" for="c-36840027">[6 more]</label></div><br/><div class="children"><div class="content">&quot;In computer science, bare machine (or bare metal) refers to a computer executing instructions directly on logic hardware without an intervening operating system.&quot;<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bare_metal" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bare_metal</a></div><br/><div id="36843656" class="c"><input type="checkbox" id="c-36843656" checked=""/><div class="controls bullet"><span class="by">kgwgk</span><span>|</span><a href="#36839174">root</a><span>|</span><a href="#36840027">parent</a><span>|</span><a href="#36838375">next</a><span>|</span><label class="collapse" for="c-36843656">[-]</label><label class="expand" for="c-36843656">[5 more]</label></div><br/><div class="children"><div class="content">I know I shouldn’t question the wisdom of downvoters but… come on!</div><br/><div id="36843868" class="c"><input type="checkbox" id="c-36843868" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36839174">root</a><span>|</span><a href="#36843656">parent</a><span>|</span><a href="#36838375">next</a><span>|</span><label class="collapse" for="c-36843868">[-]</label><label class="expand" for="c-36843868">[4 more]</label></div><br/><div class="children"><div class="content">What is stopping you from running llama2.c on bare metal?</div><br/><div id="36844080" class="c"><input type="checkbox" id="c-36844080" checked=""/><div class="controls bullet"><span class="by">kgwgk</span><span>|</span><a href="#36839174">root</a><span>|</span><a href="#36843868">parent</a><span>|</span><a href="#36838375">next</a><span>|</span><label class="collapse" for="c-36844080">[-]</label><label class="expand" for="c-36844080">[3 more]</label></div><br/><div class="children"><div class="content">Would you say that running on bare metal the huggingface.co libraries - as the comment that I replied to suggested - is pretty simple?</div><br/><div id="36844149" class="c"><input type="checkbox" id="c-36844149" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#36839174">root</a><span>|</span><a href="#36844080">parent</a><span>|</span><a href="#36838375">next</a><span>|</span><label class="collapse" for="c-36844149">[-]</label><label class="expand" for="c-36844149">[2 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t use the huggingface.co libraries?</div><br/><div id="36844843" class="c"><input type="checkbox" id="c-36844843" checked=""/><div class="controls bullet"><span class="by">kgwgk</span><span>|</span><a href="#36839174">root</a><span>|</span><a href="#36844149">parent</a><span>|</span><a href="#36838375">next</a><span>|</span><label class="collapse" for="c-36844843">[-]</label><label class="expand" for="c-36844843">[1 more]</label></div><br/><div class="children"><div class="content">lachlan_gray asked whether anyone has got a LLM to run on bare metal.<p>tomrod replied to lachlan_gray that the huggingface.co libraries make it pretty simple.<p>I pointed out to tomrod what is the meaning of the expression “bare metal”.<p>I don’t understand what’s the point of your reply to me in that context.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="36838375" class="c"><input type="checkbox" id="c-36838375" checked=""/><div class="controls bullet"><span class="by">Dwedit</span><span>|</span><a href="#36839174">prev</a><span>|</span><a href="#36839768">next</a><span>|</span><label class="collapse" for="c-36838375">[-]</label><label class="expand" for="c-36838375">[5 more]</label></div><br/><div class="children"><div class="content">Sounds like what Llama.cpp used to be.</div><br/><div id="36839104" class="c"><input type="checkbox" id="c-36839104" checked=""/><div class="controls bullet"><span class="by">avhon1</span><span>|</span><a href="#36838375">parent</a><span>|</span><a href="#36839768">next</a><span>|</span><label class="collapse" for="c-36839104">[-]</label><label class="expand" for="c-36839104">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure what you mean by &quot;used to be&quot;, the llama.cpp github repository was committed to just 4 hours ago.<p>This project cites llama.cpp as inspiration, but seems much-simplified. It <i>only</i> supports llama-2, only supports fp-32, and only runs on one CPU thread.</div><br/><div id="36839152" class="c"><input type="checkbox" id="c-36839152" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#36838375">root</a><span>|</span><a href="#36839104">parent</a><span>|</span><a href="#36839768">next</a><span>|</span><label class="collapse" for="c-36839152">[-]</label><label class="expand" for="c-36839152">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m not sure what you mean by &quot;used to be&quot;, the llama.cpp github repository was committed to just 4 hours ago.<p>It&#x27;s not really small, simple, or easily-understandable anymore; it&#x27;s pretty far into the weeds of micro-optimization. They&#x27;re quite good at it, don&#x27;t get me wrong, but it hurts one&#x27;s ability to read what exactly is going on, especially with all the options and different configurations that are supported now.<p>I know a lot about some intricacies of GGML because I was an avid contributor to rwkv.cpp for a few weeks, but I still don&#x27;t understand llama.cpp. It&#x27;s just on a completely different level.</div><br/><div id="36839940" class="c"><input type="checkbox" id="c-36839940" checked=""/><div class="controls bullet"><span class="by">enriquto</span><span>|</span><a href="#36838375">root</a><span>|</span><a href="#36839152">parent</a><span>|</span><a href="#36839768">next</a><span>|</span><label class="collapse" for="c-36839940">[-]</label><label class="expand" for="c-36839940">[2 more]</label></div><br/><div class="children"><div class="content">The beauty of a vcs is that <i>all</i> previous versions are still there for everybody to study and enjoy.  Including the glorious first commit of llama.cpp</div><br/><div id="36840091" class="c"><input type="checkbox" id="c-36840091" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#36838375">root</a><span>|</span><a href="#36839940">parent</a><span>|</span><a href="#36839768">next</a><span>|</span><label class="collapse" for="c-36840091">[-]</label><label class="expand" for="c-36840091">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, this is something that is often forgotten, but I&#x27;m guilty of a few large refactors myself on rwkv.cpp where reading the old code won&#x27;t necessarily enlighten you about where things are today. I&#x27;d be surprised if llama.cpp doesn&#x27;t have any of these.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36839768" class="c"><input type="checkbox" id="c-36839768" checked=""/><div class="controls bullet"><span class="by">eclectic29</span><span>|</span><a href="#36838375">prev</a><span>|</span><label class="collapse" for="c-36839768">[-]</label><label class="expand" for="c-36839768">[5 more]</label></div><br/><div class="children"><div class="content">This is amazing. One curious question: Why C? Why not standard C++?</div><br/><div id="36842131" class="c"><input type="checkbox" id="c-36842131" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#36839768">parent</a><span>|</span><a href="#36839787">next</a><span>|</span><label class="collapse" for="c-36842131">[-]</label><label class="expand" for="c-36842131">[2 more]</label></div><br/><div class="children"><div class="content">Why call C++ standard but not C?</div><br/><div id="36844252" class="c"><input type="checkbox" id="c-36844252" checked=""/><div class="controls bullet"><span class="by">wtarreau</span><span>|</span><a href="#36839768">root</a><span>|</span><a href="#36842131">parent</a><span>|</span><a href="#36839787">next</a><span>|</span><label class="collapse" for="c-36844252">[-]</label><label class="expand" for="c-36844252">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. In fact it would be great if llama.cpp would drop that C++ mess that makes it harder to contribute...</div><br/></div></div></div></div><div id="36839787" class="c"><input type="checkbox" id="c-36839787" checked=""/><div class="controls bullet"><span class="by">bobbyi</span><span>|</span><a href="#36839768">parent</a><span>|</span><a href="#36842131">prev</a><span>|</span><label class="collapse" for="c-36839787">[-]</label><label class="expand" for="c-36839787">[2 more]</label></div><br/><div class="children"><div class="content">That project already exists <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp</a></div><br/><div id="36840222" class="c"><input type="checkbox" id="c-36840222" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#36839768">root</a><span>|</span><a href="#36839787">parent</a><span>|</span><label class="collapse" for="c-36840222">[-]</label><label class="expand" for="c-36840222">[1 more]</label></div><br/><div class="children"><div class="content">And just made a new release less than a minute ago, by pure chance...</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>