<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1705568482567" as="style"/><link rel="stylesheet" href="styles.css?v=1705568482567"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/collabora/WhisperSpeech">WhisperSpeech – An open source text-to-speech system built by inverting Whisper</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>nickmcc</span> | <span>38 comments</span></div><br/><div><div id="39038487" class="c"><input type="checkbox" id="c-39038487" checked=""/><div class="controls bullet"><span class="by">nmfisher</span><span>|</span><a href="#39039558">next</a><span>|</span><label class="collapse" for="c-39038487">[-]</label><label class="expand" for="c-39038487">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been following jpc [0] on the LAION discord since he started building this last year, and it&#x27;s a very impressive project.<p>The key here is that the Whisper multilingual ASR model has been trained on a huge amount of data, so its encoder output is a very good representation of the semantic content of speech. This can be used as an open-source, drop-in replacement for the semantic encoder in model architectures like SPEAR-TTS&#x2F;VALL-E&#x2F;etc (whose semantic encoders are not publicly available).  This is then used to predict acoustic tokens (the output from the quantized&#x2F;low-bandwidth Encodec audio codec) which is then upsampled&#x2F;denoised&#x2F;enhanced with the Vocos vocoder.<p>I know someone is working on Hindi but it would be great to see this extended to other languages for a properly open-source [1], multilingual TTS platform. I think the main bottleneck at the moment is finding people who can procure&#x2F;clean compliant datasets.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;jpc">https:&#x2F;&#x2F;github.com&#x2F;jpc</a>
[1] jpc&#x2F;Collabora went to great efforts to ensure that they are only using properly licensed data to train this. I doubt Whisper itself was that compliant, so it&#x27;s a bit muddy.</div><br/><div id="39038903" class="c"><input type="checkbox" id="c-39038903" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#39038487">parent</a><span>|</span><a href="#39039558">next</a><span>|</span><label class="collapse" for="c-39038903">[-]</label><label class="expand" for="c-39038903">[2 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t mention the ability to add custom voices to the speech output, I wonder if that&#x27;s a feature thatbwould be supported</div><br/><div id="39039113" class="c"><input type="checkbox" id="c-39039113" checked=""/><div class="controls bullet"><span class="by">atwrk</span><span>|</span><a href="#39038487">root</a><span>|</span><a href="#39038903">parent</a><span>|</span><a href="#39039558">next</a><span>|</span><label class="collapse" for="c-39039113">[-]</label><label class="expand" for="c-39039113">[1 more]</label></div><br/><div class="children"><div class="content">They <i>do</i> mention voice cloning in the README (&quot;We’ve also added an example of voice cloning based on a reference audio file.&quot;), do you have something different in mind?</div><br/></div></div></div></div></div></div><div id="39037909" class="c"><input type="checkbox" id="c-39037909" checked=""/><div class="controls bullet"><span class="by">siraben</span><span>|</span><a href="#39039558">prev</a><span>|</span><a href="#39037879">next</a><span>|</span><label class="collapse" for="c-39037909">[-]</label><label class="expand" for="c-39037909">[8 more]</label></div><br/><div class="children"><div class="content">Interested to see how it performs for Mandarin Chinese speech synthesis, especially with prosody and emotion. The highest quality open source model I&#x27;ve seen so far is EmotiVoice[0], which I&#x27;ve made a CLI wrapper around to generate audio for flashcards.[1] For EmotiVoice, you can apparently also clone your own voice with a GPU, but I have not tested this.[2]<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;netease-youdao&#x2F;EmotiVoice">https:&#x2F;&#x2F;github.com&#x2F;netease-youdao&#x2F;EmotiVoice</a><p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;siraben&#x2F;emotivoice-cli">https:&#x2F;&#x2F;github.com&#x2F;siraben&#x2F;emotivoice-cli</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;netease-youdao&#x2F;EmotiVoice&#x2F;wiki&#x2F;Voice-Cloning-with-your-personal-data">https:&#x2F;&#x2F;github.com&#x2F;netease-youdao&#x2F;EmotiVoice&#x2F;wiki&#x2F;Voice-Clon...</a></div><br/><div id="39038024" class="c"><input type="checkbox" id="c-39038024" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#39037909">parent</a><span>|</span><a href="#39037959">next</a><span>|</span><label class="collapse" for="c-39038024">[-]</label><label class="expand" for="c-39038024">[1 more]</label></div><br/><div class="children"><div class="content">Did you try XTTS v2 for Mandarin? I&#x27;m curious how it compares with EmotiVoice.</div><br/></div></div><div id="39037959" class="c"><input type="checkbox" id="c-39037959" checked=""/><div class="controls bullet"><span class="by">wferrell</span><span>|</span><a href="#39037909">parent</a><span>|</span><a href="#39038024">prev</a><span>|</span><a href="#39038909">next</a><span>|</span><label class="collapse" for="c-39037959">[-]</label><label class="expand" for="c-39037959">[5 more]</label></div><br/><div class="children"><div class="content">Have you released your flashcard app?</div><br/><div id="39039320" class="c"><input type="checkbox" id="c-39039320" checked=""/><div class="controls bullet"><span class="by">knubie</span><span>|</span><a href="#39037909">root</a><span>|</span><a href="#39037959">parent</a><span>|</span><a href="#39038378">next</a><span>|</span><label class="collapse" for="c-39039320">[-]</label><label class="expand" for="c-39039320">[1 more]</label></div><br/><div class="children"><div class="content">Not OP, but I develop Mochi [0] which is a spaced repetition flash card app that has text-to-speech and a bunch of other stuff built in (transcription, dictionaries, etc.) that you might be interested in.<p>[0] <a href="https:&#x2F;&#x2F;mochi.cards" rel="nofollow">https:&#x2F;&#x2F;mochi.cards</a></div><br/></div></div><div id="39038378" class="c"><input type="checkbox" id="c-39038378" checked=""/><div class="controls bullet"><span class="by">nmfisher</span><span>|</span><a href="#39037909">root</a><span>|</span><a href="#39037959">parent</a><span>|</span><a href="#39039320">prev</a><span>|</span><a href="#39037967">next</a><span>|</span><label class="collapse" for="c-39038378">[-]</label><label class="expand" for="c-39038378">[2 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re interested, I have a small side project (<a href="https:&#x2F;&#x2F;imaginanki.com" rel="nofollow">https:&#x2F;&#x2F;imaginanki.com</a>) for generating Anki decks with images + speech (via SDXL&#x2F;Azure).</div><br/><div id="39039135" class="c"><input type="checkbox" id="c-39039135" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#39037909">root</a><span>|</span><a href="#39038378">parent</a><span>|</span><a href="#39037967">next</a><span>|</span><label class="collapse" for="c-39039135">[-]</label><label class="expand" for="c-39039135">[1 more]</label></div><br/><div class="children"><div class="content">Some language learning resources From &quot;Show HN: Open-source tool for creating courses like Duolingo&quot; (2023) <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38317345">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38317345</a> :<p>&gt; <i>ENH: Generate Anki decks with {IPA symbols, Greek letters w&#x2F; LaTeX for math and science, </i></div><br/></div></div></div></div><div id="39037967" class="c"><input type="checkbox" id="c-39037967" checked=""/><div class="controls bullet"><span class="by">siraben</span><span>|</span><a href="#39037909">root</a><span>|</span><a href="#39037959">parent</a><span>|</span><a href="#39038378">prev</a><span>|</span><a href="#39038909">next</a><span>|</span><label class="collapse" for="c-39037967">[-]</label><label class="expand" for="c-39037967">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just an Anki deck.</div><br/></div></div></div></div><div id="39038909" class="c"><input type="checkbox" id="c-39038909" checked=""/><div class="controls bullet"><span class="by">colordrops</span><span>|</span><a href="#39037909">parent</a><span>|</span><a href="#39037959">prev</a><span>|</span><a href="#39037879">next</a><span>|</span><label class="collapse" for="c-39038909">[-]</label><label class="expand" for="c-39038909">[1 more]</label></div><br/><div class="children"><div class="content">Just listened to the demo voices for EmotiVoice and WhisperSpeech. I think WhisperSpeech edges out EmotiVoice. EmotiVoice sounds like it was trained on English spoken by non-native speakers.</div><br/></div></div></div></div><div id="39037879" class="c"><input type="checkbox" id="c-39037879" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#39037909">prev</a><span>|</span><a href="#39036831">next</a><span>|</span><label class="collapse" for="c-39037879">[-]</label><label class="expand" for="c-39037879">[4 more]</label></div><br/><div class="children"><div class="content">I know it&#x27;s old at this point and doesn&#x27;t use the fancy new tech, but Mycroft&#x27;s Mimic 3 is still pretty impressive and is small enough to fit comfortably and generate speech in real time on a raspberry pi [0]. Some of their voices are better than others, but the best of them are definitely equal to the examples of WhisperSpeech given here.<p>[0] <a href="https:&#x2F;&#x2F;mycroft.ai&#x2F;mimic-3&#x2F;" rel="nofollow">https:&#x2F;&#x2F;mycroft.ai&#x2F;mimic-3&#x2F;</a></div><br/><div id="39038614" class="c"><input type="checkbox" id="c-39038614" checked=""/><div class="controls bullet"><span class="by">follower</span><span>|</span><a href="#39037879">parent</a><span>|</span><a href="#39038906">next</a><span>|</span><label class="collapse" for="c-39038614">[-]</label><label class="expand" for="c-39038614">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re not already aware, the primary developer of Mimic 3 (and its non-Mimic predecessor Larynx) continued TTS-related development with Larynx and the renamed project Piper: <a href="https:&#x2F;&#x2F;github.com&#x2F;rhasspy&#x2F;piper">https:&#x2F;&#x2F;github.com&#x2F;rhasspy&#x2F;piper</a><p>Last year Piper development was supported by Nabu Casa for their &quot;Year of Voice&quot; project for Home Assistant and it sounds like Mike Hansen is going to continue on it with their support this year.</div><br/></div></div><div id="39038906" class="c"><input type="checkbox" id="c-39038906" checked=""/><div class="controls bullet"><span class="by">boxed</span><span>|</span><a href="#39037879">parent</a><span>|</span><a href="#39038614">prev</a><span>|</span><a href="#39036831">next</a><span>|</span><label class="collapse" for="c-39038906">[-]</label><label class="expand" for="c-39038906">[2 more]</label></div><br/><div class="children"><div class="content">The &quot;English US&quot; voice sounds more Scottish than American to me :P</div><br/><div id="39039167" class="c"><input type="checkbox" id="c-39039167" checked=""/><div class="controls bullet"><span class="by">rcarmo</span><span>|</span><a href="#39037879">root</a><span>|</span><a href="#39038906">parent</a><span>|</span><a href="#39036831">next</a><span>|</span><label class="collapse" for="c-39039167">[-]</label><label class="expand" for="c-39039167">[1 more]</label></div><br/><div class="children"><div class="content">Which might be a good thing, nae, laddie?</div><br/></div></div></div></div></div></div><div id="39036831" class="c"><input type="checkbox" id="c-39036831" checked=""/><div class="controls bullet"><span class="by">nickmcc</span><span>|</span><a href="#39037879">prev</a><span>|</span><a href="#39037839">next</a><span>|</span><label class="collapse" for="c-39036831">[-]</label><label class="expand" for="c-39036831">[2 more]</label></div><br/><div class="children"><div class="content">I was looking at video on training a custom voice with Piper, following a tutorial at <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=b_we_jma220" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=b_we_jma220</a>, and noticed how the datasets required metadata of the text for the source audio files. This training method by Collabora seems to automate that process and only requires an audio file for training.</div><br/><div id="39037900" class="c"><input type="checkbox" id="c-39037900" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#39036831">parent</a><span>|</span><a href="#39037839">next</a><span>|</span><label class="collapse" for="c-39037900">[-]</label><label class="expand" for="c-39037900">[1 more]</label></div><br/><div class="children"><div class="content">Whisper solves it, that’s its purpose.</div><br/></div></div></div></div><div id="39037839" class="c"><input type="checkbox" id="c-39037839" checked=""/><div class="controls bullet"><span class="by">zerop</span><span>|</span><a href="#39036831">prev</a><span>|</span><a href="#39038546">next</a><span>|</span><label class="collapse" for="c-39037839">[-]</label><label class="expand" for="c-39037839">[2 more]</label></div><br/><div class="children"><div class="content">Can this be run on Mac M1?</div><br/><div id="39038122" class="c"><input type="checkbox" id="c-39038122" checked=""/><div class="controls bullet"><span class="by">abathur</span><span>|</span><a href="#39037839">parent</a><span>|</span><a href="#39038546">next</a><span>|</span><label class="collapse" for="c-39038122">[-]</label><label class="expand" for="c-39038122">[1 more]</label></div><br/><div class="children"><div class="content">Idk if it would out of the box, but it should be possible. I know that Whisper (and some variants) run on both x86 and silicon macs.</div><br/></div></div></div></div><div id="39038546" class="c"><input type="checkbox" id="c-39038546" checked=""/><div class="controls bullet"><span class="by">WhackyIdeas</span><span>|</span><a href="#39037839">prev</a><span>|</span><a href="#39038202">next</a><span>|</span><label class="collapse" for="c-39038546">[-]</label><label class="expand" for="c-39038546">[1 more]</label></div><br/><div class="children"><div class="content">Can it run local only?</div><br/></div></div><div id="39038202" class="c"><input type="checkbox" id="c-39038202" checked=""/><div class="controls bullet"><span class="by">huytersd</span><span>|</span><a href="#39038546">prev</a><span>|</span><a href="#39038051">next</a><span>|</span><label class="collapse" for="c-39038202">[-]</label><label class="expand" for="c-39038202">[6 more]</label></div><br/><div class="children"><div class="content">What’s the text to speech generator that chatGPT uses? It’s the most impressive one I’ve heard so far.</div><br/><div id="39038685" class="c"><input type="checkbox" id="c-39038685" checked=""/><div class="controls bullet"><span class="by">miki123211</span><span>|</span><a href="#39038202">parent</a><span>|</span><a href="#39038231">next</a><span>|</span><label class="collapse" for="c-39038685">[-]</label><label class="expand" for="c-39038685">[3 more]</label></div><br/><div class="children"><div class="content">If you think OpenAI&#x27;s TTS is impressive, you should check out Eleven Labs. They have the highest quality models IMO. Voice quality, emotional awareness &#x2F; inflection and support for foreign languages are top-notch, it&#x27;s that last point that OpenAI seems to have the most issues with. If you find a good voice to clone, the latest models can even replicate somewhat unusual accents and speaking styles.<p>For plain old English TTS with a stock voice, there isn&#x27;t that much of a difference (although Eleven Labs still wins IMO), but if you need either voice cloning or foreign language support, nothing else comes even close.<p>With that said, Eleven is extremely pricy, something like Azure TTS (which is the best among the cheap options) may be a better fit for less demanding applications.</div><br/><div id="39039106" class="c"><input type="checkbox" id="c-39039106" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#39038202">root</a><span>|</span><a href="#39038685">parent</a><span>|</span><a href="#39038775">next</a><span>|</span><label class="collapse" for="c-39039106">[-]</label><label class="expand" for="c-39039106">[1 more]</label></div><br/><div class="children"><div class="content">The quality difference between Eleven and OpenAI is IMO pretty small, but the price difference is enormous: for 50,000 characters (approx 1hr of audio, by Eleven&#x27;s estimates), you&#x27;d pay Eleven Labs $9 assuming you&#x27;re in their highest $330&#x2F;month payment commitment tier; for OpenAI there&#x27;s no minimum commitment and the same number of characters would cost $0.75.<p>If you&#x27;re generating speech once and replaying it many times (e.g. making podcasts), the difference is negligible and you might as well go with Eleven Labs, since it&#x27;s more customizable and possibly slightly higher quality. If you&#x27;re doing interactive speech with customers, $9&#x2F;hr is incredibly expensive (higher than hiring a minimum-wage worker in the U.S.!), and OpenAI&#x27;s TTS is a very close second best and much more reasonably priced. If you&#x27;re trying to integrate speech into an AI product, Eleven makes your hourly costs pretty unfeasible since you have to at minimum charge your customers more than it costs to hire a human being to do a task.<p>Azure&#x27;s &quot;Neural&quot; line of TTS is the best of the big cloud offerings, but it&#x27;s pretty mediocre compared to either OpenAI or Eleven Labs IMO. And it&#x27;s actually more expensive than using OpenAI: it&#x27;s $0.80 for 50,000 characters (~1hr), unless you&#x27;re willing to commit to over $1k monthly spend, at which point it&#x27;s barely cheaper than OpenAI at $0.64 per 50k characters.<p>OpenAI&#x27;s TTS is IMO the best option for anything interactive, since it&#x27;s so much higher quality than Azure&#x27;s Neural TTS and so much cheaper (with very little quality difference) as compared to Eleven Labs.</div><br/></div></div><div id="39038775" class="c"><input type="checkbox" id="c-39038775" checked=""/><div class="controls bullet"><span class="by">huytersd</span><span>|</span><a href="#39038202">root</a><span>|</span><a href="#39038685">parent</a><span>|</span><a href="#39039106">prev</a><span>|</span><a href="#39038231">next</a><span>|</span><label class="collapse" for="c-39038775">[-]</label><label class="expand" for="c-39038775">[1 more]</label></div><br/><div class="children"><div class="content">Maybe I’m not a good judge but OpenAI’s voices sound very natural to me and seem better than Eleven labs.</div><br/></div></div></div></div><div id="39038231" class="c"><input type="checkbox" id="c-39038231" checked=""/><div class="controls bullet"><span class="by">etguy</span><span>|</span><a href="#39038202">parent</a><span>|</span><a href="#39038685">prev</a><span>|</span><a href="#39038245">next</a><span>|</span><label class="collapse" for="c-39038231">[-]</label><label class="expand" for="c-39038231">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI’s own models. They’re also available commercially via API and are pretty affordable.</div><br/></div></div><div id="39038245" class="c"><input type="checkbox" id="c-39038245" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39038202">parent</a><span>|</span><a href="#39038231">prev</a><span>|</span><a href="#39038051">next</a><span>|</span><label class="collapse" for="c-39038245">[-]</label><label class="expand" for="c-39038245">[1 more]</label></div><br/><div class="children"><div class="content">They use their own models, and we don&#x27;t know anything about their architecture (I believe), but you can use them with the OpenAI API.</div><br/></div></div></div></div><div id="39038051" class="c"><input type="checkbox" id="c-39038051" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#39038202">prev</a><span>|</span><a href="#39039107">next</a><span>|</span><label class="collapse" for="c-39038051">[-]</label><label class="expand" for="c-39038051">[9 more]</label></div><br/><div class="children"><div class="content">Aside: is it just me, or is anyone else just as dumbfounded with how quickly literally every aspect of AI and LLMs and Models and blah blah blah is going?<p>Am I weird in just having my head spin - even though I&#x27;ve also been at leading edge tech before, but this is just me yelling at these new algos on my lawn?</div><br/><div id="39038162" class="c"><input type="checkbox" id="c-39038162" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#39038051">parent</a><span>|</span><a href="#39038120">next</a><span>|</span><label class="collapse" for="c-39038162">[-]</label><label class="expand" for="c-39038162">[7 more]</label></div><br/><div class="children"><div class="content">on the contrary I&#x27;m really disappointed in how long its taking anything to get into production.<p>Whisper and self-hostable LLMs had a cambrian explosion about 1 year ago, I attended a GPT4 hackathon last March and in 48 hours saw people hook up Speech2Text -&gt; LLM -&gt; Text2Speech pipelines for their live demos. I thought we would all have babelfish by June.<p>Months later I later attended some conferences with international speakers that really wanted to have live, translated-on-the-fly captions, but there wasn&#x27;t anything off the shelf they could use. I found a helpful repo to use whisper with rolling transcription but struggled to get the python prerequisites installed (involving hardlinking to a tensorflow repo for my particular version of m1 CPU). It was humbling and also hype-busting to realize that it takes time to productize, and that the LLMs are not magic that can write these applications themselves.<p>In the meantime even Google hasn&#x27;t bothered to run the improved transcription models on YouTube videos. They are still old 80% accurate tech that&#x27;s useless on anyone with an accent.</div><br/><div id="39038346" class="c"><input type="checkbox" id="c-39038346" checked=""/><div class="controls bullet"><span class="by">huijzer</span><span>|</span><a href="#39038051">root</a><span>|</span><a href="#39038162">parent</a><span>|</span><a href="#39038283">next</a><span>|</span><label class="collapse" for="c-39038346">[-]</label><label class="expand" for="c-39038346">[1 more]</label></div><br/><div class="children"><div class="content">&gt; on the contrary I&#x27;m really disappointed in how long its taking anything to get into production.<p>I agree. I was thinking about making a Jarvis like bot which should be pretty easy at this point. The main problem was that my iPhone doesn’t easily allow for pressing a button upon which it starts listening. You always need to unlock first at which the whole screen gets unlocked too. Maybe these kind of GUI-focussed interfaces are blocking a lot of ideas? At the same time it’s great that people will come up with new devices and these will compete somewhat with phones.</div><br/></div></div><div id="39038283" class="c"><input type="checkbox" id="c-39038283" checked=""/><div class="controls bullet"><span class="by">Rodeoclash</span><span>|</span><a href="#39038051">root</a><span>|</span><a href="#39038162">parent</a><span>|</span><a href="#39038346">prev</a><span>|</span><a href="#39038612">next</a><span>|</span><label class="collapse" for="c-39038283">[-]</label><label class="expand" for="c-39038283">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be interested if you ever dig anything up for this. I hacked together a kind of crude tool to snapshot audio and translate &#x2F; caption it on the fly:<p><a href="https:&#x2F;&#x2F;captioner.richardson.co.nz&#x2F;" rel="nofollow">https:&#x2F;&#x2F;captioner.richardson.co.nz&#x2F;</a><p>I would very much like to improve on this but the live translation &#x2F; captioning still has some more work to go in this space.<p>Source was here: <a href="https:&#x2F;&#x2F;github.com&#x2F;Rodeoclash&#x2F;captioner">https:&#x2F;&#x2F;github.com&#x2F;Rodeoclash&#x2F;captioner</a></div><br/><div id="39038805" class="c"><input type="checkbox" id="c-39038805" checked=""/><div class="controls bullet"><span class="by">follower</span><span>|</span><a href="#39038051">root</a><span>|</span><a href="#39038283">parent</a><span>|</span><a href="#39038612">next</a><span>|</span><label class="collapse" for="c-39038805">[-]</label><label class="expand" for="c-39038805">[1 more]</label></div><br/><div class="children"><div class="content">I <i>was</i> going to suggest considering looking into vosk but... clearly that suggestion isn&#x27;t very useful to you. :)</div><br/></div></div></div></div><div id="39038612" class="c"><input type="checkbox" id="c-39038612" checked=""/><div class="controls bullet"><span class="by">ricketycricket</span><span>|</span><a href="#39038051">root</a><span>|</span><a href="#39038162">parent</a><span>|</span><a href="#39038283">prev</a><span>|</span><a href="#39038617">next</a><span>|</span><label class="collapse" for="c-39038612">[-]</label><label class="expand" for="c-39038612">[2 more]</label></div><br/><div class="children"><div class="content">I built this last March. It captures audio from a live HLS stream and transcribes and translates into 18 languages on the fly. Used by a customer with about 25K international employees for their internal events. Works surprisingly well.</div><br/><div id="39039529" class="c"><input type="checkbox" id="c-39039529" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#39038051">root</a><span>|</span><a href="#39038612">parent</a><span>|</span><a href="#39038617">next</a><span>|</span><label class="collapse" for="c-39039529">[-]</label><label class="expand" for="c-39039529">[1 more]</label></div><br/><div class="children"><div class="content">Fabulous, guess that&#x27;s the other part of productizing: a paying customer!</div><br/></div></div></div></div><div id="39038617" class="c"><input type="checkbox" id="c-39038617" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#39038051">root</a><span>|</span><a href="#39038162">parent</a><span>|</span><a href="#39038612">prev</a><span>|</span><a href="#39038120">next</a><span>|</span><label class="collapse" for="c-39038617">[-]</label><label class="expand" for="c-39038617">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m really disappointed in how long its taking anything to get into production.<p>&gt; It was humbling and also hype-busting to realize that it takes time to productize<p>Yep, looks like you found out why it’s taking so long to get this new tech into production. The gap between nothing and a proof of concept is, in some ways, much smaller than the gap between proof of concept and commercial product.</div><br/></div></div></div></div><div id="39038120" class="c"><input type="checkbox" id="c-39038120" checked=""/><div class="controls bullet"><span class="by">colechristensen</span><span>|</span><a href="#39038051">parent</a><span>|</span><a href="#39038162">prev</a><span>|</span><a href="#39039107">next</a><span>|</span><label class="collapse" for="c-39038120">[-]</label><label class="expand" for="c-39038120">[1 more]</label></div><br/><div class="children"><div class="content">This is the structure of revolutions, particularly of this kind.  Exponential growth looks like this.<p>In particular with the generation &#x2F; recognition abilities of ML models, they have this feature of being a curiosity but not quite useful... so if a speech recognition program goes from 50% accuracy to 75% accuracy it&#x27;s a huge accomplishment but the program is still approximately as useless when it&#x27;s done.  Going from 98% to 99% accuracy on the other hand still cuts the errors in half, but it&#x27;s super impressive going from something that&#x27;s useful but makes mistakes to making half as many mistakes.  Once you hit the threshold of minimum usefulness the exponential growth seems like it&#x27;s sudden and amazing when it&#x27;s actually been going on for a long time.<p>At the same time, we&#x27;ve had a few great improvements in methodology with how models are designs (like transformers) and the first iterations showed how impressive things could be but were full of inefficiencies and we&#x27;re watching those go away rather quickly.</div><br/></div></div></div></div><div id="39039107" class="c"><input type="checkbox" id="c-39039107" checked=""/><div class="controls bullet"><span class="by">RockRobotRock</span><span>|</span><a href="#39038051">prev</a><span>|</span><label class="collapse" for="c-39039107">[-]</label><label class="expand" for="c-39039107">[1 more]</label></div><br/><div class="children"><div class="content">holy shit what. how?</div><br/></div></div></div></div></div></div></div></body></html>