<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1685178069953" as="style"/><link rel="stylesheet" href="styles.css?v=1685178069953"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://reason.com/volokh/2023/05/27/a-lawyers-filing-is-replete-with-citations-to-non-existent-cases-thanks-chatgpt/">ChatGPT-Authored Legal Filing “Replete with Citations to Non-Existent Cases&quot;</a> <span class="domain">(<a href="https://reason.com">reason.com</a>)</span></div><div class="subtext"><span>dpifke</span> | <span>12 comments</span></div><br/><div><div id="36092715" class="c"><input type="checkbox" id="c-36092715" checked=""/><div class="controls bullet"><span class="by">mk_stjames</span><span>|</span><a href="#36092787">next</a><span>|</span><label class="collapse" for="c-36092715">[-]</label><label class="expand" for="c-36092715">[2 more]</label></div><br/><div class="children"><div class="content">If a an engineering firm drew up plans for a building that didn&#x27;t meet even the most basic structural standards, and then PEs signed&#x2F;stamped those drawings... That would be grounds for people to lose licenses, companies to be majorly fined, and in the extreme case where injury was involved, criminal charges.<p>A lawyer just straight making up citations of case law, even if hallucinated by an AI assist tool, should have similar repercussions re: license to practice law.<p>There needs to be no leniency for people misusing &#x27;well, the AI told me...&#x27; as a crutch for actual knowledge and expertise in a professional field.</div><br/><div id="36092791" class="c"><input type="checkbox" id="c-36092791" checked=""/><div class="controls bullet"><span class="by">tmikaeld</span><span>|</span><a href="#36092715">parent</a><span>|</span><a href="#36092787">next</a><span>|</span><label class="collapse" for="c-36092791">[-]</label><label class="expand" for="c-36092791">[1 more]</label></div><br/><div class="children"><div class="content">Word! GPT is a tool, you don&#x27;t blame a hammer for ruining someones window after you put the hammer through the glass.</div><br/></div></div></div></div><div id="36092787" class="c"><input type="checkbox" id="c-36092787" checked=""/><div class="controls bullet"><span class="by">justinclift</span><span>|</span><a href="#36092715">prev</a><span>|</span><a href="#36092545">next</a><span>|</span><label class="collapse" for="c-36092787">[-]</label><label class="expand" for="c-36092787">[1 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t seem like they&#x27;ve asked the lawyers &quot;what other legal filings did you use ChatGPT for?&quot;, which seems kind of important. ;)</div><br/></div></div><div id="36092545" class="c"><input type="checkbox" id="c-36092545" checked=""/><div class="controls bullet"><span class="by">jruohonen</span><span>|</span><a href="#36092787">prev</a><span>|</span><a href="#36092668">next</a><span>|</span><label class="collapse" for="c-36092545">[-]</label><label class="expand" for="c-36092545">[4 more]</label></div><br/><div class="children"><div class="content">Can someone explain the potential repercussion from this kind of foolishness? Isn&#x27;t non-accredited legal counseling illegal? Do these things apply also to Europe?</div><br/><div id="36092675" class="c"><input type="checkbox" id="c-36092675" checked=""/><div class="controls bullet"><span class="by">hkt</span><span>|</span><a href="#36092545">parent</a><span>|</span><a href="#36092668">next</a><span>|</span><label class="collapse" for="c-36092675">[-]</label><label class="expand" for="c-36092675">[3 more]</label></div><br/><div class="children"><div class="content">Non-accredited legal counsel varies from place to place. In the UK there are &quot;Mackenzie friends&quot; who can&#x27;t speak but can advise in court. As for filings, paralegals don&#x27;t tend to need qualifications either, though most solicitors they work under will require it.<p>In short, for the UK at least, no, nobody needs qualifications to do that kind of thing. Though it is clearly a good idea.</div><br/><div id="36092737" class="c"><input type="checkbox" id="c-36092737" checked=""/><div class="controls bullet"><span class="by">jruohonen</span><span>|</span><a href="#36092545">root</a><span>|</span><a href="#36092675">parent</a><span>|</span><a href="#36092668">next</a><span>|</span><label class="collapse" for="c-36092737">[-]</label><label class="expand" for="c-36092737">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for the clarification. But regarding this case more specifically: it seems that the judge in question is interpreting this as a deliberate attempt to deceive a court; hence, the question about repercussions.</div><br/><div id="36092932" class="c"><input type="checkbox" id="c-36092932" checked=""/><div class="controls bullet"><span class="by">hkt</span><span>|</span><a href="#36092545">root</a><span>|</span><a href="#36092737">parent</a><span>|</span><a href="#36092668">next</a><span>|</span><label class="collapse" for="c-36092932">[-]</label><label class="expand" for="c-36092932">[1 more]</label></div><br/><div class="children"><div class="content">Good question. In the UK (and probably most common law systems) I guess it&#x27;d be treated as any vexatious application would be. As far as the judge is concerned there&#x27;s no legal ground for distinguishing between a regular bit of code and an LLM. It&#x27;ll be the human who is prosecuted, if that&#x27;s what you&#x27;re wondering.</div><br/></div></div></div></div></div></div></div></div><div id="36092668" class="c"><input type="checkbox" id="c-36092668" checked=""/><div class="controls bullet"><span class="by">hkt</span><span>|</span><a href="#36092545">prev</a><span>|</span><label class="collapse" for="c-36092668">[-]</label><label class="expand" for="c-36092668">[4 more]</label></div><br/><div class="children"><div class="content">My partner and I have also explored chatGPT for recommending cases and books. It produces more garbage than not. The dream of it being a handy librarian died pretty quickly.<p>Uncanny and compelling as it is conversationally, it really isn&#x27;t that good.</div><br/><div id="36092783" class="c"><input type="checkbox" id="c-36092783" checked=""/><div class="controls bullet"><span class="by">tmikaeld</span><span>|</span><a href="#36092668">parent</a><span>|</span><label class="collapse" for="c-36092783">[-]</label><label class="expand" for="c-36092783">[3 more]</label></div><br/><div class="children"><div class="content">How you use it matters a LOT and also which version you used. There&#x27;s a reason that GPT-4 is up to 45 times more expensive than GPT-3.5 Turbo (the default). If you&#x27;re serious about such a tool (It&#x27;s not a bad idea), then you&#x27;d use the embeddings[0] to train the AI on your specific use cases or prompt-engineer GPT-4 properly.<p>[0] <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;embeddings&#x2F;what-are-embeddings" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;embeddings&#x2F;what-are-...</a></div><br/><div id="36093076" class="c"><input type="checkbox" id="c-36093076" checked=""/><div class="controls bullet"><span class="by">usrbinbash</span><span>|</span><a href="#36092668">root</a><span>|</span><a href="#36092783">parent</a><span>|</span><a href="#36092790">next</a><span>|</span><label class="collapse" for="c-36093076">[-]</label><label class="expand" for="c-36093076">[1 more]</label></div><br/><div class="children"><div class="content">The problem can be mitigated by using bigger models or training embeddings, but it cannot be eliminated.<p>At the end of the day, it remains a sequence completion engine. An LLM can only care about the statistical likelyhood of sequences, that the entire universe it exists in.<p>And if a sequence making a wrong statement happens to be more likely than, or close enough for the set &quot;heat&quot; of the algorithm to, a sequence denoting a correct statement, then there is a chance the model will hallucinate.<p>If it were otherwise, we would already have a good solution for prompt injection attacks, and we don&#x27;t.</div><br/></div></div><div id="36092790" class="c"><input type="checkbox" id="c-36092790" checked=""/><div class="controls bullet"><span class="by">justinclift</span><span>|</span><a href="#36092668">root</a><span>|</span><a href="#36092783">parent</a><span>|</span><a href="#36093076">prev</a><span>|</span><label class="collapse" for="c-36092790">[-]</label><label class="expand" for="c-36092790">[1 more]</label></div><br/><div class="children"><div class="content">&gt; then you&#x27;d use ...<p>Sounds like a bunch of effort. :&#x2F;</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>