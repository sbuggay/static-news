<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1683190851271" as="style"/><link rel="stylesheet" href="styles.css?v=1683190851271"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2305.02301">Distilling Step-by-Step Outperforming Larger Language Models with Less Training</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>verdverm</span> | <span>11 comments</span></div><br/><div><div id="35812591" class="c"><input type="checkbox" id="c-35812591" checked=""/><div class="controls bullet"><span class="by">worldofideas123</span><span>|</span><a href="#35812441">next</a><span>|</span><label class="collapse" for="c-35812591">[-]</label><label class="expand" for="c-35812591">[1 more]</label></div><br/><div class="children"><div class="content">(1) This is like a new prolog in which the rules are linguistic rules (rationales), and the distilling step-by-step is a way of learning the rules. Call the new prolog lprorat (learning programming with rationales).<p>(2) Hypothesis: The success of distilling chain of thought learning  is that it provides a pseudo large context window.  The input pair (label,rationale) allows the student LLM mimic  a large context window (the context window of a human agent that informs the system by providing the rationale data).  So it reduces the embedding distance between words for which entailment is difficult to obtain with short context windows.<p>(3) The above hypothesis suggests to perform the  training process in two phases (a) and (b): 
 (a)  A small model is used as a teacher, this allows the learning model to connect d-distant words. (b) A larger LLM is used as a teacher, the training process continues with new data using the rationale provided by a stronger larger LLM.<p>In the first phase  the learning system learn to infer d-distant word relations, in the second phase the system  is  prepared to connect (2*d)-distant word because it have developed the necessary skills in the first phase.<p>This multi-phase approach could be tailored using three meta-parameters: the number of teachers,  the relative power of each  LLM teacher, and  the percentage of data in the training set for each teacher.<p>(4) I would like that they would release for free the LLM that they have developed, those   small but powerful LLM can provide individuals users to introduce themselves in the SOTA realm.<p>Edited: grammar and clarifying the ideas.</div><br/></div></div><div id="35812441" class="c"><input type="checkbox" id="c-35812441" checked=""/><div class="controls bullet"><span class="by">davidkunz</span><span>|</span><a href="#35812591">prev</a><span>|</span><a href="#35810666">next</a><span>|</span><label class="collapse" for="c-35812441">[-]</label><label class="expand" for="c-35812441">[5 more]</label></div><br/><div class="children"><div class="content">&gt; our 770M T5 model outperforms the 540B PaLM model using only 80% of available data on a benchmark task.<p>That sounds too good, can someone more knowledgeable comment?</div><br/><div id="35812555" class="c"><input type="checkbox" id="c-35812555" checked=""/><div class="controls bullet"><span class="by">marcyb5st</span><span>|</span><a href="#35812441">parent</a><span>|</span><a href="#35812527">next</a><span>|</span><label class="collapse" for="c-35812555">[-]</label><label class="expand" for="c-35812555">[1 more]</label></div><br/><div class="children"><div class="content">Googler here, but not in the research team that authored this paper.<p>It seems that using a &quot;teacher&quot; LLM to train a smaller model in this step-by-step fashion you get much more out of your parameters. Specifically, if you look at section 3 in the paper, they mention that they use LLMs generated rationales as additional guidance for the smaller model. This approach has already been tried, but it had some limitations (section 3.2) which they circumvent by doing things differently: 
  <i>&quot;In this work, instead of using rationales as additional model inputs, we frame learning with rationales as a multi-task problem. ... (this) enables the model to learn to generate the intermediate reasoning steps for the prediction, and could therefore guide the model in better predicting the resultant label.&quot;</i><p>It seems that this is the key for the success they are showing. I am still digesting the paper and I am not 100% sure that is the key factor.</div><br/></div></div><div id="35812527" class="c"><input type="checkbox" id="c-35812527" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#35812441">parent</a><span>|</span><a href="#35812555">prev</a><span>|</span><a href="#35812675">next</a><span>|</span><label class="collapse" for="c-35812527">[-]</label><label class="expand" for="c-35812527">[1 more]</label></div><br/><div class="children"><div class="content">Large model trains small model, I suspect you end up with a better training session than purely unsupervised. You can think of it like transfer learning<p>&gt; Knowledge distillation has been successfully used
to transfer knowledge from larger, more competent
teacher models into smaller student models affordable for practical applications<p>It sounds like a bit of Chain of Thought going on too<p>&gt; We propose a new paradigm, Distilling step-bystep, that leverages the ability of LLMs to reason
about their predictions to train smaller models in
a data-efficient way.<p>If we anthropomorphize a bit, you can think of this as a person reading all the things and learning on their own vs learning with a mentor, which results in much better results for humans. So maybe this is an interesting, expected result. It will also be interesting to see how this can be combined with DeepMind&#x27;s RETRO ideas.<p><a href="https:&#x2F;&#x2F;web.mit.edu&#x2F;5.95&#x2F;readings&#x2F;bloom-two-sigma.pdf" rel="nofollow">https:&#x2F;&#x2F;web.mit.edu&#x2F;5.95&#x2F;readings&#x2F;bloom-two-sigma.pdf</a></div><br/></div></div><div id="35812675" class="c"><input type="checkbox" id="c-35812675" checked=""/><div class="controls bullet"><span class="by">svantana</span><span>|</span><a href="#35812441">parent</a><span>|</span><a href="#35812527">prev</a><span>|</span><a href="#35810666">next</a><span>|</span><label class="collapse" for="c-35812675">[-]</label><label class="expand" for="c-35812675">[2 more]</label></div><br/><div class="children"><div class="content">The wording makes it seem that the small one outperforms the large one, all else equal. But PaLM is tested in the few-shot setting, specifically chain-of-thought prompting, which means:<p>770M model uses 80% of dataset<p>540B model uses 0.01% of dataset</div><br/><div id="35812845" class="c"><input type="checkbox" id="c-35812845" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#35812441">root</a><span>|</span><a href="#35812675">parent</a><span>|</span><a href="#35810666">next</a><span>|</span><label class="collapse" for="c-35812845">[-]</label><label class="expand" for="c-35812845">[1 more]</label></div><br/><div class="children"><div class="content">Part of what is going on here is that this paradigm is to train a smaller, task-specific model from a large, generalized model. When comparing to other methods for doing this, it outperforms. So one of these smaller, specialized models can outperform PaLM on some tasks, but not all.<p>The other benefit comes at inference time, where you can get much faster and cheaper outputs.</div><br/></div></div></div></div></div></div><div id="35810666" class="c"><input type="checkbox" id="c-35810666" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#35812441">prev</a><span>|</span><a href="#35811506">next</a><span>|</span><label class="collapse" for="c-35810666">[-]</label><label class="expand" for="c-35810666">[3 more]</label></div><br/><div class="children"><div class="content">Abstract:<p>Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled&#x2F;unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our 770M T5 model outperforms the 540B PaLM model using only 80% of available data on a benchmark task.</div><br/><div id="35812231" class="c"><input type="checkbox" id="c-35812231" checked=""/><div class="controls bullet"><span class="by">bjornsing</span><span>|</span><a href="#35810666">parent</a><span>|</span><a href="#35811506">next</a><span>|</span><label class="collapse" for="c-35812231">[-]</label><label class="expand" for="c-35812231">[2 more]</label></div><br/><div class="children"><div class="content">That last bit sounds impressiveâ¦</div><br/><div id="35812400" class="c"><input type="checkbox" id="c-35812400" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#35810666">root</a><span>|</span><a href="#35812231">parent</a><span>|</span><a href="#35811506">next</a><span>|</span><label class="collapse" for="c-35812400">[-]</label><label class="expand" for="c-35812400">[1 more]</label></div><br/><div class="children"><div class="content">It does, they are largely using a full sized LLM to guide a smaller one, but it does mean we can pool resources for the large LLM and then build a company smaller LLM for wide distribution.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>