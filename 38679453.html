<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1702890066737" as="style"/><link rel="stylesheet" href="styles.css?v=1702890066737"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/aymenfurter/microagents">Show HN: Microagents: Agents capable of self-editing their prompts / Python code</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>gourmetcode</span> | <span>45 comments</span></div><br/><div><div id="38680231" class="c"><input type="checkbox" id="c-38680231" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#38680026">next</a><span>|</span><label class="collapse" for="c-38680231">[-]</label><label class="expand" for="c-38680231">[4 more]</label></div><br/><div class="children"><div class="content">I built a few multi agent systems and went down a rabbit hole where I reached an important conclusion - From the perspective of the LLM, the prompt&#x2F;context is the only thing that ever matters. Everything about how your agent will behave ultimately boils down to this.<p>I had a bunch of fancy stuff like agents collaborating by passing messages and interpreting them with their own prompts and function calls. Then I realized I could collapse all of my &quot;agents&quot; into one dynamic prompt that tracks state in a stupid simple text region. Passing messages around was playing in very expensive traffic at the end of the day.<p>This is ultimately about information and spinning up an entire matrix of &quot;agents&quot; to process stream of info from A to B seems quite wasteful when many clear alternatives exist.<p>If we are seeking emergence, then perhaps this mental model still fits better. But, for practical targeted solutions, I think it&#x27;s a huge distraction.</div><br/><div id="38680456" class="c"><input type="checkbox" id="c-38680456" checked=""/><div class="controls bullet"><span class="by">joshcho</span><span>|</span><a href="#38680231">parent</a><span>|</span><a href="#38680304">next</a><span>|</span><label class="collapse" for="c-38680456">[-]</label><label class="expand" for="c-38680456">[1 more]</label></div><br/><div class="children"><div class="content">Interested in connecting. I have been creating a DSL for LLMs&#x2F;agents inspired by Forth&#x2F;Prolog.<p>Feel free to e-mail me at joshcho@stanford.edu</div><br/></div></div><div id="38680304" class="c"><input type="checkbox" id="c-38680304" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38680231">parent</a><span>|</span><a href="#38680456">prev</a><span>|</span><a href="#38680026">next</a><span>|</span><label class="collapse" for="c-38680304">[-]</label><label class="expand" for="c-38680304">[2 more]</label></div><br/><div class="children"><div class="content">yes, but multiprompts are fragile while dividing LLM work over multiple calls more stable<p>if you can get away with using GPT-3.5 you got speed and decent pricing</div><br/></div></div></div></div><div id="38680026" class="c"><input type="checkbox" id="c-38680026" checked=""/><div class="controls bullet"><span class="by">jumploops</span><span>|</span><a href="#38680231">prev</a><span>|</span><a href="#38680477">next</a><span>|</span><label class="collapse" for="c-38680026">[-]</label><label class="expand" for="c-38680026">[2 more]</label></div><br/><div class="children"><div class="content">We do something similar with Magic Loops[0], but within the context of generating a single &quot;loop&quot; (automation).<p>We&#x27;ve found that LLMs are pretty bad at prompting other LLMs, unless the problem at hand is very limited in scope. It&#x27;s too easy to get incorrect&#x2F;expensive behavior otherwise (e.g. starts building a framework against an imaginary API, instead of using an existing tool).<p>Our approach looks more like a state machine under the hood, mostly code with LLM-based &quot;magic&quot; sprinkled throughout.<p>The tool can both edit code and LLM &quot;blocks&quot; as it sees fit, allowing it to change it&#x27;s functionality and prompting dynamically.<p>Interestingly, we first set the validate-&gt;fix threshold to N=3, but the &quot;agent&quot; often gets stuck in a pattern of retries based on low quality user input.<p>[0] Feel free to give our tool a try, it&#x27;s very much in an alpha state: <a href="https:&#x2F;&#x2F;magicloops.dev&#x2F;">https:&#x2F;&#x2F;magicloops.dev&#x2F;</a></div><br/><div id="38680350" class="c"><input type="checkbox" id="c-38680350" checked=""/><div class="controls bullet"><span class="by">3cats-in-a-coat</span><span>|</span><a href="#38680026">parent</a><span>|</span><a href="#38680477">next</a><span>|</span><label class="collapse" for="c-38680350">[-]</label><label class="expand" for="c-38680350">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what keeps LLMs from being self-sufficient it seems. Life has this peculiar ability to always keep basic necessities at the center, and always default to them, so the loop can keep going:<p>1. Try your hardest not to die.<p>2. Try your hardest to reproduce.<p>3. Try to thrive maybe also while at it, but DON&#x27;T FORGET 1 AND 2!<p>This is why humans are this weird combination of instinct and reasoning that we find hard to control. The limbic system is very basic, but very strong and very stable. The primary loop. Everything else allows us to go farther, but when we fail we go back to relying on the limbic system.<p>If we allowed reason to take over instinct, we&#x27;d likely end ourselves. Not that we can&#x27;t end ourselves by also relying on instinct but essentially the strategy there is: sacrifice the intelligent shell and let the basic loop continue so we can rebuild the intelligent shell in a new, more stable and suited for the environment form.</div><br/></div></div></div></div><div id="38680477" class="c"><input type="checkbox" id="c-38680477" checked=""/><div class="controls bullet"><span class="by">xBeats99</span><span>|</span><a href="#38680026">prev</a><span>|</span><a href="#38680460">next</a><span>|</span><label class="collapse" for="c-38680477">[-]</label><label class="expand" for="c-38680477">[1 more]</label></div><br/><div class="children"><div class="content">Any recent book recommendations on developing agent based systems?</div><br/></div></div><div id="38680460" class="c"><input type="checkbox" id="c-38680460" checked=""/><div class="controls bullet"><span class="by">xBeats99</span><span>|</span><a href="#38680477">prev</a><span>|</span><a href="#38680160">next</a><span>|</span><label class="collapse" for="c-38680460">[-]</label><label class="expand" for="c-38680460">[1 more]</label></div><br/><div class="children"><div class="content">Any recommendations for modern books on agent based systems?</div><br/></div></div><div id="38680160" class="c"><input type="checkbox" id="c-38680160" checked=""/><div class="controls bullet"><span class="by">johntopia</span><span>|</span><a href="#38680460">prev</a><span>|</span><a href="#38680083">next</a><span>|</span><label class="collapse" for="c-38680160">[-]</label><label class="expand" for="c-38680160">[2 more]</label></div><br/><div class="children"><div class="content">I made a system that dynamically generates agents. This looks similar, so I will take a look into it! Great work :)</div><br/><div id="38680258" class="c"><input type="checkbox" id="c-38680258" checked=""/><div class="controls bullet"><span class="by">Nidhug</span><span>|</span><a href="#38680160">parent</a><span>|</span><a href="#38680083">next</a><span>|</span><label class="collapse" for="c-38680258">[-]</label><label class="expand" for="c-38680258">[1 more]</label></div><br/><div class="children"><div class="content">Was it Dave Shapiro&#x27;s agent swarm <a href="https:&#x2F;&#x2F;github.com&#x2F;daveshap&#x2F;OpenAI_Agent_Swarm">https:&#x2F;&#x2F;github.com&#x2F;daveshap&#x2F;OpenAI_Agent_Swarm</a> or something else ? Do you have a link ? I am quite curious</div><br/></div></div></div></div><div id="38680083" class="c"><input type="checkbox" id="c-38680083" checked=""/><div class="controls bullet"><span class="by">Oras</span><span>|</span><a href="#38680160">prev</a><span>|</span><a href="#38680392">next</a><span>|</span><label class="collapse" for="c-38680083">[-]</label><label class="expand" for="c-38680083">[1 more]</label></div><br/><div class="children"><div class="content">That’s a clever implementation. I can see the same method used to generate the optimum prompt based on results.</div><br/></div></div><div id="38680392" class="c"><input type="checkbox" id="c-38680392" checked=""/><div class="controls bullet"><span class="by">alfajoe</span><span>|</span><a href="#38680083">prev</a><span>|</span><a href="#38680335">next</a><span>|</span><label class="collapse" for="c-38680392">[-]</label><label class="expand" for="c-38680392">[1 more]</label></div><br/><div class="children"><div class="content">Hi  I can see tou</div><br/></div></div><div id="38680335" class="c"><input type="checkbox" id="c-38680335" checked=""/><div class="controls bullet"><span class="by">3cats-in-a-coat</span><span>|</span><a href="#38680392">prev</a><span>|</span><a href="#38679943">next</a><span>|</span><label class="collapse" for="c-38680335">[-]</label><label class="expand" for="c-38680335">[1 more]</label></div><br/><div class="children"><div class="content">If you close that loop reliably enough it can literally go haywire and take over the planet. :P</div><br/></div></div><div id="38679943" class="c"><input type="checkbox" id="c-38679943" checked=""/><div class="controls bullet"><span class="by">wg0</span><span>|</span><a href="#38680335">prev</a><span>|</span><a href="#38680021">next</a><span>|</span><label class="collapse" for="c-38679943">[-]</label><label class="expand" for="c-38679943">[12 more]</label></div><br/><div class="children"><div class="content">These and similar attempts are like those &quot;perpetual motion machines&quot; that captivated people&#x27;s imagination for a long time.<p>That the machine will generate energy and that would consume energy and thus will keep moving forever.<p>Similar hopes here and everywhere.<p>A model will generate its own input and will watch it&#x27;s own output and in process, will become more intelligent than it really is.</div><br/><div id="38680082" class="c"><input type="checkbox" id="c-38680082" checked=""/><div class="controls bullet"><span class="by">travisjungroth</span><span>|</span><a href="#38679943">parent</a><span>|</span><a href="#38680040">next</a><span>|</span><label class="collapse" for="c-38680082">[-]</label><label class="expand" for="c-38680082">[6 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not a fitting analogy because there is no equivalent to the laws of thermodynamics that says a system can&#x27;t be self improving. You&#x27;re dismissing a technological possibility without much reason.<p>We are extremely similar genetically to our ancestors of 100k years ago. The big difference is cultural inheritance. We come up with ideas and objects and pass them down. Humans have improved the capabilities of humans. There&#x27;s no reason to think computer programs aren&#x27;t capable of the same.</div><br/><div id="38680186" class="c"><input type="checkbox" id="c-38680186" checked=""/><div class="controls bullet"><span class="by">geraneum</span><span>|</span><a href="#38679943">root</a><span>|</span><a href="#38680082">parent</a><span>|</span><a href="#38680143">next</a><span>|</span><label class="collapse" for="c-38680186">[-]</label><label class="expand" for="c-38680186">[3 more]</label></div><br/><div class="children"><div class="content">It’s not that straightforward. Quoting Yann LeCun from a LinkedIn post [1]:<p>&gt; I have claimed that Auto-Regressive LLMs are exponentially diverging diffusion processes.
Here is the argument:
Let e be the probability that any generated token exits the tree of &quot;correct&quot; answers.
Then the probability that an answer of length n is correct is (1-e)^n.<p>&gt; Errors accumulate. The probability of correctness decreases exponentially.
One can mitigate the problem by making e smaller (through training) but one simply cannot eliminate the problem entirely.
A solution would require to make LLMs non auto-regressive while preserving their fluency.<p>Moreover, I think the human analogy doesn’t quite fit here. There’s no evidence that today’s human is cognitively more capable than their human ancestors. It’s a conflation of technology and transference of knowledge with intelligence.<p>1: <a href="https:&#x2F;&#x2F;www.linkedin.com&#x2F;posts&#x2F;yann-lecun_i-have-claimed-that-auto-regressive-llms-activity-7045908925660950528-hJGk" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.linkedin.com&#x2F;posts&#x2F;yann-lecun_i-have-claimed-tha...</a></div><br/><div id="38680406" class="c"><input type="checkbox" id="c-38680406" checked=""/><div class="controls bullet"><span class="by">Cyphase</span><span>|</span><a href="#38679943">root</a><span>|</span><a href="#38680186">parent</a><span>|</span><a href="#38680381">next</a><span>|</span><label class="collapse" for="c-38680406">[-]</label><label class="expand" for="c-38680406">[1 more]</label></div><br/><div class="children"><div class="content">&gt; There’s no evidence that today’s human is cognitively more capable than their human ancestors. It’s a conflation of technology and transference of knowledge with intelligence.<p>That was their point. See here:<p>&gt; We are extremely similar genetically to our ancestors of 100k years ago. The big difference is cultural inheritance. We come up with ideas and objects and pass them down.</div><br/></div></div><div id="38680381" class="c"><input type="checkbox" id="c-38680381" checked=""/><div class="controls bullet"><span class="by">spi</span><span>|</span><a href="#38679943">root</a><span>|</span><a href="#38680186">parent</a><span>|</span><a href="#38680406">prev</a><span>|</span><a href="#38680143">next</a><span>|</span><label class="collapse" for="c-38680381">[-]</label><label class="expand" for="c-38680381">[1 more]</label></div><br/><div class="children"><div class="content">I guess I should be wiser than contradicting LeCun on a public forum, but his math doesn&#x27;t really work out. It only works if there is a unique correct answer to any question, in which case e=1&#x2F;dict_size which is clearly false - even humans can just put &quot;Let me think...&quot; in front of anything else and get logically &quot;the same&quot; answer; after how many such filler words do you deem it &quot;wrong&quot;? Even taking colloquial speech aside, in Math, there is apparently a book that collects 367 different proofs of the Pythagorean Theorem; that tree of correct answers is certainly quite complicated. You can&#x27;t approximate it with a fixed probability and take the exponential - the number of possibly correct next tokens varies very strongly depending on the previous string.<p>LLMs and autoregression are very good at avoiding the 99.999[...]% of the strings that are simple gibberish. If you were to generate a string made of 20 random tokens from GPT2 tokenizer, you would get something like:<p>&quot;automakersGrand carries liberties Occupations ongoingOULDessing heartbeat Pillar intrigued Trotskymediatelyearable founding examinations lavAg redesign folds&quot;<p>and of course any half decent language model does much better than that. If the &quot;paths to truth&quot; were as unlikely as LeCun puts it there would be no hope.<p>A non-autoregressive model would certainly be &quot;better&quot; because it would be faster, which is where language models started from (BERT &amp; co.), it just doesn&#x27;t seem to work as well... similarly to how a human sometimes needs to write something down and only realizes the correct answer to a complicated question on the go.<p>If anything, we&#x27;d need to allow LLMs to realize mistakes and correct themselves out of them, i.e. making the generation non-linear. If you ask GPT4 something complicated (like math) it&#x27;s not rare at all that it logically contradicts itself in their answers. I would be surprised if, somewhere deep in the model, it doesn&#x27;t &quot;realize&quot; this, but it can&#x27;t fix it, so it falls back to what humans do at an exam or interview that started badly: try to bullshit their way out of the thing, sweeping the inconsistency under the carpet, unless you explicitly point it to them (and often even after that, both GPT4 and humans).<p>P.S. Mathematician rant: who on Earth calls a probability &quot;e&quot;??</div><br/></div></div></div></div><div id="38680143" class="c"><input type="checkbox" id="c-38680143" checked=""/><div class="controls bullet"><span class="by">yreg</span><span>|</span><a href="#38679943">root</a><span>|</span><a href="#38680082">parent</a><span>|</span><a href="#38680186">prev</a><span>|</span><a href="#38680270">next</a><span>|</span><label class="collapse" for="c-38680143">[-]</label><label class="expand" for="c-38680143">[1 more]</label></div><br/><div class="children"><div class="content">We also have proven self improving AIs - GANs. Or even evolutionary algorithms.</div><br/></div></div><div id="38680270" class="c"><input type="checkbox" id="c-38680270" checked=""/><div class="controls bullet"><span class="by">wg0</span><span>|</span><a href="#38679943">root</a><span>|</span><a href="#38680082">parent</a><span>|</span><a href="#38680143">prev</a><span>|</span><a href="#38680040">next</a><span>|</span><label class="collapse" for="c-38680270">[-]</label><label class="expand" for="c-38680270">[1 more]</label></div><br/><div class="children"><div class="content">Now you&#x27;re falsely equating biological brains with giant arrays of floating point numbers.<p>we have NOT evolved where we are today by generating inputs to ourselves, watching our own output and modifying our inputs to watch again our outputs.<p>Rather - we had fundamental strong reasoning almost flawless rigorous (formally documented) capabilities from the get go. Interference, induction, deduction. LLMs have none of that and it&#x27;s documented all over multiple times over.<p>We have built at that on top of those faculties again - by not giving input to us and observing our outputs rather poking around into the world with are reasoning and cognitive faculties and arranging&#x2F;categorising carefully what we discovered.<p>So for fanboys, this surely is very huge and I respect that sentiment wholeheartedly.<p>EDIT: Note about reasoning</div><br/></div></div></div></div><div id="38680040" class="c"><input type="checkbox" id="c-38680040" checked=""/><div class="controls bullet"><span class="by">andruby</span><span>|</span><a href="#38679943">parent</a><span>|</span><a href="#38680082">prev</a><span>|</span><a href="#38680317">next</a><span>|</span><label class="collapse" for="c-38680040">[-]</label><label class="expand" for="c-38680040">[2 more]</label></div><br/><div class="children"><div class="content">I see the resemblance, but for perpetual motion machines, we have an actual physical law, the 2nd law of thermodynamics, that makes it impossible.<p>As far as I know, we can&#x27;t yet disprove that a self-improving AI will become more intelligent than it starts as.<p>I&#x27;m not trying to say that it is likely or that I believe it will happen. Just that we can&#x27;t disprove it yet, unlike perpetual motion devices.<p>EDIT: corrected from 3rd to 2nd law of thermodynamics.</div><br/><div id="38680154" class="c"><input type="checkbox" id="c-38680154" checked=""/><div class="controls bullet"><span class="by">majoe</span><span>|</span><a href="#38679943">root</a><span>|</span><a href="#38680040">parent</a><span>|</span><a href="#38680317">next</a><span>|</span><label class="collapse" for="c-38680154">[-]</label><label class="expand" for="c-38680154">[1 more]</label></div><br/><div class="children"><div class="content">Small nitpick: I think you mean the second law of thermodynamics.<p>Also the second law of thermodynamics is an empirical law. It is based on observations and not proven.
To say, that a perpetual motion machine can&#x27;t exist because of the 2nd law, is circular logic, since the 2nd law was established precisely because a perpetual motion machine was never observed.<p>That&#x27;s from the POV of thermodynamics, though, you can show with statistical mechanics, that the probability for a process that defies this laws goes to zero.</div><br/></div></div></div></div><div id="38680317" class="c"><input type="checkbox" id="c-38680317" checked=""/><div class="controls bullet"><span class="by">conjectures</span><span>|</span><a href="#38679943">parent</a><span>|</span><a href="#38680040">prev</a><span>|</span><a href="#38679979">next</a><span>|</span><label class="collapse" for="c-38680317">[-]</label><label class="expand" for="c-38680317">[1 more]</label></div><br/><div class="children"><div class="content">&gt; A model will generate its own input and will watch it&#x27;s own output and in process, will become more intelligent than it really is.<p>You&#x27;re aware this is basically how AlphaGo was trained - playing against itself?</div><br/></div></div><div id="38679979" class="c"><input type="checkbox" id="c-38679979" checked=""/><div class="controls bullet"><span class="by">phreeza</span><span>|</span><a href="#38679943">parent</a><span>|</span><a href="#38680317">prev</a><span>|</span><a href="#38680167">next</a><span>|</span><label class="collapse" for="c-38679979">[-]</label><label class="expand" for="c-38679979">[1 more]</label></div><br/><div class="children"><div class="content">This is also the percept and possibly fallacy that underlies the  fast takeoff AI singularity thinking.</div><br/></div></div></div></div><div id="38680021" class="c"><input type="checkbox" id="c-38680021" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#38679943">prev</a><span>|</span><a href="#38679757">next</a><span>|</span><label class="collapse" for="c-38680021">[-]</label><label class="expand" for="c-38680021">[2 more]</label></div><br/><div class="children"><div class="content">If the author reads this: would highly recommend reading and thinking for a bit about safety :).  Great project!<p><a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;kpPnReyBC54KESiSn&#x2F;optimality-is-the-tiger-and-agents-are-its-teeth" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;kpPnReyBC54KESiSn&#x2F;optimality...</a></div><br/><div id="38680300" class="c"><input type="checkbox" id="c-38680300" checked=""/><div class="controls bullet"><span class="by">sho_hn</span><span>|</span><a href="#38680021">parent</a><span>|</span><a href="#38679757">next</a><span>|</span><label class="collapse" for="c-38680300">[-]</label><label class="expand" for="c-38680300">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s amusing to think that this might be a self-fulfilling prophecy because it ends up in training data.</div><br/></div></div></div></div><div id="38679757" class="c"><input type="checkbox" id="c-38679757" checked=""/><div class="controls bullet"><span class="by">dukeofdoom</span><span>|</span><a href="#38680021">prev</a><span>|</span><a href="#38679919">next</a><span>|</span><label class="collapse" for="c-38679757">[-]</label><label class="expand" for="c-38679757">[11 more]</label></div><br/><div class="children"><div class="content">Add replication and you get virus like behavior. Not sure if this a good idea.</div><br/><div id="38679908" class="c"><input type="checkbox" id="c-38679908" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#38679757">parent</a><span>|</span><a href="#38679770">next</a><span>|</span><label class="collapse" for="c-38679908">[-]</label><label class="expand" for="c-38679908">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a separate project called ReplicatorAgent. Unfortunately the Stargate program has had a number of run ins with them and the Air Force is merciless in shutting those kinds of projects down.</div><br/><div id="38680108" class="c"><input type="checkbox" id="c-38680108" checked=""/><div class="controls bullet"><span class="by">budoso</span><span>|</span><a href="#38679757">root</a><span>|</span><a href="#38679908">parent</a><span>|</span><a href="#38679770">next</a><span>|</span><label class="collapse" for="c-38680108">[-]</label><label class="expand" for="c-38680108">[3 more]</label></div><br/><div class="children"><div class="content">Why the Air Force specifically?</div><br/><div id="38680326" class="c"><input type="checkbox" id="c-38680326" checked=""/><div class="controls bullet"><span class="by">dharmab</span><span>|</span><a href="#38679757">root</a><span>|</span><a href="#38680108">parent</a><span>|</span><a href="#38680205">next</a><span>|</span><label class="collapse" for="c-38680326">[-]</label><label class="expand" for="c-38680326">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re the ones with the most experience with the Replicators: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Z45Tpp9DBq4" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Z45Tpp9DBq4</a></div><br/></div></div><div id="38680205" class="c"><input type="checkbox" id="c-38680205" checked=""/><div class="controls bullet"><span class="by">snoman</span><span>|</span><a href="#38679757">root</a><span>|</span><a href="#38680108">parent</a><span>|</span><a href="#38680326">prev</a><span>|</span><a href="#38679770">next</a><span>|</span><label class="collapse" for="c-38680205">[-]</label><label class="expand" for="c-38680205">[1 more]</label></div><br/><div class="children"><div class="content">They are the ones that run the stargate program out of Cheyenne.</div><br/></div></div></div></div></div></div><div id="38679770" class="c"><input type="checkbox" id="c-38679770" checked=""/><div class="controls bullet"><span class="by">PrayagBhakar</span><span>|</span><a href="#38679757">parent</a><span>|</span><a href="#38679908">prev</a><span>|</span><a href="#38679919">next</a><span>|</span><label class="collapse" for="c-38679770">[-]</label><label class="expand" for="c-38679770">[6 more]</label></div><br/><div class="children"><div class="content">But isn’t this also a step towards AGI? The model being able to find issues and self correct?</div><br/><div id="38679852" class="c"><input type="checkbox" id="c-38679852" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#38679757">root</a><span>|</span><a href="#38679770">parent</a><span>|</span><a href="#38679867">next</a><span>|</span><label class="collapse" for="c-38679852">[-]</label><label class="expand" for="c-38679852">[2 more]</label></div><br/><div class="children"><div class="content">As described in the essay &quot;optimality is the tiger, and agents are its teeth&quot;.<p><a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;kpPnReyBC54KESiSn&#x2F;optimality-is-the-tiger-and-agents-are-its-teeth" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;kpPnReyBC54KESiSn&#x2F;optimality...</a></div><br/><div id="38680044" class="c"><input type="checkbox" id="c-38680044" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#38679757">root</a><span>|</span><a href="#38679852">parent</a><span>|</span><a href="#38679867">next</a><span>|</span><label class="collapse" for="c-38680044">[-]</label><label class="expand" for="c-38680044">[1 more]</label></div><br/><div class="children"><div class="content">lol posted the same thing above. I’m glad I’m not the only one who thought that was an extremely powerful read! As someone currently trying to imbue silicon with an eternal soul, it’s very sobering.</div><br/></div></div></div></div><div id="38679867" class="c"><input type="checkbox" id="c-38679867" checked=""/><div class="controls bullet"><span class="by">graphe</span><span>|</span><a href="#38679757">root</a><span>|</span><a href="#38679770">parent</a><span>|</span><a href="#38679852">prev</a><span>|</span><a href="#38679832">next</a><span>|</span><label class="collapse" for="c-38679867">[-]</label><label class="expand" for="c-38679867">[2 more]</label></div><br/><div class="children"><div class="content">What does AGI man to you? Is it something they define or you think is good enough, And if the latter when?</div><br/><div id="38680038" class="c"><input type="checkbox" id="c-38680038" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#38679757">root</a><span>|</span><a href="#38679867">parent</a><span>|</span><a href="#38679832">next</a><span>|</span><label class="collapse" for="c-38680038">[-]</label><label class="expand" for="c-38680038">[1 more]</label></div><br/><div class="children"><div class="content">Eh regardless of all that the answer is just “yes” imo:  self-improvement is a necessary step to meaningful superintelligence. Perhaps not AGI but it obviously seems like a massive help if not absolutely strictly necessary.<p>In a way, that’s what modern multi-stage-trained foundational models <i>already</i> do: improve their weights intelligently. Having the same results in human readable code (what this is a first step towards) would be a lot more powerful…</div><br/></div></div></div></div><div id="38679832" class="c"><input type="checkbox" id="c-38679832" checked=""/><div class="controls bullet"><span class="by">mxsjoberg</span><span>|</span><a href="#38679757">root</a><span>|</span><a href="#38679770">parent</a><span>|</span><a href="#38679867">prev</a><span>|</span><a href="#38679919">next</a><span>|</span><label class="collapse" for="c-38679832">[-]</label><label class="expand" for="c-38679832">[1 more]</label></div><br/><div class="children"><div class="content">Yes.</div><br/></div></div></div></div></div></div><div id="38679919" class="c"><input type="checkbox" id="c-38679919" checked=""/><div class="controls bullet"><span class="by">withinboredom</span><span>|</span><a href="#38679757">prev</a><span>|</span><label class="collapse" for="c-38679919">[-]</label><label class="expand" for="c-38679919">[6 more]</label></div><br/><div class="children"><div class="content">Death by semantic error.<p>An AI tried to tell me the {} opens a new block scope in PHP and any variables in it are scoped to that block. I nearly lol’d and the code it gave me was so wrong it was cringey. Hopefully it is better at Python.</div><br/><div id="38679959" class="c"><input type="checkbox" id="c-38679959" checked=""/><div class="controls bullet"><span class="by">jacksensi</span><span>|</span><a href="#38679919">parent</a><span>|</span><a href="#38679988">next</a><span>|</span><label class="collapse" for="c-38679959">[-]</label><label class="expand" for="c-38679959">[1 more]</label></div><br/><div class="children"><div class="content">Only python is supported for code execution.</div><br/></div></div><div id="38679988" class="c"><input type="checkbox" id="c-38679988" checked=""/><div class="controls bullet"><span class="by">anonzzzies</span><span>|</span><a href="#38679919">parent</a><span>|</span><a href="#38679959">prev</a><span>|</span><label class="collapse" for="c-38679988">[-]</label><label class="expand" for="c-38679988">[4 more]</label></div><br/><div class="children"><div class="content">Google and many people will tell you the same thing. So you must have a lot of lol moments in a day.<p>Gpt4 is quite good (better than most humans I ever met) at php, but bad at facts; don’t ask it facts, ask it to write code. That’s what you would ask a human (outside interviews).<p>Disclaimer; I am formally trained with proofs and proof assistants and I hate the current timeline where we ask ai to drivel up code, but I cannot say it’s bad at doing it; it just is not necessarily sound or even working code sure, but that’s the same as with most human first tries. Then you iterate and make better. My days of ‘I have proven it correct, now I just have to type it in’ are long gone, at least for things that pay for my bread.</div><br/><div id="38680080" class="c"><input type="checkbox" id="c-38680080" checked=""/><div class="controls bullet"><span class="by">nicklecompte</span><span>|</span><a href="#38679919">root</a><span>|</span><a href="#38679988">parent</a><span>|</span><a href="#38680039">next</a><span>|</span><label class="collapse" for="c-38680080">[-]</label><label class="expand" for="c-38680080">[2 more]</label></div><br/><div class="children"><div class="content">When saying LLMs are &quot;good at writing code&quot; there&#x27;s a distinction between<p>&quot;good at taking high-level English-language solutions to programming problems and translating them into low-level implementations via a specific language &#x2F; design patterns &#x2F; etc&quot;<p>versus<p>&quot;good at finding solutions to programming problems.&quot;<p>GPT-4 is indeed quite good at the former - and the former is what most enterprise programming work actually is. A lot of the hard part of professional software development is understanding the problem well enough to describe a solution in English: once you do that, writing the C# or Java is typically somewhat rote. Likewise LLMs are genuinely useful when you know exactly what you want to do with a 3rd-party library, but have to trawl through a bunch of API documentation to figure out the magic words.<p>All that said, LLMs still really suck at the latter problem: <a href="https:&#x2F;&#x2F;www.aisnakeoil.com&#x2F;p&#x2F;gpt-4-and-professional-benchmarks" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.aisnakeoil.com&#x2F;p&#x2F;gpt-4-and-professional-benchmar...</a> OpenAI&#x27;s benchmarks fall off quite badly with actual programming benchmarks, compared to simpler tests of code generation. If it involves managing state, creating novel data structures, counting to numbers higher than 3, etc, LLMs just aren&#x27;t smart enough.</div><br/><div id="38680093" class="c"><input type="checkbox" id="c-38680093" checked=""/><div class="controls bullet"><span class="by">anonzzzies</span><span>|</span><a href="#38679919">root</a><span>|</span><a href="#38680080">parent</a><span>|</span><a href="#38680039">next</a><span>|</span><label class="collapse" for="c-38680093">[-]</label><label class="expand" for="c-38680093">[1 more]</label></div><br/><div class="children"><div class="content">You are right, hence my disclaimer. For paid work, it works fine and that is indeed enterprise kind of mindnumbing plumbing. For other work, I do not use it (but will try every few months if anything changed of course).</div><br/></div></div></div></div><div id="38680039" class="c"><input type="checkbox" id="c-38680039" checked=""/><div class="controls bullet"><span class="by">asabla</span><span>|</span><a href="#38679919">root</a><span>|</span><a href="#38679988">parent</a><span>|</span><a href="#38680080">prev</a><span>|</span><label class="collapse" for="c-38680039">[-]</label><label class="expand" for="c-38680039">[1 more]</label></div><br/><div class="children"><div class="content">&gt; My days of ‘I have proven it correct, now I just have to type it in’ are long gone, at least for things that pay for my bread.<p>Kind of having the same feeling as well the more I use ChatGPT and similar systems&#x2F;models.<p>I wonder if this will result in a general decline in code quality. Or if it&#x27;s going to help people become better IT professionals overall.<p>I mean, in order to iterate and make things better after first try&#x2F;implementation, also requires understanding what you typed and it can be improved.<p>Exciting times nonetheless</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>