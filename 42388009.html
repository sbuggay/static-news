<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1733994055633" as="style"/><link rel="stylesheet" href="styles.css?v=1733994055633"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://cowfreedom.de/#dot_product/introduction/">The GPU is not always faster</a> <span class="domain">(<a href="https://cowfreedom.de">cowfreedom.de</a>)</span></div><div class="subtext"><span>CowFreedom</span> | <span>80 comments</span></div><br/><div><div id="42389383" class="c"><input type="checkbox" id="c-42389383" checked=""/><div class="controls bullet"><span class="by">ssivark</span><span>|</span><a href="#42390369">next</a><span>|</span><label class="collapse" for="c-42389383">[-]</label><label class="expand" for="c-42389383">[14 more]</label></div><br/><div class="children"><div class="content">A good mental model is to compare the number of floats being processed -vs- the number of primitive computations. Matrix multiplication has n^3 computation with n^2 data. Multiplication of large matrices is therefore special in the potential for &quot;data re-use&quot; (each float is used against all the columns or rows of the other matrix) -- so systems are designed to have a much higher flops throughput than memory bandwidth. A dot product is at the other extreme, where each float is used only once (loosely).<p>Roofline plots [1] are framework to visualize system design from this perspective.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Roofline_model" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Roofline_model</a></div><br/><div id="42390936" class="c"><input type="checkbox" id="c-42390936" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#42389383">parent</a><span>|</span><a href="#42396911">next</a><span>|</span><label class="collapse" for="c-42390936">[-]</label><label class="expand" for="c-42390936">[1 more]</label></div><br/><div class="children"><div class="content">That property is the same reason you don&#x27;t incur substantial overhead doing large matrix multiplications sharded over disks or many machines. You apply the same chunking strategies used to optimally use L1&#x2F;L2&#x2F;L3 caches, just instead at the level of numa nodes, physical disks, machines, and clusters. So long as each &quot;cache&quot; is big enough for the N^3&#x2F;N^2 term to dominate communication overhead (especially if that communication can happen concurrently), the networked result is about as fast as the individual machines running at their max FLOPs for some smaller problem.</div><br/></div></div><div id="42396911" class="c"><input type="checkbox" id="c-42396911" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#42389383">parent</a><span>|</span><a href="#42390936">prev</a><span>|</span><a href="#42389658">next</a><span>|</span><label class="collapse" for="c-42396911">[-]</label><label class="expand" for="c-42396911">[1 more]</label></div><br/><div class="children"><div class="content">This is indeed a good moel. From the article&#x27;s context, dGPUs have more of both bandwidth and flops, the compute-intensity balance isn&#x27;t necessarily the deciding factor on whether there&#x27;s a speedup on GPU.<p>In this article the deciding factor seems to be the startup cost because the application has placed the data on the CPU memory side, and is considering shipping out to GPU memory just for this computation.</div><br/></div></div><div id="42389658" class="c"><input type="checkbox" id="c-42389658" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#42389383">parent</a><span>|</span><a href="#42396911">prev</a><span>|</span><a href="#42390369">next</a><span>|</span><label class="collapse" for="c-42389658">[-]</label><label class="expand" for="c-42389658">[11 more]</label></div><br/><div class="children"><div class="content">This is amplified even more by the fact that only the trivial implementation of matmul is O(n^3) whereas efficient ones (e.g BLAS) use things like the Strassen algorithm. You can also speed it up significantly by using cache-aware approaches when retrieving rows and columns. In practice there is a huge amount of theory behind this that is far beyond the average person&#x27;s scope if they are not actual researchers.</div><br/><div id="42389826" class="c"><input type="checkbox" id="c-42389826" checked=""/><div class="controls bullet"><span class="by">rnrn</span><span>|</span><a href="#42389383">root</a><span>|</span><a href="#42389658">parent</a><span>|</span><a href="#42394566">next</a><span>|</span><label class="collapse" for="c-42389826">[-]</label><label class="expand" for="c-42389826">[5 more]</label></div><br/><div class="children"><div class="content">Is there actually a BLAS implementation that uses strassen?<p>I don’t think it’s accurate that only trivial implementations use the direct o(n^3) algorithm. AFAIK high performance BLAS implementations just use highly optimized versions of it.</div><br/><div id="42392482" class="c"><input type="checkbox" id="c-42392482" checked=""/><div class="controls bullet"><span class="by">leecarraher</span><span>|</span><a href="#42389383">root</a><span>|</span><a href="#42389826">parent</a><span>|</span><a href="#42390688">next</a><span>|</span><label class="collapse" for="c-42392482">[-]</label><label class="expand" for="c-42392482">[1 more]</label></div><br/><div class="children"><div class="content">BLAS is just the library definition and not the implementation, so BLAS implementations could implement GEMM anyway they want. But in practice the triple loop method (n^3) is the most common, despite Strassen&#x27;s and the more numerically stable, Winograd methods being well known and available for decades. But with most things involving real computing hardware, memory access patterns and locality tend to be more important for performance than operation counts</div><br/></div></div><div id="42390688" class="c"><input type="checkbox" id="c-42390688" checked=""/><div class="controls bullet"><span class="by">chessgecko</span><span>|</span><a href="#42389383">root</a><span>|</span><a href="#42389826">parent</a><span>|</span><a href="#42392482">prev</a><span>|</span><a href="#42391378">next</a><span>|</span><label class="collapse" for="c-42390688">[-]</label><label class="expand" for="c-42390688">[1 more]</label></div><br/><div class="children"><div class="content">I remember reading that it’s too hard to get good memory bandwidth&#x2F;l2 utilization in the fancy algorithms, you need to read contiguous blocks and be able to use them repeatedly. But I also haven’t looked at the gpu blas implementations directly.</div><br/></div></div><div id="42391378" class="c"><input type="checkbox" id="c-42391378" checked=""/><div class="controls bullet"><span class="by">jcranmer</span><span>|</span><a href="#42389383">root</a><span>|</span><a href="#42389826">parent</a><span>|</span><a href="#42390688">prev</a><span>|</span><a href="#42394566">next</a><span>|</span><label class="collapse" for="c-42391378">[-]</label><label class="expand" for="c-42391378">[2 more]</label></div><br/><div class="children"><div class="content">AIUI, Strassen gets used moderately commonly with non-floating-point datatypes, where numerical stability is less of a concern and multiplications are more useful to minimize than memory traffic. But from what I can tell, every floating-point BLAS library eschews Strassen, despite a steady trickle of papers saying &quot;hey, there might be some small wins if we go to Strassen!&quot;</div><br/><div id="42392339" class="c"><input type="checkbox" id="c-42392339" checked=""/><div class="controls bullet"><span class="by">chillee</span><span>|</span><a href="#42389383">root</a><span>|</span><a href="#42391378">parent</a><span>|</span><a href="#42394566">next</a><span>|</span><label class="collapse" for="c-42392339">[-]</label><label class="expand" for="c-42392339">[1 more]</label></div><br/><div class="children"><div class="content">The big issue with Strassen isn&#x27;t performance - it&#x27;s numerical stability.</div><br/></div></div></div></div></div></div><div id="42394566" class="c"><input type="checkbox" id="c-42394566" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42389383">root</a><span>|</span><a href="#42389658">parent</a><span>|</span><a href="#42389826">prev</a><span>|</span><a href="#42389827">next</a><span>|</span><label class="collapse" for="c-42394566">[-]</label><label class="expand" for="c-42394566">[2 more]</label></div><br/><div class="children"><div class="content">Not that long ago, I tried using the FFT to do matrix multiplication since it was supposed to be asymptomatically faster. It turns out that the constant factor is huge compared to the O(n^3) grade school algorithm that BLAS optimizes via tiling and other tricks. Even if it looks expensive on paper, the cubic algorithm is fast.<p>I just wish I understood the tricks done to make it so fast so I could implement my own for variations for which there are no pre-existing BLAS implementations. The best BLAS implementations are all closed source sadly.</div><br/><div id="42395668" class="c"><input type="checkbox" id="c-42395668" checked=""/><div class="controls bullet"><span class="by">david-gpu</span><span>|</span><a href="#42389383">root</a><span>|</span><a href="#42394566">parent</a><span>|</span><a href="#42389827">next</a><span>|</span><label class="collapse" for="c-42395668">[-]</label><label class="expand" for="c-42395668">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; The best BLAS implementations are all closed source sadly.</i><p>NVidia open-sourced CUTLASS [0] some years ago and it achieves pretty competitive performance compared to e.g. the closed-source cuBLAS.<p>Keen observers will notice that Strassen is not used in CUTLASS.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;cutlass" rel="nofollow">https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;cutlass</a></div><br/></div></div></div></div><div id="42389827" class="c"><input type="checkbox" id="c-42389827" checked=""/><div class="controls bullet"><span class="by">sestep</span><span>|</span><a href="#42389383">root</a><span>|</span><a href="#42389658">parent</a><span>|</span><a href="#42394566">prev</a><span>|</span><a href="#42390369">next</a><span>|</span><label class="collapse" for="c-42389827">[-]</label><label class="expand" for="c-42389827">[3 more]</label></div><br/><div class="children"><div class="content">Really? I thought that no practical linear algebra library used the Strassen algorithm. Can you provide a source?</div><br/><div id="42389977" class="c"><input type="checkbox" id="c-42389977" checked=""/><div class="controls bullet"><span class="by">CowFreedom</span><span>|</span><a href="#42389383">root</a><span>|</span><a href="#42389827">parent</a><span>|</span><a href="#42390369">next</a><span>|</span><label class="collapse" for="c-42389977">[-]</label><label class="expand" for="c-42389977">[2 more]</label></div><br/><div class="children"><div class="content">The BLAS GEMM routines I have seen use normal blocked algorithms.</div><br/><div id="42391534" class="c"><input type="checkbox" id="c-42391534" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#42389383">root</a><span>|</span><a href="#42389977">parent</a><span>|</span><a href="#42390369">next</a><span>|</span><label class="collapse" for="c-42391534">[-]</label><label class="expand" for="c-42391534">[1 more]</label></div><br/><div class="children"><div class="content">I don’t know what the real convention is, but IMO, BLAS GEMM “is” the O(n^3) algorithm (blocked is fine of course$ in the sense that something like Strassen has stability implications and isn’t appropriate for lots of sizes. Just swapping it in would be nuts, haha.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42390369" class="c"><input type="checkbox" id="c-42390369" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#42389383">prev</a><span>|</span><a href="#42389830">next</a><span>|</span><label class="collapse" for="c-42390369">[-]</label><label class="expand" for="c-42390369">[19 more]</label></div><br/><div class="children"><div class="content">&gt; Each version is severely memory bandwidth bottlenecked, the CUDA version suffers the most with its practical 11.8 GB&#x2F;s device-to-host bandwidth due to its PCI-Express 3.0 x16 interface.<p>PCIe 3.0? What?<p><a href="https:&#x2F;&#x2F;cowfreedom.de&#x2F;#appendix&#x2F;computer_specs&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cowfreedom.de&#x2F;#appendix&#x2F;computer_specs&#x2F;</a><p>&gt; GeForce GTX 1050 Ti with Max-Q Design (PCIe 3.0 x16) (2016)<p>&gt; Intel Core i5-8300H  (2020)<p>This is a low-price 8 year old GPU and a 4 year old CPU. And he seems to be including loading the data to GPU. Newer cards have wide PCIe 5.0 or some faster interconnect, like Nvidia Grace-Hopper.<p>Also he is comparing his own CUDA implementation. He should use one of the many available in CUBLAS&#x2F;CUTLASS. Making a good CUDA GEMM is a very difficult art and very hardware specific</div><br/><div id="42390724" class="c"><input type="checkbox" id="c-42390724" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42390369">parent</a><span>|</span><a href="#42391723">next</a><span>|</span><label class="collapse" for="c-42390724">[-]</label><label class="expand" for="c-42390724">[14 more]</label></div><br/><div class="children"><div class="content">&gt; Newer cards have wide PCIe 5.0 or some faster interconnect, like Nvidia Grace-Hopper.<p>There aren&#x27;t any (edit: consumer) GPUs with PCIe5 yet, though they probably aren&#x27;t far off. Plenty already have PCIe4 though.</div><br/><div id="42390779" class="c"><input type="checkbox" id="c-42390779" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42390724">parent</a><span>|</span><a href="#42391321">next</a><span>|</span><label class="collapse" for="c-42390779">[-]</label><label class="expand" for="c-42390779">[9 more]</label></div><br/><div class="children"><div class="content">Consumer cards are PCIe 4.0 x16. H100 PCIe version is PCIe 5.0  <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hopper_(microarchitecture)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hopper_(microarchitecture)</a>. And it&#x27;s been out 2 years already.</div><br/><div id="42392577" class="c"><input type="checkbox" id="c-42392577" checked=""/><div class="controls bullet"><span class="by">yvdriess</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42390779">parent</a><span>|</span><a href="#42390809">next</a><span>|</span><label class="collapse" for="c-42392577">[-]</label><label class="expand" for="c-42392577">[2 more]</label></div><br/><div class="children"><div class="content">For the consumer GPUs, PCIe 4.0 x16 has plenty of BW headroom. The full sized x16 is more for stability reasons. Some vendors even put a couple of M.2 slots on PCI4&#x2F;5 GPU board to recuperate the unused PCIe lanes.</div><br/></div></div><div id="42390809" class="c"><input type="checkbox" id="c-42390809" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42390779">parent</a><span>|</span><a href="#42392577">prev</a><span>|</span><a href="#42390807">next</a><span>|</span><label class="collapse" for="c-42390809">[-]</label><label class="expand" for="c-42390809">[1 more]</label></div><br/><div class="children"><div class="content">TIL, I missed Hopper already having it. I assume the RTX 5000 series will bring it to consumers.</div><br/></div></div><div id="42390807" class="c"><input type="checkbox" id="c-42390807" checked=""/><div class="controls bullet"><span class="by">throwway120385</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42390779">parent</a><span>|</span><a href="#42390809">prev</a><span>|</span><a href="#42393675">next</a><span>|</span><label class="collapse" for="c-42390807">[-]</label><label class="expand" for="c-42390807">[4 more]</label></div><br/><div class="children"><div class="content">So everyone is supposed to do all of their testing on H100&#x27;s?</div><br/><div id="42391676" class="c"><input type="checkbox" id="c-42391676" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42390807">parent</a><span>|</span><a href="#42391714">next</a><span>|</span><label class="collapse" for="c-42391676">[-]</label><label class="expand" for="c-42391676">[1 more]</label></div><br/><div class="children"><div class="content">4090 (2022) PCIe 4.0 x16 is quite decent. The major limit is memory, not bandwidth. And 3090 (2020) is also PCIe 4.0 x16, and used cards are a bargain. You can hook them up with Nvlink.<p>Nvidia is withholding new releases but the current hardware has more legs with new matrix implementations. Like FlashAttention doing some significant improvement every 6 months.<p>Nvidia could make consumer chips with combined CPU-GPU. I guess they are too busy making money with the big cloud. Maybe somebody will pick up. Apple is already doing something like it even on laptops.</div><br/></div></div><div id="42391714" class="c"><input type="checkbox" id="c-42391714" checked=""/><div class="controls bullet"><span class="by">_zoltan_</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42390807">parent</a><span>|</span><a href="#42391676">prev</a><span>|</span><a href="#42393675">next</a><span>|</span><label class="collapse" for="c-42391714">[-]</label><label class="expand" for="c-42391714">[2 more]</label></div><br/><div class="children"><div class="content">get a GH100 on lambda and behold you have 900GB&#x2F;s between CPU memory and GPU, and forget PCIe.</div><br/><div id="42397169" class="c"><input type="checkbox" id="c-42397169" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42391714">parent</a><span>|</span><a href="#42393675">next</a><span>|</span><label class="collapse" for="c-42397169">[-]</label><label class="expand" for="c-42397169">[1 more]</label></div><br/><div class="children"><div class="content">Where are you seeing 900 GB&#x2F;s?</div><br/></div></div></div></div></div></div><div id="42393675" class="c"><input type="checkbox" id="c-42393675" checked=""/><div class="controls bullet"><span class="by">touisteur</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42390779">parent</a><span>|</span><a href="#42390807">prev</a><span>|</span><a href="#42391321">next</a><span>|</span><label class="collapse" for="c-42393675">[-]</label><label class="expand" for="c-42393675">[1 more]</label></div><br/><div class="children"><div class="content">Well let me tell you about the pretty high-end and expensive L40 that shipped with PCIe-4.0 to my utter dismay and disgust. Only the H100 had 5.0 although I could already saturate 4.0 (and 5.0) with Mellanox NICs and GPU&#x2F;StorageDirect. Waiting for the next one to maybe get 5.0.</div><br/></div></div></div></div><div id="42391321" class="c"><input type="checkbox" id="c-42391321" checked=""/><div class="controls bullet"><span class="by">goosedragons</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42390724">parent</a><span>|</span><a href="#42390779">prev</a><span>|</span><a href="#42391700">next</a><span>|</span><label class="collapse" for="c-42391321">[-]</label><label class="expand" for="c-42391321">[3 more]</label></div><br/><div class="children"><div class="content">Supposedly the Intel B580 releasing friday will use PCIe 5.0 8x.</div><br/><div id="42392623" class="c"><input type="checkbox" id="c-42392623" checked=""/><div class="controls bullet"><span class="by">yvdriess</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42391321">parent</a><span>|</span><a href="#42391700">next</a><span>|</span><label class="collapse" for="c-42392623">[-]</label><label class="expand" for="c-42392623">[2 more]</label></div><br/><div class="children"><div class="content">PCIe 4.0 16x<p><a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;products&#x2F;sku&#x2F;227961&#x2F;intel-arc-a580-graphics&#x2F;specifications.html" rel="nofollow">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;products&#x2F;sku&#x2F;227961&#x2F;...</a></div><br/><div id="42395682" class="c"><input type="checkbox" id="c-42395682" checked=""/><div class="controls bullet"><span class="by">phonon</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42392623">parent</a><span>|</span><a href="#42391700">next</a><span>|</span><label class="collapse" for="c-42395682">[-]</label><label class="expand" for="c-42395682">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s A580. Correct link <a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;products&#x2F;sku&#x2F;241598&#x2F;intel-arc-b580-graphics&#x2F;specifications.html" rel="nofollow">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;products&#x2F;sku&#x2F;241598&#x2F;...</a><p>&quot;PCI Express 4.0 x8&quot;</div><br/></div></div></div></div></div></div><div id="42391700" class="c"><input type="checkbox" id="c-42391700" checked=""/><div class="controls bullet"><span class="by">_zoltan_</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42390724">parent</a><span>|</span><a href="#42391321">prev</a><span>|</span><a href="#42391723">next</a><span>|</span><label class="collapse" for="c-42391700">[-]</label><label class="expand" for="c-42391700">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s not true, H100 NVL is PCIe gen5 x16.</div><br/></div></div></div></div><div id="42391723" class="c"><input type="checkbox" id="c-42391723" checked=""/><div class="controls bullet"><span class="by">_zoltan_</span><span>|</span><a href="#42390369">parent</a><span>|</span><a href="#42390724">prev</a><span>|</span><a href="#42389830">next</a><span>|</span><label class="collapse" for="c-42391723">[-]</label><label class="expand" for="c-42391723">[4 more]</label></div><br/><div class="children"><div class="content">GH100 can do 900GB&#x2F;s HtoD.</div><br/><div id="42391891" class="c"><input type="checkbox" id="c-42391891" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42391723">parent</a><span>|</span><a href="#42395050">next</a><span>|</span><label class="collapse" for="c-42391891">[-]</label><label class="expand" for="c-42391891">[1 more]</label></div><br/><div class="children"><div class="content">And both 3090 and 4090 can do 32 GB&#x2F;s host-device. Not far from CPU-RAM. You only load the matrix once. The bandwidth for the matmul is orders of magnitude larger and happens all in device and mostly in cache.</div><br/></div></div><div id="42395050" class="c"><input type="checkbox" id="c-42395050" checked=""/><div class="controls bullet"><span class="by">jing</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42391723">parent</a><span>|</span><a href="#42391891">prev</a><span>|</span><a href="#42389830">next</a><span>|</span><label class="collapse" for="c-42395050">[-]</label><label class="expand" for="c-42395050">[2 more]</label></div><br/><div class="children"><div class="content">No it can’t. That’s d to d</div><br/><div id="42397176" class="c"><input type="checkbox" id="c-42397176" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#42390369">root</a><span>|</span><a href="#42395050">parent</a><span>|</span><a href="#42389830">next</a><span>|</span><label class="collapse" for="c-42397176">[-]</label><label class="expand" for="c-42397176">[1 more]</label></div><br/><div class="children"><div class="content">Well, device to device is technically doubled because you have a read and a write. But yes</div><br/></div></div></div></div></div></div></div></div><div id="42389830" class="c"><input type="checkbox" id="c-42389830" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#42390369">prev</a><span>|</span><a href="#42390005">next</a><span>|</span><label class="collapse" for="c-42389830">[-]</label><label class="expand" for="c-42389830">[4 more]</label></div><br/><div class="children"><div class="content">There is the compute vs communicate ratio.<p>For problems like Matrix Multiplication, it costs N to communicate the problem but N^2 operations to calculate.<p>For problems like dot product, it costs N to communicate but only N operations to calculate.<p>Compute must be substantially  larger than communication costs if you hope to see any benefits. Asymptotic differences obviously help, but linear too might help.<p>You&#x27;d never transfer N data to perform a log(n) binary search for example. At that point communication dominates.</div><br/><div id="42389910" class="c"><input type="checkbox" id="c-42389910" checked=""/><div class="controls bullet"><span class="by">AnotherGoodName</span><span>|</span><a href="#42389830">parent</a><span>|</span><a href="#42391156">next</a><span>|</span><label class="collapse" for="c-42389910">[-]</label><label class="expand" for="c-42389910">[2 more]</label></div><br/><div class="children"><div class="content">For those skimming and to add to the above the article is using the gpu to work with system memory since that’s where they have the initial data and where they want the result in this case and comparing it to a cpu doing the same. The entire bottleneck is GPU to system memory.<p>If you’re willing to work entirely with the gpu memory the gpu will of course be faster even in this scenario.</div><br/><div id="42397179" class="c"><input type="checkbox" id="c-42397179" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#42389830">root</a><span>|</span><a href="#42389910">parent</a><span>|</span><a href="#42391156">next</a><span>|</span><label class="collapse" for="c-42397179">[-]</label><label class="expand" for="c-42397179">[1 more]</label></div><br/><div class="children"><div class="content">Assuming your task is larger than kernel launch overhead, of course.</div><br/></div></div></div></div><div id="42391156" class="c"><input type="checkbox" id="c-42391156" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#42389830">parent</a><span>|</span><a href="#42389910">prev</a><span>|</span><a href="#42390005">next</a><span>|</span><label class="collapse" for="c-42391156">[-]</label><label class="expand" for="c-42391156">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but once you&#x27;ve justified moving data onto the GPU you don&#x27;t want to incur the cost of moving the operation output back to the CPU unless you have to. So, for example, you might justify moving data to the GPU for a neural net convolution, but then also execute the following activation function (&amp; subsequent operators) there because that&#x27;s now where the data is.</div><br/></div></div></div></div><div id="42390005" class="c"><input type="checkbox" id="c-42390005" checked=""/><div class="controls bullet"><span class="by">lmeyerov</span><span>|</span><a href="#42389830">prev</a><span>|</span><a href="#42389408">next</a><span>|</span><label class="collapse" for="c-42390005">[-]</label><label class="expand" for="c-42390005">[5 more]</label></div><br/><div class="children"><div class="content">Comparing multicore wide AVX to CUDA is a bit of an unnecessary nuance for most folks. These make sense, but miss the forest from the trees:<p>- Either way, you&#x27;re writing &#x27;cuda style&#x27; fine-grained data parallel code that looks and works very different from regular multithreaded code. You are now in a different software universe.<p>- You now also have to think about throughput, latency hiding, etc. Nvidia has been commoditizing throughput-oriented hardware a lot better than others, and while AMD is catching up on some workloads, Nvidia is already advancing. This is where we think about bandwidth between network&#x2F;disk=&gt;compute unit. My best analogy here, when looking at things like GPU Direct Storage&#x2F;Network, is CPU systems feel like a long twisty straw, while GPU paths are fat pipes. Big compute typically needs both compute + IO, and hardware specs tell you the bandwidth ceiling.<p>To a large extent, ideas are cross-polinating -- CPUs looking more like GPUs, and GPUs getting the flexibility of CPUs -- but either way, you&#x27;re in a different universe of how code &amp; hardware works than 1990s &amp; early 2000s intel.</div><br/><div id="42391450" class="c"><input type="checkbox" id="c-42391450" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#42390005">parent</a><span>|</span><a href="#42389408">next</a><span>|</span><label class="collapse" for="c-42391450">[-]</label><label class="expand" for="c-42391450">[4 more]</label></div><br/><div class="children"><div class="content">Realistically you should use Numpy or Cupy (or whatever the appropriate&#x2F;fashionable library is) anyway, because tuning this stuff is a big pain.<p>So, GPUs have the slight disadvantages that you have to think an about data movement and the drivers are a little less convenient to install, but it isn’t really a big deal.</div><br/><div id="42394980" class="c"><input type="checkbox" id="c-42394980" checked=""/><div class="controls bullet"><span class="by">lmeyerov</span><span>|</span><a href="#42390005">root</a><span>|</span><a href="#42391450">parent</a><span>|</span><a href="#42395259">next</a><span>|</span><label class="collapse" for="c-42394980">[-]</label><label class="expand" for="c-42394980">[1 more]</label></div><br/><div class="children"><div class="content">Agreed! The bigger shift is switching to data parallel coding styles.</div><br/></div></div><div id="42395259" class="c"><input type="checkbox" id="c-42395259" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#42390005">root</a><span>|</span><a href="#42391450">parent</a><span>|</span><a href="#42394980">prev</a><span>|</span><a href="#42389408">next</a><span>|</span><label class="collapse" for="c-42395259">[-]</label><label class="expand" for="c-42395259">[2 more]</label></div><br/><div class="children"><div class="content">I am a big fan of jax for numerical computations these days.</div><br/><div id="42396310" class="c"><input type="checkbox" id="c-42396310" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#42390005">root</a><span>|</span><a href="#42395259">parent</a><span>|</span><a href="#42389408">next</a><span>|</span><label class="collapse" for="c-42396310">[-]</label><label class="expand" for="c-42396310">[1 more]</label></div><br/><div class="children"><div class="content">I’ve been seeing lots of posts about it lately. Haven’t had a chance to try it out, though.</div><br/></div></div></div></div></div></div></div></div><div id="42389408" class="c"><input type="checkbox" id="c-42389408" checked=""/><div class="controls bullet"><span class="by">glitchc</span><span>|</span><a href="#42390005">prev</a><span>|</span><a href="#42389304">next</a><span>|</span><label class="collapse" for="c-42389408">[-]</label><label class="expand" for="c-42389408">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the relative difference of transfer overhead vs degree of compute. For one single operation, sure, the transfer overhead dominates. Add multiple compute steps (operations) however, and experiments will show that the GPU is faster as the transfer cost is fixed.</div><br/></div></div><div id="42389304" class="c"><input type="checkbox" id="c-42389304" checked=""/><div class="controls bullet"><span class="by">tylermw</span><span>|</span><a href="#42389408">prev</a><span>|</span><a href="#42395967">next</a><span>|</span><label class="collapse" for="c-42389304">[-]</label><label class="expand" for="c-42389304">[2 more]</label></div><br/><div class="children"><div class="content">For simple operations like the dot product (that also map extremely well to SIMD operations), yes, the CPU is often better, as there not much actual &quot;computation&quot; being done. More complex computations where the data does not need to transfer between the host and device amortize that transfer cost across multiple operations, and the balance can quickly weigh in favor of the GPU.</div><br/></div></div><div id="42395967" class="c"><input type="checkbox" id="c-42395967" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#42389304">prev</a><span>|</span><a href="#42390135">next</a><span>|</span><label class="collapse" for="c-42395967">[-]</label><label class="expand" for="c-42395967">[1 more]</label></div><br/><div class="children"><div class="content">Memory locality depends on your perspective.<p>The CPU would always be slower if the data originated in GPU memory.</div><br/></div></div><div id="42390135" class="c"><input type="checkbox" id="c-42390135" checked=""/><div class="controls bullet"><span class="by">jsight</span><span>|</span><a href="#42395967">prev</a><span>|</span><a href="#42395442">next</a><span>|</span><label class="collapse" for="c-42390135">[-]</label><label class="expand" for="c-42390135">[2 more]</label></div><br/><div class="children"><div class="content">TBH, I&#x27;m finding that people underestimate the usefulness of CPU in both inference and fine tuning. PEFT with access to 64GB+ RAM and lots of cores can sometimes be cost effective.</div><br/><div id="42393287" class="c"><input type="checkbox" id="c-42393287" checked=""/><div class="controls bullet"><span class="by">ramoz</span><span>|</span><a href="#42390135">parent</a><span>|</span><a href="#42395442">next</a><span>|</span><label class="collapse" for="c-42393287">[-]</label><label class="expand" for="c-42393287">[1 more]</label></div><br/><div class="children"><div class="content">I think engineers learn this quickly in high-scale&#x2F;performance production environments. Even without hardware backgrounds. SLAs&#x2F;costs create constraints you need to optimize against after promising the business line these magical models can enable that cool new feature for a million users.<p>Traditional AI&#x2F;ML models (including smaller transformers) can definitely be optimized for mass scale&#x2F;performance on cpu-optimized infrastructure.</div><br/></div></div></div></div><div id="42395442" class="c"><input type="checkbox" id="c-42395442" checked=""/><div class="controls bullet"><span class="by">shihab</span><span>|</span><a href="#42390135">prev</a><span>|</span><a href="#42389329">next</a><span>|</span><label class="collapse" for="c-42395442">[-]</label><label class="expand" for="c-42395442">[3 more]</label></div><br/><div class="children"><div class="content">An otherwise valid point made using a terrible example.</div><br/><div id="42395815" class="c"><input type="checkbox" id="c-42395815" checked=""/><div class="controls bullet"><span class="by">juunpp</span><span>|</span><a href="#42395442">parent</a><span>|</span><a href="#42389329">next</a><span>|</span><label class="collapse" for="c-42395815">[-]</label><label class="expand" for="c-42395815">[2 more]</label></div><br/><div class="children"><div class="content">Terrible post, really. Did they need 5 pages to say that a silly dot product micro-bench that is PCIE-bound loses to a CPU implementation? Why are they even comparing computation vs computation + memory transfer?</div><br/><div id="42396632" class="c"><input type="checkbox" id="c-42396632" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42395442">root</a><span>|</span><a href="#42395815">parent</a><span>|</span><a href="#42389329">next</a><span>|</span><label class="collapse" for="c-42396632">[-]</label><label class="expand" for="c-42396632">[1 more]</label></div><br/><div class="children"><div class="content">Because going to the GPU adds the overhead of memory and this is a simple way to demonstrate that. Did you know that already? Congrats, you&#x27;re in the top 10% of software engineers</div><br/></div></div></div></div></div></div><div id="42389329" class="c"><input type="checkbox" id="c-42389329" checked=""/><div class="controls bullet"><span class="by">ramoz</span><span>|</span><a href="#42395442">prev</a><span>|</span><a href="#42390385">next</a><span>|</span><label class="collapse" for="c-42389329">[-]</label><label class="expand" for="c-42389329">[2 more]</label></div><br/><div class="children"><div class="content">Simpler research could&#x27;ve shown that there is a physical data transfer cost.</div><br/><div id="42394775" class="c"><input type="checkbox" id="c-42394775" checked=""/><div class="controls bullet"><span class="by">hershey890</span><span>|</span><a href="#42389329">parent</a><span>|</span><a href="#42390385">next</a><span>|</span><label class="collapse" for="c-42394775">[-]</label><label class="expand" for="c-42394775">[1 more]</label></div><br/><div class="children"><div class="content">Yeah classic use cases of GPUs like deep learning have you transfer the weights for the entire model to your GPU(s) at the of inference and after you that you only transfer your input over.<p>The use case of transferring ALL data over every time is obviously misusing the GPU.<p>If anyone’s ever tried running a model that’s too large for your GPU you will have experienced how slow this is when you have to pull in the model in parts for a single inference run.</div><br/></div></div></div></div><div id="42390385" class="c"><input type="checkbox" id="c-42390385" checked=""/><div class="controls bullet"><span class="by">hangonhn</span><span>|</span><a href="#42389329">prev</a><span>|</span><a href="#42397273">next</a><span>|</span><label class="collapse" for="c-42390385">[-]</label><label class="expand" for="c-42390385">[6 more]</label></div><br/><div class="children"><div class="content">Question from someone who doesn&#x27;t know enough about GPUs:
Recently a friend mentioned his workstation has 384 cores using 4 processors. This is starting to approach some of the core numbers of earlier GPUs.<p>Is there a possibility that in the not too distant future that GPUs and CPUs will just converge? Or are the tasks done by GPUs too specialized?</div><br/><div id="42396650" class="c"><input type="checkbox" id="c-42396650" checked=""/><div class="controls bullet"><span class="by">nox101</span><span>|</span><a href="#42390385">parent</a><span>|</span><a href="#42396205">next</a><span>|</span><label class="collapse" for="c-42396650">[-]</label><label class="expand" for="c-42396650">[1 more]</label></div><br/><div class="children"><div class="content">GPU single threads are up to 20x slower the CPU threads. GPUs get their speed from massive parallelization, SIMD style &quot;do N things 1 one instruction&quot;, and some specialized hardware (like texture samplers)<p>If you take a serial algorithm and put it on the GPU, it&#x27;s easy to verify that a single GPU thread is much slower than a single thread on the CPU. For example, just do a bubble sort on the GPU with a single thread. I&#x27;m not even including the time to transfer data or read the result. You&#x27;ll easily find the CPU is way faster.<p>The way you get GPU speed is by finding&#x2F;designing algorithms that are massively parallel.  There are lots of them. There are sorting solutions for example.<p>As an example, 100 cores * 32 execution units per core = 3200 &#x2F; 20 = 160x faster than the CPU if you can figure out a parallel solution. But, not every problem can be solved with parallel solutions and if it can&#x27;t then there&#x27;s where the CPU wins.<p>It seems unlikely GPU threads will be as fast as CPU threads. They get their massive parallelism by being simpler.<p>That said, who knows what the future holds.</div><br/></div></div><div id="42396205" class="c"><input type="checkbox" id="c-42396205" checked=""/><div class="controls bullet"><span class="by">owlbite</span><span>|</span><a href="#42390385">parent</a><span>|</span><a href="#42396650">prev</a><span>|</span><a href="#42391232">next</a><span>|</span><label class="collapse" for="c-42396205">[-]</label><label class="expand" for="c-42396205">[1 more]</label></div><br/><div class="children"><div class="content">CPUs and GPUs are fundamentally aiming at different vertices of the performance polygon.<p>CPUs aim to minimize latency (how many cycles have to pass before you can use a result), and do so by way of high clock frequencies, caches and fancy micro architectural tricks. This is what you want in most general computation cases where you don&#x27;t have other work to do whilst you wait.<p>GPUs instead just context switch to a different thread whilst waiting on a result. They hide their latency by making parallelism as cheap as possible. You can have many more cores running at a lower clock frequency and be more efficient as a result. But this only works if you have enough parallelism to keep everything busy whilst waiting for things to finish on other threads. As it happens that&#x27;s pretty common in large matrix computations done in machine learning, so they&#x27;re pretty popular there.<p>Will they converge? I don&#x27;t think so - they&#x27;re fundamentally different design points. But it may well be that they get integrated at a much closer level than current designs, pushing the heterogeneous&#x2F;dark silicon&#x2F;accelerator direction to an extreme.</div><br/></div></div><div id="42391232" class="c"><input type="checkbox" id="c-42391232" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#42390385">parent</a><span>|</span><a href="#42396205">prev</a><span>|</span><a href="#42391151">next</a><span>|</span><label class="collapse" for="c-42391232">[-]</label><label class="expand" for="c-42391232">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re really similar already. You can program a GPU much like you would a CPU (existence proof at <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42387267">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42387267</a>). There&#x27;s a lot of obfuscation and hoarding of the ancient knowledge from the GPUs are special enthusiasts but the magic doesn&#x27;t survive looking at the things. It&#x27;s a masked vector isa.<p>My dev GPU is a 6800XT. Cheapish gaming card from a little while ago, 16GB ram on the card. 72 &quot;compute units&quot; which are independent blocks of hardware containing memory ports, floating point unit, register file etc. Roughly &quot;a core&quot; from x64 world. Each of those can have up to 64 tasks ready to go, roughly a &quot;hyperthread&quot;. It&#x27;s 300W or so.<p>There&#x27;s some noise in the details, e.g. the size of the register file from the perspective of a hyperthread affects how many can be resident on the compute unit ready to run, the memory hierarchy has extra layers in it. The vector unit is 256byte wide as opposed to 64byte wide on x64.<p>But if you wanted to run a web browser entirely on the GPU and were sufficiently bloody minded you&#x27;d get it done, with the CPU routing keyboard I&#x2F;O to it and nothing else. If you want a process to sit on the GPU talking to the network and crunching numbers, don&#x27;t need the x64 or arm host to do anything at all.</div><br/></div></div><div id="42391151" class="c"><input type="checkbox" id="c-42391151" checked=""/><div class="controls bullet"><span class="by">krapht</span><span>|</span><a href="#42390385">parent</a><span>|</span><a href="#42391232">prev</a><span>|</span><a href="#42391213">next</a><span>|</span><label class="collapse" for="c-42391151">[-]</label><label class="expand" for="c-42391151">[1 more]</label></div><br/><div class="children"><div class="content">Too specialized. You can&#x27;t use GPUs as general purpose computers. The basic unit of operation is the warp, which is 32 threads operating in lockstep (simplified). If you&#x27;re not using all 32 threads, then you may as well not be using a GPU.</div><br/></div></div><div id="42391213" class="c"><input type="checkbox" id="c-42391213" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#42390385">parent</a><span>|</span><a href="#42391151">prev</a><span>|</span><a href="#42397273">next</a><span>|</span><label class="collapse" for="c-42391213">[-]</label><label class="expand" for="c-42391213">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;d also need extreme hyperthreading. A GPU can cycle between several warps in the same execution unit (barrel-processor-style), padding out the time per instruction to hide memory latency, while still getting the same throughput. That&#x27;s counter to the fundamental design of CPUs.</div><br/></div></div></div></div><div id="42397273" class="c"><input type="checkbox" id="c-42397273" checked=""/><div class="controls bullet"><span class="by">ImHereToVote</span><span>|</span><a href="#42390385">prev</a><span>|</span><a href="#42392312">next</a><span>|</span><label class="collapse" for="c-42397273">[-]</label><label class="expand" for="c-42397273">[1 more]</label></div><br/><div class="children"><div class="content">The GPU is never faster. It&#x27;s parallel.</div><br/></div></div><div id="42392312" class="c"><input type="checkbox" id="c-42392312" checked=""/><div class="controls bullet"><span class="by">ltbarcly3</span><span>|</span><a href="#42397273">prev</a><span>|</span><a href="#42394351">next</a><span>|</span><label class="collapse" for="c-42392312">[-]</label><label class="expand" for="c-42392312">[1 more]</label></div><br/><div class="children"><div class="content">Taking a helicopter is not always faster than walking.<p>Is this surprising or obvious?</div><br/></div></div><div id="42394351" class="c"><input type="checkbox" id="c-42394351" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42392312">prev</a><span>|</span><a href="#42388010">next</a><span>|</span><label class="collapse" for="c-42394351">[-]</label><label class="expand" for="c-42394351">[1 more]</label></div><br/><div class="children"><div class="content">The same thing applies to using a GPU to do inference with your weights in system memory. That is why nobody does that.</div><br/></div></div><div id="42390112" class="c"><input type="checkbox" id="c-42390112" checked=""/><div class="controls bullet"><span class="by">rnrn</span><span>|</span><a href="#42388010">prev</a><span>|</span><a href="#42389386">next</a><span>|</span><label class="collapse" for="c-42390112">[-]</label><label class="expand" for="c-42390112">[4 more]</label></div><br/><div class="children"><div class="content">how can the multicore AVX implementation do a dot product (for arrays much larger than cache) at 340 GB&#x2F;s on a system with RAM bandwidth &lt; 50 GB&#x2F;s</div><br/><div id="42395937" class="c"><input type="checkbox" id="c-42395937" checked=""/><div class="controls bullet"><span class="by">rnrn</span><span>|</span><a href="#42390112">parent</a><span>|</span><a href="#42390288">next</a><span>|</span><label class="collapse" for="c-42395937">[-]</label><label class="expand" for="c-42395937">[1 more]</label></div><br/><div class="children"><div class="content">Answer: it can’t.<p>The author has updated the post with corrected AVX measurements, with the original ~340 GB&#x2F;s revised down to 31.7 GB&#x2F;s. (Thanks CowFreedom)</div><br/></div></div><div id="42390288" class="c"><input type="checkbox" id="c-42390288" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#42390112">parent</a><span>|</span><a href="#42395937">prev</a><span>|</span><a href="#42389386">next</a><span>|</span><label class="collapse" for="c-42390288">[-]</label><label class="expand" for="c-42390288">[2 more]</label></div><br/><div class="children"><div class="content">I think the post is a bit disingenuous.<p>But about bandwidth, matrix multiplications happen mostly in cache and that has a lot more bandwidth than RAM. Blocks of the matrix are loaded to cache (explicitly in CUDA) and used multiple times there.<p>I&#x27;d exploit the better multi-level cache hierarchy in CPUs and make the code NUMA aware. But still I wouldn&#x27;t bet against a recent GPU card.</div><br/><div id="42395921" class="c"><input type="checkbox" id="c-42395921" checked=""/><div class="controls bullet"><span class="by">rnrn</span><span>|</span><a href="#42390112">root</a><span>|</span><a href="#42390288">parent</a><span>|</span><a href="#42389386">next</a><span>|</span><label class="collapse" for="c-42395921">[-]</label><label class="expand" for="c-42395921">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But about bandwidth, matrix multiplications happen mostly in cache and that has a lot more bandwidth than RAM. Blocks of the matrix are loaded to cache (explicitly in CUDA) and used multiple times there.<p>The post is about dot product, not matrix multiply. Dot product has no data reuse</div><br/></div></div></div></div></div></div><div id="42389386" class="c"><input type="checkbox" id="c-42389386" checked=""/><div class="controls bullet"><span class="by">SigmundA</span><span>|</span><a href="#42390112">prev</a><span>|</span><a href="#42389412">next</a><span>|</span><label class="collapse" for="c-42389386">[-]</label><label class="expand" for="c-42389386">[3 more]</label></div><br/><div class="children"><div class="content">Would be interesting to see what a unified memory setup can do like say an Apple M-series since this is the argument for unified memory, zero copy memory access between CPU and GPU.</div><br/><div id="42390851" class="c"><input type="checkbox" id="c-42390851" checked=""/><div class="controls bullet"><span class="by">mirsadm</span><span>|</span><a href="#42389386">parent</a><span>|</span><a href="#42389918">next</a><span>|</span><label class="collapse" for="c-42390851">[-]</label><label class="expand" for="c-42390851">[1 more]</label></div><br/><div class="children"><div class="content">Unified memory is what makes using the GPU viable in my use case (mobile). The copy operation is almost always the slowest part. This is especially true for real time work.</div><br/></div></div><div id="42389918" class="c"><input type="checkbox" id="c-42389918" checked=""/><div class="controls bullet"><span class="by">CowFreedom</span><span>|</span><a href="#42389386">parent</a><span>|</span><a href="#42390851">prev</a><span>|</span><a href="#42389412">next</a><span>|</span><label class="collapse" for="c-42389918">[-]</label><label class="expand" for="c-42389918">[1 more]</label></div><br/><div class="children"><div class="content">Even the integrated Intel HD Graphics would be an interesting comparison.</div><br/></div></div></div></div><div id="42389412" class="c"><input type="checkbox" id="c-42389412" checked=""/><div class="controls bullet"><span class="by">moomin</span><span>|</span><a href="#42389386">prev</a><span>|</span><a href="#42391129">next</a><span>|</span><label class="collapse" for="c-42389412">[-]</label><label class="expand" for="c-42389412">[8 more]</label></div><br/><div class="children"><div class="content">Even if the GPU took literally no time at all to compute the results there would be workflows where doing it on the CPU was faster.</div><br/><div id="42394971" class="c"><input type="checkbox" id="c-42394971" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#42389412">parent</a><span>|</span><a href="#42389621">next</a><span>|</span><label class="collapse" for="c-42394971">[-]</label><label class="expand" for="c-42394971">[1 more]</label></div><br/><div class="children"><div class="content">L2 cache is 1000x closer than the PCIe bus. Pretty much anything that has to respond in ~realtime to outside events will run better on the CPU. You can use the GPU to visualize the state of the system with some small delay (e.g., video games), but it is not so great at modifying state in an efficient manner - especially when serialization of events &amp; causality are important.</div><br/></div></div><div id="42389621" class="c"><input type="checkbox" id="c-42389621" checked=""/><div class="controls bullet"><span class="by">Waterluvian</span><span>|</span><a href="#42389412">parent</a><span>|</span><a href="#42394971">prev</a><span>|</span><a href="#42391129">next</a><span>|</span><label class="collapse" for="c-42389621">[-]</label><label class="expand" for="c-42389621">[6 more]</label></div><br/><div class="children"><div class="content">The GPU is the gas station across town that’s five cents cheaper.</div><br/><div id="42394483" class="c"><input type="checkbox" id="c-42394483" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#42389412">root</a><span>|</span><a href="#42389621">parent</a><span>|</span><a href="#42391757">next</a><span>|</span><label class="collapse" for="c-42394483">[-]</label><label class="expand" for="c-42394483">[1 more]</label></div><br/><div class="children"><div class="content">No, it’s the bulk order from China that’s 100x cheaper but has a minimum order quantity of 100000 units and takes 6-8 weeks to get here.</div><br/></div></div><div id="42391757" class="c"><input type="checkbox" id="c-42391757" checked=""/><div class="controls bullet"><span class="by">_zoltan_</span><span>|</span><a href="#42389412">root</a><span>|</span><a href="#42389621">parent</a><span>|</span><a href="#42394483">prev</a><span>|</span><a href="#42389768">next</a><span>|</span><label class="collapse" for="c-42391757">[-]</label><label class="expand" for="c-42391757">[3 more]</label></div><br/><div class="children"><div class="content">this is simply not true. the post just uses outdated hardware, as pointed out above.<p>a GH200 will run miles around any CPU.</div><br/><div id="42393291" class="c"><input type="checkbox" id="c-42393291" checked=""/><div class="controls bullet"><span class="by">CowFreedom</span><span>|</span><a href="#42389412">root</a><span>|</span><a href="#42391757">parent</a><span>|</span><a href="#42394501">next</a><span>|</span><label class="collapse" for="c-42393291">[-]</label><label class="expand" for="c-42393291">[1 more]</label></div><br/><div class="children"><div class="content">The gist of the post is that optimizations and interpretations thereof must always be made with respect to the underlying hardware.</div><br/></div></div><div id="42394501" class="c"><input type="checkbox" id="c-42394501" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#42389412">root</a><span>|</span><a href="#42391757">parent</a><span>|</span><a href="#42393291">prev</a><span>|</span><a href="#42389768">next</a><span>|</span><label class="collapse" for="c-42394501">[-]</label><label class="expand" for="c-42394501">[1 more]</label></div><br/><div class="children"><div class="content">In this analogy, using better hardware affects the discount per gallon, but there are still situations where the closer gas station is the better choice.</div><br/></div></div></div></div><div id="42389768" class="c"><input type="checkbox" id="c-42389768" checked=""/><div class="controls bullet"><span class="by">adamc</span><span>|</span><a href="#42389412">root</a><span>|</span><a href="#42389621">parent</a><span>|</span><a href="#42391757">prev</a><span>|</span><a href="#42391129">next</a><span>|</span><label class="collapse" for="c-42389768">[-]</label><label class="expand" for="c-42389768">[1 more]</label></div><br/><div class="children"><div class="content">Good analogy.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>