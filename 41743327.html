<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1728118872792" as="style"/><link rel="stylesheet" href="styles.css?v=1728118872792"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/livekit/agents">Show HN: Open source framework OpenAI uses for Advanced Voice</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>russ</span> | <span>24 comments</span></div><br/><div><div id="41748560" class="c"><input type="checkbox" id="c-41748560" checked=""/><div class="controls bullet"><span class="by">spuz</span><span>|</span><a href="#41746066">next</a><span>|</span><label class="collapse" for="c-41748560">[-]</label><label class="expand" for="c-41748560">[2 more]</label></div><br/><div class="children"><div class="content">Is there anyone besides OpenAI working on a speech to speech model? I find it incredibly useful and it&#x27;s the sole reason that I pay for their service but I do find it very limited. I&#x27;d be interested to know if any other groups are doing research on voice models.</div><br/><div id="41748593" class="c"><input type="checkbox" id="c-41748593" checked=""/><div class="controls bullet"><span class="by">Ey7NFZ3P0nzAe</span><span>|</span><a href="#41748560">parent</a><span>|</span><a href="#41746066">next</a><span>|</span><label class="collapse" for="c-41748593">[-]</label><label class="expand" for="c-41748593">[1 more]</label></div><br/><div class="children"><div class="content">Yes. Kyutai released an opened model called moshi : <a href="https:&#x2F;&#x2F;github.com&#x2F;kyutai-labs&#x2F;moshi">https:&#x2F;&#x2F;github.com&#x2F;kyutai-labs&#x2F;moshi</a><p>There&#x27;s also llama-omni and a few others. None of them are even close to 4o from an LLM standpoint. But moshi is called a &quot;foundational&quot; model and U&#x27;m hopeful it will be enhanced. Also there&#x27;s not yet support for those on most backends like llamacpp &#x2F; ollama etc. So I&#x27;d say we&#x27;re in a trough but we&#x27;ll get there.</div><br/></div></div></div></div><div id="41746066" class="c"><input type="checkbox" id="c-41746066" checked=""/><div class="controls bullet"><span class="by">pj_mukh</span><span>|</span><a href="#41748560">prev</a><span>|</span><a href="#41746906">next</a><span>|</span><label class="collapse" for="c-41746066">[-]</label><label class="expand" for="c-41746066">[5 more]</label></div><br/><div class="children"><div class="content">Super cool! Didn&#x27;t realize OpenAI is just using LiveKit.<p>Does the pricing breakdown to be the same as having a OpenAI Advanced Voice socket open the whole time? It&#x27;s like $9&#x2F;hr!<p>It would be theoretically cheaper to use this without keeping the advanced voice socket open the whole time and just use the GPT4o streaming service [1] for whenever inference is needed (pay per token) and use livekits other components to do the rest (TTS, VAD etc.).<p>What&#x27;s the trade off here?<p>[1]: <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;api-reference&#x2F;streaming" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;api-reference&#x2F;streaming</a></div><br/><div id="41746382" class="c"><input type="checkbox" id="c-41746382" checked=""/><div class="controls bullet"><span class="by">davidz</span><span>|</span><a href="#41746066">parent</a><span>|</span><a href="#41747550">next</a><span>|</span><label class="collapse" for="c-41746382">[-]</label><label class="expand" for="c-41746382">[3 more]</label></div><br/><div class="children"><div class="content">Currently it does: all audio is sent to the model.<p>However, we are working on turn detection within the framework, so you won&#x27;t have to send silence to the model when the user isn&#x27;t talking. It&#x27;s a fairly straight forward path to cutting down the cost by ~50%.</div><br/><div id="41747890" class="c"><input type="checkbox" id="c-41747890" checked=""/><div class="controls bullet"><span class="by">rukuu001</span><span>|</span><a href="#41746066">root</a><span>|</span><a href="#41746382">parent</a><span>|</span><a href="#41747550">next</a><span>|</span><label class="collapse" for="c-41747890">[-]</label><label class="expand" for="c-41747890">[2 more]</label></div><br/><div class="children"><div class="content">Working on this for an internal tool - detecting no speech has been a PITA so far. Interested to see how you go with this.</div><br/><div id="41748273" class="c"><input type="checkbox" id="c-41748273" checked=""/><div class="controls bullet"><span class="by">balloob</span><span>|</span><a href="#41746066">root</a><span>|</span><a href="#41747890">parent</a><span>|</span><a href="#41747550">next</a><span>|</span><label class="collapse" for="c-41748273">[-]</label><label class="expand" for="c-41748273">[1 more]</label></div><br/><div class="children"><div class="content">Use the voice activity detector we wrote for Home Assistant. It works very well: <a href="https:&#x2F;&#x2F;github.com&#x2F;rhasspy&#x2F;pymicro-vad">https:&#x2F;&#x2F;github.com&#x2F;rhasspy&#x2F;pymicro-vad</a></div><br/></div></div></div></div></div></div><div id="41747550" class="c"><input type="checkbox" id="c-41747550" checked=""/><div class="controls bullet"><span class="by">npace12</span><span>|</span><a href="#41746066">parent</a><span>|</span><a href="#41746382">prev</a><span>|</span><a href="#41746906">next</a><span>|</span><label class="collapse" for="c-41747550">[-]</label><label class="expand" for="c-41747550">[1 more]</label></div><br/><div class="children"><div class="content">You dont get charged per hour with the openai realtime api, only for tokens from detected speech and response</div><br/></div></div></div></div><div id="41746906" class="c"><input type="checkbox" id="c-41746906" checked=""/><div class="controls bullet"><span class="by">solarkraft</span><span>|</span><a href="#41746066">prev</a><span>|</span><a href="#41745747">next</a><span>|</span><label class="collapse" for="c-41746906">[-]</label><label class="expand" for="c-41746906">[2 more]</label></div><br/><div class="children"><div class="content">That’s some crazy marketing for a „our library happened to support this relatively simple use case“ situation. Impressive!<p>By the way: The cerebras voice demo <i>also</i> uses LiveKit for this: <a href="https:&#x2F;&#x2F;cerebras.vercel.app&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cerebras.vercel.app&#x2F;</a></div><br/><div id="41747366" class="c"><input type="checkbox" id="c-41747366" checked=""/><div class="controls bullet"><span class="by">russ</span><span>|</span><a href="#41746906">parent</a><span>|</span><a href="#41745747">next</a><span>|</span><label class="collapse" for="c-41747366">[-]</label><label class="expand" for="c-41747366">[1 more]</label></div><br/><div class="children"><div class="content">There’s a ton of complexity under the “relatively simple use case” when you get to a global, 200M+ user scale.</div><br/></div></div></div></div><div id="41745747" class="c"><input type="checkbox" id="c-41745747" checked=""/><div class="controls bullet"><span class="by">FanaHOVA</span><span>|</span><a href="#41746906">prev</a><span>|</span><a href="#41748157">next</a><span>|</span><label class="collapse" for="c-41745747">[-]</label><label class="expand" for="c-41745747">[3 more]</label></div><br/><div class="children"><div class="content">Olivier, Michelle, and Romain gave you guys a shoutout like 3 times in our DevDay recap podcast if you need more testimonial quotes :) <a href="https:&#x2F;&#x2F;www.latent.space&#x2F;p&#x2F;devday-2024" rel="nofollow">https:&#x2F;&#x2F;www.latent.space&#x2F;p&#x2F;devday-2024</a></div><br/><div id="41746424" class="c"><input type="checkbox" id="c-41746424" checked=""/><div class="controls bullet"><span class="by">russ</span><span>|</span><a href="#41745747">parent</a><span>|</span><a href="#41746658">next</a><span>|</span><label class="collapse" for="c-41746424">[-]</label><label class="expand" for="c-41746424">[1 more]</label></div><br/><div class="children"><div class="content">I had no idea! &lt;3 Thank you for sharing this, made my weekend.</div><br/></div></div><div id="41746658" class="c"><input type="checkbox" id="c-41746658" checked=""/><div class="controls bullet"><span class="by">shayps</span><span>|</span><a href="#41745747">parent</a><span>|</span><a href="#41746424">prev</a><span>|</span><a href="#41748157">next</a><span>|</span><label class="collapse" for="c-41746658">[-]</label><label class="expand" for="c-41746658">[1 more]</label></div><br/><div class="children"><div class="content">You guys are honestly the best</div><br/></div></div></div></div><div id="41748157" class="c"><input type="checkbox" id="c-41748157" checked=""/><div class="controls bullet"><span class="by">lolpanda</span><span>|</span><a href="#41745747">prev</a><span>|</span><a href="#41745615">next</a><span>|</span><label class="collapse" for="c-41748157">[-]</label><label class="expand" for="c-41748157">[1 more]</label></div><br/><div class="children"><div class="content">so the WebRTC helps with the unreliable network between the mobile clients and the server side. if the application is backend only, would it make sense to use WebRTC or should I go directly to realtime api?</div><br/></div></div><div id="41745615" class="c"><input type="checkbox" id="c-41745615" checked=""/><div class="controls bullet"><span class="by">mycall</span><span>|</span><a href="#41748157">prev</a><span>|</span><a href="#41745617">next</a><span>|</span><label class="collapse" for="c-41745615">[-]</label><label class="expand" for="c-41745615">[2 more]</label></div><br/><div class="children"><div class="content">I wonder when Azure OpenAI will get this.</div><br/><div id="41746393" class="c"><input type="checkbox" id="c-41746393" checked=""/><div class="controls bullet"><span class="by">davidz</span><span>|</span><a href="#41745615">parent</a><span>|</span><a href="#41745617">next</a><span>|</span><label class="collapse" for="c-41746393">[-]</label><label class="expand" for="c-41746393">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m working on a PR now :)</div><br/></div></div></div></div><div id="41745617" class="c"><input type="checkbox" id="c-41745617" checked=""/><div class="controls bullet"><span class="by">gastonmorixe</span><span>|</span><a href="#41745615">prev</a><span>|</span><a href="#41746198">next</a><span>|</span><label class="collapse" for="c-41745617">[-]</label><label class="expand" for="c-41745617">[2 more]</label></div><br/><div class="children"><div class="content">Nice they have many partners on this. I see Azure as well.<p>There is a common consensus that the new Realtime API is not actually using the same Advanced Voice model &#x2F; engine - or however it works - since at least the TTS part doesn’t seem to be as capable as the one shipped with the official OpenAI app.<p>Any idea on this?<p>Source: <a href="https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-realtime-api-beta&#x2F;issues&#x2F;2">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-realtime-api-beta&#x2F;issues&#x2F;2</a></div><br/><div id="41746421" class="c"><input type="checkbox" id="c-41746421" checked=""/><div class="controls bullet"><span class="by">russ</span><span>|</span><a href="#41745617">parent</a><span>|</span><a href="#41746198">next</a><span>|</span><label class="collapse" for="c-41746421">[-]</label><label class="expand" for="c-41746421">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s using the same model&#x2F;engine. I don&#x27;t have knowledge of the internals, but a different subsystem&#x2F;set of dedicated resources though for API traffic versus first-party apps.<p>One thing to note is there is no separate TTS-phase here, it&#x27;s happening internally within GPT-4o, in the Realtime API and Advanced Voice.</div><br/></div></div></div></div><div id="41746198" class="c"><input type="checkbox" id="c-41746198" checked=""/><div class="controls bullet"><span class="by">willsmith72</span><span>|</span><a href="#41745617">prev</a><span>|</span><label class="collapse" for="c-41746198">[-]</label><label class="expand" for="c-41746198">[6 more]</label></div><br/><div class="children"><div class="content">That was cool, but got up to $1 usage real quick</div><br/><div id="41746455" class="c"><input type="checkbox" id="c-41746455" checked=""/><div class="controls bullet"><span class="by">russ</span><span>|</span><a href="#41746198">parent</a><span>|</span><label class="collapse" for="c-41746455">[-]</label><label class="expand" for="c-41746455">[5 more]</label></div><br/><div class="children"><div class="content">We had our playground (<a href="https:&#x2F;&#x2F;playground.livekit.io" rel="nofollow">https:&#x2F;&#x2F;playground.livekit.io</a>) up for a few days using our key. Def racked up a $$$$ bill!</div><br/><div id="41746925" class="c"><input type="checkbox" id="c-41746925" checked=""/><div class="controls bullet"><span class="by">wordpad25</span><span>|</span><a href="#41746198">root</a><span>|</span><a href="#41746455">parent</a><span>|</span><label class="collapse" for="c-41746925">[-]</label><label class="expand" for="c-41746925">[4 more]</label></div><br/><div class="children"><div class="content">How much is it per minute of talking?</div><br/><div id="41746982" class="c"><input type="checkbox" id="c-41746982" checked=""/><div class="controls bullet"><span class="by">russ</span><span>|</span><a href="#41746198">root</a><span>|</span><a href="#41746925">parent</a><span>|</span><a href="#41746958">next</a><span>|</span><label class="collapse" for="c-41746982">[-]</label><label class="expand" for="c-41746982">[1 more]</label></div><br/><div class="children"><div class="content">50% human speaking at $0.06&#x2F;minute of tokens<p>50% AI speaking at $0.24&#x2F;minute of tokens<p>we (LiveKit Cloud) charge ~$0.0005&#x2F;minute for each participant (in this case there would be 2)<p>So blended is $0.151&#x2F;minute</div><br/></div></div><div id="41746958" class="c"><input type="checkbox" id="c-41746958" checked=""/><div class="controls bullet"><span class="by">shayps</span><span>|</span><a href="#41746198">root</a><span>|</span><a href="#41746925">parent</a><span>|</span><a href="#41746982">prev</a><span>|</span><label class="collapse" for="c-41746958">[-]</label><label class="expand" for="c-41746958">[2 more]</label></div><br/><div class="children"><div class="content">It shakes out to around $0.15 per minute for an average conversation. If history is a guide though, this will get a lot cheaper pretty quickly.</div><br/><div id="41747367" class="c"><input type="checkbox" id="c-41747367" checked=""/><div class="controls bullet"><span class="by">cdolan</span><span>|</span><a href="#41746198">root</a><span>|</span><a href="#41746958">parent</a><span>|</span><label class="collapse" for="c-41747367">[-]</label><label class="expand" for="c-41747367">[1 more]</label></div><br/><div class="children"><div class="content">This is cheaper than old cellular calls, inflation adjusted</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>