<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1713430860368" as="style"/><link rel="stylesheet" href="styles.css?v=1713430860368"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://asciinema.org/a/piP22yYwcaohu5cA2gyuv1W61">Show HN: Speeding up LLM inference 2x times (possibly)</a> <span class="domain">(<a href="https://asciinema.org">asciinema.org</a>)</span></div><div class="subtext"><span>kolinko</span> | <span>104 comments</span></div><br/><div><div id="40073067" class="c"><input type="checkbox" id="c-40073067" checked=""/><div class="controls bullet"><span class="by">renonce</span><span>|</span><a href="#40070293">next</a><span>|</span><label class="collapse" for="c-40073067">[-]</label><label class="expand" for="c-40073067">[4 more]</label></div><br/><div class="children"><div class="content">Essentially the algorithm is about pruning parameters <i>on the fly</i> and making the weight matrix sparse by setting least significant weights to zero, where &quot;least significant&quot; is defined in terms of the rank of the absolute value of the pruned weight within its group?<p>A search for model pruning turns out many results, including <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.11627" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.11627</a> which discusses &quot;magnitude pruning&quot; as a baseline, and refers to <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2301.00774.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2301.00774.pdf</a>, which asserts in the introduction:<p>&gt; First, as shown in Figure 1, SparseGPT can induce uniform layerwise sparsity of up to 60% in e.g. the 175-billion-parameter variant of the OPT family (Zhang et al., 2022), with minor accuracy loss. By contrast, the only known one-shot
baseline which easily extends to this scale, Magnitude Pruning (Hagiwara, 1994; Han et al., 2015), preserves accuracy only until 10% sparsity, and completely collapses beyond 30% sparsity.<p>I don&#x27;t like how these papers boast their own methods by poorly implementing a baseline and describing their own methods using lots of mathematical jargon - the OP&#x27;s blog post is a breeze in making the methods accessible to people with very little background.</div><br/><div id="40073552" class="c"><input type="checkbox" id="c-40073552" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40073067">parent</a><span>|</span><a href="#40070293">next</a><span>|</span><label class="collapse" for="c-40073552">[-]</label><label class="expand" for="c-40073552">[3 more]</label></div><br/><div class="children"><div class="content">Thanks! The last full month was all about making sure all the research is as replicable and as trustworthy as I can do it. The original implementation was extremely inefficient, and even once I had the Metal&#x2F;GPU mmul op working fast, I spent much time to bring the rest of the implementation as close to the Llama.cpp as possible to allow for easier benchmarking.<p>As for the approaches - it seems the papers you mention do this statically, and din&#x27;t produce an algorithm for actually speeding up computations with their 20-50% results - which was a large part of the difficulty. I&#x27;ll probably take some time off one day and do a thorough review of the literature finally.<p>Ultimately I want to add a citations page with these and some other papers people have posted in the comments soon. I expect sooner or later someone will find this algorithm written up by someone :)<p>While development asked gpt-4 and googled trying to find information about this method - everything seemed either static, or focused on removing full dimensions&#x2F;layers ad hoc with eventual retraining. Didn&#x27;t find anything matching this idea exactly.</div><br/><div id="40073791" class="c"><input type="checkbox" id="c-40073791" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#40073067">root</a><span>|</span><a href="#40073552">parent</a><span>|</span><a href="#40070293">next</a><span>|</span><label class="collapse" for="c-40073791">[-]</label><label class="expand" for="c-40073791">[2 more]</label></div><br/><div class="children"><div class="content">Thank you. If you want to contribute novel research, a thorough literature search of prior work is essential. And hopefully a comparison to previous approaches.<p>If you didn&#x27;t find these works in your literature search, I encourage you to improve this skill, because it&#x27;s important and valuable.</div><br/><div id="40074074" class="c"><input type="checkbox" id="c-40074074" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40073067">root</a><span>|</span><a href="#40073791">parent</a><span>|</span><a href="#40070293">next</a><span>|</span><label class="collapse" for="c-40074074">[-]</label><label class="expand" for="c-40074074">[1 more]</label></div><br/><div class="children"><div class="content">True, it will be easier next time because new llms allow to better searching through papers.<p>Having said that - the tradeoff with getting into a new field is that there is 50 years of history to dig through, and by the time you know exactly what you’re looking for, you don’t need to find it really. Although a better math framework or another approach would help a bit for sure.<p>I think in the future it will be way easier with better llm searches. Gpt right now fell short in finding some stuff, but it was already very useful in giving me an overview of the field.</div><br/></div></div></div></div></div></div></div></div><div id="40070293" class="c"><input type="checkbox" id="c-40070293" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#40073067">prev</a><span>|</span><a href="#40068300">next</a><span>|</span><label class="collapse" for="c-40070293">[-]</label><label class="expand" for="c-40070293">[2 more]</label></div><br/><div class="children"><div class="content">I love this line in the gpu implementation section.<p>&quot;Readers fresh to GPU programming may ask now - how does it work?<p>Readers experienced with GPU programming may ask - how the hell does it work?&quot;</div><br/><div id="40070709" class="c"><input type="checkbox" id="c-40070709" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40070293">parent</a><span>|</span><a href="#40068300">next</a><span>|</span><label class="collapse" for="c-40070709">[-]</label><label class="expand" for="c-40070709">[1 more]</label></div><br/><div class="children"><div class="content">Haha thanks! :) As far as I understand I had to implement the memory reads and some other things the opposite way to what is considered a proper approach.<p>Would love to have that code reviewed by someone who actually knows stuff about Metal - this is my first gpu programming attempt</div><br/></div></div></div></div><div id="40068300" class="c"><input type="checkbox" id="c-40068300" checked=""/><div class="controls bullet"><span class="by">bigcat12345678</span><span>|</span><a href="#40070293">prev</a><span>|</span><a href="#40070056">next</a><span>|</span><label class="collapse" for="c-40068300">[-]</label><label class="expand" for="c-40068300">[4 more]</label></div><br/><div class="children"><div class="content">“““
So instead, let&#x27;s flip the matrix, sort the elements row-wise, and revisit the multiplications from that direction.<p>This is called a Compressed Sparse Row (CSR) format by the smart people. To do the multiplication now, we take, say, the 1 from the vector, multiply it by 256, and add it into the output vector at the 3rd row. And so on.<p>Now, let&#x27;s see what happens if we truncate the last column - the one with the lowest values.
”””<p>How does csr works with reduced numbers multiplication?</div><br/><div id="40068319" class="c"><input type="checkbox" id="c-40068319" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40068300">parent</a><span>|</span><a href="#40070056">next</a><span>|</span><label class="collapse" for="c-40068319">[-]</label><label class="expand" for="c-40068319">[3 more]</label></div><br/><div class="children"><div class="content">Can you rephrase the question? Not sure I get it.</div><br/><div id="40070699" class="c"><input type="checkbox" id="c-40070699" checked=""/><div class="controls bullet"><span class="by">bigcat12345678</span><span>|</span><a href="#40068300">root</a><span>|</span><a href="#40068319">parent</a><span>|</span><a href="#40070056">next</a><span>|</span><label class="collapse" for="c-40070699">[-]</label><label class="expand" for="c-40070699">[2 more]</label></div><br/><div class="children"><div class="content">I could not read from the text how multiplication with CSR format works in the context of optimization.<p>The key missing piece for me is that how to utilize CSR format to find the numbers to do multiplication, in other words, how does CSR format helps with picking the numbers to multiply with the vector.</div><br/><div id="40070813" class="c"><input type="checkbox" id="c-40070813" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40068300">root</a><span>|</span><a href="#40070699">parent</a><span>|</span><a href="#40070056">next</a><span>|</span><label class="collapse" for="c-40070813">[-]</label><label class="expand" for="c-40070813">[1 more]</label></div><br/><div class="children"><div class="content">Ah, I get the question now.<p>If I understand correctly CSR, it stores indexes and values as a list. I store them also as a list - that&#x27;s why the comparison is there. The difference is that with CSR you store say 15% of the values from a given row. I store all of them, but use only the first X% of them. The X% varies and depends on the input vector.<p>They are stored sorted, from the one of a highest absolute value to the lowest absolute value.<p>It&#x27;s after midnight so my explanations may not be too good now, but I hope the pseudocode on the page and the examples explain it slightly better.<p>I&#x27;ll be fixing grammar &#x2F; typos, and asking ChatGPT to rewrite the page text for me tomorrow to make it more readable :)</div><br/></div></div></div></div></div></div></div></div><div id="40070056" class="c"><input type="checkbox" id="c-40070056" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#40068300">prev</a><span>|</span><a href="#40070944">next</a><span>|</span><label class="collapse" for="c-40070056">[-]</label><label class="expand" for="c-40070056">[2 more]</label></div><br/><div class="children"><div class="content">Having used CSR it&#x27;s not surprising, and some newer formats might have more mechanical sympathy like block ELL, since they avoid uncoalesced reads &#x2F; gathers, tho the code is trickier.</div><br/><div id="40070296" class="c"><input type="checkbox" id="c-40070296" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40070056">parent</a><span>|</span><a href="#40070944">next</a><span>|</span><label class="collapse" for="c-40070296">[-]</label><label class="expand" for="c-40070296">[1 more]</label></div><br/><div class="children"><div class="content">Oh, nice to finally bump into someone who has experience with CSR!<p>bucketMul has few uncoalesced reads, and it uses a different data structure than the regular CSR - it&#x27;s decribed here: <a href="https:&#x2F;&#x2F;kolinko.github.io&#x2F;effort&#x2F;bucketmul.html" rel="nofollow">https:&#x2F;&#x2F;kolinko.github.io&#x2F;effort&#x2F;bucketmul.html</a> It splits each Matrix row into 16 parts, and chooses which ones are necessary to read. The writes are fully linear.<p>Not sure if I speak sense though, it&#x27;s getting a bit late today, and it&#x27;s been a long day ;)</div><br/></div></div></div></div><div id="40070944" class="c"><input type="checkbox" id="c-40070944" checked=""/><div class="controls bullet"><span class="by">hatthew</span><span>|</span><a href="#40070056">prev</a><span>|</span><a href="#40067923">next</a><span>|</span><label class="collapse" for="c-40070944">[-]</label><label class="expand" for="c-40070944">[2 more]</label></div><br/><div class="children"><div class="content">This seems similar to semi-structured (aka 2:4) sparsity, may be worth explicitly comparing. As far as I can tell by skimming, your technique:<p>- is optimized for apple silicon
 - ~2x speed at 75% sparsity
 - dynamic, depends on input, applied at runtime
 - can choose amount of sparsity<p>And 2:4 semi-structured sparsity:<p>- is optimized for GPUs with sparse tensor cores (nvidia ampere and beyond)
 - ~2x speed at 50% sparsity
 - static, applied to the model at rest
 - probably worse results than your technique at 50% sparsity<p>The interesting comparison I&#x27;d want to see is semi-structured sparsity results (50% sparsity, 2x speedup) vs your results at 75% sparsity (2x speedup).</div><br/><div id="40071052" class="c"><input type="checkbox" id="c-40071052" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40070944">parent</a><span>|</span><a href="#40067923">next</a><span>|</span><label class="collapse" for="c-40071052">[-]</label><label class="expand" for="c-40071052">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for checking it out!<p>I also can’t wait to have more tests done.<p>Also, I chose Apple Silicon because it was easier for me to develop
on. It is possible that the algorithm would also have good performance on other architectures.</div><br/></div></div></div></div><div id="40067923" class="c"><input type="checkbox" id="c-40067923" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#40070944">prev</a><span>|</span><a href="#40069052">next</a><span>|</span><label class="collapse" for="c-40067923">[-]</label><label class="expand" for="c-40067923">[7 more]</label></div><br/><div class="children"><div class="content">Thank you for this really cool and open contribution!<p>I will be watching llama.cpp closely for them to implement this!<p>I’ve been looking for ways to speed up CPU inference and I really like this idea of “effort”</div><br/><div id="40068005" class="c"><input type="checkbox" id="c-40068005" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40067923">parent</a><span>|</span><a href="#40069052">next</a><span>|</span><label class="collapse" for="c-40068005">[-]</label><label class="expand" for="c-40068005">[6 more]</label></div><br/><div class="children"><div class="content">Hahah, thanks! It was a marathon to get develop this, and I&#x27;m glad it reached the front page.<p>The name was proposed by chatgpt :) It claims it doesn&#x27;t recognise this approach - so there is a chance it&#x27;s really a new thing.<p>I want to reach out to llama.cpp and the others - I hope it gets implemented. I considered just writing a patch to llama, but c++ and the scale of that project was beyond me.<p>As for CPU inference - it should speed it up just as well. But thanks to the fact that it can load up a fraction of weights (e.g. just 70%, skipping the least important ones), it should be possible now to run models on less VRAM than before (still, Q8 needs to implemented though).<p>Funnily - when I tried comparing benchmarks to llama.cpp, I couldn&#x27;t find speeds for 7B&#x2F;FP16 on MB Air 16GB, because it&#x27;s impossible to run with regular methods. It is possible with Effort.<p>Ditto, I was running full resolution, but cropped, Mixtral on my 96GB M2, even though it usually takes 114GB ram. I just loaded 75% of weights, and it was working smoothly. (before I messed something up with implementation and it now produces crap output - needs a fix)</div><br/><div id="40068134" class="c"><input type="checkbox" id="c-40068134" checked=""/><div class="controls bullet"><span class="by">dhruvdh</span><span>|</span><a href="#40067923">root</a><span>|</span><a href="#40068005">parent</a><span>|</span><a href="#40069869">next</a><span>|</span><label class="collapse" for="c-40068134">[-]</label><label class="expand" for="c-40068134">[2 more]</label></div><br/><div class="children"><div class="content">I would imagine the importance of weights depends on the prompt. How do you decide which weights are important?</div><br/><div id="40068214" class="c"><input type="checkbox" id="c-40068214" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40067923">root</a><span>|</span><a href="#40068134">parent</a><span>|</span><a href="#40069869">next</a><span>|</span><label class="collapse" for="c-40068214">[-]</label><label class="expand" for="c-40068214">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, that is the point more or less - it dynamically chise the weights layer per layer depending on the internal state.<p>A bit technical explaination here.
<a href="https:&#x2F;&#x2F;kolinko.github.io&#x2F;effort&#x2F;equations.html" rel="nofollow">https:&#x2F;&#x2F;kolinko.github.io&#x2F;effort&#x2F;equations.html</a></div><br/></div></div></div></div><div id="40069869" class="c"><input type="checkbox" id="c-40069869" checked=""/><div class="controls bullet"><span class="by">0x4139</span><span>|</span><a href="#40067923">root</a><span>|</span><a href="#40068005">parent</a><span>|</span><a href="#40068134">prev</a><span>|</span><a href="#40068590">next</a><span>|</span><label class="collapse" for="c-40069869">[-]</label><label class="expand" for="c-40069869">[1 more]</label></div><br/><div class="children"><div class="content">Implementing this approach could significantly enhance the adoption of LLMs within mobile phone libraries and other compact devices. I highly recommend opening an improvement issue for llama.cpp.</div><br/></div></div><div id="40068590" class="c"><input type="checkbox" id="c-40068590" checked=""/><div class="controls bullet"><span class="by">indymike</span><span>|</span><a href="#40067923">root</a><span>|</span><a href="#40068005">parent</a><span>|</span><a href="#40069869">prev</a><span>|</span><a href="#40069052">next</a><span>|</span><label class="collapse" for="c-40068590">[-]</label><label class="expand" for="c-40068590">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It is possible with Effort.<p>&quot;All things are possible with enough effort.&quot; -- Dad.</div><br/><div id="40068738" class="c"><input type="checkbox" id="c-40068738" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40067923">root</a><span>|</span><a href="#40068590">parent</a><span>|</span><a href="#40069052">next</a><span>|</span><label class="collapse" for="c-40068738">[-]</label><label class="expand" for="c-40068738">[1 more]</label></div><br/><div class="children"><div class="content">Hahaha :)</div><br/></div></div></div></div></div></div></div></div><div id="40069052" class="c"><input type="checkbox" id="c-40069052" checked=""/><div class="controls bullet"><span class="by">gsuuon</span><span>|</span><a href="#40067923">prev</a><span>|</span><a href="#40072617">next</a><span>|</span><label class="collapse" for="c-40069052">[-]</label><label class="expand" for="c-40069052">[4 more]</label></div><br/><div class="children"><div class="content">Nice writeup! Very curious about the performance per VRAM with this compared to just quantization. Any plans to implement a cross platform version?</div><br/><div id="40069499" class="c"><input type="checkbox" id="c-40069499" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40069052">parent</a><span>|</span><a href="#40072617">next</a><span>|</span><label class="collapse" for="c-40069499">[-]</label><label class="expand" for="c-40069499">[3 more]</label></div><br/><div class="children"><div class="content">Per VRAM - not much netter, because it still uses all the weights, just not all the time.<p>I mean - it can also load less weights, but quality seems to degrade quick after offloading more than 20-30% weights.<p>In other words - this algorithm decouples inference time from VRAM use.<p>Having said that, I’m curious as well if using effort you can get better results on Q8 cropped to 75% than on Q6.<p>But it’s still probably a few weeks to get the implementation polished enough to be well tested.</div><br/><div id="40070579" class="c"><input type="checkbox" id="c-40070579" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#40069052">root</a><span>|</span><a href="#40069499">parent</a><span>|</span><a href="#40072617">next</a><span>|</span><label class="collapse" for="c-40070579">[-]</label><label class="expand" for="c-40070579">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Having said that, I’m curious as well if using effort you can get better results on Q8 cropped to 75% than on Q6.<p>This is what I wanted to ask. This seems like the same <i>kind</i> of optimization as quantization, sacrificing a bit of quality for performance by discarding some data. So then the question is, which is better, and how do they combine?<p>You could even potentially get different results at different points in the scale. Maybe Q8 cropped to 75% isn&#x27;t better than Q6 but Q4 cropped to 75% is better than Q3, or vice versa.</div><br/><div id="40073649" class="c"><input type="checkbox" id="c-40073649" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40069052">root</a><span>|</span><a href="#40070579">parent</a><span>|</span><a href="#40072617">next</a><span>|</span><label class="collapse" for="c-40073649">[-]</label><label class="expand" for="c-40073649">[1 more]</label></div><br/><div class="children"><div class="content">Below Q8 I think it will combine poorly - bucketMul needs to keep a few bits for an index. It can be offset by the fact that the numbers from each bucket are from a limited range. My intuition is that with Q8 this trades off roughly evenly - meaning bucketed Q8 may store as much info as regular Q8, but it will be more difficult with lower quantizations, and cross the boundary of impossible after Q5. Don&#x27;t have math to back it up, but perhaps some information theoretitian could pick up the calculations.<p>You could say that these are divergent paths in the future developments (if the results hold for Q8) - <i>perhaps</i> you can crop the Q8 models to Q6 sizes and inference speeds of Q2&#x2F;Q4. On the other hand, the <i>wildly optimistic</i> scenario is that the bucketMul speeds will overcome even Q1, with dynamic computation pruning - token by token and layer by layer, by having a separate small network that chooses effort levels based on the input data. (someone already proposed it in the thread).<p>For now, the most important thing is fixing the bugs, doing more serious tests for FP16, and showing how the charts look for Q8. Especially the Q8 is the biggest unknown, although the initial results are hopeful.</div><br/></div></div></div></div></div></div></div></div><div id="40072617" class="c"><input type="checkbox" id="c-40072617" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#40069052">prev</a><span>|</span><a href="#40068275">next</a><span>|</span><label class="collapse" for="c-40072617">[-]</label><label class="expand" for="c-40072617">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been trying to think about how you&#x27;d amp up the batch size here.  it&#x27;s a bit tricky since the memory access would be way higher, but I think you can actually still save on compute by chunking things up in a clever way to utilize the tensorcores</div><br/><div id="40073598" class="c"><input type="checkbox" id="c-40073598" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40072617">parent</a><span>|</span><a href="#40068275">next</a><span>|</span><label class="collapse" for="c-40073598">[-]</label><label class="expand" for="c-40073598">[1 more]</label></div><br/><div class="children"><div class="content">What would definitely help would be a better understanding a GPU handles myVal - does it go to the cache, threadgroup memory or where. Or perhaps decompilation of the source kernel to see what&#x27;s really going on there. If we knew, we could calculate the optimal parameters given an architecture.<p>There is also another approach I tried - I&#x27;ve kept it in the wildMul.swift&#x2F;metal . More by the book. Splitting work so that one thread keeps track of only one or two output dimensions when going through the buckets. Didn&#x27;t manage to get it working with a decent speed though.<p>The idea would be to make sure that one simdgroup reads 4&#x2F;8 buckets in a batch (so still ~8-16 consecutive bytes which is considered minimum for an optimal read on Apple Silicon)  and splits each single bucket among 4-8 threads. One thread might then need to remember only 2-4 dims in myVal, and this might stay in internal GPU registers.<p>Not sure if I&#x27;m explaining this all correctly though.<p>Having said this - the priority now will be to remove the overhead to get the whole inference time closer in speed to just the multiplication speeds, but above all - fixing the bugs causing suboptimal results at 100% (and breaking Mixtral), and doing Q8. The algorithm needs to work in Q8 to be really usable.</div><br/></div></div></div></div><div id="40068275" class="c"><input type="checkbox" id="c-40068275" checked=""/><div class="controls bullet"><span class="by">toisanji</span><span>|</span><a href="#40072617">prev</a><span>|</span><a href="#40069516">next</a><span>|</span><label class="collapse" for="c-40068275">[-]</label><label class="expand" for="c-40068275">[3 more]</label></div><br/><div class="children"><div class="content">this sounds related to this: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.12456" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.12456</a> <a href="https:&#x2F;&#x2F;github.com&#x2F;SJTU-IPADS&#x2F;PowerInfer">https:&#x2F;&#x2F;github.com&#x2F;SJTU-IPADS&#x2F;PowerInfer</a></div><br/><div id="40068357" class="c"><input type="checkbox" id="c-40068357" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40068275">parent</a><span>|</span><a href="#40069516">next</a><span>|</span><label class="collapse" for="c-40068357">[-]</label><label class="expand" for="c-40068357">[2 more]</label></div><br/><div class="children"><div class="content">Similar theme, but they skip whole neurons, in my case it’s down to a level of single weights.<p>From my experiment, skipping whole neurons (so whole rows&#x2F;columns of matrixes) didn’t allow for such good results. In my case 30-50% neurons whole are skipped with 15% effort, but the rest is used partially still.<p>There are a few papers on a similar theme that a friend sent me today morning - I plan to add them in the citations part</div><br/><div id="40068503" class="c"><input type="checkbox" id="c-40068503" checked=""/><div class="controls bullet"><span class="by">toisanji</span><span>|</span><a href="#40068275">root</a><span>|</span><a href="#40068357">parent</a><span>|</span><a href="#40069516">next</a><span>|</span><label class="collapse" for="c-40068503">[-]</label><label class="expand" for="c-40068503">[1 more]</label></div><br/><div class="children"><div class="content">awesome, looking forward to seeing how your results perform. I tested powerinfer on smaller models and didn&#x27;t see large performance gains.</div><br/></div></div></div></div></div></div><div id="40069516" class="c"><input type="checkbox" id="c-40069516" checked=""/><div class="controls bullet"><span class="by">globally_unique</span><span>|</span><a href="#40068275">prev</a><span>|</span><a href="#40071173">next</a><span>|</span><label class="collapse" for="c-40069516">[-]</label><label class="expand" for="c-40069516">[2 more]</label></div><br/><div class="children"><div class="content">That looks like fantastic stuff. I just want to point out the 15ms delay looks similar to 60Hz vsync (16.7ms), if you are updating the screen once per token, maybe that’s causing a sync somehow?</div><br/><div id="40069807" class="c"><input type="checkbox" id="c-40069807" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40069516">parent</a><span>|</span><a href="#40071173">next</a><span>|</span><label class="collapse" for="c-40069807">[-]</label><label class="expand" for="c-40069807">[1 more]</label></div><br/><div class="children"><div class="content">Nah, that&#x27;s not it, I measure the CPU &amp; GPU work separately, and 15ms happens between the kernel invocations. It also happens when I don&#x27;t print out text.<p>Thanks for the idea though! I treat it as the first community contribution :D</div><br/></div></div></div></div><div id="40071173" class="c"><input type="checkbox" id="c-40071173" checked=""/><div class="controls bullet"><span class="by">yousif_123123</span><span>|</span><a href="#40069516">prev</a><span>|</span><a href="#40068696">next</a><span>|</span><label class="collapse" for="c-40071173">[-]</label><label class="expand" for="c-40071173">[2 more]</label></div><br/><div class="children"><div class="content">I know this doesn&#x27;t retrain, but I wonder if approaches like this plus quantization could get back any &quot;lost&quot; quality with some post training.<p>It&#x27;s great to see, and to know in one&#x27;s mind how much likely performance and cost will be better in the future.<p>I know it&#x27;s fun to work on, but I also want to say THANK YOU for developing open source.</div><br/><div id="40072310" class="c"><input type="checkbox" id="c-40072310" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#40071173">parent</a><span>|</span><a href="#40068696">next</a><span>|</span><label class="collapse" for="c-40072310">[-]</label><label class="expand" for="c-40072310">[1 more]</label></div><br/><div class="children"><div class="content">At first glance, that sounds like it could work. From what I&#x27;ve read, there seems to be two main ways to regain some quality with quantization: post-training which happens after, and quantization-aware training which quantizes during training but leaves the activations and gradients full precision</div><br/></div></div></div></div><div id="40068696" class="c"><input type="checkbox" id="c-40068696" checked=""/><div class="controls bullet"><span class="by">kanwisher</span><span>|</span><a href="#40071173">prev</a><span>|</span><a href="#40072482">next</a><span>|</span><label class="collapse" for="c-40068696">[-]</label><label class="expand" for="c-40068696">[7 more]</label></div><br/><div class="children"><div class="content">Could you break down a bit more about why you can skip so many calculations ?</div><br/><div id="40068845" class="c"><input type="checkbox" id="c-40068845" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40068696">parent</a><span>|</span><a href="#40068983">next</a><span>|</span><label class="collapse" for="c-40068845">[-]</label><label class="expand" for="c-40068845">[3 more]</label></div><br/><div class="children"><div class="content">It’s explained in detail here:<p><a href="https:&#x2F;&#x2F;kolinko.github.io&#x2F;effort&#x2F;equations.html" rel="nofollow">https:&#x2F;&#x2F;kolinko.github.io&#x2F;effort&#x2F;equations.html</a><p>Long story short - I kind of sort them and pick only the top % that would give a highest result.<p>One part is choosing them though - I think this was done before in some papers. But the second part was an implementation of multiplication that is efficient on both gpus and cpus when choosing weights almost at will.<p>All explained on the site, but I just got feedback that it may not be easy enough to read, so I’ll push it through gpt for grmamar fixes soon :) It’s also a bit complicated as an algorithm.</div><br/><div id="40069070" class="c"><input type="checkbox" id="c-40069070" checked=""/><div class="controls bullet"><span class="by">throwaway2562</span><span>|</span><a href="#40068696">root</a><span>|</span><a href="#40068845">parent</a><span>|</span><a href="#40068983">next</a><span>|</span><label class="collapse" for="c-40069070">[-]</label><label class="expand" for="c-40069070">[2 more]</label></div><br/><div class="children"><div class="content">Hmmm. Very interesting. I wonder if you could speed up your clever approximation still further with this approach <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2205.09120.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2205.09120.pdf</a></div><br/><div id="40070377" class="c"><input type="checkbox" id="c-40070377" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40068696">root</a><span>|</span><a href="#40069070">parent</a><span>|</span><a href="#40068983">next</a><span>|</span><label class="collapse" for="c-40070377">[-]</label><label class="expand" for="c-40070377">[1 more]</label></div><br/><div class="children"><div class="content">Nah, sadly this most likely will not work with Q1&#x2F;Q1.5 implementations - initially I was playing with monte carlo approximations (before I arrived at bucketMul), and the convergence was very slow for binary&#x2F;ternary networks.<p>Or, in simpler terms - if you have just ones and zeroes, and minus ones, you can remove zeroes from calculations, but that&#x27;s it. No good method to figure out which ones are more important than the other ones.<p>Also, there are no bits left to store positional information when bucketing.<p>There are some paths that could be explored in this fashion, but it would require a redesign of the algorithm from the ground up.</div><br/></div></div></div></div></div></div><div id="40068983" class="c"><input type="checkbox" id="c-40068983" checked=""/><div class="controls bullet"><span class="by">punnerud</span><span>|</span><a href="#40068696">parent</a><span>|</span><a href="#40068845">prev</a><span>|</span><a href="#40069621">next</a><span>|</span><label class="collapse" for="c-40068983">[-]</label><label class="expand" for="c-40068983">[2 more]</label></div><br/><div class="children"><div class="content">A bit like calculating the fastest route for a car, you probably don’t need to calculate the distances for the opposite side of earth if you will not drive there.
Then multiply that by a billion, but the optimization still holds.</div><br/><div id="40073716" class="c"><input type="checkbox" id="c-40073716" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40068696">root</a><span>|</span><a href="#40068983">parent</a><span>|</span><a href="#40069621">next</a><span>|</span><label class="collapse" for="c-40073716">[-]</label><label class="expand" for="c-40073716">[1 more]</label></div><br/><div class="children"><div class="content">Nice analogy, thanks! Yes - you still need a map, because one day you need this road, another day you need info about another road, but you&#x27;re not using info about all the roads all of the time.</div><br/></div></div></div></div></div></div><div id="40072482" class="c"><input type="checkbox" id="c-40072482" checked=""/><div class="controls bullet"><span class="by">wayoverthecloud</span><span>|</span><a href="#40068696">prev</a><span>|</span><a href="#40069589">next</a><span>|</span><label class="collapse" for="c-40072482">[-]</label><label class="expand" for="c-40072482">[2 more]</label></div><br/><div class="children"><div class="content">I was wondering if something similar can be done for CNN too.</div><br/><div id="40073688" class="c"><input type="checkbox" id="c-40073688" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40072482">parent</a><span>|</span><a href="#40069589">next</a><span>|</span><label class="collapse" for="c-40073688">[-]</label><label class="expand" for="c-40073688">[1 more]</label></div><br/><div class="children"><div class="content">A friend who first heard of the method immediately suggested this may work, perhaps even better in Diffusion models.<p>It is really a drop-in replacement for the regular matrix multiplication. The data structure is a bit more painful to work with (you have three datasets, not just one to represent a weight matrix), but it shouldn&#x27;t be too difficult for devs of the existing inference engines to implement it for a test.<p>Half of my challenge was that I wasn&#x27;t knowledgable enough to just patch llama.cpp or MLX and use their engines with bucketMul. That&#x27;s why I opted for making my own - still not sure if it was a good choice to build everything from the ground up though, although I&#x27;m proud of the name :)<p>Finally - the basic math behind approximation suggest that this should work with all the models - cosine similarity score is 0.99 until the magical 25% mark for most of the matrixes I tried. It can vary within a model though - e.g. in Llama, on the first layer, wq&#x2F;wv&#x2F;wk could be easily approximated with 5% effort, whereas some deeper layers at 25% had just 0.90 cos score - still seemed enough to not lose coherency within the model.</div><br/></div></div></div></div><div id="40069589" class="c"><input type="checkbox" id="c-40069589" checked=""/><div class="controls bullet"><span class="by">bick_nyers</span><span>|</span><a href="#40072482">prev</a><span>|</span><a href="#40068916">next</a><span>|</span><label class="collapse" for="c-40069589">[-]</label><label class="expand" for="c-40069589">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m wondering if you could sort the two inputs, add the indicies for the multiplications together, then take the largest of that.<p>In your 0.1 example, 1000 gets index 2, and 0.1 index 0, combines to 2. This will tie with the 1*8, but I think it would even out with larger vector lengths.<p>Edit: I could be wrong but I think you can precompute the indices for the weights in advance without a prompt, then you won&#x27;t need to perform those sorts at runtime.</div><br/><div id="40073744" class="c"><input type="checkbox" id="c-40073744" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40069589">parent</a><span>|</span><a href="#40068916">next</a><span>|</span><label class="collapse" for="c-40073744">[-]</label><label class="expand" for="c-40073744">[1 more]</label></div><br/><div class="children"><div class="content">That was one of the ideas I was also considering originally! Don&#x27;t remember why is skipped it though - may have been because I tried the published one and it worked well enough during the first tries.<p>As for indices for weight - not sure if I get what you mean, but the weights are sorted as a part of precomputations. Sorting is not done in runtime, because that would kill any efficiency - the moment you need to read all the weights, you&#x27;ve lost, because it&#x27;s the memory reads, not computations that matter. If you can skip one memory read by doing 5-10 multiplications, it&#x27;s a good tradeoff.</div><br/></div></div></div></div><div id="40068916" class="c"><input type="checkbox" id="c-40068916" checked=""/><div class="controls bullet"><span class="by">byyoung3</span><span>|</span><a href="#40069589">prev</a><span>|</span><a href="#40068862">next</a><span>|</span><label class="collapse" for="c-40068916">[-]</label><label class="expand" for="c-40068916">[4 more]</label></div><br/><div class="children"><div class="content">I think this is the overall idea behind the MoE LLM models right? MoE just expands upon this idea by learning which sets of weights to use</div><br/><div id="40069007" class="c"><input type="checkbox" id="c-40069007" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40068916">parent</a><span>|</span><a href="#40070683">next</a><span>|</span><label class="collapse" for="c-40069007">[-]</label><label class="expand" for="c-40069007">[1 more]</label></div><br/><div class="children"><div class="content">I’d say that this expands on MoE really - MoE chooses
dynamically which groups of weights may be needed, but it’s whole matrixes. Here it’s ingle weights.<p>Also, this works on top of MoE beautifully - most of the development and testing was done on Mixtral and it was getting (anecdotally) even better results - getting down to 15-18% effort before seriously losing quality.<p>I decided to release the Mistral version, but Mixtral was fully operational a few commits back :)<p>Also, the cool thing - because you can load only the top say 70% weights, I was running Mixtral full precision on my MB 96G - there were no bemchmarks for this in other impls because others need to load full model into the memory.<p>The real question is Q8 performance - I didn’t implement it fully so far.</div><br/></div></div><div id="40070683" class="c"><input type="checkbox" id="c-40070683" checked=""/><div class="controls bullet"><span class="by">huac</span><span>|</span><a href="#40068916">parent</a><span>|</span><a href="#40069007">prev</a><span>|</span><a href="#40068862">next</a><span>|</span><label class="collapse" for="c-40070683">[-]</label><label class="expand" for="c-40070683">[2 more]</label></div><br/><div class="children"><div class="content">It also feels similar to mixture of depths (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.02258" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.02258</a>).<p>Being able to apply this post-training is pretty cool though, makes it easier to use across a wider range of setups.</div><br/><div id="40073779" class="c"><input type="checkbox" id="c-40073779" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40068916">root</a><span>|</span><a href="#40070683">parent</a><span>|</span><a href="#40068862">next</a><span>|</span><label class="collapse" for="c-40073779">[-]</label><label class="expand" for="c-40073779">[1 more]</label></div><br/><div class="children"><div class="content">Yes! I like that, and I saw that paper last weekend iirc. I think these MoD&#x2F;MoE and other similar methods are highly compatible, and in a similar style.<p>I was originally afraid that this method wouldn&#x27;t be compatible with MoE and the other methods, but fortunately, at least for Mixtral, there seems to be an amazing synergy.<p>By the way, other tasks have higher priority now, byt there is an interesting observation about MoE. In MoE you get two experts chosen, and each expert has a different weight attached to it - e.g. expert 1 has 75% weight, and expert 2 has 25% weight. Perhaps this could allow to scale the effort to give 75% effort to one expert, and 25% to the other. There are some issues there due to non-linearity of the layers, but perhaps there is something to it.</div><br/></div></div></div></div></div></div><div id="40070392" class="c"><input type="checkbox" id="c-40070392" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#40068862">prev</a><span>|</span><a href="#40068202">next</a><span>|</span><label class="collapse" for="c-40070392">[-]</label><label class="expand" for="c-40070392">[2 more]</label></div><br/><div class="children"><div class="content">Looks like the models are missing on Huggingface, I&#x27;ve logged an issue: <a href="https:&#x2F;&#x2F;github.com&#x2F;kolinko&#x2F;effort&#x2F;issues&#x2F;3">https:&#x2F;&#x2F;github.com&#x2F;kolinko&#x2F;effort&#x2F;issues&#x2F;3</a></div><br/><div id="40070501" class="c"><input type="checkbox" id="c-40070501" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40070392">parent</a><span>|</span><a href="#40068202">next</a><span>|</span><label class="collapse" for="c-40070501">[-]</label><label class="expand" for="c-40070501">[1 more]</label></div><br/><div class="children"><div class="content">Ah yes, forgot to make the repo public. Thanks a ton for pointing it out and writing the comment - I&#x27;d miss it if you didn&#x27;t point it out on HN.</div><br/></div></div></div></div><div id="40068202" class="c"><input type="checkbox" id="c-40068202" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#40070392">prev</a><span>|</span><a href="#40068750">next</a><span>|</span><label class="collapse" for="c-40068202">[-]</label><label class="expand" for="c-40068202">[2 more]</label></div><br/><div class="children"><div class="content">we need this into llama.cpp it seems somewhat stable down to 40% effort</div><br/><div id="40068233" class="c"><input type="checkbox" id="c-40068233" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40068202">parent</a><span>|</span><a href="#40068750">next</a><span>|</span><label class="collapse" for="c-40068233">[-]</label><label class="expand" for="c-40068233">[1 more]</label></div><br/><div class="children"><div class="content">Yes! I hope, if it’s proven, that it will be implemented into the main inference engines.<p>40% effort is only a bit faster than a full base multiplication, but I hope both the speed and the accuracy could be improved further.</div><br/></div></div></div></div><div id="40068750" class="c"><input type="checkbox" id="c-40068750" checked=""/><div class="controls bullet"><span class="by">saurik</span><span>|</span><a href="#40068202">prev</a><span>|</span><a href="#40068589">next</a><span>|</span><label class="collapse" for="c-40068750">[-]</label><label class="expand" for="c-40068750">[1 more]</label></div><br/><div class="children"><div class="content">Also being discussed at: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40067489">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40067489</a></div><br/></div></div><div id="40068589" class="c"><input type="checkbox" id="c-40068589" checked=""/><div class="controls bullet"><span class="by">gcy</span><span>|</span><a href="#40068750">prev</a><span>|</span><a href="#40068667">next</a><span>|</span><label class="collapse" for="c-40068589">[-]</label><label class="expand" for="c-40068589">[3 more]</label></div><br/><div class="children"><div class="content">Could you explain the pseudo code in your equations page? Is the second approxMul call a typo (also the capitalized V)?<p>def precompute(W):
  W = W.T
  probes = get_probes(W)
  W_idx, W_val = sortMatrixRows(W)<p>def approxMul(v, W_idx, W_val, probes):
  cutoff_chart = v * probes
  cutoff = topK(cutoff_chart, effort)
  approxMul(V, W_idx, W_val, cutoff)</div><br/><div id="40070421" class="c"><input type="checkbox" id="c-40070421" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40068589">parent</a><span>|</span><a href="#40068667">next</a><span>|</span><label class="collapse" for="c-40070421">[-]</label><label class="expand" for="c-40070421">[2 more]</label></div><br/><div class="children"><div class="content">oh, thanks, I fixed it. No idea what I meant there originally.<p>There are still a few typos on the page, I&#x27;ll be fixing them tomorrow - it&#x27;s midnight now, and my mental batteries are slowly drying out :)</div><br/><div id="40071799" class="c"><input type="checkbox" id="c-40071799" checked=""/><div class="controls bullet"><span class="by">mijoharas</span><span>|</span><a href="#40068589">root</a><span>|</span><a href="#40070421">parent</a><span>|</span><a href="#40068667">next</a><span>|</span><label class="collapse" for="c-40071799">[-]</label><label class="expand" for="c-40071799">[1 more]</label></div><br/><div class="children"><div class="content">Did you deploy your changes? I just refreshed and that is still there on the page[0].<p>[0] <a href="https:&#x2F;&#x2F;kolinko.github.io&#x2F;effort&#x2F;equations.html" rel="nofollow">https:&#x2F;&#x2F;kolinko.github.io&#x2F;effort&#x2F;equations.html</a></div><br/></div></div></div></div></div></div><div id="40068667" class="c"><input type="checkbox" id="c-40068667" checked=""/><div class="controls bullet"><span class="by">coolvision</span><span>|</span><a href="#40068589">prev</a><span>|</span><a href="#40070509">next</a><span>|</span><label class="collapse" for="c-40068667">[-]</label><label class="expand" for="c-40068667">[2 more]</label></div><br/><div class="children"><div class="content">how does it compare to 8-bit&#x2F;4-bit quantization in terms of speed&#x2F;accuracy?</div><br/><div id="40068772" class="c"><input type="checkbox" id="c-40068772" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40068667">parent</a><span>|</span><a href="#40070509">next</a><span>|</span><label class="collapse" for="c-40068772">[-]</label><label class="expand" for="c-40068772">[1 more]</label></div><br/><div class="children"><div class="content">hard to say for now, I’m curious as well, but I used simpler tests so far because of the implementation issues - most test suites are geared towards testing models and not model implementation.<p>I didn’t want to wait any longer with the release, but better tests will be coming soon I hope. Anecdotally, I think 30% effort should be comparable to Q8z<p>More importantly, this algorithm should work on top of Q8. The quality is not yet certain though - I could use help with the implementation.</div><br/></div></div></div></div><div id="40070509" class="c"><input type="checkbox" id="c-40070509" checked=""/><div class="controls bullet"><span class="by">a2code</span><span>|</span><a href="#40068667">prev</a><span>|</span><a href="#40070292">next</a><span>|</span><label class="collapse" for="c-40070509">[-]</label><label class="expand" for="c-40070509">[4 more]</label></div><br/><div class="children"><div class="content">If it sounds too good to be true, it probably is not.<p>If removing weights improves some metrics, that may be a clue that the model is not optimal in some sense.</div><br/><div id="40070590" class="c"><input type="checkbox" id="c-40070590" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40070509">parent</a><span>|</span><a href="#40071774">next</a><span>|</span><label class="collapse" for="c-40070590">[-]</label><label class="expand" for="c-40070590">[1 more]</label></div><br/><div class="children"><div class="content">The algorithm still uses all the weights, just not all the time - just skips the weights when they are not important given an input vector.<p>Also, approximation methods, as a field, are not new and they have shown their use.<p>Having said all that, extraordinary claims require extraordinary evidence - that’s why I hedge the communication messages. It’s „probably” until we get serious tests going on</div><br/></div></div><div id="40071774" class="c"><input type="checkbox" id="c-40071774" checked=""/><div class="controls bullet"><span class="by">mijoharas</span><span>|</span><a href="#40070509">parent</a><span>|</span><a href="#40070590">prev</a><span>|</span><a href="#40071928">next</a><span>|</span><label class="collapse" for="c-40071774">[-]</label><label class="expand" for="c-40071774">[1 more]</label></div><br/><div class="children"><div class="content">The metric that&#x27;s improved is computation speed, and it&#x27;s achieved by essentially changing the computation (by not performing some computation that likely doesn&#x27;t have large impact on the results).<p>Give that it&#x27;s a different computation, you could argue that Mistral+effort is a new model with the improved metric of quality per amount of computation performed.<p>Otherwise - given that for every different input there&#x27;s a seperate set of weights in the model that are excluded - I don&#x27;t think you could conclude from this (if it holds up etc etc) that the base model is not optimal.<p>In a similar sense, quantization improved the &quot;quality per model size&quot; metric, but I don&#x27;t think people are arguing that Mistral is less optimal than quantised Mistral (unless you&#x27;re speaking about literally that metric). On the other hand, if you&#x27;re targeting that metric specifically, then it would make perfect sense to say that quantised Mistral is more optimisal for it.<p>I guess it comes down to optimality being dependent on the metric you&#x27;re looking at, and there being many things you might want to optimise for.<p>To note again, if this technique holds up, it&#x27;s better than model distillation (just get rid of some of the weights) because for some inputs those weights could matter and this technique should (iiuc) account for that somewhat. To me, this is what it sounds like you&#x27;re referring to when saying:<p>&gt; If removing weights improves some metrics, that may be a clue that the model is not optimal in some sense</div><br/></div></div><div id="40071928" class="c"><input type="checkbox" id="c-40071928" checked=""/><div class="controls bullet"><span class="by">addandsubtract</span><span>|</span><a href="#40070509">parent</a><span>|</span><a href="#40071774">prev</a><span>|</span><a href="#40070292">next</a><span>|</span><label class="collapse" for="c-40071928">[-]</label><label class="expand" for="c-40071928">[1 more]</label></div><br/><div class="children"><div class="content">Wait, does the second &quot;it&quot; refer to the true part? Because traditionally, it refers to the &quot;too good&quot; expression. So you&#x27;d say, &quot;it _is_ too good to be true&quot;.</div><br/></div></div></div></div><div id="40070292" class="c"><input type="checkbox" id="c-40070292" checked=""/><div class="controls bullet"><span class="by">uhlo</span><span>|</span><a href="#40070509">prev</a><span>|</span><a href="#40069364">next</a><span>|</span><label class="collapse" for="c-40070292">[-]</label><label class="expand" for="c-40070292">[4 more]</label></div><br/><div class="children"><div class="content">Okay now add a small model that decides how much effort is needed in each inference step and we are good to go</div><br/><div id="40070324" class="c"><input type="checkbox" id="c-40070324" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40070292">parent</a><span>|</span><a href="#40069364">next</a><span>|</span><label class="collapse" for="c-40070324">[-]</label><label class="expand" for="c-40070324">[3 more]</label></div><br/><div class="children"><div class="content">Yes! That would be awesome. Especially since there are ~32*6 independent effort settings for every single token.<p>I tested the most basic implementation, with a flat effort setting for all the muls, but I bet the results could be pushed even further with such an approach. Or even with just doing some ML to figure out which layer&#x2F;matrix needs more and which less effort.</div><br/><div id="40070369" class="c"><input type="checkbox" id="c-40070369" checked=""/><div class="controls bullet"><span class="by">uhlo</span><span>|</span><a href="#40070292">root</a><span>|</span><a href="#40070324">parent</a><span>|</span><a href="#40069364">next</a><span>|</span><label class="collapse" for="c-40070369">[-]</label><label class="expand" for="c-40070369">[2 more]</label></div><br/><div class="children"><div class="content">Great work! One thing: it seems the hugging face link doesn&#x27;t work... I get a 404</div><br/><div id="40073788" class="c"><input type="checkbox" id="c-40073788" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40070292">root</a><span>|</span><a href="#40070369">parent</a><span>|</span><a href="#40069364">next</a><span>|</span><label class="collapse" for="c-40073788">[-]</label><label class="expand" for="c-40073788">[1 more]</label></div><br/><div class="children"><div class="content">It should work now :)</div><br/></div></div></div></div></div></div></div></div><div id="40069364" class="c"><input type="checkbox" id="c-40069364" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#40070292">prev</a><span>|</span><a href="#40069478">next</a><span>|</span><label class="collapse" for="c-40069364">[-]</label><label class="expand" for="c-40069364">[6 more]</label></div><br/><div class="children"><div class="content">do you have a simple python impl? :)</div><br/><div id="40069527" class="c"><input type="checkbox" id="c-40069527" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40069364">parent</a><span>|</span><a href="#40071966">next</a><span>|</span><label class="collapse" for="c-40069527">[-]</label><label class="expand" for="c-40069527">[3 more]</label></div><br/><div class="children"><div class="content">It originally started as a fork to Recmo’s cria pure numpy llama impl :)<p><a href="https:&#x2F;&#x2F;github.com&#x2F;recmo&#x2F;cria">https:&#x2F;&#x2F;github.com&#x2F;recmo&#x2F;cria</a><p>Took a whole night to compute a few
tokens, but I used it to do the first tests.<p>Also, my friend pasted the paper to claude and it produced a working basic impl instantly :D<p>But in all seriousness - I think MLX implementation would be doable, or a wrapper to the Metal gou functionality</div><br/><div id="40069633" class="c"><input type="checkbox" id="c-40069633" checked=""/><div class="controls bullet"><span class="by">kwikiel</span><span>|</span><a href="#40069364">root</a><span>|</span><a href="#40069527">parent</a><span>|</span><a href="#40071966">next</a><span>|</span><label class="collapse" for="c-40069633">[-]</label><label class="expand" for="c-40069633">[2 more]</label></div><br/><div class="children"><div class="content">Will share python implementation soon as a kind of executable pseudo code which then can be ported to any platform.<p>This project is kind of like ultimate nerdsnipe as math is quite simple, you don’t need PhD to understand it and actually implementing things would teach you linear algebra faster vs just mindlessly doing exercises sets.</div><br/><div id="40070475" class="c"><input type="checkbox" id="c-40070475" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40069364">root</a><span>|</span><a href="#40069633">parent</a><span>|</span><a href="#40071966">next</a><span>|</span><label class="collapse" for="c-40070475">[-]</label><label class="expand" for="c-40070475">[1 more]</label></div><br/><div class="children"><div class="content">Haha yes :) Publish it, Kacper!<p>The project is a nerdsnipe for math geeks, because there are multiple small things that beg to be proven &#x2F; described by math there. For example - what&#x27;s the tradeoff between the number of bits we loose when embedding position vs the bits of information that we gain by knowing which bucket a weight belongs to?<p>In other words - is it possible that when storing weights in the bucketed form we can actually end up having a higher precision than using a regular form? For Q8 we get just 4 bits to store the weight (and 1 bit for sign, and 3 bits for location), but these 4 bits need to express numbers from a smaller range than before.</div><br/></div></div></div></div></div></div><div id="40071966" class="c"><input type="checkbox" id="c-40071966" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#40069364">parent</a><span>|</span><a href="#40069527">prev</a><span>|</span><a href="#40069478">next</a><span>|</span><label class="collapse" for="c-40071966">[-]</label><label class="expand" for="c-40071966">[2 more]</label></div><br/><div class="children"><div class="content">not sure why I got downvoted for asking this. seems like a reasonable thing in the ML space<p>anyway here&#x27;s a numerical simulation written in PyTorch for those who want to consider this algo on their projects before fully integrating: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;bwasti&#x2F;78d7058aad7f42dc893d906c877b710c" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;bwasti&#x2F;78d7058aad7f42dc893d906c877b7...</a><p>happy hacking</div><br/><div id="40073805" class="c"><input type="checkbox" id="c-40073805" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40069364">root</a><span>|</span><a href="#40071966">parent</a><span>|</span><a href="#40069478">next</a><span>|</span><label class="collapse" for="c-40073805">[-]</label><label class="expand" for="c-40073805">[1 more]</label></div><br/><div class="children"><div class="content">If you got downvoted, it was briefly. That was a good question. PyTorch&#x2F;numpy is awesome for initial testing.</div><br/></div></div></div></div></div></div><div id="40069478" class="c"><input type="checkbox" id="c-40069478" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#40069364">prev</a><span>|</span><a href="#40067727">next</a><span>|</span><label class="collapse" for="c-40069478">[-]</label><label class="expand" for="c-40069478">[18 more]</label></div><br/><div class="children"><div class="content">My takeaway is that this proves what was said on the recent latent.space podcast with David Luan from Adept.<p><a href="https:&#x2F;&#x2F;www.latent.space&#x2F;p&#x2F;adept" rel="nofollow">https:&#x2F;&#x2F;www.latent.space&#x2F;p&#x2F;adept</a><p>&quot;I think is going to commoditize a lot of the regular LLMs and soon regular multimodal models.&quot;<p>In other words, if you train your own models, you will not get to take advantage of breakthroughs like this that start with open models (like Mistral).<p>All the advantages are going towards the open models and this is an existential risk for OpenAI and other closed model companies.</div><br/><div id="40069776" class="c"><input type="checkbox" id="c-40069776" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40069478">parent</a><span>|</span><a href="#40071318">next</a><span>|</span><label class="collapse" for="c-40069776">[-]</label><label class="expand" for="c-40069776">[1 more]</label></div><br/><div class="children"><div class="content">In this case though, the algorithm should be just as useful to closed models as to open models. There is nothing there that is optimised specifically for Mistral - aside from the hard-coded dimensions in multiple places in the code :D<p>Having said that, it&#x27;s awesome to have open source models out there, and I hope they will ultimately win in the end.</div><br/></div></div><div id="40071318" class="c"><input type="checkbox" id="c-40071318" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#40069478">parent</a><span>|</span><a href="#40069776">prev</a><span>|</span><a href="#40069639">next</a><span>|</span><label class="collapse" for="c-40071318">[-]</label><label class="expand" for="c-40071318">[1 more]</label></div><br/><div class="children"><div class="content">That seems fundamentally flawed to me. Closed providers can copy techniques from open models but not vice versa.<p>To me that reads as closed will always be a step ahead not an existential risk to OpenAI.</div><br/></div></div><div id="40069639" class="c"><input type="checkbox" id="c-40069639" checked=""/><div class="controls bullet"><span class="by">quadrature</span><span>|</span><a href="#40069478">parent</a><span>|</span><a href="#40071318">prev</a><span>|</span><a href="#40071787">next</a><span>|</span><label class="collapse" for="c-40069639">[-]</label><label class="expand" for="c-40069639">[14 more]</label></div><br/><div class="children"><div class="content">Maybe, but theres nothing that stops OpenAI from stealing these tricks.</div><br/><div id="40070207" class="c"><input type="checkbox" id="c-40070207" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#40069478">root</a><span>|</span><a href="#40069639">parent</a><span>|</span><a href="#40069768">next</a><span>|</span><label class="collapse" for="c-40070207">[-]</label><label class="expand" for="c-40070207">[5 more]</label></div><br/><div class="children"><div class="content">Exactly, that&#x27;s the problem with the current state of things with open models, the players that keep their secret sauce keep an edge over the people doing things in open while benefiting from all of their work without contributing back.</div><br/><div id="40070695" class="c"><input type="checkbox" id="c-40070695" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40069478">root</a><span>|</span><a href="#40070207">parent</a><span>|</span><a href="#40072059">next</a><span>|</span><label class="collapse" for="c-40070695">[-]</label><label class="expand" for="c-40070695">[3 more]</label></div><br/><div class="children"><div class="content">That was the claim with a lot of the software in the past, but open source won in many places in the end.</div><br/><div id="40071368" class="c"><input type="checkbox" id="c-40071368" checked=""/><div class="controls bullet"><span class="by">iwontberude</span><span>|</span><a href="#40069478">root</a><span>|</span><a href="#40070695">parent</a><span>|</span><a href="#40072059">next</a><span>|</span><label class="collapse" for="c-40071368">[-]</label><label class="expand" for="c-40071368">[2 more]</label></div><br/><div class="children"><div class="content">At the end of the day, if their profit margins aren’t good, it doesn’t matter whether their competition is open source or not (which is often where OSS wins). I think we are seeing that AI isn’t the slam dunk for increasing productivity or we would see companies like UIPath being profitable. I don’t think we’ve seen anyone net a profit on AI software and the only company that has been investing since at least 2017, Apple, gets zero credit for their contributions and commercialization of the tech. I think about how Amazon abandoned the AI-powered, checkout-free tech because the margin of error stayed stubbornly high for too long. The clock is ticking on the industry and some players, like Apple, already have found it isn’t profitable (well to their standards of 60% return on investment).</div><br/><div id="40073842" class="c"><input type="checkbox" id="c-40073842" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40069478">root</a><span>|</span><a href="#40071368">parent</a><span>|</span><a href="#40072059">next</a><span>|</span><label class="collapse" for="c-40073842">[-]</label><label class="expand" for="c-40073842">[1 more]</label></div><br/><div class="children"><div class="content">Depends on a field.<p>The project from the thread would take me an impossible amount of time without GPT. Even the page itself would take me twice as long to generate - the charts were done by pasting source data to GPT and GPT writing plotlib code for me to chart them, and the equations were originally written by GPT as well, because I wasn&#x27;t familiar with MathJAX.<p>Ditto with the code - a bunch of it was written by GPT originally, since this is my first Swift&#x2F;Metal project. I kept telling it what I want to do in Python, and it kept rewriting it in Swift&#x2F;Metal until I learned the latter.<p>The name &quot;effort&quot; was also invented by GPT. Originally, internally, I was using &quot;quant&quot; but that would be confused with quantization. I considered &quot;perc&quot; from percentage - but that&#x27;s ugly. I described the project to GPT, and it suggested &quot;effort&quot; as a metric.<p>As for self-checkout - in Poland we have Żabka Nano which is still going on, and seems more solid than Amazon, but of course the time will tell :)</div><br/></div></div></div></div></div></div><div id="40072059" class="c"><input type="checkbox" id="c-40072059" checked=""/><div class="controls bullet"><span class="by">Hugsun</span><span>|</span><a href="#40069478">root</a><span>|</span><a href="#40070207">parent</a><span>|</span><a href="#40070695">prev</a><span>|</span><a href="#40069768">next</a><span>|</span><label class="collapse" for="c-40072059">[-]</label><label class="expand" for="c-40072059">[1 more]</label></div><br/><div class="children"><div class="content">In these fast moving early times of LLMs, they can maintain this advantage with simple things like proprietary datasets and greater compute.<p>The difference in quality between the best model that can be made with such proprietary utilities and without is likely to decrease over time as open datasets of greater quality are published and the field matures.<p>The difference in quality and number of competitors ultimately pays the bills and the harsher the competition is, the less money there will be, for each individual company, to maintain their possibly dwindling proprietary edge.<p>The greater access to compute is an edge companies will likely hold for a while. It will be interesting to see how much open models will be able to catch up and how great of an edge proprietary models will maintain.</div><br/></div></div></div></div><div id="40069768" class="c"><input type="checkbox" id="c-40069768" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40069478">root</a><span>|</span><a href="#40069639">parent</a><span>|</span><a href="#40070207">prev</a><span>|</span><a href="#40071787">next</a><span>|</span><label class="collapse" for="c-40069768">[-]</label><label class="expand" for="c-40069768">[8 more]</label></div><br/><div class="children"><div class="content">It&#x27;s extremely unlikely anyone will take any of this.<p>Quick take:<p>- it was 15x slower than llama.cpp when I used Apple&#x27;s new proprietary ML framework on my MacBook<p>- So I made it possible to skip arbitrary amounts of work.<p>- I identified an arbitrary tradeoff that seems arbitrarily good to me.<p>- I&#x27;ve confirmed this by making GPT-4 write some prompt with questions. Then I had the normal version answer, and the &quot;skip arbitrary work&quot; version answer, and it LGTM.<p>- So I threw it up on GitHub, then on HN with a title &quot;LLM inference 2x faster (possibly)&quot;, and people missed: [on my laptop] [in the ML framework I&#x27;m forcing myself to use] [based on an eval I made up] [based on an an eval where I am the evaluator]<p>This *really* shouldn&#x27;t have the title it does, very misleading.<p>Author, please feel free to correct me, I&#x27;m sorry for not taking the time to find a gentler way to communicate this. I hope you kick ass. You did put possibly in a parenthetical, but its carrying the weight of the world here, people just see LLM 2x faster. That&#x27;s why everyone is spinning off into grand speculation land, which I also see you valiantly commenting to dissuade</div><br/><div id="40069903" class="c"><input type="checkbox" id="c-40069903" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40069478">root</a><span>|</span><a href="#40069768">parent</a><span>|</span><a href="#40069986">next</a><span>|</span><label class="collapse" for="c-40069903">[-]</label><label class="expand" for="c-40069903">[3 more]</label></div><br/><div class="children"><div class="content">The whole inference is slow, but it&#x27;s matrix multiplications that count. They work reliably on all the Macbooks that I tested - at 50% effort it&#x27;s the same speed as the state of the art matrix multiplications, at 25% they are twice as fast.<p>The apple&#x27;s MPS matrix multiplications from Apple are comparable in speed to the speed of Llama.cpp and the other models. When I was doing tests, I was comparing the Llama.cpp benchmarks ( <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;discussions&#x2F;4167">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;discussions&#x2F;4167</a> ) to Apple&#x27;s MPS - they match very closely. And then I was comparing Apple&#x27;s MPS to my results.<p>Even if the end-results would show that the models somehow break (which they might on Q8), there is no other implemented method right now that would give you such speedups with matrixes of 25% sparsity. The usual methods break even with full matrix multiplications around 15% mark, and show speed improvements under 10% (as far as I know, but I&#x27;m new to the field, so I wait to be corrected).<p>As for the other metrics - I hope to get help from the community to get the implementation done properly. So far it&#x27;s been 3 months of work 12 hours a day - even during Easter - to get this version going. It is as far as I can push it without the community support, which I&#x27;m happy I received over the last hour.<p>Also, I&#x27;m not sure what you&#x27;d expect really. A full production ready system on the day one? From a solo developer? Seriously? :)<p>Let&#x27;s get the flame war going! :D</div><br/><div id="40069980" class="c"><input type="checkbox" id="c-40069980" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40069478">root</a><span>|</span><a href="#40069903">parent</a><span>|</span><a href="#40069986">next</a><span>|</span><label class="collapse" for="c-40069980">[-]</label><label class="expand" for="c-40069980">[2 more]</label></div><br/><div class="children"><div class="content">Nah it&#x27;s good work, you&#x27;ll be able to flame me in a couple weeks...months?..too when I ship my yawn-worthy yet another llama.cpp &#x2F; OpenAI wrapper. :p<p>I&#x27;d love this knob, particularly in llama.cpp, inference is a bit too slow on Android, 6 tkn&#x2F;s for 3B. just can&#x27;t stand it when people don&#x27;t actually read anything but the title, and go crazy overboard, like, how are we in a thread where people are like &quot;oh this confirm local models will definitely win like I heard on a podcast&quot; and &quot;big bad OpenAI will steal this&quot;.</div><br/><div id="40070239" class="c"><input type="checkbox" id="c-40070239" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40069478">root</a><span>|</span><a href="#40069980">parent</a><span>|</span><a href="#40069986">next</a><span>|</span><label class="collapse" for="c-40070239">[-]</label><label class="expand" for="c-40070239">[1 more]</label></div><br/><div class="children"><div class="content">Hahah thanks, although I was hoping for a flame to get adrenaline flowing to push through the night :D<p>I also hope there will be an extra knob - or more like knobs, because effort can be regulated smoothly layer by layer, token by token, matrix by matrix. Think more like an equalizer, not a volume control :)<p>The biggest question right now is how (if) it will perform with Q8 and with smaller models. The risk is that the quality dropoff will show up closer to 40-60% at Q8, negating the performance gains.</div><br/></div></div></div></div></div></div><div id="40069986" class="c"><input type="checkbox" id="c-40069986" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#40069478">root</a><span>|</span><a href="#40069768">parent</a><span>|</span><a href="#40069903">prev</a><span>|</span><a href="#40071787">next</a><span>|</span><label class="collapse" for="c-40069986">[-]</label><label class="expand" for="c-40069986">[4 more]</label></div><br/><div class="children"><div class="content">I think you are commenting on the &quot;stealing&quot; reply and not my original comment. And, I think you are making my point stronger.<p>OpenAI could easily (and will) put out a blog post or tweet saying &quot;next models do inference 2.5x faster!&quot; Koliko did that, or maybe he didn&#x27;t and someone else put words in his mouth. I don&#x27;t really care: I can validate and test your comments here (and they are great!) and I can try his experiments myself.<p>I cannot do that against &quot;GPT-5-2.5x faster (c) 2024&quot;-42B (because it isn&#x27;t released yet publicly). Putting a paper and some vague ideas on Arvix isn&#x27;t really doing much these days except adding to the confusion. Truly open work like koliko is doing is really exciting and feels like it can only be done against truly open models like Mistral.<p>Oh wait, Mistral isn&#x27;t fully open either (ducks...).</div><br/><div id="40070526" class="c"><input type="checkbox" id="c-40070526" checked=""/><div class="controls bullet"><span class="by">observationist</span><span>|</span><a href="#40069478">root</a><span>|</span><a href="#40069986">parent</a><span>|</span><a href="#40071787">next</a><span>|</span><label class="collapse" for="c-40070526">[-]</label><label class="expand" for="c-40070526">[3 more]</label></div><br/><div class="children"><div class="content">There used to be a class of software called freeware - you could download and use it without restriction, you just couldn&#x27;t resell it, or have the source to modify it. Llama and similar models are like freeware - an inscrutable binary blob crafted to work with other software, except instead of a VM or native OS environment, you have llama.cpp or similar software that runs the AI model.<p>Mistral is open source, in that you can do anything the Apache license allows you to do, even package it into your own product and resell or modify it. We&#x27;re missing the dataset details, the source to the software that produces the model, similar to not having access to an operating system and special compiler software. That&#x27;s not a huge deal, because people don&#x27;t have the resources to make use of those large datasets or the Mistral training software, which is likely highly tailored to their own training and development pipeline, and wouldn&#x27;t do much good for anyone without at least a pod of A100&#x27;s of their own.<p>Weights available and other terms are being thrown around, and Meta and the like are calling their stuff &quot;open&quot; but that use of the term bears little resemblance to the use of the word by the open source community.<p>The public Mistral models have open source licenses. The model can be used like open source software. The terms are permissive and free, requiring only attribution. Meta&#x27;s license scheme is novel and not open, with arbitrary lawyerese and absolutely, 100% will bite someone in the ass when the threshold between &quot;annoying to sue&quot; and &quot;profitable to sue&quot; gets exceeded by someone using Llama in a way that&#x27;s technically incorrect. Right now, Meta wants the goodwill more than they want a couple million dollars chasing a couple dozen startups.<p>If the model doesn&#x27;t have an open source license, it&#x27;s not open. It might be freeware. Llama is freeware. You can, technically, do whatever you want to it, but try to not attract too much notice or be too successful with it.<p>Mistral, by using Apache licensing, couldn&#x27;t go after you even if they wanted to, unless you do something deliberately stupid.</div><br/><div id="40071798" class="c"><input type="checkbox" id="c-40071798" checked=""/><div class="controls bullet"><span class="by">oceanplexian</span><span>|</span><a href="#40069478">root</a><span>|</span><a href="#40070526">parent</a><span>|</span><a href="#40071787">next</a><span>|</span><label class="collapse" for="c-40071798">[-]</label><label class="expand" for="c-40071798">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If the model doesn&#x27;t have an open source license, it&#x27;s not open.<p>Actually, OSS comes with tons of strings attached that make the term open dubious. And there are many ways they could come after you legally. Apache, GPL, etc all have terms and conditions, you have to contribute back X Y and Z, agree to our manifesto, and so on.<p>The only truly free license is MIT. Go build a billion dollar business, change it however you want with no strings attached and you can express the license terms in a single paragraph.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40067727" class="c"><input type="checkbox" id="c-40067727" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40069478">prev</a><span>|</span><a href="#40068609">next</a><span>|</span><label class="collapse" for="c-40067727">[-]</label><label class="expand" for="c-40067727">[3 more]</label></div><br/><div class="children"><div class="content">Here to answer questions!<p>A friend of mine has published the original link on HN before me, so I hope a double post won&#x27;t be an issue :)</div><br/><div id="40069929" class="c"><input type="checkbox" id="c-40069929" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#40067727">parent</a><span>|</span><a href="#40068609">next</a><span>|</span><label class="collapse" for="c-40069929">[-]</label><label class="expand" for="c-40069929">[2 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve merged them. (other one was <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40067489">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40067489</a>)</div><br/><div id="40073844" class="c"><input type="checkbox" id="c-40073844" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40067727">root</a><span>|</span><a href="#40069929">parent</a><span>|</span><a href="#40068609">next</a><span>|</span><label class="collapse" for="c-40073844">[-]</label><label class="expand" for="c-40073844">[1 more]</label></div><br/><div class="children"><div class="content">Thx! Although I&#x27;m happy you took your time and now I can brag until the end of time that I got 2 out of 5 top slots for a moment :D</div><br/></div></div></div></div></div></div><div id="40068609" class="c"><input type="checkbox" id="c-40068609" checked=""/><div class="controls bullet"><span class="by">queuebert</span><span>|</span><a href="#40067727">prev</a><span>|</span><a href="#40068879">next</a><span>|</span><label class="collapse" for="c-40068609">[-]</label><label class="expand" for="c-40068609">[5 more]</label></div><br/><div class="children"><div class="content">Please, please, please call the final product Halfwit.<p>Seriously though, this is a very interesting biologically inspired idea, since not all neuronal pathways fire all the time.<p>It seems to follow that, if you can predict which weights you won&#x27;t need, then you should be able to compress the model architecture permanently, at least for certain use cases.</div><br/><div id="40068885" class="c"><input type="checkbox" id="c-40068885" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#40068609">parent</a><span>|</span><a href="#40069901">next</a><span>|</span><label class="collapse" for="c-40068885">[-]</label><label class="expand" for="c-40068885">[1 more]</label></div><br/><div class="children"><div class="content">Haha halfwit! I’m waiting for such a fork.<p>As for predicting the weights - not necessarily so. It seems most weights are being used, just not all the time. Kind of like that saying that humans are using just 5% of their brain - perhaps they are, but it’s various parts of the 5%.<p>Interestingly, Effort works just as well on MoE, if not better. I did most of the development on Mixtral and I think it go even to 15-20% effort before losing quality, but there is some sort of a bug right now that prevents the inference on Mixtral.<p>It’s on a todo to fix, but I didn’t want to delay the release because of it.</div><br/></div></div><div id="40069901" class="c"><input type="checkbox" id="c-40069901" checked=""/><div class="controls bullet"><span class="by">LorenDB</span><span>|</span><a href="#40068609">parent</a><span>|</span><a href="#40068885">prev</a><span>|</span><a href="#40069895">next</a><span>|</span><label class="collapse" for="c-40069901">[-]</label><label class="expand" for="c-40069901">[1 more]</label></div><br/><div class="children"><div class="content">Effortless would be another great name (since you are literally reducing effort to get speed). OK, maybe not &quot;great&quot;, but &quot;an option if you&#x27;re going for puns&quot;.</div><br/></div></div><div id="40069895" class="c"><input type="checkbox" id="c-40069895" checked=""/><div class="controls bullet"><span class="by">HPsquared</span><span>|</span><a href="#40068609">parent</a><span>|</span><a href="#40069901">prev</a><span>|</span><a href="#40069935">next</a><span>|</span><label class="collapse" for="c-40069895">[-]</label><label class="expand" for="c-40069895">[1 more]</label></div><br/><div class="children"><div class="content">Halfweight. Half the weights, half the wait, half the wit.</div><br/></div></div></div></div></div></div></div></div></div></body></html>