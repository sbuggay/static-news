<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1716368468768" as="style"/><link rel="stylesheet" href="styles.css?v=1716368468768"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://win-vector.com/2024/05/21/i-want-flexible-queries-not-rag/">I want flexible queries, not RAG</a> <span class="domain">(<a href="https://win-vector.com">win-vector.com</a>)</span></div><div class="subtext"><span>jmount</span> | <span>97 comments</span></div><br/><div><div id="40437878" class="c"><input type="checkbox" id="c-40437878" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#40438130">next</a><span>|</span><label class="collapse" for="c-40437878">[-]</label><label class="expand" for="c-40437878">[3 more]</label></div><br/><div class="children"><div class="content">The retrieval part of RAG is usually vector search; but it doesn&#x27;t have to be. Or at least not exclusively.<p>I&#x27;ve worked with various search backends for about 20 years. People treat vector search like magic pixie dust but the reality is that it&#x27;s not that great unless you heavily tune your models to your use cases. A well tuned manually crafted query goes a long way.<p>Pretty much any system I&#x27;ve built over the last few years, the best way to think about search is about building a search context that includes anything relevant to answering the user&#x27;s question. The user&#x27;s direct input is only a small part of that. In the case of mobile systems, the user entered query is actually typically a very minor part of it. People type two or three letters and then expect magic to happen. Vector search is completely useless in situations like that. Why does search on mobile work anyway? Because of everything else we know to create a query (user location, time zone, locale, past searches, preferences, etc.)<p>RAG isn&#x27;t any different. It&#x27;s just search where the search results are post processed by an LLM with whatever the user typed. The better the query and retrieval, the better the result. The LLM can&#x27;t rescue a poorly tuned search. But it can dig through a massive result of search results and extract key points.</div><br/><div id="40438102" class="c"><input type="checkbox" id="c-40438102" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#40437878">parent</a><span>|</span><a href="#40438130">next</a><span>|</span><label class="collapse" for="c-40438102">[-]</label><label class="expand" for="c-40438102">[2 more]</label></div><br/><div class="children"><div class="content">&gt;But it can dig through a massive result of search results and extract key points.<p>And there is value in that.</div><br/><div id="40438171" class="c"><input type="checkbox" id="c-40438171" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#40437878">root</a><span>|</span><a href="#40438102">parent</a><span>|</span><a href="#40438130">next</a><span>|</span><label class="collapse" for="c-40438171">[-]</label><label class="expand" for="c-40438171">[1 more]</label></div><br/><div class="children"><div class="content">Reducing the signal to noise ratio improves the results though. This is a classical search relevance problem.</div><br/></div></div></div></div></div></div><div id="40438130" class="c"><input type="checkbox" id="c-40438130" checked=""/><div class="controls bullet"><span class="by">valstu</span><span>|</span><a href="#40437878">prev</a><span>|</span><a href="#40434550">next</a><span>|</span><label class="collapse" for="c-40438130">[-]</label><label class="expand" for="c-40438130">[3 more]</label></div><br/><div class="children"><div class="content">We use the term &quot;pre-googling&quot; for this sort of &quot;information retrieval&quot;. You might have some concept in your head and you want to know the exact term for it, once you get the term you&#x27;re looking for from LLM you&#x27;ll move to Google and search the &quot;facts&quot;.<p>This might be a weird example for native english speakers but recently I just couldn&#x27;t remember the term for graph where you&#x27;re allowed to move in one direction and cannot do loops. LLM gave me the answer (directed acyclic graph or DAG)right away. Once I got the term I was looking for I moved on to Google search.<p>Same &quot;pre-googling&quot; works if you don&#x27;t know if some concept exits.</div><br/><div id="40438483" class="c"><input type="checkbox" id="c-40438483" checked=""/><div class="controls bullet"><span class="by">cj</span><span>|</span><a href="#40438130">parent</a><span>|</span><a href="#40434550">next</a><span>|</span><label class="collapse" for="c-40438483">[-]</label><label class="expand" for="c-40438483">[2 more]</label></div><br/><div class="children"><div class="content">&gt; graph where you&#x27;re allowed to move in one direction and cannot do loop<p>To be fair, you didn&#x27;t need LLM for this. Googling that, the answer (DAG) is in the title of the first Google result.<p>(Not to invalidate your point, but the example has to be more obscure than that for this strategy to be useful)</div><br/><div id="40438688" class="c"><input type="checkbox" id="c-40438688" checked=""/><div class="controls bullet"><span class="by">bigfudge</span><span>|</span><a href="#40438130">root</a><span>|</span><a href="#40438483">parent</a><span>|</span><a href="#40434550">next</a><span>|</span><label class="collapse" for="c-40438688">[-]</label><label class="expand" for="c-40438688">[1 more]</label></div><br/><div class="children"><div class="content">I recently started watching fallout and it reminded me of a book I read about a future religious order which was piecing together pre-bomb scientific knowledge. It immediately pointed me to the Canticles of Leibovitz (which is great btw). Google results will do the same, but llm I’d much faster and more direct. 
I find it great for stuff like this - where you know there is an answer and will recognise it as soon as you see it. I genuinely think it can become an extension of my long-term memory, but I’m slightly nervous about the effect it will have on my actual non-memory if I just don’t need to remember stuff like this anymore!</div><br/></div></div></div></div></div></div><div id="40434550" class="c"><input type="checkbox" id="c-40434550" checked=""/><div class="controls bullet"><span class="by">bambax</span><span>|</span><a href="#40438130">prev</a><span>|</span><a href="#40431321">next</a><span>|</span><label class="collapse" for="c-40434550">[-]</label><label class="expand" for="c-40434550">[30 more]</label></div><br/><div class="children"><div class="content">&gt; <i>There is a lot of excitement around retrieval augmented generation or “RAG.” Roughly the idea is: some of the deficiencies in current generative AI or large language models (LLMs) can be papered over by augmenting their hallucinations with links, references, and extracts from definitive source documents. I.e.: knocking the LLM back into the lane.</i><p>This seems like a misunderstanding of what RAG is. RAG is not used to try to anchor to reality a general LLM by somehow making it come up with sources and links. RAG is a technology to augment search engines with vector search and, yes, a natural language interface. This concerns, typically, &quot;small&#x27; search engines indexing a specific corpus. It lets them retrieve documents or document fragments that do not contain the terms in the search query, but that are conceptually similar (according to the encoder used).<p>RAG isn&#x27;t a cure for ChatGPT&#x27;s hallucinations, at all. It&#x27;s a tool to improve and go past inverted indexes.</div><br/><div id="40434598" class="c"><input type="checkbox" id="c-40434598" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#40434550">parent</a><span>|</span><a href="#40436649">next</a><span>|</span><label class="collapse" for="c-40434598">[-]</label><label class="expand" for="c-40434598">[13 more]</label></div><br/><div class="children"><div class="content">This feels like a &quot;No true Scotsman&quot; answer. You say<p>&gt; RAG is not used to try to anchor to reality a general LLM by somehow making it come up with sources and links.<p>but that definitely is <i>one particular</i> use of RAG, i.e. to limit some potential hallucinations by grounding it in data provided in the prompt.</div><br/><div id="40437553" class="c"><input type="checkbox" id="c-40437553" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40434598">parent</a><span>|</span><a href="#40434674">next</a><span>|</span><label class="collapse" for="c-40437553">[-]</label><label class="expand" for="c-40437553">[2 more]</label></div><br/><div class="children"><div class="content">RAG really is not the right tool for that. It does not event prevent hallucinations. It is useful because it can retrieve information from documents the model never saw during its training phase and does not require a costly re-training. It is not a fine tuning either, and it cannot fundamentally change the model’s behaviour.</div><br/><div id="40438703" class="c"><input type="checkbox" id="c-40438703" checked=""/><div class="controls bullet"><span class="by">bigfudge</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40437553">parent</a><span>|</span><a href="#40434674">next</a><span>|</span><label class="collapse" for="c-40438703">[-]</label><label class="expand" for="c-40438703">[1 more]</label></div><br/><div class="children"><div class="content">In my experience, it really does reduce the risk of hallucination, especially if paired with a prompt which explicitly instruct the model to use only facts from the context window. 
Another strategy is to provide unique identifiers for the rag chunks dropped into the context and ask the model to cross reference in its response. You can then check that the response is exciting evidence from the context with Simple pattern match.</div><br/></div></div></div></div><div id="40434674" class="c"><input type="checkbox" id="c-40434674" checked=""/><div class="controls bullet"><span class="by">bambax</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40434598">parent</a><span>|</span><a href="#40437553">prev</a><span>|</span><a href="#40437656">next</a><span>|</span><label class="collapse" for="c-40434674">[-]</label><label class="expand" for="c-40434674">[9 more]</label></div><br/><div class="children"><div class="content">Not really. RAG works very differently from a general (or generalist?) LLM.<p>RAG is vector search first. It encodes the query, finds nearest vectors in the vector database, retrieves the fragments attached to those vectors, and then sends those vectors to the LLM for it to summarize them.<p>A general LLM like Gemini or Claude or ChatGPT first produces an answer to a question, based on its training. This doesn&#x27;t involve searching any external source at that point. Then after that answer is produced, the LLM can try to find sources that match what it has come up with.</div><br/><div id="40436119" class="c"><input type="checkbox" id="c-40436119" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40434674">parent</a><span>|</span><a href="#40434755">next</a><span>|</span><label class="collapse" for="c-40436119">[-]</label><label class="expand" for="c-40436119">[3 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t have to use vector search to implement RAG. You can use other search mechanisms instead or as well - whatever it takes to populate the context with the material most likely to help answer the user&#x27;s question.<p>Two common alternatives to vector search:<p>1. Ask the LLM to identify key terms in the user&#x27;s question and use those terms with a regular full-text search engine<p>2. If the content you are answering questions about is short enough - an employee handbook for example - just jam the whole thing in the context. Claude 3 supports 200,000 tokens and Gemini Pro 1.5 supports a million so this can actually be pretty effective.</div><br/><div id="40438182" class="c"><input type="checkbox" id="c-40438182" checked=""/><div class="controls bullet"><span class="by">bambax</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40436119">parent</a><span>|</span><a href="#40434755">next</a><span>|</span><label class="collapse" for="c-40438182">[-]</label><label class="expand" for="c-40438182">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right and I was imprecise. 2. is how RAG is implemented in most cases. (I wouldn&#x27;t call 1. &#x27;RAG&#x27; exactly, though?)</div><br/><div id="40438714" class="c"><input type="checkbox" id="c-40438714" checked=""/><div class="controls bullet"><span class="by">bigfudge</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40438182">parent</a><span>|</span><a href="#40434755">next</a><span>|</span><label class="collapse" for="c-40438714">[-]</label><label class="expand" for="c-40438714">[1 more]</label></div><br/><div class="children"><div class="content">HYDE is a related technique. Ask the model to generate a response with no context, then use this for semantic search agains actual data and respond by summarising these documents.</div><br/></div></div></div></div></div></div><div id="40434755" class="c"><input type="checkbox" id="c-40434755" checked=""/><div class="controls bullet"><span class="by">outofpaper</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40434674">parent</a><span>|</span><a href="#40436119">prev</a><span>|</span><a href="#40436728">next</a><span>|</span><label class="collapse" for="c-40434755">[-]</label><label class="expand" for="c-40434755">[2 more]</label></div><br/><div class="children"><div class="content">This is a generalization.  These proprietary systems do different things at different times. With GPT4o you can see little icons when a RAG is in use or when code and tests are being used.<p>People, we have to stop talking about what we know as though it&#x27;s all there is. Don&#x27;t confuse our knowledge for understanding.  Understanding only comes from repeadly trying to prove our understandings wrong and learning how things truly are.</div><br/><div id="40436659" class="c"><input type="checkbox" id="c-40436659" checked=""/><div class="controls bullet"><span class="by">acka</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40434755">parent</a><span>|</span><a href="#40436728">next</a><span>|</span><label class="collapse" for="c-40436659">[-]</label><label class="expand" for="c-40436659">[1 more]</label></div><br/><div class="children"><div class="content">This. There is no point in arguing amongst ourselves when we&#x27;re all wearing blindfolds and it is the zookeeper who decides what parts of the proprietary elephant we&#x27;re allowed to touch.</div><br/></div></div></div></div><div id="40436728" class="c"><input type="checkbox" id="c-40436728" checked=""/><div class="controls bullet"><span class="by">ozr</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40434674">parent</a><span>|</span><a href="#40434755">prev</a><span>|</span><a href="#40437656">next</a><span>|</span><label class="collapse" for="c-40436728">[-]</label><label class="expand" for="c-40436728">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not what RAG is.  RAG is the process of adding relevant information to a prompt in an LLM.  It&#x27;s a form of in-context learning.</div><br/><div id="40437001" class="c"><input type="checkbox" id="c-40437001" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40436728">parent</a><span>|</span><a href="#40437566">next</a><span>|</span><label class="collapse" for="c-40437001">[-]</label><label class="expand" for="c-40437001">[1 more]</label></div><br/><div class="children"><div class="content">It’s right there in the name - first you Retrieve relevant information (often a vector lookup) then you use it to Augment the prompt, then you Generate an answer.<p>It’s bloody useful if you can’t cram your entire proprietary code base into a prompt.</div><br/></div></div><div id="40437566" class="c"><input type="checkbox" id="c-40437566" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40436728">parent</a><span>|</span><a href="#40437001">prev</a><span>|</span><a href="#40437656">next</a><span>|</span><label class="collapse" for="c-40437566">[-]</label><label class="expand" for="c-40437566">[1 more]</label></div><br/><div class="children"><div class="content">What makes it useful is precisely that it is not learning.</div><br/></div></div></div></div></div></div><div id="40437656" class="c"><input type="checkbox" id="c-40437656" checked=""/><div class="controls bullet"><span class="by">notjoemama</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40434598">parent</a><span>|</span><a href="#40434674">prev</a><span>|</span><a href="#40436649">next</a><span>|</span><label class="collapse" for="c-40437656">[-]</label><label class="expand" for="c-40437656">[1 more]</label></div><br/><div class="children"><div class="content">This feels like a quantification fallacy. You say<p>&gt; but that definitely is one particular use of RAG<p>One use of RAG doesn&#x27;t imply that all uses of RAG are for grounding LLM hallucinations.<p>Additionally, I disdain when fallacies come up in conversation when it doesn&#x27;t appear someone is making an argument in bad faith. We all use logical fallacies by accident from time to time. A good use of calling them out is when they are used in poor taste. I find that more on Reddit than Hacker News.<p>I have now also engaged in the Tu Quoque fallacy in this reply. See how annoying they are?</div><br/></div></div></div></div><div id="40436649" class="c"><input type="checkbox" id="c-40436649" checked=""/><div class="controls bullet"><span class="by">civilized</span><span>|</span><a href="#40434550">parent</a><span>|</span><a href="#40434598">prev</a><span>|</span><a href="#40434703">next</a><span>|</span><label class="collapse" for="c-40436649">[-]</label><label class="expand" for="c-40436649">[2 more]</label></div><br/><div class="children"><div class="content">&gt; RAG is a technology to augment search engines with vector search<p>Based on the words in the acronym, that seems exactly backwards. The words suggest that it is a <i>generation</i> technology which is merely <i>augmented</i> by retrieval (search), not a retrieval technology that is augmented by a generative technology.</div><br/><div id="40436751" class="c"><input type="checkbox" id="c-40436751" checked=""/><div class="controls bullet"><span class="by">ozr</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40436649">parent</a><span>|</span><a href="#40434703">next</a><span>|</span><label class="collapse" for="c-40436751">[-]</label><label class="expand" for="c-40436751">[1 more]</label></div><br/><div class="children"><div class="content">The parent&#x27;s description isn&#x27;t quite correct.  It&#x27;s kinda sorta describing the implementation; RAG is often implemented via embeddings.  In practice, you generally get better results with a mix of vector and, e.g., TF-IDF.<p>An example of RAG could be: you have a great LLM that was trained at the end of 2023.  You want to ask it about something that happened in 2024.  You&#x27;re out of luck.<p>If you were using RAG, then that LLM would still be useful.  You could ask it<p>&gt; &quot;When does the tiktok ban take effect?&quot;<p>Your question would be converted to an embedding, and then compared against a database of other embeddings, generated from a corpus of up-to-date information and useful resources (wikipedia, news, etc).<p>Hopefully it finds a detailed article on the tiktok ban.  The input to the LLM could then be something like:<p>&gt; CONTEXT: &lt;the text of the article&gt;<p>&gt; USER: When does the tiktok ban take effect?<p>The data retrieved by the search process allows for relevant in-context learning.<p>You have <i>augmented</i> the <i>generation</i> of an LLM by <i>retrieving</i> a relevant document.</div><br/></div></div></div></div><div id="40434703" class="c"><input type="checkbox" id="c-40434703" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#40434550">parent</a><span>|</span><a href="#40436649">prev</a><span>|</span><a href="#40436105">next</a><span>|</span><label class="collapse" for="c-40434703">[-]</label><label class="expand" for="c-40434703">[6 more]</label></div><br/><div class="children"><div class="content">By far the best way I&#x27;ve found to reduce hallucinations is to explicitly allow the model to say some version of &quot;I don&#x27;t know&quot; in the prompt.</div><br/><div id="40437915" class="c"><input type="checkbox" id="c-40437915" checked=""/><div class="controls bullet"><span class="by">pfannkuchen</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40434703">parent</a><span>|</span><a href="#40436171">next</a><span>|</span><label class="collapse" for="c-40437915">[-]</label><label class="expand" for="c-40437915">[3 more]</label></div><br/><div class="children"><div class="content">This technique also helps to prevent hallucinations in human children.</div><br/><div id="40438466" class="c"><input type="checkbox" id="c-40438466" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40437915">parent</a><span>|</span><a href="#40436171">next</a><span>|</span><label class="collapse" for="c-40438466">[-]</label><label class="expand" for="c-40438466">[2 more]</label></div><br/><div class="children"><div class="content">It however has been shown not to help with politicians.</div><br/><div id="40438733" class="c"><input type="checkbox" id="c-40438733" checked=""/><div class="controls bullet"><span class="by">bigfudge</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40438466">parent</a><span>|</span><a href="#40436171">next</a><span>|</span><label class="collapse" for="c-40438733">[-]</label><label class="expand" for="c-40438733">[1 more]</label></div><br/><div class="children"><div class="content">How? Do you just mean a simple way that they could say I don’t know but tend not to?</div><br/></div></div></div></div></div></div><div id="40436171" class="c"><input type="checkbox" id="c-40436171" checked=""/><div class="controls bullet"><span class="by">knifie_spoonie</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40434703">parent</a><span>|</span><a href="#40437915">prev</a><span>|</span><a href="#40436105">next</a><span>|</span><label class="collapse" for="c-40436171">[-]</label><label class="expand" for="c-40436171">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s really interesting. Can you give any examples of how you do that?</div><br/><div id="40436580" class="c"><input type="checkbox" id="c-40436580" checked=""/><div class="controls bullet"><span class="by">batch12</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40436171">parent</a><span>|</span><a href="#40436105">next</a><span>|</span><label class="collapse" for="c-40436580">[-]</label><label class="expand" for="c-40436580">[1 more]</label></div><br/><div class="children"><div class="content">[...] if you don&#x27;t know the answer, say &#x27;I don&#x27;t know&#x27; [...]</div><br/></div></div></div></div></div></div><div id="40436105" class="c"><input type="checkbox" id="c-40436105" checked=""/><div class="controls bullet"><span class="by">whakim</span><span>|</span><a href="#40434550">parent</a><span>|</span><a href="#40434703">prev</a><span>|</span><a href="#40435957">next</a><span>|</span><label class="collapse" for="c-40436105">[-]</label><label class="expand" for="c-40436105">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no particular requirement for a RAG application to use vector search or embeddings, and there&#x27;s no requirement that semantic similarity is in play (i.e. &quot;retriev[ing] documents...that do not contain the terms in the search query&quot;). Fundamentally RAG is just doing some retrieval, and then doing some generation. While the traditional implementation definitely involves vector search and LLMs,  there are plenty of other approaches; ultimately at anything beyond toy scale it sort of begins to converge with the long history of traditional search problems rather than being its own distinct thing.</div><br/></div></div><div id="40435957" class="c"><input type="checkbox" id="c-40435957" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#40434550">parent</a><span>|</span><a href="#40436105">prev</a><span>|</span><a href="#40436825">next</a><span>|</span><label class="collapse" for="c-40435957">[-]</label><label class="expand" for="c-40435957">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It lets them retrieve documents or document fragments that do not contain the terms in the search query, but that are conceptually similar (according to the encoder used).<p>I played with Latent Semantic Indexing[1] as a teen back in early 2000s, which also does kinda that. I haven&#x27;t read much on RAG, I&#x27;m assuming it&#x27;s some next level stuff, but are there any relations or similarities?<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Latent_semantic_analysis#Latent_semantic_indexing" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Latent_semantic_analysis#Laten...</a></div><br/><div id="40437619" class="c"><input type="checkbox" id="c-40437619" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40435957">parent</a><span>|</span><a href="#40436825">next</a><span>|</span><label class="collapse" for="c-40437619">[-]</label><label class="expand" for="c-40437619">[1 more]</label></div><br/><div class="children"><div class="content">It is similar to the retrieval stage, yes. The concept of RAG is to then feed the retrieved fragments to a model so it can generate an answer that takes them into account.</div><br/></div></div></div></div><div id="40436825" class="c"><input type="checkbox" id="c-40436825" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#40434550">parent</a><span>|</span><a href="#40435957">prev</a><span>|</span><a href="#40434665">next</a><span>|</span><label class="collapse" for="c-40436825">[-]</label><label class="expand" for="c-40436825">[2 more]</label></div><br/><div class="children"><div class="content">RAG, or Retrieval Augmented Generation, has been a buzzword in the AI community for some time now. While the term has gained significant traction, its interpretation varies widely among practitioners. Some argue that it should be &quot;Reference Augmented Generation,&quot; while others insist on &quot;Retrieval Augmented Generation.&quot; However, the real question is, does the terminology really matter, or should we focus on the underlying concepts and their applications?<p>The idea behind RAG is to enhance AI-powered search by leveraging the vast amount of information available in documents. It&#x27;s a noble goal, but the acronym itself falls short in capturing the essence of what we&#x27;re trying to achieve. It&#x27;s like trying to describe the entire field of computer science with a single term - it&#x27;s just not feasible.<p>But here&#x27;s the thing: AI-powered search is not just a concept anymore; it&#x27;s a reality. I&#x27;ve been working on this problem since 2019, and I can tell you from experience that it works. By integrating OpenAI&#x27;s GPT-2 with Solr, I was able to create a search engine that could understand natural language queries and provide highly relevant results. And this is just the beginning.<p>The potential applications of AI-powered search are vast. From Playwright to FFmpeg, I&#x27;ve been applying LLMs to various services, and the results have been nothing short of impressive. But to truly unlock the potential of this technology, we need to think beyond the confines of a single acronym.<p>That&#x27;s why I propose a new term: RAISE - Retrieval Augmented Intelligent Search Engine. This term captures the essence of what we&#x27;re trying to achieve: a search engine that can understand the intent behind a query, retrieve relevant information from a vast corpus of documents, and provide intelligent, contextual responses.<p>But more importantly, RAISE is not just a term; it&#x27;s a call to action. It&#x27;s a reminder that we need to raise the bar in AI-powered search, to push the boundaries of what&#x27;s possible, and to create tools that can truly revolutionize the way we access and interact with information.<p>So let&#x27;s not get bogged down in terminology debates. Instead, let&#x27;s focus on the real challenge at hand: building intelligent search engines that can understand, retrieve, and respond to our queries in ways that were once thought impossible. And who knows, maybe one day we&#x27;ll look back at this moment and realize that RAISE was just the beginning of something much bigger.</div><br/><div id="40438757" class="c"><input type="checkbox" id="c-40438757" checked=""/><div class="controls bullet"><span class="by">bigfudge</span><span>|</span><a href="#40434550">root</a><span>|</span><a href="#40436825">parent</a><span>|</span><a href="#40434665">next</a><span>|</span><label class="collapse" for="c-40438757">[-]</label><label class="expand" for="c-40438757">[1 more]</label></div><br/><div class="children"><div class="content">RAG isn’t just search though. I can be using retrieval for generating content, but I think RAISE kind of restricts the term to question answering which is not the e only application.</div><br/></div></div></div></div><div id="40434665" class="c"><input type="checkbox" id="c-40434665" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#40434550">parent</a><span>|</span><a href="#40436825">prev</a><span>|</span><a href="#40435151">next</a><span>|</span><label class="collapse" for="c-40434665">[-]</label><label class="expand" for="c-40434665">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It lets them retrieve documents or document fragments that do not contain the terms in the search query, but that are conceptually similar (according to the encoder used).<p>That’s what GPT does. Or rather, someone hearing about RAG for the first time would have trouble distinguishing what you said from their understanding of how GPTs are already trained.</div><br/></div></div><div id="40435151" class="c"><input type="checkbox" id="c-40435151" checked=""/><div class="controls bullet"><span class="by">hirako2000</span><span>|</span><a href="#40434550">parent</a><span>|</span><a href="#40434665">prev</a><span>|</span><a href="#40435387">next</a><span>|</span><label class="collapse" for="c-40435151">[-]</label><label class="expand" for="c-40435151">[1 more]</label></div><br/><div class="children"><div class="content">Plus RAG isn&#x27;t just referenced result augmentation.<p>It isn&#x27;t just adding a vector based db<p>It isn&#x27;t about less hallucination. In fact it doesn&#x27;t even mandate that result should be referenced nor come from a vector db.<p>RAG&#x27;s point is to remove the limit LLMs alone have which is that they are limited to the mind trained data as source of information.<p>A RAG can be queried for information an LLM doesn&#x27;t have any knowledge of. The LLM part of a RAG can be instructed to use all sort of information retrieval, such as making a web search, checking the current stock market value of any particular tickers.<p>The article is nonetheless interesting as it touches on the beauty of LLMs being in their input interface rather than their ability to outputs aggregated content that reads beautifully, usually.<p>RAG is more than what the author says it is. It&#x27;s even what the author says is most wanted.<p>RAG is more comprehensively what the author wants. It can be instructed to provide relevant references, always answer a particular way, and most importantly can get asked to perform (live) search engine queries to find the most up to date information. The example in the article is OK given the recipe is pretty old and has been very likely mined during training.<p>What about:<p>&gt; I missed the games Lakers played from the 1st to 21st of April. Could you give me the list of opponents and scores please.<p>LLMs input interface won&#x27;t help. RAGs can make LLMs answer that.</div><br/></div></div><div id="40435387" class="c"><input type="checkbox" id="c-40435387" checked=""/><div class="controls bullet"><span class="by">internet101010</span><span>|</span><a href="#40434550">parent</a><span>|</span><a href="#40435151">prev</a><span>|</span><a href="#40431321">next</a><span>|</span><label class="collapse" for="c-40435387">[-]</label><label class="expand" for="c-40435387">[1 more]</label></div><br/><div class="children"><div class="content">RAG is for vlookups, not countifs.</div><br/></div></div></div></div><div id="40431321" class="c"><input type="checkbox" id="c-40431321" checked=""/><div class="controls bullet"><span class="by">mpweiher</span><span>|</span><a href="#40434550">prev</a><span>|</span><a href="#40433688">next</a><span>|</span><label class="collapse" for="c-40431321">[-]</label><label class="expand" for="c-40431321">[23 more]</label></div><br/><div class="children"><div class="content">This has been my thinking as well:  the natural language interface is <i>amazing</i> and something we&#x27;ve been wanting for some time.<p>The generation is a showy gimmick.<p>So why aren&#x27;t we separating the useful bit out?   My sneaking suspicion is that we can&#x27;t.  It&#x27;s a package deal in that there are no two parts, it&#x27;s just one big soup of free-associating some text with other text, the stochastic parrot.<p>LLM do not understand.  They generate associated text that looks like an answer to the question.  In order to separate the two parts, we&#x27;d need LLMs that understand.<p>That, apparently, is a lot harder.</div><br/><div id="40435258" class="c"><input type="checkbox" id="c-40435258" checked=""/><div class="controls bullet"><span class="by">singingfish</span><span>|</span><a href="#40431321">parent</a><span>|</span><a href="#40436806">next</a><span>|</span><label class="collapse" for="c-40435258">[-]</label><label class="expand" for="c-40435258">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been working on this as a way of better exposing organisational knowledge recently.  A few things I&#x27;ve observed:<p>* The prompt engineering side is a black art.<p>* Generation is pretty amazing but less useful than it first seems.<p>* The open source and proprietary tooling around managing and interrogating datasets for NLP stuff is way ahead of where it was last time I looked maybe a decade ago<p>* It looks to me that with appropriate thinking about indexing then there&#x27;s a lot of potential here.  For example if I have a question answer set, I can index the question and which answers point to it.  Get the LLM to identify the type of question I&#x27;ve just asked, and then use my corresponding corpus of answers to provide something useful based on the appropriate parts of the answer corpus.<p>That last bit is not the automatic panacea that the flashy and somewhat gimicky emergent properties of the generative side supply, but it seems like it can get good traction on some quite difficult problems quickly, and actually quite well using not too many local computing resources.</div><br/></div></div><div id="40436806" class="c"><input type="checkbox" id="c-40436806" checked=""/><div class="controls bullet"><span class="by">omneity</span><span>|</span><a href="#40431321">parent</a><span>|</span><a href="#40435258">prev</a><span>|</span><a href="#40434323">next</a><span>|</span><label class="collapse" for="c-40436806">[-]</label><label class="expand" for="c-40436806">[1 more]</label></div><br/><div class="children"><div class="content">Natural language search and answer generation _are_ completely separate.<p>Search is often (but not exclusively) performed using cosine similarity over semantic vectors. Such vectors are produced using embedding models, which represent the meaning of the document via an arbitrary length vector called an embedding, and 768 is a common vector size for this.<p>You calculate the embedding for all documents in your database ahead of time (during insertion), and you calculate the embedding for the user&#x27;s query, then search for documents closest to the query using a similarity metric of choice such as cosine.<p>Nothing prevents you from serving documents found this way directly, instead of using them to generate answers. Part of the Google search pipeline involves something like this. Many full-text search products also do this (Algolia is such an example <a href="https:&#x2F;&#x2F;www.algolia.com&#x2F;blog&#x2F;ai&#x2F;what-is-vector-search&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.algolia.com&#x2F;blog&#x2F;ai&#x2F;what-is-vector-search&#x2F;</a>)<p>LLMs and generation are used to synthesize the answer and tie it back into the question using a layer of soft judgment based on the LLMs prior knowledge. This does work out great in some contexts, less so in others, as you pointed out. But these components aren&#x27;t coupled in any way.</div><br/></div></div><div id="40434323" class="c"><input type="checkbox" id="c-40434323" checked=""/><div class="controls bullet"><span class="by">bunderbunder</span><span>|</span><a href="#40431321">parent</a><span>|</span><a href="#40436806">prev</a><span>|</span><a href="#40433702">next</a><span>|</span><label class="collapse" for="c-40434323">[-]</label><label class="expand" for="c-40434323">[1 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t say we <i>can&#x27;t</i>, but it would be much harder. The models were trained and optimized for next word prediction. It is possible to chop the output layers off and replace them with something else. This is often how more open models like BERT are adapted to tasks such as classification and sentiment analysis. But pulling semantically meaningful information out of internal states of the model is tricky because there&#x27;s not necessarily anything about the model architecture and training methods that forces it to develop internal representations that are particularly interpretable or have a straightforward application to some other task.<p>They may not be all that stable, either. I would not just assume that knowing how to interpret the attention heads in the base model of GPT-4 tells you anything about what the corresponding attention heads are doing in GPT-4t or GPT-4o.</div><br/></div></div><div id="40433702" class="c"><input type="checkbox" id="c-40433702" checked=""/><div class="controls bullet"><span class="by">banish-m4</span><span>|</span><a href="#40431321">parent</a><span>|</span><a href="#40434323">prev</a><span>|</span><a href="#40433673">next</a><span>|</span><label class="collapse" for="c-40433702">[-]</label><label class="expand" for="c-40433702">[3 more]</label></div><br/><div class="children"><div class="content">Most presentations of generative AI come across like amnesiac, superficial polymaths. Perhaps beyond negative and positive prompts, they need some memory across queries and across users, a feedback path (hard problem), an ability to discern reputability (very hard problem), and an ability to search and train on new domain-specific information on-the-fly.</div><br/><div id="40434735" class="c"><input type="checkbox" id="c-40434735" checked=""/><div class="controls bullet"><span class="by">bunderbunder</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40433702">parent</a><span>|</span><a href="#40433673">next</a><span>|</span><label class="collapse" for="c-40434735">[-]</label><label class="expand" for="c-40434735">[2 more]</label></div><br/><div class="children"><div class="content">Hard, and also might not be particularly commercially viable. Once you give a model an individual memory, you lose the standardization and consistency of behavior that is important for scaling up commercial applications.<p>I would not want, for example, a version of Copilot that slowly gets better at helping me with the task I&#x27;m working on and then suddenly and unpredictably reverts back to zero just because the k8s pod running the instance that I had been interacting with got recycled. Consistently mediocre behavior would be preferable to the AI equivalent of the pair programming equivalent of speed dating.</div><br/><div id="40437568" class="c"><input type="checkbox" id="c-40437568" checked=""/><div class="controls bullet"><span class="by">banish-m4</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40434735">parent</a><span>|</span><a href="#40433673">next</a><span>|</span><label class="collapse" for="c-40437568">[-]</label><label class="expand" for="c-40437568">[1 more]</label></div><br/><div class="children"><div class="content">My point is that to piece together an almost AGI agent or at least more useful generative AI requires these changes. A killer app isn&#x27;t a singular magic thing but many small advancements. OpenAI still isn&#x27;t even close, but it&#x27;s demoing bits and pieces that are closer.</div><br/></div></div></div></div></div></div><div id="40433673" class="c"><input type="checkbox" id="c-40433673" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#40431321">parent</a><span>|</span><a href="#40433702">prev</a><span>|</span><a href="#40431672">next</a><span>|</span><label class="collapse" for="c-40433673">[-]</label><label class="expand" for="c-40433673">[10 more]</label></div><br/><div class="children"><div class="content">&gt; The generation is a showy gimmick.<p>It really isn&#x27;t? You can tell it to output in a JSON structure (or some other format) of your choice and it will, with high reliability. You control the output.<p>Honestly I wonder if the people who criticize LLM&#x27;s have made a serious attempt to use them for anything</div><br/><div id="40436085" class="c"><input type="checkbox" id="c-40436085" checked=""/><div class="controls bullet"><span class="by">chipotle_coyote</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40433673">parent</a><span>|</span><a href="#40433878">next</a><span>|</span><label class="collapse" for="c-40436085">[-]</label><label class="expand" for="c-40436085">[1 more]</label></div><br/><div class="children"><div class="content">I made a serious attempt to do precisely that, and yes, it output a valid JSON structure highly reliably. The problem was stopping it from just inventing values for parameters that weren&#x27;t actually specified by the user.<p>Consider the possibility that at least some of the criticisms of LLMs are <i>a result of</i> serious attempts to use them.</div><br/></div></div><div id="40433878" class="c"><input type="checkbox" id="c-40433878" checked=""/><div class="controls bullet"><span class="by">dvt</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40433673">parent</a><span>|</span><a href="#40436085">prev</a><span>|</span><a href="#40431672">next</a><span>|</span><label class="collapse" for="c-40433878">[-]</label><label class="expand" for="c-40433878">[8 more]</label></div><br/><div class="children"><div class="content">&gt; You can tell it to output in a JSON structure (or some other format) of your choice and it will, with high reliability.<p>I mean, this is provably false. Have <i>you</i> tried to use LLMs to generate structured JSON output? Not only do all LLMs suck at reliably following a schema, you need to use all kinds of &quot;forcing&quot; to make sure the output is actually JSON anyway. By &quot;forcing&quot; I mean either (1) multi-shot prompting: &quot;no, not like that,&quot; if the output isn&#x27;t valid-ish JSON; or (2) literally stripping out—or rejecting—illegal tokens (which is what llama.cpp does[1][2]). And even with all of that, you still won&#x27;t really have a production-ready pipeline in the general case.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;1300">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;1300</a><p>[2] this is cutely called &quot;constraining&quot; a decoder; what it actually is is correcting a very clear stochastic deficiency in LLMs</div><br/><div id="40434010" class="c"><input type="checkbox" id="c-40434010" checked=""/><div class="controls bullet"><span class="by">btown</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40433878">parent</a><span>|</span><a href="#40437701">next</a><span>|</span><label class="collapse" for="c-40434010">[-]</label><label class="expand" for="c-40434010">[1 more]</label></div><br/><div class="children"><div class="content">Beyond this, an LLM can easily become confused even if outputting JSON with a valid schema. For instance, we&#x27;ve had mixed results trying to get an LLM to report structured discrepancies between two multi-paragraph pieces of text, each of which might be using flowery language that &quot;reminds&quot; the LLM of marketing language in its training set. The LLM often gets as confused as a human would, if the human were quickly skimming the text and forgetting which text they&#x27;re thinking about - or whether they&#x27;re inventing details from memory that are in line with the tone of the language they&#x27;re reading. These are very reasonable mistakes to make, and there are ways to mitigate the difficulties with multiple passes, but I wouldn&#x27;t describe the outputs as highly reliable!</div><br/></div></div><div id="40437701" class="c"><input type="checkbox" id="c-40437701" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40433878">parent</a><span>|</span><a href="#40434010">prev</a><span>|</span><a href="#40436852">next</a><span>|</span><label class="collapse" for="c-40437701">[-]</label><label class="expand" for="c-40437701">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I&#x27;m using them quite extensively with my day to day work for extracting numerical data from unstructured documents. I&#x27;ve been manually verifying the JSON structure and numerical outputs and it&#x27;s highly accurate for the corpus I&#x27;m processing.<p>FWIW I&#x27;m using GPT4o not Llama, I&#x27;ve tried Llama for local tasks and found it pretty lacking in comparison to GPT.</div><br/></div></div><div id="40436852" class="c"><input type="checkbox" id="c-40436852" checked=""/><div class="controls bullet"><span class="by">omneity</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40433878">parent</a><span>|</span><a href="#40437701">prev</a><span>|</span><a href="#40438022">next</a><span>|</span><label class="collapse" for="c-40436852">[-]</label><label class="expand" for="c-40436852">[1 more]</label></div><br/><div class="children"><div class="content">Your comment has an unnecessary and overly negative tone to it that doesn&#x27;t do this tech justice. These approaches are totally valid and can get you great results. An LLM is just a component in a pipeline. I deployed many of these in production without a hiccup.<p>Guidance (the industry term for &quot;constraining&quot; the model output) is only there to ensure the output follows a particular grammar. If you need JSON to fit a particular schema or format, then you can always validate it. In case of validation failure you can always pass the JSON and the validation result back to the LLM for it to correct it.</div><br/></div></div><div id="40438022" class="c"><input type="checkbox" id="c-40438022" checked=""/><div class="controls bullet"><span class="by">Kiro</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40433878">parent</a><span>|</span><a href="#40436852">prev</a><span>|</span><a href="#40434199">next</a><span>|</span><label class="collapse" for="c-40438022">[-]</label><label class="expand" for="c-40438022">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m generating hundreds of JSONs a day with OpenAI and it has no problem following a schema defined in TypeScript.</div><br/></div></div><div id="40434199" class="c"><input type="checkbox" id="c-40434199" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40433878">parent</a><span>|</span><a href="#40438022">prev</a><span>|</span><a href="#40434694">next</a><span>|</span><label class="collapse" for="c-40434199">[-]</label><label class="expand" for="c-40434199">[2 more]</label></div><br/><div class="children"><div class="content">I would have agreed with you six months ago, but the latest models - Claude 3, GPT-4o, maybe Llama 3 as well - are much more proficient at outputting JSON correctly.</div><br/><div id="40436950" class="c"><input type="checkbox" id="c-40436950" checked=""/><div class="controls bullet"><span class="by">pilgrim0</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40434199">parent</a><span>|</span><a href="#40434694">next</a><span>|</span><label class="collapse" for="c-40436950">[-]</label><label class="expand" for="c-40436950">[1 more]</label></div><br/><div class="children"><div class="content">Seems logical that they will always implement specialized pathways for the most critical and demanding user base. At some point they might even do it all by hand and we wouldn’t know &#x2F;s</div><br/></div></div></div></div><div id="40434694" class="c"><input type="checkbox" id="c-40434694" checked=""/><div class="controls bullet"><span class="by">msp26</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40433878">parent</a><span>|</span><a href="#40434199">prev</a><span>|</span><a href="#40431672">next</a><span>|</span><label class="collapse" for="c-40434694">[-]</label><label class="expand" for="c-40434694">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  Have you tried to use LLMs to generate structured JSON output? Not only do all LLMs suck at relaibly following a schema, you need to use all kinds of &quot;forcing&quot; to make sure the output is actually JSON anyway.<p>Yeah it&#x27;s worked about fifty thousand times for me without issues in the past few months for several NLP production pipelines.</div><br/></div></div></div></div></div></div><div id="40431672" class="c"><input type="checkbox" id="c-40431672" checked=""/><div class="controls bullet"><span class="by">dwallin</span><span>|</span><a href="#40431321">parent</a><span>|</span><a href="#40433673">prev</a><span>|</span><a href="#40435669">next</a><span>|</span><label class="collapse" for="c-40431672">[-]</label><label class="expand" for="c-40431672">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s entirely an architecture problem, there&#x27;s a huge training set problem. Almost all the content you would train on assumes outside knowledge&#x2F;facts, so the llm learns to store these in the model in order to be able to maximize it&#x27;s completion ability. However, a lot of this assumed knowledge&#x2F;generalizations are actually unhelpful&#x2F;harmful for these sorts of cases.<p>If you wanted to create an llm that significantly improved on this you would probably need to make sure to massively clean up &#x2F; reorganize your training data so that you always provide sufficient context and the llm is disincentived from baking in &quot;facts&quot;. But I&#x27;m not sure this is tractable to do currently at the scale of data needed.</div><br/><div id="40433940" class="c"><input type="checkbox" id="c-40433940" checked=""/><div class="controls bullet"><span class="by">f33d5173</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40431672">parent</a><span>|</span><a href="#40435669">next</a><span>|</span><label class="collapse" for="c-40433940">[-]</label><label class="expand" for="c-40433940">[4 more]</label></div><br/><div class="children"><div class="content">The LLM miracle comes from the massive amount of text we can use to train it on. Removing that advantage makes LLMs untenable. An idea I&#x27;ve had for a while is to do the opposite: generate nonsense text according to some complex formula, and have the AI learn to predict that. It won&#x27;t possibly be able to encode any facts, because there are no facts. Now show it english, and it will treat it just like any other sort of nonsense text that it&#x27;s gotten good at learning to interpret.</div><br/><div id="40434239" class="c"><input type="checkbox" id="c-40434239" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40433940">parent</a><span>|</span><a href="#40435669">next</a><span>|</span><label class="collapse" for="c-40434239">[-]</label><label class="expand" for="c-40434239">[3 more]</label></div><br/><div class="children"><div class="content">But that idea you describe is <i>exactly</i> what would make the LLM stop working. The &quot;LLM miracle&quot; comes from the fact that all that text is <i>not random</i>[0]. There is a lot of information encoded in what phrases, sentences, paragraphs have been written (and how often), vs. a vastly larger amount of nearly identical texts that were not written. The &quot;complex formula&quot; used is... reality, as perceived and understood by people. LLMs pick up on that.<p>--<p>[0] - Well, most of it anyway; I bet the training set contains some amount of purely random text, for example technical articles discussing RNGs and showcasing their output. Some amount of noise is unavoidable.</div><br/><div id="40434342" class="c"><input type="checkbox" id="c-40434342" checked=""/><div class="controls bullet"><span class="by">f33d5173</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40434239">parent</a><span>|</span><a href="#40435669">next</a><span>|</span><label class="collapse" for="c-40434342">[-]</label><label class="expand" for="c-40434342">[2 more]</label></div><br/><div class="children"><div class="content">The idea would to generate a false &quot;reality&quot; for the LLM to learn about. You would randomly generate a system of rules, use those rules to generate text, and then train the llm to predict the text. The goal would be to get it to stop encoding the reality proper in its weights, and focus on learning to pick up what reality looks like very quickly from text.</div><br/><div id="40434722" class="c"><input type="checkbox" id="c-40434722" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#40431321">root</a><span>|</span><a href="#40434342">parent</a><span>|</span><a href="#40435669">next</a><span>|</span><label class="collapse" for="c-40434722">[-]</label><label class="expand" for="c-40434722">[1 more]</label></div><br/><div class="children"><div class="content">Bonus points for one of the most delightfully creative ideas I’ve heard in some time. I don’t think it will work (the space of &quot;not reality&quot; is superexponentially larger than the space of &quot;this describes reality&quot;) but I’m just happy to be thinking about nonstandard ML again.<p>(I’ve dubbed this sort of thing “nonstandard ML&quot; since, like you, I have a fondness for thinking of unorthodox solutions that seem plausible.)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40435669" class="c"><input type="checkbox" id="c-40435669" checked=""/><div class="controls bullet"><span class="by">thomastjeffery</span><span>|</span><a href="#40431321">parent</a><span>|</span><a href="#40431672">prev</a><span>|</span><a href="#40433688">next</a><span>|</span><label class="collapse" for="c-40435669">[-]</label><label class="expand" for="c-40435669">[1 more]</label></div><br/><div class="children"><div class="content">To understand, we need to take a step back, and completely deconstruct the familiar narrative.<p>The moment we say &quot;an AI&quot;, we anthropomorphize the model. The narrative gets even more derailed when we say &quot;Large <i>Language</i> Model&quot;. No LLM actually contains a grammar or defined words. An LLM can&#x27;t think logically or objectively about subjects the way a person can, either.<p>I propose that we instead call them &quot;Large <i>Text</i> Models&quot;. An LTM is a model made from training a neutral net with <i>text</i>. Sure, the text itself was written with language, but no part of the training process or its result does anything about it.<p>The really cool trick that an LTM does is to construct continuations whose <i>content</i> just happens to be indistinguishable from language. This is accomplished because the intentions of the original human writer (that were encoded into the original text dataset) did consistently follow language rules. The problem is that the original writer did not encode the truthiness or falsiness of future LTM continuations. Truth and lie are written the same, and that ambiguity lives in the LTM forever.</div><br/></div></div></div></div><div id="40433688" class="c"><input type="checkbox" id="c-40433688" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#40431321">prev</a><span>|</span><a href="#40438556">next</a><span>|</span><label class="collapse" for="c-40433688">[-]</label><label class="expand" for="c-40433688">[4 more]</label></div><br/><div class="children"><div class="content">The article would be more convincing if they showed the recipe from the book so we could compare it with the one that ChatGPT output.<p>From a Google search, it looks like he&#x27;s right about the poor accuracy. It gets the basic idea of the ingredients, but is not really accurate. And is initially wrong about the region.<p>But actually, this is what RAG is for. You would typically do a vector search for something similar to the question about &quot;rice baked in an egg mixture&quot;. And assuming it found a match on the real recipe or on a few similar possibilities, feed those into the prompt for the LLM to incorporate.<p>So if you have a well indexed recipe database and large context window to include multiple possible matches, then RAG would probably work perfectly for this case.</div><br/><div id="40434566" class="c"><input type="checkbox" id="c-40434566" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#40433688">parent</a><span>|</span><a href="#40434582">next</a><span>|</span><label class="collapse" for="c-40434566">[-]</label><label class="expand" for="c-40434566">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m also super curious to know what the actual recipe was. I literally just plugged the exact text from the article into ChatGPT 4o:<p>&gt; My mother remembers growing up with a Sicilian dish that was primarily rice baked in an egg mixture. Roughly a “rice frittata.” Do you know some examples of what dish or recipe this could be?<p>And the response I got was<p>&gt; It sounds like your mother might be referring to a dish known as &quot;Frittata di Riso&quot; or &quot;Frittata di Riso al Forno.&quot; This is a traditional Sicilian dish that combines leftover rice with eggs and other ingredients, then bakes it into a savory cake. Here is a basic recipe for Frittata di Riso:<p>And then got a detailed, formatted recipe. I asked follow up questions along the lines of &quot;Where did Frittata di Riso originate&quot; and &quot;Is Frittata di Riso popular in Sicily&quot; and again got detailed, thorough answers. Fair enough that I don&#x27;t know if it&#x27;s hallucinating, but without more info from the author what can I compare it to?</div><br/><div id="40435735" class="c"><input type="checkbox" id="c-40435735" checked=""/><div class="controls bullet"><span class="by">deanputney</span><span>|</span><a href="#40433688">root</a><span>|</span><a href="#40434566">parent</a><span>|</span><a href="#40434582">next</a><span>|</span><label class="collapse" for="c-40435735">[-]</label><label class="expand" for="c-40435735">[1 more]</label></div><br/><div class="children"><div class="content">I happen to have a copy of &quot;Bruculinu, America&quot; at home. Looking for rice recipes at roughly those positions in the book, it&#x27;s either &quot;Tumala d&#x27;Andrea: Rice Bombe&quot; (blue bookmark) or &quot;Arancini al Burru: Rice Balls with Butter&quot;.<p>I&#x27;m going to guess it&#x27;s Arancini al Burru, because the Tumala d&#x27;Andrea is way more involved (and stuffed with pasta)[0]. Here&#x27;s a similar recipe for Arancini al Burru[1].<p>However, Arancini al Burru is fried and Tumala d&#x27;Andrea is baked. So I&#x27;m still speculating, it could be either or something different. It&#x27;s a nice cookbook worth adding to your collection.<p>[0] <a href="https:&#x2F;&#x2F;inasmallkitchen.wordpress.com&#x2F;2012&#x2F;08&#x2F;12&#x2F;tumala-dandrea&#x2F;" rel="nofollow">https:&#x2F;&#x2F;inasmallkitchen.wordpress.com&#x2F;2012&#x2F;08&#x2F;12&#x2F;tumala-dand...</a>
[1] <a href="https:&#x2F;&#x2F;www.196flavors.com&#x2F;arancini-al-burro&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.196flavors.com&#x2F;arancini-al-burro&#x2F;</a></div><br/></div></div></div></div><div id="40434582" class="c"><input type="checkbox" id="c-40434582" checked=""/><div class="controls bullet"><span class="by">cryptonector</span><span>|</span><a href="#40433688">parent</a><span>|</span><a href="#40434566">prev</a><span>|</span><a href="#40438556">next</a><span>|</span><label class="collapse" for="c-40434582">[-]</label><label class="expand" for="c-40434582">[1 more]</label></div><br/><div class="children"><div class="content">I feel cheated.</div><br/></div></div></div></div><div id="40438556" class="c"><input type="checkbox" id="c-40438556" checked=""/><div class="controls bullet"><span class="by">a_c</span><span>|</span><a href="#40433688">prev</a><span>|</span><a href="#40433602">next</a><span>|</span><label class="collapse" for="c-40438556">[-]</label><label class="expand" for="c-40438556">[1 more]</label></div><br/><div class="children"><div class="content">Querying knowledge is not a nail, but the hammer is generation. It is written on the tin, &quot;generation&quot; AI. People want &quot;insight&quot;, &quot;summary&quot;, &quot;workflow automation&quot;, &quot;code completion&quot; from a guided proverbial monkey hammering on the keyboard hoping that our problem will become a nail. It is getting closer though</div><br/></div></div><div id="40433602" class="c"><input type="checkbox" id="c-40433602" checked=""/><div class="controls bullet"><span class="by">advisedwang</span><span>|</span><a href="#40438556">prev</a><span>|</span><a href="#40436699">next</a><span>|</span><label class="collapse" for="c-40433602">[-]</label><label class="expand" for="c-40433602">[5 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t the point of RAGs to make (in this example) actual recipe databases accessible to the LLM? Wouldn&#x27;t it get <i>closer</i> to the articles stated goal of getting the actual recipie?</div><br/><div id="40436166" class="c"><input type="checkbox" id="c-40436166" checked=""/><div class="controls bullet"><span class="by">cjf101</span><span>|</span><a href="#40433602">parent</a><span>|</span><a href="#40435317">next</a><span>|</span><label class="collapse" for="c-40436166">[-]</label><label class="expand" for="c-40436166">[2 more]</label></div><br/><div class="children"><div class="content">Yes, but if you don&#x27;t have the LLM at the end, a good search (against a good corpus with the needed info) would still have given the user what they wanted. Which in this case, is a human vetted piece of relevant information.  The LLM really only would be useful in this case for dressing up the result and that would actually reduce the trust in the result overall. Alternatively a LLM could play a role as part of the Natural language pipeline that drives the search, hidden from the user, and I feel that that&#x27;s a much more interesting use of them.<p>The farther you go with RAGs, in my experience, the more they become an exercise in designing a good search engine, because garbage search results from the RAG stage always lead to garbage output from the LLM.</div><br/><div id="40437680" class="c"><input type="checkbox" id="c-40437680" checked=""/><div class="controls bullet"><span class="by">creshal</span><span>|</span><a href="#40433602">root</a><span>|</span><a href="#40436166">parent</a><span>|</span><a href="#40435317">next</a><span>|</span><label class="collapse" for="c-40437680">[-]</label><label class="expand" for="c-40437680">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The farther you go with RAGs, in my experience, the more they become an exercise in designing a good search engine<p>From what I&#x27;ve seen from internal corporate RAG efforts, that often seems to be the whole point of the exercise:<p>Everyone has always wanted to break up knowledge silos and create a large, properly semantically searchable knowledge base with all intelligence a corporation has.<p>Management doesn&#x27;t understand what benefits that brings and doesn&#x27;t want to break up tribal office politics, but they&#x27;re encouraged to spend money on hypes by investors and golf buddies.<p>So you tell management &quot;hey we need to spend a little bit of time on a semantic knowledge base for RAG AI and btw this needs access to all silos to work&quot;, and make the actual LLM an after thought that the intern gets to play with.</div><br/></div></div></div></div><div id="40435317" class="c"><input type="checkbox" id="c-40435317" checked=""/><div class="controls bullet"><span class="by">brigadier132</span><span>|</span><a href="#40433602">parent</a><span>|</span><a href="#40436166">prev</a><span>|</span><a href="#40434721">next</a><span>|</span><label class="collapse" for="c-40435317">[-]</label><label class="expand" for="c-40435317">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I don&#x27;t think the author fully thought through what they wrote. In essence they are saying they just want semantic search.</div><br/></div></div><div id="40434721" class="c"><input type="checkbox" id="c-40434721" checked=""/><div class="controls bullet"><span class="by">khaki54</span><span>|</span><a href="#40433602">parent</a><span>|</span><a href="#40435317">prev</a><span>|</span><a href="#40436699">next</a><span>|</span><label class="collapse" for="c-40434721">[-]</label><label class="expand" for="c-40434721">[1 more]</label></div><br/><div class="children"><div class="content">Yes.  Normally with RAG you would actually search and try to pre-filter the data for the LLM.</div><br/></div></div></div></div><div id="40436699" class="c"><input type="checkbox" id="c-40436699" checked=""/><div class="controls bullet"><span class="by">lmeyerov</span><span>|</span><a href="#40433602">prev</a><span>|</span><a href="#40434513">next</a><span>|</span><label class="collapse" for="c-40436699">[-]</label><label class="expand" for="c-40436699">[4 more]</label></div><br/><div class="children"><div class="content">To keep control in the hands of the analyst, we&#x27;ve been working on UX&#x27;s over agentic neurosymbolic RAG in louie.ai --<p>Ex: &quot;search for login alerts from the morning, and if none, expand to the full day&quot;<p>That requires generating a one-shot query combining semantic search + symbolic filters, and an LLM-reasoned agentic loop recovering if it turns up not enough such as a poorly formed query around &#x27;login alerts&#x27; and the user&#x27;s trigger around &#x27;if none&#x27;<p>Likewise, unlike Disneyified consumer tools like chatgpt and perplexity that are designed to hide what is happening, we work with analysts who need visibility and control. That means designing search so subqueries and decisions flow back to the user in an understandable way: they need to inspect what is happening and be confident they missed nothing, and edit via natural language or their own queries when they want to proceed<p>Crazy days!</div><br/><div id="40438241" class="c"><input type="checkbox" id="c-40438241" checked=""/><div class="controls bullet"><span class="by">thom</span><span>|</span><a href="#40436699">parent</a><span>|</span><a href="#40436944">next</a><span>|</span><label class="collapse" for="c-40438241">[-]</label><label class="expand" for="c-40438241">[1 more]</label></div><br/><div class="children"><div class="content">Yup, this is the way. Natural language is an excellent search&#x2F;specification front end. But without disambiguation and perfect clarity on how a query was interpreted, you cannot trust a black box for real work.</div><br/></div></div><div id="40436944" class="c"><input type="checkbox" id="c-40436944" checked=""/><div class="controls bullet"><span class="by">civilized</span><span>|</span><a href="#40436699">parent</a><span>|</span><a href="#40438241">prev</a><span>|</span><a href="#40434513">next</a><span>|</span><label class="collapse" for="c-40436944">[-]</label><label class="expand" for="c-40436944">[2 more]</label></div><br/><div class="children"><div class="content">Very good to hear this. As a data analyst, I have tended to dismiss LLMs as irrelevant because of the black-box mentality. In some industries, this even holds back adoption of technology that is now considered mature and boring, like tree ensembles in machine learning.</div><br/><div id="40437859" class="c"><input type="checkbox" id="c-40437859" checked=""/><div class="controls bullet"><span class="by">radarsat1</span><span>|</span><a href="#40436699">root</a><span>|</span><a href="#40436944">parent</a><span>|</span><a href="#40434513">next</a><span>|</span><label class="collapse" for="c-40437859">[-]</label><label class="expand" for="c-40437859">[1 more]</label></div><br/><div class="children"><div class="content">Once tree ensembles get big enough to handle the kinds of problems that LLMs can address, are they really more interpretable?</div><br/></div></div></div></div></div></div><div id="40434513" class="c"><input type="checkbox" id="c-40434513" checked=""/><div class="controls bullet"><span class="by">pseudosavant</span><span>|</span><a href="#40436699">prev</a><span>|</span><a href="#40433838">next</a><span>|</span><label class="collapse" for="c-40434513">[-]</label><label class="expand" for="c-40434513">[5 more]</label></div><br/><div class="children"><div class="content">I think complaints like this show just how amazing AI is getting. This person really expected that ChatGPT would single-shot give them this obscure recipe that took them a ton of effort to find themselves. Current AI can do so much, that people lament that it can&#x27;t do everything. It is incredible to me when people bring up bad AI generated legal fillings... like people actually expect it to already to all of the work of an attorney, without error.</div><br/><div id="40434577" class="c"><input type="checkbox" id="c-40434577" checked=""/><div class="controls bullet"><span class="by">Repulsion9513</span><span>|</span><a href="#40434513">parent</a><span>|</span><a href="#40434586">next</a><span>|</span><label class="collapse" for="c-40434577">[-]</label><label class="expand" for="c-40434577">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not that it can&#x27;t do everything. It&#x27;s that it pretends it can do everything.</div><br/></div></div><div id="40434586" class="c"><input type="checkbox" id="c-40434586" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#40434513">parent</a><span>|</span><a href="#40434577">prev</a><span>|</span><a href="#40433838">next</a><span>|</span><label class="collapse" for="c-40434586">[-]</label><label class="expand" for="c-40434586">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s like that old joke about one person being amazed at someone&#x27;s dog being able to sing, and the other person says &quot;it&#x27;s not that impressive, he&#x27;s pitchy&quot; or something. Computers can finally think, and we&#x27;re annoyed they can&#x27;t think better than us.</div><br/><div id="40436681" class="c"><input type="checkbox" id="c-40436681" checked=""/><div class="controls bullet"><span class="by">RevEng</span><span>|</span><a href="#40434513">root</a><span>|</span><a href="#40434586">parent</a><span>|</span><a href="#40433838">next</a><span>|</span><label class="collapse" for="c-40436681">[-]</label><label class="expand" for="c-40436681">[2 more]</label></div><br/><div class="children"><div class="content">I think of it more as &quot;dancing bear ware&quot;. If you&#x27;ve ever seen a dancing bear, you wouldn&#x27;t say it&#x27;s particularly good at dancing. You may even say it&#x27;s hardly dancing at all. But we don&#x27;t care that it dances well; it&#x27;s amazing that it dances at all.<p>Current AI is a dancing bear. It gives us the feeling of understanding language, semantics, logic and reason, but when you look closely, you realize it&#x27;s doing a very poor job of it in a way that suggests it is just mimicking those things without actually being capable of them.</div><br/><div id="40437869" class="c"><input type="checkbox" id="c-40437869" checked=""/><div class="controls bullet"><span class="by">radarsat1</span><span>|</span><a href="#40434513">root</a><span>|</span><a href="#40436681">parent</a><span>|</span><a href="#40433838">next</a><span>|</span><label class="collapse" for="c-40437869">[-]</label><label class="expand" for="c-40437869">[1 more]</label></div><br/><div class="children"><div class="content">But as opposed to a dancing bear, we expect AI to get better and better at dancing in the next decades.<p>The speculation is important, it both changes the perspective for prospective companies&#x2F;investors, and cannot even be said at this point to be unfounded.</div><br/></div></div></div></div></div></div></div></div><div id="40433838" class="c"><input type="checkbox" id="c-40433838" checked=""/><div class="controls bullet"><span class="by">LASR</span><span>|</span><a href="#40434513">prev</a><span>|</span><a href="#40433484">next</a><span>|</span><label class="collapse" for="c-40433838">[-]</label><label class="expand" for="c-40433838">[3 more]</label></div><br/><div class="children"><div class="content">I continue to hold the strong position that calling LLMs without injecting source truths is pointless.<p>LLMs are exceptionally powerful as a reasoning engine. It’s useless as a source of truths or facts.<p>We have chat bots, chat bots with automatic RAG etc. After the initial excitement wears off, you’re going to want a way to inspect and adjust the source queries yourself. In this case, being able to select what to search for in Google might be a good way for the cooking recipe usecase.</div><br/><div id="40434318" class="c"><input type="checkbox" id="c-40434318" checked=""/><div class="controls bullet"><span class="by">crabmusket</span><span>|</span><a href="#40433838">parent</a><span>|</span><a href="#40436916">next</a><span>|</span><label class="collapse" for="c-40434318">[-]</label><label class="expand" for="c-40434318">[1 more]</label></div><br/><div class="children"><div class="content">For those who haven&#x27;t read it yet:<p>&quot;I like to think of language models like ChatGPT as a calculator for words.<p>This is reflected in their name: a “language model” implies that they are tools for working with language. That’s what they’ve been trained to do, and it’s language manipulation where they truly excel.<p>Want them to work with specific facts? Paste those into the language model as part of your original prompt!&quot;<p><a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Apr&#x2F;2&#x2F;calculator-for-words&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Apr&#x2F;2&#x2F;calculator-for-words&#x2F;</a></div><br/></div></div></div></div><div id="40433484" class="c"><input type="checkbox" id="c-40433484" checked=""/><div class="controls bullet"><span class="by">antirez</span><span>|</span><a href="#40433838">prev</a><span>|</span><a href="#40431400">next</a><span>|</span><label class="collapse" for="c-40433484">[-]</label><label class="expand" for="c-40433484">[3 more]</label></div><br/><div class="children"><div class="content">Suggested book about traditional Sicilian dishes: <a href="https:&#x2F;&#x2F;www.amazon.it&#x2F;Profumi-Sicilia-libro-cucina-siciliana&#x2F;dp&#x2F;8886803737" rel="nofollow">https:&#x2F;&#x2F;www.amazon.it&#x2F;Profumi-Sicilia-libro-cucina-siciliana...</a></div><br/><div id="40434210" class="c"><input type="checkbox" id="c-40434210" checked=""/><div class="controls bullet"><span class="by">jmount</span><span>|</span><a href="#40433484">parent</a><span>|</span><a href="#40434059">next</a><span>|</span><label class="collapse" for="c-40434210">[-]</label><label class="expand" for="c-40434210">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the book recommendation! I found an English translation of a book by the author and I am ordering that.</div><br/></div></div><div id="40434059" class="c"><input type="checkbox" id="c-40434059" checked=""/><div class="controls bullet"><span class="by">walterbell</span><span>|</span><a href="#40433484">parent</a><span>|</span><a href="#40434210">prev</a><span>|</span><a href="#40431400">next</a><span>|</span><label class="collapse" for="c-40434059">[-]</label><label class="expand" for="c-40434059">[1 more]</label></div><br/><div class="children"><div class="content">We need a trendy TLA for publisher and author marketing of books, highlighting their strengths relative to LLMs.<p><pre><code>  HCL (Human Context Language)
  HLM (Human Language Model)
  LLH (Literate Language for Humans)
  LHM (Literate Human Model)</code></pre></div><br/></div></div></div></div><div id="40431400" class="c"><input type="checkbox" id="c-40431400" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#40433484">prev</a><span>|</span><a href="#40431332">next</a><span>|</span><label class="collapse" for="c-40431400">[-]</label><label class="expand" for="c-40431400">[1 more]</label></div><br/><div class="children"><div class="content">… you really want to be able to cook documents down to facts, as in the old A.I., and then be able to make logical queries.  Trouble is it is easy to ontologize some things (ingredients) but not so easy to ontologize the aspects of things that make things memorable.</div><br/></div></div><div id="40431332" class="c"><input type="checkbox" id="c-40431332" checked=""/><div class="controls bullet"><span class="by">jszymborski</span><span>|</span><a href="#40431400">prev</a><span>|</span><a href="#40433834">next</a><span>|</span><label class="collapse" for="c-40431332">[-]</label><label class="expand" for="c-40431332">[3 more]</label></div><br/><div class="children"><div class="content">Is that ChatGPT example representative of RAG? I thought ChatGPT was primarily generative.<p>I think of something like Brave Search&#x27;s AI feature when I think of RAG.</div><br/><div id="40431549" class="c"><input type="checkbox" id="c-40431549" checked=""/><div class="controls bullet"><span class="by">jmount</span><span>|</span><a href="#40431332">parent</a><span>|</span><a href="#40433834">next</a><span>|</span><label class="collapse" for="c-40431549">[-]</label><label class="expand" for="c-40431549">[2 more]</label></div><br/><div class="children"><div class="content">(author) Good point, sorry about that. I guess I was think the generation is low value was a bit orthogonal to retrieval being very high value.</div><br/><div id="40432629" class="c"><input type="checkbox" id="c-40432629" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#40431332">root</a><span>|</span><a href="#40431549">parent</a><span>|</span><a href="#40433834">next</a><span>|</span><label class="collapse" for="c-40432629">[-]</label><label class="expand" for="c-40432629">[1 more]</label></div><br/><div class="children"><div class="content">People are doing what you are are talking about. Use an LLM to parse non-structured data into a structured or semi-structured format (or even just create a bunch of structured metadata) that is more easily searchable, then using an LLM to parse a natural language query into a suitable query (maybe SQL), then find the result and return it.</div><br/></div></div></div></div></div></div><div id="40433834" class="c"><input type="checkbox" id="c-40433834" checked=""/><div class="controls bullet"><span class="by">WhatIsDukkha</span><span>|</span><a href="#40431332">prev</a><span>|</span><a href="#40434696">next</a><span>|</span><label class="collapse" for="c-40433834">[-]</label><label class="expand" for="c-40433834">[4 more]</label></div><br/><div class="children"><div class="content">Your input was incredibly low effort and prompted a very low effort output.<p>I took part of your blog post (which you clear were willing to put a few more tokens into) -<p>&quot;My mother remembers growing up with a sicilian dish that was primarily rice baked in an egg mixture. Roughly a &quot;rice frittata&quot;. What are some distinctly Sicilian dishes that this could be referring to?&quot;<p>Notice there is not much extra context that you&#x27;ve offered any of us, either the LLM or us. You didn&#x27;t even tell us what the recipe was...<p>How was the dish served, what did it look like?<p>What are you expecting of the LLM here? It not a psychic AGI.</div><br/><div id="40434167" class="c"><input type="checkbox" id="c-40434167" checked=""/><div class="controls bullet"><span class="by">jmount</span><span>|</span><a href="#40433834">parent</a><span>|</span><a href="#40434696">next</a><span>|</span><label class="collapse" for="c-40434167">[-]</label><label class="expand" for="c-40434167">[3 more]</label></div><br/><div class="children"><div class="content">I guess I should have emphasized how I didn&#x27;t like the LLM contradicting itself.</div><br/><div id="40434622" class="c"><input type="checkbox" id="c-40434622" checked=""/><div class="controls bullet"><span class="by">WhatIsDukkha</span><span>|</span><a href="#40433834">root</a><span>|</span><a href="#40434167">parent</a><span>|</span><a href="#40434696">next</a><span>|</span><label class="collapse" for="c-40434622">[-]</label><label class="expand" for="c-40434622">[2 more]</label></div><br/><div class="children"><div class="content">I would expect most any answer to be loopy and partially wrong (or really I&#x27;m not surprised when they are) based on the short question, its just something you build intuition around as you use them.<p>edit - btw I just noticed<p>&quot;Sartù di Riso: A baked rice dish that can include ingredients like meat, peas, and cheese, often bound together with eggs. It’s more commonly associated with Naples but has variations in Sicily.&quot;<p>Was one of the 4 dishes in the question I submitted.<p>So... was it contradictory actually?</div><br/><div id="40437573" class="c"><input type="checkbox" id="c-40437573" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#40433834">root</a><span>|</span><a href="#40434622">parent</a><span>|</span><a href="#40434696">next</a><span>|</span><label class="collapse" for="c-40437573">[-]</label><label class="expand" for="c-40437573">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I would expect most any answer to be loopy and partially wrong (or really I&#x27;m not surprised when they are) based on the short question, its just something you build intuition around as you use them.<p>That sounds pretty unpleasant as a rule to follow.<p>How should I ask instead?<p>Can I make an LLM do the question-expansion for me?</div><br/></div></div></div></div></div></div></div></div><div id="40434696" class="c"><input type="checkbox" id="c-40434696" checked=""/><div class="controls bullet"><span class="by">khaki54</span><span>|</span><a href="#40433834">prev</a><span>|</span><a href="#40436359">next</a><span>|</span><label class="collapse" for="c-40434696">[-]</label><label class="expand" for="c-40434696">[1 more]</label></div><br/><div class="children"><div class="content">Clearly this author doesn&#x27;t know what RAG is.  RAG would be if he first did a &#x27;retrieval&#x27; of all his mother&#x27;s cookbooks containing Italian recipes, then reviewed the index for rice, scanned and OCRd those pages.  That data would be submitted to ChatGPT with the query to &#x27;augment&#x27; it within the constraints of the context window so that ChatGPT could generate a response with the highly relevant cookbook info.</div><br/></div></div><div id="40436359" class="c"><input type="checkbox" id="c-40436359" checked=""/><div class="controls bullet"><span class="by">budududuroiu</span><span>|</span><a href="#40434696">prev</a><span>|</span><a href="#40438269">next</a><span>|</span><label class="collapse" for="c-40436359">[-]</label><label class="expand" for="c-40436359">[1 more]</label></div><br/><div class="children"><div class="content">An interface for vector search is 100x more helpful to me than an LLM spitting out the same content as slop.<p>The key to vector search is how you chunk your data, but I have some libraries to help with that</div><br/></div></div><div id="40438269" class="c"><input type="checkbox" id="c-40438269" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#40436359">prev</a><span>|</span><a href="#40431279">next</a><span>|</span><label class="collapse" for="c-40438269">[-]</label><label class="expand" for="c-40438269">[1 more]</label></div><br/><div class="children"><div class="content">Users don&#x27;t want what they think they want. Therefore, we won&#x27;t give users what they think they want.</div><br/></div></div></div></div></div></div></div></body></html>