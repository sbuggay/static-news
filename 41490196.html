<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1725958871810" as="style"/><link rel="stylesheet" href="styles.css?v=1725958871810"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a>Launch HN: Deepsilicon (YC S24) – Software and hardware for ternary transformers</a> </div><div class="subtext"><span>areddyyt</span> | <span>60 comments</span></div><br/><div><div id="41490812" class="c"><input type="checkbox" id="c-41490812" checked=""/><div class="controls bullet"><span class="by">danjl</span><span>|</span><a href="#41491418">next</a><span>|</span><label class="collapse" for="c-41490812">[-]</label><label class="expand" for="c-41490812">[18 more]</label></div><br/><div class="children"><div class="content">In my experience, trying to switch VFX companies from CPU-based rendering to GPU-based rendering 10+ years ago, a 2-5x performance improvement wasn&#x27;t enough. We even provided a compatible renderer that accepted Renderman files and generated matching images. Given the rate of improvement of standard hardware (CPUs in our case, and GPU-based inference in yours), a 2-5x improvement will only last a few years, and the effort to get there is large (even larger in your case). Plus, I doubt you&#x27;ll be able to get your HW everywhere (i.e. mobile) where inference is important, which means they&#x27;ll need to support their existing and your new SW stack. The other issue is entirely non-technical, and may be an even bigger blocker -- switching the infrastructure of a major LLM provider to a new upstart is just plain risky. If you do a fantastic job, though, you should get aquahired, probably with a small individual bonus, not enough to pay off your investors.</div><br/><div id="41490905" class="c"><input type="checkbox" id="c-41490905" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41490812">parent</a><span>|</span><a href="#41492211">next</a><span>|</span><label class="collapse" for="c-41490905">[-]</label><label class="expand" for="c-41490905">[12 more]</label></div><br/><div class="children"><div class="content">We&#x27;re targeting the edge market first, such as NVIDIA&#x27;s Jetson line, because it&#x27;s far less supported&#x2F;focussed on. In our experience, whenever we did training runs on H100 clusters with x86, any pip package would be easily installable, and a wide array of software just worked. This is not the case in Jetson, where we constantly have to rebuild packages from source, and in general, NVIDIA will only release a better board every five years. As for the second part of your question, we agree. Much of our work has been trying to make switching to our software layer straightforward (a single line of code). The ideal endgame is that, given an ONNX file, we can parse the generated node tree and determine if our hardware supports all the nodes. Of course, this is assuming we have a large enough share of the market using our software, so we know what operations we need to support on the hardware side of things.</div><br/><div id="41490948" class="c"><input type="checkbox" id="c-41490948" checked=""/><div class="controls bullet"><span class="by">danjl</span><span>|</span><a href="#41490812">root</a><span>|</span><a href="#41490905">parent</a><span>|</span><a href="#41490993">next</a><span>|</span><label class="collapse" for="c-41490948">[-]</label><label class="expand" for="c-41490948">[8 more]</label></div><br/><div class="children"><div class="content">I cannot see any way of building HW profitably for the Jetson market. You are really competing with Raspberry PI, not Jetson, IMO. I mean, I&#x27;m no expert, but I would suggest doing a deep dive on your business plan if you intend to target the small hardware world rather than spending any time designing HW or SW. Then reduce your estimate by at least half since doing anything in that embedded&#x2F;edge world has many more technical issues.</div><br/><div id="41490985" class="c"><input type="checkbox" id="c-41490985" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41490812">root</a><span>|</span><a href="#41490948">parent</a><span>|</span><a href="#41490993">next</a><span>|</span><label class="collapse" for="c-41490985">[-]</label><label class="expand" for="c-41490985">[7 more]</label></div><br/><div class="children"><div class="content">In general, Jetson has quite a large market. Vehicle companies use automotive-rated Jetson Orins, and defense companies also use Jetson Orins to power ML applications on the edge (Anduril). Many of the companies we currently talk to are robotics companies that are forced to use Jetsons because they are both the least of the bad options and the only edge compute provider with enough juice to run larger transformer models.</div><br/><div id="41491026" class="c"><input type="checkbox" id="c-41491026" checked=""/><div class="controls bullet"><span class="by">danjl</span><span>|</span><a href="#41490812">root</a><span>|</span><a href="#41490985">parent</a><span>|</span><a href="#41490993">next</a><span>|</span><label class="collapse" for="c-41491026">[-]</label><label class="expand" for="c-41491026">[6 more]</label></div><br/><div class="children"><div class="content">And the auto and Defense markets are so easy to enter! &#x2F;s<p>Both of these markets have long lead times, tight HW build times, and move incredibly slowly. They are not the kind of markets that like using stuff from new companies with no history. Again, I&#x27;m no expert, but I&#x27;d say you need to be concentrating on sales and market research now.</div><br/><div id="41491154" class="c"><input type="checkbox" id="c-41491154" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#41490812">root</a><span>|</span><a href="#41491026">parent</a><span>|</span><a href="#41491120">next</a><span>|</span><label class="collapse" for="c-41491154">[-]</label><label class="expand" for="c-41491154">[1 more]</label></div><br/><div class="children"><div class="content">With respect it doesn&#x27;t sound like you know much about any of these businesses. This startup is extremely early, the road to silicon is long, and there is a lot of external change and learning by doing that will happen between here and there. This is them getting started and based on my related work experience I think it&#x27;s pretty interesting.</div><br/></div></div><div id="41491120" class="c"><input type="checkbox" id="c-41491120" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41490812">root</a><span>|</span><a href="#41491026">parent</a><span>|</span><a href="#41491154">prev</a><span>|</span><a href="#41492818">next</a><span>|</span><label class="collapse" for="c-41491120">[-]</label><label class="expand" for="c-41491120">[2 more]</label></div><br/><div class="children"><div class="content">We are not under the illusion these markets are easy to enter. Still, we believe providing an effortless and compatible experience for edge ML computing is a strong competitive advantage. We have not met anyone who likes using Jetsons yet, unlike A100&#x2F;H100s in the server market.<p>Edit: I should note that if it weren&#x27;t for Dusty and his docker image generating GitHub repo for Jetson, we would have spent weeks trying to get our kernels and optimized models shipped to customers.</div><br/></div></div><div id="41492818" class="c"><input type="checkbox" id="c-41492818" checked=""/><div class="controls bullet"><span class="by">autoconfig</span><span>|</span><a href="#41490812">root</a><span>|</span><a href="#41491026">parent</a><span>|</span><a href="#41491120">prev</a><span>|</span><a href="#41490993">next</a><span>|</span><label class="collapse" for="c-41492818">[-]</label><label class="expand" for="c-41492818">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s your point? Is it that one shouldn&#x27;t attempt to enter a market just because it&#x27;s difficult? Or are you trying to educate the founders about something obvious that they likely have already spent 1000x more time thinking about than you?</div><br/><div id="41493873" class="c"><input type="checkbox" id="c-41493873" checked=""/><div class="controls bullet"><span class="by">motoxpro</span><span>|</span><a href="#41490812">root</a><span>|</span><a href="#41492818">parent</a><span>|</span><a href="#41490993">next</a><span>|</span><label class="collapse" for="c-41493873">[-]</label><label class="expand" for="c-41493873">[1 more]</label></div><br/><div class="children"><div class="content">This 1000%. Just because a business in a tangential area didn&#x27;t work, doesn&#x27;t mean innovation shouldn&#x27;t happen</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41490993" class="c"><input type="checkbox" id="c-41490993" checked=""/><div class="controls bullet"><span class="by">danjl</span><span>|</span><a href="#41490812">root</a><span>|</span><a href="#41490905">parent</a><span>|</span><a href="#41490948">prev</a><span>|</span><a href="#41492211">next</a><span>|</span><label class="collapse" for="c-41490993">[-]</label><label class="expand" for="c-41490993">[3 more]</label></div><br/><div class="children"><div class="content">I think the only way this could work is if you had the backing of one of the major LLM providers who decided that your ideas are worth doing a PoC. That way you actually have a client on board before you spend all the money. I know you guys probably like the designing of the HW and SW, and maybe the implementation of both, but really, what you need now is to do sales.</div><br/><div id="41492074" class="c"><input type="checkbox" id="c-41492074" checked=""/><div class="controls bullet"><span class="by">lumost</span><span>|</span><a href="#41490812">root</a><span>|</span><a href="#41490993">parent</a><span>|</span><a href="#41491008">next</a><span>|</span><label class="collapse" for="c-41492074">[-]</label><label class="expand" for="c-41492074">[1 more]</label></div><br/><div class="children"><div class="content">There are multiple ways to run a business like this.<p>1. Go deep on the tech, there are funders who will want equity stakes in risky startups because they operate in adjacent markets. It&#x27;s often cheaper to invest 1MM on a startup than internal R&amp;D activities. If it has promising results, those same investors may ramp up their spend or pivot to an acquisition strategy.<p>2. Get early customers, if you have 1-10 large enterprises with a committed spend - then you are likely golden. However as nice as this option sounds, there are few avenues to get this type of commitment. If you are in the fortunate position of knowing the exec&#x2F;founding&#x2F;investor team of a large LLM provider - it&#x27;s possible. But easier said than done.<p>3. Build it and they will come, business strategies take time to develop - maybe that time is poorly spent. Build the best version of your product and someone might take it up. There are a few investors who will take a flyer on this type of founder mentality. Benefit to the investor is that they can get a much larger equity stake&#x2F;board position in exchange for the early creative freedom. If it works out, the investor can get a lot of alpha.  A card which handled LLM inference at 1&#x2F;100th the cost of an H100 could produce quite a bit of value for the right buyer.</div><br/></div></div><div id="41491008" class="c"><input type="checkbox" id="c-41491008" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41490812">root</a><span>|</span><a href="#41490993">parent</a><span>|</span><a href="#41492074">prev</a><span>|</span><a href="#41492211">next</a><span>|</span><label class="collapse" for="c-41491008">[-]</label><label class="expand" for="c-41491008">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. We don&#x27;t plan on making hardware until there is enough demand from customers to make it economically viable.</div><br/></div></div></div></div></div></div><div id="41492211" class="c"><input type="checkbox" id="c-41492211" checked=""/><div class="controls bullet"><span class="by">jroesch</span><span>|</span><a href="#41490812">parent</a><span>|</span><a href="#41490905">prev</a><span>|</span><a href="#41491169">next</a><span>|</span><label class="collapse" for="c-41492211">[-]</label><label class="expand" for="c-41492211">[2 more]</label></div><br/><div class="children"><div class="content">Having been working in DL inference for now 7+ years (5 of which at startup) which makes me comparably ancient in the AI world at this point. The performance rat race&#x2F;treadmill is never ending, and to your point a large (i.e 2x+) performance improvement is not enough of a &quot;painkiller&quot; for customers unless there is something that is impossible for them to achieve without your technology.<p>The second problem is distribution: it is already hard enough to obtain good enough distribution with software, let alone software + hardware combinations. Even large silicon companies have struggled to get their HW into products across the world. Part of this is due to the actual purchase dynamics and cycle of people who buy chips, many design products and commit to N year production cycles of products built on certain hardware SKUs, meaning you have to both land large deals, and have opportune timing to catch them when they are evening shopping for a new platform. Furthermore the people with existing distribution i.e the Apple, Google, Nvidia, Intel, AMD, Qualcomms of the world already have distribution and their own offerings in this space and will not partner&#x2F;buy from you.<p>My framing (which has remained unchanged since 2018) is that for silicon platform to win you have to beat the incumbents (i.e Nvidia) on the 3Ps: Price (really TCO), Performance, and Programmability.<p>Most hardware accelerators may win on one, but even then it is often theoretical performance because it assumes their existing software can&#x2F;will work on your chip, which it often doesn&#x27;t (see AMD and friends).<p>There are many other threats that come in this form, for example if you have a fixed function accelerator and some part of the model code has to run on CPU the memory traffic&#x2F;synchronization can completely negate any performance improvements you might offer.<p>Even many of the existing silicon startups have been struggling with this since them middle of the last decade, the only thing that saved them is the consolidation to Transformers but it is very easy for a new model architecture to come out and require everyone to rework what they have built. This need for flexibility is what has given rise to the design ethos around GPGPU as flexibility in a changing world is a requirement not just a nice to have.<p>Best of luck, but these things are worth thinking deeply about as when we started in this market we were already aware of many of these things but their importance and gravity in the AI market have only become more important, not less :)</div><br/><div id="41492355" class="c"><input type="checkbox" id="c-41492355" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41490812">root</a><span>|</span><a href="#41492211">parent</a><span>|</span><a href="#41491169">next</a><span>|</span><label class="collapse" for="c-41492355">[-]</label><label class="expand" for="c-41492355">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve spent a lot of time thinking about these things, in particular, the 3Ps.<p>Part of making the one line of code work is addressing programmability. If you&#x27;re on Jetson, we should load the CUDA kernels for Jetson&#x27;s. If you&#x27;re using a CPU, we should load the CPU kernels. CPU with AVX512, load the appropriate kernels with AVX512 instruction, etc.<p>The end goal is that when we introduce our custom silicon, one line of code should make it far easier to bring customers over from Jetson&#x2F;any other platform because we handle loading the correct backend for them.<p>We know this will be bordering impossible, but it&#x27;s critical to ensure we take on that burden rather than shifting it to the ML engineer.</div><br/></div></div></div></div><div id="41491169" class="c"><input type="checkbox" id="c-41491169" checked=""/><div class="controls bullet"><span class="by">gchadwick</span><span>|</span><a href="#41490812">parent</a><span>|</span><a href="#41492211">prev</a><span>|</span><a href="#41491418">next</a><span>|</span><label class="collapse" for="c-41491169">[-]</label><label class="expand" for="c-41491169">[3 more]</label></div><br/><div class="children"><div class="content">Is GPU rendering used today for VFX? From a quick google it seems that yes GPU based rendering is definitely an option, even if there&#x27;s various reasons to still prefer CPU. So in your case was it really what you were aiming to do was pointless or simply your particular solution failed to succeed?<p>You&#x27;re right that as a small player it&#x27;s very hard to gain traction, even if the tech is fantastic because it&#x27;s risky to switch your tech stack over. Though if you do do a good job with the tech I&#x27;d say you have a decent chance of an acquisition from a bigger player who wants a ready-made (or 90% of the way there) solution they can make their own. Perhaps you can call this an aquihire but I think you&#x27;re significantly underplaying the potential upside of this exit. Imagine this startup is seen as having a great ternary transformer solution and ternary transformers are the way to go you could get multiple large players eyeing up an acquisition to get ahead pushing the price up.<p>My feeling is custom ASICs for ternary transformers is a great area to look at. There is a genuine chance of providing a significant step up from GPUs in terms of power efficiency and potentially performance. Plenty of risk of course, ternary models might just not perform as well as the full fat equivalents and building custom silicon, especially as a start-up, comes with all kinds of issues.</div><br/><div id="41491383" class="c"><input type="checkbox" id="c-41491383" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#41490812">root</a><span>|</span><a href="#41491169">parent</a><span>|</span><a href="#41491418">next</a><span>|</span><label class="collapse" for="c-41491383">[-]</label><label class="expand" for="c-41491383">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Is GPU rendering used today for VFX?<p>Yes by small studios with the agility to change their workflow without too much friction, and whose projects are small enough to fit into the constraints of GPU renderers, but largely not by huge studios who already have in-house CPU farms and whose projects need hundreds of gigs of RAM to render anyway.</div><br/><div id="41494209" class="c"><input type="checkbox" id="c-41494209" checked=""/><div class="controls bullet"><span class="by">kridsdale3</span><span>|</span><a href="#41490812">root</a><span>|</span><a href="#41491383">parent</a><span>|</span><a href="#41491418">next</a><span>|</span><label class="collapse" for="c-41494209">[-]</label><label class="expand" for="c-41494209">[1 more]</label></div><br/><div class="children"><div class="content">The Unreal Engine I hear is getting a lot of work these days.</div><br/></div></div></div></div></div></div></div></div><div id="41491418" class="c"><input type="checkbox" id="c-41491418" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#41490812">prev</a><span>|</span><a href="#41498577">next</a><span>|</span><label class="collapse" for="c-41491418">[-]</label><label class="expand" for="c-41491418">[6 more]</label></div><br/><div class="children"><div class="content">Watching the video demo was key for me. I highly recommend everyone else here watches it.[a]<p>From a software development standpoint, usability looks <i>great</i>, requiring only one import,<p><pre><code>  import deepsilicon as ds
</code></pre>
and then, later on, a single line of Python,<p><pre><code>  model = ds.convert(model)
</code></pre>
which takes care of converting all possible layers (e.g., nn.Linear layers) in the model to use ternary values. Very nice!<p>The question for which I don&#x27;t have a good answer is whether the improvement in real-world performance, using your hardware, will be sufficient to entice developers to leave the comfortable garden of CUDA and Nvidia, given that the latter is continually improving the performance of its hardware.<p>I, for one, hope you guys are hugely successful.<p>---<p>[a] At the moment, the YouTube video demo has some cropping issues, but that can be easily fixed.</div><br/><div id="41491527" class="c"><input type="checkbox" id="c-41491527" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41491418">parent</a><span>|</span><a href="#41492423">next</a><span>|</span><label class="collapse" for="c-41491527">[-]</label><label class="expand" for="c-41491527">[4 more]</label></div><br/><div class="children"><div class="content">Thank you!<p>CUDA and Nvidia are practically impenetrable on the server side. To be very concrete, we did training for our models on AWS with parallel cluster. We used P5 instances (8xH100) that were scheduled with SLURM. A problem we ran into however, was that our training jobs were containerized. Thankfully, pyxis and enroot exist to run containerized jobs on SLURM. And who else, but Nvidia, develop and maintain those plugins. For practically any weird niche use case, Nvidia seems to have some software solution - but only on x86.<p>Jetson is a whole other beast. There is no guarantee any pip package you install has an aarch64&#x2F;arm64 wheel. For example, we could not use torch_tensorrt, to compile to TensorRT via Torch Inductor. Why? Because the Bazel build system was only configured to build for Jetpack 4.6 or Jetpack 5.1, and we were using Jetpack 6. While Nvidia provides docker images for x86 systems that come with torch_tensorrt installed, their L4T (Linux for Tegra) images do not. Instead we had to manually write out a new workspace file and compile for Jetpack6 to provide TensorRT compiling support.<p>tl;dr: Nvidia and CUDA have a great walled garden on x86, not so much on their edge computing devices</div><br/><div id="41491813" class="c"><input type="checkbox" id="c-41491813" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#41491418">root</a><span>|</span><a href="#41491527">parent</a><span>|</span><a href="#41492423">next</a><span>|</span><label class="collapse" for="c-41491813">[-]</label><label class="expand" for="c-41491813">[3 more]</label></div><br/><div class="children"><div class="content">My understanding is that, so far, most deployments of AI on edge devices are on mass-market mobile and entertainment devices relying on software and hardware tightly controlled by a handful of mega-corporations, such as Apple (iOS), Google (Android), Samsung (phones, TVs, etc.), and Tesla (proprietary in-car chips for FSD), and so on. Aren&#x27;t those mega-corporations, not Nvidia, the ones who have the actual walled gardens on AI edge computing?<p>Do you think otherwise?</div><br/><div id="41491910" class="c"><input type="checkbox" id="c-41491910" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41491418">root</a><span>|</span><a href="#41491813">parent</a><span>|</span><a href="#41492423">next</a><span>|</span><label class="collapse" for="c-41491910">[-]</label><label class="expand" for="c-41491910">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re absolutely right about mobile devices (Apple, Google, etc.). However, most companies, with the exception of Tesla, do use Nvidia for edge computing capabilities. We know for a fact that most of the automotive industry uses automotive rated Orins (the 32GB unified RAM SKU) [1] and Anduril also use Orins. Our primary GTM is with robotics companies, and we have not met a single robotics company not using Jetson, I&#x27;m not exaggerating.<p>[1] Particularly vehicles with advanced self driving capabilities. Qualcomm is another large vendor of hardware for vehicles (though they have even worse support)</div><br/><div id="41492167" class="c"><input type="checkbox" id="c-41492167" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#41491418">root</a><span>|</span><a href="#41491910">parent</a><span>|</span><a href="#41492423">next</a><span>|</span><label class="collapse" for="c-41492167">[-]</label><label class="expand" for="c-41492167">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Our primary GTM is with robotics companies, and we have not met a single robotics company not using Jetson, I&#x27;m not exaggerating.<p>Huh. That&#x27;s a really good sign. I&#x27;m rooting for you!</div><br/></div></div></div></div></div></div></div></div><div id="41492423" class="c"><input type="checkbox" id="c-41492423" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41491418">parent</a><span>|</span><a href="#41491527">prev</a><span>|</span><a href="#41498577">next</a><span>|</span><label class="collapse" for="c-41492423">[-]</label><label class="expand" for="c-41492423">[1 more]</label></div><br/><div class="children"><div class="content">Video cropping issues should be fixed!</div><br/></div></div></div></div><div id="41498577" class="c"><input type="checkbox" id="c-41498577" checked=""/><div class="controls bullet"><span class="by">maratc</span><span>|</span><a href="#41491418">prev</a><span>|</span><a href="#41491044">next</a><span>|</span><label class="collapse" for="c-41498577">[-]</label><label class="expand" for="c-41498577">[1 more]</label></div><br/><div class="children"><div class="content">Is there a possibility where this can run on a specialized hardware which is neither a CPU nor GPU, e.g. NextSilicon Maverick chips?</div><br/></div></div><div id="41491044" class="c"><input type="checkbox" id="c-41491044" checked=""/><div class="controls bullet"><span class="by">0xDA7A</span><span>|</span><a href="#41498577">prev</a><span>|</span><a href="#41498048">next</a><span>|</span><label class="collapse" for="c-41491044">[-]</label><label class="expand" for="c-41491044">[2 more]</label></div><br/><div class="children"><div class="content">I think the part I find most interesting about this is the potential power implications. Ternary models may perform better in terms of RAM and that&#x27;s great, but if you manage to build a multiplication-free accelerator in silicon, you can start thinking about running things like vision models in &lt; 0.1W of power.<p>This could have insane implications for edge capabilities, robots with massively better swarm dynamics, smart glasses with super low latency speech to text, etc.<p>I think the biggest technical hurdle would be simulating the non linear layers in an efficient way, but you can also solve that since you already re-train your models and could use custom activation functions that better approximate a HW efficient non linear layer.</div><br/><div id="41491077" class="c"><input type="checkbox" id="c-41491077" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41491044">parent</a><span>|</span><a href="#41498048">next</a><span>|</span><label class="collapse" for="c-41491077">[-]</label><label class="expand" for="c-41491077">[1 more]</label></div><br/><div class="children"><div class="content">The non-linear layers, particularly the softmax(QK^T), will be crucial to getting ultra-low latency and high throughput. We&#x27;re considering some custom silicon just for that portion of every transformer block</div><br/></div></div></div></div><div id="41498048" class="c"><input type="checkbox" id="c-41498048" checked=""/><div class="controls bullet"><span class="by">stephen_cagle</span><span>|</span><a href="#41491044">prev</a><span>|</span><a href="#41497746">next</a><span>|</span><label class="collapse" for="c-41498048">[-]</label><label class="expand" for="c-41498048">[1 more]</label></div><br/><div class="children"><div class="content">Is one expectation from moving from a 2^16 state parameter to a tristate one that the tristate one will only need to learn the number of states of the 2^16 states that were actually significant? I.E. we can prune the &quot;extra&quot; bits from the 2^16 that did not really affect the result?</div><br/></div></div><div id="41497746" class="c"><input type="checkbox" id="c-41497746" checked=""/><div class="controls bullet"><span class="by">bjornsing</span><span>|</span><a href="#41498048">prev</a><span>|</span><a href="#41497730">next</a><span>|</span><label class="collapse" for="c-41497746">[-]</label><label class="expand" for="c-41497746">[1 more]</label></div><br/><div class="children"><div class="content">Have you tried implementing your ternary transformers on AVX(-512)? I think it fits relatively well with the hardware philosophy, and being able to run inference without a GPU would be a big plus.</div><br/></div></div><div id="41497730" class="c"><input type="checkbox" id="c-41497730" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#41497746">prev</a><span>|</span><a href="#41493915">next</a><span>|</span><label class="collapse" for="c-41497730">[-]</label><label class="expand" for="c-41497730">[1 more]</label></div><br/><div class="children"><div class="content">What kind of code did you try on the CPU for, say, ternary gemm?  I imagine ternary values maps nicely to vectorized mask instructions, and much of tiling etc from usual gemm</div><br/></div></div><div id="41493915" class="c"><input type="checkbox" id="c-41493915" checked=""/><div class="controls bullet"><span class="by">nicoty</span><span>|</span><a href="#41497730">prev</a><span>|</span><a href="#41492546">next</a><span>|</span><label class="collapse" for="c-41493915">[-]</label><label class="expand" for="c-41493915">[2 more]</label></div><br/><div class="children"><div class="content">Could the compression efficiency you&#x27;re seeing somehow be related to 3 being the closest natural number to the number e, which also happens to be the optimal radix choice (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Optimal_radix_choice" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Optimal_radix_choice</a>) for storage efficiency?</div><br/><div id="41494438" class="c"><input type="checkbox" id="c-41494438" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41493915">parent</a><span>|</span><a href="#41492546">next</a><span>|</span><label class="collapse" for="c-41494438">[-]</label><label class="expand" for="c-41494438">[1 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t achieve peak compression efficiency because more complex weight unpacking mechanisms kill throughput.<p>To be more explicit, the weight matrix&#x27;s values belong to the set of -1, 0, and 1. When using two bits to encode these weights, we are not effectively utilizing one possible state:<p>10 =&gt; 1,
01 =&gt; 0,
00 =&gt;-1,
11 =&gt; ?<p>I think selecting the optimal radix economy will have more of a play on custom silicon, where we can implement silicon and instructions to rapidly decompress weights or work with the compressed weights directly.</div><br/></div></div></div></div><div id="41492546" class="c"><input type="checkbox" id="c-41492546" checked=""/><div class="controls bullet"><span class="by">jacobgorm</span><span>|</span><a href="#41493915">prev</a><span>|</span><a href="#41496279">next</a><span>|</span><label class="collapse" for="c-41492546">[-]</label><label class="expand" for="c-41492546">[2 more]</label></div><br/><div class="children"><div class="content">I was part of a startup called Grazper that did the same thing for CNNs in 2016, using FPGAs. I left to found my own thing after realizing that new better architectures, SqueezeNet followed by MobileNets, could run even faster than our ternary nets on off-the-shelf hardware. I’d worry that a similar development might happen in the LLMs space.</div><br/><div id="41492608" class="c"><input type="checkbox" id="c-41492608" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41492546">parent</a><span>|</span><a href="#41496279">next</a><span>|</span><label class="collapse" for="c-41492608">[-]</label><label class="expand" for="c-41492608">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s always possible, but transformers have been around since 2017 and don&#x27;t seem to be going anywhere. I was bullish on Mamba and researched extended context for structured state-space models at Dartmouth. However, no one cared. The bet we&#x27;re taking is that Transformers will dominate for at least a few more years, but our bet could be wrong.</div><br/></div></div></div></div><div id="41496279" class="c"><input type="checkbox" id="c-41496279" checked=""/><div class="controls bullet"><span class="by">99112000</span><span>|</span><a href="#41492546">prev</a><span>|</span><a href="#41490446">next</a><span>|</span><label class="collapse" for="c-41496279">[-]</label><label class="expand" for="c-41496279">[2 more]</label></div><br/><div class="children"><div class="content">An area worth exploring are IP cameras imho<p>1. They are everywhere and aren&#x27;t going anywhere..
2. Network infrastructure to ingest  
and analyze thousands of cameras producing video footage is very demanding..
3. Low power and low latency scream asic to me</div><br/><div id="41496305" class="c"><input type="checkbox" id="c-41496305" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41496279">parent</a><span>|</span><a href="#41490446">next</a><span>|</span><label class="collapse" for="c-41496305">[-]</label><label class="expand" for="c-41496305">[1 more]</label></div><br/><div class="children"><div class="content">There was another founder that said this exact same thing. We&#x27;ll definitely look into it especially as we train more ViTs.</div><br/></div></div></div></div><div id="41490446" class="c"><input type="checkbox" id="c-41490446" checked=""/><div class="controls bullet"><span class="by">sidcool</span><span>|</span><a href="#41496279">prev</a><span>|</span><a href="#41493075">next</a><span>|</span><label class="collapse" for="c-41490446">[-]</label><label class="expand" for="c-41490446">[1 more]</label></div><br/><div class="children"><div class="content">Congrats on launching.  This is inspiring. .</div><br/></div></div><div id="41493075" class="c"><input type="checkbox" id="c-41493075" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#41490446">prev</a><span>|</span><a href="#41492962">next</a><span>|</span><label class="collapse" for="c-41493075">[-]</label><label class="expand" for="c-41493075">[4 more]</label></div><br/><div class="children"><div class="content">Since you&#x27;re flexible on the silicon side, perhaps consider designing things so that the ternary weights are loaded from an external configuration rom into a shift register chain, instead of fixed. This would allow updating the weights without having to go through the whole production chain again.</div><br/><div id="41493250" class="c"><input type="checkbox" id="c-41493250" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41493075">parent</a><span>|</span><a href="#41492962">next</a><span>|</span><label class="collapse" for="c-41493250">[-]</label><label class="expand" for="c-41493250">[3 more]</label></div><br/><div class="children"><div class="content">We actually were thinking about this to flush the weights in at initialization</div><br/><div id="41494113" class="c"><input type="checkbox" id="c-41494113" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#41493075">root</a><span>|</span><a href="#41493250">parent</a><span>|</span><a href="#41492962">next</a><span>|</span><label class="collapse" for="c-41494113">[-]</label><label class="expand" for="c-41494113">[2 more]</label></div><br/><div class="children"><div class="content">Cool.... if you want to make a general purpose compute engine out of it, you could go full BitGrid[1].   ;-)<p>[1] <a href="https:&#x2F;&#x2F;bitgrid.blogspot.com&#x2F;2005&#x2F;03&#x2F;bitgrid-story.html" rel="nofollow">https:&#x2F;&#x2F;bitgrid.blogspot.com&#x2F;2005&#x2F;03&#x2F;bitgrid-story.html</a></div><br/><div id="41494641" class="c"><input type="checkbox" id="c-41494641" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41493075">root</a><span>|</span><a href="#41494113">parent</a><span>|</span><a href="#41492962">next</a><span>|</span><label class="collapse" for="c-41494641">[-]</label><label class="expand" for="c-41494641">[1 more]</label></div><br/><div class="children"><div class="content">This seems super cool. I&#x27;ll have my cofounder look into it :)</div><br/></div></div></div></div></div></div></div></div><div id="41492962" class="c"><input type="checkbox" id="c-41492962" checked=""/><div class="controls bullet"><span class="by">lappa</span><span>|</span><a href="#41493075">prev</a><span>|</span><a href="#41494719">next</a><span>|</span><label class="collapse" for="c-41492962">[-]</label><label class="expand" for="c-41492962">[2 more]</label></div><br/><div class="children"><div class="content">Great project, looking forward to seeing more as this develops.<p>Also FYI, your mail server seems to be down.</div><br/><div id="41493280" class="c"><input type="checkbox" id="c-41493280" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41492962">parent</a><span>|</span><a href="#41494719">next</a><span>|</span><label class="collapse" for="c-41493280">[-]</label><label class="expand" for="c-41493280">[1 more]</label></div><br/><div class="children"><div class="content">Thank you, and good catch.<p>We recently acquired deepsilicon.com, and it looks like the forwarding hasn&#x27;t been registered yet. abhi@deepsilicon.net should work.</div><br/></div></div></div></div><div id="41494719" class="c"><input type="checkbox" id="c-41494719" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#41492962">prev</a><span>|</span><a href="#41490975">next</a><span>|</span><label class="collapse" for="c-41494719">[-]</label><label class="expand" for="c-41494719">[3 more]</label></div><br/><div class="children"><div class="content">The most popular interfaces (human, API and network) I can imagine are ChatGPT, OpenAI compatible HTTP API, Transformers HuggingFace API and models, Llama.cpp &#x2F; Ollama &#x2F; Llamafile, Pytorch.
USB C, USB A, RJ45, HDMI&#x2F;video(?)
If you can run a frontier model or a comparable model with the ChatGPT clone like Open UI, with a USB or LAN interface, that can work on private data quickly, securely and competitively to a used 3090 it would be super badass. It should be easy to plug in and be used for running chat or API use or fine-tune or use with raw primitives via Pytorch or a very similar compatible API. I&#x27;ve thought about this a bit. There&#x27;s more I could say but I&#x27;ve got to sleep soon... Good luck, it&#x27;s an awesome opportunity.</div><br/><div id="41494751" class="c"><input type="checkbox" id="c-41494751" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41494719">parent</a><span>|</span><a href="#41490975">next</a><span>|</span><label class="collapse" for="c-41494751">[-]</label><label class="expand" for="c-41494751">[2 more]</label></div><br/><div class="children"><div class="content">Have you sat in on my conversations with my cofounder?<p>The end plan is to have a single chip and flush all weights onto the chip at initialization. Because we are a single line of code that is Torch compatible (hence HF compatible), every other part of the codebase shouldn&#x27;t change.</div><br/><div id="41494877" class="c"><input type="checkbox" id="c-41494877" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#41494719">root</a><span>|</span><a href="#41494751">parent</a><span>|</span><a href="#41490975">next</a><span>|</span><label class="collapse" for="c-41494877">[-]</label><label class="expand" for="c-41494877">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve not but that sounds cool!
I would point out though, in terms of mind share, how memorable, and how relatable and useful the products are: it might help to have ways that directly show the application for the kinds of people buying GPUs for inference and training or using cloud for this that would love to not have to fight their ATX case in a hot sweaty corner while repeatedly dropping screwdrivers and calculating how much RAM they need to buy for the 405B while llama.cpp is recompiling again... I think people would throw money at that.
I&#x27;d be happy to listen in or have a chat some time!</div><br/></div></div></div></div></div></div><div id="41490975" class="c"><input type="checkbox" id="c-41490975" checked=""/><div class="controls bullet"><span class="by">henning</span><span>|</span><a href="#41494719">prev</a><span>|</span><a href="#41490544">next</a><span>|</span><label class="collapse" for="c-41490975">[-]</label><label class="expand" for="c-41490975">[1 more]</label></div><br/><div class="children"><div class="content">I applaud the chutzpah of doing a company where you develop both hardware and software for the hardware. If you execute well, you could build yourself a moat that is very difficult for would-be competitors to breach.</div><br/></div></div><div id="41490544" class="c"><input type="checkbox" id="c-41490544" checked=""/><div class="controls bullet"><span class="by">anirudhrahul</span><span>|</span><a href="#41490975">prev</a><span>|</span><a href="#41495050">next</a><span>|</span><label class="collapse" for="c-41490544">[-]</label><label class="expand" for="c-41490544">[3 more]</label></div><br/><div class="children"><div class="content">Can this run crysis?</div><br/><div id="41490844" class="c"><input type="checkbox" id="c-41490844" checked=""/><div class="controls bullet"><span class="by">0xDA7A</span><span>|</span><a href="#41490544">parent</a><span>|</span><a href="#41495050">next</a><span>|</span><label class="collapse" for="c-41490844">[-]</label><label class="expand" for="c-41490844">[2 more]</label></div><br/><div class="children"><div class="content">Can this run Doom?</div><br/><div id="41496831" class="c"><input type="checkbox" id="c-41496831" checked=""/><div class="controls bullet"><span class="by">campers</span><span>|</span><a href="#41490544">root</a><span>|</span><a href="#41490844">parent</a><span>|</span><a href="#41495050">next</a><span>|</span><label class="collapse" for="c-41496831">[-]</label><label class="expand" for="c-41496831">[1 more]</label></div><br/><div class="children"><div class="content">Can it generate Doom at runtime?</div><br/></div></div></div></div></div></div><div id="41495050" class="c"><input type="checkbox" id="c-41495050" checked=""/><div class="controls bullet"><span class="by">transfire</span><span>|</span><a href="#41490544">prev</a><span>|</span><a href="#41491500">next</a><span>|</span><label class="collapse" for="c-41495050">[-]</label><label class="expand" for="c-41495050">[2 more]</label></div><br/><div class="children"><div class="content">Combine it with TOC, and then you’d really be off to the races!<p><a href="https:&#x2F;&#x2F;intapi.sciendo.com&#x2F;pdf&#x2F;10.2478&#x2F;ijanmc-2022-0036#:~:text=The%20ternary%20optical%20computer%20uses,to%20change%20the%20polarization%20direction" rel="nofollow">https:&#x2F;&#x2F;intapi.sciendo.com&#x2F;pdf&#x2F;10.2478&#x2F;ijanmc-2022-0036#:~:t...</a></div><br/><div id="41495225" class="c"><input type="checkbox" id="c-41495225" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41495050">parent</a><span>|</span><a href="#41491500">next</a><span>|</span><label class="collapse" for="c-41495225">[-]</label><label class="expand" for="c-41495225">[1 more]</label></div><br/><div class="children"><div class="content">Funnily enough, our ML engineer, Eddy, did a hackathon project working with Procyon to make a neural network with a photonic chip. Unfortunately, I think Lightmatter beat us to the punch.<p>Edit: I don&#x27;t think the company exists in its current form anymore</div><br/></div></div></div></div><div id="41491500" class="c"><input type="checkbox" id="c-41491500" checked=""/><div class="controls bullet"><span class="by">nostrebored</span><span>|</span><a href="#41495050">prev</a><span>|</span><a href="#41493186">next</a><span>|</span><label class="collapse" for="c-41491500">[-]</label><label class="expand" for="c-41491500">[2 more]</label></div><br/><div class="children"><div class="content">What do you think about the tension between inference accuracy and the types of edge applications used today?<p>For instance, if you wanted to train a multimodal transformer to do inference on CCTV footage I think that this will have a big advantage over Jetson. And I think there are a lot of potentially novel use cases for a technology like that (eg. if I&#x27;m looking for a suspect wearing a red hoodie, I&#x27;m not training a new classifier to identify all possible candidates)<p>But for sectors like automotive and defense, is the accuracy loss from quantization tolerable? If you&#x27;re investing so much money in putting together a model, even considering procuring custom hardware and software, is the loss in precision worth it?</div><br/><div id="41491573" class="c"><input type="checkbox" id="c-41491573" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41491500">parent</a><span>|</span><a href="#41493186">next</a><span>|</span><label class="collapse" for="c-41491573">[-]</label><label class="expand" for="c-41491573">[1 more]</label></div><br/><div class="children"><div class="content">Great question. So a little bit of background about quantization (apologies if you are already familiar).<p>There are two types of quantization (generally), post training quantization (PTQ) and quantization aware training (QAT).<p>PTQ almost always suffers from some kind of accuracy degradation. This is because usually the loss is measured with respect to the FP16&#x2F;BF16 parameters, and so the weights and distribution are selected to minimize the loss with those weights. Once the quantization function is applied, the weights and distribution change in some way (even if it&#x27;s by a tiny amount), resulting in your model no longer being at minima.<p>We do QAT to get around the problem of PTQ. We actually quantize the weights during the forward pass of training, and measure the loss with respect to the quantized weights. As a result, once we converge the model, we have converged the ternary weights as well, and the accuracy it achieved at the end of training is the accuracy of the quantized model. At ~3B parameters the accuracy on downstream task performance between FP16 and ternary weights is identical.</div><br/></div></div></div></div><div id="41493186" class="c"><input type="checkbox" id="c-41493186" checked=""/><div class="controls bullet"><span class="by">hy3na</span><span>|</span><a href="#41491500">prev</a><span>|</span><a href="#41490376">next</a><span>|</span><label class="collapse" for="c-41493186">[-]</label><label class="expand" for="c-41493186">[1 more]</label></div><br/><div class="children"><div class="content">ternary transformers have existed for a long time before you guys TerDit, vision ones etc. Competing in the edge inference space is likely going to require a lot of capex and opex + breaking into markets like defense thatre hard asf without connections and a strong team. neither of you guys are chip architects either and taping out silicon requires a lot of foresight to changing market demands. good luck, hopefully it works out.</div><br/></div></div><div id="41490376" class="c"><input type="checkbox" id="c-41490376" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41493186">prev</a><span>|</span><a href="#41491455">next</a><span>|</span><label class="collapse" for="c-41490376">[-]</label><label class="expand" for="c-41490376">[1 more]</label></div><br/><div class="children"><div class="content">Very interesting!</div><br/></div></div><div id="41491455" class="c"><input type="checkbox" id="c-41491455" checked=""/><div class="controls bullet"><span class="by">_zoltan_</span><span>|</span><a href="#41490376">prev</a><span>|</span><label class="collapse" for="c-41491455">[-]</label><label class="expand" for="c-41491455">[3 more]</label></div><br/><div class="children"><div class="content">you might want to redo the video as it&#x27;s cropped too much, and maybe it&#x27;s only me but it&#x27;s _really_ annoying to watch like this.</div><br/><div id="41491582" class="c"><input type="checkbox" id="c-41491582" checked=""/><div class="controls bullet"><span class="by">areddyyt</span><span>|</span><a href="#41491455">parent</a><span>|</span><a href="#41492309">next</a><span>|</span><label class="collapse" for="c-41491582">[-]</label><label class="expand" for="c-41491582">[1 more]</label></div><br/><div class="children"><div class="content">Oops, good catch. Will re upload shortly.</div><br/></div></div><div id="41492309" class="c"><input type="checkbox" id="c-41492309" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#41491455">parent</a><span>|</span><a href="#41491582">prev</a><span>|</span><label class="collapse" for="c-41492309">[-]</label><label class="expand" for="c-41492309">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! We&#x27;ve updated the youtube link at the top to the fixed version.</div><br/></div></div></div></div></div></div></div></div></div></body></html>