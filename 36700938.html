<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1689325260011" as="style"/><link rel="stylesheet" href="styles.css?v=1689325260011"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="http://www.numberworld.org/y-cruncher/internals/multiplication.html">Large Multiplication</a>Â <span class="domain">(<a href="http://www.numberworld.org">www.numberworld.org</a>)</span></div><div class="subtext"><span>g0xA52A2A</span> | <span>7 comments</span></div><br/><div><div id="36718478" class="c"><input type="checkbox" id="c-36718478" checked=""/><div class="controls bullet"><span class="by">cperciva</span><span>|</span><label class="collapse" for="c-36718478">[-]</label><label class="expand" for="c-36718478">[6 more]</label></div><br/><div class="children"><div class="content"><i>It is difficult to prove that round-off error won&#x27;t affect the results.</i><p>Colin Percival, Rapid multiplication modulo the sum and difference of highly composite numbers, Mathematics of Computation, Volume 72, Number 241, Pages 387-395, 2002.</div><br/><div id="36720049" class="c"><input type="checkbox" id="c-36720049" checked=""/><div class="controls bullet"><span class="by">moonchild</span><span>|</span><a href="#36718478">parent</a><span>|</span><a href="#36719617">next</a><span>|</span><label class="collapse" for="c-36720049">[-]</label><label class="expand" for="c-36720049">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not familiar with the domain, but is there a particular reason for using floating-point numbers other than that they have faster hardware support?  It is not hard to synthesise a fast integer multiply from a float fma, and then the rest of the algorithm can be phrased in terms of that.</div><br/><div id="36720119" class="c"><input type="checkbox" id="c-36720119" checked=""/><div class="controls bullet"><span class="by">cperciva</span><span>|</span><a href="#36718478">root</a><span>|</span><a href="#36720049">parent</a><span>|</span><a href="#36719617">next</a><span>|</span><label class="collapse" for="c-36720119">[-]</label><label class="expand" for="c-36720119">[1 more]</label></div><br/><div class="children"><div class="content">Doing an FFT with integer math means doing it in a ring -- and thus dealing with modular reduction.  Using complex floating-point math the &quot;modular reduction&quot; comes for free thanks to w^N=1.<p>I would have to write some code to confirm which is faster today -- especially if you choose rings which make the modular reduction easier -- but CPUs have typically optimized FP math more for throughout than integer math, since mostly-integer code has more unpredictable branches and less ILP than FP-heavy code.</div><br/></div></div></div></div><div id="36719617" class="c"><input type="checkbox" id="c-36719617" checked=""/><div class="controls bullet"><span class="by">Strilanc</span><span>|</span><a href="#36718478">parent</a><span>|</span><a href="#36720049">prev</a><span>|</span><label class="collapse" for="c-36719617">[-]</label><label class="expand" for="c-36719617">[3 more]</label></div><br/><div class="children"><div class="content">from google scholar search: <a href="https:&#x2F;&#x2F;www.ams.org&#x2F;journals&#x2F;mcom&#x2F;2003-72-241&#x2F;S0025-5718-02-01419-9&#x2F;S0025-5718-02-01419-9.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.ams.org&#x2F;journals&#x2F;mcom&#x2F;2003-72-241&#x2F;S0025-5718-02-...</a></div><br/><div id="36719994" class="c"><input type="checkbox" id="c-36719994" checked=""/><div class="controls bullet"><span class="by">cperciva</span><span>|</span><a href="#36718478">root</a><span>|</span><a href="#36719617">parent</a><span>|</span><label class="collapse" for="c-36719994">[-]</label><label class="expand" for="c-36719994">[2 more]</label></div><br/><div class="children"><div class="content">Right, I didn&#x27;t think people would actually want to read the paper; I just wanted to make the point that, rather than being difficult, this was solved over 20 years ago.  (And I was an undergrad at the time!)</div><br/><div id="36721154" class="c"><input type="checkbox" id="c-36721154" checked=""/><div class="controls bullet"><span class="by">CodesInChaos</span><span>|</span><a href="#36718478">root</a><span>|</span><a href="#36719994">parent</a><span>|</span><label class="collapse" for="c-36721154">[-]</label><label class="expand" for="c-36721154">[1 more]</label></div><br/><div class="children"><div class="content">The article claims that the provable bounds for rounding errors are significantly worse than the empirical bounds used by y-cruncher, so an provably correct algorithm would not have competitive performance.<p>I&#x27;m not sure if you claim that the bounds proven in your paper are tight enough to be competitive, or if you simply confirm that there are slower conservative implementations that are provably correct.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>