<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1731229252712" as="style"/><link rel="stylesheet" href="styles.css?v=1731229252712"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://hanlab.mit.edu/blog/svdquant">SVDQuant: 4-Bit Quantization Powers 12B Flux on a 16GB 4090 GPU with 3x Speedup</a> <span class="domain">(<a href="https://hanlab.mit.edu">hanlab.mit.edu</a>)</span></div><div class="subtext"><span>lmxyy</span> | <span>58 comments</span></div><br/><div><div id="42094813" class="c"><input type="checkbox" id="c-42094813" checked=""/><div class="controls bullet"><span class="by">djoldman</span><span>|</span><a href="#42093652">next</a><span>|</span><label class="collapse" for="c-42094813">[-]</label><label class="expand" for="c-42094813">[15 more]</label></div><br/><div class="children"><div class="content">This is one in a long line of posts saying &quot;we took a model and made it smaller&quot; and now it can run with different requirements.<p>It is important to keep in mind that modifying a model changes the performance of the resulting model, where performance is &quot;correctness&quot; or &quot;quality&quot; of output.<p>Just because the base model is very performant does not mean the smaller model is.<p>This means that another model that is the same size as the new quantized model may outperform the quantized model.<p>Suppose there are equal sized big models A and B with their smaller quantized variants a and b. A being a more performant model than B does not guarantee a being more performant than b.</div><br/><div id="42095276" class="c"><input type="checkbox" id="c-42095276" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#42094813">parent</a><span>|</span><a href="#42096007">next</a><span>|</span><label class="collapse" for="c-42095276">[-]</label><label class="expand" for="c-42095276">[10 more]</label></div><br/><div class="children"><div class="content">While I think I agree that there are many posts here on HackerNews announcing a new model compression technique, your characterization above understates the technical innovations and practical impacts described in this MIT paper.<p>Unlike traditional model compression work that simply applies existing techniques, SVDQuant synthesizes several ideas in a comprehensive new approach to model quantization:<p>- Developing a novel outlier absorption mechanism using low-rank decomposition — this aspect alone seems quite novel, although the math is admittedly way beyond my level<p>- Combining SVD with smoothing in a way that specifically 
addresses the unique challenges of diffusion models<p>- Creating an innovative kernel fusion technique (they call it “Nunchaku”) that makes the theoretical benefits practically realizable, because without this, the extra computation required to implement the above steps would simply slow the model back down to baseline<p>This isn&#x27;t just incremental improvement - the paper achieves several breakthrough results:<p>- First successful 4-bit quantization of both weights AND activations for diffusion models<p>- 3.5x memory reduction for 12B parameter models while maintaining image quality<p>- 3.0x speedup over existing 4-bit weight-only quantization approaches<p>- Enables running 12B parameter models on consumer GPUs that previously couldn&#x27;t handle them<p>And, I’ll add, as someone who has been following the diffusion space quite actively for the last two years, the amount of creativity that can be unleashed when models are accessible to people with consumer GPUs is nothing short of astonishing.<p>The authors took pains to validate their approach by testing it against three models (Flux, PixArt-Sigma, and SDXL) and along several quality-comparison axes (FID score, Image Reward, LPIPS, and PSNR). They also did a proper ablation study to see the contribution of each component in their approach to image quality.<p>What particularly excites me about this paper is not the ability to run a model that eats 22GB of VRAM in just 7GB. The exciting thing is the prospect of running a 60GB model in 20GB of VRAM. I’m not sure whether anyone has or is planning to train such a monster, but I suspect that Midjourney, OpenAI, and Google all have significantly larger models running in their infrastructure than what can be run on consumer hardware. The more dimensions you can throw at image and video generation, the better things get.</div><br/><div id="42095440" class="c"><input type="checkbox" id="c-42095440" checked=""/><div class="controls bullet"><span class="by">djoldman</span><span>|</span><a href="#42094813">root</a><span>|</span><a href="#42095276">parent</a><span>|</span><a href="#42096007">next</a><span>|</span><label class="collapse" for="c-42095440">[-]</label><label class="expand" for="c-42095440">[9 more]</label></div><br/><div class="children"><div class="content">I definitely agree that there may be some interesting advancements here.<p>I am trying to call attention to the models used for evaluation comparison. There are 3 factors: inference speed&#x2F;latency, model size in total loaded VRAM, and model performance in terms of output.<p>Comparisons should address all of these considerations, otherwise it&#x27;s easy to hide deficiencies.</div><br/><div id="42095709" class="c"><input type="checkbox" id="c-42095709" checked=""/><div class="controls bullet"><span class="by">Jackson__</span><span>|</span><a href="#42094813">root</a><span>|</span><a href="#42095440">parent</a><span>|</span><a href="#42096007">next</a><span>|</span><label class="collapse" for="c-42095709">[-]</label><label class="expand" for="c-42095709">[8 more]</label></div><br/><div class="children"><div class="content">The site literally has a quick visual comparison near the top, which shows that theirs is the closest to 16bit performance compared to the others. I don&#x27;t get what more you&#x27;d want.<p><a href="https:&#x2F;&#x2F;cdn.prod.website-files.com&#x2F;64f4e81394e25710d22d042e&#x2F;672d1bcef3c3ec127e8078fd_672d1b2115081c1e8ac82ea9_teaser.jpeg" rel="nofollow">https:&#x2F;&#x2F;cdn.prod.website-files.com&#x2F;64f4e81394e25710d22d042e&#x2F;...</a></div><br/><div id="42096220" class="c"><input type="checkbox" id="c-42096220" checked=""/><div class="controls bullet"><span class="by">djoldman</span><span>|</span><a href="#42094813">root</a><span>|</span><a href="#42095709">parent</a><span>|</span><a href="#42096007">next</a><span>|</span><label class="collapse" for="c-42096220">[-]</label><label class="expand" for="c-42096220">[7 more]</label></div><br/><div class="children"><div class="content">These are comparisons to other quantizing methods. That is fine.<p>What I want to see is comparisons to NON-quantized models all with around the same VRAM along with associated inference latencies.<p>Also, we would want to see the same quantizing schemes applied to other base models.. because perhaps the paper&#x27;s proposed quantizing scheme only beats others using a particular base model.</div><br/><div id="42096454" class="c"><input type="checkbox" id="c-42096454" checked=""/><div class="controls bullet"><span class="by">snovv_crash</span><span>|</span><a href="#42094813">root</a><span>|</span><a href="#42096220">parent</a><span>|</span><a href="#42097101">next</a><span>|</span><label class="collapse" for="c-42096454">[-]</label><label class="expand" for="c-42096454">[2 more]</label></div><br/><div class="children"><div class="content">They tested the quantisation on 3 different models.<p>They also show it has little to no effect relative to fp16 on these models.<p>IMO that&#x27;s enough. Comparison against smaller models is much less useful because you can&#x27;t use the same random seeds. So you end up with a very objective &quot;this is worse&quot; based purely on aesthetic preferences of one person vs another. You already see this with Flux Schnell vs. the larger Flux models.</div><br/><div id="42097806" class="c"><input type="checkbox" id="c-42097806" checked=""/><div class="controls bullet"><span class="by">djoldman</span><span>|</span><a href="#42094813">root</a><span>|</span><a href="#42096454">parent</a><span>|</span><a href="#42097101">next</a><span>|</span><label class="collapse" for="c-42097806">[-]</label><label class="expand" for="c-42097806">[1 more]</label></div><br/><div class="children"><div class="content">I disagree.<p>They report that their method produces a model that is 6.5 GB from flux (22.7GB). Why wouldn&#x27;t you want to know how their 6.5GB model compares to other 6.5GB models?<p>Regarding aesthetic prefs: it&#x27;s an open problem what an appropriate metric is for GenAI... LLM arena is widely regarded as a good way to measure LLMs and that&#x27;s user preferences.<p>In any case, the authors report LPIPs etc. They could do the same for other small models.</div><br/></div></div></div></div><div id="42097101" class="c"><input type="checkbox" id="c-42097101" checked=""/><div class="controls bullet"><span class="by">aaronblohowiak</span><span>|</span><a href="#42094813">root</a><span>|</span><a href="#42096220">parent</a><span>|</span><a href="#42096454">prev</a><span>|</span><a href="#42096775">next</a><span>|</span><label class="collapse" for="c-42097101">[-]</label><label class="expand" for="c-42097101">[2 more]</label></div><br/><div class="children"><div class="content">&gt;What I want to see is comparisons to NON-quantized models<p>isnt that the first image in the diagram &#x2F; the 22GB model that took 111 seconds?</div><br/><div id="42098118" class="c"><input type="checkbox" id="c-42098118" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#42094813">root</a><span>|</span><a href="#42097101">parent</a><span>|</span><a href="#42096775">next</a><span>|</span><label class="collapse" for="c-42098118">[-]</label><label class="expand" for="c-42098118">[1 more]</label></div><br/><div class="children"><div class="content">The next six words you didn&#x27;t quote make all the difference.</div><br/></div></div></div></div><div id="42096775" class="c"><input type="checkbox" id="c-42096775" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42094813">root</a><span>|</span><a href="#42096220">parent</a><span>|</span><a href="#42097101">prev</a><span>|</span><a href="#42096007">next</a><span>|</span><label class="collapse" for="c-42096775">[-]</label><label class="expand" for="c-42096775">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m really confused, this looks like concern trolling because there&#x27;s a live demo for exactly this A&#x2F;B testing, that IIRC was near the top of the article, close enough it was the first link I clicked.<p>But you&#x27;re quite persistent in that they need to address this, so it seems much more likely they silently added it after your original post, or you didn&#x27;t click through, concern trolling would stay more vague</div><br/><div id="42098103" class="c"><input type="checkbox" id="c-42098103" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#42094813">root</a><span>|</span><a href="#42096775">parent</a><span>|</span><a href="#42096007">next</a><span>|</span><label class="collapse" for="c-42098103">[-]</label><label class="expand" for="c-42098103">[1 more]</label></div><br/><div class="children"><div class="content">The demo is not what they&#x27;re asking for.  It compares original versus quantized.  They want quantized versus a similar <i>same-size in GB</i> model.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42096007" class="c"><input type="checkbox" id="c-42096007" checked=""/><div class="controls bullet"><span class="by">boulos</span><span>|</span><a href="#42094813">parent</a><span>|</span><a href="#42095276">prev</a><span>|</span><a href="#42097085">next</a><span>|</span><label class="collapse" for="c-42096007">[-]</label><label class="expand" for="c-42096007">[2 more]</label></div><br/><div class="children"><div class="content">As others have replied, this is reasonable general feedback, but in this specific case the work was done carefully. Table 1 from the linked paper  (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2411.05007" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2411.05007</a>) includes a variety of metrics, while an entire appendix is dedicated to quality comparisons.<p>By showing their work side-by-side with other quantization schemes, you can also see a great example of the flavor of different results you can get with these slight tweaks (e.g., ViDiT INT8) <i>and</i> that their quantization does a much better job in <i>reproducing</i> the &quot;original&quot; (Figure 15).<p>In this application, it&#x27;s not strictly true that you <i>care</i> to have the same results, but this work does a pretty good job of it.</div><br/><div id="42096251" class="c"><input type="checkbox" id="c-42096251" checked=""/><div class="controls bullet"><span class="by">djoldman</span><span>|</span><a href="#42094813">root</a><span>|</span><a href="#42096007">parent</a><span>|</span><a href="#42097085">next</a><span>|</span><label class="collapse" for="c-42096251">[-]</label><label class="expand" for="c-42096251">[1 more]</label></div><br/><div class="children"><div class="content">Agreed.<p>Once a model has been trained, I believe the main metrics people care about are<p>1. inference speed<p>2. memory requirements<p>3. quality of output.<p>There are usually tradeoffs here. Generally you get a lower memory requirement (a good thing), sometimes faster inference (a good thing), but usually a lower quality of output.<p>I don&#x27;t think reproduction of original output is the typical goal.</div><br/></div></div></div></div><div id="42097085" class="c"><input type="checkbox" id="c-42097085" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#42094813">parent</a><span>|</span><a href="#42096007">prev</a><span>|</span><a href="#42094886">next</a><span>|</span><label class="collapse" for="c-42097085">[-]</label><label class="expand" for="c-42097085">[1 more]</label></div><br/><div class="children"><div class="content">Did you...did you read the technical details? This is almost all they talk about, this method was created to get around.<p>Take a look, it&#x27;s good stuff! Basically a LoRA to reconstruct outliers lost by quantization, helping keep the performance of the original model.</div><br/></div></div><div id="42094886" class="c"><input type="checkbox" id="c-42094886" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#42094813">parent</a><span>|</span><a href="#42097085">prev</a><span>|</span><a href="#42093652">next</a><span>|</span><label class="collapse" for="c-42094886">[-]</label><label class="expand" for="c-42094886">[1 more]</label></div><br/><div class="children"><div class="content">Not really. They quantized the activations here with their inference program which decreased compute as well as RAM usage (and required bandwidth). That&#x27;s a big step.</div><br/></div></div></div></div><div id="42093652" class="c"><input type="checkbox" id="c-42093652" checked=""/><div class="controls bullet"><span class="by">mesmertech</span><span>|</span><a href="#42094813">prev</a><span>|</span><a href="#42095768">next</a><span>|</span><label class="collapse" for="c-42093652">[-]</label><label class="expand" for="c-42093652">[12 more]</label></div><br/><div class="children"><div class="content">Demo on actual 4090 with flux schnell for next few hours: <a href="https:&#x2F;&#x2F;5jkdpo3rnipsem-3000.proxy.runpod.net&#x2F;" rel="nofollow">https:&#x2F;&#x2F;5jkdpo3rnipsem-3000.proxy.runpod.net&#x2F;</a><p>Its basically H100 speeds with 4090, 4.80it&#x2F;s. 1.1 sec for flux schenll(4 steps) and 5.5 seconds for flux dev(25 steps). Compared to normal speeds(comfyui fp8 with &quot;--fast&quot; optimization&quot;) which is 3 seconds for schnell and 11.5 seconds for dev</div><br/><div id="42095200" class="c"><input type="checkbox" id="c-42095200" checked=""/><div class="controls bullet"><span class="by">AzN1337c0d3r</span><span>|</span><a href="#42093652">parent</a><span>|</span><a href="#42096651">next</a><span>|</span><label class="collapse" for="c-42095200">[-]</label><label class="expand" for="c-42095200">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s worth noting this is laptop 4090 GPU which is more like in the range of desktop 4070 performance.</div><br/><div id="42096934" class="c"><input type="checkbox" id="c-42096934" checked=""/><div class="controls bullet"><span class="by">mesmertech</span><span>|</span><a href="#42093652">root</a><span>|</span><a href="#42095200">parent</a><span>|</span><a href="#42096651">next</a><span>|</span><label class="collapse" for="c-42096934">[-]</label><label class="expand" for="c-42096934">[1 more]</label></div><br/><div class="children"><div class="content">This specific link I shared is the quant running on a 4090 I rented on runpod, I have no affiliation with the repo itself</div><br/></div></div></div></div><div id="42096651" class="c"><input type="checkbox" id="c-42096651" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#42093652">parent</a><span>|</span><a href="#42095200">prev</a><span>|</span><a href="#42094950">next</a><span>|</span><label class="collapse" for="c-42096651">[-]</label><label class="expand" for="c-42096651">[2 more]</label></div><br/><div class="children"><div class="content">The compute differential between an H100 and a 4090 is not huge. The main single GPU benefits are larger memory (and thus memory bandwidth) and native fp8. But these matter less for diffusion models.</div><br/><div id="42096932" class="c"><input type="checkbox" id="c-42096932" checked=""/><div class="controls bullet"><span class="by">mesmertech</span><span>|</span><a href="#42093652">root</a><span>|</span><a href="#42096651">parent</a><span>|</span><a href="#42094950">next</a><span>|</span><label class="collapse" for="c-42096932">[-]</label><label class="expand" for="c-42096932">[1 more]</label></div><br/><div class="children"><div class="content">Thats what I thought as well, but FP8 is much faster on h100, like 2x-3x. You can check it&#x2F;s here: <a href="https:&#x2F;&#x2F;github.com&#x2F;aredden&#x2F;flux-fp8-api">https:&#x2F;&#x2F;github.com&#x2F;aredden&#x2F;flux-fp8-api</a><p>Its why fal, replicate, pretty much all big diffusion api providers use h100<p>tldr; 4090 is max 3.51 it&#x2F;s even with all the current optimizations. h100 is 11.5it&#x2F;s with all optimizations, and even without its 6.1 it&#x2F;s</div><br/></div></div></div></div><div id="42094950" class="c"><input type="checkbox" id="c-42094950" checked=""/><div class="controls bullet"><span class="by">bufferoverflow</span><span>|</span><a href="#42093652">parent</a><span>|</span><a href="#42096651">prev</a><span>|</span><a href="#42093907">next</a><span>|</span><label class="collapse" for="c-42094950">[-]</label><label class="expand" for="c-42094950">[1 more]</label></div><br/><div class="children"><div class="content">Damn, it runs very fast.</div><br/></div></div><div id="42093907" class="c"><input type="checkbox" id="c-42093907" checked=""/><div class="controls bullet"><span class="by">yakorevivan</span><span>|</span><a href="#42093652">parent</a><span>|</span><a href="#42094950">prev</a><span>|</span><a href="#42095768">next</a><span>|</span><label class="collapse" for="c-42093907">[-]</label><label class="expand" for="c-42093907">[6 more]</label></div><br/><div class="children"><div class="content">Hey, can you share the inference code please? Thanks..</div><br/><div id="42094508" class="c"><input type="checkbox" id="c-42094508" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#42093652">root</a><span>|</span><a href="#42093907">parent</a><span>|</span><a href="#42095768">next</a><span>|</span><label class="collapse" for="c-42094508">[-]</label><label class="expand" for="c-42094508">[5 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;mit-han-lab&#x2F;nunchaku">https:&#x2F;&#x2F;github.com&#x2F;mit-han-lab&#x2F;nunchaku</a></div><br/><div id="42094801" class="c"><input type="checkbox" id="c-42094801" checked=""/><div class="controls bullet"><span class="by">oneshtein</span><span>|</span><a href="#42093652">root</a><span>|</span><a href="#42094508">parent</a><span>|</span><a href="#42095768">next</a><span>|</span><label class="collapse" for="c-42094801">[-]</label><label class="expand" for="c-42094801">[4 more]</label></div><br/><div class="children"><div class="content">Cannot compile it locally on Fedora 40:<p><pre><code>  nunchaku&#x2F;third_party&#x2F;spdlog&#x2F;include&#x2F;spdlog&#x2F;common.h(144): error: namespace &quot;std&quot; has no member &quot;function&quot;
  using err_handler = std::function&lt;void(const std::string &amp;err_msg)&gt;;
                                   ^</code></pre></div><br/><div id="42094850" class="c"><input type="checkbox" id="c-42094850" checked=""/><div class="controls bullet"><span class="by">mesmertech</span><span>|</span><a href="#42093652">root</a><span>|</span><a href="#42094801">parent</a><span>|</span><a href="#42095768">next</a><span>|</span><label class="collapse" for="c-42094850">[-]</label><label class="expand" for="c-42094850">[3 more]</label></div><br/><div class="children"><div class="content">Yea its a pain, I&#x27;m trying to make an api endpoint for a website I own, and working on a docker image. This is what I have for now that &quot;just&quot; works:<p>the conda always yes thing makes sure that you can just paste the script and it all works instead of having to press &quot;y&quot; for each install. Also if you don&#x27;t feel like installing a wheel from random person on the internet, replace that step with &quot;pip install -e .&quot; as the repo suggests. I compiled that one with cuda 12.4 cause that was the part takes the most time and is what most often seems to be breaking.<p>Also I&#x27;m not sure if this will work on Fedora, I tried this on a runpod machine with 4090(apparently it only works on few cards, 3090, 4090, a100 etc) with Cuda 12.4 on host machine and &quot;runpod&#x2F;pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04&quot; this image as base.<p>EDIT: using pastebin instead as HN doesn&#x27;t seem to jive with code blocks: <a href="https:&#x2F;&#x2F;pastebin.com&#x2F;zK1z0UdM" rel="nofollow">https:&#x2F;&#x2F;pastebin.com&#x2F;zK1z0UdM</a></div><br/><div id="42095628" class="c"><input type="checkbox" id="c-42095628" checked=""/><div class="controls bullet"><span class="by">oneshtein</span><span>|</span><a href="#42093652">root</a><span>|</span><a href="#42094850">parent</a><span>|</span><a href="#42095768">next</a><span>|</span><label class="collapse" for="c-42095628">[-]</label><label class="expand" for="c-42095628">[2 more]</label></div><br/><div class="children"><div class="content">Almost working:<p><pre><code>  [2024-11-09 19:33:55.214] [info] Initializing QuantizedFluxModel
  [2024-11-09 19:33:55.359] [info] Loading weights from ~&#x2F;.cache&#x2F;huggingface&#x2F;hub&#x2F;models--mit-han-lab--svdquant-models&#x2F;snapshots&#x2F;d2a46e82a378ec70e3329a2219ac4331a444a999&#x2F;svdq-int4-flux.1-schnell.safetensors
  [2024-11-09 19:34:01.432] [warning] Unable to pin memory: invalid argument
  [2024-11-09 19:34:02.143] [info] Done.
  terminate called after throwing an instance of &#x27;CUDAError&#x27;
    what():  CUDA error: pointer does not correspond to a registered memory region (at &#x2F;nunchaku&#x2F;src&#x2F;Serialization.cpp:32)</code></pre></div><br/><div id="42095746" class="c"><input type="checkbox" id="c-42095746" checked=""/><div class="controls bullet"><span class="by">mesmertech</span><span>|</span><a href="#42093652">root</a><span>|</span><a href="#42095628">parent</a><span>|</span><a href="#42095768">next</a><span>|</span><label class="collapse" for="c-42095746">[-]</label><label class="expand" for="c-42095746">[1 more]</label></div><br/><div class="children"><div class="content">prolly make sure your host machine cuda is also 12.4 and if not, update the other cuda versions I have on the pastebin to the one you have. I don&#x27;t think it works with cuda 11.8 tho, remember trying it once<p>but yea, can&#x27;t help you outside of runpod, I haven&#x27;t even tried this on my home PCs yet. for my usecase of serverless API, it seems to work</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42095768" class="c"><input type="checkbox" id="c-42095768" checked=""/><div class="controls bullet"><span class="by">atlex2</span><span>|</span><a href="#42093652">prev</a><span>|</span><a href="#42093668">next</a><span>|</span><label class="collapse" for="c-42095768">[-]</label><label class="expand" for="c-42095768">[2 more]</label></div><br/><div class="children"><div class="content">Seriously nobody thought to use SVD on these weight matrices before?</div><br/><div id="42096314" class="c"><input type="checkbox" id="c-42096314" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#42095768">parent</a><span>|</span><a href="#42093668">next</a><span>|</span><label class="collapse" for="c-42096314">[-]</label><label class="expand" for="c-42096314">[1 more]</label></div><br/><div class="children"><div class="content">I did try, but in a wrong way (try to SVD quantization error to recover quality (I.e. SVD(W - Q(W)))). The lightbulb moment in this paper is to do SVD on W and then quantize the remaining.</div><br/></div></div></div></div><div id="42093668" class="c"><input type="checkbox" id="c-42093668" checked=""/><div class="controls bullet"><span class="by">notarealllama</span><span>|</span><a href="#42095768">prev</a><span>|</span><a href="#42093981">next</a><span>|</span><label class="collapse" for="c-42093668">[-]</label><label class="expand" for="c-42093668">[16 more]</label></div><br/><div class="children"><div class="content">I&#x27;m convinced the path to ubiquity (such as embedded in smartphones) is quantization.<p>I had to int4 a llama model to get it to properly run on my 3060.<p>I&#x27;m curious, how much resolution &#x2F; significant digits do we actually need for most genAI work? If you can draw a circle with 3.14, maybe it&#x27;s good enough for fast and ubiquitous usage.</div><br/><div id="42093712" class="c"><input type="checkbox" id="c-42093712" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#42093668">parent</a><span>|</span><a href="#42093981">next</a><span>|</span><label class="collapse" for="c-42093712">[-]</label><label class="expand" for="c-42093712">[15 more]</label></div><br/><div class="children"><div class="content">Earlier this year there was a paper from Microsoft where they trained a 1.58 bit (every parameter being ternary) LLM that matched the performance of 16 bit models. There&#x27;s also other research that you can prune up to 50% of layers with minimal loss of performance. Our current training methods are just incredibly crude and we will probably look back on those in the future and wonder how this ever worked at all.</div><br/><div id="42093896" class="c"><input type="checkbox" id="c-42093896" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42093668">root</a><span>|</span><a href="#42093712">parent</a><span>|</span><a href="#42093981">next</a><span>|</span><label class="collapse" for="c-42093896">[-]</label><label class="expand" for="c-42093896">[14 more]</label></div><br/><div class="children"><div class="content">None of those papers actually use quantized training, they are all about quantized inference.<p>Which is rather unfortunate as it means that the difference between what you can train locally and what you can run locally is growing ever larger.</div><br/><div id="42094028" class="c"><input type="checkbox" id="c-42094028" checked=""/><div class="controls bullet"><span class="by">danielEM</span><span>|</span><a href="#42093668">root</a><span>|</span><a href="#42093896">parent</a><span>|</span><a href="#42094407">next</a><span>|</span><label class="collapse" for="c-42094028">[-]</label><label class="expand" for="c-42094028">[7 more]</label></div><br/><div class="children"><div class="content">Indeed. I think &quot;AI gold rush&quot; sucks anyone with any skills in this area into it with relatively good pay, so there are no, or almost no people outside of big tech and startups to counterbalance direction where it moves. And as a side note, big tech is and always was putting their agenda first in developing any tech or standards and that usually makes milking on investments as long as possible, not necessary moving things forward.</div><br/><div id="42094141" class="c"><input type="checkbox" id="c-42094141" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42093668">root</a><span>|</span><a href="#42094028">parent</a><span>|</span><a href="#42094407">next</a><span>|</span><label class="collapse" for="c-42094141">[-]</label><label class="expand" for="c-42094141">[6 more]</label></div><br/><div class="children"><div class="content">There&#x27;s more to it than that.<p>If you could train models faster, you’d be able to build larger, more powerful models that outperform the competition.<p>The fact that Llama 3 is significantly over trained than what was considered ideal even three years ago shows there&#x27;s a strong appetite for efficient training. The lack of progress isn’t due to a lack of effort. No one has managed to do this yet because no one has figured out how.<p>I built 1-trit quantized models as a side project nearly a decade ago. Back then, no one cared because models weren’t yet using all available memory, and on devices where memory was fully utilized, compute power was the limiting factor. I spend much longer trying to figure out how to get 1-trit training to work and I never could. Of all the papers and people in the field I&#x27;ve talked to, no one else has either.</div><br/><div id="42095878" class="c"><input type="checkbox" id="c-42095878" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#42093668">root</a><span>|</span><a href="#42094141">parent</a><span>|</span><a href="#42094237">next</a><span>|</span><label class="collapse" for="c-42095878">[-]</label><label class="expand" for="c-42095878">[1 more]</label></div><br/><div class="children"><div class="content">People did care back then. This paper had jumpstarted the whole model compression field (which used to be a hot area of research in early 90s): <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1511.00363" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1511.00363</a><p>Before that, in 2012, Alexnet had to be partially split into two submodels, running on two GPUs (using a form of interlayer grouped convolutions) because it could not fit in 3GB of a single 580 card.<p>Ternary networks appeared in 2016. Unless you mean you actually tried to train in ternary precision - clearly not possible with any gradient based optimization methods.</div><br/></div></div><div id="42094237" class="c"><input type="checkbox" id="c-42094237" checked=""/><div class="controls bullet"><span class="by">sixfiveotwo</span><span>|</span><a href="#42093668">root</a><span>|</span><a href="#42094141">parent</a><span>|</span><a href="#42095878">prev</a><span>|</span><a href="#42094407">next</a><span>|</span><label class="collapse" for="c-42094237">[-]</label><label class="expand" for="c-42094237">[4 more]</label></div><br/><div class="children"><div class="content">&gt; I spend much longer trying to figure out how to get 1-trit training to work and I never could.<p>What did you try? What were the research directions at the time?</div><br/><div id="42094542" class="c"><input type="checkbox" id="c-42094542" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42093668">root</a><span>|</span><a href="#42094237">parent</a><span>|</span><a href="#42094407">next</a><span>|</span><label class="collapse" for="c-42094542">[-]</label><label class="expand" for="c-42094542">[3 more]</label></div><br/><div class="children"><div class="content">This is a big question that needs a research paper worth of explanation. Feel free to email me if you care enough to have a more in-depth discussion.</div><br/><div id="42096307" class="c"><input type="checkbox" id="c-42096307" checked=""/><div class="controls bullet"><span class="by">sixfiveotwo</span><span>|</span><a href="#42093668">root</a><span>|</span><a href="#42094542">parent</a><span>|</span><a href="#42094407">next</a><span>|</span><label class="collapse" for="c-42096307">[-]</label><label class="expand" for="c-42096307">[2 more]</label></div><br/><div class="children"><div class="content">Sorry, I understand it was a bit intrusively direct. To bring some context, I toyed a little with neural networks a few years ago and wondered myself about this topic of training a so called quantized network (I wanted to write a small multilayer perceptron based library parameterized by the coefficient type - floating point or integer of different precision), but didn&#x27;t implement it. Since you mentioned your own work in that area, it picked my interest, but I don&#x27;t want to waste your time unnecessarily.</div><br/><div id="42098458" class="c"><input type="checkbox" id="c-42098458" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42093668">root</a><span>|</span><a href="#42096307">parent</a><span>|</span><a href="#42094407">next</a><span>|</span><label class="collapse" for="c-42098458">[-]</label><label class="expand" for="c-42098458">[1 more]</label></div><br/><div class="children"><div class="content">Someone posted a paper that I didn&#x27;t know about, but goes through pretty much all the work I did in the space: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42095999">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42095999</a><p>It&#x27;s missing the colourful commentary that I&#x27;d usually give, but alas, we can&#x27;t have it all.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42094407" class="c"><input type="checkbox" id="c-42094407" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#42093668">root</a><span>|</span><a href="#42093896">parent</a><span>|</span><a href="#42094028">prev</a><span>|</span><a href="#42093981">next</a><span>|</span><label class="collapse" for="c-42094407">[-]</label><label class="expand" for="c-42094407">[6 more]</label></div><br/><div class="children"><div class="content">That&#x27;s wrong. I don&#x27;t know where you got that information from, because it is literally the opposite of what is shown in the Microsoft paper mentioned above. They explicitly introduced this extreme quantization during training from scratch and show how it can be made stable.</div><br/><div id="42094525" class="c"><input type="checkbox" id="c-42094525" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42093668">root</a><span>|</span><a href="#42094407">parent</a><span>|</span><a href="#42093981">next</a><span>|</span><label class="collapse" for="c-42094525">[-]</label><label class="expand" for="c-42094525">[5 more]</label></div><br/><div class="children"><div class="content">I got it from section 2.2<p>&gt; The number of model parameters is slightly higher in the BitLinear setting, as we both have 1.58-bit weights as well as the 16-bit shadow weights. However, this fact does not change the number of trainable&#x2F;optimized parameters in practice.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2407.09527v1" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2407.09527v1</a></div><br/><div id="42095999" class="c"><input type="checkbox" id="c-42095999" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#42093668">root</a><span>|</span><a href="#42094525">parent</a><span>|</span><a href="#42098550">next</a><span>|</span><label class="collapse" for="c-42095999">[-]</label><label class="expand" for="c-42095999">[2 more]</label></div><br/><div class="children"><div class="content">Exactly as xnornet was doing way back in 2016 - shadow 32bit weights, quantized to 1 bit during the forward pass.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1603.05279" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1603.05279</a><p>I personally have a pretty negative opinion of the bitnet paper.</div><br/><div id="42097976" class="c"><input type="checkbox" id="c-42097976" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42093668">root</a><span>|</span><a href="#42095999">parent</a><span>|</span><a href="#42098550">next</a><span>|</span><label class="collapse" for="c-42097976">[-]</label><label class="expand" for="c-42097976">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the citation, I did my work in the area around 2014 and never looked back. That&#x27;s a very good summary of the state of the field as I remember it.</div><br/></div></div></div></div><div id="42098550" class="c"><input type="checkbox" id="c-42098550" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#42093668">root</a><span>|</span><a href="#42094525">parent</a><span>|</span><a href="#42095999">prev</a><span>|</span><a href="#42093981">next</a><span>|</span><label class="collapse" for="c-42098550">[-]</label><label class="expand" for="c-42098550">[2 more]</label></div><br/><div class="children"><div class="content">What? That&#x27;s the wrong paper. It is not even from Microsoft. This is it: <a href="https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;publication&#x2F;bitnet-scaling-1-bit-transformers-for-large-language-models&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;publication&#x2F;bitnet-...</a><p>&gt;we introduce BitLinear as a drop-in replacement of the the nn.Linear layer in order to train 1-bit weights from scratch</div><br/><div id="42098737" class="c"><input type="checkbox" id="c-42098737" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42093668">root</a><span>|</span><a href="#42098550">parent</a><span>|</span><a href="#42093981">next</a><span>|</span><label class="collapse" for="c-42098737">[-]</label><label class="expand" for="c-42098737">[1 more]</label></div><br/><div class="children"><div class="content">Section 2.2 from your paper, with less clarity and more obfuscation:<p>&gt;While the weights and the activations are quantized to low precision, the gradients and the optimizer states are stored in high precision to ensure training stability and accuracy. Following the previous work [ LSL+21 ], we maintain a latent weight in a high-precision format for the learnable parameters to accumulate the parameter updates. The latent weights are binarized on the fly during the forward pass and never used for the inference process.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2310.11453" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2310.11453</a><p>The other paper had a much nicer and clearer introduction to bitlinear than the original Microsoft paper, which is why I used it. Uncharitably you might say that they aren&#x27;t hiding the lead 10 paragraphs in.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42093981" class="c"><input type="checkbox" id="c-42093981" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#42093668">prev</a><span>|</span><a href="#42093113">next</a><span>|</span><label class="collapse" for="c-42093981">[-]</label><label class="expand" for="c-42093981">[6 more]</label></div><br/><div class="children"><div class="content">Can someone explain this sentence from the article:<p><pre><code>  Diffusion models, however, are computationally bound, even for single batches, so quantizing weights alone yields limited gains.</code></pre></div><br/><div id="42094016" class="c"><input type="checkbox" id="c-42094016" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#42093981">parent</a><span>|</span><a href="#42093992">next</a><span>|</span><label class="collapse" for="c-42094016">[-]</label><label class="expand" for="c-42094016">[1 more]</label></div><br/><div class="children"><div class="content">Diffusion requires a lot more computation to get results compared to transformers. Naively when I&#x27;m running a transformer locally I get about 30% GPU utilization, when I&#x27;m running a diffusion model I&#x27;m getting 100%.<p>This means that the only saving you&#x27;re getting in speed for a diffusion model is being able to do more effective flops since the floats are smaller, e.g. instead of doing one 32bit multiplication, you&#x27;re doing 8 4bit ones.<p>By comparison for transformers you not only gain the flop increase, but also the improvement in memory shuffling that they do, e.g. it also takes you 8 times less time to load the memory into working memory from vram.<p>The above is a vast over simplification and in practice will have more asterisks than you can shake a stick at.</div><br/></div></div><div id="42093992" class="c"><input type="checkbox" id="c-42093992" checked=""/><div class="controls bullet"><span class="by">flutetornado</span><span>|</span><a href="#42093981">parent</a><span>|</span><a href="#42094016">prev</a><span>|</span><a href="#42098462">next</a><span>|</span><label class="collapse" for="c-42093992">[-]</label><label class="expand" for="c-42093992">[3 more]</label></div><br/><div class="children"><div class="content">GPU workloads are either compute bound (floating point operations) or memory bound (bytes being transferred across memory hierarchy.)<p>Quantizing in general helps with the memory bottleneck but does not help in reducing computational costs, so it’s not as useful for improving performance of diffusion models, that’s what it’s saying.</div><br/><div id="42094140" class="c"><input type="checkbox" id="c-42094140" checked=""/><div class="controls bullet"><span class="by">pkAbstract</span><span>|</span><a href="#42093981">root</a><span>|</span><a href="#42093992">parent</a><span>|</span><a href="#42098462">next</a><span>|</span><label class="collapse" for="c-42094140">[-]</label><label class="expand" for="c-42094140">[2 more]</label></div><br/><div class="children"><div class="content">Exactly. The smaller bit widths from quantization might marginally decrease the compute required for each operation, but they do not reduce the overall volume of operations. So, the effect of quantization is generally more impactful on memory use than compute.</div><br/><div id="42094807" class="c"><input type="checkbox" id="c-42094807" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#42093981">root</a><span>|</span><a href="#42094140">parent</a><span>|</span><a href="#42098462">next</a><span>|</span><label class="collapse" for="c-42094807">[-]</label><label class="expand" for="c-42094807">[1 more]</label></div><br/><div class="children"><div class="content">Except in this case they quantized both the parameters and the activations leading to decreased compute time too.</div><br/></div></div></div></div></div></div><div id="42098462" class="c"><input type="checkbox" id="c-42098462" checked=""/><div class="controls bullet"><span class="by">boulos</span><span>|</span><a href="#42093981">parent</a><span>|</span><a href="#42093992">prev</a><span>|</span><a href="#42093113">next</a><span>|</span><label class="collapse" for="c-42098462">[-]</label><label class="expand" for="c-42098462">[1 more]</label></div><br/><div class="children"><div class="content">The next sentence there:<p>&gt; To achieve measured speedups, both weights and activations must be quantized to the same bit width; otherwise, the lower precision is upcast during computation, negating any performance benefits.<p>tries to explain that.<p>What it means though is that if you only store the inputs in lower precision, but still upcast to say bf16 or fp32 to perform the operation, you&#x27;re not getting any computational speedup. In fact, you&#x27;re paying for upconverting and then downconverting afterwards.</div><br/></div></div></div></div><div id="42094023" class="c"><input type="checkbox" id="c-42094023" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#42093113">prev</a><span>|</span><a href="#42094330">next</a><span>|</span><label class="collapse" for="c-42094023">[-]</label><label class="expand" for="c-42094023">[2 more]</label></div><br/><div class="children"><div class="content">But doesn&#x27;t quantization give worse results? Don&#x27;t you trade quality for memory footprint?</div><br/><div id="42094689" class="c"><input type="checkbox" id="c-42094689" checked=""/><div class="controls bullet"><span class="by">timnetworks</span><span>|</span><a href="#42094023">parent</a><span>|</span><a href="#42094330">next</a><span>|</span><label class="collapse" for="c-42094689">[-]</label><label class="expand" for="c-42094689">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re saying this method essential does not, even when mixed with low rank models on top. &quot;Notably, while the original BF16 model
requires per-layer CPU offloading on the 16GB laptop 4090, our INT4 model fits entirely in GPU memory, resulting in a 10.1× speedup by avoiding offloading.&quot;<p>This is the whole magic, the rest of the workflow doesn&#x27;t need to unload and flush memory, causing big delays for jobs.</div><br/></div></div></div></div><div id="42094330" class="c"><input type="checkbox" id="c-42094330" checked=""/><div class="controls bullet"><span class="by">scottmas</span><span>|</span><a href="#42094023">prev</a><span>|</span><label class="collapse" for="c-42094330">[-]</label><label class="expand" for="c-42094330">[3 more]</label></div><br/><div class="children"><div class="content">Possible to run this in ComfyUI?</div><br/><div id="42094629" class="c"><input type="checkbox" id="c-42094629" checked=""/><div class="controls bullet"><span class="by">vergessenmir</span><span>|</span><a href="#42094330">parent</a><span>|</span><a href="#42097014">next</a><span>|</span><label class="collapse" for="c-42094629">[-]</label><label class="expand" for="c-42094629">[1 more]</label></div><br/><div class="children"><div class="content">The repo has sample code and it is fairly easy to create a node that will do it.<p>You won&#x27;t however have access to usual sampler, latent image, Lora nodes to do anything beyond basic t2i</div><br/></div></div><div id="42097014" class="c"><input type="checkbox" id="c-42097014" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#42094330">parent</a><span>|</span><a href="#42094629">prev</a><span>|</span><label class="collapse" for="c-42097014">[-]</label><label class="expand" for="c-42097014">[1 more]</label></div><br/><div class="children"><div class="content">Why? There is nothing to customize with Flux.</div><br/></div></div></div></div></div></div></div></div></div></body></html>