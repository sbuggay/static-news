<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1709888451440" as="style"/><link rel="stylesheet" href="styles.css?v=1709888451440"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html">Fine tune a 70B language model at home</a> <span class="domain">(<a href="https://www.answer.ai">www.answer.ai</a>)</span></div><div class="subtext"><span>jph00</span> | <span>40 comments</span></div><br/><div><div id="39638211" class="c"><input type="checkbox" id="c-39638211" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39639083">next</a><span>|</span><label class="collapse" for="c-39638211">[-]</label><label class="expand" for="c-39638211">[1 more]</label></div><br/><div class="children"><div class="content">One thing I forgot to mention in the post which I think is kinda cool: at the NeurIPS Efficiency Challenge this year, where Tim Dettmers and I both did keynotes, every single top-ranked entry used QLoRA! The challenge was to create the most accurate model on a single GPU in 24 hours.<p>I think that is a great example of how important and useful QLoRA is. Maybe we should run a dual-GPU challenge next time not that multi-GPU is working...</div><br/></div></div><div id="39639083" class="c"><input type="checkbox" id="c-39639083" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39638211">prev</a><span>|</span><a href="#39638548">next</a><span>|</span><label class="collapse" for="c-39639083">[-]</label><label class="expand" for="c-39639083">[5 more]</label></div><br/><div class="children"><div class="content">This is great, but one thing I really hoped would come sooner is fast training on Metal. As things are, you can get an M1&#x2F;M2 Ultra (~800 Gb&#x2F;s memory bandwidth; for comparison, RTX 4090 is ~1050 Gb&#x2F;s) Mac Studio with 128Gb RAM for ~$3500. For large model inference, this is already way more affordable than stacking GPUs while being &quot;fast enough&quot;, but training solutions are basically non-existent. I do wonder why; it feels like a low-hanging fruit.</div><br/><div id="39639100" class="c"><input type="checkbox" id="c-39639100" checked=""/><div class="controls bullet"><span class="by">sqreept</span><span>|</span><a href="#39639083">parent</a><span>|</span><a href="#39639105">next</a><span>|</span><label class="collapse" for="c-39639100">[-]</label><label class="expand" for="c-39639100">[1 more]</label></div><br/><div class="children"><div class="content">M1, M2, M3 still have very low number of GPU cores. Apple should release some better hardware to take advantage of their recently released MLX library.</div><br/></div></div><div id="39639105" class="c"><input type="checkbox" id="c-39639105" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#39639083">parent</a><span>|</span><a href="#39639100">prev</a><span>|</span><a href="#39638548">next</a><span>|</span><label class="collapse" for="c-39639105">[-]</label><label class="expand" for="c-39639105">[3 more]</label></div><br/><div class="children"><div class="content">Compute limited - an m2 ultra has 27 tflops, a 4090 80+</div><br/><div id="39639240" class="c"><input type="checkbox" id="c-39639240" checked=""/><div class="controls bullet"><span class="by">yumraj</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39639105">parent</a><span>|</span><a href="#39639196">next</a><span>|</span><label class="collapse" for="c-39639240">[-]</label><label class="expand" for="c-39639240">[1 more]</label></div><br/><div class="children"><div class="content">So it should just take longer..</div><br/></div></div><div id="39639196" class="c"><input type="checkbox" id="c-39639196" checked=""/><div class="controls bullet"><span class="by">erichocean</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39639105">parent</a><span>|</span><a href="#39639240">prev</a><span>|</span><a href="#39638548">next</a><span>|</span><label class="collapse" for="c-39639196">[-]</label><label class="expand" for="c-39639196">[1 more]</label></div><br/><div class="children"><div class="content">Memory limited - an m2 ultra has &gt;150GiB, a 4090 24GiB</div><br/></div></div></div></div></div></div><div id="39638548" class="c"><input type="checkbox" id="c-39638548" checked=""/><div class="controls bullet"><span class="by">jamesblonde</span><span>|</span><a href="#39639083">prev</a><span>|</span><a href="#39638526">next</a><span>|</span><label class="collapse" for="c-39638548">[-]</label><label class="expand" for="c-39638548">[2 more]</label></div><br/><div class="children"><div class="content">This is a fantastic breakthrough for those of us who fine-tune LLMs on limited hardware budgets.<p>I was curious about the choice of FSDP over DeepSpeed. I have been using Axolotl for fine-tuning, and FSDP has been broken there, whilst DeepSpeed is rock solid. Why FSDP over DeepSpeed jph00?</div><br/><div id="39638763" class="c"><input type="checkbox" id="c-39638763" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39638548">parent</a><span>|</span><a href="#39638526">next</a><span>|</span><label class="collapse" for="c-39638763">[-]</label><label class="expand" for="c-39638763">[1 more]</label></div><br/><div class="children"><div class="content">DeepSpeed has more features than FSDP, but it&#x27;s much more complex to hack on -- FSDP is written directly in python using calls to the PyTorch library, whereas DeepSpeed is 20% C++ and 10% CUDA (according to the GitHub stats).<p>We&#x27;ve found that FSDP works just as well for our needs, and we appreciated the increased &quot;hackability&quot;.<p>(Axolotl is terrific BTW. I hadn&#x27;t heard of problems with it with FSDP before -- I&#x27;ll see if that&#x27;s something we can help with.)</div><br/></div></div></div></div><div id="39638526" class="c"><input type="checkbox" id="c-39638526" checked=""/><div class="controls bullet"><span class="by">yalok</span><span>|</span><a href="#39638548">prev</a><span>|</span><a href="#39638896">next</a><span>|</span><label class="collapse" for="c-39638526">[-]</label><label class="expand" for="c-39638526">[4 more]</label></div><br/><div class="children"><div class="content">Have you guys looked at using sparsification? It would probably require true re-training of the foundation model, to go at high sparse ratios (say 90% weights excluded), which could be done  once on expensive GPU - but fine tuning such sparse models would require less RAM hopefully.<p>The trick with getting more benefit from sparse approach  is to do block sparse (iirc, Tim Dettmers used to work on this as well, a few years ago), but large block size (say 16x16) would require much longer retraining to recover for the lost accuracy…</div><br/><div id="39638793" class="c"><input type="checkbox" id="c-39638793" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39638526">parent</a><span>|</span><a href="#39639010">next</a><span>|</span><label class="collapse" for="c-39638793">[-]</label><label class="expand" for="c-39638793">[1 more]</label></div><br/><div class="children"><div class="content">Yes, sparsification is another useful approach for higher efficiency, although block sparse kernels are pretty complex to work with -- especially when combined with quantization and LoRA! Most of the sparsity papers I&#x27;ve seen use &quot;structured&quot; sparsity; i.e removing layers, attention heads, and features. But the upside from this seems somewhat limited so far.</div><br/></div></div><div id="39639010" class="c"><input type="checkbox" id="c-39639010" checked=""/><div class="controls bullet"><span class="by">AhtiK</span><span>|</span><a href="#39638526">parent</a><span>|</span><a href="#39638793">prev</a><span>|</span><a href="#39638896">next</a><span>|</span><label class="collapse" for="c-39639010">[-]</label><label class="expand" for="c-39639010">[2 more]</label></div><br/><div class="children"><div class="content">Has anyone seen an implementation of &#x27;SpQR: A Sparse-Quantized Representation,&#x27; published in June 2023 by Tim Dettmers et al.? <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.03078" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.03078</a></div><br/><div id="39639052" class="c"><input type="checkbox" id="c-39639052" checked=""/><div class="controls bullet"><span class="by">AhtiK</span><span>|</span><a href="#39638526">root</a><span>|</span><a href="#39639010">parent</a><span>|</span><a href="#39638896">next</a><span>|</span><label class="collapse" for="c-39639052">[-]</label><label class="expand" for="c-39639052">[1 more]</label></div><br/><div class="children"><div class="content">Found it from <a href="https:&#x2F;&#x2F;github.com&#x2F;Vahe1994&#x2F;SpQR">https:&#x2F;&#x2F;github.com&#x2F;Vahe1994&#x2F;SpQR</a>
Was somehow expecting it to be at <a href="https:&#x2F;&#x2F;github.com&#x2F;TimDettmers&#x2F;bitsandbytes">https:&#x2F;&#x2F;github.com&#x2F;TimDettmers&#x2F;bitsandbytes</a>. My bad.</div><br/></div></div></div></div></div></div><div id="39638896" class="c"><input type="checkbox" id="c-39638896" checked=""/><div class="controls bullet"><span class="by">tbenst</span><span>|</span><a href="#39638526">prev</a><span>|</span><a href="#39638233">next</a><span>|</span><label class="collapse" for="c-39638896">[-]</label><label class="expand" for="c-39638896">[1 more]</label></div><br/><div class="children"><div class="content">Very interesting but hard to interpret until the performance numbers &#x2F; benchmarks are available. I can already fine-tune a 70B language model at home using CPU + RAM, but it would be so slow as to be almost totally impractical (~20x slower than GPU). It would be great to see a comparison to eg 8 x A100 (available for $32&#x2F;hr on AWS on-demand) and also CPU + RAM. Presumably it’s somewhere in between, but hard to predict where!</div><br/></div></div><div id="39638233" class="c"><input type="checkbox" id="c-39638233" checked=""/><div class="controls bullet"><span class="by">pella</span><span>|</span><a href="#39638896">prev</a><span>|</span><a href="#39635617">next</a><span>|</span><label class="collapse" for="c-39638233">[-]</label><label class="expand" for="c-39638233">[2 more]</label></div><br/><div class="children"><div class="content">&gt; the ability to use multiple GPUs with QLoRA training.<p>Thorough article!<p>Question: What&#x27;s your opinion on:<p>- How viable will NVIDIA&#x27;s consumer cards be in the long run?<p>- Besides <a href="https:&#x2F;&#x2F;tinygrad.org" rel="nofollow">https:&#x2F;&#x2F;tinygrad.org</a>, what other cost-effective future alternatives could there be?</div><br/><div id="39638679" class="c"><input type="checkbox" id="c-39638679" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#39638233">parent</a><span>|</span><a href="#39635617">next</a><span>|</span><label class="collapse" for="c-39638679">[-]</label><label class="expand" for="c-39638679">[1 more]</label></div><br/><div class="children"><div class="content">Unsloth (mentioned in the Answer.AI post) is planning multi-GPU support in a future release.</div><br/></div></div></div></div><div id="39635617" class="c"><input type="checkbox" id="c-39635617" checked=""/><div class="controls bullet"><span class="by">ricopags</span><span>|</span><a href="#39638233">prev</a><span>|</span><a href="#39638606">next</a><span>|</span><label class="collapse" for="c-39635617">[-]</label><label class="expand" for="c-39635617">[2 more]</label></div><br/><div class="children"><div class="content">This is such exciting news! Huge thanks to you for your continued work in making sense of AI.<p>I wonder if the recent Bitnet 1.58 paper [the use of ternary bits in lieu of fp&#x2F;int] might be an advancement that could further reduce the computation required for inference?</div><br/><div id="39638160" class="c"><input type="checkbox" id="c-39638160" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39635617">parent</a><span>|</span><a href="#39638606">next</a><span>|</span><label class="collapse" for="c-39638160">[-]</label><label class="expand" for="c-39638160">[1 more]</label></div><br/><div class="children"><div class="content">Yes, along with the many other &lt;4 bit quant methods recently developed -- there&#x27;s been a wonderful boom in low-bit quant methods in the last 6 months, and we&#x27;ve got our own ideas for taking them further too. Along with QLoRA&#x2F;FSDP, we&#x27;re likely to see big advances in model training this year on consumer hardware.</div><br/></div></div></div></div><div id="39638606" class="c"><input type="checkbox" id="c-39638606" checked=""/><div class="controls bullet"><span class="by">itsgrimetime</span><span>|</span><a href="#39635617">prev</a><span>|</span><a href="#39638127">next</a><span>|</span><label class="collapse" for="c-39638606">[-]</label><label class="expand" for="c-39638606">[2 more]</label></div><br/><div class="children"><div class="content">Would be cool to build an “LLM@home” project like folding@home or SETI@home (rip), where tons of folks could donate their GPUs and train something huge and FOSS. I don’t know enough about how these models are trained though. Could it be chunked up and distributed in that way, then stitched&#x2F;merged back together?</div><br/><div id="39638639" class="c"><input type="checkbox" id="c-39638639" checked=""/><div class="controls bullet"><span class="by">fho</span><span>|</span><a href="#39638606">parent</a><span>|</span><a href="#39638127">next</a><span>|</span><label class="collapse" for="c-39638639">[-]</label><label class="expand" for="c-39638639">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;stablehorde.net&#x2F;" rel="nofollow">https:&#x2F;&#x2F;stablehorde.net&#x2F;</a> comes somewhat close.</div><br/></div></div></div></div><div id="39638127" class="c"><input type="checkbox" id="c-39638127" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#39638606">prev</a><span>|</span><a href="#39638435">next</a><span>|</span><label class="collapse" for="c-39638127">[-]</label><label class="expand" for="c-39638127">[1 more]</label></div><br/><div class="children"><div class="content">Nice, I tried to use QLoRA+FSDP in the past with litgpt and obviously at that time it did not work. This is very useful!</div><br/></div></div><div id="39638435" class="c"><input type="checkbox" id="c-39638435" checked=""/><div class="controls bullet"><span class="by">keeptrying</span><span>|</span><a href="#39638127">prev</a><span>|</span><a href="#39638279">next</a><span>|</span><label class="collapse" for="c-39638435">[-]</label><label class="expand" for="c-39638435">[1 more]</label></div><br/><div class="children"><div class="content">If you are gonna be doing stuff like this I’m damn excited for answer.ai!<p>It’ll be the first time we’ll have someone who knows AI create leverage to open source it.<p>Way to go!</div><br/></div></div><div id="39638279" class="c"><input type="checkbox" id="c-39638279" checked=""/><div class="controls bullet"><span class="by">carbocation</span><span>|</span><a href="#39638435">prev</a><span>|</span><a href="#39635640">next</a><span>|</span><label class="collapse" for="c-39638279">[-]</label><label class="expand" for="c-39638279">[2 more]</label></div><br/><div class="children"><div class="content">I wonder whether LoRAs could be useful for U-Net training. Especially thinking of CNN-based U-Net models with pre-trained encoders (but randomly initialized decoders). At least, it seems possible that normal weight updates on the decoder and LoRA training on the encoder could improve efficiency.</div><br/><div id="39638730" class="c"><input type="checkbox" id="c-39638730" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39638279">parent</a><span>|</span><a href="#39635640">next</a><span>|</span><label class="collapse" for="c-39638730">[-]</label><label class="expand" for="c-39638730">[1 more]</label></div><br/><div class="children"><div class="content">Diffusion unet has an &quot;extended&quot; version nowadays that applies to the resnet part as well as the cross-attention: <a href="https:&#x2F;&#x2F;github.com&#x2F;cloneofsimo&#x2F;lora">https:&#x2F;&#x2F;github.com&#x2F;cloneofsimo&#x2F;lora</a></div><br/></div></div></div></div><div id="39635640" class="c"><input type="checkbox" id="c-39635640" checked=""/><div class="controls bullet"><span class="by">artninja1988</span><span>|</span><a href="#39638279">prev</a><span>|</span><a href="#39638753">next</a><span>|</span><label class="collapse" for="c-39635640">[-]</label><label class="expand" for="c-39635640">[11 more]</label></div><br/><div class="children"><div class="content">So, as I understand it, this is for finetuning a preexisting llm? So not actually training one from scratch. I guess that would be too much to ask for. Nonetheless, cheers to Jeremy and the gang for the work.</div><br/><div id="39635991" class="c"><input type="checkbox" id="c-39635991" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39635640">parent</a><span>|</span><a href="#39638108">next</a><span>|</span><label class="collapse" for="c-39635991">[-]</label><label class="expand" for="c-39635991">[9 more]</label></div><br/><div class="children"><div class="content">For now, it&#x27;s for finetuning.<p>The issue of to what degree it might be possible to train a model from scratch using QLoRA is still an open question. The relora paper showed that it can work in some situations, but attempts to scale it up were unsuccessful. The recent DoRA paper perhaps might allow a &quot;re-DoRA&quot; approach to work. If so, that could be combined with quantization to do &quot;re-QDoRA&quot;!</div><br/><div id="39639109" class="c"><input type="checkbox" id="c-39639109" checked=""/><div class="controls bullet"><span class="by">hantusk</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39635991">parent</a><span>|</span><a href="#39638289">next</a><span>|</span><label class="collapse" for="c-39639109">[-]</label><label class="expand" for="c-39639109">[2 more]</label></div><br/><div class="children"><div class="content">Digging into the low rank structure of the gradients, instead of the weights seems like a promising direction for training from scratch with less memory requirements: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;AnimaAnandkumar&#x2F;status&#x2F;1765613815146893348" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;AnimaAnandkumar&#x2F;status&#x2F;17656138151468933...</a></div><br/><div id="39639261" class="c"><input type="checkbox" id="c-39639261" checked=""/><div class="controls bullet"><span class="by">hantusk</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39639109">parent</a><span>|</span><a href="#39638289">next</a><span>|</span><label class="collapse" for="c-39639261">[-]</label><label class="expand" for="c-39639261">[1 more]</label></div><br/><div class="children"><div class="content">Simo linked some older papers with this same idea: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;cloneofsimo&#x2F;status&#x2F;1765796493955674286" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;cloneofsimo&#x2F;status&#x2F;1765796493955674286</a></div><br/></div></div></div></div><div id="39638289" class="c"><input type="checkbox" id="c-39638289" checked=""/><div class="controls bullet"><span class="by">qsi</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39635991">parent</a><span>|</span><a href="#39639109">prev</a><span>|</span><a href="#39638108">next</a><span>|</span><label class="collapse" for="c-39638289">[-]</label><label class="expand" for="c-39638289">[6 more]</label></div><br/><div class="children"><div class="content">The headline and introduction on the linked page say &quot;You can now train a 70b language model at home. We’re releasing an open source system, based on FSDP and QLoRA, that can train a 70b model on two 24GB GPUs.&quot;<p>How does &quot;fine tuning&quot; differ from &quot;training?&quot; Reading the linked article I had assumed I could create my own trained LLM at home with two 24GB GPUs.</div><br/><div id="39638736" class="c"><input type="checkbox" id="c-39638736" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39638289">parent</a><span>|</span><a href="#39638580">next</a><span>|</span><label class="collapse" for="c-39638736">[-]</label><label class="expand" for="c-39638736">[1 more]</label></div><br/><div class="children"><div class="content">The article actually sneaks in a footnote that answers this (<a href="https:&#x2F;&#x2F;www.answer.ai&#x2F;posts&#x2F;2024-03-06-fsdp-qlora.html#fn1" rel="nofollow">https:&#x2F;&#x2F;www.answer.ai&#x2F;posts&#x2F;2024-03-06-fsdp-qlora.html#fn1</a>): &quot;Throughout this article “training” can refer to either pre-training, or fine-tuning&quot;.<p>(Generally, we&#x27;ve told students at fast.ai since 2017 that they should almost never be starting from random weights -- most of the time it&#x27;s best to start with a pretrained model and fine-tune that, even if it&#x27;s from a somewhat different domain to the problem you&#x27;re working on.)</div><br/></div></div><div id="39638580" class="c"><input type="checkbox" id="c-39638580" checked=""/><div class="controls bullet"><span class="by">keremturgutlu</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39638289">parent</a><span>|</span><a href="#39638736">prev</a><span>|</span><a href="#39638593">next</a><span>|</span><label class="collapse" for="c-39638580">[-]</label><label class="expand" for="c-39638580">[2 more]</label></div><br/><div class="children"><div class="content">You most definitely can, the main difference is that only partial ~2% of the parameters get updated during training. Say you start from a model like llama-70B which already knows english and has some world knowledge based on its pretraining dataset. It might not be ideal for drastic domain shifts, such as adapting a model to learn new languages (which might require a new tokenizer and model embeddings) but still might be possible to some extent.</div><br/><div id="39638634" class="c"><input type="checkbox" id="c-39638634" checked=""/><div class="controls bullet"><span class="by">qsi</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39638580">parent</a><span>|</span><a href="#39638593">next</a><span>|</span><label class="collapse" for="c-39638634">[-]</label><label class="expand" for="c-39638634">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for clarifying. I have been wanting to dip my toes into LLMs at home but obviously I have a steep learning curve ahead of me, and would need considerably beefier hardware!</div><br/></div></div></div></div><div id="39638593" class="c"><input type="checkbox" id="c-39638593" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39638289">parent</a><span>|</span><a href="#39638580">prev</a><span>|</span><a href="#39638108">next</a><span>|</span><label class="collapse" for="c-39638593">[-]</label><label class="expand" for="c-39638593">[2 more]</label></div><br/><div class="children"><div class="content">You can take an existing 70B model and train it to do a more specific task. You&#x27;re teaching it the task but you&#x27;re relying on a foundation model for the base understanding of the world&#x2F;words&#x2F;etc.</div><br/><div id="39638637" class="c"><input type="checkbox" id="c-39638637" checked=""/><div class="controls bullet"><span class="by">qsi</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39638593">parent</a><span>|</span><a href="#39638108">next</a><span>|</span><label class="collapse" for="c-39638637">[-]</label><label class="expand" for="c-39638637">[1 more]</label></div><br/><div class="children"><div class="content">OK, that makes sense. Thank you!</div><br/></div></div></div></div></div></div></div></div><div id="39638108" class="c"><input type="checkbox" id="c-39638108" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#39635640">parent</a><span>|</span><a href="#39635991">prev</a><span>|</span><a href="#39638753">next</a><span>|</span><label class="collapse" for="c-39638108">[-]</label><label class="expand" for="c-39638108">[1 more]</label></div><br/><div class="children"><div class="content">Lit-GPT is what I have been using to pretrain models at home: <a href="https:&#x2F;&#x2F;github.com&#x2F;Lightning-AI&#x2F;litgpt">https:&#x2F;&#x2F;github.com&#x2F;Lightning-AI&#x2F;litgpt</a>
Using the openwebtext example, I can train a 700M param model to 2.6 loss in a few days on dual 4090s. Pretty awesome!</div><br/></div></div></div></div><div id="39638753" class="c"><input type="checkbox" id="c-39638753" checked=""/><div class="controls bullet"><span class="by">lbj</span><span>|</span><a href="#39635640">prev</a><span>|</span><a href="#39638456">next</a><span>|</span><label class="collapse" for="c-39638753">[-]</label><label class="expand" for="c-39638753">[1 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t believe they didn&#x27;t name this Qolor</div><br/></div></div><div id="39638456" class="c"><input type="checkbox" id="c-39638456" checked=""/><div class="controls bullet"><span class="by">g42gregory</span><span>|</span><a href="#39638753">prev</a><span>|</span><a href="#39638375">next</a><span>|</span><label class="collapse" for="c-39638456">[-]</label><label class="expand" for="c-39638456">[1 more]</label></div><br/><div class="children"><div class="content">This is brilliant. Thank you for doing his!</div><br/></div></div><div id="39638375" class="c"><input type="checkbox" id="c-39638375" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#39638456">prev</a><span>|</span><a href="#39635520">next</a><span>|</span><label class="collapse" for="c-39638375">[-]</label><label class="expand" for="c-39638375">[2 more]</label></div><br/><div class="children"><div class="content">If they can continuously train it, it could be better than a large context as this is how a AI OS would need to work when you have constant updates to your files</div><br/><div id="39638640" class="c"><input type="checkbox" id="c-39638640" checked=""/><div class="controls bullet"><span class="by">padolsey</span><span>|</span><a href="#39638375">parent</a><span>|</span><a href="#39635520">next</a><span>|</span><label class="collapse" for="c-39638640">[-]</label><label class="expand" for="c-39638640">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think you’d be fine-tuning a whole model in such cases. That seems over the top, no? I assume you’d get sufficiently far with big context windows, vector search, RAG. Etc.</div><br/></div></div></div></div></div></div></div></div></div></body></html>