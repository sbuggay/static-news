<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1709974863926" as="style"/><link rel="stylesheet" href="styles.css?v=1709974863926"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html">Fine tune a 70B language model at home</a> <span class="domain">(<a href="https://www.answer.ai">www.answer.ai</a>)</span></div><div class="subtext"><span>jph00</span> | <span>149 comments</span></div><br/><div><div id="39639998" class="c"><input type="checkbox" id="c-39639998" checked=""/><div class="controls bullet"><span class="by">llmzero</span><span>|</span><a href="#39638211">next</a><span>|</span><label class="collapse" for="c-39639998">[-]</label><label class="expand" for="c-39639998">[13 more]</label></div><br/><div class="children"><div class="content">I liked that you link to renting  a dual 24GPU for 0.60cents&#x2F;hour, but how long could it takes to fine tune a 70b model using your system (4 bits for weights)?<p>If I were a consumer I would be interested in the final price of fine tuning, for example a table with model size, training size, cost of training,  and expected loss of quality with this technology.<p>One obvious question: Can you apply your technology with the recent  (-1,0,1) encoding?, I think you will answers that the (-1,0,1) model is not available and you can&#x27;t try it, but my question is whether once&#x2F;if that model is available answer.ai will be able to use the same technology that this post to fine tune a big model in two very small GPUs, and then I should ask for a new table with cost&#x2F;benefits analysis.<p>Edited:  I should add that I find this kind of work very useful for enhancing individual users like me to be able to compete in the applications of LLM market, this is great work and along the lines of the book &quot;from zero to one&quot; (not that I like or dislike the author) to solve the kind of problem that nobody is trying to solve.<p>Edited: Now that I have a total of 23 points in HN, I will change my password to some random one, just to cure my desire to look for votes, and try to make some work, and again some tomorrow create a new presence in HN.</div><br/><div id="39640517" class="c"><input type="checkbox" id="c-39640517" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#39639998">parent</a><span>|</span><a href="#39643643">next</a><span>|</span><label class="collapse" for="c-39640517">[-]</label><label class="expand" for="c-39640517">[5 more]</label></div><br/><div class="children"><div class="content">On how long, finetuning is influenced by your dataset size (more = slower), sequence length since attention is O(N^2), data movement etc and most important is how many steps you want to take. For QLoRA, some runs can do a few hundred steps which can complete in minutes to 1 hour. Too many can overfit. So being able to fit it on consumer GPUs can be very cost effective.<p>On the 1.58bit paper, from what I understand, this requires a total retraining from scratch. Hopefully the researchers will open source their weights :)<p>On the technicals, weights are encoded in (-1, 0, 1), whilst QLoRA uses a 4bit dynamic mapping of 16 numbers. The only change required would be the torch.matmul(X, W) step, where it&#x27;ll be torch.bitlinear_matmul(X, W). Before with QLoRA, one has to do torch.matmul(X, dequantize(W)). So one has to implement torch.bitlinear_matmul. The backward is torch.bitlinear_matmul(dY, W.T).</div><br/><div id="39643141" class="c"><input type="checkbox" id="c-39643141" checked=""/><div class="controls bullet"><span class="by">miohtama</span><span>|</span><a href="#39639998">root</a><span>|</span><a href="#39640517">parent</a><span>|</span><a href="#39643643">next</a><span>|</span><label class="collapse" for="c-39643141">[-]</label><label class="expand" for="c-39643141">[4 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the magic in 1.58bit vs. 4 bit that it makes it so much more efficient (claimed)?</div><br/><div id="39643312" class="c"><input type="checkbox" id="c-39643312" checked=""/><div class="controls bullet"><span class="by">danielhanchen</span><span>|</span><a href="#39639998">root</a><span>|</span><a href="#39643141">parent</a><span>|</span><a href="#39643426">next</a><span>|</span><label class="collapse" for="c-39643312">[-]</label><label class="expand" for="c-39643312">[1 more]</label></div><br/><div class="children"><div class="content">From what I understand, using (-1, 0, 1) removes multiplications in GPUs. Ie assume you have a weight matrix and multiply it by some activations<p><pre><code>                   [-1, 0,  1]

                   [0,  1, -1]

    [10, 20, 30] x [1,  1,  0]
</code></pre>
Instead of doing 10<i>(-1) + 20</i>(0) + 30<i>(1) + 10</i>(0) + ..., since we know beforehand the weights are simply (-1, 0, 1), we easily flip the sign and do addition, or force the hardware to do addition ie if (-1) do subtraction. If (0) do addition. If (1) do addition.<p>Floating point multiplication does addition of the exponents and multiplying of the mantissa. So just simplifying:<p>Float16 has E=5, M=10. Ie around 5 + 10^2 space needed = 105.<p>Bfloat16 has E=8, M=7.  So 8 + 7^2 = 57 space.<p>Float8(143)  E=4, M=3.  So 4 + 3^2 = 13 space.<p>1.58(16bit)  E=5, M=10. Addition only, so shift E say 5 + 10 addition = 15.<p>1.58(8bit)   E=4, M=3.  Addition only, so shift E say 4 + 3  addition = 7.<p>Obviously I&#x27;m simplifying, but with only additions, 1.58 uses say 7 space, whilst FP8 uses 13 space, so in theory 2x more transistors can be crammed, ie 2x more FLOPs than FP8.</div><br/></div></div><div id="39643426" class="c"><input type="checkbox" id="c-39643426" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#39639998">root</a><span>|</span><a href="#39643141">parent</a><span>|</span><a href="#39643312">prev</a><span>|</span><a href="#39645514">next</a><span>|</span><label class="collapse" for="c-39643426">[-]</label><label class="expand" for="c-39643426">[1 more]</label></div><br/><div class="children"><div class="content">Really simple explanation is that for inference, feed forward networks are threshold circuits and by their nature ANNs are binary output, outputting true and false (same as being a threshold circuit)<p>So if you train your models with that in mind you&#x27;re weighs can be reduced to -1,0,1 reducing the space complexity.<p>I don&#x27;t think the costs in expressiveness are captured quite yet, but as perplexity doesn&#x27;t care about correctness, if that is the metric that is important for you it will probably reduce memory requirements for inference.</div><br/></div></div><div id="39645514" class="c"><input type="checkbox" id="c-39645514" checked=""/><div class="controls bullet"><span class="by">chessgecko</span><span>|</span><a href="#39639998">root</a><span>|</span><a href="#39643141">parent</a><span>|</span><a href="#39643426">prev</a><span>|</span><a href="#39643643">next</a><span>|</span><label class="collapse" for="c-39645514">[-]</label><label class="expand" for="c-39645514">[1 more]</label></div><br/><div class="children"><div class="content">also just to add, I think the 1.58 bit is mostly faster for inference because training still had to multiply a lot of floating point gradients by integer activations, hold floating point weights&#x2F;gradients for round, and deal with norms and stuff. could be wrong about that though</div><br/></div></div></div></div></div></div><div id="39643643" class="c"><input type="checkbox" id="c-39643643" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#39639998">parent</a><span>|</span><a href="#39640517">prev</a><span>|</span><a href="#39640448">next</a><span>|</span><label class="collapse" for="c-39643643">[-]</label><label class="expand" for="c-39643643">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt; Now that I have a total of 23 points in HN, I will change my password to some random one, just to cure my desire to look for votes, and try to make some work, and again some tomorrow create a new presence in HN.</i><p>If you use Stylus (or any similar browser extension), I actually wrote a style to hide points for that very reason, replacing karma and scores with `•••`<p>This is actually the second time I see someone mentioning this need, so I&#x27;ve made it into a gist and published it to userstyles, but here&#x27;s it is also since it&#x27;s pretty short:<p><pre><code>    @-moz-document domain(&quot;news.ycombinator.com&quot;) {
        &#x2F;* Hide karma and points on replies *&#x2F;
        span.pagetop #karma, span.comhead span.score {
            visibility: hidden;
            position: relative;
            display: inline-block;
            height: 10px !important;
            overflow: hidden;
        }
        span.pagetop #karma {
            width: 0.8rem !important;
        }
        span.comhead span.score {
            width: 0.8rem !important;
        }
        span.pagetop #karma::before, span.comhead span.score::before {
            content: &quot;•••&quot;;
            visibility: visible;
            overflow: hidden;
            opacity: 0.8;
            font-family: Helvetica, Arial, sans-serif !important;
        }
    }

</code></pre>
<a href="https:&#x2F;&#x2F;gist.github.com&#x2F;airstrike&#x2F;62584e6ffb6104791c0ae48a8e5a2dd8" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;airstrike&#x2F;62584e6ffb6104791c0ae48a8e...</a><p><a href="https:&#x2F;&#x2F;userstyles.world&#x2F;style&#x2F;15164&#x2F;hackernews-hide-karma-and-comment-score" rel="nofollow">https:&#x2F;&#x2F;userstyles.world&#x2F;style&#x2F;15164&#x2F;hackernews-hide-karma-a...</a></div><br/><div id="39649521" class="c"><input type="checkbox" id="c-39649521" checked=""/><div class="controls bullet"><span class="by">SV_BubbleTime</span><span>|</span><a href="#39639998">root</a><span>|</span><a href="#39643643">parent</a><span>|</span><a href="#39640448">next</a><span>|</span><label class="collapse" for="c-39649521">[-]</label><label class="expand" for="c-39649521">[1 more]</label></div><br/><div class="children"><div class="content">I wish this was built in but understand the intentional abusive psychological exploit that it isn’t.</div><br/></div></div></div></div><div id="39640448" class="c"><input type="checkbox" id="c-39640448" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39639998">parent</a><span>|</span><a href="#39643643">prev</a><span>|</span><a href="#39649554">next</a><span>|</span><label class="collapse" for="c-39640448">[-]</label><label class="expand" for="c-39640448">[1 more]</label></div><br/><div class="children"><div class="content">As mentioned in the post, benchmarking results are coming in a later post. But in short: you can train an epoch of Alpaca in 24 hours or so, which is enough to get very significant change in model behavior.</div><br/></div></div><div id="39649554" class="c"><input type="checkbox" id="c-39649554" checked=""/><div class="controls bullet"><span class="by">gumby</span><span>|</span><a href="#39639998">parent</a><span>|</span><a href="#39640448">prev</a><span>|</span><a href="#39649489">next</a><span>|</span><label class="collapse" for="c-39649554">[-]</label><label class="expand" for="c-39649554">[2 more]</label></div><br/><div class="children"><div class="content">&gt; the recent (-1,0,1) encoding?<p>A side point, but this &quot;recent&quot; encoding goes back to a 2017 paper from the Allen Institute.  These days a seven year old paper is ancient.<p>They went further and showed you could could get away with binary, you don&#x27;t even need trinary!</div><br/><div id="39649648" class="c"><input type="checkbox" id="c-39649648" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#39639998">root</a><span>|</span><a href="#39649554">parent</a><span>|</span><a href="#39649489">next</a><span>|</span><label class="collapse" for="c-39649648">[-]</label><label class="expand" for="c-39649648">[1 more]</label></div><br/><div class="children"><div class="content">The bitnet paper was showing worse results than fp16 transformer with the same parameter count. The shocking result in the 1.58b paper (same group) is no quality loss compared to fp16.</div><br/></div></div></div></div><div id="39649489" class="c"><input type="checkbox" id="c-39649489" checked=""/><div class="controls bullet"><span class="by">throwaway14356</span><span>|</span><a href="#39639998">parent</a><span>|</span><a href="#39649554">prev</a><span>|</span><a href="#39641152">next</a><span>|</span><label class="collapse" for="c-39649489">[-]</label><label class="expand" for="c-39649489">[1 more]</label></div><br/><div class="children"><div class="content">i think those tables could be a facinating product. All parties involved could purchase them for private and public use.<p>P.S. 
I thought one was suppose to spend the HN points on mocking north-americans, shameless self-promotion, unpopular facts, general trolling and complaints about topics existing. I could go on but I haven&#x27;t the points.</div><br/></div></div><div id="39641152" class="c"><input type="checkbox" id="c-39641152" checked=""/><div class="controls bullet"><span class="by">swader999</span><span>|</span><a href="#39639998">parent</a><span>|</span><a href="#39649489">prev</a><span>|</span><a href="#39638211">next</a><span>|</span><label class="collapse" for="c-39641152">[-]</label><label class="expand" for="c-39641152">[1 more]</label></div><br/><div class="children"><div class="content">I like how you think about social media.</div><br/></div></div></div></div><div id="39638211" class="c"><input type="checkbox" id="c-39638211" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39639998">prev</a><span>|</span><a href="#39639083">next</a><span>|</span><label class="collapse" for="c-39638211">[-]</label><label class="expand" for="c-39638211">[19 more]</label></div><br/><div class="children"><div class="content">One thing I forgot to mention in the post which I think is kinda cool: at the NeurIPS Efficiency Challenge this year, where Tim Dettmers and I both did keynotes, every single top-ranked entry used QLoRA! The challenge was to create the most accurate model on a single GPU in 24 hours.<p>I think that is a great example of how important and useful QLoRA is. Maybe we should run a dual-GPU challenge next time not that multi-GPU is working...</div><br/><div id="39639858" class="c"><input type="checkbox" id="c-39639858" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#39638211">parent</a><span>|</span><a href="#39639661">next</a><span>|</span><label class="collapse" for="c-39639858">[-]</label><label class="expand" for="c-39639858">[2 more]</label></div><br/><div class="children"><div class="content">Are any of the NIPS resources available online?</div><br/><div id="39640973" class="c"><input type="checkbox" id="c-39640973" checked=""/><div class="controls bullet"><span class="by">notpublic</span><span>|</span><a href="#39638211">root</a><span>|</span><a href="#39639858">parent</a><span>|</span><a href="#39639661">next</a><span>|</span><label class="collapse" for="c-39640973">[-]</label><label class="expand" for="c-39640973">[1 more]</label></div><br/><div class="children"><div class="content">Tim Dettmers QLoRA 
<a href="https:&#x2F;&#x2F;nips.cc&#x2F;virtual&#x2F;2023&#x2F;83963" rel="nofollow">https:&#x2F;&#x2F;nips.cc&#x2F;virtual&#x2F;2023&#x2F;83963</a><p>More here:
<a href="https:&#x2F;&#x2F;nips.cc&#x2F;virtual&#x2F;2023&#x2F;competition&#x2F;66594" rel="nofollow">https:&#x2F;&#x2F;nips.cc&#x2F;virtual&#x2F;2023&#x2F;competition&#x2F;66594</a></div><br/></div></div></div></div></div></div><div id="39639083" class="c"><input type="checkbox" id="c-39639083" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39638211">prev</a><span>|</span><a href="#39639647">next</a><span>|</span><label class="collapse" for="c-39639083">[-]</label><label class="expand" for="c-39639083">[39 more]</label></div><br/><div class="children"><div class="content">This is great, but one thing I really hoped would come sooner is fast training on Metal. As things are, you can get an M1&#x2F;M2 Ultra (~800 Gb&#x2F;s memory bandwidth; for comparison, RTX 4090 is ~1050 Gb&#x2F;s) Mac Studio with 128Gb RAM for ~$3500. For large model inference, this is already way more affordable than stacking GPUs while being &quot;fast enough&quot;, but training solutions are basically non-existent. I do wonder why; it feels like a low-hanging fruit.</div><br/><div id="39639105" class="c"><input type="checkbox" id="c-39639105" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#39639083">parent</a><span>|</span><a href="#39639100">next</a><span>|</span><label class="collapse" for="c-39639105">[-]</label><label class="expand" for="c-39639105">[33 more]</label></div><br/><div class="children"><div class="content">Compute limited - an m2 ultra has 27 tflops, a 4090 80+</div><br/><div id="39639240" class="c"><input type="checkbox" id="c-39639240" checked=""/><div class="controls bullet"><span class="by">yumraj</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39639105">parent</a><span>|</span><a href="#39639196">next</a><span>|</span><label class="collapse" for="c-39639240">[-]</label><label class="expand" for="c-39639240">[26 more]</label></div><br/><div class="children"><div class="content">So it should just take longer..</div><br/><div id="39639519" class="c"><input type="checkbox" id="c-39639519" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39639240">parent</a><span>|</span><a href="#39639196">next</a><span>|</span><label class="collapse" for="c-39639519">[-]</label><label class="expand" for="c-39639519">[25 more]</label></div><br/><div class="children"><div class="content">If you don&#x27;t care how long it takes you can get an old server with 128GB of RAM for a lot less than $3500.</div><br/><div id="39639731" class="c"><input type="checkbox" id="c-39639731" checked=""/><div class="controls bullet"><span class="by">ErneX</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39639519">parent</a><span>|</span><a href="#39639196">next</a><span>|</span><label class="collapse" for="c-39639731">[-]</label><label class="expand" for="c-39639731">[24 more]</label></div><br/><div class="children"><div class="content">But that isn&#x27;t GPU memory right? On the Mac it is.</div><br/><div id="39645411" class="c"><input type="checkbox" id="c-39645411" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39639731">parent</a><span>|</span><a href="#39640012">next</a><span>|</span><label class="collapse" for="c-39645411">[-]</label><label class="expand" for="c-39645411">[7 more]</label></div><br/><div class="children"><div class="content">&gt; But that isn&#x27;t GPU memory right? On the Mac it is.<p>They call it that but it&#x27;s really LPDDR5, i.e. normal DRAM, using a wide memory bus. Which is the same thing servers do.<p>The base M3, with &quot;GPU memory&quot;, has 100GB&#x2F;s, which is less than even a cheap desktop PC with dual channel DDR5-6400. The M3 Pro has 150GB&#x2F;s. By comparison a five year old Epyc system has 8 channels of DDR4-3200 with more than 200GB&#x2F;s per socket. The M3 Max has 300-400GB&#x2F;s. Current generation servers have 12 channels of DDR5-4800 with 460GB&#x2F;s per socket, and support multi-socket systems.<p>The studio has 800GB&#x2F;s, which is almost as much as the modern dual socket system (for about the same price), but it&#x27;s not obvious it has enough compute resources to actually use that.</div><br/><div id="39650393" class="c"><input type="checkbox" id="c-39650393" checked=""/><div class="controls bullet"><span class="by">ErneX</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39645411">parent</a><span>|</span><a href="#39645638">next</a><span>|</span><label class="collapse" for="c-39650393">[-]</label><label class="expand" for="c-39650393">[1 more]</label></div><br/><div class="children"><div class="content">But if you need to get 2x consumer GPUs seems to me the reason is not for the compute capabilities but rather to be able to fit the model on the VRAM of both. So what exactly does having lots of memory on a server help with this when it’s not memory the GPU can use unlike on Apple Silicon computers?</div><br/></div></div><div id="39645638" class="c"><input type="checkbox" id="c-39645638" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39645411">parent</a><span>|</span><a href="#39650393">prev</a><span>|</span><a href="#39640012">next</a><span>|</span><label class="collapse" for="c-39645638">[-]</label><label class="expand" for="c-39645638">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s fast enough to do realtime (7 tok&#x2F;s) chat with 120b models.<p>And yes, of course it&#x27;s not magic, and in principle there&#x27;s no reason why a dedicated LLM-box with heaps of fast DDR5 couldn&#x27;t cost less. But in practice,  I&#x27;m not aware of any actual offerings in this space for comparable money that do not involve having to mess around with building things yourself. The beauty of Mac Studio is that you just plug it in, and it works.</div><br/><div id="39646444" class="c"><input type="checkbox" id="c-39646444" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39645638">parent</a><span>|</span><a href="#39640012">next</a><span>|</span><label class="collapse" for="c-39646444">[-]</label><label class="expand" for="c-39646444">[4 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s fast enough to do realtime (7 tok&#x2F;s) chat with 120b models.<p>Please list quantization for benchmarks. I&#x27;m assuming that&#x27;s not the full model because that would need 256GB and I don&#x27;t see a Studio model with that much memory, but q8 doubles performance and q4 quadruples it (with corresponding loss of quality).<p>&gt; But in practice, I&#x27;m not aware of any actual offerings in this space for comparable money that do not involve having to mess around with building things yourself.<p>You can just buy a complete server from a vendor or eBay, but this costs more because they&#x27;ll try to constrain you to a particular configuration that includes things you don&#x27;t need, or overcharge for RAM etc. Which is basically the same thing Apple does.<p>Whereas you can buy the barebones machine and then put components in it, which takes like fifteen minutes but can save you a thousand bucks.</div><br/><div id="39649718" class="c"><input type="checkbox" id="c-39649718" checked=""/><div class="controls bullet"><span class="by">sbierwagen</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39646444">parent</a><span>|</span><a href="#39647774">next</a><span>|</span><label class="collapse" for="c-39649718">[-]</label><label class="expand" for="c-39649718">[1 more]</label></div><br/><div class="children"><div class="content">6.3 tok&#x2F;s has been demonstrated on q4_0 Falcon 180B on the 192gb Mac studio: <a href="https:&#x2F;&#x2F;x.com&#x2F;ggerganov&#x2F;status&#x2F;1699791226780975439?s=46&amp;t=rumiybW6rwwWri-_imGg0Q" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;ggerganov&#x2F;status&#x2F;1699791226780975439?s=46&amp;t=ru...</a></div><br/></div></div><div id="39647774" class="c"><input type="checkbox" id="c-39647774" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39646444">parent</a><span>|</span><a href="#39649718">prev</a><span>|</span><a href="#39640012">next</a><span>|</span><label class="collapse" for="c-39647774">[-]</label><label class="expand" for="c-39647774">[2 more]</label></div><br/><div class="children"><div class="content">That is with 4-bit quantization. For practical purposes I don&#x27;t see the point of running anything higher than that for inference.</div><br/><div id="39648121" class="c"><input type="checkbox" id="c-39648121" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39647774">parent</a><span>|</span><a href="#39640012">next</a><span>|</span><label class="collapse" for="c-39648121">[-]</label><label class="expand" for="c-39648121">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s interesting though, because it implies the machine <i>is</i> compute-bound. A 4-bit 120B model is ~60GB, so you should get ~13 tokens&#x2F;second out of 800GB&#x2F;s if it was memory-bound. 7&#x2F;s implies you&#x27;re getting ~420GB&#x2F;s.<p>And the Max has half as many cores as the Ultra, implying it would be compute-bound too.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39640012" class="c"><input type="checkbox" id="c-39640012" checked=""/><div class="controls bullet"><span class="by">rthnbgrredf</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39639731">parent</a><span>|</span><a href="#39645411">prev</a><span>|</span><a href="#39639196">next</a><span>|</span><label class="collapse" for="c-39640012">[-]</label><label class="expand" for="c-39640012">[16 more]</label></div><br/><div class="children"><div class="content">The issue here isn&#x27;t specifically about the classification of memory, be it &quot;unified memory,&quot; RAM, or VRAM. The primary concern is ensuring there&#x27;s enough memory capacity for the models required for inference. The real question at hand is the Mac&#x27;s value proposition in terms of inference speed, particularly for models as large as 70 billion parameters. Utilizing a 4090 GPU can facilitate real-time inference, which is the desired outcome for most users. In contrast, a Mac Studio offers close to real-time inference speeds, which might be disappointing for users expecting a real-time experience. Then, there&#x27;s the option of CPU + RAM-based inference, which suits scenarios where immediate responses aren&#x27;t crucial, allowing for batch processing of prompts and subsequent retrieval of responses. Considering the price points of both the Mac Studio and high-end GPUs are relatively comparable, it begs the question of the practicality and value of near real-time inference in specific use cases.</div><br/><div id="39640247" class="c"><input type="checkbox" id="c-39640247" checked=""/><div class="controls bullet"><span class="by">Lalabadie</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39640012">parent</a><span>|</span><a href="#39645360">next</a><span>|</span><label class="collapse" for="c-39640247">[-]</label><label class="expand" for="c-39640247">[3 more]</label></div><br/><div class="children"><div class="content">Considering that the topic is approachability and energy efficiency, that Mac Studio will do reasonably fast inference while consuming &lt;200W at full load.<p>The speed is certainly not comparable to dedicated GPUs, but the power efficiency is ridiculous for a very usable speed and no hardware setup.</div><br/><div id="39641970" class="c"><input type="checkbox" id="c-39641970" checked=""/><div class="controls bullet"><span class="by">Applejinx</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39640247">parent</a><span>|</span><a href="#39645360">next</a><span>|</span><label class="collapse" for="c-39641970">[-]</label><label class="expand" for="c-39641970">[2 more]</label></div><br/><div class="children"><div class="content">This, and then you get to have a Mac Studio.<p>I have one, where I selected an M1 Ultra and 128G RAM to facilitate just this sort of thing. But in practice, I&#x27;m spending much more time using it to edit 4K video, and as a recording studio&#x2F;to develop audio plugins on, and to livestream while doing these things.<p>Turns out it&#x27;s good at these things, and since I have the LLAMA 70b language model at home and can run it directly unquantized (not at blinding speed, of course, but it&#x27;ll run just fine), I&#x27;m naturally interested in learning how to fine tune it :)</div><br/><div id="39645434" class="c"><input type="checkbox" id="c-39645434" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39641970">parent</a><span>|</span><a href="#39645360">next</a><span>|</span><label class="collapse" for="c-39645434">[-]</label><label class="expand" for="c-39645434">[1 more]</label></div><br/><div class="children"><div class="content">Yep, I also got mine specifically for LLMs and ended up using it as a second desktop for other things; actually strongly considering making it my primary at this point.<p>I still wouldn&#x27;t recommend it to someone just looking for a powerful desktop, just because $3K is way overpriced for what you get (non-replaceable 1Tb SSD is so <i>Apple</i>!). But it&#x27;s certainly great if you already have it...</div><br/></div></div></div></div></div></div><div id="39645360" class="c"><input type="checkbox" id="c-39645360" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39640012">parent</a><span>|</span><a href="#39640247">prev</a><span>|</span><a href="#39640115">next</a><span>|</span><label class="collapse" for="c-39645360">[-]</label><label class="expand" for="c-39645360">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Real-time&quot; is a very vague descriptor. I get 7-8 tok&#x2F;s for 70b model inference on my M1 Mac - that&#x27;s pretty real-time to me. Even Professor-155b runs &quot;good enough&quot; (~3 tok&#x2F;s) for what I&#x27;d consider real-time chat in English.</div><br/></div></div><div id="39640115" class="c"><input type="checkbox" id="c-39640115" checked=""/><div class="controls bullet"><span class="by">abtinf</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39640012">parent</a><span>|</span><a href="#39645360">prev</a><span>|</span><a href="#39639196">next</a><span>|</span><label class="collapse" for="c-39640115">[-]</label><label class="expand" for="c-39640115">[11 more]</label></div><br/><div class="children"><div class="content">Hello gpt.</div><br/><div id="39641229" class="c"><input type="checkbox" id="c-39641229" checked=""/><div class="controls bullet"><span class="by">rthnbgrredf</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39640115">parent</a><span>|</span><a href="#39639196">next</a><span>|</span><label class="collapse" for="c-39641229">[-]</label><label class="expand" for="c-39641229">[10 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not a gpt. But now you could say that this is exactly how a gpt would answer and we get stuck in a loop and there&#x27;s no obvious way to prove that I&#x27;m not a gpt.</div><br/><div id="39646415" class="c"><input type="checkbox" id="c-39646415" checked=""/><div class="controls bullet"><span class="by">abtinf</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39641229">parent</a><span>|</span><a href="#39642428">next</a><span>|</span><label class="collapse" for="c-39646415">[-]</label><label class="expand" for="c-39646415">[4 more]</label></div><br/><div class="children"><div class="content">Interesting. Let&#x27;s review the comment.<p>&gt; The issue here isn&#x27;t specifically about the classification of memory, be it &quot;unified memory,&quot; RAM, or VRAM. The primary concern is ensuring there&#x27;s enough memory capacity for the models required for inference.<p>The comment chain is about training, not inference.<p>&gt; The real question at hand is the Mac&#x27;s value proposition in terms of inference speed, particularly for models as large as 70 billion parameters.<p>Again, wrong topic.<p>&gt; Utilizing a 4090 GPU can facilitate real-time inference, which is the desired outcome for most users.<p>Generic statement. Semantically empty. Typical LLM style.<p>&gt; In contrast, a Mac Studio offers close to real-time inference speeds, which might be disappointing for users expecting a real-time experience.<p>Tautological generic statement. Semantically empty. Typical LLM style.<p>&gt; Then, there&#x27;s the option of CPU + RAM-based inference, which suits scenarios where immediate responses aren&#x27;t crucial, allowing for batch processing of prompts and subsequent retrieval of responses.<p>Contradicts first sentence that &quot;classification of memory&quot; isn&#x27;t important. Fails to recognize this the same category as previous statement. Subtle shift from first sentence that declared &quot;primary concern is ... membory capacity&quot;, to focusing purely on performance. This kind of incoherent shift is common in LLM output.<p>&gt; Considering the price points of both the Mac Studio and high-end GPUs are relatively comparable, it begs the question of the practicality and value of near real-time inference in specific use cases.<p>Completes shift from memory capacity to performance. Compares not really comparable things. &quot;Specific use cases&quot; is a tell-tale LLM marker. Semantically empty.</div><br/><div id="39649839" class="c"><input type="checkbox" id="c-39649839" checked=""/><div class="controls bullet"><span class="by">rthnbgrredf</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39646415">parent</a><span>|</span><a href="#39648141">next</a><span>|</span><label class="collapse" for="c-39649839">[-]</label><label class="expand" for="c-39649839">[1 more]</label></div><br/><div class="children"><div class="content">antinf congratulations I think you have proven, beyond any doubt, that I&#x27;m a gpt.<p>(This is a semantically empty tautological generic statement.)</div><br/></div></div><div id="39648141" class="c"><input type="checkbox" id="c-39648141" checked=""/><div class="controls bullet"><span class="by">MrYellowP</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39646415">parent</a><span>|</span><a href="#39649839">prev</a><span>|</span><a href="#39642428">next</a><span>|</span><label class="collapse" for="c-39648141">[-]</label><label class="expand" for="c-39648141">[2 more]</label></div><br/><div class="children"><div class="content">I feel the need to point out that people, who spend many hours writing with an LLM, will eventually start writing like the LLM.</div><br/><div id="39650389" class="c"><input type="checkbox" id="c-39650389" checked=""/><div class="controls bullet"><span class="by">rthnbgrredf</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39648141">parent</a><span>|</span><a href="#39642428">next</a><span>|</span><label class="collapse" for="c-39650389">[-]</label><label class="expand" for="c-39650389">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m definitely guilty of this. Especially for non native speaker who might more easily lend towards adapting phrases from others (including gpts), because they are not sure how to phrase it correctly.</div><br/></div></div></div></div></div></div><div id="39642428" class="c"><input type="checkbox" id="c-39642428" checked=""/><div class="controls bullet"><span class="by">pbhjpbhj</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39641229">parent</a><span>|</span><a href="#39646415">prev</a><span>|</span><a href="#39639196">next</a><span>|</span><label class="collapse" for="c-39642428">[-]</label><label class="expand" for="c-39642428">[5 more]</label></div><br/><div class="children"><div class="content">&#x27;Write me something profane?&#x27; That probably weeds out commercially available GPTs?</div><br/><div id="39650084" class="c"><input type="checkbox" id="c-39650084" checked=""/><div class="controls bullet"><span class="by">natch</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39642428">parent</a><span>|</span><a href="#39645141">next</a><span>|</span><label class="collapse" for="c-39650084">[-]</label><label class="expand" for="c-39650084">[2 more]</label></div><br/><div class="children"><div class="content">No, this is a common fallacy. You can tell ChatGPT, one of the most infamously hobbled GPTs, in custom instructions that you do want profanity, and it will oblige. This is not a jailbreak, this is supported behavior.</div><br/><div id="39650405" class="c"><input type="checkbox" id="c-39650405" checked=""/><div class="controls bullet"><span class="by">rthnbgrredf</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39650084">parent</a><span>|</span><a href="#39645141">next</a><span>|</span><label class="collapse" for="c-39650405">[-]</label><label class="expand" for="c-39650405">[1 more]</label></div><br/><div class="children"><div class="content">I do think that there is a certain amount of gpt bot activity present on HN. But I don&#x27;t think it makes sense to call people out and saying they are a gpt just based on one comment.</div><br/></div></div></div></div><div id="39645141" class="c"><input type="checkbox" id="c-39645141" checked=""/><div class="controls bullet"><span class="by">rthnbgrredf</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39642428">parent</a><span>|</span><a href="#39650084">prev</a><span>|</span><a href="#39639196">next</a><span>|</span><label class="collapse" for="c-39645141">[-]</label><label class="expand" for="c-39645141">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sorry, but I can&#x27;t fulfill that request. Is there anything else I might assist you with? ;)</div><br/><div id="39645516" class="c"><input type="checkbox" id="c-39645516" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39645141">parent</a><span>|</span><a href="#39639196">next</a><span>|</span><label class="collapse" for="c-39645516">[-]</label><label class="expand" for="c-39645516">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;chirper.ai&#x2F;aligned&#x2F;chirp&#x2F;65e58aa9001d853c" rel="nofollow">https:&#x2F;&#x2F;chirper.ai&#x2F;aligned&#x2F;chirp&#x2F;65e58aa9001d853c</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39639196" class="c"><input type="checkbox" id="c-39639196" checked=""/><div class="controls bullet"><span class="by">erichocean</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39639105">parent</a><span>|</span><a href="#39639240">prev</a><span>|</span><a href="#39639100">next</a><span>|</span><label class="collapse" for="c-39639196">[-]</label><label class="expand" for="c-39639196">[6 more]</label></div><br/><div class="children"><div class="content">Memory limited - an m2 ultra has &gt;150GiB, a 4090 24GiB</div><br/><div id="39641832" class="c"><input type="checkbox" id="c-39641832" checked=""/><div class="controls bullet"><span class="by">lawlessone</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39639196">parent</a><span>|</span><a href="#39639100">next</a><span>|</span><label class="collapse" for="c-39641832">[-]</label><label class="expand" for="c-39641832">[5 more]</label></div><br/><div class="children"><div class="content">So why is nobody doing this?</div><br/><div id="39641983" class="c"><input type="checkbox" id="c-39641983" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39641832">parent</a><span>|</span><a href="#39639100">next</a><span>|</span><label class="collapse" for="c-39641983">[-]</label><label class="expand" for="c-39641983">[4 more]</label></div><br/><div class="children"><div class="content">My personal experience with Apple Silicon and machine learning in comparison with Nvidia is that the libraries are often missing various features on Apple, leading to headaches when trying to use various libraries and tools. Apple is working to bridge the gap and I am excited for the gap to be closed because the memory bandwidth on big M2 and M3 machines is monstrous.</div><br/><div id="39642585" class="c"><input type="checkbox" id="c-39642585" checked=""/><div class="controls bullet"><span class="by">lawlessone</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39641983">parent</a><span>|</span><a href="#39639100">next</a><span>|</span><label class="collapse" for="c-39642585">[-]</label><label class="expand" for="c-39642585">[3 more]</label></div><br/><div class="children"><div class="content">sounds similar to how people have described game dev for Mac. The hardware is there. It just isn&#x27;t supported.</div><br/><div id="39642948" class="c"><input type="checkbox" id="c-39642948" checked=""/><div class="controls bullet"><span class="by">imhoguy</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39642585">parent</a><span>|</span><a href="#39639100">next</a><span>|</span><label class="collapse" for="c-39642948">[-]</label><label class="expand" for="c-39642948">[2 more]</label></div><br/><div class="children"><div class="content">Apple could single-handedly kill consumer dGPU market if they have released proper low-level APIs for their M1&#x2F;2&#x2F;3. I feel they have some huge coming out in the pipe to tumble the &quot;AI&quot; market.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39639100" class="c"><input type="checkbox" id="c-39639100" checked=""/><div class="controls bullet"><span class="by">sqreept</span><span>|</span><a href="#39639083">parent</a><span>|</span><a href="#39639105">prev</a><span>|</span><a href="#39639647">next</a><span>|</span><label class="collapse" for="c-39639100">[-]</label><label class="expand" for="c-39639100">[5 more]</label></div><br/><div class="children"><div class="content">M1, M2, M3 still have very low number of GPU cores. Apple should release some better hardware to take advantage of their recently released MLX library.</div><br/><div id="39640418" class="c"><input type="checkbox" id="c-39640418" checked=""/><div class="controls bullet"><span class="by">sbinnee</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39639100">parent</a><span>|</span><a href="#39642074">next</a><span>|</span><label class="collapse" for="c-39640418">[-]</label><label class="expand" for="c-39640418">[2 more]</label></div><br/><div class="children"><div class="content">At this moment it looks clear to me that Apple won’t go that way. It’s enough for them to focus on inference and actual application not the heavy training part. They have been probably training models on a cluster with non Apple silicon and make them available for their chips only for inference.</div><br/><div id="39642009" class="c"><input type="checkbox" id="c-39642009" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39640418">parent</a><span>|</span><a href="#39642074">next</a><span>|</span><label class="collapse" for="c-39642009">[-]</label><label class="expand" for="c-39642009">[1 more]</label></div><br/><div class="children"><div class="content">Not to mention entirely outsourcing training workloads to specialist firms. Apple does a lot of secretive outsourcing of things you might think they would or should do in-house. This contrasts with Google and Meta who seem to like keeping everything in-house.</div><br/></div></div></div></div><div id="39642074" class="c"><input type="checkbox" id="c-39642074" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39639100">parent</a><span>|</span><a href="#39640418">prev</a><span>|</span><a href="#39645334">next</a><span>|</span><label class="collapse" for="c-39642074">[-]</label><label class="expand" for="c-39642074">[1 more]</label></div><br/><div class="children"><div class="content">It’s true that their GPUs are slower than Nvidia’s. But keep in mind that cores are really different and cannot be compared across architectures. You want more Gflops, not necessarily more cores.</div><br/></div></div><div id="39645334" class="c"><input type="checkbox" id="c-39645334" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39639083">root</a><span>|</span><a href="#39639100">parent</a><span>|</span><a href="#39642074">prev</a><span>|</span><a href="#39639647">next</a><span>|</span><label class="collapse" for="c-39645334">[-]</label><label class="expand" for="c-39645334">[1 more]</label></div><br/><div class="children"><div class="content">They do, but for inference at least, it&#x27;s memory bandwidth that is the primary limiting factor for home LLMs right now, not raw compute.</div><br/></div></div></div></div></div></div><div id="39639647" class="c"><input type="checkbox" id="c-39639647" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#39639083">prev</a><span>|</span><a href="#39638548">next</a><span>|</span><label class="collapse" for="c-39639647">[-]</label><label class="expand" for="c-39639647">[6 more]</label></div><br/><div class="children"><div class="content">This might be the most interesting constructive approach in &quot;Open Source&quot; LLMs I&#x27;ve seen. Grounded, reasonable and inviting to replicate! I wish academia took that as a standard.<p>Great job!</div><br/><div id="39639851" class="c"><input type="checkbox" id="c-39639851" checked=""/><div class="controls bullet"><span class="by">carlossouza</span><span>|</span><a href="#39639647">parent</a><span>|</span><a href="#39641123">next</a><span>|</span><label class="collapse" for="c-39639851">[-]</label><label class="expand" for="c-39639851">[3 more]</label></div><br/><div class="children"><div class="content">Answer.ai is truly open AI. :)</div><br/><div id="39640925" class="c"><input type="checkbox" id="c-39640925" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#39639647">root</a><span>|</span><a href="#39639851">parent</a><span>|</span><a href="#39641123">next</a><span>|</span><label class="collapse" for="c-39640925">[-]</label><label class="expand" for="c-39640925">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what was said about OpenAI, Mistral, before the VCs and investors came in.<p>After that, the larger flagship AI models were then closed up again and used as an server only offering.</div><br/><div id="39641289" class="c"><input type="checkbox" id="c-39641289" checked=""/><div class="controls bullet"><span class="by">ericd</span><span>|</span><a href="#39639647">root</a><span>|</span><a href="#39640925">parent</a><span>|</span><a href="#39641123">next</a><span>|</span><label class="collapse" for="c-39641289">[-]</label><label class="expand" for="c-39641289">[1 more]</label></div><br/><div class="children"><div class="content">I doubt it, Jeremy’s been walking the walk for quite a while now, when it comes to opening up access to AI, especially with his excellent, free fast.ai course, it seems pretty clear that his primary motivations are in helping others. (If you’re in this thread, Jeremy, thanks for fast.ai, it helped me immensely in getting started in training models).</div><br/></div></div></div></div></div></div><div id="39641123" class="c"><input type="checkbox" id="c-39641123" checked=""/><div class="controls bullet"><span class="by">20wenty</span><span>|</span><a href="#39639647">parent</a><span>|</span><a href="#39639851">prev</a><span>|</span><a href="#39638548">next</a><span>|</span><label class="collapse" for="c-39641123">[-]</label><label class="expand" for="c-39641123">[2 more]</label></div><br/><div class="children"><div class="content">For the most part this post was easy to read, and I could feel the collective excitement of the team. I came away feeling like I&#x27;d learned something and ready to try it myself. The only time the post gets a little fuzzy is &quot;...store the quantized parameters in a selectable data type, where that storage data type is the same data type as the “computation type” of the mode&quot;. I assume &quot;selectable datatype&quot; is the float size of the quantization?</div><br/><div id="39648784" class="c"><input type="checkbox" id="c-39648784" checked=""/><div class="controls bullet"><span class="by">Yenrabbit</span><span>|</span><a href="#39639647">root</a><span>|</span><a href="#39641123">parent</a><span>|</span><a href="#39638548">next</a><span>|</span><label class="collapse" for="c-39648784">[-]</label><label class="expand" for="c-39648784">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve got a technical post with all the juicy details coming next week. But that bit refers to packing the 4-bit weights into a type FSDP is happy to shard (like float16 or float32) which matches the other non-quantized bits of the model. This way FSDP will happily wrap and shard all the parameters as if they were just normal floats.</div><br/></div></div></div></div></div></div><div id="39638548" class="c"><input type="checkbox" id="c-39638548" checked=""/><div class="controls bullet"><span class="by">jamesblonde</span><span>|</span><a href="#39639647">prev</a><span>|</span><a href="#39635617">next</a><span>|</span><label class="collapse" for="c-39638548">[-]</label><label class="expand" for="c-39638548">[5 more]</label></div><br/><div class="children"><div class="content">This is a fantastic breakthrough for those of us who fine-tune LLMs on limited hardware budgets.<p>I was curious about the choice of FSDP over DeepSpeed. I have been using Axolotl for fine-tuning, and FSDP has been broken there, whilst DeepSpeed is rock solid. Why FSDP over DeepSpeed jph00?</div><br/><div id="39638763" class="c"><input type="checkbox" id="c-39638763" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39638548">parent</a><span>|</span><a href="#39635617">next</a><span>|</span><label class="collapse" for="c-39638763">[-]</label><label class="expand" for="c-39638763">[4 more]</label></div><br/><div class="children"><div class="content">DeepSpeed has more features than FSDP, but it&#x27;s much more complex to hack on -- FSDP is written directly in python using calls to the PyTorch library, whereas DeepSpeed is 20% C++ and 10% CUDA (according to the GitHub stats).<p>We&#x27;ve found that FSDP works just as well for our needs, and we appreciated the increased &quot;hackability&quot;.<p>(Axolotl is terrific BTW. I hadn&#x27;t heard of problems with it with FSDP before -- I&#x27;ll see if that&#x27;s something we can help with.)</div><br/><div id="39645632" class="c"><input type="checkbox" id="c-39645632" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39638548">root</a><span>|</span><a href="#39638763">parent</a><span>|</span><a href="#39646994">next</a><span>|</span><label class="collapse" for="c-39645632">[-]</label><label class="expand" for="c-39645632">[1 more]</label></div><br/><div class="children"><div class="content">Good news -- axolotl has just merged support for FSDP&#x2F;QLoRA training, thanks to a rapid collaboration between the axolotl and Answr.AI teams!</div><br/></div></div><div id="39646994" class="c"><input type="checkbox" id="c-39646994" checked=""/><div class="controls bullet"><span class="by">bradfox2</span><span>|</span><a href="#39638548">root</a><span>|</span><a href="#39638763">parent</a><span>|</span><a href="#39645632">prev</a><span>|</span><a href="#39635617">next</a><span>|</span><label class="collapse" for="c-39646994">[-]</label><label class="expand" for="c-39646994">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a long gh issues thread with technium struggling with Mistral 7 and loss spikes. Easy to find googling.</div><br/><div id="39647715" class="c"><input type="checkbox" id="c-39647715" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39638548">root</a><span>|</span><a href="#39646994">parent</a><span>|</span><a href="#39635617">next</a><span>|</span><label class="collapse" for="c-39647715">[-]</label><label class="expand" for="c-39647715">[1 more]</label></div><br/><div class="children"><div class="content">Yes I&#x27;m familiar with Teknium&#x27;s Mistral issues, which were resolved some time ago. IIRC they weren&#x27;t related to FSDP.</div><br/></div></div></div></div></div></div></div></div><div id="39635617" class="c"><input type="checkbox" id="c-39635617" checked=""/><div class="controls bullet"><span class="by">ricopags</span><span>|</span><a href="#39638548">prev</a><span>|</span><a href="#39640878">next</a><span>|</span><label class="collapse" for="c-39635617">[-]</label><label class="expand" for="c-39635617">[2 more]</label></div><br/><div class="children"><div class="content">This is such exciting news! Huge thanks to you for your continued work in making sense of AI.<p>I wonder if the recent Bitnet 1.58 paper [the use of ternary bits in lieu of fp&#x2F;int] might be an advancement that could further reduce the computation required for inference?</div><br/><div id="39638160" class="c"><input type="checkbox" id="c-39638160" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39635617">parent</a><span>|</span><a href="#39640878">next</a><span>|</span><label class="collapse" for="c-39638160">[-]</label><label class="expand" for="c-39638160">[1 more]</label></div><br/><div class="children"><div class="content">Yes, along with the many other &lt;4 bit quant methods recently developed -- there&#x27;s been a wonderful boom in low-bit quant methods in the last 6 months, and we&#x27;ve got our own ideas for taking them further too. Along with QLoRA&#x2F;FSDP, we&#x27;re likely to see big advances in model training this year on consumer hardware.</div><br/></div></div></div></div><div id="39640878" class="c"><input type="checkbox" id="c-39640878" checked=""/><div class="controls bullet"><span class="by">chasd00</span><span>|</span><a href="#39635617">prev</a><span>|</span><a href="#39638606">next</a><span>|</span><label class="collapse" for="c-39640878">[-]</label><label class="expand" for="c-39640878">[6 more]</label></div><br/><div class="children"><div class="content">What’s the best way for people to contribute to AI open source? I can’t produce things like this for many reasons so how can I and others like me do our part to keep SOTA AI open?</div><br/><div id="39641705" class="c"><input type="checkbox" id="c-39641705" checked=""/><div class="controls bullet"><span class="by">ativzzz</span><span>|</span><a href="#39640878">parent</a><span>|</span><a href="#39641009">next</a><span>|</span><label class="collapse" for="c-39641705">[-]</label><label class="expand" for="c-39641705">[1 more]</label></div><br/><div class="children"><div class="content">Off the top of my head<p>- try to implement techniques that are doable on home hardware like the one described in OP (requires some $$$ investment) and give feedback or contribute to documentation &#x2F; guides<p>- learn about different techniques and do educational writeups or documentation (like <a href="https:&#x2F;&#x2F;vickiboykis.com&#x2F;2024&#x2F;02&#x2F;28&#x2F;gguf-the-long-way-around&#x2F;" rel="nofollow">https:&#x2F;&#x2F;vickiboykis.com&#x2F;2024&#x2F;02&#x2F;28&#x2F;gguf-the-long-way-around&#x2F;</a>)<p>- build a tool &#x2F; library that wrap academic techinques and expose them more easily to end users (like A1111 or comfyUI for stable diffusion)<p>Anything that can translate the high end research down to something a moderately technical user can use or learn from is a gigantic win</div><br/></div></div><div id="39641009" class="c"><input type="checkbox" id="c-39641009" checked=""/><div class="controls bullet"><span class="by">sophrocyne</span><span>|</span><a href="#39640878">parent</a><span>|</span><a href="#39641705">prev</a><span>|</span><a href="#39643758">next</a><span>|</span><label class="collapse" for="c-39641009">[-]</label><label class="expand" for="c-39641009">[2 more]</label></div><br/><div class="children"><div class="content">There is a ton you can do to help SOTA AI remain open.<p>Join the community building the tools - Help with UI&#x2F;UX, documentation, keeping up with the latest, and evangelizing whatever method the team building it has devised to keep it sustained.<p>Being part of the community itself is more valuable than you realize.</div><br/><div id="39643594" class="c"><input type="checkbox" id="c-39643594" checked=""/><div class="controls bullet"><span class="by">SamPatt</span><span>|</span><a href="#39640878">root</a><span>|</span><a href="#39641009">parent</a><span>|</span><a href="#39643758">next</a><span>|</span><label class="collapse" for="c-39643594">[-]</label><label class="expand" for="c-39643594">[1 more]</label></div><br/><div class="children"><div class="content">Where are you finding this community?</div><br/></div></div></div></div><div id="39643758" class="c"><input type="checkbox" id="c-39643758" checked=""/><div class="controls bullet"><span class="by">hamilyon2</span><span>|</span><a href="#39640878">parent</a><span>|</span><a href="#39641009">prev</a><span>|</span><a href="#39640960">next</a><span>|</span><label class="collapse" for="c-39643758">[-]</label><label class="expand" for="c-39643758">[1 more]</label></div><br/><div class="children"><div class="content">I am random software engineer, but from what I learned high-quality open source data sets seems to be enabler. There is shortage of golden datasets for training and evaluation in every popular and niche area you can imagine.</div><br/></div></div></div></div><div id="39638606" class="c"><input type="checkbox" id="c-39638606" checked=""/><div class="controls bullet"><span class="by">itsgrimetime</span><span>|</span><a href="#39640878">prev</a><span>|</span><a href="#39638435">next</a><span>|</span><label class="collapse" for="c-39638606">[-]</label><label class="expand" for="c-39638606">[5 more]</label></div><br/><div class="children"><div class="content">Would be cool to build an “LLM@home” project like folding@home or SETI@home (rip), where tons of folks could donate their GPUs and train something huge and FOSS. I don’t know enough about how these models are trained though. Could it be chunked up and distributed in that way, then stitched&#x2F;merged back together?</div><br/><div id="39638639" class="c"><input type="checkbox" id="c-39638639" checked=""/><div class="controls bullet"><span class="by">fho</span><span>|</span><a href="#39638606">parent</a><span>|</span><a href="#39643210">next</a><span>|</span><label class="collapse" for="c-39638639">[-]</label><label class="expand" for="c-39638639">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;stablehorde.net&#x2F;" rel="nofollow">https:&#x2F;&#x2F;stablehorde.net&#x2F;</a> comes somewhat close.</div><br/></div></div><div id="39643210" class="c"><input type="checkbox" id="c-39643210" checked=""/><div class="controls bullet"><span class="by">miohtama</span><span>|</span><a href="#39638606">parent</a><span>|</span><a href="#39638639">prev</a><span>|</span><a href="#39641623">next</a><span>|</span><label class="collapse" for="c-39643210">[-]</label><label class="expand" for="c-39643210">[1 more]</label></div><br/><div class="children"><div class="content">Golem has been building this since 2017<p><a href="https:&#x2F;&#x2F;www.golem.network&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.golem.network&#x2F;</a><p>They also have on option to get paid in crypto for your GPU power.<p>The challenge is that the AIsoftware architectures are not made &quot;to run over Internet.&quot;</div><br/></div></div><div id="39641623" class="c"><input type="checkbox" id="c-39641623" checked=""/><div class="controls bullet"><span class="by">woctordho</span><span>|</span><a href="#39638606">parent</a><span>|</span><a href="#39643210">prev</a><span>|</span><a href="#39642291">next</a><span>|</span><label class="collapse" for="c-39641623">[-]</label><label class="expand" for="c-39641623">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;learning-at-home&#x2F;hivemind">https:&#x2F;&#x2F;github.com&#x2F;learning-at-home&#x2F;hivemind</a> is also relevant</div><br/></div></div><div id="39642291" class="c"><input type="checkbox" id="c-39642291" checked=""/><div class="controls bullet"><span class="by">humansareok1</span><span>|</span><a href="#39638606">parent</a><span>|</span><a href="#39641623">prev</a><span>|</span><a href="#39638435">next</a><span>|</span><label class="collapse" for="c-39642291">[-]</label><label class="expand" for="c-39642291">[1 more]</label></div><br/><div class="children"><div class="content">Always figured it would be too slow. Distributed training on clusters is usually done with 1+ gb&#x2F;s interconnects.</div><br/></div></div></div></div><div id="39638435" class="c"><input type="checkbox" id="c-39638435" checked=""/><div class="controls bullet"><span class="by">keeptrying</span><span>|</span><a href="#39638606">prev</a><span>|</span><a href="#39640831">next</a><span>|</span><label class="collapse" for="c-39638435">[-]</label><label class="expand" for="c-39638435">[2 more]</label></div><br/><div class="children"><div class="content">If you are gonna be doing stuff like this I’m damn excited for answer.ai!<p>It’ll be the first time we’ll have someone who knows AI create leverage to open source it.<p>Way to go!</div><br/><div id="39640508" class="c"><input type="checkbox" id="c-39640508" checked=""/><div class="controls bullet"><span class="by">chasd00</span><span>|</span><a href="#39638435">parent</a><span>|</span><a href="#39640831">next</a><span>|</span><label class="collapse" for="c-39640508">[-]</label><label class="expand" for="c-39640508">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It’ll be the first time we’ll have someone who knows AI create leverage to open source it.<p>It can’t be overstated how important this is. Thank you again.</div><br/></div></div></div></div><div id="39640831" class="c"><input type="checkbox" id="c-39640831" checked=""/><div class="controls bullet"><span class="by">iandanforth</span><span>|</span><a href="#39638435">prev</a><span>|</span><a href="#39638526">next</a><span>|</span><label class="collapse" for="c-39640831">[-]</label><label class="expand" for="c-39640831">[1 more]</label></div><br/><div class="children"><div class="content">This is great, however there were many opportunities to use the word &#x27;nibble&#x27; in this post and they were all missed.</div><br/></div></div><div id="39638526" class="c"><input type="checkbox" id="c-39638526" checked=""/><div class="controls bullet"><span class="by">yalok</span><span>|</span><a href="#39640831">prev</a><span>|</span><a href="#39642477">next</a><span>|</span><label class="collapse" for="c-39638526">[-]</label><label class="expand" for="c-39638526">[5 more]</label></div><br/><div class="children"><div class="content">Have you guys looked at using sparsification? It would probably require true re-training of the foundation model, to go at high sparse ratios (say 90% weights excluded), which could be done  once on expensive GPU - but fine tuning such sparse models would require less RAM hopefully.<p>The trick with getting more benefit from sparse approach  is to do block sparse (iirc, Tim Dettmers used to work on this as well, a few years ago), but large block size (say 16x16) would require much longer retraining to recover for the lost accuracy…</div><br/><div id="39638793" class="c"><input type="checkbox" id="c-39638793" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39638526">parent</a><span>|</span><a href="#39639010">next</a><span>|</span><label class="collapse" for="c-39638793">[-]</label><label class="expand" for="c-39638793">[2 more]</label></div><br/><div class="children"><div class="content">Yes, sparsification is another useful approach for higher efficiency, although block sparse kernels are pretty complex to work with -- especially when combined with quantization and LoRA! Most of the sparsity papers I&#x27;ve seen use &quot;structured&quot; sparsity; i.e removing layers, attention heads, and features. But the upside from this seems somewhat limited so far.</div><br/><div id="39647166" class="c"><input type="checkbox" id="c-39647166" checked=""/><div class="controls bullet"><span class="by">yalok</span><span>|</span><a href="#39638526">root</a><span>|</span><a href="#39638793">parent</a><span>|</span><a href="#39639010">next</a><span>|</span><label class="collapse" for="c-39647166">[-]</label><label class="expand" for="c-39647166">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure about structured sparsity, but for the weights sparsity in my experience going to around 50-70% of excluded weights (even with block sparsity - say 4x4) did not cause any noticeable degradation to training &amp; quality at all (original paper on sparsity from LeCun suggests much higher sparsity ratios - like 90% - but for DNNs I didn&#x27;t find those attainable if accuracy is important)<p>The block sparsity can really help with saving RAM - because you only need to keep a short array of indexes for the excluded weights. The trouble is the kernel mult functions become complex, so it&#x27;s a bit of a trade-off between RAM and GPU cycles.</div><br/></div></div></div></div><div id="39639010" class="c"><input type="checkbox" id="c-39639010" checked=""/><div class="controls bullet"><span class="by">AhtiK</span><span>|</span><a href="#39638526">parent</a><span>|</span><a href="#39638793">prev</a><span>|</span><a href="#39642477">next</a><span>|</span><label class="collapse" for="c-39639010">[-]</label><label class="expand" for="c-39639010">[2 more]</label></div><br/><div class="children"><div class="content">Has anyone seen an implementation of &#x27;SpQR: A Sparse-Quantized Representation,&#x27; published in June 2023 by Tim Dettmers et al.? <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.03078" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.03078</a></div><br/><div id="39639052" class="c"><input type="checkbox" id="c-39639052" checked=""/><div class="controls bullet"><span class="by">AhtiK</span><span>|</span><a href="#39638526">root</a><span>|</span><a href="#39639010">parent</a><span>|</span><a href="#39642477">next</a><span>|</span><label class="collapse" for="c-39639052">[-]</label><label class="expand" for="c-39639052">[1 more]</label></div><br/><div class="children"><div class="content">Found it from <a href="https:&#x2F;&#x2F;github.com&#x2F;Vahe1994&#x2F;SpQR">https:&#x2F;&#x2F;github.com&#x2F;Vahe1994&#x2F;SpQR</a>
Was somehow expecting it to be at <a href="https:&#x2F;&#x2F;github.com&#x2F;TimDettmers&#x2F;bitsandbytes">https:&#x2F;&#x2F;github.com&#x2F;TimDettmers&#x2F;bitsandbytes</a>. My bad.</div><br/></div></div></div></div></div></div><div id="39642477" class="c"><input type="checkbox" id="c-39642477" checked=""/><div class="controls bullet"><span class="by">curl-up</span><span>|</span><a href="#39638526">prev</a><span>|</span><a href="#39649696">next</a><span>|</span><label class="collapse" for="c-39642477">[-]</label><label class="expand" for="c-39642477">[3 more]</label></div><br/><div class="children"><div class="content">Does anyone have sources, or experience, about fine tuning primarily to teach the model some factual data, especially when it comes to later &quot;higher level&quot; question answering.<p>For example, giving the model a bunch of text (academic papers and such) about 19th century writers, then asking things like &quot;Who were the main influences on writer X&quot;?<p>Obviously simple RAG-like approaches don&#x27;t work, as such information is rarely available in the text as-is, and needs to be &quot;extrapolated&quot; to some extent. Long context models might work (just dumping everything into the prompt), but are way too expensive for my needs.</div><br/><div id="39642733" class="c"><input type="checkbox" id="c-39642733" checked=""/><div class="controls bullet"><span class="by">armcat</span><span>|</span><a href="#39642477">parent</a><span>|</span><a href="#39649696">next</a><span>|</span><label class="collapse" for="c-39642733">[-]</label><label class="expand" for="c-39642733">[2 more]</label></div><br/><div class="children"><div class="content">RAG approaches should work quite well for the examples you mentioned. It&#x27;s a matter of how you approach the retrieval part - you can opt for a larger recall on retrieval, and leverage the large context window for the LLM to figure out the answer. Even if it&#x27;s not &quot;as-is&quot;, semantically if it&#x27;s in there, it should be able to find it.<p>Other things to try out is how you approach the question &quot;expansion&quot; part, for example using Hypothetical Document Embeddings (HyDE); or how you approach the filtering-out part, e.g. using &quot;System 2 Attention&quot;, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.11829" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.11829</a>.</div><br/><div id="39642817" class="c"><input type="checkbox" id="c-39642817" checked=""/><div class="controls bullet"><span class="by">curl-up</span><span>|</span><a href="#39642477">root</a><span>|</span><a href="#39642733">parent</a><span>|</span><a href="#39649696">next</a><span>|</span><label class="collapse" for="c-39642817">[-]</label><label class="expand" for="c-39642817">[1 more]</label></div><br/><div class="children"><div class="content">I tried most of such techniques, but the point is that this information really isn&#x27;t in there directly, and to perform the question expansion, the model needs to know about the domain already.<p>For example, imagine that one paper is about how author X was French, in early 19th-c.and how they were one of the first ones to write about topic T. Another paper is about how author Y was inspired by the early 19th-c. French writers writing about T. However, this second article does not mention X at all. Asking about &quot;who were the main influences on X&quot; would not give you the second article.<p>Of course, I could run &quot;multiple-hop&quot; RAG-like process, where the model keeps asking questions itself and so on in a loop, but this becomes extremely clumsy, and the models (even GPT-4) tend to get out of hand. It is also extremely slow, of course.</div><br/></div></div></div></div></div></div><div id="39649696" class="c"><input type="checkbox" id="c-39649696" checked=""/><div class="controls bullet"><span class="by">erwincoumans</span><span>|</span><a href="#39642477">prev</a><span>|</span><a href="#39635640">next</a><span>|</span><label class="collapse" for="c-39649696">[-]</label><label class="expand" for="c-39649696">[2 more]</label></div><br/><div class="children"><div class="content">NVlink + two 3090 gives 48GB relatively easy (appears as unified memory). I only skimmed the article briefly, was it considered?</div><br/><div id="39650182" class="c"><input type="checkbox" id="c-39650182" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39649696">parent</a><span>|</span><a href="#39635640">next</a><span>|</span><label class="collapse" for="c-39650182">[-]</label><label class="expand" for="c-39650182">[1 more]</label></div><br/><div class="children"><div class="content">NVlink does not appear as unified memory. But NVlink + two 3090 does work great with FSDP&#x2F;QLoRA (I have just such a machine at home!)</div><br/></div></div></div></div><div id="39635640" class="c"><input type="checkbox" id="c-39635640" checked=""/><div class="controls bullet"><span class="by">artninja1988</span><span>|</span><a href="#39649696">prev</a><span>|</span><a href="#39638233">next</a><span>|</span><label class="collapse" for="c-39635640">[-]</label><label class="expand" for="c-39635640">[18 more]</label></div><br/><div class="children"><div class="content">So, as I understand it, this is for finetuning a preexisting llm? So not actually training one from scratch. I guess that would be too much to ask for. Nonetheless, cheers to Jeremy and the gang for the work.</div><br/><div id="39635991" class="c"><input type="checkbox" id="c-39635991" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39635640">parent</a><span>|</span><a href="#39638108">next</a><span>|</span><label class="collapse" for="c-39635991">[-]</label><label class="expand" for="c-39635991">[15 more]</label></div><br/><div class="children"><div class="content">For now, it&#x27;s for finetuning.<p>The issue of to what degree it might be possible to train a model from scratch using QLoRA is still an open question. The relora paper showed that it can work in some situations, but attempts to scale it up were unsuccessful. The recent DoRA paper perhaps might allow a &quot;re-DoRA&quot; approach to work. If so, that could be combined with quantization to do &quot;re-QDoRA&quot;!</div><br/><div id="39638289" class="c"><input type="checkbox" id="c-39638289" checked=""/><div class="controls bullet"><span class="by">qsi</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39635991">parent</a><span>|</span><a href="#39639109">next</a><span>|</span><label class="collapse" for="c-39638289">[-]</label><label class="expand" for="c-39638289">[12 more]</label></div><br/><div class="children"><div class="content">The headline and introduction on the linked page say &quot;You can now train a 70b language model at home. We’re releasing an open source system, based on FSDP and QLoRA, that can train a 70b model on two 24GB GPUs.&quot;<p>How does &quot;fine tuning&quot; differ from &quot;training?&quot; Reading the linked article I had assumed I could create my own trained LLM at home with two 24GB GPUs.</div><br/><div id="39638736" class="c"><input type="checkbox" id="c-39638736" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39638289">parent</a><span>|</span><a href="#39638580">next</a><span>|</span><label class="collapse" for="c-39638736">[-]</label><label class="expand" for="c-39638736">[6 more]</label></div><br/><div class="children"><div class="content">The article actually sneaks in a footnote that answers this (<a href="https:&#x2F;&#x2F;www.answer.ai&#x2F;posts&#x2F;2024-03-06-fsdp-qlora.html#fn1" rel="nofollow">https:&#x2F;&#x2F;www.answer.ai&#x2F;posts&#x2F;2024-03-06-fsdp-qlora.html#fn1</a>): &quot;Throughout this article “training” can refer to either pre-training, or fine-tuning&quot;.<p>(Generally, we&#x27;ve told students at fast.ai since 2017 that they should almost never be starting from random weights -- most of the time it&#x27;s best to start with a pretrained model and fine-tune that, even if it&#x27;s from a somewhat different domain to the problem you&#x27;re working on.)</div><br/><div id="39639586" class="c"><input type="checkbox" id="c-39639586" checked=""/><div class="controls bullet"><span class="by">Tomte</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39638736">parent</a><span>|</span><a href="#39638580">next</a><span>|</span><label class="collapse" for="c-39639586">[-]</label><label class="expand" for="c-39639586">[5 more]</label></div><br/><div class="children"><div class="content">Have you changed your mind on „The End of Finetuning“ (<a href="https:&#x2F;&#x2F;www.latent.space&#x2F;p&#x2F;fastai" rel="nofollow">https:&#x2F;&#x2F;www.latent.space&#x2F;p&#x2F;fastai</a> ) or did I simply misunderstand that?<p>Oh, and thanks for quirky stuff like your APL video!</div><br/><div id="39639788" class="c"><input type="checkbox" id="c-39639788" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39639586">parent</a><span>|</span><a href="#39638580">next</a><span>|</span><label class="collapse" for="c-39639788">[-]</label><label class="expand" for="c-39639788">[4 more]</label></div><br/><div class="children"><div class="content">The title of that podcast isn&#x27;t something I actually said (IIRC). I commented in that interview that I feel we should not consider pre-training and fine-tuning to be as separate as we do now.</div><br/><div id="39648894" class="c"><input type="checkbox" id="c-39648894" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39639788">parent</a><span>|</span><a href="#39639817">next</a><span>|</span><label class="collapse" for="c-39648894">[-]</label><label class="expand" for="c-39648894">[1 more]</label></div><br/><div class="children"><div class="content">&quot;The right way to fine-tune language models... is to actually throw away the idea of fine-tuning. There&#x27;s no such thing. There&#x27;s only continued pre-training.&quot;<p>:) i hope i didnt pervert your intent too too much for clickbait or something, i thought it was the spirit of what you said</div><br/></div></div><div id="39639817" class="c"><input type="checkbox" id="c-39639817" checked=""/><div class="controls bullet"><span class="by">Tomte</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39639788">parent</a><span>|</span><a href="#39648894">prev</a><span>|</span><a href="#39638580">next</a><span>|</span><label class="collapse" for="c-39639817">[-]</label><label class="expand" for="c-39639817">[2 more]</label></div><br/><div class="children"><div class="content">So you‘re generally in favor of mixing training data without separating them in phases, but when I use pretrained weights (as you recommend instead of random weights) I generally do not have access to whatever the neural net was pretrained with by someone else, so I have to make do with my finetuning data, yes?<p>Thank you!</div><br/><div id="39647385" class="c"><input type="checkbox" id="c-39647385" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39639817">parent</a><span>|</span><a href="#39638580">next</a><span>|</span><label class="collapse" for="c-39647385">[-]</label><label class="expand" for="c-39647385">[1 more]</label></div><br/><div class="children"><div class="content">Yes.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39638580" class="c"><input type="checkbox" id="c-39638580" checked=""/><div class="controls bullet"><span class="by">keremturgutlu</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39638289">parent</a><span>|</span><a href="#39638736">prev</a><span>|</span><a href="#39638593">next</a><span>|</span><label class="collapse" for="c-39638580">[-]</label><label class="expand" for="c-39638580">[3 more]</label></div><br/><div class="children"><div class="content">You most definitely can, the main difference is that only partial ~2% of the parameters get updated during training. Say you start from a model like llama-70B which already knows english and has some world knowledge based on its pretraining dataset. It might not be ideal for drastic domain shifts, such as adapting a model to learn new languages (which might require a new tokenizer and model embeddings) but still might be possible to some extent.</div><br/><div id="39638634" class="c"><input type="checkbox" id="c-39638634" checked=""/><div class="controls bullet"><span class="by">qsi</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39638580">parent</a><span>|</span><a href="#39638593">next</a><span>|</span><label class="collapse" for="c-39638634">[-]</label><label class="expand" for="c-39638634">[2 more]</label></div><br/><div class="children"><div class="content">Thank you for clarifying. I have been wanting to dip my toes into LLMs at home but obviously I have a steep learning curve ahead of me, and would need considerably beefier hardware!</div><br/><div id="39640531" class="c"><input type="checkbox" id="c-39640531" checked=""/><div class="controls bullet"><span class="by">chasd00</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39638634">parent</a><span>|</span><a href="#39638593">next</a><span>|</span><label class="collapse" for="c-39640531">[-]</label><label class="expand" for="c-39640531">[1 more]</label></div><br/><div class="children"><div class="content">It’s steep but manageable, absolutely go for it. The more people who understand the tech the better.</div><br/></div></div></div></div></div></div><div id="39638593" class="c"><input type="checkbox" id="c-39638593" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39638289">parent</a><span>|</span><a href="#39638580">prev</a><span>|</span><a href="#39639109">next</a><span>|</span><label class="collapse" for="c-39638593">[-]</label><label class="expand" for="c-39638593">[2 more]</label></div><br/><div class="children"><div class="content">You can take an existing 70B model and train it to do a more specific task. You&#x27;re teaching it the task but you&#x27;re relying on a foundation model for the base understanding of the world&#x2F;words&#x2F;etc.</div><br/><div id="39638637" class="c"><input type="checkbox" id="c-39638637" checked=""/><div class="controls bullet"><span class="by">qsi</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39638593">parent</a><span>|</span><a href="#39639109">next</a><span>|</span><label class="collapse" for="c-39638637">[-]</label><label class="expand" for="c-39638637">[1 more]</label></div><br/><div class="children"><div class="content">OK, that makes sense. Thank you!</div><br/></div></div></div></div></div></div><div id="39639109" class="c"><input type="checkbox" id="c-39639109" checked=""/><div class="controls bullet"><span class="by">hantusk</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39635991">parent</a><span>|</span><a href="#39638289">prev</a><span>|</span><a href="#39638108">next</a><span>|</span><label class="collapse" for="c-39639109">[-]</label><label class="expand" for="c-39639109">[2 more]</label></div><br/><div class="children"><div class="content">Digging into the low rank structure of the gradients, instead of the weights seems like a promising direction for training from scratch with less memory requirements: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;AnimaAnandkumar&#x2F;status&#x2F;1765613815146893348" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;AnimaAnandkumar&#x2F;status&#x2F;17656138151468933...</a></div><br/><div id="39639261" class="c"><input type="checkbox" id="c-39639261" checked=""/><div class="controls bullet"><span class="by">hantusk</span><span>|</span><a href="#39635640">root</a><span>|</span><a href="#39639109">parent</a><span>|</span><a href="#39638108">next</a><span>|</span><label class="collapse" for="c-39639261">[-]</label><label class="expand" for="c-39639261">[1 more]</label></div><br/><div class="children"><div class="content">Simo linked some older papers with this same idea: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;cloneofsimo&#x2F;status&#x2F;1765796493955674286" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;cloneofsimo&#x2F;status&#x2F;1765796493955674286</a></div><br/></div></div></div></div></div></div><div id="39638108" class="c"><input type="checkbox" id="c-39638108" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#39635640">parent</a><span>|</span><a href="#39635991">prev</a><span>|</span><a href="#39646576">next</a><span>|</span><label class="collapse" for="c-39638108">[-]</label><label class="expand" for="c-39638108">[1 more]</label></div><br/><div class="children"><div class="content">Lit-GPT is what I have been using to pretrain models at home: <a href="https:&#x2F;&#x2F;github.com&#x2F;Lightning-AI&#x2F;litgpt">https:&#x2F;&#x2F;github.com&#x2F;Lightning-AI&#x2F;litgpt</a>
Using the openwebtext example, I can train a 700M param model to 2.6 loss in a few days on dual 4090s. Pretty awesome!</div><br/></div></div><div id="39646576" class="c"><input type="checkbox" id="c-39646576" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#39635640">parent</a><span>|</span><a href="#39638108">prev</a><span>|</span><a href="#39638233">next</a><span>|</span><label class="collapse" for="c-39646576">[-]</label><label class="expand" for="c-39646576">[1 more]</label></div><br/><div class="children"><div class="content">Training a 70b model from scratch uses 80,000 GPU hours (4,6 years if you have two of those GPUs).<p>The electricity would cost more than 10,000€ in Germany, just for the GPUs.</div><br/></div></div></div></div><div id="39638233" class="c"><input type="checkbox" id="c-39638233" checked=""/><div class="controls bullet"><span class="by">pella</span><span>|</span><a href="#39635640">prev</a><span>|</span><a href="#39642853">next</a><span>|</span><label class="collapse" for="c-39638233">[-]</label><label class="expand" for="c-39638233">[2 more]</label></div><br/><div class="children"><div class="content">&gt; the ability to use multiple GPUs with QLoRA training.<p>Thorough article!<p>Question: What&#x27;s your opinion on:<p>- How viable will NVIDIA&#x27;s consumer cards be in the long run?<p>- Besides <a href="https:&#x2F;&#x2F;tinygrad.org" rel="nofollow">https:&#x2F;&#x2F;tinygrad.org</a>, what other cost-effective future alternatives could there be?</div><br/><div id="39638679" class="c"><input type="checkbox" id="c-39638679" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#39638233">parent</a><span>|</span><a href="#39642853">next</a><span>|</span><label class="collapse" for="c-39638679">[-]</label><label class="expand" for="c-39638679">[1 more]</label></div><br/><div class="children"><div class="content">Unsloth (mentioned in the Answer.AI post) is planning multi-GPU support in a future release.</div><br/></div></div></div></div><div id="39642853" class="c"><input type="checkbox" id="c-39642853" checked=""/><div class="controls bullet"><span class="by">ericd</span><span>|</span><a href="#39638233">prev</a><span>|</span><a href="#39640762">next</a><span>|</span><label class="collapse" for="c-39642853">[-]</label><label class="expand" for="c-39642853">[1 more]</label></div><br/><div class="children"><div class="content">This is the best news I’ve seen all month. I think one of the great near-term dangers of AI is the bulk of the economic benefit going mainly to relatively few companies. That risk seems substantially reduced if they have to compete with a great variety of models.</div><br/></div></div><div id="39640762" class="c"><input type="checkbox" id="c-39640762" checked=""/><div class="controls bullet"><span class="by">jl6</span><span>|</span><a href="#39642853">prev</a><span>|</span><a href="#39640108">next</a><span>|</span><label class="collapse" for="c-39640762">[-]</label><label class="expand" for="c-39640762">[1 more]</label></div><br/><div class="children"><div class="content">Besides being a great result, the quality and clarity of the technical writing here is excellent.</div><br/></div></div><div id="39640108" class="c"><input type="checkbox" id="c-39640108" checked=""/><div class="controls bullet"><span class="by">Kelteseth</span><span>|</span><a href="#39640762">prev</a><span>|</span><a href="#39638127">next</a><span>|</span><label class="collapse" for="c-39640108">[-]</label><label class="expand" for="c-39640108">[2 more]</label></div><br/><div class="children"><div class="content">Any plans on supporting AMD? In Germany, the price of an 7900XTX is HALF of a NV 4090...</div><br/><div id="39641693" class="c"><input type="checkbox" id="c-39641693" checked=""/><div class="controls bullet"><span class="by">slices</span><span>|</span><a href="#39640108">parent</a><span>|</span><a href="#39638127">next</a><span>|</span><label class="collapse" for="c-39641693">[-]</label><label class="expand" for="c-39641693">[1 more]</label></div><br/><div class="children"><div class="content">take a look at recent posts from <a href="https:&#x2F;&#x2F;twitter.com&#x2F;__tinygrad__" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;__tinygrad__</a> re: the state of AMD for AI work</div><br/></div></div></div></div><div id="39638127" class="c"><input type="checkbox" id="c-39638127" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#39640108">prev</a><span>|</span><a href="#39645628">next</a><span>|</span><label class="collapse" for="c-39638127">[-]</label><label class="expand" for="c-39638127">[1 more]</label></div><br/><div class="children"><div class="content">Nice, I tried to use QLoRA+FSDP in the past with litgpt and obviously at that time it did not work. This is very useful!</div><br/></div></div><div id="39645628" class="c"><input type="checkbox" id="c-39645628" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#39638127">prev</a><span>|</span><a href="#39641881">next</a><span>|</span><label class="collapse" for="c-39645628">[-]</label><label class="expand" for="c-39645628">[1 more]</label></div><br/><div class="children"><div class="content">If I wanted to use this software to finetune a 70b model on two 3090s to write fiction, what is the maximum sequence length that would be practical? I&#x27;m at the dataset collection stage, but I&#x27;m not sure whether to aim for bigger or smaller sequence lengths at the moment.</div><br/></div></div><div id="39641881" class="c"><input type="checkbox" id="c-39641881" checked=""/><div class="controls bullet"><span class="by">openquery</span><span>|</span><a href="#39645628">prev</a><span>|</span><a href="#39645330">next</a><span>|</span><label class="collapse" for="c-39641881">[-]</label><label class="expand" for="c-39641881">[2 more]</label></div><br/><div class="children"><div class="content">This article is very well written and super informative. One thing I didn&#x27;t understand is:<p>&gt; At Answer.AI our north star is making useful AI more accessible. $150,000 to create your own high-quality personalized model definitely doesn’t count as accessible!<p>Renting an A100 on RunPod is ~$1.89 &#x2F; hour. So you&#x27;d need ~80,000 A100 hours to train a useful AI model?</div><br/><div id="39642169" class="c"><input type="checkbox" id="c-39642169" checked=""/><div class="controls bullet"><span class="by">humansareok1</span><span>|</span><a href="#39641881">parent</a><span>|</span><a href="#39645330">next</a><span>|</span><label class="collapse" for="c-39642169">[-]</label><label class="expand" for="c-39642169">[1 more]</label></div><br/><div class="children"><div class="content">In the post it explicitly says you can train on 2 3090 level cards which are significantly cheaper and the headline literally says &quot;Finetune&quot; Not &quot;Pretrain&quot;</div><br/></div></div></div></div><div id="39645330" class="c"><input type="checkbox" id="c-39645330" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#39641881">prev</a><span>|</span><a href="#39642662">next</a><span>|</span><label class="collapse" for="c-39645330">[-]</label><label class="expand" for="c-39645330">[1 more]</label></div><br/><div class="children"><div class="content">This is great.<p>I don&#x27;t think local will be competitive in future IN GENERAL...but if I have a specific use case and I have a specific training dataset...local with specific training will murder the big commercial models.</div><br/></div></div><div id="39642662" class="c"><input type="checkbox" id="c-39642662" checked=""/><div class="controls bullet"><span class="by">Nouser76</span><span>|</span><a href="#39645330">prev</a><span>|</span><a href="#39646297">next</a><span>|</span><label class="collapse" for="c-39642662">[-]</label><label class="expand" for="c-39642662">[2 more]</label></div><br/><div class="children"><div class="content">Is there any framework&#x2F;system that distributes the work across multiple GPUs on different computers over a network (LAN or WAN)? I&#x27;m not concerned much about latency or generation time, but would love to train or load up huge models and send jobs to run overnight.</div><br/><div id="39648825" class="c"><input type="checkbox" id="c-39648825" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#39642662">parent</a><span>|</span><a href="#39646297">next</a><span>|</span><label class="collapse" for="c-39648825">[-]</label><label class="expand" for="c-39648825">[1 more]</label></div><br/><div class="children"><div class="content">Yes, you can run FSDP&#x2F;QLoRA over multi-node. There&#x27;s a slurm script in the repo showing how to do it.</div><br/></div></div></div></div><div id="39646297" class="c"><input type="checkbox" id="c-39646297" checked=""/><div class="controls bullet"><span class="by">JoelJacobson</span><span>|</span><a href="#39642662">prev</a><span>|</span><a href="#39643051">next</a><span>|</span><label class="collapse" for="c-39646297">[-]</label><label class="expand" for="c-39646297">[1 more]</label></div><br/><div class="children"><div class="content">Do the two 4090 GPUs need to be on the same machine, or is it possible to somehow use two separate machines, each with its own 4090, and link them somehow via e.g. InfiniBand?</div><br/></div></div><div id="39643051" class="c"><input type="checkbox" id="c-39643051" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#39646297">prev</a><span>|</span><a href="#39639353">next</a><span>|</span><label class="collapse" for="c-39643051">[-]</label><label class="expand" for="c-39643051">[1 more]</label></div><br/><div class="children"><div class="content">Nice, i&#x27;ve been hoping this would be possible for a while.  I&#x27;ll have to do a new fine-tune of Inkbot on top of one of the 70b models.<p>What are the max context lengths &#x2F; batch sizes you can train at with this method for 2x24gb? What about 4x24gb?</div><br/></div></div><div id="39639353" class="c"><input type="checkbox" id="c-39639353" checked=""/><div class="controls bullet"><span class="by">delegate</span><span>|</span><a href="#39643051">prev</a><span>|</span><a href="#39638896">next</a><span>|</span><label class="collapse" for="c-39639353">[-]</label><label class="expand" for="c-39639353">[2 more]</label></div><br/><div class="children"><div class="content">Maybe I&#x27;ve missed it in the article - but how long would a full training run take on 2 consumer GPUs (local or rented) ? Ballpark - hours, days... ?</div><br/><div id="39639855" class="c"><input type="checkbox" id="c-39639855" checked=""/><div class="controls bullet"><span class="by">gardnr</span><span>|</span><a href="#39639353">parent</a><span>|</span><a href="#39638896">next</a><span>|</span><label class="collapse" for="c-39639855">[-]</label><label class="expand" for="c-39639855">[1 more]</label></div><br/><div class="children"><div class="content">The author is discussing fine-tuning a base model. How long it takes really depends on the dataset, the method, and the hyperparameters. DPO, for example, can achieve some great results with a fraction of the steps of other methods.<p>Just like with unsloth or axolotl, the people that use this will have to make compromises that give results in a reasonable amount of time.</div><br/></div></div></div></div><div id="39638896" class="c"><input type="checkbox" id="c-39638896" checked=""/><div class="controls bullet"><span class="by">tbenst</span><span>|</span><a href="#39639353">prev</a><span>|</span><a href="#39646556">next</a><span>|</span><label class="collapse" for="c-39638896">[-]</label><label class="expand" for="c-39638896">[1 more]</label></div><br/><div class="children"><div class="content">Very interesting but hard to interpret until the performance numbers &#x2F; benchmarks are available. I can already fine-tune a 70B language model at home using CPU + RAM, but it would be so slow as to be almost totally impractical (~20x slower than GPU). It would be great to see a comparison to eg 8 x A100 (available for $32&#x2F;hr on AWS on-demand) and also CPU + RAM. Presumably it’s somewhere in between, but hard to predict where!</div><br/></div></div><div id="39646556" class="c"><input type="checkbox" id="c-39646556" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#39638896">prev</a><span>|</span><a href="#39642630">next</a><span>|</span><label class="collapse" for="c-39646556">[-]</label><label class="expand" for="c-39646556">[1 more]</label></div><br/><div class="children"><div class="content">Congratulations, fantastic contribution to open source AI. 
Why does the website headline say &quot;train&quot; instead of &quot;finetune&quot;?</div><br/></div></div><div id="39642630" class="c"><input type="checkbox" id="c-39642630" checked=""/><div class="controls bullet"><span class="by">jiwidi</span><span>|</span><a href="#39646556">prev</a><span>|</span><a href="#39640693">next</a><span>|</span><label class="collapse" for="c-39642630">[-]</label><label class="expand" for="c-39642630">[1 more]</label></div><br/><div class="children"><div class="content">&gt; home<p>&gt;  two 24GB GPUs.<p>geez</div><br/></div></div><div id="39640693" class="c"><input type="checkbox" id="c-39640693" checked=""/><div class="controls bullet"><span class="by">zerop</span><span>|</span><a href="#39642630">prev</a><span>|</span><label class="collapse" for="c-39640693">[-]</label><label class="expand" for="c-39640693">[2 more]</label></div><br/><div class="children"><div class="content">Question - Can I use this to retrain an LLM (70B) weights on my own data? I am using RAG as of now for asking questions on my text, but always wonder if I could retrain an LLM on my own text. Thoughts?</div><br/><div id="39640944" class="c"><input type="checkbox" id="c-39640944" checked=""/><div class="controls bullet"><span class="by">AnthusAI</span><span>|</span><a href="#39640693">parent</a><span>|</span><label class="collapse" for="c-39640944">[-]</label><label class="expand" for="c-39640944">[1 more]</label></div><br/><div class="children"><div class="content">Fine tuning is generally not the best way to teach an LLM new knowledge.  RAG is still more appropriate.   Fine tuning is generally more effective for controlling the format of the responses but it&#x27;s not going to teach the model a lot of new concepts.  The model can learn how to handle new vocabulary through fine tuning but it&#x27;s not a great way to teach the model new facts.  Giving it access to a knowledge base is a better way to do that.</div><br/></div></div></div></div></div></div></div></div></div></body></html>