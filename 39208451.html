<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1706778072749" as="style"/><link rel="stylesheet" href="styles.css?v=1706778072749"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://2mb.codes/~cmb/ollama-bot/">Show HN: Some blind hackers are bridging IRC to LMMs running locally</a> <span class="domain">(<a href="https://2mb.codes">2mb.codes</a>)</span></div><div class="subtext"><span>blindgeek</span> | <span>37 comments</span></div><br/><div><div id="39212959" class="c"><input type="checkbox" id="c-39212959" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#39213155">next</a><span>|</span><label class="collapse" for="c-39212959">[-]</label><label class="expand" for="c-39212959">[3 more]</label></div><br/><div class="children"><div class="content">Interesting, I also manage an IRC bot with multimodal capability for months now. It&#x27;s not a real LMM - rather, a combination of 3 models. It uses Llava for images and Whisper for audio. The pipeline is simple: if it finds a URL which looks like an image - it feeds it to Llava (same with audio). Llava&#x27;s response is injected back to the main LLM (a round robin of Solar 10.7B and Llama 13B) to provide the response in the style of the bot&#x27;s character (persona) and in the context of the conversation. I run it locally on my RTX 3060 using llama.cpp. Additionally, it&#x27;s also able to search on Wikipedia, in the news (provided by Yahoo RSS) and can open HTML pages (if it sees a URL which is not an image or audio).<p>Llava is a surprisingly good model for its size. However, what I found is that it often hallucinates &quot;2 people in the background&quot; for many images.<p>I made the bot just to explore how far I can go with local off-the-shelf LLMs, I never thought it could be useful for blind people, interesting. A practical idea I had on my mind was to hook it to a webcam so that if something interesting happens in front of my house, I can be notified by the bot, for example. I guess it could also be useful for blind people if the camera is mounted on the body.</div><br/><div id="39213214" class="c"><input type="checkbox" id="c-39213214" checked=""/><div class="controls bullet"><span class="by">gs17</span><span>|</span><a href="#39212959">parent</a><span>|</span><a href="#39213155">next</a><span>|</span><label class="collapse" for="c-39213214">[-]</label><label class="expand" for="c-39213214">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Llava is a surprisingly good model for its size. However, what I found is that it often hallucinates &quot;2 people in the background&quot; for many images.<p>There&#x27;s a creepypasta or an SCP entry in there. You do not recognize the people in the background.</div><br/><div id="39213971" class="c"><input type="checkbox" id="c-39213971" checked=""/><div class="controls bullet"><span class="by">dontupvoteme</span><span>|</span><a href="#39212959">root</a><span>|</span><a href="#39213214">parent</a><span>|</span><a href="#39213155">next</a><span>|</span><label class="collapse" for="c-39213971">[-]</label><label class="expand" for="c-39213971">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re reverse vampires.  Humans can only see their reflection.</div><br/></div></div></div></div></div></div><div id="39213155" class="c"><input type="checkbox" id="c-39213155" checked=""/><div class="controls bullet"><span class="by">codeofdusk</span><span>|</span><a href="#39212959">prev</a><span>|</span><a href="#39212709">next</a><span>|</span><label class="collapse" for="c-39213155">[-]</label><label class="expand" for="c-39213155">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m also totally blind and, somewhat relatedly, I&#x27;ve built Gptcmd, a small console app to ease GPT conversation and experimentation (see the readme for more on what it does, with inline demo). Version 2.0 will get GPT vision (image) support:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;codeofdusk&#x2F;gptcmd">https:&#x2F;&#x2F;github.com&#x2F;codeofdusk&#x2F;gptcmd</a></div><br/></div></div><div id="39212709" class="c"><input type="checkbox" id="c-39212709" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39213155">prev</a><span>|</span><a href="#39210469">next</a><span>|</span><label class="collapse" for="c-39212709">[-]</label><label class="expand" for="c-39212709">[2 more]</label></div><br/><div class="children"><div class="content">I had an interesting conversation the other day about how best to make ChatGPT style &quot;streaming&quot; interfaces accessible to screenreaders, where text updates as it streams in.<p>It&#x27;s not easy! <a href="https:&#x2F;&#x2F;fedi.simonwillison.net&#x2F;@simon&#x2F;111836275974119220" rel="nofollow">https:&#x2F;&#x2F;fedi.simonwillison.net&#x2F;@simon&#x2F;111836275974119220</a></div><br/><div id="39213142" class="c"><input type="checkbox" id="c-39213142" checked=""/><div class="controls bullet"><span class="by">codeofdusk</span><span>|</span><a href="#39212709">parent</a><span>|</span><a href="#39210469">next</a><span>|</span><label class="collapse" for="c-39213142">[-]</label><label class="expand" for="c-39213142">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m totally blind and built Gptcmd, a small console app to make interacting with GPT, manipulating context&#x2F;conversations, etc. easier. Since it&#x27;s just a console app, when streaming is enabled, output is seemlessly reported.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;codeofdusk&#x2F;gptcmd">https:&#x2F;&#x2F;github.com&#x2F;codeofdusk&#x2F;gptcmd</a></div><br/></div></div></div></div><div id="39210469" class="c"><input type="checkbox" id="c-39210469" checked=""/><div class="controls bullet"><span class="by">jpsouth</span><span>|</span><a href="#39212709">prev</a><span>|</span><a href="#39213841">next</a><span>|</span><label class="collapse" for="c-39210469">[-]</label><label class="expand" for="c-39210469">[21 more]</label></div><br/><div class="children"><div class="content">Hey! I don’t understand too much about AI&#x2F;ML&#x2F;LLMs (and now LMMs!) so hoping someone could explain a little further for me?<p>What I gather is this is an IRC bot&#x2F;plugin&#x2F;add-on that will allow a user to prompt an ‘LMM’ which is essentially an LLM with multiple output capabilities (text, audio, images etc) which on the surface sounds awesome.<p>How does an LMM benefit blind users over an LLM with voice capability? Is the addition of image&#x2F;video just for accessibility to none-blind people?<p>What’s the difference between this and integrating an LLM with voice&#x2F;image&#x2F;video capability?<p>Is there any reason that this has been made over other available uncensored&#x2F;free&#x2F;local LLMs (aside from this being an LMM)?<p>Thanks in advance.</div><br/><div id="39210597" class="c"><input type="checkbox" id="c-39210597" checked=""/><div class="controls bullet"><span class="by">petercooper</span><span>|</span><a href="#39210469">parent</a><span>|</span><a href="#39210520">next</a><span>|</span><label class="collapse" for="c-39210597">[-]</label><label class="expand" for="c-39210597">[10 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the multimodal <i>input</i> capability that seems to be of value here – see the transcript at <a href="https:&#x2F;&#x2F;2mb.codes&#x2F;~cmb&#x2F;ollama-bot&#x2F;#chat-transcript" rel="nofollow">https:&#x2F;&#x2F;2mb.codes&#x2F;~cmb&#x2F;ollama-bot&#x2F;#chat-transcript</a> .. Namely, being able to interrogate images in a verbal fashion, such that someone without sight (or perhaps even someone who just doesn&#x27;t <i>want</i> to see an image) can get an appreciation for their contents.</div><br/><div id="39210705" class="c"><input type="checkbox" id="c-39210705" checked=""/><div class="controls bullet"><span class="by">blindgeek</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39210597">parent</a><span>|</span><a href="#39210776">next</a><span>|</span><label class="collapse" for="c-39210705">[-]</label><label class="expand" for="c-39210705">[7 more]</label></div><br/><div class="children"><div class="content">Yes, the image interrogation is exactly the point.
This all started out when my friend said that it would be cool to be
able to chat on IRC with an LLM running on his own hardware.  And then we were
like, oh hey, we can get this thing to describe images for us
if we use an LMM.<p>The next thing we want to do is obtain some glasses with cameras and wi-fi and
send images to ollama from them for real-time description.  The benefits
are obvious, especially for mobility purposes.</div><br/><div id="39210848" class="c"><input type="checkbox" id="c-39210848" checked=""/><div class="controls bullet"><span class="by">jpsouth</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39210705">parent</a><span>|</span><a href="#39211888">next</a><span>|</span><label class="collapse" for="c-39210848">[-]</label><label class="expand" for="c-39210848">[5 more]</label></div><br/><div class="children"><div class="content">This is so cool. I’d ask how it works, however I feel like I wouldn’t understand at a fundamental level, even if I read through your codebase. Interpreting an image in the concept of a machine baffles me, it doesn’t have eyes. It surely can’t sense light like humans can. It can’t possibly understand depth (the sofa is in the far left background?!). It can’t know what a goatee is, based on some pixels that are mildly different colours than the skin or background. These are all assumptions I’ve made coming into this, and I am relatively sure I’m wrong at this stage.<p>If you’d like to briefly post I’m sure a lot of HN denizens would appreciate it however. I’ll just stand at the sidelines, post this and spectate the commentary and try it myself with a small group.</div><br/><div id="39210942" class="c"><input type="checkbox" id="c-39210942" checked=""/><div class="controls bullet"><span class="by">blindgeek</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39210848">parent</a><span>|</span><a href="#39213938">next</a><span>|</span><label class="collapse" for="c-39210942">[-]</label><label class="expand" for="c-39210942">[1 more]</label></div><br/><div class="children"><div class="content">To be completely honest, I don&#x27;t really know what I&#x27;m doing.  The IRC
bot I wrote isn&#x27;t complicated at all; it basically just acts as a bridge
between IRC and a program that has an HTTP API.
FWIW I&#x27;ve never written an IRC bot before, so this is &quot;baby&#x27;s first bot&quot;.
I also wrote it in Go, even though I&#x27;m not a Go programmer.  Probably all of
that shines through in the code.<p>The real magic happens in [ollama](<a href="https:&#x2F;&#x2F;ollama.ai&#x2F;">https:&#x2F;&#x2F;ollama.ai&#x2F;</a>), which lets you run
LMMs locally.</div><br/></div></div><div id="39213938" class="c"><input type="checkbox" id="c-39213938" checked=""/><div class="controls bullet"><span class="by">justsomehnguy</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39210848">parent</a><span>|</span><a href="#39210942">prev</a><span>|</span><a href="#39211411">next</a><span>|</span><label class="collapse" for="c-39213938">[-]</label><label class="expand" for="c-39213938">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Interpreting an image in the concept of a machine baffles me, it doesn’t have eyes<p>Your mistake here is thinking what machine has understanding of anything. It doesn&#x27;t. But if you know how human learning works, what is a compression and what is a lossy compression then it is quite easy to understand.<p>Machine is fed with tons of images with word references what is in the image. Then it finds what is similar in the images of a similar objects, ie works just like a compression algo, except it doesn&#x27;t store the exact matches but relationships of some markers it finds in the images. That&#x27;s why it doesn&#x27;t and doesn&#x27;t need to understand where is sofa and what is a sofa, it just have a relationship between something what has a relationship to the word &#x27;sofa&#x27; and relationship with something what we, human describe as &#x27;position&#x27;.</div><br/></div></div><div id="39211411" class="c"><input type="checkbox" id="c-39211411" checked=""/><div class="controls bullet"><span class="by">carom</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39210848">parent</a><span>|</span><a href="#39213938">prev</a><span>|</span><a href="#39211888">next</a><span>|</span><label class="collapse" for="c-39211411">[-]</label><label class="expand" for="c-39211411">[2 more]</label></div><br/><div class="children"><div class="content">Have you tried ChatGPT yet? It can describe images quite well.</div><br/><div id="39211701" class="c"><input type="checkbox" id="c-39211701" checked=""/><div class="controls bullet"><span class="by">rolltrunhert</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39211411">parent</a><span>|</span><a href="#39211888">next</a><span>|</span><label class="collapse" for="c-39211701">[-]</label><label class="expand" for="c-39211701">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t quite fit the bill of running on their own hardware</div><br/></div></div></div></div></div></div><div id="39211888" class="c"><input type="checkbox" id="c-39211888" checked=""/><div class="controls bullet"><span class="by">loa_in_</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39210705">parent</a><span>|</span><a href="#39210848">prev</a><span>|</span><a href="#39210776">next</a><span>|</span><label class="collapse" for="c-39211888">[-]</label><label class="expand" for="c-39211888">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s already a thing like this from Google. It&#x27;s called lookout I think</div><br/></div></div></div></div><div id="39210776" class="c"><input type="checkbox" id="c-39210776" checked=""/><div class="controls bullet"><span class="by">jpsouth</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39210597">parent</a><span>|</span><a href="#39210705">prev</a><span>|</span><a href="#39210520">next</a><span>|</span><label class="collapse" for="c-39210776">[-]</label><label class="expand" for="c-39210776">[2 more]</label></div><br/><div class="children"><div class="content">Thanks! I saw that bit but honestly, skipped past it due to being on mobile and assuming it was just a list of commands. Must’ve missed the header!<p>Very impressed with the capability here given that transcript, I’ll certainly try it myself. Thank you!</div><br/><div id="39210929" class="c"><input type="checkbox" id="c-39210929" checked=""/><div class="controls bullet"><span class="by">baffled</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39210776">parent</a><span>|</span><a href="#39210520">next</a><span>|</span><label class="collapse" for="c-39210929">[-]</label><label class="expand" for="c-39210929">[1 more]</label></div><br/><div class="children"><div class="content">You can give the current version a test drive at irc.oftc.net channel
#speakup. The journey has been fun so far.</div><br/></div></div></div></div></div></div><div id="39210520" class="c"><input type="checkbox" id="c-39210520" checked=""/><div class="controls bullet"><span class="by">jpsouth</span><span>|</span><a href="#39210469">parent</a><span>|</span><a href="#39210597">prev</a><span>|</span><a href="#39213841">next</a><span>|</span><label class="collapse" for="c-39210520">[-]</label><label class="expand" for="c-39210520">[10 more]</label></div><br/><div class="children"><div class="content">As a follow up to this I’d like to ask any partially sighted or blind people the issues they currently experience using a LLM such as ChatGPT, Bard, Llama or otherwise - both from a UI perspective and an API perspective.</div><br/><div id="39210745" class="c"><input type="checkbox" id="c-39210745" checked=""/><div class="controls bullet"><span class="by">blindgeek</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39210520">parent</a><span>|</span><a href="#39211043">next</a><span>|</span><label class="collapse" for="c-39210745">[-]</label><label class="expand" for="c-39210745">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m blind, but I wouldn&#x27;t know.  I haven&#x27;t used bard or gpt or any of that,
and I don&#x27;t plan on doing so.  I thought about playing with GPT back in
early 2023, when the hype really started picking up.  But when I realized
that they wanted my phone number, I noped out.</div><br/><div id="39210899" class="c"><input type="checkbox" id="c-39210899" checked=""/><div class="controls bullet"><span class="by">jpsouth</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39210745">parent</a><span>|</span><a href="#39212711">next</a><span>|</span><label class="collapse" for="c-39210899">[-]</label><label class="expand" for="c-39210899">[4 more]</label></div><br/><div class="children"><div class="content">Understood, thanks for the reply, blindgeek.<p>An additional question, if you don’t mind answering (and there’s zero obligation to). How have you found accessibility has changed on the web over the years? We have many tools these days to assist but do you feel there’s been a notable improvement to what used to be in place?</div><br/><div id="39211037" class="c"><input type="checkbox" id="c-39211037" checked=""/><div class="controls bullet"><span class="by">blindgeek</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39210899">parent</a><span>|</span><a href="#39212711">next</a><span>|</span><label class="collapse" for="c-39211037">[-]</label><label class="expand" for="c-39211037">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been online in some form or other since 1993.  Back in 1993, everything
was basically accessible by default, because it was plain text.<p>No, there has not been an improvement, and in fact, things have gotten worse
in a lot of ways.  Most of that is due to SPAs, and people who decide
to use JavaScript when HTML widgets would suffice.<p>I&#x27;m also involved with a text mode web browser project, [edbrowse](<a href="https:&#x2F;&#x2F;edbrowse.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;edbrowse.org&#x2F;</a>).
Ten years ago, it was feasible to use edbrowse for a great deal of online
activity.  For instance, I used it to make purchases from Amazon and other
online stores.  I could log into Paypal and send money with it.<p>Then, in the mid 2010s or so, SPAs started becoming a thing and edbrowse
broke on more and more sites.  At this point, in 2024, I can&#x27;t even use it
to read READMEs on Github.<p>And yes, I have accessibility trouble when using mainstream browsers too,
all the time.</div><br/><div id="39212325" class="c"><input type="checkbox" id="c-39212325" checked=""/><div class="controls bullet"><span class="by">jpsouth</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39211037">parent</a><span>|</span><a href="#39211421">next</a><span>|</span><label class="collapse" for="c-39212325">[-]</label><label class="expand" for="c-39212325">[1 more]</label></div><br/><div class="children"><div class="content">This is shocking, I can’t do much about the wider industry but I’m a huge advocate for accessibility on the web, I’ll continue being a pain in the arse to POs, PMs, devs and general management pushing for this kind of change.<p>If everyone put their own loved ones in the situation of others, life would be different.</div><br/></div></div><div id="39211421" class="c"><input type="checkbox" id="c-39211421" checked=""/><div class="controls bullet"><span class="by">zardo</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39211037">parent</a><span>|</span><a href="#39212325">prev</a><span>|</span><a href="#39212711">next</a><span>|</span><label class="collapse" for="c-39211421">[-]</label><label class="expand" for="c-39211421">[1 more]</label></div><br/><div class="children"><div class="content">The DOJ takes the position that the ADA applies to websites and they&#x27;ve done something about it like six times. If only they would increase enforcement efforts about 10000 fold.</div><br/></div></div></div></div></div></div></div></div><div id="39211043" class="c"><input type="checkbox" id="c-39211043" checked=""/><div class="controls bullet"><span class="by">devinprater</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39210520">parent</a><span>|</span><a href="#39210745">prev</a><span>|</span><a href="#39212267">next</a><span>|</span><label class="collapse" for="c-39211043">[-]</label><label class="expand" for="c-39211043">[1 more]</label></div><br/><div class="children"><div class="content">I answer a lot of this on a forum post on the OpenAI forum [1], but basically, thank goodness for API&#x27;s and screen reader addons that make use of them, because the only two AI&#x27;s that are even remotely easy to use and accessible are Bard and Bing.<p>[1] <a href="https:&#x2F;&#x2F;community.openai.com&#x2F;t&#x2F;how-are-blind-people-using-openai-technology&#x2F;591018&#x2F;" rel="nofollow">https:&#x2F;&#x2F;community.openai.com&#x2F;t&#x2F;how-are-blind-people-using-op...</a></div><br/></div></div><div id="39212267" class="c"><input type="checkbox" id="c-39212267" checked=""/><div class="controls bullet"><span class="by">miki123211</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39210520">parent</a><span>|</span><a href="#39211043">prev</a><span>|</span><a href="#39213841">next</a><span>|</span><label class="collapse" for="c-39212267">[-]</label><label class="expand" for="c-39212267">[2 more]</label></div><br/><div class="children"><div class="content">Blind person here. GPT Vision is extremely sensitive and refuses to describe any content that it finds obscene. It&#x27;s not just porn, its false positive rate is extremely high and it&#x27;s not unusual for completely innocent images to be blocked.</div><br/><div id="39212425" class="c"><input type="checkbox" id="c-39212425" checked=""/><div class="controls bullet"><span class="by">jpsouth</span><span>|</span><a href="#39210469">root</a><span>|</span><a href="#39212267">parent</a><span>|</span><a href="#39213841">next</a><span>|</span><label class="collapse" for="c-39212425">[-]</label><label class="expand" for="c-39212425">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for your input! It’s interesting it’s oversensitive to this, I wonder what the safeguards are, and how strict they can be. This is a game changer I would imagine for anyone not able to see, so hopefully you get a better bit of kit very soon.<p>If there’s anything you need additional support with, check out ‘Be My Eyes’ app and hopefully someone will be useful for you!</div><br/></div></div></div></div></div></div></div></div><div id="39213841" class="c"><input type="checkbox" id="c-39213841" checked=""/><div class="controls bullet"><span class="by">nathias</span><span>|</span><a href="#39210469">prev</a><span>|</span><a href="#39209415">next</a><span>|</span><label class="collapse" for="c-39213841">[-]</label><label class="expand" for="c-39213841">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been waiting 25 years for this</div><br/></div></div><div id="39209415" class="c"><input type="checkbox" id="c-39209415" checked=""/><div class="controls bullet"><span class="by">DustinBrett</span><span>|</span><a href="#39213841">prev</a><span>|</span><a href="#39210319">next</a><span>|</span><label class="collapse" for="c-39209415">[-]</label><label class="expand" for="c-39209415">[1 more]</label></div><br/><div class="children"><div class="content">You could run an LLM in the browser with WebLLM and then connect to IRC via WebSockets using something like KiwiIRC. Fully client side AI on IRC.</div><br/></div></div><div id="39210319" class="c"><input type="checkbox" id="c-39210319" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#39209415">prev</a><span>|</span><a href="#39208902">next</a><span>|</span><label class="collapse" for="c-39210319">[-]</label><label class="expand" for="c-39210319">[2 more]</label></div><br/><div class="children"><div class="content">If you didn&#x27;t know... LMM = Large Multimodal Models</div><br/></div></div></div></div></div></div></div></body></html>