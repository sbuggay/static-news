<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1703408453838" as="style"/><link rel="stylesheet" href="styles.css?v=1703408453838"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2304.06835">Translation and accelerated solving of differential equations on GPU platforms</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>UncleOxidant</span> | <span>31 comments</span></div><br/><div><div id="38749876" class="c"><input type="checkbox" id="c-38749876" checked=""/><div class="controls bullet"><span class="by">yablak</span><span>|</span><a href="#38750046">next</a><span>|</span><label class="collapse" for="c-38749876">[-]</label><label class="expand" for="c-38749876">[13 more]</label></div><br/><div class="children"><div class="content">Uhh they time the vmap of the jit on Jax, basically skipping a ton of optimizations,.esp if there is any linear algebra in there.  They also include the cost of building the vmap functional.  Not a valid comparison.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;utkarsh530&#x2F;GPUODEBenchmarks&#x2F;blob&#x2F;ef807198584e4634c82bfd6a82bb9241c51d56d1&#x2F;GPU_ODE_JAX&#x2F;bench_diffrax.py#L87">https:&#x2F;&#x2F;github.com&#x2F;utkarsh530&#x2F;GPUODEBenchmarks&#x2F;blob&#x2F;ef807198...</a></div><br/><div id="38749952" class="c"><input type="checkbox" id="c-38749952" checked=""/><div class="controls bullet"><span class="by">yablak</span><span>|</span><a href="#38749876">parent</a><span>|</span><a href="#38749997">next</a><span>|</span><label class="collapse" for="c-38749952">[-]</label><label class="expand" for="c-38749952">[1 more]</label></div><br/><div class="children"><div class="content">Same for pytorch.  I don&#x27;t know enough pyt, but guessing they didn&#x27;t jit anything.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;utkarsh530&#x2F;GPUODEBenchmarks&#x2F;blob&#x2F;ef807198584e4634c82bfd6a82bb9241c51d56d1&#x2F;GPU_ODE_PyTorch&#x2F;bench_torchdiffeq.py#L82">https:&#x2F;&#x2F;github.com&#x2F;utkarsh530&#x2F;GPUODEBenchmarks&#x2F;blob&#x2F;ef807198...</a></div><br/></div></div><div id="38749997" class="c"><input type="checkbox" id="c-38749997" checked=""/><div class="controls bullet"><span class="by">geysersam</span><span>|</span><a href="#38749876">parent</a><span>|</span><a href="#38749952">prev</a><span>|</span><a href="#38749896">next</a><span>|</span><label class="collapse" for="c-38749997">[-]</label><label class="expand" for="c-38749997">[4 more]</label></div><br/><div class="children"><div class="content">This is very interesting. The claim does sound too good to be true. Am I understanding you correctly that you are saying including the vmap operation in the timing is wrong because it involves compilation time that could have been amortized over all the runs, and that the compilation time is considerable compared to the ode-solve itself?</div><br/><div id="38750178" class="c"><input type="checkbox" id="c-38750178" checked=""/><div class="controls bullet"><span class="by">yablak</span><span>|</span><a href="#38749876">root</a><span>|</span><a href="#38749997">parent</a><span>|</span><a href="#38749896">next</a><span>|</span><label class="collapse" for="c-38750178">[-]</label><label class="expand" for="c-38750178">[3 more]</label></div><br/><div class="children"><div class="content">There are two things going on.  First, you&#x27;re right that the vmap should have been done once outside the timing.  But equally important, vmap(jit(...)) speed will generally be lower than jit(vmap(...)).<p>There are many reasons for this. First, the former will loop iterations on the GPU in a serial fashion.  Second, the internal jit makes optimization options opaque to Jax.  For example, if there&#x27;s a loop of matmuls inside main, that loop can be converted to a loop of einsums if you vmap first.  It can also be fused into sometimes into a bigger operation that doesn&#x27;t jump control variables back and forth between CPU and GPU between time steps.  Between the two you both increase throughput and decrease latency.<p>I think in Jax, jit(vmap(jit(...))) will also reoptimize the same way as jit(vmap(...)) but I&#x27;m not 100% certain.</div><br/><div id="38751307" class="c"><input type="checkbox" id="c-38751307" checked=""/><div class="controls bullet"><span class="by">ubj</span><span>|</span><a href="#38749876">root</a><span>|</span><a href="#38750178">parent</a><span>|</span><a href="#38749896">next</a><span>|</span><label class="collapse" for="c-38751307">[-]</label><label class="expand" for="c-38751307">[2 more]</label></div><br/><div class="children"><div class="content">On your last point, as long as you jit the topmost level, it doesn&#x27;t matter whether or not you have inner jitted functions. The end result should be the same.<p>Source: <a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;jax&#x2F;discussions&#x2F;5199#discussioncomment-219142">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;jax&#x2F;discussions&#x2F;5199#discussioncom...</a></div><br/><div id="38751776" class="c"><input type="checkbox" id="c-38751776" checked=""/><div class="controls bullet"><span class="by">ChrisRackauckas</span><span>|</span><a href="#38749876">root</a><span>|</span><a href="#38751307">parent</a><span>|</span><a href="#38749896">next</a><span>|</span><label class="collapse" for="c-38751776">[-]</label><label class="expand" for="c-38751776">[1 more]</label></div><br/><div class="children"><div class="content">Confirmed in this collab notebook it doesn&#x27;t make a tangible difference: <a href="https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1d7G-O5JX31lHbg7jTzzozbo5-Gp7DBEv#scrollTo=ExZj1Ak9UTuj" rel="nofollow noreferrer">https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1d7G-O5JX31lHbg7jTzz...</a> .</div><br/></div></div></div></div></div></div></div></div><div id="38749896" class="c"><input type="checkbox" id="c-38749896" checked=""/><div class="controls bullet"><span class="by">yablak</span><span>|</span><a href="#38749876">parent</a><span>|</span><a href="#38749997">prev</a><span>|</span><a href="#38750051">next</a><span>|</span><label class="collapse" for="c-38749896">[-]</label><label class="expand" for="c-38749896">[6 more]</label></div><br/><div class="children"><div class="content">What they should do is build the vmap and jit <i>that</i>, then run timing on calling the resulting function.</div><br/><div id="38750013" class="c"><input type="checkbox" id="c-38750013" checked=""/><div class="controls bullet"><span class="by">geysersam</span><span>|</span><a href="#38749876">root</a><span>|</span><a href="#38749896">parent</a><span>|</span><a href="#38750051">next</a><span>|</span><label class="collapse" for="c-38750013">[-]</label><label class="expand" for="c-38750013">[5 more]</label></div><br/><div class="children"><div class="content">So instead of jitting main they should do something like<p><pre><code>  @jax.jit
  @jax.vmap
  def main(...

?</code></pre></div><br/><div id="38751762" class="c"><input type="checkbox" id="c-38751762" checked=""/><div class="controls bullet"><span class="by">ChrisRackauckas</span><span>|</span><a href="#38749876">root</a><span>|</span><a href="#38750013">parent</a><span>|</span><a href="#38750135">next</a><span>|</span><label class="collapse" for="c-38751762">[-]</label><label class="expand" for="c-38751762">[1 more]</label></div><br/><div class="children"><div class="content">This collab notebook shows effectively no difference from doing this: <a href="https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1d7G-O5JX31lHbg7jTzzozbo5-Gp7DBEv?usp=sharing" rel="nofollow noreferrer">https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1d7G-O5JX31lHbg7jTzz...</a>.<p>The average for diffrax on this collab machine goes from 20.5 to something like 20.3 seconds. You can see DiffEqGPU.jl running from Python via diffeqpy at around 2.3 seconds. This is a very rough benchmark of course since using DiffEqGPU has a fairly high (documented) overhead, and the free tier T4 GPU is not great, but it shows the ballpark of an order of magnitude or so. Note that you can also see that compile times are pretty negligible even at this scale (and the paper benchmarks are a few orders of magnitude larger than this, so at that point it&#x27;s really negligible).<p>That shouldn&#x27;t be surprising though since we&#x27;re talking about operations involving hundreds of thousands or millions of ODEs. At this scale, micro optimizations tend to have a much more minor effect. And the paper describes in detail that we developed two versions of the methods in Julia, one that was an array-based approach like Jax and PyTorch vmaps (EnsembleGPUArray), and another that was a kernel generating approach like MPGOS (EnsembleGPUKernel). Jax, PyTorch, and EnsembleGPUArray all performed similarly while MPGOS performed similarly to EnsembleGPUKernel. To us, this was a pretty strong indicator that the performance difference comes from the fact that the way EnsembleGPUKernel is performing the parallelism is very different from the approach that an ML library takes. And yes, there&#x27;s small differences in the groups, but those are like 2x-3x or so, while the paper benchmarks are in log-scale because the difference between the two classes of designs are much larger.</div><br/></div></div><div id="38750135" class="c"><input type="checkbox" id="c-38750135" checked=""/><div class="controls bullet"><span class="by">yablak</span><span>|</span><a href="#38749876">root</a><span>|</span><a href="#38750013">parent</a><span>|</span><a href="#38751762">prev</a><span>|</span><a href="#38750051">next</a><span>|</span><label class="collapse" for="c-38750135">[-]</label><label class="expand" for="c-38750135">[3 more]</label></div><br/><div class="children"><div class="content">Yes, that&#x27;s right</div><br/><div id="38751346" class="c"><input type="checkbox" id="c-38751346" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#38749876">root</a><span>|</span><a href="#38750135">parent</a><span>|</span><a href="#38750051">next</a><span>|</span><label class="collapse" for="c-38751346">[-]</label><label class="expand" for="c-38751346">[2 more]</label></div><br/><div class="children"><div class="content">Here?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;utkarsh530&#x2F;GPUODEBenchmarks&#x2F;blob&#x2F;ef807198584e4634c82bfd6a82bb9241c51d56d1&#x2F;GPU_ODE_JAX&#x2F;bench_diffrax.py#L45">https:&#x2F;&#x2F;github.com&#x2F;utkarsh530&#x2F;GPUODEBenchmarks&#x2F;blob&#x2F;ef807198...</a></div><br/></div></div></div></div></div></div></div></div><div id="38750051" class="c"><input type="checkbox" id="c-38750051" checked=""/><div class="controls bullet"><span class="by">npalli</span><span>|</span><a href="#38749876">parent</a><span>|</span><a href="#38749896">prev</a><span>|</span><a href="#38750046">next</a><span>|</span><label class="collapse" for="c-38750051">[-]</label><label class="expand" for="c-38750051">[1 more]</label></div><br/><div class="children"><div class="content">Seems like the benchmarking code is a small script and what you are suggesting might be a few lines of code. Might be worthwhile to take a stab and see if there is a difference.</div><br/></div></div></div></div><div id="38750046" class="c"><input type="checkbox" id="c-38750046" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#38749876">prev</a><span>|</span><a href="#38751860">next</a><span>|</span><label class="collapse" for="c-38750046">[-]</label><label class="expand" for="c-38750046">[1 more]</label></div><br/><div class="children"><div class="content">Submitters: &quot;<i>Please use the original title, unless it is misleading or linkbait; don&#x27;t editorialize.</i>&quot;<p>If you want to say what you think is important about an article, that&#x27;s fine, but do it by adding a comment to the thread. Then your view will be on a level playing field with everyone else&#x27;s: <a href="https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateRange=all&amp;page=0&amp;prefix=false&amp;sort=byDate&amp;type=comment&amp;query=%22level%20playing%20field%22%20by:dang" rel="nofollow noreferrer">https:&#x2F;&#x2F;hn.algolia.com&#x2F;?dateRange=all&amp;page=0&amp;prefix=false&amp;so...</a><p>(Submitted title was &quot;Julia GPU-based ODE solver 20x-100x faster than those in Jax and PyTorch&quot;. We&#x27;ve changed that to a shortened version of the paper title, to fit HN&#x27;s 80 char limit.)</div><br/></div></div><div id="38751860" class="c"><input type="checkbox" id="c-38751860" checked=""/><div class="controls bullet"><span class="by">e12e</span><span>|</span><a href="#38750046">prev</a><span>|</span><a href="#38749184">next</a><span>|</span><label class="collapse" for="c-38751860">[-]</label><label class="expand" for="c-38751860">[3 more]</label></div><br/><div class="children"><div class="content">OT: I was hoping for a html version in light of:<p><a href="https:&#x2F;&#x2F;blog.arxiv.org&#x2F;2023&#x2F;12&#x2F;21&#x2F;accessibility-update-arxiv-now-offers-papers-in-html-format&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;blog.arxiv.org&#x2F;2023&#x2F;12&#x2F;21&#x2F;accessibility-update-arxiv...</a><p>I guess this wasn&#x27;t uploaded in LaTeX?<p>Ed: Oh, this may be a date of submission thing:<p>&gt; as long as papers were submitted on or after December 1st, 2023 and HTML conversion is successful</div><br/><div id="38751937" class="c"><input type="checkbox" id="c-38751937" checked=""/><div class="controls bullet"><span class="by">namibj</span><span>|</span><a href="#38751860">parent</a><span>|</span><a href="#38749184">next</a><span>|</span><label class="collapse" for="c-38751937">[-]</label><label class="expand" for="c-38751937">[2 more]</label></div><br/><div class="children"><div class="content">Replace the x with a 5 in the abstract URL.</div><br/><div id="38752007" class="c"><input type="checkbox" id="c-38752007" checked=""/><div class="controls bullet"><span class="by">e12e</span><span>|</span><a href="#38751860">root</a><span>|</span><a href="#38751937">parent</a><span>|</span><a href="#38749184">next</a><span>|</span><label class="collapse" for="c-38752007">[-]</label><label class="expand" for="c-38752007">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;ar5iv.org&#x2F;abs&#x2F;2304.06835" rel="nofollow noreferrer">https:&#x2F;&#x2F;ar5iv.org&#x2F;abs&#x2F;2304.06835</a><p>Thank you! I guess that&#x27;s what:<p>&gt; If you are familiar with ar5iv, an arXivLabs collaboration, our HTML offering is essentially bringing this impactful project fully âin-houseâ. Our ultimate goal is to backfill arXivâs entire corpus so that every paper will have an HTML version, but for now this feature is reserved for new papers.<p>Refer to.</div><br/></div></div></div></div></div></div><div id="38749184" class="c"><input type="checkbox" id="c-38749184" checked=""/><div class="controls bullet"><span class="by">towhom</span><span>|</span><a href="#38751860">prev</a><span>|</span><a href="#38752002">next</a><span>|</span><label class="collapse" for="c-38749184">[-]</label><label class="expand" for="c-38749184">[9 more]</label></div><br/><div class="children"><div class="content">&quot;Instead of relying on high level array intrinsics that machine learning libraries use, it uses a direct kernel generation approach to greatly reduce the overhead.&quot; Chris Rackauckas on LinkedIn earlier today.</div><br/><div id="38749730" class="c"><input type="checkbox" id="c-38749730" checked=""/><div class="controls bullet"><span class="by">the__alchemist</span><span>|</span><a href="#38749184">parent</a><span>|</span><a href="#38749514">next</a><span>|</span><label class="collapse" for="c-38749730">[-]</label><label class="expand" for="c-38749730">[1 more]</label></div><br/><div class="children"><div class="content">CR is a hero. The work he does on ODE and related libs in Julia is one of the selling pts of the language. He is the Alex Crichton of Julia.</div><br/></div></div><div id="38749514" class="c"><input type="checkbox" id="c-38749514" checked=""/><div class="controls bullet"><span class="by">iamcreasy</span><span>|</span><a href="#38749184">parent</a><span>|</span><a href="#38749730">prev</a><span>|</span><a href="#38752002">next</a><span>|</span><label class="collapse" for="c-38749514">[-]</label><label class="expand" for="c-38749514">[7 more]</label></div><br/><div class="children"><div class="content">What is direct kernel generation?</div><br/><div id="38749877" class="c"><input type="checkbox" id="c-38749877" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#38749184">root</a><span>|</span><a href="#38749514">parent</a><span>|</span><a href="#38749554">next</a><span>|</span><label class="collapse" for="c-38749877">[-]</label><label class="expand" for="c-38749877">[2 more]</label></div><br/><div class="children"><div class="content">In this context I would imagine it&#x27;s constructing source code for a kernel- the engine that implements a step in a neural network- that is closer to optimal.  See <a href="https:&#x2F;&#x2F;cuda.juliagpu.org&#x2F;stable&#x2F;tutorials&#x2F;performance&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;cuda.juliagpu.org&#x2F;stable&#x2F;tutorials&#x2F;performance&#x2F;</a> for related work</div><br/><div id="38750226" class="c"><input type="checkbox" id="c-38750226" checked=""/><div class="controls bullet"><span class="by">Zentrik</span><span>|</span><a href="#38749184">root</a><span>|</span><a href="#38749877">parent</a><span>|</span><a href="#38749554">next</a><span>|</span><label class="collapse" for="c-38750226">[-]</label><label class="expand" for="c-38750226">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;cuda.juliagpu.org&#x2F;stable&#x2F;tutorials&#x2F;performance&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;cuda.juliagpu.org&#x2F;stable&#x2F;tutorials&#x2F;performance&#x2F;</a> provides various tips that someone who has written a kernel can use to speed it up, like using 32 bit integers and minimising runtime exceptions. Perhaps, I&#x27;m misunderstanding but it&#x27;s not part of direct kernel generation, whatever that is.</div><br/></div></div></div></div><div id="38749554" class="c"><input type="checkbox" id="c-38749554" checked=""/><div class="controls bullet"><span class="by">yread</span><span>|</span><a href="#38749184">root</a><span>|</span><a href="#38749514">parent</a><span>|</span><a href="#38749877">prev</a><span>|</span><a href="#38751913">next</a><span>|</span><label class="collapse" for="c-38749554">[-]</label><label class="expand" for="c-38749554">[3 more]</label></div><br/><div class="children"><div class="content">Maybe something like this? 
<a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;articles&#x2F;technical&#x2F;onemkl-improved-small-matrix-performance-using-just-in-time-jit-code.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;articles&#x2F;t...</a></div><br/><div id="38749826" class="c"><input type="checkbox" id="c-38749826" checked=""/><div class="controls bullet"><span class="by">toasted-subs</span><span>|</span><a href="#38749184">root</a><span>|</span><a href="#38749554">parent</a><span>|</span><a href="#38751913">next</a><span>|</span><label class="collapse" for="c-38749826">[-]</label><label class="expand" for="c-38749826">[2 more]</label></div><br/><div class="children"><div class="content">Yeah you&#x27;d be surprised what the performance gain is for hand written kernels.<p>There&#x27;s probably a ton left on the table if you really want to go fast.</div><br/><div id="38751823" class="c"><input type="checkbox" id="c-38751823" checked=""/><div class="controls bullet"><span class="by">ChrisRackauckas</span><span>|</span><a href="#38749184">root</a><span>|</span><a href="#38749826">parent</a><span>|</span><a href="#38751913">next</a><span>|</span><label class="collapse" for="c-38751823">[-]</label><label class="expand" for="c-38751823">[1 more]</label></div><br/><div class="children"><div class="content">In this case it&#x27;s not tricks like those done in BLAS kernels. However, there are some intricacies about the algorithms that are chosen as noted in the paper. That said, most the difference here is simply from the difference in the high level details of how the parallelism is designed, not necessarily low level bit hacking optimizations. We leave those for another day.</div><br/></div></div></div></div></div></div><div id="38751913" class="c"><input type="checkbox" id="c-38751913" checked=""/><div class="controls bullet"><span class="by">ChrisRackauckas</span><span>|</span><a href="#38749184">root</a><span>|</span><a href="#38749514">parent</a><span>|</span><a href="#38749554">prev</a><span>|</span><a href="#38752002">next</a><span>|</span><label class="collapse" for="c-38751913">[-]</label><label class="expand" for="c-38751913">[1 more]</label></div><br/><div class="children"><div class="content">The difference is really the level at which you are calling functions on the GPU. Say you have a function `f(x,y,z) = x .+ y .* sin.(z)`. If CUDA (simplify here, the paper does this for Intel OneAPI, Metal, IPUs, and AMD GPUs simultaneously but it&#x27;s basically the same), then at some point you need to be calling some kernel function, a CUDA-compiled .ptx function which is then operated on over all of the inputs. One way to parallelize this is to have a set of primitive functions, `x .+ y`, `x .* y`, `sin.(x)`, and then decompose the execution into those kernels: first call sin, then call multiply, then call plus. The other way to do this is to on-demand build a specialized .ptx kernel for the function `f` and call that. Machine learning libraries do the former approach, but we demonstrate here that the latter is much better in this scenario because the call overhead to kernels is non-trivial and this ends up slowing down the process. If there&#x27;s a tl;dr for the paper it&#x27;s this, and then scale this approach to all GPU architectures from one codebase.<p>Now I&#x27;ll simultaneously say that the choice machine learning libraries are making here is not stupid. You may look at this example and go &quot;no duh call 1 kernel instead of 3&quot;, but you never want to over optimize. For the domain that ML libraries are designed for, these kernel calls are typically things like large matrix multiplications (that&#x27;s the core of any deep neural network, with a few things around it). These kinds of operations are O(n^3) or O(n^2) on very large arrays. With that amount of compute to do on the memory, the overhead cost can go to nearly zero. Thus for the use case targeted by ML libraries, approaching the design of the GPU library as &quot;just make enough optimized kernels&quot; is a good design. For example, it was counted in 2021 that PyTorch had about 2,000 such kernels (<a href="https:&#x2F;&#x2F;dev-discuss.pytorch.org&#x2F;t&#x2F;where-do-the-2000-pytorch-operators-come-from-more-than-you-wanted-to-know&#x2F;373" rel="nofollow noreferrer">https:&#x2F;&#x2F;dev-discuss.pytorch.org&#x2F;t&#x2F;where-do-the-2000-pytorch-...</a>). Sit down, optimize the CUDA kernels, then make the high level code call the most appropriate one. That&#x27;s a good design if the kernels are expensive enough, like in deep learning.<p>While Jax has a few other things going on, both the PyTorch and Jax vmap parallelism approach are effectively high level tools to shove larger arrays more nicely into such existing kernels. For example, one optimization that vmap does is fuse matrix-vector multiplications into matrix multiplications, i.e. A<i>v1 + A</i>v2 -&gt; A*[v1;v2]. The purpose is to still use a small set of primitives and shove as big of array operations as you can into it.<p>However, that is not a good idea in all domains. In ODE solvers, you have lots of control flow and O(n) operations. This can make that &quot;negligible&quot; overhead very not negligible, and thus one needs to design the parallelism very differently in order to not run into the performance issues that one would hit with the &quot;small kernel array based approach&quot;. The better approach in this domain (as demonstrated in the paper) is to build completely new kernels of the functions you&#x27;re trying to compute, i.e. build a CUDA code and .ptx kernel for f directly, compile that, and do the one call. This has some downsides of course, as this kernel is effectively unable to be reused for other things, which then means that the you need to be able to do this kernel generation automatically for it to be useful at a package level.<p>In other words, domain-specific languages optimize to their respective domain of choice, but that may be leaving performance on the table for use cases outside of their directly targeted audience.</div><br/></div></div></div></div></div></div><div id="38752002" class="c"><input type="checkbox" id="c-38752002" checked=""/><div class="controls bullet"><span class="by">glouwbug</span><span>|</span><a href="#38749184">prev</a><span>|</span><a href="#38749764">next</a><span>|</span><label class="collapse" for="c-38752002">[-]</label><label class="expand" for="c-38752002">[1 more]</label></div><br/><div class="children"><div class="content">Anyone remember analog computers? They were really good at solving differential equations</div><br/></div></div><div id="38749764" class="c"><input type="checkbox" id="c-38749764" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#38752002">prev</a><span>|</span><a href="#38750324">next</a><span>|</span><label class="collapse" for="c-38749764">[-]</label><label class="expand" for="c-38749764">[1 more]</label></div><br/><div class="children"><div class="content">Link to GitHub repo from the abstract: <a href="https:&#x2F;&#x2F;github.com&#x2F;SciML&#x2F;DiffEqGPU.jl">https:&#x2F;&#x2F;github.com&#x2F;SciML&#x2F;DiffEqGPU.jl</a></div><br/></div></div><div id="38750324" class="c"><input type="checkbox" id="c-38750324" checked=""/><div class="controls bullet"><span class="by">programjames</span><span>|</span><a href="#38749764">prev</a><span>|</span><label class="collapse" for="c-38750324">[-]</label><label class="expand" for="c-38750324">[2 more]</label></div><br/><div class="children"><div class="content">This... doesn&#x27;t seem to do anything special? Everyone already knew it was bad to &quot;batch&quot; ODEs by making them bigger, e.g. in &quot;Neural Ordinary Differential Equations&quot; (the paper that introduced neural ODEs):<p>&gt; One can still batch together evaluations through the ODE solver by concatenating the states of each batch element together, creating a combined ODE with dimension D Ã K. In some cases, controlling error on all batch elements together might require evaluating the combined system K times more often than if each system was solved individually. However, in practice the number of evaluations did not increase substantially when using minibatches.<p>I don&#x27;t understand why someone wrote a 30-page, obfuscated paper on just... parallelizing it the obvious way.</div><br/><div id="38751333" class="c"><input type="checkbox" id="c-38751333" checked=""/><div class="controls bullet"><span class="by">ChrisRackauckas</span><span>|</span><a href="#38750324">parent</a><span>|</span><label class="collapse" for="c-38751333">[-]</label><label class="expand" for="c-38751333">[1 more]</label></div><br/><div class="children"><div class="content">I mean, it at least must not be obvious to the poster that says &quot;the claim does sound too good to be true&quot;. But yes anyone with enough of an HPC background can look at how vmap is doing its parallelization and instantly know that ML frameworks like Jax and PyTorch are most likely losing an order of magnitude or two of performance. And of course we are very explicit in the paper that this is not novel because we show that the kernels that we are generating match the performance of MPGOS, which is a CUDA library which has the same architecture.<p>But of course, all of this discussion leaves off half of the title of the paper, &quot;on Multiple GPU Platforms&quot;. The point is not that we are able to generate kernels which are doing the fast thing that a dedicated CUDA library does (i.e. not the slow thing that ML libraries are doing), rather the point is that we are doing this in a way where CUDA is not special. We generate similarly optimized kernels for AMD GPUs, Intel GPUs, and Apple silicon (Metal) using this approach. Mose also showed this same codebase can generate kernels for GraphCore IPUs without modifications too (see <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=-fxB0kmcCVE" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=-fxB0kmcCVE</a>). Showing matching performance with good kernel codes was step 1 but portability (with a full feature set) is step 2. I&#x27;d be interested to know if you have any examples of ODE solvers which achieve this level of performance portability because we were unable to find one in the literature or open source.</div><br/></div></div></div></div></div></div></div></div></div></body></html>