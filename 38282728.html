<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700125279412" as="style"/><link rel="stylesheet" href="styles.css?v=1700125279412"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.nature.com/articles/s42256-023-00703-8">Language models and linguistic theories beyond words</a> <span class="domain">(<a href="https://www.nature.com">www.nature.com</a>)</span></div><div class="subtext"><span>Anon84</span> | <span>21 comments</span></div><br/><div><div id="38285243" class="c"><input type="checkbox" id="c-38285243" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#38285398">next</a><span>|</span><label class="collapse" for="c-38285243">[-]</label><label class="expand" for="c-38285243">[4 more]</label></div><br/><div class="children"><div class="content">The article focuses on linguistic theories that regard natural language in a somewhat narrow sense: language as a cognitive phenomenon, language as a symbolic system, language as something that can be modeled and imitated by a computer. While those aspects are important, language is also a social phenomenon, something used for interaction among people for many different purposes.<p>Both traditional approaches to natural language—the cognitive and the social—face a challenge, because language is no longer used only for communication between human beings. What does it mean for linguistics now that human beings are increasingly using natural language to interact with computers? I don’t know the answer, but it’s a question that deserves attention.</div><br/><div id="38286385" class="c"><input type="checkbox" id="c-38286385" checked=""/><div class="controls bullet"><span class="by">patcon</span><span>|</span><a href="#38285243">parent</a><span>|</span><a href="#38285398">next</a><span>|</span><label class="collapse" for="c-38286385">[-]</label><label class="expand" for="c-38286385">[3 more]</label></div><br/><div class="children"><div class="content">&gt; What does it mean for linguistics now that human beings are increasingly using natural language to interact with computers?<p>Yeah, it&#x27;s going to be strange. So much of our etiquette and norms and basic cultural assumption have to do with this idea:<p>This communication protocol of &quot;language&quot; is for humans. If something else is &quot;on the wire&quot; with us, it must be like us, and worthy of our attention or cycles. But this is increasingly going to be very untrue.<p>Culture is about to be weird.</div><br/><div id="38286501" class="c"><input type="checkbox" id="c-38286501" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#38285243">root</a><span>|</span><a href="#38286385">parent</a><span>|</span><a href="#38285398">next</a><span>|</span><label class="collapse" for="c-38286501">[-]</label><label class="expand" for="c-38286501">[2 more]</label></div><br/><div class="children"><div class="content">Curious about what ChatGPT might have to say on this issue, I asked it:<p><a href="https:&#x2F;&#x2F;www.gally.net&#x2F;temp&#x2F;20231116languageandai&#x2F;index.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.gally.net&#x2F;temp&#x2F;20231116languageandai&#x2F;index.html</a></div><br/><div id="38286909" class="c"><input type="checkbox" id="c-38286909" checked=""/><div class="controls bullet"><span class="by">pharmakom</span><span>|</span><a href="#38285243">root</a><span>|</span><a href="#38286501">parent</a><span>|</span><a href="#38285398">next</a><span>|</span><label class="collapse" for="c-38286909">[-]</label><label class="expand" for="c-38286909">[1 more]</label></div><br/><div class="children"><div class="content">Why do people keep asking models about models?</div><br/></div></div></div></div></div></div></div></div><div id="38285398" class="c"><input type="checkbox" id="c-38285398" checked=""/><div class="controls bullet"><span class="by">gizajob</span><span>|</span><a href="#38285243">prev</a><span>|</span><a href="#38283153">next</a><span>|</span><label class="collapse" for="c-38285398">[-]</label><label class="expand" for="c-38285398">[6 more]</label></div><br/><div class="children"><div class="content">Get a neural network, same as you&#x27;d use at the bottom of an LLM.<p>Don&#x27;t feed it a corpus of text! Ensure it&#x27;s data is the opposite of a large language model, i.e. zero language.<p>Let it passively listen to the same sounds and speech as a baby hears in its first five years, over five years of operation.<p>Observe it do absolutely nothing of any use.<p>Get back to me about any useful comparisons with linguistics or philosophy of language.</div><br/><div id="38285473" class="c"><input type="checkbox" id="c-38285473" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38285398">parent</a><span>|</span><a href="#38286454">next</a><span>|</span><label class="collapse" for="c-38285473">[-]</label><label class="expand" for="c-38285473">[3 more]</label></div><br/><div class="children"><div class="content">Text is not the important thing here at all. audio language models exist, they are relatively trivial to engineer and they will model audio just as well vs they&#x27;ll model text or any arbitrary sequence to sequence data.<p><a href="https:&#x2F;&#x2F;google-research.github.io&#x2F;seanet&#x2F;audiolm&#x2F;examples&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;google-research.github.io&#x2F;seanet&#x2F;audiolm&#x2F;examples&#x2F;</a><p>Linguistics is useful fiction. It doesn&#x27;t accurately describe the complexity of language anymore than Newton&#x27;s laws accurately describe gravity. You&#x27;d think people would understand that now with symbolic approaches failing this sort of stuff after decades of trying but i guess not. We don&#x27;t know how we work. We really really don&#x27;t.<p>The obsession with how we <i>think</i> we work over methods that are actually working is really something to behold.</div><br/><div id="38286361" class="c"><input type="checkbox" id="c-38286361" checked=""/><div class="controls bullet"><span class="by">patcon</span><span>|</span><a href="#38285398">root</a><span>|</span><a href="#38285473">parent</a><span>|</span><a href="#38285495">next</a><span>|</span><label class="collapse" for="c-38286361">[-]</label><label class="expand" for="c-38286361">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We don&#x27;t know how we work. We really really don&#x27;t.<p>Amen.</div><br/></div></div><div id="38285495" class="c"><input type="checkbox" id="c-38285495" checked=""/><div class="controls bullet"><span class="by">gizajob</span><span>|</span><a href="#38285398">root</a><span>|</span><a href="#38285473">parent</a><span>|</span><a href="#38286361">prev</a><span>|</span><a href="#38286454">next</a><span>|</span><label class="collapse" for="c-38285495">[-]</label><label class="expand" for="c-38285495">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;tmms.co.uk&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;01&#x2F;imgpsh_fullsize_anim-5.png" rel="nofollow noreferrer">https:&#x2F;&#x2F;tmms.co.uk&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;01&#x2F;imgpsh_fullsiz...</a></div><br/></div></div></div></div><div id="38286454" class="c"><input type="checkbox" id="c-38286454" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#38285398">parent</a><span>|</span><a href="#38285473">prev</a><span>|</span><a href="#38285969">next</a><span>|</span><label class="collapse" for="c-38286454">[-]</label><label class="expand" for="c-38286454">[1 more]</label></div><br/><div class="children"><div class="content">A fun pastime is to go find old papers by philosophers and linguists that make concrete claims about things language models will never be able to do, and then go type their examples into ChatGPT and see the model do just fine.</div><br/></div></div><div id="38285969" class="c"><input type="checkbox" id="c-38285969" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#38285398">parent</a><span>|</span><a href="#38286454">prev</a><span>|</span><a href="#38283153">next</a><span>|</span><label class="collapse" for="c-38285969">[-]</label><label class="expand" for="c-38285969">[1 more]</label></div><br/><div class="children"><div class="content">You are aware the same transformer architecture that underpins LLMs has shown great success in multimodal applications? Such as successfully modeling sound and music?<p><a href="https:&#x2F;&#x2F;bytedance.github.io&#x2F;SALMONN&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;bytedance.github.io&#x2F;SALMONN&#x2F;</a></div><br/></div></div></div></div><div id="38283153" class="c"><input type="checkbox" id="c-38283153" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38285398">prev</a><span>|</span><a href="#38284140">next</a><span>|</span><label class="collapse" for="c-38283153">[-]</label><label class="expand" for="c-38283153">[7 more]</label></div><br/><div class="children"><div class="content">Paraphrasing and summarizing parts of this article, <a href="https:&#x2F;&#x2F;hedgehogreview.com&#x2F;issues&#x2F;markets-and-the-good&#x2F;articles&#x2F;language-machinery" rel="nofollow noreferrer">https:&#x2F;&#x2F;hedgehogreview.com&#x2F;issues&#x2F;markets-and-the-good&#x2F;artic...</a><p>Some ~72 years ago in 1951, Claude Shannon released his Paper, &quot;Prediction and Entropy of Printed English&quot;, an extremely fascinating read now.<p>It begins with a game. Claude pulls a book down from the shelf, concealing the title in the process. After selecting a passage at random, he challenges his wife, Mary to guess its contents letter by letter. The space between words will count as a twenty-seventh symbol in the set. If Mary fails to guess a letter correctly, Claude promises to supply the right one so that the game can continue.<p>In some cases, a corrected mistake allows her to fill in the remainder of the word; elsewhere a few letters unlock a phrase. All in all, she guesses 89 of 129 possible letters correctly—69 percent accuracy.<p>Discovery 1: It illustrated, in the first place, that a proficient speaker of a language possesses an “enormous” but implicit knowledge of the statistics of that language. Shannon would have us see that we make similar calculations regularly in everyday life—such as when we “fill in missing or incorrect letters in proof-reading” or “complete an unfinished phrase in conversation.” As we speak, read, and write, we are regularly engaged in predication games.<p>Discovery 2: Perhaps the most striking of all, Claude argues that that a complete text and the subsequent “reduced text” consisting of letters and dashes “actually…contain the same information” under certain conditions. How?? (Surely, the first line contains more information!).The answer depends on the peculiar notion about information that Shannon had hatched in his 1948 paper “A Mathematical Theory of Communication” (hereafter “MTC”), the founding charter of information theory.<p>He argues that transfer of a message&#x27;s components, rather than its &quot;meaning&quot;, should be the focus for the engineer. You ought to be agnostic about a message’s “meaning” (or “semantic aspects”). The message could be nonsense, and the engineer’s problem—to transfer its components faithfully—would be the same.<p>a highly predictable message contains less information than an unpredictable one. More information is at stake in (“villapleach, vollapluck”) than in (“Twinkle, twinkle”).<p>Does &quot;Flinkle, fli- - - -&quot; really contain less information than &quot;Flinkle, flinkle&quot; ?<p>Shannon concludes then that the complete text and the &quot;reduced text&quot; are equivalent in information content under certain conditions because predictable letters become redundant in information transfer.<p>Fueled by this, Claude then proposes an illuminating thought experiment: Imagine that Mary has a truly identical twin (call her “Martha”). If we supply Martha with the “reduced text,” she should be able to recreate the entirety of Chandler’s passage, since she possesses the same statistical knowledge of English as Mary. Martha would make Mary’s guesses in reverse.<p>Of course, Shannon admitted, there are no “mathematically identical twins” to be found, <i>but</i> and here&#x27;s the reveal, “we do have mathematically identical computing machines.”<p>Those machines could be given a model for making informed predictions about letters, words, maybe larger phrases and messages. In one fell swoop, Shannon had demonstrated that language use has a statistical side, that languages are, in turn, predictable, and that computers too can play the prediction game.</div><br/><div id="38283333" class="c"><input type="checkbox" id="c-38283333" checked=""/><div class="controls bullet"><span class="by">hyeonwho22</span><span>|</span><a href="#38283153">parent</a><span>|</span><a href="#38283451">next</a><span>|</span><label class="collapse" for="c-38283333">[-]</label><label class="expand" for="c-38283333">[3 more]</label></div><br/><div class="children"><div class="content">There was a fun recent variant on this game using LLMs, asking GPT3 (3.5?) to encode text in a way that it will be able to decode the meaning. Some of the encodings are insane:<p><a href="https:&#x2F;&#x2F;www.piratewires.com&#x2F;p&#x2F;compression-prompts-gpt-hidden-dialects" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.piratewires.com&#x2F;p&#x2F;compression-prompts-gpt-hidden...</a></div><br/><div id="38285992" class="c"><input type="checkbox" id="c-38285992" checked=""/><div class="controls bullet"><span class="by">jonahrd</span><span>|</span><a href="#38283153">root</a><span>|</span><a href="#38283333">parent</a><span>|</span><a href="#38283451">next</a><span>|</span><label class="collapse" for="c-38285992">[-]</label><label class="expand" for="c-38285992">[2 more]</label></div><br/><div class="children"><div class="content">This is super interesting. Are there more examples I can see? The one in the article is a famous song which makes me wonder if it&#x27;s really &quot;decompressing&quot; the data, or just being hunted towards a very common popular pattern of tokens.</div><br/><div id="38286324" class="c"><input type="checkbox" id="c-38286324" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38283153">root</a><span>|</span><a href="#38285992">parent</a><span>|</span><a href="#38283451">next</a><span>|</span><label class="collapse" for="c-38286324">[-]</label><label class="expand" for="c-38286324">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;aps.arxiv.org&#x2F;abs&#x2F;2309.10668" rel="nofollow noreferrer">https:&#x2F;&#x2F;aps.arxiv.org&#x2F;abs&#x2F;2309.10668</a></div><br/></div></div></div></div></div></div><div id="38283451" class="c"><input type="checkbox" id="c-38283451" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#38283153">parent</a><span>|</span><a href="#38283333">prev</a><span>|</span><a href="#38285943">next</a><span>|</span><label class="collapse" for="c-38283451">[-]</label><label class="expand" for="c-38283451">[1 more]</label></div><br/><div class="children"><div class="content">If you want to go there, you could say that natural languages are error-correcting codes -- somewhat robust to corruption (typos). <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Error_correction_code" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Error_correction_code</a></div><br/></div></div><div id="38285943" class="c"><input type="checkbox" id="c-38285943" checked=""/><div class="controls bullet"><span class="by">thomastjeffery</span><span>|</span><a href="#38283153">parent</a><span>|</span><a href="#38283451">prev</a><span>|</span><a href="#38284140">next</a><span>|</span><label class="collapse" for="c-38285943">[-]</label><label class="expand" for="c-38285943">[2 more]</label></div><br/><div class="children"><div class="content">The information wasn&#x27;t removed: it was <i>moved</i>.<p>Whenever a human remembers something specific, they actually <i>don&#x27;t</i>. Instead, they remember a few small details, and the <i>patterns</i> that organize them. Then, the brain hallucinates more details that fit the overall pattern of the story. This phenomenon is called &quot;Reconstructive Memory&quot;, and is one reason why eyewitness testimony is unreliable.<p>An LLM is similar to memory: you feed its neurons a bunch of data that has meaning encoded into it, and it becomes a model for any patterns present in that data. When an LLM generates a continuation, it is continuing the patterns that it modeled; including the data it was trained on, and whatever prompt it was given.<p>Natural language solved! Right?<p>---<p>Not so fast!<p>The human mind performs <i>much more</i> than memory reconstruction. How else would we encode <i>meaning</i> into the semantics of language, and write that data into text?<p>There is more to this story. There is more to... <i>story</i>.<p>Remember when I said the information is &quot;moved&quot;? Where did it go? More importantly, how can we use that data?<p>---<p>Let&#x27;s consider a human named Dave, reading a book about boats. What is Dave using to read it? The short version: empathy.<p>Story is held together with semantics. Semantics are already-known patterns that define logical relationships between words.<p>When Dave reads the statement, &quot;Sue got in the boat&quot;, he interprets that information into <i>meaning</i>. Sue is a person, probably a woman. She entered the top of a vessel that was floating on water.<p>But Dave was <i>wrong</i>! Sue is a cat, and the boat was lying on dry beach.<p>Here&#x27;s the interesting part: Dave was totally correct until I declared otherwise. His interpretation matched his internal worldview: all of the ambiguity present in what he read was <i>resolved</i> by his assumptions. Making false assumptions is a completely valid result of the process that we call &quot;reading&quot;. It&#x27;s a <i>feature</i>.<p>After Dave overheard me talking to you just now, he learned the truth of the story, and immediately resolved his mistake. In an instant, Dave took the semantic information he had just read, and he <i>reread it</i> with a completely different worldview. But where did he read it from?<p><i>His worldview</i>. You see, after reading the book, its <i>meaning</i> was added neatly into his worldview. Because of this, he was prepared to interpret what I was telling you: about Sue being a cat and so on. Dave performed the same &quot;reading&quot; process on my new story, and he used the statement he read in the book to do just that.<p>---<p>Worldview is <i>context</i>. Context is the tool that resolves ambiguity. We use this tool to interpret <i>story</i>, particularly when story is ambiguous.<p>So what is context made of? Story.<p>It&#x27;s recursive. Everything that we read is added to our internal story. One giant convoluted mess of logic is carved into our neurons. We fold those logical constructs together into a small set of coherent ideas.<p>But where is the base case? What&#x27;s the smallest part of a story? <i>What are the axioms?</i><p>This is the part I struggled with the most: there isn&#x27;t one. Somehow, we manage to perform these recursive algorithms from the middle. We read the story down the ladder of abstraction, as close to its roots as we can find; but we can only read the story as far as we have read it already.<p>We can navigate the logical structure of ideas without ever proving that logic to be sound. We can even navigate logic that is outright false! Constraining this behavior to proven logic has to be <i>intentional</i>. That&#x27;s why we have a word for it: mathematics. Math is the special story: it&#x27;s rooted in axioms, and built up exclusively using <i>theorems</i>.<p>Theorems are an optimization: they let us skip from one page of a story to another. They let us fold and compress logical structure into a something more practical.<p>---<p>LLMs do not use logic at all. The logic of <i>invalidation</i> is missing. When story categorizes an idea into the realm of fiction, the LLM simply can&#x27;t react accordingly. The LLM has no internal story: only memory.</div><br/><div id="38286248" class="c"><input type="checkbox" id="c-38286248" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38283153">root</a><span>|</span><a href="#38285943">parent</a><span>|</span><a href="#38284140">next</a><span>|</span><label class="collapse" for="c-38286248">[-]</label><label class="expand" for="c-38286248">[1 more]</label></div><br/><div class="children"><div class="content">The comment was just to tell a fascinating story of the conceptual origins of what we have today. But the predictor Claude imagined actually works quite a bit differently than what we have today.<p>Yes Shannon argued meaning and semantic wasn&#x27;t necessary but today, we <i>know</i> that our language models develop meaning and semantics. We <i>know</i> they build models. We <i>know</i> they try to model the casual processes that generate this data and implicit structure that was never explicitly stated in the text can find themselves emerging in the inner layers.<p>&gt;LLMs do not use logic at all. The logic of invalidation is missing.<p>This is a fascinating idea that i see that just doesn&#x27;t square with reality. In fact, this is all they do. What do you imagine training to be ?<p>Prediction requires <i>a</i> model of some sort. It need not be completely accurate, or how you imagine it. But to performantly make predictions, you must model your data in some way.<p>The important bit here is that the current paradigm doesn&#x27;t just stop at that. Here, the predictor is <i>learning</i> to predict.<p>We have some optimizer that is tirelessly working to reduce loss. But what does a reduction in loss of internet skill data distribution mean?<p>It means better and better models of the data set. Every single time a language model fails a prediction, it&#x27;s a signal to the optimizer that the current model is incomplete, insufficient in some way, work needs to be done, and work will be done, bit by bit. The models in a LLM at any point in time, A, is different from the models at any point in time, B during the training process but it&#x27;s not a random difference. It&#x27;s a difference that trends in the direction of a more robust worldview of the data.<p><i>This</i> is why language model don&#x27;t bottleneck on some arbitrary competence level humans like to shoe-horn it on.<p>There is a projection of the world in text. Text is the world and the language model is very much interacting with it.<p>The optimizer may be dumb but this restructuring of neurons to better represent the world as seen in the text is absolutely happening.</div><br/></div></div></div></div></div></div><div id="38284038" class="c"><input type="checkbox" id="c-38284038" checked=""/><div class="controls bullet"><span class="by">foobarqux</span><span>|</span><a href="#38284140">prev</a><span>|</span><label class="collapse" for="c-38284038">[-]</label><label class="expand" for="c-38284038">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=DQuiso493ro&amp;t=3309s">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=DQuiso493ro&amp;t=3309s</a><p>&quot;Have you seen the paper that&#x27;s called Modern Language Models Refute Chomsky&#x27;s Approach to
Language? The one by Steven Piantadosi. If so, what do you make of it?<p>Well, I mean, unlike a lot of people who write about this, he does know about
large language models, but the article makes absolutely no sense. It has a minor problem
and a major problem. The minor problem is that it&#x27;s beyond absurdity to think that you can
learn anything about a two
bunch of supercomputers scanning 50 terabytes of data, looking for statistical regularities,
and stringing them together. To think that from that you can learn anything about an infant is
so beyond absurdity that it&#x27;s not worth talking about. That&#x27;s the minor problem.
The major problem is that, in principle, you can learn nothing. In principle,
make it 100 terabytes, use 20 supercomputers. Just bring out the, in principle, impossibility.
Even worse, for more clearly, for a very simple reason, these systems work just as well for
impossible languages that children can&#x27;t acquire as they do for possible languages that children
do acquire. It&#x27;s kind of as if a physicist came along and said, I&#x27;ve got a great new theory.
It accounts for a lot of things that happen, a lot of things that can&#x27;t possibly happen,
and I can&#x27;t make any distinction among them. We would just laugh. I mean, any explanation of
anything says, here&#x27;s why things happen, here&#x27;s why other things don&#x27;t happen. You can&#x27;t make
that distinction. You&#x27;re doing nothing. That&#x27;s irremediable. It&#x27;s built into the nature of the
systems. The more sophisticated they become at dealing with actual language, the more it&#x27;s
demonstrated that they&#x27;re telling us nothing, in principle, about language, about learning,
or about cognition. So, there&#x27;s a minor problem with this paper, and a major one.
The minor problem is the simple absurdity of thinking that looking at anything of the scale
could tell you about what&#x27;s happening. The major problem is you can&#x27;t do it in principle.
Yes, I heard you describe this as if a physicist came along and said,
I have a theory, and it&#x27;s two words. Anything goes.
Well, that&#x27;s basically the large language models. You give it a system that&#x27;s designed in order to
violate the principles of language, it&#x27;ll do just as well. Maybe better, because it can use simple
algorithms that aren&#x27;t used by natural language. So, it&#x27;s basically, like I said, or like I say,
suppose some guy comes along with an improvement on the periodic table,
says I got a theory that includes all the possible elements, even those that haven&#x27;t
been discovered, and all kinds of impossible ones, and I can&#x27;t tell any difference.
It&#x27;s not an improvement on the periodic table. That&#x27;s telling you nothing about chemistry.
That&#x27;s built into the design of the system. It&#x27;s not remediable.&quot;</div><br/><div id="38285131" class="c"><input type="checkbox" id="c-38285131" checked=""/><div class="controls bullet"><span class="by">thomashop</span><span>|</span><a href="#38284038">parent</a><span>|</span><label class="collapse" for="c-38285131">[-]</label><label class="expand" for="c-38285131">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s just like your opinion man.</div><br/></div></div></div></div></div></div></div></div></div></body></html>