<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1706000462890" as="style"/><link rel="stylesheet" href="styles.css?v=1706000462890"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://danluu.com/2choices-eviction/">Caches: LRU vs. Random (2014)</a> <span class="domain">(<a href="https://danluu.com">danluu.com</a>)</span></div><div class="subtext"><span>eatonphil</span> | <span>18 comments</span></div><br/><div><div id="39097802" class="c"><input type="checkbox" id="c-39097802" checked=""/><div class="controls bullet"><span class="by">jlhawn</span><span>|</span><a href="#39096095">next</a><span>|</span><label class="collapse" for="c-39097802">[-]</label><label class="expand" for="c-39097802">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been playing a lot of Baldur&#x27;s Gate 3 lately and this kind of reminds me of when you have to roll a d20 in a skill check. Early in the game there are skill checks with a difficultly class of 2 (need to roll a 2 or higher to succeed) but you might have proficiency in that skill so you get a +3 modifier to your roll.<p>But if your base d20 roll result is a 1 then this is considered a <i>critical fail</i> and your modifiers are not added – you fail no matter what. So there is a 5% chance of failing no matter what (1 in 20).<p>However, you can have special items or spells which can grant you <i>advantage</i> in a roll: you roll 2 d20 instead of 1 and take the higher of each. Now the only way to critical fail is if you roll a 1 with both d20s. This has only a 0.25% chance (1 in 400).<p>So the &quot;2 random choices&quot; cache eviction policy feels kinda like rolling with advantage in D&amp;D: Roll 2 die (pick to random keys) and evict the higher time since last access to evict.</div><br/><div id="39099816" class="c"><input type="checkbox" id="c-39099816" checked=""/><div class="controls bullet"><span class="by">jvanderbot</span><span>|</span><a href="#39097802">parent</a><span>|</span><a href="#39096095">next</a><span>|</span><label class="collapse" for="c-39099816">[-]</label><label class="expand" for="c-39099816">[1 more]</label></div><br/><div class="children"><div class="content">I love that game and all the source material, but I really dislike the d20 as a base. I&#x27;ve been working on a system that uses a fixed number of d6, since with just a few d6 you get something more normally distributed, and so you can actually model real life skills more accurately, and provide a better spread of outcomes for higher level players over &quot; I have a 5% chance to fail, or a 90+% chance in something I&#x27;m not good at &quot;<p>20 years of d&amp;d will motivate some unofficial errata.</div><br/></div></div></div></div><div id="39096095" class="c"><input type="checkbox" id="c-39096095" checked=""/><div class="controls bullet"><span class="by">gnulinux</span><span>|</span><a href="#39097802">prev</a><span>|</span><a href="#39095271">next</a><span>|</span><label class="collapse" for="c-39096095">[-]</label><label class="expand" for="c-39096095">[7 more]</label></div><br/><div class="children"><div class="content">&gt; So we&#x27;ve seen that this works, but why would anyone think to do this in the first place? The Power of Two Random Choices: A Survey of Techniques and Results by Mitzenmacher, Richa, and Sitaraman has a great explanation. The mathematical intuition is that if we (randomly) throw n balls into n bins, the maximum number of balls in any bin is O(log n &#x2F; log log n) with high probability, which is pretty much just O(log n). But if (instead of choosing randomly) we choose the least loaded of k random bins, the maximum is O(log log n &#x2F; log k) with high probability, i.e., even with two random choices, it&#x27;s basically O(log log n) and each additional choice only reduces the load by a constant factor.<p>This sounds very powerful! And intuitively doesn&#x27;t make any sense to me. So, say I have n choices, I do not know which choice is better. Does this result say that it&#x27;s more efficient to randomly choose k and find the best among them (even for k=2), instead of randomly choosing a single choice? Could someone point me in the right direction?</div><br/><div id="39096215" class="c"><input type="checkbox" id="c-39096215" checked=""/><div class="controls bullet"><span class="by">aidenn0</span><span>|</span><a href="#39096095">parent</a><span>|</span><a href="#39095271">next</a><span>|</span><label class="collapse" for="c-39096215">[-]</label><label class="expand" for="c-39096215">[6 more]</label></div><br/><div class="children"><div class="content">&gt; This sounds very powerful! And intuitively doesn&#x27;t make any sense to me. So, say I have n choices, I do not know which choice is better. Does this result say that it&#x27;s more efficient to randomly choose k and find the best among them (even for k=2), instead of randomly choosing a single choice? Could someone point me in the right direction?<p>Yes, let&#x27;s consider the k=2 vs selecting a random item from a set of N items:<p>Selecting one item uniformly randomly from a set of N is identical to selecting two distinct items and then picking one of those two uniformly randomly.<p>So if you select items A and B, then pick the best item, you end up with an item of MAX(A,B) utility instead of MEAN(A,B) utility.  MAX(A,B) &gt;= MEAN(A,B) should hopefully be obvious.</div><br/><div id="39098359" class="c"><input type="checkbox" id="c-39098359" checked=""/><div class="controls bullet"><span class="by">girvo</span><span>|</span><a href="#39096095">root</a><span>|</span><a href="#39096215">parent</a><span>|</span><a href="#39095271">next</a><span>|</span><label class="collapse" for="c-39098359">[-]</label><label class="expand" for="c-39098359">[5 more]</label></div><br/><div class="children"><div class="content">You&#x27;ve explained that wonderfully, thank you! I&#x27;m not the original commenter, but was also confused by my intuition here. That makes a lot more sense now :)</div><br/><div id="39099028" class="c"><input type="checkbox" id="c-39099028" checked=""/><div class="controls bullet"><span class="by">waynesonfire</span><span>|</span><a href="#39096095">root</a><span>|</span><a href="#39098359">parent</a><span>|</span><a href="#39095271">next</a><span>|</span><label class="collapse" for="c-39099028">[-]</label><label class="expand" for="c-39099028">[4 more]</label></div><br/><div class="children"><div class="content">It didnt make any sense to me.<p>I think an intuitive perspective is that if you pick one item randomly, you cant use the LRU signal. So, you pick two items. Now you get to also use LRU. Notice that it doesnt add any value to pick three random items and then apply LRU.</div><br/><div id="39099097" class="c"><input type="checkbox" id="c-39099097" checked=""/><div class="controls bullet"><span class="by">aidenn0</span><span>|</span><a href="#39096095">root</a><span>|</span><a href="#39099028">parent</a><span>|</span><a href="#39095271">next</a><span>|</span><label class="collapse" for="c-39099097">[-]</label><label class="expand" for="c-39099097">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Notice that it doesnt add any value to pick three random items and then apply LRU.<p>It actually does.  The more items you pick, the better it works, because at the ultimate stage of k=N you will always be picking the best item.  For a lot of real-world distributions, there are rapidly diminishing returns with higher k, but the returns are still there.<p>[edit]<p>The same example as above works for k=3, observing that discarding one of the 3 chosen uniformly randomly devolves to the k=2 form.<p>k=3: MAX(A,B,C)<p>k=2: MEAN(MAX(A,B),MAX(B,C),MAX(A,C))<p>k=1:</div><br/><div id="39099406" class="c"><input type="checkbox" id="c-39099406" checked=""/><div class="controls bullet"><span class="by">waynesonfire</span><span>|</span><a href="#39096095">root</a><span>|</span><a href="#39099097">parent</a><span>|</span><a href="#39095271">next</a><span>|</span><label class="collapse" for="c-39099406">[-]</label><label class="expand" for="c-39099406">[2 more]</label></div><br/><div class="children"><div class="content">If you think you need a better K, because you have insight into your distribtution, then don&#x27;t use uniform, instead, use something more appropriate.<p>When k=N you degrade to LRU.<p>Your MAX &#x2F; MEAN logic is completely flawed. If k=N then you&#x27;d have MAX(0..N), wouldn&#x27;t this be optimal? But, this is LRU and it&#x27;s not optimal.<p>All this is hand-waving. The proper response is to use the language of mathematics to prove how this algorithm behaves. Perhaps there is an optimal k.</div><br/><div id="39100368" class="c"><input type="checkbox" id="c-39100368" checked=""/><div class="controls bullet"><span class="by">aidenn0</span><span>|</span><a href="#39096095">root</a><span>|</span><a href="#39099406">parent</a><span>|</span><a href="#39095271">next</a><span>|</span><label class="collapse" for="c-39100368">[-]</label><label class="expand" for="c-39100368">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t realize we had moved on to talking about cache-replacement rather than putting balls in buckets.  For putting balls in buckets (or any case where it is possible to compare options and come out with a &quot;best&quot; then k=N is optimal.<p>For cache-replacement strategies, you can prove very little for the general case, since for any two reasonable replacement strategies, there is usually an access pattern that favors one over the other.  When TFA says &quot;Random and FIFO are both strictly worse than either LRU or 2-random&quot; the context is in running SPEC CPU with an 8-way associative cache.  It&#x27;s not hard to come up with artificial benchmarks that invert that relationship.<p>Also TFA contradicts your earlier assertion that k=3 is not better than k=2, at least when used with a pseudo-LRU: &quot;Also, we can see that pseudo 3-random is substantially better than pseudo 2-random, which indicates that k-random is probably an improvement over 2-random for the k.&quot;</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39095271" class="c"><input type="checkbox" id="c-39095271" checked=""/><div class="controls bullet"><span class="by">bhaney</span><span>|</span><a href="#39096095">prev</a><span>|</span><a href="#39100620">next</a><span>|</span><label class="collapse" for="c-39095271">[-]</label><label class="expand" for="c-39095271">[3 more]</label></div><br/><div class="children"><div class="content">The algorithm described here (randomly picking some small number of keys and evicting the oldest one among them) is the core of how Redis&#x27; &quot;LRU&quot; cache eviction policies work, in case anyone was wondering.</div><br/><div id="39095905" class="c"><input type="checkbox" id="c-39095905" checked=""/><div class="controls bullet"><span class="by">refset</span><span>|</span><a href="#39095271">parent</a><span>|</span><a href="#39100620">next</a><span>|</span><label class="collapse" for="c-39095905">[-]</label><label class="expand" for="c-39095905">[2 more]</label></div><br/><div class="children"><div class="content">In a similar vein, there&#x27;s a neat &quot;Efficient Page Replacement&quot; strategy described in a LeanStore paper [0] that combines random selection with FIFO:<p>&gt; Instead of tracking frequently accessed pages in order to avoid evicting them, our replacement strategy identifies infrequently-accessed pages. We argue that with the large buffer pool sizes that are common today, this is much more efficient as it avoids any additional work when accessing a hot page<p>&gt; by speculatively unswizzling random pages, we identify infrequently-accessed pages without having to track each access. In addition, a FIFO queue serves as a probational cooling stage during which pages have a chance to be swizzled. Together, these techniques implement an effective replacement strategy at low cost<p>[0] <a href="https:&#x2F;&#x2F;db.in.tum.de&#x2F;~leis&#x2F;papers&#x2F;leanstore.pdf" rel="nofollow">https:&#x2F;&#x2F;db.in.tum.de&#x2F;~leis&#x2F;papers&#x2F;leanstore.pdf</a></div><br/><div id="39099554" class="c"><input type="checkbox" id="c-39099554" checked=""/><div class="controls bullet"><span class="by">cmrdporcupine</span><span>|</span><a href="#39095271">root</a><span>|</span><a href="#39095905">parent</a><span>|</span><a href="#39100620">next</a><span>|</span><label class="collapse" for="c-39099554">[-]</label><label class="expand" for="c-39099554">[1 more]</label></div><br/><div class="children"><div class="content">Came here to say the same thing.<p>There&#x27;s a follow-up paper from last year that talks about some refinement of this strategy (6.2 page replacement), among other things.<p>&quot;The Evolution of LeanStore&quot;<p><a href="https:&#x2F;&#x2F;dl.gi.de&#x2F;server&#x2F;api&#x2F;core&#x2F;bitstreams&#x2F;edd344ab-d765-4454-9dbe-fcfa25c8059c&#x2F;content" rel="nofollow">https:&#x2F;&#x2F;dl.gi.de&#x2F;server&#x2F;api&#x2F;core&#x2F;bitstreams&#x2F;edd344ab-d765-44...</a></div><br/></div></div></div></div></div></div><div id="39100620" class="c"><input type="checkbox" id="c-39100620" checked=""/><div class="controls bullet"><span class="by">chmike</span><span>|</span><a href="#39095271">prev</a><span>|</span><a href="#39099327">next</a><span>|</span><label class="collapse" for="c-39100620">[-]</label><label class="expand" for="c-39100620">[1 more]</label></div><br/><div class="children"><div class="content">It depends on the distribution of probabilities of being used. If least recently used data has the same probability to be requested than the others, then random picking will do as well as LRU.<p>When the least recently used data does have a lower probability to be requested, than LRU will outperform random picking.<p>There is no silver bullet algorithm.</div><br/></div></div><div id="39099327" class="c"><input type="checkbox" id="c-39099327" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39100620">prev</a><span>|</span><a href="#39097588">next</a><span>|</span><label class="collapse" for="c-39099327">[-]</label><label class="expand" for="c-39099327">[1 more]</label></div><br/><div class="children"><div class="content">See also <a href="https:&#x2F;&#x2F;www.eecs.harvard.edu&#x2F;~michaelm&#x2F;postscripts&#x2F;handbook2001.pdf" rel="nofollow">https:&#x2F;&#x2F;www.eecs.harvard.edu&#x2F;~michaelm&#x2F;postscripts&#x2F;handbook2...</a> and <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;2-choice_hashing" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;2-choice_hashing</a></div><br/></div></div><div id="39097588" class="c"><input type="checkbox" id="c-39097588" checked=""/><div class="controls bullet"><span class="by">shiandow</span><span>|</span><a href="#39099327">prev</a><span>|</span><a href="#39094623">next</a><span>|</span><label class="collapse" for="c-39097588">[-]</label><label class="expand" for="c-39097588">[2 more]</label></div><br/><div class="children"><div class="content">Even just as a cheap way of approximating LRU the k-random algorithm is quite nice.<p>Honestly I think LRU doesn&#x27;t get enough recognition for being provably only a fixed factor away from what an optimal algorithm could do with less memory [1]. In fact LRU does about as well as an online algorithm <i>can</i> do, without knowing up front which memory is about to be accessed (basically you can do statistical analysis, but this means other patterns must perform worse). My take from it is that more memory beats clever algorithms.<p>The paper is interesting by the way, I think it&#x27;s one of the first to use potentials to prove amortized optimality, and it actually generalizes a bit further showing that &#x27;move-to-front&#x27; is similarly close to optimal if the cost function of accessing an item increases with some convex function.<p>[1]: <a href="https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;2786.2793" rel="nofollow">https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;2786.2793</a></div><br/><div id="39099369" class="c"><input type="checkbox" id="c-39099369" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39097588">parent</a><span>|</span><a href="#39094623">next</a><span>|</span><label class="collapse" for="c-39099369">[-]</label><label class="expand" for="c-39099369">[1 more]</label></div><br/><div class="children"><div class="content">&gt; My take from it is that more memory beats clever algorithms.<p>Well, that&#x27;s about to be expected from a cache?</div><br/></div></div></div></div></div></div></div></div></div></body></html>