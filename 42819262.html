<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1737795674365" as="style"/><link rel="stylesheet" href="styles.css?v=1737795674365"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/Jiayi-Pan/TinyZero">TinyZero</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>fzliu</span> | <span>15 comments</span></div><br/><div><div id="42820198" class="c"><input type="checkbox" id="c-42820198" checked=""/><div class="controls bullet"><span class="by">serialx</span><span>|</span><a href="#42820310">next</a><span>|</span><label class="collapse" for="c-42820198">[-]</label><label class="expand" for="c-42820198">[6 more]</label></div><br/><div class="children"><div class="content">So to my understanding, this work reproduces DeepSeek R1&#x27;s reinforcement learning mechanism in a very small language model.<p>The AI gets &quot;rewards&quot; (like points) for doing two things correctly:<p>Accuracy : Getting the right answer. For example, math answers must be in a specific format (e.g., inside a box) so a computer can easily check them. For coding problems, test cases verify if the code works.<p>Format : Using the &lt;think&gt; and &lt;answer&gt; tags properly. This forces the AI to organize its responses clearly.<p>So in this case, the training program can extract the model&#x27;s answer by parsing &lt;answer&gt; tag. We can eval the answer and evaluate if it&#x27;s correct or not. If it&#x27;s correct give reward, else: no reward.<p>Create N such answers from a single question, create N reward array. This is enough for the RL algorithm to guide the model to be more smart.</div><br/><div id="42820291" class="c"><input type="checkbox" id="c-42820291" checked=""/><div class="controls bullet"><span class="by">krackers</span><span>|</span><a href="#42820198">parent</a><span>|</span><a href="#42820263">next</a><span>|</span><label class="collapse" for="c-42820291">[-]</label><label class="expand" for="c-42820291">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been trying to follow the literature on PPO&#x2F;GRPO as applied to LLMs. From what I understand, since reward is only given once the entire COT sequence is sampled, traditional RL techniques would require some form of credit-assignment to distribute that reward amongst individual tokens – which is where the critic&#x2F;value network comes in, right?<p>Instead DeepSeek (with GRPO) seems to just omit that value function entirely and use only sparse rewards. How does this end up being more efficient, since I thought the sparse nature of rewards makes it harder to converge to the optimal policy?</div><br/><div id="42820331" class="c"><input type="checkbox" id="c-42820331" checked=""/><div class="controls bullet"><span class="by">serialx</span><span>|</span><a href="#42820198">root</a><span>|</span><a href="#42820291">parent</a><span>|</span><a href="#42820263">next</a><span>|</span><label class="collapse" for="c-42820331">[-]</label><label class="expand" for="c-42820331">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s only using sparse rewards because of the format rewards. The training recipe is pretty comprehensive and involves multiple stages.[1] The paper mentions that when only using the RL technique, the output is often not suitable for reading. (Language mixing, etc) That feels like a AlphaZero moment for LLMs?<p>[1]: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1i8rujw&#x2F;notes_on_deepseek_r1_just_how_good_it_is_compared&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1i8rujw&#x2F;notes_o...</a></div><br/><div id="42820340" class="c"><input type="checkbox" id="c-42820340" checked=""/><div class="controls bullet"><span class="by">krackers</span><span>|</span><a href="#42820198">root</a><span>|</span><a href="#42820331">parent</a><span>|</span><a href="#42820263">next</a><span>|</span><label class="collapse" for="c-42820340">[-]</label><label class="expand" for="c-42820340">[2 more]</label></div><br/><div class="children"><div class="content">The R1 paper says that they didn&#x27;t use &quot;process reward modeling&quot;. And the paper that introduced GPRO says that it can be used either with &quot;outcome supervision&quot; or &quot;process supervision&quot;, with outcome supervision &quot;only provid[ing] a reward at the end of each output&quot;. Put together, doesn&#x27;t that imply R1 uses sparse rewards provided only at end of COT sequence?</div><br/><div id="42820379" class="c"><input type="checkbox" id="c-42820379" checked=""/><div class="controls bullet"><span class="by">serialx</span><span>|</span><a href="#42820198">root</a><span>|</span><a href="#42820340">parent</a><span>|</span><a href="#42820263">next</a><span>|</span><label class="collapse" for="c-42820379">[-]</label><label class="expand" for="c-42820379">[1 more]</label></div><br/><div class="children"><div class="content">Ah sorry, you might be right. I meant &quot;sparse reward&quot; as a reward system that is mostly 0 but occasionally 1. Your &quot;sparse reward&quot; means only providing reward at the end of each output.</div><br/></div></div></div></div></div></div></div></div><div id="42820263" class="c"><input type="checkbox" id="c-42820263" checked=""/><div class="controls bullet"><span class="by">suraci</span><span>|</span><a href="#42820198">parent</a><span>|</span><a href="#42820291">prev</a><span>|</span><a href="#42820310">next</a><span>|</span><label class="collapse" for="c-42820263">[-]</label><label class="expand" for="c-42820263">[1 more]</label></div><br/><div class="children"><div class="content">It looks like the &#x27;old-school&#x27; RL to me, which makes me wonder why it took so long to get here</div><br/></div></div></div></div><div id="42820080" class="c"><input type="checkbox" id="c-42820080" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#42820310">prev</a><span>|</span><a href="#42820101">next</a><span>|</span><label class="collapse" for="c-42820080">[-]</label><label class="expand" for="c-42820080">[1 more]</label></div><br/><div class="children"><div class="content">Unrolled non-X link with the announcement: <a href="https:&#x2F;&#x2F;threadreaderapp.com&#x2F;thread&#x2F;1882839370505621655.html" rel="nofollow">https:&#x2F;&#x2F;threadreaderapp.com&#x2F;thread&#x2F;1882839370505621655.html</a></div><br/></div></div><div id="42820101" class="c"><input type="checkbox" id="c-42820101" checked=""/><div class="controls bullet"><span class="by">blackeyeblitzar</span><span>|</span><a href="#42820080">prev</a><span>|</span><label class="collapse" for="c-42820101">[-]</label><label class="expand" for="c-42820101">[6 more]</label></div><br/><div class="children"><div class="content">What does it mean to reproduce DeepSeek R1-Zero? Like they have a model of equivalent performance? Is there a simple explanation of this post for those who aren&#x27;t machine learning experts?<p>Also is the technique here related at all to the technique people think DeepSeek themselves used, where they apparently trained the model using OpenAI outputs?</div><br/><div id="42820376" class="c"><input type="checkbox" id="c-42820376" checked=""/><div class="controls bullet"><span class="by">dvh</span><span>|</span><a href="#42820101">parent</a><span>|</span><a href="#42820323">next</a><span>|</span><label class="collapse" for="c-42820376">[-]</label><label class="expand" for="c-42820376">[1 more]</label></div><br/><div class="children"><div class="content">Reminds me of old polish encyclopedia: horse - everybody knows what horse is<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nowe_Ateny" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nowe_Ateny</a></div><br/></div></div><div id="42820323" class="c"><input type="checkbox" id="c-42820323" checked=""/><div class="controls bullet"><span class="by">evertedsphere</span><span>|</span><a href="#42820101">parent</a><span>|</span><a href="#42820376">prev</a><span>|</span><a href="#42820129">next</a><span>|</span><label class="collapse" for="c-42820323">[-]</label><label class="expand" for="c-42820323">[1 more]</label></div><br/><div class="children"><div class="content">reproducing the alphazero-like &quot;model learns to reason on its own without supervised fine-tuning&quot; phenomenon that deepseek-r1-zero exhibited</div><br/></div></div><div id="42820129" class="c"><input type="checkbox" id="c-42820129" checked=""/><div class="controls bullet"><span class="by">3nthusia5t</span><span>|</span><a href="#42820101">parent</a><span>|</span><a href="#42820323">prev</a><span>|</span><a href="#42820236">next</a><span>|</span><label class="collapse" for="c-42820129">[-]</label><label class="expand" for="c-42820129">[2 more]</label></div><br/><div class="children"><div class="content">Could you provide source for the training the model on OpenAI outputs? I can’t find any news about that.</div><br/><div id="42820158" class="c"><input type="checkbox" id="c-42820158" checked=""/><div class="controls bullet"><span class="by">blackeyeblitzar</span><span>|</span><a href="#42820101">root</a><span>|</span><a href="#42820129">parent</a><span>|</span><a href="#42820236">next</a><span>|</span><label class="collapse" for="c-42820158">[-]</label><label class="expand" for="c-42820158">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have a source to share, but I saw this claim on social media a few times in the last couple days, where people said their conversation with the model revealed that it thought it was some other OpenAI model. I have no idea how such training can work using another model&#x27;s output, but I saw comments claiming that this is why their training was so cheap.</div><br/></div></div></div></div><div id="42820236" class="c"><input type="checkbox" id="c-42820236" checked=""/><div class="controls bullet"><span class="by">suraci</span><span>|</span><a href="#42820101">parent</a><span>|</span><a href="#42820129">prev</a><span>|</span><label class="collapse" for="c-42820236">[-]</label><label class="expand" for="c-42820236">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What does it mean to reproduce DeepSeek R1-Zero?<p>means it&#x27;s reproducible</div><br/></div></div></div></div></div></div></div></div></div></body></html>