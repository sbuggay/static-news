<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1684054842543" as="style"/><link rel="stylesheet" href="styles.css?v=1684054842543"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/s-JoL/Open-Llama">Open-Lamam: A “real” open-source project to train LLM not just checkpoints</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>bayes-song</span> | <span>4 comments</span></div><br/><div><div id="35936056" class="c"><input type="checkbox" id="c-35936056" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#35936450">next</a><span>|</span><label class="collapse" for="c-35936056">[-]</label><label class="expand" for="c-35936056">[1 more]</label></div><br/><div class="children"><div class="content">Namespace collisions are inevitable, especially w&#x2F; how fast-moving the LLM space is right now, just wanted to point out that besides this &quot;Open-Llama&quot; project (which looks really interesting, and well documented in the Github repo), there is also another group training &quot;OpenLLaMA&quot; <a href="https:&#x2F;&#x2F;github.com&#x2F;openlm-research&#x2F;open_llama">https:&#x2F;&#x2F;github.com&#x2F;openlm-research&#x2F;open_llama</a> (which looks like an effort by two Berkeley PhD students, <a href="https:&#x2F;&#x2F;www.haoliu.site&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.haoliu.site&#x2F;</a> and <a href="http:&#x2F;&#x2F;young-geng.xyz&#x2F;" rel="nofollow">http:&#x2F;&#x2F;young-geng.xyz&#x2F;</a> to reproduce LLaMA using the 1.2T token Together RedPajama dataset. They&#x27;ve released up to a 300B checkpoint so far.)<p>Feedback for &#x2F;u&#x2F;bayes-song - it&#x27;d be great to have a more info on the model card on HF - right now it&#x27;s unclear the parameter count, # of total tokens you&#x27;re planning on training on&#x2F;how many you&#x27;ve trained on so far. An Evaluation section (maybe using lm-evaluation-harness) might be good as well?</div><br/></div></div><div id="35936450" class="c"><input type="checkbox" id="c-35936450" checked=""/><div class="controls bullet"><span class="by">christkv</span><span>|</span><a href="#35936056">prev</a><span>|</span><a href="#35934466">next</a><span>|</span><label class="collapse" for="c-35936450">[-]</label><label class="expand" for="c-35936450">[1 more]</label></div><br/><div class="children"><div class="content">Feels like its still the area of wait and see as the space shakes out. It would be great to be able to run our own models in some near future for applications but the amount of hardware needed to delivery service to a significant audience is pretty crazy. Right now I don&#x27;t see any way but to re-bill the cost with a markup to end customers unless you have a giant pile of VC money that you can light on fire.</div><br/></div></div><div id="35934466" class="c"><input type="checkbox" id="c-35934466" checked=""/><div class="controls bullet"><span class="by">bayes-song</span><span>|</span><a href="#35936450">prev</a><span>|</span><label class="collapse" for="c-35934466">[-]</label><label class="expand" for="c-35934466">[1 more]</label></div><br/><div class="children"><div class="content">Check out this model trained using the Open-Llama project at <a href="http:&#x2F;&#x2F;home.ustc.edu.cn&#x2F;~sl9292" rel="nofollow">http:&#x2F;&#x2F;home.ustc.edu.cn&#x2F;~sl9292</a> . This model is trained primarily on English and Chinese, but also has capabilities in other languages like Japanese and Korean.
Now, let&#x27;s dive into Open-Llama. It&#x27;s a truly open-source project for pre-training and instruct-tuning AI models. One of the key features of this project is its support for a wide range of model sizes, from 7B to 65B parameters.
What sets Open-Llama apart is the incorporation of performance acceleration via xformers from Llama, enabling 95% of the original Llama speed on the 65B models. In fact, for the 7B models, Open-Llama&#x27;s performance surpasses the original Llama.
By providing full access to the codebase, we believe that Open-Llama will contribute greatly to the advancement of open-source AI technologies. We invite developers and researchers to join us on this exciting journey!</div><br/></div></div></div></div></div></div></div></body></html>