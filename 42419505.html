<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1734253257894" as="style"/><link rel="stylesheet" href="styles.css?v=1734253257894"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/ggerganov/llama.cpp/pull/10361">Llama.cpp Now Supports Qwen2-VL (Vision Language Model)</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>BUFU</span> | <span>23 comments</span></div><br/><div><div id="42420840" class="c"><input type="checkbox" id="c-42420840" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42421033">next</a><span>|</span><label class="collapse" for="c-42420840">[-]</label><label class="expand" for="c-42420840">[17 more]</label></div><br/><div class="children"><div class="content">The Qwen family of models are REALLY impressive. I would encourage anyone who hasn&#x27;t paid them any attention to at least add them to your mental list of LLMs worth knowing about.<p>Qwen2-VL is a decent vision model. You can try it out online here: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;GanymedeNil&#x2F;Qwen2-VL-7B" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;GanymedeNil&#x2F;Qwen2-VL-7B</a> - I got great results from it for OCR against handwritten text: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Sep&#x2F;4&#x2F;qwen2-vl&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Sep&#x2F;4&#x2F;qwen2-vl&#x2F;</a><p>Qwen2.5-Coder-32B is an excellent (I&#x27;d say even GPT-4 class) model at generating code which I can run on a 64GB M2 MacBook Pro: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Nov&#x2F;12&#x2F;qwen25-coder&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Nov&#x2F;12&#x2F;qwen25-coder&#x2F;</a><p>QwQ is the Qwen team&#x27;s exploration of the o1-style of model that has built in chain-of-thought. It&#x27;s absolutely fascinating, partly because if you ask it a question in English it will often <i>think in Chinese</i> before spitting out an answer in English. My notes on that one here: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Nov&#x2F;27&#x2F;qwq&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Nov&#x2F;27&#x2F;qwq&#x2F;</a><p>Most of the Qwen models are Apache 2 licensed, which makes them more open than many of the other open weights models (Llama etc).<p>(Unsurprisingly they all get quite stubborn if you ask them about topics like Tiananmen Square)</div><br/><div id="42422274" class="c"><input type="checkbox" id="c-42422274" checked=""/><div class="controls bullet"><span class="by">teej</span><span>|</span><a href="#42420840">parent</a><span>|</span><a href="#42422258">next</a><span>|</span><label class="collapse" for="c-42422274">[-]</label><label class="expand" for="c-42422274">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Unsurprisingly they all get quite stubborn if you ask them about topics like Tiananmen Square<p>Has anyone made a political censorship eval yet?</div><br/></div></div><div id="42422258" class="c"><input type="checkbox" id="c-42422258" checked=""/><div class="controls bullet"><span class="by">jl6</span><span>|</span><a href="#42420840">parent</a><span>|</span><a href="#42422274">prev</a><span>|</span><a href="#42420915">next</a><span>|</span><label class="collapse" for="c-42422258">[-]</label><label class="expand" for="c-42422258">[1 more]</label></div><br/><div class="children"><div class="content">Why does this model think in Chinese and o1 think in English? Is this because chain-of-thought is achieved by training these models on examples of what “thinking” looks like, which have been constructed by their respective model developers, as opposed to being a more generic feature?</div><br/></div></div><div id="42420915" class="c"><input type="checkbox" id="c-42420915" checked=""/><div class="controls bullet"><span class="by">sharih</span><span>|</span><a href="#42420840">parent</a><span>|</span><a href="#42422258">prev</a><span>|</span><a href="#42420908">next</a><span>|</span><label class="collapse" for="c-42420915">[-]</label><label class="expand" for="c-42420915">[1 more]</label></div><br/><div class="children"><div class="content">Agree. It is amazing that you can run an o1 style model on a Mac. I was able to run QwQ on my 24GB M3 MacBook Air, though results on complex reasoning on domain specific tasks did not work well, and I saw the Chinese &#x27;thinking&#x27; too (they don&#x27;t work well in o1 either). It opens up experimentation which is great, and the reasoning traces for domain specific tasks for RL is where the next improvements are going to come from</div><br/></div></div><div id="42420908" class="c"><input type="checkbox" id="c-42420908" checked=""/><div class="controls bullet"><span class="by">mkl</span><span>|</span><a href="#42420840">parent</a><span>|</span><a href="#42420915">prev</a><span>|</span><a href="#42421545">next</a><span>|</span><label class="collapse" for="c-42420908">[-]</label><label class="expand" for="c-42420908">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty good for handwritten maths too - I just tried that demo.  Do you know any other open models good at maths notation?</div><br/><div id="42420958" class="c"><input type="checkbox" id="c-42420958" checked=""/><div class="controls bullet"><span class="by">sadboi31</span><span>|</span><a href="#42420840">root</a><span>|</span><a href="#42420908">parent</a><span>|</span><a href="#42421545">next</a><span>|</span><label class="collapse" for="c-42420958">[-]</label><label class="expand" for="c-42420958">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;TIGER-Lab&#x2F;MathInstruct" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;TIGER-Lab&#x2F;MathInstruct</a><p>Works with +700 year old books w some tweaks. took like $400 to train. can&#x27;t share more because i don&#x27;t know more.</div><br/><div id="42421434" class="c"><input type="checkbox" id="c-42421434" checked=""/><div class="controls bullet"><span class="by">mkl</span><span>|</span><a href="#42420840">root</a><span>|</span><a href="#42420958">parent</a><span>|</span><a href="#42421545">next</a><span>|</span><label class="collapse" for="c-42421434">[-]</label><label class="expand" for="c-42421434">[1 more]</label></div><br/><div class="children"><div class="content">That seems to be just for LLMs, not visual.  I&#x27;m wanting to go from images of maths notation (photos, scans, digital handwriting) to formulas in Latex or MathML or something.  Qwen2-VL can do it, but it&#x27;s pretty heavyweight for just that.</div><br/></div></div></div></div></div></div><div id="42421545" class="c"><input type="checkbox" id="c-42421545" checked=""/><div class="controls bullet"><span class="by">nothrowaways</span><span>|</span><a href="#42420840">parent</a><span>|</span><a href="#42420908">prev</a><span>|</span><a href="#42421814">next</a><span>|</span><label class="collapse" for="c-42421545">[-]</label><label class="expand" for="c-42421545">[1 more]</label></div><br/><div class="children"><div class="content">Just tested it with a meme, and it nailed it.</div><br/></div></div><div id="42421814" class="c"><input type="checkbox" id="c-42421814" checked=""/><div class="controls bullet"><span class="by">therein</span><span>|</span><a href="#42420840">parent</a><span>|</span><a href="#42421545">prev</a><span>|</span><a href="#42421718">next</a><span>|</span><label class="collapse" for="c-42421814">[-]</label><label class="expand" for="c-42421814">[2 more]</label></div><br/><div class="children"><div class="content">Are we at a point where we could run nonquantized models from QwQ&#x2F;Qwen series on a 128GB Macbook Pro?</div><br/><div id="42421857" class="c"><input type="checkbox" id="c-42421857" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42420840">root</a><span>|</span><a href="#42421814">parent</a><span>|</span><a href="#42421718">next</a><span>|</span><label class="collapse" for="c-42421857">[-]</label><label class="expand" for="c-42421857">[1 more]</label></div><br/><div class="children"><div class="content">I think so. Are bf16 models nonquantized? There&#x27;s an MLX one here that should fit on that machine: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;mlx-community&#x2F;QwQ-32B-Preview-bf16" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;mlx-community&#x2F;QwQ-32B-Preview-bf16</a></div><br/></div></div></div></div><div id="42421718" class="c"><input type="checkbox" id="c-42421718" checked=""/><div class="controls bullet"><span class="by">annon2003</span><span>|</span><a href="#42420840">parent</a><span>|</span><a href="#42421814">prev</a><span>|</span><a href="#42421033">next</a><span>|</span><label class="collapse" for="c-42421718">[-]</label><label class="expand" for="c-42421718">[7 more]</label></div><br/><div class="children"><div class="content">so far I was pretty courageous in giving my language models full access to the console so they can perform terminal comments whenever necessary.<p>thinking that the Chinese government might have built in a back door gives me a little pause though</div><br/><div id="42422267" class="c"><input type="checkbox" id="c-42422267" checked=""/><div class="controls bullet"><span class="by">lodovic</span><span>|</span><a href="#42420840">root</a><span>|</span><a href="#42421718">parent</a><span>|</span><a href="#42421832">next</a><span>|</span><label class="collapse" for="c-42422267">[-]</label><label class="expand" for="c-42422267">[1 more]</label></div><br/><div class="children"><div class="content">There is no backdoor but the model is heavily censored and biased towards China. It refuses to discuss Chinese or North Korean politicians, Tiananmen square, Uyghurs or anything sensitive to China. It&#x27;s quite positive about Putin - it doesn&#x27;t mind trashing Western leaders, though. It may write clever code, and I understand that Chinese researchers have to abide by local laws, but it certainly has opinions that are incompatible with mine.</div><br/></div></div><div id="42421832" class="c"><input type="checkbox" id="c-42421832" checked=""/><div class="controls bullet"><span class="by">mkl</span><span>|</span><a href="#42420840">root</a><span>|</span><a href="#42421718">parent</a><span>|</span><a href="#42422267">prev</a><span>|</span><a href="#42421771">next</a><span>|</span><label class="collapse" for="c-42421832">[-]</label><label class="expand" for="c-42421832">[4 more]</label></div><br/><div class="children"><div class="content">How would a bunch of weights make a backdoor?  The worst it could do is detect it&#x27;s accessing an actual console and run a logged, visible command that tries to mess with your config or phone home, which is more of a front door with flashing lights saying &quot;here I am!&quot;, so why would they bother?<p>Letting an LLM run arbitrary commands in your main user account seems risky even without worrying about conspiracies.</div><br/><div id="42422285" class="c"><input type="checkbox" id="c-42422285" checked=""/><div class="controls bullet"><span class="by">teej</span><span>|</span><a href="#42420840">root</a><span>|</span><a href="#42421832">parent</a><span>|</span><a href="#42421874">next</a><span>|</span><label class="collapse" for="c-42422285">[-]</label><label class="expand" for="c-42422285">[1 more]</label></div><br/><div class="children"><div class="content">It’s absolutely possible to put a backdoor into an LLM.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.12798" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.12798</a></div><br/></div></div><div id="42421874" class="c"><input type="checkbox" id="c-42421874" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#42420840">root</a><span>|</span><a href="#42421832">parent</a><span>|</span><a href="#42422285">prev</a><span>|</span><a href="#42421771">next</a><span>|</span><label class="collapse" for="c-42421874">[-]</label><label class="expand" for="c-42421874">[2 more]</label></div><br/><div class="children"><div class="content">Just to wear my tin foil hat for fun, it&#x27;s not that the model would attempt to phone home itself (what would it have to say, anyway?) but that given the opportunity it would go around kicking doors open for later infiltration by an outside party. Subtle bugs being introduced to your Django app, invisible characters that break your ssh configs, that sort of thing.</div><br/><div id="42421953" class="c"><input type="checkbox" id="c-42421953" checked=""/><div class="controls bullet"><span class="by">mkl</span><span>|</span><a href="#42420840">root</a><span>|</span><a href="#42421874">parent</a><span>|</span><a href="#42421771">next</a><span>|</span><label class="collapse" for="c-42421953">[-]</label><label class="expand" for="c-42421953">[1 more]</label></div><br/><div class="children"><div class="content">Yes, deliberately introducing vulnerabilities when generating code is a good one, and could be quite subtle.  For running console commands though, anything touching configuration for ssh, gpg, bash aliases, ~&#x2F;bin, cron, etc., should be immediately obvious.<p>I was thinking &quot;here&#x27;s an IP address and ssh key&quot; would be what to phone home with, and that could be encrypted&#x2F;hidden pretty well, but any network access should be pretty suspicious right away.</div><br/></div></div></div></div></div></div><div id="42421771" class="c"><input type="checkbox" id="c-42421771" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#42420840">root</a><span>|</span><a href="#42421718">parent</a><span>|</span><a href="#42421832">prev</a><span>|</span><a href="#42421033">next</a><span>|</span><label class="collapse" for="c-42421771">[-]</label><label class="expand" for="c-42421771">[1 more]</label></div><br/><div class="children"><div class="content">Why do you assume backdoors are limited to the Chinese government?</div><br/></div></div></div></div></div></div><div id="42421033" class="c"><input type="checkbox" id="c-42421033" checked=""/><div class="controls bullet"><span class="by">mythz</span><span>|</span><a href="#42420840">prev</a><span>|</span><label class="collapse" for="c-42421033">[-]</label><label class="expand" for="c-42421033">[5 more]</label></div><br/><div class="children"><div class="content">IMHO Qwen are shipping the best OSS models you can run locally on consumer GPUs right now, getting great results from both qwen2.5-coder:32b and qwq:32b running at ~18 tok&#x2F;s and running on older NVidia A4000 GPUs. Definitely my first choices for local workloads.<p>It&#x27;s also great to see qwq&#x27;s open chain of thought baked in a OSS LLM so you can see it reason with itself in real-time, it&#x27;s the kind of secret sauce that proprietary LLMs like o1 would prefer to keep hidden to try build a moat around.<p>We&#x27;ve got a lot to thank Meta and Qwen for in continually releasing improving high quality OSS models which also encourages others to follow. High quality OSS models are the best thing keeping the cost of LLMs down, you can get unbelievable value on OpenRouter with qwen2.5-coder:32b at $0.08&#x2F;$0.18 M&#x2F;tok qwq:32 available at $0.15&#x2F;$0.60 M&#x2F;tok which is more than 18x cheaper than Anthropic&#x27;s latest budget Haiku 3.5 model at $0.80&#x2F;$4 M&#x2F;tok (4x price hike over Haiku 3.0).</div><br/><div id="42422404" class="c"><input type="checkbox" id="c-42422404" checked=""/><div class="controls bullet"><span class="by">foo42</span><span>|</span><a href="#42421033">parent</a><span>|</span><a href="#42421121">next</a><span>|</span><label class="collapse" for="c-42422404">[-]</label><label class="expand" for="c-42422404">[1 more]</label></div><br/><div class="children"><div class="content">Previously I&#x27;ve always been very skeptical of rosy pictures of a possible future where &quot;everyone has an ai that&#x27;s there to accomplish tasks <i>for them</i>&quot; - given that I imagined such ai (if it ever came to exist) being run by the usual big tech who have their own incentives not so cleanly aligned with our own.<p>Right now, with the availability of open weights for cutting-edge models, it feels like this wave of technological advance is pleasantly decentralised however. I can download and run a model and tinker with things which  at least feel like the seeds of such a future, where I _might_ be able to build things with my own interests at heart.<p>But what happens if these models stop being shared, and how likely is that? Reading about the vast quantities of compute deployed to train them, replicating the successes of the main players with a community of volunteers just seems an order of magnitude less achievable than traditional OSS efforts like Linux. This wave feels so tied to massive scale for its success, what do we do if big-tech stop handing out models?</div><br/></div></div><div id="42421121" class="c"><input type="checkbox" id="c-42421121" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42421033">parent</a><span>|</span><a href="#42422404">prev</a><span>|</span><label class="collapse" for="c-42421121">[-]</label><label class="expand" for="c-42421121">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, Haiku 3.5 doesn&#x27;t really count as a budget model - Anthropic have kept the original Claude 3 Haiku around for their budget entry.<p>I collected notes on the lowest cost hosted LLMs from the major vendors when I wrote up Amazon Nova last week: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Dec&#x2F;4&#x2F;amazon-nova&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Dec&#x2F;4&#x2F;amazon-nova&#x2F;</a><p>Nova Micro is $0.035&#x2F;$0.14 and Google&#x27;s Gemini 1.5 Flash 8B is $0.0375&#x2F;$0.15 - just beating those OpenRouter prices, but it may well be that the Qwen models provide better results.</div><br/><div id="42421175" class="c"><input type="checkbox" id="c-42421175" checked=""/><div class="controls bullet"><span class="by">mythz</span><span>|</span><a href="#42421033">root</a><span>|</span><a href="#42421121">parent</a><span>|</span><label class="collapse" for="c-42421175">[-]</label><label class="expand" for="c-42421175">[2 more]</label></div><br/><div class="children"><div class="content">Yeah I&#x27;m currently using Gemini 2 Flash (exp) free quota for a premium hosted model, it&#x27;s a surprisingly great model, IMO Google has caught up with the leaders with their latest experimental models. I&#x27;ve also tested Nova&#x27;s models, which are pretty high quality and exception value (lite&#x2F;micro) for their performance.<p>Also worth shouting out you can get Meta&#x27;s latest llama-3.3:70b (comparable to llama3.1:405b but must faster and cheaper) within GroqCloud&#x27;s free quotas running at an impressive 276 tok&#x2F;s.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>