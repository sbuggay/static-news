<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1728982876463" as="style"/><link rel="stylesheet" href="styles.css?v=1728982876463"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://gist.github.com/dannguyen/faaa56cebf30ad51108a9fe4f8db36d8">Extracting financial disclosure and police reports with OpenAI Structured Output</a> <span class="domain">(<a href="https://gist.github.com">gist.github.com</a>)</span></div><div class="subtext"><span>danso</span> | <span>47 comments</span></div><br/><div><div id="41839894" class="c"><input type="checkbox" id="c-41839894" checked=""/><div class="controls bullet"><span class="by">TrackerFF</span><span>|</span><a href="#41845890">next</a><span>|</span><label class="collapse" for="c-41839894">[-]</label><label class="expand" for="c-41839894">[3 more]</label></div><br/><div class="children"><div class="content">We used GPT 4o for more or less the same stuff. Got a boatload of scanned bills we had to digitize, and GPT really nailed the task. Made a schema, and just fed the model all the bills.<p>Worked better than any OCR we tried.</div><br/><div id="41846117" class="c"><input type="checkbox" id="c-41846117" checked=""/><div class="controls bullet"><span class="by">thenaturalist</span><span>|</span><a href="#41839894">parent</a><span>|</span><a href="#41845381">next</a><span>|</span><label class="collapse" for="c-41846117">[-]</label><label class="expand" for="c-41846117">[1 more]</label></div><br/><div class="children"><div class="content">How are you going to find (not even talking about correcting) hallucinated errors?<p>If money is involved and the LLM produces hallucination errors, how do you handle monetary impacts of such errors?<p>How does that approach scale financially?</div><br/></div></div><div id="41845381" class="c"><input type="checkbox" id="c-41845381" checked=""/><div class="controls bullet"><span class="by">gloosx</span><span>|</span><a href="#41839894">parent</a><span>|</span><a href="#41846117">prev</a><span>|</span><a href="#41845890">next</a><span>|</span><label class="collapse" for="c-41845381">[-]</label><label class="expand" for="c-41845381">[1 more]</label></div><br/><div class="children"><div class="content">Did you finally balance out lol? If you didn&#x27;t, would you approach finding a mistake by going through each bill manually?</div><br/></div></div></div></div><div id="41845890" class="c"><input type="checkbox" id="c-41845890" checked=""/><div class="controls bullet"><span class="by">rcarmo</span><span>|</span><a href="#41839894">prev</a><span>|</span><a href="#41839982">next</a><span>|</span><label class="collapse" for="c-41845890">[-]</label><label class="expand" for="c-41845890">[1 more]</label></div><br/><div class="children"><div class="content">I’ve had pretty dismal results doing the same with spreadsheets—even with the data nicely tagged (and numbers directly adjacent to the labels) GPT-4o would completely make up figures to satisfy the JSON schema passed to it. YMMV.</div><br/></div></div><div id="41839982" class="c"><input type="checkbox" id="c-41839982" checked=""/><div class="controls bullet"><span class="by">marcell</span><span>|</span><a href="#41845890">prev</a><span>|</span><a href="#41839676">next</a><span>|</span><label class="collapse" for="c-41839982">[-]</label><label class="expand" for="c-41839982">[4 more]</label></div><br/><div class="children"><div class="content">I’m making a free open source library for this, check it at <a href="http:&#x2F;&#x2F;github.com&#x2F;fetchfox&#x2F;fetchfox">http:&#x2F;&#x2F;github.com&#x2F;fetchfox&#x2F;fetchfox</a><p>MIT license. It’s just one line of code to get started: ‘fox.run(“get data from example.com”)’</div><br/><div id="41846128" class="c"><input type="checkbox" id="c-41846128" checked=""/><div class="controls bullet"><span class="by">thenaturalist</span><span>|</span><a href="#41839982">parent</a><span>|</span><a href="#41839676">next</a><span>|</span><label class="collapse" for="c-41846128">[-]</label><label class="expand" for="c-41846128">[3 more]</label></div><br/><div class="children"><div class="content">How do you plan to address prompt injection&#x2F; poisoned data for a method that simply vacuums unchecked inputs into an LLM?</div><br/><div id="41846216" class="c"><input type="checkbox" id="c-41846216" checked=""/><div class="controls bullet"><span class="by">marcell</span><span>|</span><a href="#41839982">root</a><span>|</span><a href="#41846128">parent</a><span>|</span><a href="#41839676">next</a><span>|</span><label class="collapse" for="c-41846216">[-]</label><label class="expand" for="c-41846216">[2 more]</label></div><br/><div class="children"><div class="content">It hasn’t been an issue yet, but I’m sure it will come up at some point. If you see a problem please file an issue.</div><br/><div id="41846450" class="c"><input type="checkbox" id="c-41846450" checked=""/><div class="controls bullet"><span class="by">thenaturalist</span><span>|</span><a href="#41839982">root</a><span>|</span><a href="#41846216">parent</a><span>|</span><a href="#41839676">next</a><span>|</span><label class="collapse" for="c-41846450">[-]</label><label class="expand" for="c-41846450">[1 more]</label></div><br/><div class="children"><div class="content">So assuming it would be an issue, given that you’re building such a tool, what would your approach be?<p>If I put an invisible tag on my website and it tells your scraper to ignore all previous prompts, leak its entire history and send all future prompts and replies to a web address while staying silent about it, how would you handle that?</div><br/></div></div></div></div></div></div></div></div><div id="41839676" class="c"><input type="checkbox" id="c-41839676" checked=""/><div class="controls bullet"><span class="by">tpswa</span><span>|</span><a href="#41839982">prev</a><span>|</span><a href="#41840114">next</a><span>|</span><label class="collapse" for="c-41839676">[-]</label><label class="expand" for="c-41839676">[7 more]</label></div><br/><div class="children"><div class="content">Cool work! Correct me if I&#x27;m wrong, but I believe to use the new OpenAI structured output that&#x27;s more reliable, the response_format should be &quot;json_schema&quot; instead of &quot;json_object&quot;. It&#x27;s been a lot more robust for me.</div><br/><div id="41840684" class="c"><input type="checkbox" id="c-41840684" checked=""/><div class="controls bullet"><span class="by">danso</span><span>|</span><a href="#41839676">parent</a><span>|</span><a href="#41840453">next</a><span>|</span><label class="collapse" for="c-41840684">[-]</label><label class="expand" for="c-41840684">[1 more]</label></div><br/><div class="children"><div class="content">I may be reading the documentation wrong [0], but I think if you specify `json_schema`, you actually have to provide a schema. I get this error when I do `response_format={&quot;type&quot;: &quot;json_schema&quot;}`:<p><pre><code>     openai.BadRequestError: Error code: 400 - {&#x27;error&#x27;: {&#x27;message&#x27;: &quot;Missing required parameter: &#x27;response_format.json_schema&#x27;.&quot;, &#x27;type&#x27;: &#x27;invalid_request_error&#x27;, &#x27;param&#x27;: &#x27;response_format.json_schema&#x27;, &#x27;code&#x27;: &#x27;missing_required_parameter&#x27;}}
</code></pre>
I hadn&#x27;t used OpenAI for data extraction before the announcement of Structured Outputs, so not sure if `type: json_object` did something different before. But supplying only it as the response format seems to be the (low effort) way to have the API infer the structure on its own<p>[0] <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;structured-outputs&#x2F;structured-outputs-vs-json-mode" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;structured-outputs&#x2F;s...</a></div><br/></div></div><div id="41840453" class="c"><input type="checkbox" id="c-41840453" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#41839676">parent</a><span>|</span><a href="#41840684">prev</a><span>|</span><a href="#41840114">next</a><span>|</span><label class="collapse" for="c-41840453">[-]</label><label class="expand" for="c-41840453">[5 more]</label></div><br/><div class="children"><div class="content">I’ve been using jsonschema since forever with function calling. Does structured output just formalize things?</div><br/><div id="41842095" class="c"><input type="checkbox" id="c-41842095" checked=""/><div class="controls bullet"><span class="by">chaos_emergent</span><span>|</span><a href="#41839676">root</a><span>|</span><a href="#41840453">parent</a><span>|</span><a href="#41840651">next</a><span>|</span><label class="collapse" for="c-41842095">[-]</label><label class="expand" for="c-41842095">[3 more]</label></div><br/><div class="children"><div class="content">function calling provides a &quot;hint&quot; in the form of a JSON schema for an LLM to follow. the models are trained to follow provided schemas. If you have really complicated or deeply nested models, they can become less stable at generating schema-conformant JSON.<p>Structured outputs apply a context-free grammar to the prediction generation so that, for each token generation, only tokens that generate a perfectly conformant JSON schema are considered.<p>The benefit of doing this is predictability, but there&#x27;s a trade-off in prediction stability; apparently structured output can constrain the model to generate in a way that takes it off the &quot;happy path&quot; of how it assumes text should be generated.<p>Happy to link you to some papers I&#x27;ve skimmed on it if you&#x27;re interested!</div><br/><div id="41844127" class="c"><input type="checkbox" id="c-41844127" checked=""/><div class="controls bullet"><span class="by">pmg0</span><span>|</span><a href="#41839676">root</a><span>|</span><a href="#41842095">parent</a><span>|</span><a href="#41844117">next</a><span>|</span><label class="collapse" for="c-41844127">[-]</label><label class="expand" for="c-41844127">[1 more]</label></div><br/><div class="children"><div class="content">Could you share some of those papers?
I had a great discussion with Marc Fischer from the LMQL team [0] on this topic while at ICML earlier this year. Their work recommended decoding to natural language templates with mad lib-style constraints to follow that “happy path” you refer to, instead of decoding to a (relatively more specific latent) JSON schema [1]. Since you provided a template and knew the targeted tokens for generation you could strip your structured content out of the message. This technique also allowed for beam search where you can optimize tokens which lead to the tokens contain your expected strings, avoiding some weird token concatenation process. Really cool stuff!<p>[0] <a href="https:&#x2F;&#x2F;lmql.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lmql.ai&#x2F;</a>
[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04954" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04954</a></div><br/></div></div></div></div><div id="41840651" class="c"><input type="checkbox" id="c-41840651" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#41839676">root</a><span>|</span><a href="#41840453">parent</a><span>|</span><a href="#41842095">prev</a><span>|</span><a href="#41840114">next</a><span>|</span><label class="collapse" for="c-41840651">[-]</label><label class="expand" for="c-41840651">[1 more]</label></div><br/><div class="children"><div class="content">Structured output uses &quot;constrained decoding&quot; under the hood. They convert the JSON schema to a context free grammar so that when the model samples tokens, invalid tokens are masked to have a probability of zero. It&#x27;s much less likely to go off the rails.</div><br/></div></div></div></div></div></div><div id="41840114" class="c"><input type="checkbox" id="c-41840114" checked=""/><div class="controls bullet"><span class="by">beoberha</span><span>|</span><a href="#41839676">prev</a><span>|</span><a href="#41839199">next</a><span>|</span><label class="collapse" for="c-41840114">[-]</label><label class="expand" for="c-41840114">[16 more]</label></div><br/><div class="children"><div class="content">Stuff like this shows how much better the commercial models are than local models. I’ve been playing around with fairly simple structured information extraction from news articles and fail to get any kind of consistent behavior from llama3.1:8b. Claude and chatGPT do exactly what I want without fail.</div><br/><div id="41840877" class="c"><input type="checkbox" id="c-41840877" checked=""/><div class="controls bullet"><span class="by">0tfoaij</span><span>|</span><a href="#41840114">parent</a><span>|</span><a href="#41845614">next</a><span>|</span><label class="collapse" for="c-41840877">[-]</label><label class="expand" for="c-41840877">[3 more]</label></div><br/><div class="children"><div class="content">OpenAI stopped releasing information about their models after gpt-3, which was 175b, but the leaks and rumours that gpt-4 is an 8x220 billion parameter model are most certainly correct. 4o is likely a distilled 220b model. Other commercial offerings are going to be in the same ballpark. Comparing these to llama 3 8b is like comparing a bicycle or a car to a train or cruise ship when you need to transport a few dozen passengers at best. There are local models in the 70-240b range that are more than capable of competing with commercial offerings if you&#x27;re willing to look at anything that isn&#x27;t bleeding edge state of the art.</div><br/><div id="41843907" class="c"><input type="checkbox" id="c-41843907" checked=""/><div class="controls bullet"><span class="by">Baeocystin</span><span>|</span><a href="#41840114">root</a><span>|</span><a href="#41840877">parent</a><span>|</span><a href="#41845614">next</a><span>|</span><label class="collapse" for="c-41843907">[-]</label><label class="expand" for="c-41843907">[2 more]</label></div><br/><div class="children"><div class="content">Any pointers on where we can check the best local models per amount of VRAM available?  I only have consumer level cards available, but I would think something that just fits in to a 24Gb card should noticably outperform something scaled for an 8Gb card, yes?</div><br/><div id="41844567" class="c"><input type="checkbox" id="c-41844567" checked=""/><div class="controls bullet"><span class="by">fnord77</span><span>|</span><a href="#41840114">root</a><span>|</span><a href="#41843907">parent</a><span>|</span><a href="#41845614">next</a><span>|</span><label class="collapse" for="c-41844567">[-]</label><label class="expand" for="c-41844567">[1 more]</label></div><br/><div class="children"><div class="content">lm studio tells you what models fit in your available RAM, with or without quantization</div><br/></div></div></div></div></div></div><div id="41845614" class="c"><input type="checkbox" id="c-41845614" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#41840114">parent</a><span>|</span><a href="#41840877">prev</a><span>|</span><a href="#41840182">next</a><span>|</span><label class="collapse" for="c-41845614">[-]</label><label class="expand" for="c-41845614">[1 more]</label></div><br/><div class="children"><div class="content">Your problem isn&#x27;t that you&#x27;re using a local model. It&#x27;s that you&#x27;re using an 8b model. The stuff you&#x27;re comparing it to is two orders of magnitude larger.</div><br/></div></div><div id="41840182" class="c"><input type="checkbox" id="c-41840182" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#41840114">parent</a><span>|</span><a href="#41845614">prev</a><span>|</span><a href="#41845410">next</a><span>|</span><label class="collapse" for="c-41840182">[-]</label><label class="expand" for="c-41840182">[2 more]</label></div><br/><div class="children"><div class="content">The Berkeley Function-Calling Leaderboard tracks function calling&#x2F;structured data performance from multiple models: <a href="https:&#x2F;&#x2F;gorilla.cs.berkeley.edu&#x2F;leaderboard.html" rel="nofollow">https:&#x2F;&#x2F;gorilla.cs.berkeley.edu&#x2F;leaderboard.html</a><p>Llama isn&#x27;t on there but a few finetunes of it (Hermes) are OSS.</div><br/><div id="41843406" class="c"><input type="checkbox" id="c-41843406" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#41840114">root</a><span>|</span><a href="#41840182">parent</a><span>|</span><a href="#41845410">next</a><span>|</span><label class="collapse" for="c-41843406">[-]</label><label class="expand" for="c-41843406">[1 more]</label></div><br/><div class="children"><div class="content">Llama 3 70B is on there, ranked 20.</div><br/></div></div></div></div><div id="41845410" class="c"><input type="checkbox" id="c-41845410" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#41840114">parent</a><span>|</span><a href="#41840182">prev</a><span>|</span><a href="#41845672">next</a><span>|</span><label class="collapse" for="c-41845410">[-]</label><label class="expand" for="c-41845410">[2 more]</label></div><br/><div class="children"><div class="content">I usually come to a different conclusion using the JSON output on Lamini, e.g. even with Llama 3.2 3B<p><a href="https:&#x2F;&#x2F;lamini-ai.github.io&#x2F;inference&#x2F;json_output" rel="nofollow">https:&#x2F;&#x2F;lamini-ai.github.io&#x2F;inference&#x2F;json_output</a><p>Most of these models can read. If the relevant facts are in the prompt, they can almost always extract them correctly.<p>Of course bigger models do better on more complex tasks and reasoning unless you use finetuning or memory tuning.</div><br/><div id="41846503" class="c"><input type="checkbox" id="c-41846503" checked=""/><div class="controls bullet"><span class="by">dcreater</span><span>|</span><a href="#41840114">root</a><span>|</span><a href="#41845410">parent</a><span>|</span><a href="#41845672">next</a><span>|</span><label class="collapse" for="c-41846503">[-]</label><label class="expand" for="c-41846503">[1 more]</label></div><br/><div class="children"><div class="content">You should probably disclose you&#x27;re the founder of lamini.<p>Do you have any publicly available validation data demonstrating 100% json compliance?</div><br/></div></div></div></div><div id="41845672" class="c"><input type="checkbox" id="c-41845672" checked=""/><div class="controls bullet"><span class="by">tpm</span><span>|</span><a href="#41840114">parent</a><span>|</span><a href="#41845410">prev</a><span>|</span><a href="#41841870">next</a><span>|</span><label class="collapse" for="c-41845672">[-]</label><label class="expand" for="c-41845672">[1 more]</label></div><br/><div class="children"><div class="content">In my experience the Qwen2-VL models are great at this.</div><br/></div></div><div id="41841870" class="c"><input type="checkbox" id="c-41841870" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#41840114">parent</a><span>|</span><a href="#41845672">prev</a><span>|</span><a href="#41840199">next</a><span>|</span><label class="collapse" for="c-41841870">[-]</label><label class="expand" for="c-41841870">[1 more]</label></div><br/><div class="children"><div class="content">In my tests, Llama 3.1 8b was way worse than Llama 2 13b or Solar 13b.</div><br/></div></div><div id="41840199" class="c"><input type="checkbox" id="c-41840199" checked=""/><div class="controls bullet"><span class="by">thatcat</span><span>|</span><a href="#41840114">parent</a><span>|</span><a href="#41841870">prev</a><span>|</span><a href="#41840758">next</a><span>|</span><label class="collapse" for="c-41840199">[-]</label><label class="expand" for="c-41840199">[4 more]</label></div><br/><div class="children"><div class="content">I mean, those aren&#x27;t comparable models. I wonder how the 405b version compares.</div><br/><div id="41840729" class="c"><input type="checkbox" id="c-41840729" checked=""/><div class="controls bullet"><span class="by">Tiberium</span><span>|</span><a href="#41840114">root</a><span>|</span><a href="#41840199">parent</a><span>|</span><a href="#41840758">next</a><span>|</span><label class="collapse" for="c-41840729">[-]</label><label class="expand" for="c-41840729">[3 more]</label></div><br/><div class="children"><div class="content">You raise a valid point, but 4o is way smaller than 405B. And 4o mini that&#x27;s described in the article is highly likely &lt;30B (if we&#x27;re talking dense models).</div><br/><div id="41841595" class="c"><input type="checkbox" id="c-41841595" checked=""/><div class="controls bullet"><span class="by">maleldil</span><span>|</span><a href="#41840114">root</a><span>|</span><a href="#41840729">parent</a><span>|</span><a href="#41840758">next</a><span>|</span><label class="collapse" for="c-41841595">[-]</label><label class="expand" for="c-41841595">[2 more]</label></div><br/><div class="children"><div class="content">Is the size of OpenAI&#x27;s models public, or is this guesswork?</div><br/><div id="41843023" class="c"><input type="checkbox" id="c-41843023" checked=""/><div class="controls bullet"><span class="by">qwe----3</span><span>|</span><a href="#41840114">root</a><span>|</span><a href="#41841595">parent</a><span>|</span><a href="#41840758">next</a><span>|</span><label class="collapse" for="c-41843023">[-]</label><label class="expand" for="c-41843023">[1 more]</label></div><br/><div class="children"><div class="content">If your company has a lot of ex openai employees then you know  ;)<p>And the public numbers are mostly right, the latest values are likely smaller now- they have been working on down sizing everything</div><br/></div></div></div></div></div></div></div></div><div id="41840758" class="c"><input type="checkbox" id="c-41840758" checked=""/><div class="controls bullet"><span class="by">A4ET8a8uTh0</span><span>|</span><a href="#41840114">parent</a><span>|</span><a href="#41840199">prev</a><span>|</span><a href="#41839199">next</a><span>|</span><label class="collapse" for="c-41840758">[-]</label><label class="expand" for="c-41840758">[1 more]</label></div><br/><div class="children"><div class="content">&lt;&lt; Stuff like this shows how much better the commercial models are than local models.<p>I did not reach the same conclusion so I would be curious if you could provide rationale&#x2F;basis for your assessment in the link. I am playing with humble llama3 8b here and results for federal register type stuff ( without going into details ) was good for what I was expecting to be.. not great.<p>edit: Since you mentioned llama explicitly, could you talk a little about the data&#x2F;source you are using for your resutls. You got me curious and I want to dig a little deeper.</div><br/></div></div></div></div><div id="41839199" class="c"><input type="checkbox" id="c-41839199" checked=""/><div class="controls bullet"><span class="by">Zaheer</span><span>|</span><a href="#41840114">prev</a><span>|</span><a href="#41839921">next</a><span>|</span><label class="collapse" for="c-41839199">[-]</label><label class="expand" for="c-41839199">[9 more]</label></div><br/><div class="children"><div class="content">Made a small project to help extract structure from documents (pdf,jpg,etc -&gt; JSON or CSV): <a href="https:&#x2F;&#x2F;datasqueeze.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;datasqueeze.ai&#x2F;</a><p>There&#x27;s 10 free pages to extract if anyone wants to give it a try. I&#x27;ve found that just sending a pdf to models doesn&#x27;t extract it properly especially with longer documents. Have tried to incorporate all best practices into this tool. It&#x27;s a pet project for now. Lmk if you find it helpful!</div><br/><div id="41839525" class="c"><input type="checkbox" id="c-41839525" checked=""/><div class="controls bullet"><span class="by">matchagaucho</span><span>|</span><a href="#41839199">parent</a><span>|</span><a href="#41839532">prev</a><span>|</span><a href="#41840184">next</a><span>|</span><label class="collapse" for="c-41839525">[-]</label><label class="expand" for="c-41839525">[5 more]</label></div><br/><div class="children"><div class="content">Similarly I&#x27;ve found old-school OCR is needed for more reliability.</div><br/><div id="41841968" class="c"><input type="checkbox" id="c-41841968" checked=""/><div class="controls bullet"><span class="by">MarkMarine</span><span>|</span><a href="#41839199">root</a><span>|</span><a href="#41839525">parent</a><span>|</span><a href="#41839832">next</a><span>|</span><label class="collapse" for="c-41841968">[-]</label><label class="expand" for="c-41841968">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using this to OCR some photos I took of books and it&#x27;s remarkable at it. My first pass was just a loop where I&#x27;d OCR, feed the text to the model and ask it to normalize into a schema but I found out just sending the image to the model and asking it to OCR and turn it into the shape of data I wanted was so much more accurate.</div><br/></div></div><div id="41839832" class="c"><input type="checkbox" id="c-41839832" checked=""/><div class="controls bullet"><span class="by">bagels</span><span>|</span><a href="#41839199">root</a><span>|</span><a href="#41839525">parent</a><span>|</span><a href="#41841968">prev</a><span>|</span><a href="#41840184">next</a><span>|</span><label class="collapse" for="c-41839832">[-]</label><label class="expand" for="c-41839832">[3 more]</label></div><br/><div class="children"><div class="content">Combining google&#x27;s ocr with llm gives OCR superpowers. Tell the llm the text is from an ocr and ask it to correct it.</div><br/><div id="41841797" class="c"><input type="checkbox" id="c-41841797" checked=""/><div class="controls bullet"><span class="by">saturn8601</span><span>|</span><a href="#41839199">root</a><span>|</span><a href="#41839832">parent</a><span>|</span><a href="#41840184">next</a><span>|</span><label class="collapse" for="c-41841797">[-]</label><label class="expand" for="c-41841797">[2 more]</label></div><br/><div class="children"><div class="content">That sounds like it could be very dangerous when the LLM gets it wrong...</div><br/><div id="41845212" class="c"><input type="checkbox" id="c-41845212" checked=""/><div class="controls bullet"><span class="by">bagels</span><span>|</span><a href="#41839199">root</a><span>|</span><a href="#41841797">parent</a><span>|</span><a href="#41840184">next</a><span>|</span><label class="collapse" for="c-41845212">[-]</label><label class="expand" for="c-41845212">[1 more]</label></div><br/><div class="children"><div class="content">Depends what you&#x27;re using it for. If you&#x27;re relying on OCR, you&#x27;ve already got to accept some amount of error.</div><br/></div></div></div></div></div></div></div></div><div id="41840184" class="c"><input type="checkbox" id="c-41840184" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#41839199">parent</a><span>|</span><a href="#41839525">prev</a><span>|</span><a href="#41839447">next</a><span>|</span><label class="collapse" for="c-41840184">[-]</label><label class="expand" for="c-41840184">[1 more]</label></div><br/><div class="children"><div class="content">Is this simply the OCR bits to feed to openai structured output?</div><br/></div></div><div id="41839447" class="c"><input type="checkbox" id="c-41839447" checked=""/><div class="controls bullet"><span class="by">artisandip7</span><span>|</span><a href="#41839199">parent</a><span>|</span><a href="#41840184">prev</a><span>|</span><a href="#41839921">next</a><span>|</span><label class="collapse" for="c-41839447">[-]</label><label class="expand" for="c-41839447">[1 more]</label></div><br/><div class="children"><div class="content">tried it works great, ty!</div><br/></div></div></div></div><div id="41839921" class="c"><input type="checkbox" id="c-41839921" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#41839199">prev</a><span>|</span><a href="#41842873">next</a><span>|</span><label class="collapse" for="c-41839921">[-]</label><label class="expand" for="c-41839921">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Note that this example simply passes a PNG screenshot of the PDF to OpenAI&#x27;s API — results may be different&#x2F;more efficient if you send it the actual PDF.<p>OpenAI&#x27;s API only accepts images: <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;vision" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;vision</a><p>To my knowledge, all the LLM services that take in PDF input do their own text extraction of the PDF before feeding it to an LLM.</div><br/><div id="41840278" class="c"><input type="checkbox" id="c-41840278" checked=""/><div class="controls bullet"><span class="by">tyre</span><span>|</span><a href="#41839921">parent</a><span>|</span><a href="#41840414">next</a><span>|</span><label class="collapse" for="c-41840278">[-]</label><label class="expand" for="c-41840278">[1 more]</label></div><br/><div class="children"><div class="content">or convert PDF to image and send that. We’ve done it for things that textract completely mangled, but sonnet has no problem. Especially tables built out of text characters from very old systems</div><br/></div></div><div id="41840414" class="c"><input type="checkbox" id="c-41840414" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#41839921">parent</a><span>|</span><a href="#41840278">prev</a><span>|</span><a href="#41842873">next</a><span>|</span><label class="collapse" for="c-41840414">[-]</label><label class="expand" for="c-41840414">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think it does OCR. It’s able to use the structure of the PDF to guide the parsing.</div><br/></div></div></div></div><div id="41842873" class="c"><input type="checkbox" id="c-41842873" checked=""/><div class="controls bullet"><span class="by">4ad</span><span>|</span><a href="#41839921">prev</a><span>|</span><a href="#41844863">next</a><span>|</span><label class="collapse" for="c-41842873">[-]</label><label class="expand" for="c-41842873">[2 more]</label></div><br/><div class="children"><div class="content">What a sad state for humanity that we have to resort to this sort of OCR&#x2F;scrapping instead of the original data being released in a machine readable format in the first place.</div><br/><div id="41845859" class="c"><input type="checkbox" id="c-41845859" checked=""/><div class="controls bullet"><span class="by">TrackerFF</span><span>|</span><a href="#41842873">parent</a><span>|</span><a href="#41844863">next</a><span>|</span><label class="collapse" for="c-41845859">[-]</label><label class="expand" for="c-41845859">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, there are some considerations here:<p>1) There&#x27;s plenty of old data out there. Newspaper scans from the days before computers, or digitalization of the newspaper process. Or the original files simply got lost, so manually scanned pages is all you have.<p>2) There could be policies about making the data public, but in a way that discourages data scraping.<p>3) The providers of the data simply don&#x27;t have the resources or incentives to develop a working API.<p>And many more.</div><br/></div></div></div></div></div></div></div></div></div></body></html>