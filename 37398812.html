<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1693990869238" as="style"/><link rel="stylesheet" href="styles.css?v=1693990869238"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://perplexity.vercel.app/">Show HN: Fully client-side GPT2 prediction visualizer</a> <span class="domain">(<a href="https://perplexity.vercel.app">perplexity.vercel.app</a>)</span></div><div class="subtext"><span>thesephist</span> | <span>11 comments</span></div><br/><div><div id="37401157" class="c"><input type="checkbox" id="c-37401157" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#37402682">prev</a><span>|</span><a href="#37400608">next</a><span>|</span><label class="collapse" for="c-37401157">[-]</label><label class="expand" for="c-37401157">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a video of a previous version of this tool here which I found really helped me understand what it was demonstrating: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;thesephist&#x2F;status&#x2F;1617747154231259137" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;thesephist&#x2F;status&#x2F;1617747154231259137</a><p>It&#x27;s really neat to see how this sentence:<p>&gt; The first time I write this sentence, the model is quite confused about what token is about to come next, especially if I throw in weird words like pumpkin, clown, tweets, alpha, teddy bear.<p>Shows that the words pumpkin, clown etc are considered really unlikely. But when the sentence is repeated a moment later, all of the words become extremely predictable to the model.<p>Also worth noting: this demo runs entirely in the browser! It loads a 120MB ONNX version of GPT-2 using Transformers.js.</div><br/></div></div><div id="37400608" class="c"><input type="checkbox" id="c-37400608" checked=""/><div class="controls bullet"><span class="by">didgeoridoo</span><span>|</span><a href="#37401157">prev</a><span>|</span><a href="#37402196">next</a><span>|</span><label class="collapse" for="c-37400608">[-]</label><label class="expand" for="c-37400608">[2 more]</label></div><br/><div class="children"><div class="content">Really interesting! I wonder how well this syncs up with human intuition and general “information density”. If it’s a close match, maybe you could use this as a tool to help with skimming documents — the red (“hard to predict”) areas might be a good hint to slow down and read more carefully, while the green (“easy to predict”) areas might mean you could skim without losing too much unpredictable information.</div><br/><div id="37401390" class="c"><input type="checkbox" id="c-37401390" checked=""/><div class="controls bullet"><span class="by">thesephist</span><span>|</span><a href="#37400608">parent</a><span>|</span><a href="#37402196">next</a><span>|</span><label class="collapse" for="c-37401390">[-]</label><label class="expand" for="c-37401390">[1 more]</label></div><br/><div class="children"><div class="content">This is definitely an interesting idea I&#x27;ve also pondered before. In my experience (just speaking from intuition) what&#x27;s &quot;easy&quot; for LMs to predict often doesn&#x27;t line up with our human expectations for what&#x27;s &quot;obvious&quot;. Often LLMs will learn seemingly &quot;low information content&quot; statistical correlations that just helps it lower its training loss.</div><br/></div></div></div></div><div id="37401383" class="c"><input type="checkbox" id="c-37401383" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#37402196">prev</a><span>|</span><a href="#37398858">next</a><span>|</span><label class="collapse" for="c-37401383">[-]</label><label class="expand" for="c-37401383">[2 more]</label></div><br/><div class="children"><div class="content">Any chance of Llama2 support?</div><br/><div id="37401398" class="c"><input type="checkbox" id="c-37401398" checked=""/><div class="controls bullet"><span class="by">thesephist</span><span>|</span><a href="#37401383">parent</a><span>|</span><a href="#37398858">next</a><span>|</span><label class="collapse" for="c-37401398">[-]</label><label class="expand" for="c-37401398">[1 more]</label></div><br/><div class="children"><div class="content">Definitely possible if supported by transformers.js. If I see enough folks wanting it I&#x27;ll likely add it at some point.</div><br/></div></div></div></div><div id="37401721" class="c"><input type="checkbox" id="c-37401721" checked=""/><div class="controls bullet"><span class="by">atgctg</span><span>|</span><a href="#37398858">prev</a><span>|</span><label class="collapse" for="c-37401721">[-]</label><label class="expand" for="c-37401721">[2 more]</label></div><br/><div class="children"><div class="content">It would be interesting to have attention visualized as well, similar to how it&#x27;s done in BertViz:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;jessevig&#x2F;bertviz">https:&#x2F;&#x2F;github.com&#x2F;jessevig&#x2F;bertviz</a></div><br/></div></div></div></div></div></div></div></body></html>