<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1717578075940" as="style"/><link rel="stylesheet" href="styles.css?v=1717578075940"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.opencv.ai/blog/camera-calibration">Why camera calibration is so important in computer vision</a> <span class="domain">(<a href="https://www.opencv.ai">www.opencv.ai</a>)</span></div><div class="subtext"><span>zubkovnn</span> | <span>31 comments</span></div><br/><div><div id="40581974" class="c"><input type="checkbox" id="c-40581974" checked=""/><div class="controls bullet"><span class="by">dima55</span><span>|</span><a href="#40582804">next</a><span>|</span><label class="collapse" for="c-40581974">[-]</label><label class="expand" for="c-40581974">[4 more]</label></div><br/><div class="children"><div class="content">Calibrating cameras is still important; it&#x27;s only &quot;mostly irrelevant in 2024&quot; if you don&#x27;t care about accuracy. Incidentally, tools like opencv and their ilk are also what you use if you don&#x27;t care about accuracy. Modern tools like mrcal (<a href="https:&#x2F;&#x2F;mrcal.secretsauce.net&#x2F;tour.html" rel="nofollow">https:&#x2F;&#x2F;mrcal.secretsauce.net&#x2F;tour.html</a>) are essential if you&#x27;re trying to do long-range stereo or use wide lenses or have calibration instability or any number of other ever-present issues.</div><br/><div id="40582165" class="c"><input type="checkbox" id="c-40582165" checked=""/><div class="controls bullet"><span class="by">seventytwo</span><span>|</span><a href="#40581974">parent</a><span>|</span><a href="#40581985">prev</a><span>|</span><a href="#40582804">next</a><span>|</span><label class="collapse" for="c-40582165">[-]</label><label class="expand" for="c-40582165">[2 more]</label></div><br/><div class="children"><div class="content">What makes this better than OpenCV?</div><br/><div id="40582221" class="c"><input type="checkbox" id="c-40582221" checked=""/><div class="controls bullet"><span class="by">dima55</span><span>|</span><a href="#40581974">root</a><span>|</span><a href="#40582165">parent</a><span>|</span><a href="#40582804">next</a><span>|</span><label class="collapse" for="c-40582221">[-]</label><label class="expand" for="c-40582221">[1 more]</label></div><br/><div class="children"><div class="content">Uncertainty propagation. Richer models that fit better. Lots of feedback and metrics and visualization to evaluate the quality of the solve. Flexibility of the tool. Documentation.</div><br/></div></div></div></div></div></div><div id="40582804" class="c"><input type="checkbox" id="c-40582804" checked=""/><div class="controls bullet"><span class="by">whiplash451</span><span>|</span><a href="#40581974">prev</a><span>|</span><a href="#40582703">next</a><span>|</span><label class="collapse" for="c-40582804">[-]</label><label class="expand" for="c-40582804">[1 more]</label></div><br/><div class="children"><div class="content">You can also do some navigation guidance without calibration. Old work but still relevant today (ICCV’09)<p><a href="https:&#x2F;&#x2F;www.semanticscholar.org&#x2F;paper&#x2F;Body-relative-navigation-guidance-using-cameras-Koch-Teller&#x2F;0ecccbe6d92295513c440ae0e1ae24230cd09a40" rel="nofollow">https:&#x2F;&#x2F;www.semanticscholar.org&#x2F;paper&#x2F;Body-relative-navigati...</a></div><br/></div></div><div id="40582703" class="c"><input type="checkbox" id="c-40582703" checked=""/><div class="controls bullet"><span class="by">horsellama</span><span>|</span><a href="#40582804">prev</a><span>|</span><a href="#40582151">next</a><span>|</span><label class="collapse" for="c-40582703">[-]</label><label class="expand" for="c-40582703">[1 more]</label></div><br/><div class="children"><div class="content">Is there a good resource I can read about the last point in the article?<p>&gt; We don&#x27;t know where the camera will be. For example, we ask players to use the cameras of their mobile devices to explore the size and proportions of a room — to work with the augmented reality helmet. We don&#x27;t know anything about the rooms, and we can&#x27;t ask users to use a pattern.<p>Are there automatic techniques to tackle this problem?</div><br/></div></div><div id="40582151" class="c"><input type="checkbox" id="c-40582151" checked=""/><div class="controls bullet"><span class="by">moandcompany</span><span>|</span><a href="#40582703">prev</a><span>|</span><a href="#40581719">next</a><span>|</span><label class="collapse" for="c-40582151">[-]</label><label class="expand" for="c-40582151">[1 more]</label></div><br/><div class="children"><div class="content">Concur with the article&#x27;s premise.<p>I&#x27;ve been working in AI&#x2F;ML and some CV projects over the last decade.<p>I&#x27;ve seen far too many cases where the &quot;algorithms&quot; and modeling teams had no concern or even a concept for how the input systems for data that would be used to train models, and later inference, mattered to the quality of outcomes.<p>In CV and computational photography cases, there was little concern or understanding for how photography and imaging actually works, nor consideration for how variances in hardware, configuration, or the capture pipeline in general might affect those models when doing training data capture in parallel. Adding another layer, consideration for how variances between the capture pipeline and actual inference pipelines might need to be accounted for when designing the overall system and how to approach data collection and curation for training + evaluation, as well as to build-in robustness. (similar thoughts apply to concepts like bias and fairness in models)<p>Example:  training vision models using one set of imaging hardware and configurations while applying those models on very different imaging hardware with different characteristics.<p>To summarize the above, not caring about calibration in CV is like not caring about how variances in feature extraction&#x2F;embedding generation will affect the overall quality of your results.</div><br/></div></div><div id="40581719" class="c"><input type="checkbox" id="c-40581719" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#40582151">prev</a><span>|</span><a href="#40581944">next</a><span>|</span><label class="collapse" for="c-40581719">[-]</label><label class="expand" for="c-40581719">[5 more]</label></div><br/><div class="children"><div class="content">This seems like a temporary problem as AI models improve. If we can figure out how to extract meaning from distorted images, AI will be able to as well.<p>OCR for distorted text is a solved problem. Relatively simple math based on normalizing text size and shape can be used to calculate how to reverse distortion.</div><br/><div id="40581735" class="c"><input type="checkbox" id="c-40581735" checked=""/><div class="controls bullet"><span class="by">ryandamm</span><span>|</span><a href="#40581719">parent</a><span>|</span><a href="#40581944">next</a><span>|</span><label class="collapse" for="c-40581735">[-]</label><label class="expand" for="c-40581735">[4 more]</label></div><br/><div class="children"><div class="content">If calibration is a temporary problem, it&#x27;s because the people building those AI models understand imaging really deeply, and engage with some of the ideas in the linked post. To put it another way: if you want to build AI models that understand the world in 3D via 2D images, you should probably understand projective geometry, camera calibration, etc, extremely well.</div><br/><div id="40581881" class="c"><input type="checkbox" id="c-40581881" checked=""/><div class="controls bullet"><span class="by">lelag</span><span>|</span><a href="#40581719">root</a><span>|</span><a href="#40581735">parent</a><span>|</span><a href="#40581944">next</a><span>|</span><label class="collapse" for="c-40581881">[-]</label><label class="expand" for="c-40581881">[3 more]</label></div><br/><div class="children"><div class="content">Or does it ? Drawing on a parallel from my experience in NLP during the early 2000s, the field was largely dominated by linguists trying to grok language and structuring rules manually. However, the most significant advancements came when we shifted towards using massive datasets to train models, without requiring explicit, deep linguistic knowledge.<p>Similarly, maybe the next frontier in 3D vision is employing large datasets to train a black-box AI without us having to understand and get the math right.</div><br/><div id="40582067" class="c"><input type="checkbox" id="c-40582067" checked=""/><div class="controls bullet"><span class="by">ryandamm</span><span>|</span><a href="#40581719">root</a><span>|</span><a href="#40581881">parent</a><span>|</span><a href="#40581944">next</a><span>|</span><label class="collapse" for="c-40582067">[-]</label><label class="expand" for="c-40582067">[2 more]</label></div><br/><div class="children"><div class="content">I think we have objective information about how protective geometry works.<p>Ergo, I would not bet against teams that understand that objective technical field; AI is not magic, and knowing the fundamentals will always help. Better models, smaller models, faster models; the less you leave for the model to solve in latent space the better you’ll do, I think.</div><br/><div id="40582816" class="c"><input type="checkbox" id="c-40582816" checked=""/><div class="controls bullet"><span class="by">whiplash451</span><span>|</span><a href="#40581719">root</a><span>|</span><a href="#40582067">parent</a><span>|</span><a href="#40581944">next</a><span>|</span><label class="collapse" for="c-40582816">[-]</label><label class="expand" for="c-40582816">[1 more]</label></div><br/><div class="children"><div class="content">If that was true, transformers would have been useless for NLP and deep learning for Go.<p>You can argue the exact opposite: the more you leave to the model to learn by itself, the more likely you are to find a solution that was not accessible to humans and their limited feature engineering capability.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40581940" class="c"><input type="checkbox" id="c-40581940" checked=""/><div class="controls bullet"><span class="by">alexcnwy</span><span>|</span><a href="#40581944">prev</a><span>|</span><a href="#40582462">next</a><span>|</span><label class="collapse" for="c-40581940">[-]</label><label class="expand" for="c-40581940">[1 more]</label></div><br/><div class="children"><div class="content">i&#x27;ve done loads of computer vision projects and they live or die by 2 things:<p>1. camera calibration
2. lighting<p>Lighting is the easy part</div><br/></div></div><div id="40582462" class="c"><input type="checkbox" id="c-40582462" checked=""/><div class="controls bullet"><span class="by">anArbitraryOne</span><span>|</span><a href="#40581940">prev</a><span>|</span><a href="#40560262">next</a><span>|</span><label class="collapse" for="c-40582462">[-]</label><label class="expand" for="c-40582462">[1 more]</label></div><br/><div class="children"><div class="content">Yeah wow who would have thought that calibrating a sensor is important for the algorithms that process it&#x27;s output</div><br/></div></div><div id="40560262" class="c"><input type="checkbox" id="c-40560262" checked=""/><div class="controls bullet"><span class="by">zubkovnn</span><span>|</span><a href="#40582462">prev</a><span>|</span><a href="#40581970">next</a><span>|</span><label class="collapse" for="c-40560262">[-]</label><label class="expand" for="c-40560262">[1 more]</label></div><br/><div class="children"><div class="content">he cameras are used to capture the world around us. But frames are just representations of the world, and the actual relationship between the flat image and the real thing is not always obvious. In order to extract meaningful insights about the real objects from the images, developers and users of computer vision systems need to understand camera characteristics — with the help of the calibration solutions</div><br/></div></div><div id="40581970" class="c"><input type="checkbox" id="c-40581970" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40560262">prev</a><span>|</span><a href="#40581678">next</a><span>|</span><label class="collapse" for="c-40581970">[-]</label><label class="expand" for="c-40581970">[1 more]</label></div><br/><div class="children"><div class="content">I once messed up and input the RGB values in the wrong order. This lost me 2mIoU in evaluation. If anything it was a pain in the ass to figure out, because the error was so subtle that I thought maybe I was imagining it. So I don&#x27;t believe that camera calibration is important anymore. What you need is to have a dataset taken by a variety of cameras.</div><br/></div></div><div id="40581678" class="c"><input type="checkbox" id="c-40581678" checked=""/><div class="controls bullet"><span class="by">ryandamm</span><span>|</span><a href="#40581970">prev</a><span>|</span><a href="#40581463">next</a><span>|</span><label class="collapse" for="c-40581678">[-]</label><label class="expand" for="c-40581678">[4 more]</label></div><br/><div class="children"><div class="content">As a camera nerd, I have to weigh in with some minor annoyances: there are errors in this article that reflect common misconceptions or elisions that computer vision engineers often believe in. Let me debunk a few...<p>&quot;Cameras, by design, also introduce a level of distortion&quot;: distortion is an artifact that is usually minimized in lens design if you are trying to create a rectilinear lens, though you are typically forced to trade it off versus other design criteria (like resolving power, or optical complexity). If you&#x27;re trying to design a rectilinear lens, you explicitly try to remove any distortion; for &quot;normal&quot; focal lengths (focal length ~= image circle diagonal) you can often get vanishingly low distortion, even with relatively simple lenses. (There was a 19th century lens design that achieved &quot;zero&quot; distortion with just a few elements, and a precisely-chosen aperture position; the name escapes me at the moment.) If you are not designing a rectilinear lens, there are other lens mappings, in which case it&#x27;s not really proper to describe the effect as distortion, though in technical literature it&#x27;s often still described this way. (For further reading, check out F-theta vs. F-tan-theta lenses: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fisheye_lens#Focal_length;" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fisheye_lens#Focal_length;</a> <a href="https:&#x2F;&#x2F;www.thorlabs.com&#x2F;newgrouppage9.cfm?objectgroup_id=10766" rel="nofollow">https:&#x2F;&#x2F;www.thorlabs.com&#x2F;newgrouppage9.cfm?objectgroup_id=10...</a>.)<p>&quot;... the proportions of objects closer to the edges of the frame are distorted.&quot; This is technically incorrect; objects subtend the same number of pixels in an f*theta fisheye image whether they&#x27;re at the center of the frame or at the edge, for a given camera-subject distance. The apparent distortion exists because you&#x27;re viewing it further away than the focal length implies, so the viewer at a &quot;normal&quot; viewing distance is applying a transform to the image that is not distance-preserving. If you get <i>extremely</i> close to the image, and if it were displayed on a curved surface (so the image&#x27;s angular extent to the viewer matched the camera&#x27;s FOV), you&#x27;d see no distortion. Also, we think of arranging subject matter for, e.g., a portrait, along a plane that&#x27;s orthogonal to the camera&#x27;s axis -- but then people closer to the edge are farther from the camera, and would naturally appear smaller. It&#x27;s a weird artifact of rectilinear lenses that they&#x27;re actually magnified in the resulting image so people standing in a line are all equal size; to get the same effect with a fisheye lens, you just have people arranged in an arc centered on the camera. (Most portraits are not shot on fisheye lenses though, for very good reasons, so this is more of an edge case.)<p>Related: &quot;there are no straight lines&quot; -- if you&#x27;re using a camera to make an image, you&#x27;re operating in projective space. Whether a line is straight in 3D space, and is then straight in the projective space, is a function of your projection. Rectilinear lenses -- for which the relation f*tan(theta) applies -- straight lines remain straight. But for most (all?) other mappings they do not; for example, f*theta lenses preserve angular extent: an object that subtends a particular angle from the camera&#x27;s perspective will occupy the same number of pixels regardless of where they appear in the image. This is arguably superior for many computer vision tasks.<p>... I need to stop here, because this is already getting tedious. For anyone interested in developing a deep intuition for lenses and imaging, I would recommend playing around with a view camera, understanding the Scheimpflug relationship (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Scheimpflug_principle" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Scheimpflug_principle</a>), and really thinking through imaging from first principles if you want to understand these things more deeply.<p>Or just, you know, use Colmap and don&#x27;t look back. That works too.</div><br/><div id="40581822" class="c"><input type="checkbox" id="c-40581822" checked=""/><div class="controls bullet"><span class="by">dcanelhas</span><span>|</span><a href="#40581678">parent</a><span>|</span><a href="#40581463">next</a><span>|</span><label class="collapse" for="c-40581822">[-]</label><label class="expand" for="c-40581822">[3 more]</label></div><br/><div class="children"><div class="content">Mathematically, the camera most often used in computer vision is a pinhole camera. When we talk about &quot;distortions&quot; I think it&#x27;s usually with regards to how the real device systematically deviates from that model.<p>Calibration in this context is essentially the task of finding the optimal parameters of some (usually nonlinear) function (u,v)=f(x,y) that remaps positions in the original image frame to a rectified frame, where all straight lines in the world appear straight in the image. Technically, a skewed and squashed image would also fulfill those requirements, too. But this is a customer-oriented blog post, to give someone enough of an understanding to convince them of the importance of calibration, it&#x27;s not a rigorous technical paper, so I actually think it&#x27;s fine to skip&#x2F;simplify some details.</div><br/><div id="40581848" class="c"><input type="checkbox" id="c-40581848" checked=""/><div class="controls bullet"><span class="by">ryandamm</span><span>|</span><a href="#40581678">root</a><span>|</span><a href="#40581822">parent</a><span>|</span><a href="#40581866">next</a><span>|</span><label class="collapse" for="c-40581848">[-]</label><label class="expand" for="c-40581848">[1 more]</label></div><br/><div class="children"><div class="content">You are, of course, absolutely right. I nonetheless think it may have value for some people to understand where the deviations from that simplifying assumption lie, and at least understand the stakes.<p>Now, I may be biased, because I work in imaging-for-humans, and I&#x27;ve had many a conversations with engineers about why a particular simplification doesn&#x27;t work for, e.g., filmmakers, but I think that even for purely technical disciplines, understanding the assumptions that go into the pinhole model can be useful. At the margins. Which sometimes matter.</div><br/></div></div><div id="40581866" class="c"><input type="checkbox" id="c-40581866" checked=""/><div class="controls bullet"><span class="by">ryandamm</span><span>|</span><a href="#40581678">root</a><span>|</span><a href="#40581822">parent</a><span>|</span><a href="#40581848">prev</a><span>|</span><a href="#40581463">next</a><span>|</span><label class="collapse" for="c-40581866">[-]</label><label class="expand" for="c-40581866">[1 more]</label></div><br/><div class="children"><div class="content">A specific example of where it&#x27;s useful to know the underlying mechanics: for very wide angle lenses, you will typically get brightness falloff at the edges due to a phenomenon called the cos^4 phenomenon (<a href="https:&#x2F;&#x2F;nvlpubs.nist.gov&#x2F;nistpubs&#x2F;jres&#x2F;39&#x2F;jresv39n3p213_A1b.pdf" rel="nofollow">https:&#x2F;&#x2F;nvlpubs.nist.gov&#x2F;nistpubs&#x2F;jres&#x2F;39&#x2F;jresv39n3p213_A1b....</a>).<p>This is often elided by camera systems, that apply a gain to peripheral pixels to correct for this phenomenon. If you understand imaging, you will expect that, and understand why, for example, your wide angle lens displays a lower signal-to-noise ratio for a given illumination value than you might otherwise expect at the edges of the image.<p>This is a really specific example, but there are dozens. Imaging is its own deep, technical field that is abstracted, and occasionally obscured, by the pinhole model.</div><br/></div></div></div></div></div></div><div id="40581463" class="c"><input type="checkbox" id="c-40581463" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#40581678">prev</a><span>|</span><a href="#40582692">next</a><span>|</span><label class="collapse" for="c-40581463">[-]</label><label class="expand" for="c-40581463">[8 more]</label></div><br/><div class="children"><div class="content">Hot take: Offline camera calibration is largely unneeded anymore. As long as you have an approximate starting point, collect about 5 minutes of data from all sensors and you have everything you need to reverse solve for the calibration.<p>Not saying it&#x27;s easy, but solving it in software is O(1) and solving it by calibrating each device is O(N), and allows you to be resilient to things like temperature deformations and other nasty things that can happen in the field.</div><br/><div id="40581608" class="c"><input type="checkbox" id="c-40581608" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#40581463">parent</a><span>|</span><a href="#40581587">next</a><span>|</span><label class="collapse" for="c-40581608">[-]</label><label class="expand" for="c-40581608">[5 more]</label></div><br/><div class="children"><div class="content">Nobody is going to sit around and wait for your app or website to collect 5 minutes of data for calibration.<p>The browser doesn&#x27;t let you collect all this sensor data.<p>Mobile Safari doesn&#x27;t tell you the known camera calibration.<p>The Apple Vision Pro doesn&#x27;t give you any data at all.<p>Nonetheless, 8thwall&#x27;s XR8 takes seconds to calibrate absolute scale. It has to do device lookup for approximate calibrations for your iPhone model.</div><br/><div id="40581642" class="c"><input type="checkbox" id="c-40581642" checked=""/><div class="controls bullet"><span class="by">jfim</span><span>|</span><a href="#40581463">root</a><span>|</span><a href="#40581608">parent</a><span>|</span><a href="#40582606">next</a><span>|</span><label class="collapse" for="c-40581642">[-]</label><label class="expand" for="c-40581642">[3 more]</label></div><br/><div class="children"><div class="content">Depends on the use case. A car that requires five minutes of driving to calibrate its cameras once after exiting the factory would be completely acceptable. A drone that has a pre flight calibration when one first takes it out of the box would be okay.</div><br/><div id="40582617" class="c"><input type="checkbox" id="c-40582617" checked=""/><div class="controls bullet"><span class="by">tjoff</span><span>|</span><a href="#40581463">root</a><span>|</span><a href="#40581642">parent</a><span>|</span><a href="#40582606">next</a><span>|</span><label class="collapse" for="c-40582617">[-]</label><label class="expand" for="c-40582617">[2 more]</label></div><br/><div class="children"><div class="content">Without calibration do you know you are in spec?<p>If I bought a car and 5 min into driving it tells me that the camera system is defective you are not gonna have a happy customer.</div><br/><div id="40582835" class="c"><input type="checkbox" id="c-40582835" checked=""/><div class="controls bullet"><span class="by">watt</span><span>|</span><a href="#40581463">root</a><span>|</span><a href="#40582617">parent</a><span>|</span><a href="#40582606">next</a><span>|</span><label class="collapse" for="c-40582835">[-]</label><label class="expand" for="c-40582835">[1 more]</label></div><br/><div class="children"><div class="content">How do you jump from OP &quot;after exiting the factory&quot; to your &quot;bought a car and 5 min into driving&quot; ? Those are two rather different things. Are you just looking for a pointless argument?</div><br/></div></div></div></div></div></div><div id="40582606" class="c"><input type="checkbox" id="c-40582606" checked=""/><div class="controls bullet"><span class="by">tomp</span><span>|</span><a href="#40581463">root</a><span>|</span><a href="#40581608">parent</a><span>|</span><a href="#40581642">prev</a><span>|</span><a href="#40581587">next</a><span>|</span><label class="collapse" for="c-40582606">[-]</label><label class="expand" for="c-40582606">[1 more]</label></div><br/><div class="children"><div class="content">Do you know what does 8thwall use for SLAM? is it OrbSLAM or something else?</div><br/></div></div></div></div><div id="40581587" class="c"><input type="checkbox" id="c-40581587" checked=""/><div class="controls bullet"><span class="by">ryandamm</span><span>|</span><a href="#40581463">parent</a><span>|</span><a href="#40581608">prev</a><span>|</span><a href="#40581501">next</a><span>|</span><label class="collapse" for="c-40581587">[-]</label><label class="expand" for="c-40581587">[1 more]</label></div><br/><div class="children"><div class="content">I agree, and I don&#x27;t think this is a particularly hot take; if you&#x27;ve been working with camera systems, you&#x27;ve seen calibration without explicit calibration routines get better and better over the past few years.<p>For critical imaging, you still want to calibrate, but the gap between explicit calibration and &quot;implicit calibration&quot; (just using your subject matter) is definitely getting smaller, and quickly.</div><br/></div></div><div id="40581501" class="c"><input type="checkbox" id="c-40581501" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#40581463">parent</a><span>|</span><a href="#40581587">prev</a><span>|</span><a href="#40582692">next</a><span>|</span><label class="collapse" for="c-40581501">[-]</label><label class="expand" for="c-40581501">[1 more]</label></div><br/><div class="children"><div class="content">There have been papers published about making robots and autopilot-like systems robust to all sorts of &quot;system degradation&quot; such as obstructed optics, missing signals, noisy signals, etc...<p>I can&#x27;t find the reference any more, but one particularly impressive result was a four-wheeled robot that could tolerate the loss of a wheel, distortions of the entire chassis, <i>and</i> multiple faulty sensors all at the same time!</div><br/></div></div></div></div></div></div></div></div></div></body></html>