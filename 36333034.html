<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1686819663727" as="style"/><link rel="stylesheet" href="styles.css?v=1686819663727"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://software.rajivprab.com/2018/04/29/myths-programmers-believe-about-cpu-caches/">Myths Programmers Believe about CPU Caches</a> <span class="domain">(<a href="https://software.rajivprab.com">software.rajivprab.com</a>)</span></div><div class="subtext"><span>whack</span> | <span>59 comments</span></div><br/><div><div id="36334945" class="c"><input type="checkbox" id="c-36334945" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#36337849">next</a><span>|</span><label class="collapse" for="c-36334945">[-]</label><label class="expand" for="c-36334945">[16 more]</label></div><br/><div class="children"><div class="content">This article gives the impression that everything is the compiler&#x27;s fault when you end up with conflicting reads in different cores, but that&#x27;s not right.<p>From the point of view of someone outside the CPU, yes you can say that simultaneous reads might never give different answers.  But everything happening at that level barely resembles the original software.  Dozens of instructions are happening at any moment, overlapping each other, starting and finishing in very different orders from how they&#x27;re stored in memory.  This happens even if you directly write machine code byte by byte.<p>From the point of view of the software, running a bunch of instructions in sequence, you do get stale values.  You can have two threads wait for a signal, then both read a value, and both get different results.  You can have a thread set a flag after it&#x27;s done editing some values, have another thread wait for the flag, and then after waiting it sees the edits as still incomplete.<p>The exact types of nonsense depend on the memory model, but things as simple as two MOV instructions in a row can violate strict ordering.  It&#x27;s not just that the compiler might stash something in a register, but that the CPU itself will make swaths of very observable changes within the rules set by the memory model.<p>You can&#x27;t trust the hardware coherency protocol to do much for you until you follow the platform-specific rules to <i>tell the CPU to make something act coherent</i>.</div><br/><div id="36335540" class="c"><input type="checkbox" id="c-36335540" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#36334945">parent</a><span>|</span><a href="#36335367">next</a><span>|</span><label class="collapse" for="c-36335540">[-]</label><label class="expand" for="c-36335540">[4 more]</label></div><br/><div class="children"><div class="content">Which is why the Java Memory Model exists, and why a number of other languages just copied it.<p>A lot of problems with threading got sorted out under the auspices of making Java work right on your hardware&#x2F;operating system.</div><br/><div id="36335809" class="c"><input type="checkbox" id="c-36335809" checked=""/><div class="controls bullet"><span class="by">jcranmer</span><span>|</span><a href="#36334945">root</a><span>|</span><a href="#36335540">parent</a><span>|</span><a href="#36335367">next</a><span>|</span><label class="collapse" for="c-36335809">[-]</label><label class="expand" for="c-36335809">[3 more]</label></div><br/><div class="children"><div class="content">Strictly speaking, the Java Memory Model derives from the data-race-free model of the early &#x27;90s. Java was the first programming language to explicitly incorporate it as part of the specification, but the main derivation actually comes from the C++ memory model, which built into it the basic atomic memory model that most derivatives rely on--Java doesn&#x27;t have the weaker atomics support that C++ added, just sequentially-consistent. Java also introduced some frankly confusing (and incorrect, per my understanding) semantics for what happens in the face of data races that everybody else just shrugged and said &quot;let&#x27;s make it be UB and call it a day.&quot;</div><br/><div id="36335930" class="c"><input type="checkbox" id="c-36335930" checked=""/><div class="controls bullet"><span class="by">hashmash</span><span>|</span><a href="#36334945">root</a><span>|</span><a href="#36335809">parent</a><span>|</span><a href="#36335367">next</a><span>|</span><label class="collapse" for="c-36335930">[-]</label><label class="expand" for="c-36335930">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Java doesn&#x27;t have the weaker atomics support that C++ added<p>Yes it does, but it doesn&#x27;t have first-class keywords to represent these modes. You have to use the VarHandle class, which is a bit kludgy.<p><a href="https:&#x2F;&#x2F;docs.oracle.com&#x2F;en&#x2F;java&#x2F;javase&#x2F;20&#x2F;docs&#x2F;api&#x2F;java.base&#x2F;java&#x2F;lang&#x2F;invoke&#x2F;VarHandle.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.oracle.com&#x2F;en&#x2F;java&#x2F;javase&#x2F;20&#x2F;docs&#x2F;api&#x2F;java.base...</a></div><br/><div id="36335964" class="c"><input type="checkbox" id="c-36335964" checked=""/><div class="controls bullet"><span class="by">jcranmer</span><span>|</span><a href="#36334945">root</a><span>|</span><a href="#36335930">parent</a><span>|</span><a href="#36335367">next</a><span>|</span><label class="collapse" for="c-36335964">[-]</label><label class="expand" for="c-36335964">[1 more]</label></div><br/><div class="children"><div class="content">Okay, it looks like those were added in Java 9, which was after I stopped following Java (and well after the C++ memory model was largely settled, in 2007).</div><br/></div></div></div></div></div></div></div></div><div id="36335367" class="c"><input type="checkbox" id="c-36335367" checked=""/><div class="controls bullet"><span class="by">zerohp</span><span>|</span><a href="#36334945">parent</a><span>|</span><a href="#36335540">prev</a><span>|</span><a href="#36336174">next</a><span>|</span><label class="collapse" for="c-36335367">[-]</label><label class="expand" for="c-36335367">[3 more]</label></div><br/><div class="children"><div class="content">The author has a good grasp of coherence but either ignores or fails to understand memory ordering.</div><br/><div id="36336459" class="c"><input type="checkbox" id="c-36336459" checked=""/><div class="controls bullet"><span class="by">rossjudson</span><span>|</span><a href="#36334945">root</a><span>|</span><a href="#36335367">parent</a><span>|</span><a href="#36336125">next</a><span>|</span><label class="collapse" for="c-36336459">[-]</label><label class="expand" for="c-36336459">[1 more]</label></div><br/><div class="children"><div class="content">Uh, that&#x27;s uncalled for. I think your hint is right there in the title: Myths Programmers Believe about CPU Caches. Was there an inaccuracy in the article with respect to CPU Caches?</div><br/></div></div><div id="36336125" class="c"><input type="checkbox" id="c-36336125" checked=""/><div class="controls bullet"><span class="by">tylerhou</span><span>|</span><a href="#36334945">root</a><span>|</span><a href="#36335367">parent</a><span>|</span><a href="#36336459">prev</a><span>|</span><a href="#36336174">next</a><span>|</span><label class="collapse" for="c-36336125">[-]</label><label class="expand" for="c-36336125">[1 more]</label></div><br/><div class="children"><div class="content">Where did you get this impression? That’s not how I understood the article; I thought it was good.</div><br/></div></div></div></div><div id="36336174" class="c"><input type="checkbox" id="c-36336174" checked=""/><div class="controls bullet"><span class="by">throwawaylinux</span><span>|</span><a href="#36334945">parent</a><span>|</span><a href="#36335367">prev</a><span>|</span><a href="#36337849">next</a><span>|</span><label class="collapse" for="c-36336174">[-]</label><label class="expand" for="c-36336174">[8 more]</label></div><br/><div class="children"><div class="content">&gt; You can&#x27;t trust the hardware coherency protocol to do much for you until you follow the platform-specific rules to tell the CPU to make something act coherent.<p>If software has to perform some operation to maintain coherency then you hardware is not cache coherent. Which some aren&#x27;t, and especially some sub-sets of operations aren&#x27;t (instruction cache vs data ops, or CPU vs DMA). But for load&#x2F;store operations by CPUs, they&#x27;re all coherent.<p>Barrier&#x2F;fence instructions are about enforcing ordering on when memory operations to more than one location can be performed or become visible, with respect to one another.</div><br/><div id="36336305" class="c"><input type="checkbox" id="c-36336305" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#36334945">root</a><span>|</span><a href="#36336174">parent</a><span>|</span><a href="#36337849">next</a><span>|</span><label class="collapse" for="c-36336305">[-]</label><label class="expand" for="c-36336305">[7 more]</label></div><br/><div class="children"><div class="content">It&#x27;s coherent behind the scenes but it often presents an incoherent view to the software.  It&#x27;s not <i>acting</i> coherent when the rearrangement of memory operations makes you see different orderings from different cores.<p>When a CPU has a loose memory model and is aggressively making use of the reordering capabilities, the cache being coherent internally is basically just an implementation detail.  It&#x27;s not part of the visible ABI.</div><br/><div id="36336488" class="c"><input type="checkbox" id="c-36336488" checked=""/><div class="controls bullet"><span class="by">throwawaylinux</span><span>|</span><a href="#36334945">root</a><span>|</span><a href="#36336305">parent</a><span>|</span><a href="#36337849">next</a><span>|</span><label class="collapse" for="c-36336488">[-]</label><label class="expand" for="c-36336488">[6 more]</label></div><br/><div class="children"><div class="content">It never presents an incoherent view to software.<p>I&#x27;m using coherency as in the term of art, not a colloquial meaning. Every agent observes stores to a location in the same order[*]. Cache coherency says nothing about observed ordering of stores to different locations.<p>[*] Although store forwarding throws a bit of a spanner in that definition, there can still be reordering occurring absent that local reordering.</div><br/><div id="36337322" class="c"><input type="checkbox" id="c-36337322" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#36334945">root</a><span>|</span><a href="#36336488">parent</a><span>|</span><a href="#36336560">next</a><span>|</span><label class="collapse" for="c-36337322">[-]</label><label class="expand" for="c-36337322">[1 more]</label></div><br/><div class="children"><div class="content">Hum I don&#x27;t see how store forwarding breaks the illusion of total order of stores on a single memory location, at least in 5 minutes of thinking I can&#x27;t come up with a litmus that would demonstrate it. In fact even c++ relaxed stores and loads preserve this ordering.<p>I think your definition is correct without the asterisk.<p>edit: tweaked working</div><br/></div></div><div id="36336560" class="c"><input type="checkbox" id="c-36336560" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#36334945">root</a><span>|</span><a href="#36336488">parent</a><span>|</span><a href="#36337322">prev</a><span>|</span><a href="#36337849">next</a><span>|</span><label class="collapse" for="c-36336560">[-]</label><label class="expand" for="c-36336560">[4 more]</label></div><br/><div class="children"><div class="content">Fine, with that specific term of art meaning then ignore my second post.  I stand by my original statement that you can&#x27;t trust it to &quot;do much for you&quot;.   Just replace the last word with &quot;act consistent&quot; or &quot;act ordered&quot;.  Per-address ordering is nearly useless by itself.  And if you had a CPU that didn&#x27;t guarantee that, you&#x27;d observe almost no difference.</div><br/><div id="36336718" class="c"><input type="checkbox" id="c-36336718" checked=""/><div class="controls bullet"><span class="by">throwawaylinux</span><span>|</span><a href="#36334945">root</a><span>|</span><a href="#36336560">parent</a><span>|</span><a href="#36337849">next</a><span>|</span><label class="collapse" for="c-36336718">[-]</label><label class="expand" for="c-36336718">[3 more]</label></div><br/><div class="children"><div class="content">Well no, if you don&#x27;t have a coherent system then your memory operations aren&#x27;t reliable. You can lose updates or read stale data. Look at what software has to do in incoherent systems, specific flush and invalidate points which is not the same as ordering barriers.<p>Your CPU guarantees a lot, cache coherency to start with. But also a very well defined ordering model and ordering instructions. It&#x27;s not necessarily trivial to program for, but that doesn&#x27;t mean you can&#x27;t trust it.</div><br/><div id="36337451" class="c"><input type="checkbox" id="c-36337451" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#36334945">root</a><span>|</span><a href="#36336718">parent</a><span>|</span><a href="#36337849">next</a><span>|</span><label class="collapse" for="c-36337451">[-]</label><label class="expand" for="c-36337451">[2 more]</label></div><br/><div class="children"><div class="content">&gt; You can lose updates<p>A system without cache coherency can still promise that updates won&#x27;t be lost.  There are lots of way to write rules around update propagation, and cache coherency is just one of them.<p>&gt; or read stale data<p>Cache coherency doesn&#x27;t protect you from stale data unless you only read one memory address ever.<p>&gt; Look at what software has to do in incoherent systems, specific flush and invalidate points which is not the same as ordering barriers.<p>That depends on the memory model.  You could have a system that doesn&#x27;t guarantee cache coherency in general but works fine if you put in ordinary memory barriers.</div><br/><div id="36337911" class="c"><input type="checkbox" id="c-36337911" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#36334945">root</a><span>|</span><a href="#36337451">parent</a><span>|</span><a href="#36337849">next</a><span>|</span><label class="collapse" for="c-36337911">[-]</label><label class="expand" for="c-36337911">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You could have a system that doesn&#x27;t guarantee cache coherency in general but works fine if you put in ordinary memory barriers.<p>How would that work? In such a system, either you have no caches or memory barriers would need to pessimistically flush all dirty lines to memory <i>and</i> send invalidation and synchronization messages to all other cores. In practice such system, far from being fine, would be so slow to be unusable if barriers had such semantics.  Even implementing c++ relaxed semantics would be very expensive.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="36337849" class="c"><input type="checkbox" id="c-36337849" checked=""/><div class="controls bullet"><span class="by">anonymousDan</span><span>|</span><a href="#36334945">prev</a><span>|</span><a href="#36336651">next</a><span>|</span><label class="collapse" for="c-36337849">[-]</label><label class="expand" for="c-36337849">[1 more]</label></div><br/><div class="children"><div class="content">On the related topic of store buffers and write coalescing, can anyone point me to some good articles discussing how this works in practice for merging small writes to the same cache line? I have a workload where we are appending different size payloads (of sizes between 1 and 64 bytes) to an in memory log. One option is to just store each entry in its own cache line. A more space efficient option is to pack each write tightly one after the other. My hypothesis is that due to write coalescing in store buffers this will also be more memory bandwidth efficient since writes to the same cache line will be merged. Is this correct? Note that metadata for each entry (e.g. the size) will be stored separately.</div><br/></div></div><div id="36336651" class="c"><input type="checkbox" id="c-36336651" checked=""/><div class="controls bullet"><span class="by">101011</span><span>|</span><a href="#36337849">prev</a><span>|</span><a href="#36334618">next</a><span>|</span><label class="collapse" for="c-36336651">[-]</label><label class="expand" for="c-36336651">[1 more]</label></div><br/><div class="children"><div class="content">Loved the article, appreciate the share. A meta topic but I wish people would spend more time discussing what they like about something than the opposite.<p>This author took great pains to give many caveats around their writing and distilled down a very complex topic into something that could be consumed in less than 10 minutes.<p>I&#x27;m smarter for having read this.<p>Best of luck to any poor soul who attempts to summarize the work of many architects and then share it with this group of people.</div><br/></div></div><div id="36334618" class="c"><input type="checkbox" id="c-36334618" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#36336651">prev</a><span>|</span><a href="#36334439">next</a><span>|</span><label class="collapse" for="c-36334618">[-]</label><label class="expand" for="c-36334618">[16 more]</label></div><br/><div class="children"><div class="content">My favourite myth is that I still believe it&#x27;s possible to write code which operates totally out of L1 and L2 cache. Not just tight ASM on bare metal: C code or similar, compiled down, to run under a modern UNIX&#x2F;POSIX os on a multi-core host.<p>I have never explored HOW this would work, or WHAT I would do to achieve it, but I believe it, implicitly.  For it to be true I would have to understand the implications of every function and procedure call, stack operations, actual size of objects&#x2F;structs under alignment, optimisations in the assembler to align or select code which runs faster in the given ALU, none of which I know (if I ever did, certainly not any more)<p>But still: I believe this myth. I believe I know somebody who has achieved it, to test some ideas about CPU&#x27;s ability to saturate a NIC at wire-rate, with pre-formed packets. He was able to show by binding affinity to specific cores and running this code, he could basically flood any link his CPU was capable of being exposed to, for a given PCI generation of NIC speeds available to him. (as I understand it) -But that assumes that walking off L2 cache would somehow make it run SLOW enough, to not be able to do this.<p>So I think it remains a myth, to me.</div><br/><div id="36334722" class="c"><input type="checkbox" id="c-36334722" checked=""/><div class="controls bullet"><span class="by">edrxty</span><span>|</span><a href="#36334618">parent</a><span>|</span><a href="#36336188">next</a><span>|</span><label class="collapse" for="c-36334722">[-]</label><label class="expand" for="c-36334722">[7 more]</label></div><br/><div class="children"><div class="content">Modern CPUs are pushing 1MB L1 and 8MB L2 or more.  You can fit a dozen FreeRTOS instances in that space.  It would be pretty cool to see someone build a system that used a high end CPU but didn&#x27;t have any installed ram, though I&#x27;m not sure if the CPU microcode would be ok with that.</div><br/><div id="36334905" class="c"><input type="checkbox" id="c-36334905" checked=""/><div class="controls bullet"><span class="by">Tuna-Fish</span><span>|</span><a href="#36334618">root</a><span>|</span><a href="#36334722">parent</a><span>|</span><a href="#36336188">next</a><span>|</span><label class="collapse" for="c-36334905">[-]</label><label class="expand" for="c-36334905">[6 more]</label></div><br/><div class="children"><div class="content">Those numbers are for all cores. No modern CPU has anywhere near that of L1 and L2 per core.</div><br/><div id="36334949" class="c"><input type="checkbox" id="c-36334949" checked=""/><div class="controls bullet"><span class="by">Kirby64</span><span>|</span><a href="#36334618">root</a><span>|</span><a href="#36334905">parent</a><span>|</span><a href="#36336188">next</a><span>|</span><label class="collapse" for="c-36334949">[-]</label><label class="expand" for="c-36334949">[5 more]</label></div><br/><div class="children"><div class="content">For context, the latest 7950x has 64KB of L1 per core, and 1MB L2</div><br/><div id="36337041" class="c"><input type="checkbox" id="c-36337041" checked=""/><div class="controls bullet"><span class="by">girvo</span><span>|</span><a href="#36334618">root</a><span>|</span><a href="#36334949">parent</a><span>|</span><a href="#36335027">next</a><span>|</span><label class="collapse" for="c-36337041">[-]</label><label class="expand" for="c-36337041">[2 more]</label></div><br/><div class="children"><div class="content">The L2 especially is more than enough for FreeRTOS though, thats 3 times the amount of memory my board has.</div><br/><div id="36337922" class="c"><input type="checkbox" id="c-36337922" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#36334618">root</a><span>|</span><a href="#36337041">parent</a><span>|</span><a href="#36335027">next</a><span>|</span><label class="collapse" for="c-36337922">[-]</label><label class="expand" for="c-36337922">[1 more]</label></div><br/><div class="children"><div class="content">In the &#x27;80s you could run a fully preemptive-multitasking OS with a point-and-click GUI, games and office applications on 1MB of RAM.</div><br/></div></div></div></div><div id="36335027" class="c"><input type="checkbox" id="c-36335027" checked=""/><div class="controls bullet"><span class="by">thfuran</span><span>|</span><a href="#36334618">root</a><span>|</span><a href="#36334949">parent</a><span>|</span><a href="#36337041">prev</a><span>|</span><a href="#36336188">next</a><span>|</span><label class="collapse" for="c-36335027">[-]</label><label class="expand" for="c-36335027">[2 more]</label></div><br/><div class="children"><div class="content">And I know we were talking about L1&#x2F;2, but Epycs can have over 1GB of total L3.</div><br/><div id="36335900" class="c"><input type="checkbox" id="c-36335900" checked=""/><div class="controls bullet"><span class="by">slashdev</span><span>|</span><a href="#36334618">root</a><span>|</span><a href="#36335027">parent</a><span>|</span><a href="#36336188">next</a><span>|</span><label class="collapse" for="c-36335900">[-]</label><label class="expand" for="c-36335900">[1 more]</label></div><br/><div class="children"><div class="content">You could boot Windows XP or Linux in that comfortably. Even run an older web browser or computer game. That’s crazy!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36336188" class="c"><input type="checkbox" id="c-36336188" checked=""/><div class="controls bullet"><span class="by">mjg59</span><span>|</span><a href="#36334618">parent</a><span>|</span><a href="#36334722">prev</a><span>|</span><a href="#36334745">next</a><span>|</span><label class="collapse" for="c-36336188">[-]</label><label class="expand" for="c-36336188">[2 more]</label></div><br/><div class="children"><div class="content">As mentioned in another comment, this literally happens during system bringup when you don&#x27;t have working RAM yet - link training involves a lot of state and you can&#x27;t do it in registers alone, so the cache is configured in such a way that you can use it as RAM (I &#x2F;think&#x2F; this is just a special case of write-back where you never actually do the write?). As long as you&#x27;re not doing DMA I don&#x27;t see any reason why you wouldn&#x27;t be able to bring up a system with empty RAM sockets and just launch Doom out of firmware flash.</div><br/><div id="36337358" class="c"><input type="checkbox" id="c-36337358" checked=""/><div class="controls bullet"><span class="by">sweetjuly</span><span>|</span><a href="#36334618">root</a><span>|</span><a href="#36336188">parent</a><span>|</span><a href="#36334745">next</a><span>|</span><label class="collapse" for="c-36337358">[-]</label><label class="expand" for="c-36337358">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I &#x2F;think&#x2F; this is just a special case of write-back where you never actually do the write?<p>Sort of! A typical implementation (from what I&#x27;ve seen, anyways) is that you have a memory mapped region which, when cache-as-RAM is activated, directly indexes into some cache (typically the LLC). From a hardware perspective, it&#x27;s a full second address decode mode where you essentially just access the data array without performing tag check&#x2F;write. When coming in to CAR mode, the cache typically needs to flush, but when leaving it really doesn&#x27;t need to do anything (assuming it didn&#x27;t update the tag array and left all lines as invalid).<p>With the size of some modern SoC&#x27;s LLC, you could fairly easily run DOOM out of CAR. It&#x27;ll depend on the SoC, but there&#x27;s no reason why DMA wouldn&#x27;t work as other agents would still be able to send read and write requests to the CAR memory region.</div><br/></div></div></div></div><div id="36334745" class="c"><input type="checkbox" id="c-36334745" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#36334618">parent</a><span>|</span><a href="#36336188">prev</a><span>|</span><a href="#36336043">next</a><span>|</span><label class="collapse" for="c-36334745">[-]</label><label class="expand" for="c-36334745">[1 more]</label></div><br/><div class="children"><div class="content">I mean, your computer likely does this during boot when it’s in cache-as-RAM mode</div><br/></div></div><div id="36334888" class="c"><input type="checkbox" id="c-36334888" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#36334618">parent</a><span>|</span><a href="#36336043">prev</a><span>|</span><a href="#36336235">next</a><span>|</span><label class="collapse" for="c-36334888">[-]</label><label class="expand" for="c-36334888">[2 more]</label></div><br/><div class="children"><div class="content">If you only boot one EFI process, or a small linux kernel + single binary small enough, you can get there.</div><br/><div id="36334970" class="c"><input type="checkbox" id="c-36334970" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#36334618">root</a><span>|</span><a href="#36334888">parent</a><span>|</span><a href="#36336235">next</a><span>|</span><label class="collapse" for="c-36334970">[-]</label><label class="expand" for="c-36334970">[1 more]</label></div><br/><div class="children"><div class="content">so I&#x27;m imagining speedyOS which assumes one core for the OS, and runs its userspace jobs on the other cores, and if you wire a job to a core and its been coded to fit in L1&#x2F;L2 then.. it just &quot;does&quot; -as long as the other jobs are somehow co-erced to run in the other cores, &quot;swapping&quot; in and out as need be. If you can prevent any clicktick disrupting your run state on this one &quot;golden&quot; user core, it just runs as fast as it can, subject to the OS timeslice effects on it, and any unavoidable synchronisation into other bits of the combined CPU&#x2F;ALU&#x2F;Memory&#x2F;Bus system as a whole.</div><br/></div></div></div></div><div id="36336235" class="c"><input type="checkbox" id="c-36336235" checked=""/><div class="controls bullet"><span class="by">FridgeSeal</span><span>|</span><a href="#36334618">parent</a><span>|</span><a href="#36334888">prev</a><span>|</span><a href="#36334439">next</a><span>|</span><label class="collapse" for="c-36336235">[-]</label><label class="expand" for="c-36336235">[2 more]</label></div><br/><div class="children"><div class="content">Doesn’t KDB+ and one of the more esoteric array-based languages (J&#x2F;Q or such?) essentially claim this?</div><br/><div id="36336676" class="c"><input type="checkbox" id="c-36336676" checked=""/><div class="controls bullet"><span class="by">moonchild</span><span>|</span><a href="#36334618">root</a><span>|</span><a href="#36336235">parent</a><span>|</span><a href="#36334439">next</a><span>|</span><label class="collapse" for="c-36336676">[-]</label><label class="expand" for="c-36336676">[1 more]</label></div><br/><div class="children"><div class="content">Sort of (not in the sense meant by the parent), and it&#x27;s nonsense regardless <a href="https:&#x2F;&#x2F;mlochbaum.github.io&#x2F;BQN&#x2F;implementation&#x2F;kclaims.html#instruction-cache" rel="nofollow noreferrer">https:&#x2F;&#x2F;mlochbaum.github.io&#x2F;BQN&#x2F;implementation&#x2F;kclaims.html#...</a></div><br/></div></div></div></div></div></div><div id="36334439" class="c"><input type="checkbox" id="c-36334439" checked=""/><div class="controls bullet"><span class="by">ninepoints</span><span>|</span><a href="#36334618">prev</a><span>|</span><a href="#36337005">next</a><span>|</span><label class="collapse" for="c-36334439">[-]</label><label class="expand" for="c-36334439">[6 more]</label></div><br/><div class="children"><div class="content">Hmm, I think the discussion is missing a few things needed for a complete picture of the situation.<p>First, ARM and x86 coherency models differ, so a big disclaimer is needed regarding the protocol. Most ARM processors use the MOESI protocol instead of the MESI protocol.<p>Second, synchronization isn&#x27;t just because of register volatility and such. Synchronization is needed in general because without the appropriate lock&#x2F;barrier instructions, compilers make assumptions about how loads and stores may be reordered with respect to one another.</div><br/><div id="36335214" class="c"><input type="checkbox" id="c-36335214" checked=""/><div class="controls bullet"><span class="by">monocasa</span><span>|</span><a href="#36334439">parent</a><span>|</span><a href="#36335117">next</a><span>|</span><label class="collapse" for="c-36335214">[-]</label><label class="expand" for="c-36335214">[3 more]</label></div><br/><div class="children"><div class="content">&gt; First, ARM and x86 coherency models differ, so a big disclaimer is needed regarding the protocol. Most ARM processors use the MOESI protocol instead of the MESI protocol.<p>MOESI is called out.<p>&gt; The above are just some of the possible scenarios that can occur. In reality, there are numerous variations of the above design, and no 2 implementations are the same. For example, some designs have an O&#x2F;F state.<p>However that&#x27;s not an ARM v x86 thing; AMD has at least previously used MOESI.<p>&gt; Second, synchronization isn&#x27;t just because of register volatility and such. Synchronization is needed in general because without the appropriate lock&#x2F;barrier instructions, compilers make assumptions about how loads and stores may be reordered with respect to one another.<p>Compiler barriers are different than hardware memory barriers.  There are reasons to have one, but not the other.</div><br/><div id="36335434" class="c"><input type="checkbox" id="c-36335434" checked=""/><div class="controls bullet"><span class="by">ninepoints</span><span>|</span><a href="#36334439">root</a><span>|</span><a href="#36335214">parent</a><span>|</span><a href="#36335117">next</a><span>|</span><label class="collapse" for="c-36335434">[-]</label><label class="expand" for="c-36335434">[2 more]</label></div><br/><div class="children"><div class="content">Thanks I missed that line regarding the O states. Still, a word about write reordering on ARM would probably be useful (unless I missed that also).<p>I understand that synchronization in code vs hardware is different, but the blog explicitly moves out of hardware-land into source code land with references to Java volatile and such.</div><br/><div id="36337964" class="c"><input type="checkbox" id="c-36337964" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#36334439">root</a><span>|</span><a href="#36335434">parent</a><span>|</span><a href="#36335117">next</a><span>|</span><label class="collapse" for="c-36337964">[-]</label><label class="expand" for="c-36337964">[1 more]</label></div><br/><div class="children"><div class="content">The blog mentions about java volatiles (but it would also apply to C++ atomic) to explicitly mention that volatile has no cache coherency implications on a typical MESI (and variants) machine. The fences required to maintain language level memory model guarantees act at a level above the L1 cache, once the data reaches L1 (i.e. the coherence point), the fences have done their job.<p>[I&#x27;m ignoring remote fences which are a specialized and not yet mainstream feature]</div><br/></div></div></div></div></div></div><div id="36335117" class="c"><input type="checkbox" id="c-36335117" checked=""/><div class="controls bullet"><span class="by">chrismarlow9</span><span>|</span><a href="#36334439">parent</a><span>|</span><a href="#36335214">prev</a><span>|</span><a href="#36337005">next</a><span>|</span><label class="collapse" for="c-36335117">[-]</label><label class="expand" for="c-36335117">[2 more]</label></div><br/><div class="children"><div class="content">Any recommendations for something more thorough to read or watch?</div><br/><div id="36336868" class="c"><input type="checkbox" id="c-36336868" checked=""/><div class="controls bullet"><span class="by">ninepoints</span><span>|</span><a href="#36334439">root</a><span>|</span><a href="#36335117">parent</a><span>|</span><a href="#36337005">next</a><span>|</span><label class="collapse" for="c-36336868">[-]</label><label class="expand" for="c-36336868">[1 more]</label></div><br/><div class="children"><div class="content">Honestly, the best resources I know are the ISA&#x2F;architecture manuals directly from each vendor (Intel, AMD, Arm) for the various CPUs and&#x2F;or GPUs of interest. Prior to that, my experience largely comes from doing (working with lower level code, benchmarks, looking at assembly, profiling hardware counters, etc.).</div><br/></div></div></div></div></div></div><div id="36337005" class="c"><input type="checkbox" id="c-36337005" checked=""/><div class="controls bullet"><span class="by">garfieldandthe</span><span>|</span><a href="#36334439">prev</a><span>|</span><a href="#36335004">next</a><span>|</span><label class="collapse" for="c-36337005">[-]</label><label class="expand" for="c-36337005">[3 more]</label></div><br/><div class="children"><div class="content">The central myth is that the average programmer should care.<p>The typical programmer should treat CPU caches as what they are designed to be: mostly transparent. You work in a high level language and leave the tricky details to a library and your compiler.<p>It&#x27;s only a small minority that should really worry about these things.<p>In my daily work, I see more often premature microoptimizations (in part using the myths from the article) which are entirely unnecessary rather than code that needs to optimize for those things.</div><br/><div id="36337367" class="c"><input type="checkbox" id="c-36337367" checked=""/><div class="controls bullet"><span class="by">flohofwoe</span><span>|</span><a href="#36337005">parent</a><span>|</span><a href="#36337095">next</a><span>|</span><label class="collapse" for="c-36337367">[-]</label><label class="expand" for="c-36337367">[1 more]</label></div><br/><div class="children"><div class="content">You should still at least care about &#x27;CPU friendly&#x27; data layout in memory and data access patterns in your code to make the CPU&#x27;s life easier, compiler magic won&#x27;t help all that much there.<p>This can often trivially give you a 10x, and sometimes a 100x performance difference for real-world single-threaded code, especially if it needs to work on big data sets. Your fancy high level compiler won&#x27;t magically reshuffle your data in memory to help with prefetching (at least I&#x27;m not aware of a language that does).<p>Most high level languages popular today are &quot;rooted&quot; in the 90&#x27;s when the latency gap between CPU and memory practically didn&#x27;t exist and thus don&#x27;t care about his specific aspect. Explicit control over memory layout is probably also one of the a main reasons why C stood the test of time so well.</div><br/></div></div><div id="36337095" class="c"><input type="checkbox" id="c-36337095" checked=""/><div class="controls bullet"><span class="by">pajko</span><span>|</span><a href="#36337005">parent</a><span>|</span><a href="#36337367">prev</a><span>|</span><a href="#36335004">next</a><span>|</span><label class="collapse" for="c-36337095">[-]</label><label class="expand" for="c-36337095">[1 more]</label></div><br/><div class="children"><div class="content">This is partly wrong. For example, it&#x27;s important how the fields are organized in a structure, and only the developer knows what belongs together and only they know the access patterns across threads (or at least should know). This is far from being a micro-optimization.</div><br/></div></div></div></div><div id="36335004" class="c"><input type="checkbox" id="c-36335004" checked=""/><div class="controls bullet"><span class="by">ChrisMarshallNY</span><span>|</span><a href="#36337005">prev</a><span>|</span><a href="#36334548">next</a><span>|</span><label class="collapse" for="c-36335004">[-]</label><label class="expand" for="c-36335004">[3 more]</label></div><br/><div class="children"><div class="content">We worked with the Intel guys, in my last gig. They were incredibly helpful.<p>They were an impressive lot, and they helped us out, quite a bit. They often sent engineers over, for weeks at a time, to help us optimize.<p>The cache thing was a 100X improvement thing, and it came from the oddest places. There&#x27;s a lot of &quot;that doesn&#x27;t make sense!&quot; stuff, with preserving caches.<p>I don&#x27;t remember all the tricks, but we were constantly surprised.<p>One thing that saved us, was instrumentation. Intel had a bunch of utilities that they wrote, and that kept showing us that the clever thing we did, was not so clever.</div><br/><div id="36335046" class="c"><input type="checkbox" id="c-36335046" checked=""/><div class="controls bullet"><span class="by">tayo42</span><span>|</span><a href="#36335004">parent</a><span>|</span><a href="#36334548">next</a><span>|</span><label class="collapse" for="c-36335046">[-]</label><label class="expand" for="c-36335046">[2 more]</label></div><br/><div class="children"><div class="content">What kind of work were you doing that&#x27;s required that? Like domain of programing? Sounds interesting<p>&gt;that kept showing us that the clever thing we did, was not so clever.<p>I had the same experience talking with some people network people from Intel. Was pretty funny. To any Intel lurkers here, nice job! Lol</div><br/><div id="36335109" class="c"><input type="checkbox" id="c-36335109" checked=""/><div class="controls bullet"><span class="by">ChrisMarshallNY</span><span>|</span><a href="#36335004">root</a><span>|</span><a href="#36335046">parent</a><span>|</span><a href="#36334548">next</a><span>|</span><label class="collapse" for="c-36335109">[-]</label><label class="expand" for="c-36335109">[1 more]</label></div><br/><div class="children"><div class="content">We wrote image processing pipeline code.<p>Very complex algorithms, on lots of data, that needed to be done quickly.</div><br/></div></div></div></div></div></div><div id="36334548" class="c"><input type="checkbox" id="c-36334548" checked=""/><div class="controls bullet"><span class="by">chasil</span><span>|</span><a href="#36335004">prev</a><span>|</span><a href="#36335139">next</a><span>|</span><label class="collapse" for="c-36334548">[-]</label><label class="expand" for="c-36334548">[1 more]</label></div><br/><div class="children"><div class="content">SQLite database locks can be more quickly obtained and released if CPU affinity is set for the database processes, allowing all I&#x2F;O activity to share the same cache(es).<p>I have read (but cannot remember where) that this can increase performance by thousands of DML operations per second.</div><br/></div></div><div id="36335139" class="c"><input type="checkbox" id="c-36335139" checked=""/><div class="controls bullet"><span class="by">notacoward</span><span>|</span><a href="#36334548">prev</a><span>|</span><a href="#36334380">next</a><span>|</span><label class="collapse" for="c-36335139">[-]</label><label class="expand" for="c-36335139">[9 more]</label></div><br/><div class="children"><div class="content">Dealing with caches, memory ordering, and memory barriers can be truly mind-warping stuff, even for those who have spent years dealing with basic cache coherency before. If you want a challenge, try to absorb all this in one sitting.<p><a href="https:&#x2F;&#x2F;www.kernel.org&#x2F;doc&#x2F;Documentation&#x2F;memory-barriers.txt" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.kernel.org&#x2F;doc&#x2F;Documentation&#x2F;memory-barriers.txt</a><p>I kept an earlier version of this close to hand at all times a couple of jobs ago where we were using our own chips with a very weak memory ordering. The implementation team had mostly come from Alpha, which had the weakest memory ordering ever, and in the intervening years a lot of bugs related to missing memory barriers had crept into the kernel because nobody was using anything nearly as weak. I specifically remember at least one in NBD, at least one in NFS, and many in Lustre. Pain in the ass to debug, because by the time you can look at anything the values have &quot;settled&quot; and seem correct.<p>For extra fun, as weak as the memory ordering was, the first run of chips didn&#x27;t even get that right. LL&#x2F;SC wouldn&#x27;t work reliably unless the LL was issued twice, so we actually modified compilers to do that. Ew.</div><br/><div id="36335431" class="c"><input type="checkbox" id="c-36335431" checked=""/><div class="controls bullet"><span class="by">jcranmer</span><span>|</span><a href="#36335139">parent</a><span>|</span><a href="#36334380">next</a><span>|</span><label class="collapse" for="c-36335431">[-]</label><label class="expand" for="c-36335431">[8 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need that full documentation. Really, I could simplify what most programmers would need to understand about memory ordering down to this text:<p>There are a few models of cross-thread memory ordering that you can choose between. If you have never been exposed to this field before, the naïve model of memory ordering you probably think is going on is sequential consistency. This is not implemented in hardware because oh-gosh-it&#x27;s-expensive, and if there&#x27;s no indication of what memory ordering model your library or language is using, it&#x27;s likely defaulting to sequential consistency.<p>But don&#x27;t worry, you don&#x27;t have to worry about the more complex memory orderings, because you get to pretend everything is sequentially consistent if you write proper synchronization. The easiest way to satisfy proper synchronization is an acquire-release model. Before reading any data that may have been written by a different thread, you need to do an <i>acquire</i> load. After writing any data that may be read by a different thread, you need to do a <i>release</i> store. The basic flow is write then release store, then change thread, then acquire load, then read. Follow this rules, and things stay simple.<p>There is a theoretical slight relaxation of the above model that works on most hardware called release-consume in the C&#x2F;C++ memory model. It isn&#x27;t implemented by any compiler for arcane compiler reasons I won&#x27;t get into, but the Linux kernel (which implements the memory model itself for $REASONS) does rely heavily on it.<p>The final major memory ordering is relaxed atomics. Their semantics are... weird. Essentially, you get what the hardware gives you, plus whatever fun the compiler can toss in, and the guarantees are minimal. I can&#x27;t recommend many cases where you can safely use it, and if you have to ask if you should use it, the answer is no.</div><br/><div id="36336908" class="c"><input type="checkbox" id="c-36336908" checked=""/><div class="controls bullet"><span class="by">klabb3</span><span>|</span><a href="#36335139">root</a><span>|</span><a href="#36335431">parent</a><span>|</span><a href="#36337078">next</a><span>|</span><label class="collapse" for="c-36336908">[-]</label><label class="expand" for="c-36336908">[4 more]</label></div><br/><div class="children"><div class="content">I’m nowhere near as deep in this rabbit hole. But I recall running&#x2F;seeing some benchmarks on different memory orderings and the differences were not.. that big, on x86 I believe. Obviously there’s a lot that can go wrong with micro-benchmarks in these <i>incredibly</i> complex systems, but I still got the feeling that memory orderings are perhaps not worth the immense complexity that they introduce, for let’s say the majority of programmers, even low level folks.<p>What are your thoughts on this?</div><br/><div id="36337193" class="c"><input type="checkbox" id="c-36337193" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#36335139">root</a><span>|</span><a href="#36336908">parent</a><span>|</span><a href="#36337078">next</a><span>|</span><label class="collapse" for="c-36337193">[-]</label><label class="expand" for="c-36337193">[3 more]</label></div><br/><div class="children"><div class="content">At leas on x86, acq&#x2F;rel load&#x2F;stores vs relaxed is basically free. Seq&#x2F;cst loads are also free. Seq&#x2F;cst are relatively fast, but at around 20-30 clock cycles still measurably slower thant everything else.<p>The catch is that x86 only has seq&#x2F;cst atomic RMW so even if you ask for, say, a relaxed CAS or XADD, you will still get an expensive one.<p>So the c++11 memory model allows you to more easily maintain correctness (and your sanity), but for performance you still have to know how to map to the underlying microarchitecture.</div><br/><div id="36337598" class="c"><input type="checkbox" id="c-36337598" checked=""/><div class="controls bullet"><span class="by">klabb3</span><span>|</span><a href="#36335139">root</a><span>|</span><a href="#36337193">parent</a><span>|</span><a href="#36337078">next</a><span>|</span><label class="collapse" for="c-36337598">[-]</label><label class="expand" for="c-36337598">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Seq&#x2F;cst are relatively fast, but at around 20-30 clock cycles<p>Did you mean to say seq&#x2F;cst <i>store</i>?<p>Also, what operation is “set the value to X unconditionally and return me the previous value”? Is that possible with a store or something different? (Golang calls this op atomic swap)<p>In either case, sounds like the room for optimizing for performance with granular memory models on x86 is even narrower than I thought.</div><br/><div id="36337751" class="c"><input type="checkbox" id="c-36337751" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#36335139">root</a><span>|</span><a href="#36337598">parent</a><span>|</span><a href="#36337078">next</a><span>|</span><label class="collapse" for="c-36337751">[-]</label><label class="expand" for="c-36337751">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Did you mean to say seq&#x2F;cst store?<p>Indeed!<p>&gt; set the value to X unconditionally and return me the previous value<p>That would be atomic::exchange that maps to XCHG on x86, which, as all atomic RMW is sequentially consistent.<p>Incidentally seq-cst stores are also typically lowered to XCHG on x86 as opposed to the more obvious MFENCE+MOV.<p>There is still room for optimization, as if you can implement your algos with just load&#x2F;stores and as few strategically placed RMW as you can, it can be a win.<p>Of course if there is any contention, cache coherence traffic is going to dominate over  any atomic cost.</div><br/></div></div></div></div></div></div></div></div><div id="36337078" class="c"><input type="checkbox" id="c-36337078" checked=""/><div class="controls bullet"><span class="by">gpderetta</span><span>|</span><a href="#36335139">root</a><span>|</span><a href="#36335431">parent</a><span>|</span><a href="#36336908">prev</a><span>|</span><a href="#36335518">next</a><span>|</span><label class="collapse" for="c-36337078">[-]</label><label class="expand" for="c-36337078">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You get to pretend everything is sequentially consistent if you write proper synchronization. The easiest way to satisfy proper synchronization is an acquire-release model<p>But acquire&#x2F;release give, generally,a partial ordering, not a total order like sequential consistency. That&#x27;s often enough of course.</div><br/></div></div><div id="36335518" class="c"><input type="checkbox" id="c-36335518" checked=""/><div class="controls bullet"><span class="by">notacoward</span><span>|</span><a href="#36335139">root</a><span>|</span><a href="#36335431">parent</a><span>|</span><a href="#36337078">prev</a><span>|</span><a href="#36334380">next</a><span>|</span><label class="collapse" for="c-36335518">[-]</label><label class="expand" for="c-36335518">[2 more]</label></div><br/><div class="children"><div class="content">Oh, you suppose the authors spent their time writing that <i>for no reason</i>? You know better than them? Your explanation is so incomplete I hardly know where to begin. Maybe with the fact that you&#x27;re talking about <i>local</i> ordering as it may or may not be perturbed by a compiler, while the document is talking about the observed order <i>elsewhere</i> (other processors or main memory). That observed order can vary depending on the processor&#x27;s ordering model, and to ensure it (e.g. across all modifications to a complex shared data structure) without doing the memory equivalent of an fsync() on every file write (disastrous!) you need memory barriers. All of the bugs I mentioned, all of which were crashes, were related to exactly the distinction you&#x27;re missing. Yours is the kind of mythology that OP sought to address. Please read the document - particularly the section on CPU memory barriers and anywhere that mentions Alpha - before spreading more misinformation.</div><br/><div id="36335776" class="c"><input type="checkbox" id="c-36335776" checked=""/><div class="controls bullet"><span class="by">jcranmer</span><span>|</span><a href="#36335139">root</a><span>|</span><a href="#36335518">parent</a><span>|</span><a href="#36334380">next</a><span>|</span><label class="collapse" for="c-36335776">[-]</label><label class="expand" for="c-36335776">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Oh, you suppose the authors spent their time writing that for no reason? Your explanation is so incomplete I hardly know where to begin.<p>No. I am suggesting that you don&#x27;t <i>need</i> the full rigor of understanding the memory barrier semantics to be able to effectively write multithreaded code correctly. Completeness was never a goal of my explanation; sufficiency was. Furthermore, my focus is on a <i>software</i> memory model, not the <i>hardware</i> memory model.<p>&gt; Maybe with the fact that doing acquire&#x2F;release for one value has varying effect (sometimes none) on others, so you&#x27;d have to do it separately for all of the many values that might have changed.<p>You have misinterpreted my words, I think. At no point do I suggest that you should insert an acquire&#x2F;release for each, individual value. Rather, you need to do an acquire at the beginning of a block of code (wherein you can read and write multiple values) and a release at the end of a block of code, much as you would with a mutex (except it&#x27;s not necessary that the regions of code actually be mutually excluded from executing on multiple threads). The <i>only</i> requirement is that the store be followed by a release operation that crosses threads to an acquire operation that is followed by a load.<p>&gt; Then we can move on to the concept of dependent reads, and so on.<p>I allude to those under the part where I mention &quot;There is a theoretical slight relaxation of the above model that works on most hardware called release-consume in the C&#x2F;C++ memory model.&quot; That you did not pick up on that is perhaps because you yourself are unfamiliar with the release-consume portion of the C&#x2F;C++ memory model. If you need a refresher, I might point you to the thread where I actually go into why it exists, and why it&#x27;s not implemented by compilers, here: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36059369">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36059369</a>. (There&#x27;s a reason I&#x27;m not going into it in a facile explanation!)<p>&gt;  Please read the document before you spread more misinformation.<p>Oh, do trust me, I <i>have</i> read that document, and far more, on memory models. I am not spreading misinformation. Perhaps my word choice and my writing is inartful or oversimplified in the goal of making the topic more accessible rather than seeking to enshrine comprehensive knowledge in thick tomes that many will give up reading. Perhaps you yourself may not be sufficiently informed as to pick up on the specific terminology that I use, which derives from the dominant memory model in use by, well, everybody.</div><br/></div></div></div></div></div></div></div></div><div id="36334380" class="c"><input type="checkbox" id="c-36334380" checked=""/><div class="controls bullet"><span class="by">Nuzzerino</span><span>|</span><a href="#36335139">prev</a><span>|</span><a href="#36335679">next</a><span>|</span><label class="collapse" for="c-36334380">[-]</label><label class="expand" for="c-36334380">[1 more]</label></div><br/><div class="children"><div class="content">2018</div><br/></div></div></div></div></div></div></div></body></html>