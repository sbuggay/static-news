<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1707555659427" as="style"/><link rel="stylesheet" href="styles.css?v=1707555659427"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.quantamagazine.org/scientists-find-optimal-balance-of-data-storage-and-time-20240208/">Scientists find optimal space-time balance for hash tables</a> <span class="domain">(<a href="https://www.quantamagazine.org">www.quantamagazine.org</a>)</span></div><div class="subtext"><span>Brajeshwar</span> | <span>49 comments</span></div><br/><div><div id="39322853" class="c"><input type="checkbox" id="c-39322853" checked=""/><div class="controls bullet"><span class="by">klyrs</span><span>|</span><a href="#39320038">next</a><span>|</span><label class="collapse" for="c-39322853">[-]</label><label class="expand" for="c-39322853">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a bit sad to see the anti-curious commentary here.  This result is really cool, and finds the asymptotic sweet spot for hash table memory usage.  Often times, the first algorithm to establish an asymptotic limit is entirely too complicated and doesn&#x27;t help at human scale.  If the world collectively ignores such results, as many here seem inclined to do, that&#x27;s the end of the story.  But when somebody continues bashing their head into this wall, sometimes a really good algorithm falls out, which hits that asymptotic performance benefit at a human-useful scale.<p>The thing I&#x27;m curious about (as I haven&#x27;t had time to cozy up with the paper, and I&#x27;m about to run) is <i>how</i> they hit that runtime.  What&#x27;s the big idea?  Or is it just the gestalt of a dozen slightly clever ideas?</div><br/></div></div><div id="39320038" class="c"><input type="checkbox" id="c-39320038" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#39322853">prev</a><span>|</span><a href="#39323470">next</a><span>|</span><label class="collapse" for="c-39320038">[-]</label><label class="expand" for="c-39320038">[8 more]</label></div><br/><div class="children"><div class="content">For anyone curious this is about a 2023 paper proving that a 2022 theoretical hash table construction achieved not only an upper bound in performance in terms of time and space, the construction also achieved a lower bound. It’s a theoretical construction so not actually a practical one you’d find in your software toolkit (constants are too large for most applications, likely even realistic databases). The referenced “practical” IcebergHT data structure isn’t even one you’d likely encounter as it seems to be a research hash table focusing on persistent memory which isn’t hardware you typically would encounter.</div><br/><div id="39323693" class="c"><input type="checkbox" id="c-39323693" checked=""/><div class="controls bullet"><span class="by">randomizedalgs</span><span>|</span><a href="#39320038">parent</a><span>|</span><a href="#39320223">next</a><span>|</span><label class="collapse" for="c-39323693">[-]</label><label class="expand" for="c-39323693">[1 more]</label></div><br/><div class="children"><div class="content">IcebergHT isn&#x27;t just for persistent memory (although I can see why you might think it is based on the paper&#x27;s title). The paper also gives experiments showing that the hash table performs very well in standard RAM, much better than other concurrent implementations.</div><br/></div></div><div id="39320223" class="c"><input type="checkbox" id="c-39320223" checked=""/><div class="controls bullet"><span class="by">jakedowns</span><span>|</span><a href="#39320038">parent</a><span>|</span><a href="#39323693">prev</a><span>|</span><a href="#39322660">next</a><span>|</span><label class="collapse" for="c-39320223">[-]</label><label class="expand" for="c-39320223">[4 more]</label></div><br/><div class="children"><div class="content">love when things like this are ready and waiting on the shelf for when we need them</div><br/><div id="39321873" class="c"><input type="checkbox" id="c-39321873" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#39320038">root</a><span>|</span><a href="#39320223">parent</a><span>|</span><a href="#39320580">next</a><span>|</span><label class="collapse" for="c-39321873">[-]</label><label class="expand" for="c-39321873">[1 more]</label></div><br/><div class="children"><div class="content">Kinda but often times such research isn’t even “waiting on the shelf”. You can see that in the Iceberg paper which describes how earlier constructions optimized for PMEM Hw did a bad job because they were built against simulated hardware. In practice you need to actually iterate with real designs.<p>I doubt such designs will find practical uses (iceberg maybe but the pure math designs seem unlikely).</div><br/></div></div><div id="39320580" class="c"><input type="checkbox" id="c-39320580" checked=""/><div class="controls bullet"><span class="by">opticfluorine</span><span>|</span><a href="#39320038">root</a><span>|</span><a href="#39320223">parent</a><span>|</span><a href="#39321873">prev</a><span>|</span><a href="#39322660">next</a><span>|</span><label class="collapse" for="c-39320580">[-]</label><label class="expand" for="c-39320580">[2 more]</label></div><br/><div class="children"><div class="content">In 200 years this is going to be like when we discover yet another proof of Euler or Gauss, 20 years after a current mathematician arrived at the same result.</div><br/><div id="39321504" class="c"><input type="checkbox" id="c-39321504" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#39320038">root</a><span>|</span><a href="#39320580">parent</a><span>|</span><a href="#39322660">next</a><span>|</span><label class="collapse" for="c-39321504">[-]</label><label class="expand" for="c-39321504">[1 more]</label></div><br/><div class="children"><div class="content">Or it could be like multipoint touch screens, early version of which were a solved problem in 1980s or earlier, and just needed compute power to catch up, which it did over subsequent decades.<p>Or it could be like code=data and homoiconity and other CS fundamentals that were figured out 40+ years ago, but are <i>still</i> mostly ignored by software industry&#x2F;culture.</div><br/></div></div></div></div></div></div><div id="39322660" class="c"><input type="checkbox" id="c-39322660" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39320038">parent</a><span>|</span><a href="#39320223">prev</a><span>|</span><a href="#39323470">next</a><span>|</span><label class="collapse" for="c-39322660">[-]</label><label class="expand" for="c-39322660">[2 more]</label></div><br/><div class="children"><div class="content">This is so strange to me.<p>I&#x27;m hoping because Quanta&#x27;s explanation was approachable, but ultimately, wrong when I try applying it the following way:<p>Theorists will spend an enormous amount of time developing algorithms that are O(kNlog(N)) that are impractical in practice because<p>- K approximates infinity<p>- It is well-known K approximates infinity.<p>- It is not expected for K to decrease.</div><br/><div id="39323659" class="c"><input type="checkbox" id="c-39323659" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#39320038">root</a><span>|</span><a href="#39322660">parent</a><span>|</span><a href="#39323470">next</a><span>|</span><label class="collapse" for="c-39323659">[-]</label><label class="expand" for="c-39323659">[1 more]</label></div><br/><div class="children"><div class="content">&gt; “But in practice, constants really matter,” he said. “In the real world, a factor of 10 is a game ender.”<p>Working at a place who kept losing customers to a competitor whose software was less than 2x as fast as ours fundamentally changed how I view optimization and how I view constant overhead C. And crystalized once I saw how delivering steady gains milestone after milestone can buy a lot more goodwill than one fast and dirty optimization.<p>Speed doesn&#x27;t matter if you&#x27;re the only game in town (a monopoly). For everything else it matters.</div><br/></div></div></div></div></div></div><div id="39323470" class="c"><input type="checkbox" id="c-39323470" checked=""/><div class="controls bullet"><span class="by">yalok</span><span>|</span><a href="#39320038">prev</a><span>|</span><a href="#39324276">next</a><span>|</span><label class="collapse" for="c-39323470">[-]</label><label class="expand" for="c-39323470">[12 more]</label></div><br/><div class="children"><div class="content">Hash tables are nice &amp; all, and it&#x27;s very convenient to use them,
BUT I really regret how they create propensity for junior developers to use them even in cases where they are not really needed and more memory-efficient (and perf too) data structure could be used.<p>Anecdotally, in my recent year of interviewing new candidates, I keep asking this simple question:  come up with the most efficient data structure to store highly sparse data (say vector), that will need to be access only sequentially. E.g., we have a 1M int32 values vector, and only 10K values are non-zero.<p>90% of the candidates suggest using the hash table (especially if they prefer to use Python for the interview).<p>I then ask them to estimate (roughly) the size of the memory, required to store those 10K values.<p>Some say they need 10KB (they struggle to convert int32 to the count of bytes as well ;( ). 
Some say 40KB (a bit better, but they forget about keys).
Less than 20% arrive to 80KB.
Very few suggest that it&#x27;s something higher than 80KB...<p>Most struggle to account for the pointers that inevitably should be there if values are not allocated in contiguous memory. Most forget about the hashmap itself.<p>Fwiw, here&#x27;s some primitive comparison of memory taken by Python&#x27;s dict, list and numpy&#x27;s array:<p>10000  values dict size:  500568<p>10000  values list size:  426516<p>10000  values array size:  80056<p>500KB&#x2F;80KB - &gt; 6x times overhead...</div><br/><div id="39324347" class="c"><input type="checkbox" id="c-39324347" checked=""/><div class="controls bullet"><span class="by">iamflimflam1</span><span>|</span><a href="#39323470">parent</a><span>|</span><a href="#39323473">next</a><span>|</span><label class="collapse" for="c-39324347">[-]</label><label class="expand" for="c-39324347">[2 more]</label></div><br/><div class="children"><div class="content">The trap with asking these kind of questions is that you have spent a long time thinking of your optimum solution and the arguments for and against it.<p>You now have an expectation that it should be obvious and easy.</div><br/><div id="39324552" class="c"><input type="checkbox" id="c-39324552" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#39323470">root</a><span>|</span><a href="#39324347">parent</a><span>|</span><a href="#39323473">next</a><span>|</span><label class="collapse" for="c-39324552">[-]</label><label class="expand" for="c-39324552">[1 more]</label></div><br/><div class="children"><div class="content">This solution is&#x2F;should be obvious and easy to anyone who knows algorithms and data structures.<p>I didn&#x27;t invent op&#x27;s question, but an answer that it should be a list is obvious right after reading the question. The question is very similar to the most basic exercises from the very first lessons of any basic algorithmics course.<p>If I was programming in python, I&#x27;d still probably use a hashmap because it&#x27;s the quickest to implement. But once it shows to be a bottleneck in terms of speed or memory use, I&#x27;d switch to lists.</div><br/></div></div></div></div><div id="39323473" class="c"><input type="checkbox" id="c-39323473" checked=""/><div class="controls bullet"><span class="by">yalok</span><span>|</span><a href="#39323470">parent</a><span>|</span><a href="#39324347">prev</a><span>|</span><a href="#39324120">next</a><span>|</span><label class="collapse" for="c-39323473">[-]</label><label class="expand" for="c-39323473">[7 more]</label></div><br/><div class="children"><div class="content">(from <a href="https:&#x2F;&#x2F;python-fiddle.com&#x2F;saved&#x2F;lN9D7DOQl0bQ8sqnDA94?code=true&amp;output=true&amp;run=true" rel="nofollow">https:&#x2F;&#x2F;python-fiddle.com&#x2F;saved&#x2F;lN9D7DOQl0bQ8sqnDA94?code=tr...</a>)</div><br/><div id="39323537" class="c"><input type="checkbox" id="c-39323537" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#39323470">root</a><span>|</span><a href="#39323473">parent</a><span>|</span><a href="#39324120">next</a><span>|</span><label class="collapse" for="c-39323537">[-]</label><label class="expand" for="c-39323537">[6 more]</label></div><br/><div class="children"><div class="content">&gt; come up with the most efficient data structure to store highly sparse data (say vector), that will need to be access only sequentially.<p>There&#x27;s something even more efficient -- a sparse array. I worked in the sparse linear algebra space, and you can gain a lot from sparsity. Add this to your fiddle:<p><pre><code>    from scipy.sparse import csc_array
    sp_array = csc_array(my_list, dtype=np.int32)
    print(SZ, &quot; values sparse array size: &quot;, get_obj_size(sp_array))
</code></pre>
And you&#x27;ll get this result:<p><pre><code>    10000  values dict size:  501060
    10000  values list size:  426620
    10000  values array size:  80056
    10000  values sparse array size:  80276
</code></pre>
EDIT: looks like I was wrong -- I made a mistake in the code. The sparse structure is actually larger.<p>Also in my earlier result of 280 bytes, the get_obj_size might be reading the metadata part of the data structure. 10k int32 objects (4 bytes) each will not compress to 280 bytes.<p>But my point in general holds -- sparse structures are usually more efficient to work with than dense structures, especially when you have really large matrices.</div><br/><div id="39323791" class="c"><input type="checkbox" id="c-39323791" checked=""/><div class="controls bullet"><span class="by">FabHK</span><span>|</span><a href="#39323470">root</a><span>|</span><a href="#39323537">parent</a><span>|</span><a href="#39324176">next</a><span>|</span><label class="collapse" for="c-39323791">[-]</label><label class="expand" for="c-39323791">[4 more]</label></div><br/><div class="children"><div class="content">Sorry, you have a sorted list of 10,000 Int32, so each of them needs around 32 bits, for a total of 40 KB. I can see how you can store them in 80 KB. I could also see how you would only store the distance to the next one (say 32-log_2(10,0000) = 20 bits on average), for a total of 200,000 bits or 25 KB, if you manage to package it very efficiently.<p>But how do you store that info in 264 bytes? Something is off there.</div><br/><div id="39324013" class="c"><input type="checkbox" id="c-39324013" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#39323470">root</a><span>|</span><a href="#39323791">parent</a><span>|</span><a href="#39323941">prev</a><span>|</span><a href="#39323875">next</a><span>|</span><label class="collapse" for="c-39324013">[-]</label><label class="expand" for="c-39324013">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for pointing that out. It was my mistake (corrected in parent post)</div><br/></div></div><div id="39323875" class="c"><input type="checkbox" id="c-39323875" checked=""/><div class="controls bullet"><span class="by">anonymoushn</span><span>|</span><a href="#39323470">root</a><span>|</span><a href="#39323791">parent</a><span>|</span><a href="#39324013">prev</a><span>|</span><a href="#39324176">next</a><span>|</span><label class="collapse" for="c-39323875">[-]</label><label class="expand" for="c-39323875">[1 more]</label></div><br/><div class="children"><div class="content">You put all the other bytes outside of python&#x27;s heap :)</div><br/></div></div></div></div><div id="39324176" class="c"><input type="checkbox" id="c-39324176" checked=""/><div class="controls bullet"><span class="by">yalok</span><span>|</span><a href="#39323470">root</a><span>|</span><a href="#39323537">parent</a><span>|</span><a href="#39323791">prev</a><span>|</span><a href="#39324120">next</a><span>|</span><label class="collapse" for="c-39324176">[-]</label><label class="expand" for="c-39324176">[1 more]</label></div><br/><div class="children"><div class="content">To validate your point, I think you need to modify my test and create a 1M elements array with 99% of zeros - then your sparse class will be way more efficient, also hiding the complexity of creating a compact sparse representation at the native level.</div><br/></div></div></div></div></div></div><div id="39324120" class="c"><input type="checkbox" id="c-39324120" checked=""/><div class="controls bullet"><span class="by">TheDudeMan</span><span>|</span><a href="#39323470">parent</a><span>|</span><a href="#39323473">prev</a><span>|</span><a href="#39324276">next</a><span>|</span><label class="collapse" for="c-39324120">[-]</label><label class="expand" for="c-39324120">[2 more]</label></div><br/><div class="children"><div class="content">OK but what is your point?</div><br/><div id="39324161" class="c"><input type="checkbox" id="c-39324161" checked=""/><div class="controls bullet"><span class="by">yalok</span><span>|</span><a href="#39323470">root</a><span>|</span><a href="#39324120">parent</a><span>|</span><a href="#39324276">next</a><span>|</span><label class="collapse" for="c-39324161">[-]</label><label class="expand" for="c-39324161">[1 more]</label></div><br/><div class="children"><div class="content">That junior engineers tend to use hash map in situations where it’s way less efficient and is not necessary…</div><br/></div></div></div></div></div></div><div id="39324276" class="c"><input type="checkbox" id="c-39324276" checked=""/><div class="controls bullet"><span class="by">rurban</span><span>|</span><a href="#39323470">prev</a><span>|</span><a href="#39320723">next</a><span>|</span><label class="collapse" for="c-39324276">[-]</label><label class="expand" for="c-39324276">[1 more]</label></div><br/><div class="children"><div class="content">Here is the code <a href="https:&#x2F;&#x2F;github.com&#x2F;splatlab&#x2F;iceberghashtable">https:&#x2F;&#x2F;github.com&#x2F;splatlab&#x2F;iceberghashtable</a><p>(from <a href="https:&#x2F;&#x2F;prashantpandey.github.io&#x2F;publication&#x2F;sigmod23_iceberg&#x2F;" rel="nofollow">https:&#x2F;&#x2F;prashantpandey.github.io&#x2F;publication&#x2F;sigmod23_iceber...</a>)<p>Need to compare it against my other concurrent hash tables, as they measured only 64bit int performance for keys and values, which is a bit unrealistic. And they only used  murmurhash.</div><br/></div></div><div id="39320723" class="c"><input type="checkbox" id="c-39320723" checked=""/><div class="controls bullet"><span class="by">madars</span><span>|</span><a href="#39324276">prev</a><span>|</span><a href="#39324416">next</a><span>|</span><label class="collapse" for="c-39320723">[-]</label><label class="expand" for="c-39320723">[6 more]</label></div><br/><div class="children"><div class="content">Somewhat off-topic but does anyone have a bookmarklet that would remove annoying scrolling behavior on sites like Quanta? I.e. pressing space bar should immediately scroll down a page, not do it slowly with awkward mechanics.<p>In exchange I&#x27;m happy to share a bookmarklet that &quot;unsticks&quot; sticky elements of the page (e.g. an ever-visible header like on WaPo, though they are not the worst offender): <a href="https:&#x2F;&#x2F;pastebin.com&#x2F;Vh594168" rel="nofollow">https:&#x2F;&#x2F;pastebin.com&#x2F;Vh594168</a></div><br/><div id="39321160" class="c"><input type="checkbox" id="c-39321160" checked=""/><div class="controls bullet"><span class="by">btdmaster</span><span>|</span><a href="#39320723">parent</a><span>|</span><a href="#39321843">next</a><span>|</span><label class="collapse" for="c-39321160">[-]</label><label class="expand" for="c-39321160">[1 more]</label></div><br/><div class="children"><div class="content">I think they&#x27;re using sscroll from npm, but not entirely sure. I was able to fix it, but too well, in that now it&#x27;ll scroll with unbounded speed: <a href="https:&#x2F;&#x2F;tio.run&#x2F;##LY5BDsIgEEX3PcXsgKQS9wYXRpMu3HkCAlMdbYFQ2qYxPTtidTWT935m&#x2F;lNPejCRQtoNgSzG3rsXLjlbb8YeXZLa2stUlisNCR1GzoofA6sBvxjUEd4VALXANyCNtwhKKWC3oA0ysXn4xWWI2zxjq8cucXGYyVk&#x2F;y1LCd91p4fv6T8iVbw3S&#x2F;ZFEObBWq4CcPw" rel="nofollow">https:&#x2F;&#x2F;tio.run&#x2F;##LY5BDsIgEEX3PcXsgKQS9wYXRpMu3HkCAlMdbYFQ2q...</a></div><br/></div></div><div id="39321843" class="c"><input type="checkbox" id="c-39321843" checked=""/><div class="controls bullet"><span class="by">abbeyj</span><span>|</span><a href="#39320723">parent</a><span>|</span><a href="#39321160">prev</a><span>|</span><a href="#39324336">next</a><span>|</span><label class="collapse" for="c-39321843">[-]</label><label class="expand" for="c-39321843">[1 more]</label></div><br/><div class="children"><div class="content">This is quick and dirty but it works on this particular site:<p>javascript:void(addEventListener(&#x27;keydown&#x27;,e=&gt;e.keyCode==32&amp;&amp;e.stopPropagation(),true)</div><br/></div></div><div id="39324336" class="c"><input type="checkbox" id="c-39324336" checked=""/><div class="controls bullet"><span class="by">NooneAtAll3</span><span>|</span><a href="#39320723">parent</a><span>|</span><a href="#39321843">prev</a><span>|</span><a href="#39324416">next</a><span>|</span><label class="collapse" for="c-39324336">[-]</label><label class="expand" for="c-39324336">[3 more]</label></div><br/><div class="children"><div class="content">...spacebar?<p>shouldn&#x27;t &quot;down a page&quot; be done with PageDown?<p>who came up with binding this behavior to a spacebar as well??</div><br/><div id="39324575" class="c"><input type="checkbox" id="c-39324575" checked=""/><div class="controls bullet"><span class="by">eviks</span><span>|</span><a href="#39320723">root</a><span>|</span><a href="#39324336">parent</a><span>|</span><a href="#39324564">next</a><span>|</span><label class="collapse" for="c-39324575">[-]</label><label class="expand" for="c-39324575">[1 more]</label></div><br/><div class="children"><div class="content">Someone who realized that page down is far away, space is close, and paging down is frequent</div><br/></div></div><div id="39324564" class="c"><input type="checkbox" id="c-39324564" checked=""/><div class="controls bullet"><span class="by">gniv</span><span>|</span><a href="#39320723">root</a><span>|</span><a href="#39324336">parent</a><span>|</span><a href="#39324575">prev</a><span>|</span><a href="#39324416">next</a><span>|</span><label class="collapse" for="c-39324564">[-]</label><label class="expand" for="c-39324564">[1 more]</label></div><br/><div class="children"><div class="content">There is no PageDown on my MacBook keyboard.</div><br/></div></div></div></div></div></div><div id="39324416" class="c"><input type="checkbox" id="c-39324416" checked=""/><div class="controls bullet"><span class="by">NooneAtAll3</span><span>|</span><a href="#39320723">prev</a><span>|</span><a href="#39323921">next</a><span>|</span><label class="collapse" for="c-39324416">[-]</label><label class="expand" for="c-39324416">[1 more]</label></div><br/><div class="children"><div class="content">Second paper&#x27;s lower bound has some strange &quot;log(^U_n)&quot; term. Can anyone explain what does that even mean? Logarithm of combinations, as in &quot;log(U!&#x2F;n!&#x2F;(U-n)!)&quot;?</div><br/></div></div><div id="39323921" class="c"><input type="checkbox" id="c-39323921" checked=""/><div class="controls bullet"><span class="by">cushpush</span><span>|</span><a href="#39324416">prev</a><span>|</span><a href="#39320907">next</a><span>|</span><label class="collapse" for="c-39323921">[-]</label><label class="expand" for="c-39323921">[1 more]</label></div><br/><div class="children"><div class="content">What is the ratio.  Tell me.</div><br/></div></div><div id="39320907" class="c"><input type="checkbox" id="c-39320907" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#39323921">prev</a><span>|</span><a href="#39322191">next</a><span>|</span><label class="collapse" for="c-39320907">[-]</label><label class="expand" for="c-39320907">[13 more]</label></div><br/><div class="children"><div class="content">This kind of thing is unfortunately not useful at any scale.<p>Real computer systems have performance that varies due to memory locality and size.<p>Locality because of things such as cache hierarchy and even the size of cache lines and memory pages.<p>Size because of physical implementation: larger memories are physically bigger and hence further away. The speed of light makes access to larger memories inescapably slower.<p>The best current hashtable implementations are all quite far from the purely theoretical computer science optimums, but are faster despite this because they take these factors into account.<p>Back when CPUs were simple and had no virtual memory or caches, there was a good correspondence between theoretical CS algorithms and their real implementations.<p>Now? Everything I see published in this space is basically pure maths with little or no practical utility. It’s still interesting, sure, but it’s a bit sad that the theorists have retreated into a virtual world to escape the messy details of our reality.</div><br/><div id="39323430" class="c"><input type="checkbox" id="c-39323430" checked=""/><div class="controls bullet"><span class="by">dgacmu</span><span>|</span><a href="#39320907">parent</a><span>|</span><a href="#39321483">next</a><span>|</span><label class="collapse" for="c-39323430">[-]</label><label class="expand" for="c-39323430">[1 more]</label></div><br/><div class="children"><div class="content">This is a bit pessimistic.<p>Most STOC papers don&#x27;t directly give an incredibly practical algorithm. But what they do give is insight, and that insight can often be used to improve the practical state of the art. And new bounds get you thinking; there&#x27;s power in just knowing we can do better. It&#x27;s like breaking the 4 minute mile -- a lot follows.<p>From a personal perspective: I quite like some of the techniques they use in the STOC&#x27;22 paper, and I have some ideas about how to turn that into a practical improvement for some of my data structures, which _do_ take into account CPU and cache because I&#x27;m that kind of geek. (To save you some googling, I&#x27;m one of the co-creators of the cuckoo filter, and of the techniques that underlie many practical implementations of cuckoo hashing, particularly those with concurrent access, and some other stuff. A lot of what I do in practical data structures-meets-systems comes from seeing what&#x27;s happening in theory and finding ways to apply it, even though in doing so I often give up on the optimality bounds, the insights that come from the theory folks are the basis of it.)<p>Also, note that there are a few flavors of theory. A lot of US theory is quite on the math side of things, with a few exceptions (Mitz, quoted in the article, being one of them). But there&#x27;s a very thriving applied algorithms community in the world, with a lot of energy in the area in both Europe and South America, and the stuff published in those venues -- again, often derived from or inspired by work such as this STOC paper -- is more immediately applicable.</div><br/></div></div><div id="39321483" class="c"><input type="checkbox" id="c-39321483" checked=""/><div class="controls bullet"><span class="by">trentnelson</span><span>|</span><a href="#39320907">parent</a><span>|</span><a href="#39323430">prev</a><span>|</span><a href="#39321500">next</a><span>|</span><label class="collapse" for="c-39321483">[-]</label><label class="expand" for="c-39321483">[3 more]</label></div><br/><div class="children"><div class="content">Hey, if you&#x27;re looking for a real-world pragmatic and performant implementation of a theoretically-cool algorithm, my <a href="https:&#x2F;&#x2F;github.com&#x2F;tpn&#x2F;perfecthash">https:&#x2F;&#x2F;github.com&#x2F;tpn&#x2F;perfecthash</a> project might fit the bill.<p>It&#x27;s geared to generating perfect hash tables with the fastest possible lookup&#x2F;index times (for 32-bit keys), for key sets in the &lt;=100,000 range.  (It scales well up to millions of keys, but the solving time takes a lot longer.)</div><br/><div id="39321801" class="c"><input type="checkbox" id="c-39321801" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#39320907">root</a><span>|</span><a href="#39321483">parent</a><span>|</span><a href="#39321500">next</a><span>|</span><label class="collapse" for="c-39321801">[-]</label><label class="expand" for="c-39321801">[2 more]</label></div><br/><div class="children"><div class="content">My hunch is that boomphf will outperform and also supports any key type: <a href="https:&#x2F;&#x2F;github.com&#x2F;10XGenomics&#x2F;rust-boomphf">https:&#x2F;&#x2F;github.com&#x2F;10XGenomics&#x2F;rust-boomphf</a><p>Construction is ~10m keys&#x2F;s on old hardware and uses very few bits per key.<p>Implementation is based on the bbhash paper: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1702.03154" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1702.03154</a></div><br/><div id="39322832" class="c"><input type="checkbox" id="c-39322832" checked=""/><div class="controls bullet"><span class="by">trentnelson</span><span>|</span><a href="#39320907">root</a><span>|</span><a href="#39321801">parent</a><span>|</span><a href="#39321500">next</a><span>|</span><label class="collapse" for="c-39322832">[-]</label><label class="expand" for="c-39322832">[1 more]</label></div><br/><div class="children"><div class="content">What’s its fastest index function look like in assembly?  My MultiplyShiftRX clocks in at like 5 cycles on x64 and 3 cycles on my M1.  Mine is optimized for offline table generation so construction speed isn’t really relevant for its primary use.</div><br/></div></div></div></div></div></div><div id="39321500" class="c"><input type="checkbox" id="c-39321500" checked=""/><div class="controls bullet"><span class="by">kevinventullo</span><span>|</span><a href="#39320907">parent</a><span>|</span><a href="#39321483">prev</a><span>|</span><a href="#39322085">next</a><span>|</span><label class="collapse" for="c-39321500">[-]</label><label class="expand" for="c-39321500">[3 more]</label></div><br/><div class="children"><div class="content">What is a theorist if not someone who works on interesting problems which have a tenuous-at-best relationship with reality?</div><br/><div id="39321695" class="c"><input type="checkbox" id="c-39321695" checked=""/><div class="controls bullet"><span class="by">zogrodea</span><span>|</span><a href="#39320907">root</a><span>|</span><a href="#39321500">parent</a><span>|</span><a href="#39322085">next</a><span>|</span><label class="collapse" for="c-39321695">[-]</label><label class="expand" for="c-39321695">[2 more]</label></div><br/><div class="children"><div class="content">Related sentiment to the above post: <a href="https:&#x2F;&#x2F;golem.ph.utexas.edu&#x2F;category&#x2F;2009&#x2F;09&#x2F;the_mathematical_vocation.html" rel="nofollow">https:&#x2F;&#x2F;golem.ph.utexas.edu&#x2F;category&#x2F;2009&#x2F;09&#x2F;the_mathematica...</a> .</div><br/><div id="39321919" class="c"><input type="checkbox" id="c-39321919" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#39320907">root</a><span>|</span><a href="#39321695">parent</a><span>|</span><a href="#39322085">next</a><span>|</span><label class="collapse" for="c-39321919">[-]</label><label class="expand" for="c-39321919">[1 more]</label></div><br/><div class="children"><div class="content">For reference, I enjoy (and have formally studied) both theoretical physics and pure theoretical mathematics. I get the &quot;value&quot; of both, even when the value is extremely abstract and removed from practical utility.<p>However.<p>Theoretical physics especially is grounded in reality. Sure, it has its frictionless cows and whatnot, but <i>generally</i> a connection with the real world is maintained.[1] In other words, it&#x27;s possible to take an idealised theory and then sprinkle the messy details on top, such as friction and air resistance.<p>What I&#x27;m seeing in computer science is different. Their theories are not the type that need a slight quantitative adjustment to match reality in the sense of adding a 1% extra fine-tuning factors, but they&#x27;re <i>qualitatively</i> wrong. They&#x27;re wrong not by constant factors or constant offsets, but big-O notation wrong. The equations have the wrong powers in them! Missing square roots or logarithms!<p>It&#x27;s as if the working engineers had switched from Newtonian to Relativistic Mechanics because we&#x27;ve colonised the Solar System, but all theoretical physicists are basically pretending that only Newtonian mechanics is worthy of study and that Lorentzian mechanics is just a fudge factor that can be ignored forever and ever. Even when we all live in space with multi-hour communication delays caused by the speed of light. <i>&quot;Just set that to zero and then...&quot;</i><p>[1] One notable exception to this that grinds my gears is a habit of publishing papers about results that only apply in non-real toy models, such as 2+2 dimensional spacetime, but not putting a big warning box before the abstract to warn journalists that nothing in the paper applies to our reality.</div><br/></div></div></div></div></div></div><div id="39322085" class="c"><input type="checkbox" id="c-39322085" checked=""/><div class="controls bullet"><span class="by">mabster</span><span>|</span><a href="#39320907">parent</a><span>|</span><a href="#39321500">prev</a><span>|</span><a href="#39321520">next</a><span>|</span><label class="collapse" for="c-39322085">[-]</label><label class="expand" for="c-39322085">[2 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t kept up in this space, but there was a bunch of papers under &quot;cache agnostic algorithms&quot; that were actually taking the concept of cache into account. The cache agnostic here being regardless of cache size, sort of treating cache in the O() sense.<p>You could still beat them by tweaking for your hardware. But for the first time there was research into why Q sort is faster than heap sort</div><br/><div id="39323707" class="c"><input type="checkbox" id="c-39323707" checked=""/><div class="controls bullet"><span class="by">randomizedalgs</span><span>|</span><a href="#39320907">root</a><span>|</span><a href="#39322085">parent</a><span>|</span><a href="#39321520">next</a><span>|</span><label class="collapse" for="c-39323707">[-]</label><label class="expand" for="c-39323707">[1 more]</label></div><br/><div class="children"><div class="content">I think these are more-often called &quot;cache oblivious&quot; algorithms</div><br/></div></div></div></div><div id="39321520" class="c"><input type="checkbox" id="c-39321520" checked=""/><div class="controls bullet"><span class="by">andersa</span><span>|</span><a href="#39320907">parent</a><span>|</span><a href="#39322085">prev</a><span>|</span><a href="#39322191">next</a><span>|</span><label class="collapse" for="c-39321520">[-]</label><label class="expand" for="c-39321520">[3 more]</label></div><br/><div class="children"><div class="content">Yep, it&#x27;s the same thing every time. &quot;We propose this new thing with optimal time complexity blah blah blah&quot; - don&#x27;t care. Show benchmark. What do you mean it&#x27;s slower than this basic but cache efficient hash table implemented in 100 lines of c++?</div><br/><div id="39321853" class="c"><input type="checkbox" id="c-39321853" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#39320907">root</a><span>|</span><a href="#39321520">parent</a><span>|</span><a href="#39322191">next</a><span>|</span><label class="collapse" for="c-39321853">[-]</label><label class="expand" for="c-39321853">[2 more]</label></div><br/><div class="children"><div class="content">This is a totally different result. It’s providing a proof of an algorithmic complexity. So let’s say you have 10^18 items, it’s likely you’d be using this even if certain algorithms are faster at smaller numbers. It’s also important to note the particularly interesting result is <i>how</i> the mathematicians proved that the earlier design was optimal (upper and lower bound). The technique is the valuable part as it adds to the mathematician tool kit of how to prove such results. This article is about math papers &#x2F; functional theoretic math, not about applies CS ideas.</div><br/><div id="39324147" class="c"><input type="checkbox" id="c-39324147" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#39320907">root</a><span>|</span><a href="#39321853">parent</a><span>|</span><a href="#39322191">next</a><span>|</span><label class="collapse" for="c-39324147">[-]</label><label class="expand" for="c-39324147">[1 more]</label></div><br/><div class="children"><div class="content">&gt; let’s say you have 10^18 items<p>You won&#x27;t.  Not on a single node.<p>&gt; The technique is the valuable part as it adds to the mathematician tool kit of how to prove such results.<p>Agreed.</div><br/></div></div></div></div></div></div></div></div><div id="39321530" class="c"><input type="checkbox" id="c-39321530" checked=""/><div class="controls bullet"><span class="by">sylware</span><span>|</span><a href="#39322191">prev</a><span>|</span><a href="#39322133">next</a><span>|</span><label class="collapse" for="c-39321530">[-]</label><label class="expand" for="c-39321530">[1 more]</label></div><br/><div class="children"><div class="content">optimal for which use cases?</div><br/></div></div><div id="39322133" class="c"><input type="checkbox" id="c-39322133" checked=""/><div class="controls bullet"><span class="by">andrewp123</span><span>|</span><a href="#39321530">prev</a><span>|</span><label class="collapse" for="c-39322133">[-]</label><label class="expand" for="c-39322133">[3 more]</label></div><br/><div class="children"><div class="content">In my opinion space limited solutions are not very interesting. Time complexity is always &gt;= space complexity, so finding optimal time solutions is always more interesting. And we already knew the optimal time complexity. I appreciate a theoretical time vs space relation, but it doesn’t seem applicable to other areas.</div><br/><div id="39322463" class="c"><input type="checkbox" id="c-39322463" checked=""/><div class="controls bullet"><span class="by">dkural</span><span>|</span><a href="#39322133">parent</a><span>|</span><a href="#39323717">next</a><span>|</span><label class="collapse" for="c-39322463">[-]</label><label class="expand" for="c-39322463">[1 more]</label></div><br/><div class="children"><div class="content">For achieving speed, often you&#x27;d like to keep your hash table in RAM. Space limits result in speed trade-offs.  This is very much a real-world concern when writing genome aligners. Hash-based mappers were at a disadvantage early on due to the memory constraints of most servers back around 2009-2012,  thus the more efficient Burrows-Wheeler transform based algorithms became popular, partly due to significantly reduced RAM needs - one could now align genomes on a laptop!</div><br/></div></div><div id="39323717" class="c"><input type="checkbox" id="c-39323717" checked=""/><div class="controls bullet"><span class="by">tdstein</span><span>|</span><a href="#39322133">parent</a><span>|</span><a href="#39322463">prev</a><span>|</span><label class="collapse" for="c-39323717">[-]</label><label class="expand" for="c-39323717">[1 more]</label></div><br/><div class="children"><div class="content">It’s a great thing that this report addresses both space and time!</div><br/></div></div></div></div></div></div></div></div></div></body></html>