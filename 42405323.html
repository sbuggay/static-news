<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1734339678010" as="style"/><link rel="stylesheet" href="styles.css?v=1734339678010"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090">Phi-4: Microsoft&#x27;s Newest Small Language Model Specializing in Complex Reasoning</a> <span class="domain">(<a href="https://techcommunity.microsoft.com">techcommunity.microsoft.com</a>)</span></div><div class="subtext"><span>lappa</span> | <span>85 comments</span></div><br/><div><div id="42426729" class="c"><input type="checkbox" id="c-42426729" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42426789">next</a><span>|</span><label class="collapse" for="c-42426729">[-]</label><label class="expand" for="c-42426729">[27 more]</label></div><br/><div class="children"><div class="content">The most interesting thing about this is the way it was trained using synthetic data, which is described in quite a bit of detail in the technical report: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2412.08905" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2412.08905</a><p>Microsoft haven&#x27;t officially released the weights yet but there are unofficial GGUFs up on Hugging Face already. I tried this one: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;matteogeniaccio&#x2F;phi-4&#x2F;tree&#x2F;main" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;matteogeniaccio&#x2F;phi-4&#x2F;tree&#x2F;main</a><p>I got it working with my LLM tool like this:<p><pre><code>  llm install llm-gguf
  llm gguf download-model https:&#x2F;&#x2F;huggingface.co&#x2F;matteogeniaccio&#x2F;phi-4&#x2F;resolve&#x2F;main&#x2F;phi-4-Q4_K_M.gguf
  llm chat -m gguf&#x2F;phi-4-Q4_K_M
</code></pre>
Here are some initial transcripts: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;0235fd9f8c7809d0ae078495dd630b67" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;0235fd9f8c7809d0ae078495dd630...</a><p>More of my notes on Phi-4 here: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Dec&#x2F;15&#x2F;phi-4-technical-report&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Dec&#x2F;15&#x2F;phi-4-technical-report...</a></div><br/><div id="42429083" class="c"><input type="checkbox" id="c-42429083" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#42426729">parent</a><span>|</span><a href="#42428413">next</a><span>|</span><label class="collapse" for="c-42429083">[-]</label><label class="expand" for="c-42429083">[1 more]</label></div><br/><div class="children"><div class="content">This &quot;draw pelican riding on bicycle&quot; is quite deep if you think about it.<p>Phi is all about synthetic training and prompt -&gt; svg -&gt; render -&gt; evaluate image -&gt; feedback loop feels like ideal fit for synthetic learning.<p>You can push it quite far with stuff like basic 2d physics etc with plotting scene after N seconds or optics&#x2F;rays, magnetic force etc.<p>SVG as LLM window to physical world.</div><br/></div></div><div id="42428413" class="c"><input type="checkbox" id="c-42428413" checked=""/><div class="controls bullet"><span class="by">algo_trader</span><span>|</span><a href="#42426729">parent</a><span>|</span><a href="#42429083">prev</a><span>|</span><a href="#42426805">next</a><span>|</span><label class="collapse" for="c-42428413">[-]</label><label class="expand" for="c-42428413">[1 more]</label></div><br/><div class="children"><div class="content">&gt; More of my notes on Phi-4 here: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Dec&#x2F;15&#x2F;phi-4-technical-report" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Dec&#x2F;15&#x2F;phi-4-technical-report</a>...<p>Nice. Thanks.<p>Do you think sampling the stack traces of millions of machines is a good dataset for improving code performance? Maybe sample android&#x2F;jvm bytecode.<p>Maybe a sort of novelty sampling to avoid re-sampling hot-path?</div><br/></div></div><div id="42426805" class="c"><input type="checkbox" id="c-42426805" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#42426729">parent</a><span>|</span><a href="#42428413">prev</a><span>|</span><a href="#42428155">next</a><span>|</span><label class="collapse" for="c-42426805">[-]</label><label class="expand" for="c-42426805">[3 more]</label></div><br/><div class="children"><div class="content">Wow, those responses are better than I expected. Part of me was expecting terrible responses since Phi-3 was amazing on paper too but terrible in practice.</div><br/><div id="42426946" class="c"><input type="checkbox" id="c-42426946" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42426805">parent</a><span>|</span><a href="#42428155">next</a><span>|</span><label class="collapse" for="c-42426946">[-]</label><label class="expand" for="c-42426946">[2 more]</label></div><br/><div class="children"><div class="content">One of the funniest tech subplots in recent memory.<p>TL;DR it was nigh-impossible to get it to emit the proper &quot;end of message&quot; token. (IMHO the chat training was too rushed). So all the local LLM apps tried silently hacking around it. The funny thing to me was <i>no one</i> would say it out loud. Field isn&#x27;t very consumer friendly, yet.</div><br/><div id="42429243" class="c"><input type="checkbox" id="c-42429243" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42426946">parent</a><span>|</span><a href="#42428155">next</a><span>|</span><label class="collapse" for="c-42429243">[-]</label><label class="expand" for="c-42429243">[1 more]</label></div><br/><div class="children"><div class="content">Speaking of, I wonder if and how many of the existing frontends, interfaces and support packages that generalize over multiple LLMs, and include Anthropic, actually know how to prompt it correctly. Seems like most developers missed the memo on <a href="https:&#x2F;&#x2F;docs.anthropic.com&#x2F;en&#x2F;docs&#x2F;build-with-claude&#x2F;prompt-engineering&#x2F;use-xml-tags" rel="nofollow">https:&#x2F;&#x2F;docs.anthropic.com&#x2F;en&#x2F;docs&#x2F;build-with-claude&#x2F;prompt-...</a>, and I regularly end up in situation in which I wish they gave more minute control on how the request is assembled (proprietary), and&#x2F;or am considering gutting the app&#x2F;library myself (OSS; looking at you, Aider), just to have file uploads, or tools, or whatever other smarts the app&#x2F;library does, encoded in a way that uses Claude to its full potential.<p>I sometimes wonder how many other model or vendor-specific improvements there are, that are missed by third-party tools despite being well-documented by the vendors.</div><br/></div></div></div></div></div></div><div id="42428155" class="c"><input type="checkbox" id="c-42428155" checked=""/><div class="controls bullet"><span class="by">fisherjeff</span><span>|</span><a href="#42426729">parent</a><span>|</span><a href="#42426805">prev</a><span>|</span><a href="#42426863">next</a><span>|</span><label class="collapse" for="c-42428155">[-]</label><label class="expand" for="c-42428155">[1 more]</label></div><br/><div class="children"><div class="content">Looks like someone’s <i>finally</i> caught up with The Hallmark Channel’s LLM performance</div><br/></div></div><div id="42426863" class="c"><input type="checkbox" id="c-42426863" checked=""/><div class="controls bullet"><span class="by">lifeisgood99</span><span>|</span><a href="#42426729">parent</a><span>|</span><a href="#42428155">prev</a><span>|</span><a href="#42427092">next</a><span>|</span><label class="collapse" for="c-42426863">[-]</label><label class="expand" for="c-42426863">[16 more]</label></div><br/><div class="children"><div class="content">The SVG created for the first prompt is valid but is a garbage image.</div><br/><div id="42427045" class="c"><input type="checkbox" id="c-42427045" checked=""/><div class="controls bullet"><span class="by">bentcorner</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42426863">parent</a><span>|</span><a href="#42427721">next</a><span>|</span><label class="collapse" for="c-42427045">[-]</label><label class="expand" for="c-42427045">[12 more]</label></div><br/><div class="children"><div class="content">In general I&#x27;ve had poor results with LLMs generating pictures using text instructions (in my case I&#x27;ve tried to get them to generate pictures using plots in KQL).  They work but the pictures are very very basic.<p>I&#x27;d be interested for any LLM emitting any kind of text-to-picture instructions to get results that are beyond a kindergartner-cardboard-cutout levels of art.</div><br/><div id="42427413" class="c"><input type="checkbox" id="c-42427413" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42427045">parent</a><span>|</span><a href="#42427058">next</a><span>|</span><label class="collapse" for="c-42427413">[-]</label><label class="expand" for="c-42427413">[1 more]</label></div><br/><div class="children"><div class="content">I do with Claude: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42351796#42355665">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42351796#42355665</a></div><br/></div></div><div id="42427058" class="c"><input type="checkbox" id="c-42427058" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42427045">parent</a><span>|</span><a href="#42427413">prev</a><span>|</span><a href="#42427721">next</a><span>|</span><label class="collapse" for="c-42427058">[-]</label><label class="expand" for="c-42427058">[10 more]</label></div><br/><div class="children"><div class="content">That&#x27;s why I use the SVG pelican riding a bicycle thing as a benchmark: it&#x27;s a deliberately absurd and extremely difficult task.</div><br/><div id="42427351" class="c"><input type="checkbox" id="c-42427351" checked=""/><div class="controls bullet"><span class="by">accrual</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42427058">parent</a><span>|</span><a href="#42427206">next</a><span>|</span><label class="collapse" for="c-42427351">[-]</label><label class="expand" for="c-42427351">[4 more]</label></div><br/><div class="children"><div class="content">Appreciate your rapid analysis of new models, Simon. Have any models you&#x27;ve tested performed well on the pelican SVG task?</div><br/><div id="42427479" class="c"><input type="checkbox" id="c-42427479" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42427351">parent</a><span>|</span><a href="#42427206">next</a><span>|</span><label class="collapse" for="c-42427479">[-]</label><label class="expand" for="c-42427479">[3 more]</label></div><br/><div class="children"><div class="content">gemini-exp-1206 is my new favorite: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Dec&#x2F;6&#x2F;gemini-exp-1206&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Dec&#x2F;6&#x2F;gemini-exp-1206&#x2F;</a><p>Claude 3.5 Sonnet is in second place: <a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;pelican-bicycle?tab=readme-ov-file#claude-35-sonnet-2024-06-20">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;pelican-bicycle?tab=readme-ov-file...</a></div><br/><div id="42429041" class="c"><input type="checkbox" id="c-42429041" checked=""/><div class="controls bullet"><span class="by">codedokode</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42427479">parent</a><span>|</span><a href="#42427702">next</a><span>|</span><label class="collapse" for="c-42429041">[-]</label><label class="expand" for="c-42429041">[1 more]</label></div><br/><div class="children"><div class="content">They probably trained it for this specific task (generating SVG images), right?</div><br/></div></div><div id="42427702" class="c"><input type="checkbox" id="c-42427702" checked=""/><div class="controls bullet"><span class="by">accrual</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42427479">parent</a><span>|</span><a href="#42429041">prev</a><span>|</span><a href="#42427206">next</a><span>|</span><label class="collapse" for="c-42427702">[-]</label><label class="expand" for="c-42427702">[1 more]</label></div><br/><div class="children"><div class="content">The Gemini result is quite impressive, thanks for sharing these!</div><br/></div></div></div></div></div></div><div id="42427206" class="c"><input type="checkbox" id="c-42427206" checked=""/><div class="controls bullet"><span class="by">Teever</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42427058">parent</a><span>|</span><a href="#42427351">prev</a><span>|</span><a href="#42427721">next</a><span>|</span><label class="collapse" for="c-42427206">[-]</label><label class="expand" for="c-42427206">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m really glad that I see someone else doing something similar.  I had the epiphany a while ago that if LLMs can interpret textual instructions to draw a picture and output the design in another textual format that this a strong indicator that they&#x27;re more than just stochastic parrots.<p>My personal test has been &quot;A horse eating apples next to a tree&quot; but the deliberate absurdity of your example is a much more useful test.<p>Do you know if this is a recognized technique that people use to study LLMs?</div><br/><div id="42427967" class="c"><input type="checkbox" id="c-42427967" checked=""/><div class="controls bullet"><span class="by">memhole</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42427206">parent</a><span>|</span><a href="#42427255">next</a><span>|</span><label class="collapse" for="c-42427967">[-]</label><label class="expand" for="c-42427967">[1 more]</label></div><br/><div class="children"><div class="content">Not sure if this counts. I recently went from description of a screenshot of graph to generate pandas code and plot from description. Conceptually it was accurate.<p>I don’t think it reflects any understanding. But to go from screenshot to conceptually accurate and working code was impressive.</div><br/></div></div><div id="42427255" class="c"><input type="checkbox" id="c-42427255" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42427206">parent</a><span>|</span><a href="#42427967">prev</a><span>|</span><a href="#42427721">next</a><span>|</span><label class="collapse" for="c-42427255">[-]</label><label class="expand" for="c-42427255">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen people using &quot;draw a unicorn using tikz&quot; <a href="https:&#x2F;&#x2F;adamkdean.co.uk&#x2F;posts&#x2F;gpt-unicorn-a-daily-exploration-of-gpt-4s-image-generation-capabilities" rel="nofollow">https:&#x2F;&#x2F;adamkdean.co.uk&#x2F;posts&#x2F;gpt-unicorn-a-daily-exploratio...</a></div><br/><div id="42427715" class="c"><input type="checkbox" id="c-42427715" checked=""/><div class="controls bullet"><span class="by">girvo</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42427255">parent</a><span>|</span><a href="#42427721">next</a><span>|</span><label class="collapse" for="c-42427715">[-]</label><label class="expand" for="c-42427715">[2 more]</label></div><br/><div class="children"><div class="content">That came, IIRC, from one of the OpenAI or Microsoft people (Sebastian Bubeck); it was recounted in an NPR podcast &quot;Greetings from Earth&quot;<p><a href="https:&#x2F;&#x2F;www.thisamericanlife.org&#x2F;803&#x2F;transcript" rel="nofollow">https:&#x2F;&#x2F;www.thisamericanlife.org&#x2F;803&#x2F;transcript</a></div><br/><div id="42428326" class="c"><input type="checkbox" id="c-42428326" checked=""/><div class="controls bullet"><span class="by">krackers</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42427715">parent</a><span>|</span><a href="#42427721">next</a><span>|</span><label class="collapse" for="c-42428326">[-]</label><label class="expand" for="c-42428326">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s in this presentation <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=qbIk7-JPB2c" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=qbIk7-JPB2c</a><p>The most significant part I took away is that when safety &quot;alignment&quot; was done the ability plummeted. So that really makes me wonder how much better these models would be if they weren&#x27;t lobotomized to prevent them from saying bad words.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42427721" class="c"><input type="checkbox" id="c-42427721" checked=""/><div class="controls bullet"><span class="by">chen_dev</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42426863">parent</a><span>|</span><a href="#42427045">prev</a><span>|</span><a href="#42426871">next</a><span>|</span><label class="collapse" for="c-42427721">[-]</label><label class="expand" for="c-42427721">[1 more]</label></div><br/><div class="children"><div class="content">Amazon Nova models:<p><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;uschen&#x2F;38fc65fa7e43f5765a584c6cd24e137f" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;uschen&#x2F;38fc65fa7e43f5765a584c6cd24e1...</a></div><br/></div></div><div id="42426871" class="c"><input type="checkbox" id="c-42426871" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42426863">parent</a><span>|</span><a href="#42427721">prev</a><span>|</span><a href="#42427039">next</a><span>|</span><label class="collapse" for="c-42426871">[-]</label><label class="expand" for="c-42426871">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, it didn&#x27;t do very well on that one. The best I&#x27;ve had from a local model there was from QwQ: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Nov&#x2F;27&#x2F;qwq&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Nov&#x2F;27&#x2F;qwq&#x2F;</a></div><br/></div></div><div id="42427039" class="c"><input type="checkbox" id="c-42427039" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42426863">parent</a><span>|</span><a href="#42426871">prev</a><span>|</span><a href="#42427092">next</a><span>|</span><label class="collapse" for="c-42427039">[-]</label><label class="expand" for="c-42427039">[1 more]</label></div><br/><div class="children"><div class="content">For context, pelican riding a bicycle: <a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;2nhm0XM" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;2nhm0XM</a><p>Copied SVG from gist into figma, added dark gray #444444 background, exported as PNG 1x.</div><br/></div></div></div></div><div id="42427092" class="c"><input type="checkbox" id="c-42427092" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#42426729">parent</a><span>|</span><a href="#42426863">prev</a><span>|</span><a href="#42426789">next</a><span>|</span><label class="collapse" for="c-42427092">[-]</label><label class="expand" for="c-42427092">[4 more]</label></div><br/><div class="children"><div class="content">&gt;Microsoft haven&#x27;t officially released the weights<p>Thought it was official just not on huggingface but rather whatever azure competitor thing they&#x27;re pushing?</div><br/><div id="42427193" class="c"><input type="checkbox" id="c-42427193" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42427092">parent</a><span>|</span><a href="#42426789">next</a><span>|</span><label class="collapse" for="c-42427193">[-]</label><label class="expand" for="c-42427193">[3 more]</label></div><br/><div class="children"><div class="content">I found their AI Foundry thing so hard to figure out I couldn&#x27;t tell if they had released weights (as opposed to a way of running it via an API).<p>Since there are GGUFs now so someone must have released some weights somewhere.</div><br/><div id="42428270" class="c"><input type="checkbox" id="c-42428270" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42427193">parent</a><span>|</span><a href="#42427355">next</a><span>|</span><label class="collapse" for="c-42428270">[-]</label><label class="expand" for="c-42428270">[1 more]</label></div><br/><div class="children"><div class="content">The safetensors are in the phi-4 folder of the very repo you linked in your OP.</div><br/></div></div><div id="42427355" class="c"><input type="checkbox" id="c-42427355" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#42426729">root</a><span>|</span><a href="#42427193">parent</a><span>|</span><a href="#42428270">prev</a><span>|</span><a href="#42426789">next</a><span>|</span><label class="collapse" for="c-42427355">[-]</label><label class="expand" for="c-42427355">[1 more]</label></div><br/><div class="children"><div class="content">Yeah the weights were on there apparently.<p>Planned week delay between release on their own platform and hf<p>But much like you I decided I can be patient &#x2F; use the ggufs</div><br/></div></div></div></div></div></div></div></div><div id="42426789" class="c"><input type="checkbox" id="c-42426789" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#42426729">prev</a><span>|</span><a href="#42406047">next</a><span>|</span><label class="collapse" for="c-42426789">[-]</label><label class="expand" for="c-42426789">[4 more]</label></div><br/><div class="children"><div class="content">For prompt adherence it still fails on tasks that Gemma2 27b nails every time. I haven&#x27;t been impressed with any of the Phi family of models. The large context is very nice, though Gemma2 plays very well with self-extend.</div><br/><div id="42428994" class="c"><input type="checkbox" id="c-42428994" checked=""/><div class="controls bullet"><span class="by">impossiblefork</span><span>|</span><a href="#42426789">parent</a><span>|</span><a href="#42427036">next</a><span>|</span><label class="collapse" for="c-42428994">[-]</label><label class="expand" for="c-42428994">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a much smaller model though.<p>I think the point is more the demonstration that such a small model can have such good performance than any actual usefulness.</div><br/></div></div><div id="42427036" class="c"><input type="checkbox" id="c-42427036" checked=""/><div class="controls bullet"><span class="by">jacoblambda</span><span>|</span><a href="#42426789">parent</a><span>|</span><a href="#42428994">prev</a><span>|</span><a href="#42406047">next</a><span>|</span><label class="collapse" for="c-42427036">[-]</label><label class="expand" for="c-42427036">[2 more]</label></div><br/><div class="children"><div class="content">Yeah they mention this in the weaknesses section.<p>&gt; While phi-4 demonstrates relatively strong performance in answering questions and performing reasoning tasks, it is less proficient at rigorously following detailed instructions, particularly those involving specific formatting requirements.</div><br/><div id="42427269" class="c"><input type="checkbox" id="c-42427269" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#42426789">root</a><span>|</span><a href="#42427036">parent</a><span>|</span><a href="#42406047">next</a><span>|</span><label class="collapse" for="c-42427269">[-]</label><label class="expand" for="c-42427269">[1 more]</label></div><br/><div class="children"><div class="content">Ah good catch, I am forever cursed in my preference for snake over camel.</div><br/></div></div></div></div></div></div><div id="42406047" class="c"><input type="checkbox" id="c-42406047" checked=""/><div class="controls bullet"><span class="by">xeckr</span><span>|</span><a href="#42426789">prev</a><span>|</span><a href="#42428049">next</a><span>|</span><label class="collapse" for="c-42406047">[-]</label><label class="expand" for="c-42406047">[39 more]</label></div><br/><div class="children"><div class="content">Looks like it punches way above its weight(s).<p>How far are we from running a GPT-3&#x2F;GPT-4 level LLM on regular consumer hardware, like a MacBook Pro?</div><br/><div id="42407365" class="c"><input type="checkbox" id="c-42407365" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#42406047">parent</a><span>|</span><a href="#42428219">next</a><span>|</span><label class="collapse" for="c-42407365">[-]</label><label class="expand" for="c-42407365">[24 more]</label></div><br/><div class="children"><div class="content">We’re already past that point!  MacBooks can easily run models exceeding GPT-3.5, such as Llama 3.1 8B, Qwen 2.5 8B, or Gemma 2 9B. These models run at very comfortable speeds on Apple Silicon. And they are distinctly more capable and less prone to hallucination than GPT-3.5 was.<p>Llama 3.3 70B and Qwen 2.5 72B are certainly comparable to GPT-4, and they will run on MacBook Pros with at least 64GB of RAM. However, I have an M3 Max and I can’t say that  models of this size run at comfortable speeds. They’re a bit sluggish.</div><br/><div id="42418763" class="c"><input type="checkbox" id="c-42418763" checked=""/><div class="controls bullet"><span class="by">noman-land</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42407365">parent</a><span>|</span><a href="#42428138">next</a><span>|</span><label class="collapse" for="c-42418763">[-]</label><label class="expand" for="c-42418763">[10 more]</label></div><br/><div class="children"><div class="content">The coolness of local LLMs is THE only reason I am sadly eyeing upgrading from M1 64GB to M4&#x2F;5 128+GB.</div><br/><div id="42429241" class="c"><input type="checkbox" id="c-42429241" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42418763">parent</a><span>|</span><a href="#42423039">next</a><span>|</span><label class="collapse" for="c-42429241">[-]</label><label class="expand" for="c-42429241">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m waiting for next gen hardware. All the companies are aiming for AI acceleration.</div><br/></div></div><div id="42423039" class="c"><input type="checkbox" id="c-42423039" checked=""/><div class="controls bullet"><span class="by">Terretta</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42418763">parent</a><span>|</span><a href="#42429241">prev</a><span>|</span><a href="#42428929">next</a><span>|</span><label class="collapse" for="c-42423039">[-]</label><label class="expand" for="c-42423039">[6 more]</label></div><br/><div class="children"><div class="content">Compare performance on various Macs here as it gets updated:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;discussions&#x2F;4167">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;discussions&#x2F;4167</a><p>OMM, Llama 3.3 70B runs at ~7 text generation tokens per second on Macbook Pro Max 128GB, while generating GPT-4 feeling text with more in depth responses and fewer bullets.  Llama 3.3 70B also doesn&#x27;t fight the system prompt, it leans in.<p>Consider e.g. LM Studio (0.3.5 or newer) for a Metal (MLX) centered UI, include MLX in your search term when downloading models.<p>Also, do not scrimp on the storage. At 60GB - 100GB per model, it takes a day of experimentation to use 2.5TB of storage in your model cache.  And remember to exclude that path from your TimeMachine backups.</div><br/><div id="42424931" class="c"><input type="checkbox" id="c-42424931" checked=""/><div class="controls bullet"><span class="by">noman-land</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42423039">parent</a><span>|</span><a href="#42426972">next</a><span>|</span><label class="collapse" for="c-42424931">[-]</label><label class="expand" for="c-42424931">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for all the tips! I&#x27;d probably go 128GB 8TB because of masochism. Curious, what makes so many of the M4s in the red currently.</div><br/></div></div><div id="42426972" class="c"><input type="checkbox" id="c-42426972" checked=""/><div class="controls bullet"><span class="by">ant6n</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42423039">parent</a><span>|</span><a href="#42424931">prev</a><span>|</span><a href="#42428929">next</a><span>|</span><label class="collapse" for="c-42426972">[-]</label><label class="expand" for="c-42426972">[4 more]</label></div><br/><div class="children"><div class="content">What if you have a Macbook Air with 16GB (the bechmarks dont seem to show memory).</div><br/><div id="42427783" class="c"><input type="checkbox" id="c-42427783" checked=""/><div class="controls bullet"><span class="by">evilduck</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42426972">parent</a><span>|</span><a href="#42427227">next</a><span>|</span><label class="collapse" for="c-42427783">[-]</label><label class="expand" for="c-42427783">[1 more]</label></div><br/><div class="children"><div class="content">8B models with larger contexts, or even 9-14B parameter models quantized.<p>Qwen2.5 Coder 14B at a 4 bit quantization could run but you will need to be diligent about what else you have in memory at the same time.</div><br/></div></div><div id="42427227" class="c"><input type="checkbox" id="c-42427227" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42426972">parent</a><span>|</span><a href="#42427783">prev</a><span>|</span><a href="#42427555">next</a><span>|</span><label class="collapse" for="c-42427227">[-]</label><label class="expand" for="c-42427227">[1 more]</label></div><br/><div class="children"><div class="content">You could definitely run an 8B model on that, and some of those are getting very capable now.<p>The problem is that often you can&#x27;t run anything else. I&#x27;ve had trouble running larger models in 64GB when I&#x27;ve had a bunch of Firefox and VS Code tabs open at the same time.</div><br/></div></div><div id="42427555" class="c"><input type="checkbox" id="c-42427555" checked=""/><div class="controls bullet"><span class="by">chris_st</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42426972">parent</a><span>|</span><a href="#42427227">prev</a><span>|</span><a href="#42428929">next</a><span>|</span><label class="collapse" for="c-42427555">[-]</label><label class="expand" for="c-42427555">[1 more]</label></div><br/><div class="children"><div class="content">I have a M2 Air with 24GB, and have successfully run some 12B models such as mistral-nemo. Had other stuff going as well, but it&#x27;s best to give it as much of the machine as possible.</div><br/></div></div></div></div></div></div><div id="42428929" class="c"><input type="checkbox" id="c-42428929" checked=""/><div class="controls bullet"><span class="by">stkdump</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42418763">parent</a><span>|</span><a href="#42423039">prev</a><span>|</span><a href="#42429027">next</a><span>|</span><label class="collapse" for="c-42428929">[-]</label><label class="expand" for="c-42428929">[1 more]</label></div><br/><div class="children"><div class="content">I bought an old used desktop computer, a used 3090, and upgraded the power supply, all for around 900€. Didn&#x27;t assemble it all yet. But it will be able to comfortably run 30B parameter models with 30-40 T&#x2F;s. The M4 Max can do ~10 T&#x2F;s, which is not great once you really want to rely on it for your productivity.<p>Yes, it is not &quot;local&quot; as I will have to use the internet when not at home. But it will also not drain the battery very quickly when using it, which I suspect would happen to a Macbook Pro running such models. Also 70B models are out of reach of my setup, but I think they are painfully slow on Mac hardware.</div><br/></div></div><div id="42429027" class="c"><input type="checkbox" id="c-42429027" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42418763">parent</a><span>|</span><a href="#42428929">prev</a><span>|</span><a href="#42428138">next</a><span>|</span><label class="collapse" for="c-42429027">[-]</label><label class="expand" for="c-42429027">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m returning my 96GB m2 max. It can run unquantized llama 3.3 70B but tokens per second is slow as molasses and still I couldn&#x27;t find any use for it, just kept going back to perplexity when I actually needed to find an answer to something.</div><br/></div></div></div></div><div id="42428138" class="c"><input type="checkbox" id="c-42428138" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42407365">parent</a><span>|</span><a href="#42418763">prev</a><span>|</span><a href="#42427405">next</a><span>|</span><label class="collapse" for="c-42428138">[-]</label><label class="expand" for="c-42428138">[5 more]</label></div><br/><div class="children"><div class="content">&gt;MacBooks can easily run models exceeding GPT-3.5, such as Llama 3.1 8B, Qwen 2.5 8B, or Gemma 2 9B.<p>If only those models supported anything other than English</div><br/><div id="42428161" class="c"><input type="checkbox" id="c-42428161" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42428138">parent</a><span>|</span><a href="#42428342">next</a><span>|</span><label class="collapse" for="c-42428161">[-]</label><label class="expand" for="c-42428161">[2 more]</label></div><br/><div class="children"><div class="content">Llama 3.1 8B advertises itself as multilingual.<p>All of the Qwen models are basically fluent in both English and Chinese.</div><br/><div id="42428306" class="c"><input type="checkbox" id="c-42428306" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42428161">parent</a><span>|</span><a href="#42428342">next</a><span>|</span><label class="collapse" for="c-42428306">[-]</label><label class="expand" for="c-42428306">[1 more]</label></div><br/><div class="children"><div class="content">Llama 8B is multilingual on paper, but the quality is very bad compared to English. It generally understands grammar, and you can understand what it&#x27;s trying to say, but the choice of words is very off most of the time, often complete gibberish. If you can imagine the output of an undertrained model, this is it. Meanwhile GPT3.5 had far better output that you could use in production.</div><br/></div></div></div></div><div id="42428342" class="c"><input type="checkbox" id="c-42428342" checked=""/><div class="controls bullet"><span class="by">barrell</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42428138">parent</a><span>|</span><a href="#42428161">prev</a><span>|</span><a href="#42428636">next</a><span>|</span><label class="collapse" for="c-42428342">[-]</label><label class="expand" for="c-42428342">[1 more]</label></div><br/><div class="children"><div class="content">Cohere just announced Command R7B. I haven’t tried it yet but their larger models are the best multilingual models I’ve used</div><br/></div></div><div id="42428636" class="c"><input type="checkbox" id="c-42428636" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42428138">parent</a><span>|</span><a href="#42428342">prev</a><span>|</span><a href="#42427405">next</a><span>|</span><label class="collapse" for="c-42428636">[-]</label><label class="expand" for="c-42428636">[1 more]</label></div><br/><div class="children"><div class="content">Is subtext to this uncensored Chinese support?</div><br/></div></div></div></div><div id="42427282" class="c"><input type="checkbox" id="c-42427282" checked=""/><div class="controls bullet"><span class="by">noodletheworld</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42407365">parent</a><span>|</span><a href="#42427405">prev</a><span>|</span><a href="#42428219">next</a><span>|</span><label class="collapse" for="c-42427282">[-]</label><label class="expand" for="c-42427282">[7 more]</label></div><br/><div class="children"><div class="content">&gt; MacBooks can easily run models exceeding GPT-3.5, such as Llama 3.1 8B, Qwen 2.5 8B, or Gemma 2 9B.<p>gpt-3.5-turbo is generally considered to be about 20B params. An 8B model does not exceed it.<p>&gt; Llama 3.3 70B and Qwen 2.5 72B are certainly comparable to GPT-4<p>I&#x27;m skeptical; the llama 3.1 405B model is the only comparable model I&#x27;ve used, and it&#x27;s <i>significantly</i> larger than the 70B models you can run locally.<p>The 405B model takes a bit of effort to run (1), and your average mac book doesn&#x27;t ship with 128GB of ram, but <i>technically</i> yes, if you get the max spec M4 Max + 128GB unified ram, you can run it.<p>...and it&#x27;s similar, but (see again, AI leader boards) not as good as what you can get from gpt-4o.<p>&gt; How far are we from running a GPT-3&#x2F;GPT-4 level LLM on regular consumer hardware, like a MacBook Pro?<p>Is a $8000 MBP regular consumer hardware? If you don&#x27;t think so, then the answer is probably no.<p>Lots of research into good smaller models is going on right now, but <i>right now</i>, they are not comparable to the larger models in terms of quality-of-output.<p>[1] - <a href="https:&#x2F;&#x2F;medium.com&#x2F;@aleksej.gudkov&#x2F;how-to-run-llama-405b-a-comprehensive-guide-caeda184e997" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@aleksej.gudkov&#x2F;how-to-run-llama-405b-a-c...</a></div><br/><div id="42428791" class="c"><input type="checkbox" id="c-42428791" checked=""/><div class="controls bullet"><span class="by">PhilippGille</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42427282">parent</a><span>|</span><a href="#42427852">next</a><span>|</span><label class="collapse" for="c-42428791">[-]</label><label class="expand" for="c-42428791">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; Llama 3.3 70B and Qwen 2.5 72B are certainly comparable to GPT-4
&gt;
&gt;I&#x27;m skeptical; the llama 3.1 405B model is the only comparable model I&#x27;ve used, and it&#x27;s significantly larger than the 70B models you can run locally.<p>Every new Llama generation achieved to beat larger models of the previous generation with smaller ones.<p>Check Kagi&#x27;s LLM benchmark: <a href="https:&#x2F;&#x2F;help.kagi.com&#x2F;kagi&#x2F;ai&#x2F;llm-benchmark.html" rel="nofollow">https:&#x2F;&#x2F;help.kagi.com&#x2F;kagi&#x2F;ai&#x2F;llm-benchmark.html</a><p>Check the HN thread around the 3.3 70b release: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42341388">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42341388</a><p>And their own benchmark results in their model card: <a href="https:&#x2F;&#x2F;github.com&#x2F;meta-llama&#x2F;llama-models&#x2F;blob&#x2F;main&#x2F;models%2Fllama3_3%2FMODEL_CARD.md">https:&#x2F;&#x2F;github.com&#x2F;meta-llama&#x2F;llama-models&#x2F;blob&#x2F;main&#x2F;models%...</a><p>Groq&#x27;s post about it: <a href="https:&#x2F;&#x2F;groq.com&#x2F;a-new-scaling-paradigm-metas-llama-3-3-70b-challenges-death-of-scaling-law&#x2F;" rel="nofollow">https:&#x2F;&#x2F;groq.com&#x2F;a-new-scaling-paradigm-metas-llama-3-3-70b-...</a><p>Etc</div><br/></div></div><div id="42427852" class="c"><input type="checkbox" id="c-42427852" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42427282">parent</a><span>|</span><a href="#42428791">prev</a><span>|</span><a href="#42428905">next</a><span>|</span><label class="collapse" for="c-42427852">[-]</label><label class="expand" for="c-42427852">[1 more]</label></div><br/><div class="children"><div class="content">&gt; gpt-3.5-turbo is generally considered to be about 20B params. An 8B model does not exceed it.<p>The industry has moved on from the old Chinchilla scaling regime, and with it the conviction that LLM capability is mainly dictated by parameter count. OpenAI didn&#x27;t disclose how much pretraining they did for 3.5-Turbo, but GPT 3 was trained on 300 billion tokens of text data. In contrast, Llama 3.1 was trained on <i>15 trillion</i> tokens of data.<p>Objectively, Llama 3.1 8B and other small models have exceeded GPT-3.5-Turbo in benchmarks and human preference scores.<p>&gt; Is a $8000 MBP regular consumer hardware?<p>As user `bloomingkales` notes down below, a $499 Mac Mini can run 8B parameter models. An $8,000 expenditure is not required.</div><br/></div></div><div id="42428905" class="c"><input type="checkbox" id="c-42428905" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42427282">parent</a><span>|</span><a href="#42427852">prev</a><span>|</span><a href="#42427682">next</a><span>|</span><label class="collapse" for="c-42428905">[-]</label><label class="expand" for="c-42428905">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Is a $8000 MBP regular consumer hardware? If you don&#x27;t think so, then the answer is probably no.<p>The very first Apple McIntosh was not far from that price at its release.  Adjusted for inflation of course.</div><br/></div></div><div id="42427682" class="c"><input type="checkbox" id="c-42427682" checked=""/><div class="controls bullet"><span class="by">runako</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42427282">parent</a><span>|</span><a href="#42428905">prev</a><span>|</span><a href="#42428312">next</a><span>|</span><label class="collapse" for="c-42427682">[-]</label><label class="expand" for="c-42427682">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Is a $8000 MBP regular consumer hardware?<p>May want to double-check your specs. 16&quot; w&#x2F;128GB &amp; 2TB is $5,400.</div><br/></div></div><div id="42428312" class="c"><input type="checkbox" id="c-42428312" checked=""/><div class="controls bullet"><span class="by">tosh</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42427282">parent</a><span>|</span><a href="#42427682">prev</a><span>|</span><a href="#42428219">next</a><span>|</span><label class="collapse" for="c-42428312">[-]</label><label class="expand" for="c-42428312">[2 more]</label></div><br/><div class="children"><div class="content">A Mac with 16GB RAM can run qwen 7b, gemma 9b and similar models that are somewhere between GPT3.5 and GPT4.<p>Quite impressive.</div><br/><div id="42429044" class="c"><input type="checkbox" id="c-42429044" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42428312">parent</a><span>|</span><a href="#42428219">next</a><span>|</span><label class="collapse" for="c-42429044">[-]</label><label class="expand" for="c-42429044">[1 more]</label></div><br/><div class="children"><div class="content">on what metric?<p>Why would OpenAI bother serving GPT4 if customers would be just as happy with a tiny 9B model?</div><br/></div></div></div></div></div></div></div></div><div id="42428219" class="c"><input type="checkbox" id="c-42428219" checked=""/><div class="controls bullet"><span class="by">ActorNightly</span><span>|</span><a href="#42406047">parent</a><span>|</span><a href="#42407365">prev</a><span>|</span><a href="#42427138">next</a><span>|</span><label class="collapse" for="c-42428219">[-]</label><label class="expand" for="c-42428219">[1 more]</label></div><br/><div class="children"><div class="content">Why would you want to though? You already can get free access to large LLMs and nobody is doing anything groundbreaking with them.</div><br/></div></div><div id="42427138" class="c"><input type="checkbox" id="c-42427138" checked=""/><div class="controls bullet"><span class="by">bloomingkales</span><span>|</span><a href="#42406047">parent</a><span>|</span><a href="#42428219">prev</a><span>|</span><a href="#42427040">next</a><span>|</span><label class="collapse" for="c-42427138">[-]</label><label class="expand" for="c-42427138">[1 more]</label></div><br/><div class="children"><div class="content">M4 Mac mini 16gb for $500. It&#x27;s literally an inferencing block (small too, fits in my palm). I feel like the whole world needs one.</div><br/></div></div><div id="42406078" class="c"><input type="checkbox" id="c-42406078" checked=""/><div class="controls bullet"><span class="by">lappa</span><span>|</span><a href="#42406047">parent</a><span>|</span><a href="#42427040">prev</a><span>|</span><a href="#42426796">next</a><span>|</span><label class="collapse" for="c-42406078">[-]</label><label class="expand" for="c-42406078">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s easy to argue that Llama-3.3 8B performs better than GPT-3.5. Compare their benchmarks, and try the two side-by-side.<p>Phi-4 is yet another <i>step</i> towards a small, open, GPT-4 level model. I think we&#x27;re getting quite close.<p>Check the benchmarks comparing to GPT-4o on the first page of their technical report if you haven&#x27;t already <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2412.08905" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2412.08905</a></div><br/><div id="42426775" class="c"><input type="checkbox" id="c-42426775" checked=""/><div class="controls bullet"><span class="by">vulcanash999</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42406078">parent</a><span>|</span><a href="#42426796">next</a><span>|</span><label class="collapse" for="c-42426775">[-]</label><label class="expand" for="c-42426775">[1 more]</label></div><br/><div class="children"><div class="content">Did you mean Llama-3.1 8B? Llama 3.3 currently only has a 70B model as far as I’m aware.</div><br/></div></div></div></div><div id="42426796" class="c"><input type="checkbox" id="c-42426796" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42406047">parent</a><span>|</span><a href="#42406078">prev</a><span>|</span><a href="#42426745">next</a><span>|</span><label class="collapse" for="c-42426796">[-]</label><label class="expand" for="c-42426796">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re there, Llama 3.1 8B beats Gemini Advanced for $20&#x2F;month. Telosnex with llama 3.1 8b GGUF from bartowski. <a href="https:&#x2F;&#x2F;telosnex.com&#x2F;compare&#x2F;" rel="nofollow">https:&#x2F;&#x2F;telosnex.com&#x2F;compare&#x2F;</a> (How!? tl;dr: I assume Google is sandbagging and hasn&#x27;t updated the underlying Gemini)</div><br/></div></div><div id="42426745" class="c"><input type="checkbox" id="c-42426745" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42406047">parent</a><span>|</span><a href="#42426796">prev</a><span>|</span><a href="#42428049">next</a><span>|</span><label class="collapse" for="c-42426745">[-]</label><label class="expand" for="c-42426745">[8 more]</label></div><br/><div class="children"><div class="content">We&#x27;re there. Llama 3.3 70B is GPT-4 level and runs on my 64GB MacBook Pro: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Dec&#x2F;9&#x2F;llama-33-70b&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Dec&#x2F;9&#x2F;llama-33-70b&#x2F;</a><p>The Qwen2 models that run on my MacBook Pro are GPT-4 level too.</div><br/><div id="42426800" class="c"><input type="checkbox" id="c-42426800" checked=""/><div class="controls bullet"><span class="by">n144q</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42426745">parent</a><span>|</span><a href="#42426795">next</a><span>|</span><label class="collapse" for="c-42426800">[-]</label><label class="expand" for="c-42426800">[5 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t call 64GB MacBook Pro &quot;regular consumer hardware&quot;.</div><br/><div id="42426987" class="c"><input type="checkbox" id="c-42426987" checked=""/><div class="controls bullet"><span class="by">russellbeattie</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42426800">parent</a><span>|</span><a href="#42426864">next</a><span>|</span><label class="collapse" for="c-42426987">[-]</label><label class="expand" for="c-42426987">[2 more]</label></div><br/><div class="children"><div class="content">I have to disagree. I understand it&#x27;s very expensive, but it&#x27;s still a consumer product available to anyone with a credit card.<p>The comparison is between something you can buy off the shelf like a powerful Mac, vs something powered by a Grace Hopper CPU from Nvidia, which would require both lots of money and a business relationship.<p>Honestly, people pay $4k for nice TVs, refrigerators and even couches, and those are not professional tools by any stretch. If LLMs needed a $50k Mac Pro with maxed out everything, that might be different. But anything that&#x27;s a laptop is definitely regular consumer hardware.</div><br/><div id="42427875" class="c"><input type="checkbox" id="c-42427875" checked=""/><div class="controls bullet"><span class="by">PhunkyPhil</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42426987">parent</a><span>|</span><a href="#42426864">next</a><span>|</span><label class="collapse" for="c-42427875">[-]</label><label class="expand" for="c-42427875">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s definitely been plenty sources of hardware capable of running LLMs out there for a while, Mac or not. A couple 4090s or P40s will run 3.1 70b. Or, since price isn&#x27;t a limit, there are other easier &amp; more powerful options like a [tinybox](<a href="https:&#x2F;&#x2F;tinygrad.org&#x2F;#tinybox:~:text=won%27t%20be%20considered.-,tinybox,-(now%20shipping)" rel="nofollow">https:&#x2F;&#x2F;tinygrad.org&#x2F;#tinybox:~:text=won%27t%20be%20consider...</a>).</div><br/></div></div></div></div><div id="42426864" class="c"><input type="checkbox" id="c-42426864" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42426800">parent</a><span>|</span><a href="#42426987">prev</a><span>|</span><a href="#42426795">next</a><span>|</span><label class="collapse" for="c-42426864">[-]</label><label class="expand" for="c-42426864">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, a computer which starts at $3900 is really stretching that classification. Plus if you&#x27;re that serious about local LLMs then you&#x27;d probably want the even bigger RAM option, which adds another $800...</div><br/><div id="42427824" class="c"><input type="checkbox" id="c-42427824" checked=""/><div class="controls bullet"><span class="by">evilduck</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42426864">parent</a><span>|</span><a href="#42426795">next</a><span>|</span><label class="collapse" for="c-42427824">[-]</label><label class="expand" for="c-42427824">[1 more]</label></div><br/><div class="children"><div class="content">An optioned up minivan is also expensive but doesn’t cost as much as a firetruck. It’s expensive but still very much consumer hardware. A 3x4090 rig is more expensive and still consumer hardware. An H100 is not, you can buy like 7 of these optioned up MBP for a single H100.</div><br/></div></div></div></div></div></div><div id="42426795" class="c"><input type="checkbox" id="c-42426795" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42426745">parent</a><span>|</span><a href="#42426800">prev</a><span>|</span><a href="#42428049">next</a><span>|</span><label class="collapse" for="c-42426795">[-]</label><label class="expand" for="c-42426795">[2 more]</label></div><br/><div class="children"><div class="content">Saying these models are at GPT-4 level is setting anyone who doesn&#x27;t place special value on the local aspect up for disappointment.<p>Some people do place value on running locally, and I&#x27;m not against then for it, but realistically no 70B class model has the amount of general knowledge or understanding of nuance as any recent GPT-4 checkpoint.<p>That being said these models are still very strong compared to what we had a year ago and capable of useful work</div><br/><div id="42426814" class="c"><input type="checkbox" id="c-42426814" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42406047">root</a><span>|</span><a href="#42426795">parent</a><span>|</span><a href="#42428049">next</a><span>|</span><label class="collapse" for="c-42426814">[-]</label><label class="expand" for="c-42426814">[1 more]</label></div><br/><div class="children"><div class="content">I said GPT-4, not GPT-4o. I&#x27;m talking about a model that feels equivalent to the GPT-4 we were using in March of 2023.</div><br/></div></div></div></div></div></div></div></div><div id="42428049" class="c"><input type="checkbox" id="c-42428049" checked=""/><div class="controls bullet"><span class="by">excerionsforte</span><span>|</span><a href="#42406047">prev</a><span>|</span><a href="#42427933">next</a><span>|</span><label class="collapse" for="c-42428049">[-]</label><label class="expand" for="c-42428049">[1 more]</label></div><br/><div class="children"><div class="content">Looks like someone converted it for Ollama use already: <a href="https:&#x2F;&#x2F;ollama.com&#x2F;vanilj&#x2F;Phi-4">https:&#x2F;&#x2F;ollama.com&#x2F;vanilj&#x2F;Phi-4</a></div><br/></div></div><div id="42427933" class="c"><input type="checkbox" id="c-42427933" checked=""/><div class="controls bullet"><span class="by">jsight</span><span>|</span><a href="#42428049">prev</a><span>|</span><a href="#42427295">next</a><span>|</span><label class="collapse" for="c-42427933">[-]</label><label class="expand" for="c-42427933">[1 more]</label></div><br/><div class="children"><div class="content">I really like the ~3B param version of phi-3. It wasn&#x27;t very powerful and overused memory, but was surprisingly strong for such a small model.<p>I&#x27;m not sure how I can be impressed by a 14B Phi-4. That isn&#x27;t really small any more, and I doubt it will be significantly better than llama 3 or Mistral at this point. Maybe that will be wrong, but I don&#x27;t have high hopes.</div><br/></div></div><div id="42427295" class="c"><input type="checkbox" id="c-42427295" checked=""/><div class="controls bullet"><span class="by">travisgriggs</span><span>|</span><a href="#42427933">prev</a><span>|</span><a href="#42429110">next</a><span>|</span><label class="collapse" for="c-42427295">[-]</label><label class="expand" for="c-42427295">[9 more]</label></div><br/><div class="children"><div class="content">Where have I been? What is a “small” language model? Wikipedia just talks about LLMs. Is this a sort of spectrum? Are there medium language models? Or is it a more nuanced classifier?</div><br/><div id="42427319" class="c"><input type="checkbox" id="c-42427319" checked=""/><div class="controls bullet"><span class="by">narag</span><span>|</span><a href="#42427295">parent</a><span>|</span><a href="#42427389">next</a><span>|</span><label class="collapse" for="c-42427319">[-]</label><label class="expand" for="c-42427319">[5 more]</label></div><br/><div class="children"><div class="content">7B vs 70B parameters... I think. The small ones fit in the memory of consumer grade cards. That&#x27;s what I more or less know (waiting for my new computer to arrive this week)</div><br/><div id="42427410" class="c"><input type="checkbox" id="c-42427410" checked=""/><div class="controls bullet"><span class="by">agnishom</span><span>|</span><a href="#42427295">root</a><span>|</span><a href="#42427319">parent</a><span>|</span><a href="#42427389">next</a><span>|</span><label class="collapse" for="c-42427410">[-]</label><label class="expand" for="c-42427410">[4 more]</label></div><br/><div class="children"><div class="content">How many parameters did ChatGPT have in Dec 2022 when it first broke into mainstream news?</div><br/><div id="42427460" class="c"><input type="checkbox" id="c-42427460" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#42427295">root</a><span>|</span><a href="#42427410">parent</a><span>|</span><a href="#42427454">next</a><span>|</span><label class="collapse" for="c-42427460">[-]</label><label class="expand" for="c-42427460">[2 more]</label></div><br/><div class="children"><div class="content">GPT-3 had 175B, and the original ChatGPT was probably just a GPT-3 finetune (although they called it gpt-3.5, so it <i>could</i> have been different). However, it was severely undertrained. Llama-3.1-8B is better in most ways than the original ChatGPT; a well-trained ~70B usually feels GPT-4-level. The latest Llama release, llama-3.3-70b, goes toe-to-toe even with much larger models (albeit is bad at coding, like all Llama models so far; it&#x27;s not inherent to the size, since Qwen is good, so I&#x27;m hoping the Llama 4 series is trained on more coding tokens).</div><br/><div id="42428850" class="c"><input type="checkbox" id="c-42428850" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#42427295">root</a><span>|</span><a href="#42427460">parent</a><span>|</span><a href="#42427454">next</a><span>|</span><label class="collapse" for="c-42428850">[-]</label><label class="expand" for="c-42428850">[1 more]</label></div><br/><div class="children"><div class="content">&gt; However, it was severely undertrained<p>by modern standards. at the time, it was trained according to neural scaling laws oai believed to hold.</div><br/></div></div></div></div><div id="42427454" class="c"><input type="checkbox" id="c-42427454" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42427295">root</a><span>|</span><a href="#42427410">parent</a><span>|</span><a href="#42427460">prev</a><span>|</span><a href="#42427389">next</a><span>|</span><label class="collapse" for="c-42427454">[-]</label><label class="expand" for="c-42427454">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that&#x27;s ever been shared, but it&#x27;s predecessor GPT-3 Da Vinci was 175B.<p>One of the most exciting trends of the past year has been models getting dramatically smaller while maintaining similar levels of capability.</div><br/></div></div></div></div></div></div><div id="42427389" class="c"><input type="checkbox" id="c-42427389" checked=""/><div class="controls bullet"><span class="by">dboreham</span><span>|</span><a href="#42427295">parent</a><span>|</span><a href="#42427319">prev</a><span>|</span><a href="#42427796">next</a><span>|</span><label class="collapse" for="c-42427389">[-]</label><label class="expand" for="c-42427389">[1 more]</label></div><br/><div class="children"><div class="content">There are all sizes of models from a few GB to hundreds of GB. Small presumably means small enough to run on end-user hardware.</div><br/></div></div><div id="42427796" class="c"><input type="checkbox" id="c-42427796" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#42427295">parent</a><span>|</span><a href="#42427389">prev</a><span>|</span><a href="#42427668">next</a><span>|</span><label class="collapse" for="c-42427796">[-]</label><label class="expand" for="c-42427796">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a marketing term for the idea that quality over quantity in training data will lead to smaller models that work as well as larger models.</div><br/></div></div><div id="42427668" class="c"><input type="checkbox" id="c-42427668" checked=""/><div class="controls bullet"><span class="by">hagen_dogs</span><span>|</span><a href="#42427295">parent</a><span>|</span><a href="#42427796">prev</a><span>|</span><a href="#42429110">next</a><span>|</span><label class="collapse" for="c-42427668">[-]</label><label class="expand" for="c-42427668">[1 more]</label></div><br/><div class="children"><div class="content">I <i>think</i> it came from this paper, TinyStories (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.07759" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.07759</a>).  iirc this was also the inspiration for the Phi family of models. The essential point (of the TinyStories paper), &quot;if we train a model on text meant for 3-4 year olds, since that&#x27;s much simpler shouldn&#x27;t we need fewer parameters?&quot; Which is correct. In the original they have a model that&#x27;s 32 Million parameters and they compare it GPT-2 (1.5 Billion parameters) and the 32M model does much better. Microsoft has been interesed in this because &quot;lower models == less resource usage&quot; which means they can run on consumer devices. You can easily run TinyStories from your phone, which is presumably what Microsoft wants to do too.</div><br/></div></div></div></div><div id="42429110" class="c"><input type="checkbox" id="c-42429110" checked=""/><div class="controls bullet"><span class="by">mupuff1234</span><span>|</span><a href="#42427295">prev</a><span>|</span><a href="#42428405">next</a><span>|</span><label class="collapse" for="c-42429110">[-]</label><label class="expand" for="c-42429110">[1 more]</label></div><br/><div class="children"><div class="content">So we moved from &quot;reasoning&quot; to &quot;complex reasoning&quot;.<p>I wonder what will be next month&#x27;s buzzphrase.</div><br/></div></div><div id="42428405" class="c"><input type="checkbox" id="c-42428405" checked=""/><div class="controls bullet"><span class="by">ai_biden</span><span>|</span><a href="#42429110">prev</a><span>|</span><a href="#42405423">next</a><span>|</span><label class="collapse" for="c-42428405">[-]</label><label class="expand" for="c-42428405">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not too excited by Phi-4 benchmark results - It is#BenchmarkInflation.<p>Microsoft Research just dropped Phi-4 14B, an open-source model that’s turning heads. It claims to rival Llama 3.3 70B with a fraction of the parameters — 5x fewer, to be exact.<p>What’s the secret? Synthetic data. 
-&gt; Higher quality, Less misinformation, More diversity<p>But the Phi models always have great benchmark scores, but they always disappoint me in real-world use cases.<p>Phi series is famous for to be trained on benchmarks.<p>I tried again with the hashtag#phi4 through Ollama - but its not satisfactory.<p>To me, at the moment - IFEval is the most important llm benchmark.<p>But look the smart business strategy of Microsoft:<p>have unlimited access to gpt-4
 the input prompt it to generate 30B tokens
 train a 1B parameter model
 call it phi-1
 show benchmarks beating models 10x the size
 never release the data
 never detail how to generate the data( this time they told in very high level) 
 claim victory over small models</div><br/></div></div><div id="42405423" class="c"><input type="checkbox" id="c-42405423" checked=""/><div class="controls bullet"><span class="by">parmesean</span><span>|</span><a href="#42428405">prev</a><span>|</span><label class="collapse" for="c-42405423">[-]</label><label class="expand" for="c-42405423">[1 more]</label></div><br/><div class="children"><div class="content">13.8 epochs of the benchmarks?</div><br/></div></div></div></div></div></div></div></body></html>