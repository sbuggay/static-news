<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708419661424" as="style"/><link rel="stylesheet" href="styles.css?v=1708419661424"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://groq.com/">Groq runs Mixtral 8x7B-32k with 500 T/s</a> <span class="domain">(<a href="https://groq.com">groq.com</a>)</span></div><div class="subtext"><span>tin7in</span> | <span>296 comments</span></div><br/><div><div id="39432502" class="c"><input type="checkbox" id="c-39432502" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#39431989">next</a><span>|</span><label class="collapse" for="c-39432502">[-]</label><label class="expand" for="c-39432502">[39 more]</label></div><br/><div class="children"><div class="content">I just want to say that this is one of the most impressive tech demos I’ve ever seen in my life, and I love that it’s truly an open demo that anyone can try without even signing up for an account or anything like that. It’s surreal to see the thing spitting out tokens at such a crazy rate when you’re used to watching them generate at one less than one fifth that speed. I’m surprised you guys haven’t been swallowed up by Microsoft, Apple, or Google already for a huge premium.</div><br/><div id="39432727" class="c"><input type="checkbox" id="c-39432727" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39432502">parent</a><span>|</span><a href="#39435049">next</a><span>|</span><label class="collapse" for="c-39432727">[-]</label><label class="expand" for="c-39432727">[12 more]</label></div><br/><div class="children"><div class="content">Really glad you like it!  We&#x27;ve been working hard on it.</div><br/><div id="39436121" class="c"><input type="checkbox" id="c-39436121" checked=""/><div class="controls bullet"><span class="by">jonplackett</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39432727">parent</a><span>|</span><a href="#39432753">next</a><span>|</span><label class="collapse" for="c-39436121">[-]</label><label class="expand" for="c-39436121">[4 more]</label></div><br/><div class="children"><div class="content">Is this useful for training as well as running a model. Or is this approach specifically for running an already-trained model faster?</div><br/><div id="39436188" class="c"><input type="checkbox" id="c-39436188" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39436121">parent</a><span>|</span><a href="#39436169">next</a><span>|</span><label class="collapse" for="c-39436188">[-]</label><label class="expand" for="c-39436188">[1 more]</label></div><br/><div class="children"><div class="content">Currently graphics processors work well for training.  Language processors (LPUs) excel at inference.</div><br/></div></div><div id="39436169" class="c"><input type="checkbox" id="c-39436169" checked=""/><div class="controls bullet"><span class="by">frozenport</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39436121">parent</a><span>|</span><a href="#39436188">prev</a><span>|</span><a href="#39432753">next</a><span>|</span><label class="collapse" for="c-39436169">[-]</label><label class="expand" for="c-39436169">[2 more]</label></div><br/><div class="children"><div class="content">In principle, training is basically the same as running inference but iteratively, in practice training would use a different software stack.</div><br/><div id="39437290" class="c"><input type="checkbox" id="c-39437290" checked=""/><div class="controls bullet"><span class="by">robrenaud</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39436169">parent</a><span>|</span><a href="#39432753">next</a><span>|</span><label class="collapse" for="c-39437290">[-]</label><label class="expand" for="c-39437290">[1 more]</label></div><br/><div class="children"><div class="content">Training requires a lot more memory to keep gradients + gradient stats for the optimizer, and needs higher precision weights for the optimization. It&#x27;s also much more parallelizable. But inference is kind of a subroutine of training.</div><br/></div></div></div></div></div></div><div id="39432753" class="c"><input type="checkbox" id="c-39432753" checked=""/><div class="controls bullet"><span class="by">lokimedes</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39432727">parent</a><span>|</span><a href="#39436121">prev</a><span>|</span><a href="#39435049">next</a><span>|</span><label class="collapse" for="c-39432753">[-]</label><label class="expand" for="c-39432753">[7 more]</label></div><br/><div class="children"><div class="content">The speed part or the being swallowed part?</div><br/><div id="39432889" class="c"><input type="checkbox" id="c-39432889" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39432753">parent</a><span>|</span><a href="#39435049">next</a><span>|</span><label class="collapse" for="c-39432889">[-]</label><label class="expand" for="c-39432889">[6 more]</label></div><br/><div class="children"><div class="content">The speed part. We&#x27;re not interested in being swallowed.  The aim is to be bigger than Nvidia in three years :)</div><br/><div id="39436022" class="c"><input type="checkbox" id="c-39436022" checked=""/><div class="controls bullet"><span class="by">jonplackett</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39432889">parent</a><span>|</span><a href="#39432948">next</a><span>|</span><label class="collapse" for="c-39436022">[-]</label><label class="expand" for="c-39436022">[1 more]</label></div><br/><div class="children"><div class="content">Is Sam going to give you some of his $7T to help with that?</div><br/></div></div><div id="39432948" class="c"><input type="checkbox" id="c-39432948" checked=""/><div class="controls bullet"><span class="by">dazzaji</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39432889">parent</a><span>|</span><a href="#39436022">prev</a><span>|</span><a href="#39435210">next</a><span>|</span><label class="collapse" for="c-39432948">[-]</label><label class="expand" for="c-39432948">[1 more]</label></div><br/><div class="children"><div class="content">Go for it!</div><br/></div></div><div id="39435210" class="c"><input type="checkbox" id="c-39435210" checked=""/><div class="controls bullet"><span class="by">nurettin</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39432889">parent</a><span>|</span><a href="#39432948">prev</a><span>|</span><a href="#39436006">next</a><span>|</span><label class="collapse" for="c-39435210">[-]</label><label class="expand" for="c-39435210">[2 more]</label></div><br/><div class="children"><div class="content">Can you warn us pre-IPO?</div><br/><div id="39435233" class="c"><input type="checkbox" id="c-39435233" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39435210">parent</a><span>|</span><a href="#39436006">next</a><span>|</span><label class="collapse" for="c-39435233">[-]</label><label class="expand" for="c-39435233">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure you&#x27;ll hear all about our IPO on HN :) :)</div><br/></div></div></div></div><div id="39436006" class="c"><input type="checkbox" id="c-39436006" checked=""/><div class="controls bullet"><span class="by">FpUser</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39432889">parent</a><span>|</span><a href="#39435210">prev</a><span>|</span><a href="#39435049">next</a><span>|</span><label class="collapse" for="c-39436006">[-]</label><label class="expand" for="c-39436006">[1 more]</label></div><br/><div class="children"><div class="content">Yes please</div><br/></div></div></div></div></div></div></div></div><div id="39435049" class="c"><input type="checkbox" id="c-39435049" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#39432502">parent</a><span>|</span><a href="#39432727">prev</a><span>|</span><a href="#39435583">next</a><span>|</span><label class="collapse" for="c-39435049">[-]</label><label class="expand" for="c-39435049">[2 more]</label></div><br/><div class="children"><div class="content">Perplexity Labs also has an open demo of Mixtral 8x7b although it&#x27;s nowhere near as fast as this.<p><a href="https:&#x2F;&#x2F;labs.perplexity.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;labs.perplexity.ai&#x2F;</a></div><br/><div id="39436718" class="c"><input type="checkbox" id="c-39436718" checked=""/><div class="controls bullet"><span class="by">vitorgrs</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39435049">parent</a><span>|</span><a href="#39435583">next</a><span>|</span><label class="collapse" for="c-39436718">[-]</label><label class="expand" for="c-39436718">[1 more]</label></div><br/><div class="children"><div class="content">Poe has a bunch of them, including Groq as well!</div><br/></div></div></div></div><div id="39435583" class="c"><input type="checkbox" id="c-39435583" checked=""/><div class="controls bullet"><span class="by">larodi</span><span>|</span><a href="#39432502">parent</a><span>|</span><a href="#39435049">prev</a><span>|</span><a href="#39433118">next</a><span>|</span><label class="collapse" for="c-39435583">[-]</label><label class="expand" for="c-39435583">[1 more]</label></div><br/><div class="children"><div class="content">why sell? it would be much more delightful to beat them on their own game?</div><br/></div></div><div id="39433118" class="c"><input type="checkbox" id="c-39433118" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#39432502">parent</a><span>|</span><a href="#39435583">prev</a><span>|</span><a href="#39432980">next</a><span>|</span><label class="collapse" for="c-39433118">[-]</label><label class="expand" for="c-39433118">[22 more]</label></div><br/><div class="children"><div class="content">I have it on good authority Apple was very closing to acquiring Groq</div><br/><div id="39433241" class="c"><input type="checkbox" id="c-39433241" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39433118">parent</a><span>|</span><a href="#39432980">next</a><span>|</span><label class="collapse" for="c-39433241">[-]</label><label class="expand" for="c-39433241">[21 more]</label></div><br/><div class="children"><div class="content">If this is true, expect a call from the SEC...</div><br/><div id="39434139" class="c"><input type="checkbox" id="c-39434139" checked=""/><div class="controls bullet"><span class="by">sheepscreek</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39433241">parent</a><span>|</span><a href="#39435774">next</a><span>|</span><label class="collapse" for="c-39434139">[-]</label><label class="expand" for="c-39434139">[1 more]</label></div><br/><div class="children"><div class="content">TIL that SEC has authority over private company dealings wrt sale of shares[1].<p>[1] <a href="https:&#x2F;&#x2F;www.sec.gov&#x2F;education&#x2F;capitalraising&#x2F;building-blocks&#x2F;sec-have-do-my-private-company" rel="nofollow">https:&#x2F;&#x2F;www.sec.gov&#x2F;education&#x2F;capitalraising&#x2F;building-blocks...</a></div><br/></div></div><div id="39435774" class="c"><input type="checkbox" id="c-39435774" checked=""/><div class="controls bullet"><span class="by">belter</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39433241">parent</a><span>|</span><a href="#39434139">prev</a><span>|</span><a href="#39433628">next</a><span>|</span><label class="collapse" for="c-39435774">[-]</label><label class="expand" for="c-39435774">[1 more]</label></div><br/><div class="children"><div class="content">Not if poster is in a crashing plane...</div><br/></div></div><div id="39433295" class="c"><input type="checkbox" id="c-39433295" checked=""/><div class="controls bullet"><span class="by">317070</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39433241">parent</a><span>|</span><a href="#39433628">prev</a><span>|</span><a href="#39432980">next</a><span>|</span><label class="collapse" for="c-39433295">[-]</label><label class="expand" for="c-39433295">[17 more]</label></div><br/><div class="children"><div class="content">Even if it isn&#x27;t true.<p>Disclosing inside information is illegal, _even if it is false and fabricated_, if it leads to personal gains.</div><br/><div id="39433459" class="c"><input type="checkbox" id="c-39433459" checked=""/><div class="controls bullet"><span class="by">KRAKRISMOTT</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39433295">parent</a><span>|</span><a href="#39432980">next</a><span>|</span><label class="collapse" for="c-39433459">[-]</label><label class="expand" for="c-39433459">[16 more]</label></div><br/><div class="children"><div class="content">You have to prove the OP had personal gains. If he&#x27;s just a troll, it will be difficult.</div><br/><div id="39433569" class="c"><input type="checkbox" id="c-39433569" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39433459">parent</a><span>|</span><a href="#39432980">next</a><span>|</span><label class="collapse" for="c-39433569">[-]</label><label class="expand" for="c-39433569">[15 more]</label></div><br/><div class="children"><div class="content">You also have to be an insider.<p>If I go to a bar, and overhear a pair of Googlers discussing something secret and overhear it, I can:<p>1) Trade on it.<p>2) Talk about it.<p>Because I&#x27;m not an insider. On the other hand, if I&#x27;m sleeping with the CEO, I become an insider.<p>Not a lawyer. Above is not legal advice. Just a comment that the line is much more complex, and talking about a potential acquisition is usually okay (if you&#x27;re not under NDA).</div><br/><div id="39438068" class="c"><input type="checkbox" id="c-39438068" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39433569">parent</a><span>|</span><a href="#39434821">next</a><span>|</span><label class="collapse" for="c-39438068">[-]</label><label class="expand" for="c-39438068">[2 more]</label></div><br/><div class="children"><div class="content">just so you know no one&#x27;s ever been taken to court for discussing the law, it doesn&#x27;t matter that you&#x27;re not a lawyer, it&#x27;s basically a meme</div><br/><div id="39438125" class="c"><input type="checkbox" id="c-39438125" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39438068">parent</a><span>|</span><a href="#39434821">next</a><span>|</span><label class="collapse" for="c-39438125">[-]</label><label class="expand" for="c-39438125">[1 more]</label></div><br/><div class="children"><div class="content">Are you a lawyer?</div><br/></div></div></div></div><div id="39434821" class="c"><input type="checkbox" id="c-39434821" checked=""/><div class="controls bullet"><span class="by">throwawayurlife</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39433569">parent</a><span>|</span><a href="#39438068">prev</a><span>|</span><a href="#39432980">next</a><span>|</span><label class="collapse" for="c-39434821">[-]</label><label class="expand" for="c-39434821">[12 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t matter if you overheard it at a bar or if you&#x27;re just some HN commenter posting completely incorrect legal advice; the law prohibits trading on material nonpublic information.<p>I would pay a lot to see you try your ridiculous legal hokey-pokey on how to define an &quot;insider.&quot;</div><br/><div id="39436379" class="c"><input type="checkbox" id="c-39436379" checked=""/><div class="controls bullet"><span class="by">jahewson</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39434821">parent</a><span>|</span><a href="#39436497">next</a><span>|</span><label class="collapse" for="c-39436379">[-]</label><label class="expand" for="c-39436379">[1 more]</label></div><br/><div class="children"><div class="content">No, a bar is a public place so this counts as a public disclosure. The people having the conversation would be in trouble with the SEC for making a disclosure in this manner.</div><br/></div></div><div id="39436497" class="c"><input type="checkbox" id="c-39436497" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39434821">parent</a><span>|</span><a href="#39436379">prev</a><span>|</span><a href="#39436138">next</a><span>|</span><label class="collapse" for="c-39436497">[-]</label><label class="expand" for="c-39436497">[2 more]</label></div><br/><div class="children"><div class="content">&gt; the law prohibits trading on material nonpublic information.<p>Isn’t it public information the moment it’s said audibly in a public space?</div><br/><div id="39436762" class="c"><input type="checkbox" id="c-39436762" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39436497">parent</a><span>|</span><a href="#39436138">next</a><span>|</span><label class="collapse" for="c-39436762">[-]</label><label class="expand" for="c-39436762">[1 more]</label></div><br/><div class="children"><div class="content">No. It&#x27;s not. However, as pointed out elsewhere, you can trade on many types of non-public information. Indeed, hedge funds engage in all sorts of surveillance in order to get non-public material information to trade on which gives them a proprietary edge.<p>You just can&#x27;t trade on insider information.<p>That&#x27;s a very complex legal line.</div><br/></div></div></div></div><div id="39436138" class="c"><input type="checkbox" id="c-39436138" checked=""/><div class="controls bullet"><span class="by">windexh8er</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39434821">parent</a><span>|</span><a href="#39436497">prev</a><span>|</span><a href="#39435325">next</a><span>|</span><label class="collapse" for="c-39436138">[-]</label><label class="expand" for="c-39436138">[1 more]</label></div><br/><div class="children"><div class="content">Feel free to share some legal precedence where this situation has fared poorly for someone who &quot;overheard it at a bar&quot;.<p>It&#x27;d also be a good time to watch you lose all that money on your hokey-pokey assumption.</div><br/></div></div><div id="39435325" class="c"><input type="checkbox" id="c-39435325" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39434821">parent</a><span>|</span><a href="#39436138">prev</a><span>|</span><a href="#39434956">next</a><span>|</span><label class="collapse" for="c-39435325">[-]</label><label class="expand" for="c-39435325">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re an idiot.<p><a href="https:&#x2F;&#x2F;www.kiplinger.com&#x2F;article&#x2F;investing&#x2F;t052-c008-s001-would-you-be-guilty-of-insider-trading.html" rel="nofollow">https:&#x2F;&#x2F;www.kiplinger.com&#x2F;article&#x2F;investing&#x2F;t052-c008-s001-w...</a><p>Case #1.</div><br/><div id="39438259" class="c"><input type="checkbox" id="c-39438259" checked=""/><div class="controls bullet"><span class="by">hnfong</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39435325">parent</a><span>|</span><a href="#39434956">next</a><span>|</span><label class="collapse" for="c-39438259">[-]</label><label class="expand" for="c-39438259">[1 more]</label></div><br/><div class="children"><div class="content">Unless you earn enough money to retain good lawyers and are prepared to get into complicated legal troubles, getting sued isn&#x27;t a great outcome even if you win.<p>The prudent thing to do is to stay away from <i>anything</i> that might make you become a target of investigation, unless the gains outweigh the risk by a significant margin.</div><br/></div></div></div></div><div id="39434956" class="c"><input type="checkbox" id="c-39434956" checked=""/><div class="controls bullet"><span class="by">programmarchy</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39434821">parent</a><span>|</span><a href="#39435325">prev</a><span>|</span><a href="#39434959">next</a><span>|</span><label class="collapse" for="c-39434956">[-]</label><label class="expand" for="c-39434956">[2 more]</label></div><br/><div class="children"><div class="content">If you did hear it in a bar, could you tweet it out before your trade, so the information is made public?</div><br/><div id="39436440" class="c"><input type="checkbox" id="c-39436440" checked=""/><div class="controls bullet"><span class="by">jahewson</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39434956">parent</a><span>|</span><a href="#39434959">next</a><span>|</span><label class="collapse" for="c-39436440">[-]</label><label class="expand" for="c-39436440">[1 more]</label></div><br/><div class="children"><div class="content">If you hear it in a bar it’s already public.</div><br/></div></div></div></div><div id="39434959" class="c"><input type="checkbox" id="c-39434959" checked=""/><div class="controls bullet"><span class="by">sofixa</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39434821">parent</a><span>|</span><a href="#39434956">prev</a><span>|</span><a href="#39432980">next</a><span>|</span><label class="collapse" for="c-39434959">[-]</label><label class="expand" for="c-39434959">[3 more]</label></div><br/><div class="children"><div class="content">Had insider trading training, and yes, that&#x27;s the gist of it. If you know or presume that the information is material (makes a difference) and not public, it&#x27;s illegal to act on it.</div><br/><div id="39435339" class="c"><input type="checkbox" id="c-39435339" checked=""/><div class="controls bullet"><span class="by">tripletao</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39434959">parent</a><span>|</span><a href="#39435341">next</a><span>|</span><label class="collapse" for="c-39435339">[-]</label><label class="expand" for="c-39435339">[1 more]</label></div><br/><div class="children"><div class="content">Roughly, it&#x27;s illegal only if you have some duty not to trade on it. If you acquired the information without misappropriating it (like overhearing it from strangers in a normal public bar), then you&#x27;re free to trade.<p><a href="https:&#x2F;&#x2F;corpgov.law.harvard.edu&#x2F;2017&#x2F;01&#x2F;18&#x2F;insider-trading-law-after-salman&#x2F;" rel="nofollow">https:&#x2F;&#x2F;corpgov.law.harvard.edu&#x2F;2017&#x2F;01&#x2F;18&#x2F;insider-trading-l...</a><p>There&#x27;s no reason for normal corporate training to discuss that element, because an employee who trades their employer&#x27;s stock based on MNPI has near-certainly misappropriated it. The question of whether a non-employee has misappropriated information is much more complex, though.</div><br/></div></div><div id="39435341" class="c"><input type="checkbox" id="c-39435341" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#39432502">root</a><span>|</span><a href="#39434959">parent</a><span>|</span><a href="#39435339">prev</a><span>|</span><a href="#39432980">next</a><span>|</span><label class="collapse" for="c-39435341">[-]</label><label class="expand" for="c-39435341">[1 more]</label></div><br/><div class="children"><div class="content">Training is designed to protect the corporation, not to provide accurate lega ladvice. That&#x27;s true of most corporate trainings, for that matter, be that bribes &#x2F; corruption, harassment, discrimination, or whatnot. Corporations want employees very far from the line.<p>That&#x27;s the right way to run them.<p>If you want more nuance, talk to a lawyer or read case law.<p>Generally, insider trading requires something along the lines of a fiduciary duty to keep the information secret, albeit a very weak one. I&#x27;m not going to slice that line, but you see references in-thread.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39432980" class="c"><input type="checkbox" id="c-39432980" checked=""/><div class="controls bullet"><span class="by">timomaxgalvin</span><span>|</span><a href="#39432502">parent</a><span>|</span><a href="#39433118">prev</a><span>|</span><a href="#39431989">next</a><span>|</span><label class="collapse" for="c-39432980">[-]</label><label class="expand" for="c-39432980">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but the responses are very poor compared to MS tools.</div><br/></div></div></div></div><div id="39431989" class="c"><input type="checkbox" id="c-39431989" checked=""/><div class="controls bullet"><span class="by">treesciencebot</span><span>|</span><a href="#39432502">prev</a><span>|</span><a href="#39431752">next</a><span>|</span><label class="collapse" for="c-39431989">[-]</label><label class="expand" for="c-39431989">[64 more]</label></div><br/><div class="children"><div class="content">The main problem with the Groq LPUs is, they don&#x27;t have any HBM on them at all. Just a miniscule (230 MiB) [0] amount of ultra-fast SRAM (20x faster than HBM3, just to be clear). Which means you need ~256 LPUs (4 full server racks of compute, each unit on the rack contains 8x LPUs and there are 8x of those units on a single rack) just to serve a single model [1] where as you can get a single H200 (1&#x2F;256 of the server rack density) and serve these models reasonably well.<p>It might work well if you have a single model with lots of customers, but as soon as you need more than a single model and a lot of finetunes&#x2F;high rank LoRAs etc., these won&#x27;t be usable. Or for any on-prem deployment since the main advantage is consolidating people to use the same model, together.<p>[0]: <a href="https:&#x2F;&#x2F;wow.groq.com&#x2F;groqcard-accelerator&#x2F;" rel="nofollow">https:&#x2F;&#x2F;wow.groq.com&#x2F;groqcard-accelerator&#x2F;</a><p>[1]: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;tomjaguarpaw&#x2F;status&#x2F;1759615563586744334" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;tomjaguarpaw&#x2F;status&#x2F;1759615563586744334</a></div><br/><div id="39432163" class="c"><input type="checkbox" id="c-39432163" checked=""/><div class="controls bullet"><span class="by">matanyal</span><span>|</span><a href="#39431989">parent</a><span>|</span><a href="#39432384">next</a><span>|</span><label class="collapse" for="c-39432163">[-]</label><label class="expand" for="c-39432163">[26 more]</label></div><br/><div class="children"><div class="content">Groq Engineer here, I&#x27;m not seeing why being able to scale compute outside of a single card&#x2F;node is somehow a problem. My preferred analogy is to a car factory: Yes, you could build a car with say only one or two drills, but a modern automated factory has hundreds of drills! With a single drill, you could probably build all sorts of cars, but a factory assembly line is only able to make specific cars in that configuration. Does that mean that factories are inefficient?<p>You also say that H200&#x27;s work reasonably well, and that&#x27;s reasonable (but debatable) for synchronous, human interaction use cases. Show me a 30b+ parameter model doing RAG as part of a conversation with voice responses in less than a second, running on Nvidia.</div><br/><div id="39432294" class="c"><input type="checkbox" id="c-39432294" checked=""/><div class="controls bullet"><span class="by">pbalcer</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432163">parent</a><span>|</span><a href="#39434382">next</a><span>|</span><label class="collapse" for="c-39432294">[-]</label><label class="expand" for="c-39432294">[11 more]</label></div><br/><div class="children"><div class="content">Just curious, how does this work out in terms of TCO (even assuming the price of a Groq LPU is 0$)? What you say makes sense, but I&#x27;m wondering how you strike a balance between massive horizontal scaling vs vertical scaling. Sometimes (quite often in my experience) having a few beefy servers is much simpler&#x2F;cheaper&#x2F;faster than scaling horizontally across many small nodes.<p>Or I got this completely wrong, and your solution enables use-cases that are simply unattainable on mainstream (Nvidia&#x2F;AMD) hardware, making TCO argument less relevant?</div><br/><div id="39432310" class="c"><input type="checkbox" id="c-39432310" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432294">parent</a><span>|</span><a href="#39434382">next</a><span>|</span><label class="collapse" for="c-39432310">[-]</label><label class="expand" for="c-39432310">[10 more]</label></div><br/><div class="children"><div class="content">We&#x27;re providing by far the lowest latency LLM engine on the planet.  You can&#x27;t reduce  latency by scaling horizontally.</div><br/><div id="39432750" class="c"><input type="checkbox" id="c-39432750" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432310">parent</a><span>|</span><a href="#39434382">next</a><span>|</span><label class="collapse" for="c-39432750">[-]</label><label class="expand" for="c-39432750">[9 more]</label></div><br/><div class="children"><div class="content">Distributed, shared memory machines used to do exactly that in HPC space. They were a NUMA alternative. It works if the processing plus high-speed interconnect are collectively faster than the request rate. The 8x setups with NVLink are kind of like that model.<p>You may have meant that nobody has a stack that uses clustering or DSM with low-latency interconnects. If so, then that might be worth developing given prior results in other low-latency domains.</div><br/><div id="39432912" class="c"><input type="checkbox" id="c-39432912" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432750">parent</a><span>|</span><a href="#39434382">next</a><span>|</span><label class="collapse" for="c-39432912">[-]</label><label class="expand" for="c-39432912">[8 more]</label></div><br/><div class="children"><div class="content">I think existing players will have trouble developing a low latency solution like us whilst they are still running on non-deterministic hardware.</div><br/><div id="39433272" class="c"><input type="checkbox" id="c-39433272" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432912">parent</a><span>|</span><a href="#39434670">next</a><span>|</span><label class="collapse" for="c-39433272">[-]</label><label class="expand" for="c-39433272">[5 more]</label></div><br/><div class="children"><div class="content">What do you mean by non-deterministic hardware? cuBLAS on a laptop GPU was deterministic when I tried it last iirc</div><br/><div id="39433406" class="c"><input type="checkbox" id="c-39433406" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39433272">parent</a><span>|</span><a href="#39433434">next</a><span>|</span><label class="collapse" for="c-39433406">[-]</label><label class="expand" for="c-39433406">[1 more]</label></div><br/><div class="children"><div class="content">Non-deterministic timing characteristics.</div><br/></div></div><div id="39433434" class="c"><input type="checkbox" id="c-39433434" checked=""/><div class="controls bullet"><span class="by">frozenport</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39433272">parent</a><span>|</span><a href="#39433406">prev</a><span>|</span><a href="#39434670">next</a><span>|</span><label class="collapse" for="c-39433434">[-]</label><label class="expand" for="c-39433434">[3 more]</label></div><br/><div class="children"><div class="content">Tip of the ice-berg.<p>DRAM needs to be refreshed every X cycles.<p>This means you don&#x27;t know the time it takes to read from memory. You could be reading at a refresh cycle. This circuitry also adds latency.</div><br/><div id="39436488" class="c"><input type="checkbox" id="c-39436488" checked=""/><div class="controls bullet"><span class="by">LtdJorge</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39433434">parent</a><span>|</span><a href="#39434670">next</a><span>|</span><label class="collapse" for="c-39436488">[-]</label><label class="expand" for="c-39436488">[2 more]</label></div><br/><div class="children"><div class="content">OP says SRAM, which doesn&#x27;t decay so no refreshing.</div><br/><div id="39438862" class="c"><input type="checkbox" id="c-39438862" checked=""/><div class="controls bullet"><span class="by">ndjdbdjdbev</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39436488">parent</a><span>|</span><a href="#39434670">next</a><span>|</span><label class="collapse" for="c-39438862">[-]</label><label class="expand" for="c-39438862">[1 more]</label></div><br/><div class="children"><div class="content">Timing can simply mean the FETs that make up the logic circuits of a chip. The transition from high to low and low to high has a minimum safe time to register properly...</div><br/></div></div></div></div></div></div></div></div><div id="39434670" class="c"><input type="checkbox" id="c-39434670" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432912">parent</a><span>|</span><a href="#39433272">prev</a><span>|</span><a href="#39434382">next</a><span>|</span><label class="collapse" for="c-39434670">[-]</label><label class="expand" for="c-39434670">[2 more]</label></div><br/><div class="children"><div class="content">While you’re here, I have a quick, off-topic question. We‘ve seen incredible results with GPT3-176B (Davinci) and GPT4 (MoE). Making attempts at open models that reuse their architectural strategies could have a high impact on everyone. Those models took 2500-25000 GPU’s to train, though. It would be great to have a low-cost option for pre training Davinci-class models.<p>It would great if a company or others with AI hardware were willing to do production runs of chips sold at cost specifically to make open, permissive-licensed models. As in, since you’d lose profit, the cluster owner and users would be legally required to only make permissive models. Maybe at least one in each category (eg text, visual).<p>Do you think your company or any other hardware supplier would do that? Or someone sell 2500 GPU’s at cost for open models?<p>(Note to anyone involved in CHIPS Act: please fund a cluster or accelerator specifically for this.)</div><br/><div id="39434905" class="c"><input type="checkbox" id="c-39434905" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39434670">parent</a><span>|</span><a href="#39434382">next</a><span>|</span><label class="collapse" for="c-39434905">[-]</label><label class="expand" for="c-39434905">[1 more]</label></div><br/><div class="children"><div class="content">Great idea, but Groq doesn&#x27;t have a product suitable for training at the moment.  Our LPUs shine in inference.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39434382" class="c"><input type="checkbox" id="c-39434382" checked=""/><div class="controls bullet"><span class="by">huac</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432163">parent</a><span>|</span><a href="#39432294">prev</a><span>|</span><a href="#39438359">next</a><span>|</span><label class="collapse" for="c-39434382">[-]</label><label class="expand" for="c-39434382">[6 more]</label></div><br/><div class="children"><div class="content">&gt; 30b+ parameter model doing RAG as part of a conversation with voice responses in less than a second, running on Nvidia.<p>I believe that this is doable - my pipeline is generally closer to 400ms without RAG and with Mixtral, with a lot of non-ML hacks to get there. It would also definitely be doable with a joint speech-language model that removes the transcription step.<p>For these use cases, time to first byte is the most important metric, not total throughput.</div><br/><div id="39436490" class="c"><input type="checkbox" id="c-39436490" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39434382">parent</a><span>|</span><a href="#39438359">next</a><span>|</span><label class="collapse" for="c-39436490">[-]</label><label class="expand" for="c-39436490">[5 more]</label></div><br/><div class="children"><div class="content">It’s important…if you’re building a chatbot.<p>The most interesting applications of LLMs are not chatbots.</div><br/><div id="39437652" class="c"><input type="checkbox" id="c-39437652" checked=""/><div class="controls bullet"><span class="by">chasd00</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39436490">parent</a><span>|</span><a href="#39437801">next</a><span>|</span><label class="collapse" for="c-39437652">[-]</label><label class="expand" for="c-39437652">[3 more]</label></div><br/><div class="children"><div class="content">&gt; The most interesting applications of LLMs are not chatbots.<p>What are they then? Every use case I’ve seen is either a chatbot or like a copy editor which is just a long form chatbot.</div><br/><div id="39438037" class="c"><input type="checkbox" id="c-39438037" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39437652">parent</a><span>|</span><a href="#39438069">next</a><span>|</span><label class="collapse" for="c-39438037">[-]</label><label class="expand" for="c-39438037">[1 more]</label></div><br/><div class="children"><div class="content">Obviously not op, but these days LLMs can be fuzzy functions with reliably structured output, and are multi-modal.<p>Think about the implications of that. I bet you can come up with some pretty cool use cases that don&#x27;t involve you talking to something over chat.<p>One example:<p>I think we&#x27;ll be seeing a lot of &quot;general detectors&quot; soon. Without training or predefined categories, get pinged when (whatever you specify) happens. Whether it&#x27;s a security camera, web search, event data, etc</div><br/></div></div><div id="39438069" class="c"><input type="checkbox" id="c-39438069" checked=""/><div class="controls bullet"><span class="by">nycdatasci</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39437652">parent</a><span>|</span><a href="#39438037">prev</a><span>|</span><a href="#39437801">next</a><span>|</span><label class="collapse" for="c-39438069">[-]</label><label class="expand" for="c-39438069">[1 more]</label></div><br/><div class="children"><div class="content">Complex data tagging&#x2F;enrichment tasks.</div><br/></div></div></div></div><div id="39437801" class="c"><input type="checkbox" id="c-39437801" checked=""/><div class="controls bullet"><span class="by">throwaway2037</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39436490">parent</a><span>|</span><a href="#39437652">prev</a><span>|</span><a href="#39438359">next</a><span>|</span><label class="collapse" for="c-39437801">[-]</label><label class="expand" for="c-39437801">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The most interesting applications of LLMs are not chatbots.<p>In your opinion, what are the most interesting?</div><br/></div></div></div></div></div></div><div id="39438359" class="c"><input type="checkbox" id="c-39438359" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432163">parent</a><span>|</span><a href="#39434382">prev</a><span>|</span><a href="#39437323">next</a><span>|</span><label class="collapse" for="c-39438359">[-]</label><label class="expand" for="c-39438359">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Show me a 30b+ parameter model doing RAG as part of a conversation with voice responses in less than a second, running on Nvidia.<p>Is your version of that on a different page from this chat bot?</div><br/></div></div><div id="39437323" class="c"><input type="checkbox" id="c-39437323" checked=""/><div class="controls bullet"><span class="by">startupsfail</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432163">parent</a><span>|</span><a href="#39438359">prev</a><span>|</span><a href="#39434372">next</a><span>|</span><label class="collapse" for="c-39437323">[-]</label><label class="expand" for="c-39437323">[2 more]</label></div><br/><div class="children"><div class="content">I have one, with 13B, on a 5-year-old 48GB Q8000 GPU. It’s also can see, it’s LLaVA.  And it is very important that it is local, as privacy is important and streaming images to the cloud is time consuming.<p>You only need a few tokens, not the full 500 tokens response to run TTS. And you can pre-generate responses online, as ASR is still in progress. With a bit of clever engineering the response starts with virtually no delay, the moment its natural to start the response.</div><br/><div id="39437808" class="c"><input type="checkbox" id="c-39437808" checked=""/><div class="controls bullet"><span class="by">yaknh</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39437323">parent</a><span>|</span><a href="#39434372">next</a><span>|</span><label class="collapse" for="c-39437808">[-]</label><label class="expand" for="c-39437808">[1 more]</label></div><br/><div class="children"><div class="content">Did you find anything cheaper for local installation?</div><br/></div></div></div></div><div id="39434372" class="c"><input type="checkbox" id="c-39434372" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432163">parent</a><span>|</span><a href="#39437323">prev</a><span>|</span><a href="#39435121">next</a><span>|</span><label class="collapse" for="c-39434372">[-]</label><label class="expand" for="c-39434372">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Show me a 30b+ parameter model doing RAG as part of a conversation with voice responses in less than a second, running on Nvidia<p>I built one, should be live soon ;-)</div><br/><div id="39434614" class="c"><input type="checkbox" id="c-39434614" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39434372">parent</a><span>|</span><a href="#39435121">next</a><span>|</span><label class="collapse" for="c-39434614">[-]</label><label class="expand" for="c-39434614">[1 more]</label></div><br/><div class="children"><div class="content">Exciting!  Looking forward to seeing it.</div><br/></div></div></div></div><div id="39435121" class="c"><input type="checkbox" id="c-39435121" checked=""/><div class="controls bullet"><span class="by">mlazos</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432163">parent</a><span>|</span><a href="#39434372">prev</a><span>|</span><a href="#39437361">next</a><span>|</span><label class="collapse" for="c-39435121">[-]</label><label class="expand" for="c-39435121">[1 more]</label></div><br/><div class="children"><div class="content">You can’t scale horizontally forever because of communication. I think HBM would provide a lot more flexibility with the number of chips you need.</div><br/></div></div><div id="39437361" class="c"><input type="checkbox" id="c-39437361" checked=""/><div class="controls bullet"><span class="by">fennecbutt</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432163">parent</a><span>|</span><a href="#39435121">prev</a><span>|</span><a href="#39432384">next</a><span>|</span><label class="collapse" for="c-39437361">[-]</label><label class="expand" for="c-39437361">[2 more]</label></div><br/><div class="children"><div class="content">Are there voice responses in the demo? I couldn&#x27;t find em?</div><br/><div id="39439020" class="c"><input type="checkbox" id="c-39439020" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39437361">parent</a><span>|</span><a href="#39432384">next</a><span>|</span><label class="collapse" for="c-39439020">[-]</label><label class="expand" for="c-39439020">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s a live demo of CNN of Groq plugged into a voice API<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pRUddK6sxDg&amp;t=235s" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pRUddK6sxDg&amp;t=235s</a></div><br/></div></div></div></div></div></div><div id="39432384" class="c"><input type="checkbox" id="c-39432384" checked=""/><div class="controls bullet"><span class="by">trsohmers</span><span>|</span><a href="#39431989">parent</a><span>|</span><a href="#39432163">prev</a><span>|</span><a href="#39432119">next</a><span>|</span><label class="collapse" for="c-39432384">[-]</label><label class="expand" for="c-39432384">[6 more]</label></div><br/><div class="children"><div class="content">Groq states in this article [0] that they used 576 chips to achieve these results, and continuing with your analysis, you also need to factor in that for each additional user you want to have requires a separate KV cache, which can add multiple more gigabytes per user.<p>My professional independent observer opinion (not based on my 2 years of working at Groq) would have me assume that their COGS to achieve these performance numbers would exceed several million dollars, so depreciating that over expected usage at the theoretical prices they have posted seems impractical, so from an actual performance per dollar standpoint they don’t seem viable, but do have a very cool demo of an insane level of performance if you throw cost concerns out the window.<p>[0]: <a href="https:&#x2F;&#x2F;www.nextplatform.com&#x2F;2023&#x2F;11&#x2F;27&#x2F;groq-says-it-can-deploy-1-million-ai-inference-chips-in-two-years&#x2F;?amp" rel="nofollow">https:&#x2F;&#x2F;www.nextplatform.com&#x2F;2023&#x2F;11&#x2F;27&#x2F;groq-says-it-can-dep...</a></div><br/><div id="39433285" class="c"><input type="checkbox" id="c-39433285" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432384">parent</a><span>|</span><a href="#39436136">next</a><span>|</span><label class="collapse" for="c-39433285">[-]</label><label class="expand" for="c-39433285">[2 more]</label></div><br/><div class="children"><div class="content">Thomas, I think for full disclosure you should also state that you left Groq to start a competitor (a competitor which doesn&#x27;t have the world&#x27;s lowest latency LLM engine nor a guarantee to match the cheapest per token prices, like Groq does.).<p>Anyone with a serious interest in the total cost of ownership of Groq&#x27;s system is welcome to email contact@groq.com.</div><br/><div id="39433379" class="c"><input type="checkbox" id="c-39433379" checked=""/><div class="controls bullet"><span class="by">trsohmers</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39433285">parent</a><span>|</span><a href="#39436136">next</a><span>|</span><label class="collapse" for="c-39433379">[-]</label><label class="expand" for="c-39433379">[1 more]</label></div><br/><div class="children"><div class="content">I thought that was clear through my profile, but yes, Positron AI is focused on providing the best performance per dollar while providing the best quality of service and capabilities rather than just focusing on a single metric of speed.<p>A guarantee to match the cheapest per token prices is sure a great way to lose a race to the bottom, but I do wish Groq (and everyone else trying to compete against NVIDIA) the greatest luck and success. I really do think that the great single batch&#x2F;user performance by Groq is a great demo, but is not the best solution for a wide variety of applications, but I hope it can find its niche.</div><br/></div></div></div></div><div id="39436136" class="c"><input type="checkbox" id="c-39436136" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432384">parent</a><span>|</span><a href="#39433285">prev</a><span>|</span><a href="#39437997">next</a><span>|</span><label class="collapse" for="c-39436136">[-]</label><label class="expand" for="c-39436136">[1 more]</label></div><br/><div class="children"><div class="content">I think that just means it’s for people that <i>really</i> want it?<p>John doe and his friends will never have a need to have their fart jokes generated at this speed, and are more interested in low costs.<p>But we’d recently been doing call center operations and being able to quickly figure out what someone said was a major issue. You kind of don’t want your system to wait for a second before responding each time. I can imagine it making sense if it reduces the latency to 10ms there as well. Though you might still run up against the ‘good enough’ factor.<p>I guess few people want to spend millions to go from 1000ms to 10ms, but when they do they really want it.</div><br/></div></div><div id="39437997" class="c"><input type="checkbox" id="c-39437997" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432384">parent</a><span>|</span><a href="#39436136">prev</a><span>|</span><a href="#39432119">next</a><span>|</span><label class="collapse" for="c-39437997">[-]</label><label class="expand" for="c-39437997">[2 more]</label></div><br/><div class="children"><div class="content">What happened to Rex? Did it hit production or get abandoned?<p>It was also on my list of things to consider modifying for an AI accelerator. :)</div><br/><div id="39438579" class="c"><input type="checkbox" id="c-39438579" checked=""/><div class="controls bullet"><span class="by">trsohmers</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39437997">parent</a><span>|</span><a href="#39432119">next</a><span>|</span><label class="collapse" for="c-39438579">[-]</label><label class="expand" for="c-39438579">[1 more]</label></div><br/><div class="children"><div class="content">Long story, but technically REX is still around but has not been able to continue to develop due to lack of funding and my cofounder and I needing to pay bills. We produced initial test silicon, but due to us having very little money after silicon bringup, most of our conversations turned to acquihire discussions.<p>There should be a podcast release (<a href="https:&#x2F;&#x2F;microarch.club&#x2F;" rel="nofollow">https:&#x2F;&#x2F;microarch.club&#x2F;</a>) in the near future that covers REX&#x27;s history and a lot of lessons learned.</div><br/></div></div></div></div></div></div><div id="39432119" class="c"><input type="checkbox" id="c-39432119" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431989">parent</a><span>|</span><a href="#39432384">prev</a><span>|</span><a href="#39432033">next</a><span>|</span><label class="collapse" for="c-39432119">[-]</label><label class="expand" for="c-39432119">[16 more]</label></div><br/><div class="children"><div class="content">If you want low latency you have to be really careful with HBM, not only because of the delay involved, but also the non-determinacy.  One of the huge benefits of our LPU architecture is that we can build systems of hundreds of chips with fast interconnect and we know the precise timing of the whole system to within a few parts per million.  Once you start integrating non-deterministic components your latency guarantees disappear very quickly.</div><br/><div id="39432309" class="c"><input type="checkbox" id="c-39432309" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432119">parent</a><span>|</span><a href="#39433607">next</a><span>|</span><label class="collapse" for="c-39432309">[-]</label><label class="expand" for="c-39432309">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know about HBM specifically, but DDR and GDDR at a protocol level are both deterministic. It&#x27;s the memory controller doing a bunch of reordering that makes them non-deterministic. Presumably, if that is the reason you don&#x27;t like DRAM, you could build your compiler to be memory-layout aware and have the memory controller issue commands without reordering.</div><br/><div id="39432365" class="c"><input type="checkbox" id="c-39432365" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432309">parent</a><span>|</span><a href="#39432929">next</a><span>|</span><label class="collapse" for="c-39432365">[-]</label><label class="expand" for="c-39432365">[1 more]</label></div><br/><div class="children"><div class="content">That could be possible.  It&#x27;s out of my area of expertise so I can&#x27;t say for sure.  My understanding was HBM forces on you specific access patterns and non-deterministic delays.  Our compiler already deals with many other forms of resource-aware scheduling so it could take into account DRAM refreshes easily, so I feel like there must be something else that makes SRAM more suitable in our case.  I&#x27;ll have to leave that to someone more knowledgeable to explain though ...</div><br/></div></div><div id="39432929" class="c"><input type="checkbox" id="c-39432929" checked=""/><div class="controls bullet"><span class="by">johntb86</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432309">parent</a><span>|</span><a href="#39432365">prev</a><span>|</span><a href="#39433607">next</a><span>|</span><label class="collapse" for="c-39432929">[-]</label><label class="expand" for="c-39432929">[3 more]</label></div><br/><div class="children"><div class="content">Presumably with dram you also have to worry about refreshes, which can come along at arbitrary times relative to the workload.</div><br/><div id="39435364" class="c"><input type="checkbox" id="c-39435364" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432929">parent</a><span>|</span><a href="#39433607">next</a><span>|</span><label class="collapse" for="c-39435364">[-]</label><label class="expand" for="c-39435364">[2 more]</label></div><br/><div class="children"><div class="content">You can control when those happen, too.</div><br/><div id="39437588" class="c"><input type="checkbox" id="c-39437588" checked=""/><div class="controls bullet"><span class="by">Temporary_31337</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39435364">parent</a><span>|</span><a href="#39433607">next</a><span>|</span><label class="collapse" for="c-39437588">[-]</label><label class="expand" for="c-39437588">[1 more]</label></div><br/><div class="children"><div class="content">not without affecting performance though? If you delay refreshes, this lowers performance as far as I remember...</div><br/></div></div></div></div></div></div></div></div><div id="39433607" class="c"><input type="checkbox" id="c-39433607" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432119">parent</a><span>|</span><a href="#39432309">prev</a><span>|</span><a href="#39435184">next</a><span>|</span><label class="collapse" for="c-39433607">[-]</label><label class="expand" for="c-39433607">[8 more]</label></div><br/><div class="children"><div class="content">From a theoretical perspective, this is absolutely not true. Asynchronous logic can achieve much lower latency guarantees than synchronous logic.<p>Come to think of it, this is one of the few places where asynchronous logic might be more than academic... Async logic is hard with complex control flows, which deep learning inference does not have.<p>(From a practical perspective, I know you were comparing to independently-clocked logic, rather than async logic)</div><br/><div id="39435703" class="c"><input type="checkbox" id="c-39435703" checked=""/><div class="controls bullet"><span class="by">foundval</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39433607">parent</a><span>|</span><a href="#39435184">next</a><span>|</span><label class="collapse" for="c-39435703">[-]</label><label class="expand" for="c-39435703">[7 more]</label></div><br/><div class="children"><div class="content">(Groq Employee) You&#x27;re right - we are comparing to independently-clocked logic.<p>I wonder whether async logic would be feasible for reconfigurable &quot;Spatial Processor&quot; type architectures [1]. As far as LPU architectures go, they fall in the &quot;Matrix of Processing Engines&quot;[1] family of architectures, which I would naively guess is not the best suited to leverage async logic.<p>1: I&#x27;m using the &quot;Spatial Processor&quot; (7:14) and &quot;Matrix of Processing Engines&quot;  (8:57) terms as defined in <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=LUPWZ-LC0XE" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=LUPWZ-LC0XE</a>. Sorry for a video link, I just can&#x27;t think of another single reference that explains the two approaches.</div><br/><div id="39436223" class="c"><input type="checkbox" id="c-39436223" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39435703">parent</a><span>|</span><a href="#39435184">next</a><span>|</span><label class="collapse" for="c-39436223">[-]</label><label class="expand" for="c-39436223">[6 more]</label></div><br/><div class="children"><div class="content">Curiously, almost all of this video is mostly covered by computer architectures lit in the late 90&#x27;s early 00&#x27;s. At the time, I recall Tom Knight had done most of the analysis in this video, but I don&#x27;t know if he ever published it. It was extrapolating into the distant future.<p>To answer your questions:<p>- Spatial processors are an insanely good fit for async logic<p>- Matrix of processing engines are a moderately good fit -- definitely could be done, but I have no clue if it&#x27;d be a good idea.<p>In SP, especially in an ASIC, each computation can start as soon as the previous one finishes. If you have a 4-bit layer, and 8-bit layer, and a 32-bit layer, those will take different amounts of time to run. Individual computations can take different amounts of time too (e.g. an ADD with a lot of carries versus one with just a few). In an SP, a compute will take as much time as it needs, and no more.<p>Footnote: Personally, I think there are a lot of good ideas in 80&#x27;s era and earlier processors for the design of individual compute units which have been forgotten. The basic move in architectures up through 2005 was optimizing serial computation speed at the cost of power and die size (Netburst went up to 3.8GHz two decades ago). With much simpler old-school compute units, we can have *many* more of them than a modern multiply unit. Critically, they could be positioned closer to the data, so there would be less data moving around. Especially the early pipelined &#x2F; scalar &#x2F; RISC cores seem very relevant. As a point of reference, a 4090 has 16k CUDA cores running at just north of 2GHz. It has the same number of transistors as 32,000 SA-110 processors (running at 200MHz on a 350 nanometer process in 1994).<p>TL;DR: I&#x27;m getting old and either nostalgic or grumpy. Dunno which.</div><br/><div id="39437136" class="c"><input type="checkbox" id="c-39437136" checked=""/><div class="controls bullet"><span class="by">foundval</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39436223">parent</a><span>|</span><a href="#39436276">next</a><span>|</span><label class="collapse" for="c-39437136">[-]</label><label class="expand" for="c-39437136">[1 more]</label></div><br/><div class="children"><div class="content">Sweet, thanks! It seems like this research ecosystem was incredibly rich, but Moore&#x27;s law was in full swing, and statically known workloads weren&#x27;t useful at the compute scale of back then.<p>So these specialized approach never stood a chance next to CPUS. Nowadays the ground is.. more fertile.</div><br/></div></div><div id="39436276" class="c"><input type="checkbox" id="c-39436276" checked=""/><div class="controls bullet"><span class="by">frozenport</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39436223">parent</a><span>|</span><a href="#39437136">prev</a><span>|</span><a href="#39435184">next</a><span>|</span><label class="collapse" for="c-39436276">[-]</label><label class="expand" for="c-39436276">[4 more]</label></div><br/><div class="children"><div class="content">This was sort of the dream of KNL but today I noticed<p><pre><code>    Xeon Phi CPUs support (a.k.a. Knight Landing and Knight Mill) are marked as deprecated. GCC will emit a warning when using the -mavx5124fmaps, -mavx5124vnniw, -mavx512er, -mavx512pf, -mprefetchwt1, -march=knl, -march=knm, -mtune=knl or -mtune=knm compiler switches. Support will be removed in GCC 15.
</code></pre>
the issue was that coordinating across this kind of hierarchy wasted a bunch of time. If you already knew how to coordinate, mostly, you could instead get better performance<p>you might be surprised but we&#x27;re getting to the point that communicating over a super computer is on the same order of magnitude as talking across a numa node.</div><br/><div id="39436731" class="c"><input type="checkbox" id="c-39436731" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39436276">parent</a><span>|</span><a href="#39435184">next</a><span>|</span><label class="collapse" for="c-39436731">[-]</label><label class="expand" for="c-39436731">[3 more]</label></div><br/><div class="children"><div class="content">I actually wasn&#x27;t so much talking from that perspective, as simply from the perspective of the design of individual pieces. There were rather clever things done in e.g. older multipliers or adders or similar which, I <i>think</i>, could apply to most modern parallel architectures, be that GPGPU, SP, MPE, FPGA, or whatever, in order to significantly increase density at a cost of slightly reduced serial performance.<p>For machine learning, that&#x27;s a good tradeoff.<p>Indeed, with some of the simpler architectures, I think computation could be moved into the memory itself, as long dreamed of.<p>(Simply sticking 32,000 SA-110 processors on a die would be very, very limited by interconnect; there&#x27;s a good reason for the types of architectures we&#x27;re seeing <i>not</i> being that)</div><br/><div id="39436813" class="c"><input type="checkbox" id="c-39436813" checked=""/><div class="controls bullet"><span class="by">frozenport</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39436731">parent</a><span>|</span><a href="#39435184">next</a><span>|</span><label class="collapse" for="c-39436813">[-]</label><label class="expand" for="c-39436813">[2 more]</label></div><br/><div class="children"><div class="content">Truth is that there is another startup called graph core that is doing exactly that, and also a really big chip</div><br/><div id="39437373" class="c"><input type="checkbox" id="c-39437373" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39436813">parent</a><span>|</span><a href="#39435184">next</a><span>|</span><label class="collapse" for="c-39437373">[-]</label><label class="expand" for="c-39437373">[1 more]</label></div><br/><div class="children"><div class="content">They do what you were talking about, not what I was.<p>They seem annoying. &quot;The IPU has a unique memory architecture consisting of large amounts of In-Processor-Memory™ within the IPU made up of SRAM (organised as a set of smaller independent distributed memory units) and a set of attached DRAM chips which can transfer to the In-Processor-Memory via explicit copies within the software. The memory contained in the external DRAM chips is referred to as Streaming Memory™.&quot;<p>There&#x27;s a ™ every few words. Those seem like pretty generic terms. That&#x27;s their technical documentation.<p>The architecture is reminiscent of some ideas from circa-2000 which didn&#x27;t pan out. It reminds me of Tilera (the guy who ran it was the Donald Trump of computer architectures; company was acquihired by EZchip for a fraction of the investment which was put into it, which went to Mellanox, and then to NVidia).</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39435184" class="c"><input type="checkbox" id="c-39435184" checked=""/><div class="controls bullet"><span class="by">SilverBirch</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432119">parent</a><span>|</span><a href="#39433607">prev</a><span>|</span><a href="#39432033">next</a><span>|</span><label class="collapse" for="c-39435184">[-]</label><label class="expand" for="c-39435184">[2 more]</label></div><br/><div class="children"><div class="content">Surely once you&#x27;re scaling over multiple chips&#x2F;servers&#x2F;racks you&#x27;re dealing with retries and checksums and sequence numbers anyway? How do you get around the non-determinacy of networking beyond just hoping that you don&#x27;t see any errors?</div><br/><div id="39435291" class="c"><input type="checkbox" id="c-39435291" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39435184">parent</a><span>|</span><a href="#39432033">next</a><span>|</span><label class="collapse" for="c-39435291">[-]</label><label class="expand" for="c-39435291">[1 more]</label></div><br/><div class="children"><div class="content">Our interconnect between chips is also deterministic!  You can read more about our interconnect, synchronisation, and error correction in our paper.<p><a href="https:&#x2F;&#x2F;wow.groq.com&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;05&#x2F;GroqISCAPaper2022_ASoftwareDefinedTensorStreamingMultiprocessorForLargeScaleMachineLearning-1.pdf" rel="nofollow">https:&#x2F;&#x2F;wow.groq.com&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;05&#x2F;GroqISCAPape...</a></div><br/></div></div></div></div></div></div><div id="39432033" class="c"><input type="checkbox" id="c-39432033" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#39431989">parent</a><span>|</span><a href="#39432119">prev</a><span>|</span><a href="#39432974">next</a><span>|</span><label class="collapse" for="c-39432033">[-]</label><label class="expand" for="c-39432033">[5 more]</label></div><br/><div class="children"><div class="content">Groq devices are really well set up for small-batch-size inference because of the use of SRAM.<p>I&#x27;m not so convinced they have a Tok&#x2F;sec&#x2F;$ advantage at all, though, and especially at medium to large batch sizes which would be the groups who can afford to buy so much silicon.<p>I assume given the architecture that Groq actually doesn&#x27;t get any faster for batch sizes &gt;1, and Nvidia cards do get meaningfully higher throughput as batch size gets into the 100&#x27;s.</div><br/><div id="39435818" class="c"><input type="checkbox" id="c-39435818" checked=""/><div class="controls bullet"><span class="by">foundval</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432033">parent</a><span>|</span><a href="#39435231">next</a><span>|</span><label class="collapse" for="c-39435818">[-]</label><label class="expand" for="c-39435818">[1 more]</label></div><br/><div class="children"><div class="content">(Groq Employee) It&#x27;s hard to discuss Tok&#x2F;sec&#x2F;$ outside of the context of a hardware sales engagement.<p>This is because the relationship between Tok&#x2F;s&#x2F;u, Tok&#x2F;s&#x2F;system, Batching, and Pipelining is a complex one that involves compute utilization, network utilization, and (in particular) a host of compilation techniques that we wouldn&#x27;t want to share publicly. Maybe we&#x27;ll get to that level of transparency at some point, though!<p>As far as Batching goes, you should consider that with synchronous systems, if all the stars align, Batch=1 is all you need. Of course, the devil is in the details, and sometimes small batch numbers still give you benefits. But Batch 100&#x27;s generally gives no advantages. In fact, the entire point of developing deterministic hardware and synchronous systems is to avoid batching in the first place.</div><br/></div></div><div id="39435231" class="c"><input type="checkbox" id="c-39435231" checked=""/><div class="controls bullet"><span class="by">nabakin</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432033">parent</a><span>|</span><a href="#39435818">prev</a><span>|</span><a href="#39435833">next</a><span>|</span><label class="collapse" for="c-39435231">[-]</label><label class="expand" for="c-39435231">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been thinking the same but on the other hand, that would mean they are operating at a huge loss which doesn&#x27;t scale</div><br/></div></div><div id="39435833" class="c"><input type="checkbox" id="c-39435833" checked=""/><div class="controls bullet"><span class="by">frozenport</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432033">parent</a><span>|</span><a href="#39435231">prev</a><span>|</span><a href="#39432974">next</a><span>|</span><label class="collapse" for="c-39435833">[-]</label><label class="expand" for="c-39435833">[2 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>    I assume given the architecture that Groq actually doesn&#x27;t get any faster for batch sizes &gt;1
</code></pre>
I guess if you don&#x27;t have any extra junk you can pack more processing into the chip?</div><br/><div id="39436019" class="c"><input type="checkbox" id="c-39436019" checked=""/><div class="controls bullet"><span class="by">foundval</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39435833">parent</a><span>|</span><a href="#39432974">next</a><span>|</span><label class="collapse" for="c-39436019">[-]</label><label class="expand" for="c-39436019">[1 more]</label></div><br/><div class="children"><div class="content">(Groq Employee) Yes! Determinism + Simplicity are superpowers for ALU and interconnect utilization rates. This system is powered by 14nm chips, and even the interconnects aren&#x27;t best in class.<p>We&#x27;re just that much better at squeezing tokens out of transistors and optic cables than GPUs are - and you can imagine the implications on Watt&#x2F;Token.<p>Anyways.. wait until you see our 4nm. :)</div><br/></div></div></div></div></div></div><div id="39432974" class="c"><input type="checkbox" id="c-39432974" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#39431989">parent</a><span>|</span><a href="#39432033">prev</a><span>|</span><a href="#39432364">next</a><span>|</span><label class="collapse" for="c-39432974">[-]</label><label class="expand" for="c-39432974">[3 more]</label></div><br/><div class="children"><div class="content">&gt; more than a single model and a lot of finetunes&#x2F;high rank LoRAs<p>I can imagine a way might be found to host a base model and a bunch of LoRA&#x27;s whilst using barely more ram than the base model alone.<p>The fine-tuning could perhaps be done in such a way that only perhaps 0.1% of the weights are changed, and for every computation the difference is computed not over the weights, but of the output layer activations.</div><br/><div id="39435609" class="c"><input type="checkbox" id="c-39435609" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432974">parent</a><span>|</span><a href="#39435965">next</a><span>|</span><label class="collapse" for="c-39435609">[-]</label><label class="expand" for="c-39435609">[1 more]</label></div><br/><div class="children"><div class="content">This actually already exists! We did a writeup of the relevant optimizations here: <a href="https:&#x2F;&#x2F;openpipe.ai&#x2F;blog&#x2F;s-lora">https:&#x2F;&#x2F;openpipe.ai&#x2F;blog&#x2F;s-lora</a></div><br/></div></div><div id="39435965" class="c"><input type="checkbox" id="c-39435965" checked=""/><div class="controls bullet"><span class="by">azeirah</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432974">parent</a><span>|</span><a href="#39435609">prev</a><span>|</span><a href="#39432364">next</a><span>|</span><label class="collapse" for="c-39435965">[-]</label><label class="expand" for="c-39435965">[1 more]</label></div><br/><div class="children"><div class="content">I recall a recent discussion about a technique to load the diff in weights between a lora and base model, zip it and transfer it on a per-needs basis.</div><br/></div></div></div></div><div id="39432364" class="c"><input type="checkbox" id="c-39432364" checked=""/><div class="controls bullet"><span class="by">moralestapia</span><span>|</span><a href="#39431989">parent</a><span>|</span><a href="#39432974">prev</a><span>|</span><a href="#39432517">next</a><span>|</span><label class="collapse" for="c-39432364">[-]</label><label class="expand" for="c-39432364">[4 more]</label></div><br/><div class="children"><div class="content">&gt;The main problem with the Groq LPUs is, they don&#x27;t have any HBM on them at all. Just a miniscule (230 MiB) [0] amount of ultra-fast SRAM [...]<p>IDGAF about any of that, lol. I just want an API endpoint.<p>480 tokens&#x2F;sec at $0.27 per million tokens? Sign me in, I don&#x27;t care about their hardware, at all.</div><br/><div id="39432526" class="c"><input type="checkbox" id="c-39432526" checked=""/><div class="controls bullet"><span class="by">treesciencebot</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432364">parent</a><span>|</span><a href="#39432517">next</a><span>|</span><label class="collapse" for="c-39432526">[-]</label><label class="expand" for="c-39432526">[3 more]</label></div><br/><div class="children"><div class="content">there are providers out there offering for $0 per million tokens, that doesn&#x27;t mean it is sustainable and won&#x27;t disappear as soon as the VC well runs dry. Am not saying this is the case for Groq, but in general you probably should care if you want to build something serious on top of anything.</div><br/><div id="39435926" class="c"><input type="checkbox" id="c-39435926" checked=""/><div class="controls bullet"><span class="by">foundval</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432526">parent</a><span>|</span><a href="#39432517">next</a><span>|</span><label class="collapse" for="c-39435926">[-]</label><label class="expand" for="c-39435926">[2 more]</label></div><br/><div class="children"><div class="content">(Groq Employee) Agreed, one should care, and especially since this particular service is very differentiated by its speed and has no competitors.<p>That being said, until there&#x27;s another option at anywhere that speed.. That point is moot, isn&#x27;t it :)<p>For now, Groq is the only option that can let you build an UX with near-instant response times. Or a live agents that help with a human-to-human interaction. I could go on and on about the product categories this opens.</div><br/><div id="39438236" class="c"><input type="checkbox" id="c-39438236" checked=""/><div class="controls bullet"><span class="by">bethekind</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39435926">parent</a><span>|</span><a href="#39432517">next</a><span>|</span><label class="collapse" for="c-39438236">[-]</label><label class="expand" for="c-39438236">[1 more]</label></div><br/><div class="children"><div class="content">Why go so fast? Aren&#x27;t Nvidias products fast enough from a TPS perspective?</div><br/></div></div></div></div></div></div></div></div><div id="39432517" class="c"><input type="checkbox" id="c-39432517" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#39431989">parent</a><span>|</span><a href="#39432364">prev</a><span>|</span><a href="#39431752">next</a><span>|</span><label class="collapse" for="c-39432517">[-]</label><label class="expand" for="c-39432517">[3 more]</label></div><br/><div class="children"><div class="content">I honestly don&#x27;t see the problem.<p>&quot;just to serve a single model&quot; could be easily fixed by adding a single LPDDR4 channel per LPU. Then you can reload the model sixty times per second and serve 60 different models per second.</div><br/><div id="39432578" class="c"><input type="checkbox" id="c-39432578" checked=""/><div class="controls bullet"><span class="by">treesciencebot</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432517">parent</a><span>|</span><a href="#39431752">next</a><span>|</span><label class="collapse" for="c-39432578">[-]</label><label class="expand" for="c-39432578">[2 more]</label></div><br/><div class="children"><div class="content">per-chip compute is not the main thing this chip innovates for fast inference, it is the extremely fast memory bandwith. when you do that, you&#x27;ll loose all of that and will be much worse off than any off the shelf accelerators.</div><br/><div id="39434606" class="c"><input type="checkbox" id="c-39434606" checked=""/><div class="controls bullet"><span class="by">QuadmasterXLII</span><span>|</span><a href="#39431989">root</a><span>|</span><a href="#39432578">parent</a><span>|</span><a href="#39431752">next</a><span>|</span><label class="collapse" for="c-39434606">[-]</label><label class="expand" for="c-39434606">[1 more]</label></div><br/><div class="children"><div class="content">load model, compute a 1k token response (ie, do a thousand forward passes in sequence, one per token), load a different model, compute a response,<p>I would expect the model loading to take basically zero percent of the time in the above workflow</div><br/></div></div></div></div></div></div></div></div><div id="39431752" class="c"><input type="checkbox" id="c-39431752" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#39431989">prev</a><span>|</span><a href="#39429047">next</a><span>|</span><label class="collapse" for="c-39431752">[-]</label><label class="expand" for="c-39431752">[23 more]</label></div><br/><div class="children"><div class="content">Very impressive looking! Just wanted to caution it&#x27;s worth being a bit skeptical without benchmarks as there are a number of ways to cut corners. One prominent example is heavy model quantization, which speeds up the model at a cost of model quality. Otherwise I&#x27;d love to see LLM tok&#x2F;s progress exactly like CPU instructions&#x2F;s did a few decades ago.</div><br/><div id="39434543" class="c"><input type="checkbox" id="c-39434543" checked=""/><div class="controls bullet"><span class="by">bsima</span><span>|</span><a href="#39431752">parent</a><span>|</span><a href="#39431888">next</a><span>|</span><label class="collapse" for="c-39434543">[-]</label><label class="expand" for="c-39434543">[5 more]</label></div><br/><div class="children"><div class="content">As tome mentioned we don’t quantize, all activations are FP16<p>And here are some independent benchmarks <a href="https:&#x2F;&#x2F;artificialanalysis.ai&#x2F;models&#x2F;llama-2-chat-70b" rel="nofollow">https:&#x2F;&#x2F;artificialanalysis.ai&#x2F;models&#x2F;llama-2-chat-70b</a></div><br/><div id="39435199" class="c"><input type="checkbox" id="c-39435199" checked=""/><div class="controls bullet"><span class="by">xvector</span><span>|</span><a href="#39431752">root</a><span>|</span><a href="#39434543">parent</a><span>|</span><a href="#39431888">next</a><span>|</span><label class="collapse" for="c-39435199">[-]</label><label class="expand" for="c-39435199">[4 more]</label></div><br/><div class="children"><div class="content">Jesus Christ, these speeds with FP16? That is simply insane.</div><br/><div id="39435338" class="c"><input type="checkbox" id="c-39435338" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#39431752">root</a><span>|</span><a href="#39435199">parent</a><span>|</span><a href="#39431888">next</a><span>|</span><label class="collapse" for="c-39435338">[-]</label><label class="expand" for="c-39435338">[3 more]</label></div><br/><div class="children"><div class="content">Ask how much hardware is behind it.</div><br/><div id="39435809" class="c"><input type="checkbox" id="c-39435809" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#39431752">root</a><span>|</span><a href="#39435338">parent</a><span>|</span><a href="#39431888">next</a><span>|</span><label class="collapse" for="c-39435809">[-]</label><label class="expand" for="c-39435809">[2 more]</label></div><br/><div class="children"><div class="content">All that matters is the cost. Their price is cheap, so the real question is whether they are subsidizing the cost to achieve that price or not.</div><br/><div id="39436991" class="c"><input type="checkbox" id="c-39436991" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#39431752">root</a><span>|</span><a href="#39435809">parent</a><span>|</span><a href="#39431888">next</a><span>|</span><label class="collapse" for="c-39436991">[-]</label><label class="expand" for="c-39436991">[1 more]</label></div><br/><div class="children"><div class="content">The point of asking how much hardware is to estimate the cost? (Both capital and operational, i.e. power)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39431888" class="c"><input type="checkbox" id="c-39431888" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431752">parent</a><span>|</span><a href="#39434543">prev</a><span>|</span><a href="#39434378">next</a><span>|</span><label class="collapse" for="c-39431888">[-]</label><label class="expand" for="c-39431888">[3 more]</label></div><br/><div class="children"><div class="content">As a fellow scientist I concur with the approach of skepticism by default. Our chat app and API are available for everyone to experiment with and compare output quality with any other provider.<p>I hope you are enjoying your time of having an empty calendar :)</div><br/><div id="39435925" class="c"><input type="checkbox" id="c-39435925" checked=""/><div class="controls bullet"><span class="by">mr_luc</span><span>|</span><a href="#39431752">root</a><span>|</span><a href="#39431888">parent</a><span>|</span><a href="#39434378">next</a><span>|</span><label class="collapse" for="c-39435925">[-]</label><label class="expand" for="c-39435925">[2 more]</label></div><br/><div class="children"><div class="content">Wait you have an API now??? Is it open, is there a waitlist? I’m on a plane but going to try to find that on the site. Absolutely loved your demo, been showing it around for a few months.</div><br/><div id="39436049" class="c"><input type="checkbox" id="c-39436049" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431752">root</a><span>|</span><a href="#39435925">parent</a><span>|</span><a href="#39434378">next</a><span>|</span><label class="collapse" for="c-39436049">[-]</label><label class="expand" for="c-39436049">[1 more]</label></div><br/><div class="children"><div class="content">There is an API and there is a waitlist.  Sign up at <a href="http:&#x2F;&#x2F;wow.groq.com&#x2F;" rel="nofollow">http:&#x2F;&#x2F;wow.groq.com&#x2F;</a></div><br/></div></div></div></div></div></div><div id="39434378" class="c"><input type="checkbox" id="c-39434378" checked=""/><div class="controls bullet"><span class="by">Gcam</span><span>|</span><a href="#39431752">parent</a><span>|</span><a href="#39431888">prev</a><span>|</span><a href="#39432333">next</a><span>|</span><label class="collapse" for="c-39434378">[-]</label><label class="expand" for="c-39434378">[1 more]</label></div><br/><div class="children"><div class="content">As part of our benchmarking of Groq we have asked Groq regarding quantization and they have assured us they are running models at full FP-16. It&#x27;s a good point and important to check.<p>Link to benchmarking: <a href="https:&#x2F;&#x2F;artificialanalysis.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;artificialanalysis.ai&#x2F;</a>
(Note question was regarding API rather than their chat demo)</div><br/></div></div><div id="39432333" class="c"><input type="checkbox" id="c-39432333" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#39431752">parent</a><span>|</span><a href="#39434378">prev</a><span>|</span><a href="#39432687">next</a><span>|</span><label class="collapse" for="c-39432333">[-]</label><label class="expand" for="c-39432333">[9 more]</label></div><br/><div class="children"><div class="content">At least for the earlier Llama 70B demo, they claimed to be running unquantized. <a href="https:&#x2F;&#x2F;twitter.com&#x2F;lifebypixels&#x2F;status&#x2F;1757619926360096852" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;lifebypixels&#x2F;status&#x2F;1757619926360096852</a><p>Update: This comment says &quot;some data is stored as FP8 at rest&quot; and I don&#x27;t know what that means. <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39432025">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39432025</a></div><br/><div id="39432756" class="c"><input type="checkbox" id="c-39432756" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431752">root</a><span>|</span><a href="#39432333">parent</a><span>|</span><a href="#39434917">next</a><span>|</span><label class="collapse" for="c-39432756">[-]</label><label class="expand" for="c-39432756">[7 more]</label></div><br/><div class="children"><div class="content">The weights are quantized to FP8 when they&#x27;re stored in memory, but all the activations are computed at full FP16 precision.</div><br/><div id="39433751" class="c"><input type="checkbox" id="c-39433751" checked=""/><div class="controls bullet"><span class="by">youssefabdelm</span><span>|</span><a href="#39431752">root</a><span>|</span><a href="#39432756">parent</a><span>|</span><a href="#39434917">next</a><span>|</span><label class="collapse" for="c-39433751">[-]</label><label class="expand" for="c-39433751">[6 more]</label></div><br/><div class="children"><div class="content">Can you explain if this affects quality relative to fp16? And is mixtral quantized?</div><br/><div id="39433907" class="c"><input type="checkbox" id="c-39433907" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431752">root</a><span>|</span><a href="#39433751">parent</a><span>|</span><a href="#39434917">next</a><span>|</span><label class="collapse" for="c-39433907">[-]</label><label class="expand" for="c-39433907">[5 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t think so, but you be the judge!  I believe we quantize both Mixtral and Llama 2 in this way.</div><br/><div id="39434572" class="c"><input type="checkbox" id="c-39434572" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39431752">root</a><span>|</span><a href="#39433907">parent</a><span>|</span><a href="#39434917">next</a><span>|</span><label class="collapse" for="c-39434572">[-]</label><label class="expand" for="c-39434572">[4 more]</label></div><br/><div class="children"><div class="content">Is your confidence rooted in quantified testing, or just vibes? I&#x27;m sure you&#x27;re right, just curious. (My reasoning: running inference at full fp16 is borderline wasteful. You can use q7 with almost no loss.)</div><br/><div id="39437599" class="c"><input type="checkbox" id="c-39437599" checked=""/><div class="controls bullet"><span class="by">monkmartinez</span><span>|</span><a href="#39431752">root</a><span>|</span><a href="#39434572">parent</a><span>|</span><a href="#39434608">next</a><span>|</span><label class="collapse" for="c-39437599">[-]</label><label class="expand" for="c-39437599">[2 more]</label></div><br/><div class="children"><div class="content">I know some fancy benchmark says &quot;almost no loss&quot;, but... subjectively, there is a clear quality loss. You can try for yourself, I can run Mixtral at 5.8bpw and there is an OBVIOUS difference between what I have seen from Groq and my local setup beside the sound barrier shattering speed of Groq. I didn&#x27;t know Mixtral could output such nice code and I have used it A LOT locally.</div><br/><div id="39438700" class="c"><input type="checkbox" id="c-39438700" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#39431752">root</a><span>|</span><a href="#39437599">parent</a><span>|</span><a href="#39434608">next</a><span>|</span><label class="collapse" for="c-39438700">[-]</label><label class="expand" for="c-39438700">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but this gray area underperformance that lets them claim they are the cheapest and fastest appeals to people for whom qualitative (aka real) performance doesn’t matter.</div><br/></div></div></div></div><div id="39434608" class="c"><input type="checkbox" id="c-39434608" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431752">root</a><span>|</span><a href="#39434572">parent</a><span>|</span><a href="#39437599">prev</a><span>|</span><a href="#39434917">next</a><span>|</span><label class="collapse" for="c-39434608">[-]</label><label class="expand" for="c-39434608">[1 more]</label></div><br/><div class="children"><div class="content">What quantified testing would you like to see?  We&#x27;ve had a lot of very good feedback from our users, particularly about Mixtral.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39434917" class="c"><input type="checkbox" id="c-39434917" checked=""/><div class="controls bullet"><span class="by">bearjaws</span><span>|</span><a href="#39431752">root</a><span>|</span><a href="#39432333">parent</a><span>|</span><a href="#39432756">prev</a><span>|</span><a href="#39432687">next</a><span>|</span><label class="collapse" for="c-39434917">[-]</label><label class="expand" for="c-39434917">[1 more]</label></div><br/><div class="children"><div class="content">Nothing really wrong with FP8 IMO, it performs pretty damn well usually within 98% while significantly reducing memory usage.</div><br/></div></div></div></div><div id="39432687" class="c"><input type="checkbox" id="c-39432687" checked=""/><div class="controls bullet"><span class="by">losvedir</span><span>|</span><a href="#39431752">parent</a><span>|</span><a href="#39432333">prev</a><span>|</span><a href="#39431770">next</a><span>|</span><label class="collapse" for="c-39432687">[-]</label><label class="expand" for="c-39432687">[2 more]</label></div><br/><div class="children"><div class="content">Maybe I&#x27;m stretching the analogy too far, but are we in the transistor regime of LLMs already? Sometimes I see these 70 billion parameter monstrosities and think we&#x27;re still building ENIAC out of vacuum tubes.<p>In other words, are we ready to steadily march on, improving LLM tok&#x2F;s year by year, or are we a major breakthrough or two away before that can even happen?</div><br/></div></div><div id="39431770" class="c"><input type="checkbox" id="c-39431770" checked=""/><div class="controls bullet"><span class="by">binary132</span><span>|</span><a href="#39431752">parent</a><span>|</span><a href="#39432687">prev</a><span>|</span><a href="#39432291">next</a><span>|</span><label class="collapse" for="c-39431770">[-]</label><label class="expand" for="c-39431770">[1 more]</label></div><br/><div class="children"><div class="content">The thing is that tokens aren&#x27;t an apples to apples metric....  Stupid tokens are a lot faster than clever tokens.  I&#x27;d rather see token cleverness improving exponentially....</div><br/></div></div><div id="39432291" class="c"><input type="checkbox" id="c-39432291" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#39431752">parent</a><span>|</span><a href="#39431770">prev</a><span>|</span><a href="#39429047">next</a><span>|</span><label class="collapse" for="c-39432291">[-]</label><label class="expand" for="c-39432291">[1 more]</label></div><br/><div class="children"><div class="content">tangent: Great to see you again on HN!</div><br/></div></div></div></div><div id="39429047" class="c"><input type="checkbox" id="c-39429047" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431752">prev</a><span>|</span><a href="#39439216">next</a><span>|</span><label class="collapse" for="c-39429047">[-]</label><label class="expand" for="c-39429047">[81 more]</label></div><br/><div class="children"><div class="content">Hi folks, I work for Groq.  Feel free to ask me any questions.<p>(If you check my HN post history you&#x27;ll see I post a lot about Haskell.  That&#x27;s right, part of Groq&#x27;s compilation pipeline is written in Haskell!)</div><br/><div id="39431789" class="c"><input type="checkbox" id="c-39431789" checked=""/><div class="controls bullet"><span class="by">michaelbuckbee</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39431470">next</a><span>|</span><label class="collapse" for="c-39431789">[-]</label><label class="expand" for="c-39431789">[6 more]</label></div><br/><div class="children"><div class="content">Friendly fyi - I think this might just be a web interface bug but but I submitted a prompt with the Mixtral model and got a response (great!) then switched the dropdown to Llama and submitted the same prompt and got the exact same response.<p>It may be caching or it didn&#x27;t change the model being queried or something else.</div><br/><div id="39431921" class="c"><input type="checkbox" id="c-39431921" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39431789">parent</a><span>|</span><a href="#39431470">next</a><span>|</span><label class="collapse" for="c-39431921">[-]</label><label class="expand" for="c-39431921">[5 more]</label></div><br/><div class="children"><div class="content">Thanks, I think it&#x27;s because the chat context is fed back to the model for the next generation even when you switch models.  If you refresh the page that should erase the history and you should get results purely from the model you choose.</div><br/><div id="39432744" class="c"><input type="checkbox" id="c-39432744" checked=""/><div class="controls bullet"><span class="by">michaelbuckbee</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39431921">parent</a><span>|</span><a href="#39431470">next</a><span>|</span><label class="collapse" for="c-39432744">[-]</label><label class="expand" for="c-39432744">[4 more]</label></div><br/><div class="children"><div class="content">Appreciate the quick reply! That&#x27;s interesting.</div><br/><div id="39432977" class="c"><input type="checkbox" id="c-39432977" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432744">parent</a><span>|</span><a href="#39431470">next</a><span>|</span><label class="collapse" for="c-39432977">[-]</label><label class="expand" for="c-39432977">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re welcome.  Thanks for reporting. It&#x27;s pretty confusing so maybe we should change it :)</div><br/><div id="39433927" class="c"><input type="checkbox" id="c-39433927" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432977">parent</a><span>|</span><a href="#39431470">next</a><span>|</span><label class="collapse" for="c-39433927">[-]</label><label class="expand" for="c-39433927">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve always liked how openrouter.ai does it<p>They allow you to configure chat participants (a model + params like context or temp) and then each AI answers each question independently in-line so you can compare and remix outputs.</div><br/><div id="39437534" class="c"><input type="checkbox" id="c-39437534" checked=""/><div class="controls bullet"><span class="by">xanderatallah</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39433927">parent</a><span>|</span><a href="#39431470">next</a><span>|</span><label class="collapse" for="c-39437534">[-]</label><label class="expand" for="c-39437534">[1 more]</label></div><br/><div class="children"><div class="content">openrouter dev here - would love to get Groq access and include it!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39431470" class="c"><input type="checkbox" id="c-39431470" checked=""/><div class="controls bullet"><span class="by">itishappy</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39431789">prev</a><span>|</span><a href="#39431088">next</a><span>|</span><label class="collapse" for="c-39431470">[-]</label><label class="expand" for="c-39431470">[2 more]</label></div><br/><div class="children"><div class="content">Alright, I&#x27;ll bite. Haskell seems pretty unique in the ML space! Any unique benefits to this decision, and would you recommend it for others? What areas of your project do&#x2F;don&#x27;t use Haskell?</div><br/><div id="39432223" class="c"><input type="checkbox" id="c-39432223" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39431470">parent</a><span>|</span><a href="#39431088">next</a><span>|</span><label class="collapse" for="c-39432223">[-]</label><label class="expand" for="c-39432223">[1 more]</label></div><br/><div class="children"><div class="content">Haskell is a great language for writing compilers!  The end of our compilation pipeline is written in Haskell.  Other stages are written in C++ (MLIR) and Python.  I&#x27;d recommend anyone to look at Haskell if they have a compiler-shaped problem, for sure.<p>We also use Haskell on our infra team.  Most of our CI infra is written in Haskell and Nix.  Some of the chip itself was designed in Haskell (or maybe Bluespec, a Haskell-like language for chip design, I&#x27;m not sure).</div><br/></div></div></div></div><div id="39431088" class="c"><input type="checkbox" id="c-39431088" checked=""/><div class="controls bullet"><span class="by">mechagodzilla</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39431470">prev</a><span>|</span><a href="#39432794">next</a><span>|</span><label class="collapse" for="c-39431088">[-]</label><label class="expand" for="c-39431088">[8 more]</label></div><br/><div class="children"><div class="content">You all seem like one of the only companies targeting low-latency inference rather than focusing on throughput (and thus $&#x2F;inference) - what do you see as your primary market?</div><br/><div id="39431929" class="c"><input type="checkbox" id="c-39431929" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39431088">parent</a><span>|</span><a href="#39432794">next</a><span>|</span><label class="collapse" for="c-39431929">[-]</label><label class="expand" for="c-39431929">[7 more]</label></div><br/><div class="children"><div class="content">Yes, because we&#x27;re one of the only companies whose hardware can actually support low latency!  Everyone else is stuck with traditional designs and they try to make up for their high latency by batching to get higher throughput.  But not all applications work with high throughput&#x2F;high latency ... Low latency unlocks feeding the result of one model into the input of another model.  Check out this conversational AI demo on CNN.  You can&#x27;t do that kind of thing unless you have low latency.<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pRUddK6sxDg&amp;t=235s" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pRUddK6sxDg&amp;t=235s</a></div><br/><div id="39433426" class="c"><input type="checkbox" id="c-39433426" checked=""/><div class="controls bullet"><span class="by">vimarsh6739</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39431929">parent</a><span>|</span><a href="#39432794">next</a><span>|</span><label class="collapse" for="c-39433426">[-]</label><label class="expand" for="c-39433426">[6 more]</label></div><br/><div class="children"><div class="content">Might be a bit out of context, but isn&#x27;t the TPU also optimized for low latency inference? (Judging by reading the original TPU architecture paper here - <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1704.04760" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1704.04760</a>). If so, does Groq actually provide hardware support for LLM inference?</div><br/><div id="39433487" class="c"><input type="checkbox" id="c-39433487" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39433426">parent</a><span>|</span><a href="#39432794">next</a><span>|</span><label class="collapse" for="c-39433487">[-]</label><label class="expand" for="c-39433487">[5 more]</label></div><br/><div class="children"><div class="content">Jonathan Ross on that paper is Groq&#x27;s founder and CEO.  Groq&#x27;s LPU is an natural continuation of the breakthrough ideas he had when designing Google&#x27;s TPU.<p>Could you clarify your question about hardware support?  Currently we build out our hardware to support our cloud offering, and we sell systems to enterprise customers.</div><br/><div id="39433652" class="c"><input type="checkbox" id="c-39433652" checked=""/><div class="controls bullet"><span class="by">vimarsh6739</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39433487">parent</a><span>|</span><a href="#39434882">next</a><span>|</span><label class="collapse" for="c-39433652">[-]</label><label class="expand" for="c-39433652">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for the quick reply! About hardware support, I was wondering if the LPU has a hardware instruction to compute the attention matrix similar to the MatrixMultiply&#x2F;Convolve instruction in the TPU ISA. (Maybe a hardware instruction which fuses a softmax on the matmul epilogue?)</div><br/><div id="39433713" class="c"><input type="checkbox" id="c-39433713" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39433652">parent</a><span>|</span><a href="#39434882">next</a><span>|</span><label class="collapse" for="c-39433713">[-]</label><label class="expand" for="c-39433713">[1 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t have a hardware instruction but we do have some patented technology around using a matrix engine to efficiently calculate other linear algebra operations such as convolution.</div><br/></div></div></div></div><div id="39434882" class="c"><input type="checkbox" id="c-39434882" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39433487">parent</a><span>|</span><a href="#39433652">prev</a><span>|</span><a href="#39432794">next</a><span>|</span><label class="collapse" for="c-39434882">[-]</label><label class="expand" for="c-39434882">[2 more]</label></div><br/><div class="children"><div class="content">Are you considering targeting consumer market? There are a lot of people throwing $2k-$4k into local setups and they primarily care about inference.</div><br/><div id="39434930" class="c"><input type="checkbox" id="c-39434930" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39434882">parent</a><span>|</span><a href="#39432794">next</a><span>|</span><label class="collapse" for="c-39434930">[-]</label><label class="expand" for="c-39434930">[1 more]</label></div><br/><div class="children"><div class="content">At the moment we&#x27;re concentrating on building out our API and serving the enterprise market.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39432794" class="c"><input type="checkbox" id="c-39432794" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39431088">prev</a><span>|</span><a href="#39429162">next</a><span>|</span><label class="collapse" for="c-39432794">[-]</label><label class="expand" for="c-39432794">[6 more]</label></div><br/><div class="children"><div class="content">If I understand correctly, you&#x27;re using specialized hardware to improve token generation speed, which is very latency bound on the speed of computation. However generating tokens only requires multiplying 1-dimensional matrices usually. If I enter a prompt with ~100 tokens then your service goes much slower. Probably because you have to multiply 2-dimensional matrices. What are you doing to improve the computation speed of prompt processing?</div><br/><div id="39432953" class="c"><input type="checkbox" id="c-39432953" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432794">parent</a><span>|</span><a href="#39429162">next</a><span>|</span><label class="collapse" for="c-39432953">[-]</label><label class="expand" for="c-39432953">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it should be quadratic in input length.  Why do you think it is?</div><br/><div id="39435545" class="c"><input type="checkbox" id="c-39435545" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432953">parent</a><span>|</span><a href="#39433375">next</a><span>|</span><label class="collapse" for="c-39435545">[-]</label><label class="expand" for="c-39435545">[3 more]</label></div><br/><div class="children"><div class="content">You can ask your website: &quot;What is the computational complexity of self-attention with respect to input sequence length?&quot;<p>It&#x27;ll answer something along the lines of self-attention being O(n^2) (where n is the sequence length) because you have to compute an attention matrix of size n^2.<p>There are other attention mechanisms with better computational complexity, but they usually result in worse large language models. To answer jart: We&#x27;ll have to wait until someone finds a good linear attention mechanism and then wait some more until someone trains a huge model with it (not Groq, they only do inference).</div><br/><div id="39439149" class="c"><input type="checkbox" id="c-39439149" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39435545">parent</a><span>|</span><a href="#39437129">next</a><span>|</span><label class="collapse" for="c-39439149">[-]</label><label class="expand" for="c-39439149">[1 more]</label></div><br/><div class="children"><div class="content">OK, thanks, that&#x27;s useful to know.  Personally I&#x27;m not involved directly in implementing the model, so I don&#x27;t know what we do there.</div><br/></div></div><div id="39437129" class="c"><input type="checkbox" id="c-39437129" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39435545">parent</a><span>|</span><a href="#39439149">prev</a><span>|</span><a href="#39433375">next</a><span>|</span><label class="collapse" for="c-39437129">[-]</label><label class="expand" for="c-39437129">[1 more]</label></div><br/><div class="children"><div class="content">Changing the way transformer models works is orthogonal to gaining good performance on Mistral. Groq did great work reducing the latency considerably of generating tokens during inference. But I wouldn&#x27;t be surprised if they etched the A matrix weights in some kind of fast ROM, used expensive SRAM for the the skinny B matrix, and sent everything else that didn&#x27;t fit to good old fashioned hardware. That&#x27;s great for generating text, but prompt processing is where the power is in AI. In order to process prompts fast, you need to multiply weights against 2-dimensional matrices. There is significant inequality in software implementations alone in terms of how quickly they&#x27;re able to do this, irrespective of hardware. That&#x27;s why things like BLAS libraries exist. So it&#x27;d be super interesting to hear about how a company like Groq that leverages both software and hardware specifically for inference is focusing on tackling its most important aspect.</div><br/></div></div></div></div><div id="39433375" class="c"><input type="checkbox" id="c-39433375" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432953">parent</a><span>|</span><a href="#39435545">prev</a><span>|</span><a href="#39429162">next</a><span>|</span><label class="collapse" for="c-39433375">[-]</label><label class="expand" for="c-39433375">[1 more]</label></div><br/><div class="children"><div class="content">all I know is that when I run llama.cpp a lot of the matrices that get multiplied have their shapes defined by how many tokens are in my prompt. <a href="https:&#x2F;&#x2F;justine.lol&#x2F;tmp&#x2F;shapes.png" rel="nofollow">https:&#x2F;&#x2F;justine.lol&#x2F;tmp&#x2F;shapes.png</a> Notice how the B matrix is always skinny for generating tokens. But for batch processing of the initial prompt, it&#x27;s fat. It&#x27;s not very hard to multiply a skinny matrix but once it&#x27;s fat it gets harder. Handling the initial batch processing of the prompt appears to be what your service goes slow at.</div><br/></div></div></div></div></div></div><div id="39429162" class="c"><input type="checkbox" id="c-39429162" checked=""/><div class="controls bullet"><span class="by">ppsreejith</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39432794">prev</a><span>|</span><a href="#39431433">next</a><span>|</span><label class="collapse" for="c-39429162">[-]</label><label class="expand" for="c-39429162">[16 more]</label></div><br/><div class="children"><div class="content">Thank you for doing this AMA<p>1. How many GroqCards are you using to run the Demo?<p>2. Is there a newer version you&#x27;re using which has more SRAM (since the one I see online only has 230MB)? Since this seems to be the number that will drive down your cost (to take advantage of batch processing, CMIIW!)<p>3. Can TTS pipelines be integrated with your stack? If so, we can truly have very low latency calls!<p>*Assuming you&#x27;re using this: <a href="https:&#x2F;&#x2F;www.bittware.com&#x2F;products&#x2F;groq&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.bittware.com&#x2F;products&#x2F;groq&#x2F;</a></div><br/><div id="39429243" class="c"><input type="checkbox" id="c-39429243" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39429162">parent</a><span>|</span><a href="#39431433">next</a><span>|</span><label class="collapse" for="c-39429243">[-]</label><label class="expand" for="c-39429243">[15 more]</label></div><br/><div class="children"><div class="content">1. I think our GroqChat demo is using 568 GroqChips.  I&#x27;m not sure exactly, but it&#x27;s about that number.<p>2. We&#x27;re working on our second generation chip.  I don&#x27;t know how much SRAM it has exactly but we don&#x27;t need to increase the SRAM to get efficient scaling.  Our system is deterministic, which means no need for waiting or queuing anywhere, and we can have very low latency interconnect between cards.<p>3. Yeah absolutely, see this video of a live demo on CNN!<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?t=235&amp;v=pRUddK6sxDg" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?t=235&amp;v=pRUddK6sxDg</a></div><br/><div id="39429446" class="c"><input type="checkbox" id="c-39429446" checked=""/><div class="controls bullet"><span class="by">ppsreejith</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39429243">parent</a><span>|</span><a href="#39433092">next</a><span>|</span><label class="collapse" for="c-39429446">[-]</label><label class="expand" for="c-39429446">[4 more]</label></div><br/><div class="children"><div class="content">Thank you, that demo was insane!<p>Follow up (noob) question: Are you using a KV cache? That would significantly increase your memory requirements. Or are you forwarding the whole prompt for each auto-regressive pass?</div><br/><div id="39429575" class="c"><input type="checkbox" id="c-39429575" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39429446">parent</a><span>|</span><a href="#39433092">next</a><span>|</span><label class="collapse" for="c-39429575">[-]</label><label class="expand" for="c-39429575">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re welcome! Yes, we have KV cache.  Being able to implement this efficiently in terms of hardware requirements and compute time is one of the benefits of our deterministic chip architecture (and deterministic system architecture).</div><br/><div id="39429609" class="c"><input type="checkbox" id="c-39429609" checked=""/><div class="controls bullet"><span class="by">ppsreejith</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39429575">parent</a><span>|</span><a href="#39433092">next</a><span>|</span><label class="collapse" for="c-39429609">[-]</label><label class="expand" for="c-39429609">[2 more]</label></div><br/><div class="children"><div class="content">Thanks again! Hope I&#x27;m not overwhelming but one more question: Are you decoding with batch size = 1 or is it more?</div><br/><div id="39429663" class="c"><input type="checkbox" id="c-39429663" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39429609">parent</a><span>|</span><a href="#39433092">next</a><span>|</span><label class="collapse" for="c-39429663">[-]</label><label class="expand" for="c-39429663">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s OK, feel free to keep asking!<p>I think currently 1.  Unlike with graphics processors, which really need data parallelism to get good throughput, our LPU architecture allows us to deliver good throughput even at batch size 1.</div><br/></div></div></div></div></div></div></div></div><div id="39433092" class="c"><input type="checkbox" id="c-39433092" checked=""/><div class="controls bullet"><span class="by">gautamcgoel</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39429243">parent</a><span>|</span><a href="#39429446">prev</a><span>|</span><a href="#39430863">next</a><span>|</span><label class="collapse" for="c-39433092">[-]</label><label class="expand" for="c-39433092">[2 more]</label></div><br/><div class="children"><div class="content">Can you talk about the interconnect? Is it fully custom as well? How do you achieve low latency?</div><br/><div id="39433166" class="c"><input type="checkbox" id="c-39433166" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39433092">parent</a><span>|</span><a href="#39430863">next</a><span>|</span><label class="collapse" for="c-39433166">[-]</label><label class="expand" for="c-39433166">[1 more]</label></div><br/><div class="children"><div class="content">You can find out about the chip to chip interconnect from our paper below, section 2.3.  I don&#x27;t think that&#x27;s custom.<p>We achieve low latency by basically being a software-defined architecture.  Our functional units operate completely orthoganal to each other.  We don&#x27;t have to batch in order to achieve parallelism and the system behaviour is completely deterministic, so we can schedule all operations precisely.<p><a href="https:&#x2F;&#x2F;wow.groq.com&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;05&#x2F;GroqISCAPaper2022_ASoftwareDefinedTensorStreamingMultiprocessorForLargeScaleMachineLearning-1.pdf" rel="nofollow">https:&#x2F;&#x2F;wow.groq.com&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;05&#x2F;GroqISCAPape...</a></div><br/></div></div></div></div><div id="39430863" class="c"><input type="checkbox" id="c-39430863" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39429243">parent</a><span>|</span><a href="#39433092">prev</a><span>|</span><a href="#39431433">next</a><span>|</span><label class="collapse" for="c-39430863">[-]</label><label class="expand" for="c-39430863">[8 more]</label></div><br/><div class="children"><div class="content">How much do 568 chips cost? What’s the cost ratio of it comparing to setup with roughly the same throughput using A100?</div><br/><div id="39430971" class="c"><input type="checkbox" id="c-39430971" checked=""/><div class="controls bullet"><span class="by">benchess</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39430863">parent</a><span>|</span><a href="#39431433">next</a><span>|</span><label class="collapse" for="c-39430971">[-]</label><label class="expand" for="c-39430971">[7 more]</label></div><br/><div class="children"><div class="content">They’re for sale on Mouser for $20625 each <a href="https:&#x2F;&#x2F;www.mouser.com&#x2F;ProductDetail&#x2F;BittWare&#x2F;RS-GQ-GC1-0109?qs=ST9lo4GX8V2eGrFMeVQmFw%3D%3D" rel="nofollow">https:&#x2F;&#x2F;www.mouser.com&#x2F;ProductDetail&#x2F;BittWare&#x2F;RS-GQ-GC1-0109...</a><p>At that price 568 chips would be $11.7M</div><br/><div id="39437226" class="c"><input type="checkbox" id="c-39437226" checked=""/><div class="controls bullet"><span class="by">fennecbutt</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39430971">parent</a><span>|</span><a href="#39431004">next</a><span>|</span><label class="collapse" for="c-39437226">[-]</label><label class="expand" for="c-39437226">[2 more]</label></div><br/><div class="children"><div class="content">I presume that&#x27;s because it&#x27;s a custom asic not yet in mass production?<p>If they can get costs down and put more dies into each card then it&#x27;ll be business&#x2F;consumer friendly.<p>Let&#x27;s see if they can scale production.<p>Also, where tf is the next coral chip, alphabet been slacking hard.</div><br/><div id="39438297" class="c"><input type="checkbox" id="c-39438297" checked=""/><div class="controls bullet"><span class="by">bethekind</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39437226">parent</a><span>|</span><a href="#39431004">next</a><span>|</span><label class="collapse" for="c-39438297">[-]</label><label class="expand" for="c-39438297">[1 more]</label></div><br/><div class="children"><div class="content">I think Coral has been taken to the wooden shed out back. Nothing new out of them for years sadly</div><br/></div></div></div></div><div id="39431004" class="c"><input type="checkbox" id="c-39431004" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39430971">parent</a><span>|</span><a href="#39437226">prev</a><span>|</span><a href="#39431020">next</a><span>|</span><label class="collapse" for="c-39431004">[-]</label><label class="expand" for="c-39431004">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I don&#x27;t know what the cost to us is to build out our own hardware but it&#x27;s significantly less expensive than retail.</div><br/></div></div><div id="39431020" class="c"><input type="checkbox" id="c-39431020" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39430971">parent</a><span>|</span><a href="#39431004">prev</a><span>|</span><a href="#39431433">next</a><span>|</span><label class="collapse" for="c-39431020">[-]</label><label class="expand" for="c-39431020">[3 more]</label></div><br/><div class="children"><div class="content">That seems to be per card instead of chip. I would expect it has multiple chips on a single card.</div><br/><div id="39431099" class="c"><input type="checkbox" id="c-39431099" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39431020">parent</a><span>|</span><a href="#39431433">next</a><span>|</span><label class="collapse" for="c-39431099">[-]</label><label class="expand" for="c-39431099">[2 more]</label></div><br/><div class="children"><div class="content">From the description that doesn&#x27;t seem to be the case, but I don&#x27;t know this product well<p>&gt; <i>Accelerator Cards GroqCard low latency AI&#x2F;ML Inference PCIe accelerator card with single GroqChip</i></div><br/><div id="39431121" class="c"><input type="checkbox" id="c-39431121" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39431099">parent</a><span>|</span><a href="#39431433">next</a><span>|</span><label class="collapse" for="c-39431121">[-]</label><label class="expand" for="c-39431121">[1 more]</label></div><br/><div class="children"><div class="content">Missed that! Thanks for pointing out!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39431433" class="c"><input type="checkbox" id="c-39431433" checked=""/><div class="controls bullet"><span class="by">andy_xor_andrew</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39429162">prev</a><span>|</span><a href="#39432013">next</a><span>|</span><label class="collapse" for="c-39431433">[-]</label><label class="expand" for="c-39431433">[2 more]</label></div><br/><div class="children"><div class="content">are your accelerator chips designed in-house? or they&#x27;re some specialized silicon or FPGPU or something that you wrote very optimized code for inference?<p>it&#x27;s really amazing! the first time I tried the demo, I had to try a few prompts to believe it wasn&#x27;t just an animation :)</div><br/><div id="39432000" class="c"><input type="checkbox" id="c-39432000" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39431433">parent</a><span>|</span><a href="#39432013">next</a><span>|</span><label class="collapse" for="c-39432000">[-]</label><label class="expand" for="c-39432000">[1 more]</label></div><br/><div class="children"><div class="content">Yup, custom ASIC, designed in-house, built into a system of several racks, hundreds of chips, with fast interconnect.  Really glad you enjoyed it!</div><br/></div></div></div></div><div id="39432013" class="c"><input type="checkbox" id="c-39432013" checked=""/><div class="controls bullet"><span class="by">UncleOxidant</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39431433">prev</a><span>|</span><a href="#39432040">next</a><span>|</span><label class="collapse" for="c-39432013">[-]</label><label class="expand" for="c-39432013">[9 more]</label></div><br/><div class="children"><div class="content">When will we be able to buy Groq accelerator cards that would be affordable for hobbyists?</div><br/><div id="39432157" class="c"><input type="checkbox" id="c-39432157" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432013">parent</a><span>|</span><a href="#39432040">next</a><span>|</span><label class="collapse" for="c-39432157">[-]</label><label class="expand" for="c-39432157">[8 more]</label></div><br/><div class="children"><div class="content">We are prioritising building out whole systems at the moment I don&#x27;t think we&#x27;ll have a consumer level offering in the near  future.</div><br/><div id="39433748" class="c"><input type="checkbox" id="c-39433748" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432157">parent</a><span>|</span><a href="#39432040">next</a><span>|</span><label class="collapse" for="c-39433748">[-]</label><label class="expand" for="c-39433748">[7 more]</label></div><br/><div class="children"><div class="content">I will mention: A lot of innovation in this space comes bottom-up. The sooner you can get something in the hands of individuals and smaller institutions, the better your market position will be.<p>I&#x27;m coding to NVidia right now. That builds them a moat. The instant I can get other hardware working, the less of a moat they will have. The more open it is, the more likely I am to adopt it.</div><br/><div id="39433969" class="c"><input type="checkbox" id="c-39433969" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39433748">parent</a><span>|</span><a href="#39432040">next</a><span>|</span><label class="collapse" for="c-39433969">[-]</label><label class="expand" for="c-39433969">[6 more]</label></div><br/><div class="children"><div class="content">Definitely, that&#x27;s why we&#x27;ve opened our API to everyone.</div><br/><div id="39434223" class="c"><input type="checkbox" id="c-39434223" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39433969">parent</a><span>|</span><a href="#39432040">next</a><span>|</span><label class="collapse" for="c-39434223">[-]</label><label class="expand" for="c-39434223">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that quite does it. What I&#x27;d want -- if you want me to support you -- is access to the chip, libraries, and API documentation.<p>Best-case would be something I buy for &lt;$2k (if out-of-pocket) or under $5k (if employer). Next best case would be a cloud service with a limited free tier. It&#x27;s okay if it has barely enough quota that I can develop to it, but the quota should never expire.<p>(The mistake a lot of services make is to limit free tier to e.g. 30 day or 1 year, rather than hours&#x2F;month; if I didn&#x27;t get around to evaluating, switch employers, switch projects, etc. the free tier is gone).<p>I did sign up for your API service. I won&#x27;t be able to use it in prod before your (very nice) privacy guarantees are turned into lawyer-compliant regulatory language. But it&#x27;s an almost ideal fit for my application.</div><br/><div id="39436642" class="c"><input type="checkbox" id="c-39436642" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39434223">parent</a><span>|</span><a href="#39434442">next</a><span>|</span><label class="collapse" for="c-39436642">[-]</label><label class="expand" for="c-39436642">[1 more]</label></div><br/><div class="children"><div class="content">I don’t really understand this. If you are happy to buy a &lt;2K card, then what does it matter if the service is paid or not? Clearly you have enough disposable income to not care about a ‘free’ tier.</div><br/></div></div><div id="39434442" class="c"><input type="checkbox" id="c-39434442" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39434223">parent</a><span>|</span><a href="#39436642">prev</a><span>|</span><a href="#39432040">next</a><span>|</span><label class="collapse" for="c-39434442">[-]</label><label class="expand" for="c-39434442">[3 more]</label></div><br/><div class="children"><div class="content">Yup, understood. Access to consumer hardware like this is not something that we provide at the moment, I&#x27;m afraid.</div><br/><div id="39435269" class="c"><input type="checkbox" id="c-39435269" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39434442">parent</a><span>|</span><a href="#39432040">next</a><span>|</span><label class="collapse" for="c-39435269">[-]</label><label class="expand" for="c-39435269">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t blame you. Been at plenty of startups, resources are finite, and focus is important.<p>My only point was to, well, perhaps bump this up from #100 on your personal priority list perhaps to #87, to the limited extent that influences your business.</div><br/><div id="39435896" class="c"><input type="checkbox" id="c-39435896" checked=""/><div class="controls bullet"><span class="by">matanyal</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39435269">parent</a><span>|</span><a href="#39432040">next</a><span>|</span><label class="collapse" for="c-39435896">[-]</label><label class="expand" for="c-39435896">[1 more]</label></div><br/><div class="children"><div class="content">Groq Engineer here as well; we actually built our compiler to compile pytorch, TensorFlow, and Onnx natively, so a lot of the amazing work being done by y&#x27;all isn&#x27;t building much of a moat. We got LLama2 working on our hardware in just a couple of days!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39432040" class="c"><input type="checkbox" id="c-39432040" checked=""/><div class="controls bullet"><span class="by">kkzz99</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39432013">prev</a><span>|</span><a href="#39435181">next</a><span>|</span><label class="collapse" for="c-39432040">[-]</label><label class="expand" for="c-39432040">[6 more]</label></div><br/><div class="children"><div class="content">How does the Groq PCIE Card work exactly? Does it use system ram to stream the model data to the card? How many T&#x2F;s could one expect with e.g. 36000Mhz DDR4 Ram?</div><br/><div id="39432265" class="c"><input type="checkbox" id="c-39432265" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432040">parent</a><span>|</span><a href="#39435181">next</a><span>|</span><label class="collapse" for="c-39432265">[-]</label><label class="expand" for="c-39432265">[5 more]</label></div><br/><div class="children"><div class="content">We build out large systems where we stream in the model weights to the system once and then run multiple inferences on it. We don&#x27;t really recommend streaming model weights repeatedly onto the chip because you&#x27;ll lose the benefits of low latency.</div><br/><div id="39432340" class="c"><input type="checkbox" id="c-39432340" checked=""/><div class="controls bullet"><span class="by">kkzz99</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432265">parent</a><span>|</span><a href="#39435181">next</a><span>|</span><label class="collapse" for="c-39432340">[-]</label><label class="expand" for="c-39432340">[4 more]</label></div><br/><div class="children"><div class="content">How does that work when the card only has 230MB of SRAM?</div><br/><div id="39432392" class="c"><input type="checkbox" id="c-39432392" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432340">parent</a><span>|</span><a href="#39435181">next</a><span>|</span><label class="collapse" for="c-39432392">[-]</label><label class="expand" for="c-39432392">[3 more]</label></div><br/><div class="children"><div class="content">We connect hundreds of chips across several racks with fast interconnect.</div><br/><div id="39433817" class="c"><input type="checkbox" id="c-39433817" checked=""/><div class="controls bullet"><span class="by">AhtiK</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432392">parent</a><span>|</span><a href="#39435181">next</a><span>|</span><label class="collapse" for="c-39433817">[-]</label><label class="expand" for="c-39433817">[2 more]</label></div><br/><div class="children"><div class="content">How fast is the memory bandwidth of that fast interconnect?</div><br/><div id="39433955" class="c"><input type="checkbox" id="c-39433955" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39433817">parent</a><span>|</span><a href="#39435181">next</a><span>|</span><label class="collapse" for="c-39433955">[-]</label><label class="expand" for="c-39433955">[1 more]</label></div><br/><div class="children"><div class="content">Have a look at section 2.3 of our paper. Between any two chips we get 100 Gbps.  The overall bandwidth depends on the connection topology used. I don&#x27;t know if we make that public.<p><a href="https:&#x2F;&#x2F;wow.groq.com&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;05&#x2F;GroqISCAPaper2022_ASoftwareDefinedTensorStreamingMultiprocessorForLargeScaleMachineLearning-1.pdf" rel="nofollow">https:&#x2F;&#x2F;wow.groq.com&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;05&#x2F;GroqISCAPape...</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39435181" class="c"><input type="checkbox" id="c-39435181" checked=""/><div class="controls bullet"><span class="by">amirhirsch</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39432040">prev</a><span>|</span><a href="#39431757">next</a><span>|</span><label class="collapse" for="c-39435181">[-]</label><label class="expand" for="c-39435181">[2 more]</label></div><br/><div class="children"><div class="content">It seems like you are making general purpose chips to run many models. Are we at a stage where we can consider taping out inference networks directly propagating the weights as constants in the RTL design?<p>Are chips and models obsoleted on roughly the same timelines?</div><br/><div id="39435222" class="c"><input type="checkbox" id="c-39435222" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39435181">parent</a><span>|</span><a href="#39431757">next</a><span>|</span><label class="collapse" for="c-39435222">[-]</label><label class="expand" for="c-39435222">[1 more]</label></div><br/><div class="children"><div class="content">I think the models change far too quickly for that to be viable.  A chip has to last several years.  Currently we&#x27;re seeing groundbreaking models released every few months.</div><br/></div></div></div></div><div id="39432587" class="c"><input type="checkbox" id="c-39432587" checked=""/><div class="controls bullet"><span class="by">tudorw</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39431757">prev</a><span>|</span><a href="#39431052">next</a><span>|</span><label class="collapse" for="c-39432587">[-]</label><label class="expand" for="c-39432587">[1 more]</label></div><br/><div class="children"><div class="content">As it works at inference do you think &#x27;Representation Engineering &#x27; could be applied to give a sort of fine-tuning ability? <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39414532">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39414532</a></div><br/></div></div><div id="39431052" class="c"><input type="checkbox" id="c-39431052" checked=""/><div class="controls bullet"><span class="by">dkhudia</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39432587">prev</a><span>|</span><a href="#39431513">next</a><span>|</span><label class="collapse" for="c-39431052">[-]</label><label class="expand" for="c-39431052">[3 more]</label></div><br/><div class="children"><div class="content">@tome for the deterministic system, what if the timing for one chip&#x2F;part is off due to manufacturing&#x2F;environmental factors (e.g., temperature) ? How does the system handle this?</div><br/><div id="39431194" class="c"><input type="checkbox" id="c-39431194" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39431052">parent</a><span>|</span><a href="#39431113">next</a><span>|</span><label class="collapse" for="c-39431194">[-]</label><label class="expand" for="c-39431194">[1 more]</label></div><br/><div class="children"><div class="content">We know the maximum possible clock drift and so we know when we need to do a resynchronisation to keep all the chips in sync.  You can read about it in section 3.3 of our recent whitepaper: <a href="https:&#x2F;&#x2F;wow.groq.com&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;05&#x2F;GroqISCAPaper2022_ASoftwareDefinedTensorStreamingMultiprocessorForLargeScaleMachineLearning-1.pdf" rel="nofollow">https:&#x2F;&#x2F;wow.groq.com&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;05&#x2F;GroqISCAPape...</a></div><br/></div></div><div id="39431113" class="c"><input type="checkbox" id="c-39431113" checked=""/><div class="controls bullet"><span class="by">mechagodzilla</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39431052">parent</a><span>|</span><a href="#39431194">prev</a><span>|</span><a href="#39431513">next</a><span>|</span><label class="collapse" for="c-39431113">[-]</label><label class="expand" for="c-39431113">[1 more]</label></div><br/><div class="children"><div class="content">Those sorts of issues are part of timing analysis for a chip, but once a chip&#x27;s clock rate is set, they don&#x27;t really factor in unless there is some kind of dynamic voltage&#x2F;frequency scaling scheme going on. This chip probably does not do any of that and just uses a fixed frequency, so timing is perfectly predictable.</div><br/></div></div></div></div><div id="39431513" class="c"><input type="checkbox" id="c-39431513" checked=""/><div class="controls bullet"><span class="by">ianpurton</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39431052">prev</a><span>|</span><a href="#39433099">next</a><span>|</span><label class="collapse" for="c-39431513">[-]</label><label class="expand" for="c-39431513">[4 more]</label></div><br/><div class="children"><div class="content">Is it possible to buy Groq chips and how much do they cost?</div><br/><div id="39431951" class="c"><input type="checkbox" id="c-39431951" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39431513">parent</a><span>|</span><a href="#39433099">next</a><span>|</span><label class="collapse" for="c-39431951">[-]</label><label class="expand" for="c-39431951">[3 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.mouser.com&#x2F;ProductDetail&#x2F;BittWare&#x2F;RS-GQ-GC1-0109?qs=ST9lo4GX8V2eGrFMeVQmFw%3D%3D" rel="nofollow">https:&#x2F;&#x2F;www.mouser.com&#x2F;ProductDetail&#x2F;BittWare&#x2F;RS-GQ-GC1-0109...</a></div><br/><div id="39432024" class="c"><input type="checkbox" id="c-39432024" checked=""/><div class="controls bullet"><span class="by">UncleOxidant</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39431951">parent</a><span>|</span><a href="#39433099">next</a><span>|</span><label class="collapse" for="c-39432024">[-]</label><label class="expand" for="c-39432024">[2 more]</label></div><br/><div class="children"><div class="content">Only $20,625.00!</div><br/><div id="39438586" class="c"><input type="checkbox" id="c-39438586" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432024">parent</a><span>|</span><a href="#39433099">next</a><span>|</span><label class="collapse" for="c-39438586">[-]</label><label class="expand" for="c-39438586">[1 more]</label></div><br/><div class="children"><div class="content">Per chip? So the current demo with 568 chips costs.... $11,715,000?!?!</div><br/></div></div></div></div></div></div></div></div><div id="39433099" class="c"><input type="checkbox" id="c-39433099" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39431513">prev</a><span>|</span><a href="#39432758">next</a><span>|</span><label class="collapse" for="c-39433099">[-]</label><label class="expand" for="c-39433099">[1 more]</label></div><br/><div class="children"><div class="content">FYI, I only see a repeating animation and nothing else in my iPhone on lockdown mode, with Safari or Firefox.</div><br/></div></div><div id="39432758" class="c"><input type="checkbox" id="c-39432758" checked=""/><div class="controls bullet"><span class="by">karthityrion</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39433099">prev</a><span>|</span><a href="#39433247">next</a><span>|</span><label class="collapse" for="c-39432758">[-]</label><label class="expand" for="c-39432758">[4 more]</label></div><br/><div class="children"><div class="content">Hi. Are these ASICs only for LLMs or could they accelerate other kinds of models(vision) as well?</div><br/><div id="39432996" class="c"><input type="checkbox" id="c-39432996" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432758">parent</a><span>|</span><a href="#39433247">next</a><span>|</span><label class="collapse" for="c-39432996">[-]</label><label class="expand" for="c-39432996">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a general purpose compute engine for numerical computing and linear algebra, so it can accelerate any ML workloads.  Previously we&#x27;ve accelerated models for stabilising fusion reactions and for COVID drug discovery<p>* <a href="https:&#x2F;&#x2F;alcf.anl.gov&#x2F;news&#x2F;researchers-accelerate-fusion-research-argonne-s-groq-ai-platform" rel="nofollow">https:&#x2F;&#x2F;alcf.anl.gov&#x2F;news&#x2F;researchers-accelerate-fusion-rese...</a><p>* <a href="https:&#x2F;&#x2F;wow.groq.com&#x2F;groq-accelerates-covid-drug-discovery-333x-versus-legacy-solutions&#x2F;" rel="nofollow">https:&#x2F;&#x2F;wow.groq.com&#x2F;groq-accelerates-covid-drug-discovery-3...</a></div><br/><div id="39433163" class="c"><input type="checkbox" id="c-39433163" checked=""/><div class="controls bullet"><span class="by">karthityrion</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432996">parent</a><span>|</span><a href="#39433247">next</a><span>|</span><label class="collapse" for="c-39433163">[-]</label><label class="expand" for="c-39433163">[2 more]</label></div><br/><div class="children"><div class="content">So, is this specific chip only for LLMs as the name LPU suggests Language Processing Unit, right?</div><br/><div id="39433758" class="c"><input type="checkbox" id="c-39433758" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39433163">parent</a><span>|</span><a href="#39433247">next</a><span>|</span><label class="collapse" for="c-39433758">[-]</label><label class="expand" for="c-39433758">[1 more]</label></div><br/><div class="children"><div class="content">The chip is capable of running general numerical compute, but because we&#x27;re focusing almost entirely on LLMs at the moment we&#x27;ve branded it the LPU.</div><br/></div></div></div></div></div></div></div></div><div id="39433247" class="c"><input type="checkbox" id="c-39433247" checked=""/><div class="controls bullet"><span class="by">liberix</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39432758">prev</a><span>|</span><a href="#39432943">next</a><span>|</span><label class="collapse" for="c-39433247">[-]</label><label class="expand" for="c-39433247">[1 more]</label></div><br/><div class="children"><div class="content">How do I sign up for API access? What payment methods do you support?</div><br/></div></div><div id="39432943" class="c"><input type="checkbox" id="c-39432943" checked=""/><div class="controls bullet"><span class="by">BryanLegend</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39433247">prev</a><span>|</span><a href="#39430924">next</a><span>|</span><label class="collapse" for="c-39432943">[-]</label><label class="expand" for="c-39432943">[2 more]</label></div><br/><div class="children"><div class="content">How well would your hardware work for image&#x2F;video generation?</div><br/><div id="39432968" class="c"><input type="checkbox" id="c-39432968" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39432943">parent</a><span>|</span><a href="#39430924">next</a><span>|</span><label class="collapse" for="c-39432968">[-]</label><label class="expand" for="c-39432968">[1 more]</label></div><br/><div class="children"><div class="content">It should work great as far as I know.  We&#x27;ve implemented some diffusion models for image generation but we don&#x27;t offer them at the moment.  I&#x27;m not aware of us having implemented any video models.</div><br/></div></div></div></div><div id="39430924" class="c"><input type="checkbox" id="c-39430924" checked=""/><div class="controls bullet"><span class="by">Oras</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39432943">prev</a><span>|</span><a href="#39431431">next</a><span>|</span><label class="collapse" for="c-39430924">[-]</label><label class="expand" for="c-39430924">[2 more]</label></div><br/><div class="children"><div class="content">Impressive speed. Are there any plans to run fine-tuned models?</div><br/><div id="39431364" class="c"><input type="checkbox" id="c-39431364" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39430924">parent</a><span>|</span><a href="#39431431">next</a><span>|</span><label class="collapse" for="c-39431364">[-]</label><label class="expand" for="c-39431364">[1 more]</label></div><br/><div class="children"><div class="content">Yes, we&#x27;re working on a feature to give our partners the ability to deploy their own fine-tuned models.</div><br/></div></div></div></div><div id="39431431" class="c"><input type="checkbox" id="c-39431431" checked=""/><div class="controls bullet"><span class="by">phh</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39430924">prev</a><span>|</span><a href="#39433119">next</a><span>|</span><label class="collapse" for="c-39431431">[-]</label><label class="expand" for="c-39431431">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re running fp32 models, fp16 or quantized?</div><br/><div id="39432025" class="c"><input type="checkbox" id="c-39432025" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39431431">parent</a><span>|</span><a href="#39433119">next</a><span>|</span><label class="collapse" for="c-39432025">[-]</label><label class="expand" for="c-39432025">[1 more]</label></div><br/><div class="children"><div class="content">FP16 for calculating all activations.  Some data is stored as FP8 at rest.</div><br/></div></div></div></div><div id="39433119" class="c"><input type="checkbox" id="c-39433119" checked=""/><div class="controls bullet"><span class="by">karthityrion</span><span>|</span><a href="#39429047">parent</a><span>|</span><a href="#39431431">prev</a><span>|</span><a href="#39439216">next</a><span>|</span><label class="collapse" for="c-39433119">[-]</label><label class="expand" for="c-39433119">[2 more]</label></div><br/><div class="children"><div class="content">What is the underlying architecture of the ASICs. Does it use systolic arrays?</div><br/><div id="39433506" class="c"><input type="checkbox" id="c-39433506" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39429047">root</a><span>|</span><a href="#39433119">parent</a><span>|</span><a href="#39439216">next</a><span>|</span><label class="collapse" for="c-39433506">[-]</label><label class="expand" for="c-39433506">[1 more]</label></div><br/><div class="children"><div class="content">Yes, our matrix engine is quite similar to a systolic array.  You can find more details about our architecture in our paper:<p><a href="https:&#x2F;&#x2F;wow.groq.com&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;05&#x2F;GroqISCAPaper2022_ASoftwareDefinedTensorStreamingMultiprocessorForLargeScaleMachineLearning-1.pdf" rel="nofollow">https:&#x2F;&#x2F;wow.groq.com&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;05&#x2F;GroqISCAPape...</a></div><br/></div></div></div></div></div></div><div id="39439216" class="c"><input type="checkbox" id="c-39439216" checked=""/><div class="controls bullet"><span class="by">blackoil</span><span>|</span><a href="#39429047">prev</a><span>|</span><a href="#39432376">next</a><span>|</span><label class="collapse" for="c-39439216">[-]</label><label class="expand" for="c-39439216">[1 more]</label></div><br/><div class="children"><div class="content">If Nvidia adds L1&#x2F;2&#x2F;3 cache in next gen of AI cards, will they work similar or is this something more?</div><br/></div></div><div id="39432376" class="c"><input type="checkbox" id="c-39432376" checked=""/><div class="controls bullet"><span class="by">imiric</span><span>|</span><a href="#39439216">prev</a><span>|</span><a href="#39433823">next</a><span>|</span><label class="collapse" for="c-39432376">[-]</label><label class="expand" for="c-39432376">[8 more]</label></div><br/><div class="children"><div class="content">Impressive demo!<p>However, the hardware requirements and cost make this inaccessible for anyone but large companies. When do you envision that the price could be affordable for hobbyists?<p>Also, while the CNN Vapi demo was impressive as well, a few weeks ago here[1] someone shared <a href="https:&#x2F;&#x2F;smarterchild.chat&#x2F;" rel="nofollow">https:&#x2F;&#x2F;smarterchild.chat&#x2F;</a>. That also has _very_ low audio latency, making natural conversation possible. From that discussion it seems that <a href="https:&#x2F;&#x2F;www.sindarin.tech&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.sindarin.tech&#x2F;</a> is behind it. Do we know if they use Groq LPUs or something else?<p>I think that once you reach ~50 t&#x2F;s, real-time interaction is possible. Anything higher than that is useful for generating large volumes of data quickly, but there are diminishing returns as it&#x27;s far beyond what humans can process. Maybe such speeds would be useful for AI-AI communication, transferring knowledge&#x2F;context, etc.<p>So an LPU product that&#x27;s only focused on AI-human interaction could have much lower capabilities, and thus much lower cost, no?<p>[1]: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39180237">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39180237</a></div><br/><div id="39432432" class="c"><input type="checkbox" id="c-39432432" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39432376">parent</a><span>|</span><a href="#39434837">next</a><span>|</span><label class="collapse" for="c-39432432">[-]</label><label class="expand" for="c-39432432">[5 more]</label></div><br/><div class="children"><div class="content">&gt; However, the hardware requirements and cost make this inaccessible for anyone but large companies. When do you envision that the price could be affordable for hobbyists?<p>For API access to our tokens as a service we guarantee to beat any other provider on cost per token (see <a href="https:&#x2F;&#x2F;wow.groq.com" rel="nofollow">https:&#x2F;&#x2F;wow.groq.com</a>).  In terms of selling hardware, we&#x27;re focused on selling whole systems, and they&#x27;re only really suitable for corporations or research institutions.</div><br/><div id="39436467" class="c"><input type="checkbox" id="c-39436467" checked=""/><div class="controls bullet"><span class="by">pwillia7</span><span>|</span><a href="#39432376">root</a><span>|</span><a href="#39432432">parent</a><span>|</span><a href="#39437770">next</a><span>|</span><label class="collapse" for="c-39436467">[-]</label><label class="expand" for="c-39436467">[2 more]</label></div><br/><div class="children"><div class="content">Do you have any data on how many more tokens I would use with the increased speed?<p>In the demo alone I just used way more tokens than I normally would testing an LLM since it was so amazingly fast.</div><br/><div id="39439062" class="c"><input type="checkbox" id="c-39439062" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39432376">root</a><span>|</span><a href="#39436467">parent</a><span>|</span><a href="#39437770">next</a><span>|</span><label class="collapse" for="c-39439062">[-]</label><label class="expand" for="c-39439062">[1 more]</label></div><br/><div class="children"><div class="content">Interesting question!  Hopefully being faster is so much more useful to you that you use a lot more :)</div><br/></div></div></div></div><div id="39437770" class="c"><input type="checkbox" id="c-39437770" checked=""/><div class="controls bullet"><span class="by">dsrtslnd23</span><span>|</span><a href="#39432376">root</a><span>|</span><a href="#39432432">parent</a><span>|</span><a href="#39436467">prev</a><span>|</span><a href="#39434837">next</a><span>|</span><label class="collapse" for="c-39437770">[-]</label><label class="expand" for="c-39437770">[2 more]</label></div><br/><div class="children"><div class="content">How open is your early access? i.e. likelihood to get API access granted right now</div><br/><div id="39439058" class="c"><input type="checkbox" id="c-39439058" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39432376">root</a><span>|</span><a href="#39437770">parent</a><span>|</span><a href="#39434837">next</a><span>|</span><label class="collapse" for="c-39439058">[-]</label><label class="expand" for="c-39439058">[1 more]</label></div><br/><div class="children"><div class="content">We are absolutely slammed with requests right now, so I don&#x27;t know, sorry.</div><br/></div></div></div></div></div></div><div id="39434837" class="c"><input type="checkbox" id="c-39434837" checked=""/><div class="controls bullet"><span class="by">stormfather</span><span>|</span><a href="#39432376">parent</a><span>|</span><a href="#39432432">prev</a><span>|</span><a href="#39433971">next</a><span>|</span><label class="collapse" for="c-39434837">[-]</label><label class="expand" for="c-39434837">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt;50 t&#x2F;s is absolutely necessary for real-time interaction with AI systems. Most of the LLM&#x27;s output will be internal monologue and planning, performing RAG and summarization, etc, with only the final output being communicated to you. Imagine a blazingly fast GPT-5 that goes through multiple cycles of planning out how to answer you, searching the web, writing book reports, debating itself, distilling what it finds, critiquing and rewriting its answer, all while you blink a few times.</div><br/></div></div><div id="39433971" class="c"><input type="checkbox" id="c-39433971" checked=""/><div class="controls bullet"><span class="by">dmw_ng</span><span>|</span><a href="#39432376">parent</a><span>|</span><a href="#39434837">prev</a><span>|</span><a href="#39433823">next</a><span>|</span><label class="collapse" for="c-39433971">[-]</label><label class="expand" for="c-39433971">[1 more]</label></div><br/><div class="children"><div class="content">Given the size of the Sindarin team (3 AFAICT), that mostly looks like a clever combination of existing tech. There are some speech APIs that offer word-by-word realtime transcription (Google has one), assuming most of the special sauce is very well thought out pipelining between speech recognition-&gt;LLM-&gt;TTS<p>(not to denigrate their awesome achievement, I would not be interested if I were not curious about how to reproduce their result!)</div><br/></div></div></div></div><div id="39433823" class="c"><input type="checkbox" id="c-39433823" checked=""/><div class="controls bullet"><span class="by">SeanAnderson</span><span>|</span><a href="#39432376">prev</a><span>|</span><a href="#39433126">next</a><span>|</span><label class="collapse" for="c-39433823">[-]</label><label class="expand" for="c-39433823">[16 more]</label></div><br/><div class="children"><div class="content">Sorry, I&#x27;m a bit naïve about all of this.<p>Why is this impressive? Can this result not be achieved by throwing more compute at the problem to speed up responses? Isn&#x27;t the fact that there is a queue when under load just indicative that there&#x27;s a trade-off between &quot;# of request to process per unit of time&quot; and &quot;amount of compute to put into a response to respond quicker&quot;?<p><a href="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;NVIDIA&#x2F;TensorRT-LLM&#x2F;rel&#x2F;docs&#x2F;source&#x2F;blogs&#x2F;media&#x2F;TRT_LLM_v0-5-0_H100vA100_tps.png" rel="nofollow">https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;NVIDIA&#x2F;TensorRT-LLM&#x2F;rel&#x2F;do...</a><p>This chart from NVIDIA implies their H100 runs llama v2 70B at &gt;500 tok&#x2F;s.</div><br/><div id="39433929" class="c"><input type="checkbox" id="c-39433929" checked=""/><div class="controls bullet"><span class="by">MasterScrat</span><span>|</span><a href="#39433823">parent</a><span>|</span><a href="#39433932">next</a><span>|</span><label class="collapse" for="c-39433929">[-]</label><label class="expand" for="c-39433929">[6 more]</label></div><br/><div class="children"><div class="content">Scaling up compute can improve throughput, but can&#x27;t easily improve latency between tokens. Generation is usually bottlenecked by the time it takes to go through the network for each token. To speed that up, you need to perform these computations faster, which is a hard problem after you&#x27;ve exhausted all the obvious options (use the fastest accelerator you can find, cache what you can etc).</div><br/><div id="39436994" class="c"><input type="checkbox" id="c-39436994" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#39433823">root</a><span>|</span><a href="#39433929">parent</a><span>|</span><a href="#39434023">next</a><span>|</span><label class="collapse" for="c-39436994">[-]</label><label class="expand" for="c-39436994">[1 more]</label></div><br/><div class="children"><div class="content">At batch size 1 LLMs are memory bandwidth bound, not compute bound…as in you spend most time waiting for model weights to load from vram. At higher batch sizes this flips.<p>But this is why Groq is built around large numbers of chips with small amount of very fast sram.</div><br/></div></div><div id="39434023" class="c"><input type="checkbox" id="c-39434023" checked=""/><div class="controls bullet"><span class="by">SeanAnderson</span><span>|</span><a href="#39433823">root</a><span>|</span><a href="#39433929">parent</a><span>|</span><a href="#39436994">prev</a><span>|</span><a href="#39433932">next</a><span>|</span><label class="collapse" for="c-39434023">[-]</label><label class="expand" for="c-39434023">[4 more]</label></div><br/><div class="children"><div class="content">Yeah. That makes sense, thank you for clarifying. I updated my original post with a chart from NVIDIA which highlights the H100&#x27;s capabilities. It doesn&#x27;t seem unreasonable to expect a 7B model to run at 500 tok&#x2F;s on that hardware.</div><br/><div id="39434818" class="c"><input type="checkbox" id="c-39434818" checked=""/><div class="controls bullet"><span class="by">snowfield</span><span>|</span><a href="#39433823">root</a><span>|</span><a href="#39434023">parent</a><span>|</span><a href="#39433932">next</a><span>|</span><label class="collapse" for="c-39434818">[-]</label><label class="expand" for="c-39434818">[3 more]</label></div><br/><div class="children"><div class="content">This is a 50B model. (Mixtral 8x7b)</div><br/><div id="39435105" class="c"><input type="checkbox" id="c-39435105" checked=""/><div class="controls bullet"><span class="by">SeanAnderson</span><span>|</span><a href="#39433823">root</a><span>|</span><a href="#39434818">parent</a><span>|</span><a href="#39433932">next</a><span>|</span><label class="collapse" for="c-39435105">[-]</label><label class="expand" for="c-39435105">[2 more]</label></div><br/><div class="children"><div class="content">Oh, sorry, I assumed the 8 was for quantization. 8x7b is a new syntax for me.<p>Still, the NVIDIA chart shows Llama v2 70B at 750 tok&#x2F;s, no?</div><br/><div id="39435179" class="c"><input type="checkbox" id="c-39435179" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39433823">root</a><span>|</span><a href="#39435105">parent</a><span>|</span><a href="#39433932">next</a><span>|</span><label class="collapse" for="c-39435179">[-]</label><label class="expand" for="c-39435179">[1 more]</label></div><br/><div class="children"><div class="content">I guess that&#x27;s total throughput, rather than per user?  You can increase total throughput by scaling horizontally.  You can&#x27;t increase throughput per user that way.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39433932" class="c"><input type="checkbox" id="c-39433932" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39433823">parent</a><span>|</span><a href="#39433929">prev</a><span>|</span><a href="#39435411">next</a><span>|</span><label class="collapse" for="c-39433932">[-]</label><label class="expand" for="c-39433932">[4 more]</label></div><br/><div class="children"><div class="content">LLM inference is inherently a sequential problem.  You can&#x27;t speed it up by doing more in parallel.  You can&#x27;t generate the 101st token before you&#x27;ve generated the 100th.</div><br/><div id="39435684" class="c"><input type="checkbox" id="c-39435684" checked=""/><div class="controls bullet"><span class="by">NorwegianDude</span><span>|</span><a href="#39433823">root</a><span>|</span><a href="#39433932">parent</a><span>|</span><a href="#39436594">next</a><span>|</span><label class="collapse" for="c-39435684">[-]</label><label class="expand" for="c-39435684">[2 more]</label></div><br/><div class="children"><div class="content">Technically, I guess you can use speculative execution to speed it up, and in that way take a guess at what the 100th token will be and start on the 101st token at the same time? Though it probably has it&#x27;s own unforeseen challenges.<p>Everything is predictable with enough guesses.</div><br/><div id="39435893" class="c"><input type="checkbox" id="c-39435893" checked=""/><div class="controls bullet"><span class="by">jsmith12673</span><span>|</span><a href="#39433823">root</a><span>|</span><a href="#39435684">parent</a><span>|</span><a href="#39436594">next</a><span>|</span><label class="collapse" for="c-39435893">[-]</label><label class="expand" for="c-39435893">[1 more]</label></div><br/><div class="children"><div class="content">People are pretty cagey about what they use in production, but yes, speculative sampling can offer massive speedups in inference</div><br/></div></div></div></div><div id="39436594" class="c"><input type="checkbox" id="c-39436594" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#39433823">root</a><span>|</span><a href="#39433932">parent</a><span>|</span><a href="#39435684">prev</a><span>|</span><a href="#39435411">next</a><span>|</span><label class="collapse" for="c-39436594">[-]</label><label class="expand" for="c-39436594">[1 more]</label></div><br/><div class="children"><div class="content">They’re using several hundred cards here. Clearly there is ‘something’ that can be done in parallel.</div><br/></div></div></div></div><div id="39435411" class="c"><input type="checkbox" id="c-39435411" checked=""/><div class="controls bullet"><span class="by">nabakin</span><span>|</span><a href="#39433823">parent</a><span>|</span><a href="#39433932">prev</a><span>|</span><a href="#39434544">next</a><span>|</span><label class="collapse" for="c-39435411">[-]</label><label class="expand" for="c-39435411">[3 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a difference between token throughput and latency. Token throughput is the token throughput of the whole GPU&#x2F;system and latency is the token throughput for an individual user. Groq offers extremely low latency (aka extremely high token throughput per user) but we still don&#x27;t have numbers on the token throughput of their entire system. Nvidia&#x27;s metrics here on the other hand, show us the token throughput of the whole GPU&#x2F;system. So, in reality, while you might be able to get 1.5k t&#x2F;s on an H100, the latency (token throughput per user) will be something much lower like 20 t&#x2F;s.<p>The really important metric to look for is cost per token because even though Groq is able to run at low latency, that doesn&#x27;t mean it&#x27;s able to do it cheaply. Determining the cost per token can be done many ways but a useful way for us is approximately the cost of the system divided by the total token throughput of the system per second. We don&#x27;t have the total token throughput per second of Groq&#x27;s system so we can&#x27;t really say how efficient it is. It could very well be that Groq is subsidizing the cost of their system to lower prices and gain PR and will increase their prices later on.</div><br/><div id="39435659" class="c"><input type="checkbox" id="c-39435659" checked=""/><div class="controls bullet"><span class="by">frozenport</span><span>|</span><a href="#39433823">root</a><span>|</span><a href="#39435411">parent</a><span>|</span><a href="#39434544">next</a><span>|</span><label class="collapse" for="c-39435659">[-]</label><label class="expand" for="c-39435659">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;wow.groq.com&#x2F;artificialanalysis-ai-llm-benchmark-doubles-axis-to-fit-new-groq-lpu-inference-engine-performance-results&#x2F;" rel="nofollow">https:&#x2F;&#x2F;wow.groq.com&#x2F;artificialanalysis-ai-llm-benchmark-dou...</a><p>Seems to have it. Looks cost competitive but a lot faster.</div><br/><div id="39436037" class="c"><input type="checkbox" id="c-39436037" checked=""/><div class="controls bullet"><span class="by">nabakin</span><span>|</span><a href="#39433823">root</a><span>|</span><a href="#39435659">parent</a><span>|</span><a href="#39434544">next</a><span>|</span><label class="collapse" for="c-39436037">[-]</label><label class="expand" for="c-39436037">[1 more]</label></div><br/><div class="children"><div class="content">People are using throughput and latency differently in different locations&#x2F;contexts. Here they are referring to token throughput per user and first token&#x2F;chunk latency. They don&#x27;t mention the token throughput of the entire 576-chip system[0] that runs Llama 2 70b which would be the number we&#x27;re looking for.<p>[0] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38742581">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38742581</a></div><br/></div></div></div></div></div></div><div id="39434544" class="c"><input type="checkbox" id="c-39434544" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#39433823">parent</a><span>|</span><a href="#39435411">prev</a><span>|</span><a href="#39433876">next</a><span>|</span><label class="collapse" for="c-39434544">[-]</label><label class="expand" for="c-39434544">[1 more]</label></div><br/><div class="children"><div class="content">I guess it depends on how much the infrastracture from TFA costs, as the H100 only costs ~$3300 to produce, but gets sold for ~$30k on average.<p><a href="https:&#x2F;&#x2F;www.hpcwire.com&#x2F;2023&#x2F;08&#x2F;17&#x2F;nvidia-h100-are-550000-gpus-enough-for-this-year&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.hpcwire.com&#x2F;2023&#x2F;08&#x2F;17&#x2F;nvidia-h100-are-550000-gp...</a></div><br/></div></div></div></div><div id="39433126" class="c"><input type="checkbox" id="c-39433126" checked=""/><div class="controls bullet"><span class="by">neilv</span><span>|</span><a href="#39433823">prev</a><span>|</span><a href="#39433978">next</a><span>|</span><label class="collapse" for="c-39433126">[-]</label><label class="expand" for="c-39433126">[4 more]</label></div><br/><div class="children"><div class="content">If the page can&#x27;t access certain fonts, it will fail to work, while it keeps retrying requests:<p><pre><code>    https:&#x2F;&#x2F;fonts.gstatic.com&#x2F;s&#x2F;notosansarabic&#x2F;[...]
    https:&#x2F;&#x2F;fonts.gstatic.com&#x2F;s&#x2F;notosanshebrew&#x2F;[...]
    https:&#x2F;&#x2F;fonts.gstatic.com&#x2F;s&#x2F;notosanssc&#x2F;[...]
</code></pre>
(I noticed this because my browser blocks these de facto trackers by default.)</div><br/><div id="39433665" class="c"><input type="checkbox" id="c-39433665" checked=""/><div class="controls bullet"><span class="by">sebastiennight</span><span>|</span><a href="#39433126">parent</a><span>|</span><a href="#39433948">next</a><span>|</span><label class="collapse" for="c-39433665">[-]</label><label class="expand" for="c-39433665">[2 more]</label></div><br/><div class="children"><div class="content">Same problem when trying to use font replacements with a privacy plugin.<p>This is a very weird dependency to have :-)</div><br/><div id="39433704" class="c"><input type="checkbox" id="c-39433704" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39433126">root</a><span>|</span><a href="#39433665">parent</a><span>|</span><a href="#39433948">next</a><span>|</span><label class="collapse" for="c-39433704">[-]</label><label class="expand" for="c-39433704">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, I&#x27;ve reported this internally.</div><br/></div></div></div></div><div id="39433948" class="c"><input type="checkbox" id="c-39433948" checked=""/><div class="controls bullet"><span class="by">rasz</span><span>|</span><a href="#39433126">parent</a><span>|</span><a href="#39433665">prev</a><span>|</span><a href="#39433978">next</a><span>|</span><label class="collapse" for="c-39433948">[-]</label><label class="expand" for="c-39433948">[1 more]</label></div><br/><div class="children"><div class="content">How to show Google how popular and interesting for acquisition you are without directly installing google trackers on your website.</div><br/></div></div></div></div><div id="39433978" class="c"><input type="checkbox" id="c-39433978" checked=""/><div class="controls bullet"><span class="by">Gcam</span><span>|</span><a href="#39433126">prev</a><span>|</span><a href="#39432711">next</a><span>|</span><label class="collapse" for="c-39433978">[-]</label><label class="expand" for="c-39433978">[1 more]</label></div><br/><div class="children"><div class="content">Groq&#x27;s API performance reaches close to this level of performance as well. We&#x27;ve benchmarked performance over time and &gt;400 tokens&#x2F;s has sustained - can see here <a href="https:&#x2F;&#x2F;artificialanalysis.ai&#x2F;models&#x2F;mixtral-8x7b-instruct" rel="nofollow">https:&#x2F;&#x2F;artificialanalysis.ai&#x2F;models&#x2F;mixtral-8x7b-instruct</a> (bottom of page for over time view)</div><br/></div></div><div id="39432711" class="c"><input type="checkbox" id="c-39432711" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#39433978">prev</a><span>|</span><a href="#39430865">next</a><span>|</span><label class="collapse" for="c-39432711">[-]</label><label class="expand" for="c-39432711">[2 more]</label></div><br/><div class="children"><div class="content">Jaw dropping. Both groq and mixtral.<p>I used following prompt:<p>Generate gitlab ci yaml file for a
hybrid front-end&#x2F;backend
project. Fronted is under &#x2F;frontend and is a node project,
packaged with yarn, built with
vite to the &#x2F;backend&#x2F;public
folder. The backend is a python
flask server</div><br/><div id="39435899" class="c"><input type="checkbox" id="c-39435899" checked=""/><div class="controls bullet"><span class="by">logtempo</span><span>|</span><a href="#39432711">parent</a><span>|</span><a href="#39430865">next</a><span>|</span><label class="collapse" for="c-39435899">[-]</label><label class="expand" for="c-39435899">[1 more]</label></div><br/><div class="children"><div class="content">And yet, it made a simple mistake in some python code :&#x27;(
&gt; particles = np.zeros((2, 3))  # position, velocity, and acceleration
particles[:, 0] = [0.0, 0.0, 0.0]  # initial position</div><br/></div></div></div></div><div id="39430865" class="c"><input type="checkbox" id="c-39430865" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#39432711">prev</a><span>|</span><a href="#39436654">next</a><span>|</span><label class="collapse" for="c-39430865">[-]</label><label class="expand" for="c-39430865">[9 more]</label></div><br/><div class="children"><div class="content">So this has nothing to do with `Grok`, the model provided by x.ai?<p>EDIT: Tried using it, very impressed with the speed.</div><br/><div id="39430989" class="c"><input type="checkbox" id="c-39430989" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39430865">parent</a><span>|</span><a href="#39435850">next</a><span>|</span><label class="collapse" for="c-39430989">[-]</label><label class="expand" for="c-39430989">[6 more]</label></div><br/><div class="children"><div class="content">Yeah, it&#x27;s nothing to do with Elon and we (Groq) had the name first.  It&#x27;s a natural choice of name for something in the field of AI because of the connections to the hacker ethos, but we have the trademark and Elon doesn&#x27;t.<p><a href="https:&#x2F;&#x2F;wow.groq.com&#x2F;hey-elon-its-time-to-cease-de-grok&#x2F;" rel="nofollow">https:&#x2F;&#x2F;wow.groq.com&#x2F;hey-elon-its-time-to-cease-de-grok&#x2F;</a></div><br/><div id="39431048" class="c"><input type="checkbox" id="c-39431048" checked=""/><div class="controls bullet"><span class="by">terhechte</span><span>|</span><a href="#39430865">root</a><span>|</span><a href="#39430989">parent</a><span>|</span><a href="#39432534">next</a><span>|</span><label class="collapse" for="c-39431048">[-]</label><label class="expand" for="c-39431048">[2 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t Chamath (he&#x27;s one of your investors, right), do a thing there? Every person I pitch Groq to is confused and thinks its about Elons unspectacular LLM.</div><br/><div id="39431166" class="c"><input type="checkbox" id="c-39431166" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39430865">root</a><span>|</span><a href="#39431048">parent</a><span>|</span><a href="#39432534">next</a><span>|</span><label class="collapse" for="c-39431166">[-]</label><label class="expand" for="c-39431166">[1 more]</label></div><br/><div class="children"><div class="content">Yeah the confusion has happened a lot to me too.  All I know is that it&#x27;s in the hands of our legal team.</div><br/></div></div></div></div><div id="39432534" class="c"><input type="checkbox" id="c-39432534" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39430865">root</a><span>|</span><a href="#39430989">parent</a><span>|</span><a href="#39431048">prev</a><span>|</span><a href="#39435850">next</a><span>|</span><label class="collapse" for="c-39432534">[-]</label><label class="expand" for="c-39432534">[3 more]</label></div><br/><div class="children"><div class="content">I mean it sucks that Elon went and claimed Grok when you want Groq, plus you were there first, but getting stuck on the name seems like it&#x27;s going to be a distraction, so why not choose something different?<p>When Grok eventually makes the news for some negative thing, so you really want that erroneously associated with your product? Do you really want to pick a fight with the billionaire that owns Twitter, is that a core competency of the company?</div><br/><div id="39437947" class="c"><input type="checkbox" id="c-39437947" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39430865">root</a><span>|</span><a href="#39432534">parent</a><span>|</span><a href="#39435850">next</a><span>|</span><label class="collapse" for="c-39437947">[-]</label><label class="expand" for="c-39437947">[2 more]</label></div><br/><div class="children"><div class="content">If anything, getting in a very public fight with Musk may well be beneficial wrt brand recognition. Especially if he responds in his usual douchy way and it gets framed accordingly in the media.</div><br/><div id="39438020" class="c"><input type="checkbox" id="c-39438020" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39430865">root</a><span>|</span><a href="#39437947">parent</a><span>|</span><a href="#39435850">next</a><span>|</span><label class="collapse" for="c-39438020">[-]</label><label class="expand" for="c-39438020">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;ll be great for publicity, for sure</div><br/></div></div></div></div></div></div></div></div><div id="39435850" class="c"><input type="checkbox" id="c-39435850" checked=""/><div class="controls bullet"><span class="by">kopirgan</span><span>|</span><a href="#39430865">parent</a><span>|</span><a href="#39430989">prev</a><span>|</span><a href="#39431465">next</a><span>|</span><label class="collapse" for="c-39435850">[-]</label><label class="expand" for="c-39435850">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for asking just the question I wanted to ask!</div><br/></div></div><div id="39431465" class="c"><input type="checkbox" id="c-39431465" checked=""/><div class="controls bullet"><span class="by">mtlmtlmtlmtl</span><span>|</span><a href="#39430865">parent</a><span>|</span><a href="#39435850">prev</a><span>|</span><a href="#39436654">next</a><span>|</span><label class="collapse" for="c-39431465">[-]</label><label class="expand" for="c-39431465">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s also a children&#x27;s toy named Grok which uses LLMs to talk to the kid.</div><br/></div></div></div></div><div id="39436654" class="c"><input type="checkbox" id="c-39436654" checked=""/><div class="controls bullet"><span class="by">matanyal</span><span>|</span><a href="#39430865">prev</a><span>|</span><a href="#39437887">next</a><span>|</span><label class="collapse" for="c-39436654">[-]</label><label class="expand" for="c-39436654">[1 more]</label></div><br/><div class="children"><div class="content">Hey y&#x27;all, we have a discord now for more discussion and announcements: <a href="https:&#x2F;&#x2F;discord.com&#x2F;invite&#x2F;TQcy5EBdCP" rel="nofollow">https:&#x2F;&#x2F;discord.com&#x2F;invite&#x2F;TQcy5EBdCP</a></div><br/></div></div><div id="39437887" class="c"><input type="checkbox" id="c-39437887" checked=""/><div class="controls bullet"><span class="by">zmmmmm</span><span>|</span><a href="#39436654">prev</a><span>|</span><a href="#39431063">next</a><span>|</span><label class="collapse" for="c-39437887">[-]</label><label class="expand" for="c-39437887">[1 more]</label></div><br/><div class="children"><div class="content">As a virtual reality geek, this is super exciting because although there are numerous people experimenting with voicing NPCs with LLMs, they all have horrible latency and are unusable in practice. This looks like the first one that can actually potentially work for an application like that. I can see it won&#x27;t be long before we can have open ended realistic conversations with &quot;real&quot; simulated people!</div><br/></div></div><div id="39431063" class="c"><input type="checkbox" id="c-39431063" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#39437887">prev</a><span>|</span><a href="#39438229">next</a><span>|</span><label class="collapse" for="c-39431063">[-]</label><label class="expand" for="c-39431063">[11 more]</label></div><br/><div class="children"><div class="content">This is pretty sweet.  The speed is nice but what I really care about is you bringing the per token cost down compared with models on the level of mistral medium&#x2F;gpt4.  GPT3.5 is pretty close in terms of cost&#x2F;token but the quality isn&#x27;t there and GPT4 is overpriced.  Having GPT4 quality at sub-gpt3.5 prices will enable a lot of things though.</div><br/><div id="39433370" class="c"><input type="checkbox" id="c-39433370" checked=""/><div class="controls bullet"><span class="by">emporas</span><span>|</span><a href="#39431063">parent</a><span>|</span><a href="#39437954">next</a><span>|</span><label class="collapse" for="c-39433370">[-]</label><label class="expand" for="c-39433370">[1 more]</label></div><br/><div class="children"><div class="content">Mixtral&#x27;s quality is definitely up there with Gpt3.5. Specifically for coding, i consider them almost equivalent in quality. In fact Mixtral 8x7 is starting to be my go-to coding assistant instead of Gpt. It is fast, it is accurate, and i think i like his responses better than Gpt.<p>Reducing LLM size almost 10 times in the span of a little more than a year, that&#x27;s great stuff. Next step i think is 3 billion parameters MoE with 20 experts.</div><br/></div></div><div id="39437954" class="c"><input type="checkbox" id="c-39437954" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39431063">parent</a><span>|</span><a href="#39433370">prev</a><span>|</span><a href="#39431731">next</a><span>|</span><label class="collapse" for="c-39437954">[-]</label><label class="expand" for="c-39437954">[1 more]</label></div><br/><div class="children"><div class="content">You seem to be implying that Mistral Medium is on the same level as GPT-4?</div><br/></div></div><div id="39431731" class="c"><input type="checkbox" id="c-39431731" checked=""/><div class="controls bullet"><span class="by">MuffinFlavored</span><span>|</span><a href="#39431063">parent</a><span>|</span><a href="#39437954">prev</a><span>|</span><a href="#39431691">next</a><span>|</span><label class="collapse" for="c-39431731">[-]</label><label class="expand" for="c-39431731">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the difference in your own words&#x2F;opinion in quality between GPT-3.5 and GPT-4? For what usecases?</div><br/><div id="39431784" class="c"><input type="checkbox" id="c-39431784" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#39431063">root</a><span>|</span><a href="#39431731">parent</a><span>|</span><a href="#39431691">next</a><span>|</span><label class="collapse" for="c-39431784">[-]</label><label class="expand" for="c-39431784">[1 more]</label></div><br/><div class="children"><div class="content">GPT3.5 is great at spitting out marketing babble, summarizing documents and performing superficial analysis but it doesn&#x27;t take style prompts as well as gpt-4 and its reasoning is significantly worse when you want it to chain of thought follow a complex process while referencing context guidance.</div><br/></div></div></div></div><div id="39431691" class="c"><input type="checkbox" id="c-39431691" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#39431063">parent</a><span>|</span><a href="#39431731">prev</a><span>|</span><a href="#39436602">next</a><span>|</span><label class="collapse" for="c-39431691">[-]</label><label class="expand" for="c-39431691">[5 more]</label></div><br/><div class="children"><div class="content">I wonder if Gemini Pro 1.5 will act as a forcing function to lower GPT4 pricing.</div><br/><div id="39431911" class="c"><input type="checkbox" id="c-39431911" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#39431063">root</a><span>|</span><a href="#39431691">parent</a><span>|</span><a href="#39436602">next</a><span>|</span><label class="collapse" for="c-39431911">[-]</label><label class="expand" for="c-39431911">[4 more]</label></div><br/><div class="children"><div class="content">Is that available via an API now?</div><br/><div id="39432363" class="c"><input type="checkbox" id="c-39432363" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#39431063">root</a><span>|</span><a href="#39431911">parent</a><span>|</span><a href="#39436602">next</a><span>|</span><label class="collapse" for="c-39432363">[-]</label><label class="expand" for="c-39432363">[3 more]</label></div><br/><div class="children"><div class="content">Kind of, it&#x27;s in a &quot;Private Preview&quot; with a waitlist.</div><br/><div id="39432468" class="c"><input type="checkbox" id="c-39432468" checked=""/><div class="controls bullet"><span class="by">sturza</span><span>|</span><a href="#39431063">root</a><span>|</span><a href="#39432363">parent</a><span>|</span><a href="#39432743">next</a><span>|</span><label class="collapse" for="c-39432468">[-]</label><label class="expand" for="c-39432468">[1 more]</label></div><br/><div class="children"><div class="content">And in non EU countries.</div><br/></div></div><div id="39432743" class="c"><input type="checkbox" id="c-39432743" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#39431063">root</a><span>|</span><a href="#39432363">parent</a><span>|</span><a href="#39432468">prev</a><span>|</span><a href="#39436602">next</a><span>|</span><label class="collapse" for="c-39432743">[-]</label><label class="expand" for="c-39432743">[1 more]</label></div><br/><div class="children"><div class="content">Via GCP only?</div><br/></div></div></div></div></div></div></div></div><div id="39436602" class="c"><input type="checkbox" id="c-39436602" checked=""/><div class="controls bullet"><span class="by">Zpalmtree</span><span>|</span><a href="#39431063">parent</a><span>|</span><a href="#39431691">prev</a><span>|</span><a href="#39438229">next</a><span>|</span><label class="collapse" for="c-39436602">[-]</label><label class="expand" for="c-39436602">[1 more]</label></div><br/><div class="children"><div class="content">GPT-4 is overpriced vs what?</div><br/></div></div></div></div><div id="39438229" class="c"><input type="checkbox" id="c-39438229" checked=""/><div class="controls bullet"><span class="by">botanical</span><span>|</span><a href="#39431063">prev</a><span>|</span><a href="#39436329">next</a><span>|</span><label class="collapse" for="c-39438229">[-]</label><label class="expand" for="c-39438229">[3 more]</label></div><br/><div class="children"><div class="content">I always ask LLMs this:<p>&gt; If I initially set a timer for 45 minutes but decided to make the total timer time 60 minutes when there&#x27;s 5 minutes left in the initial 45, how much should I add to make it 60?<p>And they never get it correct.</div><br/><div id="39438602" class="c"><input type="checkbox" id="c-39438602" checked=""/><div class="controls bullet"><span class="by">nmca</span><span>|</span><a href="#39438229">parent</a><span>|</span><a href="#39436329">next</a><span>|</span><label class="collapse" for="c-39438602">[-]</label><label class="expand" for="c-39438602">[2 more]</label></div><br/><div class="children"><div class="content">gpt4 first go:<p>If you initially set a timer for 45 minutes and there are 5 minutes left, that means 40 minutes have already passed. To make the total timer time 60 minutes, you need to add an additional 20 minutes. This will give you a total of 60 minutes when combined with the initial 40 minutes that have already passed.</div><br/><div id="39438628" class="c"><input type="checkbox" id="c-39438628" checked=""/><div class="controls bullet"><span class="by">BasilPH</span><span>|</span><a href="#39438229">root</a><span>|</span><a href="#39438602">parent</a><span>|</span><a href="#39436329">next</a><span>|</span><label class="collapse" for="c-39438628">[-]</label><label class="expand" for="c-39438628">[1 more]</label></div><br/><div class="children"><div class="content">Bard&#x2F;Gemini gets it wrong the same way too. Interestingly, if I tell either GPT-4 or Gemini the right answer, they figure it out.</div><br/></div></div></div></div></div></div><div id="39436329" class="c"><input type="checkbox" id="c-39436329" checked=""/><div class="controls bullet"><span class="by">tandr</span><span>|</span><a href="#39438229">prev</a><span>|</span><a href="#39433220">next</a><span>|</span><label class="collapse" for="c-39436329">[-]</label><label class="expand" for="c-39436329">[1 more]</label></div><br/><div class="children"><div class="content">@tome
Cannot sign up  with sneakemail.com, snkml.com, snkmail, liamekaens.com etc... I pay for these services so my email is a bit more protected. Why do you insist on getting well-known email providers instead, datamining or something else?</div><br/></div></div><div id="39433220" class="c"><input type="checkbox" id="c-39433220" checked=""/><div class="controls bullet"><span class="by">anybodyz</span><span>|</span><a href="#39436329">prev</a><span>|</span><a href="#39437732">next</a><span>|</span><label class="collapse" for="c-39433220">[-]</label><label class="expand" for="c-39433220">[1 more]</label></div><br/><div class="children"><div class="content">I have this hooked up experimentally to my universal Dungeon Master simulator DungeonGod and it seems to work quite well.<p>I had been using Together AI Mixtral (which is serving the Hermes Mixtrals) and it is pretty snappy, but nothing close to Groq. I think the next closes that I&#x27;ve tested is Perplexity Labs Mixtral.<p>A key blocker in just hanging out a shingle for an open source AI project is the fear that anything that might scale will bankrupt you (or just be offline if you get any significant traction). I think we&#x27;re nearing the phase that we could potentially just turn these things &quot;on&quot; and eat the reasonable inference fees to see what people engage with - with a pretty decently cool free tier available.<p>I&#x27;d add that the simulator does multiple calls to the api for one response to do analysis and function selection in the underlying python game engine, which Groq makes less of a problem as it&#x27;s close to instant. This adds a pretty significant pause in the OpenAI version. Also since this simulator runs on Discord with multiple users, I&#x27;ve had problems in the past with &#x27;user response storms&#x27; where the AI couldn&#x27;t keep up. Also less of a problem with Groq.</div><br/></div></div><div id="39437732" class="c"><input type="checkbox" id="c-39437732" checked=""/><div class="controls bullet"><span class="by">mrg3_2013</span><span>|</span><a href="#39433220">prev</a><span>|</span><a href="#39431179">next</a><span>|</span><label class="collapse" for="c-39437732">[-]</label><label class="expand" for="c-39437732">[1 more]</label></div><br/><div class="children"><div class="content">This is unreal. I have never seen anything this fast. How ? I mean, how can you physically ship the bits this fast, let alone a LLM.<p>Something about the UI. Doesn&#x27;t work for me. May be I like openAI chat interface too much. Can someone bring their own data and train ? That would be crazy!</div><br/></div></div><div id="39431179" class="c"><input type="checkbox" id="c-39431179" checked=""/><div class="controls bullet"><span class="by">deepsquirrelnet</span><span>|</span><a href="#39437732">prev</a><span>|</span><a href="#39436295">next</a><span>|</span><label class="collapse" for="c-39431179">[-]</label><label class="expand" for="c-39431179">[2 more]</label></div><br/><div class="children"><div class="content">Incredible job. Feels dumb or obvious to say this, but this really changes the way I think of using it. The slow autoregression really sucks because it inhibits your ability to skim sections. For me, that creates an unnatural reading environment. This makes chatgpt feel antiqued.</div><br/><div id="39431331" class="c"><input type="checkbox" id="c-39431331" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431179">parent</a><span>|</span><a href="#39436295">next</a><span>|</span><label class="collapse" for="c-39431331">[-]</label><label class="expand" for="c-39431331">[1 more]</label></div><br/><div class="children"><div class="content">Yes, agreed. We believe the benefits of reducing latency are non-linear.  You can hit different phase changes as the latency reduces and new applications become viable.  Roundtripping text-to-speech and speech-to-text is one example.  We&#x27;re looking forward to seeing what low latency applications are unlocked by our new users!</div><br/></div></div></div></div><div id="39436295" class="c"><input type="checkbox" id="c-39436295" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#39431179">prev</a><span>|</span><a href="#39438169">next</a><span>|</span><label class="collapse" for="c-39436295">[-]</label><label class="expand" for="c-39436295">[1 more]</label></div><br/><div class="children"><div class="content">That sort of speed will be amazing for code completion. Need to find a way to hook this into vscode somehome...</div><br/></div></div><div id="39435152" class="c"><input type="checkbox" id="c-39435152" checked=""/><div class="controls bullet"><span class="by">codedokode</span><span>|</span><a href="#39438169">prev</a><span>|</span><a href="#39438093">next</a><span>|</span><label class="collapse" for="c-39435152">[-]</label><label class="expand" for="c-39435152">[2 more]</label></div><br/><div class="children"><div class="content">Is it normal that I have asked two networks (llama&#x2F;mixtral) the same question (&quot;tell me about most popular audio pitch detection algorithms&quot;) and they gave almost the same answer? Both answers start with &quot;Sure, here are some of the most popular pitch detection algorithms used in audio signal processing&quot; and end with &quot;Each of these algorithms has its own strengths and weaknesses, and the choice of algorithm depends on the specific application and the characteristics of the input signal.&quot;. And the content is 95% the same. How can it be?</div><br/><div id="39435161" class="c"><input type="checkbox" id="c-39435161" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39435152">parent</a><span>|</span><a href="#39438093">next</a><span>|</span><label class="collapse" for="c-39435161">[-]</label><label class="expand" for="c-39435161">[1 more]</label></div><br/><div class="children"><div class="content">Yeah it&#x27;s a bit confusing.  See here for details: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39431921">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39431921</a></div><br/></div></div></div></div><div id="39438093" class="c"><input type="checkbox" id="c-39438093" checked=""/><div class="controls bullet"><span class="by">uptownfunk</span><span>|</span><a href="#39435152">prev</a><span>|</span><a href="#39428981">next</a><span>|</span><label class="collapse" for="c-39438093">[-]</label><label class="expand" for="c-39438093">[1 more]</label></div><br/><div class="children"><div class="content">It is fast, but if it spits useless garbage, then useless. I don&#x27;t mind waiting for chatGPT, the quality of what it produces is quite remarkable, and I am excited to see it better. I think this has more to do with mistral model v GPT4 than Groq. If Groq can host GPT4, wow, then that is amazing.</div><br/></div></div><div id="39428981" class="c"><input type="checkbox" id="c-39428981" checked=""/><div class="controls bullet"><span class="by">ppsreejith</span><span>|</span><a href="#39438093">prev</a><span>|</span><a href="#39433394">next</a><span>|</span><label class="collapse" for="c-39428981">[-]</label><label class="expand" for="c-39428981">[2 more]</label></div><br/><div class="children"><div class="content">Relevant thread from 5 months ago: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37469434">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37469434</a><p>I&#x27;m achieving consistent 450+ tokens&#x2F;sec for Mixtral 8x7b 32k and ~200 tps for Llama 2 70B-4k.<p>As an aside, seeing that this is built with flutter Web, perhaps a mobile app is coming soon?</div><br/><div id="39429093" class="c"><input type="checkbox" id="c-39429093" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39428981">parent</a><span>|</span><a href="#39433394">next</a><span>|</span><label class="collapse" for="c-39429093">[-]</label><label class="expand" for="c-39429093">[1 more]</label></div><br/><div class="children"><div class="content">There was also another discussion about Groq a couple of months ago <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38739199">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38739199</a></div><br/></div></div></div></div><div id="39433394" class="c"><input type="checkbox" id="c-39433394" checked=""/><div class="controls bullet"><span class="by">roomey</span><span>|</span><a href="#39428981">prev</a><span>|</span><a href="#39436537">next</a><span>|</span><label class="collapse" for="c-39433394">[-]</label><label class="expand" for="c-39433394">[1 more]</label></div><br/><div class="children"><div class="content">Oh hell yes, this is the first &quot;fast&quot; one, superhuman fast.<p>I know you gave suggestions of what to ask, but I threw a few curveballs and it was really good! Well done this is a big step forwards</div><br/></div></div><div id="39436537" class="c"><input type="checkbox" id="c-39436537" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#39433394">prev</a><span>|</span><a href="#39438483">next</a><span>|</span><label class="collapse" for="c-39436537">[-]</label><label class="expand" for="c-39436537">[2 more]</label></div><br/><div class="children"><div class="content">I think we’re kind of past the point where we post prompts because it’s interesting, but this one still had me thinking.<p>Obviously it doesn’t have memory, but it’s the first time I’ve seen a model actually respond instead of hedge (having mostly used ChatGPT).<p>&gt; what is the longest prompt you have ever received?<p>&gt; The length of a prompt can vary greatly, and it&#x27;s not uncommon for me to receive prompts that are several sentences long. However, I don&#x27;t think I have ever received a prompt that could be considered &quot;super long&quot; in terms of physical length. The majority of prompts I receive are concise and to the point, typically consisting of a single sentence or a short paragraph.</div><br/><div id="39436593" class="c"><input type="checkbox" id="c-39436593" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#39436537">parent</a><span>|</span><a href="#39438483">next</a><span>|</span><label class="collapse" for="c-39436593">[-]</label><label class="expand" for="c-39436593">[1 more]</label></div><br/><div class="children"><div class="content">Been using it exclusively since December, 5bit quantized, 8,000 token context window<p>Sometimes you need a model that just gives you the feeling “that’ll do”<p>I did switch to Miqu a few weeks back though. 4 bit quantized</div><br/></div></div></div></div><div id="39438483" class="c"><input type="checkbox" id="c-39438483" checked=""/><div class="controls bullet"><span class="by">cheptsov</span><span>|</span><a href="#39436537">prev</a><span>|</span><a href="#39431788">next</a><span>|</span><label class="collapse" for="c-39438483">[-]</label><label class="expand" for="c-39438483">[2 more]</label></div><br/><div class="children"><div class="content">Any chance you plan to offer the API to cloud LPUs? And not  just the LLM API? It would be cool run custom code (training, serving, etc).</div><br/><div id="39439075" class="c"><input type="checkbox" id="c-39439075" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39438483">parent</a><span>|</span><a href="#39431788">next</a><span>|</span><label class="collapse" for="c-39439075">[-]</label><label class="expand" for="c-39439075">[1 more]</label></div><br/><div class="children"><div class="content">Yes, in the future we&#x27;d like to do that.</div><br/></div></div></div></div><div id="39431788" class="c"><input type="checkbox" id="c-39431788" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#39438483">prev</a><span>|</span><a href="#39432600">next</a><span>|</span><label class="collapse" for="c-39431788">[-]</label><label class="expand" for="c-39431788">[6 more]</label></div><br/><div class="children"><div class="content">In how far is the API compatible with OpenAI? Does it offer logprobs[0] and top_logprobs[1]?<p>0. <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;api-reference&#x2F;chat&#x2F;create#chat-create-logprobs" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;api-reference&#x2F;chat&#x2F;create#c...</a><p>1. <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;api-reference&#x2F;chat&#x2F;create#chat-create-top_logprobs" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;api-reference&#x2F;chat&#x2F;create#c...</a></div><br/><div id="39432058" class="c"><input type="checkbox" id="c-39432058" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431788">parent</a><span>|</span><a href="#39432600">next</a><span>|</span><label class="collapse" for="c-39432058">[-]</label><label class="expand" for="c-39432058">[5 more]</label></div><br/><div class="children"><div class="content">You can find our API docs here, including details of our OpenAI compatibility<p><a href="https:&#x2F;&#x2F;docs.api.groq.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;docs.api.groq.com&#x2F;</a></div><br/><div id="39432715" class="c"><input type="checkbox" id="c-39432715" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431788">root</a><span>|</span><a href="#39432058">parent</a><span>|</span><a href="#39432819">next</a><span>|</span><label class="collapse" for="c-39432715">[-]</label><label class="expand" for="c-39432715">[1 more]</label></div><br/><div class="children"><div class="content">By the way, we also have a new Discord server where we are hosting our developer community.  If you find anything missing in our API you can ask about there:<p><a href="https:&#x2F;&#x2F;discord.com&#x2F;invite&#x2F;TQcy5EBdCP" rel="nofollow">https:&#x2F;&#x2F;discord.com&#x2F;invite&#x2F;TQcy5EBdCP</a></div><br/></div></div><div id="39432819" class="c"><input type="checkbox" id="c-39432819" checked=""/><div class="controls bullet"><span class="by">kumarm</span><span>|</span><a href="#39431788">root</a><span>|</span><a href="#39432058">parent</a><span>|</span><a href="#39432715">prev</a><span>|</span><a href="#39432600">next</a><span>|</span><label class="collapse" for="c-39432819">[-]</label><label class="expand" for="c-39432819">[3 more]</label></div><br/><div class="children"><div class="content">Filled the form for API Access last night. Is there a delay with increased demand now?</div><br/><div id="39433004" class="c"><input type="checkbox" id="c-39433004" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39431788">root</a><span>|</span><a href="#39432819">parent</a><span>|</span><a href="#39432600">next</a><span>|</span><label class="collapse" for="c-39433004">[-]</label><label class="expand" for="c-39433004">[2 more]</label></div><br/><div class="children"><div class="content">Yes, there&#x27;s a huge amount of demand because Twitter discovered us yesterday.  There will be a backlog, so sorry about that.</div><br/><div id="39433855" class="c"><input type="checkbox" id="c-39433855" checked=""/><div class="controls bullet"><span class="by">kumarm</span><span>|</span><a href="#39431788">root</a><span>|</span><a href="#39433004">parent</a><span>|</span><a href="#39432600">next</a><span>|</span><label class="collapse" for="c-39433855">[-]</label><label class="expand" for="c-39433855">[1 more]</label></div><br/><div class="children"><div class="content">Understandable. Wish you guys best of luck irrespective.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39432600" class="c"><input type="checkbox" id="c-39432600" checked=""/><div class="controls bullet"><span class="by">supercharger9</span><span>|</span><a href="#39431788">prev</a><span>|</span><a href="#39433196">next</a><span>|</span><label class="collapse" for="c-39432600">[-]</label><label class="expand" for="c-39432600">[3 more]</label></div><br/><div class="children"><div class="content">Do they make money from LLM service or by selling hardware? Homepage is confusing without any reference to other products.</div><br/><div id="39433998" class="c"><input type="checkbox" id="c-39433998" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39432600">parent</a><span>|</span><a href="#39433196">next</a><span>|</span><label class="collapse" for="c-39433998">[-]</label><label class="expand" for="c-39433998">[2 more]</label></div><br/><div class="children"><div class="content">Both, we sell tokens as a service and we sell enterprise systems.</div><br/><div id="39434410" class="c"><input type="checkbox" id="c-39434410" checked=""/><div class="controls bullet"><span class="by">supercharger9</span><span>|</span><a href="#39432600">root</a><span>|</span><a href="#39433998">parent</a><span>|</span><a href="#39433196">next</a><span>|</span><label class="collapse" for="c-39434410">[-]</label><label class="expand" for="c-39434410">[1 more]</label></div><br/><div class="children"><div class="content">Then reference that in the homepage?  If not for this HN thread, I wouldn&#x27;t have know you sell hardware.</div><br/></div></div></div></div></div></div><div id="39433196" class="c"><input type="checkbox" id="c-39433196" checked=""/><div class="controls bullet"><span class="by">totalhack</span><span>|</span><a href="#39432600">prev</a><span>|</span><label class="collapse" for="c-39433196">[-]</label><label class="expand" for="c-39433196">[5 more]</label></div><br/><div class="children"><div class="content">Where is the data center located? The fastest response time I could get from some quick testing from the northeast US, having it output just one letter, was 670ms. Just wondering if that&#x27;s an expected result, as it&#x27;s on a par or slower than GPT 3.5 via API.</div><br/><div id="39433440" class="c"><input type="checkbox" id="c-39433440" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39433196">parent</a><span>|</span><a href="#39433346">next</a><span>|</span><label class="collapse" for="c-39433440">[-]</label><label class="expand" for="c-39433440">[3 more]</label></div><br/><div class="children"><div class="content">West Coast US.  You would have been placed in our queuing system because with all the attention we are getting we are very busy right now!</div><br/><div id="39433654" class="c"><input type="checkbox" id="c-39433654" checked=""/><div class="controls bullet"><span class="by">totalhack</span><span>|</span><a href="#39433196">root</a><span>|</span><a href="#39433440">parent</a><span>|</span><a href="#39433346">next</a><span>|</span><label class="collapse" for="c-39433654">[-]</label><label class="expand" for="c-39433654">[2 more]</label></div><br/><div class="children"><div class="content">Thanks! I did notice the queue count showing up occasionally but not every time. Maybe someone could repeat the test who has access without the queue so we can get an understanding of the potential latency once scaled and geo-distributed. What I&#x27;m really trying to understand is time to first token output actually faster than GPT 3.5 via API or just the rate of token output once it begins.</div><br/><div id="39433790" class="c"><input type="checkbox" id="c-39433790" checked=""/><div class="controls bullet"><span class="by">tome</span><span>|</span><a href="#39433196">root</a><span>|</span><a href="#39433654">parent</a><span>|</span><a href="#39433346">next</a><span>|</span><label class="collapse" for="c-39433790">[-]</label><label class="expand" for="c-39433790">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know about GPT 3.5 specifically, but on this independent benchmark (LLMPerf) Groq&#x27;s time to first token is also lowest:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ray-project&#x2F;llmperf-leaderboard?tab=readme-ov-file#time-to-first-token-seconds">https:&#x2F;&#x2F;github.com&#x2F;ray-project&#x2F;llmperf-leaderboard?tab=readm...</a></div><br/></div></div></div></div></div></div><div id="39433346" class="c"><input type="checkbox" id="c-39433346" checked=""/><div class="controls bullet"><span class="by">MaxLeiter</span><span>|</span><a href="#39433196">parent</a><span>|</span><a href="#39433440">prev</a><span>|</span><label class="collapse" for="c-39433346">[-]</label><label class="expand" for="c-39433346">[1 more]</label></div><br/><div class="children"><div class="content">There’s a queueing system if too many requests are being processed at once. You may have hit that.</div><br/></div></div></div></div></div></div></div></div></div></body></html>