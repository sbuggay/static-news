<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1736758873826" as="style"/><link rel="stylesheet" href="styles.css?v=1736758873826"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://surfingcomplexity.blog/2024/12/21/the-canva-outage-another-tale-of-saturation-and-resilience/">The Canva outage: another tale of saturation and resilience</a> <span class="domain">(<a href="https://surfingcomplexity.blog">surfingcomplexity.blog</a>)</span></div><div class="subtext"><span>mooreds</span> | <span>27 comments</span></div><br/><div><div id="42678319" class="c"><input type="checkbox" id="c-42678319" checked=""/><div class="controls bullet"><span class="by">shaggie76</span><span>|</span><a href="#42678560">next</a><span>|</span><label class="collapse" for="c-42678319">[-]</label><label class="expand" for="c-42678319">[7 more]</label></div><br/><div class="children"><div class="content">We had a similar CDN problem with releasing major Warframe updates: our CDN partner would inadvertently DDoS our origin servers when we launched an update because thousands of cold edges would call home simultaneously when all players players relogged at the same time.<p>One CDN vendor didn&#x27;t even offer a tiered distribution system so every edge called home at the same time, another vendor did have a tiered distribution system designed to avoid this problem but it was overwhelmed by the absurd number of files we&#x27;d serve multiplied by the large user count and so we&#x27;d still end up with too much traffic hitting the origin.<p>The interesting thing was that no vendor we evaluated offered a robust preheating solution if they offered one at all. One vendor even went so far as to say that they wouldn&#x27;t allow it because it would let customers unfairly dominate the shared storage cache at the edge (which sort of felt like airlines overbooking seats on a flight to me).<p>These days we run an army of VMs that fetch all assets from every point of presence we can cover right before launching an update.<p>Another thing we&#x27;ve had to deal with mentioned in the article is overloading back-end nodes; our solution is somewhat ham-fisted but works quite well for us: we cap the connection counts to the back end and return 503s when we saturate. The trick, however, is getting your load-balancer to leave the client connection open when this happens -- by default multiple LBs we&#x27;ve used would slam the connection closed so that when you&#x27;re serving up 50K 503s a second the firewall would buckle under the runaway connection pool lingering in TIME_WAIT. Good times.</div><br/><div id="42680166" class="c"><input type="checkbox" id="c-42680166" checked=""/><div class="controls bullet"><span class="by">snackbroken</span><span>|</span><a href="#42678319">parent</a><span>|</span><a href="#42679633">next</a><span>|</span><label class="collapse" for="c-42680166">[-]</label><label class="expand" for="c-42680166">[2 more]</label></div><br/><div class="children"><div class="content">Something I&#x27;ve been wondering for a while is if BitTorrent or other P2P protocols are ever a consideration for pushing game updates? Naively, it seems like an ideal fit since a large swarm of leechers quickly turns into a large swarm of (partial) seeders mostly chattering amongst themselves. I recall Facebook and Twitter used to internally torrent their updates in the 2010s and BT scales just fine to thousands of peers and tens of GB files at least, but I think I&#x27;ve only ever played one game whose updater was a torrent client so I&#x27;m guessing it&#x27;s a nonstarter for one reason or another. Are game publishers just allergic to it due to the piracy association? Are end-user upload speeds too slow to meaningfully make a difference? Are swarms of ~100k just too large to manage?<p>Edit: Silly me for posting while sleep deprived. It&#x27;s not the update itself that you&#x27;re saying is causing thundering herd issues, but the log-ins all being synced up afterwards much like in TFA, duh. My curiosity wrt the apparent lack of P2P game updaters still stands though.</div><br/><div id="42680712" class="c"><input type="checkbox" id="c-42680712" checked=""/><div class="controls bullet"><span class="by">masklinn</span><span>|</span><a href="#42678319">root</a><span>|</span><a href="#42680166">parent</a><span>|</span><a href="#42679633">next</a><span>|</span><label class="collapse" for="c-42680712">[-]</label><label class="expand" for="c-42680712">[1 more]</label></div><br/><div class="children"><div class="content">Blizzard used to have p2p support, they removed it around 2015. It’s not hard to think of a bunch of problematic cases which become absolute hell to diagnose because they’re client side.</div><br/></div></div></div></div><div id="42679633" class="c"><input type="checkbox" id="c-42679633" checked=""/><div class="controls bullet"><span class="by">donavanm</span><span>|</span><a href="#42678319">parent</a><span>|</span><a href="#42680166">prev</a><span>|</span><a href="#42678520">next</a><span>|</span><label class="collapse" for="c-42679633">[-]</label><label class="expand" for="c-42679633">[2 more]</label></div><br/><div class="children"><div class="content">As someone who worked on a major CDN I have some perspective.<p>&gt;  thousands of cold edges would call home simultaneously when all players players relogged at the same time.<p>Our more mature customers (esp console gaming) would enable early background downloads, spaced out over a few hours, the day&#x2F;hours before &#x27;launch&#x27;. Otherwise adhoc&#x2F;jit is definitely best effort, though we did a few things to help:<p>Conceptually each CDN POP is ~3 logical layers 1) a client-request-terminating &#x27;hot&#x27; cache distributed across all nodes in the POP 2) a shared POP cache segmented by content&#x2F;resource ID 3) a shared origin-request-facing egress layer. Every layer would attempt to perform request coalescing, with 90% efficacy or more. eg, 10 client requests to the same layer 1 node _should_ only generate a single request to the segmented layer 2 cache. The same layer 2 node would we serving multiple requests to the layer 1 nodes, while making a single request back towards the origin.<p>Some exceptional behavior occurred, or was driven by, &#x27;load&#x27; and trying to account for 1) head of line blocking 2) tail latencies etc from inequal load distribution. Based on load for an object, or a nodes current total load, we used forward signaling to distribute requests to peers. That is a &#x27;busy&#x27; layer 2 node would signal to the layer 1 nodes to use additional&#x2F;alternate peers. This increased the number of copies of a popular object in the segmented cache, increasing the total throughput available to populate the &#x27;hot&#x27; L1 cache nodes _or_ to serve objects that were not consistently popular enough to stay in that distributed L1 cache. And relevant to your example we had similar problems when going back to the origin; In the first case we want to minimize the number of new TCP&#x2F;TLS connections, which have a large RTT setup penalty, by reusing active &amp; idle &#x27;layer 3&#x27; connections to the origin. This, however, introduces hotspots and head of line blocking for those active origin connections. Which, again, based on &#x27;load&#x27; would be forward signaled so that additional layer 3 nodes&#x2F;processes would be used to fetch _additional_ origin content.<p>Normally this all means 1 origin request can serve a few orders of magnitude more concurrent client requests. For very large content, or exceedingly large client numbers, you&#x27;d see the CDN &#x27;scale out&#x27; on concurrency in an effort to minimize blocking and maximize throughput in the system.<p>&gt; One CDN vendor didn&#x27;t even offer a tiered distribution system so every edge called home at the same time, another vendor did have a tiered distribution system designed to avoid this problem<p>See above on request coalescing. In the vast vast majority of cases it was effective in reducing the problem by a few orders of magnitude; AFAIK every CDN does&#x2F;did that. _In addition_ we did have an distributed hierarchal system for caching between edge POPs and origins _but_ it was non-public&#x2F;invite&#x2F;managed by us for a long time. The reason being that the _vast_ majority of customers incurred additional latency (&amp; cost to us) without meaningful benefit from this intermediate cache layer.<p>&gt; The interesting thing was that no vendor we evaluated offered a robust preheating solution if they offered one at all.<p>This is interesting to me. AFAIK Akamai Netstorage was sold to solve the origin distribution angle, _and_ drove something like 50% of the revenue from large object distribution customers. For us the customer use case of &#x27;prefetch&#x27; was perennial &#x27;top 5&#x27; but never one that would drive revenue, and conflicted with other system tenets.<p>&gt; One vendor even went so far as to say that they wouldn&#x27;t allow it because it would let customers unfairly dominate the shared storage cache at the edge<p>That could have been us. And yes a huge problem is that you&#x27;re fundamentally asking for control over a shared resource so that you can bias performance to _your content_ at the expense of _all other customers_. Even without intentional &#x27;prefetch&#x27; control we had still had some customers with pseudo-degenerate access patterns that might consume 25-50% of the shared cache space in a POP. We did build shared quotas and such but (when I was there) we couldn&#x27;t see a way to align the pricing &amp; incentives to confidently expose that to customers. It also felt very very bad to tell a customer &#x27;pay us $$$ to care about your bits&#x27; when that was our entire job, and what we were doing to the best extent possible already.<p>&gt; we cap the connection counts to the back end and return 503s when we saturate.<p>Depending on the CDN you may be able to use `max-age` or `s-maxage` to implement psuedo backoff from the CDN. For us at least those &#x27;negative hits&#x27; would be cached with a short (seconds by default) TTL to act a dampener in failure scenarios. Ensure that your client can handle&#x2F;recover from the 503 as well, I&#x27;d expect the CDN to return those all the way through in the response.</div><br/><div id="42679689" class="c"><input type="checkbox" id="c-42679689" checked=""/><div class="controls bullet"><span class="by">donavanm</span><span>|</span><a href="#42678319">root</a><span>|</span><a href="#42679633">parent</a><span>|</span><a href="#42678520">next</a><span>|</span><label class="collapse" for="c-42679689">[-]</label><label class="expand" for="c-42679689">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Otherwise adhoc&#x2F;jit is definitely best effort, though we did a few things to help<p>I should also give a sense of scale here. Hundreds of GB&#x2F;s to multi TB&#x2F;s of throughput for a single customer was pretty normal a decade ago. CDNs, classically, are also biased towards latency &amp; throughput. Once you have millions of client requests per second and pushing that kind of volume its kind of expected&#x2F;implied that the origin is capable of meeting the demand necessary to maximize that throughput.<p>While cost efficiency maximizing CDNs _were_ a thing they kind of died out with Red Swoosh AFAIK. We repeatedly investigated &#x27;follow the moon&#x27; use cases to maximize the diurnal cycle. Outside of a handful of game companies there wasnt any real interest, and the price&#x2F;revenue wasnt worth investing compared to other priorities. The market wanted better performance, not minimal costs, in the 2000-10s.</div><br/></div></div></div></div><div id="42678520" class="c"><input type="checkbox" id="c-42678520" checked=""/><div class="controls bullet"><span class="by">bolognafairy</span><span>|</span><a href="#42678319">parent</a><span>|</span><a href="#42679633">prev</a><span>|</span><a href="#42678560">next</a><span>|</span><label class="collapse" for="c-42678520">[-]</label><label class="expand" for="c-42678520">[2 more]</label></div><br/><div class="children"><div class="content">Really one of those “has anyone that built this tried using it for its intended purpose?” things. Not having a carefully considered cache warning solution* is like…if someone built a CDN based on a description someone gave them, instead of actually understanding the problem a CDN sets out to solve.<p>* EDIT: actually, any solution that at least attempts to mitigate a thundering herd. I am at least somewhat empathetic to the “indiscriminately allowing pre-warming destroys the shared cache” viewpoint.  But there are still plenty of things that can be done!</div><br/><div id="42679509" class="c"><input type="checkbox" id="c-42679509" checked=""/><div class="controls bullet"><span class="by">bombcar</span><span>|</span><a href="#42678319">root</a><span>|</span><a href="#42678520">parent</a><span>|</span><a href="#42678560">next</a><span>|</span><label class="collapse" for="c-42679509">[-]</label><label class="expand" for="c-42679509">[1 more]</label></div><br/><div class="children"><div class="content">The easiest solution to the pre-warming problem is charge quite a bit for it. Then only those who really need it will pay (or you’ll collect more money to build out the cache).</div><br/></div></div></div></div></div></div><div id="42678560" class="c"><input type="checkbox" id="c-42678560" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#42678319">prev</a><span>|</span><a href="#42681458">next</a><span>|</span><label class="collapse" for="c-42678560">[-]</label><label class="expand" for="c-42678560">[7 more]</label></div><br/><div class="children"><div class="content">This problem is similar to what electric utilities call &quot;load takeup&quot;. After a power outage, when power is turned back on, there are many loads that draw more power at startup.<p>The shortest term effects are power supplies recharging their capacitors and incandescent bulbs warming up. That&#x27;s over within a second.<p>Then it&#x27;s the motors, which have 2x-3x their running load when starting as they bring their rotating mass up to speed. That extra load lasts for tens of seconds.<p>If power has been off for more than a few minutes, everything in heating and cooling which normally cycles on and off will want to start. That high load lasts for minutes.<p>Bringing up a power grid is thus done by sections, not all at once.</div><br/><div id="42680308" class="c"><input type="checkbox" id="c-42680308" checked=""/><div class="controls bullet"><span class="by">EvanAnderson</span><span>|</span><a href="#42678560">parent</a><span>|</span><a href="#42680915">next</a><span>|</span><label class="collapse" for="c-42680308">[-]</label><label class="expand" for="c-42680308">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re subject to peak load billing it&#x27;s also a good idea to bring your loads online in sections, too. My family owns a small grocery store. I was taught the process for &quot;booting-up&quot; the store after a power outage. It basically amounted to a one-by-one startup of the refrigeration compressors, waiting between each for them to come up to operating pressure and stabilize their current demand.</div><br/></div></div><div id="42680915" class="c"><input type="checkbox" id="c-42680915" checked=""/><div class="controls bullet"><span class="by">ElusiveA</span><span>|</span><a href="#42678560">parent</a><span>|</span><a href="#42680308">prev</a><span>|</span><a href="#42678682">next</a><span>|</span><label class="collapse" for="c-42680915">[-]</label><label class="expand" for="c-42680915">[1 more]</label></div><br/><div class="children"><div class="content">An insightful share. You might be interested to know that startup current is called &#x27;inrush current&#x27;. For a Direct On Line (DOL) start, (no soft starters or variable speed drives) electrical engineers usually model it as 6x normal full load current.<p>Other electrical devices such as transformers and long overhead power lines also exhibit inrush when they are energised.</div><br/></div></div><div id="42678682" class="c"><input type="checkbox" id="c-42678682" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#42678560">parent</a><span>|</span><a href="#42680915">prev</a><span>|</span><a href="#42681458">next</a><span>|</span><label class="collapse" for="c-42678682">[-]</label><label class="expand" for="c-42678682">[4 more]</label></div><br/><div class="children"><div class="content">I live in a somewhat rural area and we got bit hard by this last winter.<p>Our road used to have a handful of houses on it but now has around 85 (a mix of smaller lots around an acre and larger farming parcels).  Power infrastructure to our street hasn&#x27;t been updated recently and it just barely keeps up.<p>We had a few days that didn&#x27;t get above freezing (very unusual here). Power was out for about 6 hours after a limb fell on a line. The power company was actually pretty quick to fix it, but the power went out 3 more times in pretty quick succession.<p>Apparently a breaker kept blowing as every house regained power and all the various compressors surged on. The solution at the time was for them to jam in a larger breaker. I hope they came back pretty quickly to undo that &quot;fix&quot; but we still haven&#x27;t had any infrastructure updates to increase capacity.</div><br/><div id="42679078" class="c"><input type="checkbox" id="c-42679078" checked=""/><div class="controls bullet"><span class="by">alvah</span><span>|</span><a href="#42678560">root</a><span>|</span><a href="#42678682">parent</a><span>|</span><a href="#42680293">next</a><span>|</span><label class="collapse" for="c-42679078">[-]</label><label class="expand" for="c-42679078">[2 more]</label></div><br/><div class="children"><div class="content">&quot;The solution at the time was for them to jam in a larger breaker&quot;<p>I&#x27;ve seen some cowboy sh!t in my time but jeez, that&#x27;s rough.</div><br/><div id="42679507" class="c"><input type="checkbox" id="c-42679507" checked=""/><div class="controls bullet"><span class="by">cr125rider</span><span>|</span><a href="#42678560">root</a><span>|</span><a href="#42679078">parent</a><span>|</span><a href="#42680293">next</a><span>|</span><label class="collapse" for="c-42679507">[-]</label><label class="expand" for="c-42679507">[1 more]</label></div><br/><div class="children"><div class="content">That’s “it can’t keep tripping if I jam in a penny instead” level of engineering from the utility! Wow!</div><br/></div></div></div></div><div id="42680293" class="c"><input type="checkbox" id="c-42680293" checked=""/><div class="controls bullet"><span class="by">cudgy</span><span>|</span><a href="#42678560">root</a><span>|</span><a href="#42678682">parent</a><span>|</span><a href="#42679078">prev</a><span>|</span><a href="#42681458">next</a><span>|</span><label class="collapse" for="c-42680293">[-]</label><label class="expand" for="c-42680293">[1 more]</label></div><br/><div class="children"><div class="content">Good thing none of your houses burnt down.</div><br/></div></div></div></div></div></div><div id="42681458" class="c"><input type="checkbox" id="c-42681458" checked=""/><div class="controls bullet"><span class="by">jongjong</span><span>|</span><a href="#42678560">prev</a><span>|</span><a href="#42678878">next</a><span>|</span><label class="collapse" for="c-42681458">[-]</label><label class="expand" for="c-42681458">[1 more]</label></div><br/><div class="children"><div class="content">Many outages can be summarized simply as &quot;Too many clients attempting to perform an action at the same time.&quot; This is a common situation after a sudden crash or reboot... After recovery, sometimes clients try to reconnect to the servers so quickly that it crashes the servers again and the cycle repeats... Particularly problematic with WebSockets and other stateful connections; hence we use mechanisms like exponential backoff with randomization to spread out the load over time.</div><br/></div></div><div id="42678878" class="c"><input type="checkbox" id="c-42678878" checked=""/><div class="controls bullet"><span class="by">emmanueloga_</span><span>|</span><a href="#42681458">prev</a><span>|</span><a href="#42679759">next</a><span>|</span><label class="collapse" for="c-42678878">[-]</label><label class="expand" for="c-42678878">[1 more]</label></div><br/><div class="children"><div class="content">The whole incident report is interesting, but I feel like the most important part of the solution is buried here [0]:<p>* &quot;We&#x27;re adding timeouts to prevent user requests from waiting excessively long to retrieve assets.&quot;<p>When you get to the size of Canva, you can&#x27;t forget your AbortController and exponential backoff on your Fetch API calls.<p>--<p>0: <a href="https:&#x2F;&#x2F;www.canva.dev&#x2F;blog&#x2F;engineering&#x2F;canva-incident-report-api-gateway-outage&#x2F;#improvements-to-detecting-page-deployment-failures" rel="nofollow">https:&#x2F;&#x2F;www.canva.dev&#x2F;blog&#x2F;engineering&#x2F;canva-incident-report...</a></div><br/></div></div><div id="42679759" class="c"><input type="checkbox" id="c-42679759" checked=""/><div class="controls bullet"><span class="by">benatkin</span><span>|</span><a href="#42678878">prev</a><span>|</span><a href="#42679771">next</a><span>|</span><label class="collapse" for="c-42679759">[-]</label><label class="expand" for="c-42679759">[1 more]</label></div><br/><div class="children"><div class="content">I happened to prefer the original article: <a href="https:&#x2F;&#x2F;www.canva.dev&#x2F;blog&#x2F;engineering&#x2F;canva-incident-report-api-gateway-outage&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.canva.dev&#x2F;blog&#x2F;engineering&#x2F;canva-incident-report...</a></div><br/></div></div><div id="42679771" class="c"><input type="checkbox" id="c-42679771" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#42679759">prev</a><span>|</span><a href="#42680724">next</a><span>|</span><label class="collapse" for="c-42679771">[-]</label><label class="expand" for="c-42679771">[1 more]</label></div><br/><div class="children"><div class="content">The incident report said, “the growth of off-heap memory” was a cause for the OOM.<p>Why would have too much traffic caused that to increase specifically? The overhead of a connection in the kernel isn’t that high.<p>To reduce pressure in the future, they could smear the downloading of new assets over time by background fetching. E.g. when canary release of a new canva release starts they probabilistically could download the asset in the background for the existing version, so when they switch, there’s nothing new to download.<p>Features like collapse forwarding and stale-while-revalidate are powerful features for CDN’s, but there are these non-intuitive failure modes that you have to be aware of. Anything that synchronizes huge numbers of requests is dangerous to stability.</div><br/></div></div><div id="42680724" class="c"><input type="checkbox" id="c-42680724" checked=""/><div class="controls bullet"><span class="by">deathanatos</span><span>|</span><a href="#42679771">prev</a><span>|</span><a href="#42679743">next</a><span>|</span><label class="collapse" for="c-42680724">[-]</label><label class="expand" for="c-42680724">[1 more]</label></div><br/><div class="children"><div class="content">As the OG post states, CF uses &quot;Concurrent Streaming Acceleration&quot; to batch those &quot;270,000+&quot; requests into one to the origin.<p>Now, let&#x27;s grant that the public Internet is not CF&#x27;s private backbone … but TFA makes it out to be more akin to a mobile connection in a tunnel than the Internet? Like transferring across the planet isn&#x27;t going to be <i>amazing</i> … but that fails to explain how a download couldn&#x27;t complete <i>at all</i> over multiple minutes…?</div><br/></div></div><div id="42679280" class="c"><input type="checkbox" id="c-42679280" checked=""/><div class="controls bullet"><span class="by">tryauuum</span><span>|</span><a href="#42679743">prev</a><span>|</span><a href="#42680193">next</a><span>|</span><label class="collapse" for="c-42679280">[-]</label><label class="expand" for="c-42679280">[4 more]</label></div><br/><div class="children"><div class="content">fuck canva, I remember visiting it from Georgia and being greeted a non-working page and a banner shaming me for the war in Ukraine<p>I know there&#x27;s probably some US sanctions list somewhere which the company had to adhere to. But experiencing it in Georgia, where streets are covered with Ukrainian flags and people are very open with their opinion on the war is just surreal</div><br/><div id="42679340" class="c"><input type="checkbox" id="c-42679340" checked=""/><div class="controls bullet"><span class="by">perching_aix</span><span>|</span><a href="#42679280">parent</a><span>|</span><a href="#42680648">next</a><span>|</span><label class="collapse" for="c-42679340">[-]</label><label class="expand" for="c-42679340">[1 more]</label></div><br/><div class="children"><div class="content">that indeed sounds remarkably puzzling, so much so that i find it a bit hard to believe</div><br/></div></div><div id="42680648" class="c"><input type="checkbox" id="c-42680648" checked=""/><div class="controls bullet"><span class="by">stef25</span><span>|</span><a href="#42679280">parent</a><span>|</span><a href="#42679340">prev</a><span>|</span><a href="#42680193">next</a><span>|</span><label class="collapse" for="c-42680648">[-]</label><label class="expand" for="c-42680648">[2 more]</label></div><br/><div class="children"><div class="content">Maybe your IP was mistakenly seen as being in Russia ? Obviously should never have happened</div><br/><div id="42681294" class="c"><input type="checkbox" id="c-42681294" checked=""/><div class="controls bullet"><span class="by">tryauuum</span><span>|</span><a href="#42679280">root</a><span>|</span><a href="#42680648">parent</a><span>|</span><a href="#42680193">next</a><span>|</span><label class="collapse" for="c-42681294">[-]</label><label class="expand" for="c-42681294">[1 more]</label></div><br/><div class="children"><div class="content">&quot;obviously?&quot; I&#x27;ve seen Georgia in US embargo list, although it&#x27;s hard to comprehend what&#x27;s actually embargoed <a href="https:&#x2F;&#x2F;www.bis.gov&#x2F;ear&#x2F;title-15&#x2F;subtitle-b&#x2F;chapter-vii&#x2F;subchapter-c&#x2F;part-740&#x2F;supplement-no-1-part-740-country-groups" rel="nofollow">https:&#x2F;&#x2F;www.bis.gov&#x2F;ear&#x2F;title-15&#x2F;subtitle-b&#x2F;chapter-vii&#x2F;subc...</a></div><br/></div></div></div></div></div></div><div id="42680193" class="c"><input type="checkbox" id="c-42680193" checked=""/><div class="controls bullet"><span class="by">cpatil</span><span>|</span><a href="#42679280">prev</a><span>|</span><a href="#42680021">next</a><span>|</span><label class="collapse" for="c-42680193">[-]</label><label class="expand" for="c-42680193">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps a canary deployment per region might help in such situations? Prime the CDN assets with a smaller set of users.</div><br/></div></div><div id="42680021" class="c"><input type="checkbox" id="c-42680021" checked=""/><div class="controls bullet"><span class="by">faramarz</span><span>|</span><a href="#42680193">prev</a><span>|</span><label class="collapse" for="c-42680021">[-]</label><label class="expand" for="c-42680021">[1 more]</label></div><br/><div class="children"><div class="content">So what is the suggestion at the end of the post? Did I understand correctly that a sandboxed-replica simulator with the fundamental training would harden the system design? Cool! Can you run the simulator based on the basic but complete input architectural drawing? I’d be curious to know if LLMs are able to go and abstract it all across the public network and come back with an attention for all possible known scenarios. Frankly, you can even serve the scenarios into financial forecast models to serve and move the right levers for appropriate actions.<p>These blind spots are exploits waiting to be discovered.</div><br/></div></div></div></div></div></div></div></body></html>