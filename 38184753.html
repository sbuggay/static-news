<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1699434060476" as="style"/><link rel="stylesheet" href="styles.css?v=1699434060476"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2311.00176">ChipNeMo: Domain-Adapted LLMs for Chip Design</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>RafelMri</span> | <span>7 comments</span></div><br/><div><div id="38186206" class="c"><input type="checkbox" id="c-38186206" checked=""/><div class="controls bullet"><span class="by">enoch2090</span><span>|</span><a href="#38186188">next</a><span>|</span><label class="collapse" for="c-38186206">[-]</label><label class="expand" for="c-38186206">[4 more]</label></div><br/><div class="children"><div class="content">The work is solid, the approach of domain-specific tokenizer is definitely effective. But the topic seems to be somehow misleading, as I expect the researchers to somehow find a way to connect LLMs for direct design of chips, while this paper focuses on three topics:
    1. Engineering assistant chatbot, 
    2. EDA tool script generation, and 
    3. Bug summarization and analysis.
Where (1) is mostly irrelavent with the core of &quot;design&quot;, and only (2) is directly related with a useful workflow. What&#x27;s more, it seems that they are only generating at most 10 lines of Python&#x2F;Tcl code (more line &quot;commands&quot; in this case) for the EDA tools.</div><br/><div id="38186286" class="c"><input type="checkbox" id="c-38186286" checked=""/><div class="controls bullet"><span class="by">namibj</span><span>|</span><a href="#38186206">parent</a><span>|</span><a href="#38186188">next</a><span>|</span><label class="collapse" for="c-38186286">[-]</label><label class="expand" for="c-38186286">[3 more]</label></div><br/><div class="children"><div class="content">Their paper from ~6 weeks earlier is on actual LLM-designed chips, though &quot;just&quot; from an LLM benchmark POV: <a href="https:&#x2F;&#x2F;research.nvidia.com&#x2F;index.php&#x2F;publication&#x2F;2023-09_verilogeval-evaluating-large-language-models-verilog-code-generation" rel="nofollow noreferrer">https:&#x2F;&#x2F;research.nvidia.com&#x2F;index.php&#x2F;publication&#x2F;2023-09_ve...</a></div><br/><div id="38187373" class="c"><input type="checkbox" id="c-38187373" checked=""/><div class="controls bullet"><span class="by">pbazarnik</span><span>|</span><a href="#38186206">root</a><span>|</span><a href="#38186286">parent</a><span>|</span><a href="#38187675">next</a><span>|</span><label class="collapse" for="c-38187373">[-]</label><label class="expand" for="c-38187373">[1 more]</label></div><br/><div class="children"><div class="content">It looks like availability of good quality training sets will be a stumbling block for LLM use in Verilog chip design since pretraining with other programming language corpus is not transferable. (see below for quote from Nvidia paper)
A lot of high quality Verilog is locked in licensed, close source IP blocks covered by NDAs.<p>HDLBits problems are a toy level complexity circuits suitable for Verilog 101 course material.<p>I would set a benchmark for serious HDL design LLM at reaching ability to implement AXI bus components with specified by user functionality, e.g. AXI4 Slave (address, data widths, burst capability) with memory implemented as banked synchronous SRAM.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.07544.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.07544.pdf</a><p>* Despite the fact that multi models undergo pretraining on an extensive corpus of multi-lingual code data,
they exhibit only marginal enhancements of approximately
3% when applied to Verilog coding task. This observation
potentially suggests that there is limited positive knowledge transfer between software programming languages like C++ and hardware descriptive languages such as Verilog. This highlights the significance of pretraining on substantial Verilog corpora, as it can significantly enhance model performance in Verilog-related tasks *</div><br/></div></div><div id="38187675" class="c"><input type="checkbox" id="c-38187675" checked=""/><div class="controls bullet"><span class="by">enoch2090</span><span>|</span><a href="#38186206">root</a><span>|</span><a href="#38186286">parent</a><span>|</span><a href="#38187373">prev</a><span>|</span><a href="#38186188">next</a><span>|</span><label class="collapse" for="c-38187675">[-]</label><label class="expand" for="c-38187675">[1 more]</label></div><br/><div class="children"><div class="content">Ooh, this is what I&#x27;ve been looking for!<p>But I agree with pbazarnik that their benchmark are still tackling with toy-level problems. Bits operations, Karnaugh map and other stuff are just one-level of transformation logic, really want to see more in-depth examples such as high-speed circuit optimization (say a clk over 600MHz), complex filters and PID controls, etc. Wouldn&#x27;t hurt even if they only achieve 20% correctness on these problems, as long as the approach can work once every few times.</div><br/></div></div></div></div></div></div><div id="38186188" class="c"><input type="checkbox" id="c-38186188" checked=""/><div class="controls bullet"><span class="by">thatxliner</span><span>|</span><a href="#38186206">prev</a><span>|</span><label class="collapse" for="c-38186188">[-]</label><label class="expand" for="c-38186188">[2 more]</label></div><br/><div class="children"><div class="content">Here’s an idea: to specialize an LLM for a specific field, invent a programming language for that field and fine-tune&#x2F;restrict the LLM to that programming language.</div><br/><div id="38186225" class="c"><input type="checkbox" id="c-38186225" checked=""/><div class="controls bullet"><span class="by">enoch2090</span><span>|</span><a href="#38186188">parent</a><span>|</span><label class="collapse" for="c-38186225">[-]</label><label class="expand" for="c-38186225">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s called DSL(domain-specific language). Domains that have their DSLs will largely benefit as DSLs can work as an LLM interface. Even LangChain is inventing their own DSL (LCEL).</div><br/></div></div></div></div></div></div></div></div></div></body></html>