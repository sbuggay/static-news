<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1734771666194" as="style"/><link rel="stylesheet" href="styles.css?v=1734771666194"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">OpenAI O3 breakthrough high score on ARC-AGI-PUB</a> <span class="domain">(<a href="https://arcprize.org">arcprize.org</a>)</span></div><div class="subtext"><span>maurycy</span> | <span>649 comments</span></div><br/><div><div id="42477713" class="c"><input type="checkbox" id="c-42477713" checked=""/><div class="controls bullet"><span class="by">duluca</span><span>|</span><a href="#42473876">next</a><span>|</span><label class="collapse" for="c-42477713">[-]</label><label class="expand" for="c-42477713">[41 more]</label></div><br/><div class="children"><div class="content">The first computers cost millions of dollars and filled entire rooms to accomplish what we would now consider simple computational tasks. That same computing power now fits into the width of a finger nail. I don’t get how technologists balk at the cost of experimental tech or assume current tech will run at the same efficiency for decades to come and melt the planet into a puddle. 
AGI won’t happen until you can fit enough compute that’d take several data center’s worth of compute into a brain sized vessel. So the thing can move around process the world in real time. This is all going to take some time to say the least. Progress is progress.</div><br/><div id="42478084" class="c"><input type="checkbox" id="c-42478084" checked=""/><div class="controls bullet"><span class="by">8n4vidtmkvmk</span><span>|</span><a href="#42477713">parent</a><span>|</span><a href="#42478368">next</a><span>|</span><label class="collapse" for="c-42478084">[-]</label><label class="expand" for="c-42478084">[10 more]</label></div><br/><div class="children"><div class="content">I thought you were going to say that now we&#x27;re back to bigger-than-room sized computers that cost many millions just to perform the same tasks we could 40 years ago.<p>I of course mean we&#x27;re using these LLMs for a lot of tasks that they&#x27;re inappropriate for, and a clever manually coded algorithm could do better and much more efficiently.</div><br/><div id="42478097" class="c"><input type="checkbox" id="c-42478097" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478084">parent</a><span>|</span><a href="#42478329">next</a><span>|</span><label class="collapse" for="c-42478097">[-]</label><label class="expand" for="c-42478097">[4 more]</label></div><br/><div class="children"><div class="content">just ask the LLM to solve enough problems (even new problems), cache the best, do inference time compute for the rest, figure out the  best&#x2F; fastest implementations, and boom, you have new training data for future AIs</div><br/><div id="42478148" class="c"><input type="checkbox" id="c-42478148" checked=""/><div class="controls bullet"><span class="by">owenpalmer</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478097">parent</a><span>|</span><a href="#42478329">next</a><span>|</span><label class="collapse" for="c-42478148">[-]</label><label class="expand" for="c-42478148">[3 more]</label></div><br/><div class="children"><div class="content">&gt; cache the best<p>How do you quantify that?</div><br/><div id="42478170" class="c"><input type="checkbox" id="c-42478170" checked=""/><div class="controls bullet"><span class="by">martinkallstrom</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478148">parent</a><span>|</span><a href="#42478329">next</a><span>|</span><label class="collapse" for="c-42478170">[-]</label><label class="expand" for="c-42478170">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Assume the role of an expert in cache invalidation...&quot;</div><br/><div id="42478276" class="c"><input type="checkbox" id="c-42478276" checked=""/><div class="controls bullet"><span class="by">DyslexicAtheist</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478170">parent</a><span>|</span><a href="#42478329">next</a><span>|</span><label class="collapse" for="c-42478276">[-]</label><label class="expand" for="c-42478276">[1 more]</label></div><br/><div class="children"><div class="content">&quot;one does not just assume&quot;, &quot;because the hardest problems in Tech are Johnny Cash invalidations&quot; --Lao Tzi</div><br/></div></div></div></div></div></div></div></div><div id="42478329" class="c"><input type="checkbox" id="c-42478329" checked=""/><div class="controls bullet"><span class="by">globalise83</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478084">parent</a><span>|</span><a href="#42478097">prev</a><span>|</span><a href="#42478099">next</a><span>|</span><label class="collapse" for="c-42478329">[-]</label><label class="expand" for="c-42478329">[1 more]</label></div><br/><div class="children"><div class="content">The LLMs are now writing their own algorithms to answer questions. Not long before they can design a more efficient algorithm to complete any feasible computational task, in a millionth of the time needed by the best human.</div><br/></div></div><div id="42478099" class="c"><input type="checkbox" id="c-42478099" checked=""/><div class="controls bullet"><span class="by">adwn</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478084">parent</a><span>|</span><a href="#42478329">prev</a><span>|</span><a href="#42478368">next</a><span>|</span><label class="collapse" for="c-42478099">[-]</label><label class="expand" for="c-42478099">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>and a clever manually coded algorithm could do better and much more efficiently.</i><p>Sure, but how long would it take to implement this algorithm, and would that be worth it for one-off cases?<p>Just today I asked Claude to create a <i>jq</i> query that looks for objects with a certain value for one field, but which lack a certain other field. I could have spent a long time trying to make sense of jq&#x27;s man page, but instead I spent 30 seconds writing a short description of what I&#x27;m looking for in natural language, and the AI returned the correct jq invocation within seconds.</div><br/><div id="42478156" class="c"><input type="checkbox" id="c-42478156" checked=""/><div class="controls bullet"><span class="by">freehorse</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478099">parent</a><span>|</span><a href="#42478368">next</a><span>|</span><label class="collapse" for="c-42478156">[-]</label><label class="expand" for="c-42478156">[3 more]</label></div><br/><div class="children"><div class="content">I don’t think this is a bad use. A bad use would be to give Claude the dataset and ask it to tell you which elements have that value.</div><br/><div id="42478340" class="c"><input type="checkbox" id="c-42478340" checked=""/><div class="controls bullet"><span class="by">globalise83</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478156">parent</a><span>|</span><a href="#42478332">next</a><span>|</span><label class="collapse" for="c-42478340">[-]</label><label class="expand" for="c-42478340">[1 more]</label></div><br/><div class="children"><div class="content">Claude answers a lot of its questions by first writing and then running code to generate the results. Its only limitation is the access to databases and size of context window, both of which will be radically improved over the next 5 years.</div><br/></div></div><div id="42478332" class="c"><input type="checkbox" id="c-42478332" checked=""/><div class="controls bullet"><span class="by">adwn</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478156">parent</a><span>|</span><a href="#42478340">prev</a><span>|</span><a href="#42478368">next</a><span>|</span><label class="collapse" for="c-42478332">[-]</label><label class="expand" for="c-42478332">[1 more]</label></div><br/><div class="children"><div class="content">Ha, I tried that before. However, the file was too large for its context window, so it only seemed to analyze the first part and gave a wrong result.</div><br/></div></div></div></div></div></div></div></div><div id="42478368" class="c"><input type="checkbox" id="c-42478368" checked=""/><div class="controls bullet"><span class="by">Existenceblinks</span><span>|</span><a href="#42477713">parent</a><span>|</span><a href="#42478084">prev</a><span>|</span><a href="#42477763">next</a><span>|</span><label class="collapse" for="c-42478368">[-]</label><label class="expand" for="c-42478368">[1 more]</label></div><br/><div class="children"><div class="content">Honestly, it doesn&#x27;t need to be local, API is some 200ms away is ok-ish, make it 50ms it will be practically usable for every majority of interaction.</div><br/></div></div><div id="42477763" class="c"><input type="checkbox" id="c-42477763" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#42477713">parent</a><span>|</span><a href="#42478368">prev</a><span>|</span><a href="#42478271">next</a><span>|</span><label class="collapse" for="c-42477763">[-]</label><label class="expand" for="c-42477763">[4 more]</label></div><br/><div class="children"><div class="content">&gt; take several data center’s worth of compute into a brain sized vessel. So the thing can move around process the world in real time<p>How so? I&#x27;d imagine a robot connected to the data center embodying its mind, connected via low-latency links, would have to walk pretty far to get into trouble when it comes to interacting with the environment.<p>The speed of light is about three orders of magnitude faster than the speed of signal propagation in biological neurons, after all.</div><br/><div id="42478024" class="c"><input type="checkbox" id="c-42478024" checked=""/><div class="controls bullet"><span class="by">byw</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42477763">parent</a><span>|</span><a href="#42477949">next</a><span>|</span><label class="collapse" for="c-42478024">[-]</label><label class="expand" for="c-42478024">[2 more]</label></div><br/><div class="children"><div class="content">The robot brain could be layered so that more basic functions are embedded locally while higher-level reasonings and offloaded to the cloud.</div><br/><div id="42478100" class="c"><input type="checkbox" id="c-42478100" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478024">parent</a><span>|</span><a href="#42477949">next</a><span>|</span><label class="collapse" for="c-42478100">[-]</label><label class="expand" for="c-42478100">[1 more]</label></div><br/><div class="children"><div class="content">blue strip from iRobot?</div><br/></div></div></div></div><div id="42477949" class="c"><input type="checkbox" id="c-42477949" checked=""/><div class="controls bullet"><span class="by">waldrews</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42477763">parent</a><span>|</span><a href="#42478024">prev</a><span>|</span><a href="#42478271">next</a><span>|</span><label class="collapse" for="c-42477949">[-]</label><label class="expand" for="c-42477949">[1 more]</label></div><br/><div class="children"><div class="content">6 orders of magnitude if we use 120 m&#x2F;s vs 300 km&#x2F;s</div><br/></div></div></div></div><div id="42478271" class="c"><input type="checkbox" id="c-42478271" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#42477713">parent</a><span>|</span><a href="#42477763">prev</a><span>|</span><a href="#42477771">next</a><span>|</span><label class="collapse" for="c-42478271">[-]</label><label class="expand" for="c-42478271">[3 more]</label></div><br/><div class="children"><div class="content">Many of humans&#x27; capabilities are pretrained with massive computing through evolution. Inference results of o3 and its successors might be used to train the next generation of small models to be highly capable. Recent advances in the capabilities of small models such as Gemini-2.0 Flash suggest the same.<p>Recent research from NVIDIA suggests such an efficiency gain is quite possible in the physical realm as well. They trained a tiny model to control the full body of a robot via simulations.<p>---<p>&quot;We trained a 1.5M-parameter neural network to control the body of a humanoid robot. It takes a lot of subconscious processing for us humans to walk, maintain balance, and maneuver our arms and legs into desired positions. We capture this “subconsciousness” in HOVER, a single model that learns how to coordinate the motors of a humanoid robot to support locomotion and manipulation.&quot;<p>...<p>&quot;HOVER supports any humanoid that can be simulated in Isaac. Bring your own robot, and watch it come to life!&quot;<p>More here: <a href="https:&#x2F;&#x2F;x.com&#x2F;DrJimFan&#x2F;status&#x2F;1851643431803830551" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;DrJimFan&#x2F;status&#x2F;1851643431803830551</a><p>---<p>This demonstrates that with proper training, small models can perform at a high level in both cognitive and physical domains.</div><br/><div id="42478315" class="c"><input type="checkbox" id="c-42478315" checked=""/><div class="controls bullet"><span class="by">bigprof</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478271">parent</a><span>|</span><a href="#42477771">next</a><span>|</span><label class="collapse" for="c-42478315">[-]</label><label class="expand" for="c-42478315">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Similarly, many of humans&#x27; capabilities are pretrained with massive computing through evolution.<p>Hmm .. my intuition is that humans&#x27; capabilities are gained during early childhood (walking, running, speaking .. etc) ... what are examples of capabilities pretrained by evolution, and how does this work?</div><br/><div id="42478333" class="c"><input type="checkbox" id="c-42478333" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478315">parent</a><span>|</span><a href="#42477771">next</a><span>|</span><label class="collapse" for="c-42478333">[-]</label><label class="expand" for="c-42478333">[1 more]</label></div><br/><div class="children"><div class="content">The brain is predisposed to learn those skills. Early childhood experiences are necessary to complete the training. Perhaps that could be likened to post-training. It&#x27;s not a one-to-one comparison but a rather loose analogy which I didn&#x27;t make it precise because it is not the key point of the argument.<p>Maybe evolution could be better thought of as neural architecture search combined with some pretraining. Evidence suggests we are prebuilt with &quot;core knowledge&quot; by the time we&#x27;re born.<p>See: Summary of cool research gained from clever &amp; benign experiments with babies here [1].<p>[1] Core knowledge. Elizabeth S. Spelke and Katherine D. Kinzler. <a href="https:&#x2F;&#x2F;www.harvardlds.org&#x2F;wp-content&#x2F;uploads&#x2F;2017&#x2F;01&#x2F;SpelkeKinzler07-1.pdf" rel="nofollow">https:&#x2F;&#x2F;www.harvardlds.org&#x2F;wp-content&#x2F;uploads&#x2F;2017&#x2F;01&#x2F;Spelke...</a></div><br/></div></div></div></div></div></div><div id="42477771" class="c"><input type="checkbox" id="c-42477771" checked=""/><div class="controls bullet"><span class="by">lumost</span><span>|</span><a href="#42477713">parent</a><span>|</span><a href="#42478271">prev</a><span>|</span><a href="#42478026">next</a><span>|</span><label class="collapse" for="c-42477771">[-]</label><label class="expand" for="c-42477771">[8 more]</label></div><br/><div class="children"><div class="content">The concern here is mainly on practicality. The original mainframes did not command startup valuations counted in fractions of the US economy, they did qualify for billions in investment.<p>This is a great milestone, but OpenAI will not be successful charging 10x the cost of a human to perform a task.</div><br/><div id="42477901" class="c"><input type="checkbox" id="c-42477901" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42477771">parent</a><span>|</span><a href="#42478169">next</a><span>|</span><label class="collapse" for="c-42477901">[-]</label><label class="expand" for="c-42477901">[4 more]</label></div><br/><div class="children"><div class="content">The cost of inference has be dropping by ~100x in the past 2 years.<p><a href="https:&#x2F;&#x2F;a16z.com&#x2F;llmflation-llm-inference-cost&#x2F;" rel="nofollow">https:&#x2F;&#x2F;a16z.com&#x2F;llmflation-llm-inference-cost&#x2F;</a></div><br/><div id="42477963" class="c"><input type="checkbox" id="c-42477963" checked=""/><div class="controls bullet"><span class="by">christianqchung</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42477901">parent</a><span>|</span><a href="#42477947">next</a><span>|</span><label class="collapse" for="c-42477963">[-]</label><label class="expand" for="c-42477963">[1 more]</label></div><br/><div class="children"><div class="content">Hmm the link is saying the price of an LLM that scores 42 or above on MMLU has dropped 100x in 2 years, equating gpt 3.5 and llama 3.2 3B. In my opinion gpt 3.5 was significantly better than llama 3B, and certainly much better than the also-equated llama 2 7B. MMLU isn&#x27;t a great marker of overall model capabilities.<p>Obviously the drop in cost for capability in the last 2 years is big, but I&#x27;d wager it&#x27;s closer to 10x than 100x.</div><br/></div></div><div id="42477947" class="c"><input type="checkbox" id="c-42477947" checked=""/><div class="controls bullet"><span class="by">gritzko</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42477901">parent</a><span>|</span><a href="#42477963">prev</a><span>|</span><a href="#42477917">next</a><span>|</span><label class="collapse" for="c-42477947">[-]</label><label class="expand" for="c-42477947">[1 more]</label></div><br/><div class="children"><div class="content">*infernonce</div><br/></div></div><div id="42477917" class="c"><input type="checkbox" id="c-42477917" checked=""/><div class="controls bullet"><span class="by">nico</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42477901">parent</a><span>|</span><a href="#42477947">prev</a><span>|</span><a href="#42478169">next</a><span>|</span><label class="collapse" for="c-42477917">[-]</label><label class="expand" for="c-42477917">[1 more]</label></div><br/><div class="children"><div class="content">*inference</div><br/></div></div></div></div><div id="42478169" class="c"><input type="checkbox" id="c-42478169" checked=""/><div class="controls bullet"><span class="by">owenpalmer</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42477771">parent</a><span>|</span><a href="#42477901">prev</a><span>|</span><a href="#42477816">next</a><span>|</span><label class="collapse" for="c-42478169">[-]</label><label class="expand" for="c-42478169">[2 more]</label></div><br/><div class="children"><div class="content">&gt; OpenAI will not be successful charging 10x the cost of a human to perform a task.<p>True, but they might be successful charging 20x for 2x the skill of a human.</div><br/><div id="42478361" class="c"><input type="checkbox" id="c-42478361" checked=""/><div class="controls bullet"><span class="by">threatripper</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478169">parent</a><span>|</span><a href="#42477816">next</a><span>|</span><label class="collapse" for="c-42478361">[-]</label><label class="expand" for="c-42478361">[1 more]</label></div><br/><div class="children"><div class="content">Or 10x the skill and speed of a human in some specific class of recurrent tasks. We don&#x27;t need full super-human AGI for AI to become economically viable.</div><br/></div></div></div></div><div id="42477816" class="c"><input type="checkbox" id="c-42477816" checked=""/><div class="controls bullet"><span class="by">BriggyDwiggs42</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42477771">parent</a><span>|</span><a href="#42478169">prev</a><span>|</span><a href="#42478026">next</a><span>|</span><label class="collapse" for="c-42477816">[-]</label><label class="expand" for="c-42477816">[1 more]</label></div><br/><div class="children"><div class="content">I wouldn’t expect it to cost 10x in five years, if only because parallel computing still seems to be roughly obeying moore’s.</div><br/></div></div></div></div><div id="42478026" class="c"><input type="checkbox" id="c-42478026" checked=""/><div class="controls bullet"><span class="by">pera</span><span>|</span><a href="#42477713">parent</a><span>|</span><a href="#42477771">prev</a><span>|</span><a href="#42477951">next</a><span>|</span><label class="collapse" for="c-42478026">[-]</label><label class="expand" for="c-42478026">[6 more]</label></div><br/><div class="children"><div class="content">Maybe AGI as a goal is overvalued: If you have a machine that can, on average, perform symbolic reasoning better than humans, and at a lower cost, that&#x27;s basically the end game, isn&#x27;t it? You won capitalism.</div><br/><div id="42478111" class="c"><input type="checkbox" id="c-42478111" checked=""/><div class="controls bullet"><span class="by">harrall</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478026">parent</a><span>|</span><a href="#42477951">next</a><span>|</span><label class="collapse" for="c-42478111">[-]</label><label class="expand" for="c-42478111">[5 more]</label></div><br/><div class="children"><div class="content">Right now I can ask an (experienced) human to do something for me and they will either just get it done or tell me that they can’t do it.<p>Right now when I ask an LLM… I have to sit there and verify everything. It may have done some helpful reasoning for me but the whole point of me asking someone else (or something else) was to do nothing at all…<p>I’m not sure you can reliably fulfill the first scenario without achieving AGI. Maybe you can, but we are not at that point yet so we don’t know yet.</div><br/><div id="42478279" class="c"><input type="checkbox" id="c-42478279" checked=""/><div class="controls bullet"><span class="by">vbezhenar</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478111">parent</a><span>|</span><a href="#42478210">next</a><span>|</span><label class="collapse" for="c-42478279">[-]</label><label class="expand" for="c-42478279">[1 more]</label></div><br/><div class="children"><div class="content">There are so called &quot;yes-men&quot; who can&#x27;t say &quot;no&quot; in no situation. That&#x27;s rooted in their culture. I suspect that AI was trained using their assistance. I mean, answering &quot;I can&#x27;t do that&quot; is the simplest LLM path that should work often unless they gone out of their way to downrank it.</div><br/></div></div><div id="42478210" class="c"><input type="checkbox" id="c-42478210" checked=""/><div class="controls bullet"><span class="by">pera</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478111">parent</a><span>|</span><a href="#42478279">prev</a><span>|</span><a href="#42478189">next</a><span>|</span><label class="collapse" for="c-42478210">[-]</label><label class="expand" for="c-42478210">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not clear to me whether AGI is necessary for solving most of the issues in the current generation of LLMs. It is possible you can get there by hacking together CoTs with automated theorem provers and bruteforcing your way to the solution or something like that.<p>But if it&#x27;s not enough then maybe it might come as a second-order effect (e.g. reasoning machines having to bootstrap an AGI so then you can have a Waymo taxi driver who is also a Fields medalist)</div><br/></div></div><div id="42478189" class="c"><input type="checkbox" id="c-42478189" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478111">parent</a><span>|</span><a href="#42478210">prev</a><span>|</span><a href="#42478335">next</a><span>|</span><label class="collapse" for="c-42478189">[-]</label><label class="expand" for="c-42478189">[1 more]</label></div><br/><div class="children"><div class="content">You do need to verify humans work though.<p>The difference, to me, is that humans seem to be good at canceling each other&#x27;s mistakes when put in a proper environment.</div><br/></div></div><div id="42478335" class="c"><input type="checkbox" id="c-42478335" checked=""/><div class="controls bullet"><span class="by">concordDance</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42478111">parent</a><span>|</span><a href="#42478189">prev</a><span>|</span><a href="#42477951">next</a><span>|</span><label class="collapse" for="c-42478335">[-]</label><label class="expand" for="c-42478335">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Right now I can ask an (experienced) human to do something for me and they will either just get it done or tell me that they can’t do it.<p>Finding reliable honest humans is a problem governments have struggled with for over a hundred years. If you have cracked this problem at scale you really need to write it up! There are a lot of people who would be extremely interested in a solution here.</div><br/></div></div></div></div></div></div><div id="42477951" class="c"><input type="checkbox" id="c-42477951" checked=""/><div class="controls bullet"><span class="by">TechDebtDevin</span><span>|</span><a href="#42477713">parent</a><span>|</span><a href="#42478026">prev</a><span>|</span><a href="#42477856">next</a><span>|</span><label class="collapse" for="c-42477951">[-]</label><label class="expand" for="c-42477951">[1 more]</label></div><br/><div class="children"><div class="content">Batteries..</div><br/></div></div><div id="42477856" class="c"><input type="checkbox" id="c-42477856" checked=""/><div class="controls bullet"><span class="by">otabdeveloper4</span><span>|</span><a href="#42477713">parent</a><span>|</span><a href="#42477951">prev</a><span>|</span><a href="#42477957">next</a><span>|</span><label class="collapse" for="c-42477856">[-]</label><label class="expand" for="c-42477856">[6 more]</label></div><br/><div class="children"><div class="content">Intelligence has nothing at all whatever to do with compute.</div><br/><div id="42477867" class="c"><input type="checkbox" id="c-42477867" checked=""/><div class="controls bullet"><span class="by">oefnak</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42477856">parent</a><span>|</span><a href="#42477944">next</a><span>|</span><label class="collapse" for="c-42477867">[-]</label><label class="expand" for="c-42477867">[2 more]</label></div><br/><div class="children"><div class="content">Unless you&#x27;re a dualist who believes in a magic spirit, I cannot understand how you think that&#x27;s the case. Can you please explain?</div><br/><div id="42478130" class="c"><input type="checkbox" id="c-42478130" checked=""/><div class="controls bullet"><span class="by">freehorse</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42477867">parent</a><span>|</span><a href="#42477944">next</a><span>|</span><label class="collapse" for="c-42478130">[-]</label><label class="expand" for="c-42478130">[1 more]</label></div><br/><div class="children"><div class="content">Intelligence is about learning from few examples and generalising to novel solutions. Increasing compute so that exploring the whole problem space is possible is not intelligence. There is a reason the actual ARC-AGI price has efficiency as one of the success requirements. It is not so that the solutions scale to production and whatnot, these are toy tasks. It is to help ensure that it is actually an intelligent system solving these.<p>So yeah, the o3 result is impressive but if the difference between o3 and the previous state of art is more compute to do a much longer CoT&#x2F;evaluation loop, I am not so impressed. Reminder that these problems are solved by humans in seconds, ARC-AGI is supposed to be easy.</div><br/></div></div></div></div><div id="42477944" class="c"><input type="checkbox" id="c-42477944" checked=""/><div class="controls bullet"><span class="by">patrickhogan1</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42477856">parent</a><span>|</span><a href="#42477867">prev</a><span>|</span><a href="#42477957">next</a><span>|</span><label class="collapse" for="c-42477944">[-]</label><label class="expand" for="c-42477944">[3 more]</label></div><br/><div class="children"><div class="content">Do you think intelligence exists without prior experience? For instance, can someone instantly acquire a skill—like playing the piano—as if downloading it in The Matrix? Even prodigies like Mozart had prior exposure. His father, a composer and music teacher, introduced him to music from an early age. Does true intelligence require a foundation of prior knowledge?</div><br/><div id="42478342" class="c"><input type="checkbox" id="c-42478342" checked=""/><div class="controls bullet"><span class="by">owenpalmer</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42477944">parent</a><span>|</span><a href="#42478060">next</a><span>|</span><label class="collapse" for="c-42478342">[-]</label><label class="expand" for="c-42478342">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Does true intelligence require a foundation of prior knowledge?<p>This is the way I think about it.<p>I = E &#x2F; K<p>where I is the intelligence of the system, E is the effectiveness of the system, and K is the prior knowledge.<p>For example, a math problem is given to two students, each solving the problem with the same effectiveness (both get the correct answer in the same amount of time). However, student A happens to have more prior knowledge of math than student B. In this case, the intelligence of B is greater than the intelligence of A, even though they have the same effectiveness. B was able to &quot;figure out&quot; the math, without using any of the &quot;tricks&quot; that A already knew.<p>Now back to your question of whether or not prior knowledge is required. As K approaches 0, intelligence approaches infinity. But when K=0, intelligence is undefined. Tada! I think that answers your question.<p>Most LLM benchmarks simply measure effectiveness, not intelligence. I conceptualize LLMs as a person with a photographic memory and a low IQ of 85, who was given 100 billion years to learn everything humans have ever created.<p>IK = E<p>low intelligence * vast knowledge = reasonable effectiveness</div><br/></div></div><div id="42478060" class="c"><input type="checkbox" id="c-42478060" checked=""/><div class="controls bullet"><span class="by">1659447091</span><span>|</span><a href="#42477713">root</a><span>|</span><a href="#42477944">parent</a><span>|</span><a href="#42478342">prev</a><span>|</span><a href="#42477957">next</a><span>|</span><label class="collapse" for="c-42478060">[-]</label><label class="expand" for="c-42478060">[1 more]</label></div><br/><div class="children"><div class="content">Intelligence requires the ability to separate the wheat from the chaff on one&#x27;s own to create a foundation of knowledge to build on.<p>It is also entirely possible to learn a skill without prior experience. That&#x27;s how it(whatever skill) was first done</div><br/></div></div></div></div></div></div></div></div><div id="42473876" class="c"><input type="checkbox" id="c-42473876" checked=""/><div class="controls bullet"><span class="by">bluecoconut</span><span>|</span><a href="#42477713">prev</a><span>|</span><a href="#42478098">next</a><span>|</span><label class="collapse" for="c-42473876">[-]</label><label class="expand" for="c-42473876">[91 more]</label></div><br/><div class="children"><div class="content">Efficiency is now key.<p>~=$3400 per single task to meet human performance on this benchmark is a lot. Also it shows the bullets as &quot;ARC-AGI-TUNED&quot;, which makes me think they did some undisclosed amount of fine-tuning (eg. via the API they showed off last week), so even more compute went into this task.<p>We can compare this roughly to a human doing ARC-AGI puzzles, where a human will take (high variance in my subjective experience) between 5 second and 5 minutes to solve the task.
(So i&#x27;d argue a human is at 0.03USD - 1.67USD per puzzle at 20USD&#x2F;hr, and they include in their document an average mechancal turker at $2 USD task in their document)<p>Going the other direction: I am interpreting this result as human level reasoning now costs (approximately) 41k&#x2F;hr to 2.5M&#x2F;hr with current compute.<p>Super exciting that OpenAI pushed the compute out this far so we could see he O-series scaling continue and intersect humans on ARC, now we get to work towards making this economical!</div><br/><div id="42474120" class="c"><input type="checkbox" id="c-42474120" checked=""/><div class="controls bullet"><span class="by">bluecoconut</span><span>|</span><a href="#42473876">parent</a><span>|</span><a href="#42477966">next</a><span>|</span><label class="collapse" for="c-42474120">[-]</label><label class="expand" for="c-42474120">[65 more]</label></div><br/><div class="children"><div class="content">some other imporant quotes: &quot;Average human off the street: 70-80%. STEM college grad: &gt;95%. Panel of 10 random humans: 99-100%&quot; -@fchollet on X<p>So, considering that the $3400&#x2F;task system isn&#x27;t able to compete with STEM college grad yet, we still have some room (but it is shrinking, i expect even more compute will be thrown and we&#x27;ll see these barriers broken in coming years)<p>Also, some other back of envelope calculations:<p>The gap in cost is roughly 10^3 between O3 High and Avg. mechanical turkers (humans). 
Via Pure GPU cost improvement (~doubling every 2-2.5 years) puts us at 20~25 years.<p>The question is now, can we close this &quot;to human&quot; gap (10^3) quickly with algorithms, or are we stuck waiting for the 20-25 years for GPU improvements. (I think it feels obvious: this is new technology, things are moving fast, the chance for algorithmic innovation here is high!)<p>I also personally think that we need to adjust our efficiency priors, and start looking not at &quot;humans&quot; as the bar to beat, but theoretical computatble limits (show gaps much larger ~10^9-10^15 for modest problems). Though, it may simply be the case that tool&#x2F;code use + AGI at near human cost covers a lot of that gap.</div><br/><div id="42476817" class="c"><input type="checkbox" id="c-42476817" checked=""/><div class="controls bullet"><span class="by">miki123211</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474120">parent</a><span>|</span><a href="#42476627">next</a><span>|</span><label class="collapse" for="c-42476817">[-]</label><label class="expand" for="c-42476817">[19 more]</label></div><br/><div class="children"><div class="content">It&#x27;s also worth keeping in mind that AIs are a lot less risky to deploy for businesses than humans.<p>You can scale them up and down at any time, they can work 24&#x2F;7 (including holidays) with no overtime pay and no breaks, they need no corporate campuses, office space, HR personnel or travel budgets, you don&#x27;t have to worry about key employees going on sick&#x2F;maternity leave or taking time off the moment they&#x27;re needed most, they won&#x27;t assault a coworker, sue for discrimination or secretly turn out to be a pedophile and tarnish the reputation of your company, they won&#x27;t leak internal documents to the press or rage quit because of new company policies, they won&#x27;t even stop working when a pandemic stops most of the world from running.</div><br/><div id="42477759" class="c"><input type="checkbox" id="c-42477759" checked=""/><div class="controls bullet"><span class="by">rockskon</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476817">parent</a><span>|</span><a href="#42476892">next</a><span>|</span><label class="collapse" for="c-42477759">[-]</label><label class="expand" for="c-42477759">[5 more]</label></div><br/><div class="children"><div class="content">AI has a different risk profile than humans.  They are a <i>lot</i> more risky for business operations where failure is wholly unacceptable under any circumstance.<p>They&#x27;re risky in that they fail in ways that aren&#x27;t readily deterministic.<p>And would you trust your life to a self-driving car in New York City traffic?</div><br/><div id="42477774" class="c"><input type="checkbox" id="c-42477774" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477759">parent</a><span>|</span><a href="#42477805">next</a><span>|</span><label class="collapse" for="c-42477774">[-]</label><label class="expand" for="c-42477774">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t everybody in NYC already? (The dangers of bad driving are much higher for pedestrians than for people in cars; there are more of the former than of the latter in NYC; I&#x27;d expect there to be a non-zero number of fully self driving cars already in the city.)</div><br/><div id="42477893" class="c"><input type="checkbox" id="c-42477893" checked=""/><div class="controls bullet"><span class="by">rockskon</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477774">parent</a><span>|</span><a href="#42477805">next</a><span>|</span><label class="collapse" for="c-42477893">[-]</label><label class="expand" for="c-42477893">[1 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t answer my question.</div><br/></div></div></div></div><div id="42477805" class="c"><input type="checkbox" id="c-42477805" checked=""/><div class="controls bullet"><span class="by">wwweston</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477759">parent</a><span>|</span><a href="#42477774">prev</a><span>|</span><a href="#42478008">next</a><span>|</span><label class="collapse" for="c-42477805">[-]</label><label class="expand" for="c-42477805">[1 more]</label></div><br/><div class="children"><div class="content">We can just insulate businesses employing AI from any liability, problem solved.</div><br/></div></div><div id="42478008" class="c"><input type="checkbox" id="c-42478008" checked=""/><div class="controls bullet"><span class="by">ijidak</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477759">parent</a><span>|</span><a href="#42477805">prev</a><span>|</span><a href="#42476892">next</a><span>|</span><label class="collapse" for="c-42478008">[-]</label><label class="expand" for="c-42478008">[1 more]</label></div><br/><div class="children"><div class="content">It is amazing to me that we have reached an era where we are debating the trade-off of hiring thinking machines!<p>I mean, this is an incredible moment from that standpoint.<p>Regarding the topic at hand, I think that there will always be room for humans for the reasons you listed.<p>But even replacing 5% of humans with AI&#x27;s will have mind boggling consequences.<p>I think you&#x27;re right that there are jobs that humans will be preferred for for quite some time.<p>But, I&#x27;m already using AI with success where I would previously hire a human, and this is in this primitive stage.<p>With the leaps we are seeing, AI is coming for jobs.<p>Your concerns relate to exactly how many jobs.<p>And only time will tell.<p>But, I think some meaningful percentage of the population -- even if just 5% of humanity will be replaced by AI.</div><br/></div></div></div></div><div id="42476892" class="c"><input type="checkbox" id="c-42476892" checked=""/><div class="controls bullet"><span class="by">antihipocrat</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476817">parent</a><span>|</span><a href="#42477759">prev</a><span>|</span><a href="#42477381">next</a><span>|</span><label class="collapse" for="c-42476892">[-]</label><label class="expand" for="c-42476892">[12 more]</label></div><br/><div class="children"><div class="content">AI brings similar risks - they can leak internal information, they can be tricked into performing prohibited tasks (with catastrophic effects if this is connected to core systems), they could be accused of actions that are discriminatory (biased training sets are very common).<p>Sure, if a business deploys it to perform tasks that are inherently low risk e.g. no client interface, no core system connection and low error impact, then the human performing these tasks is going to be replaced.</div><br/><div id="42477292" class="c"><input type="checkbox" id="c-42477292" checked=""/><div class="controls bullet"><span class="by">snozolli</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476892">parent</a><span>|</span><a href="#42477381">next</a><span>|</span><label class="collapse" for="c-42477292">[-]</label><label class="expand" for="c-42477292">[11 more]</label></div><br/><div class="children"><div class="content"><i>they can be tricked into performing prohibited tasks</i><p>This reminds me of the school principal who sent $100k to a scammer claiming to be Elon Musk.  The kicker is that she was repeatedly told that it was a scam.<p><a href="https:&#x2F;&#x2F;abc7chicago.com&#x2F;fake-elon-musk-jan-mcgee-principal-burns-science-and-technology-charter-school-scam&#x2F;13044937&#x2F;" rel="nofollow">https:&#x2F;&#x2F;abc7chicago.com&#x2F;fake-elon-musk-jan-mcgee-principal-b...</a></div><br/><div id="42477720" class="c"><input type="checkbox" id="c-42477720" checked=""/><div class="controls bullet"><span class="by">tstrimple</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477292">parent</a><span>|</span><a href="#42477381">next</a><span>|</span><label class="collapse" for="c-42477720">[-]</label><label class="expand" for="c-42477720">[10 more]</label></div><br/><div class="children"><div class="content">This is one of the things which annoys me most about anti-LLM hate. Your peers aren&#x27;t right all the time either. They believe incorrect things and will pursue worse solutions because they won&#x27;t acknowledge a better way. How is this any different from a LLM? You have to question <i>everything</i> you&#x27;re presented with. Sometimes that Stack Overflow answer isn&#x27;t directly applicable to your exact problem but you can extrapolate from it to resolve your problem. Why is an LLM viewed any differently? Of course you can&#x27;t just blindly accept it as the one true answer, but you literally cannot do that with humans either. Humans produce a ton of shit code and non-solutions and it&#x27;s fine. But when an LLM does it, it&#x27;s a serious problem that means the tech is useless. Much of the modern world is built on shit solutions and we still hobble along.</div><br/><div id="42477738" class="c"><input type="checkbox" id="c-42477738" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477720">parent</a><span>|</span><a href="#42478029">next</a><span>|</span><label class="collapse" for="c-42477738">[-]</label><label class="expand" for="c-42477738">[6 more]</label></div><br/><div class="children"><div class="content">Everyone knows humans can be idiots. The problem is that people seem to think LLMs can’t be idiots, and because they aren’t human there is no way to punish them. And then people give them too much credit&#x2F;power, for their own purposes.<p>Which makes LLMs far more dangerous than idiot humans in most cases.</div><br/><div id="42477921" class="c"><input type="checkbox" id="c-42477921" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477738">parent</a><span>|</span><a href="#42478029">next</a><span>|</span><label class="collapse" for="c-42477921">[-]</label><label class="expand" for="c-42477921">[5 more]</label></div><br/><div class="children"><div class="content">No. Nobody thinks LLMs are perfect. That’s a strawman.<p>And… I am really not sure punishment is the answer to fallibility, outside of almost kinky Catholicism.<p>The reality is these things are very good, but imperfect, much like people.</div><br/><div id="42478073" class="c"><input type="checkbox" id="c-42478073" checked=""/><div class="controls bullet"><span class="by">thecupisblue</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477921">parent</a><span>|</span><a href="#42477933">next</a><span>|</span><label class="collapse" for="c-42478073">[-]</label><label class="expand" for="c-42478073">[3 more]</label></div><br/><div class="children"><div class="content">Sorry man, but I literally know of startups invested into by YC where CEO&#x27;s for 80% of their management decisions&#x2F;vision&#x2F;comms use ChatGPT ... or should I say some use Claude now, as they think it&#x27;s smarter and does not make mistakes.<p>Let that sink in.</div><br/><div id="42478138" class="c"><input type="checkbox" id="c-42478138" checked=""/><div class="controls bullet"><span class="by">onion2k</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42478073">parent</a><span>|</span><a href="#42477933">next</a><span>|</span><label class="collapse" for="c-42478138">[-]</label><label class="expand" for="c-42478138">[2 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t be surprised if GPT genuinely makes better decisions than an inexperienced, first-time CEO who has only been a dev before, especially if the person prompting it has actually put some effort into understanding their own weaknesses. It certainly wouldn&#x27;t be any worse than someone who&#x27;s only experience is reading a few management books.</div><br/><div id="42478316" class="c"><input type="checkbox" id="c-42478316" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42478138">parent</a><span>|</span><a href="#42477933">next</a><span>|</span><label class="collapse" for="c-42478316">[-]</label><label class="expand" for="c-42478316">[1 more]</label></div><br/><div class="children"><div class="content">And here is a great example of the problem.<p>An LLM doesn’t make decisions. It generates text that plausibly looks like it made a decision, when prompted with the right text.</div><br/></div></div></div></div></div></div><div id="42477933" class="c"><input type="checkbox" id="c-42477933" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477921">parent</a><span>|</span><a href="#42478073">prev</a><span>|</span><a href="#42478029">next</a><span>|</span><label class="collapse" for="c-42477933">[-]</label><label class="expand" for="c-42477933">[1 more]</label></div><br/><div class="children"><div class="content">Clearly you haven’t been listening to any CEO press releases lately?<p>And when was the last time a support chatbot let you actually complain or bypass to a human?</div><br/></div></div></div></div></div></div><div id="42478029" class="c"><input type="checkbox" id="c-42478029" checked=""/><div class="controls bullet"><span class="by">pineaux</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477720">parent</a><span>|</span><a href="#42477738">prev</a><span>|</span><a href="#42477767">next</a><span>|</span><label class="collapse" for="c-42478029">[-]</label><label class="expand" for="c-42478029">[1 more]</label></div><br/><div class="children"><div class="content">Its quite stunning to frame it as anti-LLM hate. It&#x27;s on the pro-LLM people to convince the anti-LLM people that choosing for LLMs is an ethically correct choice with all the necessary guardrails. 
It&#x27;s also on the pro-LLM people to show the usefulness of the product. If pro-LLM people are right, it will be a matter of time before these people will see the errors of their ways. But doing an ad-hominem is a sure way of creating a divide...</div><br/></div></div><div id="42477767" class="c"><input type="checkbox" id="c-42477767" checked=""/><div class="controls bullet"><span class="by">mplewis</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477720">parent</a><span>|</span><a href="#42478029">prev</a><span>|</span><a href="#42477381">next</a><span>|</span><label class="collapse" for="c-42477767">[-]</label><label class="expand" for="c-42477767">[2 more]</label></div><br/><div class="children"><div class="content">Humans can tell you how confident they are in something being right or wrong. An LLM has no internal model and cannot do such a thing.</div><br/><div id="42478037" class="c"><input type="checkbox" id="c-42478037" checked=""/><div class="controls bullet"><span class="by">swiftcoder</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477767">parent</a><span>|</span><a href="#42477381">next</a><span>|</span><label class="collapse" for="c-42478037">[-]</label><label class="expand" for="c-42478037">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Humans can tell you how confident they are in something being right or wrong<p>Humans are also very confidently wrong a considerable portion of the time. Particularly about anything outside their direct expertise</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42477381" class="c"><input type="checkbox" id="c-42477381" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476817">parent</a><span>|</span><a href="#42476892">prev</a><span>|</span><a href="#42476627">next</a><span>|</span><label class="collapse" for="c-42477381">[-]</label><label class="expand" for="c-42477381">[1 more]</label></div><br/><div class="children"><div class="content">&gt;secretly turn out to be a pedophile and tarnish the reputation of your company<p>This is interesting because it&#x27;s both Oddly Specific and also something I have seen happen and I still feel really sorry for the company involved. Now that I think about it, I&#x27;ve actually seen it happen twice.</div><br/></div></div></div></div><div id="42476627" class="c"><input type="checkbox" id="c-42476627" checked=""/><div class="controls bullet"><span class="by">xbmcuser</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474120">parent</a><span>|</span><a href="#42476817">prev</a><span>|</span><a href="#42474856">next</a><span>|</span><label class="collapse" for="c-42476627">[-]</label><label class="expand" for="c-42476627">[18 more]</label></div><br/><div class="children"><div class="content">You are missing that cost of electricity is also going to keep falling because of solar and batteries. This year in China my table cloth math says it is $0.05 pkwh and following the cost decline trajectory be under $0.01 in 10 years</div><br/><div id="42477182" class="c"><input type="checkbox" id="c-42477182" checked=""/><div class="controls bullet"><span class="by">barney54</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476627">parent</a><span>|</span><a href="#42476826">next</a><span>|</span><label class="collapse" for="c-42477182">[-]</label><label class="expand" for="c-42477182">[6 more]</label></div><br/><div class="children"><div class="content">But the cost of electricity is not falling—it’s increasing.  Wholesale prices have decreased, but retail rates are up.  In the U.S. rates are up 27% over the past 4 years.  In Europe prices are up too.</div><br/><div id="42477448" class="c"><input type="checkbox" id="c-42477448" checked=""/><div class="controls bullet"><span class="by">xbmcuser</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477182">parent</a><span>|</span><a href="#42477389">next</a><span>|</span><label class="collapse" for="c-42477448">[-]</label><label class="expand" for="c-42477448">[2 more]</label></div><br/><div class="children"><div class="content">Most large compute clusters would be buying electricity at wholesale price not at retail price. 
But anyway solar and battery prices have just reached the tipping point this year only now the longer power companies keep retail prices high the more people will defect from the grid and install their own solar + batteries.</div><br/></div></div><div id="42477389" class="c"><input type="checkbox" id="c-42477389" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477182">parent</a><span>|</span><a href="#42477448">prev</a><span>|</span><a href="#42477778">next</a><span>|</span><label class="collapse" for="c-42477389">[-]</label><label class="expand" for="c-42477389">[1 more]</label></div><br/><div class="children"><div class="content">I am not certain because I&#x27;ve been very focused on the o3 news, but at least yesterday neither the US nor Europe were part of China.</div><br/></div></div><div id="42477778" class="c"><input type="checkbox" id="c-42477778" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477182">parent</a><span>|</span><a href="#42477389">prev</a><span>|</span><a href="#42477875">next</a><span>|</span><label class="collapse" for="c-42477778">[-]</label><label class="expand" for="c-42477778">[1 more]</label></div><br/><div class="children"><div class="content">But data centers pay wholesale prices or even less (given that especially AI training and, to a lesser extend, inference clusters can load shed like few other consumers of electricity).</div><br/></div></div><div id="42477875" class="c"><input type="checkbox" id="c-42477875" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477182">parent</a><span>|</span><a href="#42477778">prev</a><span>|</span><a href="#42476826">next</a><span>|</span><label class="collapse" for="c-42477875">[-]</label><label class="expand" for="c-42477875">[1 more]</label></div><br/><div class="children"><div class="content">And this is great news as long as marginal production (the most expensive to produce, first to turn on&#x2F;off according to demand) of electricity is fossils.</div><br/></div></div></div></div><div id="42476826" class="c"><input type="checkbox" id="c-42476826" checked=""/><div class="controls bullet"><span class="by">patrickhogan1</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476627">parent</a><span>|</span><a href="#42477182">prev</a><span>|</span><a href="#42477812">next</a><span>|</span><label class="collapse" for="c-42476826">[-]</label><label class="expand" for="c-42476826">[6 more]</label></div><br/><div class="children"><div class="content">Bingo! Solar energy moves us toward a future where a household&#x27;s energy needs become nearly cost-free.<p>Energy Need: The average home uses 30 kWh&#x2F;day, requiring 6 kW&#x2F;hour over 5 peak sunlight hours.<p>Multijunction Panels: Lab efficiencies are already at 47% (2023), and with multiple years of progress, 60% efficiency is probable.<p>Efficiency Impact: At 60% efficiency, panels generate 600 W&#x2F;m², requiring 10 m² (e.g., 2 m × 5 m) to meet energy needs.<p>This size can fit on most home roofs, be mounted on a pole with stacked layers, or even be hung through an apartment window.</div><br/><div id="42476867" class="c"><input type="checkbox" id="c-42476867" checked=""/><div class="controls bullet"><span class="by">arcticbull</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476826">parent</a><span>|</span><a href="#42477812">next</a><span>|</span><label class="collapse" for="c-42476867">[-]</label><label class="expand" for="c-42476867">[5 more]</label></div><br/><div class="children"><div class="content">Everyone always forgets that they only perform at less than half of their rated capacity and require significant battery installations. Rooftop solar plus storage is actually more expensive than nuclear on a comparable system LCOE due to their lack of efficiency of scale. Rooftop solar plus storage is about the most expensive form of electricity on earth, maybe excluding gas peaker plants.</div><br/><div id="42477142" class="c"><input type="checkbox" id="c-42477142" checked=""/><div class="controls bullet"><span class="by">xbmcuser</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476867">parent</a><span>|</span><a href="#42477455">next</a><span>|</span><label class="collapse" for="c-42477142">[-]</label><label class="expand" for="c-42477142">[1 more]</label></div><br/><div class="children"><div class="content">Everyone also forgets the speed of price decline for solar and battery your statement is completely false propaganda made up by power companies. Today rooftop solar and battery is cost competitive to nuclear already in many countries like India</div><br/></div></div><div id="42477455" class="c"><input type="checkbox" id="c-42477455" checked=""/><div class="controls bullet"><span class="by">patrickhogan1</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476867">parent</a><span>|</span><a href="#42477142">prev</a><span>|</span><a href="#42477894">next</a><span>|</span><label class="collapse" for="c-42477455">[-]</label><label class="expand" for="c-42477455">[1 more]</label></div><br/><div class="children"><div class="content">You’re right that rooftop solar and storage have costs and efficiency limits, but those are improving quickly.<p>Rooftop solar harnesses energy from the sun, which is powered by nuclear fusion—arguably the most effective nuclear reactor in our solar system.</div><br/></div></div><div id="42477894" class="c"><input type="checkbox" id="c-42477894" checked=""/><div class="controls bullet"><span class="by">theendisney</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476867">parent</a><span>|</span><a href="#42477455">prev</a><span>|</span><a href="#42476998">next</a><span>|</span><label class="collapse" for="c-42477894">[-]</label><label class="expand" for="c-42477894">[1 more]</label></div><br/><div class="children"><div class="content">The thing everyone forgets is that all good energy technology is seized by governments for military purposes and to preserve the status quo. God knows how far it progressed.<p>What a joke</div><br/></div></div><div id="42476998" class="c"><input type="checkbox" id="c-42476998" checked=""/><div class="controls bullet"><span class="by">nateglims</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476867">parent</a><span>|</span><a href="#42477894">prev</a><span>|</span><a href="#42477812">next</a><span>|</span><label class="collapse" for="c-42476998">[-]</label><label class="expand" for="c-42476998">[1 more]</label></div><br/><div class="children"><div class="content">It varies by a lot of factors but it’s way less than half. Photovoltaic panels have around 10% capacity utilization vs 50-70% for a gas or nuke plant.</div><br/></div></div></div></div></div></div><div id="42477812" class="c"><input type="checkbox" id="c-42477812" checked=""/><div class="controls bullet"><span class="by">necovek</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476627">parent</a><span>|</span><a href="#42476826">prev</a><span>|</span><a href="#42477031">next</a><span>|</span><label class="collapse" for="c-42477812">[-]</label><label class="expand" for="c-42477812">[1 more]</label></div><br/><div class="children"><div class="content">If climate change ends up changing weather profiles and we start seeing many more cloudy days or dust&#x2F;mist in the air, we&#x27;ll need to push those solar panel above (all the way to space?) or have many more of them, figure out transmission to the ground and costs will very much balloon.<p>Not saying this will happen, but it&#x27;s risky to rely on solar as the only long-term solution.</div><br/></div></div><div id="42477031" class="c"><input type="checkbox" id="c-42477031" checked=""/><div class="controls bullet"><span class="by">nateglims</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476627">parent</a><span>|</span><a href="#42477812">prev</a><span>|</span><a href="#42474856">next</a><span>|</span><label class="collapse" for="c-42477031">[-]</label><label class="expand" for="c-42477031">[4 more]</label></div><br/><div class="children"><div class="content">Is it going to fall significantly for data centers? Industrial policy for consumer power is different from subsidizing it for data centers and if you own grid infrastructure why would you tank the price by putting up massive amounts of capital?</div><br/><div id="42477561" class="c"><input type="checkbox" id="c-42477561" checked=""/><div class="controls bullet"><span class="by">xbmcuser</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477031">parent</a><span>|</span><a href="#42474856">next</a><span>|</span><label class="collapse" for="c-42477561">[-]</label><label class="expand" for="c-42477561">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the same about using the cloud or using your own infrastructure there will be a point where building your own solar and battery plant is cheaper than what they are charging they will need to follow the price decline if they want to keep the customers if not there will be mass scale grid defections.</div><br/><div id="42477658" class="c"><input type="checkbox" id="c-42477658" checked=""/><div class="controls bullet"><span class="by">nateglims</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477561">parent</a><span>|</span><a href="#42474856">next</a><span>|</span><label class="collapse" for="c-42477658">[-]</label><label class="expand" for="c-42477658">[2 more]</label></div><br/><div class="children"><div class="content">I don’t think this reflects the reality of the power industry. Data centers are the only significant growth in actual generated power in decades and hyperscalers are already looking at very bespoke solutions.<p>The heavy commodification of networking and compute brought about by the internet and cloud aligned with tech company interests in delivering services or content to consumers. There does not seem to be an emerging consensus that data center operators also need to provide consumer power.</div><br/><div id="42477748" class="c"><input type="checkbox" id="c-42477748" checked=""/><div class="controls bullet"><span class="by">xbmcuser</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477658">parent</a><span>|</span><a href="#42474856">next</a><span>|</span><label class="collapse" for="c-42477748">[-]</label><label class="expand" for="c-42477748">[1 more]</label></div><br/><div class="children"><div class="content">It was not the reality of the power industry but will be soon as we have not had a source of electricity that is the cheapest and is getting cheaper and easy to install this is something unique.<p>I don&#x27;t see Google, Amazon, Microsoft or any company pay $10 for something if building it themselves will cost them $5. Either the price difference will reach a point where investing into power production themselves makes sense or the power companies decrease prices. Looking at how all 3 have already been investing in power production over the last decade themselves either to get better prices or for PR.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42474856" class="c"><input type="checkbox" id="c-42474856" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474120">parent</a><span>|</span><a href="#42476627">prev</a><span>|</span><a href="#42478019">next</a><span>|</span><label class="collapse" for="c-42474856">[-]</label><label class="expand" for="c-42474856">[13 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t follow how 10 random humans can beat the average STEM college grad and average humans in that tweet. I suspect it&#x27;s really &quot;a panel of 10 randomly chosen experts in the space&quot; or something?<p>I agree the most interesting thing to watch will be cost for a given score more than maximum possible score achieved (not that the latter won&#x27;t be interesting by any means).</div><br/><div id="42475267" class="c"><input type="checkbox" id="c-42475267" checked=""/><div class="controls bullet"><span class="by">bcrosby95</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474856">parent</a><span>|</span><a href="#42477162">next</a><span>|</span><label class="collapse" for="c-42475267">[-]</label><label class="expand" for="c-42475267">[7 more]</label></div><br/><div class="children"><div class="content">Two heads is better than 1.  10 is way better.  Even if they aren&#x27;t a field of experts.  You&#x27;re bound to get random people that remember random stuff from high school, college, work, and life in general, allowing them to piece together a solution.</div><br/><div id="42475408" class="c"><input type="checkbox" id="c-42475408" checked=""/><div class="controls bullet"><span class="by">inerte</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42475267">parent</a><span>|</span><a href="#42476581">next</a><span>|</span><label class="collapse" for="c-42475408">[-]</label><label class="expand" for="c-42475408">[3 more]</label></div><br/><div class="children"><div class="content">Aaaah thanks for the explanation. PANEL of 10 humans, as in, they were all together. I parsed the phrase as &quot;10 random people&quot; &gt; &quot;average human&quot; which made little sense.</div><br/><div id="42475522" class="c"><input type="checkbox" id="c-42475522" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42475408">parent</a><span>|</span><a href="#42476581">next</a><span>|</span><label class="collapse" for="c-42475522">[-]</label><label class="expand" for="c-42475522">[2 more]</label></div><br/><div class="children"><div class="content">Actually I believe that he did mean 10 random people tested individually, not a committee of 10 people. The key being that the question is considered to be answered correctly if any one of the 10 people got it right. This is similar to how LLMs are evaluated with pass@5 or pass@10 criteria (because the LLM has no memory so running it 10 times is more like asking 10 random people than asking the same person 10 times in a row).<p>I would expect 10 random people to do better than a committee of 10 people because 10 people have 10 chances to get it right while a committee only has one. Even if the committee gets 10 guesses (which must be made simultaneously, not iteratively) it might not do better because people might go along with a wrong consensus rather than push for the answer they would have chosen independently.</div><br/><div id="42476168" class="c"><input type="checkbox" id="c-42476168" checked=""/><div class="controls bullet"><span class="by">elcomet</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42475522">parent</a><span>|</span><a href="#42476581">next</a><span>|</span><label class="collapse" for="c-42476168">[-]</label><label class="expand" for="c-42476168">[1 more]</label></div><br/><div class="children"><div class="content">He means 10 humans voting for the answer</div><br/></div></div></div></div></div></div><div id="42476581" class="c"><input type="checkbox" id="c-42476581" checked=""/><div class="controls bullet"><span class="by">generic92034</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42475267">parent</a><span>|</span><a href="#42475408">prev</a><span>|</span><a href="#42476410">next</a><span>|</span><label class="collapse" for="c-42476581">[-]</label><label class="expand" for="c-42476581">[1 more]</label></div><br/><div class="children"><div class="content">If that works that way at all depends on the group dynamic. It is easily possible that a not so bright individual takes an (unofficial) leadership position in the group and overrides the input of smarter members. Think of any meetings with various hierarchy levels in a company.</div><br/></div></div><div id="42476410" class="c"><input type="checkbox" id="c-42476410" checked=""/><div class="controls bullet"><span class="by">herval</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42475267">parent</a><span>|</span><a href="#42476581">prev</a><span>|</span><a href="#42477162">next</a><span>|</span><label class="collapse" for="c-42476410">[-]</label><label class="expand" for="c-42476410">[2 more]</label></div><br/><div class="children"><div class="content">Depends on the task, no?<p>Do you have a sense of what kind of task this benchmark includes? Are they more “general” such that random people would fare well or more specialized (ie something a STEM grad studied and isn’t common knowledge)?</div><br/><div id="42476466" class="c"><input type="checkbox" id="c-42476466" checked=""/><div class="controls bullet"><span class="by">judge2020</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476410">parent</a><span>|</span><a href="#42477162">next</a><span>|</span><label class="collapse" for="c-42476466">[-]</label><label class="expand" for="c-42476466">[1 more]</label></div><br/><div class="children"><div class="content">It does, which is why I don’t really subscribe to any test like this being great for actually determining “AGI”. A true AGI would be able to continuously train and create new LLMs that enable it to become a SME in entirely new areas.</div><br/></div></div></div></div></div></div><div id="42477162" class="c"><input type="checkbox" id="c-42477162" checked=""/><div class="controls bullet"><span class="by">dlkf</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474856">parent</a><span>|</span><a href="#42475267">prev</a><span>|</span><a href="#42474971">next</a><span>|</span><label class="collapse" for="c-42477162">[-]</label><label class="expand" for="c-42477162">[1 more]</label></div><br/><div class="children"><div class="content">If you take a vote of 10 random people, then as long as their errors are not perfectly correlated, you’ll do better than asking one person.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Ensemble_learning" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Ensemble_learning</a></div><br/></div></div><div id="42474971" class="c"><input type="checkbox" id="c-42474971" checked=""/><div class="controls bullet"><span class="by">hmottestad</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474856">parent</a><span>|</span><a href="#42477162">prev</a><span>|</span><a href="#42476671">next</a><span>|</span><label class="collapse" for="c-42474971">[-]</label><label class="expand" for="c-42474971">[2 more]</label></div><br/><div class="children"><div class="content">Might be that within a group of 10 people, randomly chosen, when each person attempts to solve the tasks at least 99% of the time 1 person out of the 10 people will get it right.</div><br/></div></div><div id="42476671" class="c"><input type="checkbox" id="c-42476671" checked=""/><div class="controls bullet"><span class="by">HDThoreaun</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474856">parent</a><span>|</span><a href="#42474971">prev</a><span>|</span><a href="#42476477">next</a><span>|</span><label class="collapse" for="c-42476671">[-]</label><label class="expand" for="c-42476671">[1 more]</label></div><br/><div class="children"><div class="content">ARC-AGI is essentially an IQ test. There is no &quot;expert in the space&quot;. Its just a question of if youre able to spot the pattern.</div><br/></div></div><div id="42476477" class="c"><input type="checkbox" id="c-42476477" checked=""/><div class="controls bullet"><span class="by">shkkmo</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474856">parent</a><span>|</span><a href="#42476671">prev</a><span>|</span><a href="#42478019">next</a><span>|</span><label class="collapse" for="c-42476477">[-]</label><label class="expand" for="c-42476477">[1 more]</label></div><br/><div class="children"><div class="content">It is fairly well documented that groups of people can show cognitive abilities that exceed that of any individual member. The classic example of this is if you ask a group of people to estimate the number of jellybeans in a jar, you can get a more accurate result than if you test to find the person with the highest accuracy and use their guess.<p>This isn&#x27;t to say groups always outperform their members on all tasks, just that it isn&#x27;t unusual to see a result like that.</div><br/></div></div></div></div><div id="42478019" class="c"><input type="checkbox" id="c-42478019" checked=""/><div class="controls bullet"><span class="by">acchow</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474120">parent</a><span>|</span><a href="#42474856">prev</a><span>|</span><a href="#42475397">next</a><span>|</span><label class="collapse" for="c-42478019">[-]</label><label class="expand" for="c-42478019">[1 more]</label></div><br/><div class="children"><div class="content">&gt; ~doubling every 2-2.5 years) puts us at 20~25 years.<p>The trend for power consumption of compute (Megaflops per watt) has generally tracked with Koomey’s law for a doubling every 1.57 years<p>Then you also have model performance improving with compression. For example, Llama 3.1’s 8B outperforming the original Llama 65B</div><br/></div></div><div id="42475397" class="c"><input type="checkbox" id="c-42475397" checked=""/><div class="controls bullet"><span class="by">iandanforth</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474120">parent</a><span>|</span><a href="#42478019">prev</a><span>|</span><a href="#42474901">next</a><span>|</span><label class="collapse" for="c-42475397">[-]</label><label class="expand" for="c-42475397">[4 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s say that Google is already 1 generation ahead of nvidia in terms of efficient AI compute. ($1700)<p>Then let&#x27;s say that OpenAI brute forced this without any meta-optimization of the hypothesized search component (they just set a compute budget). This is probably low hanging fruit and another 2x in compute reduction. ($850)<p>Then let&#x27;s say that OpenAI was pushing really really hard for the numbers and was willing to burn cash and so didn&#x27;t bother with serious thought around hardware aware distributed inference. This could be <i>more</i> than a 2x decrease in cost like we&#x27;ve seen deliver 10x reductions in cost via better attention mechanisms, but let&#x27;s go with 2x for now. ($425).<p>So I think we&#x27;ve got about an 8x reduction in cost sitting there once Google steps up. This is probably 4-6 months of work flat out if they haven&#x27;t already started down this path, but with what they&#x27;ve got with deep research, maybe it&#x27;s sooner?<p>Then if &quot;all&quot; we get is hardware improvements we&#x27;re down to what 10-14 years?</div><br/><div id="42477618" class="c"><input type="checkbox" id="c-42477618" checked=""/><div class="controls bullet"><span class="by">qingcharles</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42475397">parent</a><span>|</span><a href="#42476445">next</a><span>|</span><label class="collapse" for="c-42477618">[-]</label><label class="expand" for="c-42477618">[1 more]</label></div><br/><div class="children"><div class="content">Until 2022 most AI research was aimed at improving the <i>quality</i> of the output, not the <i>quantity</i>.<p>Since then there has been a tsunami of optimizations in the way training and inference is done. I don&#x27;t think we&#x27;ve even begun to find all the ways that inference can be further optimized at both hardware and software levels.<p>Look at the huge models that you can happily run on an M3 Mac. The cost reduction in inference is going to vastly outpace Moore&#x27;s law, even as chip design continues on its own path.</div><br/></div></div><div id="42476445" class="c"><input type="checkbox" id="c-42476445" checked=""/><div class="controls bullet"><span class="by">promptdaddy</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42475397">parent</a><span>|</span><a href="#42477618">prev</a><span>|</span><a href="#42474901">next</a><span>|</span><label class="collapse" for="c-42476445">[-]</label><label class="expand" for="c-42476445">[2 more]</label></div><br/><div class="children"><div class="content">*deep mind research ?</div><br/><div id="42476662" class="c"><input type="checkbox" id="c-42476662" checked=""/><div class="controls bullet"><span class="by">iandanforth</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476445">parent</a><span>|</span><a href="#42474901">next</a><span>|</span><label class="collapse" for="c-42476662">[-]</label><label class="expand" for="c-42476662">[1 more]</label></div><br/><div class="children"><div class="content">Nope, Gemini Advanced with Deep Research. New mode of operation that does more &quot;thinking&quot; and web searches to answer your question.</div><br/></div></div></div></div></div></div><div id="42474901" class="c"><input type="checkbox" id="c-42474901" checked=""/><div class="controls bullet"><span class="by">cchance</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474120">parent</a><span>|</span><a href="#42475397">prev</a><span>|</span><a href="#42477329">next</a><span>|</span><label class="collapse" for="c-42474901">[-]</label><label class="expand" for="c-42474901">[4 more]</label></div><br/><div class="children"><div class="content">I mean considering the big breaththrough this year for o1&#x2F;o3 seems to have been &quot;models having internal thoughts might help reasoning&quot;, seems to everyone outside of the AI field was sort of a &quot;duh&quot; moment.<p>I&#x27;d hope we see more internal optimizations and improvements to the models. The idea that the big breakthrough being &quot;don&#x27;t spit out the first thought that pops into your head&quot; seems obvious to everyone outside of the field, but guess what turns out it was a big improvement when the devs decided to add it.</div><br/><div id="42476202" class="c"><input type="checkbox" id="c-42476202" checked=""/><div class="controls bullet"><span class="by">versteegen</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474901">parent</a><span>|</span><a href="#42476218">next</a><span>|</span><label class="collapse" for="c-42476202">[-]</label><label class="expand" for="c-42476202">[2 more]</label></div><br/><div class="children"><div class="content">&gt; seems obvious to everyone outside of the field<p>It&#x27;s obvious to people inside the field too.<p>Honestly, these things seem to be less obvious to people outside the field. I&#x27;ve heard so many uninformed takes about LLMs not representing real progress towards intelligence (even here on HN of all places; I don&#x27;t know why I torture myself reading them), that they&#x27;re just dumb memorizers. No, they are an incredible breakthrough, because extending them with things like internal thoughts will so obviously lead to results such as o3, and far beyond. Maybe a few more people will start to understand the trajectory we&#x27;re on.</div><br/><div id="42476656" class="c"><input type="checkbox" id="c-42476656" checked=""/><div class="controls bullet"><span class="by">Agentus</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476202">parent</a><span>|</span><a href="#42476218">next</a><span>|</span><label class="collapse" for="c-42476656">[-]</label><label class="expand" for="c-42476656">[1 more]</label></div><br/><div class="children"><div class="content">a trickle of people sure, but most people never accidentally stumble upon good evaluation skills let alone reason themselves to that level, so i dont see how most people will have the semblance of an idea of a realistic trajectory of ai progress.  i think most people have very little conceptualization of their own thinking&#x2F;cognitive patterns, at least not enough to sensibly extrapolate it onto ai.<p>doesnt help that most people are just mimics when talking about stuff thats outside their expertise.<p>Hell, my cousin a quality-college educated individual, high social&#x2F; emotional iq, will go down the conspiracy theory rabbit hole so quickly based on some baseless crap printed on the internet.  then he’ll talk about people being satan worshipers.</div><br/></div></div></div></div><div id="42476218" class="c"><input type="checkbox" id="c-42476218" checked=""/><div class="controls bullet"><span class="by">dogma1138</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474901">parent</a><span>|</span><a href="#42476202">prev</a><span>|</span><a href="#42477329">next</a><span>|</span><label class="collapse" for="c-42476218">[-]</label><label class="expand" for="c-42476218">[1 more]</label></div><br/><div class="children"><div class="content">Reflection isn’t a new concept, but a) actually proving that it’s an effective tool for these types of models and b) finding an effective method for reflection that doesn’t just locks you into circular “thinking” were the hard parts and hence the “breakthrough”.<p>It’s very easy to say hey ofc it’s obvious but there is nothing obvious about it because you are anthropomorphizing these models and then using that bias after the fact as a proof of your conjecture.<p>This isn’t how real progress is achieved.</div><br/></div></div></div></div><div id="42477329" class="c"><input type="checkbox" id="c-42477329" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474120">parent</a><span>|</span><a href="#42474901">prev</a><span>|</span><a href="#42475568">next</a><span>|</span><label class="collapse" for="c-42477329">[-]</label><label class="expand" for="c-42477329">[1 more]</label></div><br/><div class="children"><div class="content">Don’t forget humans which is real GI paired with increasing capable AI can create a feed back loop to accelerate new advances.</div><br/></div></div><div id="42475568" class="c"><input type="checkbox" id="c-42475568" checked=""/><div class="controls bullet"><span class="by">bjornsing</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474120">parent</a><span>|</span><a href="#42477329">prev</a><span>|</span><a href="#42477966">next</a><span>|</span><label class="collapse" for="c-42475568">[-]</label><label class="expand" for="c-42475568">[4 more]</label></div><br/><div class="children"><div class="content">&gt; are we stuck waiting for the 20-25 years for GPU improvements<p>If this turns out to be hard to optimize &#x2F; improve then there will be a <i>huge</i> economic incentive for efficient ASICs. No freaking way we’ll be running on GPUs for 20-25 years, or even 2.</div><br/><div id="42476277" class="c"><input type="checkbox" id="c-42476277" checked=""/><div class="controls bullet"><span class="by">coolspot</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42475568">parent</a><span>|</span><a href="#42477966">next</a><span>|</span><label class="collapse" for="c-42476277">[-]</label><label class="expand" for="c-42476277">[3 more]</label></div><br/><div class="children"><div class="content">LLMs need efficient matrix multiiliers. GPUs are specialized ASICs for massive matrix multiplication.</div><br/><div id="42476512" class="c"><input type="checkbox" id="c-42476512" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476277">parent</a><span>|</span><a href="#42477966">next</a><span>|</span><label class="collapse" for="c-42476512">[-]</label><label class="expand" for="c-42476512">[2 more]</label></div><br/><div class="children"><div class="content">LLMs get to maybe ~20% of the rated max FLOPS for a GPU. It’s not hard to imagine that a purpose built ASIC with maybe adjusted software stack gets us significantly more real performance.</div><br/><div id="42477351" class="c"><input type="checkbox" id="c-42477351" checked=""/><div class="controls bullet"><span class="by">boroboro4</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476512">parent</a><span>|</span><a href="#42477966">next</a><span>|</span><label class="collapse" for="c-42477351">[-]</label><label class="expand" for="c-42477351">[1 more]</label></div><br/><div class="children"><div class="content">They get more than this. For prefill we can get 70% matmul utilization, for generation less than this but we’ll get to &gt;50 too eventually.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42477966" class="c"><input type="checkbox" id="c-42477966" checked=""/><div class="controls bullet"><span class="by">daxfohl</span><span>|</span><a href="#42473876">parent</a><span>|</span><a href="#42474120">prev</a><span>|</span><a href="#42476203">next</a><span>|</span><label class="collapse" for="c-42477966">[-]</label><label class="expand" for="c-42477966">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if we&#x27;ll start seeing a shift in compute spend, moving away from training time,  and toward inference time instead. As we get closer to AGI, we probably reach some limit in terms of how smart the thing can get just training on existing docs or data or whatever. At some point it knows everything it&#x27;ll ever know, no matter how much training compute you throw at it.<p>To move beyond that, the thing has to start thinking for itself, some auto feedback loop, training itself on its own thoughts. Interestingly, this could plausibly be vastly more efficient than training on external data because it&#x27;s a much tighter feedback loop and a smaller dataset. So it&#x27;s possible that &quot;nearly AGI&quot; leads to ASI pretty quickly and efficiently.<p>Of course it&#x27;s also possible that the feedback loop, while efficient as a computation process, isn&#x27;t efficient as a learning &#x2F; reasoning &#x2F; learning-how-to-reason process, and the thing, while as intelligent as a human, still barely competes with a worm in true reasoning ability.<p>Interesting times.</div><br/></div></div><div id="42476203" class="c"><input type="checkbox" id="c-42476203" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#42473876">parent</a><span>|</span><a href="#42477966">prev</a><span>|</span><a href="#42477943">next</a><span>|</span><label class="collapse" for="c-42476203">[-]</label><label class="expand" for="c-42476203">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Super exciting that OpenAI pushed the compute out this far<p>it&#x27;s even more exciting than that. the fact that you even <i>can</i> use more compute to get more intelligence is a breakthrough. if they spent even more on inference, would they get even better scores on arc agi?</div><br/><div id="42476520" class="c"><input type="checkbox" id="c-42476520" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476203">parent</a><span>|</span><a href="#42477943">next</a><span>|</span><label class="collapse" for="c-42476520">[-]</label><label class="expand" for="c-42476520">[1 more]</label></div><br/><div class="children"><div class="content">Maybe it&#x27;s not linear spend.</div><br/></div></div></div></div><div id="42477943" class="c"><input type="checkbox" id="c-42477943" checked=""/><div class="controls bullet"><span class="by">madduci</span><span>|</span><a href="#42473876">parent</a><span>|</span><a href="#42476203">prev</a><span>|</span><a href="#42476733">next</a><span>|</span><label class="collapse" for="c-42477943">[-]</label><label class="expand" for="c-42477943">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s see when this will be released to the free tier. Looks promising, although I hope they will also be able to publish more details on this, as part of the &quot;open&quot; in their name</div><br/></div></div><div id="42476733" class="c"><input type="checkbox" id="c-42476733" checked=""/><div class="controls bullet"><span class="by">freehorse</span><span>|</span><a href="#42473876">parent</a><span>|</span><a href="#42477943">prev</a><span>|</span><a href="#42473935">next</a><span>|</span><label class="collapse" for="c-42476733">[-]</label><label class="expand" for="c-42476733">[4 more]</label></div><br/><div class="children"><div class="content">&gt; I am interpreting this result as human level reasoning now costs (approximately) 41k&#x2F;hr to 2.5M&#x2F;hr with current compute.<p>On a very simple, toy task, which arc-agi basically is. Arc-agi tests are not hard per se, just LLM’s find them hard. 
We do not know how this scales for more complex, real world tasks.</div><br/><div id="42476921" class="c"><input type="checkbox" id="c-42476921" checked=""/><div class="controls bullet"><span class="by">SamPatt</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476733">parent</a><span>|</span><a href="#42473935">next</a><span>|</span><label class="collapse" for="c-42476921">[-]</label><label class="expand" for="c-42476921">[3 more]</label></div><br/><div class="children"><div class="content">Right. Arc is meant to test the ability of a model to generalize. It&#x27;s neat to see it succeed, but it&#x27;s not yet a guarantee that it can generalize when given other tasks.<p>The other benchmarks are a good indication though.</div><br/><div id="42477800" class="c"><input type="checkbox" id="c-42477800" checked=""/><div class="controls bullet"><span class="by">criddell</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476921">parent</a><span>|</span><a href="#42473935">next</a><span>|</span><label class="collapse" for="c-42477800">[-]</label><label class="expand" for="c-42477800">[2 more]</label></div><br/><div class="children"><div class="content">Does it mean anything for more general tasks like driving a car?</div><br/><div id="42477930" class="c"><input type="checkbox" id="c-42477930" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42477800">parent</a><span>|</span><a href="#42473935">next</a><span>|</span><label class="collapse" for="c-42477930">[-]</label><label class="expand" for="c-42477930">[1 more]</label></div><br/><div class="children"><div class="content">Is every smart person a good driver?</div><br/></div></div></div></div></div></div></div></div><div id="42473935" class="c"><input type="checkbox" id="c-42473935" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#42473876">parent</a><span>|</span><a href="#42476733">prev</a><span>|</span><a href="#42474030">next</a><span>|</span><label class="collapse" for="c-42473935">[-]</label><label class="expand" for="c-42473935">[16 more]</label></div><br/><div class="children"><div class="content">&gt; ~=$3400 per single task<p>report says it is $17 per task, and $6k for whole dataset of 400 tasks.</div><br/><div id="42474152" class="c"><input type="checkbox" id="c-42474152" checked=""/><div class="controls bullet"><span class="by">binarymax</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42473935">parent</a><span>|</span><a href="#42474010">next</a><span>|</span><label class="collapse" for="c-42474152">[-]</label><label class="expand" for="c-42474152">[2 more]</label></div><br/><div class="children"><div class="content"><i>&quot;Note: OpenAI has requested that we not publish the high-compute costs. The amount of compute was roughly 172x the low-compute configuration.&quot;</i><p>The low compute was $17 per task.  Speculate 172*$17 for the high compute is $2,924 per task, so I am also confused on the $3400 number.</div><br/><div id="42474167" class="c"><input type="checkbox" id="c-42474167" checked=""/><div class="controls bullet"><span class="by">bluecoconut</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474152">parent</a><span>|</span><a href="#42474010">next</a><span>|</span><label class="collapse" for="c-42474167">[-]</label><label class="expand" for="c-42474167">[1 more]</label></div><br/><div class="children"><div class="content">3400 came from counting pixels on the plot.<p>Also its $20 on for the o3-low via the table for the semi-private, which x172 is 3440, also coming in close to the 3400 number</div><br/></div></div></div></div><div id="42474010" class="c"><input type="checkbox" id="c-42474010" checked=""/><div class="controls bullet"><span class="by">bluecoconut</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42473935">parent</a><span>|</span><a href="#42474152">prev</a><span>|</span><a href="#42474188">next</a><span>|</span><label class="collapse" for="c-42474010">[-]</label><label class="expand" for="c-42474010">[5 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the low-compute mode. In the plot at the top where they score 88%, O3 High (tuned) is ~3.4k</div><br/><div id="42476704" class="c"><input type="checkbox" id="c-42476704" checked=""/><div class="controls bullet"><span class="by">HDThoreaun</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474010">parent</a><span>|</span><a href="#42475224">next</a><span>|</span><label class="collapse" for="c-42476704">[-]</label><label class="expand" for="c-42476704">[1 more]</label></div><br/><div class="children"><div class="content">The low compute one did as well as the average person though</div><br/></div></div><div id="42475224" class="c"><input type="checkbox" id="c-42475224" checked=""/><div class="controls bullet"><span class="by">ionwake</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474010">parent</a><span>|</span><a href="#42476704">prev</a><span>|</span><a href="#42474188">next</a><span>|</span><label class="collapse" for="c-42475224">[-]</label><label class="expand" for="c-42475224">[3 more]</label></div><br/><div class="children"><div class="content">sorry to be a noob, but can someone tell me doe sths mena o3 will be unaffordable for a typical user? Will only companies with thousands to spend per query be able to use this?<p>Sorry for being thick Im just confused how they can turn this into an addordable service?</div><br/><div id="42476015" class="c"><input type="checkbox" id="c-42476015" checked=""/><div class="controls bullet"><span class="by">JohnnyMarcone</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42475224">parent</a><span>|</span><a href="#42474188">next</a><span>|</span><label class="collapse" for="c-42476015">[-]</label><label class="expand" for="c-42476015">[2 more]</label></div><br/><div class="children"><div class="content">There are likely many efficiency gains that will be made before it&#x27;s released, and after. Also they showed o3 mini to be better than o1 for less cost in multiple benchmarks, so there&#x27;re already improvements there at a lower cost than what available.</div><br/><div id="42476194" class="c"><input type="checkbox" id="c-42476194" checked=""/><div class="controls bullet"><span class="by">ionwake</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42476015">parent</a><span>|</span><a href="#42474188">next</a><span>|</span><label class="collapse" for="c-42476194">[-]</label><label class="expand" for="c-42476194">[1 more]</label></div><br/><div class="children"><div class="content">Great thank you</div><br/></div></div></div></div></div></div></div></div><div id="42474188" class="c"><input type="checkbox" id="c-42474188" checked=""/><div class="controls bullet"><span class="by">xrendan</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42473935">parent</a><span>|</span><a href="#42474010">prev</a><span>|</span><a href="#42474020">next</a><span>|</span><label class="collapse" for="c-42474188">[-]</label><label class="expand" for="c-42474188">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re misreading it, there&#x27;s two different runs, a low and a high compute run.<p>The number for the high-compute one is ~172x the first one according to the article so ~=$2900</div><br/></div></div><div id="42474020" class="c"><input type="checkbox" id="c-42474020" checked=""/><div class="controls bullet"><span class="by">jhrmnn</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42473935">parent</a><span>|</span><a href="#42474188">prev</a><span>|</span><a href="#42474079">next</a><span>|</span><label class="collapse" for="c-42474020">[-]</label><label class="expand" for="c-42474020">[6 more]</label></div><br/><div class="children"><div class="content">That’s for the low-compute configuration that doesn’t reach human-level performance (not far though)</div><br/><div id="42474028" class="c"><input type="checkbox" id="c-42474028" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474020">parent</a><span>|</span><a href="#42474079">next</a><span>|</span><label class="collapse" for="c-42474028">[-]</label><label class="expand" for="c-42474028">[5 more]</label></div><br/><div class="children"><div class="content">I referred on high compute mode. They have table with breakdown here: <a href="https:&#x2F;&#x2F;arcprize.org&#x2F;blog&#x2F;oai-o3-pub-breakthrough" rel="nofollow">https:&#x2F;&#x2F;arcprize.org&#x2F;blog&#x2F;oai-o3-pub-breakthrough</a></div><br/><div id="42474129" class="c"><input type="checkbox" id="c-42474129" checked=""/><div class="controls bullet"><span class="by">junipertea</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474028">parent</a><span>|</span><a href="#42474137">next</a><span>|</span><label class="collapse" for="c-42474129">[-]</label><label class="expand" for="c-42474129">[1 more]</label></div><br/><div class="children"><div class="content">The table row with 6k figure refers to high efficiency, not high compute mode. From the blog post:<p>Note: OpenAI has requested that we not publish the high-compute costs. The amount of compute was roughly 172x the low-compute configuration.</div><br/></div></div><div id="42474137" class="c"><input type="checkbox" id="c-42474137" checked=""/><div class="controls bullet"><span class="by">gbnwl</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474028">parent</a><span>|</span><a href="#42474129">prev</a><span>|</span><a href="#42474141">next</a><span>|</span><label class="collapse" for="c-42474137">[-]</label><label class="expand" for="c-42474137">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s &quot;efficiency&quot; high, which actually means less compute. The 87.5% score using low efficiency (more compute) doesn&#x27;t have cost listed.</div><br/></div></div><div id="42474141" class="c"><input type="checkbox" id="c-42474141" checked=""/><div class="controls bullet"><span class="by">bluecoconut</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474028">parent</a><span>|</span><a href="#42474137">prev</a><span>|</span><a href="#42474105">next</a><span>|</span><label class="collapse" for="c-42474141">[-]</label><label class="expand" for="c-42474141">[1 more]</label></div><br/><div class="children"><div class="content">they use some poor language.<p>&quot;High Efficiency&quot; is O3 Low
&quot;Low Efficiency&quot; is O3 High<p>They left the &quot;Low efficiency&quot; (O3 High) values as `-` but you can infer them from the plot at the top.<p>Note the $20 and $17 per task aligns with the X-axis of the O3-low</div><br/></div></div><div id="42474105" class="c"><input type="checkbox" id="c-42474105" checked=""/><div class="controls bullet"><span class="by">EVa5I7bHFq9mnYK</span><span>|</span><a href="#42473876">root</a><span>|</span><a href="#42474028">parent</a><span>|</span><a href="#42474141">prev</a><span>|</span><a href="#42474079">next</a><span>|</span><label class="collapse" for="c-42474105">[-]</label><label class="expand" for="c-42474105">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s high EFFICIENCY. High efficiency = low compute.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42478098" class="c"><input type="checkbox" id="c-42478098" checked=""/><div class="controls bullet"><span class="by">miga89</span><span>|</span><a href="#42473876">prev</a><span>|</span><a href="#42473669">next</a><span>|</span><label class="collapse" for="c-42478098">[-]</label><label class="expand" for="c-42478098">[4 more]</label></div><br/><div class="children"><div class="content">How do the organisers keep the private test set private? Does openAI hand them the model for testing?<p>If they use a model API, then surely OpenAI has access to the private test set questions and can include it in the next round of training?<p>(I am sure I am missing something.)</div><br/><div id="42478121" class="c"><input type="checkbox" id="c-42478121" checked=""/><div class="controls bullet"><span class="by">7734128</span><span>|</span><a href="#42478098">parent</a><span>|</span><a href="#42478143">next</a><span>|</span><label class="collapse" for="c-42478121">[-]</label><label class="expand" for="c-42478121">[2 more]</label></div><br/><div class="children"><div class="content">I suppose that&#x27;s why they are calling it &quot;semi-private&quot;.</div><br/><div id="42478141" class="c"><input type="checkbox" id="c-42478141" checked=""/><div class="controls bullet"><span class="by">freehorse</span><span>|</span><a href="#42478098">root</a><span>|</span><a href="#42478121">parent</a><span>|</span><a href="#42478143">next</a><span>|</span><label class="collapse" for="c-42478141">[-]</label><label class="expand" for="c-42478141">[1 more]</label></div><br/><div class="children"><div class="content">And why o3 or any OpenAI llm is not evaluated in the actual private dataset.</div><br/></div></div></div></div><div id="42478143" class="c"><input type="checkbox" id="c-42478143" checked=""/><div class="controls bullet"><span class="by">owenpalmer</span><span>|</span><a href="#42478098">parent</a><span>|</span><a href="#42478121">prev</a><span>|</span><a href="#42473669">next</a><span>|</span><label class="collapse" for="c-42478143">[-]</label><label class="expand" for="c-42478143">[1 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t be surprised if the term &quot;benchmark fraud&quot; will soon been coined.</div><br/></div></div></div></div><div id="42473669" class="c"><input type="checkbox" id="c-42473669" checked=""/><div class="controls bullet"><span class="by">croemer</span><span>|</span><a href="#42478098">prev</a><span>|</span><a href="#42473497">next</a><span>|</span><label class="collapse" for="c-42473669">[-]</label><label class="expand" for="c-42473669">[15 more]</label></div><br/><div class="children"><div class="content">The programming task they gave o3-mini high (creating Python server that allows chatting with OpenAI API and run some code in terminal) didn&#x27;t seem very hard? Strange choice of example for something that&#x27;s claimed to be a big step forwards.<p>YT timestamped link: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SKBG1sqdyIU&amp;t=768s" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SKBG1sqdyIU&amp;t=768s</a> (thanks for the fixed link @photonboom)<p>Updated: I gave the task to Claude 3.5 Sonnet and it worked first shot: <a href="https:&#x2F;&#x2F;claude.site&#x2F;artifacts&#x2F;36cecd49-0e0b-4a8c-befa-faa5aaa102e6" rel="nofollow">https:&#x2F;&#x2F;claude.site&#x2F;artifacts&#x2F;36cecd49-0e0b-4a8c-befa-faa5aa...</a></div><br/><div id="42473701" class="c"><input type="checkbox" id="c-42473701" checked=""/><div class="controls bullet"><span class="by">bearjaws</span><span>|</span><a href="#42473669">parent</a><span>|</span><a href="#42476501">next</a><span>|</span><label class="collapse" for="c-42473701">[-]</label><label class="expand" for="c-42473701">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s good that it works since if you ask GPT-4o to use the openai sdk it will often produce invalid and out of date code.</div><br/></div></div><div id="42476501" class="c"><input type="checkbox" id="c-42476501" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#42473669">parent</a><span>|</span><a href="#42473701">prev</a><span>|</span><a href="#42474095">next</a><span>|</span><label class="collapse" for="c-42476501">[-]</label><label class="expand" for="c-42476501">[3 more]</label></div><br/><div class="children"><div class="content">Sonnet isn&#x27;t a &quot;mini&quot; sized model. Try it with Haiku.</div><br/><div id="42476631" class="c"><input type="checkbox" id="c-42476631" checked=""/><div class="controls bullet"><span class="by">croemer</span><span>|</span><a href="#42473669">root</a><span>|</span><a href="#42476501">parent</a><span>|</span><a href="#42477710">next</a><span>|</span><label class="collapse" for="c-42476631">[-]</label><label class="expand" for="c-42476631">[1 more]</label></div><br/><div class="children"><div class="content">How mini is o3-mini compared to Sonnet and why does it matter whether it&#x27;s mini or not? Isn&#x27;t the point of the demo to show what&#x27;s now possible that wasn&#x27;t before?<p>4o is cheaper than o1 mini so mini doesn&#x27;t mean much for costs.</div><br/></div></div></div></div><div id="42474095" class="c"><input type="checkbox" id="c-42474095" checked=""/><div class="controls bullet"><span class="by">phil917</span><span>|</span><a href="#42473669">parent</a><span>|</span><a href="#42476501">prev</a><span>|</span><a href="#42473769">next</a><span>|</span><label class="collapse" for="c-42474095">[-]</label><label class="expand" for="c-42474095">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I agree that wasn&#x27;t particularly mind blowing to me and seems fairly in line with what existing SOTA models can do. Especially since they did it in steps. Maybe I&#x27;m missing something.</div><br/></div></div><div id="42473769" class="c"><input type="checkbox" id="c-42473769" checked=""/><div class="controls bullet"><span class="by">photonboom</span><span>|</span><a href="#42473669">parent</a><span>|</span><a href="#42474095">prev</a><span>|</span><a href="#42473738">next</a><span>|</span><label class="collapse" for="c-42473769">[-]</label><label class="expand" for="c-42473769">[1 more]</label></div><br/><div class="children"><div class="content">heres the right timestamp: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SKBG1sqdyIU&amp;t=768s" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SKBG1sqdyIU&amp;t=768s</a></div><br/></div></div><div id="42473738" class="c"><input type="checkbox" id="c-42473738" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#42473669">parent</a><span>|</span><a href="#42473769">prev</a><span>|</span><a href="#42475843">next</a><span>|</span><label class="collapse" for="c-42473738">[-]</label><label class="expand" for="c-42473738">[7 more]</label></div><br/><div class="children"><div class="content">I would say they didn’t need to demo anything, because if you are gonna use the output code live on a demo it may make compile errors and then look stupid trying to fix it live</div><br/><div id="42474434" class="c"><input type="checkbox" id="c-42474434" checked=""/><div class="controls bullet"><span class="by">croemer</span><span>|</span><a href="#42473669">root</a><span>|</span><a href="#42473738">parent</a><span>|</span><a href="#42475250">next</a><span>|</span><label class="collapse" for="c-42474434">[-]</label><label class="expand" for="c-42474434">[1 more]</label></div><br/><div class="children"><div class="content">If it was a safe bet problem, then they should have said that. To me it looks like they faked excitement for something not exciting which lowers credibility of the whole presentation.</div><br/></div></div><div id="42475250" class="c"><input type="checkbox" id="c-42475250" checked=""/><div class="controls bullet"><span class="by">sunaookami</span><span>|</span><a href="#42473669">root</a><span>|</span><a href="#42473738">parent</a><span>|</span><a href="#42474434">prev</a><span>|</span><a href="#42476399">next</a><span>|</span><label class="collapse" for="c-42475250">[-]</label><label class="expand" for="c-42475250">[2 more]</label></div><br/><div class="children"><div class="content">They actually did that the last time when they showed the apps integration. First try in Xcode didn&#x27;t work.</div><br/><div id="42475739" class="c"><input type="checkbox" id="c-42475739" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#42473669">root</a><span>|</span><a href="#42475250">parent</a><span>|</span><a href="#42476399">next</a><span>|</span><label class="collapse" for="c-42475739">[-]</label><label class="expand" for="c-42475739">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I think that time it was ok because they were demoing the app function, but for this they are demoing the model smarts</div><br/></div></div></div></div><div id="42476399" class="c"><input type="checkbox" id="c-42476399" checked=""/><div class="controls bullet"><span class="by">csomar</span><span>|</span><a href="#42473669">root</a><span>|</span><a href="#42473738">parent</a><span>|</span><a href="#42475250">prev</a><span>|</span><a href="#42475843">next</a><span>|</span><label class="collapse" for="c-42476399">[-]</label><label class="expand" for="c-42476399">[3 more]</label></div><br/><div class="children"><div class="content">Models are predictable at 0 temperatures. They might have tested the output beforehand.</div><br/><div id="42476500" class="c"><input type="checkbox" id="c-42476500" checked=""/><div class="controls bullet"><span class="by">fzzzy</span><span>|</span><a href="#42473669">root</a><span>|</span><a href="#42476399">parent</a><span>|</span><a href="#42475843">next</a><span>|</span><label class="collapse" for="c-42476500">[-]</label><label class="expand" for="c-42476500">[2 more]</label></div><br/><div class="children"><div class="content">Models in practice haven&#x27;t been deterministic at 0 temperature, although nobody knows exactly why. Either hardware or software bugs.</div><br/><div id="42476517" class="c"><input type="checkbox" id="c-42476517" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42473669">root</a><span>|</span><a href="#42476500">parent</a><span>|</span><a href="#42475843">next</a><span>|</span><label class="collapse" for="c-42476517">[-]</label><label class="expand" for="c-42476517">[1 more]</label></div><br/><div class="children"><div class="content">We know exactly why, it is because floating point operations aren&#x27;t associative but the GPU scheduler assumes they are, and the scheduler isn&#x27;t deterministic. Running the model strictly hurts performance so they don&#x27;t do that.</div><br/></div></div></div></div></div></div></div></div><div id="42475843" class="c"><input type="checkbox" id="c-42475843" checked=""/><div class="controls bullet"><span class="by">MyFirstSass</span><span>|</span><a href="#42473669">parent</a><span>|</span><a href="#42473738">prev</a><span>|</span><a href="#42473497">next</a><span>|</span><label class="collapse" for="c-42475843">[-]</label><label class="expand" for="c-42475843">[1 more]</label></div><br/><div class="children"><div class="content">What? Is this what this is? Either this is a complete joke or we&#x27;re missing something.<p>I&#x27;ve been doing similar stuff in Claude for months and it&#x27;s not that impressive when you see how limited they really are when going non boilerplate.</div><br/></div></div></div></div><div id="42473497" class="c"><input type="checkbox" id="c-42473497" checked=""/><div class="controls bullet"><span class="by">obblekk</span><span>|</span><a href="#42473669">prev</a><span>|</span><a href="#42473810">next</a><span>|</span><label class="collapse" for="c-42473497">[-]</label><label class="expand" for="c-42473497">[56 more]</label></div><br/><div class="children"><div class="content">Human performance is 85% [1]. o3 high gets 87.5%.<p>This means we have an algorithm to get to human level performance on this task.<p>If you think this task is an eval of general reasoning ability, we have an algorithm for that now.<p>There&#x27;s a lot of work ahead to generalize o3 performance to all domains. I think this explains why many researchers feel AGI is within reach, now that we have an algorithm that works.<p>Congrats to both Francois Chollet for developing this compelling eval, and to the researchers who saturated it!<p>[1] <a href="https:&#x2F;&#x2F;x.com&#x2F;SmokeAwayyy&#x2F;status&#x2F;1870171624403808366" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;SmokeAwayyy&#x2F;status&#x2F;1870171624403808366</a>, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2409.01374v1" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2409.01374v1</a></div><br/><div id="42473526" class="c"><input type="checkbox" id="c-42473526" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#42473497">parent</a><span>|</span><a href="#42473627">next</a><span>|</span><label class="collapse" for="c-42473526">[-]</label><label class="expand" for="c-42473526">[13 more]</label></div><br/><div class="children"><div class="content">As excited as I am by this, I still feel like this is still just a small approximation of a small chunk of human reasoning ability at large. o3 (and whatever comes next) feels to me like it will head down the path of being a reasoning coprocessor for various tasks.<p>But, still, this is incredibly impressive.</div><br/><div id="42473839" class="c"><input type="checkbox" id="c-42473839" checked=""/><div class="controls bullet"><span class="by">qt31415926</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473526">parent</a><span>|</span><a href="#42476316">next</a><span>|</span><label class="collapse" for="c-42473839">[-]</label><label class="expand" for="c-42473839">[11 more]</label></div><br/><div class="children"><div class="content">Which parts of reasoning do you think is missing? I do feel like it covers a lot of &#x27;reasoning&#x27; ground despite its on the surface simplicity</div><br/><div id="42477562" class="c"><input type="checkbox" id="c-42477562" checked=""/><div class="controls bullet"><span class="by">john_minsk</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473839">parent</a><span>|</span><a href="#42478061">next</a><span>|</span><label class="collapse" for="c-42477562">[-]</label><label class="expand" for="c-42477562">[1 more]</label></div><br/><div class="children"><div class="content">My personal 5 cents is that reasoning will be there when LLM gives you some kind of outcome and then when questioned about it can explain every bit of result it produced.<p>For example, if we asked an LLM to produce an image of a &quot;human woman photorealistic&quot; it produces result.  After that you should be able to ask it &quot;tell me about its background&quot; and it should be able to explain &quot;Since user didn&#x27;t specify background in the query I randomly decided to draw her standing in front of a fantasy background of Amsterdam iconic houses. Usually Amsterdam houses are 3 stories tall, attached to each other and 10 meters wide. Amsterdam houses usually have cranes on the top floor, which help to bring goods to the top floor since doors are too narrow for any object wider than 1m. The woman stands in front of the houses approximately 25 meters in front of them. She is 1,59m tall, which gives us correct perspective. It is 11:16am of August 22nd which I used to calculate correct position of the sun and align all shadows according to projected lighting conditions. The color of her skin is set at RGB:xxxxxx randomly&quot; etc.<p>And it is not too much to ask LLMs for it. LLMs have access to all the information above as they read all the internet. So there is definitely a description of Amsterdam architecture, what a human body looks like or how to correctly estimate time of day based on shadows (and vise versa). The only thing missing is logic that connects all this information and which is applied correctly to generate final image.<p>I like to think about LLMs as a fancy genius compressing engines. They took all the information in the internet, compressed it and are able to cleverly query this information for end user. It is a tremendously valuable thing, but if intelligence emerges out of it - not sure. Digital information doesn&#x27;t necessarily contain everything needed to understand how it was generated and why.</div><br/></div></div><div id="42475283" class="c"><input type="checkbox" id="c-42475283" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473839">parent</a><span>|</span><a href="#42478061">prev</a><span>|</span><a href="#42477958">next</a><span>|</span><label class="collapse" for="c-42475283">[-]</label><label class="expand" for="c-42475283">[3 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s hard to enumerate the unknown, but I&#x27;d personally love to see how models like this perform on things like word problems where you introduce red herrings. Right now, LLMs at large tend to struggle mightily to understand when some of the given information is not only irrelevant, but may explicitly serve to distract from the real problem.</div><br/><div id="42475537" class="c"><input type="checkbox" id="c-42475537" checked=""/><div class="controls bullet"><span class="by">KaoruAoiShiho</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42475283">parent</a><span>|</span><a href="#42477526">next</a><span>|</span><label class="collapse" for="c-42475537">[-]</label><label class="expand" for="c-42475537">[1 more]</label></div><br/><div class="children"><div class="content">o1 already fixed the red herrings...</div><br/></div></div><div id="42477526" class="c"><input type="checkbox" id="c-42477526" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42475283">parent</a><span>|</span><a href="#42475537">prev</a><span>|</span><a href="#42477958">next</a><span>|</span><label class="collapse" for="c-42477526">[-]</label><label class="expand" for="c-42477526">[1 more]</label></div><br/><div class="children"><div class="content">That’s not inability to reason though, that’s having a social context.<p>Humans also don’t tend to operate in a rigorously logical mode and understand that math word problems are an exception where the language may be adversarial: they’re trained for that special context in school. If you tell the LLM that social context, eg that language may be deceptive, their “mistakes” disappear.<p>What you’re actually measuring is the LLM defaults to assuming you misspoke trying to include relevant information rather than that you were trying to trick it — which is the social context you’d expect when trained on general chat interactions.<p>Establishing context in psychology is hard.</div><br/></div></div></div></div><div id="42477958" class="c"><input type="checkbox" id="c-42477958" checked=""/><div class="controls bullet"><span class="by">Xmd5a</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473839">parent</a><span>|</span><a href="#42475283">prev</a><span>|</span><a href="#42476995">next</a><span>|</span><label class="collapse" for="c-42477958">[-]</label><label class="expand" for="c-42477958">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are still bound to a prompting session. They can&#x27;t form long term memories, can&#x27;t ponder on it and can&#x27;t develop experience. They have no cognitive architecture.<p>&#x27;Agents&#x27; (i.e. workflows intermingling code and calls to LLMs) are still a thing (as shown by the fact there is a post by anthropic on this subject on the front page right now) and they are very hard to build.<p>Consequence of that for instance: it&#x27;s not possible to have a LLM explore <i>exhaustively</i> a topic.</div><br/></div></div><div id="42476995" class="c"><input type="checkbox" id="c-42476995" checked=""/><div class="controls bullet"><span class="by">Agentus</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473839">parent</a><span>|</span><a href="#42477958">prev</a><span>|</span><a href="#42476316">next</a><span>|</span><label class="collapse" for="c-42476995">[-]</label><label class="expand" for="c-42476995">[4 more]</label></div><br/><div class="children"><div class="content">kinda interesting, every single CS person (especially phds) when talking about reasoning are unable to concisely quantify, enumerate, qualify, or define reasoning.<p>people with (high) intelligence talking and building (artificial) intelligence but never able to convincingly explain aspects of intelligence.  just often talk ambiguously and circularly around it.<p>what are we humans getting ourselves into inventing skynet :wink.<p>its been an ongoing pet project to tackle reasoning, but i cant answer your question with regards to llms.</div><br/><div id="42477225" class="c"><input type="checkbox" id="c-42477225" checked=""/><div class="controls bullet"><span class="by">YeGoblynQueenne</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42476995">parent</a><span>|</span><a href="#42476316">next</a><span>|</span><label class="collapse" for="c-42477225">[-]</label><label class="expand" for="c-42477225">[3 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; Kinda interesting, every single CS person (especially phds) when talking about reasoning are unable to concisely quantify, enumerate, qualify, or define reasoning.<p>Kinda interesting that mathematicians also can&#x27;t do the same for mathematics.<p>And yet.</div><br/><div id="42477273" class="c"><input type="checkbox" id="c-42477273" checked=""/><div class="controls bullet"><span class="by">Agentus</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42477225">parent</a><span>|</span><a href="#42476316">next</a><span>|</span><label class="collapse" for="c-42477273">[-]</label><label class="expand" for="c-42477273">[2 more]</label></div><br/><div class="children"><div class="content">well lets just say i think i can explain reasoning better than anyone ive encountered.  i have my own hypothesized theory on what it is and how it manifests in neural networks.<p>i doubt your mathmatician example is equivalent.<p>examples that are fresh on the mind that further my point.
ive heard yann lecun baffled by llms instantiation&#x2F;emergence of reasoning, along with other ai researchers.  eric Schmidt thinks the agentic reasoning is the current frontier and people should be focusing on that.  was listening to the start of an ai machine learning interview a week ago with some cs phd asked to explain reasoning and the best he could muster up is you know it when you see it…. not to mention the guy responding to the grandparent that gave a cop out answer ( all the most respect to him).</div><br/><div id="42478040" class="c"><input type="checkbox" id="c-42478040" checked=""/><div class="controls bullet"><span class="by">necovek</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42477273">parent</a><span>|</span><a href="#42476316">next</a><span>|</span><label class="collapse" for="c-42478040">[-]</label><label class="expand" for="c-42478040">[1 more]</label></div><br/><div class="children"><div class="content">Care to enlighten us with your explanation of what &quot;reasoning&quot; is?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42476316" class="c"><input type="checkbox" id="c-42476316" checked=""/><div class="controls bullet"><span class="by">azeirah</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473526">parent</a><span>|</span><a href="#42473839">prev</a><span>|</span><a href="#42473627">next</a><span>|</span><label class="collapse" for="c-42476316">[-]</label><label class="expand" for="c-42476316">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d like to see this o3 thing play 5d chess with multiverse time travel or baba is you.<p>The only effect smarter models will have is that intelligent people will have to use less of their brain to do their work. As has always been the case, the medium  is the message, and climate change is one of the most difficult and worst problems of our time.<p>If this gets software people to quit en-masse and start working in energy, biology, ecology and preservation? Then it has succeeded.</div><br/></div></div></div></div><div id="42473627" class="c"><input type="checkbox" id="c-42473627" checked=""/><div class="controls bullet"><span class="by">cryptoegorophy</span><span>|</span><a href="#42473497">parent</a><span>|</span><a href="#42473526">prev</a><span>|</span><a href="#42477740">next</a><span>|</span><label class="collapse" for="c-42473627">[-]</label><label class="expand" for="c-42473627">[6 more]</label></div><br/><div class="children"><div class="content">What’s interesting is it might be very close to human intelligence than some “alien” intelligence, because after all it is a LLM and trained on human made text, which kind of represents human intelligence.</div><br/><div id="42473684" class="c"><input type="checkbox" id="c-42473684" checked=""/><div class="controls bullet"><span class="by">hammock</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473627">parent</a><span>|</span><a href="#42477067">next</a><span>|</span><label class="collapse" for="c-42473684">[-]</label><label class="expand" for="c-42473684">[4 more]</label></div><br/><div class="children"><div class="content">In that vein, perhaps the delta between o3 @ 87.5% and Human @ 85% represents a deficit in the ability of text to communicate human reasoning.<p>In other words, it&#x27;s possible humans can reason better than o3, but cannot articulate that reasoning as well through text - only in our heads, or through some alternative medium.</div><br/><div id="42475309" class="c"><input type="checkbox" id="c-42475309" checked=""/><div class="controls bullet"><span class="by">unsupp0rted</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473684">parent</a><span>|</span><a href="#42473740">next</a><span>|</span><label class="collapse" for="c-42475309">[-]</label><label class="expand" for="c-42475309">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s possible humans reason better through text than not through text, so these models, having been trained on text, should be able to out-reason any person who&#x27;s not currently sitting down to write.</div><br/></div></div><div id="42473740" class="c"><input type="checkbox" id="c-42473740" checked=""/><div class="controls bullet"><span class="by">85392_school</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473684">parent</a><span>|</span><a href="#42475309">prev</a><span>|</span><a href="#42477067">next</a><span>|</span><label class="collapse" for="c-42473740">[-]</label><label class="expand" for="c-42473740">[2 more]</label></div><br/><div class="children"><div class="content">I wonder how much of an effect amount of time to answer has on human performance.</div><br/><div id="42474131" class="c"><input type="checkbox" id="c-42474131" checked=""/><div class="controls bullet"><span class="by">yunwal</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473740">parent</a><span>|</span><a href="#42477067">next</a><span>|</span><label class="collapse" for="c-42474131">[-]</label><label class="expand" for="c-42474131">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, this is sort of meaningless without some idea of cost or consequences of a wrong answer. One of the nice things about working with a competent human is being able to tell them &quot;all of our jobs are on the line&quot; and knowing with certainty that they&#x27;ll come to a good answer.</div><br/></div></div></div></div></div></div><div id="42477067" class="c"><input type="checkbox" id="c-42477067" checked=""/><div class="controls bullet"><span class="by">hamburga</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473627">parent</a><span>|</span><a href="#42473684">prev</a><span>|</span><a href="#42477740">next</a><span>|</span><label class="collapse" for="c-42477067">[-]</label><label class="expand" for="c-42477067">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. I think what really makes them alien is everything else about them besides intelligence. Namely, no emotional&#x2F;physiological grounding in empathy, shame, pride, and love (on the positive side) or hatred (negative side).</div><br/></div></div></div></div><div id="42477740" class="c"><input type="checkbox" id="c-42477740" checked=""/><div class="controls bullet"><span class="by">lastdong</span><span>|</span><a href="#42473497">parent</a><span>|</span><a href="#42473627">prev</a><span>|</span><a href="#42473783">next</a><span>|</span><label class="collapse" for="c-42477740">[-]</label><label class="expand" for="c-42477740">[1 more]</label></div><br/><div class="children"><div class="content">Curious about how many tests were performed. Did it consistently manage to successfully solve many of these types of problems?</div><br/></div></div><div id="42473783" class="c"><input type="checkbox" id="c-42473783" checked=""/><div class="controls bullet"><span class="by">antirez</span><span>|</span><a href="#42473497">parent</a><span>|</span><a href="#42477740">prev</a><span>|</span><a href="#42473582">next</a><span>|</span><label class="collapse" for="c-42473783">[-]</label><label class="expand" for="c-42473783">[19 more]</label></div><br/><div class="children"><div class="content">NNs are not algorithms.</div><br/><div id="42474033" class="c"><input type="checkbox" id="c-42474033" checked=""/><div class="controls bullet"><span class="by">benlivengood</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473783">parent</a><span>|</span><a href="#42478070">next</a><span>|</span><label class="collapse" for="c-42474033">[-]</label><label class="expand" for="c-42474033">[1 more]</label></div><br/><div class="children"><div class="content">Deterministic (ieee 754 floats), terminates on all inputs, correctness (produces loss &lt; X on N training&#x2F;test inputs)<p>At most you can argue that there isn&#x27;t a useful bounded loss on every possible input, but it turns out that humans don&#x27;t achieve useful bounded loss on identifying arbitrary sets of pixels as a cat or whatever, either.  Most problems NNs are aimed at are qualitative or probabilistic where provable bounds are less useful than Nth-percentile performance on real-world data.</div><br/></div></div><div id="42478070" class="c"><input type="checkbox" id="c-42478070" checked=""/><div class="controls bullet"><span class="by">necovek</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473783">parent</a><span>|</span><a href="#42474033">prev</a><span>|</span><a href="#42473823">next</a><span>|</span><label class="collapse" for="c-42478070">[-]</label><label class="expand" for="c-42478070">[1 more]</label></div><br/><div class="children"><div class="content">NN is a very wide term applied in different contexts.<p>When a NN is trained, it produces a set of parameters that basically define an algorithm to do inference with: it&#x27;s a very big one though.<p>We also call that a NN (the joy of natural language).</div><br/></div></div><div id="42473823" class="c"><input type="checkbox" id="c-42473823" checked=""/><div class="controls bullet"><span class="by">notfish</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473783">parent</a><span>|</span><a href="#42478070">prev</a><span>|</span><a href="#42475325">next</a><span>|</span><label class="collapse" for="c-42473823">[-]</label><label class="expand" for="c-42473823">[14 more]</label></div><br/><div class="children"><div class="content">An algorithm is “a process or set of rules to be followed in calculations or other problem-solving operations, especially by a computer”<p>How does a giant pile of linear algebra not meet that definition?</div><br/><div id="42473874" class="c"><input type="checkbox" id="c-42473874" checked=""/><div class="controls bullet"><span class="by">antirez</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473823">parent</a><span>|</span><a href="#42475325">next</a><span>|</span><label class="collapse" for="c-42473874">[-]</label><label class="expand" for="c-42473874">[13 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not made of &quot;steps&quot;, it&#x27;s an almost continuous function to its inputs. And a function is not an algorithm: it is not an object made of conditions, jumps, terminations, ... Obviously it has computation capabilities and is Turing-complete, but is the opposite of an algorithm.</div><br/><div id="42475531" class="c"><input type="checkbox" id="c-42475531" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473874">parent</a><span>|</span><a href="#42477240">next</a><span>|</span><label class="collapse" for="c-42475531">[-]</label><label class="expand" for="c-42475531">[6 more]</label></div><br/><div class="children"><div class="content">If it wasn’t made of steps then Turing machines wouldn’t be able to execute them.<p>Further, this is probably running an algorithm on top of an NN. Some kind of tree search.<p>I get what you’re saying though. You’re trying to draw a distinction between statistical methods and symbolic methods. Someday we will have an algorithm which uses statistical methods that can match human performance on most cognitive tasks, and it won’t look or act like a brain. In some sense that’s disappointing. We can build supersonic jets without fully understanding how birds fly.</div><br/><div id="42475859" class="c"><input type="checkbox" id="c-42475859" checked=""/><div class="controls bullet"><span class="by">antirez</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42475531">parent</a><span>|</span><a href="#42477240">next</a><span>|</span><label class="collapse" for="c-42475859">[-]</label><label class="expand" for="c-42475859">[5 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s see that Turing machines can approximate the execution of NN :) That&#x27;s why there are issues related to numerical precision, but the contrary is also true indeed, NNs can discover and use similar techniques used by traditional algorithms. However: the two remain two different methods to do computations, and probably it&#x27;s not just by chance that many things we can&#x27;t do algorithmically, we can do with NNs, what I mean is that this is not <i>just</i> related to the fact that NNs discover complex algorithms via gradient descent, but also that the computational model of NNs is more adapt to solving certain tasks. So the inference algorithm of NNs (doing multiplications and other batch transformations) is just needed for standard computers to approximate the NN computational model. You can do this analogically, and nobody would claim much (maybe?) it&#x27;s running an algorithm. Or that brains themselves are algorithms.</div><br/><div id="42478101" class="c"><input type="checkbox" id="c-42478101" checked=""/><div class="controls bullet"><span class="by">necovek</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42475859">parent</a><span>|</span><a href="#42475869">next</a><span>|</span><label class="collapse" for="c-42478101">[-]</label><label class="expand" for="c-42478101">[2 more]</label></div><br/><div class="children"><div class="content">Computers can execute precise computations, it&#x27;s just not efficient (and it&#x27;s very much slow).<p>NNs are exactly what &quot;computers&quot; are good for and we&#x27;ve been using since their inception: doing lots of computations quickly.<p>&quot;Analog neural networks&quot; (brains) work much differently from what are &quot;neural networks&quot; in computing, and we have no understanding of their operation to claim they are or aren&#x27;t algorithmic. But computing NNs are simply implementations of an algorithm.<p>Edit: upon further rereading, it seems you equate &quot;neural networks&quot; with brain-like operation. But brain was an inspiration for NNs, they are not an &quot;approximation&quot; of it.</div><br/><div id="42478149" class="c"><input type="checkbox" id="c-42478149" checked=""/><div class="controls bullet"><span class="by">antirez</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42478101">parent</a><span>|</span><a href="#42475869">next</a><span>|</span><label class="collapse" for="c-42478149">[-]</label><label class="expand" for="c-42478149">[1 more]</label></div><br/><div class="children"><div class="content">But the inference itself is orthogonal to the computation the NN is going. Obviously the inference (and training) are algorithms.</div><br/></div></div></div></div><div id="42477923" class="c"><input type="checkbox" id="c-42477923" checked=""/><div class="controls bullet"><span class="by">zeroonetwothree</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42475859">parent</a><span>|</span><a href="#42475869">prev</a><span>|</span><a href="#42477240">next</a><span>|</span><label class="collapse" for="c-42477923">[-]</label><label class="expand" for="c-42477923">[1 more]</label></div><br/><div class="children"><div class="content">We don’t have evidence that a TM can simulate a brain. But we know for a fact that it can execute a NN.</div><br/></div></div></div></div></div></div><div id="42478078" class="c"><input type="checkbox" id="c-42478078" checked=""/><div class="controls bullet"><span class="by">necovek</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473874">parent</a><span>|</span><a href="#42477240">prev</a><span>|</span><a href="#42474947">next</a><span>|</span><label class="collapse" for="c-42478078">[-]</label><label class="expand" for="c-42478078">[1 more]</label></div><br/><div class="children"><div class="content">I would say you are right that function is not an algorithm, but it is an implementation of an algorithm.<p>Is that your point?<p>If so, I&#x27;ve long learned to accept imprecise language as long as the message can be reasonably extracted from it.</div><br/></div></div><div id="42474947" class="c"><input type="checkbox" id="c-42474947" checked=""/><div class="controls bullet"><span class="by">raegis</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473874">parent</a><span>|</span><a href="#42478078">prev</a><span>|</span><a href="#42477303">next</a><span>|</span><label class="collapse" for="c-42474947">[-]</label><label class="expand" for="c-42474947">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s not made of &quot;steps&quot;, it&#x27;s an almost continuous function to its inputs.<p>Can you define &quot;almost continuous function&quot;?  Or explain what you mean by this, and how it is used in the A.I. stuff?</div><br/><div id="42476716" class="c"><input type="checkbox" id="c-42476716" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42474947">parent</a><span>|</span><a href="#42477303">next</a><span>|</span><label class="collapse" for="c-42476716">[-]</label><label class="expand" for="c-42476716">[1 more]</label></div><br/><div class="children"><div class="content">Well, it&#x27;s a bunch of steps, but they&#x27;re smaller. &#x2F;s</div><br/></div></div></div></div><div id="42477303" class="c"><input type="checkbox" id="c-42477303" checked=""/><div class="controls bullet"><span class="by">mvkel</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473874">parent</a><span>|</span><a href="#42474947">prev</a><span>|</span><a href="#42475325">next</a><span>|</span><label class="collapse" for="c-42477303">[-]</label><label class="expand" for="c-42477303">[2 more]</label></div><br/><div class="children"><div class="content">&gt; continuous<p>So, steps?</div><br/><div id="42478056" class="c"><input type="checkbox" id="c-42478056" checked=""/><div class="controls bullet"><span class="by">necovek</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42477303">parent</a><span>|</span><a href="#42475325">next</a><span>|</span><label class="collapse" for="c-42478056">[-]</label><label class="expand" for="c-42478056">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Continuous&quot; would imply infinitely small steps, and as such, would certainly be used as a differentiator (differential? ;) between larger discrete stepped approach.<p>In essence, infinite calculus provides a link between &quot;steps&quot; and continuous, but those are different things indeed.</div><br/></div></div></div></div></div></div></div></div><div id="42475325" class="c"><input type="checkbox" id="c-42475325" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473783">parent</a><span>|</span><a href="#42473823">prev</a><span>|</span><a href="#42474701">next</a><span>|</span><label class="collapse" for="c-42475325">[-]</label><label class="expand" for="c-42475325">[1 more]</label></div><br/><div class="children"><div class="content">How do you define &quot;algorithm&quot;? I suspect it is a definition I would find somewhat unusual. Not to say that I strictly disagree, but only because to my mind &quot;neural net&quot; suggests something a bit more concrete than &quot;algorithm&quot;, so I might instead say that an artificial neural net is an implementation of an algorithm, rather than  or something like that.<p>But, to my mind, something of the form &quot;Train a neural network with an architecture generally like [blah], with a training method+data like [bleh], and save the result. Then, when inputs are received, run them through the NN in such-and-such way.&quot; would constitute an algorithm.</div><br/></div></div><div id="42474701" class="c"><input type="checkbox" id="c-42474701" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473783">parent</a><span>|</span><a href="#42475325">prev</a><span>|</span><a href="#42473582">next</a><span>|</span><label class="collapse" for="c-42474701">[-]</label><label class="expand" for="c-42474701">[1 more]</label></div><br/><div class="children"><div class="content">Running inference on a model certainly is a algorithm.</div><br/></div></div></div></div><div id="42473582" class="c"><input type="checkbox" id="c-42473582" checked=""/><div class="controls bullet"><span class="by">scotty79</span><span>|</span><a href="#42473497">parent</a><span>|</span><a href="#42473783">prev</a><span>|</span><a href="#42474245">next</a><span>|</span><label class="collapse" for="c-42473582">[-]</label><label class="expand" for="c-42473582">[8 more]</label></div><br/><div class="children"><div class="content">Still it&#x27;s comparing average human level performance with best AI performance. Examples of things o3 failed at are insanely easy for humans.</div><br/><div id="42474922" class="c"><input type="checkbox" id="c-42474922" checked=""/><div class="controls bullet"><span class="by">cchance</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473582">parent</a><span>|</span><a href="#42473844">next</a><span>|</span><label class="collapse" for="c-42474922">[-]</label><label class="expand" for="c-42474922">[5 more]</label></div><br/><div class="children"><div class="content">You&#x27;d be surprised what the AVERAGE human fails to do that you think is easy, my mom can&#x27;t fucking send an email without downloading a virus, i have a coworker that believes beyond a shadow of a doubt the world is flat.<p>The Average human is a lot dumber than people on hackernews and reddit seem to realize, shit the people on mturk are likely smarter than the AVERAGE person</div><br/><div id="42477043" class="c"><input type="checkbox" id="c-42477043" checked=""/><div class="controls bullet"><span class="by">mirkodrummer</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42474922">parent</a><span>|</span><a href="#42475188">next</a><span>|</span><label class="collapse" for="c-42477043">[-]</label><label class="expand" for="c-42477043">[1 more]</label></div><br/><div class="children"><div class="content">Not being able to send an email or believing the world is flat it’s not a sign of intelligence, I’d rather say it’s more about culture or being more or less scholarized. Your mom or coworker still can do stuff instinctively that is outperforming every algorithm out there and still unexplained how we do it. We still have no idea what intelligence is</div><br/></div></div><div id="42475188" class="c"><input type="checkbox" id="c-42475188" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42474922">parent</a><span>|</span><a href="#42477043">prev</a><span>|</span><a href="#42473844">next</a><span>|</span><label class="collapse" for="c-42475188">[-]</label><label class="expand" for="c-42475188">[3 more]</label></div><br/><div class="children"><div class="content">Yet the average human can drive a car a lot better than ChatGPT can, which shows that the way you frame &quot;intelligence&quot; dictates your conclusion about who is &quot;intelligent&quot;.</div><br/><div id="42475350" class="c"><input type="checkbox" id="c-42475350" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42475188">parent</a><span>|</span><a href="#42475417">next</a><span>|</span><label class="collapse" for="c-42475350">[-]</label><label class="expand" for="c-42475350">[1 more]</label></div><br/><div class="children"><div class="content">Pretty sure a waymo car drives better than an average SF driver.</div><br/></div></div><div id="42475417" class="c"><input type="checkbox" id="c-42475417" checked=""/><div class="controls bullet"><span class="by">tracerbulletx</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42475188">parent</a><span>|</span><a href="#42475350">prev</a><span>|</span><a href="#42473844">next</a><span>|</span><label class="collapse" for="c-42475417">[-]</label><label class="expand" for="c-42475417">[1 more]</label></div><br/><div class="children"><div class="content">If you take an electrical sensory input signal sequence, and transform it to a electrical muscle output signal sequence you&#x27;ve got a brain. ChatGPT isn&#x27;t going to drive a car because it&#x27;s trained on verbal tokens, and it&#x27;s not optimized for the type of latency you need for physical interaction.<p>And the brain doesn&#x27;t use the same network to do verbal reasoning as real time coordination either.<p>But that work is moving along fine. All of these models and lessons are going to be combined into AGI. It is happening. There isn&#x27;t really that much in the way.</div><br/></div></div></div></div></div></div><div id="42473844" class="c"><input type="checkbox" id="c-42473844" checked=""/><div class="controls bullet"><span class="by">FrustratedMonky</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42473582">parent</a><span>|</span><a href="#42474922">prev</a><span>|</span><a href="#42474245">next</a><span>|</span><label class="collapse" for="c-42473844">[-]</label><label class="expand" for="c-42473844">[2 more]</label></div><br/><div class="children"><div class="content">There are things Chimps do easily that humans fail at, and vice&#x2F;versa of course.<p>There are blind spots, doesn&#x27;t take away from &#x27;general&#x27;.</div><br/></div></div></div></div><div id="42474245" class="c"><input type="checkbox" id="c-42474245" checked=""/><div class="controls bullet"><span class="by">6gvONxR4sf7o</span><span>|</span><a href="#42473497">parent</a><span>|</span><a href="#42473582">prev</a><span>|</span><a href="#42474659">next</a><span>|</span><label class="collapse" for="c-42474245">[-]</label><label class="expand" for="c-42474245">[1 more]</label></div><br/><div class="children"><div class="content">Human performance is much closer to 100% on this, depending on your human. It&#x27;s easy to miss the dot in the corner of the headline graph in TFA that says &quot;STEM grad.&quot;</div><br/></div></div><div id="42474659" class="c"><input type="checkbox" id="c-42474659" checked=""/><div class="controls bullet"><span class="by">hypoxia</span><span>|</span><a href="#42473497">parent</a><span>|</span><a href="#42474245">prev</a><span>|</span><a href="#42473573">next</a><span>|</span><label class="collapse" for="c-42474659">[-]</label><label class="expand" for="c-42474659">[3 more]</label></div><br/><div class="children"><div class="content">It actually beats the human average by a wide margin:<p>- 64.2% for humans vs. 82.8%+ for o3.<p>...<p>Private Eval:<p>- 85%: threshold for winning the prize [1]<p>Semi-Private Eval:<p>- 87.5%: o3 (unlimited compute) [2]<p>- 75.7%: o3 (limited compute) [2]<p>Public Eval:<p>- 91.5%: o3 (unlimited compute) [2]<p>- 82.8%: o3 (limited compute) [2]<p>- 64.2%: human average (Mechanical Turk) [1] [3]<p>Public Training:<p>- 76.2%: human average (Mechanical Turk) [1] [3]<p>...<p>References:<p>[1] <a href="https:&#x2F;&#x2F;arcprize.org&#x2F;guide" rel="nofollow">https:&#x2F;&#x2F;arcprize.org&#x2F;guide</a><p>[2] <a href="https:&#x2F;&#x2F;arcprize.org&#x2F;blog&#x2F;oai-o3-pub-breakthrough" rel="nofollow">https:&#x2F;&#x2F;arcprize.org&#x2F;blog&#x2F;oai-o3-pub-breakthrough</a><p>[3] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2409.01374" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2409.01374</a></div><br/><div id="42474889" class="c"><input type="checkbox" id="c-42474889" checked=""/><div class="controls bullet"><span class="by">usaar333</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42474659">parent</a><span>|</span><a href="#42473573">next</a><span>|</span><label class="collapse" for="c-42474889">[-]</label><label class="expand" for="c-42474889">[2 more]</label></div><br/><div class="children"><div class="content">Super human isn&#x27;t beating rando mech turk.<p>Their post has stem grad at nearly 100%</div><br/><div id="42475760" class="c"><input type="checkbox" id="c-42475760" checked=""/><div class="controls bullet"><span class="by">tripletao</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42474889">parent</a><span>|</span><a href="#42473573">next</a><span>|</span><label class="collapse" for="c-42475760">[-]</label><label class="expand" for="c-42475760">[1 more]</label></div><br/><div class="children"><div class="content">This is correct. It&#x27;s easy to get arbitrarily bad results on Mechanical Turk, since without any quality control people will just click as fast as they can to get paid (or bot it and get paid even faster).<p>So in practice, there&#x27;s always some kind of quality control. Stricter quality control will improve your results, and the right amount of quality control is subjective. This makes any assessment of human quality meaningless without explanation of how those humans were selected and incentivized. Chollet is careful to provide that, but many posters here are not.<p>In any case, the ensemble of task-specific, low-compute Kaggle solutions is reportedly also super-Turk, at 81%. I don&#x27;t think anyone would call that AGI, since it&#x27;s not general; but if the &quot;(tuned)&quot; in the figure means o3 was tuned specifically for these tasks, that&#x27;s not obviously general either.</div><br/></div></div></div></div></div></div><div id="42473573" class="c"><input type="checkbox" id="c-42473573" checked=""/><div class="controls bullet"><span class="by">ALittleLight</span><span>|</span><a href="#42473497">parent</a><span>|</span><a href="#42474659">prev</a><span>|</span><a href="#42476351">next</a><span>|</span><label class="collapse" for="c-42473573">[-]</label><label class="expand" for="c-42473573">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not saturated.  85% is average human performance, not &quot;best human&quot; performance.  There is still room for the model to go up to 100% on this eval.</div><br/></div></div><div id="42476351" class="c"><input type="checkbox" id="c-42476351" checked=""/><div class="controls bullet"><span class="by">dyauspitr</span><span>|</span><a href="#42473497">parent</a><span>|</span><a href="#42473573">prev</a><span>|</span><a href="#42473810">next</a><span>|</span><label class="collapse" for="c-42476351">[-]</label><label class="expand" for="c-42476351">[3 more]</label></div><br/><div class="children"><div class="content">I’ll believe it when the AI can earn money on its own. I obviously don’t mean someone paying a subscription to use the AI I mean, letting the AI lose on the Internet with only the goal of making money and putting it into a bank account.</div><br/><div id="42477105" class="c"><input type="checkbox" id="c-42477105" checked=""/><div class="controls bullet"><span class="by">hamburga</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42476351">parent</a><span>|</span><a href="#42473810">next</a><span>|</span><label class="collapse" for="c-42477105">[-]</label><label class="expand" for="c-42477105">[2 more]</label></div><br/><div class="children"><div class="content">Do trading bots count?</div><br/><div id="42477980" class="c"><input type="checkbox" id="c-42477980" checked=""/><div class="controls bullet"><span class="by">1659447091</span><span>|</span><a href="#42473497">root</a><span>|</span><a href="#42477105">parent</a><span>|</span><a href="#42473810">next</a><span>|</span><label class="collapse" for="c-42477980">[-]</label><label class="expand" for="c-42477980">[1 more]</label></div><br/><div class="children"><div class="content">No, the AI would have to start from zero and reason it&#x27;s way to making itself money online, such as the humans who were first in their online field of interest (e-commerce, scams, ads etc from the 80&#x27;s and 90&#x27;s) when there was no guidance, only general human intelligence that could reason their way into money making opportunities and reason their way into making it work.</div><br/></div></div></div></div></div></div></div></div><div id="42473810" class="c"><input type="checkbox" id="c-42473810" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#42473497">prev</a><span>|</span><a href="#42477937">next</a><span>|</span><label class="collapse" for="c-42473810">[-]</label><label class="expand" for="c-42473810">[42 more]</label></div><br/><div class="children"><div class="content">Let me go against some skeptics and explain why I think full o3 is pretty much AGI or at least embodies most essential aspects of AGI.<p>What has been lacking so far in frontier LLMs is the ability to reliably deal with the right level of abstraction for a given problem. Reasoning is useful but often comes out lacking if one cannot reason at the right level of abstraction. (Note that many humans can&#x27;t either when they deal with unfamiliar domains, although that is not the case with these models.)<p>ARC has been challenging precisely because solving its problems often requires:<p><pre><code>   1) using multiple different *kinds* of core knowledge [1], such as symmetry, counting, color, AND

   2) using the right level(s) of abstraction
</code></pre>
Achieving human-level performance in the ARC benchmark, <i>as well as</i> top human performance in GPQA, Codeforces, AIME, and Frontier Math suggests the model can potentially solve any problem at the human level if it possesses essential knowledge about it. Yes, this includes out-of-distribution problems that most humans can solve.<p>It might not <i>yet</i> be able to generate highly novel theories, frameworks, or artifacts to the degree that Einstein, Grothendieck, or van Gogh could. But not many humans can either.<p>[1] <a href="https:&#x2F;&#x2F;www.harvardlds.org&#x2F;wp-content&#x2F;uploads&#x2F;2017&#x2F;01&#x2F;SpelkeKinzler07-1.pdf" rel="nofollow">https:&#x2F;&#x2F;www.harvardlds.org&#x2F;wp-content&#x2F;uploads&#x2F;2017&#x2F;01&#x2F;Spelke...</a><p>ADDED:<p>Thanks to the link to Chollet&#x27;s posts by lswainemoore below. I&#x27;ve analyzed some easy problems that o3 failed at. They involve spatial intelligence, including connection and movement. This skill is very hard to learn from textual and still image data.<p>I believe this sort of core knowledge is learnable through movement and interaction data in a simulated world and it will <i>not</i> present a very difficult barrier to cross. (OpenAI purchased a company behind a Minecraft clone a while ago. I&#x27;ve wondered if this is the purpose.)</div><br/><div id="42474046" class="c"><input type="checkbox" id="c-42474046" checked=""/><div class="controls bullet"><span class="by">phil917</span><span>|</span><a href="#42473810">parent</a><span>|</span><a href="#42476406">next</a><span>|</span><label class="collapse" for="c-42474046">[-]</label><label class="expand" for="c-42474046">[12 more]</label></div><br/><div class="children"><div class="content">Quote from the creators of the AGI-ARC benchmark: &quot;Passing ARC-AGI does not equate achieving AGI, and, as a matter of fact, I don&#x27;t think o3 is AGI yet. o3 still fails on some very easy tasks, indicating fundamental differences with human intelligence.&quot;</div><br/><div id="42474140" class="c"><input type="checkbox" id="c-42474140" checked=""/><div class="controls bullet"><span class="by">CooCooCaCha</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474046">parent</a><span>|</span><a href="#42474201">next</a><span>|</span><label class="collapse" for="c-42474140">[-]</label><label class="expand" for="c-42474140">[4 more]</label></div><br/><div class="children"><div class="content">Yeah the real goalpost is <i>reliable</i> intelligence. A supposed phd level AI failing simple problems is a red flag that we’re still missing something.</div><br/><div id="42474464" class="c"><input type="checkbox" id="c-42474464" checked=""/><div class="controls bullet"><span class="by">gremlinsinc</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474140">parent</a><span>|</span><a href="#42474201">next</a><span>|</span><label class="collapse" for="c-42474464">[-]</label><label class="expand" for="c-42474464">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;ve never met a Doctor who couldn&#x27;t figure out how to work their email? Or use street smarts? You can have a PHD but be unable to reliably handle soft skills, or any number of things you might &#x27;expect&#x27; someone to be able to do.<p>Just playing devils&#x27; advocate or nitpicking the language a bit...</div><br/><div id="42475203" class="c"><input type="checkbox" id="c-42475203" checked=""/><div class="controls bullet"><span class="by">nuancebydefault</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474464">parent</a><span>|</span><a href="#42474853">next</a><span>|</span><label class="collapse" for="c-42475203">[-]</label><label class="expand" for="c-42475203">[1 more]</label></div><br/><div class="children"><div class="content">A coworker of mine has a phd in physics. Showing the difference to him between little and big endian in a hex editor, showing file sizes of raw image files and how to compute it... I explained 3 times and maybe he understood part of it now.</div><br/></div></div><div id="42474853" class="c"><input type="checkbox" id="c-42474853" checked=""/><div class="controls bullet"><span class="by">CooCooCaCha</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474464">parent</a><span>|</span><a href="#42475203">prev</a><span>|</span><a href="#42474201">next</a><span>|</span><label class="collapse" for="c-42474853">[-]</label><label class="expand" for="c-42474853">[1 more]</label></div><br/><div class="children"><div class="content">An important distinction here is you’re comparing skill across very different tasks.<p>I’m not even going that far, I’m talking about performance on similar tasks. Something many people have noticed about modern AI is it can go from genius to baby-level performance seemingly at random.<p>Take self driving cars for example, a reasonably intelligent human of sound mind and body would never accidentally mistake a concrete pillar for a road. Yet that happens with self-driving cars, and seemingly here with ARC-AGI problems which all have a similar flavor.</div><br/></div></div></div></div></div></div><div id="42474201" class="c"><input type="checkbox" id="c-42474201" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474046">parent</a><span>|</span><a href="#42474140">prev</a><span>|</span><a href="#42474297">next</a><span>|</span><label class="collapse" for="c-42474201">[-]</label><label class="expand" for="c-42474201">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;d need to see what kinds of easy tasks those are and would be happy to revise my hypothesis if that&#x27;s warranted.<p>Also, it depends a great deal on what we define as AGI and whether they need to be a strict superset of typical human intelligence. o3&#x27;s intelligence is probably superhuman in some aspects but inferior in others. We can find many humans who exhibit such tendencies as well. We&#x27;d probably say they think differently but would still call them generally intelligent.</div><br/><div id="42474369" class="c"><input type="checkbox" id="c-42474369" checked=""/><div class="controls bullet"><span class="by">lswainemoore</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474201">parent</a><span>|</span><a href="#42474297">next</a><span>|</span><label class="collapse" for="c-42474369">[-]</label><label class="expand" for="c-42474369">[5 more]</label></div><br/><div class="children"><div class="content">They&#x27;re in the original post. Also here: <a href="https:&#x2F;&#x2F;x.com&#x2F;fchollet&#x2F;status&#x2F;1870172872641261979" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;fchollet&#x2F;status&#x2F;1870172872641261979</a> &#x2F; <a href="https:&#x2F;&#x2F;x.com&#x2F;fchollet&#x2F;status&#x2F;1870173137234727219" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;fchollet&#x2F;status&#x2F;1870173137234727219</a><p>Personally, I think it&#x27;s fair to call them &quot;very easy&quot;. If a person I otherwise thought was intelligent was unable to solve these, I&#x27;d be quite surprised.</div><br/><div id="42474547" class="c"><input type="checkbox" id="c-42474547" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474369">parent</a><span>|</span><a href="#42474297">next</a><span>|</span><label class="collapse" for="c-42474547">[-]</label><label class="expand" for="c-42474547">[4 more]</label></div><br/><div class="children"><div class="content">Thanks! I&#x27;ve analyzed some easy problems that o3 failed at. They involve spatial intelligence including connection and movement. This skill is very hard to learn from textual and still image data.<p>I believe this sort of core knowledge is learnable through movement and interaction data in a simulated world and it will not present a very difficult barrier to cross.<p>(OpenAI purchased a company behind a Minecraft clone a while ago. I&#x27;ve wondered if this is the purpose.)</div><br/><div id="42474689" class="c"><input type="checkbox" id="c-42474689" checked=""/><div class="controls bullet"><span class="by">lswainemoore</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474547">parent</a><span>|</span><a href="#42474297">next</a><span>|</span><label class="collapse" for="c-42474689">[-]</label><label class="expand" for="c-42474689">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I believe this sort of core knowledge is learnable through movement and interaction data in a simulated world and it will not present a very difficult barrier to cross.<p>Maybe! I suppose time will tell. That said, spatial intelligence (connection&#x2F;movement included) is the whole game in this evaluation set. I think it&#x27;s revealing that they can&#x27;t handle these particular examples, and problematic for claims of AGI.</div><br/><div id="42477172" class="c"><input type="checkbox" id="c-42477172" checked=""/><div class="controls bullet"><span class="by">MVissers</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474689">parent</a><span>|</span><a href="#42474297">next</a><span>|</span><label class="collapse" for="c-42477172">[-]</label><label class="expand" for="c-42477172">[2 more]</label></div><br/><div class="children"><div class="content">Probably just not trained on this kind of data. We could create a benchmark about it, and they&#x27;d shatter it within a year or so.<p>I&#x27;m starting to really see no limits on intelligence in these models.</div><br/><div id="42478197" class="c"><input type="checkbox" id="c-42478197" checked=""/><div class="controls bullet"><span class="by">sungho_</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42477172">parent</a><span>|</span><a href="#42474297">next</a><span>|</span><label class="collapse" for="c-42478197">[-]</label><label class="expand" for="c-42478197">[1 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t the fact that it can only accomplish tasks with benchmarks imply that it has limitations in intelligence?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42474297" class="c"><input type="checkbox" id="c-42474297" checked=""/><div class="controls bullet"><span class="by">93po</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474046">parent</a><span>|</span><a href="#42474201">prev</a><span>|</span><a href="#42476406">next</a><span>|</span><label class="collapse" for="c-42474297">[-]</label><label class="expand" for="c-42474297">[1 more]</label></div><br/><div class="children"><div class="content">they say it isn&#x27;t AGI but i think the way o3 functions can be refined to AGI - it&#x27;s learning to solve a new, novel problems. we just need to make it do that more consistently, which seems achievable</div><br/></div></div></div></div><div id="42476406" class="c"><input type="checkbox" id="c-42476406" checked=""/><div class="controls bullet"><span class="by">dimitri-vs</span><span>|</span><a href="#42473810">parent</a><span>|</span><a href="#42474046">prev</a><span>|</span><a href="#42473982">next</a><span>|</span><label class="collapse" for="c-42476406">[-]</label><label class="expand" for="c-42476406">[4 more]</label></div><br/><div class="children"><div class="content">Have we really watered down the definition of AGI that much?<p>LLMs aren&#x27;t really capable of &quot;learning&quot; anything outside their training data. Which I feel is a very basic and fundamental capability of humans.<p>Every new request thread is a blank slate utilizing whatever context you provide for the specific task and after the tread is done (or context limit runs out) it&#x27;s like it never happened. Sure you can use databases, do web queries, etc. but these are inflexible bandaid solutions, far from what&#x27;s needed for AGI.</div><br/><div id="42477776" class="c"><input type="checkbox" id="c-42477776" checked=""/><div class="controls bullet"><span class="by">bubblyworld</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42476406">parent</a><span>|</span><a href="#42476463">next</a><span>|</span><label class="collapse" for="c-42477776">[-]</label><label class="expand" for="c-42477776">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s true for vanilla LLMs, but also keep in mind that there are no details about o3&#x27;s architecture at the moment. Clearly they are doing <i>something</i> different given the huge performance jump on a lot of benchmarks, and it may well involve in-context learning.</div><br/></div></div><div id="42476463" class="c"><input type="checkbox" id="c-42476463" checked=""/><div class="controls bullet"><span class="by">theptip</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42476406">parent</a><span>|</span><a href="#42477776">prev</a><span>|</span><a href="#42473982">next</a><span>|</span><label class="collapse" for="c-42476463">[-]</label><label class="expand" for="c-42476463">[2 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs aren&#x27;t really capable of &quot;learning&quot; anything outside their training data.<p>ChatGPT has had for some time the feature of storing memories about its conversations with users. And you can use function calling to make this more generic.<p>I think drawing the boundary at “model + scaffolding” is more interesting.</div><br/><div id="42477497" class="c"><input type="checkbox" id="c-42477497" checked=""/><div class="controls bullet"><span class="by">dimitri-vs</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42476463">parent</a><span>|</span><a href="#42473982">next</a><span>|</span><label class="collapse" for="c-42477497">[-]</label><label class="expand" for="c-42477497">[1 more]</label></div><br/><div class="children"><div class="content">Calling the sentence or two it arbitrarily saves when you statd your preferences and profile info  &quot;memories&quot; is a stretch.<p>True equivalent to human memories would require something like a multimodal trillion token context window.<p>RAG is just not going to cut it, and if anything will exacerbated problems with hallucinations.</div><br/></div></div></div></div></div></div><div id="42473982" class="c"><input type="checkbox" id="c-42473982" checked=""/><div class="controls bullet"><span class="by">timabdulla</span><span>|</span><a href="#42473810">parent</a><span>|</span><a href="#42476406">prev</a><span>|</span><a href="#42474189">next</a><span>|</span><label class="collapse" for="c-42473982">[-]</label><label class="expand" for="c-42473982">[5 more]</label></div><br/><div class="children"><div class="content">What&#x27;s your explanation for why it can only get ~70% on SWE-bench Verified?<p>I believe about 90% of the tasks were estimated by humans to take less than one hour to solve, so we aren&#x27;t talking about very complex problems, and to boot, the contamination factor is huge: o3 (or any big model) will have in-depth knowledge of the internals of these projects, and often even know about the individual issues themselves (e.g. you can say what was Github issue #4145 in project foo, and there&#x27;s a decent chance it can tell you exactly what the issue was about!)</div><br/><div id="42474098" class="c"><input type="checkbox" id="c-42474098" checked=""/><div class="controls bullet"><span class="by">slewis</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42473982">parent</a><span>|</span><a href="#42474109">next</a><span>|</span><label class="collapse" for="c-42474098">[-]</label><label class="expand" for="c-42474098">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve spent tons of time evaluating o1-preview on SWEBench-Verified.<p>For one, I speculate OpenAI is using a very basic agent harness to get the results they&#x27;ve published on SWEBench. I believe there is a fair amount of headroom to improve results above what they published, using the same models.<p>For two, some of the instances, even in SWEBench-Verified, require a bit of &quot;going above and beyond&quot; to get right. One example is an instance where the user states that a TypeError isn&#x27;t properly handled. The developer who fixed it handled the TypeError but also handled a ValueError, and the golden test checks for both. I don&#x27;t know how many instances fall in this category, but I suspect its more than on a simpler benchmark like MATH.</div><br/></div></div><div id="42474109" class="c"><input type="checkbox" id="c-42474109" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42473982">parent</a><span>|</span><a href="#42474098">prev</a><span>|</span><a href="#42474189">next</a><span>|</span><label class="collapse" for="c-42474109">[-]</label><label class="expand" for="c-42474109">[3 more]</label></div><br/><div class="children"><div class="content">One possibility is that it may not yet have sufficient <i>experience and real-world feedback</i> for resolving coding issues in professional repos, as this involves multiple steps and very diverse actions (or branching factor, in AI terms). They have committed to not training on API usage, which limits their ability to directly acquire training data from it. However, their upcoming agentic efforts may address this gap in training data.</div><br/><div id="42474182" class="c"><input type="checkbox" id="c-42474182" checked=""/><div class="controls bullet"><span class="by">timabdulla</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474109">parent</a><span>|</span><a href="#42474189">next</a><span>|</span><label class="collapse" for="c-42474182">[-]</label><label class="expand" for="c-42474182">[2 more]</label></div><br/><div class="children"><div class="content">Right, but the branching factor increases exponentially with the scope of the work.<p>I think it&#x27;s obvious that they&#x27;ve cracked the formula for solving well-defined, small-in-scope problems at a superhuman level. That&#x27;s an amazing thing.<p>To me, it&#x27;s less obvious that this implies that they will in short order with just more training data be able to solve ambiguous, large-in-scope problems at even just a skilled human level.<p>There are far more paths to consider, much more context to use, and in an RL setting, the rewards are much more ambiguously defined.</div><br/><div id="42475389" class="c"><input type="checkbox" id="c-42475389" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474182">parent</a><span>|</span><a href="#42474189">next</a><span>|</span><label class="collapse" for="c-42475389">[-]</label><label class="expand" for="c-42475389">[1 more]</label></div><br/><div class="children"><div class="content">Their reasoning models can learn from procedures and methods, which generalize far better than data. Software tasks are diverse but most tasks are still fairly limited in scope. Novel tasks might remain challenging for these models, as they do for humans.<p>That said, o3 might still lack some kind of interaction intelligence that’s hard to learn. We’ll see.</div><br/></div></div></div></div></div></div></div></div><div id="42474189" class="c"><input type="checkbox" id="c-42474189" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#42473810">parent</a><span>|</span><a href="#42473982">prev</a><span>|</span><a href="#42474782">next</a><span>|</span><label class="collapse" for="c-42474189">[-]</label><label class="expand" for="c-42474189">[4 more]</label></div><br/><div class="children"><div class="content">GPQA scores are mostly from pre-training, against content in the corpus. They have gone silent but look at the GPT4 technical report which calls this out.<p>We are nowhere close to what Sam Altman calls AGI and transformers are still limited to what uniform-TC0 can do.<p>As an example the Boolean Formula Value Problem is NC1-complete, thus beyond transformers but trivial to solve with a TM.<p>As it is now proven that the frame problem is equivalent to the halting problem, even if we can move past uniform-TC0 limits, novelty is still a problem.<p>I think the advancements are truly extraordinary, but unless you set the bar very low, we aren&#x27;t close to AGI.<p>Heck we aren&#x27;t close to P with commercial models.</div><br/><div id="42474479" class="c"><input type="checkbox" id="c-42474479" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474189">parent</a><span>|</span><a href="#42474782">next</a><span>|</span><label class="collapse" for="c-42474479">[-]</label><label class="expand" for="c-42474479">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t any physically realizable computer (including our brains) limited to what uniform-TC0 can do?</div><br/><div id="42475694" class="c"><input type="checkbox" id="c-42475694" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474479">parent</a><span>|</span><a href="#42475536">next</a><span>|</span><label class="collapse" for="c-42475694">[-]</label><label class="expand" for="c-42475694">[1 more]</label></div><br/><div class="children"><div class="content">Neither TC0 nor uniform-TC0 are physically realizable, they are tools not physical devices.<p>The default nonuniform circuits classes are allowed to have a different circuit per input size, the uniform types have unbounded fan-in<p>Similar to how a k-tape TM doesn&#x27;t get &#x27;charged&#x27; for the input size.<p>With Nick Class (NC) the number of components is similar to traditional compute time while depth relates to the ability to parallelize operations.<p>These are different than biological neurons, not better or worse but just different.<p>Human neurons can use dendritic compartmentalization, use spike timing, can retime spikes etc...<p>While the perceptron model we use in ML is useful, it is not able to do xor in one layer, while biological neurons do that without anything even reaching the soma, purely in the dendrites.<p>Statistical learning models still comes down to a choice function, no matter if you call that set shattering or...<p>With physical computers the time hierarchy does apply and if TIME(g(n)) is given more time than TIME(f(n)), g(n) can solve more problems.<p>So you can simulate a NTM with exhaustive search with a physical computer.<p>Physical computers also tend to have NAND and XOR gates, and can have different circuit depths.<p>When you are in TC0, you only have AND, OR and Threshold (or majority) gates.<p>Think of instruction level parallelism in a typical CPU, it can return early, vs Itanium EPIC, which had to wait for the longest operation.  Predicated execution is also how GPUs work.<p>They can send a mask and save on load store ops as an example, but the cost of that parallelism is the consent depth.<p>It is the parallelism tradeoff that both makes transformers practical as well as limit what they can do.<p>The IID assumption and autograd requiring smooth manifolds plays a role too.<p>The frame problem, which causes hard problems to become unsolvable for computers and people alike does also.<p>But the fact that we have polynomial time solutions for the Boolean Formula Value Problem, as mentioned in my post above is probably a simpler way of realizing physical computers aren&#x27;t limited to uniform-TC0.</div><br/></div></div><div id="42475536" class="c"><input type="checkbox" id="c-42475536" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474479">parent</a><span>|</span><a href="#42475694">prev</a><span>|</span><a href="#42474782">next</a><span>|</span><label class="collapse" for="c-42475536">[-]</label><label class="expand" for="c-42475536">[1 more]</label></div><br/><div class="children"><div class="content">Do you just mean because any physically realizable computer is a finite state machine? Or...?<p>I wouldn&#x27;t describe a computer&#x27;s usual behavior as having constant depth.<p>It is fairly typical to talk about problems in P as being feasible (though when the constant factors are too big, this isn&#x27;t strictly true of course).<p>Just because for unreasonably large inputs, my computer can&#x27;t run a particular program and produce the correct answer for that input, due to my computer running out of memory, we don&#x27;t generally say that my computer is fundamentally incapable of executing that algorithm.</div><br/></div></div></div></div></div></div><div id="42474782" class="c"><input type="checkbox" id="c-42474782" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#42473810">parent</a><span>|</span><a href="#42474189">prev</a><span>|</span><a href="#42477889">next</a><span>|</span><label class="collapse" for="c-42474782">[-]</label><label class="expand" for="c-42474782">[1 more]</label></div><br/><div class="children"><div class="content">The problem with ARC is that there are a finite number of heuristics that could be enumerated and trained for, which would give model a substantial leg up on this evaluation, but not be generalized to other domains.<p>For example, if they produce millions of examples of the type of problems o3 still struggles on, it would probably do better at similar questions.<p>Perhaps the private data set is different enough that this isn’t a problem, but the ideal situation would be unveiling a truly novel dataset, which it seems like arc aims to do.</div><br/></div></div><div id="42474040" class="c"><input type="checkbox" id="c-42474040" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#42473810">parent</a><span>|</span><a href="#42477889">prev</a><span>|</span><a href="#42476959">next</a><span>|</span><label class="collapse" for="c-42474040">[-]</label><label class="expand" for="c-42474040">[4 more]</label></div><br/><div class="children"><div class="content">&gt;Achieving human-level performance in the ARC benchmark, as well as top human performance in GPQA, Codeforce, AIME, and Frontier Math strongly suggests the model can potentially solve any problem at the human level if it possesses essential knowledge about it.<p>The article notes, &quot;o3 still fails on some very easy tasks&quot;. What explains these failures if o3 can solve &quot;any problem&quot; at the human level? Do these failed cases require some essential knowledge that has eluded the massive OpenAI training set?</div><br/><div id="42474368" class="c"><input type="checkbox" id="c-42474368" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474040">parent</a><span>|</span><a href="#42476959">next</a><span>|</span><label class="collapse" for="c-42474368">[-]</label><label class="expand" for="c-42474368">[3 more]</label></div><br/><div class="children"><div class="content">Great point. I&#x27;d love to see what these easy tasks are and would be happy to revise my hypothesis accordingly. o3&#x27;s intelligence is unlikely to be a strict superset of human intelligence. It is certainly superior to humans in some respects and probably inferior in others. Whether it&#x27;s sufficiently generally intelligent would be both a matter of definition and empirical fact.</div><br/><div id="42474425" class="c"><input type="checkbox" id="c-42474425" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474368">parent</a><span>|</span><a href="#42476959">next</a><span>|</span><label class="collapse" for="c-42474425">[-]</label><label class="expand" for="c-42474425">[2 more]</label></div><br/><div class="children"><div class="content">Chollet has a few examples here:<p><a href="https:&#x2F;&#x2F;x.com&#x2F;fchollet&#x2F;status&#x2F;1870172872641261979" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;fchollet&#x2F;status&#x2F;1870172872641261979</a><p><a href="https:&#x2F;&#x2F;x.com&#x2F;fchollet&#x2F;status&#x2F;1870173137234727219" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;fchollet&#x2F;status&#x2F;1870173137234727219</a><p>I would definitely consider them legitimately easy for humans.</div><br/><div id="42474601" class="c"><input type="checkbox" id="c-42474601" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42474425">parent</a><span>|</span><a href="#42476959">next</a><span>|</span><label class="collapse" for="c-42474601">[-]</label><label class="expand" for="c-42474601">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! I added some comments on this at the bottom of the post above.</div><br/></div></div></div></div></div></div></div></div><div id="42476959" class="c"><input type="checkbox" id="c-42476959" checked=""/><div class="controls bullet"><span class="by">uncomplexity_</span><span>|</span><a href="#42473810">parent</a><span>|</span><a href="#42474040">prev</a><span>|</span><a href="#42476099">next</a><span>|</span><label class="collapse" for="c-42476959">[-]</label><label class="expand" for="c-42476959">[1 more]</label></div><br/><div class="children"><div class="content">on the spatial data i see it as a highly intelligent head of a machine that just needs better limbs and better senses.<p>i think that&#x27;s where most hardware startups will specialize with in the coming decades, different industries with different needs.</div><br/></div></div><div id="42476099" class="c"><input type="checkbox" id="c-42476099" checked=""/><div class="controls bullet"><span class="by">puttycat</span><span>|</span><a href="#42473810">parent</a><span>|</span><a href="#42476959">prev</a><span>|</span><a href="#42476051">next</a><span>|</span><label class="collapse" for="c-42476099">[-]</label><label class="expand" for="c-42476099">[1 more]</label></div><br/><div class="children"><div class="content">Great comment. See this as well for another potential reason for failure:<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.10013" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.10013</a></div><br/></div></div><div id="42476051" class="c"><input type="checkbox" id="c-42476051" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#42473810">parent</a><span>|</span><a href="#42476099">prev</a><span>|</span><a href="#42477074">next</a><span>|</span><label class="collapse" for="c-42476051">[-]</label><label class="expand" for="c-42476051">[2 more]</label></div><br/><div class="children"><div class="content">In order to replace actual humans doing their job I think LLMs are lacking in judgement, sense of time and agenticism.</div><br/><div id="42476467" class="c"><input type="checkbox" id="c-42476467" checked=""/><div class="controls bullet"><span class="by">Kostchei</span><span>|</span><a href="#42473810">root</a><span>|</span><a href="#42476051">parent</a><span>|</span><a href="#42477074">next</a><span>|</span><label class="collapse" for="c-42476467">[-]</label><label class="expand" for="c-42476467">[1 more]</label></div><br/><div class="children"><div class="content">I mean fkcu me when they have those things, however, maybe they are just lazy and their judgement is fine, for a lazy intelligence. Inner-self thinks &quot;why are these bastards asking me to do this? &quot;. I doubt that is actually happening, but now, .. prove it isn&#x27;t.</div><br/></div></div></div></div><div id="42477074" class="c"><input type="checkbox" id="c-42477074" checked=""/><div class="controls bullet"><span class="by">mirkodrummer</span><span>|</span><a href="#42473810">parent</a><span>|</span><a href="#42476051">prev</a><span>|</span><a href="#42477129">next</a><span>|</span><label class="collapse" for="c-42477074">[-]</label><label class="expand" for="c-42477074">[1 more]</label></div><br/><div class="children"><div class="content">Please stop it calling AGI, we don’t even know or agree universally what that should actually mean. How far did we get with hype calling a lossy probabilistic compressor firing slowly at us words AGI? That’s a real bummer to me</div><br/></div></div><div id="42477129" class="c"><input type="checkbox" id="c-42477129" checked=""/><div class="controls bullet"><span class="by">ryoshu</span><span>|</span><a href="#42473810">parent</a><span>|</span><a href="#42477074">prev</a><span>|</span><a href="#42473947">next</a><span>|</span><label class="collapse" for="c-42477129">[-]</label><label class="expand" for="c-42477129">[1 more]</label></div><br/><div class="children"><div class="content">Ask o3 is P=NP?</div><br/></div></div><div id="42473947" class="c"><input type="checkbox" id="c-42473947" checked=""/><div class="controls bullet"><span class="by">xvector</span><span>|</span><a href="#42473810">parent</a><span>|</span><a href="#42477129">prev</a><span>|</span><a href="#42474365">next</a><span>|</span><label class="collapse" for="c-42473947">[-]</label><label class="expand" for="c-42473947">[2 more]</label></div><br/><div class="children"><div class="content">Agree. AGI is here. I feel such a sense of pride in our species.</div><br/></div></div><div id="42474365" class="c"><input type="checkbox" id="c-42474365" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#42473810">parent</a><span>|</span><a href="#42473947">prev</a><span>|</span><a href="#42474241">next</a><span>|</span><label class="collapse" for="c-42474365">[-]</label><label class="expand" for="c-42474365">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It might not yet be able to generate highly novel theories, frameworks, or artifacts to the degree that Einstein, Grothendieck, or van Gogh could.<p>Every human does this dozens, hundreds or thousands of times ... during childhood.</div><br/></div></div><div id="42474241" class="c"><input type="checkbox" id="c-42474241" checked=""/><div class="controls bullet"><span class="by">norir</span><span>|</span><a href="#42473810">parent</a><span>|</span><a href="#42474365">prev</a><span>|</span><a href="#42477937">next</a><span>|</span><label class="collapse" for="c-42474241">[-]</label><label class="expand" for="c-42474241">[1 more]</label></div><br/><div class="children"><div class="content">Personally I find &quot;human-level&quot; to be a borderline meaningless and limiting term. Are we now super human as a species relative to ourselves just five years ago because of our advances in developing computer programs that better imitate what many (but far from all) of us were already capable of doing? Have we reached a limit to human potential that can only be surpassed by digital machines? Who decides what human level is and when we have surpassed it? I have seen some ridiculous claims about ai in art that don&#x27;t stand up to even the slightest scrutiny by domain experts but that easily fool the masses.</div><br/></div></div></div></div><div id="42477937" class="c"><input type="checkbox" id="c-42477937" checked=""/><div class="controls bullet"><span class="by">vicentwu</span><span>|</span><a href="#42473810">prev</a><span>|</span><a href="#42473419">next</a><span>|</span><label class="collapse" for="c-42477937">[-]</label><label class="expand" for="c-42477937">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Note on &quot;tuned&quot;: OpenAI shared they trained the o3 we tested on 75% of the Public Training set. They have not shared more details. We have not yet tested the ARC-untrained model to understand how much of the performance is due to ARC-AGI data.&quot;<p>Really want to see the number of training pairs needed to achieve this socre. If it only takes a few pairs, say 100 pairs, I would say it is amazing!</div><br/></div></div><div id="42473419" class="c"><input type="checkbox" id="c-42473419" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#42477937">prev</a><span>|</span><a href="#42476532">next</a><span>|</span><label class="collapse" for="c-42473419">[-]</label><label class="expand" for="c-42473419">[68 more]</label></div><br/><div class="children"><div class="content">Congratulations to Francois Chollet on making the most interesting and challenging LLM benchmark so far.<p>A lot of people have criticized ARC as not being relevant or indicative of true reasoning, but I think it was exactly the right thing. The fact that scaled reasoning models are finally showing progress on ARC proves that what it measures really is relevant and important for reasoning.<p>It&#x27;s obvious to everyone that these models can&#x27;t perform as well as humans on everyday tasks despite blowout scores on the hardest tests we give to humans. Yet nobody could quantify exactly the ways the models were deficient. ARC is the best effort in that direction so far.<p>We don&#x27;t need more &quot;hard&quot; benchmarks. What we need right now are &quot;easy&quot; benchmarks that these models nevertheless fail. I hope Francois has something good cooked up for ARC 2!</div><br/><div id="42478318" class="c"><input type="checkbox" id="c-42478318" checked=""/><div class="controls bullet"><span class="by">internet_points</span><span>|</span><a href="#42473419">parent</a><span>|</span><a href="#42475261">next</a><span>|</span><label class="collapse" for="c-42478318">[-]</label><label class="expand" for="c-42478318">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The fact that scaled reasoning models are finally showing progress on ARC proves that what it measures really is relevant and important for reasoning.<p>One might also interpret that as &quot;the fact that models which are studying to the test are getting better at the test&quot; (Goodhart&#x27;s law), not that they&#x27;re actually reasoning.</div><br/></div></div><div id="42475261" class="c"><input type="checkbox" id="c-42475261" checked=""/><div class="controls bullet"><span class="by">adamgordonbell</span><span>|</span><a href="#42473419">parent</a><span>|</span><a href="#42478318">prev</a><span>|</span><a href="#42474960">next</a><span>|</span><label class="collapse" for="c-42475261">[-]</label><label class="expand" for="c-42475261">[10 more]</label></div><br/><div class="children"><div class="content">There is a benchmark, NovelQA, that LLMs don&#x27;t dominate when it feels like they should. The benchmark is to read a novel and answer questions about it.<p>LLMs are below human evaluation, as I last looked, but it doesn&#x27;t get much attention.<p>Once it is passed, I&#x27;d like to see one that is solving the mystery in a mystery book right before it&#x27;s revealed.<p>We&#x27;d need unpublished mystery novels to use for that benchmark, but I think it gets at what I think of as reasoning.<p><a href="https:&#x2F;&#x2F;novelqa.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;novelqa.github.io&#x2F;</a></div><br/><div id="42476628" class="c"><input type="checkbox" id="c-42476628" checked=""/><div class="controls bullet"><span class="by">loxias</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475261">parent</a><span>|</span><a href="#42475414">next</a><span>|</span><label class="collapse" for="c-42476628">[-]</label><label class="expand" for="c-42476628">[1 more]</label></div><br/><div class="children"><div class="content">NovelQA is a great one!   I also like GSM-Symbolic -- a benchmark based on making _symbolic templates_ of quite easy questions, and sampling them repeatedly, varying things like which proper nouns are used, what order relevant details appear, how many irrelevant details (GSM-NoOp) and where they are in the question, things like that.<p>LLMs are far, _far_ below human on elementary problems, once you allow any variation and stop spoonfeeding perfectly phrased word problems. :)<p><a href="https:&#x2F;&#x2F;machinelearning.apple.com&#x2F;research&#x2F;gsm-symbolic" rel="nofollow">https:&#x2F;&#x2F;machinelearning.apple.com&#x2F;research&#x2F;gsm-symbolic</a><p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2410.05229" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2410.05229</a><p>Paper came out in October, I don&#x27;t think many have fully absorbed the implications.<p>It&#x27;s hard to take any of the claims of &quot;LLMs can do reasoning!&quot; seriously, once you understand that simply changing what names are used in a 8th grade math word problem can have dramatic impact on the accuracy.</div><br/></div></div><div id="42475414" class="c"><input type="checkbox" id="c-42475414" checked=""/><div class="controls bullet"><span class="by">meta_x_ai</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475261">parent</a><span>|</span><a href="#42476628">prev</a><span>|</span><a href="#42478007">next</a><span>|</span><label class="collapse" for="c-42475414">[-]</label><label class="expand" for="c-42475414">[2 more]</label></div><br/><div class="children"><div class="content">Looks like it&#x27;s not updated for nearly a year and I&#x27;m guessing Gemini 2.0 Flash with 2m context will simply crush it</div><br/><div id="42475771" class="c"><input type="checkbox" id="c-42475771" checked=""/><div class="controls bullet"><span class="by">adamgordonbell</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475414">parent</a><span>|</span><a href="#42478007">next</a><span>|</span><label class="collapse" for="c-42475771">[-]</label><label class="expand" for="c-42475771">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s true. They don&#x27;t have Claude 3.5 on there either. So maybe it&#x27;s not relevant anymore, but I&#x27;m not sure.<p>If so, let&#x27;s move on to the murder mysteries or more complex literary analysis.</div><br/></div></div></div></div><div id="42478007" class="c"><input type="checkbox" id="c-42478007" checked=""/><div class="controls bullet"><span class="by">usaar333</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475261">parent</a><span>|</span><a href="#42475414">prev</a><span>|</span><a href="#42476706">next</a><span>|</span><label class="collapse" for="c-42478007">[-]</label><label class="expand" for="c-42478007">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s an old leaderboard -- has no one checked any SOTA LLM in the last 8 months?</div><br/></div></div><div id="42476706" class="c"><input type="checkbox" id="c-42476706" checked=""/><div class="controls bullet"><span class="by">latency-guy2</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475261">parent</a><span>|</span><a href="#42478007">prev</a><span>|</span><a href="#42476115">next</a><span>|</span><label class="collapse" for="c-42476706">[-]</label><label class="expand" for="c-42476706">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;d like to see one that is solving the mystery in a mystery book right before it&#x27;s revealed.<p>I would think this is a not so good bench. Author does not write logically, they write for entertainment.</div><br/><div id="42476766" class="c"><input type="checkbox" id="c-42476766" checked=""/><div class="controls bullet"><span class="by">adamgordonbell</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42476706">parent</a><span>|</span><a href="#42476115">next</a><span>|</span><label class="collapse" for="c-42476766">[-]</label><label class="expand" for="c-42476766">[1 more]</label></div><br/><div class="children"><div class="content">So I&#x27;m thinking of something like Locked-room mystery where the idea is it&#x27;s solvable, and the reader is given a chance to solve.<p>The reason it seems like an interesting bench, is it&#x27;s a puzzle presented in a long context. Its like testing if an LLm is at Sherlock Holmes level of world and motivation modelling.</div><br/></div></div></div></div><div id="42476115" class="c"><input type="checkbox" id="c-42476115" checked=""/><div class="controls bullet"><span class="by">rowanG077</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475261">parent</a><span>|</span><a href="#42476706">prev</a><span>|</span><a href="#42475302">next</a><span>|</span><label class="collapse" for="c-42476115">[-]</label><label class="expand" for="c-42476115">[1 more]</label></div><br/><div class="children"><div class="content">Benchmark how? Is it good if the LLM can or can&#x27;t solve it?</div><br/></div></div><div id="42475302" class="c"><input type="checkbox" id="c-42475302" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475261">parent</a><span>|</span><a href="#42476115">prev</a><span>|</span><a href="#42474960">next</a><span>|</span><label class="collapse" for="c-42475302">[-]</label><label class="expand" for="c-42475302">[2 more]</label></div><br/><div class="children"><div class="content">Does it work on short stories, but not novels?  If so, then that&#x27;s just a minor question of context length that should self-resolve over time.</div><br/><div id="42475327" class="c"><input type="checkbox" id="c-42475327" checked=""/><div class="controls bullet"><span class="by">adamgordonbell</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475302">parent</a><span>|</span><a href="#42474960">next</a><span>|</span><label class="collapse" for="c-42475327">[-]</label><label class="expand" for="c-42475327">[1 more]</label></div><br/><div class="children"><div class="content">The books fit in the current long context models, so it&#x27;s not merely the context size constraint but the length is part of the issue, for sure.</div><br/></div></div></div></div></div></div><div id="42474960" class="c"><input type="checkbox" id="c-42474960" checked=""/><div class="controls bullet"><span class="by">jug</span><span>|</span><a href="#42473419">parent</a><span>|</span><a href="#42475261">prev</a><span>|</span><a href="#42475198">next</a><span>|</span><label class="collapse" for="c-42474960">[-]</label><label class="expand" for="c-42474960">[1 more]</label></div><br/><div class="children"><div class="content">I liked the SimpleQA benchmark that measures hallucinations. OpenAI models did surprisingly poorly, even o1. In fact, it looks like OpenAI often does well on benchmarks by taking the shortcut to be more risk prone than both Anthropic and Google.</div><br/></div></div><div id="42475198" class="c"><input type="checkbox" id="c-42475198" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#42473419">parent</a><span>|</span><a href="#42474960">prev</a><span>|</span><a href="#42475136">next</a><span>|</span><label class="collapse" for="c-42475198">[-]</label><label class="expand" for="c-42475198">[4 more]</label></div><br/><div class="children"><div class="content">Highly challenging for LLMs because it has nothing to do with language. LLMs and their training processes have all kinds of optimizations for language and how it&#x27;s presented.<p>This benchmark has done a wonderful job with marketing by picking a great name. It&#x27;s largely irrelevant for LLMs despite the fact it&#x27;s difficult.<p>Consider how much of the model is just noise for a task like this given the low amount of information in each token and the high embedding dimensions used in LLMs.</div><br/><div id="42476537" class="c"><input type="checkbox" id="c-42476537" checked=""/><div class="controls bullet"><span class="by">computerex</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475198">parent</a><span>|</span><a href="#42475136">next</a><span>|</span><label class="collapse" for="c-42476537">[-]</label><label class="expand" for="c-42476537">[3 more]</label></div><br/><div class="children"><div class="content">The benchmark is designed to test for AGI and intelligence, specifically the ability to solve novel problems.<p>If the hypothesis is that LLMs are the “computer” that drives the AGI then of course the benchmark is relevant in testing for AGI.<p>I don’t think you understand the benchmark and its motivation. ARC AGI benchmark problems are extremely easy and simple for humans. But LLMs fail spectacularly at them. Why they fail is irrelevant, the fact they fail though means that we don’t have AGI.</div><br/><div id="42476759" class="c"><input type="checkbox" id="c-42476759" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42476537">parent</a><span>|</span><a href="#42475136">next</a><span>|</span><label class="collapse" for="c-42476759">[-]</label><label class="expand" for="c-42476759">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The benchmark is designed to test for AGI and intelligence, specifically the ability to solve novel problems.<p>It&#x27;s a bunch of visual puzzles. They aren&#x27;t a test for AGI because it&#x27;s not general. If models (or any other system for that matter) could solve it, we&#x27;d be saying &quot;this is a stupid puzzle, it has no practical significance&quot;. It&#x27;s a test of some sort of specific intelligence. On top of that, the vast majority of blind people would fail - are they not generally intelligent?<p>The name is marketing hype.<p>The benchmark could be called &quot;random puzzles LLMs are not good at because they haven&#x27;t been optimized for it because it&#x27;s not valuable benchmark&quot;. Sure, it wasn&#x27;t designed <i>for</i> LLMs, but throwing LLMs at it and saying &quot;see?&quot; is dumb. We can throw in benchmarks for tennis playing, chess playing, video game playing, car driving and a bajillion other things while we are at it.</div><br/><div id="42477975" class="c"><input type="checkbox" id="c-42477975" checked=""/><div class="controls bullet"><span class="by">NateEag</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42476759">parent</a><span>|</span><a href="#42475136">next</a><span>|</span><label class="collapse" for="c-42477975">[-]</label><label class="expand" for="c-42477975">[1 more]</label></div><br/><div class="children"><div class="content">And all that is kind of irrelevant, because if LLMs were human-level general intelligence, they would solve all these questions correctly without blinking.<p>But they don&#x27;t. Not even the best ones.</div><br/></div></div></div></div></div></div></div></div><div id="42475136" class="c"><input type="checkbox" id="c-42475136" checked=""/><div class="controls bullet"><span class="by">zone411</span><span>|</span><a href="#42473419">parent</a><span>|</span><a href="#42475198">prev</a><span>|</span><a href="#42474781">next</a><span>|</span><label class="collapse" for="c-42475136">[-]</label><label class="expand" for="c-42475136">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the least interesting benchmark for language models among all they&#x27;ve released, especially now that we already had a large jump in its best scores this year. It might be more useful as a multimodal reasoning task since it clearly involves visual elements, but with o3 already performing so well, this has proven unnecessary. ARC-AGI served a very specific purpose well: showcasing tasks where humans easily outperformed language models, so these simple puzzles had their uses. But tasks like proving math theorems or programming are far more impactful.</div><br/><div id="42476408" class="c"><input type="checkbox" id="c-42476408" checked=""/><div class="controls bullet"><span class="by">versteegen</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475136">parent</a><span>|</span><a href="#42474781">next</a><span>|</span><label class="collapse" for="c-42476408">[-]</label><label class="expand" for="c-42476408">[1 more]</label></div><br/><div class="children"><div class="content">ARC wasn&#x27;t designed as a benchmark for LLMs, and it doesn&#x27;t make much sense to compare them on it since it&#x27;s the wrong modality. Even a MLM with image inputs can&#x27;t be expected to do well, since they&#x27;re nothing like 99.999% of the training data. The fact that even a text-only LLM can solve ARC problems with the proper framework is important, however.</div><br/></div></div></div></div><div id="42474781" class="c"><input type="checkbox" id="c-42474781" checked=""/><div class="controls bullet"><span class="by">skywhopper</span><span>|</span><a href="#42473419">parent</a><span>|</span><a href="#42475136">prev</a><span>|</span><a href="#42476242">next</a><span>|</span><label class="collapse" for="c-42474781">[-]</label><label class="expand" for="c-42474781">[6 more]</label></div><br/><div class="children"><div class="content">&quot;The fact that scaled reasoning models are finally showing progress on ARC proves that what it measures really is relevant and important for reasoning.&quot;<p>Not sure I understand how this follows. The fact that a certain type of model does well on a certain benchmark means that the benchmark is relevant for a real-world reasoning? That doesn&#x27;t make sense.</div><br/><div id="42475219" class="c"><input type="checkbox" id="c-42475219" checked=""/><div class="controls bullet"><span class="by">munchler</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474781">parent</a><span>|</span><a href="#42476461">next</a><span>|</span><label class="collapse" for="c-42475219">[-]</label><label class="expand" for="c-42475219">[4 more]</label></div><br/><div class="children"><div class="content">It shows objectively that the models are getting better at some form of reasoning, which is at least worth noting. Whether that improved reasoning is relevant for the real world is a different question.</div><br/><div id="42475494" class="c"><input type="checkbox" id="c-42475494" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475219">parent</a><span>|</span><a href="#42476461">next</a><span>|</span><label class="collapse" for="c-42475494">[-]</label><label class="expand" for="c-42475494">[3 more]</label></div><br/><div class="children"><div class="content">It shows objectively that one model got better at this specific kind of weird puzzle that doesn&#x27;t translate to anything because it is just a pointless pattern matching puzzle that can be trained for, just like anything else. In fact they specifically trained for it, they say so upfront.<p>It&#x27;s like the modern equivalent of saying &quot;oh when AI solves chess it&#x27;ll be as smart as a person, so it&#x27;s a good benchmark&quot; and we all know how that nonsense went.</div><br/><div id="42475745" class="c"><input type="checkbox" id="c-42475745" checked=""/><div class="controls bullet"><span class="by">munchler</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475494">parent</a><span>|</span><a href="#42476461">next</a><span>|</span><label class="collapse" for="c-42475745">[-]</label><label class="expand" for="c-42475745">[2 more]</label></div><br/><div class="children"><div class="content">Hmm, you could be right, but you could also be very wrong. Jury&#x27;s still out, so the next few years will be interesting.<p>Regarding the value of &quot;pointless pattern matching&quot; in particular, I would refer you to Douglas Hofstadter&#x27;s discussion of Bongard problems starting on page 652 of _Godel, Escher, Bach_. Money quote: &quot;I believe that the skill of solving Bongard [pattern recognition] problems lies very close to the core of &#x27;pure&#x27; intelligence, if there is such a thing.&quot;</div><br/><div id="42476373" class="c"><input type="checkbox" id="c-42476373" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475745">parent</a><span>|</span><a href="#42476461">next</a><span>|</span><label class="collapse" for="c-42476373">[-]</label><label class="expand" for="c-42476373">[1 more]</label></div><br/><div class="children"><div class="content">Well I certainly at least agree with that second part, the doubt if there is such a thing ;)<p>The problem with pattern matching of sequences and transformers as an architecture is that it&#x27;s something they&#x27;re explicitly designed to be good at with self attention. Translation is mainly matching patterns to equivalents in different languages, and continuing a piece of text is following a pattern that exists inside it. This is primarily why it&#x27;s so hard to draw a line between what an LLM actually understands and what it just wings naturally through pattern memorization and why everything about them is so controversial.<p>Honestly I was really surprised that all models did so poorly on ARC in general thus far, since it really should be something they ought to be superhuman at from the get-go. Probably more of a problem that it&#x27;s visual in concept than anything else.</div><br/></div></div></div></div></div></div></div></div><div id="42476461" class="c"><input type="checkbox" id="c-42476461" checked=""/><div class="controls bullet"><span class="by">bagels</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474781">parent</a><span>|</span><a href="#42475219">prev</a><span>|</span><a href="#42476242">next</a><span>|</span><label class="collapse" for="c-42476461">[-]</label><label class="expand" for="c-42476461">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t follow, faulty logic. The two are probably correlated though.</div><br/></div></div></div></div><div id="42476242" class="c"><input type="checkbox" id="c-42476242" checked=""/><div class="controls bullet"><span class="by">aimanbenbaha</span><span>|</span><a href="#42473419">parent</a><span>|</span><a href="#42474781">prev</a><span>|</span><a href="#42477656">next</a><span>|</span><label class="collapse" for="c-42476242">[-]</label><label class="expand" for="c-42476242">[1 more]</label></div><br/><div class="children"><div class="content">Because LLMs are on an off-ramp path towards AGI. A generally intelligent system can brute force its way with just memory.<p>Once a model recognizes a weakness through reasoning with CoT when posed to a certain problem and gets the agency to adapt to solve that problem that&#x27;s a precursor towards real AGI capability!</div><br/></div></div><div id="42477656" class="c"><input type="checkbox" id="c-42477656" checked=""/><div class="controls bullet"><span class="by">justanotherjoe</span><span>|</span><a href="#42473419">parent</a><span>|</span><a href="#42476242">prev</a><span>|</span><a href="#42474745">next</a><span>|</span><label class="collapse" for="c-42477656">[-]</label><label class="expand" for="c-42477656">[1 more]</label></div><br/><div class="children"><div class="content">i am confused cause this dataset is visual-based, and yet being used to measure &#x27;LLM&#x27;.  I feel like the visual nature of it was really the biggest hurdle to solving it.</div><br/></div></div><div id="42474745" class="c"><input type="checkbox" id="c-42474745" checked=""/><div class="controls bullet"><span class="by">lossolo</span><span>|</span><a href="#42473419">parent</a><span>|</span><a href="#42477656">prev</a><span>|</span><a href="#42473449">next</a><span>|</span><label class="collapse" for="c-42474745">[-]</label><label class="expand" for="c-42474745">[4 more]</label></div><br/><div class="children"><div class="content">&gt; making the most interesting and challenging LLM benchmark so far.<p>This[1] is currently the most challenging benchmark. I would like to see how O3 handles it, as O1 solved only 1%.<p>1. <a href="https:&#x2F;&#x2F;epoch.ai&#x2F;frontiermath&#x2F;the-benchmark" rel="nofollow">https:&#x2F;&#x2F;epoch.ai&#x2F;frontiermath&#x2F;the-benchmark</a></div><br/><div id="42474838" class="c"><input type="checkbox" id="c-42474838" checked=""/><div class="controls bullet"><span class="by">pynappo</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474745">parent</a><span>|</span><a href="#42475068">next</a><span>|</span><label class="collapse" for="c-42474838">[-]</label><label class="expand" for="c-42474838">[2 more]</label></div><br/><div class="children"><div class="content">Apparently o3 scored about 25%<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;SKBG1sqdyIU?t=4m40s" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;SKBG1sqdyIU?t=4m40s</a></div><br/><div id="42475025" class="c"><input type="checkbox" id="c-42475025" checked=""/><div class="controls bullet"><span class="by">FiberBundle</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474838">parent</a><span>|</span><a href="#42475068">next</a><span>|</span><label class="collapse" for="c-42475025">[-]</label><label class="expand" for="c-42475025">[1 more]</label></div><br/><div class="children"><div class="content">This is actually the result that I find way more impressive. Elite mathematicians think these problems are challenging and thought they were years away from being solvable by AI.</div><br/></div></div></div></div><div id="42475068" class="c"><input type="checkbox" id="c-42475068" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474745">parent</a><span>|</span><a href="#42474838">prev</a><span>|</span><a href="#42473449">next</a><span>|</span><label class="collapse" for="c-42475068">[-]</label><label class="expand" for="c-42475068">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right, I was wrong to say &quot;most challenging&quot; as there have been harder ones coming out recently. I think the correct statement would be &quot;most challenging long-standing benchmark&quot; as I don&#x27;t believe any other test designed in 2019 has resisted progress for so long. FrontierMath is only a month old. And of course the real key feature of ARC is that it is easy for humans. FrontierMath is (intentionally) not.</div><br/></div></div></div></div><div id="42473449" class="c"><input type="checkbox" id="c-42473449" checked=""/><div class="controls bullet"><span class="by">dtquad</span><span>|</span><a href="#42473419">parent</a><span>|</span><a href="#42474745">prev</a><span>|</span><a href="#42473597">next</a><span>|</span><label class="collapse" for="c-42473449">[-]</label><label class="expand" for="c-42473449">[8 more]</label></div><br/><div class="children"><div class="content">Are there any single-step non-reasoner models that do well on this benchmark?<p>I wonder how well the latest Claude 3.5 Sonnet does on this benchmark and if it&#x27;s near o1.</div><br/><div id="42473486" class="c"><input type="checkbox" id="c-42473486" checked=""/><div class="controls bullet"><span class="by">throwaway71271</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473449">parent</a><span>|</span><a href="#42473494">next</a><span>|</span><label class="collapse" for="c-42473486">[-]</label><label class="expand" for="c-42473486">[4 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>    | Name                                 | Semi-private eval | Public eval |
    |--------------------------------------|-------------------|-------------|
    | Jeremy Berman                        | 53.6%             | 58.5%       |
    | Akyürek et al.                       | 47.5%             | 62.8%       |
    | Ryan Greenblatt                      | 43%               | 42%         |
    | OpenAI o1-preview (pass@1)           | 18%               | 21%         |
    | Anthropic Claude 3.5 Sonnet (pass@1) | 14%               | 21%         |
    | OpenAI GPT-4o (pass@1)               | 5%                | 9%          |
    | Google Gemini 1.5 (pass@1)           | 4.5%              | 8%          |

</code></pre>
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2412.04604" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2412.04604</a></div><br/><div id="42474264" class="c"><input type="checkbox" id="c-42474264" checked=""/><div class="controls bullet"><span class="by">kandesbunzler</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473486">parent</a><span>|</span><a href="#42476252">next</a><span>|</span><label class="collapse" for="c-42474264">[-]</label><label class="expand" for="c-42474264">[2 more]</label></div><br/><div class="children"><div class="content">why is this missing the o1 release &#x2F; o1 pro models? Would love to know how much better they are</div><br/><div id="42476996" class="c"><input type="checkbox" id="c-42476996" checked=""/><div class="controls bullet"><span class="by">Freebytes</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474264">parent</a><span>|</span><a href="#42476252">next</a><span>|</span><label class="collapse" for="c-42476996">[-]</label><label class="expand" for="c-42476996">[1 more]</label></div><br/><div class="children"><div class="content">This might be because they are referencing single step, and I do not think o1 is single step.</div><br/></div></div></div></div><div id="42476252" class="c"><input type="checkbox" id="c-42476252" checked=""/><div class="controls bullet"><span class="by">aimanbenbaha</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473486">parent</a><span>|</span><a href="#42474264">prev</a><span>|</span><a href="#42473494">next</a><span>|</span><label class="collapse" for="c-42476252">[-]</label><label class="expand" for="c-42476252">[1 more]</label></div><br/><div class="children"><div class="content">Akyürek et al uses test-time compute.</div><br/></div></div></div></div><div id="42473494" class="c"><input type="checkbox" id="c-42473494" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473449">parent</a><span>|</span><a href="#42473486">prev</a><span>|</span><a href="#42473597">next</a><span>|</span><label class="collapse" for="c-42473494">[-]</label><label class="expand" for="c-42473494">[3 more]</label></div><br/><div class="children"><div class="content">Here are the results for base models[1]:<p><pre><code>  o3 (coming soon)  75.7% 82.8%
  o1-preview        18%   21%
  Claude 3.5 Sonnet 14%   21%
  GPT-4o            5%    9%
  Gemini 1.5        4.5%  8%
</code></pre>
Score (semi-private eval) &#x2F; Score (public eval)<p>[1]: <a href="https:&#x2F;&#x2F;arcprize.org&#x2F;2024-results" rel="nofollow">https:&#x2F;&#x2F;arcprize.org&#x2F;2024-results</a></div><br/><div id="42475326" class="c"><input type="checkbox" id="c-42475326" checked=""/><div class="controls bullet"><span class="by">Bjorkbat</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473494">parent</a><span>|</span><a href="#42474843">next</a><span>|</span><label class="collapse" for="c-42475326">[-]</label><label class="expand" for="c-42475326">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s easy to miss, but if you look closely at the first sentence of the announcement they mention that they used a version of o3 trained on a public dataset of ARC-AGI, so technically it doesn&#x27;t belong on this list.</div><br/></div></div><div id="42474843" class="c"><input type="checkbox" id="c-42474843" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473494">parent</a><span>|</span><a href="#42475326">prev</a><span>|</span><a href="#42473597">next</a><span>|</span><label class="collapse" for="c-42474843">[-]</label><label class="expand" for="c-42474843">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to know how Claude 3.5 Sonnet does so well despite (presumably) not having the same tricks as the o-series models.</div><br/></div></div></div></div></div></div><div id="42473597" class="c"><input type="checkbox" id="c-42473597" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42473419">parent</a><span>|</span><a href="#42473449">prev</a><span>|</span><a href="#42475820">next</a><span>|</span><label class="collapse" for="c-42473597">[-]</label><label class="expand" for="c-42473597">[28 more]</label></div><br/><div class="children"><div class="content">This emphasizes persons and a self-conceived victory narrative over the ground truth.<p>Models have regularly made progress on it, this is not new with the o-series.<p>Doing astoundingly well on it, and having a mutually shared PR interest with OpenAI in this instance, doesn&#x27;t mean a pile of visual puzzles is actually AGI or some well thought out and designed benchmark of True Intelligence(tm). It&#x27;s one type of visual puzzle.<p>I don&#x27;t mean to be negative, but to inject a memento mori. Real story is some guys get together and ride off Chollet&#x27;s name with some visual puzzles from ye olde IQ test, and the deal was Chollet then gets to show up and say it proves program synthesis is required for True Intelligence.<p>Getting this score is extremely impressive but I don&#x27;t assign more signal to it than any other  benchmark with some thought to it.</div><br/><div id="42473688" class="c"><input type="checkbox" id="c-42473688" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473597">parent</a><span>|</span><a href="#42475145">next</a><span>|</span><label class="collapse" for="c-42473688">[-]</label><label class="expand" for="c-42473688">[25 more]</label></div><br/><div class="children"><div class="content">Solving ARC doesn&#x27;t mean we have AGI. Also o3 presumably isn&#x27;t doing program synthesis, seemingly proving Francois wrong on that front. (Not sure I believe the speculation about o3&#x27;s internals in the link.)<p>What I&#x27;m saying is the fact that as models are getting better at reasoning they are also scoring better on ARC proves that it <i>is</i> measuring something relating to reasoning. And nobody else has come up with a comparable benchmark that is so easy for humans and so hard for LLMs. Even today, let alone five years ago when ARC was released. ARC was visionary.</div><br/><div id="42476568" class="c"><input type="checkbox" id="c-42476568" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473688">parent</a><span>|</span><a href="#42473808">next</a><span>|</span><label class="collapse" for="c-42476568">[-]</label><label class="expand" for="c-42476568">[1 more]</label></div><br/><div class="children"><div class="content">&gt; o3 presumably isn&#x27;t doing program synthesis<p>I&#x27;d guess it&#x27;s doing natural language procedural synthesis, the same way a human might (i.e. figuring the sequence of steps to effect the transformation), but it may well be doing (sub-)solution verification by using the procedural description to generate code whose output can then be compared to the provided examples.<p>While OpenAI haven&#x27;t said exactly what the architecture of o1&#x2F;o3 are, the gist of it is pretty clear - basically adding &quot;tree&quot; search and iteration on top of the underlying LLM, driven by some RL-based post-training that imparts generic problem solving biases to the model. Maybe there is a separate model orchestrating the search and solution evaluation.<p>I think there are many tasks that are easy enough for humans but hard&#x2F;impossible for these models - the ultimate one in terms of commercial value would be to take an &quot;off the shelf model&quot; and treat it as an intern&#x2F;apprentice and teach it to become competent in a entire job it was never trained on. Have it participate in team meetings and communications, and become a drop-in replacement for a human performing that job (any job that an be performed remotely without a physical presence).</div><br/></div></div><div id="42473808" class="c"><input type="checkbox" id="c-42473808" checked=""/><div class="controls bullet"><span class="by">hdjjhhvvhga</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473688">parent</a><span>|</span><a href="#42476568">prev</a><span>|</span><a href="#42473847">next</a><span>|</span><label class="collapse" for="c-42473808">[-]</label><label class="expand" for="c-42473808">[5 more]</label></div><br/><div class="children"><div class="content">Your argumentation seems convincing but I&#x27;d like to offer a competitive narrative: any benchmark that is public becomes completely useless because companies optimize for it - especially AI that depends on piles of money and they need some proof they are developing.<p>That&#x27;s why I have some private benchmarks and I&#x27;m sorry to say that the transition from GTP4 to o1 wasn&#x27;t unambiguously a step forward (in some tasks yes, in some not).<p>On the other hand, private benchmarks are even less useful to the general public than the public ones, so we have to deal with what we have - but many of us just treat it as noise and don&#x27;t give it much significance. Ultimately, the models should defend themselves by performing the tasks individual users want them to do.</div><br/><div id="42473961" class="c"><input type="checkbox" id="c-42473961" checked=""/><div class="controls bullet"><span class="by">stonemetal12</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473808">parent</a><span>|</span><a href="#42473847">next</a><span>|</span><label class="collapse" for="c-42473961">[-]</label><label class="expand" for="c-42473961">[4 more]</label></div><br/><div class="children"><div class="content">Rather any Logic puzzle you post on the internet as something AIs are bad at is in the next round of training data so AIs get better at that specific question.  Not because AI companies are optimizing for a benchmark but because they suck up everything.</div><br/><div id="42474071" class="c"><input type="checkbox" id="c-42474071" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473961">parent</a><span>|</span><a href="#42473847">next</a><span>|</span><label class="collapse" for="c-42474071">[-]</label><label class="expand" for="c-42474071">[3 more]</label></div><br/><div class="children"><div class="content">ARC has two test sets that are not posted on the Internet. One is kept completely private and never shared. It is used when testing open source models and the models are run locally with no internet access. The other test set is used when testing closed source models that are only available as APIs. So it could be leaked in theory, but it is still not posted on the internet and can&#x27;t be in any web crawls.<p>You could argue that the models can get an advantage by looking at the training set which is on the internet. But all of the tasks are unique and generalizing from the training set to the test set is the whole point of the benchmark. So it&#x27;s not a serious objection.</div><br/><div id="42476419" class="c"><input type="checkbox" id="c-42476419" checked=""/><div class="controls bullet"><span class="by">foobiekr</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474071">parent</a><span>|</span><a href="#42473847">next</a><span>|</span><label class="collapse" for="c-42476419">[-]</label><label class="expand" for="c-42476419">[2 more]</label></div><br/><div class="children"><div class="content">Given the delivery mechanism for OpenAI, how do they actually keep it private?</div><br/><div id="42476607" class="c"><input type="checkbox" id="c-42476607" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42476419">parent</a><span>|</span><a href="#42473847">next</a><span>|</span><label class="collapse" for="c-42476607">[-]</label><label class="expand" for="c-42476607">[1 more]</label></div><br/><div class="children"><div class="content">&gt; So it could be leaked in theory<p>That&#x27;s why they have two test sets. But OpenAI has legally committed to not training on data passed to the API. I don&#x27;t believe OpenAI would burn their reputation and risk legal action just to cheat on ARC. And what they&#x27;ve reported is not implausible IMO.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42473847" class="c"><input type="checkbox" id="c-42473847" checked=""/><div class="controls bullet"><span class="by">QuantumGood</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473688">parent</a><span>|</span><a href="#42473808">prev</a><span>|</span><a href="#42474180">next</a><span>|</span><label class="collapse" for="c-42473847">[-]</label><label class="expand" for="c-42473847">[10 more]</label></div><br/><div class="children"><div class="content">Gaming the benchmarks usually needs to be considered first when evaluating new results.</div><br/><div id="42474053" class="c"><input type="checkbox" id="c-42474053" checked=""/><div class="controls bullet"><span class="by">bubblyworld</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473847">parent</a><span>|</span><a href="#42473949">next</a><span>|</span><label class="collapse" for="c-42474053">[-]</label><label class="expand" for="c-42474053">[1 more]</label></div><br/><div class="children"><div class="content">I think gaming the benchmarks is <i>encouraged</i> in the ARC AGI context. If you look at the public test cases you&#x27;ll see they test a ton of pretty abstract concepts - space, colour, basic laws of physics like gravity&#x2F;magnetism, movement, identity and lots of other stuff (highly recommend exploring them). Getting an AI to do well <i>at all</i>, regardless of whether it was gamed or not, is the whole challenge!</div><br/></div></div><div id="42473949" class="c"><input type="checkbox" id="c-42473949" checked=""/><div class="controls bullet"><span class="by">chaps</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473847">parent</a><span>|</span><a href="#42474053">prev</a><span>|</span><a href="#42474180">next</a><span>|</span><label class="collapse" for="c-42473949">[-]</label><label class="expand" for="c-42473949">[8 more]</label></div><br/><div class="children"><div class="content">Honestly, is gaming benchmarks actually a problem in this space in that it still shows something useful? Just means we need more benchmarks, yeah? It really feels not unlike keggle competitions.<p>We do the same exact stuff with real people with programming challenges and such where people just study common interview questions rather than learning the material holistically. And since we know that people game these interview type questions, we can adjust the interview processes to minimize gamification.... which itself leads to gamification and back to step one. That&#x27;s not ideal an ideal feedback loop of course, but people still get jobs and churn out &quot;productive work&quot; out of it.</div><br/><div id="42474013" class="c"><input type="checkbox" id="c-42474013" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473949">parent</a><span>|</span><a href="#42474180">next</a><span>|</span><label class="collapse" for="c-42474013">[-]</label><label class="expand" for="c-42474013">[7 more]</label></div><br/><div class="children"><div class="content">AI are very good at gaming benchmarks. Both as overfitting and as Goodhart&#x27;s law, gaming benchmarks has been a core problem during training for as long as I&#x27;ve been interested in the field.<p>Sometimes this manifests as &quot;outside the box thinking&quot;, like how a genetic algorithm got an &quot;oscillator&quot; which was really just an antenna.<p>It is a hard problem, and yes we still both need and can make more and better benchmarks; but it&#x27;s still a problem because it means the benchmarks we do have are overstating competence.</div><br/><div id="42474217" class="c"><input type="checkbox" id="c-42474217" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474013">parent</a><span>|</span><a href="#42474378">next</a><span>|</span><label class="collapse" for="c-42474217">[-]</label><label class="expand" for="c-42474217">[5 more]</label></div><br/><div class="children"><div class="content">The <i>idea</i> behind this particular benchmark, at least, is that it can&#x27;t be gamed.  What are some ways to game ARC-AGI, meaning to pass it without developing the required internal model and insights?<p>In principle you can&#x27;t optimize specifically for ARC-AGI, train against it, or overfit to it, because only a few of the puzzles are publicly disclosed.<p>Whether it lives up to that goal, I don&#x27;t know, but their approach sounded good when I first heard about it.</div><br/><div id="42474674" class="c"><input type="checkbox" id="c-42474674" checked=""/><div class="controls bullet"><span class="by">psb217</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474217">parent</a><span>|</span><a href="#42474378">next</a><span>|</span><label class="collapse" for="c-42474674">[-]</label><label class="expand" for="c-42474674">[4 more]</label></div><br/><div class="children"><div class="content">Well, with billions in funding you could task a hundred or so very well paid researchers to do their best at reverse engineering the general thought process which went into ARC-AGI, and then generate fresh training data and labeled CoTs until the numbers go up.</div><br/><div id="42474722" class="c"><input type="checkbox" id="c-42474722" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474674">parent</a><span>|</span><a href="#42474378">next</a><span>|</span><label class="collapse" for="c-42474722">[-]</label><label class="expand" for="c-42474722">[3 more]</label></div><br/><div class="children"><div class="content">Right, but the ARC-AGI people would counter by saying they&#x27;re welcome to do just that.  In doing so -- again in their view -- the researchers would create a model that could be considered capable of AGI.<p>I spent a couple of hours looking at the publicly-available puzzles, and was really impressed at how much room for creativity the format provides.  Supposedly the puzzles are &quot;easy for humans,&quot; but some of them were not... at least not for me.<p>(It did occur to me that a better test of AGI might be the ability to generate new, innovative ARC-AGI puzzles.)</div><br/><div id="42476547" class="c"><input type="checkbox" id="c-42476547" checked=""/><div class="controls bullet"><span class="by">psb217</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474722">parent</a><span>|</span><a href="#42474906">next</a><span>|</span><label class="collapse" for="c-42476547">[-]</label><label class="expand" for="c-42476547">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s tricky to judge the difficulty of these sorts of things. Eg, breadth of possibilities isn&#x27;t an automatic sign of difficulty. I imagine the space of programming problems permits as much variety as ARC-AGI, but since we&#x27;re more familiar with problems presented as natural language descriptions of programming tasks, and since we know there&#x27;s tons of relevant text on the web, we see the abstract pictographic ARC-AGI tasks as more novel, challenging, etc. But, to an LLM, any task we can conceive of will be (roughly) as familiar as the amount of relevant training data it&#x27;s seen. It&#x27;s legitimately hard to internalize this.<p>For a space of tasks which are well-suited to programmatic generation, as ARC-AGI is by design, if we can do a decent job of reverse engineering the underlying problem generating grammar, then we can make an LLM as familiar with the task as we&#x27;re willing to spend on compute.<p>To be clear, I&#x27;m not saying solving these sorts of tasks is unimpressive. I&#x27;m saying that I find it unsuprising (in light of past results) and not that strong of a signal about further progress towards the singularity, or FOOM, or whatever. For any of these closed-ish domain tasks, I feel a bit like they&#x27;re solving Go for the umpteenth time. We now know that if you collect enough relevant training data and train a big enough model with enough GPUs, the training loss will go down and you&#x27;ll probably get solid performance on the test set. Trillions of reasonably diverse training tokens buys you a lot of generalization. Ie, supervised learning works. This is the horse Ilya Sutskever&#x27;s ridden to many glorious victories and the big driver of OpenAI&#x27;s success -- a firm belief that other folks were leaving A LOT of performance on the table due to a lack of belief in the power of their own inventions.</div><br/></div></div></div></div></div></div></div></div><div id="42474378" class="c"><input type="checkbox" id="c-42474378" checked=""/><div class="controls bullet"><span class="by">chaps</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474013">parent</a><span>|</span><a href="#42474217">prev</a><span>|</span><a href="#42474180">next</a><span>|</span><label class="collapse" for="c-42474378">[-]</label><label class="expand" for="c-42474378">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re in agreement!<p>What&#x27;s endlessly interesting to me with all of this is how surprisingly quick the benchmarking feedback loops have become plus the level of scrutiny each one receives. We (as a culture&#x2F;society&#x2F;whatever) don&#x27;t really treat human benchmarking criteria with the same scrutiny such that feedback loops are useful and lead to productive changes to the benchmarking system itself. So from that POV it feels like substantial progress continues to be made through these benchmarks.</div><br/></div></div></div></div></div></div></div></div><div id="42474180" class="c"><input type="checkbox" id="c-42474180" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473688">parent</a><span>|</span><a href="#42473847">prev</a><span>|</span><a href="#42475145">next</a><span>|</span><label class="collapse" for="c-42474180">[-]</label><label class="expand" for="c-42474180">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Solving ARC doesn&#x27;t mean we have AGI. Also o3 presumably isn&#x27;t doing program synthesis, seemingly proving Francois wrong on that front.<p>Agreed.<p>&gt; And nobody else has come up with a comparable benchmark that is so easy for humans and so hard for LLMs.<p>? There&#x27;s plenty.</div><br/><div id="42474336" class="c"><input type="checkbox" id="c-42474336" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474180">parent</a><span>|</span><a href="#42475145">next</a><span>|</span><label class="collapse" for="c-42474336">[-]</label><label class="expand" for="c-42474336">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to hear about more. Which ones are you thinking of?</div><br/><div id="42474585" class="c"><input type="checkbox" id="c-42474585" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474336">parent</a><span>|</span><a href="#42475145">next</a><span>|</span><label class="collapse" for="c-42474585">[-]</label><label class="expand" for="c-42474585">[6 more]</label></div><br/><div class="children"><div class="content">-  &quot;Are You Human&quot; <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2410.09569" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2410.09569</a> is designed to be directly on target, i.e. cross cutting set of questions that are easy for humans, but challenging for LLMs, Instead of one type of visual puzzle. Much better than ARC for the purpose you&#x27;re looking for.<p>-  SimpleBench <a href="https:&#x2F;&#x2F;simple-bench.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simple-bench.com&#x2F;</a> (similar to above; great landing page w&#x2F;scores that show human &#x2F; ai gap)<p>-  PIQA (physical question answering, i.e. &quot;how do i get a yolk out of a water bottle&quot;, common favorite of local llm enthusiasts in &#x2F;r&#x2F;localllama <a href="https:&#x2F;&#x2F;paperswithcode.com&#x2F;dataset&#x2F;piqa" rel="nofollow">https:&#x2F;&#x2F;paperswithcode.com&#x2F;dataset&#x2F;piqa</a><p>- Berkeley Function-Calling (I prefer <a href="https:&#x2F;&#x2F;gorilla.cs.berkeley.edu&#x2F;leaderboard.html" rel="nofollow">https:&#x2F;&#x2F;gorilla.cs.berkeley.edu&#x2F;leaderboard.html</a>)<p>AI search googled &quot;llm benchmarks challenging for ai easy for humans&quot;, and &quot;language model benchmarks that humans excel at but ai struggles with&quot;, and &quot;tasks that are easy for humans but difficult for natural language ai&quot;.<p>It also mentioned Moravec&#x27;s Paradox is a known framing of this concept, started going down that rabbit hole because the resources were fascinating, but, had to hold back and submit this reply first. :)</div><br/><div id="42475208" class="c"><input type="checkbox" id="c-42475208" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474585">parent</a><span>|</span><a href="#42475474">next</a><span>|</span><label class="collapse" for="c-42475208">[-]</label><label class="expand" for="c-42475208">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for the pointers! I hadn&#x27;t seen Are You Human. Looks like it&#x27;s only two months old. Of course it is much easier to design a test specifically to thwart LLMs now that we have them. It seems to me that it is designed to exploit details of LLM structure like tokenizers (e.g. character counting tasks) rather than to provide any sort of general reasoning benchmark. As such it seems relatively straightforward to improve performance in ways that wouldn&#x27;t necessarily represent progress in general reasoning. And today&#x27;s LLMs are not nearly as far from human performance on the benchmark as they were on ARC for many years after it was released.<p>SimpleBench looks more interesting. Also less than two months old. It doesn&#x27;t look as challenging for LLMs as ARC, since o1-preview and Sonnet 3.5 already got half of the human baseline score; they did much worse on ARC. But I like the direction!<p>PIQA is cool but not hard enough for LLMs.<p>I&#x27;m not sure Berkeley Function-Calling represents tasks that are &quot;easy&quot; for average humans. Maybe programmers could perform well on it. But I like ARC in part because the tasks do seem like they should be quite straightforward even for non-expert humans.<p>Moravec&#x27;s paradox isn&#x27;t a benchmark per se. I tend to believe that there is no real paradox and all we need is larger datasets to see the same scaling laws that we have for LLMs. I see good evidence in this direction: <a href="https:&#x2F;&#x2F;www.physicalintelligence.company&#x2F;blog&#x2F;pi0" rel="nofollow">https:&#x2F;&#x2F;www.physicalintelligence.company&#x2F;blog&#x2F;pi0</a></div><br/><div id="42475995" class="c"><input type="checkbox" id="c-42475995" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475208">parent</a><span>|</span><a href="#42475474">next</a><span>|</span><label class="collapse" for="c-42475995">[-]</label><label class="expand" for="c-42475995">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;I&#x27;m not sure Berkeley Function-Calling represents tasks that are easy for average humans. Maybe programmers could perform well on it.&quot;<p>Functions in this context are not programming function calls. In this context, function calls are a now-deprecated LLM API name for &quot;parse input into this JSON template.&quot; No programmer experience needed. Entity extraction by another name, except, that&#x27;d be harder: here, you&#x27;re told up front exactly the set of entities to identify. :)<p>&gt; &quot;Moravec&#x27;s paradox isn&#x27;t a benchmark per se.&quot;<p>Yup! It&#x27;s a paradox :)<p>&gt; &quot;Of course it is much easier to design a test specifically to thwart LLMs now that we have them&quot;<p>Yes.<p>Though, I&#x27;m concerned a simple yes might be insufficient for illumination here.<p>It is a tautology (it&#x27;s easier to design a test that $X fails when you have access to $X), and it&#x27;s unlikely you meant to just share a tautology.<p>A potential unstated-but-maybe-intended-communication is &quot;it was hard to come up with ARC before LLMs existed&quot; --- LLMs existed in 2019 :)<p>If they didn&#x27;t, a hacky way to come up with a test that&#x27;s hard for the top AIs at the time, BERT-era, would be to use one type of visual puzzle.<p>If, for conversations sake, we ignore that it is exactly one type of visual puzzle, and that it wasn&#x27;t designed to be easy for humans, then we can engage with: &quot;its the only one thats easy for humans, but hard for LLMs&quot; --- this was demonstrated as untrue as well.<p>I don&#x27;t think I have much to contribute past that, once we&#x27;re at &quot;It is a singular example of a benchmark thats easy for humans but nigh-impossible for llms, at least in 2019, and this required singular insight&quot;, there&#x27;s just too much that&#x27;s not even wrong, in the Pauli sense, and it&#x27;s in a different universe from the original claims:<p>- &quot;Congratulations to Francois Chollet on making the most interesting and challenging LLM benchmark so far.&quot;<p>- &quot;A lot of people have criticized ARC as not being relevant or indicative of true reasoning...The fact that [o-series models show progress on ARC proves that what it measures really is relevant and important for reasoning.&quot;<p>- &quot;...nobody could quantify exactly the ways the models were deficient...&quot;<p>- &quot;What we need right now are &quot;easy&quot; benchmarks that these models nevertheless fail.&quot;</div><br/></div></div></div></div><div id="42475474" class="c"><input type="checkbox" id="c-42475474" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42474585">parent</a><span>|</span><a href="#42475208">prev</a><span>|</span><a href="#42475145">next</a><span>|</span><label class="collapse" for="c-42475474">[-]</label><label class="expand" for="c-42475474">[3 more]</label></div><br/><div class="children"><div class="content">How long has SimpleBench been posted?  Out of the first 6 questions at <a href="https:&#x2F;&#x2F;simple-bench.com&#x2F;try-yourself" rel="nofollow">https:&#x2F;&#x2F;simple-bench.com&#x2F;try-yourself</a>, o1-pro got 5&#x2F;6 right.<p>It was interesting to see how it failed on question 6:
<a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;c&#x2F;6765e70e-44b0-800b-97bd-928919f04fbe" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;c&#x2F;6765e70e-44b0-800b-97bd-928919f04fbe</a><p>Apparently LLMs do not consider global thermonuclear war to be all that big a deal, for better or worse.</div><br/><div id="42475619" class="c"><input type="checkbox" id="c-42475619" checked=""/><div class="controls bullet"><span class="by">Pannoniae</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475474">parent</a><span>|</span><a href="#42475145">next</a><span>|</span><label class="collapse" for="c-42475619">[-]</label><label class="expand" for="c-42475619">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t worry, I also got that wrong :) I thought her affair would be the biggest problem for John.</div><br/><div id="42476000" class="c"><input type="checkbox" id="c-42476000" checked=""/><div class="controls bullet"><span class="by">jquery</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42475619">parent</a><span>|</span><a href="#42475145">next</a><span>|</span><label class="collapse" for="c-42476000">[-]</label><label class="expand" for="c-42476000">[1 more]</label></div><br/><div class="children"><div class="content">John was an ex, not her partner. Tricky.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42475145" class="c"><input type="checkbox" id="c-42475145" checked=""/><div class="controls bullet"><span class="by">stego-tech</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473597">parent</a><span>|</span><a href="#42473688">prev</a><span>|</span><a href="#42475211">next</a><span>|</span><label class="collapse" for="c-42475145">[-]</label><label class="expand" for="c-42475145">[1 more]</label></div><br/><div class="children"><div class="content">I won&#x27;t be as brutal in my wording, but I agree with the sentiment. This was something drilled into me as someone with a hobby in PC Gaming <i>and</i> Photography: benchmarks, while handy measures of <i>potential</i> capabilities, are not <i>guarantees</i> of real world performance.  Very few PC gamers completely reinstall the OS before benchmarking to remove all potential cruft or performance impacts, just as very few photographers exclusively take photos of test materials.<p>While I appreciate the benchmark and its goals (not to mention the puzzles - I quite enjoy figuring them out), successfully passing this benchmark does not demonstrate or guarantee real world capabilities or performance.  This is why I increasingly side-eye this field and its obsession with constantly passing benchmarks and then moving the goal posts to a newer, harder benchmark that claims to be a better simulation of human capabilities than the last one: it reeks of squandered capital and a lack of a viable&#x2F;profitable product, at least to my sniff test.  Rather than simply capitalize on their actual accomplishments (which LLMs are - natural language interaction is huge!), they&#x27;re trying to prove to Capital that with a few (hundred) billion more in investments, they can make AGI out of this and replace all those expensive humans.<p>They&#x27;ve built the most advanced prediction engines ever conceived, and insist they&#x27;re best used to replace labor.  I&#x27;m not sure how they reached that conclusion, but considering even their own models refute this use case for LLMs, I doubt their execution ability on that lofty promise.</div><br/></div></div><div id="42475211" class="c"><input type="checkbox" id="c-42475211" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#42473419">root</a><span>|</span><a href="#42473597">parent</a><span>|</span><a href="#42475145">prev</a><span>|</span><a href="#42475820">next</a><span>|</span><label class="collapse" for="c-42475211">[-]</label><label class="expand" for="c-42475211">[1 more]</label></div><br/><div class="children"><div class="content">100%. The hype is misguided. I doubt half the people excited about the result have even looked at what the benchmark is.</div><br/></div></div></div></div></div></div><div id="42476532" class="c"><input type="checkbox" id="c-42476532" checked=""/><div class="controls bullet"><span class="by">__MatrixMan__</span><span>|</span><a href="#42473419">prev</a><span>|</span><a href="#42475012">next</a><span>|</span><label class="collapse" for="c-42476532">[-]</label><label class="expand" for="c-42476532">[34 more]</label></div><br/><div class="children"><div class="content">With only a 100x increase in cost, we improved performance by 0.1x and continued plotting this concave-down diminishing-returns type graph!  Hurray for logarithmic x-axes!<p>Joking aside, better than ever before at <i>any</i> cost is an achievement, it just doesn&#x27;t exactly scream &quot;breakthrough&quot; to me.</div><br/><div id="42476660" class="c"><input type="checkbox" id="c-42476660" checked=""/><div class="controls bullet"><span class="by">whalee</span><span>|</span><a href="#42476532">parent</a><span>|</span><a href="#42477533">next</a><span>|</span><label class="collapse" for="c-42476660">[-]</label><label class="expand" for="c-42476660">[2 more]</label></div><br/><div class="children"><div class="content">imo it&#x27;s a mistake to interpret the marginal increases in the upper echelons of benchmarks as materially marginal gains. Chess is an example. ELO narrows heavily at the top, but each ELO point carries more relative weight. This is a bit apples and oranges since chess is adversarial, but I think the point stands.</div><br/><div id="42477664" class="c"><input type="checkbox" id="c-42477664" checked=""/><div class="controls bullet"><span class="by">wavemode</span><span>|</span><a href="#42476532">root</a><span>|</span><a href="#42476660">parent</a><span>|</span><a href="#42477533">next</a><span>|</span><label class="collapse" for="c-42477664">[-]</label><label class="expand" for="c-42477664">[1 more]</label></div><br/><div class="children"><div class="content">&gt; ELO narrows heavily at the top<p>What do you mean by this? I&#x27;m assuming you&#x27;re not speaking about simple absolute differences in value - there have been top players rated over 100 points higher than the average of the rest of the top ten.</div><br/></div></div></div></div><div id="42477533" class="c"><input type="checkbox" id="c-42477533" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#42476532">parent</a><span>|</span><a href="#42476660">prev</a><span>|</span><a href="#42478164">next</a><span>|</span><label class="collapse" for="c-42477533">[-]</label><label class="expand" for="c-42477533">[1 more]</label></div><br/><div class="children"><div class="content">o3-mini (high) uses 1&#x2F;3rd of the compute of o1, and performs about 200 Elo higher than o1 on Codeforces.<p>o1 is the best code generation model according to Livebench.<p>So how is this not a breakthrough? It&#x27;s a genuine movement of the frontier.</div><br/></div></div><div id="42478164" class="c"><input type="checkbox" id="c-42478164" checked=""/><div class="controls bullet"><span class="by">handzhiev</span><span>|</span><a href="#42476532">parent</a><span>|</span><a href="#42477533">prev</a><span>|</span><a href="#42477054">next</a><span>|</span><label class="collapse" for="c-42478164">[-]</label><label class="expand" for="c-42478164">[1 more]</label></div><br/><div class="children"><div class="content">How much time does a top sprinter take a 100 m run for compared to a mediocre sprinter?</div><br/></div></div><div id="42477054" class="c"><input type="checkbox" id="c-42477054" checked=""/><div class="controls bullet"><span class="by">dyauspitr</span><span>|</span><a href="#42476532">parent</a><span>|</span><a href="#42478164">prev</a><span>|</span><a href="#42476632">next</a><span>|</span><label class="collapse" for="c-42477054">[-]</label><label class="expand" for="c-42477054">[2 more]</label></div><br/><div class="children"><div class="content">I mean going from 10% to 85% doesn’t seem like a 0.1% improvement</div><br/><div id="42477829" class="c"><input type="checkbox" id="c-42477829" checked=""/><div class="controls bullet"><span class="by">__MatrixMan__</span><span>|</span><a href="#42476532">root</a><span>|</span><a href="#42477054">parent</a><span>|</span><a href="#42476632">next</a><span>|</span><label class="collapse" for="c-42477829">[-]</label><label class="expand" for="c-42477829">[1 more]</label></div><br/><div class="children"><div class="content">Oh crap I made a mistake. I was comparing o3 low to o3 high.<p>I&#x27;m a little disappointed by all the upvotes I got for being flat wrong. I guess as long as you&#x27;re trashing AI you can get away with anything.<p>Really I was just trying to nitpick the chart parameters.</div><br/></div></div></div></div><div id="42476632" class="c"><input type="checkbox" id="c-42476632" checked=""/><div class="controls bullet"><span class="by">HDThoreaun</span><span>|</span><a href="#42476532">parent</a><span>|</span><a href="#42477054">prev</a><span>|</span><a href="#42476643">next</a><span>|</span><label class="collapse" for="c-42476632">[-]</label><label class="expand" for="c-42476632">[5 more]</label></div><br/><div class="children"><div class="content">compute gets cheaper and cheaper every year. This model will be in your phone by 2030 if we continue at the pace we&#x27;ve been at the last few years.</div><br/><div id="42477496" class="c"><input type="checkbox" id="c-42477496" checked=""/><div class="controls bullet"><span class="by">hajile</span><span>|</span><a href="#42476532">root</a><span>|</span><a href="#42476632">parent</a><span>|</span><a href="#42476773">next</a><span>|</span><label class="collapse" for="c-42477496">[-]</label><label class="expand" for="c-42477496">[1 more]</label></div><br/><div class="children"><div class="content">These models are nearing 2+ trillion parameters. At 4 bits each, we&#x27;re talking about somewhere around 1tb of RAM.<p>The problem is that RAM stopped scaling a long time ago now. We&#x27;re down to the size where a single capacitor&#x27;s charge is held by a mere 40,000 or so electrons and all we&#x27;ve been doing is making skinnier, longer cells of that size because we can&#x27;t find reliable ways to boost even weaker signals, but this is a dead end because as the math shows, if the volume is consistent and you are reducing X and Y dimensions, that Z dimension starts to get crazy big really fast. The chemistry issues of burning a hole a little at a time while keeping wall thickness somewhat similar all the way down is a very hard problem.<p>Another problem is that Moore&#x27;s law hit a wall when Dennard Scaling failed. When you look at SRAM (it&#x27;s generally the smallest and most reliable stuff we can make), you see that most recent shrinks can hardly be called shrinks.<p>Unless we do something very different like compute in storage or have some radical breakthrough in a new technology, I don&#x27;t know that we will ever get a 2T parameter model inside a phone (I&#x27;d love for someone in 10 years to show up and say how wrong I was).</div><br/></div></div><div id="42476773" class="c"><input type="checkbox" id="c-42476773" checked=""/><div class="controls bullet"><span class="by">agentultra</span><span>|</span><a href="#42476532">root</a><span>|</span><a href="#42476632">parent</a><span>|</span><a href="#42477496">prev</a><span>|</span><a href="#42476643">next</a><span>|</span><label class="collapse" for="c-42476773">[-]</label><label class="expand" for="c-42476773">[3 more]</label></div><br/><div class="children"><div class="content">There’s probably enough VC money to subsidize the costs for a few more years.<p>But the data centres running the training for models like this are bringing up new methane power plants at a fast rate at a time when we need to be reducing reliance on O&amp;G.<p>But let’s assume that the efficiency gains out pace the resource consumption with the help of all the subsidies being thrown in and we achieve AGI.<p>What’s the benefit? Do we get more fresh water?</div><br/><div id="42476933" class="c"><input type="checkbox" id="c-42476933" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#42476532">root</a><span>|</span><a href="#42476773">parent</a><span>|</span><a href="#42476886">next</a><span>|</span><label class="collapse" for="c-42476933">[-]</label><label class="expand" for="c-42476933">[1 more]</label></div><br/><div class="children"><div class="content">Politically anything can happen. Maybe the billionaire class controls everything with an army of robots and it&#x27;s a horrible prison-like dystopia, or maybe we end up in a post-scarcity utopia a la The Culture.<p>Regardless, once we have AGI (and it can scale), I don&#x27;t think O&amp;G reliance (&#x2F; climate change) is going to be something that we need concern ourselves with.</div><br/></div></div><div id="42476886" class="c"><input type="checkbox" id="c-42476886" checked=""/><div class="controls bullet"><span class="by">hamburga</span><span>|</span><a href="#42476532">root</a><span>|</span><a href="#42476773">parent</a><span>|</span><a href="#42476933">prev</a><span>|</span><a href="#42476643">next</a><span>|</span><label class="collapse" for="c-42476886">[-]</label><label class="expand" for="c-42476886">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, good question. I think it depends on our politics. If we’re in a techno-capital-oligarchy, people are going to have a hard time making fresh water a priority when the robots would prefer to build nuclear power everywhere and use it to desalinate sea water.<p>OTOH if these data centers are sufficiently decentralized and run for public benefit, maybe there’s a chance we use them to solve collective action problems.</div><br/></div></div></div></div></div></div><div id="42476595" class="c"><input type="checkbox" id="c-42476595" checked=""/><div class="controls bullet"><span class="by">kvetching</span><span>|</span><a href="#42476532">parent</a><span>|</span><a href="#42476643">prev</a><span>|</span><a href="#42475012">next</a><span>|</span><label class="collapse" for="c-42476595">[-]</label><label class="expand" for="c-42476595">[2 more]</label></div><br/><div class="children"><div class="content">It may eventually be able to solve any problem</div><br/><div id="42476651" class="c"><input type="checkbox" id="c-42476651" checked=""/><div class="controls bullet"><span class="by">iterance</span><span>|</span><a href="#42476532">root</a><span>|</span><a href="#42476595">parent</a><span>|</span><a href="#42475012">next</a><span>|</span><label class="collapse" for="c-42476651">[-]</label><label class="expand" for="c-42476651">[1 more]</label></div><br/><div class="children"><div class="content">Ah. Me, too.</div><br/></div></div></div></div></div></div><div id="42475012" class="c"><input type="checkbox" id="c-42475012" checked=""/><div class="controls bullet"><span class="by">w4</span><span>|</span><a href="#42476532">prev</a><span>|</span><a href="#42477276">next</a><span>|</span><label class="collapse" for="c-42475012">[-]</label><label class="expand" for="c-42475012">[5 more]</label></div><br/><div class="children"><div class="content">The cost to run the highest performance o3 model is estimated to be somewhere between $2,000 and $3,400 per task.[1] Based on these estimates, o3 costs about 100x what it would cost to have a human perform the exact same task. Many people are therefore dismissing the near-term impact of these models because of these extremely expensive costs.<p>I think this is a mistake.<p>Even if very high costs make o3 uneconomic for businesses, it could be an epoch defining development for nation states, assuming that it is true that o3 can reason like an averagely intelligent person.<p>Consider the following questions that a state actor might ask itself: What is the cost to raise and educate an average person? Correspondingly, what is the cost to build and run a datacenter with a nuclear power plant attached to it? And finally, how many person-equivilant AIs could be run in parallel per datacenter?<p>There are many state actors, corporations, and even individual people who can afford to ask these questions. There are also many things that they&#x27;d like to do but can&#x27;t because there just aren&#x27;t enough people available to do them. o3 might change that despite its high cost.<p>So <i>if</i> it is true that we&#x27;ve now got something like human-equivilant intelligence on demand - and that&#x27;s a really big if - then we may see its impacts much sooner than we would otherwise intuit, especially in areas where economics takes a back seat to other priorities like national security and state competitiveness.<p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42473876">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42473876</a></div><br/><div id="42475401" class="c"><input type="checkbox" id="c-42475401" checked=""/><div class="controls bullet"><span class="by">istjohn</span><span>|</span><a href="#42475012">parent</a><span>|</span><a href="#42477276">next</a><span>|</span><label class="collapse" for="c-42475401">[-]</label><label class="expand" for="c-42475401">[4 more]</label></div><br/><div class="children"><div class="content">Your economic analysis is deeply flawed. If there was anything that valuable and that required that much manpower, it would already have driven up the cost of labor accordingly. The one property that could conceivably justify a substantially higher cost is secrecy. After all, you can&#x27;t (legally) kill a human after your project ends to ensure total secrecy. But that takes us into thriller novel territory.</div><br/><div id="42475600" class="c"><input type="checkbox" id="c-42475600" checked=""/><div class="controls bullet"><span class="by">w4</span><span>|</span><a href="#42475012">root</a><span>|</span><a href="#42475401">parent</a><span>|</span><a href="#42476284">next</a><span>|</span><label class="collapse" for="c-42475600">[-]</label><label class="expand" for="c-42475600">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that&#x27;s right. Free societies don&#x27;t tolerate total mobilization by their governments outside of war time, no matter how valuable the outcomes might be in the long term, in part because of the very economic impacts you describe. Human-level AI - even if it&#x27;s very expensive - puts something that looks a lot like total mobilization within reach without the societal pushback. This is especially true when it comes to tasks that society as a whole may not sufficiently value, but that a state actor might value very much, and when paired with something like a co-located reactor and data center that does not impact the grid.<p>That said, this is all predicated on o3 or similar actually having achieved human level reasoning. That&#x27;s yet to be fully proven. We&#x27;ll see!</div><br/><div id="42477202" class="c"><input type="checkbox" id="c-42477202" checked=""/><div class="controls bullet"><span class="by">daemonologist</span><span>|</span><a href="#42475012">root</a><span>|</span><a href="#42475600">parent</a><span>|</span><a href="#42476284">next</a><span>|</span><label class="collapse" for="c-42477202">[-]</label><label class="expand" for="c-42477202">[1 more]</label></div><br/><div class="children"><div class="content">This is interesting to consider, but I think the flaw here is that you&#x27;d need a &quot;total mobilization&quot; level workforce in order to build this mega datacenter in the first place.  You put one human-hour into making B200s and cooling systems and power plants, you get less than one human-hour-equivalent of thinking back out.</div><br/></div></div></div></div><div id="42476284" class="c"><input type="checkbox" id="c-42476284" checked=""/><div class="controls bullet"><span class="by">lurking_swe</span><span>|</span><a href="#42475012">root</a><span>|</span><a href="#42475401">parent</a><span>|</span><a href="#42475600">prev</a><span>|</span><a href="#42477276">next</a><span>|</span><label class="collapse" for="c-42476284">[-]</label><label class="expand" for="c-42476284">[1 more]</label></div><br/><div class="children"><div class="content">i disagree because the job market is not a true free market. I mean it mostly is, but there’s a LOT of politics and shady stuff that employers do to purposely drive wages down. Even in the tech sector.<p>Your secrecy comment is really intriguing actually. And morbid lol.</div><br/></div></div></div></div></div></div><div id="42477276" class="c"><input type="checkbox" id="c-42477276" checked=""/><div class="controls bullet"><span class="by">hamburga</span><span>|</span><a href="#42475012">prev</a><span>|</span><a href="#42474032">next</a><span>|</span><label class="collapse" for="c-42477276">[-]</label><label class="expand" for="c-42477276">[6 more]</label></div><br/><div class="children"><div class="content">I’m not sure if people realize what a weird test  this is. They’re these simple visual puzzles that people can usually solve at a glance, but for the LLMs, they’re converted into a json format, and then the LLMs have to reconstruct the 2D visual scene from the json and pick up the patterns.<p>If humans were given the json as input rather than the images, they’d have a hard time, too.</div><br/><div id="42478300" class="c"><input type="checkbox" id="c-42478300" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#42477276">parent</a><span>|</span><a href="#42477945">next</a><span>|</span><label class="collapse" for="c-42478300">[-]</label><label class="expand" for="c-42478300">[1 more]</label></div><br/><div class="children"><div class="content">Not sure how much that matters - I&#x27;m not an AI expert, but I did some intro courses where we had to train a classifier to recognize digits. How it worked basically was that we fed each pixel of the 2d grid of the image into an input of the network, essentially flattening it in a similar fashion. 
It worked just fine, and that was a tiny network.</div><br/></div></div><div id="42477945" class="c"><input type="checkbox" id="c-42477945" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42477276">parent</a><span>|</span><a href="#42478300">prev</a><span>|</span><a href="#42477810">next</a><span>|</span><label class="collapse" for="c-42477945">[-]</label><label class="expand" for="c-42477945">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If humans were given the json as input rather than the images, they’d have a hard time, too.<p>We shine light in text patterns at humans rather than inject the text directly into the brain as well, that is extremely unfair! Imagine how much better humans would be at text processing if we injected and extracted information from their brains using the neurons instead of eyes and hands.</div><br/></div></div><div id="42477810" class="c"><input type="checkbox" id="c-42477810" checked=""/><div class="controls bullet"><span class="by">causal</span><span>|</span><a href="#42477276">parent</a><span>|</span><a href="#42477945">prev</a><span>|</span><a href="#42477577">next</a><span>|</span><label class="collapse" for="c-42477810">[-]</label><label class="expand" for="c-42477810">[1 more]</label></div><br/><div class="children"><div class="content">I think that&#x27;s part of what feels odd about this- in some ways it feels like the wrong type of test for an LLM, but in many ways it makes this achievement that much more remarkable</div><br/></div></div><div id="42477577" class="c"><input type="checkbox" id="c-42477577" checked=""/><div class="controls bullet"><span class="by">ImaCake</span><span>|</span><a href="#42477276">parent</a><span>|</span><a href="#42477810">prev</a><span>|</span><a href="#42474032">next</a><span>|</span><label class="collapse" for="c-42477577">[-]</label><label class="expand" for="c-42477577">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, this entire thread seems utterly detached from my lived experience. LLMs are immensely useful for me at work but they certainly don&#x27;t come close to the hype spouted by many commenters here. It would be great if it could handle more of our quite modest codebase but it&#x27;s not able to yet</div><br/><div id="42477766" class="c"><input type="checkbox" id="c-42477766" checked=""/><div class="controls bullet"><span class="by">m_ke</span><span>|</span><a href="#42477276">root</a><span>|</span><a href="#42477577">parent</a><span>|</span><a href="#42474032">next</a><span>|</span><label class="collapse" for="c-42477766">[-]</label><label class="expand" for="c-42477766">[1 more]</label></div><br/><div class="children"><div class="content">ARC is a silly benchmark, the other results in math and coding are much more impressive.<p>o3 is just o1 scaled up, the main takeaway from this line of work that people should walk away with is that we now have a proven way to RL our way to super human performance on tasks where it’s cheap to sample and easy to verify the final output. Programming falls in that category, they focused on known benchmarks but the same process can be done for normal programs, using parsers, compilers, existing functions and unit tests as verifiers.<p>Pre o1 we only really had next token prediction, which required high quality human produced data, with o1  you optimize for success instead of MLE of next token. Explained in simpler terms, it means it can get reward for any implementation of a function that reproduces the expected result, instead of the exact implementation in the training set.<p>Put another way, it’s just like RLHF but instead of optimizing against learned human preferences, the model is trained to satisfy a verifier.<p>This should work just as well in VLA models for robotics, self driving and computer agents.</div><br/></div></div></div></div></div></div><div id="42474032" class="c"><input type="checkbox" id="c-42474032" checked=""/><div class="controls bullet"><span class="by">phil917</span><span>|</span><a href="#42477276">prev</a><span>|</span><a href="#42476825">next</a><span>|</span><label class="collapse" for="c-42474032">[-]</label><label class="expand" for="c-42474032">[10 more]</label></div><br/><div class="children"><div class="content">Direct quote from the ARC-AGI blog:<p>“SO IS IT AGI?<p>ARC-AGI serves as a critical benchmark for detecting such breakthroughs, highlighting generalization power in a way that saturated or less demanding benchmarks cannot. However, it is important to note that ARC-AGI is not an acid test for AGI – as we&#x27;ve repeated dozens of times this year. It&#x27;s a research tool designed to focus attention on the most challenging unsolved problems in AI, a role it has fulfilled well over the past five years.<p>Passing ARC-AGI does not equate achieving AGI, and, as a matter of fact, I don&#x27;t think o3 is AGI yet. o3 still fails on some very easy tasks, indicating fundamental differences with human intelligence.<p>Furthermore, early data points suggest that the upcoming ARC-AGI-2 benchmark will still pose a significant challenge to o3, potentially reducing its score to under 30% even at high compute (while a smart human would still be able to score over 95% with no training). This demonstrates the continued possibility of creating challenging, unsaturated benchmarks without having to rely on expert domain knowledge. You&#x27;ll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible.”<p>The high compute variant sounds like it costed around *$350,000* which is kinda wild. Lol the blog post specifically mentioned how OpenAPI asked ARC-AGI to not disclose the exact cost for the high compute version.<p>Also, 1 odd thing I noticed is that the graph in their blog post shows the top 2 scores as “tuned” (this was not displayed in the live demo graph). This suggest in those cases that the model was trained to better handle these types of questions, so I do wonder about data &#x2F; answer contamination in those cases…</div><br/><div id="42474544" class="c"><input type="checkbox" id="c-42474544" checked=""/><div class="controls bullet"><span class="by">Bjorkbat</span><span>|</span><a href="#42474032">parent</a><span>|</span><a href="#42474709">next</a><span>|</span><label class="collapse" for="c-42474544">[-]</label><label class="expand" for="c-42474544">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Also, 1 odd thing I noticed is that the graph in their blog post shows the top 2 scores as “tuned”<p>Something I missed until I scrolled back to the top and reread the page was this<p>&gt; OpenAI&#x27;s new o3 system - trained on the ARC-AGI-1 Public Training set<p>So yeah, the results were specifically from a version of o3 trained on the public training set<p>Which on the one hand I think is a completely fair thing to do.  It&#x27;s reasonable that you should teach your AI the rules of the game, so to speak.  There really aren&#x27;t any spoken rules though, just pattern observation.  Thus, if you want to teach the AI how to play the game, you must train it.<p>On the other hand though, I don&#x27;t think the o1 models nor Claude were trained on the dataset, in which case it isn&#x27;t a completely fair competition.  If I had to guess, you could probably get 60% on o1 if you trained it on the public dataset as well.</div><br/><div id="42474783" class="c"><input type="checkbox" id="c-42474783" checked=""/><div class="controls bullet"><span class="by">skepticATX</span><span>|</span><a href="#42474032">root</a><span>|</span><a href="#42474544">parent</a><span>|</span><a href="#42475154">next</a><span>|</span><label class="collapse" for="c-42474783">[-]</label><label class="expand" for="c-42474783">[4 more]</label></div><br/><div class="children"><div class="content">Great catch. Super disappointing that AI companies continue to do things like this. It’s a great result either way but predictably the excitement is focused on the jump from o1, which is now in question.</div><br/><div id="42474979" class="c"><input type="checkbox" id="c-42474979" checked=""/><div class="controls bullet"><span class="by">Bjorkbat</span><span>|</span><a href="#42474032">root</a><span>|</span><a href="#42474783">parent</a><span>|</span><a href="#42475154">next</a><span>|</span><label class="collapse" for="c-42474979">[-]</label><label class="expand" for="c-42474979">[3 more]</label></div><br/><div class="children"><div class="content">To me it&#x27;s very frustrating because such little caveats make benchmarks less reliable.  Implicitly, benchmarks are no different from tests in that someone&#x2F;something who scores high on a benchmark&#x2F;test <i>should</i> be able to generalize that knowledge out into the real world.<p>While that is true with humans taking tests, it&#x27;s not really true with AIs evaluating on benchmarks.<p>SWE-bench is a great example.  Claude Sonnet can get something like a 50% on verified, whereas I think I might be able to score a 20-25%?  So, Claude is a better programmer than me.<p>Except that isn&#x27;t really true.  Claude can still make a lot of clumsy mistakes.  I wouldn&#x27;t even say these are junior engineer mistakes.  I&#x27;ve used it for creative programming tasks and have found one example where it tried to use a library written for d3js for a p5js programming example.  The confusion is kind of understandable, but it&#x27;s also a really dumb mistake.<p>Some very simple explanations, the models were probably overfitted to a degree on Python given its popularity in AI&#x2F;ML work, and SWE-bench is all Python.  Also, the underlying Github issues are quite old, so they probably contaminated the training data and the models have simply memorized the answers.<p>Or maybe benchmarks are just bad at measuring intelligence in general.<p>Regardless, every time a model beats a benchmark I&#x27;m annoyed by the fact that I have no clue whatsoever how much this actually translates into real world performance.  Did OpenAI&#x2F;Anthropic&#x2F;Google actually create something that will automate wide swathes of the software engineering profession?  Or did they create the world&#x27;s most knowledgeable junior engineer?</div><br/><div id="42475471" class="c"><input type="checkbox" id="c-42475471" checked=""/><div class="controls bullet"><span class="by">throwaway0123_5</span><span>|</span><a href="#42474032">root</a><span>|</span><a href="#42474979">parent</a><span>|</span><a href="#42475154">next</a><span>|</span><label class="collapse" for="c-42475471">[-]</label><label class="expand" for="c-42475471">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Some very simple explanations, the models were probably overfitted to a degree on Python given its popularity in AI&#x2F;ML work, and SWE-bench is all Python. Also, the underlying Github issues are quite old, so they probably contaminated the training data and the models have simply memorized the answers.<p>My understanding is that it works by checking if the proposed solution passes test-cases included in the original (human) PR. This seems to present some problems too, because there are surely ways to write code that passes the tests but would fail human review for one reason or another. It would be interesting to not only see the pass rate but also the rate at which the proposed solutions are preferred to the original ones (preferably evaluated by a human but even an LLM comparing the two solutions would be interesting).</div><br/><div id="42475523" class="c"><input type="checkbox" id="c-42475523" checked=""/><div class="controls bullet"><span class="by">Bjorkbat</span><span>|</span><a href="#42474032">root</a><span>|</span><a href="#42475471">parent</a><span>|</span><a href="#42475154">next</a><span>|</span><label class="collapse" for="c-42475523">[-]</label><label class="expand" for="c-42475523">[1 more]</label></div><br/><div class="children"><div class="content">If I recall correctly the authors of the benchmark did mention on Twitter that for certain issues models will submit an answer that technically passes the test but is kind of questionable, so yeah, good point.</div><br/></div></div></div></div></div></div></div></div><div id="42475154" class="c"><input type="checkbox" id="c-42475154" checked=""/><div class="controls bullet"><span class="by">phil917</span><span>|</span><a href="#42474032">root</a><span>|</span><a href="#42474544">parent</a><span>|</span><a href="#42474783">prev</a><span>|</span><a href="#42474709">next</a><span>|</span><label class="collapse" for="c-42475154">[-]</label><label class="expand" for="c-42475154">[1 more]</label></div><br/><div class="children"><div class="content">Lol I missed that even though it&#x27;s literally the first sentence of the blog, good catch.<p>Yeah, that makes this result a lot less impressive for me.</div><br/></div></div></div></div><div id="42474709" class="c"><input type="checkbox" id="c-42474709" checked=""/><div class="controls bullet"><span class="by">hartator</span><span>|</span><a href="#42474032">parent</a><span>|</span><a href="#42474544">prev</a><span>|</span><a href="#42476825">next</a><span>|</span><label class="collapse" for="c-42474709">[-]</label><label class="expand" for="c-42474709">[3 more]</label></div><br/><div class="children"><div class="content">&gt; acid test<p>The css acid test? This can be gamed too.</div><br/><div id="42476663" class="c"><input type="checkbox" id="c-42476663" checked=""/><div class="controls bullet"><span class="by">sundarurfriend</span><span>|</span><a href="#42474032">root</a><span>|</span><a href="#42474709">parent</a><span>|</span><a href="#42476825">next</a><span>|</span><label class="collapse" for="c-42476663">[-]</label><label class="expand" for="c-42476663">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Acid_test" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Acid_test</a>:<p>&gt; An acid test is a qualitative chemical or metallurgical assay utilizing acid. Historically, it often involved the use of a robust acid to distinguish gold from base metals. Figuratively, the term represents any definitive test for attributes, such as gauging a person&#x27;s character or evaluating a product&#x27;s performance.<p>Specifically here, they&#x27;re using the figurative sense of &quot;definitive test&quot;.</div><br/><div id="42477716" class="c"><input type="checkbox" id="c-42477716" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#42474032">root</a><span>|</span><a href="#42476663">parent</a><span>|</span><a href="#42476825">next</a><span>|</span><label class="collapse" for="c-42477716">[-]</label><label class="expand" for="c-42477716">[1 more]</label></div><br/><div class="children"><div class="content">also a &quot;litmus test&quot; but I guess that&#x27;s a different chemistry test...</div><br/></div></div></div></div></div></div></div></div><div id="42476825" class="c"><input type="checkbox" id="c-42476825" checked=""/><div class="controls bullet"><span class="by">ripped_britches</span><span>|</span><a href="#42474032">prev</a><span>|</span><a href="#42473525">next</a><span>|</span><label class="collapse" for="c-42476825">[-]</label><label class="expand" for="c-42476825">[5 more]</label></div><br/><div class="children"><div class="content">Sad to see everyone so focused on compute expense during this massive breakthrough. GPT-2 originally cost $50k to train, but now can be trained for ~$150.<p>The key part is that scaling test-time compute will likely be a key to achieving AGI&#x2F;ASI. Costs will definitely come down as is evidenced by precedents, Moore’s law, o3-mini being cheaper than o1 with improved performance, etc.</div><br/><div id="42477407" class="c"><input type="checkbox" id="c-42477407" checked=""/><div class="controls bullet"><span class="by">stocknoob</span><span>|</span><a href="#42476825">parent</a><span>|</span><a href="#42476843">next</a><span>|</span><label class="collapse" for="c-42477407">[-]</label><label class="expand" for="c-42477407">[1 more]</label></div><br/><div class="children"><div class="content">It’s wild, are people purposefully overlooking that inference costs are dropping 10-100x each year?<p><a href="https:&#x2F;&#x2F;a16z.com&#x2F;llmflation-llm-inference-cost&#x2F;" rel="nofollow">https:&#x2F;&#x2F;a16z.com&#x2F;llmflation-llm-inference-cost&#x2F;</a><p>Look at the log scale slope, especially the orange MMLU &gt; 83 data points.</div><br/></div></div><div id="42476843" class="c"><input type="checkbox" id="c-42476843" checked=""/><div class="controls bullet"><span class="by">yawnxyz</span><span>|</span><a href="#42476825">parent</a><span>|</span><a href="#42477407">prev</a><span>|</span><a href="#42477511">next</a><span>|</span><label class="collapse" for="c-42476843">[-]</label><label class="expand" for="c-42476843">[1 more]</label></div><br/><div class="children"><div class="content">I think the question everyone has in their minds isn&#x27;t &quot;when will AGI get here&quot; or even &quot;how soon will it get here&quot; — it&#x27;s &quot;how soon will AGI get so cheap that everyone will get their hands on it&quot;<p>that&#x27;s why everyone&#x27;s thinking about compute expense.
but I guess in terms of a &quot;lifetime expense of a person&quot; even someone who costs $10&#x2F;hr isn&#x27;t actually all that cheap, considering what it takes to grow a human into a fully functioning person that&#x27;s able to just do stuff</div><br/></div></div></div></div><div id="42473525" class="c"><input type="checkbox" id="c-42473525" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#42476825">prev</a><span>|</span><a href="#42474799">next</a><span>|</span><label class="collapse" for="c-42473525">[-]</label><label class="expand" for="c-42473525">[6 more]</label></div><br/><div class="children"><div class="content">Whenever a benchmark that was thought to be extremely difficult is (nearly) solved, it&#x27;s a mix of two causes. One is that progress on AI capabilities was faster than we expected, and the other is that there was an approach that made the task easier than we expected. I feel like the there&#x27;s a lot of the former here, but the compute cost per task (thousands of dollars to solve one little color grid puzzle??) suggests to me that there&#x27;s some amount of the latter. Chollet also mentions ARC-AGI-2 might be more resistant to this approach.<p>Of course, o3 looks strong on other benchmarks as well, and sometimes &quot;spend a huge amount of compute for one problem&quot; is a great feature to have available if it gets you the answer you needed. So even if there&#x27;s some amount of &quot;ARC-AGI wasn&#x27;t quite as robust as we thought&quot;, o3 is clearly a very powerful model.</div><br/><div id="42477208" class="c"><input type="checkbox" id="c-42477208" checked=""/><div class="controls bullet"><span class="by">solidasparagus</span><span>|</span><a href="#42473525">parent</a><span>|</span><a href="#42473651">next</a><span>|</span><label class="collapse" for="c-42477208">[-]</label><label class="expand" for="c-42477208">[1 more]</label></div><br/><div class="children"><div class="content">Or the test wasn&#x27;t testing anything meaningful, which IMO is what happened here. I think ARC was basically looking at the distribution of what AI is capable of, picked an area that it was bad at and no one had cared enough to go solve, and put together a benchmark. And then we got good at it because someone cared and we had a measurement. Which is essentially the goal of ARC.<p>But I don&#x27;t much agree that it is any meaningful step towards AGI. Maybe it&#x27;s a nice proofpoint that that AI can solve simple problems presented in intentionally opaque ways.</div><br/></div></div><div id="42473651" class="c"><input type="checkbox" id="c-42473651" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#42473525">parent</a><span>|</span><a href="#42477208">prev</a><span>|</span><a href="#42474799">next</a><span>|</span><label class="collapse" for="c-42473651">[-]</label><label class="expand" for="c-42473651">[4 more]</label></div><br/><div class="children"><div class="content">&gt; the other is that there was an approach that made the task easier than we expected.<p>from reading Dennett&#x27;s philosophy, I&#x27;m convinced that that&#x27;s how human intelligence works - for each task that &quot;only a human could do that&quot;, there&#x27;s a trick that makes it easier than it seems. We are bags of tricks.</div><br/><div id="42476079" class="c"><input type="checkbox" id="c-42476079" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42473525">root</a><span>|</span><a href="#42473651">parent</a><span>|</span><a href="#42474799">next</a><span>|</span><label class="collapse" for="c-42476079">[-]</label><label class="expand" for="c-42476079">[3 more]</label></div><br/><div class="children"><div class="content">&gt; We are bags of tricks.<p>We are trick generators, that is what it means to be a general intelligence. Adding another trick in the bag doesn&#x27;t make you a general intelligence, being able to discover and add new tricks yourself makes you a general intelligence.</div><br/><div id="42476374" class="c"><input type="checkbox" id="c-42476374" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#42473525">root</a><span>|</span><a href="#42476079">parent</a><span>|</span><a href="#42474799">next</a><span>|</span><label class="collapse" for="c-42476374">[-]</label><label class="expand" for="c-42476374">[2 more]</label></div><br/><div class="children"><div class="content">Not the parent, but remembering my reading of Dennett, he was referring to the tricks that we got through evolution, rather than ones we invented ourselves. As particular examples, we have neural functional areas for capabilities like facial recognition and spatial reasoning which seems to rely on dedicated &quot;wetware&quot; somewhat distinct from other parts of the brain.</div><br/><div id="42476462" class="c"><input type="checkbox" id="c-42476462" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42473525">root</a><span>|</span><a href="#42476374">parent</a><span>|</span><a href="#42474799">next</a><span>|</span><label class="collapse" for="c-42476462">[-]</label><label class="expand" for="c-42476462">[1 more]</label></div><br/><div class="children"><div class="content">But humans being able to develop new tricks is core to their intelligence, saying its just a bag of tricks means you don&#x27;t understand what AGI is. So either the poster misunderstood Dennett or Dennett weren&#x27;t talking about AGI or Dennett didn&#x27;t understand this well.<p>Of course there are many tricks you will need special training for, like many of the skills human share with animals, but the ability to construct useful shareable large knowledge bases based on observations is unique to humans and isn&#x27;t just a &quot;trick&quot;.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42474799" class="c"><input type="checkbox" id="c-42474799" checked=""/><div class="controls bullet"><span class="by">highfrequency</span><span>|</span><a href="#42473525">prev</a><span>|</span><a href="#42473442">next</a><span>|</span><label class="collapse" for="c-42474799">[-]</label><label class="expand" for="c-42474799">[5 more]</label></div><br/><div class="children"><div class="content">Very cool. I recommend scrolling down to look at the example problem that O3 still can’t solve. It’s clear what goes on in the human brain to solve this problem: we look at one example, hypothesize a simple rule that explains it, and then check that hypothesis against the other examples. It doesn’t quite work, so we zoom into an example that we got wrong and refine the hypothesis so that it solves that sample. We keep iterating in this fashion until we have the simplest hypothesis that satisfies all the examples. In other words, how humans do science - iteratively formulating, rejecting and refining hypotheses against collected data.<p>From this it makes sense why the original models did poorly and why iterative chain of thought is required - the challenge is designed to be inherently iterative such that a zero shot model, no matter how big, is extremely unlikely to get it right on the first try. Of course, it also requires a broad set of human-like priors about what hypotheses are “simple”, based on things like object permanence, directionality and cardinality. But as the author says, these basic world models were already encoded in the GPT 3&#x2F;4 line by simply training a gigantic model on a gigantic dataset. What was missing was iterative hypothesis generation and testing against contradictory examples. My guess is that O3 does something like this:<p>1. Prompt the model to produce a simple rule to explain the nth example (randomly chosen)<p>2. Choose a different example, ask the model to check whether the hypothesis explains this case as well. If yes, keep going. If no, ask the model to <i>revise</i> the hypothesis in the simplest possible way that also explains this example.<p>3. Keep iterating over examples like this until the hypothesis explains all cases. Occasionally, new revisions will invalidate already solved examples. That’s fine, just keep iterating.<p>4. Induce randomness in the process (through next-word sampling noise, example ordering, etc) to run this process a large number of times, resulting in say 1,000 hypotheses which all explain all examples. Due to path dependency, anchoring and consistency effects, some of these paths will end in awful hypotheses - super convoluted and involving a large number of arbitrary rules. But some will be simple.<p>5. Ask the model to select among the valid hypotheses (meaning those that satisfy all examples) and choose the one that it views as the simplest for a human to discover.</div><br/><div id="42475051" class="c"><input type="checkbox" id="c-42475051" checked=""/><div class="controls bullet"><span class="by">hmottestad</span><span>|</span><a href="#42474799">parent</a><span>|</span><a href="#42473442">next</a><span>|</span><label class="collapse" for="c-42475051">[-]</label><label class="expand" for="c-42475051">[4 more]</label></div><br/><div class="children"><div class="content">I took a look at those examples that o3 can&#x27;t solve. Looks similar to an IQ-test.<p>Took me less time to figure out the 3 examples that it took to read your post.<p>I was honestly a bit surprised to see how visual the tasks were. I had thought they were text based. So now I&#x27;m quite impressed that o3 can solve this type of task at all.</div><br/><div id="42475190" class="c"><input type="checkbox" id="c-42475190" checked=""/><div class="controls bullet"><span class="by">neom</span><span>|</span><a href="#42474799">root</a><span>|</span><a href="#42475051">parent</a><span>|</span><a href="#42475117">next</a><span>|</span><label class="collapse" for="c-42475190">[-]</label><label class="expand" for="c-42475190">[2 more]</label></div><br/><div class="children"><div class="content">I also took some time to look at the ones it couldn&#x27;t solve. I stopped after this one: <a href="https:&#x2F;&#x2F;kts.github.io&#x2F;arc-viewer&#x2F;page6&#x2F;#47996f11" rel="nofollow">https:&#x2F;&#x2F;kts.github.io&#x2F;arc-viewer&#x2F;page6&#x2F;#47996f11</a></div><br/><div id="42477911" class="c"><input type="checkbox" id="c-42477911" checked=""/><div class="controls bullet"><span class="by">hmottestad</span><span>|</span><a href="#42474799">root</a><span>|</span><a href="#42475190">parent</a><span>|</span><a href="#42475117">next</a><span>|</span><label class="collapse" for="c-42477911">[-]</label><label class="expand" for="c-42477911">[1 more]</label></div><br/><div class="children"><div class="content">That one&#x27;s cool. All pink pixels need to be repaired so they match the symmetry in the picture.</div><br/></div></div></div></div><div id="42475117" class="c"><input type="checkbox" id="c-42475117" checked=""/><div class="controls bullet"><span class="by">highfrequency</span><span>|</span><a href="#42474799">root</a><span>|</span><a href="#42475051">parent</a><span>|</span><a href="#42475190">prev</a><span>|</span><a href="#42473442">next</a><span>|</span><label class="collapse" for="c-42475117">[-]</label><label class="expand" for="c-42475117">[1 more]</label></div><br/><div class="children"><div class="content">You must be a stem grad! Or perhaps an ensemble of Kaggle submissions?</div><br/></div></div></div></div></div></div><div id="42473442" class="c"><input type="checkbox" id="c-42473442" checked=""/><div class="controls bullet"><span class="by">zebomon</span><span>|</span><a href="#42474799">prev</a><span>|</span><a href="#42473883">next</a><span>|</span><label class="collapse" for="c-42473442">[-]</label><label class="expand" for="c-42473442">[116 more]</label></div><br/><div class="children"><div class="content">My initial impression: it&#x27;s very impressive and very exciting.<p>My skeptical impression: it&#x27;s complete hubris to conflate ARC or any benchmark with truly general intelligence.<p>I know my skepticism here is identical to moving goalposts. More and more I am shifting my personal understanding of general intelligence as a phenomenon we will only ever be able to identify with the benefit of substantial retrospect.<p>As it is with any sufficiently complex program, if you could discern the result beforehand, you wouldn&#x27;t have had to execute the program in the first place.<p>I&#x27;m not trying to be a downer on the 12th day of Christmas. Perhaps because my first instinct is childlike excitement, I&#x27;m trying to temper it with a little reason.</div><br/><div id="42473481" class="c"><input type="checkbox" id="c-42473481" checked=""/><div class="controls bullet"><span class="by">hansonkd</span><span>|</span><a href="#42473442">parent</a><span>|</span><a href="#42473576">next</a><span>|</span><label class="collapse" for="c-42473481">[-]</label><label class="expand" for="c-42473481">[47 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t need to be general intelligence or perfectly map to human intelligence.<p>All it needs to be is useful. Reading constant comments about LLMs can&#x27;t be general intelligence or lack reasoning etc, to me seems like people witnessing the airplane and complaining that it isn&#x27;t &quot;real flying&quot; because it isn&#x27;t a bird flapping its wings (a large portion of the population held that point of view back then).<p>It doesn&#x27;t need to be general intelligence for the rapid advancement of LLM capabilities to be the most societal shifting development in the past decades.</div><br/><div id="42473858" class="c"><input type="checkbox" id="c-42473858" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473481">parent</a><span>|</span><a href="#42473687">next</a><span>|</span><label class="collapse" for="c-42473858">[-]</label><label class="expand" for="c-42473858">[15 more]</label></div><br/><div class="children"><div class="content">And look at the airplanes, they really can’t just land on a mountain slope or a tree without heavy maintenance afterwards. Those people weren’t all stupid, they questioned the promise of flying servicemen delivering mail or milk to their window and flying on a personal aircar to their workplace. Just like todays promises about whatever the CEOs telltales are. Imagining bullshit isn’t unique to this century.<p>Aerospace is still a highly regulated area that requires training and responsibility. If parallels can be drawn here, they don’t look so cool for a regular guy.</div><br/><div id="42474104" class="c"><input type="checkbox" id="c-42474104" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473858">parent</a><span>|</span><a href="#42474005">next</a><span>|</span><label class="collapse" for="c-42474104">[-]</label><label class="expand" for="c-42474104">[9 more]</label></div><br/><div class="children"><div class="content">What people always leave out is that society will bend to the abilities of the new technology. Planes can&#x27;t land in your backyard so we built airports. We didn&#x27;t abandon planes.</div><br/><div id="42474346" class="c"><input type="checkbox" id="c-42474346" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474104">parent</a><span>|</span><a href="#42474274">next</a><span>|</span><label class="collapse" for="c-42474346">[-]</label><label class="expand" for="c-42474346">[3 more]</label></div><br/><div class="children"><div class="content">Yes but the idea was lost in the process. It became a faster transportation system that uses air as a medium, but that’s it. Personal planes are still either big business or an expensive and dangerous personal toy thing. I don’t think it’s the same for LLMs (would be naive). But where are promises like “we’re gonna change travel economics etc”? All headlines scream is “AGI around the corner”. Yeah, now where’s my damn postman flying? I need my mail.</div><br/><div id="42475478" class="c"><input type="checkbox" id="c-42475478" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474346">parent</a><span>|</span><a href="#42475630">next</a><span>|</span><label class="collapse" for="c-42475478">[-]</label><label class="expand" for="c-42475478">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It became a faster transportation system that uses air as a medium, but that’s it.<p>On the one hand, yes; on the other, this understates the impact that had.<p>My uncle moved from the UK to Australia because, I&#x27;m told*, he didn&#x27;t like his mum and travel was so expensive that he assumed they&#x27;d never meet again. My first trip abroad… I&#x27;m not 100% sure how old I was, but it must have been between age 6 and 10, was my gran (his mum) paying for herself, for both my parents, and for me, to fly to Singapore, then on to various locations in Australia including my uncle, and back via Thailand, on her pension.<p>That was a gap of around one and a half generations.<p>* both of them are long-since dead now so I can&#x27;t ask</div><br/></div></div></div></div><div id="42474274" class="c"><input type="checkbox" id="c-42474274" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474104">parent</a><span>|</span><a href="#42474346">prev</a><span>|</span><a href="#42475207">next</a><span>|</span><label class="collapse" for="c-42474274">[-]</label><label class="expand" for="c-42474274">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but that also vindicates the GP&#x27;s point that the initial claims of the boosters for planes contained more than their fair share of bullshit and lies.</div><br/></div></div><div id="42475207" class="c"><input type="checkbox" id="c-42475207" checked=""/><div class="controls bullet"><span class="by">tivert</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474104">parent</a><span>|</span><a href="#42474274">prev</a><span>|</span><a href="#42474721">next</a><span>|</span><label class="collapse" for="c-42475207">[-]</label><label class="expand" for="c-42475207">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What people always leave out is that society will bend to the abilities of the new technology.<p>Do they really? I don&#x27;t think they do.<p>&gt; Planes can&#x27;t land in your backyard so we built airports. We didn&#x27;t abandon planes.<p>But then what do you do with the all the fantasies and hype about the new technology (like planes that land in your backyard and you fly them to work)?<p>And it&#x27;s quite possible and fairly common that the new technology <i>actually ends up being mostly hype</i>, and there&#x27;s actually no &quot;airports&quot; use case in the wings. I mean, how much did society &quot;bend to the abilities of&quot; NFTs?<p>And then what if the mature &quot;airports&quot; use case is actually something <i>most people do not want</i>?</div><br/></div></div><div id="42474721" class="c"><input type="checkbox" id="c-42474721" checked=""/><div class="controls bullet"><span class="by">ForHackernews</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474104">parent</a><span>|</span><a href="#42475207">prev</a><span>|</span><a href="#42475532">next</a><span>|</span><label class="collapse" for="c-42474721">[-]</label><label class="expand" for="c-42474721">[1 more]</label></div><br/><div class="children"><div class="content">This is already happening. A few days ago Microsoft turned down a documentation PR because the formatting was better for humans but worse for LLMs: <a href="https:&#x2F;&#x2F;github.com&#x2F;MicrosoftDocs&#x2F;WSL&#x2F;pull&#x2F;2021#issuecomment-2546627586">https:&#x2F;&#x2F;github.com&#x2F;MicrosoftDocs&#x2F;WSL&#x2F;pull&#x2F;2021#issuecomment-...</a><p>They changed their mind after a public outcry including here on HN.</div><br/></div></div><div id="42475532" class="c"><input type="checkbox" id="c-42475532" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474104">parent</a><span>|</span><a href="#42474721">prev</a><span>|</span><a href="#42474892">next</a><span>|</span><label class="collapse" for="c-42475532">[-]</label><label class="expand" for="c-42475532">[1 more]</label></div><br/><div class="children"><div class="content">No, we built helicopters.</div><br/></div></div><div id="42474892" class="c"><input type="checkbox" id="c-42474892" checked=""/><div class="controls bullet"><span class="by">oblio</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474104">parent</a><span>|</span><a href="#42475532">prev</a><span>|</span><a href="#42474005">next</a><span>|</span><label class="collapse" for="c-42474892">[-]</label><label class="expand" for="c-42474892">[1 more]</label></div><br/><div class="children"><div class="content">We are slowly discovering that many of our wonderful inventions from 60-80-100 years ago have serious side effects.<p>Plastics, cars, planes, etc.<p>One could say that a balanced situation, where vested interests are put back in the box (close to impossible since it would mean fighting trillions of dollars), would mean that for example all 3 in the list above are used a lot less than we use them now, for example. And only used where truly appropriate.</div><br/></div></div></div></div><div id="42474005" class="c"><input type="checkbox" id="c-42474005" checked=""/><div class="controls bullet"><span class="by">skydhash</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473858">parent</a><span>|</span><a href="#42474104">prev</a><span>|</span><a href="#42474573">next</a><span>|</span><label class="collapse" for="c-42474005">[-]</label><label class="expand" for="c-42474005">[3 more]</label></div><br/><div class="children"><div class="content">This pretty much. Everyone knows that LLMs are great for text generation and processing. What people has been questioning is the end goals as promised by its builders, i.e. is it useful? And from most of what I saw, it&#x27;s very much a toy.</div><br/><div id="42477252" class="c"><input type="checkbox" id="c-42477252" checked=""/><div class="controls bullet"><span class="by">MVissers</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474005">parent</a><span>|</span><a href="#42474573">next</a><span>|</span><label class="collapse" for="c-42477252">[-]</label><label class="expand" for="c-42477252">[2 more]</label></div><br/><div class="children"><div class="content">What would you need to see to call it useful?<p>To give you an example– I&#x27;ve used it for legal work such as an EB2-NIW visa application. Saved me countless of hours. My next visa I&#x27;ll try to do without a lawyer using just LLMs. I would never try this without having LLMs at my disposal.<p>As a hobby– And as someone with a scientific background I&#x27;ve been able to build an artificial ecosystem simulation from scratch without programming experience in Rust: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;@GenecraftSimulator" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;@GenecraftSimulator</a><p>I recently moved from fish to plants and believe I&#x27;ve developed some new science at the intersection of CS and Evolutionary Biology that I&#x27;m looking to publish.<p>This tool is extremely useful. For now– You do require a human in the loop for coordination.<p>My guess is that these will be benchmarks that we see within a few years: How good an AI coordinate multiple other AIs to build, deploy and iterate something that functions in the real world. Basically manager AI.<p>Because they&#x27;ll literally be able to solve every single one shot problem so we won&#x27;t be able to create benchmarks anymore.<p>But that&#x27;s also when these models will be able to build functioning companies in a few hours.</div><br/><div id="42477551" class="c"><input type="checkbox" id="c-42477551" checked=""/><div class="controls bullet"><span class="by">skydhash</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42477252">parent</a><span>|</span><a href="#42474573">next</a><span>|</span><label class="collapse" for="c-42477551">[-]</label><label class="expand" for="c-42477551">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>...me countless of...would never try this without having LLMs...is extremely useful...they&#x27;ll literally be able to solve...will be able to... in a few hours.</i><p>That&#x27;s marketing language, not scientific or even casual language. So much outstanding claims, without even some basic explanations. Like how did it help you save these hours? Terms explanations? Outlining processes? Going to the post office for you? You don&#x27;t need to sell me anything, I just want the how.</div><br/></div></div></div></div></div></div><div id="42474573" class="c"><input type="checkbox" id="c-42474573" checked=""/><div class="controls bullet"><span class="by">throwaway4aday</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473858">parent</a><span>|</span><a href="#42474005">prev</a><span>|</span><a href="#42473687">next</a><span>|</span><label class="collapse" for="c-42474573">[-]</label><label class="expand" for="c-42474573">[2 more]</label></div><br/><div class="children"><div class="content">Your point is on the verge of nullification with the rapid improvement and adoption of autonomous drones don&#x27;t you think?</div><br/><div id="42476265" class="c"><input type="checkbox" id="c-42476265" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474573">parent</a><span>|</span><a href="#42473687">next</a><span>|</span><label class="collapse" for="c-42476265">[-]</label><label class="expand" for="c-42476265">[1 more]</label></div><br/><div class="children"><div class="content">Sort of, but doesn’t that sit on a far-fetch horizon? I doubt that drone companies are all the same who sold aircraft retrofuturism to people back then.</div><br/></div></div></div></div></div></div><div id="42473687" class="c"><input type="checkbox" id="c-42473687" checked=""/><div class="controls bullet"><span class="by">surgical_fire</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473481">parent</a><span>|</span><a href="#42473858">prev</a><span>|</span><a href="#42473519">next</a><span>|</span><label class="collapse" for="c-42473687">[-]</label><label class="expand" for="c-42473687">[5 more]</label></div><br/><div class="children"><div class="content">&gt; to me seems like people witnessing the airplane and complaining that it isn&#x27;t &quot;real flying&quot; because it isn&#x27;t a bird flapping its wings<p>To me it is more like there is someone jumping on a pogo ball while flapping their arms and saying that they are flying whenever they hop off the ground.<p>Skeptics say that they are not really flying, while adherents say that &quot;with current pogo ball advancements, they will be flying any day now&quot;</div><br/><div id="42474299" class="c"><input type="checkbox" id="c-42474299" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473687">parent</a><span>|</span><a href="#42473872">next</a><span>|</span><label class="collapse" for="c-42474299">[-]</label><label class="expand" for="c-42474299">[1 more]</label></div><br/><div class="children"><div class="content">An old quote, quite famous: &quot;... is like saying that an ape who climbs to the top of a tree for the first time is one step closer to landing on the moon&quot;.</div><br/></div></div><div id="42473872" class="c"><input type="checkbox" id="c-42473872" checked=""/><div class="controls bullet"><span class="by">intelVISA</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473687">parent</a><span>|</span><a href="#42474299">prev</a><span>|</span><a href="#42474541">next</a><span>|</span><label class="collapse" for="c-42473872">[-]</label><label class="expand" for="c-42473872">[2 more]</label></div><br/><div class="children"><div class="content">Between skeptics and adherents who is more easily able to extract VC money for vaporware? If you limit yourself to &#x27;the facts&#x27; you&#x27;re leaving tons of $$ on the table...</div><br/><div id="42474099" class="c"><input type="checkbox" id="c-42474099" checked=""/><div class="controls bullet"><span class="by">surgical_fire</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473872">parent</a><span>|</span><a href="#42474541">next</a><span>|</span><label class="collapse" for="c-42474099">[-]</label><label class="expand" for="c-42474099">[1 more]</label></div><br/><div class="children"><div class="content">By all means, if this is the goal, AI is a success.<p>I understand that in this forum too many people are invested in putting lipstick on this particular pig.</div><br/></div></div></div></div><div id="42474541" class="c"><input type="checkbox" id="c-42474541" checked=""/><div class="controls bullet"><span class="by">DonHopkins</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473687">parent</a><span>|</span><a href="#42473872">prev</a><span>|</span><a href="#42473519">next</a><span>|</span><label class="collapse" for="c-42474541">[-]</label><label class="expand" for="c-42474541">[1 more]</label></div><br/><div class="children"><div class="content">Is that what Elon Musk was trying to do on stage?</div><br/></div></div></div></div><div id="42473519" class="c"><input type="checkbox" id="c-42473519" checked=""/><div class="controls bullet"><span class="by">zebomon</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473481">parent</a><span>|</span><a href="#42473687">prev</a><span>|</span><a href="#42473685">next</a><span>|</span><label class="collapse" for="c-42473519">[-]</label><label class="expand" for="c-42473519">[1 more]</label></div><br/><div class="children"><div class="content">I agree. If the LLMs we have today never got any smarter, the world would still be transformed over the next ten years.</div><br/></div></div><div id="42473685" class="c"><input type="checkbox" id="c-42473685" checked=""/><div class="controls bullet"><span class="by">handsclean</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473481">parent</a><span>|</span><a href="#42473519">prev</a><span>|</span><a href="#42473771">next</a><span>|</span><label class="collapse" for="c-42473685">[-]</label><label class="expand" for="c-42473685">[1 more]</label></div><br/><div class="children"><div class="content">People aren’t responding to their own assumption that AGI is necessary, they’re responding to OpenAI and the chorus constantly and loudly singing hymns to AGI.</div><br/></div></div><div id="42473771" class="c"><input type="checkbox" id="c-42473771" checked=""/><div class="controls bullet"><span class="by">billyp-rva</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473481">parent</a><span>|</span><a href="#42473685">prev</a><span>|</span><a href="#42474890">next</a><span>|</span><label class="collapse" for="c-42473771">[-]</label><label class="expand" for="c-42473771">[10 more]</label></div><br/><div class="children"><div class="content">&gt; It doesn&#x27;t need to be general intelligence or perfectly map to human intelligence.<p>&gt; All it needs to be is useful.<p>Computers were already useful.<p>The only definition we have for &quot;intelligence&quot; is human (or, generally, animal) intelligence. If LLMs aren&#x27;t that, let&#x27;s call it something else.</div><br/><div id="42473828" class="c"><input type="checkbox" id="c-42473828" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473771">parent</a><span>|</span><a href="#42474890">next</a><span>|</span><label class="collapse" for="c-42473828">[-]</label><label class="expand" for="c-42473828">[9 more]</label></div><br/><div class="children"><div class="content">What exactly is human (or animal) intelligence? How do you define that?</div><br/><div id="42473903" class="c"><input type="checkbox" id="c-42473903" checked=""/><div class="controls bullet"><span class="by">billyp-rva</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473828">parent</a><span>|</span><a href="#42474890">next</a><span>|</span><label class="collapse" for="c-42473903">[-]</label><label class="expand" for="c-42473903">[8 more]</label></div><br/><div class="children"><div class="content">Does it matter? If LLMs <i>aren&#x27;t</i> that, whatever it is, then we should use a different word. Finders keepers.</div><br/><div id="42473966" class="c"><input type="checkbox" id="c-42473966" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473903">parent</a><span>|</span><a href="#42474890">next</a><span>|</span><label class="collapse" for="c-42473966">[-]</label><label class="expand" for="c-42473966">[7 more]</label></div><br/><div class="children"><div class="content">How do you know that LLMs “aren’t that” if you can’t even define what <i>that</i> is?<p>“I’ll know it when I see it” isn’t a compelling argument.</div><br/><div id="42475395" class="c"><input type="checkbox" id="c-42475395" checked=""/><div class="controls bullet"><span class="by">Aperocky</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473966">parent</a><span>|</span><a href="#42475185">next</a><span>|</span><label class="collapse" for="c-42475395">[-]</label><label class="expand" for="c-42475395">[1 more]</label></div><br/><div class="children"><div class="content">I think a successful high level intelligence should quickly accelerate or converge to infinity&#x2F;physical resource exhaustion because they can now work on improving themselves.<p>So if above human intelligence does happen, I&#x27;d assume we&#x27;d know it, quite soon.</div><br/></div></div><div id="42475185" class="c"><input type="checkbox" id="c-42475185" checked=""/><div class="controls bullet"><span class="by">jonny_eh</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473966">parent</a><span>|</span><a href="#42475395">prev</a><span>|</span><a href="#42474305">next</a><span>|</span><label class="collapse" for="c-42475185">[-]</label><label class="expand" for="c-42475185">[1 more]</label></div><br/><div class="children"><div class="content">&gt; “I’ll know it when I see it” isn’t a compelling argument.<p>It feels compelling to me.</div><br/></div></div><div id="42474305" class="c"><input type="checkbox" id="c-42474305" checked=""/><div class="controls bullet"><span class="by">grahamj</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473966">parent</a><span>|</span><a href="#42475185">prev</a><span>|</span><a href="#42474890">next</a><span>|</span><label class="collapse" for="c-42474305">[-]</label><label class="expand" for="c-42474305">[4 more]</label></div><br/><div class="children"><div class="content">they can&#x27;t do what we do therefore they aren&#x27;t what we are</div><br/><div id="42474521" class="c"><input type="checkbox" id="c-42474521" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474305">parent</a><span>|</span><a href="#42474890">next</a><span>|</span><label class="collapse" for="c-42474521">[-]</label><label class="expand" for="c-42474521">[3 more]</label></div><br/><div class="children"><div class="content">And what is that, in concrete terms? Many humans can’t do what other humans can do. What is the common subset that counts as human intelligence?</div><br/><div id="42477580" class="c"><input type="checkbox" id="c-42477580" checked=""/><div class="controls bullet"><span class="by">dimitri-vs</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474521">parent</a><span>|</span><a href="#42475177">next</a><span>|</span><label class="collapse" for="c-42477580">[-]</label><label class="expand" for="c-42477580">[1 more]</label></div><br/><div class="children"><div class="content">Process vision and sounds in parallel for 80+ years, rapidly adapt to changing environments and scenarios, correlate seemingly irrelevant details that happened a week ago or years ago, be able to selectively ignore instructions and know when to disagree</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="42474890" class="c"><input type="checkbox" id="c-42474890" checked=""/><div class="controls bullet"><span class="by">skywhopper</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473481">parent</a><span>|</span><a href="#42473771">prev</a><span>|</span><a href="#42474301">next</a><span>|</span><label class="collapse" for="c-42474890">[-]</label><label class="expand" for="c-42474890">[1 more]</label></div><br/><div class="children"><div class="content">On the contrary, the pushback is critical because many employers are buying the hype from AI companies that AGI is imminent, that LLMs can replace professional humans, and that computers are about to eliminate all work (except VCs and CEOs apparently).<p>Every person that believes that LLMs are near sentient or actually do a good job at reasoning is one more person handing over their responsibilities to a zero-accountability highly flawed robot. We&#x27;ve already seen LLMs generate bad legal documents, bad academic papers, and extremely bad code. Similar technology is making bad decisions about who to arrest, who to give loans to, who to hire, who to bomb, and who to refuse heart surgery for. Overconfident humans employing this tech for these purposes have been bamboozled by the lies from OpenAI, Microsoft, Google, et al. It&#x27;s crucial to call out overstatement and overhype about this tech wherever it crops up.</div><br/></div></div><div id="42474301" class="c"><input type="checkbox" id="c-42474301" checked=""/><div class="controls bullet"><span class="by">alexalx666</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473481">parent</a><span>|</span><a href="#42474890">prev</a><span>|</span><a href="#42473637">next</a><span>|</span><label class="collapse" for="c-42474301">[-]</label><label class="expand" for="c-42474301">[1 more]</label></div><br/><div class="children"><div class="content">If I could put it into Tesla style robot and it could do dishes and help me figure out tech stuff, it would be more than enough.</div><br/></div></div><div id="42473637" class="c"><input type="checkbox" id="c-42473637" checked=""/><div class="controls bullet"><span class="by">AyyEye</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473481">parent</a><span>|</span><a href="#42474301">prev</a><span>|</span><a href="#42475237">next</a><span>|</span><label class="collapse" for="c-42473637">[-]</label><label class="expand" for="c-42473637">[9 more]</label></div><br/><div class="children"><div class="content">&gt; Reading constant comments about LLMs can&#x27;t be general intelligence or lack reasoning etc, to me seems like people witnessing the airplane and complaining that it isn&#x27;t &quot;real flying&quot; because it isn&#x27;t a bird flapping its wings (a large portion of the population held that point of view back then).<p>That is a natural reaction to the incessant techbro, AIbro, marketing, and corporate lies that &quot;AI&quot; (or worse AGI) is a real thing, and can be directly compared to real humans.<p>There are people on this very thread saying it&#x27;s better at reasoning than real humans (LOL) because it scored higher on some benchmark than humans... Yet this technology still can&#x27;t reliably determine what number is circled, if two lines intersect, or count the letters in a word. (That said behaviour may have been somewhat finetuned out of newer models only reinforces the fact that the technology inherently not capable of understanding <i>anything</i>.)</div><br/><div id="42474076" class="c"><input type="checkbox" id="c-42474076" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473637">parent</a><span>|</span><a href="#42475237">next</a><span>|</span><label class="collapse" for="c-42474076">[-]</label><label class="expand" for="c-42474076">[8 more]</label></div><br/><div class="children"><div class="content">I encounter &quot;spicy auto complete&quot; style comments far more often than techbro AI-everything comments and its frankly getting boring.<p>I&#x27;ve been doing AI things for about 20+ years and llms are wild. We&#x27;ve gone from specialized things being pretty bad as those jobs to general purpose things better at that and everything else. The idea you could make and API call with &quot;is this sarcasm?&quot; and get a better than chance guess is incredible.</div><br/><div id="42474939" class="c"><input type="checkbox" id="c-42474939" checked=""/><div class="controls bullet"><span class="by">surgical_fire</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474076">parent</a><span>|</span><a href="#42474587">next</a><span>|</span><label class="collapse" for="c-42474939">[-]</label><label class="expand" for="c-42474939">[3 more]</label></div><br/><div class="children"><div class="content">Eh, I see far more &quot;AI is the second coming of Jesus&quot; type of comments than healthy skepticism. A lot of anxiety from people afraid that their source of income will dry and a lot of excitement of people with an axe to grind that &quot;those entitled expensive peasants will get what they deserve&quot;.<p>I think I count myself among the skeptics nowadays for that reason. And I say this as someone that thinks LLM is an interesting piece of technology, but with somewhat limited use and unclear economics.<p>If the hype was about &quot;look at this thing that can parse natural language surprisingly well and generate coherent responses&quot;, I would be excited too. As someone that had to do natural language processing in the past, that is a damn hard task to solve, and LLMs excel at it.<p>But that is not the hype is it? We have people beating the drums of how this is just shy of taking the world by storm, and AGI is just around the corner, and it will revolutionize all economy and society and nothing will ever be the same.<p>So, yeah, it gets tiresome. I wish the hype would die down a little so this could be appreciated for what it is.</div><br/><div id="42475392" class="c"><input type="checkbox" id="c-42475392" checked=""/><div class="controls bullet"><span class="by">williamcotton</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474939">parent</a><span>|</span><a href="#42474587">next</a><span>|</span><label class="collapse" for="c-42475392">[-]</label><label class="expand" for="c-42475392">[2 more]</label></div><br/><div class="children"><div class="content"><i>We have people beating the drums of how this is just shy of taking the world by storm, and AGI is just around the corner, and it will revolutionize all economy and society and nothing will ever be the same.</i><p>Where are you seeing this? I pretty much only read HN and football blogs so maybe I’m out of the loop.</div><br/><div id="42475679" class="c"><input type="checkbox" id="c-42475679" checked=""/><div class="controls bullet"><span class="by">sensanaty</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42475392">parent</a><span>|</span><a href="#42474587">next</a><span>|</span><label class="collapse" for="c-42475679">[-]</label><label class="expand" for="c-42475679">[1 more]</label></div><br/><div class="children"><div class="content">In this very thread there are multiple people espousing their views that the high score here is proof that o3 has achieved AGI.</div><br/></div></div></div></div></div></div><div id="42474587" class="c"><input type="checkbox" id="c-42474587" checked=""/><div class="controls bullet"><span class="by">AyyEye</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474076">parent</a><span>|</span><a href="#42474939">prev</a><span>|</span><a href="#42475237">next</a><span>|</span><label class="collapse" for="c-42474587">[-]</label><label class="expand" for="c-42474587">[4 more]</label></div><br/><div class="children"><div class="content">Nobody is disputing the coolness factor, only the intelligence factor.</div><br/><div id="42475826" class="c"><input type="checkbox" id="c-42475826" checked=""/><div class="controls bullet"><span class="by">hansonkd</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474587">parent</a><span>|</span><a href="#42475237">next</a><span>|</span><label class="collapse" for="c-42475826">[-]</label><label class="expand" for="c-42475826">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m saying the intelligence factor doesn&#x27;t matter. Only the utility factor. Today LLMs are incredibly useful and every few months there appears to be bigger and bigger leaps.<p>Analyzing whether or not LLMs have intelligence is missing the forest from the trees. This technology is emerging in a capitalist society that is hyper optimized to adopt useful things at the expense of almost everything else. If the utility&#x2F;price point gets hit for a problem, it will replace it regardless of if it is intelligent or not.</div><br/><div id="42476616" class="c"><input type="checkbox" id="c-42476616" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42475826">parent</a><span>|</span><a href="#42475237">next</a><span>|</span><label class="collapse" for="c-42476616">[-]</label><label class="expand" for="c-42476616">[2 more]</label></div><br/><div class="children"><div class="content">But if you want to predict the future utility of these models you want to look at their current intelligence, compare that to humans and try to figure out roughly what skills they lack and which of those are likely to get fixed.<p>For example, a team of humans are extremely reliable, much more reliable than one human, but a team of AI&#x27;s isn&#x27;t mean reliable than one AI since an AI is already an ensemble model. That means even if an AI could replace a person, it probably can&#x27;t replace a team for a long time, meaning you still need the other team members there, meaning the AI didn&#x27;t really replace a human it just became a tool for huamns to use.</div><br/><div id="42477278" class="c"><input type="checkbox" id="c-42477278" checked=""/><div class="controls bullet"><span class="by">MVissers</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42476616">parent</a><span>|</span><a href="#42475237">next</a><span>|</span><label class="collapse" for="c-42477278">[-]</label><label class="expand" for="c-42477278">[1 more]</label></div><br/><div class="children"><div class="content">I think this is a fair criticism of capability.<p>I personally wouldn&#x27;t be surprised if we start to see benchmarks around this type of cooperation and ability to orchestrate complex systems in the next few years or so.<p>Most benchmarks really focus on one problem, not on multiple real-time problems while orchestrating 3rd party actors who might or might not be able to succeed at certain tasks.<p>But I don&#x27;t think anything is prohibiting these models from not being able to do that.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42475237" class="c"><input type="checkbox" id="c-42475237" checked=""/><div class="controls bullet"><span class="by">jasondigitized</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473481">parent</a><span>|</span><a href="#42473637">prev</a><span>|</span><a href="#42475322">next</a><span>|</span><label class="collapse" for="c-42475237">[-]</label><label class="expand" for="c-42475237">[1 more]</label></div><br/><div class="children"><div class="content">This a thousand times.</div><br/></div></div><div id="42475322" class="c"><input type="checkbox" id="c-42475322" checked=""/><div class="controls bullet"><span class="by">colordrops</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473481">parent</a><span>|</span><a href="#42475237">prev</a><span>|</span><a href="#42473576">next</a><span>|</span><label class="collapse" for="c-42475322">[-]</label><label class="expand" for="c-42475322">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think many informed people doubt the utility of LLMs at this point. The potential of human-like AGI has profound implications far beyond utility models, which is why people are so eager to bring it up. A true human-like AGI basically means that most intellectual&#x2F;white collar work will not be needed, and probably manual labor before too long as well. Huge huge implications for humanity, e.g. how does an economy and society even work without workers?</div><br/><div id="42475789" class="c"><input type="checkbox" id="c-42475789" checked=""/><div class="controls bullet"><span class="by">vouaobrasil</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42475322">parent</a><span>|</span><a href="#42473576">next</a><span>|</span><label class="collapse" for="c-42475789">[-]</label><label class="expand" for="c-42475789">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Huge huge implications for humanity, e.g. how does an economy and society even work without workers?<p>I don&#x27;t think those that create AI care about that. They just to come out on top before someone else does.</div><br/></div></div></div></div></div></div><div id="42473576" class="c"><input type="checkbox" id="c-42473576" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#42473442">parent</a><span>|</span><a href="#42473481">prev</a><span>|</span><a href="#42473478">next</a><span>|</span><label class="collapse" for="c-42473576">[-]</label><label class="expand" for="c-42473576">[36 more]</label></div><br/><div class="children"><div class="content">These comments are getting ridiculous. I remember when this test was first discussed here on HN and everyone agreed that it clearly proves current AI models are not &quot;intelligent&quot; (whatever that means). And people tried to talk me down when I theorised this test will get nuked soon - like all the ones before. It&#x27;s time people woke up and realised that the old age of AI is over. This new kind is here to stay and it <i>will</i> take over the world. And you better guess it&#x27;ll be sooner rather than later and start to prepare.</div><br/><div id="42474164" class="c"><input type="checkbox" id="c-42474164" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473576">parent</a><span>|</span><a href="#42473915">next</a><span>|</span><label class="collapse" for="c-42474164">[-]</label><label class="expand" for="c-42474164">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>These comments are getting ridiculous.</i><p>Not really. Francois (co-creator of the ARC Prize) has this to say:<p><pre><code>  The v1 version of the benchmark is starting to saturate. There were already signs of this in the Kaggle competition this year: an ensemble of all submissions would score 81%

  Early indications are that ARC-AGI-v2 will represent a complete reset of the state-of-the-art, and it will remain extremely difficult for o3. Meanwhile, a smart human or a small panel of average humans would still be able to score &gt;95% ... This shows that it&#x27;s still feasible to create unsaturated, interesting benchmarks that are easy for humans, yet impossible for AI, without involving specialist knowledge. We will have AGI when creating such evals becomes outright impossible.

  For me, the main open question is where the scaling bottlenecks for the techniques behind o3 are going to be. If human-annotated CoT data is a major bottleneck, for instance, capabilities would start to plateau quickly like they did for LLMs (until the next architecture). If the only bottleneck is test-time search, we will see continued scaling in the future.
</code></pre>
<a href="https:&#x2F;&#x2F;x.com&#x2F;fchollet&#x2F;status&#x2F;1870169764762710376" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;fchollet&#x2F;status&#x2F;1870169764762710376</a> &#x2F; <a href="https:&#x2F;&#x2F;ghostarchive.org&#x2F;archive&#x2F;Sqjbf" rel="nofollow">https:&#x2F;&#x2F;ghostarchive.org&#x2F;archive&#x2F;Sqjbf</a></div><br/></div></div><div id="42473915" class="c"><input type="checkbox" id="c-42473915" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473576">parent</a><span>|</span><a href="#42474164">prev</a><span>|</span><a href="#42473884">next</a><span>|</span><label class="collapse" for="c-42473915">[-]</label><label class="expand" for="c-42473915">[8 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s time people woke up and realised that the old age of AI is over. This new kind is here to stay and it will take over the world. And you better guess it&#x27;ll be sooner rather than later and start to prepare.<p>I was just thinking about how 3D game engines were perceived in the 90s. Every six months some new engine came out, blew people&#x27;s minds, was declared photorealistic, and was forgotten a year later. The best of those engines kept improving and are still here, and kinda did change the world in their own way.<p>Software development seemed rapid and exciting until about Halo or Half Life 2, then it was shallow but shiny press releases for 15 years, and only became so again when OpenAI&#x27;s InstructGPT was demonstrated.<p>While I&#x27;m really impressed with current AI, and value the best models greatly, and agree that they will change (and have already changed) the world… I can&#x27;t help but think of the <i>Next Generation</i> front cover, February 1997 when considering how much further we may be from what we want: <a href="https:&#x2F;&#x2F;www.giantbomb.com&#x2F;pc&#x2F;3045-94&#x2F;forums&#x2F;unreal-yes-this-is-an-actual-pc-screenshot-419874&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.giantbomb.com&#x2F;pc&#x2F;3045-94&#x2F;forums&#x2F;unreal-yes-this-...</a></div><br/><div id="42475832" class="c"><input type="checkbox" id="c-42475832" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473915">parent</a><span>|</span><a href="#42475108">next</a><span>|</span><label class="collapse" for="c-42475832">[-]</label><label class="expand" for="c-42475832">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Software development seemed rapid and exciting until about Halo or Half Life 2, then it was shallow but shiny press releases for 15 years</i><p>The transition seems to map well to the point where engines got sophisticated enough, that highly dedicated high-schoolers couldn&#x27;t keep up. Until then, people would routinely make hobby game engines (for games they&#x27;d then never finish) that were MVPs of what the game industry had a year or three earlier. I.e. close enough to compete on visuals with top photorealistic games of a given year - but more importantly, this was a time where <i>you could do cool nerdy shit to impress your friends and community</i>.<p>Then Unreal and Unity came out, with a business model that killed the motivation to write your own engine from scratch (except for purely educational purposes), we got more games, more progress, but the excitement was gone.<p>Maybe it&#x27;s just a spurious correlation, but it seems to track with:<p>&gt; <i>and only became so again when OpenAI&#x27;s InstructGPT was demonstrated.</i><p>Which is again, if you exclude training SOTA models - which is still mostly out of reach for anyone but a few entities on the planet - the time where <i>anyone</i> can do something cool that doesn&#x27;t have a better market alternative yet, and any dedicated high-schooler can make truly impressive and useful work, outpacing commercial and academic work based on pure motivation and focus alone (it&#x27;s easier when you&#x27;re not being distracted by bullshit incentives like <i>user growth</i> or <i>making VCs happy</i> or <i>churning out publications, farming citations</i>).<p>It&#x27;s, once again, a time of dreams, where anyone with some technical interest and a bit of free time can <i>make the future happen in front of their eyes</i>.</div><br/></div></div><div id="42475108" class="c"><input type="checkbox" id="c-42475108" checked=""/><div class="controls bullet"><span class="by">hansonkd</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473915">parent</a><span>|</span><a href="#42475832">prev</a><span>|</span><a href="#42474094">next</a><span>|</span><label class="collapse" for="c-42475108">[-]</label><label class="expand" for="c-42475108">[4 more]</label></div><br/><div class="children"><div class="content">&gt;  how much further we may be from what we wan<p>The timescale you are describing for 3D graphics is 4 years from the 1997 cover you posted to the release of Halo which you are saying plateaued excitement because it got advanced enough.<p>An almost infinitesimally small amount of time in terms of history human development and you are mocking the magazine being excited for the advancement because it was... 4 years yearly?</div><br/><div id="42475285" class="c"><input type="checkbox" id="c-42475285" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42475108">parent</a><span>|</span><a href="#42474094">next</a><span>|</span><label class="collapse" for="c-42475285">[-]</label><label class="expand" for="c-42475285">[3 more]</label></div><br/><div class="children"><div class="content">No, the timescale is &quot;the 90s&quot;, the <i>the specific example</i> is from 1997, and chosen because of how badly it aged. Nobody looks at the original single-player Unreal graphics today and thinks &quot;this is amazing!&quot;, but we all did at the time — Reflections! Dynamic lighting! It was amazing for the era — but it was also a long way from photorealism. ChatGPT is amazing… but how far is it from Brent Spiner&#x27;s Data?<p>The era was people getting wowed from Wolfenstein (1992) to &quot;about Halo or Half Life 2&quot; (2001 or 2004).<p>And I&#x27;m not saying the flattening of excitement was for any specific reason, just that this was roughly when it stopped getting exciting — it might have been because the engines were good enough for 3D art styles beyond &quot;as realistic as we can make it&quot;, but for all I know it was the War On Terror which changed the tone of press releases and how much the news in general cared. Or perhaps it was a culture shift which came with more people getting online and less media being printed on glossy paper and sold in newsagents.<p>Whatever the cause, it happened around that time.</div><br/><div id="42475941" class="c"><input type="checkbox" id="c-42475941" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42475285">parent</a><span>|</span><a href="#42474094">next</a><span>|</span><label class="collapse" for="c-42475941">[-]</label><label class="expand" for="c-42475941">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m still holding on to my hypothesis in that the excitement was sustained in large part because this progress was something a regular person could partake in. Most didn&#x27;t, but they likely known some kid who was. And some of those kids run the gaming magazines.<p>This was a time where, for 3D graphics, barriers to entry got low (math got figured out, hardware was good enough, knowledge spread), but the commercial market didn&#x27;t yet capture everything. Hell, a bulk of those excited kids I remember, trying to do a better Unreal Tournament after school instead of homework (and almost succeeding!), they went on create and staff the next generation of commercial gamedev.<p>(Which is maybe why this period lasted for about as long as it takes for a schoolkid to grow up, graduate, and spend few years in the workforce doing the stuff they were so excited about.)</div><br/><div id="42476214" class="c"><input type="checkbox" id="c-42476214" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42475941">parent</a><span>|</span><a href="#42474094">next</a><span>|</span><label class="collapse" for="c-42476214">[-]</label><label class="expand" for="c-42476214">[1 more]</label></div><br/><div class="children"><div class="content">Could be.<p>I was one of those kids, my focus was Marathon 2 even before I saw Unreal. I managed to figure out enough maths from scratch to end up with the basics of ray casting, but not enough at the time to realise the tricks needed to make that real time on a 75 MHz CPU… and then we all got OpenGL and I went through university where they explained the algorithms.</div><br/></div></div></div></div></div></div></div></div><div id="42474094" class="c"><input type="checkbox" id="c-42474094" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473915">parent</a><span>|</span><a href="#42475108">prev</a><span>|</span><a href="#42473884">next</a><span>|</span><label class="collapse" for="c-42474094">[-]</label><label class="expand" for="c-42474094">[2 more]</label></div><br/><div class="children"><div class="content">The weird thing about the phenomenon you mention is only after the field of software engineering has plateaued 15 years ago, as you mentioned, that this insane demand for engineers did arise, with corresponding insane salaries.<p>It&#x27;s a very strange thing I&#x27;ve never understood.</div><br/><div id="42475069" class="c"><input type="checkbox" id="c-42475069" checked=""/><div class="controls bullet"><span class="by">dwaltrip</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474094">parent</a><span>|</span><a href="#42473884">next</a><span>|</span><label class="collapse" for="c-42475069">[-]</label><label class="expand" for="c-42475069">[1 more]</label></div><br/><div class="children"><div class="content">My guess: It’s a very lengthy, complex, and error-prone process to “digitize” human civilization (government, commerce, leisure, military, etc). The tech existed, we just didn’t know how to use it.<p>We still barely know how to use computers effectively, and they have already transformed the world. For better or worse.</div><br/></div></div></div></div></div></div><div id="42473884" class="c"><input type="checkbox" id="c-42473884" checked=""/><div class="controls bullet"><span class="by">jcims</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473576">parent</a><span>|</span><a href="#42473915">prev</a><span>|</span><a href="#42473758">next</a><span>|</span><label class="collapse" for="c-42473884">[-]</label><label class="expand" for="c-42473884">[1 more]</label></div><br/><div class="children"><div class="content">I agree, it&#x27;s like watching a meadow ablaze and dismissing it because it&#x27;s not a &#x27;real forest fire&#x27; yet.  No it&#x27;s not &#x27;real AGI&#x27; yet, but *this is how we get there* and the pace is relentless, incredible and wholly overwhelming.<p>I&#x27;ve been blessed with grandchildren recently, a little boy that&#x27;s 2 1&#x2F;2 and just this past Saturday a granddaughter.  Major events notwithstanding, the world will largely resemble today when they are teenagers, but the future is going to look very very very different.  I can&#x27;t even imagine what the capability and pervasiveness of it all will be like in ten years, when they are still just kids.  For me as someone that&#x27;s invested in their future I&#x27;m interested in all of the educational opportunities (technical, philosphical and self-awareness) but obviously am concerned about the potential for pernicious side effects.</div><br/></div></div><div id="42473758" class="c"><input type="checkbox" id="c-42473758" checked=""/><div class="controls bullet"><span class="by">lawlessone</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473576">parent</a><span>|</span><a href="#42473884">prev</a><span>|</span><a href="#42473910">next</a><span>|</span><label class="collapse" for="c-42473758">[-]</label><label class="expand" for="c-42473758">[5 more]</label></div><br/><div class="children"><div class="content">Failing the test may prove the AI is not intelligent. Passing the test doesn&#x27;t necessarily prove it is.</div><br/><div id="42473826" class="c"><input type="checkbox" id="c-42473826" checked=""/><div class="controls bullet"><span class="by">NitpickLawyer</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473758">parent</a><span>|</span><a href="#42474867">next</a><span>|</span><label class="collapse" for="c-42473826">[-]</label><label class="expand" for="c-42473826">[3 more]</label></div><br/><div class="children"><div class="content">Your comment reminds me of this quote from a book published in the 80s:<p>&gt; There is a related “Theorem” about progress in AI: once some mental function is programmed, people soon cease to consider it as an essential ingredient of “real thinking”. The ineluctable core of intelligence is always in that next thing which hasn’t yet been programmed. This “Theorem” was first proposed to me by Larry Tesler, so I call it Tesler’s Theorem: “AI is whatever hasn’t been done yet.”</div><br/><div id="42474158" class="c"><input type="checkbox" id="c-42474158" checked=""/><div class="controls bullet"><span class="by">6gvONxR4sf7o</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473826">parent</a><span>|</span><a href="#42475312">next</a><span>|</span><label class="collapse" for="c-42474158">[-]</label><label class="expand" for="c-42474158">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve always disliked this argument. A person can do something well without devising a general solution to the thing. Devising a general solution to the thing is a step we&#x27;re talking all the time with all sorts of things, but it doesn&#x27;t invalidate the cool fact about intelligence: whatever it is that lets us do the thing well <i>without</i> the general solution is hard to pin down and hard to reproduce.<p>All that&#x27;s invalidated each time is the idea that a general solution to that task requires a general solution to all tasks, or that a general solution to that task requires our special sauce. It&#x27;s the idea that something able to to that task will also be able to do XYZ.<p>And yet people keep coming up with a new task that people point to saying, &#x27;this is the one! there&#x27;s no way something could solve this one without also being able to do XYZ!&#x27;</div><br/></div></div></div></div><div id="42474867" class="c"><input type="checkbox" id="c-42474867" checked=""/><div class="controls bullet"><span class="by">8note</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473758">parent</a><span>|</span><a href="#42473826">prev</a><span>|</span><a href="#42473910">next</a><span>|</span><label class="collapse" for="c-42474867">[-]</label><label class="expand" for="c-42474867">[1 more]</label></div><br/><div class="children"><div class="content">id consider that it doing the test at all, without proper compensation is a sign that it isnt intelligent</div><br/></div></div></div></div><div id="42473910" class="c"><input type="checkbox" id="c-42473910" checked=""/><div class="controls bullet"><span class="by">philipkglass</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473576">parent</a><span>|</span><a href="#42473758">prev</a><span>|</span><a href="#42473880">next</a><span>|</span><label class="collapse" for="c-42473910">[-]</label><label class="expand" for="c-42473910">[4 more]</label></div><br/><div class="children"><div class="content">If AI takes over white collar work that&#x27;s still half of the world&#x27;s labor needs untouched. There are some promising early demos of robotics plus AI. I also saw some promising demos of robotics 10 and 20 years that didn&#x27;t reach mass adoption. I&#x27;d like to believe that by the time I reach old age the robots will be fully qualified replacements for plumbers and home health aides. Nothing I&#x27;ve seen so far makes me think that&#x27;s especially likely.<p>I&#x27;d love more progress on tasks in the physical world, though. There are only a few paths for countries to deal with a growing ratio of old retired people to young workers:<p>1) Prioritize the young people at the expense of the old by e.g. cutting old age benefits (not especially likely since older voters have greater numbers and higher participation rates in elections)<p>2) Prioritize the old people at the expense of the young by raising the demands placed on young people (either directly as labor, e.g. nurses and aides, or indirectly through higher taxation)<p>3) Rapidly increase the population of young people through high fertility or immigration (the historically favored path, but eventually turns back into case 1 or 2 with an even larger numerical burden of older people)<p>4) Increase the health span of older people, so that they are more capable of independent self-care (a good idea, but difficult to achieve at scale, since most effective approaches require behavioral changes)<p>5) Decouple goods and services from labor, so that old people with diminished capabilities can get everything they need without forcing young people to labor for them</div><br/><div id="42474899" class="c"><input type="checkbox" id="c-42474899" checked=""/><div class="controls bullet"><span class="by">reducesuffering</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473910">parent</a><span>|</span><a href="#42473880">next</a><span>|</span><label class="collapse" for="c-42474899">[-]</label><label class="expand" for="c-42474899">[3 more]</label></div><br/><div class="children"><div class="content">&gt; If AI takes over white collar work that&#x27;s still half of the world&#x27;s labor needs untouched.<p>I am continually <i>baffled</i> that people here throw this argument out and can&#x27;t imagine the second-order effects. If white collar work is automated by AGI, all the RnD to solve robotics beyond imagination will happen in a flash. The top AI labs, the people smartest enough to make this technology, all are focusing on automating AGI Researchers and from there follows everything, obviously.</div><br/><div id="42475019" class="c"><input type="checkbox" id="c-42475019" checked=""/><div class="controls bullet"><span class="by">brotchie</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474899">parent</a><span>|</span><a href="#42473880">next</a><span>|</span><label class="collapse" for="c-42475019">[-]</label><label class="expand" for="c-42475019">[2 more]</label></div><br/><div class="children"><div class="content">+1, the second and third order effects aren&#x27;t trivial.<p>We&#x27;re already seeing escape velocity in world modeling (see Google Veo2 and the latest Genesis LLM-based physics modeling framework).<p>The hardware for humanoid robots is 95% of the way there, the gap is control logic and intelligence, which is rapidly being closed.<p>Combine Veo2 world model, Genesis control planning, o3-style reasoning, and you&#x27;re pretty much there with blue collar work automation.<p>We&#x27;re only a few turns (&lt;12 months) away from an existence proof of a humanoid robot that can watch a Youtube video and then replicate the task in a novel environment. May take longer than that to productionize.<p>It&#x27;s really hard to think and project forward on an exponential. We&#x27;ve been on an exponential technology curve since the discovery of fire (at least). The 2nd order has kicked up over the last few years.<p>Not a rational approach to look back at robotics 2000-2022 and project that pace forwards. There&#x27;s more happening every month than in decades past.</div><br/><div id="42475158" class="c"><input type="checkbox" id="c-42475158" checked=""/><div class="controls bullet"><span class="by">philipkglass</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42475019">parent</a><span>|</span><a href="#42473880">next</a><span>|</span><label class="collapse" for="c-42475158">[-]</label><label class="expand" for="c-42475158">[1 more]</label></div><br/><div class="children"><div class="content">I hope that you&#x27;re both right. In 2004-2007 I saw self driving vehicles make lightning progress from the weak showing of the 2004 DARPA Grand Challenge to the impressive 2005 Grand Challenge winners and the even more impressive performance in the 2007 Urban Challenge. At the time I thought that full self driving vehicles would have a major commercial impact within 5 years. I expected truck and taxi drivers to be obsolete jobs in 10 years. 17 years after the Urban Challenge there are still millions of truck driver jobs in America and only Waymo seems to have a credible alternative to taxi drivers (even then, only in a small number of cities).</div><br/></div></div></div></div></div></div></div></div><div id="42473880" class="c"><input type="checkbox" id="c-42473880" checked=""/><div class="controls bullet"><span class="by">QuantumGood</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473576">parent</a><span>|</span><a href="#42473910">prev</a><span>|</span><a href="#42474132">next</a><span>|</span><label class="collapse" for="c-42473880">[-]</label><label class="expand" for="c-42473880">[1 more]</label></div><br/><div class="children"><div class="content">&quot;it will take over the world&quot;<p>Calibrating to the current hype cycle has been challenging with AI pronouncements.</div><br/></div></div><div id="42474132" class="c"><input type="checkbox" id="c-42474132" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473576">parent</a><span>|</span><a href="#42473880">prev</a><span>|</span><a href="#42473711">next</a><span>|</span><label class="collapse" for="c-42474132">[-]</label><label class="expand" for="c-42474132">[1 more]</label></div><br/><div class="children"><div class="content">You are telling a bunch of high earning individuals ($150k+) that they may be dramatically less valuable in the  eat future. Of course the goal posts will keep being pushed back and the acknowledgements will never come.</div><br/></div></div><div id="42473711" class="c"><input type="checkbox" id="c-42473711" checked=""/><div class="controls bullet"><span class="by">foobarqux</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473576">parent</a><span>|</span><a href="#42474132">prev</a><span>|</span><a href="#42473592">next</a><span>|</span><label class="collapse" for="c-42473711">[-]</label><label class="expand" for="c-42473711">[4 more]</label></div><br/><div class="children"><div class="content">You should look up the terms necessary and sufficient.</div><br/><div id="42473784" class="c"><input type="checkbox" id="c-42473784" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473711">parent</a><span>|</span><a href="#42473592">next</a><span>|</span><label class="collapse" for="c-42473784">[-]</label><label class="expand" for="c-42473784">[3 more]</label></div><br/><div class="children"><div class="content">The real issue is people constantly making up new goalposts to keep their outdated world view somewhat aligned with what we are seeing. But these two things are drifting apart faster and faster. Even I got surprised by how quickly the ARC benchmark was blown out of the water, and I&#x27;m pretty bullish on AI.</div><br/><div id="42473929" class="c"><input type="checkbox" id="c-42473929" checked=""/><div class="controls bullet"><span class="by">foobarqux</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473784">parent</a><span>|</span><a href="#42474447">next</a><span>|</span><label class="collapse" for="c-42473929">[-]</label><label class="expand" for="c-42473929">[1 more]</label></div><br/><div class="children"><div class="content">The ARC maintainers have explicitly said that passing the test was necessary but not sufficient so I don&#x27;t know where you come up with goal-post moving. (I personally don&#x27;t like the test; it is more about &quot;intuition&quot; or in-built priors, not reasoning).</div><br/></div></div><div id="42474447" class="c"><input type="checkbox" id="c-42474447" checked=""/><div class="controls bullet"><span class="by">manmal</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473784">parent</a><span>|</span><a href="#42473929">prev</a><span>|</span><a href="#42473592">next</a><span>|</span><label class="collapse" for="c-42474447">[-]</label><label class="expand" for="c-42474447">[1 more]</label></div><br/><div class="children"><div class="content">Are you like invested in LLM companies or something? You‘re pushing the agenda hard in this thread.</div><br/></div></div></div></div></div></div><div id="42473592" class="c"><input type="checkbox" id="c-42473592" checked=""/><div class="controls bullet"><span class="by">samvher</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473576">parent</a><span>|</span><a href="#42473711">prev</a><span>|</span><a href="#42475486">next</a><span>|</span><label class="collapse" for="c-42473592">[-]</label><label class="expand" for="c-42473592">[8 more]</label></div><br/><div class="children"><div class="content">What kind of preparation are you suggesting?</div><br/><div id="42473764" class="c"><input type="checkbox" id="c-42473764" checked=""/><div class="controls bullet"><span class="by">johnny_canuck</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473592">parent</a><span>|</span><a href="#42473668">next</a><span>|</span><label class="collapse" for="c-42473764">[-]</label><label class="expand" for="c-42473764">[3 more]</label></div><br/><div class="children"><div class="content">Start learning a trade</div><br/><div id="42474507" class="c"><input type="checkbox" id="c-42474507" checked=""/><div class="controls bullet"><span class="by">whynotminot</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473764">parent</a><span>|</span><a href="#42473881">next</a><span>|</span><label class="collapse" for="c-42474507">[-]</label><label class="expand" for="c-42474507">[1 more]</label></div><br/><div class="children"><div class="content">I feel like that’s just kicking the can a little further down the road.<p>Our value proposition as humans in a capitalist society is an increasingly fragile thing.</div><br/></div></div><div id="42473881" class="c"><input type="checkbox" id="c-42473881" checked=""/><div class="controls bullet"><span class="by">jorblumesea</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473764">parent</a><span>|</span><a href="#42474507">prev</a><span>|</span><a href="#42473668">next</a><span>|</span><label class="collapse" for="c-42473881">[-]</label><label class="expand" for="c-42473881">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s going to work when every white collar worker goes into the trades &#x2F;s<p>who is going to pay for residential electrical work lol and how much will you make if some guy from MIT is going to compete with you</div><br/></div></div></div></div><div id="42473668" class="c"><input type="checkbox" id="c-42473668" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473592">parent</a><span>|</span><a href="#42473764">prev</a><span>|</span><a href="#42475486">next</a><span>|</span><label class="collapse" for="c-42473668">[-]</label><label class="expand" for="c-42473668">[4 more]</label></div><br/><div class="children"><div class="content">This is far too broad to summarise here. You can read up on Sutskever or Bostrom or hell even Steven Hawking&#x27;s ideas (going in order from really deep to general topics). We need to discuss <i>everything</i> - from education over jobs and taxes all the way to the principles of politics, our economy and even the military. If we fail at this as a society, we will at the very least create a world where the people who own capital today massively benefit and become rich beyond imagination (despite having contributed nothing to it), while the majority of the population will be unemployable and forever left behind. And the worst case probably falls somewhere between the end of human civilisation and the end of our species.</div><br/><div id="42474176" class="c"><input type="checkbox" id="c-42474176" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473668">parent</a><span>|</span><a href="#42473691">next</a><span>|</span><label class="collapse" for="c-42474176">[-]</label><label class="expand" for="c-42474176">[2 more]</label></div><br/><div class="children"><div class="content">One way you can tell this isn&#x27;t realistic is that it&#x27;s the plot of Atlas Shrugged. If your economic intuitions produce that book it means they are wrong.<p>&gt; while the majority of the population will be unemployable and forever left behind<p>Productivity improvements increase employment. A superhuman AI is a productivity improvement.</div><br/></div></div><div id="42473691" class="c"><input type="checkbox" id="c-42473691" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473668">parent</a><span>|</span><a href="#42474176">prev</a><span>|</span><a href="#42475486">next</a><span>|</span><label class="collapse" for="c-42473691">[-]</label><label class="expand" for="c-42473691">[1 more]</label></div><br/><div class="children"><div class="content">What we&#x27;re going to do is punt the questions and then convince ourselves the outcome was inevitable and if anything it&#x27;s actually our fault.</div><br/></div></div></div></div></div></div><div id="42475486" class="c"><input type="checkbox" id="c-42475486" checked=""/><div class="controls bullet"><span class="by">bluerooibos</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473576">parent</a><span>|</span><a href="#42473592">prev</a><span>|</span><a href="#42473916">next</a><span>|</span><label class="collapse" for="c-42475486">[-]</label><label class="expand" for="c-42475486">[1 more]</label></div><br/><div class="children"><div class="content">The goalposts have moved, again and again.<p>It&#x27;s gone from &quot;well the output is incoherent&quot; to &quot;well it&#x27;s just spitting out stuff it&#x27;s already seen online&quot; to &quot;WELL...uhh IT CAN&#x27;T CREATE NEW&#x2F;NOVEL KNOWLEDGE&quot; in the space of 3-4 years.<p>It&#x27;s incredible.<p>We already have AGI.</div><br/></div></div><div id="42473916" class="c"><input type="checkbox" id="c-42473916" checked=""/><div class="controls bullet"><span class="by">levocardia</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473576">parent</a><span>|</span><a href="#42475486">prev</a><span>|</span><a href="#42473478">next</a><span>|</span><label class="collapse" for="c-42473916">[-]</label><label class="expand" for="c-42473916">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a little torn. ARC is really hard, and Francois is extremely smart and thoughtful about what intelligence means (the original &quot;On the Measure of Intelligence&quot; heavily influenced my ideas on how to think about AI).<p>On the other hand, there is a long, long history of AI achieving X but not being what we would casually refer to as &quot;generally intelligent,&quot; then people deciding X isn&#x27;t really intelligence; only when AI achieves Y will it be intelligence. Then AI achieves Y and...</div><br/></div></div></div></div><div id="42473478" class="c"><input type="checkbox" id="c-42473478" checked=""/><div class="controls bullet"><span class="by">amarcheschi</span><span>|</span><a href="#42473442">parent</a><span>|</span><a href="#42473576">prev</a><span>|</span><a href="#42475506">next</a><span>|</span><label class="collapse" for="c-42473478">[-]</label><label class="expand" for="c-42473478">[9 more]</label></div><br/><div class="children"><div class="content">I just googled arc agi questions, and it looks like it is similar to an iq test with raven matrix. Similar as in you have some examples of images before and after, then an image before and you have to guess the after.<p>Could anyone confirm if this is the only kind of questions in the benchmark? If yes, how come there is such a direct connection to &quot;oh this performs better than humans&quot; when llm can be quite better than us in understanding and forecasting patterns? I&#x27;m just curious, not trying to stir up controversies</div><br/><div id="42473537" class="c"><input type="checkbox" id="c-42473537" checked=""/><div class="controls bullet"><span class="by">zebomon</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473478">parent</a><span>|</span><a href="#42473635">next</a><span>|</span><label class="collapse" for="c-42473537">[-]</label><label class="expand" for="c-42473537">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a test on which (apparently until now) the vast majority of humans have far outperformed all machine systems.</div><br/><div id="42473559" class="c"><input type="checkbox" id="c-42473559" checked=""/><div class="controls bullet"><span class="by">patrickhogan1</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473537">parent</a><span>|</span><a href="#42473635">next</a><span>|</span><label class="collapse" for="c-42473559">[-]</label><label class="expand" for="c-42473559">[4 more]</label></div><br/><div class="children"><div class="content">But it’s not a test that directly shows general intelligence.<p>I am excited no less! This is huge improvement.<p>How does this do on SWE Bench?</div><br/><div id="42473613" class="c"><input type="checkbox" id="c-42473613" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473559">parent</a><span>|</span><a href="#42473635">next</a><span>|</span><label class="collapse" for="c-42473613">[-]</label><label class="expand" for="c-42473613">[3 more]</label></div><br/><div class="children"><div class="content">&gt;How does this do on SWE Bench?<p>71.7%</div><br/><div id="42473815" class="c"><input type="checkbox" id="c-42473815" checked=""/><div class="controls bullet"><span class="by">throwaway0123_5</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473613">parent</a><span>|</span><a href="#42473635">next</a><span>|</span><label class="collapse" for="c-42473815">[-]</label><label class="expand" for="c-42473815">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen this figure on a few tech news websites and reddit but can&#x27;t find an official source. If it was in the video I must have missed it, where is this coming from?</div><br/><div id="42473836" class="c"><input type="checkbox" id="c-42473836" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473815">parent</a><span>|</span><a href="#42473635">next</a><span>|</span><label class="collapse" for="c-42473836">[-]</label><label class="expand" for="c-42473836">[1 more]</label></div><br/><div class="children"><div class="content">It was in the video. I don&#x27;t know if Open ai have a page up yet</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42473635" class="c"><input type="checkbox" id="c-42473635" checked=""/><div class="controls bullet"><span class="by">Eridrus</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473478">parent</a><span>|</span><a href="#42473537">prev</a><span>|</span><a href="#42473552">next</a><span>|</span><label class="collapse" for="c-42473635">[-]</label><label class="expand" for="c-42473635">[2 more]</label></div><br/><div class="children"><div class="content">ML is quite good at understanding and forecasting patterns when you train on the data you want to forecast. LLMs manage to do so much because we just decided to train on everything on the internet and hope that it included everything we ever wanted to know.<p>This tries to create patterns that are intentionally not in the data and see if a system can generalize to them, which o3 super impressively does!</div><br/><div id="42473952" class="c"><input type="checkbox" id="c-42473952" checked=""/><div class="controls bullet"><span class="by">yunwal</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473635">parent</a><span>|</span><a href="#42473552">next</a><span>|</span><label class="collapse" for="c-42473952">[-]</label><label class="expand" for="c-42473952">[1 more]</label></div><br/><div class="children"><div class="content">ARC is in the dataset though? I mean I&#x27;m aware that there are new puzzles every day, but there&#x27;s still a very specific format and set of skills required to solve it. I&#x27;d bet a decent amount of money that humans get better at ARC with practice, so it seems strange to suggest that AI wouldn&#x27;t.</div><br/></div></div></div></div><div id="42473552" class="c"><input type="checkbox" id="c-42473552" checked=""/><div class="controls bullet"><span class="by">ALittleLight</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473478">parent</a><span>|</span><a href="#42473635">prev</a><span>|</span><a href="#42475506">next</a><span>|</span><label class="collapse" for="c-42473552">[-]</label><label class="expand" for="c-42473552">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it&#x27;s pretty similar to Raven&#x27;s.  The reason it is an interesting benchmark is because humans, even very young humans, &quot;get&quot; the test in the sense of understanding what it&#x27;s asking and being able to do pretty well on it - but LLMs have really struggled with the benchmark in the past.<p>Chollett (one of the creators of the ARC benchmark) has been saying it proves LLMs can&#x27;t reason.  The test questions are supposed to be unique and not in the model&#x27;s training set.  The fact that LLMs struggled with the ARC challenge suggested (to Chollett and others) that models weren&#x27;t &quot;Truly reasoning&quot; but rather just completing based on things they&#x27;d seen before - when the models were confronted with things they hadn&#x27;t seen before, the novel visual patterns, they really struggled.</div><br/></div></div></div></div><div id="42475506" class="c"><input type="checkbox" id="c-42475506" checked=""/><div class="controls bullet"><span class="by">Bjorkbat</span><span>|</span><a href="#42473442">parent</a><span>|</span><a href="#42473478">prev</a><span>|</span><a href="#42474145">next</a><span>|</span><label class="collapse" for="c-42475506">[-]</label><label class="expand" for="c-42475506">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s still an interesting way to measure general intellience, it&#x27;s just that o3 has demonstrated that you can actually achieve human performance on it by training it on the public training set and giving it ridiculous amounts of compute, which I imagine equates to ludicrously long chains-of-thought, and if I understand correctly more than one chain-of-thought per task (they mention sample sizes in the blog post, with o3-low using 6 and o3-high using 1024.  Not sure if these are chains-of-thought per task or what).<p>Once you look at it that way it the approach really doesn&#x27;t look like intelligence that&#x27;s able to generalize to novel domains.  It doesn&#x27;t pass the sniff test.  It looks a lot more like brute-forcing.<p>Which is probably why, in order to actually qualify for the leaderboard, they stipulate that you can&#x27;t use more than $10k more of compute.  Otherwise, it just sounds like brute-forcing.</div><br/></div></div><div id="42474145" class="c"><input type="checkbox" id="c-42474145" checked=""/><div class="controls bullet"><span class="by">Agentus</span><span>|</span><a href="#42473442">parent</a><span>|</span><a href="#42475506">prev</a><span>|</span><a href="#42473645">next</a><span>|</span><label class="collapse" for="c-42474145">[-]</label><label class="expand" for="c-42474145">[3 more]</label></div><br/><div class="children"><div class="content">how about a extra large dose of your skepticism.  is true intelligence really a thing and not just a vague human construct that tries to point out the mysterious unquantifiable combination of human behaviors?<p>humans clearly dont know what intelligence is unambiguously.  theres also no divinely ordained objective dictionary that one can point at to reference what true intelligence is.  a deep reflection of trying to pattern associate different human cognitive abilities indicates human cognitive capabilities arent that spectacular really.</div><br/><div id="42477321" class="c"><input type="checkbox" id="c-42477321" checked=""/><div class="controls bullet"><span class="by">MVissers</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474145">parent</a><span>|</span><a href="#42473645">next</a><span>|</span><label class="collapse" for="c-42477321">[-]</label><label class="expand" for="c-42477321">[2 more]</label></div><br/><div class="children"><div class="content">My guess as an amateur neuroscientist is that what we call intelligence is just a &#x27;measurement&#x27; of problem solving ability in different domains. Can be emotional, spatial, motor, reasoning, etc etc.<p>There is no special sauce in our brain. And we know how much compute there is in our brain– So we can roughly estimate when we&#x27;ll hit that with these &#x27;LLMs&#x27;.<p>Language is important in a human brain development as well. Kids who grow up deaf grow up vastly less intelligent unless they learn sign language. Language allow us to process complex concepts that our brain can learn to solve, without having to be in those complex environments.<p>So in hindsight, it&#x27;s easy to see why it took a language model to be able to solve general tasks and other types deep learning networks couldn&#x27;t.<p>I don&#x27;t really see any limits on these models.</div><br/><div id="42477470" class="c"><input type="checkbox" id="c-42477470" checked=""/><div class="controls bullet"><span class="by">Agentus</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42477321">parent</a><span>|</span><a href="#42473645">next</a><span>|</span><label class="collapse" for="c-42477470">[-]</label><label class="expand" for="c-42477470">[1 more]</label></div><br/><div class="children"><div class="content">interesting point about language.  but i wonder if people misattribute the reason why language is pivotal to human development.  your points are valid.
i see human behavior with regard to learning as 90% mimicry and 10% autonomous learning.  most of what humans believe in is taken on faith and passed on from the tribe to the individual.  rarely is it verified even partially let alone fully.  humans simple dont have the time or processing power to do that.  learning a thing without outside aid is vastly slower and more energy or brain intensive process than copy learning or learning through social institutions by dissemination.
the stunted development from lack of language might come more from the less ability to access the collective learning process that language enables and or greatly enhances.
i think a lot of learning even when combined with reasoning, deduction, etc really is at the mercy of brute force exploration to find a solution, which individuals are bad at but a society that collects random experienced “ah hah!” occurrences and passes them along is actually okay at.<p>i wonder if llms and language dont as so much allow us to process these complex environments but instead preload our brains to get a head start in processing those complex environments once we arrive in them.  i think llms store compressed relationships of the world which obviously has information loss from a neural mapping of the world that isnt just language based.  but that compressed relationships ie knowledge doesnt exactly backwardly map onto the world without it having a reverse key.  like artificially learning about real world stuff in school abstractly and then going into the real world, it takes time for that abstraction to snap fit upon the real world.<p>could you further elaborate on what you mean by limits, because im happy to play contrarian on what i think i interpret you to be saying there.<p>also to your main point:  what intelligence is.  yeah you sort of hit up my thoughts on intelligence.  its a combination of problem solving abilities in different domains.  its like an amalgam of cognitive processes that achieve an amalgam of capabilities.  while we can label alllllll that with a singular word, doesnt mean its all a singular process.  seems like its a composite.  moreover i think a big chunk of intelligence (but not all) is just brute forcing finding associations and then encoding those by some reflexive search&#x2F;retrieval.  a different part of intelligence of course is adaptibility and pattern finding.</div><br/></div></div></div></div></div></div><div id="42473645" class="c"><input type="checkbox" id="c-42473645" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#42473442">parent</a><span>|</span><a href="#42474145">prev</a><span>|</span><a href="#42473678">next</a><span>|</span><label class="collapse" for="c-42473645">[-]</label><label class="expand" for="c-42473645">[3 more]</label></div><br/><div class="children"><div class="content">From the statement where - this is a pretty tough test where AI scores low vs humans just last year, and AI can do it as good as humans may not be AGI which I agree, but it means something with all caps</div><br/><div id="42474576" class="c"><input type="checkbox" id="c-42474576" checked=""/><div class="controls bullet"><span class="by">manmal</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473645">parent</a><span>|</span><a href="#42473678">next</a><span>|</span><label class="collapse" for="c-42474576">[-]</label><label class="expand" for="c-42474576">[2 more]</label></div><br/><div class="children"><div class="content">Obviously, the multi billion dollar companies will try to satisfy the benchmarks they are not yet good in, as has always been the case.</div><br/><div id="42477283" class="c"><input type="checkbox" id="c-42477283" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474576">parent</a><span>|</span><a href="#42473678">next</a><span>|</span><label class="collapse" for="c-42477283">[-]</label><label class="expand" for="c-42477283">[1 more]</label></div><br/><div class="children"><div class="content">A valid conspiracy theory but I’ve heard that one everystep of the way to this point</div><br/></div></div></div></div></div></div><div id="42473678" class="c"><input type="checkbox" id="c-42473678" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#42473442">parent</a><span>|</span><a href="#42473645">prev</a><span>|</span><a href="#42473650">next</a><span>|</span><label class="collapse" for="c-42473678">[-]</label><label class="expand" for="c-42473678">[5 more]</label></div><br/><div class="children"><div class="content">&gt; truly general intelligence<p>Indistinguishable from goalpost moving like you said, but also no true Scotsman.<p>I&#x27;m curious what would happen in your eyes if we misattributed general intelligence to an AI model? What are the consequences of a false positive and how would they affect your life?<p>It&#x27;s really clear to me how intelligence fits into our reality as part of our social ontology. The attributes and their expression that each of us uses to ground our concept of the intelligent predicate differs wildly.<p>My personal theory is that we tend to have an exemplar-based dataset of intelligence, and each of us attempts to construct a parsimonious model of intelligence, but like all (mental) models, they can be useful but wrong. These models operate in a space where the trade off is completeness or consistency, and most folks, uncomfortable saying &quot;I don&#x27;t know&quot; lean toward being complete in their specification rather than consistent. The unfortunate side-effect is that we&#x27;re able to easily generate test data that highlights our model inconsistency - AI being a case in point.</div><br/><div id="42474351" class="c"><input type="checkbox" id="c-42474351" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473678">parent</a><span>|</span><a href="#42473650">next</a><span>|</span><label class="collapse" for="c-42474351">[-]</label><label class="expand" for="c-42474351">[4 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m curious what would happen in your eyes if we misattributed general intelligence to an AI model? What are the consequences of a false positive and how would they affect your life?<p>Rich people will think they can use the AI model instead of paying other people to do certain tasks.<p>The consequences could range from brilliant to utterly catastrophic, depending on the context and precise way in which this is done. But I&#x27;d lean toward the catastrophic.</div><br/><div id="42474615" class="c"><input type="checkbox" id="c-42474615" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474351">parent</a><span>|</span><a href="#42473650">next</a><span>|</span><label class="collapse" for="c-42474615">[-]</label><label class="expand" for="c-42474615">[3 more]</label></div><br/><div class="children"><div class="content">Any specifics? It&#x27;s difficult to separate this from generalized concern.</div><br/><div id="42474766" class="c"><input type="checkbox" id="c-42474766" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474615">parent</a><span>|</span><a href="#42473650">next</a><span>|</span><label class="collapse" for="c-42474766">[-]</label><label class="expand" for="c-42474766">[2 more]</label></div><br/><div class="children"><div class="content">someone wants a &quot;personal assistant&quot; and believes that the LLM has AGI ...<p>someone wants a &quot;planning officer&quot; and believes that the LLM has AGI ...<p>someone wants a &quot;hiring consultant&quot; and believes that the LLM has AGI ...<p>etc. etc.</div><br/><div id="42474865" class="c"><input type="checkbox" id="c-42474865" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474766">parent</a><span>|</span><a href="#42473650">next</a><span>|</span><label class="collapse" for="c-42474865">[-]</label><label class="expand" for="c-42474865">[1 more]</label></div><br/><div class="children"><div class="content">My apologies, but would it be possible to list the catastrophic consequences of these?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42473650" class="c"><input type="checkbox" id="c-42473650" checked=""/><div class="controls bullet"><span class="by">wslh</span><span>|</span><a href="#42473442">parent</a><span>|</span><a href="#42473678">prev</a><span>|</span><a href="#42473640">next</a><span>|</span><label class="collapse" for="c-42473650">[-]</label><label class="expand" for="c-42473650">[2 more]</label></div><br/><div class="children"><div class="content">&gt; My skeptical impression: it&#x27;s complete hubris to conflate ARC or any benchmark with truly general intelligence.<p>But isn’t it interesting to have several benchmarks? Even if it’s not about passing the Turing test, benchmarks serve a purpose—similar to how we measure microprocessors or other devices. Intelligence may be more elusive, but even if we had an oracle delivering the ultimate intelligence benchmark, we&#x27;d still argue about its limitations. Perhaps we&#x27;d claim it doesn&#x27;t measure creativity well, and we&#x27;d find ourselves revisiting the same debates about different kinds of intelligences.</div><br/><div id="42473673" class="c"><input type="checkbox" id="c-42473673" checked=""/><div class="controls bullet"><span class="by">zebomon</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473650">parent</a><span>|</span><a href="#42473640">next</a><span>|</span><label class="collapse" for="c-42473673">[-]</label><label class="expand" for="c-42473673">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s certainly interesting. I&#x27;m just not convinced it&#x27;s a test of general intelligence, and I don&#x27;t think we&#x27;ll know whether or not it is until it&#x27;s been able to operate in the real world to the same degree that our general intelligence does.</div><br/></div></div></div></div><div id="42473640" class="c"><input type="checkbox" id="c-42473640" checked=""/><div class="controls bullet"><span class="by">FrustratedMonky</span><span>|</span><a href="#42473442">parent</a><span>|</span><a href="#42473650">prev</a><span>|</span><a href="#42475810">next</a><span>|</span><label class="collapse" for="c-42473640">[-]</label><label class="expand" for="c-42473640">[8 more]</label></div><br/><div class="children"><div class="content">&quot; it&#x27;s complete hubris to conflate ARC or any benchmark with truly general intelligence.&quot;<p>Maybe it would help to include some human results in the AI ranking.<p>I think we&#x27;d find that Humans score lower?</div><br/><div id="42473824" class="c"><input type="checkbox" id="c-42473824" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473640">parent</a><span>|</span><a href="#42475810">next</a><span>|</span><label class="collapse" for="c-42473824">[-]</label><label class="expand" for="c-42473824">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure it&#x27;d help what they are talking about much.<p>E.g. go back in time and imagine you didn&#x27;t know there are ways for computers to be really good at performing integration yet as nobody had tried to make them. If someone asked you how to tell if something is intelligent &quot;the ability to easily reason integrations or calculate extremely large multiplications in mathematics&quot; might seem like a great test to make.<p>Skip forward to the modern era and it&#x27;s blatantly obvious CASes like Mathematica on a modern computer range between &quot;ridiculously better than the average person&quot; to &quot;impossibly better than the best person&quot; depending on the test. At the same time, it becomes painfully obvious a CAS is wholly unrelated to general intelligence and just because your test might have been solvable by an AGI doesn&#x27;t mean solving it proves something must have been an AGI.<p>So you come up with a new test... but you have the same problem as originally, it seems like anything non-human completely bombs and an AGI would do well... but how do you know the thing that solves it will have been an AGI for sure and not just another system clearly unrelated?<p>Short of a more clever way what GP is saying is the goalposts must keep being moved until it&#x27;s not so obvious the thing isn&#x27;t AGI, not that the average human gets a certain score which is worse.<p>.<p>All that aside, to answer your original question, in the presentation it was said the average human gets 85% and this was the first model to beat that. It was also said a second version is being worked on. They have some papers on their site about clear examples of why the current test clearly has a lot of testing unrelated to whether something is really AGI (a brute force method was shown to get &gt;50% in 2020) so their aim is to create a new goalpost test and see how things shake out this time.</div><br/><div id="42474228" class="c"><input type="checkbox" id="c-42474228" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473824">parent</a><span>|</span><a href="#42474084">next</a><span>|</span><label class="collapse" for="c-42474228">[-]</label><label class="expand" for="c-42474228">[5 more]</label></div><br/><div class="children"><div class="content">Generality is not binary. It&#x27;s a spectrum. And these models are already general in ways those things you&#x27;ve mentioned simply weren&#x27;t.<p>What exactly is AGI to you ? If it&#x27;s simply a generally intelligent machine then what are you waiting for ? What else is there to be sure of ? There&#x27;s nothing narrow about these models.<p>Humans love to believe they&#x27;re oh so special so much that there will always be debates on whether &#x27;AGI&#x27; has arrived. If you are waiting for that then you&#x27;ll be waiting a very long time, even if a machine arrives that takes us to the next frontier in science.</div><br/><div id="42476283" class="c"><input type="checkbox" id="c-42476283" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42474228">parent</a><span>|</span><a href="#42474084">next</a><span>|</span><label class="collapse" for="c-42476283">[-]</label><label class="expand" for="c-42476283">[4 more]</label></div><br/><div class="children"><div class="content">&gt; There&#x27;s nothing narrow about these models.<p>There is, they can&#x27;t create new ideas like humanity can. AGI should be able to replace humanity in terms of thinking, otherwise it isn&#x27;t general, you would just have a model specialized at reproducing thoughts and patterns human have thought before, it still can&#x27;t recreate science from scratch etc like humanity did, meaning it can&#x27;t do science properly.<p>Comparing an AI to a single individual is not how you measure AGI, if a group of humans perform better then you can&#x27;t use the AI to replace that group of humans, and thus the AI isn&#x27;t an AGI since it couldn&#x27;t replace the group humans.<p>So for example, if a group of programmers write more reliable programs than the AI, then you can&#x27;t replace that group of programmers with the AI, even if you duplicate that AI many times, since the AI isn&#x27;t capable of reproducing that same level of reliability when ran in parallel. This is due to an AI being run in parallel is still just an AI, an ensemble model is still just an AI, so the model the AI has to beat is the human ensemble called humanity.<p>If we lower the bar a bit at least it has to beat 100 000 humans working together to make a job obsolete, since all the tutorials etc and all such things are made by other humans as well if you remove the job those would also disappear and the AI would have to do the work of all of those, so if it can&#x27;t humans will still be needed.<p>Its possible you will be able to substitute part of those human ensembles with AI much sooner, but then we just call it a tool. (We also call narrow humans tools, it is fair)</div><br/><div id="42476448" class="c"><input type="checkbox" id="c-42476448" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42476283">parent</a><span>|</span><a href="#42474084">next</a><span>|</span><label class="collapse" for="c-42476448">[-]</label><label class="expand" for="c-42476448">[3 more]</label></div><br/><div class="children"><div class="content">I see these models create new ideas. At least at the standard humans are beholden to, so this just falls flat for me.</div><br/><div id="42476644" class="c"><input type="checkbox" id="c-42476644" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42476448">parent</a><span>|</span><a href="#42476629">next</a><span>|</span><label class="collapse" for="c-42476644">[-]</label><label class="expand" for="c-42476644">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t just need to create an idea, you need to be able to create ideas that on average progress in a positive direction. Humans can evidently do that, AI can&#x27;t, when AI work too much without human input you always end up with nonsense.<p>In order to write general program you need to have that skill. Every new code snipped needs to be evaluated by that system, whether it makes the codebase better or not. The lack of that ability is why you can&#x27;t just loop an LLM today to replace programmers. It might be possible to automate it for specific programming tasks, but not general purpose programming.<p>Overcoming that hurdle is not something I think LLM ever can do, you need a totally different kind of architecture, not something that is trained to mimic but trained to reason. I don&#x27;t know how to train something that can reason about noisy unstructured data, we will probably figure that out at some point but it probably wont be LLM as they are today.</div><br/></div></div></div></div></div></div></div></div><div id="42474084" class="c"><input type="checkbox" id="c-42474084" checked=""/><div class="controls bullet"><span class="by">FrustratedMonky</span><span>|</span><a href="#42473442">root</a><span>|</span><a href="#42473824">parent</a><span>|</span><a href="#42474228">prev</a><span>|</span><a href="#42475810">next</a><span>|</span><label class="collapse" for="c-42474084">[-]</label><label class="expand" for="c-42474084">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Short of a more clever way what GP is saying is the goalposts must keep being moved until it&#x27;s not so obvious the thing isn&#x27;t AGI, not that the average human gets a certain score which is worse.&quot;<p>Best way of stating that I&#x27;ve heard.<p>The Goal Post must keep moving, until we understand enough what is happening.<p>I usually poo-poo the goal post moving, but this makes sense.</div><br/></div></div></div></div></div></div></div></div><div id="42473883" class="c"><input type="checkbox" id="c-42473883" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42473442">prev</a><span>|</span><a href="#42474214">next</a><span>|</span><label class="collapse" for="c-42473883">[-]</label><label class="expand" for="c-42473883">[2 more]</label></div><br/><div class="children"><div class="content">I would like to see this repeated with my highly innovative HARC-HAGI, which is ARC-AGI but it uses hexagons instead of squares. I suspect humans would only make slightly more brain farts on HARC-HAGI than ARC-AGI, but O3 would fail very badly since it almost certainly has been specifically trained on squares.<p>I am not really trying to downplay O3. But this would be a simple test as to whether O3 is truly &quot;a system capable of adapting to tasks it has never encountered before&quot; versus novel ARC-AGI tasks it hasn&#x27;t encountered before.</div><br/><div id="42476439" class="c"><input type="checkbox" id="c-42476439" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#42473883">parent</a><span>|</span><a href="#42474214">next</a><span>|</span><label class="collapse" for="c-42476439">[-]</label><label class="expand" for="c-42476439">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s my take - even if the o3 as currently implemented is utterly useless on your HARC-HAGI, it is obvious that o3 coupled with its existing training pipeline trained briefly on the hexagons would excel on it, such that passing your benchmark doesn&#x27;t require any new technology.<p>Taking this a level of abstraction higher, I expect that in the next couple of years we&#x27;ll see systems like o3 given a runtime budget that they can use for training&#x2F;fine-tuning smaller models in an ad-hoc manner.</div><br/></div></div></div></div><div id="42474214" class="c"><input type="checkbox" id="c-42474214" checked=""/><div class="controls bullet"><span class="by">Balgair</span><span>|</span><a href="#42473883">prev</a><span>|</span><a href="#42476965">next</a><span>|</span><label class="collapse" for="c-42474214">[-]</label><label class="expand" for="c-42474214">[72 more]</label></div><br/><div class="children"><div class="content">Complete aside here: I used to do work with amputees and prosthetics. There is a standardized test (and I just cannot remember the name) that fits in a briefcase. It&#x27;s used for measuring the level of damage to the upper limbs and for prosthetic grading.<p>Basically, it&#x27;s got the dumbest and simplest things in it. Stuff like a lock and key, a glass of water and jug, common units of currency, a zipper, etc. It tests if you can do any of those common human tasks. Like pouring a glass of water, picking up coins from a flat surface (I chew off my nails so even an able person like me fails that), zip up a jacket, lock your own door, put on lipstick, etc.<p>We had hand prosthetics that could play Mozart at 5x speed on a baby grand, but could not pick up a silver dollar or zip a jacket even a little bit. To the patients, the hands were therefore about as useful as a metal hook (a common solution with amputees today, not just pirates!).<p>Again, a total aside here, but your comment just reminded me of that brown briefcase. Life, it turns out, is a lot more complex than we give it credit for. Even pouring the OJ can be, in rare cases, transcendent.</div><br/><div id="42474389" class="c"><input type="checkbox" id="c-42474389" checked=""/><div class="controls bullet"><span class="by">ubj</span><span>|</span><a href="#42474214">parent</a><span>|</span><a href="#42475434">next</a><span>|</span><label class="collapse" for="c-42474389">[-]</label><label class="expand" for="c-42474389">[33 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a lot of truth in this. I sometimes joke that robot benchmarks should focus on common household chores. Given a basket of mixed laundry, sort and fold everything into organized piles. Load a dishwasher given a sink and counters overflowing with dishes piled up haphazardly. Clean a bedroom that kids have trashed. We do these tasks almost without thinking, but the unstructured nature presents challenges for robots.</div><br/><div id="42474804" class="c"><input type="checkbox" id="c-42474804" checked=""/><div class="controls bullet"><span class="by">Balgair</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474389">parent</a><span>|</span><a href="#42474969">next</a><span>|</span><label class="collapse" for="c-42474804">[-]</label><label class="expand" for="c-42474804">[21 more]</label></div><br/><div class="children"><div class="content">I maintain that whoever invents a robust laundry <i>folding</i> robot will be a trillionaire. In that, I dump jumbled clean clothes straight from a dryer at it and out comes folded and sorted clothes (and those loner socks). I know we&#x27;re getting close, but I also know we&#x27;re not there yet.</div><br/><div id="42474932" class="c"><input type="checkbox" id="c-42474932" checked=""/><div class="controls bullet"><span class="by">smokel</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474804">parent</a><span>|</span><a href="#42474862">next</a><span>|</span><label class="collapse" for="c-42474932">[-]</label><label class="expand" for="c-42474932">[1 more]</label></div><br/><div class="children"><div class="content">We are certainly getting close!  In 2010, watching PR2 fold some unseen towels is similar to watching paint dry [1], but we can now enjoy robots attain lazy student-level laundry folding in real-time, as demonstrated by π₀[2].<p>[1] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=gy5g33S0Gzo" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=gy5g33S0Gzo</a><p>[2] <a href="https:&#x2F;&#x2F;www.physicalintelligence.company&#x2F;blog&#x2F;pi0" rel="nofollow">https:&#x2F;&#x2F;www.physicalintelligence.company&#x2F;blog&#x2F;pi0</a></div><br/></div></div><div id="42474862" class="c"><input type="checkbox" id="c-42474862" checked=""/><div class="controls bullet"><span class="by">yongjik</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474804">parent</a><span>|</span><a href="#42474932">prev</a><span>|</span><a href="#42475031">next</a><span>|</span><label class="collapse" for="c-42474862">[-]</label><label class="expand" for="c-42474862">[8 more]</label></div><br/><div class="children"><div class="content">I can live without folding laundry (I can just shove my undershirts in the closet, who cares if it&#x27;s not folded), but whoever manufactures a reliable auto-loading dishwasher will have my dollars.  Like, just put all your dishes in the sink and let the machine handle them.</div><br/><div id="42475282" class="c"><input type="checkbox" id="c-42475282" checked=""/><div class="controls bullet"><span class="by">Brybry</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474862">parent</a><span>|</span><a href="#42475031">next</a><span>|</span><label class="collapse" for="c-42475282">[-]</label><label class="expand" for="c-42475282">[7 more]</label></div><br/><div class="children"><div class="content">But if your dishwasher is empty is takes nearly the same amount of time&#x2F;effort to put dishes straight into the dishwasher that it does to put them in the sink.<p>I think I&#x27;d only really save time by having a robot that could unload my dishwasher and put up the clean dishes.</div><br/><div id="42475390" class="c"><input type="checkbox" id="c-42475390" checked=""/><div class="controls bullet"><span class="by">namibj</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42475282">parent</a><span>|</span><a href="#42475623">next</a><span>|</span><label class="collapse" for="c-42475390">[-]</label><label class="expand" for="c-42475390">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s called a second dishwasher: one is for taking out, the other for putting in. When the latter is full, turn it on, dirty dishes wait outside until the cycle finishes, when the dishwashers switch roles.</div><br/><div id="42475611" class="c"><input type="checkbox" id="c-42475611" checked=""/><div class="controls bullet"><span class="by">ptsneves</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42475390">parent</a><span>|</span><a href="#42476025">next</a><span>|</span><label class="collapse" for="c-42475611">[-]</label><label class="expand" for="c-42475611">[2 more]</label></div><br/><div class="children"><div class="content">I thought about this and it gets even better. You do not really need shelves as you just use the clean dishwasher as the storage place. I honestly don’t know why this is not a thing in big or wealthy homes.</div><br/><div id="42475780" class="c"><input type="checkbox" id="c-42475780" checked=""/><div class="controls bullet"><span class="by">jannyfer</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42475611">parent</a><span>|</span><a href="#42476025">next</a><span>|</span><label class="collapse" for="c-42475780">[-]</label><label class="expand" for="c-42475780">[1 more]</label></div><br/><div class="children"><div class="content">Another thing that bothers me is that dishwashers are low. As I get older, I’m finding it really annoying to bend down.<p>So get me a counter-level dishwasher cabinet and I’ll be happy!</div><br/></div></div></div></div><div id="42476025" class="c"><input type="checkbox" id="c-42476025" checked=""/><div class="controls bullet"><span class="by">oangemangut</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42475390">parent</a><span>|</span><a href="#42475611">prev</a><span>|</span><a href="#42475623">next</a><span>|</span><label class="collapse" for="c-42476025">[-]</label><label class="expand" for="c-42476025">[1 more]</label></div><br/><div class="children"><div class="content">We have a double drawer dishwasher and it hurts my brain watching friends plan around their nightly wash.</div><br/></div></div></div></div><div id="42475623" class="c"><input type="checkbox" id="c-42475623" checked=""/><div class="controls bullet"><span class="by">yongjik</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42475282">parent</a><span>|</span><a href="#42475390">prev</a><span>|</span><a href="#42475031">next</a><span>|</span><label class="collapse" for="c-42475623">[-]</label><label class="expand" for="c-42475623">[2 more]</label></div><br/><div class="children"><div class="content">Hmm, that doesn&#x27;t match my experience.  It takes me a lot more time to put dishes into the dishwasher, because it has different places for cutlery, bowls, dishes, and so on, and of course the existing structure never matches my bowls&#x27; size perfectly so I have to play tetris or run it with only 2&#x2F;3 filled (which will cause me to waste more time as I have to do dishes again sooner).<p>And that&#x27;s before we get to bits of sticky rice left on bowls, which somehow dishwashers never scrape off clean.  YMMV.</div><br/><div id="42475950" class="c"><input type="checkbox" id="c-42475950" checked=""/><div class="controls bullet"><span class="by">HPsquared</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42475623">parent</a><span>|</span><a href="#42475031">next</a><span>|</span><label class="collapse" for="c-42475950">[-]</label><label class="expand" for="c-42475950">[1 more]</label></div><br/><div class="children"><div class="content">1. Get a set of dishes that does fit nicely together in the dishwasher.<p>2. Start with a cold prewash, preferably with a little powder in there too. This massively helps with stubborn stuff. This one is annoying though because you might have to come back and switch it on after the prewash. A good job for the robot butler.</div><br/></div></div></div></div></div></div></div></div><div id="42475031" class="c"><input type="checkbox" id="c-42475031" checked=""/><div class="controls bullet"><span class="by">dweekly</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474804">parent</a><span>|</span><a href="#42474862">prev</a><span>|</span><a href="#42475291">next</a><span>|</span><label class="collapse" for="c-42475031">[-]</label><label class="expand" for="c-42475031">[1 more]</label></div><br/><div class="children"><div class="content">I was a believer in Gal&#x27;s FoldiMate but sadly it...folded.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;FoldiMate" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;FoldiMate</a></div><br/></div></div><div id="42475291" class="c"><input type="checkbox" id="c-42475291" checked=""/><div class="controls bullet"><span class="by">blargey</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474804">parent</a><span>|</span><a href="#42475031">prev</a><span>|</span><a href="#42474976">next</a><span>|</span><label class="collapse" for="c-42475291">[-]</label><label class="expand" for="c-42475291">[1 more]</label></div><br/><div class="children"><div class="content">At this point I&#x27;m not sure we&#x27;ll actually get a task-specific machine for laundry folding&#x2F;sorting before humanoid robots gain the capability to do it well enough.</div><br/></div></div><div id="42474976" class="c"><input type="checkbox" id="c-42474976" checked=""/><div class="controls bullet"><span class="by">sss111</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474804">parent</a><span>|</span><a href="#42475291">prev</a><span>|</span><a href="#42474851">next</a><span>|</span><label class="collapse" for="c-42474976">[-]</label><label class="expand" for="c-42474976">[1 more]</label></div><br/><div class="children"><div class="content">Honestly, a robot that can hang jumbled clean clothes instead of folding them would be good enough, it&#x27;s crazy how we don&#x27;t even have those.</div><br/></div></div><div id="42474851" class="c"><input type="checkbox" id="c-42474851" checked=""/><div class="controls bullet"><span class="by">jessekv</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474804">parent</a><span>|</span><a href="#42474976">prev</a><span>|</span><a href="#42474871">next</a><span>|</span><label class="collapse" for="c-42474851">[-]</label><label class="expand" for="c-42474851">[3 more]</label></div><br/><div class="children"><div class="content">I want it to lay out an outfit  every day too. Hopefully without hallucination.</div><br/><div id="42474931" class="c"><input type="checkbox" id="c-42474931" checked=""/><div class="controls bullet"><span class="by">stefs</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474851">parent</a><span>|</span><a href="#42474871">next</a><span>|</span><label class="collapse" for="c-42474931">[-]</label><label class="expand" for="c-42474931">[2 more]</label></div><br/><div class="children"><div class="content">it&#x27;s not hallucination, it&#x27;s high fashion</div><br/><div id="42475561" class="c"><input type="checkbox" id="c-42475561" checked=""/><div class="controls bullet"><span class="by">tanseydavid</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474931">parent</a><span>|</span><a href="#42474871">next</a><span>|</span><label class="collapse" for="c-42475561">[-]</label><label class="expand" for="c-42475561">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but the stupid robot laid out your Thursday-black-Turtleneck for you on Saturday morning.  That just won&#x27;t suffice.</div><br/></div></div></div></div></div></div><div id="42474871" class="c"><input type="checkbox" id="c-42474871" checked=""/><div class="controls bullet"><span class="by">nradov</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474804">parent</a><span>|</span><a href="#42474851">prev</a><span>|</span><a href="#42474812">next</a><span>|</span><label class="collapse" for="c-42474871">[-]</label><label class="expand" for="c-42474871">[2 more]</label></div><br/><div class="children"><div class="content">There is the Foldimate robot. I don&#x27;t know how well it works. It doesn&#x27;t seem to pair up socks.
(Deleted the web link, it might not be legitimate.)</div><br/><div id="42475018" class="c"><input type="checkbox" id="c-42475018" checked=""/><div class="controls bullet"><span class="by">smokel</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474871">parent</a><span>|</span><a href="#42474812">next</a><span>|</span><label class="collapse" for="c-42475018">[-]</label><label class="expand" for="c-42475018">[1 more]</label></div><br/><div class="children"><div class="content">Beware, this website is probably a scam.<p>Foldimate has gone bankrupt in 2021 [1], and the domain referral from foldimate.com to a 404 page at miele.com, suggests that it was Miele who bought up the remains, not a sketchy company with a &quot;.website&quot; top-level domain.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;FoldiMate" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;FoldiMate</a></div><br/></div></div></div></div><div id="42474812" class="c"><input type="checkbox" id="c-42474812" checked=""/><div class="controls bullet"><span class="by">oblio</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474804">parent</a><span>|</span><a href="#42474871">prev</a><span>|</span><a href="#42474850">next</a><span>|</span><label class="collapse" for="c-42474812">[-]</label><label class="expand" for="c-42474812">[2 more]</label></div><br/><div class="children"><div class="content">Laundry folding and laundry ironing, I would say.</div><br/><div id="42475335" class="c"><input type="checkbox" id="c-42475335" checked=""/><div class="controls bullet"><span class="by">musicale</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474812">parent</a><span>|</span><a href="#42474850">next</a><span>|</span><label class="collapse" for="c-42475335">[-]</label><label class="expand" for="c-42475335">[1 more]</label></div><br/><div class="children"><div class="content">Hopefully will detect whether a small child is inside or not.</div><br/></div></div></div></div><div id="42474850" class="c"><input type="checkbox" id="c-42474850" checked=""/><div class="controls bullet"><span class="by">imafish</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474804">parent</a><span>|</span><a href="#42474812">prev</a><span>|</span><a href="#42474969">next</a><span>|</span><label class="collapse" for="c-42474850">[-]</label><label class="expand" for="c-42474850">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I maintain that whoever invents a robust laundry folding robot will be a trillionaire<p>… so Elon Musk? :D</div><br/></div></div></div></div><div id="42474969" class="c"><input type="checkbox" id="c-42474969" checked=""/><div class="controls bullet"><span class="by">zamalek</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474389">parent</a><span>|</span><a href="#42474804">prev</a><span>|</span><a href="#42475434">next</a><span>|</span><label class="collapse" for="c-42474969">[-]</label><label class="expand" for="c-42474969">[11 more]</label></div><br/><div class="children"><div class="content">Slightly tangential, we already have amazing laundry robots. They are called washing and drying machines. We don&#x27;t give these marvels enough credit, mostly because they aren&#x27;t shaped like humans.<p>Humanoid robots are mostly a waste of time. Task-shaped robots are <i>much</i> easier to design, build, and maintain... and are more reliable. Some of the things you mention might needs humanoid versatility (loading the dishwasher), others would be far better served by purpose-built robots (laundry sorting).</div><br/><div id="42475009" class="c"><input type="checkbox" id="c-42475009" checked=""/><div class="controls bullet"><span class="by">jkaptur</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474969">parent</a><span>|</span><a href="#42474998">next</a><span>|</span><label class="collapse" for="c-42475009">[-]</label><label class="expand" for="c-42475009">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m embarrassed to say that I spent a few moments daydreaming about a robot that could wash my dishes. Then I thought about what to call it...</div><br/><div id="42475184" class="c"><input type="checkbox" id="c-42475184" checked=""/><div class="controls bullet"><span class="by">musicale</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42475009">parent</a><span>|</span><a href="#42475255">next</a><span>|</span><label class="collapse" for="c-42475184">[-]</label><label class="expand" for="c-42475184">[2 more]</label></div><br/><div class="children"><div class="content">Sadly current &quot;dishwasher&quot; models are neither self-loading nor unloading. (Seems like they should be able to take a tray of dishes, sort them, load them, and stack them after cleaning.)<p>Maybe &quot;busbot&quot; or &quot;scullerybot&quot;.</div><br/><div id="42475582" class="c"><input type="checkbox" id="c-42475582" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42475184">parent</a><span>|</span><a href="#42475255">next</a><span>|</span><label class="collapse" for="c-42475582">[-]</label><label class="expand" for="c-42475582">[1 more]</label></div><br/><div class="children"><div class="content">The problem is more doing it in sufficiently little space, and using little enough water and energy. Doing one that you just feed dishes individually and that immediate wash them and feed them to storage should be entirely viable, but it&#x27;d be wasteful, and it&#x27;d compete with people having multiple small drawer-style dishwashers, offering relatively little convenience over that.<p>It seems most people aren&#x27;t willing to pay for multiple dishwashers - even multiple small ones or set aside enough space, and that places severe constraints on trying to do better.</div><br/></div></div></div></div><div id="42475255" class="c"><input type="checkbox" id="c-42475255" checked=""/><div class="controls bullet"><span class="by">wsintra2022</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42475009">parent</a><span>|</span><a href="#42475184">prev</a><span>|</span><a href="#42474998">next</a><span>|</span><label class="collapse" for="c-42475255">[-]</label><label class="expand" for="c-42475255">[1 more]</label></div><br/><div class="children"><div class="content">Was it a dishwasher? Just give it all your unclean dishes and tell it to go, come back an hour later and they all washed and mostly dried!</div><br/></div></div></div></div><div id="42475216" class="c"><input type="checkbox" id="c-42475216" checked=""/><div class="controls bullet"><span class="by">rytis</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474969">parent</a><span>|</span><a href="#42474998">prev</a><span>|</span><a href="#42475551">next</a><span>|</span><label class="collapse" for="c-42475216">[-]</label><label class="expand" for="c-42475216">[4 more]</label></div><br/><div class="children"><div class="content">I agree. I don’t know where this obsession comes from. Obsession with resembling as close to humans as possible. We’re so far from being perfect. If you need proof just look at your teeth. Yes, we’re relatively universal, but a screwdriver is more efficient at driving in screws that our fingers. So please, stop wasting time building perfect universal robots, build more purpose-build ones.</div><br/><div id="42475535" class="c"><input type="checkbox" id="c-42475535" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42475216">parent</a><span>|</span><a href="#42475328">next</a><span>|</span><label class="collapse" for="c-42475535">[-]</label><label class="expand" for="c-42475535">[1 more]</label></div><br/><div class="children"><div class="content">The shape doesn&#x27;t matter! Non-humanoid shapes give minir advantages on specific tasks but for a general robot you&#x27;ll have a hard time finding a shape much more optimal than humanoid. And if you go with humanoid you have so much data available! Videos contain the information of which movements a robot should execude. Teleoperation is easy.
This is the bitter lesson! The shape doesn&#x27;t matter, any shape will work with the right architecture, data and training!</div><br/></div></div><div id="42475328" class="c"><input type="checkbox" id="c-42475328" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42475216">parent</a><span>|</span><a href="#42475535">prev</a><span>|</span><a href="#42475852">next</a><span>|</span><label class="collapse" for="c-42475328">[-]</label><label class="expand" for="c-42475328">[1 more]</label></div><br/><div class="children"><div class="content">Given we have shaped so many tasks to fit our bodies, it will be a long time before a bot able to do a variety&#x2F;majority of human tasks the human way won’t be valuable.<p>1000 machines specialized for 1000 tasks are great, but don’t deliver the same value as a single bot that can interchange with people flexibly.<p>Costly today, but wont be forever.</div><br/></div></div><div id="42475852" class="c"><input type="checkbox" id="c-42475852" checked=""/><div class="controls bullet"><span class="by">rowanG077</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42475216">parent</a><span>|</span><a href="#42475328">prev</a><span>|</span><a href="#42475551">next</a><span>|</span><label class="collapse" for="c-42475852">[-]</label><label class="expand" for="c-42475852">[1 more]</label></div><br/><div class="children"><div class="content">Purpose build robots are basically solved. Dishwashers, laundry machines, assembly robots, etc. the moat is a general purpose robot that can do what a human can do.</div><br/></div></div></div></div><div id="42475551" class="c"><input type="checkbox" id="c-42475551" checked=""/><div class="controls bullet"><span class="by">graemep</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474969">parent</a><span>|</span><a href="#42475216">prev</a><span>|</span><a href="#42475434">next</a><span>|</span><label class="collapse" for="c-42475551">[-]</label><label class="expand" for="c-42475551">[1 more]</label></div><br/><div class="children"><div class="content">Great examples. They are simple, reliable, efficient and effective. Far better than blindly copying what a human being does. Maybe there are equally clever ways of doing things like folding clothes.</div><br/></div></div></div></div></div></div><div id="42475434" class="c"><input type="checkbox" id="c-42475434" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#42474214">parent</a><span>|</span><a href="#42474389">prev</a><span>|</span><a href="#42474401">next</a><span>|</span><label class="collapse" for="c-42475434">[-]</label><label class="expand" for="c-42475434">[1 more]</label></div><br/><div class="children"><div class="content">This is expressed in AI research as Moravec&#x27;s paradox: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Moravec%27s_paradox" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Moravec%27s_paradox</a><p>Getting to LLMs that could talk to us turned out to be a lot easier than making something that could control even a robotic arm without precise programming, let alone a humanoid.</div><br/></div></div><div id="42474401" class="c"><input type="checkbox" id="c-42474401" checked=""/><div class="controls bullet"><span class="by">ecshafer</span><span>|</span><a href="#42474214">parent</a><span>|</span><a href="#42475434">prev</a><span>|</span><a href="#42474657">next</a><span>|</span><label class="collapse" for="c-42474401">[-]</label><label class="expand" for="c-42474401">[2 more]</label></div><br/><div class="children"><div class="content">I had a pretty bad case of tendinitis once, that basically made my thumb useless  since using it would cause extreme pain. That test seems really good. I could use a computer keyboard without any issue, but putting a belt on or pouring water was impossible.</div><br/><div id="42475591" class="c"><input type="checkbox" id="c-42475591" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474401">parent</a><span>|</span><a href="#42474657">next</a><span>|</span><label class="collapse" for="c-42475591">[-]</label><label class="expand" for="c-42475591">[1 more]</label></div><br/><div class="children"><div class="content">I had a swollen elbow a short while ago, and the amount of things I&#x27;ve never thought about that were affected by reduced elbow join mobility and an inability to put pressure on the elbow was disturbing.</div><br/></div></div></div></div><div id="42474657" class="c"><input type="checkbox" id="c-42474657" checked=""/><div class="controls bullet"><span class="by">alexose</span><span>|</span><a href="#42474214">parent</a><span>|</span><a href="#42474401">prev</a><span>|</span><a href="#42475097">next</a><span>|</span><label class="collapse" for="c-42474657">[-]</label><label class="expand" for="c-42474657">[1 more]</label></div><br/><div class="children"><div class="content">It feels like there&#x27;s a whole class of information that easily shorthanded, but really hard to explain to novices.<p>I think a lot about carpentry.  From the outside, it&#x27;s pretty easy:  Just make the wood into the right shape and stick it together.  But as one progresses, the intricacies become more apparent.  Variations in the wood, the direction of the grain, the seasonal variations in thickness, joinery techniques that are durable but also time efficient.<p>The way this information connects is highly multisensory and multimodal.  I now know which species of wood to use for which applications.  This knowledge was hard won through many, many mistakes and trials that took place at my home, the hardware store, the lumberyard, on YouTube, from my neighbor Steve, and in books written by experts.</div><br/></div></div><div id="42475097" class="c"><input type="checkbox" id="c-42475097" checked=""/><div class="controls bullet"><span class="by">drdrey</span><span>|</span><a href="#42474214">parent</a><span>|</span><a href="#42474657">prev</a><span>|</span><a href="#42474695">next</a><span>|</span><label class="collapse" for="c-42475097">[-]</label><label class="expand" for="c-42475097">[2 more]</label></div><br/><div class="children"><div class="content">I think assembling Legos would be a cool robot benchmark: you need to parse the instructions, locate the pieces you need, pick them up, orient them, snap them to your current assembly, visually check if you achieved the desired state, repeat</div><br/><div id="42477919" class="c"><input type="checkbox" id="c-42477919" checked=""/><div class="controls bullet"><span class="by">serpix</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42475097">parent</a><span>|</span><a href="#42474695">next</a><span>|</span><label class="collapse" for="c-42477919">[-]</label><label class="expand" for="c-42477919">[1 more]</label></div><br/><div class="children"><div class="content">I agree. Watching my toddler daughter build with small legos makes me understand how incredible fine motor skills are as even with small fingers some of the blocks are just too hard to snap together.</div><br/></div></div></div></div><div id="42474695" class="c"><input type="checkbox" id="c-42474695" checked=""/><div class="controls bullet"><span class="by">Method-X</span><span>|</span><a href="#42474214">parent</a><span>|</span><a href="#42475097">prev</a><span>|</span><a href="#42474384">next</a><span>|</span><label class="collapse" for="c-42474695">[-]</label><label class="expand" for="c-42474695">[2 more]</label></div><br/><div class="children"><div class="content">Was it the Southampton hand assessment procedure?</div><br/><div id="42474774" class="c"><input type="checkbox" id="c-42474774" checked=""/><div class="controls bullet"><span class="by">Balgair</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474695">parent</a><span>|</span><a href="#42474384">next</a><span>|</span><label class="collapse" for="c-42474774">[-]</label><label class="expand" for="c-42474774">[1 more]</label></div><br/><div class="children"><div class="content">Yes! Thank you!<p><a href="https:&#x2F;&#x2F;www.shap.ecs.soton.ac.uk&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.shap.ecs.soton.ac.uk&#x2F;</a></div><br/></div></div></div></div><div id="42474384" class="c"><input type="checkbox" id="c-42474384" checked=""/><div class="controls bullet"><span class="by">m463</span><span>|</span><a href="#42474214">parent</a><span>|</span><a href="#42474695">prev</a><span>|</span><a href="#42474497">next</a><span>|</span><label class="collapse" for="c-42474384">[-]</label><label class="expand" for="c-42474384">[7 more]</label></div><br/><div class="children"><div class="content">It would be interesting to see trick questions.<p>Like in your test<p>a hand grenade and a pin - don&#x27;t pull the pin.<p>Or maybe a mousetrap? but maybe that would be defused?<p>in the ai test...<p>or Global Thermonuclear War, the only winning move is...</div><br/><div id="42474556" class="c"><input type="checkbox" id="c-42474556" checked=""/><div class="controls bullet"><span class="by">HPsquared</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474384">parent</a><span>|</span><a href="#42474553">next</a><span>|</span><label class="collapse" for="c-42474556">[-]</label><label class="expand" for="c-42474556">[4 more]</label></div><br/><div class="children"><div class="content">Gaming streams being in the training data, it might pull the pin because &quot;that&#x27;s what you do&quot;.</div><br/><div id="42474723" class="c"><input type="checkbox" id="c-42474723" checked=""/><div class="controls bullet"><span class="by">8note</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474556">parent</a><span>|</span><a href="#42474553">next</a><span>|</span><label class="collapse" for="c-42474723">[-]</label><label class="expand" for="c-42474723">[3 more]</label></div><br/><div class="children"><div class="content">or, because it has to give an output, and pulling the pin is the only option</div><br/><div id="42475692" class="c"><input type="checkbox" id="c-42475692" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474723">parent</a><span>|</span><a href="#42474940">next</a><span>|</span><label class="collapse" for="c-42475692">[-]</label><label class="expand" for="c-42475692">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s also the option of not pulling the pin, and shooting your enemies as they instinctively run from what they think is a live grenade. Saw it on a TV show the other day.</div><br/></div></div></div></div></div></div><div id="42474553" class="c"><input type="checkbox" id="c-42474553" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474384">parent</a><span>|</span><a href="#42474556">prev</a><span>|</span><a href="#42474497">next</a><span>|</span><label class="collapse" for="c-42474553">[-]</label><label class="expand" for="c-42474553">[2 more]</label></div><br/><div class="children"><div class="content">to move first!</div><br/><div id="42475851" class="c"><input type="checkbox" id="c-42475851" checked=""/><div class="controls bullet"><span class="by">m463</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474553">parent</a><span>|</span><a href="#42474497">next</a><span>|</span><label class="collapse" for="c-42475851">[-]</label><label class="expand" for="c-42475851">[1 more]</label></div><br/><div class="children"><div class="content">oh crap. lol!</div><br/></div></div></div></div></div></div><div id="42474497" class="c"><input type="checkbox" id="c-42474497" checked=""/><div class="controls bullet"><span class="by">croemer</span><span>|</span><a href="#42474214">parent</a><span>|</span><a href="#42474384">prev</a><span>|</span><a href="#42476076">next</a><span>|</span><label class="collapse" for="c-42474497">[-]</label><label class="expand" for="c-42474497">[13 more]</label></div><br/><div class="children"><div class="content">&gt; We had hand prosthetics that could play Mozart at 5x speed on a baby grand, but could not pick up a silver dollar or zip a jacket even a little bit. &quot;<p>I must be missing something, how can they be able to play Mozart at 5x speed with their prosthetics but not zip a jacket? They could press keys but not do tasks requiring feedback?<p>Or did you mean they used to play Mozart at 5x speed before they became amputees?</div><br/><div id="42474725" class="c"><input type="checkbox" id="c-42474725" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474497">parent</a><span>|</span><a href="#42474667">next</a><span>|</span><label class="collapse" for="c-42474725">[-]</label><label class="expand" for="c-42474725">[3 more]</label></div><br/><div class="children"><div class="content">Playing a piano involves pushing down on the right keys with the right force at the right time, but that could be pre-programmed well before computers. The self-playing piano in the saloon in Westworld wasn&#x27;t a <i>huge</i> anachronism, such things slightly overlapped with the Wild West era: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Player_piano" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Player_piano</a><p>Picking up a 1mm thick metal disk from a flat surface requires the user gives the exact time, place, and force, and I&#x27;m not even sure what considerations it needs for surface materials (e.g. slightly squishy fake skin) and&#x2F;or tip shapes (e.g. fake nails).</div><br/><div id="42474777" class="c"><input type="checkbox" id="c-42474777" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474725">parent</a><span>|</span><a href="#42474667">next</a><span>|</span><label class="collapse" for="c-42474777">[-]</label><label class="expand" for="c-42474777">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Picking up a 1mm thick metal disk from a flat surface requires the user gives the exact time, place, and force<p>place sure but can&#x27;t you cheat a bit for time and force with compliance(&quot;impedance control&quot;)?</div><br/><div id="42475181" class="c"><input type="checkbox" id="c-42475181" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474777">parent</a><span>|</span><a href="#42474667">next</a><span>|</span><label class="collapse" for="c-42475181">[-]</label><label class="expand" for="c-42475181">[1 more]</label></div><br/><div class="children"><div class="content">In theory, apparently not in practice.</div><br/></div></div></div></div></div></div><div id="42474667" class="c"><input type="checkbox" id="c-42474667" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474497">parent</a><span>|</span><a href="#42474725">prev</a><span>|</span><a href="#42475842">next</a><span>|</span><label class="collapse" for="c-42474667">[-]</label><label class="expand" for="c-42474667">[1 more]</label></div><br/><div class="children"><div class="content">Imagine a prosthetic &#x27;hand&#x27; that has 5 regular fingers, rather than 4 fingers and a thumb. It would be able to play a piano just fine, but be unable to grasp anything small, like a zipper.</div><br/></div></div><div id="42475842" class="c"><input type="checkbox" id="c-42475842" checked=""/><div class="controls bullet"><span class="by">n144q</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474497">parent</a><span>|</span><a href="#42474667">prev</a><span>|</span><a href="#42474711">next</a><span>|</span><label class="collapse" for="c-42475842">[-]</label><label class="expand" for="c-42475842">[1 more]</label></div><br/><div class="children"><div class="content">Well, you see, while the original comment says they could play at 5x speed, it does not say it plays at that speed <i>well</i> or play it beautifully. Any teacher or any student who learned piano for a while will tell you that this matters a lot, especially for classical music -- being able to accurately play at an even tempo with the correct dynamics and articulation is hard and is what differentiates a beginner&#x2F;intermediate player from an advanced one. In fact, one mistake many students make is playing a piece too fast when they are not ready, and teachers really want students to practice very slowly.<p>My point is -- being able to zip a jacket is all about those subtle actions, and could actually be harder than &quot;just&quot; playing piano fast.</div><br/></div></div><div id="42474711" class="c"><input type="checkbox" id="c-42474711" checked=""/><div class="controls bullet"><span class="by">8note</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474497">parent</a><span>|</span><a href="#42475842">prev</a><span>|</span><a href="#42474810">next</a><span>|</span><label class="collapse" for="c-42474711">[-]</label><label class="expand" for="c-42474711">[2 more]</label></div><br/><div class="children"><div class="content">zipping up a jacket is really hard to do, and requires very precise movements and coordination between hands.<p>playing mozart is much more forgiving in terms of the number of different motions you have to make in different directions, the amount of pressure to apply, and even the black keys are much bigger than large sized zipper tongues.</div><br/><div id="42474793" class="c"><input type="checkbox" id="c-42474793" checked=""/><div class="controls bullet"><span class="by">Balgair</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474711">parent</a><span>|</span><a href="#42474810">next</a><span>|</span><label class="collapse" for="c-42474793">[-]</label><label class="expand" for="c-42474793">[1 more]</label></div><br/><div class="children"><div class="content">Pretty much. The issue with zippers is that the fabric moves about in unpredictable ways. Piano playing was just movement programs. Zipping required (surprisingly) fast feedback. Also, gripping is somewhat tough compared to pressing.</div><br/></div></div></div></div><div id="42474810" class="c"><input type="checkbox" id="c-42474810" checked=""/><div class="controls bullet"><span class="by">oblio</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474497">parent</a><span>|</span><a href="#42474711">prev</a><span>|</span><a href="#42474696">next</a><span>|</span><label class="collapse" for="c-42474810">[-]</label><label class="expand" for="c-42474810">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m far from a piano player, but I can definitely push piano buttons quite quickly while zipping up my jacket when it&#x27;s cold and&#x2F;or wet outside is really difficult.<p>Even more so for picking up coins from a flat surface.<p>For robotics, it&#x27;s kind of obvious, speed is rarely an issue, so the &quot;5x&quot; part is almost trivial. And you can program the sequence quite easily, so that&#x27;s also doable. Piano keys are big and obvious and an ergonomically designed interface meant to be relatively easy to press, ergo easy even for a prosthetic. A small coin on a flat surface is far from ergonomic.</div><br/><div id="42474913" class="c"><input type="checkbox" id="c-42474913" checked=""/><div class="controls bullet"><span class="by">yongjik</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474810">parent</a><span>|</span><a href="#42474846">next</a><span>|</span><label class="collapse" for="c-42474913">[-]</label><label class="expand" for="c-42474913">[1 more]</label></div><br/><div class="children"><div class="content">I play piano as a hobby, and the funny thing is, if my hands are so cold that I can&#x27;t zip up my jacket, there&#x27;s no way I can play anything well.  I know it&#x27;s not quite zipping up jackets ;) but a human playing the piano does require a fast feedback loop.</div><br/></div></div><div id="42474846" class="c"><input type="checkbox" id="c-42474846" checked=""/><div class="controls bullet"><span class="by">croemer</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474810">parent</a><span>|</span><a href="#42474913">prev</a><span>|</span><a href="#42474696">next</a><span>|</span><label class="collapse" for="c-42474846">[-]</label><label class="expand" for="c-42474846">[2 more]</label></div><br/><div class="children"><div class="content">But how do you deliberately control those fingers to actually play yourself what you have in mind rather than something preprogrammed? Surely the idea of a prosthetic does not just mean &quot;a robot that is connected to your body&quot;, but something that the owner control with your mind.</div><br/><div id="42475635" class="c"><input type="checkbox" id="c-42475635" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474846">parent</a><span>|</span><a href="#42474696">next</a><span>|</span><label class="collapse" for="c-42475635">[-]</label><label class="expand" for="c-42475635">[1 more]</label></div><br/><div class="children"><div class="content">Nobody said anything about deliberately controlling those fingers to play yourself. Clearly it&#x27;s not something you do for the sake of the enjoyment of playing, but more likely a demonstration of the dexterity of the prosthesis and ability to program it for complex tasks.<p>The idea of a prosthesis is to help you regain functionality. If the best way of doing that is through automation, then it&#x27;d make little sense not to.</div><br/></div></div></div></div></div></div><div id="42474696" class="c"><input type="checkbox" id="c-42474696" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474497">parent</a><span>|</span><a href="#42474810">prev</a><span>|</span><a href="#42476076">next</a><span>|</span><label class="collapse" for="c-42474696">[-]</label><label class="expand" for="c-42474696">[1 more]</label></div><br/><div class="children"><div class="content">Thumb not opposable?</div><br/></div></div></div></div><div id="42476076" class="c"><input type="checkbox" id="c-42476076" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#42474214">parent</a><span>|</span><a href="#42474497">prev</a><span>|</span><a href="#42474381">next</a><span>|</span><label class="collapse" for="c-42476076">[-]</label><label class="expand" for="c-42476076">[1 more]</label></div><br/><div class="children"><div class="content">We detached this subthread from <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42473419">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42473419</a><p>(nothing wrong with it! I&#x27;m just trying to prune the top subthread)</div><br/></div></div><div id="42475804" class="c"><input type="checkbox" id="c-42475804" checked=""/><div class="controls bullet"><span class="by">xnx</span><span>|</span><a href="#42474214">parent</a><span>|</span><a href="#42474381">prev</a><span>|</span><a href="#42474790">next</a><span>|</span><label class="collapse" for="c-42475804">[-]</label><label class="expand" for="c-42475804">[1 more]</label></div><br/><div class="children"><div class="content">Despite lake of fearsome teeth or claws, humans are <i>way</i> op due to brain, hand dexterity, and balance.</div><br/></div></div><div id="42474790" class="c"><input type="checkbox" id="c-42474790" checked=""/><div class="controls bullet"><span class="by">oblio</span><span>|</span><a href="#42474214">parent</a><span>|</span><a href="#42475804">prev</a><span>|</span><a href="#42475603">next</a><span>|</span><label class="collapse" for="c-42474790">[-]</label><label class="expand" for="c-42474790">[5 more]</label></div><br/><div class="children"><div class="content">This was actually discovered quite early on in the history of AI:<p>&gt; Rodney Brooks explains that, according to early AI research, intelligence was &quot;best characterized as the things that highly educated male scientists found challenging&quot;, such as chess, symbolic integration, proving mathematical theorems and solving complicated word algebra problems. &quot;The things that children of four or five years could do effortlessly, such as visually distinguishing between a coffee cup and a chair, or walking around on two legs, or finding their way from their bedroom to the living room were not thought of as activities requiring intelligence.&quot;<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Moravec%27s_paradox" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Moravec%27s_paradox</a></div><br/><div id="42474924" class="c"><input type="checkbox" id="c-42474924" checked=""/><div class="controls bullet"><span class="by">bawolff</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474790">parent</a><span>|</span><a href="#42475603">next</a><span>|</span><label class="collapse" for="c-42474924">[-]</label><label class="expand" for="c-42474924">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know why people always feel the need to gender these things. Highly educated female scientists generally find the same things challenging.</div><br/><div id="42475209" class="c"><input type="checkbox" id="c-42475209" checked=""/><div class="controls bullet"><span class="by">robocat</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474924">parent</a><span>|</span><a href="#42475576">next</a><span>|</span><label class="collapse" for="c-42475209">[-]</label><label class="expand" for="c-42475209">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know why anyone would blame people as though someone is making an explicit choice. I find your choice of words to be insulting to the OP.<p>We learn our language and stereotypes subconciously from our society, and it is no easy thing to fight against that.</div><br/></div></div><div id="42475576" class="c"><input type="checkbox" id="c-42475576" checked=""/><div class="controls bullet"><span class="by">Barrin92</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42474924">parent</a><span>|</span><a href="#42475209">prev</a><span>|</span><a href="#42475603">next</a><span>|</span><label class="collapse" for="c-42475576">[-]</label><label class="expand" for="c-42475576">[2 more]</label></div><br/><div class="children"><div class="content">&gt;I don&#x27;t know why people always feel the need to gender these things<p>Because it&#x27;s relevant to the point being made, i.e. that these tests reflect the biases and interests of the people who make them. This is true not just for AI tests, but intelligence test applied to humans. That Demis Hassabis, a chess player and video game designer, decided to test his machine on video games, Go and chess probably is not an accident.<p>The more interesting question is why people respond so apprehensively to pointing out a very obvious problem and bias in test design.</div><br/><div id="42476335" class="c"><input type="checkbox" id="c-42476335" checked=""/><div class="controls bullet"><span class="by">bawolff</span><span>|</span><a href="#42474214">root</a><span>|</span><a href="#42475576">parent</a><span>|</span><a href="#42475603">next</a><span>|</span><label class="collapse" for="c-42476335">[-]</label><label class="expand" for="c-42476335">[1 more]</label></div><br/><div class="children"><div class="content">&gt; i.e. that these tests reflect the biases and interests of the people who make them<p>Of course. However i believe we can&#x27;t move past that without being honest about where these biases are coming from. Many things in our world are the result of gender bias, both subtle and overt. However, at least at first glance, this does not appear to be one of them, and statements like the grandparent&#x27;s quote serve to perpetuate such biases further.</div><br/></div></div></div></div></div></div></div></div><div id="42475603" class="c"><input type="checkbox" id="c-42475603" checked=""/><div class="controls bullet"><span class="by">MarcelOlsz</span><span>|</span><a href="#42474214">parent</a><span>|</span><a href="#42474790">prev</a><span>|</span><a href="#42474430">next</a><span>|</span><label class="collapse" for="c-42475603">[-]</label><label class="expand" for="c-42475603">[1 more]</label></div><br/><div class="children"><div class="content">&gt;We had hand prosthetics that could play Mozart at 5x speed on a baby grand<p>I&#x27;d love to know more about this.</div><br/></div></div><div id="42474430" class="c"><input type="checkbox" id="c-42474430" checked=""/><div class="controls bullet"><span class="by">CooCooCaCha</span><span>|</span><a href="#42474214">parent</a><span>|</span><a href="#42475603">prev</a><span>|</span><a href="#42476965">next</a><span>|</span><label class="collapse" for="c-42474430">[-]</label><label class="expand" for="c-42474430">[1 more]</label></div><br/><div class="children"><div class="content">That’s why the goal isn’t just benchmark scores, it’s <i>reliable</i> and robust intelligence.<p>In that sense, the goalposts haven’t moved in a long time despite claims from AI enthusiasts that people are constantly moving goalposts.</div><br/></div></div></div></div><div id="42476965" class="c"><input type="checkbox" id="c-42476965" checked=""/><div class="controls bullet"><span class="by">Engineering-MD</span><span>|</span><a href="#42474214">prev</a><span>|</span><a href="#42473456">next</a><span>|</span><label class="collapse" for="c-42476965">[-]</label><label class="expand" for="c-42476965">[15 more]</label></div><br/><div class="children"><div class="content">Can I just say what a dick move it was to do this as a 12 days of Christmas. I mean to be honest I agree with the arguments this isn’t as impressive as my initial impression, but they clearly intended it to be shocking&#x2F;a show of possible AGI, which is rightly scary.<p>It feels so insensitive to that right before a major holiday when the likely outcome is a lot of people feeling less secure in their career&#x2F;job&#x2F;life.<p>Thanks again openAI for showing us you don’t give a shit about actual people.</div><br/><div id="42477000" class="c"><input type="checkbox" id="c-42477000" checked=""/><div class="controls bullet"><span class="by">XenophileJKO</span><span>|</span><a href="#42476965">parent</a><span>|</span><a href="#42478207">next</a><span>|</span><label class="collapse" for="c-42477000">[-]</label><label class="expand" for="c-42477000">[2 more]</label></div><br/><div class="children"><div class="content">Or maybe the target audience that watches 12 launch videos in the morning are genuninely excited about the new model. The intended it to be a preview of something to look forward to.<p>What a weird way to react to this.</div><br/><div id="42478183" class="c"><input type="checkbox" id="c-42478183" checked=""/><div class="controls bullet"><span class="by">achierius</span><span>|</span><a href="#42476965">root</a><span>|</span><a href="#42477000">parent</a><span>|</span><a href="#42478207">next</a><span>|</span><label class="collapse" for="c-42478183">[-]</label><label class="expand" for="c-42478183">[1 more]</label></div><br/><div class="children"><div class="content">It sounds like you aren&#x27;t thinking about this that deeply then. Or at least not understanding that many smart (and financially disinterested) people who are, are coming to concerning conclusions.<p><a href="https:&#x2F;&#x2F;www.transformernews.ai&#x2F;p&#x2F;richard-ngo-openai-resign-safety" rel="nofollow">https:&#x2F;&#x2F;www.transformernews.ai&#x2F;p&#x2F;richard-ngo-openai-resign-s...</a><p>&gt;But while the “making AGI” part of the mission seems well on track, it feels like I (and others) have gradually realized how much harder it is to contribute in a robustly positive way to the “succeeding” part of the mission, especially when it comes to preventing existential risks to humanity.<p>Almost every single one of the people OpenAI had hired to work on AI safety have left the firm with similar messages. Perhaps you should at least consider the thinking of experts?</div><br/></div></div></div></div><div id="42478207" class="c"><input type="checkbox" id="c-42478207" checked=""/><div class="controls bullet"><span class="by">achierius</span><span>|</span><a href="#42476965">parent</a><span>|</span><a href="#42477000">prev</a><span>|</span><a href="#42478144">next</a><span>|</span><label class="collapse" for="c-42478207">[-]</label><label class="expand" for="c-42478207">[1 more]</label></div><br/><div class="children"><div class="content">I feel you. It&#x27;s tough trying to think about what we can do to avert this; even to the extent that individuals are often powerless, in this regard it feels worse than almost anything that&#x27;s come before.</div><br/></div></div><div id="42476986" class="c"><input type="checkbox" id="c-42476986" checked=""/><div class="controls bullet"><span class="by">mirkodrummer</span><span>|</span><a href="#42476965">parent</a><span>|</span><a href="#42478144">prev</a><span>|</span><a href="#42477227">next</a><span>|</span><label class="collapse" for="c-42476986">[-]</label><label class="expand" for="c-42476986">[2 more]</label></div><br/><div class="children"><div class="content">There is no AGI it’s just marketing, this stuff if over hyped, enjoy your holidays you won’t lose your job ;)</div><br/><div id="42478244" class="c"><input type="checkbox" id="c-42478244" checked=""/><div class="controls bullet"><span class="by">Engineering-MD</span><span>|</span><a href="#42476965">root</a><span>|</span><a href="#42476986">parent</a><span>|</span><a href="#42477227">next</a><span>|</span><label class="collapse" for="c-42478244">[-]</label><label class="expand" for="c-42478244">[1 more]</label></div><br/><div class="children"><div class="content">I agree, it’s just more about the intent than anything else, like boasting about your amazing new job when someone has recently been made redundant, just before Christmas.</div><br/></div></div></div></div><div id="42477227" class="c"><input type="checkbox" id="c-42477227" checked=""/><div class="controls bullet"><span class="by">OldGreenYodaGPT</span><span>|</span><a href="#42476965">parent</a><span>|</span><a href="#42476986">prev</a><span>|</span><a href="#42477896">next</a><span>|</span><label class="collapse" for="c-42477227">[-]</label><label class="expand" for="c-42477227">[1 more]</label></div><br/><div class="children"><div class="content">Blaming OpenAI for progress is like blaming a calendar for Christmas—it’s not the timing, it’s your unwillingness to adapt</div><br/></div></div><div id="42477896" class="c"><input type="checkbox" id="c-42477896" checked=""/><div class="controls bullet"><span class="by">t0lo</span><span>|</span><a href="#42476965">parent</a><span>|</span><a href="#42477227">prev</a><span>|</span><a href="#42477410">next</a><span>|</span><label class="collapse" for="c-42477896">[-]</label><label class="expand" for="c-42477896">[1 more]</label></div><br/><div class="children"><div class="content">I hate the deliberate fear-mongering that these companies pedal on the population to get higher valuations</div><br/></div></div><div id="42477410" class="c"><input type="checkbox" id="c-42477410" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#42476965">parent</a><span>|</span><a href="#42477896">prev</a><span>|</span><a href="#42477665">next</a><span>|</span><label class="collapse" for="c-42477410">[-]</label><label class="expand" for="c-42477410">[4 more]</label></div><br/><div class="children"><div class="content">This is a you problem. Yes there will be pain in short term, but it will be worth it in long term.<p>Many of us look forward to what a future with AGI can do to help humanity and hopefully change society for the better, mainly to achieve a post scarcity economy.</div><br/><div id="42478204" class="c"><input type="checkbox" id="c-42478204" checked=""/><div class="controls bullet"><span class="by">achierius</span><span>|</span><a href="#42476965">root</a><span>|</span><a href="#42477410">parent</a><span>|</span><a href="#42477629">next</a><span>|</span><label class="collapse" for="c-42478204">[-]</label><label class="expand" for="c-42478204">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.transformernews.ai&#x2F;p&#x2F;richard-ngo-openai-resign-safety" rel="nofollow">https:&#x2F;&#x2F;www.transformernews.ai&#x2F;p&#x2F;richard-ngo-openai-resign-s...</a><p>&gt;But while the “making AGI” part of the mission seems well on track, it feels like I (and others) have gradually realized how much harder it is to contribute in a robustly positive way to the “succeeding” part of the mission, especially when it comes to preventing existential risks to humanity.<p>Almost every single one of the people OpenAI had hired to work on AI safety have left the firm with similar messages. Perhaps you should at least consider the thinking of experts? There is a real chance that this ends with significant good. There is also a real chance that this ends with the death of every single human being. That&#x27;s never been a choice we&#x27;ve had to make before, and it seems like we as a species are unprepared to approach it.</div><br/></div></div><div id="42477629" class="c"><input type="checkbox" id="c-42477629" checked=""/><div class="controls bullet"><span class="by">jakebasile</span><span>|</span><a href="#42476965">root</a><span>|</span><a href="#42477410">parent</a><span>|</span><a href="#42478204">prev</a><span>|</span><a href="#42477674">next</a><span>|</span><label class="collapse" for="c-42477629">[-]</label><label class="expand" for="c-42477629">[1 more]</label></div><br/><div class="children"><div class="content">Surely the elites that control this fancy new technology will share the benefits with all of us _this_ time!</div><br/></div></div><div id="42477674" class="c"><input type="checkbox" id="c-42477674" checked=""/><div class="controls bullet"><span class="by">randyrand</span><span>|</span><a href="#42476965">root</a><span>|</span><a href="#42477410">parent</a><span>|</span><a href="#42477629">prev</a><span>|</span><a href="#42477665">next</a><span>|</span><label class="collapse" for="c-42477674">[-]</label><label class="expand" for="c-42477674">[1 more]</label></div><br/><div class="children"><div class="content">Post scarcity seems very unlikely. Humans might be worthless, but there will still be a finite number of AIs, compute, space, resources.</div><br/></div></div></div></div><div id="42477665" class="c"><input type="checkbox" id="c-42477665" checked=""/><div class="controls bullet"><span class="by">_cs2017_</span><span>|</span><a href="#42476965">parent</a><span>|</span><a href="#42477410">prev</a><span>|</span><a href="#42473456">next</a><span>|</span><label class="collapse" for="c-42477665">[-]</label><label class="expand" for="c-42477665">[2 more]</label></div><br/><div class="children"><div class="content">Wtf is wrong with you dude? It&#x27;s just another tech, some jobs will get worse some jobs will get better. Happens every couple of decades. Stop freaking out.</div><br/><div id="42478187" class="c"><input type="checkbox" id="c-42478187" checked=""/><div class="controls bullet"><span class="by">achierius</span><span>|</span><a href="#42476965">root</a><span>|</span><a href="#42477665">parent</a><span>|</span><a href="#42473456">next</a><span>|</span><label class="collapse" for="c-42478187">[-]</label><label class="expand" for="c-42478187">[1 more]</label></div><br/><div class="children"><div class="content">This is not a very kind or humble comment. There are real experts talking about how this time is different -- as an analogy, think about how horses, for thousands of years, always had new things to do -- until one day they didn&#x27;t. It&#x27;s hubris  to think that we&#x27;re somehow so different from them.<p>Notably, the last key AI safety researcher just left OpenAI: <a href="https:&#x2F;&#x2F;www.transformernews.ai&#x2F;p&#x2F;richard-ngo-openai-resign-safety" rel="nofollow">https:&#x2F;&#x2F;www.transformernews.ai&#x2F;p&#x2F;richard-ngo-openai-resign-s...</a><p>&gt;But while the “making AGI” part of the mission seems well on track, it feels like I (and others) have gradually realized how much harder it is to contribute in a robustly positive way to the “succeeding” part of the mission, especially when it comes to preventing existential risks to humanity.<p>Are you that upset that this guy chose to trust the people that OpenAI hired to talk about AI safety, on the topic of AI safety?</div><br/></div></div></div></div></div></div><div id="42473456" class="c"><input type="checkbox" id="c-42473456" checked=""/><div class="controls bullet"><span class="by">yawnxyz</span><span>|</span><a href="#42476965">prev</a><span>|</span><a href="#42473475">next</a><span>|</span><label class="collapse" for="c-42473456">[-]</label><label class="expand" for="c-42473456">[21 more]</label></div><br/><div class="children"><div class="content">O3 High (tuned) model scored an 88% at what looks like $6,000&#x2F;task haha<p>I think soon we&#x27;ll be pricing any kind of tasks by their compute costs. So basically, human = $50&#x2F;task, AI = $6,000&#x2F;task, use human. If AI beats human, use AI? Ofc that&#x27;s considering both get 100% scores on the task</div><br/><div id="42473493" class="c"><input type="checkbox" id="c-42473493" checked=""/><div class="controls bullet"><span class="by">cchance</span><span>|</span><a href="#42473456">parent</a><span>|</span><a href="#42473643">next</a><span>|</span><label class="collapse" for="c-42473493">[-]</label><label class="expand" for="c-42473493">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that generally what ... all jobs are? Automation Cost vs Longterm Human cost... its why amazon did the weird &quot;our stores are AI driven&quot; but in reality was cheaper to higher a bunch of guys in a sweat shop to look at the cameras and write things down lol.<p>The thing is given what we&#x27;ve seen from distillation and tech, even if its 6,000&#x2F;task... that will come down drastically over time through optimization and just... faster more efficient processing hardware and software.</div><br/><div id="42473647" class="c"><input type="checkbox" id="c-42473647" checked=""/><div class="controls bullet"><span class="by">cryptoegorophy</span><span>|</span><a href="#42473456">root</a><span>|</span><a href="#42473493">parent</a><span>|</span><a href="#42473643">next</a><span>|</span><label class="collapse" for="c-42473647">[-]</label><label class="expand" for="c-42473647">[1 more]</label></div><br/><div class="children"><div class="content">I remember hearing Tesla trying to automate all of production but some things just couldn’t , like the wiring which humans still had to do.</div><br/></div></div></div></div><div id="42473643" class="c"><input type="checkbox" id="c-42473643" checked=""/><div class="controls bullet"><span class="by">Benjaminsen</span><span>|</span><a href="#42473456">parent</a><span>|</span><a href="#42473493">prev</a><span>|</span><a href="#42473512">next</a><span>|</span><label class="collapse" for="c-42473643">[-]</label><label class="expand" for="c-42473643">[2 more]</label></div><br/><div class="children"><div class="content">Compute costs on AI with the same roughly the same capabilities have been halving every ~7 months.<p>That makes something like this competitive in ~3 years</div><br/><div id="42475946" class="c"><input type="checkbox" id="c-42475946" checked=""/><div class="controls bullet"><span class="by">seizethecheese</span><span>|</span><a href="#42473456">root</a><span>|</span><a href="#42473643">parent</a><span>|</span><a href="#42473512">next</a><span>|</span><label class="collapse" for="c-42475946">[-]</label><label class="expand" for="c-42475946">[1 more]</label></div><br/><div class="children"><div class="content">And human costs have been increasing a few percent per year for a few centuries!</div><br/></div></div></div></div><div id="42473512" class="c"><input type="checkbox" id="c-42473512" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#42473456">parent</a><span>|</span><a href="#42473643">prev</a><span>|</span><a href="#42473801">next</a><span>|</span><label class="collapse" for="c-42473512">[-]</label><label class="expand" for="c-42473512">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the elephant in the room with the reasoning&#x2F;COT approach, it shifts what was previously a scaling of training costs into scaling of training <i>and</i> inference costs. The promise of doing expensive training once and then running the model cheaply forever falls apart once you&#x27;re burning tens, hundreds or thousands of dollars worth of compute every time you run a query.</div><br/><div id="42474985" class="c"><input type="checkbox" id="c-42474985" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#42473456">root</a><span>|</span><a href="#42473512">parent</a><span>|</span><a href="#42473600">next</a><span>|</span><label class="collapse" for="c-42474985">[-]</label><label class="expand" for="c-42474985">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re gonna figure it out. Something is being missed somewhere, as human brains can do all this computation on 20 watts. Maybe it will be a hardware shift or maybe just a software one, but I strongly suspect that modern transformers are grossly inefficient.</div><br/></div></div><div id="42473600" class="c"><input type="checkbox" id="c-42473600" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#42473456">root</a><span>|</span><a href="#42473512">parent</a><span>|</span><a href="#42474985">prev</a><span>|</span><a href="#42473801">next</a><span>|</span><label class="collapse" for="c-42473600">[-]</label><label class="expand" for="c-42473600">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, but next year they&#x27;ll come out with a faster GPU, and the year after that another still faster one, and so on. Compute costs are a temporary problem.</div><br/><div id="42474034" class="c"><input type="checkbox" id="c-42474034" checked=""/><div class="controls bullet"><span class="by">freehorse</span><span>|</span><a href="#42473456">root</a><span>|</span><a href="#42473600">parent</a><span>|</span><a href="#42473801">next</a><span>|</span><label class="collapse" for="c-42474034">[-]</label><label class="expand" for="c-42474034">[1 more]</label></div><br/><div class="children"><div class="content">The issue is not just scaling compute, but scaling it in a rate that meets the increase in complexity of the problems that are not currently  solved. If that is O(n) then what you say probably stands. If that is eg O(n^8) or exponential etc, then there is no hope to actually get good enough scaling by just increasing compute in a normal rate. Then AI technology will still be improving, but improving to a halt, practically stagnating.<p>o3 will be interesting if it offers indeed a novel technology to handle problem solving, something that is able to learn from few novel examples efficiently and adapt. That&#x27;s what intelligence actually is. Maybe this is the case. If, on the other hand, it is a smart way to pair CoT within an evaluation loop (as the author hints as possibility) then it is probable that, while this _can_ handle a class of problems that current LLMs cannot, it is not really this kind of learning, meaning that it will not be able to scale to more complex, real world tasks with a problem space that is too large and thus less amenable to such a technique. It is still interesting, because having a good enough evaluator may be very important step, but it would mean that we are not yet there.<p>We will learn soon enough I suppose.</div><br/></div></div></div></div></div></div><div id="42473801" class="c"><input type="checkbox" id="c-42473801" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42473456">parent</a><span>|</span><a href="#42473512">prev</a><span>|</span><a href="#42473736">next</a><span>|</span><label class="collapse" for="c-42473801">[-]</label><label class="expand" for="c-42473801">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not 6000&#x2F;task (i.e per question). 6000 is about the retail cost for evaluating the entire benchmark on high efficiency (about 400 questions)</div><br/><div id="42473944" class="c"><input type="checkbox" id="c-42473944" checked=""/><div class="controls bullet"><span class="by">Tiberium</span><span>|</span><a href="#42473456">root</a><span>|</span><a href="#42473801">parent</a><span>|</span><a href="#42473736">next</a><span>|</span><label class="collapse" for="c-42473944">[-]</label><label class="expand" for="c-42473944">[3 more]</label></div><br/><div class="children"><div class="content">From reading the blog post and Twitter, and cost of other models, I think it&#x27;s evident that it IS actually cost per task, see this tweet: <a href="https:&#x2F;&#x2F;files.catbox.moe&#x2F;z1n8dc.jpg" rel="nofollow">https:&#x2F;&#x2F;files.catbox.moe&#x2F;z1n8dc.jpg</a><p>And o1 cost $15&#x2F;$60 for 1M in&#x2F;out, so the estimated costs on the graph would match for a single task, not the whole benchmark.</div><br/><div id="42474003" class="c"><input type="checkbox" id="c-42474003" checked=""/><div class="controls bullet"><span class="by">slibhb</span><span>|</span><a href="#42473456">root</a><span>|</span><a href="#42473944">parent</a><span>|</span><a href="#42473736">next</a><span>|</span><label class="collapse" for="c-42474003">[-]</label><label class="expand" for="c-42474003">[2 more]</label></div><br/><div class="children"><div class="content">The blog clarifies that it&#x27;s $17-20 per task. Maybe it runs into thousands for tasks it can&#x27;t solve?</div><br/><div id="42474896" class="c"><input type="checkbox" id="c-42474896" checked=""/><div class="controls bullet"><span class="by">Tiberium</span><span>|</span><a href="#42473456">root</a><span>|</span><a href="#42474003">parent</a><span>|</span><a href="#42473736">next</a><span>|</span><label class="collapse" for="c-42474896">[-]</label><label class="expand" for="c-42474896">[1 more]</label></div><br/><div class="children"><div class="content">That cost is for o3 low, o3 high goes into thousands per task.</div><br/></div></div></div></div></div></div></div></div><div id="42473736" class="c"><input type="checkbox" id="c-42473736" checked=""/><div class="controls bullet"><span class="by">freehorse</span><span>|</span><a href="#42473456">parent</a><span>|</span><a href="#42473801">prev</a><span>|</span><a href="#42474107">next</a><span>|</span><label class="collapse" for="c-42473736">[-]</label><label class="expand" for="c-42473736">[1 more]</label></div><br/><div class="children"><div class="content">This makes me think and speculate if the solution comprises of a &quot;solver&quot; trying semi-random or more targeted things and a &quot;checker&quot; checking these? Usually checking a solution is cognitively (and computationally) easier than coming up with it. Else I cannot think what sort of compute would burn 6000$ per task, unless you are going through a lot of loops and you have somehow solved the part of the problem that can figure out if a solution is correct or not, while coming up with the actual correct solution is not as solved yet to the same degree. Or maybe I am just naive and these prices are just like breakfast for companies like that.</div><br/></div></div><div id="42474107" class="c"><input type="checkbox" id="c-42474107" checked=""/><div class="controls bullet"><span class="by">gbnwl</span><span>|</span><a href="#42473456">parent</a><span>|</span><a href="#42473736">prev</a><span>|</span><a href="#42474951">next</a><span>|</span><label class="collapse" for="c-42474107">[-]</label><label class="expand" for="c-42474107">[1 more]</label></div><br/><div class="children"><div class="content">Well they got 75.7% at $17&#x2F;task. Did you see that?</div><br/></div></div><div id="42474951" class="c"><input type="checkbox" id="c-42474951" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#42473456">parent</a><span>|</span><a href="#42474107">prev</a><span>|</span><a href="#42473485">next</a><span>|</span><label class="collapse" for="c-42474951">[-]</label><label class="expand" for="c-42474951">[1 more]</label></div><br/><div class="children"><div class="content">What if we use those humans to generate energy for the tasks?</div><br/></div></div><div id="42473516" class="c"><input type="checkbox" id="c-42473516" checked=""/><div class="controls bullet"><span class="by">redeux</span><span>|</span><a href="#42473456">parent</a><span>|</span><a href="#42473663">prev</a><span>|</span><a href="#42473498">next</a><span>|</span><label class="collapse" for="c-42473516">[-]</label><label class="expand" for="c-42473516">[1 more]</label></div><br/><div class="children"><div class="content">Time and availability would also be factors.</div><br/></div></div><div id="42473498" class="c"><input type="checkbox" id="c-42473498" checked=""/><div class="controls bullet"><span class="by">dyauspitr</span><span>|</span><a href="#42473456">parent</a><span>|</span><a href="#42473516">prev</a><span>|</span><a href="#42473475">next</a><span>|</span><label class="collapse" for="c-42473498">[-]</label><label class="expand" for="c-42473498">[2 more]</label></div><br/><div class="children"><div class="content">Compute can get optimized and cheap quickly.</div><br/><div id="42474192" class="c"><input type="checkbox" id="c-42474192" checked=""/><div class="controls bullet"><span class="by">karmasimida</span><span>|</span><a href="#42473456">root</a><span>|</span><a href="#42473498">parent</a><span>|</span><a href="#42473475">next</a><span>|</span><label class="collapse" for="c-42474192">[-]</label><label class="expand" for="c-42474192">[1 more]</label></div><br/><div class="children"><div class="content">Is it? The moore’s law is dead dead, I don’t think this is a given.</div><br/></div></div></div></div></div></div><div id="42473475" class="c"><input type="checkbox" id="c-42473475" checked=""/><div class="controls bullet"><span class="by">spaceman_2020</span><span>|</span><a href="#42473456">prev</a><span>|</span><a href="#42473976">next</a><span>|</span><label class="collapse" for="c-42473475">[-]</label><label class="expand" for="c-42473475">[14 more]</label></div><br/><div class="children"><div class="content">Just as an aside, I&#x27;ve personally found o1 to be completely useless for coding.<p>Sonnet 3.5 remains the king of the hill by quite some margin</div><br/><div id="42473648" class="c"><input type="checkbox" id="c-42473648" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#42473475">parent</a><span>|</span><a href="#42473511">next</a><span>|</span><label class="collapse" for="c-42473648">[-]</label><label class="expand" for="c-42473648">[1 more]</label></div><br/><div class="children"><div class="content">To fill this out, I find o1-pro (and -preview when it was live) to be pretty good at filling in blindspots&#x2F;spotting holistic bugs. I use Claude for day to day, and when Claude is spinning, o1 often can point out why. It&#x27;s too slow for AI coding, and I agree that at default its responses aren&#x27;t always satisfying.<p>That said, I think its code style is arguably better, more concise and has better patterns -- Claude needs a fair amount of prompting and oversight to not put out semi-shitty code in terms of structure and architecture.<p>In my mind: going from Slowest to Fastest, and Best Holistically to Worst, the list is:<p>1. o1-pro
2. Claude 3.5
3. Gemini 2 Flash<p>Flash is so fast, that it&#x27;s tempting to use more, but it really needs to be kept to specific work on strong codebases without complex interactions.</div><br/></div></div><div id="42473511" class="c"><input type="checkbox" id="c-42473511" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#42473475">parent</a><span>|</span><a href="#42473648">prev</a><span>|</span><a href="#42473790">next</a><span>|</span><label class="collapse" for="c-42473511">[-]</label><label class="expand" for="c-42473511">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, until the last checkpoint released 2 days ago, o1 didn&#x27;t really beat sonnet (and if so, barely) in most non-competitive coding benchmarks</div><br/></div></div><div id="42473790" class="c"><input type="checkbox" id="c-42473790" checked=""/><div class="controls bullet"><span class="by">InkCanon</span><span>|</span><a href="#42473475">parent</a><span>|</span><a href="#42473511">prev</a><span>|</span><a href="#42474898">next</a><span>|</span><label class="collapse" for="c-42473790">[-]</label><label class="expand" for="c-42473790">[1 more]</label></div><br/><div class="children"><div class="content">I just asked o1 a simple yes or no question about x86 atomics and it did one of those A or B replies. The first answer was yes, the second answer was no.</div><br/></div></div><div id="42474898" class="c"><input type="checkbox" id="c-42474898" checked=""/><div class="controls bullet"><span class="by">bitbuilder</span><span>|</span><a href="#42473475">parent</a><span>|</span><a href="#42473790">prev</a><span>|</span><a href="#42473712">next</a><span>|</span><label class="collapse" for="c-42474898">[-]</label><label class="expand" for="c-42474898">[2 more]</label></div><br/><div class="children"><div class="content">I find myself hoping between o1 and Sonnet pretty frequently these days, and my personal observation is that the quality of output from o1 scales more directly to the quality of the prompting you&#x27;re giving it.<p>In a way it almost feels like it&#x27;s become <i>too</i> good at following instructions and simply just takes your direction more literally. It doesn&#x27;t seem to take the initiative of going the extra mile of filling in the blanks from your lazy input (note: many would see this as a good thing). Claude on the other hand feels more intuitive in discerning intent from a lazy prompt, which I may be prone to offering it at times when I&#x27;m simply trying out ideas.<p>However, if I take the time to write up a well thought out prompt detailing my expectations, I find I much prefer the code o1 creates. It&#x27;s smarter in its approach, offers clever ideas I wouldn&#x27;t have thought of, and generally cleaner.<p>Or put another way, I can give Sonnet a lazy or detailed prompt and get a good result, while o1 will give me an excellent result with a well thought out prompt.<p>What this boils down to is I find myself using Sonnet while brainstorming ideas, or when I simply don&#x27;t know how I want to approach a problem. I can pitch it a feature idea the same way a product owner might pitch an idea to an engineer, and then iterate through sensible and intuitive ways of looking at the problem. Once I get a handle on how I&#x27;d like to implement a solution, I type up a spec and hand it off to o1 to crank out the code I&#x27;d intend to implement.</div><br/><div id="42475111" class="c"><input type="checkbox" id="c-42475111" checked=""/><div class="controls bullet"><span class="by">jules</span><span>|</span><a href="#42473475">root</a><span>|</span><a href="#42474898">parent</a><span>|</span><a href="#42473712">next</a><span>|</span><label class="collapse" for="c-42475111">[-]</label><label class="expand" for="c-42475111">[1 more]</label></div><br/><div class="children"><div class="content">Can you solve this by putting your lazy prompt through GPT-4o or Sonnet 3.6 and asking it to expand the prompt to a full prompt for o1?</div><br/></div></div></div></div><div id="42473712" class="c"><input type="checkbox" id="c-42473712" checked=""/><div class="controls bullet"><span class="by">bearjaws</span><span>|</span><a href="#42473475">parent</a><span>|</span><a href="#42474898">prev</a><span>|</span><a href="#42473501">next</a><span>|</span><label class="collapse" for="c-42473712">[-]</label><label class="expand" for="c-42473712">[1 more]</label></div><br/><div class="children"><div class="content">o1 is pretty good at spotting OWASP defects, compared to most other models.<p><a href="https:&#x2F;&#x2F;myswamp.substack.com&#x2F;p&#x2F;benchmarking-llms-against-common" rel="nofollow">https:&#x2F;&#x2F;myswamp.substack.com&#x2F;p&#x2F;benchmarking-llms-against-com...</a></div><br/></div></div><div id="42473501" class="c"><input type="checkbox" id="c-42473501" checked=""/><div class="controls bullet"><span class="by">cchance</span><span>|</span><a href="#42473475">parent</a><span>|</span><a href="#42473712">prev</a><span>|</span><a href="#42475439">next</a><span>|</span><label class="collapse" for="c-42473501">[-]</label><label class="expand" for="c-42473501">[2 more]</label></div><br/><div class="children"><div class="content">The new gemini&#x27;s are pretty good too</div><br/><div id="42473639" class="c"><input type="checkbox" id="c-42473639" checked=""/><div class="controls bullet"><span class="by">lysecret</span><span>|</span><a href="#42473475">root</a><span>|</span><a href="#42473501">parent</a><span>|</span><a href="#42475439">next</a><span>|</span><label class="collapse" for="c-42473639">[-]</label><label class="expand" for="c-42473639">[1 more]</label></div><br/><div class="children"><div class="content">Actually prefer new geminis too. 2.0 experimental especially.</div><br/></div></div></div></div><div id="42475439" class="c"><input type="checkbox" id="c-42475439" checked=""/><div class="controls bullet"><span class="by">leumon</span><span>|</span><a href="#42473475">parent</a><span>|</span><a href="#42473501">prev</a><span>|</span><a href="#42476684">next</a><span>|</span><label class="collapse" for="c-42475439">[-]</label><label class="expand" for="c-42475439">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve found gemini-1206 to be best. and we can use it free (for now), in google&#x27;s aistudio. It&#x27;s number 1 on lmarena.ai for coding, and generally, and number 1 on bigcodebench.</div><br/></div></div><div id="42476684" class="c"><input type="checkbox" id="c-42476684" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#42473475">parent</a><span>|</span><a href="#42475439">prev</a><span>|</span><a href="#42474222">next</a><span>|</span><label class="collapse" for="c-42476684">[-]</label><label class="expand" for="c-42476684">[1 more]</label></div><br/><div class="children"><div class="content">Which o1? A new version was released a few days ago and beats Sonnet 3.5 on Livebench</div><br/></div></div><div id="42474222" class="c"><input type="checkbox" id="c-42474222" checked=""/><div class="controls bullet"><span class="by">karmasimida</span><span>|</span><a href="#42473475">parent</a><span>|</span><a href="#42476684">prev</a><span>|</span><a href="#42473970">next</a><span>|</span><label class="collapse" for="c-42474222">[-]</label><label class="expand" for="c-42474222">[2 more]</label></div><br/><div class="children"><div class="content">Yeah I feel for chat use case, o1 is just too slow for me, and my queries aren’t that complicated.<p>For coding, o1 is marvelous at Leetcode question I think it is the best teacher I would ever afford to teach me leetcoding, but I don’t find myself have a lot of other use cases for o1 that is complex and requires really long reasoning chain</div><br/></div></div><div id="42473970" class="c"><input type="checkbox" id="c-42473970" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#42473475">parent</a><span>|</span><a href="#42474222">prev</a><span>|</span><a href="#42473976">next</a><span>|</span><label class="collapse" for="c-42473970">[-]</label><label class="expand" for="c-42473970">[1 more]</label></div><br/><div class="children"><div class="content">o1 is when all else fails, sometimes it does the same mistakes as weaker models if you give it simple tasks with very little context, but when a good precise context is given it usually outperforms other 
Models</div><br/></div></div></div></div><div id="42473976" class="c"><input type="checkbox" id="c-42473976" checked=""/><div class="controls bullet"><span class="by">neuroelectron</span><span>|</span><a href="#42473475">prev</a><span>|</span><a href="#42478290">next</a><span>|</span><label class="collapse" for="c-42473976">[-]</label><label class="expand" for="c-42473976">[7 more]</label></div><br/><div class="children"><div class="content">OpenAI spent approximately $1,503,077 to smash the SOTA on ARC-AGI with their new o3 model<p>semi-private evals (100 tasks):
75.7% @ $2,012 total&#x2F;100 tasks (~$20&#x2F;task) with just 6 samples &amp; 33M tokens processed in ~1.3 min&#x2F;task and a cost of $2012<p>The “low-efficiency” setting with 1024 samples scored 87.5% but required 172x more compute.<p>If we assume compute spent and cost are proportional, then OpenAI might have just spent ~$346.064 for the low efficiency run on the semi-private eval.<p>On the public eval they might have spent ~$1.148.444 to achieve 91.5% with the low efficiency setting. (high-efficiency mode: $6677)<p>OpenAI just spent more money to run an eval on ARC than most people spend on a full training run.</div><br/><div id="42474534" class="c"><input type="checkbox" id="c-42474534" checked=""/><div class="controls bullet"><span class="by">bluecoconut</span><span>|</span><a href="#42473976">parent</a><span>|</span><a href="#42474937">next</a><span>|</span><label class="collapse" for="c-42474534">[-]</label><label class="expand" for="c-42474534">[1 more]</label></div><br/><div class="children"><div class="content">By my estimates, for this single benchmark, this is comparable cost to training a ~70B model from scratch today. Literally from 0 to a GPT-3 scale model for the compute they ran on 100 ARC tasks.<p>I double checked with some flop estimates (P100 for 12 hours = Kaggle limit, they claim ~100-1000x for O3-low, and x172 for O3-high) so roughly on the order of 10^22-10^23 flops.<p>In another way, using H100 market price $2&#x2F;chip -&gt; at $350k, that&#x27;s ~175k hours.  Or 10^24 FLOPs in total.<p>So, huge margin, but 10^22 - 10^24 flop is the band I think we can estimate.<p>These are the scale of numbers that show up in the chinchilla optimal paper, haha. Truly GPT-3 scale models.</div><br/></div></div><div id="42474937" class="c"><input type="checkbox" id="c-42474937" checked=""/><div class="controls bullet"><span class="by">rvnx</span><span>|</span><a href="#42473976">parent</a><span>|</span><a href="#42474534">prev</a><span>|</span><a href="#42474446">next</a><span>|</span><label class="collapse" for="c-42474937">[-]</label><label class="expand" for="c-42474937">[1 more]</label></div><br/><div class="children"><div class="content">It sounds like they essentially brute-forced the solutions ?
Ask LLM for answer, answer for LLM to verify the answer. Ask LLM for answer, answer for LLM to verify the answer. Add a bit of randomness. Ask LLM for answer, answer for LLM to verify the answer. Add a bit of randomness. Repeat 5B times (this is what the paper says).</div><br/></div></div><div id="42474446" class="c"><input type="checkbox" id="c-42474446" checked=""/><div class="controls bullet"><span class="by">rfoo</span><span>|</span><a href="#42473976">parent</a><span>|</span><a href="#42474937">prev</a><span>|</span><a href="#42475888">next</a><span>|</span><label class="collapse" for="c-42474446">[-]</label><label class="expand" for="c-42474446">[3 more]</label></div><br/><div class="children"><div class="content">Pretty sure this &quot;cost&quot; is based on their retail price instead of actual inference cost.</div><br/><div id="42475187" class="c"><input type="checkbox" id="c-42475187" checked=""/><div class="controls bullet"><span class="by">neuroelectron</span><span>|</span><a href="#42473976">root</a><span>|</span><a href="#42474446">parent</a><span>|</span><a href="#42476807">next</a><span>|</span><label class="collapse" for="c-42475187">[-]</label><label class="expand" for="c-42475187">[1 more]</label></div><br/><div class="children"><div class="content">Yes that&#x27;s correct and there&#x27;s a bit of &quot;pixel math&quot; as well so take these numbers with a pinch of salt. Preliminary model sizes from the temporarily public HF repository puts the full model size at 8tb or roughly 80 H100s</div><br/></div></div><div id="42476807" class="c"><input type="checkbox" id="c-42476807" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#42473976">root</a><span>|</span><a href="#42474446">parent</a><span>|</span><a href="#42475187">prev</a><span>|</span><a href="#42475888">next</a><span>|</span><label class="collapse" for="c-42476807">[-]</label><label class="expand" for="c-42476807">[1 more]</label></div><br/><div class="children"><div class="content">Yeah and can run off peak, etc.<p>Does seem to show an absolutely massive market for inference compute…</div><br/></div></div></div></div></div></div><div id="42478290" class="c"><input type="checkbox" id="c-42478290" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#42473976">prev</a><span>|</span><a href="#42478153">next</a><span>|</span><label class="collapse" for="c-42478290">[-]</label><label class="expand" for="c-42478290">[1 more]</label></div><br/><div class="children"><div class="content">Headline could also just be OpenAI discovers exponential scaling wall for inference time compute.</div><br/></div></div><div id="42478153" class="c"><input type="checkbox" id="c-42478153" checked=""/><div class="controls bullet"><span class="by">digitcatphd</span><span>|</span><a href="#42478290">prev</a><span>|</span><a href="#42478264">next</a><span>|</span><label class="collapse" for="c-42478153">[-]</label><label class="expand" for="c-42478153">[1 more]</label></div><br/><div class="children"><div class="content">o3 fixes the fundamental limitation of the LLM paradigm – the inability to recombine knowledge at test time – and it does so via a form of LLM-guided natural language program search<p>&gt; This is significant, but I am doubtful it will be as meaningful as people expect aside from potentially greater coding tasks. Without a &#x27;world model&#x27; that has a contextual understanding of what it is doing, things will remain fundamentally throttled.</div><br/></div></div><div id="42478264" class="c"><input type="checkbox" id="c-42478264" checked=""/><div class="controls bullet"><span class="by">madsgarff</span><span>|</span><a href="#42478153">prev</a><span>|</span><a href="#42478115">next</a><span>|</span><label class="collapse" for="c-42478264">[-]</label><label class="expand" for="c-42478264">[1 more]</label></div><br/><div class="children"><div class="content">Moreover, ARC-AGI-1 is now saturating – besides o3&#x27;s new score, the fact is that a large ensemble of low-compute Kaggle solutions can now score 81% on the private eval.<p>If low-compute Kaggle solutions already does 81% - then why is o3&#x27;s 75.7% considered such a breakthrough?</div><br/></div></div><div id="42478115" class="c"><input type="checkbox" id="c-42478115" checked=""/><div class="controls bullet"><span class="by">DiscourseFan</span><span>|</span><a href="#42478264">prev</a><span>|</span><a href="#42476797">next</a><span>|</span><label class="collapse" for="c-42478115">[-]</label><label class="expand" for="c-42478115">[1 more]</label></div><br/><div class="children"><div class="content">a little from column A, a little from column B<p>I don&#x27;t think this is AGI; nor is it something to scoff at. Its impressive, but its also not human-like intelligence. Perhaps human-like intelligence is not the goal, since that would imply we have even a remotely comprehensive understanding of the human mind. I doubt the mind operates as a single unit anyway, a human&#x27;s first words are &quot;Mama,&quot; not &quot;I am a self-conscious freely self-determining being that recognizes my own reasoning ability and autonomy.&quot; And the latter would be easily programmable anyway. The goal here might, then, be infeasible: the concept of free will is a kind of technology in and of itself, it has already augmented human cognition. How will these technologies not augment the &quot;mind&quot; such that our own understanding of our consciousness is altered?  And why should we try to determine ahead of time what will hold weight for us, why the &quot;human&quot; part of the intelligence will matter in the future? Technology should not be compared to the world it transforms.</div><br/></div></div><div id="42476797" class="c"><input type="checkbox" id="c-42476797" checked=""/><div class="controls bullet"><span class="by">mortehu</span><span>|</span><a href="#42478115">prev</a><span>|</span><a href="#42474068">next</a><span>|</span><label class="collapse" for="c-42476797">[-]</label><label class="expand" for="c-42476797">[1 more]</label></div><br/><div class="children"><div class="content">The chart is super misleading, since the test was obscure until recently. A few months ago he announced he&#x27;d made the only good AGI test and offered a cash prize for solving it, only to find out in as much time that it&#x27;s no different from other benchmarks.</div><br/></div></div><div id="42474068" class="c"><input type="checkbox" id="c-42474068" checked=""/><div class="controls bullet"><span class="by">nxobject</span><span>|</span><a href="#42476797">prev</a><span>|</span><a href="#42477784">next</a><span>|</span><label class="collapse" for="c-42474068">[-]</label><label class="expand" for="c-42474068">[5 more]</label></div><br/><div class="children"><div class="content">As an aside, I&#x27;m a little miffed that the benchmark calls out &quot;AGI&quot; in the name, but then heavily cautions that it&#x27;s necessary but insufficient for AGI.<p>&gt; ARC-AGI serves as a critical benchmark for detecting such breakthroughs, highlighting generalization power in a way that saturated or less demanding benchmarks cannot. However, it is important to note that ARC-AGI is not an acid test for AGI</div><br/><div id="42475824" class="c"><input type="checkbox" id="c-42475824" checked=""/><div class="controls bullet"><span class="by">mmcnl</span><span>|</span><a href="#42474068">parent</a><span>|</span><a href="#42477652">next</a><span>|</span><label class="collapse" for="c-42475824">[-]</label><label class="expand" for="c-42475824">[3 more]</label></div><br/><div class="children"><div class="content">I immediately thought so too. Why confuse everyone?</div><br/><div id="42476819" class="c"><input type="checkbox" id="c-42476819" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#42474068">root</a><span>|</span><a href="#42475824">parent</a><span>|</span><a href="#42477652">next</a><span>|</span><label class="collapse" for="c-42476819">[-]</label><label class="expand" for="c-42476819">[2 more]</label></div><br/><div class="children"><div class="content">Because ARC somehow convinced people that solving it was an indicator of AGI.</div><br/><div id="42476863" class="c"><input type="checkbox" id="c-42476863" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#42474068">root</a><span>|</span><a href="#42476819">parent</a><span>|</span><a href="#42477652">next</a><span>|</span><label class="collapse" for="c-42476863">[-]</label><label class="expand" for="c-42476863">[1 more]</label></div><br/><div class="children"><div class="content">Its like the &quot;Open&quot; in OpenAI or the &quot;Democratic&quot; in North Koreas DPRK. Naming things helps fool a lot of people.</div><br/></div></div></div></div></div></div><div id="42477652" class="c"><input type="checkbox" id="c-42477652" checked=""/><div class="controls bullet"><span class="by">EthanHeilman</span><span>|</span><a href="#42474068">parent</a><span>|</span><a href="#42475824">prev</a><span>|</span><a href="#42477784">next</a><span>|</span><label class="collapse" for="c-42477652">[-]</label><label class="expand" for="c-42477652">[1 more]</label></div><br/><div class="children"><div class="content">It is a necessary but not sufficient condition to AGI.</div><br/></div></div></div></div><div id="42477784" class="c"><input type="checkbox" id="c-42477784" checked=""/><div class="controls bullet"><span class="by">joshdavham</span><span>|</span><a href="#42474068">prev</a><span>|</span><a href="#42476475">next</a><span>|</span><label class="collapse" for="c-42477784">[-]</label><label class="expand" for="c-42477784">[1 more]</label></div><br/><div class="children"><div class="content">A lot of the comments seem very dismissive and a little overly-skeptical in my opinion. Why is this?</div><br/></div></div><div id="42476475" class="c"><input type="checkbox" id="c-42476475" checked=""/><div class="controls bullet"><span class="by">mukunda_johnson</span><span>|</span><a href="#42477784">prev</a><span>|</span><label class="collapse" for="c-42476475">[-]</label><label class="expand" for="c-42476475">[1 more]</label></div><br/><div class="children"><div class="content">Deciphering patterns in natural language is more complex than these puzzles. If you train your AI to solve these puzzles, we end up in the same spot. The difficulty of solving would be with creating training data for a foreign medium. The &quot;tokens&quot; are the grids and squares instead of words (for words, we have the internet of words, solving that).<p>If we&#x27;re inferring the answers of the block patterns from minimal or no additional training, it&#x27;s very impressive, but how much time have they had to work on O3 after sharing puzzle data with O1? Seems there&#x27;s some room for questionable antics!</div><br/></div></div></div></div></div></div></div></body></html>