<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1709802058744" as="style"/><link rel="stylesheet" href="styles.css?v=1709802058744"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness">Training LLMs from ground zero as a startup</a> <span class="domain">(<a href="https://www.yitay.net">www.yitay.net</a>)</span></div><div class="subtext"><span>swyx</span> | <span>101 comments</span></div><br/><div><div id="39621069" class="c"><input type="checkbox" id="c-39621069" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#39610011">next</a><span>|</span><label class="collapse" for="c-39621069">[-]</label><label class="expand" for="c-39621069">[13 more]</label></div><br/><div class="children"><div class="content">So essentially a startup in this context has a small number of people and a large amount of money for training clusters. The article describes many operation leasing servers - that you assume to go many startups (or existing firms).<p>So it seems like you have the various LLM creators all doing roughly the same sort of thing (training with text and image data) with similar hardware and similar data. Each of these naturally has their own brand of &quot;secret sauce&quot; for distinguishing their venture. The various secret sauces can make a difference in the quality of an LLM&#x27;s output.<p>Yet overall, this seems like a massive, energy intensive exercise in redundancy.</div><br/><div id="39625762" class="c"><input type="checkbox" id="c-39625762" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#39621069">parent</a><span>|</span><a href="#39621148">next</a><span>|</span><label class="collapse" for="c-39625762">[-]</label><label class="expand" for="c-39625762">[1 more]</label></div><br/><div class="children"><div class="content">&quot;this seems like a massive, energy intensive exercise in redundancy&quot;<p>This is commonly refered to as a market working as intended. Yes, the waste from this type of redundency can be <i>massive</i>, especially if you realize that ultimately just a tiny percentage of these efforts will result in  even moderate success. But it is the price to pay at the edge of progress. A planned monopoly might be more efficient (despite popular banter that just compares a megacorp or a gov, which is basically the same, to a single succesfull startup ignoring the 999 that tried and failed), but those seldom beat a market on innovation.</div><br/></div></div><div id="39621148" class="c"><input type="checkbox" id="c-39621148" checked=""/><div class="controls bullet"><span class="by">dauertewigkeit</span><span>|</span><a href="#39621069">parent</a><span>|</span><a href="#39625762">prev</a><span>|</span><a href="#39625943">next</a><span>|</span><label class="collapse" for="c-39621148">[-]</label><label class="expand" for="c-39621148">[8 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think most of them have any kind of secret sauce. I think the founders hope to get bought out simply for being able to train &quot;near-SOTA&quot; LLMs. I guess achieving that level of skill and infra could be valuable enough to build upon.</div><br/><div id="39622021" class="c"><input type="checkbox" id="c-39622021" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#39621069">root</a><span>|</span><a href="#39621148">parent</a><span>|</span><a href="#39625943">next</a><span>|</span><label class="collapse" for="c-39622021">[-]</label><label class="expand" for="c-39622021">[7 more]</label></div><br/><div class="children"><div class="content">Sure, that&#x27;s also a factor but I&#x27;d say it reinforces my main point.</div><br/><div id="39623352" class="c"><input type="checkbox" id="c-39623352" checked=""/><div class="controls bullet"><span class="by">DeepChill</span><span>|</span><a href="#39621069">root</a><span>|</span><a href="#39622021">parent</a><span>|</span><a href="#39625943">next</a><span>|</span><label class="collapse" for="c-39623352">[-]</label><label class="expand" for="c-39623352">[6 more]</label></div><br/><div class="children"><div class="content">Good point, so the only real differentiator would be the size &amp; quality of the data being fed and the fine tuning done on the model? I wonder what else differentiates LLMs from each other</div><br/><div id="39623744" class="c"><input type="checkbox" id="c-39623744" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#39621069">root</a><span>|</span><a href="#39623352">parent</a><span>|</span><a href="#39623571">next</a><span>|</span><label class="collapse" for="c-39623744">[-]</label><label class="expand" for="c-39623744">[1 more]</label></div><br/><div class="children"><div class="content">Also getting a golden ticket.<p>Golliath 120b is still the best open source model and no one knows why since it&#x27;s just two llama2 60b glued together.</div><br/></div></div><div id="39623571" class="c"><input type="checkbox" id="c-39623571" checked=""/><div class="controls bullet"><span class="by">Iulioh</span><span>|</span><a href="#39621069">root</a><span>|</span><a href="#39623352">parent</a><span>|</span><a href="#39623744">prev</a><span>|</span><a href="#39625943">next</a><span>|</span><label class="collapse" for="c-39623571">[-]</label><label class="expand" for="c-39623571">[4 more]</label></div><br/><div class="children"><div class="content">Alignment and censorship ?</div><br/><div id="39623791" class="c"><input type="checkbox" id="c-39623791" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#39621069">root</a><span>|</span><a href="#39623571">parent</a><span>|</span><a href="#39625943">next</a><span>|</span><label class="collapse" for="c-39623791">[-]</label><label class="expand" for="c-39623791">[3 more]</label></div><br/><div class="children"><div class="content">Alignment just means making it do what you want. LLMs just continue the sequence, the chat questions and response style we have now is an example of alignment (to what humans want).</div><br/><div id="39625302" class="c"><input type="checkbox" id="c-39625302" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#39621069">root</a><span>|</span><a href="#39623791">parent</a><span>|</span><a href="#39625943">next</a><span>|</span><label class="collapse" for="c-39625302">[-]</label><label class="expand" for="c-39625302">[2 more]</label></div><br/><div class="children"><div class="content">Alignment can mean making sure your LLM doesn&#x27;t continue the sequence in embarrassing ways, eg by spouting politically incorrect sequences of words (even though those might have been common in the training data).</div><br/><div id="39626138" class="c"><input type="checkbox" id="c-39626138" checked=""/><div class="controls bullet"><span class="by">friendzis</span><span>|</span><a href="#39621069">root</a><span>|</span><a href="#39625302">parent</a><span>|</span><a href="#39625943">next</a><span>|</span><label class="collapse" for="c-39626138">[-]</label><label class="expand" for="c-39626138">[1 more]</label></div><br/><div class="children"><div class="content">In what way does this do more good than harm?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39625943" class="c"><input type="checkbox" id="c-39625943" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39621069">parent</a><span>|</span><a href="#39621148">prev</a><span>|</span><a href="#39623721">next</a><span>|</span><label class="collapse" for="c-39625943">[-]</label><label class="expand" for="c-39625943">[1 more]</label></div><br/><div class="children"><div class="content">There are not that many of these startups actually. Most use cases of LLM can be backed with a fine-tune of an off-the-shelf foundation model. If you&#x27;re training foundation models from scratch, you&#x27;re entering a difficult-to-monetize market where the big boys could eat your lunch by just releasing a new foundation model that might be able to do more than 95% of what yours does.</div><br/></div></div><div id="39623721" class="c"><input type="checkbox" id="c-39623721" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#39621069">parent</a><span>|</span><a href="#39625943">prev</a><span>|</span><a href="#39623736">next</a><span>|</span><label class="collapse" for="c-39623721">[-]</label><label class="expand" for="c-39623721">[1 more]</label></div><br/><div class="children"><div class="content">Maybe it’s simpler than that. Instead of spending money on compute that costs X and that cloud providers charge 20*X for, they could spend the money creating training data, but that story is way too hard to tell to investors.</div><br/></div></div><div id="39623736" class="c"><input type="checkbox" id="c-39623736" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#39621069">parent</a><span>|</span><a href="#39623721">prev</a><span>|</span><a href="#39610011">next</a><span>|</span><label class="collapse" for="c-39623736">[-]</label><label class="expand" for="c-39623736">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Yet overall, this seems like a massive, energy intensive exercise in redundancy.<p>Keep in mind that this is also chaff to distract people from the real secret sauce. I imagine that just as many startups are hiring writers and photographers to create extremely well labelled uncontaminated data for training.<p>One only need to look at the perverts over at civitai to see how far you can go with intensive labeling on a tiny compute budget.</div><br/></div></div></div></div><div id="39610011" class="c"><input type="checkbox" id="c-39610011" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#39621069">prev</a><span>|</span><a href="#39625158">next</a><span>|</span><label class="collapse" for="c-39610011">[-]</label><label class="expand" for="c-39610011">[7 more]</label></div><br/><div class="children"><div class="content">for context Yi Tay was Tech Lead on Google PaLM, UL2, Flan, Bard, etc and now is cofoudner at Reka (which has shipped some v interesting small multimodal models that have featured on here). I prompted him for this post as an ex-Googler now training LLMs as an independent startup <a href="https:&#x2F;&#x2F;twitter.com&#x2F;YiTayML&#x2F;status&#x2F;1765105066263052718" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;YiTayML&#x2F;status&#x2F;1765105066263052718</a><p>our conversation was recorded here <a href="https:&#x2F;&#x2F;sub.thursdai.news&#x2F;p&#x2F;thursdai-feb-15-2024-openai-changes" rel="nofollow">https:&#x2F;&#x2F;sub.thursdai.news&#x2F;p&#x2F;thursdai-feb-15-2024-openai-chan...</a></div><br/><div id="39621698" class="c"><input type="checkbox" id="c-39621698" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#39610011">parent</a><span>|</span><a href="#39624639">next</a><span>|</span><label class="collapse" for="c-39621698">[-]</label><label class="expand" for="c-39621698">[4 more]</label></div><br/><div class="children"><div class="content">(update: i submitted this yesterday and it didnt get traction, i guess @dang must’ve merged the old submission in here. you really didnt have to, but its a nice gesture. thanks dang!!)</div><br/><div id="39621984" class="c"><input type="checkbox" id="c-39621984" checked=""/><div class="controls bullet"><span class="by">axpy906</span><span>|</span><a href="#39610011">root</a><span>|</span><a href="#39621698">parent</a><span>|</span><a href="#39624639">next</a><span>|</span><label class="collapse" for="c-39621984">[-]</label><label class="expand" for="c-39621984">[3 more]</label></div><br/><div class="children"><div class="content">Great too see you on here. Love Latent Space podcast.</div><br/><div id="39623580" class="c"><input type="checkbox" id="c-39623580" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#39610011">root</a><span>|</span><a href="#39621984">parent</a><span>|</span><a href="#39624639">next</a><span>|</span><label class="collapse" for="c-39623580">[-]</label><label class="expand" for="c-39623580">[2 more]</label></div><br/><div class="children"><div class="content">aw thank you for listening. some weeks its very much a labor of love lol.<p>no events planned near term but come to the big shindig in june <a href="https:&#x2F;&#x2F;ti.to&#x2F;software-3&#x2F;ai-engineer-worlds-fair" rel="nofollow">https:&#x2F;&#x2F;ti.to&#x2F;software-3&#x2F;ai-engineer-worlds-fair</a> . last year&#x27;s summit was the first time i really understood how much of a reach we have and how many good AI people we&#x27;ve managed to gather as friends.</div><br/><div id="39624415" class="c"><input type="checkbox" id="c-39624415" checked=""/><div class="controls bullet"><span class="by">dwaltrip</span><span>|</span><a href="#39610011">root</a><span>|</span><a href="#39623580">parent</a><span>|</span><a href="#39624639">next</a><span>|</span><label class="collapse" for="c-39624415">[-]</label><label class="expand" for="c-39624415">[1 more]</label></div><br/><div class="children"><div class="content">I love it as well, it’s a fantastic resource :)</div><br/></div></div></div></div></div></div></div></div><div id="39624639" class="c"><input type="checkbox" id="c-39624639" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#39610011">parent</a><span>|</span><a href="#39621698">prev</a><span>|</span><a href="#39625158">next</a><span>|</span><label class="collapse" for="c-39624639">[-]</label><label class="expand" for="c-39624639">[2 more]</label></div><br/><div class="children"><div class="content">Is he the person after the Yi LLM model?</div><br/><div id="39624929" class="c"><input type="checkbox" id="c-39624929" checked=""/><div class="controls bullet"><span class="by">bigcat12345678</span><span>|</span><a href="#39610011">root</a><span>|</span><a href="#39624639">parent</a><span>|</span><a href="#39625158">next</a><span>|</span><label class="collapse" for="c-39624929">[-]</label><label class="expand" for="c-39624929">[1 more]</label></div><br/><div class="children"><div class="content">No
Yi LLM models are from [0], Kaifu Li&#x27;s LLM startup.<p>[0] <a href="https:&#x2F;&#x2F;www.lingyiwanwu.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.lingyiwanwu.com&#x2F;</a></div><br/></div></div></div></div></div></div><div id="39625158" class="c"><input type="checkbox" id="c-39625158" checked=""/><div class="controls bullet"><span class="by">a_bonobo</span><span>|</span><a href="#39610011">prev</a><span>|</span><a href="#39621024">next</a><span>|</span><label class="collapse" for="c-39625158">[-]</label><label class="expand" for="c-39625158">[2 more]</label></div><br/><div class="children"><div class="content">But what is the product they&#x27;re selling?<p>The main Reka.AI page looks like a regular ChatGPT clone, an LLM you pay for by the token. How is this different from all these other companies? Pricing seems to be comparable to ChatGPT 3.5-Turbo.</div><br/><div id="39625420" class="c"><input type="checkbox" id="c-39625420" checked=""/><div class="controls bullet"><span class="by">polygamous_bat</span><span>|</span><a href="#39625158">parent</a><span>|</span><a href="#39621024">next</a><span>|</span><label class="collapse" for="c-39625420">[-]</label><label class="expand" for="c-39625420">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps a cure for venture capitalist FOMO for not having invested in AI?</div><br/></div></div></div></div><div id="39621024" class="c"><input type="checkbox" id="c-39621024" checked=""/><div class="controls bullet"><span class="by">abeppu</span><span>|</span><a href="#39625158">prev</a><span>|</span><a href="#39625920">next</a><span>|</span><label class="collapse" for="c-39621024">[-]</label><label class="expand" for="c-39621024">[10 more]</label></div><br/><div class="children"><div class="content">It&#x27;s worth taking a second to note that the author just assumes that readers understand &quot;the wilderness&quot; to mean &quot;not Google&quot;.<p>This post gives a lot of credit to Google&#x27;s infra and hardware teams, and I&#x27;d love to read a perspective from one of those insiders who then went on to do related work elsewhere.</div><br/><div id="39623169" class="c"><input type="checkbox" id="c-39623169" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#39621024">parent</a><span>|</span><a href="#39623274">next</a><span>|</span><label class="collapse" for="c-39623169">[-]</label><label class="expand" for="c-39623169">[1 more]</label></div><br/><div class="children"><div class="content">I took the phrase to mean &quot;outside any large company&quot;. It seems like a fairly obvious metaphor; if you have a starup working on a large scale infrastructure project, you have to set your own logistics just a camp in the literal wildness.</div><br/></div></div><div id="39623274" class="c"><input type="checkbox" id="c-39623274" checked=""/><div class="controls bullet"><span class="by">choppaface</span><span>|</span><a href="#39621024">parent</a><span>|</span><a href="#39623169">prev</a><span>|</span><a href="#39623541">next</a><span>|</span><label class="collapse" for="c-39623274">[-]</label><label class="expand" for="c-39623274">[3 more]</label></div><br/><div class="children"><div class="content">Really telling quote:<p>&gt; I was completely taken aback by the failure rate of GPUs as opposed to my experiences on TPUs at Google<p>Should be &quot;I was completely unaware of the failure modes of GPUs, because all my career I&#x27;ve been inside Google and used Google TPUs and was well-acquainted with those failure modes.&quot;<p>I&#x27;ve used GPUs mostly, and when I tried TPUs the jobs failed <i>all the time</i> for really hard-to-debug reasons.  Often the indirection between the x86 chip and the TPU device caused hours of hair-pulling, stuff you never get with x86+nvidia+pytorch.<p>10-15 years ago, Google minted many $10m+ data scientists (aka Sawzall engineers) who also ventured &quot;into the wilderness&quot; and had very similar reactions.  This blog post is much more about the OP hyping his company and personal brand than contributing useful notes to the community.</div><br/><div id="39626424" class="c"><input type="checkbox" id="c-39626424" checked=""/><div class="controls bullet"><span class="by">quadrature</span><span>|</span><a href="#39621024">root</a><span>|</span><a href="#39623274">parent</a><span>|</span><a href="#39626171">next</a><span>|</span><label class="collapse" for="c-39626424">[-]</label><label class="expand" for="c-39626424">[1 more]</label></div><br/><div class="children"><div class="content">I think the OP is referring to hardware failures rather than software not playing well together.</div><br/></div></div><div id="39626171" class="c"><input type="checkbox" id="c-39626171" checked=""/><div class="controls bullet"><span class="by">StarCyan</span><span>|</span><a href="#39621024">root</a><span>|</span><a href="#39623274">parent</a><span>|</span><a href="#39626424">prev</a><span>|</span><a href="#39623541">next</a><span>|</span><label class="collapse" for="c-39626171">[-]</label><label class="expand" for="c-39626171">[1 more]</label></div><br/><div class="children"><div class="content">When was this? I use JAX+TPUs to train LLMs and haven&#x27;t experienced many issues. IMO it was way easier to set up distributed training, sharding, etc compared to Pytorch+GPUs.</div><br/></div></div></div></div><div id="39623541" class="c"><input type="checkbox" id="c-39623541" checked=""/><div class="controls bullet"><span class="by">ganeshkrishnan</span><span>|</span><a href="#39621024">parent</a><span>|</span><a href="#39623274">prev</a><span>|</span><a href="#39625920">next</a><span>|</span><label class="collapse" for="c-39623541">[-]</label><label class="expand" for="c-39623541">[5 more]</label></div><br/><div class="children"><div class="content">OP mentions the failure rate of GPUs as &quot;If this were in GPU land, it would have failed within the first few days for sure.&quot;.<p>In my humble opinion, we never had failures of GPU even for large scale training. Our current training batch job is a 20GB json file which takes 6 hours just to load and has been running for more than 15 days with not a hiccup. And we are using the older Tesla T4.<p>GPUs have memory constraint issues but if you can plan and work around it, I havent seen it crash in real life.</div><br/><div id="39624346" class="c"><input type="checkbox" id="c-39624346" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#39621024">root</a><span>|</span><a href="#39623541">parent</a><span>|</span><a href="#39625501">next</a><span>|</span><label class="collapse" for="c-39624346">[-]</label><label class="expand" for="c-39624346">[1 more]</label></div><br/><div class="children"><div class="content">&gt; And we are using the older Tesla T4.<p>That&#x27;s an undemanding and well-debugged chip by this point (6 years ago!). So you aren&#x27;t experiencing any of the pain people using A100s or H100s (never mind people who have to stand up clusters with B100s soon) are going through now.</div><br/></div></div><div id="39625501" class="c"><input type="checkbox" id="c-39625501" checked=""/><div class="controls bullet"><span class="by">shrubble</span><span>|</span><a href="#39621024">root</a><span>|</span><a href="#39623541">parent</a><span>|</span><a href="#39624346">prev</a><span>|</span><a href="#39625624">next</a><span>|</span><label class="collapse" for="c-39625501">[-]</label><label class="expand" for="c-39625501">[1 more]</label></div><br/><div class="children"><div class="content">Have you checked if there is a faster way to parse your JSON?  3Gbytes&#x2F;hour to load a file seems slow on today&#x27;s CPUs...</div><br/></div></div><div id="39624336" class="c"><input type="checkbox" id="c-39624336" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#39621024">root</a><span>|</span><a href="#39623541">parent</a><span>|</span><a href="#39625624">prev</a><span>|</span><a href="#39625920">next</a><span>|</span><label class="collapse" for="c-39624336">[-]</label><label class="expand" for="c-39624336">[1 more]</label></div><br/><div class="children"><div class="content">Ha! We’re also committing great sins of computation against T4s at our company. Hopefully, as I learn, things get less janky.</div><br/></div></div></div></div></div></div><div id="39625920" class="c"><input type="checkbox" id="c-39625920" checked=""/><div class="controls bullet"><span class="by">TrackerFF</span><span>|</span><a href="#39621024">prev</a><span>|</span><a href="#39621034">next</a><span>|</span><label class="collapse" for="c-39625920">[-]</label><label class="expand" for="c-39625920">[1 more]</label></div><br/><div class="children"><div class="content">Big question is, how do small startups manage to get funding for LLM products if they don’t have the “correct” background &#x2F; pedigree?<p>The world of LLM startups is beginning to look like the world of hedge funds and private equity firms - where the prerequisites for seed&#x2F;funding are:<p>A) Prestigious employment history &#x2F; correct pedigree.<p>B) Solid network of investors ready to jump before any product has even begun.</div><br/></div></div><div id="39621034" class="c"><input type="checkbox" id="c-39621034" checked=""/><div class="controls bullet"><span class="by">yalok</span><span>|</span><a href="#39625920">prev</a><span>|</span><a href="#39620858">next</a><span>|</span><label class="collapse" for="c-39621034">[-]</label><label class="expand" for="c-39621034">[1 more]</label></div><br/><div class="children"><div class="content">&gt; All in all, this is only a small part of the story of how we started a company, raised some money, bought some chips and matched Gemini pro&#x2F;GPT 3.5 and outperformed many others in less than a year having to build everything from scratch.<p>I wonder what was the budget spent for the chips&#x2F;cloud GPUs to achieve GPT 3.5 level LLM - at least in the order to magnitude - 2-5 millions?</div><br/></div></div><div id="39620858" class="c"><input type="checkbox" id="c-39620858" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#39621034">prev</a><span>|</span><a href="#39626490">next</a><span>|</span><label class="collapse" for="c-39620858">[-]</label><label class="expand" for="c-39620858">[11 more]</label></div><br/><div class="children"><div class="content">Training LLM from scratch is a super important issue that affects the pace and breadth of iteration of AI almost as much as the raw hardware improvements do.  The blog is fun but somewhat shallow and not technical or very surprising if you’ve worked with clusters of GPUs in any capacity over the years. (I liked the perspective of a former googler, but I’m not sure why past colleagues would recommend Jax over pytorch for LLMs outside of Google.) I hope this newco eventually releases a more technical report about their training adventures, like the PDF file here: <a href="https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;metaseq&#x2F;tree&#x2F;main&#x2F;projects&#x2F;OPT&#x2F;chronicles">https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;metaseq&#x2F;tree&#x2F;main&#x2F;projec...</a></div><br/><div id="39621957" class="c"><input type="checkbox" id="c-39621957" checked=""/><div class="controls bullet"><span class="by">axpy906</span><span>|</span><a href="#39620858">parent</a><span>|</span><a href="#39626490">next</a><span>|</span><label class="collapse" for="c-39621957">[-]</label><label class="expand" for="c-39621957">[10 more]</label></div><br/><div class="children"><div class="content">If you’re doing research JAX makes some sense. Probably some Google bias in there too.</div><br/><div id="39622645" class="c"><input type="checkbox" id="c-39622645" checked=""/><div class="controls bullet"><span class="by">lyapunova</span><span>|</span><a href="#39620858">root</a><span>|</span><a href="#39621957">parent</a><span>|</span><a href="#39626490">next</a><span>|</span><label class="collapse" for="c-39622645">[-]</label><label class="expand" for="c-39622645">[9 more]</label></div><br/><div class="children"><div class="content">To be honest, most researchers in applied ML in the bay say the opposite. If you are trying to be nimble and prototype, use pytorch. If you&#x27;re trying to gain some optimizations as you near deployment, rewrite in Jax.</div><br/><div id="39625709" class="c"><input type="checkbox" id="c-39625709" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#39620858">root</a><span>|</span><a href="#39622645">parent</a><span>|</span><a href="#39623729">next</a><span>|</span><label class="collapse" for="c-39625709">[-]</label><label class="expand" for="c-39625709">[1 more]</label></div><br/><div class="children"><div class="content">Interesting perspective about possible Jax optimizations. Assuming these models are trained and deployed on non-TPU hardware, are there any real advantages in using Jax for deployment on GPU?  I’d have assumed that inference is largely a solved optimization for large transformer based models (with any low hanging fruits from custom CUDA code already written) and the details are shifting towards infrastructure tradeoffs and availability of efficient GPUs. But I may be out of the loop with the latest gossip. Or do you simply mean that maybe there exist cases where TPU inference makes sense financially and using jax makes a difference?</div><br/></div></div><div id="39623729" class="c"><input type="checkbox" id="c-39623729" checked=""/><div class="controls bullet"><span class="by">axpy906</span><span>|</span><a href="#39620858">root</a><span>|</span><a href="#39622645">parent</a><span>|</span><a href="#39625709">prev</a><span>|</span><a href="#39623510">next</a><span>|</span><label class="collapse" for="c-39623729">[-]</label><label class="expand" for="c-39623729">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. I’ve never heard that. I could see that argument going both ways as PyTorch has the larger ecosystem and is published the most.</div><br/></div></div><div id="39623510" class="c"><input type="checkbox" id="c-39623510" checked=""/><div class="controls bullet"><span class="by">plumeria</span><span>|</span><a href="#39620858">root</a><span>|</span><a href="#39622645">parent</a><span>|</span><a href="#39623729">prev</a><span>|</span><a href="#39626490">next</a><span>|</span><label class="collapse" for="c-39623510">[-]</label><label class="expand" for="c-39623510">[6 more]</label></div><br/><div class="children"><div class="content">Where does Tensorflow stand in this?</div><br/><div id="39624076" class="c"><input type="checkbox" id="c-39624076" checked=""/><div class="controls bullet"><span class="by">rockinghigh</span><span>|</span><a href="#39620858">root</a><span>|</span><a href="#39623510">parent</a><span>|</span><a href="#39623694">next</a><span>|</span><label class="collapse" for="c-39624076">[-]</label><label class="expand" for="c-39624076">[1 more]</label></div><br/><div class="children"><div class="content">Tensorflow has been falling behind since they stopped caring about backward compatibility. PyTorch is the leading framework. Jax is getting some traction at Google and was used to train Gemini.</div><br/></div></div><div id="39623694" class="c"><input type="checkbox" id="c-39623694" checked=""/><div class="controls bullet"><span class="by">axpy906</span><span>|</span><a href="#39620858">root</a><span>|</span><a href="#39623510">parent</a><span>|</span><a href="#39624076">prev</a><span>|</span><a href="#39624196">next</a><span>|</span><label class="collapse" for="c-39623694">[-]</label><label class="expand" for="c-39623694">[3 more]</label></div><br/><div class="children"><div class="content">Somewhere next to Theano, Mxnet or Caffe.</div><br/><div id="39623967" class="c"><input type="checkbox" id="c-39623967" checked=""/><div class="controls bullet"><span class="by">plumeria</span><span>|</span><a href="#39620858">root</a><span>|</span><a href="#39623694">parent</a><span>|</span><a href="#39624318">next</a><span>|</span><label class="collapse" for="c-39623967">[-]</label><label class="expand" for="c-39623967">[1 more]</label></div><br/><div class="children"><div class="content">So, obsolete?</div><br/></div></div><div id="39624318" class="c"><input type="checkbox" id="c-39624318" checked=""/><div class="controls bullet"><span class="by">omneity</span><span>|</span><a href="#39620858">root</a><span>|</span><a href="#39623694">parent</a><span>|</span><a href="#39623967">prev</a><span>|</span><a href="#39624196">next</a><span>|</span><label class="collapse" for="c-39624318">[-]</label><label class="expand" for="c-39624318">[1 more]</label></div><br/><div class="children"><div class="content">What about Keras?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39626490" class="c"><input type="checkbox" id="c-39626490" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#39620858">prev</a><span>|</span><a href="#39624581">next</a><span>|</span><label class="collapse" for="c-39626490">[-]</label><label class="expand" for="c-39626490">[1 more]</label></div><br/><div class="children"><div class="content">Then what happens when the LLM or AI performs worse than expected? Spend more 
 money fine tuning?<p>By the time you get it all working, not only you&#x27;ve spend lots of your VC capital on training alone, your competitors (Google, Meta, etc) already released a more powerful model much better and quicker than you before you could your run the second training epoch.<p>Another example of a startup incinerating the VC pump and dump scheme for vaporware AI snake-oil.</div><br/></div></div><div id="39624581" class="c"><input type="checkbox" id="c-39624581" checked=""/><div class="controls bullet"><span class="by">julianh65</span><span>|</span><a href="#39626490">prev</a><span>|</span><a href="#39622774">next</a><span>|</span><label class="collapse" for="c-39624581">[-]</label><label class="expand" for="c-39624581">[1 more]</label></div><br/><div class="children"><div class="content">So which compute providers have folks had a good experience with?</div><br/></div></div><div id="39622774" class="c"><input type="checkbox" id="c-39622774" checked=""/><div class="controls bullet"><span class="by">bo1024</span><span>|</span><a href="#39624581">prev</a><span>|</span><a href="#39624314">next</a><span>|</span><label class="collapse" for="c-39622774">[-]</label><label class="expand" for="c-39622774">[1 more]</label></div><br/><div class="children"><div class="content">This is very interesting, but I really want to hear about the training data process!</div><br/></div></div><div id="39624314" class="c"><input type="checkbox" id="c-39624314" checked=""/><div class="controls bullet"><span class="by">planet_y</span><span>|</span><a href="#39622774">prev</a><span>|</span><a href="#39624769">next</a><span>|</span><label class="collapse" for="c-39624314">[-]</label><label class="expand" for="c-39624314">[5 more]</label></div><br/><div class="children"><div class="content">I’m wondering if the title should read “from the ground up” instead of “ground zero”? <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hypocenter" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hypocenter</a></div><br/><div id="39626432" class="c"><input type="checkbox" id="c-39626432" checked=""/><div class="controls bullet"><span class="by">dotancohen</span><span>|</span><a href="#39624314">parent</a><span>|</span><a href="#39626544">next</a><span>|</span><label class="collapse" for="c-39626432">[-]</label><label class="expand" for="c-39626432">[1 more]</label></div><br/><div class="children"><div class="content">Yes, the title sounds like somebody confused two idioms. That&#x27;s not the type of author from whom I want to learn.</div><br/></div></div><div id="39626544" class="c"><input type="checkbox" id="c-39626544" checked=""/><div class="controls bullet"><span class="by">makoto12</span><span>|</span><a href="#39624314">parent</a><span>|</span><a href="#39626432">prev</a><span>|</span><a href="#39624390">next</a><span>|</span><label class="collapse" for="c-39626544">[-]</label><label class="expand" for="c-39626544">[1 more]</label></div><br/><div class="children"><div class="content">could be intentional. Implying LLMs are a proverbial nuclear bomb to the tech landscape. but honestly it threw me as well</div><br/></div></div><div id="39624390" class="c"><input type="checkbox" id="c-39624390" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#39624314">parent</a><span>|</span><a href="#39626544">prev</a><span>|</span><a href="#39624769">next</a><span>|</span><label class="collapse" for="c-39624390">[-]</label><label class="expand" for="c-39624390">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.merriam-webster.com&#x2F;dictionary&#x2F;ground%20zero" rel="nofollow">https:&#x2F;&#x2F;www.merriam-webster.com&#x2F;dictionary&#x2F;ground%20zero</a><p>It is a perfectly acceptable use of the idiom.</div><br/><div id="39624519" class="c"><input type="checkbox" id="c-39624519" checked=""/><div class="controls bullet"><span class="by">davidmurdoch</span><span>|</span><a href="#39624314">root</a><span>|</span><a href="#39624390">parent</a><span>|</span><a href="#39624769">next</a><span>|</span><label class="collapse" for="c-39624519">[-]</label><label class="expand" for="c-39624519">[1 more]</label></div><br/><div class="children"><div class="content">Acceptable, but maybe not perfectly.</div><br/></div></div></div></div></div></div><div id="39624769" class="c"><input type="checkbox" id="c-39624769" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39624314">prev</a><span>|</span><a href="#39623764">next</a><span>|</span><label class="collapse" for="c-39624769">[-]</label><label class="expand" for="c-39624769">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In the end it took us only a very small number of smaller scale &amp; shorter ablation runs to get to the strong 21B Reka Flash and 7B edge model (and also our upcoming largest core model). Finding a solid recipe with a very limited number of runs is challenging and requires changing many variables at once given the ridiculously enormous search space. In order to do this, one has to abandon the systematicity of Bigtech and rely a lot on “Yolo”, gut feeling and instinct.<p>&gt; Thankfully, I (and many of us in the team) have built up this intuition quite a bit in our ML careers to get it right within a substantially short amount of tries. While we’ve trained really good models before in our previous jobs, differences in training infrastructure, data, incorporation of new ideas and other environmental issues can still cause non-trivial differences in outcomes. That said, a strong prior helps to significantly cut down the search space and is probably one of the easiest explanations to why we were able to train really strong models with so few trials, resources and experimentation.</div><br/></div></div><div id="39623764" class="c"><input type="checkbox" id="c-39623764" checked=""/><div class="controls bullet"><span class="by">LZ_Khan</span><span>|</span><a href="#39624769">prev</a><span>|</span><a href="#39625258">next</a><span>|</span><label class="collapse" for="c-39623764">[-]</label><label class="expand" for="c-39623764">[1 more]</label></div><br/><div class="children"><div class="content">I wish I knew how to do yolo runs.<p>- signed, a compute resource hog at FAANG</div><br/></div></div><div id="39625258" class="c"><input type="checkbox" id="c-39625258" checked=""/><div class="controls bullet"><span class="by">classified</span><span>|</span><a href="#39623764">prev</a><span>|</span><a href="#39621537">next</a><span>|</span><label class="collapse" for="c-39625258">[-]</label><label class="expand" for="c-39625258">[1 more]</label></div><br/><div class="children"><div class="content">Absorbing the risk of copyright and license violations en masse for the training data as a service?</div><br/></div></div><div id="39621537" class="c"><input type="checkbox" id="c-39621537" checked=""/><div class="controls bullet"><span class="by">twelfthnight</span><span>|</span><a href="#39625258">prev</a><span>|</span><a href="#39625961">next</a><span>|</span><label class="collapse" for="c-39621537">[-]</label><label class="expand" for="c-39621537">[24 more]</label></div><br/><div class="children"><div class="content">&gt; To be very frank, I would have to say the quality of codebases externally significantly lag behind those I’ve been used to at Google<p>Haven&#x27;t worked at Google, anyone else share this sentiment? I always feel like working with Google code is typically not idiomatic and super difficult to go &quot;under the hood&quot; if anything isn&#x27;t precisely on the happy path.</div><br/><div id="39622176" class="c"><input type="checkbox" id="c-39622176" checked=""/><div class="controls bullet"><span class="by">titanomachy</span><span>|</span><a href="#39621537">parent</a><span>|</span><a href="#39625469">next</a><span>|</span><label class="collapse" for="c-39622176">[-]</label><label class="expand" for="c-39622176">[3 more]</label></div><br/><div class="children"><div class="content">I thought the quality was pretty high, largely because there were a lot of rails constraining how code should be written. Most of the code I dealt with was written using somewhat rigid (but generally well-designed) frameworks with programmatically-enforced style guides.<p>Also, most work seemed to involve some balance of junior and more experienced people, which helped keep quality higher. Outside of Google, I&#x27;ve seen pretty large projects written by new grads with little supervision (and on a tight timeline). Those codebases can be pretty hairy.</div><br/><div id="39624735" class="c"><input type="checkbox" id="c-39624735" checked=""/><div class="controls bullet"><span class="by">rokkitmensch</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39622176">parent</a><span>|</span><a href="#39622568">next</a><span>|</span><label class="collapse" for="c-39624735">[-]</label><label class="expand" for="c-39624735">[1 more]</label></div><br/><div class="children"><div class="content">The thing that impressed me most about Google was the encoding-of-cultural-norms-in-various-CI-jobs.<p>It lets them extract usable SWE horsepower from pretty much anyone who steps inside and at least tries to be useful and not just coast. They can ingest a startup engineer, someone who&#x27;s been a mid-tier enterprise codemonkey, yr mythical 10xer, the whole statistical gamut.</div><br/></div></div><div id="39622568" class="c"><input type="checkbox" id="c-39622568" checked=""/><div class="controls bullet"><span class="by">twelfthnight</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39622176">parent</a><span>|</span><a href="#39624735">prev</a><span>|</span><a href="#39625469">next</a><span>|</span><label class="collapse" for="c-39622568">[-]</label><label class="expand" for="c-39622568">[1 more]</label></div><br/><div class="children"><div class="content">That honestly does seem like a recipe for good code. And sure, there&#x27;s tons of open source out there of dubious quality.<p>@resource0x in a sibling comment made the point that it&#x27;s possible to write great code even if the program is a flawed design. I&#x27;m probably conflating those things.</div><br/></div></div></div></div><div id="39625469" class="c"><input type="checkbox" id="c-39625469" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39621537">parent</a><span>|</span><a href="#39622176">prev</a><span>|</span><a href="#39622761">next</a><span>|</span><label class="collapse" for="c-39625469">[-]</label><label class="expand" for="c-39625469">[2 more]</label></div><br/><div class="children"><div class="content">A recent ex-googler here: quality of Google3 in general is pretty good, but the LLM training bits are so abysmal that I know people who have resigned instead of working on it. And it’s also extra slow because getting a couple local GPUs is not really an option. So you’re forced to “develop in Colab” which works for some things and not for others and in general sucks ass if you’re working on anything substantial. For anything more substantial you’ll be launching stuff on some resource pool, waiting for like 10-15 minutes until it starts (much longer for large models), and then trying to divine why it failed from voluminous and sometimes indecipherable crash logs which also hang your browser when cluster UI tries to load them.<p>Rumors of Google’s AI code superiority are vastly overblown in 2024. I’m currently at another major AI lab, and the code here can actually be understood and worked on, which I consider to be a massive advantage.</div><br/><div id="39626608" class="c"><input type="checkbox" id="c-39626608" checked=""/><div class="controls bullet"><span class="by">alsoworkedthere</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39625469">parent</a><span>|</span><a href="#39622761">next</a><span>|</span><label class="collapse" for="c-39626608">[-]</label><label class="expand" for="c-39626608">[1 more]</label></div><br/><div class="children"><div class="content">Finally, an accurate portrayal!<p>Google has superb robustness and code quality, with garbage-level usability. Once you&#x27;re setup, you can kick off many massive training jobs and compare results easily. However, getting to that point is really hard. You&#x27;ll never figure out how to use the ML infrastructure and libraries on your own. You can only get it to work by meeting with the teams that wrote the infra so they can find and fix every error and misconfiguration. Usually, there is one single way to get things working together, and neither the documentation nor the error messages will get you to that brittle state.<p>It&#x27;s near impossible to get a VM with a TPU or GPU attached, so there&#x27;s no way to debug issues that happen between the library and the accelerator. Plus somehow they&#x27;ve made Python take longer to build (??!!) and run than C++ takes, so your iteration cycle is several minutes for what would take seconds at any other place. Fun stuff! Somehow it&#x27;s still one of the best places to do ML work, but they sure try to make it as difficult as possible.</div><br/></div></div></div></div><div id="39622761" class="c"><input type="checkbox" id="c-39622761" checked=""/><div class="controls bullet"><span class="by">danans</span><span>|</span><a href="#39621537">parent</a><span>|</span><a href="#39625469">prev</a><span>|</span><a href="#39621658">next</a><span>|</span><label class="collapse" for="c-39622761">[-]</label><label class="expand" for="c-39622761">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Haven&#x27;t worked at Google, anyone else share this sentiment?<p>I worked there, and the quality is definitely much higher and the code tends to be far more maintainable.  However, there is often a cost for that, which is velocity.<p>Some of this is reduced by the sheer amount of automation in tooling (i.e. bots that block style violations and common bugs before a code change is submitted).<p>In other cases, it slows things down quite a bit.</div><br/></div></div><div id="39621658" class="c"><input type="checkbox" id="c-39621658" checked=""/><div class="controls bullet"><span class="by">winwang</span><span>|</span><a href="#39621537">parent</a><span>|</span><a href="#39622761">prev</a><span>|</span><a href="#39621824">next</a><span>|</span><label class="collapse" for="c-39621658">[-]</label><label class="expand" for="c-39621658">[13 more]</label></div><br/><div class="children"><div class="content">(not googler)<p>Google&#x27;s codebase is idiomatic to Google due to their strict language tooling. e.g. their C++ code stays away from advanced features. The tooling teams at Google have very strong say.</div><br/><div id="39621782" class="c"><input type="checkbox" id="c-39621782" checked=""/><div class="controls bullet"><span class="by">twelfthnight</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39621658">parent</a><span>|</span><a href="#39622519">next</a><span>|</span><label class="collapse" for="c-39621782">[-]</label><label class="expand" for="c-39621782">[11 more]</label></div><br/><div class="children"><div class="content">I get that sense too. Probably does work awesome if you&#x27;re inside. But man it&#x27;s a mess when they externalize stuff. Just one example: their cloud platform CLI includes an entire python installation and takes 1.7G on disk, just to make API calls...</div><br/><div id="39622481" class="c"><input type="checkbox" id="c-39622481" checked=""/><div class="controls bullet"><span class="by">marcyb5st</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39621782">parent</a><span>|</span><a href="#39621809">next</a><span>|</span><label class="collapse" for="c-39622481">[-]</label><label class="expand" for="c-39622481">[1 more]</label></div><br/><div class="children"><div class="content">Did you install all the components? Because if so you also installed emulators for the pubsub and big table (maybe others, I don&#x27;t remember) which explain the big footprint.</div><br/></div></div><div id="39621809" class="c"><input type="checkbox" id="c-39621809" checked=""/><div class="controls bullet"><span class="by">jen20</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39621782">parent</a><span>|</span><a href="#39622481">prev</a><span>|</span><a href="#39622519">next</a><span>|</span><label class="collapse" for="c-39621809">[-]</label><label class="expand" for="c-39621809">[9 more]</label></div><br/><div class="children"><div class="content">I have never understood why cloud providers seem to think it is OK to write their CLIs in Python. The AWS one is too, and the Azure one went from Node.js to Python some time ago.</div><br/><div id="39621906" class="c"><input type="checkbox" id="c-39621906" checked=""/><div class="controls bullet"><span class="by">anonymous-panda</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39621809">parent</a><span>|</span><a href="#39622084">next</a><span>|</span><label class="collapse" for="c-39621906">[-]</label><label class="expand" for="c-39621906">[5 more]</label></div><br/><div class="children"><div class="content">Packaging and stability reasons. Same for why it’s a 1.7gb install - probably where they landed after having tons of support issues on some random Python version they didn’t test or some issue with a dependency that had that issue. Freezing the entire set of artifacts is more stable and Python lets you move pretty quick. I can’t speak to why nodejs vs Python though - maybe Python is easier to embed?</div><br/><div id="39624975" class="c"><input type="checkbox" id="c-39624975" checked=""/><div class="controls bullet"><span class="by">jen20</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39621906">parent</a><span>|</span><a href="#39622096">next</a><span>|</span><label class="collapse" for="c-39624975">[-]</label><label class="expand" for="c-39624975">[1 more]</label></div><br/><div class="children"><div class="content">Of course, writing them in Go would solve all of these problems while producing packages which are much smaller.</div><br/></div></div><div id="39622096" class="c"><input type="checkbox" id="c-39622096" checked=""/><div class="controls bullet"><span class="by">twelfthnight</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39621906">parent</a><span>|</span><a href="#39624975">prev</a><span>|</span><a href="#39621966">next</a><span>|</span><label class="collapse" for="c-39622096">[-]</label><label class="expand" for="c-39622096">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I imagine that was the decision calculus. &quot;Instead of spending some more effort to save millions of unnecessary downloads of python&#x27;s runtime using a different language, let&#x27;s just bundle Python!&quot;<p>I wouldn&#x27;t be surprised if it was version 2.7 too...</div><br/></div></div><div id="39621966" class="c"><input type="checkbox" id="c-39621966" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39621906">parent</a><span>|</span><a href="#39622096">prev</a><span>|</span><a href="#39622084">next</a><span>|</span><label class="collapse" for="c-39621966">[-]</label><label class="expand" for="c-39621966">[2 more]</label></div><br/><div class="children"><div class="content">What? They only get package and stability because they include the runtime. If they just went with a compiled language they could distribute native binaries and have actual packaging and stability.</div><br/><div id="39622471" class="c"><input type="checkbox" id="c-39622471" checked=""/><div class="controls bullet"><span class="by">anonymous-panda</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39621966">parent</a><span>|</span><a href="#39622084">next</a><span>|</span><label class="collapse" for="c-39622471">[-]</label><label class="expand" for="c-39622471">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but it’s not just a single metric. Another is how easy it is for them to hire productive members of the team and how much that costs them - middling Python developers churning out fine”ish” code are cheaper than Rust developers doing the same. It’s hard to find a language where you can be as productive as a developer in Python that also has AOT compilation to generate standalone binaries.<p>Tldr: there’s multiple factors to consider here and it’s more interesting to understand the pressures that cause the decisions, especially if you want to try to create a world where different decisions are made.</div><br/></div></div></div></div></div></div><div id="39622084" class="c"><input type="checkbox" id="c-39622084" checked=""/><div class="controls bullet"><span class="by">jyap</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39621809">parent</a><span>|</span><a href="#39621906">prev</a><span>|</span><a href="#39621948">next</a><span>|</span><label class="collapse" for="c-39622084">[-]</label><label class="expand" for="c-39622084">[1 more]</label></div><br/><div class="children"><div class="content">It makes “sense” based on the domain of the cloud provider being DevOps teams who are maintaining and using these CLI tools. Ie. What they use day to day.<p>For anything more advanced they offer language specific SDKs in Rust, Swift, Kolton, etc…<p>For example integrating storage in an iOS app.</div><br/></div></div><div id="39621948" class="c"><input type="checkbox" id="c-39621948" checked=""/><div class="controls bullet"><span class="by">twelfthnight</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39621809">parent</a><span>|</span><a href="#39622084">prev</a><span>|</span><a href="#39622519">next</a><span>|</span><label class="collapse" for="c-39621948">[-]</label><label class="expand" for="c-39621948">[2 more]</label></div><br/><div class="children"><div class="content">There probably is a sense in which the API&#x27;s are constantly changing, so maybe an interpreted language might make sense? I imagine there has to be a better way to do with with Go or Rust though (even lua?) for a smaller binary.</div><br/><div id="39623015" class="c"><input type="checkbox" id="c-39623015" checked=""/><div class="controls bullet"><span class="by">candiodari</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39621948">parent</a><span>|</span><a href="#39622519">next</a><span>|</span><label class="collapse" for="c-39623015">[-]</label><label class="expand" for="c-39623015">[1 more]</label></div><br/><div class="children"><div class="content">Google python binaries are more akin to docker or even vm images, even if the actual technology used predates docker and even linux VMs. They contain something like a slimmed-down linux distribution, not just a binary.<p>EXTREME predictability (e.g. as never ever using the system&#x27;s libssl), in trade for huge binaries. They go pretty damn far in this: you won&#x27;t catch a Google binary even using most of libc.</div><br/></div></div></div></div></div></div></div></div><div id="39622519" class="c"><input type="checkbox" id="c-39622519" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39621658">parent</a><span>|</span><a href="#39621782">prev</a><span>|</span><a href="#39621824">next</a><span>|</span><label class="collapse" for="c-39622519">[-]</label><label class="expand" for="c-39622519">[1 more]</label></div><br/><div class="children"><div class="content">&gt; e.g. their C++ code stays away from advanced features<p>Which honestly is a GOOD thing because it would make it much easier for newcomers to ramp up on existing codebases. Most people aren&#x27;t used to working with spaceships and constexprs.<p>Readability is also far more valuable to a large team than efficiency for anything that isn&#x27;t a number-crunching loop.</div><br/></div></div></div></div><div id="39621824" class="c"><input type="checkbox" id="c-39621824" checked=""/><div class="controls bullet"><span class="by">renegade-otter</span><span>|</span><a href="#39621537">parent</a><span>|</span><a href="#39621658">prev</a><span>|</span><a href="#39623508">next</a><span>|</span><label class="collapse" for="c-39621824">[-]</label><label class="expand" for="c-39621824">[3 more]</label></div><br/><div class="children"><div class="content">&quot;Externally&quot;, no one could possibly beat Google&#x27;s track record of not committing to products before finally killing them. But the code was beautiful, though!</div><br/><div id="39621841" class="c"><input type="checkbox" id="c-39621841" checked=""/><div class="controls bullet"><span class="by">twelfthnight</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39621824">parent</a><span>|</span><a href="#39623508">next</a><span>|</span><label class="collapse" for="c-39621841">[-]</label><label class="expand" for="c-39621841">[2 more]</label></div><br/><div class="children"><div class="content">I mean, was Angular ever &quot;beautiful&quot;?</div><br/><div id="39622065" class="c"><input type="checkbox" id="c-39622065" checked=""/><div class="controls bullet"><span class="by">resource0x</span><span>|</span><a href="#39621537">root</a><span>|</span><a href="#39621841">parent</a><span>|</span><a href="#39623508">next</a><span>|</span><label class="collapse" for="c-39622065">[-]</label><label class="expand" for="c-39622065">[1 more]</label></div><br/><div class="children"><div class="content">Pretty sure it was. A lousy idea might still be implemented beautifully under the hood. :-)</div><br/></div></div></div></div></div></div></div></div><div id="39623350" class="c"><input type="checkbox" id="c-39623350" checked=""/><div class="controls bullet"><span class="by">stealthcat</span><span>|</span><a href="#39624748">prev</a><span>|</span><label class="collapse" for="c-39623350">[-]</label><label class="expand" for="c-39623350">[1 more]</label></div><br/><div class="children"><div class="content">Should list most of the technical debt accumulated so far and rank them. At this stage, lots of corners have been cut.</div><br/></div></div></div></div></div></div></div></body></html>