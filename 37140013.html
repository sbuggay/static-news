<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1692176461062" as="style"/><link rel="stylesheet" href="styles.css?v=1692176461062"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://finbarr.ca/how-is-llama-cpp-possible/">How Is LLaMa.cpp Possible?</a> <span class="domain">(<a href="https://finbarr.ca">finbarr.ca</a>)</span></div><div class="subtext"><span>birriel</span> | <span>165 comments</span></div><br/><div><div id="37140409" class="c"><input type="checkbox" id="c-37140409" checked=""/><div class="controls bullet"><span class="by">tomohelix</span><span>|</span><a href="#37142084">next</a><span>|</span><label class="collapse" for="c-37140409">[-]</label><label class="expand" for="c-37140409">[34 more]</label></div><br/><div class="children"><div class="content">In case anyone is wondering, yes, there is a cost when a model is quantized.<p><a href="https:&#x2F;&#x2F;oobabooga.github.io&#x2F;blog&#x2F;posts&#x2F;perplexities&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;oobabooga.github.io&#x2F;blog&#x2F;posts&#x2F;perplexities&#x2F;</a><p>Essentially, you lose some accuracy and there might be some weird answers and probably more likely to go off the rail and hallucinate. But the quality loss is lower the more parameters you have. So for very large model sizes the differences might be negligible. Also, this is the cost of inference only. Training is a whole other beast and requires much more power.<p>Still, we are looking at GPT3 level of performance on one server rack. That says something when less than a year ago, such AI was literally magic and only run on a massive datacenter. Bandwidth and memory size are probably, in my ignorance mind, easier to increase than raw compute so maybe we will soon actually have &quot;smart&quot; devices.</div><br/><div id="37140771" class="c"><input type="checkbox" id="c-37140771" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#37140409">parent</a><span>|</span><a href="#37143536">next</a><span>|</span><label class="collapse" for="c-37140771">[-]</label><label class="expand" for="c-37140771">[13 more]</label></div><br/><div class="children"><div class="content">I was hoping that link would answer the question that&#x27;s been bugging me for months: what are the penalties that you pay for using a quantized model?<p>Sadly it didn&#x27;t. It talked about &quot;perplexities&quot; and showed some floating point numbers.<p>I want to see examples like &quot;here&#x27;s a prompt against a model and the same prompt against a quantized version of that model, see how they differ.&quot;</div><br/><div id="37142756" class="c"><input type="checkbox" id="c-37142756" checked=""/><div class="controls bullet"><span class="by">mikeravkine</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37140771">parent</a><span>|</span><a href="#37142320">next</a><span>|</span><label class="collapse" for="c-37142756">[-]</label><label class="expand" for="c-37142756">[1 more]</label></div><br/><div class="children"><div class="content">I have several sets of quant comparisons posted on my HF spaces, the caveat is my prompts are all &quot;English to code&quot;:  <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mike-ravkine&#x2F;can-ai-code-compare" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mike-ravkine&#x2F;can-ai-code-compa...</a><p>The dropdown at the top selects which comparison: Falcon compares GGML, Vicuna compares bits and bytes. I have some more comparisons planned, feel free to open an issue if you&#x27;d like to see something specific: <a href="https:&#x2F;&#x2F;github.com&#x2F;the-crypt-keeper&#x2F;can-ai-code">https:&#x2F;&#x2F;github.com&#x2F;the-crypt-keeper&#x2F;can-ai-code</a></div><br/></div></div><div id="37142320" class="c"><input type="checkbox" id="c-37142320" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37140771">parent</a><span>|</span><a href="#37142756">prev</a><span>|</span><a href="#37140948">next</a><span>|</span><label class="collapse" for="c-37142320">[-]</label><label class="expand" for="c-37142320">[1 more]</label></div><br/><div class="children"><div class="content">It makes the model dumber.<p>That seems simplistic, but its really simple as that. Naive 3 bit quantization will turn llama 7B into blubbering nonsense.<p>But llama.cpp quantization is good! I recommend checking out the graphs ikawrakow made for their K-quants implementation:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1684">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1684</a><p>Basically, the more you quantize with K-quant, the dumber the model gets. 2 bit llama 13B quant, for instance, is about as dumb as 7B F16, but the dropoff is not nearly as severe from 3-6 bits.</div><br/></div></div><div id="37140948" class="c"><input type="checkbox" id="c-37140948" checked=""/><div class="controls bullet"><span class="by">version_five</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37140771">parent</a><span>|</span><a href="#37142320">prev</a><span>|</span><a href="#37142740">next</a><span>|</span><label class="collapse" for="c-37140948">[-]</label><label class="expand" for="c-37140948">[6 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  I want to see examples like &quot;here&#x27;s a prompt against a model and the same prompt against a quantized version of that model, see how they differ.&quot;
</code></pre>
We suck at evaluating and comparing models imo. There are metrics and evaluation task, but it&#x27;s still very subjective.<p>The closer we get to assessing human like performance, the tougher it is, because it becomes more subjective and less deterministic by the nature of the task. I don&#x27;t know the answer, but I know that for the metrics we have it&#x27;s not so easy to translate them into any idea about the kind of performance on some specific thing you might want to do with the model.</div><br/><div id="37142038" class="c"><input type="checkbox" id="c-37142038" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37140948">parent</a><span>|</span><a href="#37141609">next</a><span>|</span><label class="collapse" for="c-37142038">[-]</label><label class="expand" for="c-37142038">[1 more]</label></div><br/><div class="children"><div class="content">Not mathematically, at the very least. Perplexity is a translation of the best measure we have for informing us how a model is doing empirically over a test dataset (both pre and post). It is enough to be, usably, at least the final word on how different quantization methods perform.<p>Subjective ratings are different, but for compression things are quite well defined.</div><br/></div></div><div id="37141609" class="c"><input type="checkbox" id="c-37141609" checked=""/><div class="controls bullet"><span class="by">cj</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37140948">parent</a><span>|</span><a href="#37142038">prev</a><span>|</span><a href="#37142740">next</a><span>|</span><label class="collapse" for="c-37141609">[-]</label><label class="expand" for="c-37141609">[4 more]</label></div><br/><div class="children"><div class="content">&gt; some specific thing you might want to do with the model.<p>I think this right here is the answer to measuring and comparing model performance.<p>Instead of trying to compare models holistically, we should be comparing them for specific problem sets and use cases... the same as we compare humans against one another.<p>Using people as an example, a hiring manager doesn&#x27;t compare 2 people holistically, they compare 2 people based on how well they&#x27;re expected to perform a certain task or set of tasks.<p>We should be measuring and comparing models discriminately rather than holistically.</div><br/><div id="37141896" class="c"><input type="checkbox" id="c-37141896" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37141609">parent</a><span>|</span><a href="#37142740">next</a><span>|</span><label class="collapse" for="c-37141896">[-]</label><label class="expand" for="c-37141896">[3 more]</label></div><br/><div class="children"><div class="content">You could have two models answer 100 questions the same way, and differ on the 101st.  They’re unpredictable by nature - if we could accurately predict them we’d just use the predictions instead.</div><br/><div id="37142090" class="c"><input type="checkbox" id="c-37142090" checked=""/><div class="controls bullet"><span class="by">cj</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37141896">parent</a><span>|</span><a href="#37142740">next</a><span>|</span><label class="collapse" for="c-37142090">[-]</label><label class="expand" for="c-37142090">[2 more]</label></div><br/><div class="children"><div class="content">(Stupid question) are models still non-deterministic if you set the temperature to zero?<p>Would setting the temperature to zero degrade the quality of response?</div><br/><div id="37142827" class="c"><input type="checkbox" id="c-37142827" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37142090">parent</a><span>|</span><a href="#37142740">next</a><span>|</span><label class="collapse" for="c-37142827">[-]</label><label class="expand" for="c-37142827">[1 more]</label></div><br/><div class="children"><div class="content">Some GPU operations give different results depending on the order they are done. This happens because floating point numbers are approximations and lose associativity. Requiring a strict order causes a big slowdown.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37142740" class="c"><input type="checkbox" id="c-37142740" checked=""/><div class="controls bullet"><span class="by">sharms</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37140771">parent</a><span>|</span><a href="#37140948">prev</a><span>|</span><a href="#37143582">next</a><span>|</span><label class="collapse" for="c-37142740">[-]</label><label class="expand" for="c-37142740">[1 more]</label></div><br/><div class="children"><div class="content">I have been using nat.dev to compare quantized models and it works great.</div><br/></div></div><div id="37143582" class="c"><input type="checkbox" id="c-37143582" checked=""/><div class="controls bullet"><span class="by">dustymcp</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37140771">parent</a><span>|</span><a href="#37142740">prev</a><span>|</span><a href="#37142961">next</a><span>|</span><label class="collapse" for="c-37143582">[-]</label><label class="expand" for="c-37143582">[1 more]</label></div><br/><div class="children"><div class="content">it will be different for every usecase, the only way to find out is spinning one up..</div><br/></div></div><div id="37142961" class="c"><input type="checkbox" id="c-37142961" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37140771">parent</a><span>|</span><a href="#37143582">prev</a><span>|</span><a href="#37140951">next</a><span>|</span><label class="collapse" for="c-37142961">[-]</label><label class="expand" for="c-37142961">[1 more]</label></div><br/><div class="children"><div class="content">would an answer &quot;there aren&#x27;t much significant penalties&quot; suffice?</div><br/></div></div><div id="37140951" class="c"><input type="checkbox" id="c-37140951" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37140771">parent</a><span>|</span><a href="#37142961">prev</a><span>|</span><a href="#37143536">next</a><span>|</span><label class="collapse" for="c-37140951">[-]</label><label class="expand" for="c-37140951">[1 more]</label></div><br/><div class="children"><div class="content">The problem is that it&#x27;s not consistent enough for a good demo. Not even two different models, but even two different fine tunes of the same base model may be wildly differently affected by quantization. It can range from making hardly a difference to complete garbage output.</div><br/></div></div></div></div><div id="37143536" class="c"><input type="checkbox" id="c-37143536" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#37140409">parent</a><span>|</span><a href="#37140771">prev</a><span>|</span><a href="#37140967">next</a><span>|</span><label class="collapse" for="c-37143536">[-]</label><label class="expand" for="c-37143536">[2 more]</label></div><br/><div class="children"><div class="content">The effect is lesser than you think. 5 bit quantization has negligible performance loss compared to 16 bits: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1684">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1684</a></div><br/><div id="37143764" class="c"><input type="checkbox" id="c-37143764" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37143536">parent</a><span>|</span><a href="#37140967">next</a><span>|</span><label class="collapse" for="c-37143764">[-]</label><label class="expand" for="c-37143764">[1 more]</label></div><br/><div class="children"><div class="content">This paper from last month has a method for acceptable 3-bit quantization and a start at 2-bit.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.13304" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.13304</a></div><br/></div></div></div></div><div id="37140967" class="c"><input type="checkbox" id="c-37140967" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37140409">parent</a><span>|</span><a href="#37143536">prev</a><span>|</span><a href="#37144010">next</a><span>|</span><label class="collapse" for="c-37140967">[-]</label><label class="expand" for="c-37140967">[8 more]</label></div><br/><div class="children"><div class="content">&gt;Still, we are looking at GPT3 level of performance on one server rack. That says something when less than a year ago, such AI was literally magic and only run on a massive datacenter.<p>I&#x27;m not sure what you mean by this. You&#x27;ve always been able to run GPT3 on a single server (your typical 8xA100).</div><br/><div id="37142341" class="c"><input type="checkbox" id="c-37142341" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37140967">parent</a><span>|</span><a href="#37141056">next</a><span>|</span><label class="collapse" for="c-37142341">[-]</label><label class="expand" for="c-37142341">[1 more]</label></div><br/><div class="children"><div class="content">8xA100 is <i>technically</i> a single server, but I think OP is talking about affordable and plentiful CPU hosts, or even relatively modest single GPU instances.<p>DGX boxes do not grow on trees, especially these days</div><br/></div></div><div id="37141056" class="c"><input type="checkbox" id="c-37141056" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37140967">parent</a><span>|</span><a href="#37142341">prev</a><span>|</span><a href="#37144010">next</a><span>|</span><label class="collapse" for="c-37141056">[-]</label><label class="expand" for="c-37141056">[6 more]</label></div><br/><div class="children"><div class="content">Am I missing something or how do you know this? Also I think the OP was talking about a single card not multiple but that was just my reading.</div><br/><div id="37141096" class="c"><input type="checkbox" id="c-37141096" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37141056">parent</a><span>|</span><a href="#37144010">next</a><span>|</span><label class="collapse" for="c-37141096">[-]</label><label class="expand" for="c-37141096">[5 more]</label></div><br/><div class="children"><div class="content">Because 175B parameters (350GB for the weights FP16, let&#x27;s say a bit over 400GB  for actual inference), fit very comfortably on 8xA100 (640GB VRAM total).<p>And basically all servers will have 8xA100 (<i>maybe</i> 4xA100). Nobody bothers with a single A100 (of course in a VM you might have access to only one)</div><br/><div id="37141353" class="c"><input type="checkbox" id="c-37141353" checked=""/><div class="controls bullet"><span class="by">axiom92</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37141096">parent</a><span>|</span><a href="#37144010">next</a><span>|</span><label class="collapse" for="c-37141353">[-]</label><label class="expand" for="c-37141353">[4 more]</label></div><br/><div class="children"><div class="content">&gt; And basically all servers will have 8xA100<p>for those wondering: no this is not the norm. My lab at CMU doesn&#x27;t own any A100s  (we have A6000s).</div><br/><div id="37144443" class="c"><input type="checkbox" id="c-37144443" checked=""/><div class="controls bullet"><span class="by">_zoltan_</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37141353">parent</a><span>|</span><a href="#37141430">next</a><span>|</span><label class="collapse" for="c-37144443">[-]</label><label class="expand" for="c-37144443">[1 more]</label></div><br/><div class="children"><div class="content">who&#x27;s norm? I assure you it&#x27;s the norm. :)</div><br/></div></div><div id="37141430" class="c"><input type="checkbox" id="c-37141430" checked=""/><div class="controls bullet"><span class="by">doctorpangloss</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37141353">parent</a><span>|</span><a href="#37144443">prev</a><span>|</span><a href="#37144010">next</a><span>|</span><label class="collapse" for="c-37141430">[-]</label><label class="expand" for="c-37141430">[2 more]</label></div><br/><div class="children"><div class="content">The servers the commenter is talking about are DGX machines from NVIDIA.<p>It doesn’t really make sense to BTO. What you gain economically you lose in the science you can do.<p>But nobody could have anticipated this.</div><br/><div id="37144436" class="c"><input type="checkbox" id="c-37144436" checked=""/><div class="controls bullet"><span class="by">_zoltan_</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37141430">parent</a><span>|</span><a href="#37144010">next</a><span>|</span><label class="collapse" for="c-37144436">[-]</label><label class="expand" for="c-37144436">[1 more]</label></div><br/><div class="children"><div class="content">you could also get HGX from any of the vendors.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="37144010" class="c"><input type="checkbox" id="c-37144010" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#37140409">parent</a><span>|</span><a href="#37140967">prev</a><span>|</span><a href="#37143097">next</a><span>|</span><label class="collapse" for="c-37144010">[-]</label><label class="expand" for="c-37144010">[4 more]</label></div><br/><div class="children"><div class="content">This is not generally true, sometimes quantisation can improve accuracy. I haven&#x27;t seen that with LLMs yet though.</div><br/><div id="37144085" class="c"><input type="checkbox" id="c-37144085" checked=""/><div class="controls bullet"><span class="by">arijun</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37144010">parent</a><span>|</span><a href="#37143097">next</a><span>|</span><label class="collapse" for="c-37144085">[-]</label><label class="expand" for="c-37144085">[3 more]</label></div><br/><div class="children"><div class="content">Interesting, how would that work? Are there any well-known examples?<p>Is it: the weights all happen to be where float is sparse, so quantization ends up increasing fidelity? Or is it more of a “worse is better” dropout-type situation?</div><br/><div id="37144344" class="c"><input type="checkbox" id="c-37144344" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37144085">parent</a><span>|</span><a href="#37144311">next</a><span>|</span><label class="collapse" for="c-37144344">[-]</label><label class="expand" for="c-37144344">[1 more]</label></div><br/><div class="children"><div class="content">I suspect it works as regularisation of the network. It usually happens when you train with quantisation instead of post-training quantisation, an I haven&#x27;t seen that done with LLMs yet.</div><br/></div></div><div id="37144311" class="c"><input type="checkbox" id="c-37144311" checked=""/><div class="controls bullet"><span class="by">matsemann</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37144085">parent</a><span>|</span><a href="#37144344">prev</a><span>|</span><a href="#37143097">next</a><span>|</span><label class="collapse" for="c-37144311">[-]</label><label class="expand" for="c-37144311">[1 more]</label></div><br/><div class="children"><div class="content">For image recognition it can sometimes be like that. My gut feeling is that lowering from fp32 to fp16 can get rid of some kind of overfitting or so.</div><br/></div></div></div></div></div></div><div id="37143097" class="c"><input type="checkbox" id="c-37143097" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#37140409">parent</a><span>|</span><a href="#37144010">prev</a><span>|</span><a href="#37140710">next</a><span>|</span><label class="collapse" for="c-37143097">[-]</label><label class="expand" for="c-37143097">[1 more]</label></div><br/><div class="children"><div class="content">Good blog post, shame the site has no RSS feed!</div><br/></div></div><div id="37140710" class="c"><input type="checkbox" id="c-37140710" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#37140409">parent</a><span>|</span><a href="#37143097">prev</a><span>|</span><a href="#37140545">next</a><span>|</span><label class="collapse" for="c-37140710">[-]</label><label class="expand" for="c-37140710">[1 more]</label></div><br/><div class="children"><div class="content">Yes, there is a logarithmically-bound (or exponential if you&#x27;re viewing it from another angle) falloff in the information lost in quantization. This comes from the non-uniform &quot;value&quot; of different weights. We can try to get around them with different methods, but at the end of the day, some parameters just hurt more to squeeze.<p>What is insane though is how far we&#x27;ve taken it. I remember when INT8 from NVIDIA seemed like a nigh-pipedream!</div><br/></div></div><div id="37140545" class="c"><input type="checkbox" id="c-37140545" checked=""/><div class="controls bullet"><span class="by">ripvanwinkle</span><span>|</span><a href="#37140409">parent</a><span>|</span><a href="#37140710">prev</a><span>|</span><a href="#37143227">next</a><span>|</span><label class="collapse" for="c-37140545">[-]</label><label class="expand" for="c-37140545">[2 more]</label></div><br/><div class="children"><div class="content">Thank you! Is there a sweet spot with quantization. how much can you quantize for given model type and size and still be useful.</div><br/><div id="37140770" class="c"><input type="checkbox" id="c-37140770" checked=""/><div class="controls bullet"><span class="by">pseudonom-</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37140545">parent</a><span>|</span><a href="#37143227">next</a><span>|</span><label class="collapse" for="c-37140770">[-]</label><label class="expand" for="c-37140770">[1 more]</label></div><br/><div class="children"><div class="content">Tim Dettmers recently (<a href="https:&#x2F;&#x2F;www.manifold1.com&#x2F;episodes&#x2F;ai-on-your-phone-tim-dettmers-on-quantization-of-neural-networks-41&#x2F;transcript" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.manifold1.com&#x2F;episodes&#x2F;ai-on-your-phone-tim-dett...</a>):<p>&quot;But what we found with these neural networks is, if you use 32 bits, they&#x27;re just fine. And then you use 16 bits, and they&#x27;re just fine. And then with eight bits, you need to use a couple of tricks and then it&#x27;s just fine.<p>And now we find if you can go to four bits, and for some networks, that&#x27;s much easier. For some networks, it&#x27;s much more difficult, but then you need a couple more tricks. And so it seems they&#x27;re much more robust.&quot;</div><br/></div></div></div></div><div id="37143227" class="c"><input type="checkbox" id="c-37143227" checked=""/><div class="controls bullet"><span class="by">prvc</span><span>|</span><a href="#37140409">parent</a><span>|</span><a href="#37140545">prev</a><span>|</span><a href="#37142084">next</a><span>|</span><label class="collapse" for="c-37143227">[-]</label><label class="expand" for="c-37143227">[2 more]</label></div><br/><div class="children"><div class="content">Any use case for using the 7B model over the 13B, quantized?</div><br/><div id="37143874" class="c"><input type="checkbox" id="c-37143874" checked=""/><div class="controls bullet"><span class="by">clarionbell</span><span>|</span><a href="#37140409">root</a><span>|</span><a href="#37143227">parent</a><span>|</span><a href="#37142084">next</a><span>|</span><label class="collapse" for="c-37143874">[-]</label><label class="expand" for="c-37143874">[1 more]</label></div><br/><div class="children"><div class="content">SBC</div><br/></div></div></div></div></div></div><div id="37142084" class="c"><input type="checkbox" id="c-37142084" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37140409">prev</a><span>|</span><a href="#37140460">next</a><span>|</span><label class="collapse" for="c-37142084">[-]</label><label class="expand" for="c-37142084">[6 more]</label></div><br/><div class="children"><div class="content">This leaves a ton of stuff out.<p>- Token generation is serial and bandwidth bound, but <i>prompt ingestion</i> is not and runs in batches of 512+. Short tests are fast on pure CPU llama.cpp, but long prompting (such as with ongoing conversation) is extremely slow compared to other backends.<p>- Llama.cpp now has very good ~4 bit quantization that doesn&#x27;t affect perplexity much. Q6_K almost has the same perplexity as FP16, but is still massively smaller.<p>- Batching is a <i>big</i> thing to ignore outside of personal deployments.<p>- The <i>real</i> magic of llama.cpp is model splitting. A small discrete GPU can completely offload prompt ingestion and part of the model inference. And it doesn&#x27;t have to be an Nvidia GPU! There is no other backend that will do that so efficiently in the generative AI space.<p>- Hence the GPU backends (OpenCL, Metal, CUDA, soon ROCm and Vulkan) are the defacto way to run llama.cpp these days. Without them, I couldn&#x27;t even run 70B on my desktop, or 33B on my (16GB RAM) laptop.</div><br/><div id="37144198" class="c"><input type="checkbox" id="c-37144198" checked=""/><div class="controls bullet"><span class="by">PoignardAzur</span><span>|</span><a href="#37142084">parent</a><span>|</span><a href="#37143994">prev</a><span>|</span><a href="#37144107">next</a><span>|</span><label class="collapse" for="c-37144198">[-]</label><label class="expand" for="c-37144198">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s prompt ingestion?</div><br/></div></div><div id="37144107" class="c"><input type="checkbox" id="c-37144107" checked=""/><div class="controls bullet"><span class="by">ant6n</span><span>|</span><a href="#37142084">parent</a><span>|</span><a href="#37144198">prev</a><span>|</span><a href="#37140460">next</a><span>|</span><label class="collapse" for="c-37144107">[-]</label><label class="expand" for="c-37144107">[2 more]</label></div><br/><div class="children"><div class="content">What’s perplexity?</div><br/><div id="37144416" class="c"><input type="checkbox" id="c-37144416" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#37142084">root</a><span>|</span><a href="#37144107">parent</a><span>|</span><a href="#37140460">next</a><span>|</span><label class="collapse" for="c-37144416">[-]</label><label class="expand" for="c-37144416">[1 more]</label></div><br/><div class="children"><div class="content">Perplexity is a measure of how certain the model is of the next token.  It&#x27;s calculated by looking at the probabilities that the model calculates for the next token in a stream.  If there are several choices for the next token with similar probabilities, that&#x27;s telling you that the model is having a hard time telling what the right answer should be: the model is more perplexed, perplexity is higher.  If there&#x27;s a single option with a much higher implied probability than any other, that means the model is more certain, and perplexity is lower.<p>Note that this has nothing to do with whether the answer is objectively correct.  It&#x27;s just measuring how confident it is.</div><br/></div></div></div></div></div></div><div id="37140460" class="c"><input type="checkbox" id="c-37140460" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37142084">prev</a><span>|</span><a href="#37143280">next</a><span>|</span><label class="collapse" for="c-37140460">[-]</label><label class="expand" for="c-37140460">[6 more]</label></div><br/><div class="children"><div class="content">This project&#x27;s been a blast to work with. While it&#x27;s written in C++, it provides a C interface to compile against which makes it especially easy to extend with Go, Python and other runtimes.<p>A few folks and I have been building a tool with it in Go for pulling &amp; running multiple models, and serving them on a REST API: <a href="https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama">https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama</a><p>In similar light, you haven&#x27;t checked it out, llama.cpp also has a pretty extensive &quot;server&quot; tool (in its <i>examples</i> directory in the repo) with a web ui and support for grammar (e.g. forcing the output to be JSON)</div><br/><div id="37144355" class="c"><input type="checkbox" id="c-37144355" checked=""/><div class="controls bullet"><span class="by">lovelyviking</span><span>|</span><a href="#37140460">parent</a><span>|</span><a href="#37140963">next</a><span>|</span><label class="collapse" for="c-37144355">[-]</label><label class="expand" for="c-37144355">[1 more]</label></div><br/><div class="children"><div class="content">&gt; A few folks and I have been building a tool with it in Go for pulling &amp; running multiple models, and serving them on a REST API: <a href="https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama">https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama</a><p>Why this tool was created? What was the reason for creating such tool? Why it was named this way?</div><br/></div></div><div id="37140963" class="c"><input type="checkbox" id="c-37140963" checked=""/><div class="controls bullet"><span class="by">jay-barronville</span><span>|</span><a href="#37140460">parent</a><span>|</span><a href="#37144355">prev</a><span>|</span><a href="#37141294">next</a><span>|</span><label class="collapse" for="c-37140963">[-]</label><label class="expand" for="c-37140963">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for the link to your project! I&#x27;ll be playing around with it.</div><br/></div></div><div id="37141294" class="c"><input type="checkbox" id="c-37141294" checked=""/><div class="controls bullet"><span class="by">jcims</span><span>|</span><a href="#37140460">parent</a><span>|</span><a href="#37140963">prev</a><span>|</span><a href="#37141788">next</a><span>|</span><label class="collapse" for="c-37141294">[-]</label><label class="expand" for="c-37141294">[1 more]</label></div><br/><div class="children"><div class="content">Just wanted to say thanks homie for making something so hairy so accessible.  Love it!!!</div><br/></div></div><div id="37141788" class="c"><input type="checkbox" id="c-37141788" checked=""/><div class="controls bullet"><span class="by">wfurney</span><span>|</span><a href="#37140460">parent</a><span>|</span><a href="#37141294">prev</a><span>|</span><a href="#37143280">next</a><span>|</span><label class="collapse" for="c-37141788">[-]</label><label class="expand" for="c-37141788">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve also been experimenting with the C# implementation<p><a href="https:&#x2F;&#x2F;github.com&#x2F;trrahul&#x2F;llama2.cs">https:&#x2F;&#x2F;github.com&#x2F;trrahul&#x2F;llama2.cs</a></div><br/><div id="37143518" class="c"><input type="checkbox" id="c-37143518" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#37140460">root</a><span>|</span><a href="#37141788">parent</a><span>|</span><a href="#37143280">next</a><span>|</span><label class="collapse" for="c-37143518">[-]</label><label class="expand" for="c-37143518">[1 more]</label></div><br/><div class="children"><div class="content">Why Parallel.For() instead of SIMD types?<p>I would expect a much better performance.</div><br/></div></div></div></div></div></div><div id="37143280" class="c"><input type="checkbox" id="c-37143280" checked=""/><div class="controls bullet"><span class="by">Roark66</span><span>|</span><a href="#37140460">prev</a><span>|</span><a href="#37140519">next</a><span>|</span><label class="collapse" for="c-37143280">[-]</label><label class="expand" for="c-37143280">[2 more]</label></div><br/><div class="children"><div class="content">It is useful to mention running inference on modern cpus that have AVX2 is not that bad. Sure it is slower than on the gpu, but you get the benefit of having a single long continuous region of ram.<p>But there is one huge problem why this is not that popular on x86_64. Having to run in fp32. As far as I know our most common ml libraries (pytorch, tf, onnx etc) do not have an option to quantize to 4 bits and they don&#x27;t have an option to run inference at anything other than fp32 on the x86_64 cpus.<p>It is a huge shame. There is openvino which supports int8, but if you can&#x27;t easily quantize large models without a gpu, what use is it? (For small models I suppose).<p>So if anyone figured out a way to quantize a transformer model to 4&#x2F;8 bit and run it on the x86_64 cpu platform I&#x27;m very interested in hearing about it.</div><br/><div id="37143956" class="c"><input type="checkbox" id="c-37143956" checked=""/><div class="controls bullet"><span class="by">_nalply</span><span>|</span><a href="#37143280">parent</a><span>|</span><a href="#37140519">next</a><span>|</span><label class="collapse" for="c-37143956">[-]</label><label class="expand" for="c-37143956">[1 more]</label></div><br/><div class="children"><div class="content">Subjective experience: AVX512 helps a lot. I would have liked to read more about this. It seems that AVX512 supports fp16 in hardware and allows 32 fused multiplication-add operations per core. So I imagine on a Ryzen 9 with 12 cores you can have 384 simultaneous fused multiplication-add operations. I am not sure whether my estimation is off. Anyone know more than me?</div><br/></div></div></div></div><div id="37140519" class="c"><input type="checkbox" id="c-37140519" checked=""/><div class="controls bullet"><span class="by">cameron_b</span><span>|</span><a href="#37143280">prev</a><span>|</span><a href="#37140752">next</a><span>|</span><label class="collapse" for="c-37140519">[-]</label><label class="expand" for="c-37140519">[6 more]</label></div><br/><div class="children"><div class="content">I’ve been working through that repo and managed the 13B dataset on a single Pi4 8gig<p>I’ve also replicated the work in OpenMPI ( from a thread on the llama.cpp GitHub repo ) and today I managed to get the 65B dataset operational on three pi4 nodes.<p>I’m not saying this as any achievement of mine, but as a comment on the current reality of reproducible LLM At home on anything you’ve got.<p>It really feels like this technique has arrived.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;cameronbunce&#x2F;ClusterConfig">https:&#x2F;&#x2F;github.com&#x2F;cameronbunce&#x2F;ClusterConfig</a></div><br/><div id="37142168" class="c"><input type="checkbox" id="c-37142168" checked=""/><div class="controls bullet"><span class="by">loxias</span><span>|</span><a href="#37140519">parent</a><span>|</span><a href="#37140711">next</a><span>|</span><label class="collapse" for="c-37142168">[-]</label><label class="expand" for="c-37142168">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I’ve also replicated the work in OpenMPI...<p>Oh cool!  How did it perform?<p>I wonder if this would be an exciting test for Amazon&#x27;s SRD protocol which appears to be built for HPC.  I&#x27;m looking for an excuse to play with it...</div><br/></div></div><div id="37140711" class="c"><input type="checkbox" id="c-37140711" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#37140519">parent</a><span>|</span><a href="#37142168">prev</a><span>|</span><a href="#37140752">next</a><span>|</span><label class="collapse" for="c-37140711">[-]</label><label class="expand" for="c-37140711">[4 more]</label></div><br/><div class="children"><div class="content">How many tokens a second?</div><br/><div id="37141066" class="c"><input type="checkbox" id="c-37141066" checked=""/><div class="controls bullet"><span class="by">cameron_b</span><span>|</span><a href="#37140519">root</a><span>|</span><a href="#37140711">parent</a><span>|</span><a href="#37140752">next</a><span>|</span><label class="collapse" for="c-37141066">[-]</label><label class="expand" for="c-37141066">[3 more]</label></div><br/><div class="children"><div class="content">The other way around is whole number math. I added the 3-node output from the 13B model to github, the timings are below. The 3-node 65B job hasn&#x27;t finished yet.<p>llama_print_timings:        load time = 17766.29 ms
llama_print_timings:      sample time =   264.42 ms &#x2F;   128 runs   (    2.07 ms per token,   484.07 tokens per second)
llama_print_timings: prompt eval time = 10146.71 ms &#x2F;     8 tokens ( 1268.34 ms per token,     0.79 tokens per second)
llama_print_timings:        eval time = 287157.12 ms &#x2F;   127 runs   ( 2261.08 ms per token,     0.44 tokens per second)
llama_print_timings:       total time = 297598.22 ms</div><br/><div id="37141175" class="c"><input type="checkbox" id="c-37141175" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#37140519">root</a><span>|</span><a href="#37141066">parent</a><span>|</span><a href="#37140752">next</a><span>|</span><label class="collapse" for="c-37141175">[-]</label><label class="expand" for="c-37141175">[2 more]</label></div><br/><div class="children"><div class="content">This is very interesting and actually in the usable realm, for some use cases</div><br/><div id="37141263" class="c"><input type="checkbox" id="c-37141263" checked=""/><div class="controls bullet"><span class="by">cameron_b</span><span>|</span><a href="#37140519">root</a><span>|</span><a href="#37141175">parent</a><span>|</span><a href="#37140752">next</a><span>|</span><label class="collapse" for="c-37141263">[-]</label><label class="expand" for="c-37141263">[1 more]</label></div><br/><div class="children"><div class="content">My networking setup is not optimal, but it was quite surprising how easy it was to get it all to work.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37140752" class="c"><input type="checkbox" id="c-37140752" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#37140519">prev</a><span>|</span><a href="#37140790">next</a><span>|</span><label class="collapse" for="c-37140752">[-]</label><label class="expand" for="c-37140752">[1 more]</label></div><br/><div class="children"><div class="content">In my best estimation, Finbarr makes pretty great content, he and I have had a number of positive interactions on Twitter. I tend to have a pretty grumpy disposition towards a lot of modern ML and such as I feel it&#x27;s shovelware, but whenever Finbarr puts out work, I tend to set aside some time to give it a good gander, as I feel like it&#x27;s generally pretty &quot;meaty&quot; (which I honestly find pretty hard to do past a certain pace). Well worth the subscribe if you have not done so already (I&#x27;m not affiliated with him, I just really like his work!).</div><br/></div></div><div id="37140790" class="c"><input type="checkbox" id="c-37140790" checked=""/><div class="controls bullet"><span class="by">gautamcgoel</span><span>|</span><a href="#37140752">prev</a><span>|</span><a href="#37143650">next</a><span>|</span><label class="collapse" for="c-37140790">[-]</label><label class="expand" for="c-37140790">[4 more]</label></div><br/><div class="children"><div class="content">I enjoyed this article, but it seems to me that the latency numbers should have units of nanoseconds or maybe CPU cycles. I feel like the article was a bit sloppy with units.<p>Another question that occurs to me is: why do chipmakers even bother putting so many functional units on the chip if almost all workloads are memory bound? Based of the calculations in this article, you could decrease the number of teraflops a modern GPU can perform by a factor of 2 and not notice any appreciable difference in ML performance.</div><br/><div id="37140899" class="c"><input type="checkbox" id="c-37140899" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#37140790">parent</a><span>|</span><a href="#37140891">next</a><span>|</span><label class="collapse" for="c-37140899">[-]</label><label class="expand" for="c-37140899">[2 more]</label></div><br/><div class="children"><div class="content">1. I think nanosecond-scale latency numbers on operations taking dozens to hundreds of ms are probably overkill?<p>2. Inference is only one aspect of what GPUs are used for. Many other workloads are compute-bound. That being said, given the recent rise of these kinds of open-source, pre-trained large language models, I wouldn&#x27;t be surprised if future Nvidia product launches offered variants with significantly more VRAM. There would probably be a lot of interest in &quot;3080-equivalent compute, but 48GB VRAM&quot; these days — certainly I would take one over a 4090 with 24GB VRAM. (Then again, that&#x27;s basically an A6000, and those go for nearly $7k...)</div><br/><div id="37141309" class="c"><input type="checkbox" id="c-37141309" checked=""/><div class="controls bullet"><span class="by">sbierwagen</span><span>|</span><a href="#37140790">root</a><span>|</span><a href="#37140899">parent</a><span>|</span><a href="#37140891">next</a><span>|</span><label class="collapse" for="c-37141309">[-]</label><label class="expand" for="c-37141309">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, Nvidia won&#x27;t do big VRAM consumer cards until AMD forces them to. They&#x27;re running flat out just trying to keep up with demand for H100s at forty thousand USD each.</div><br/></div></div></div></div><div id="37140891" class="c"><input type="checkbox" id="c-37140891" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#37140790">parent</a><span>|</span><a href="#37140899">prev</a><span>|</span><a href="#37143650">next</a><span>|</span><label class="collapse" for="c-37140891">[-]</label><label class="expand" for="c-37140891">[1 more]</label></div><br/><div class="children"><div class="content">The primary use case for GPUs in devices, graphics, is not as often memory bound. Even for other general data compute purposes that may not always be the case. It&#x27;s specifically neural nets that have extremely wide batches of extremely simple operations occurring across extremely large chunks of memory where being memory bound is the default case.</div><br/></div></div></div></div><div id="37143650" class="c"><input type="checkbox" id="c-37143650" checked=""/><div class="controls bullet"><span class="by">aargh_aargh</span><span>|</span><a href="#37140790">prev</a><span>|</span><a href="#37142593">next</a><span>|</span><label class="collapse" for="c-37143650">[-]</label><label class="expand" for="c-37143650">[1 more]</label></div><br/><div class="children"><div class="content">Originally posted here:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35192102">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35192102</a></div><br/></div></div><div id="37142593" class="c"><input type="checkbox" id="c-37142593" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#37143650">prev</a><span>|</span><a href="#37140845">next</a><span>|</span><label class="collapse" for="c-37142593">[-]</label><label class="expand" for="c-37142593">[2 more]</label></div><br/><div class="children"><div class="content">Memory bound token generation is a limitation of transformer decoders.<p>In the past, hardware has motivation algorithm innovations.<p>I’m curious how long it will take until we see more hardware friendly models.</div><br/><div id="37144169" class="c"><input type="checkbox" id="c-37144169" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#37142593">parent</a><span>|</span><a href="#37140845">next</a><span>|</span><label class="collapse" for="c-37144169">[-]</label><label class="expand" for="c-37144169">[1 more]</label></div><br/><div class="children"><div class="content">The Rwkv family of models qualifies, since it computes like a recurrent network at inference time.</div><br/></div></div></div></div><div id="37140845" class="c"><input type="checkbox" id="c-37140845" checked=""/><div class="controls bullet"><span class="by">oars</span><span>|</span><a href="#37142593">prev</a><span>|</span><a href="#37141308">next</a><span>|</span><label class="collapse" for="c-37140845">[-]</label><label class="expand" for="c-37140845">[7 more]</label></div><br/><div class="children"><div class="content">Great article. Don&#x27;t see content like this anywhere else outside of HN.</div><br/><div id="37141069" class="c"><input type="checkbox" id="c-37141069" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37140845">parent</a><span>|</span><a href="#37141308">next</a><span>|</span><label class="collapse" for="c-37141069">[-]</label><label class="expand" for="c-37141069">[6 more]</label></div><br/><div class="children"><div class="content">You can find content like this on Twitter if you follow the right people. In fact I read this article before it was even posted here because @karpathy tweeted about it.</div><br/><div id="37141462" class="c"><input type="checkbox" id="c-37141462" checked=""/><div class="controls bullet"><span class="by">notuseful</span><span>|</span><a href="#37140845">root</a><span>|</span><a href="#37141069">parent</a><span>|</span><a href="#37141308">next</a><span>|</span><label class="collapse" for="c-37141462">[-]</label><label class="expand" for="c-37141462">[5 more]</label></div><br/><div class="children"><div class="content">Twitter no longer exists. It&#x27;s called X now and it&#x27;s a little shameful to continue to use it.</div><br/><div id="37143109" class="c"><input type="checkbox" id="c-37143109" checked=""/><div class="controls bullet"><span class="by">DiggyJohnson</span><span>|</span><a href="#37140845">root</a><span>|</span><a href="#37141462">parent</a><span>|</span><a href="#37141883">next</a><span>|</span><label class="collapse" for="c-37143109">[-]</label><label class="expand" for="c-37143109">[2 more]</label></div><br/><div class="children"><div class="content">I’ve never been a regular Twitter user, and don&#x27;t really enjoy the platform, but this comment of yours is either abusing the word “shameful” or betraying a major lack of understanding that you can’t expect other people to care deeply about the things you care deeply about.<p>There’s a lot of shameful things in this world, GP using twitter isn’t one of them. Not even a “little.”</div><br/><div id="37143508" class="c"><input type="checkbox" id="c-37143508" checked=""/><div class="controls bullet"><span class="by">class4behavior</span><span>|</span><a href="#37140845">root</a><span>|</span><a href="#37143109">parent</a><span>|</span><a href="#37141883">next</a><span>|</span><label class="collapse" for="c-37143508">[-]</label><label class="expand" for="c-37143508">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s betraying the word shameful in that it&#x27;s an utter understatement.<p>If people don&#x27;t care about supporting companies that enable and spread far-right content and groups, it&#x27;s they who are a problem.<p>To say nothing of the pure disregard of the human right to privacy (and with AI now IP as well) that is forced on the the rest of the world by the dominance of the US market.</div><br/></div></div></div></div><div id="37141883" class="c"><input type="checkbox" id="c-37141883" checked=""/><div class="controls bullet"><span class="by">Joe_Boogz</span><span>|</span><a href="#37140845">root</a><span>|</span><a href="#37141462">parent</a><span>|</span><a href="#37143109">prev</a><span>|</span><a href="#37141935">next</a><span>|</span><label class="collapse" for="c-37141883">[-]</label><label class="expand" for="c-37141883">[1 more]</label></div><br/><div class="children"><div class="content">Shameful?  Just because you dislike it doesn’t mean everyone has to.</div><br/></div></div><div id="37141935" class="c"><input type="checkbox" id="c-37141935" checked=""/><div class="controls bullet"><span class="by">PKop</span><span>|</span><a href="#37140845">root</a><span>|</span><a href="#37141462">parent</a><span>|</span><a href="#37141883">prev</a><span>|</span><a href="#37141308">next</a><span>|</span><label class="collapse" for="c-37141935">[-]</label><label class="expand" for="c-37141935">[1 more]</label></div><br/><div class="children"><div class="content">Says you. I&#x27;d prefer to take advantage of the resources available there. Your silly moralism is easy to ignore</div><br/></div></div></div></div></div></div></div></div><div id="37141308" class="c"><input type="checkbox" id="c-37141308" checked=""/><div class="controls bullet"><span class="by">pedrovhb</span><span>|</span><a href="#37140845">prev</a><span>|</span><a href="#37141354">next</a><span>|</span><label class="collapse" for="c-37141308">[-]</label><label class="expand" for="c-37141308">[2 more]</label></div><br/><div class="children"><div class="content">Given the massive imbalance in the memory bandwidth bottleneck, I wonder why specialized hardware is the way it is. Is there some use case in which processing is the bottleneck, or at least it&#x27;s more even? Are we expecting some software paradigm shift which will change the balance? Why couldn&#x27;t they just make a cheaper, more rounded card which isn&#x27;t heavily underutilized because of a large bottleneck?</div><br/><div id="37142385" class="c"><input type="checkbox" id="c-37142385" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37141308">parent</a><span>|</span><a href="#37141354">next</a><span>|</span><label class="collapse" for="c-37142385">[-]</label><label class="expand" for="c-37142385">[1 more]</label></div><br/><div class="children"><div class="content">See: <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Random-access_memory#Memory_wall" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Random-access_memory#Memory_...</a><p>But llama.cpp, and llms in general, are very atypically memory bound, even for AI workloads. Other models&#x2F;backends will typically make more use of compute and cache.<p>For llms specifically, cloud hosts will batch requests to make better use of GPUs.<p>But you are not far off. There is a proposition to pipe chips with very fast memory (aka no external memory at all) together: <a href="https:&#x2F;&#x2F;www.nextplatform.com&#x2F;2023&#x2F;07&#x2F;12&#x2F;microsofts-chiplet-cloud-to-bring-the-cost-of-llms-way-down&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nextplatform.com&#x2F;2023&#x2F;07&#x2F;12&#x2F;microsofts-chiplet-c...</a></div><br/></div></div></div></div><div id="37141354" class="c"><input type="checkbox" id="c-37141354" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#37141308">prev</a><span>|</span><a href="#37144095">next</a><span>|</span><label class="collapse" for="c-37141354">[-]</label><label class="expand" for="c-37141354">[9 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; Memory bandwidth is the limiting factor in almost everything to do with sampling from transformers.<p>So how about using an APU - a CPU with GPU built in.  The GPU shares the CPU memory, so if you want you can have 128GB RAM and allocate 100GB to the GPU.<p>Sure the GPU i not fast, but if memory is important.....</div><br/><div id="37142237" class="c"><input type="checkbox" id="c-37142237" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37141354">parent</a><span>|</span><a href="#37142591">next</a><span>|</span><label class="collapse" for="c-37142237">[-]</label><label class="expand" for="c-37142237">[1 more]</label></div><br/><div class="children"><div class="content">You can, right now, with the OpenCL backend.<p>And for the moment, its slower than pure CPU. Optimizing for IGPs is not trivial.<p>MLC&#x27;s Vulkan backend is actually quite good on my AMD APU, but unfortunately it won&#x27;t split the model to a dGPU.</div><br/></div></div><div id="37142591" class="c"><input type="checkbox" id="c-37142591" checked=""/><div class="controls bullet"><span class="by">ladberg</span><span>|</span><a href="#37141354">parent</a><span>|</span><a href="#37142237">prev</a><span>|</span><a href="#37141383">next</a><span>|</span><label class="collapse" for="c-37142591">[-]</label><label class="expand" for="c-37142591">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re thinking about the wrong bandwidth here. The article is talking about going from the GPU&#x27;s RAM &lt;-&gt; GPU cores (i.e. through load&#x2F;store instructions in a cuda kernel), not from CPU&#x27;s RAM &lt;-&gt; GPU&#x27;s RAM. That kind of bandwidth is still important but usually not the bottleneck on most ML workloads.</div><br/><div id="37142736" class="c"><input type="checkbox" id="c-37142736" checked=""/><div class="controls bullet"><span class="by">thrdbndndn</span><span>|</span><a href="#37141354">root</a><span>|</span><a href="#37142591">parent</a><span>|</span><a href="#37141383">next</a><span>|</span><label class="collapse" for="c-37142736">[-]</label><label class="expand" for="c-37142736">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m still confused.<p>The author (correctly) made a distinction about the two like you said, but at the end when talking about Raspberry Pi 4 they use a number (~4GB&#x2F;s of memory bandwidth) from an article [1] which I can only assume is NOT about graphics memory or its bandwidth (do Raspberry PIs even have it?).<p>And how exactly is the bandwidth counted if I use integrated GPU (like i5 13600K)? Or pure CPU?<p>[1] <a href="https:&#x2F;&#x2F;forums.raspberrypi.com&#x2F;viewtopic.php?t=281183" rel="nofollow noreferrer">https:&#x2F;&#x2F;forums.raspberrypi.com&#x2F;viewtopic.php?t=281183</a></div><br/></div></div></div></div><div id="37141383" class="c"><input type="checkbox" id="c-37141383" checked=""/><div class="controls bullet"><span class="by">BobbyJo</span><span>|</span><a href="#37141354">parent</a><span>|</span><a href="#37142591">prev</a><span>|</span><a href="#37141786">next</a><span>|</span><label class="collapse" for="c-37141383">[-]</label><label class="expand" for="c-37141383">[4 more]</label></div><br/><div class="children"><div class="content">Most CPU RAM is much slower than GPU RAM. GPUs typically pack RAM 2 generations ahead with a wider bus than anything you&#x27;d find on a consumer motherboard.</div><br/><div id="37141414" class="c"><input type="checkbox" id="c-37141414" checked=""/><div class="controls bullet"><span class="by">winwang</span><span>|</span><a href="#37141354">root</a><span>|</span><a href="#37141383">parent</a><span>|</span><a href="#37141722">next</a><span>|</span><label class="collapse" for="c-37141414">[-]</label><label class="expand" for="c-37141414">[2 more]</label></div><br/><div class="children"><div class="content">For reference, DDR4-3200 in quad channel is ~100 GB&#x2F;s while a 3090&#x27;s VRAM is 960 GB&#x2F;s. Of course, most consumers only have dual channel.<p>M1 Pro is 200 and M1 Max is 400. Which is slow for GPU memory, but incredible for main memory -- although I&#x27;m not sure how much of that a single core can actually pull.</div><br/><div id="37141601" class="c"><input type="checkbox" id="c-37141601" checked=""/><div class="controls bullet"><span class="by">KolenCh</span><span>|</span><a href="#37141354">root</a><span>|</span><a href="#37141414">parent</a><span>|</span><a href="#37141722">next</a><span>|</span><label class="collapse" for="c-37141601">[-]</label><label class="expand" for="c-37141601">[1 more]</label></div><br/><div class="children"><div class="content">The Anand tech article has profiled this. IIRC the CPU cores has access to half that band width only, which is still quite a lot.</div><br/></div></div></div></div><div id="37141722" class="c"><input type="checkbox" id="c-37141722" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#37141354">root</a><span>|</span><a href="#37141383">parent</a><span>|</span><a href="#37141414">prev</a><span>|</span><a href="#37141786">next</a><span>|</span><label class="collapse" for="c-37141722">[-]</label><label class="expand" for="c-37141722">[1 more]</label></div><br/><div class="children"><div class="content">The AMD Ryzen™ 9 7940HS uses DDR5-5600, which I understand to be about 89.6GB&#x2F;s in a dual channel setup.</div><br/></div></div></div></div><div id="37141786" class="c"><input type="checkbox" id="c-37141786" checked=""/><div class="controls bullet"><span class="by">mhh__</span><span>|</span><a href="#37141354">parent</a><span>|</span><a href="#37141383">prev</a><span>|</span><a href="#37144095">next</a><span>|</span><label class="collapse" for="c-37141786">[-]</label><label class="expand" for="c-37141786">[1 more]</label></div><br/><div class="children"><div class="content">This is basically the appeal of the apple chips in this domain. Apple have fuck-you money so they have a bunch of high-bandwidth decent-latency soldered onto the chip.</div><br/></div></div></div></div><div id="37144095" class="c"><input type="checkbox" id="c-37144095" checked=""/><div class="controls bullet"><span class="by">ant6n</span><span>|</span><a href="#37141354">prev</a><span>|</span><a href="#37141490">next</a><span>|</span><label class="collapse" for="c-37144095">[-]</label><label class="expand" for="c-37144095">[1 more]</label></div><br/><div class="children"><div class="content">This article would probably be useful for a lot more people if it spent just a couple of sentences introducing the various parameters, rather than just throwing variable names at the reader. Interestingly, a whole paragraph is spent on explaining n_bytes.</div><br/></div></div><div id="37141490" class="c"><input type="checkbox" id="c-37141490" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37144095">prev</a><span>|</span><a href="#37140225">next</a><span>|</span><label class="collapse" for="c-37141490">[-]</label><label class="expand" for="c-37141490">[1 more]</label></div><br/><div class="children"><div class="content">80&#x2F;20 rule. Approximations have smaller, less accurate approximates which do in many contexts. If your goal is to reach America, a crude compass and speed reasoning works. If you want to target an ICBM you need better positional accuracy.</div><br/></div></div><div id="37140225" class="c"><input type="checkbox" id="c-37140225" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#37141490">prev</a><span>|</span><a href="#37140278">next</a><span>|</span><label class="collapse" for="c-37140225">[-]</label><label class="expand" for="c-37140225">[60 more]</label></div><br/><div class="children"><div class="content">What I find more stunning is what this implies going forward. If tech advances as it tends to do then having a 200bn model fit into consumer hardware isn&#x27;t that far away.<p>Might not be AGI but I think cliched as it is that would &quot;change everything&quot;. If not at 200 then 400 or whatever. Doesn&#x27;t matter - the direction of travel seems certain.</div><br/><div id="37140237" class="c"><input type="checkbox" id="c-37140237" checked=""/><div class="controls bullet"><span class="by">gct</span><span>|</span><a href="#37140225">parent</a><span>|</span><a href="#37140667">next</a><span>|</span><label class="collapse" for="c-37140237">[-]</label><label class="expand" for="c-37140237">[51 more]</label></div><br/><div class="children"><div class="content">Basically Ray Kurzweil&#x27;s argument, he&#x27;s been saying $1000 worth of compute will be able to match human performance around 2029 for decades now.</div><br/><div id="37140506" class="c"><input type="checkbox" id="c-37140506" checked=""/><div class="controls bullet"><span class="by">KerrAvon</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140237">parent</a><span>|</span><a href="#37140546">next</a><span>|</span><label class="collapse" for="c-37140506">[-]</label><label class="expand" for="c-37140506">[25 more]</label></div><br/><div class="children"><div class="content">First, there has to be something capable of matching human performance at a much higher cost. This is still just spicy autocomplete.</div><br/><div id="37140559" class="c"><input type="checkbox" id="c-37140559" checked=""/><div class="controls bullet"><span class="by">Waterluvian</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140506">parent</a><span>|</span><a href="#37140546">next</a><span>|</span><label class="collapse" for="c-37140559">[-]</label><label class="expand" for="c-37140559">[24 more]</label></div><br/><div class="children"><div class="content">Humans just do spicy autocomplete too.</div><br/><div id="37140631" class="c"><input type="checkbox" id="c-37140631" checked=""/><div class="controls bullet"><span class="by">not2b</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140559">parent</a><span>|</span><a href="#37140921">next</a><span>|</span><label class="collapse" for="c-37140631">[-]</label><label class="expand" for="c-37140631">[12 more]</label></div><br/><div class="children"><div class="content">No, a human isn&#x27;t born with a set of knowledge like a freshly trained LLM, keeping the model fixed and responding to input. The analog to the model changes based on the human&#x27;s experience. Just making bigger and bigger LLMs won&#x27;t give you this.</div><br/><div id="37141911" class="c"><input type="checkbox" id="c-37141911" checked=""/><div class="controls bullet"><span class="by">mlboss</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140631">parent</a><span>|</span><a href="#37140781">next</a><span>|</span><label class="collapse" for="c-37141911">[-]</label><label class="expand" for="c-37141911">[1 more]</label></div><br/><div class="children"><div class="content">Humans are born with millions of years of evolutionary training embedded in their dna and brain. We are not born with nothing.</div><br/></div></div><div id="37140781" class="c"><input type="checkbox" id="c-37140781" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140631">parent</a><span>|</span><a href="#37141911">prev</a><span>|</span><a href="#37140650">next</a><span>|</span><label class="collapse" for="c-37140781">[-]</label><label class="expand" for="c-37140781">[2 more]</label></div><br/><div class="children"><div class="content">Ah, but I believe you forget the implicit biases of genetic programming. Instincts in my experience are the skeleton, and in a sense the default basis functions for the structure of how we live, see, do, and learn.</div><br/><div id="37141157" class="c"><input type="checkbox" id="c-37141157" checked=""/><div class="controls bullet"><span class="by">not2b</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140781">parent</a><span>|</span><a href="#37140650">next</a><span>|</span><label class="collapse" for="c-37141157">[-]</label><label class="expand" for="c-37141157">[1 more]</label></div><br/><div class="children"><div class="content">No, I don&#x27;t forget that. There&#x27;s obviously a starting point, behaviors and abilities that newborns already have. The point is that the model is not static.</div><br/></div></div></div></div><div id="37140650" class="c"><input type="checkbox" id="c-37140650" checked=""/><div class="controls bullet"><span class="by">Waterluvian</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140631">parent</a><span>|</span><a href="#37140781">prev</a><span>|</span><a href="#37140921">next</a><span>|</span><label class="collapse" for="c-37140650">[-]</label><label class="expand" for="c-37140650">[8 more]</label></div><br/><div class="children"><div class="content">So a human is different because it keeps training its neural network?</div><br/><div id="37140765" class="c"><input type="checkbox" id="c-37140765" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140650">parent</a><span>|</span><a href="#37141804">next</a><span>|</span><label class="collapse" for="c-37140765">[-]</label><label class="expand" for="c-37140765">[5 more]</label></div><br/><div class="children"><div class="content">Whoa, imagine you get a good base LLM model and save all conversations with it. Run a batch process every night to fine tune a LORA on convo dataset. If I ever came across such a chat bot I&#x27;d probably freak out as to why it remembers things outside of the context window, without summarisation</div><br/><div id="37141050" class="c"><input type="checkbox" id="c-37141050" checked=""/><div class="controls bullet"><span class="by">MichaelZuo</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140765">parent</a><span>|</span><a href="#37141804">next</a><span>|</span><label class="collapse" for="c-37141050">[-]</label><label class="expand" for="c-37141050">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a pretty neat idea, I would be surprised if no one is already working on that.</div><br/><div id="37141495" class="c"><input type="checkbox" id="c-37141495" checked=""/><div class="controls bullet"><span class="by">notuseful</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37141050">parent</a><span>|</span><a href="#37141804">next</a><span>|</span><label class="collapse" for="c-37141495">[-]</label><label class="expand" for="c-37141495">[3 more]</label></div><br/><div class="children"><div class="content">I did it years ago on a lark with a seq2seq model in a matrix chat room.</div><br/><div id="37142075" class="c"><input type="checkbox" id="c-37142075" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37141495">parent</a><span>|</span><a href="#37141804">next</a><span>|</span><label class="collapse" for="c-37142075">[-]</label><label class="expand" for="c-37142075">[2 more]</label></div><br/><div class="children"><div class="content">How did it perform? Was it well received by members?</div><br/><div id="37142840" class="c"><input type="checkbox" id="c-37142840" checked=""/><div class="controls bullet"><span class="by">notuseful</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37142075">parent</a><span>|</span><a href="#37141804">next</a><span>|</span><label class="collapse" for="c-37142840">[-]</label><label class="expand" for="c-37142840">[1 more]</label></div><br/><div class="children"><div class="content">Poorly! It was a small seq2seq and was gibberish to start with. Although it did tell my friend that it loved him which was nice.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37141804" class="c"><input type="checkbox" id="c-37141804" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140650">parent</a><span>|</span><a href="#37140765">prev</a><span>|</span><a href="#37140921">next</a><span>|</span><label class="collapse" for="c-37141804">[-]</label><label class="expand" for="c-37141804">[2 more]</label></div><br/><div class="children"><div class="content">one reason why a human is different: just based on word count alone, most LLM&#x27;s are trained on 3-5 <i>orders of magnitude</i> more input.<p>could be a difference that makes no difference, or ...</div><br/><div id="37141974" class="c"><input type="checkbox" id="c-37141974" checked=""/><div class="controls bullet"><span class="by">dTal</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37141804">parent</a><span>|</span><a href="#37140921">next</a><span>|</span><label class="collapse" for="c-37141974">[-]</label><label class="expand" for="c-37141974">[1 more]</label></div><br/><div class="children"><div class="content">Bit of an unfair comparison when humans also have a bunch of senses that LLMs don&#x27;t have. They might be trained on orders of magnitude more <i>words</i>, but more data? Doubtful.</div><br/></div></div></div></div></div></div></div></div><div id="37140921" class="c"><input type="checkbox" id="c-37140921" checked=""/><div class="controls bullet"><span class="by">gerdesj</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140559">parent</a><span>|</span><a href="#37140631">prev</a><span>|</span><a href="#37140585">next</a><span>|</span><label class="collapse" for="c-37140921">[-]</label><label class="expand" for="c-37140921">[1 more]</label></div><br/><div class="children"><div class="content">Really?  I can guess at what spicy autocomplete might actually mean but I doubt a LLM ... OK ChatGPT did a pretty good job of it (I&#x27;ve just asked it), whilst sidestepping the definition of spicy in this context.  It is after all a very good next word guesser, given a context, and its not ... me!  I am capable of hallucinating but it was 30 odd years ago since I hunted out certain fungi on Dartmoor, or smoked hemp.<p>To be fair, we humans do often interrupt each other to second guess a sentence completion.  Done correctly it is a brief satisfying collaboration.  Done wrong ...  I&#x27;ve been married for 18 years and know when to bite my tongue, but I still get it wrong from time to time - sometimes deliberately.  Despite that, me and the wiff can autocomplete each other&#x27;s sentences with uncanny accuracy and end up with perfect harmony or a <i>cough</i> slight disagreement as a result.<p>We are getting some phenomenal slide rules these days but the darleks are not going to be flying up the stairwell just yet, nor will SkyNet be taking over tomorrow.<p>That said, you just know that some noddy is trying to sell a nuclear &quot;deterrent&quot; LLM AI thingie somewhere.  Thankfully, production military equipment takes quite a while to get to deployment.  There is a good chance that we will get to grips with all this stuff before SkyNet is let loose for real 8)</div><br/></div></div><div id="37140585" class="c"><input type="checkbox" id="c-37140585" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140559">parent</a><span>|</span><a href="#37140921">prev</a><span>|</span><a href="#37140706">next</a><span>|</span><label class="collapse" for="c-37140585">[-]</label><label class="expand" for="c-37140585">[8 more]</label></div><br/><div class="children"><div class="content">No they don&#x27;t. You&#x27;re &quot;just&quot; doing what everyone else in the past has done with the brain&#x2F;human intelligence and using the latest technology as a metaphor without realizing it.</div><br/><div id="37140644" class="c"><input type="checkbox" id="c-37140644" checked=""/><div class="controls bullet"><span class="by">Waterluvian</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140585">parent</a><span>|</span><a href="#37140706">next</a><span>|</span><label class="collapse" for="c-37140644">[-]</label><label class="expand" for="c-37140644">[7 more]</label></div><br/><div class="children"><div class="content">We want to think we’re exceptional but all we can do is say “human consciousness is special” without having any way of measuring it or disproving the assertion that we’re just really fancy pattern matchers.<p>Take any metaphor you want, it’s the same outcome: we may all be philosophical zombies.</div><br/><div id="37140819" class="c"><input type="checkbox" id="c-37140819" checked=""/><div class="controls bullet"><span class="by">roarcher</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140644">parent</a><span>|</span><a href="#37141840">next</a><span>|</span><label class="collapse" for="c-37140819">[-]</label><label class="expand" for="c-37140819">[5 more]</label></div><br/><div class="children"><div class="content">We may &quot;just&quot; be neural networks that run on meat instead of silicon, but that does not mean that we&#x27;re LLMs.</div><br/><div id="37140916" class="c"><input type="checkbox" id="c-37140916" checked=""/><div class="controls bullet"><span class="by">Waterluvian</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140819">parent</a><span>|</span><a href="#37141840">next</a><span>|</span><label class="collapse" for="c-37140916">[-]</label><label class="expand" for="c-37140916">[4 more]</label></div><br/><div class="children"><div class="content">Why doesn’t it?</div><br/><div id="37142748" class="c"><input type="checkbox" id="c-37142748" checked=""/><div class="controls bullet"><span class="by">roarcher</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140916">parent</a><span>|</span><a href="#37141005">next</a><span>|</span><label class="collapse" for="c-37142748">[-]</label><label class="expand" for="c-37142748">[1 more]</label></div><br/><div class="children"><div class="content">Because not all neural networks are LLMs.<p>A GAN is a neural network, does that make it an LLM?</div><br/></div></div><div id="37141005" class="c"><input type="checkbox" id="c-37141005" checked=""/><div class="controls bullet"><span class="by">marginalia_nu</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140916">parent</a><span>|</span><a href="#37142748">prev</a><span>|</span><a href="#37142850">next</a><span>|</span><label class="collapse" for="c-37141005">[-]</label><label class="expand" for="c-37141005">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a formal logical error. One does not follow from the other without affirming the consequent.</div><br/></div></div><div id="37142850" class="c"><input type="checkbox" id="c-37142850" checked=""/><div class="controls bullet"><span class="by">hkt</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140916">parent</a><span>|</span><a href="#37141005">prev</a><span>|</span><a href="#37141840">next</a><span>|</span><label class="collapse" for="c-37142850">[-]</label><label class="expand" for="c-37142850">[1 more]</label></div><br/><div class="children"><div class="content">We have inputs other than words, for a start</div><br/></div></div></div></div></div></div><div id="37141840" class="c"><input type="checkbox" id="c-37141840" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140644">parent</a><span>|</span><a href="#37140819">prev</a><span>|</span><a href="#37140706">next</a><span>|</span><label class="collapse" for="c-37141840">[-]</label><label class="expand" for="c-37141840">[1 more]</label></div><br/><div class="children"><div class="content">I’m conscious, maybe you’re not? Not that I really believe that. I think you probably see colors and hear sounds, even in your dreams! But engineering types tend to be persuaded by a particular view of the world, failing to understand that it is a view and not nature itself.</div><br/></div></div></div></div></div></div><div id="37140706" class="c"><input type="checkbox" id="c-37140706" checked=""/><div class="controls bullet"><span class="by">catchnear4321</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140559">parent</a><span>|</span><a href="#37140585">prev</a><span>|</span><a href="#37140546">next</a><span>|</span><label class="collapse" for="c-37140706">[-]</label><label class="expand" for="c-37140706">[2 more]</label></div><br/><div class="children"><div class="content">monkeys make monkeys accidentally.<p>monkeys make meseeks on purpose.<p>there is a difference, but will it be fun?</div><br/><div id="37140727" class="c"><input type="checkbox" id="c-37140727" checked=""/><div class="controls bullet"><span class="by">Waterluvian</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140706">parent</a><span>|</span><a href="#37140546">next</a><span>|</span><label class="collapse" for="c-37140727">[-]</label><label class="expand" for="c-37140727">[1 more]</label></div><br/><div class="children"><div class="content">I am 100% invested in how much ridiculous fun this era is going to be. Right up until the moment when it becomes a horror.</div><br/></div></div></div></div></div></div></div></div><div id="37140546" class="c"><input type="checkbox" id="c-37140546" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140237">parent</a><span>|</span><a href="#37140506">prev</a><span>|</span><a href="#37140667">next</a><span>|</span><label class="collapse" for="c-37140546">[-]</label><label class="expand" for="c-37140546">[25 more]</label></div><br/><div class="children"><div class="content">The irony in your statement is immense. Yes, Kurzeweil has been saying this for decades. No it doesn&#x27;t mean AGI is close. These llms do nothing to advance AGI. There is no theoretical basis to the belief in emergent intelligence from statistical language models and the answers are amazingly good, highly unreliable and parrot meaning at best. There is no inductance, and no inteospection and no understanding of the deep semantic meaning of the language presented. There&#x27;s no intelligence.</div><br/><div id="37141274" class="c"><input type="checkbox" id="c-37141274" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140546">parent</a><span>|</span><a href="#37140685">next</a><span>|</span><label class="collapse" for="c-37141274">[-]</label><label class="expand" for="c-37141274">[1 more]</label></div><br/><div class="children"><div class="content">The lack of concept of &quot;knowledge&quot; is a big one for me - if that&#x27;s an emergent thing it hasn&#x27;t even shown hints of this yet. This to me seems a pretty hard line right now, as it limits their capability of things even inexperienced humans can do - namely decide if they actually know something, and identify when they don&#x27;t know something and attempt to fix that - e.g. asking for clarification on vague inputs, or deciding if something is actually truth or fiction.<p>That then ties into another limitation right now - how after a training the model is pretty static, so cannot learn and has no state outside it&#x27;s context buffer. This could just be another point where a few orders of magnitude more computing power can &quot;fix&quot; it, doing whole training steps between each input to actually incorporate new information and corrections into itself instead of relying on a fixed size small context.<p>But I&#x27;m not deep enough into things to say if they&#x27;re fundamental issues, or current techniques will start displaying those effects as emergent characteristics as the complexity and training increases. There&#x27;s been a few other examples when &quot;known&quot; techniques start to show unexpected characteristics down the line as they are scaled up, so can&#x27;t really say for sure they&#x27;ll &#x2F;never&#x2F; be shown, just that the current examples don&#x27;t seem to show even the beginnings of that sort of thing.</div><br/></div></div><div id="37140685" class="c"><input type="checkbox" id="c-37140685" checked=""/><div class="controls bullet"><span class="by">coolspot</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140546">parent</a><span>|</span><a href="#37141274">prev</a><span>|</span><a href="#37140768">next</a><span>|</span><label class="collapse" for="c-37140685">[-]</label><label class="expand" for="c-37140685">[3 more]</label></div><br/><div class="children"><div class="content">&gt; the answers are amazingly good, highly unreliable and parrot meaning at best. There is no inductance, and no inteospection and no understanding of the deep semantic meaning of the language presented. There&#x27;s no intelligence.<p>Can say the same about a half of population, tbh</div><br/><div id="37140808" class="c"><input type="checkbox" id="c-37140808" checked=""/><div class="controls bullet"><span class="by">binary132</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140685">parent</a><span>|</span><a href="#37140768">next</a><span>|</span><label class="collapse" for="c-37140808">[-]</label><label class="expand" for="c-37140808">[2 more]</label></div><br/><div class="children"><div class="content">So what you’re saying is....  we’re going to have human-level AI, and it’s going to be incredibly stupid</div><br/><div id="37141593" class="c"><input type="checkbox" id="c-37141593" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140808">parent</a><span>|</span><a href="#37140768">next</a><span>|</span><label class="collapse" for="c-37141593">[-]</label><label class="expand" for="c-37141593">[1 more]</label></div><br/><div class="children"><div class="content">BAAGI (Below Average Artificial General Intelligence).</div><br/></div></div></div></div></div></div><div id="37140768" class="c"><input type="checkbox" id="c-37140768" checked=""/><div class="controls bullet"><span class="by">glimshe</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140546">parent</a><span>|</span><a href="#37140685">prev</a><span>|</span><a href="#37141110">next</a><span>|</span><label class="collapse" for="c-37140768">[-]</label><label class="expand" for="c-37140768">[11 more]</label></div><br/><div class="children"><div class="content">Why do you say they do nothing to advance AGI? Do you know what it takes to advance AGI? It&#x27;s hard to state that without knowing how AGI would work yourself.<p>LLMs would be considered magic just a couple years ago. Sure, not AGI but behaves just like one for certain workloads. I find hard to believe we&#x27;re not a bit closer now - or maybe even a lot closer.</div><br/><div id="37140875" class="c"><input type="checkbox" id="c-37140875" checked=""/><div class="controls bullet"><span class="by">shrimpx</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140768">parent</a><span>|</span><a href="#37141110">next</a><span>|</span><label class="collapse" for="c-37140875">[-]</label><label class="expand" for="c-37140875">[10 more]</label></div><br/><div class="children"><div class="content">AGI should have morals, opinions, self-reflection, learn continuously from sensor data, reason, realize when they’re proven wrong and update their model of the world, and be creative. So far LLMs exhibit none of those. But LLMs exhibit a digestible distillation of a very large body of data which may be a component of an AGI.<p>But you can have an AGI that doesn’t have encyclopedic knowledge but it’s still highly intelligent, so I don’t think LLMs have to be an intrinsic component.</div><br/><div id="37141267" class="c"><input type="checkbox" id="c-37141267" checked=""/><div class="controls bullet"><span class="by">zarzavat</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140875">parent</a><span>|</span><a href="#37141110">next</a><span>|</span><label class="collapse" for="c-37141267">[-]</label><label class="expand" for="c-37141267">[9 more]</label></div><br/><div class="children"><div class="content">That is not what AGI means. AGI = Artificial General Intelligence.<p>1. Artificial = we made it<p>2. General = it can solve problems in any field<p>3. Intelligence = the ability to solve problems<p>A chess engine is a very strong <i>Artificial Intelligence</i>. But it’s not very <i>General</i>, it can only evaluate chess positions.<p>GPT-4 is very <i>General</i>, you can ask it about any question and get a somewhat reasonable answer. But it’s not very intelligent, often the answer is wrong.<p>You’re talking about an Artificial <i>Human</i>. That’s a different problem. Intelligence is not species dependent. Dolphins are intelligent (a bit), aliens can be intelligent and have zero emotions or conception of self. There’s certainly plenty of amoral intelligent serial killers.</div><br/><div id="37144171" class="c"><input type="checkbox" id="c-37144171" checked=""/><div class="controls bullet"><span class="by">shrimpx</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37141267">parent</a><span>|</span><a href="#37141838">next</a><span>|</span><label class="collapse" for="c-37144171">[-]</label><label class="expand" for="c-37144171">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That’s a different problem.<p>That&#x27;s news to me. AGI (or strong AI) is typically defined as &quot;human-level intelligence&quot;, or &quot;perform any task that a human or animal can.&quot; Humans and animals often perform tasks that are critically reliant on being conscious, emoting, reading body language, reasoning, etc.<p>Not only that but prominent thinkers who have carved out the notion of AGI (or Strong AI) tend to have consciousness, mental states, and emotions at the core of it.<p>I think what you&#x27;re talking about is a multi-task AI, not an AGI.</div><br/></div></div><div id="37141838" class="c"><input type="checkbox" id="c-37141838" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37141267">parent</a><span>|</span><a href="#37144171">prev</a><span>|</span><a href="#37141831">next</a><span>|</span><label class="collapse" for="c-37141838">[-]</label><label class="expand" for="c-37141838">[2 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t have a good computer model of dolphin intelligence and the llms are not even remotely close to consciousness or dolphins, dogs, parrots on the intelligence Front.</div><br/><div id="37142339" class="c"><input type="checkbox" id="c-37142339" checked=""/><div class="controls bullet"><span class="by">zarzavat</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37141838">parent</a><span>|</span><a href="#37141831">next</a><span>|</span><label class="collapse" for="c-37142339">[-]</label><label class="expand" for="c-37142339">[1 more]</label></div><br/><div class="children"><div class="content">Consciousness ≠ intelligence.<p>Consciousness: being “awake” and perceiving the world.<p>Intelligence: solving problems, finding the truth.<p>Consciousness is perceiving the world, whereas intelligence is understanding it.</div><br/></div></div></div></div><div id="37141831" class="c"><input type="checkbox" id="c-37141831" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37141267">parent</a><span>|</span><a href="#37141838">prev</a><span>|</span><a href="#37141110">next</a><span>|</span><label class="collapse" for="c-37141831">[-]</label><label class="expand" for="c-37141831">[5 more]</label></div><br/><div class="children"><div class="content">GPT-4 solves only 1 problem: what is the most likely stream of tokens to follow what we already have.<p>It is remarkably good at this. But there&#x27;s absolutely zero reason to believe it can solve any other problem at all.</div><br/><div id="37142537" class="c"><input type="checkbox" id="c-37142537" checked=""/><div class="controls bullet"><span class="by">zarzavat</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37141831">parent</a><span>|</span><a href="#37143815">next</a><span>|</span><label class="collapse" for="c-37142537">[-]</label><label class="expand" for="c-37142537">[3 more]</label></div><br/><div class="children"><div class="content">How do humans write if not by intuiting what word comes after another?<p>Intelligence is the ability of that next word decision procedure to determine a next word that is aligned with our human intuition and model of truth.<p>I believe what you’re getting at is <i>modality</i>, that GPT-4 only provides responses in text. You can’t ask it to drive a car, or paint like Dall-e. And that’s a fair criticism, but it’s mostly just because it would make the models too large and slow, not because we don’t know how to do it. The thing we don’t know how to do is make a model reason as well as a human, and it makes sense to try to solve that in the text domain first rather than making highly multimodal models that reason poorly in all domains.</div><br/><div id="37142694" class="c"><input type="checkbox" id="c-37142694" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37142537">parent</a><span>|</span><a href="#37143815">next</a><span>|</span><label class="collapse" for="c-37142694">[-]</label><label class="expand" for="c-37142694">[2 more]</label></div><br/><div class="children"><div class="content">&gt; How do humans write if not by intuiting what word comes after another?<p>We don&#x27;t know. It may turn out that we use mechanisms similar to LLMs, or it might be something entirely different.<p>As for the rest: nobody knows how to make ChatGPT butter a piece of toast, let alone drive a car.<p>ChatGPT does not <i>reason</i> about text, either.</div><br/><div id="37142885" class="c"><input type="checkbox" id="c-37142885" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37142694">parent</a><span>|</span><a href="#37143815">next</a><span>|</span><label class="collapse" for="c-37142885">[-]</label><label class="expand" for="c-37142885">[1 more]</label></div><br/><div class="children"><div class="content">&gt;nobody knows how to make ChatGPT butter a piece of toast<p>There is plenty of research on LLMs successfully piloting robots.<p>It&#x27;s but no means a solved problem but &quot;Nobody knows how&quot; is a stretch.<p><a href="https:&#x2F;&#x2F;tidybot.cs.princeton.edu&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;tidybot.cs.princeton.edu&#x2F;</a>
<a href="https:&#x2F;&#x2F;innermonologue.github.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;innermonologue.github.io&#x2F;</a><p>&gt;ChatGPT does not reason about text, either.<p>It does and there&#x27;s plenty of output to demonstrate that.</div><br/></div></div></div></div></div></div><div id="37143815" class="c"><input type="checkbox" id="c-37143815" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37141831">parent</a><span>|</span><a href="#37142537">prev</a><span>|</span><a href="#37141110">next</a><span>|</span><label class="collapse" for="c-37143815">[-]</label><label class="expand" for="c-37143815">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s an interface, not an implementation.<p>(&quot;The most likely&quot; out of what distribution? The model&#x27;s distribution. So that just means &quot;what the model thinks the answer to your question is&quot;.)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37141110" class="c"><input type="checkbox" id="c-37141110" checked=""/><div class="controls bullet"><span class="by">panarky</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140546">parent</a><span>|</span><a href="#37140768">prev</a><span>|</span><a href="#37140780">next</a><span>|</span><label class="collapse" for="c-37141110">[-]</label><label class="expand" for="c-37141110">[4 more]</label></div><br/><div class="children"><div class="content">His prediction was that one human brain&#x27;s worth of computing power could be acquired for $1000 by 2029. That still seems reasonable.<p>That&#x27;s not the same as AGI or the singularity.</div><br/><div id="37141813" class="c"><input type="checkbox" id="c-37141813" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37141110">parent</a><span>|</span><a href="#37140780">next</a><span>|</span><label class="collapse" for="c-37141813">[-]</label><label class="expand" for="c-37141813">[3 more]</label></div><br/><div class="children"><div class="content">You have a metric for human brains worth of computing power which hasn&#x27;t already been exceeded? I can&#x27;t do infinite precision arithmetic or the RSA algorithm in my head, or index a billion strings into lexical sort order.<p>But I am human, I am conscious and no visible VLSI work or algorithmic model will lead to AGI or a human equivalent computing power by 2029. Let alone for $1000.</div><br/><div id="37142221" class="c"><input type="checkbox" id="c-37142221" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37141813">parent</a><span>|</span><a href="#37140780">next</a><span>|</span><label class="collapse" for="c-37142221">[-]</label><label class="expand" for="c-37142221">[2 more]</label></div><br/><div class="children"><div class="content">well, you could just be hallucinating your own consciousness.  By 2029 it seems not unreasonable to expect that the most sophisticated models will carry out visual and auditory interactions which could fool even the most sophisticated viewer.  At that point, what really does consciousness mean?  If the robot insisted to me it was conscious, how can I really say no?</div><br/><div id="37142776" class="c"><input type="checkbox" id="c-37142776" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37142221">parent</a><span>|</span><a href="#37140780">next</a><span>|</span><label class="collapse" for="c-37142776">[-]</label><label class="expand" for="c-37142776">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>it seems not unreasonable</i><p>This is where we differ.</div><br/></div></div></div></div></div></div></div></div><div id="37140780" class="c"><input type="checkbox" id="c-37140780" checked=""/><div class="controls bullet"><span class="by">circuit10</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140546">parent</a><span>|</span><a href="#37141110">prev</a><span>|</span><a href="#37140667">next</a><span>|</span><label class="collapse" for="c-37140780">[-]</label><label class="expand" for="c-37140780">[5 more]</label></div><br/><div class="children"><div class="content">By most measures you could think of for intelligence languages models are improving, so I don’t see why you think this wouldn’t lead to something at least almost human-level if you scaled it up enough<p>Of course there could be some wall somewhere but I don’t see why there would be</div><br/><div id="37141323" class="c"><input type="checkbox" id="c-37141323" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140780">parent</a><span>|</span><a href="#37141829">next</a><span>|</span><label class="collapse" for="c-37141323">[-]</label><label class="expand" for="c-37141323">[3 more]</label></div><br/><div class="children"><div class="content">Because you need more training data for better results and they are running out of new training data.</div><br/><div id="37142511" class="c"><input type="checkbox" id="c-37142511" checked=""/><div class="controls bullet"><span class="by">ddingus</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37141323">parent</a><span>|</span><a href="#37141829">next</a><span>|</span><label class="collapse" for="c-37142511">[-]</label><label class="expand" for="c-37142511">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think so.<p>While it may be true that new data is coming in at a trickle these days, due to things like Discord, Slack, et al. all locking conversation and context up, as well as the daily volume of chapter is small relative to what is out there now.<p>The fact is that training data can be used in many different ways and I bet you we see the products of that fairly quickly as those who see this same as I do reach a point where they want to show n tell and test.</div><br/><div id="37143417" class="c"><input type="checkbox" id="c-37143417" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37142511">parent</a><span>|</span><a href="#37141829">next</a><span>|</span><label class="collapse" for="c-37143417">[-]</label><label class="expand" for="c-37143417">[1 more]</label></div><br/><div class="children"><div class="content">&gt;The fact is that training data can be used in many different ways and I bet you we see the products of that fairly quickly as those who see this same as I do reach a point where they want to show n tell and test.<p>Sounds like wishful thinking to overcome the limitations of LLMs.<p>At the same time we get more and more texts generated by LLMs so it gets harder to get actual man made texts.</div><br/></div></div></div></div></div></div><div id="37141829" class="c"><input type="checkbox" id="c-37141829" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140780">parent</a><span>|</span><a href="#37141323">prev</a><span>|</span><a href="#37140667">next</a><span>|</span><label class="collapse" for="c-37141829">[-]</label><label class="expand" for="c-37141829">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s &quot;we need a larger cowbell&quot; thinking. It&#x27;s not a theory of mind, it&#x27;s wishful thinking that it will.. emerge. Absent  theory I don&#x27;t think moar will make it emerge, no.</div><br/></div></div></div></div></div></div></div></div><div id="37140667" class="c"><input type="checkbox" id="c-37140667" checked=""/><div class="controls bullet"><span class="by">Aerroon</span><span>|</span><a href="#37140225">parent</a><span>|</span><a href="#37140237">prev</a><span>|</span><a href="#37140304">next</a><span>|</span><label class="collapse" for="c-37140667">[-]</label><label class="expand" for="c-37140667">[6 more]</label></div><br/><div class="children"><div class="content">A 200b 4-bit quantized model could potentially fit into 128 GB of RAM. The inference would just be really slow.<p>Ie you could technically run something like that today.<p>I think more VRAM on GPUs isn&#x27;t necessarily a technical limitation either. I think GPU manufacturers could add a lot more VRAM to their cards if they wanted to. The question is whether it would be worth the price increase.</div><br/><div id="37140852" class="c"><input type="checkbox" id="c-37140852" checked=""/><div class="controls bullet"><span class="by">rootusrootus</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140667">parent</a><span>|</span><a href="#37140865">next</a><span>|</span><label class="collapse" for="c-37140852">[-]</label><label class="expand" for="c-37140852">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Ie you could technically run something like that today.<p>Yep, on higher end machines it should already be feasible.  I can do 2.5-3 tok&#x2F;sec on a 70B model quantized at 4 bit today with my MacBook Pro M2 MAX w&#x2F;96GB.  It&#x27;s a little slower than a 30B, but the difference is less than I had guessed it would be.  That&#x27;s not super fast, but it&#x27;s usable.<p>And that&#x27;s on a machine that isn&#x27;t designed for this workload.  Over the next few years things should improve quite a bit.  200B does not seem like a reach.</div><br/></div></div><div id="37140865" class="c"><input type="checkbox" id="c-37140865" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140667">parent</a><span>|</span><a href="#37140852">prev</a><span>|</span><a href="#37140304">next</a><span>|</span><label class="collapse" for="c-37140865">[-]</label><label class="expand" for="c-37140865">[4 more]</label></div><br/><div class="children"><div class="content">About the RAM. I doubt they wanted to do that, since basic gpu function is to render a frame in as little ms as possible. Currently VRAM is latency optimized on consumer gpus and all memory chips are an inch away from the gpu. Light only travels as far in the gigahertz realm. Thats why they started mounting vram chips on both sides of the board, cause there was no more place left on the first side.<p>Just checked: light travels 30cm in one nanosecond. So if the gpu is running at 4GHz it goes only 7.5 cm.</div><br/><div id="37141019" class="c"><input type="checkbox" id="c-37141019" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37140865">parent</a><span>|</span><a href="#37140304">next</a><span>|</span><label class="collapse" for="c-37141019">[-]</label><label class="expand" for="c-37141019">[3 more]</label></div><br/><div class="children"><div class="content">VRAM is not latency optimized. VRAM has worse latency than your CPU RAM. The reason why it&#x27;s mounted closer is because of signal integrity because of higher frequencies, not because of latency.</div><br/><div id="37141143" class="c"><input type="checkbox" id="c-37141143" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37141019">parent</a><span>|</span><a href="#37140304">next</a><span>|</span><label class="collapse" for="c-37141143">[-]</label><label class="expand" for="c-37141143">[2 more]</label></div><br/><div class="children"><div class="content">Interesting. Where can I read more about that?</div><br/><div id="37141390" class="c"><input type="checkbox" id="c-37141390" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37140225">root</a><span>|</span><a href="#37141143">parent</a><span>|</span><a href="#37140304">next</a><span>|</span><label class="collapse" for="c-37141390">[-]</label><label class="expand" for="c-37141390">[1 more]</label></div><br/><div class="children"><div class="content">Sorry can&#x27;t provide any resources right now. If you search a bit I&#x27;m sure you&#x27;ll find some latency comparisons between DDR and GDDR.<p>But basically GPU memory (GDDR5&#x2F;6&#x2F;6X&#x2F;etc) is optimized for bandwidth (because GPUs need to move a lot of data, have few branches, few unknown data dependencies, high spatial locality). CPU memory is more optimized for latency (because of branchy code).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37140304" class="c"><input type="checkbox" id="c-37140304" checked=""/><div class="controls bullet"><span class="by">csjh</span><span>|</span><a href="#37140225">parent</a><span>|</span><a href="#37140667">prev</a><span>|</span><a href="#37141082">next</a><span>|</span><label class="collapse" for="c-37140304">[-]</label><label class="expand" for="c-37140304">[1 more]</label></div><br/><div class="children"><div class="content">IMO the direction we&#x27;re going seems more like having a few small models in a MoE that are equivalent to a current 200bn model</div><br/></div></div><div id="37141082" class="c"><input type="checkbox" id="c-37141082" checked=""/><div class="controls bullet"><span class="by">Mertax</span><span>|</span><a href="#37140225">parent</a><span>|</span><a href="#37140304">prev</a><span>|</span><a href="#37140278">next</a><span>|</span><label class="collapse" for="c-37141082">[-]</label><label class="expand" for="c-37141082">[1 more]</label></div><br/><div class="children"><div class="content">And then things like neural implants and BCIs -- seems like your dog could have language capabilities sooner than you&#x27;d think ;)</div><br/></div></div></div></div><div id="37140278" class="c"><input type="checkbox" id="c-37140278" checked=""/><div class="controls bullet"><span class="by">TMWNN</span><span>|</span><a href="#37140225">prev</a><span>|</span><a href="#37140798">next</a><span>|</span><label class="collapse" for="c-37140278">[-]</label><label class="expand" for="c-37140278">[3 more]</label></div><br/><div class="children"><div class="content">Bah. We still haven&#x27;t equaled the rude and hateful AI achieved in a microcomputer in 1981. &lt;<a href="https:&#x2F;&#x2F;scp-wiki.wikidot.com&#x2F;scp-079" rel="nofollow noreferrer">https:&#x2F;&#x2F;scp-wiki.wikidot.com&#x2F;scp-079</a>&gt;</div><br/><div id="37140389" class="c"><input type="checkbox" id="c-37140389" checked=""/><div class="controls bullet"><span class="by">RosanaAnaDana</span><span>|</span><a href="#37140278">parent</a><span>|</span><a href="#37140798">next</a><span>|</span><label class="collapse" for="c-37140389">[-]</label><label class="expand" for="c-37140389">[2 more]</label></div><br/><div class="children"><div class="content">We can keep reaching for that rainbow.</div><br/><div id="37140574" class="c"><input type="checkbox" id="c-37140574" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37140278">root</a><span>|</span><a href="#37140389">parent</a><span>|</span><a href="#37140798">next</a><span>|</span><label class="collapse" for="c-37140574">[-]</label><label class="expand" for="c-37140574">[1 more]</label></div><br/><div class="children"><div class="content">A good analogy: as you approach the rainbow moves off. Others see you in it, but you can confirm it&#x27;s somewhere else. It&#x27;s an effect, a side effect, it&#x27;s pretty and we value it. There&#x27;s no pot of gold in literal sense, it&#x27;s ephemeral value in other products.</div><br/></div></div></div></div></div></div><div id="37140798" class="c"><input type="checkbox" id="c-37140798" checked=""/><div class="controls bullet"><span class="by">thomasfromcdnjs</span><span>|</span><a href="#37140278">prev</a><span>|</span><a href="#37142917">next</a><span>|</span><label class="collapse" for="c-37140798">[-]</label><label class="expand" for="c-37140798">[2 more]</label></div><br/><div class="children"><div class="content">Thank you very much for writing this out!</div><br/><div id="37143432" class="c"><input type="checkbox" id="c-37143432" checked=""/><div class="controls bullet"><span class="by">fnbr</span><span>|</span><a href="#37140798">parent</a><span>|</span><a href="#37142917">next</a><span>|</span><label class="collapse" for="c-37143432">[-]</label><label class="expand" for="c-37143432">[1 more]</label></div><br/><div class="children"><div class="content">You’re welcome :)</div><br/></div></div></div></div><div id="37142917" class="c"><input type="checkbox" id="c-37142917" checked=""/><div class="controls bullet"><span class="by">fouc</span><span>|</span><a href="#37140798">prev</a><span>|</span><a href="#37140656">next</a><span>|</span><label class="collapse" for="c-37142917">[-]</label><label class="expand" for="c-37142917">[3 more]</label></div><br/><div class="children"><div class="content">Did anyone notice that apparently a M2 Macbook Pro is only 16x faster than a Pixel 5?  Not sure that makes sense.</div><br/><div id="37143016" class="c"><input type="checkbox" id="c-37143016" checked=""/><div class="controls bullet"><span class="by">sharkjacobs</span><span>|</span><a href="#37142917">parent</a><span>|</span><a href="#37143825">next</a><span>|</span><label class="collapse" for="c-37143016">[-]</label><label class="expand" for="c-37143016">[1 more]</label></div><br/><div class="children"><div class="content">I’m not an expert in this but my “does that feel right wrong” sense isn’t going off<p>Pixel 5 is 2-3 years old. CPUs aren’t doubling in speed every 2 years, but let’s very generously say we expect current designs to be 4x faster than 2-3 year old equivalent.<p>Apple silicon is faster than other ARM chips, so if we imagine that’s another 2x we’re up to 8x<p>Common wisdom is that “real computers” are faster than phones, but the difference between the A16 and M2 is less than 2x for multi core, and much much less for single core benchmarks. Rounding up, another 2x is 16x<p>Maybe there are characteristics of the two devices which make this more surprising, I’d be interested to learn more.</div><br/></div></div><div id="37143825" class="c"><input type="checkbox" id="c-37143825" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37142917">parent</a><span>|</span><a href="#37143016">prev</a><span>|</span><a href="#37140656">next</a><span>|</span><label class="collapse" for="c-37143825">[-]</label><label class="expand" for="c-37143825">[1 more]</label></div><br/><div class="children"><div class="content">The phone slows down as it heats up, and then slows down infinitely when the battery dies.<p>(With some workloads and chargers, it&#x27;s possible running on a phone would drain the battery faster than it can recharge even if you plugged it in.)</div><br/></div></div></div></div><div id="37140656" class="c"><input type="checkbox" id="c-37140656" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#37142917">prev</a><span>|</span><a href="#37140322">next</a><span>|</span><label class="collapse" for="c-37140656">[-]</label><label class="expand" for="c-37140656">[3 more]</label></div><br/><div class="children"><div class="content">Is this single-thread? Or are they putting all available CPUs on the problem?</div><br/><div id="37141173" class="c"><input type="checkbox" id="c-37141173" checked=""/><div class="controls bullet"><span class="by">sharms</span><span>|</span><a href="#37140656">parent</a><span>|</span><a href="#37142266">next</a><span>|</span><label class="collapse" for="c-37141173">[-]</label><label class="expand" for="c-37141173">[1 more]</label></div><br/><div class="children"><div class="content">The problem is memory bandwidth rather than CPU cores: &quot;Memory bandwidth is the limiting factor in almost everything to do with sampling from transformers. Anything that reduces the memory requirements for these models makes them much easier to serve&quot;</div><br/></div></div><div id="37142266" class="c"><input type="checkbox" id="c-37142266" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37140656">parent</a><span>|</span><a href="#37141173">prev</a><span>|</span><a href="#37140322">next</a><span>|</span><label class="collapse" for="c-37142266">[-]</label><label class="expand" for="c-37142266">[1 more]</label></div><br/><div class="children"><div class="content">Its complicated.<p>If you dont have a GPU, prompt ingestion is totally threaded. The more cores the better.<p>For generating tokens, more cores helps to a point, but then:<p>- You start saturating the memory bus, and performance plateaus.<p>- There is some overhead from the threading implementation, and too many threads <i>hurts</i> performance.<p>The ideal number of threads varys per CPU. For instance, using hyperthreaded cores or Apple&#x2F;Intel e cores typically hurts performance... But not always. You just have to test and see.</div><br/></div></div></div></div><div id="37140322" class="c"><input type="checkbox" id="c-37140322" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#37140656">prev</a><span>|</span><a href="#37141250">next</a><span>|</span><label class="collapse" for="c-37140322">[-]</label><label class="expand" for="c-37140322">[2 more]</label></div><br/><div class="children"><div class="content">Will be interesting to see what people can do with local models, particularly for open source programming tools and PCG models for video games.</div><br/><div id="37140872" class="c"><input type="checkbox" id="c-37140872" checked=""/><div class="controls bullet"><span class="by">cratermoon</span><span>|</span><a href="#37140322">parent</a><span>|</span><a href="#37141250">next</a><span>|</span><label class="collapse" for="c-37140872">[-]</label><label class="expand" for="c-37140872">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;ll probably write fanfic and some Harry Potter ships.</div><br/></div></div></div></div><div id="37141250" class="c"><input type="checkbox" id="c-37141250" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#37140322">prev</a><span>|</span><a href="#37142350">next</a><span>|</span><label class="collapse" for="c-37141250">[-]</label><label class="expand" for="c-37141250">[4 more]</label></div><br/><div class="children"><div class="content">Is there a Llama2 65b quantized version for Mac M2?</div><br/><div id="37141514" class="c"><input type="checkbox" id="c-37141514" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#37141250">parent</a><span>|</span><a href="#37141635">prev</a><span>|</span><a href="#37141889">next</a><span>|</span><label class="collapse" for="c-37141514">[-]</label><label class="expand" for="c-37141514">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d look for llama2 thebloke 70b GGML on hugginface.<p>Update, this might work: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Llama-2-70B-GGML" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;Llama-2-70B-GGML</a><p>Example command mentions the 4bit variant:<p>.&#x2F;main -m llama-2-70b.ggmlv3.q4_0.bin -gqa 8 -t 13 -p &quot;Llamas are&quot;</div><br/></div></div></div></div></div></div></div></div></div></body></html>