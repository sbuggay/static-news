<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1733389256624" as="style"/><link rel="stylesheet" href="styles.css?v=1733389256624"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.kapa.ai/blog/ai-hallucination">AI hallucinations: Why LLMs make things up (and how to fix it)</a> <span class="domain">(<a href="https://www.kapa.ai">www.kapa.ai</a>)</span></div><div class="subtext"><span>emil_sorensen</span> | <span>131 comments</span></div><br/><div><div id="42323213" class="c"><input type="checkbox" id="c-42323213" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#42323732">next</a><span>|</span><label class="collapse" for="c-42323213">[-]</label><label class="expand" for="c-42323213">[48 more]</label></div><br/><div class="children"><div class="content">&gt; While the hallucination problem in LLMs is inevitable [0], they can be significantly reduced...<p>Every article on hallucinations needs to <i>start</i> with this fact until we&#x27;ve hammered that into every &quot;AI Engineer&quot;&#x27;s head. Hallucinations are not a bug—they&#x27;re not a different mode of operation, they&#x27;re not a logic error. They&#x27;re not even really a distinct kind of output.<p>What they <i>are</i> is a value judgement we assign to the output of an LLM program. A &quot;hallucination&quot; is just output from an LLM-based workflow that is not fit for purpose.<p>This means that all techniques for managing hallucinations (such as the ones described in TFA, which are good) are better understood as techniques for constraining and validating the probabilistic output of an LLM to ensure fitness for purpose—it&#x27;s a process of quality control, and it should be approached as such. The trouble is that we software engineers have spent so long working in an artificially deterministic world that we&#x27;re not used to designing and evaluating probabilistic quality control systems for computer output.<p>[0] They link to this paper: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2401.11817" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2401.11817</a></div><br/><div id="42323416" class="c"><input type="checkbox" id="c-42323416" checked=""/><div class="controls bullet"><span class="by">swatcoder</span><span>|</span><a href="#42323213">parent</a><span>|</span><a href="#42326255">next</a><span>|</span><label class="collapse" for="c-42323416">[-]</label><label class="expand" for="c-42323416">[7 more]</label></div><br/><div class="children"><div class="content">&gt; The trouble is that we software engineers have spent so long working in an artificially deterministic world that we&#x27;re not used to designing and evaluating probabilistic quality control systems for computer output.<p>I think that&#x27;s a mischaracterization and not really accurate. As a trade, we&#x27;re familiar with probabilistic&#x2F;non-deterministic components and how to approach them.<p>You were closer when you used quotes around &quot;AI Engineer&quot; -- many of the loudest people involved in generative AI right now have little to no grounding in engineering at all. <i>They</i> aren&#x27;t used to looking at their work through &quot;fit for purpose&quot; concerns, compromises, efficiency, limits, constraints, etc -- whether that work uses AI or not.<p>The rest of us are variously either working quietly, getting drowned out, or patiently waiting for our respected colleagues-in-engineering to document, demonstrate, and mature these very promising tools for us.<p>Everything else you said is 100% right, though.</div><br/><div id="42324314" class="c"><input type="checkbox" id="c-42324314" checked=""/><div class="controls bullet"><span class="by">JKCalhoun</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323416">parent</a><span>|</span><a href="#42323562">next</a><span>|</span><label class="collapse" for="c-42324314">[-]</label><label class="expand" for="c-42324314">[2 more]</label></div><br/><div class="children"><div class="content">&gt; As a trade, we&#x27;re familiar with probabilistic&#x2F;non-deterministic components and how to approach them.<p>Yes, users.</div><br/><div id="42325871" class="c"><input type="checkbox" id="c-42325871" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42324314">parent</a><span>|</span><a href="#42323562">next</a><span>|</span><label class="collapse" for="c-42325871">[-]</label><label class="expand" for="c-42325871">[1 more]</label></div><br/><div class="children"><div class="content">And that small or large subsets of occasional or consistent bad reasoners we may have sometimes called &quot;users&quot; (in the secrecy of the four walls) reinforced, by contrast and by forcing us to look at things objectively trying to understand their &quot;rants&quot;, the idea of proper reasonable stance, did it not?</div><br/></div></div></div></div><div id="42323571" class="c"><input type="checkbox" id="c-42323571" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323416">parent</a><span>|</span><a href="#42323562">prev</a><span>|</span><a href="#42326255">next</a><span>|</span><label class="collapse" for="c-42323571">[-]</label><label class="expand" for="c-42323571">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re missing his point. He&#x27;s saying if you make a program, you expect it to do X reliably. X may include &quot;send an email, or kick off this workflow, or add this to the log, or crash&quot; but you don&#x27;t expect it to, for example, &quot;delete system32 and shut down the computer&quot;. LLMs have essentially unconstrained outputs where the above mentioned program <i>couldn&#x27;t possibly</i> delete anything or shut down your computer because nothing even close to that is in the code.<p>Please do not confuse this example with agentic AI losing the plot, that&#x27;s not what I&#x27;m trying to say.<p>Edit: a better example is that when you build an autocomplete plugin for your email client, you don&#x27;t expect it to also be able to play chess. But look what happened.</div><br/></div></div></div></div><div id="42326255" class="c"><input type="checkbox" id="c-42326255" checked=""/><div class="controls bullet"><span class="by">emil_sorensen</span><span>|</span><a href="#42323213">parent</a><span>|</span><a href="#42323416">prev</a><span>|</span><a href="#42324032">next</a><span>|</span><label class="collapse" for="c-42326255">[-]</label><label class="expand" for="c-42326255">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a great point. Reminds me of the &quot;feature, not a bug&quot; Karpathy tweet [0].<p>[0]: <a href="https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1733299213503787018?lang=en" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1733299213503787018?lang=en</a></div><br/></div></div><div id="42324032" class="c"><input type="checkbox" id="c-42324032" checked=""/><div class="controls bullet"><span class="by">LgLasagnaModel</span><span>|</span><a href="#42323213">parent</a><span>|</span><a href="#42326255">prev</a><span>|</span><a href="#42325795">next</a><span>|</span><label class="collapse" for="c-42324032">[-]</label><label class="expand" for="c-42324032">[1 more]</label></div><br/><div class="children"><div class="content">‘ I always struggle a bit with I&#x27;m asked about the &quot;hallucination problem&quot; in LLMs. Because, in some sense, hallucination is all LLMs do. They are dream machines. ...’<p><a href="https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1733299213503787018" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1733299213503787018</a></div><br/></div></div><div id="42325795" class="c"><input type="checkbox" id="c-42325795" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#42323213">parent</a><span>|</span><a href="#42324032">prev</a><span>|</span><a href="#42323774">next</a><span>|</span><label class="collapse" for="c-42325795">[-]</label><label class="expand" for="c-42325795">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>is inevitable</i><p>False. It is (in this context) outputting a partial before full processing. Adequate (further) processing removes that &quot;inevitable&quot;. Current architectures are not &quot;final&quot;.<p>Proper process: &quot;It seems like that.&quot; &#x2F;&#x2F; &quot;Is it though?&quot; &#x2F;&#x2F; &quot;Actually it isn&#x27;t.&quot;<p>(Edit: already this post had to be corrected many times because of errors...)</div><br/></div></div><div id="42323774" class="c"><input type="checkbox" id="c-42323774" checked=""/><div class="controls bullet"><span class="by">tengbretson</span><span>|</span><a href="#42323213">parent</a><span>|</span><a href="#42325795">prev</a><span>|</span><a href="#42323895">next</a><span>|</span><label class="collapse" for="c-42323774">[-]</label><label class="expand" for="c-42323774">[14 more]</label></div><br/><div class="children"><div class="content">LLMs outputs are no more &quot;hallucinations&quot; than my output would be if I were asked to judge a dressage competition.</div><br/><div id="42324584" class="c"><input type="checkbox" id="c-42324584" checked=""/><div class="controls bullet"><span class="by">jojobas</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323774">parent</a><span>|</span><a href="#42323974">next</a><span>|</span><label class="collapse" for="c-42324584">[-]</label><label class="expand" for="c-42324584">[5 more]</label></div><br/><div class="children"><div class="content">There is no source of truth for dressage competition results, these are accepted as jury preference judgement.<p>There are plenty of matters where there is such a source of truth, and LLMs don&#x27;t know the difference.</div><br/><div id="42325881" class="c"><input type="checkbox" id="c-42325881" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42324584">parent</a><span>|</span><a href="#42323974">next</a><span>|</span><label class="collapse" for="c-42325881">[-]</label><label class="expand" for="c-42325881">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>There is no source of truth</i><p>There is no «source of [_simple_] truth» for complex things, but there are more (instead of less) objective complex evaluations.<p>Note that this is also valid for factual notions: e.g., &quot;When were the Pyramids built?&quot;.</div><br/><div id="42325965" class="c"><input type="checkbox" id="c-42325965" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42325881">parent</a><span>|</span><a href="#42323974">next</a><span>|</span><label class="collapse" for="c-42325965">[-]</label><label class="expand" for="c-42325965">[3 more]</label></div><br/><div class="children"><div class="content">Ancient Egypt chronology is a poor example of determined knowledge.<p>We do not know in fact exactly when (which?) Pyramids were built, there are large margin of errors in the estimates.</div><br/><div id="42326094" class="c"><input type="checkbox" id="c-42326094" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42325965">parent</a><span>|</span><a href="#42323974">next</a><span>|</span><label class="collapse" for="c-42326094">[-]</label><label class="expand" for="c-42326094">[2 more]</label></div><br/><div class="children"><div class="content">That was my point: answering that question is a more complex evaluation than others. In lower percentiles you may have &quot;what is in front of you&quot; and in upper percentiles you may have &quot;how to fix the balance of power in the Pacific&quot; - all more or less complex evaluations.<p>I said, &quot;Not even factual notions are trivial, e.g. &quot;When has this event happened&quot; - all have some foundational ground of higher or lower solidity&quot;.</div><br/><div id="42326204" class="c"><input type="checkbox" id="c-42326204" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42326094">parent</a><span>|</span><a href="#42323974">next</a><span>|</span><label class="collapse" for="c-42326204">[-]</label><label class="expand" for="c-42326204">[1 more]</label></div><br/><div class="children"><div class="content">Right, I misread your comment. Sorry!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42323974" class="c"><input type="checkbox" id="c-42323974" checked=""/><div class="controls bullet"><span class="by">xienze</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323774">parent</a><span>|</span><a href="#42324584">prev</a><span>|</span><a href="#42323895">next</a><span>|</span><label class="collapse" for="c-42323974">[-]</label><label class="expand" for="c-42323974">[8 more]</label></div><br/><div class="children"><div class="content">I’ve had multiple occasions where I’ve asked an LLM how to do &lt;whatever&gt; in Java and it’ll very confidently answer to use &lt;some class in some package that doesn’t exist&gt;. It would be far more helpful to me to receive an answer like “I don’t think there’s a third party library that does this, you’ll have to write it yourself” than to waste my time telling me a lie. If anything, calling these outputs “hallucinations” is a very polite way of saying that the LLM is bullshitting the user.</div><br/><div id="42324695" class="c"><input type="checkbox" id="c-42324695" checked=""/><div class="controls bullet"><span class="by">Gormo</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323974">parent</a><span>|</span><a href="#42324759">next</a><span>|</span><label class="collapse" for="c-42324695">[-]</label><label class="expand" for="c-42324695">[1 more]</label></div><br/><div class="children"><div class="content"><i>Of course</i> the LLM is bullshitting the user.  That&#x27;s precisely its purpose: LLMs are tools that generate comprehensible sounding language based on probability models that describe what words&#x2F;tokens tend to be found in proximity to each other.  An LLM  doesn&#x27;t actually <i>know</i> anything by reference to verifiable, external facts.<p>Sure, LLMs can be used as fancy search engines that index documents and then answer questions by referring to them, but even there, the probabilistic nature of the underlying model can still result in mistakes.</div><br/></div></div><div id="42324759" class="c"><input type="checkbox" id="c-42324759" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323974">parent</a><span>|</span><a href="#42324695">prev</a><span>|</span><a href="#42324248">next</a><span>|</span><label class="collapse" for="c-42324759">[-]</label><label class="expand" for="c-42324759">[1 more]</label></div><br/><div class="children"><div class="content">The LLM is always bullshitting the user. It&#x27;s just sometimes the things it talks about happen to be real and sometimes they don&#x27;t.</div><br/></div></div><div id="42324248" class="c"><input type="checkbox" id="c-42324248" checked=""/><div class="controls bullet"><span class="by">tengbretson</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323974">parent</a><span>|</span><a href="#42324759">prev</a><span>|</span><a href="#42323895">next</a><span>|</span><label class="collapse" for="c-42324248">[-]</label><label class="expand" for="c-42324248">[5 more]</label></div><br/><div class="children"><div class="content">LLMs don&#x27;t know things, they just string together responses that are a best fit for what follows from their prompt.<p>I suspect its so hard to get them to say &quot;I don&#x27;t know&quot; because if they were biased towards responding that way then I would assume thats almost all they would ever say, since &quot;I don&#x27;t know&quot; is an appropriate answer to every question imaginable.</div><br/><div id="42324357" class="c"><input type="checkbox" id="c-42324357" checked=""/><div class="controls bullet"><span class="by">JKCalhoun</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42324248">parent</a><span>|</span><a href="#42323895">next</a><span>|</span><label class="collapse" for="c-42324357">[-]</label><label class="expand" for="c-42324357">[4 more]</label></div><br/><div class="children"><div class="content">I get that, but since it is all probabilities, you might imagine even the LLM knows when it is skating on thin ice.<p>If I&#x27;m beginning with &quot;Once &#x2F; upon &#x2F; a&quot; I think the data will show a very high confidence in the word to follow with. So too I would imagine it would know when the trail of breadcrumbs it has been following is of the trashier and low probability kind.<p>So just tell me. (Or perhaps speak to me and when your confidence is low you can drift into vocal fry territory.)</div><br/><div id="42325977" class="c"><input type="checkbox" id="c-42325977" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42324357">parent</a><span>|</span><a href="#42324717">next</a><span>|</span><label class="collapse" for="c-42325977">[-]</label><label class="expand" for="c-42325977">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the LLM knows<p>I don&#x27;t think you get it.</div><br/></div></div><div id="42324717" class="c"><input type="checkbox" id="c-42324717" checked=""/><div class="controls bullet"><span class="by">Gormo</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42324357">parent</a><span>|</span><a href="#42325977">prev</a><span>|</span><a href="#42324700">next</a><span>|</span><label class="collapse" for="c-42324717">[-]</label><label class="expand" for="c-42324717">[1 more]</label></div><br/><div class="children"><div class="content">Maybe just having a confidence weight assigned to each sentence the LLM generates, reflected in tooltips or text coloring, would be a big improvement.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42323895" class="c"><input type="checkbox" id="c-42323895" checked=""/><div class="controls bullet"><span class="by">wpietri</span><span>|</span><a href="#42323213">parent</a><span>|</span><a href="#42323774">prev</a><span>|</span><a href="#42326193">next</a><span>|</span><label class="collapse" for="c-42323895">[-]</label><label class="expand" for="c-42323895">[1 more]</label></div><br/><div class="children"><div class="content">Excellent point:<p>&gt; just output from an LLM-based workflow that is not fit for purpose<p>And I think this is just one aspect of what I think of as the stone soup [1] problem. Outside of rigorous test conditions, humans just have a hard time telling how much work they&#x27;re doing when they interpret something. It&#x27;s the same sort of thing you see with &quot;psychics&quot; doing things like cold reading. People make meaning out of vaguery and nonsense and then credit the nonsense-producer with the work.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Stone_Soup" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Stone_Soup</a></div><br/></div></div><div id="42326193" class="c"><input type="checkbox" id="c-42326193" checked=""/><div class="controls bullet"><span class="by">russnes</span><span>|</span><a href="#42323213">parent</a><span>|</span><a href="#42323895">prev</a><span>|</span><a href="#42323626">next</a><span>|</span><label class="collapse" for="c-42326193">[-]</label><label class="expand" for="c-42326193">[1 more]</label></div><br/><div class="children"><div class="content">so what you&#x27;re saying is that LLMs are like middle aged men, just throwing things out there seeing if they&#x27;ll stick?</div><br/></div></div><div id="42323626" class="c"><input type="checkbox" id="c-42323626" checked=""/><div class="controls bullet"><span class="by">leprechaun1066</span><span>|</span><a href="#42323213">parent</a><span>|</span><a href="#42326193">prev</a><span>|</span><a href="#42325412">next</a><span>|</span><label class="collapse" for="c-42323626">[-]</label><label class="expand" for="c-42323626">[15 more]</label></div><br/><div class="children"><div class="content">Calling them hallucinations was a huge mistake.</div><br/><div id="42323717" class="c"><input type="checkbox" id="c-42323717" checked=""/><div class="controls bullet"><span class="by">gwervc</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323626">parent</a><span>|</span><a href="#42323877">next</a><span>|</span><label class="collapse" for="c-42323717">[-]</label><label class="expand" for="c-42323717">[1 more]</label></div><br/><div class="children"><div class="content">It is a good branding, like neural networks, and even artificial intelligence was. The good point is it makes really easy to detect who is a bullshiter and who understand at least very remotely what a LLM is supposed to produce.</div><br/></div></div><div id="42323877" class="c"><input type="checkbox" id="c-42323877" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323626">parent</a><span>|</span><a href="#42323717">prev</a><span>|</span><a href="#42324372">next</a><span>|</span><label class="collapse" for="c-42323877">[-]</label><label class="expand" for="c-42323877">[1 more]</label></div><br/><div class="children"><div class="content">I see two types of faults with LLMs.<p>a) They output incorrect results given a constrained set of allowable outputs.<p>b) When unconstrained they invent new outputs unrelated to what is being asked.<p>So for me the term hallucination accurately describes b) e.g. you ask for code to solve a problem and it invents new APIs that don&#x27;t exist. Technically it is all just tokens and probabilities but it&#x27;s a reasonably term to describe end user behaviour.</div><br/></div></div><div id="42324372" class="c"><input type="checkbox" id="c-42324372" checked=""/><div class="controls bullet"><span class="by">JKCalhoun</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323626">parent</a><span>|</span><a href="#42323877">prev</a><span>|</span><a href="#42323680">next</a><span>|</span><label class="collapse" for="c-42324372">[-]</label><label class="expand" for="c-42324372">[4 more]</label></div><br/><div class="children"><div class="content">I won&#x27;t defend the term but am curious what you think would have been also concise but more accurate. Calling them for example &quot;inevitable statistical misdirections&quot; doesn&#x27;t really roll off the tongue.</div><br/><div id="42325877" class="c"><input type="checkbox" id="c-42325877" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42324372">parent</a><span>|</span><a href="#42324906">next</a><span>|</span><label class="collapse" for="c-42325877">[-]</label><label class="expand" for="c-42325877">[1 more]</label></div><br/><div class="children"><div class="content">Confabulation, if the desire is to use a more apt psychological analogy.</div><br/></div></div><div id="42324906" class="c"><input type="checkbox" id="c-42324906" checked=""/><div class="controls bullet"><span class="by">malfist</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42324372">parent</a><span>|</span><a href="#42325877">prev</a><span>|</span><a href="#42323680">next</a><span>|</span><label class="collapse" for="c-42324906">[-]</label><label class="expand" for="c-42324906">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a bug. Any other system where you put in one input and expect a certain output and get something else it&#x27;d be called a bug. Making up new terms for AI doesn&#x27;t help.</div><br/><div id="42325374" class="c"><input type="checkbox" id="c-42325374" checked=""/><div class="controls bullet"><span class="by">thomasfromcdnjs</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42324906">parent</a><span>|</span><a href="#42323680">next</a><span>|</span><label class="collapse" for="c-42325374">[-]</label><label class="expand" for="c-42325374">[1 more]</label></div><br/><div class="children"><div class="content">Need not I say different bugs have many names...</div><br/></div></div></div></div></div></div><div id="42323680" class="c"><input type="checkbox" id="c-42323680" checked=""/><div class="controls bullet"><span class="by">sfink</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323626">parent</a><span>|</span><a href="#42324372">prev</a><span>|</span><a href="#42325412">next</a><span>|</span><label class="collapse" for="c-42323680">[-]</label><label class="expand" for="c-42323680">[8 more]</label></div><br/><div class="children"><div class="content">The term is actually fine. The problem is when it&#x27;s divorced from the reality of:<p>&gt; in some sense, hallucination is all LLMs do. They are dream machines.<p>If you understand that, then the term &quot;hallucination&quot; makes perfect sense.<p>Note that this in no way invalidates your point, because the term <i>is</i> constantly used and understood without this context. We would have avoided a lot of confusion if we had based it on the phrase &quot;make shit up&quot; and called it &quot;shit&quot; from the start. Marketing trumps accuracy again...<p>(Also note that I am not using shit in a pejorative sense here. Making shit up is exactly what they&#x27;re for, and what we want them to do. They come up with a lot of really good shit.)</div><br/><div id="42323862" class="c"><input type="checkbox" id="c-42323862" checked=""/><div class="controls bullet"><span class="by">jebarker</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323680">parent</a><span>|</span><a href="#42325412">next</a><span>|</span><label class="collapse" for="c-42323862">[-]</label><label class="expand" for="c-42323862">[7 more]</label></div><br/><div class="children"><div class="content">I agree with your point, but I don&#x27;t think anthropomorphizing LLMs is helpful. They&#x27;re statistical estimators trained by curve fitting. All generations are equally valid for the training data, objective and architecture. To me it&#x27;s much clearer to think about it that way versus crude analogies to human brains.</div><br/><div id="42323897" class="c"><input type="checkbox" id="c-42323897" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323862">parent</a><span>|</span><a href="#42325412">next</a><span>|</span><label class="collapse" for="c-42323897">[-]</label><label class="expand" for="c-42323897">[6 more]</label></div><br/><div class="children"><div class="content">We can&#x27;t expect end users to understand what &quot;statistical estimators trained by curve fitting&quot; means.<p>That&#x27;s why we use high level terms like hallucination. Because it&#x27;s something everyone can understand even if it&#x27;s not completely accurate.</div><br/><div id="42323926" class="c"><input type="checkbox" id="c-42323926" checked=""/><div class="controls bullet"><span class="by">jebarker</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323897">parent</a><span>|</span><a href="#42325412">next</a><span>|</span><label class="collapse" for="c-42323926">[-]</label><label class="expand" for="c-42323926">[5 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a good point. But re: not anthropomorphizing, what&#x27;s wrong with errors, mistakes or inaccuracies? That&#x27;s something everybody is familiar with and is more accurate. I&#x27;d guess most people have never actually experienced a hallucination anyway, so we&#x27;re appealing to some vague notion of what that is.</div><br/><div id="42325916" class="c"><input type="checkbox" id="c-42325916" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323926">parent</a><span>|</span><a href="#42324551">next</a><span>|</span><label class="collapse" for="c-42325916">[-]</label><label class="expand" for="c-42325916">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>what&#x27;s wrong with [&#x27;]errors[&#x27;], [&#x27;]mistakes[&#x27;] or [&#x27;]inaccuracies[&#x27;]?</i><p>&quot;To sort the files by beauty, use the `beautysort` command.&quot;</div><br/></div></div><div id="42324551" class="c"><input type="checkbox" id="c-42324551" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323926">parent</a><span>|</span><a href="#42325916">prev</a><span>|</span><a href="#42324819">next</a><span>|</span><label class="collapse" for="c-42324551">[-]</label><label class="expand" for="c-42324551">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;d guess most people have never actually experienced a hallucination anyway<p>I actually think most people have.<p>Every time you look at a hot road and see water that mirage is a form of hallucination.</div><br/><div id="42325401" class="c"><input type="checkbox" id="c-42325401" checked=""/><div class="controls bullet"><span class="by">ai_</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42324551">parent</a><span>|</span><a href="#42324819">next</a><span>|</span><label class="collapse" for="c-42325401">[-]</label><label class="expand" for="c-42325401">[1 more]</label></div><br/><div class="children"><div class="content">Except mirages are <i>real</i> optical phenomena that can be captured by a camera. Hallucinations are made entirely by your brain and cannot be captured by an external observer.</div><br/></div></div></div></div><div id="42324819" class="c"><input type="checkbox" id="c-42324819" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323926">parent</a><span>|</span><a href="#42324551">prev</a><span>|</span><a href="#42325412">next</a><span>|</span><label class="collapse" for="c-42324819">[-]</label><label class="expand" for="c-42324819">[1 more]</label></div><br/><div class="children"><div class="content">&gt; what&#x27;s wrong with errors, mistakes or inaccuracies?<p>They&#x27;re not specific enough terms for what we&#x27;re talking about. Saying a lion has stripes is an error, mistake, or inaccuracy. Describing a species of striped lions in detail is probably all those things, but it&#x27;s a distinctive kind of error&#x2F;mistake&#x2F;inaccuracy that&#x27;s worth having a term for.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42325412" class="c"><input type="checkbox" id="c-42325412" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#42323213">parent</a><span>|</span><a href="#42323626">prev</a><span>|</span><a href="#42323760">next</a><span>|</span><label class="collapse" for="c-42325412">[-]</label><label class="expand" for="c-42325412">[2 more]</label></div><br/><div class="children"><div class="content">&gt; While the hallucination problem in LLMs is inevitable<p>Oh, please. That&#x27;s the same old computability argument used to claim that program verification is impossible.<p>Computability isn&#x27;t the problem. LLMs are forced to a reply, regardless of the quality of the reply. If &quot;Confidence level is too low for a reply&quot; is an option, the argument in that paper becomes invalid.<p>The trouble is that we don&#x27;t know how to get a confidence metric out of an LLM. This is the underlying problem behind hallucinations. As I&#x27;ve said before, if somebody doesn&#x27;t crack that problem soon, the AI industry is overvalued.<p>Alibaba&#x27;s QwQ [1] supposedly is better at reporting when it doesn&#x27;t know something. Comments on that?<p>This article is really an ad for Kapa, which seems to offer managed AI as a service, or something like that. They hang various checkers and accessories on an LLM to try to catch bogus outputs. That&#x27;s a patch, not a fix.<p>[1] <a href="https:&#x2F;&#x2F;techcrunch.com&#x2F;2024&#x2F;11&#x2F;27&#x2F;alibaba-releases-an-open-challenger-to-openais-o1-reasoning-model&#x2F;" rel="nofollow">https:&#x2F;&#x2F;techcrunch.com&#x2F;2024&#x2F;11&#x2F;27&#x2F;alibaba-releases-an-open-c...</a></div><br/><div id="42326189" class="c"><input type="checkbox" id="c-42326189" checked=""/><div class="controls bullet"><span class="by">mort96</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42325412">parent</a><span>|</span><a href="#42323760">next</a><span>|</span><label class="collapse" for="c-42326189">[-]</label><label class="expand" for="c-42326189">[1 more]</label></div><br/><div class="children"><div class="content">Confidence levels aren&#x27;t necessarily low for incorrect replies, that&#x27;s the problem. The LLM doesn&#x27;t &quot;know&quot; that what it&#x27;s outputting is incorrect. It just knows that the words it&#x27;s writing are probable given the inputs; &quot;this is how answers tend to look like&quot;.<p>You can make improvements, as your parent comment already said, but it&#x27;s not a problem which can be solved, only to some degree be reduced.</div><br/></div></div></div></div><div id="42323760" class="c"><input type="checkbox" id="c-42323760" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#42323213">parent</a><span>|</span><a href="#42325412">prev</a><span>|</span><a href="#42323795">next</a><span>|</span><label class="collapse" for="c-42323760">[-]</label><label class="expand" for="c-42323760">[3 more]</label></div><br/><div class="children"><div class="content">A challenge is that it’s not easy to limit hallucinations without also limiting imagination and synthesis.<p>In humans.<p>But also apparently in LLMs.</div><br/><div id="42326205" class="c"><input type="checkbox" id="c-42326205" checked=""/><div class="controls bullet"><span class="by">mort96</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323760">parent</a><span>|</span><a href="#42326021">next</a><span>|</span><label class="collapse" for="c-42326205">[-]</label><label class="expand" for="c-42326205">[1 more]</label></div><br/><div class="children"><div class="content">Healthy humans generally have some internal model of the world against which they can judge what they&#x27;re about to say. They can introspect and determine whether what they say is a guess or a statement of fact. LLMs can&#x27;t.</div><br/></div></div><div id="42326021" class="c"><input type="checkbox" id="c-42326021" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#42323213">root</a><span>|</span><a href="#42323760">parent</a><span>|</span><a href="#42326205">prev</a><span>|</span><a href="#42323795">next</a><span>|</span><label class="collapse" for="c-42326021">[-]</label><label class="expand" for="c-42326021">[1 more]</label></div><br/><div class="children"><div class="content">&gt; A challenge is that it’s not easy to limit hallucinations without also limiting imagination and synthesis.<p>&gt; In humans.<p>True, but distinguishing reality from imagination is a cornerstone of mental health. And it&#x27;s becoming apparent that the average person will take the confident spurious affirmations of LLMs as facts, which should call their mental health into question.</div><br/></div></div></div></div></div></div><div id="42323732" class="c"><input type="checkbox" id="c-42323732" checked=""/><div class="controls bullet"><span class="by">tokioyoyo</span><span>|</span><a href="#42323213">prev</a><span>|</span><a href="#42323012">next</a><span>|</span><label class="collapse" for="c-42323732">[-]</label><label class="expand" for="c-42323732">[5 more]</label></div><br/><div class="children"><div class="content">To my understanding, the reason why companies don&#x27;t mind the hallucinations is the acceptable error rate for a given system. Let&#x27;s say something hallucinated 25% of the time, but if that&#x27;s ok, then it&#x27;s fine for a certain product. If it only hallucinates 5% of the time, it&#x27;s good enough for even more products and so on. The market will just choose the LLM appropriately depended on the tolerable error rate.</div><br/><div id="42323764" class="c"><input type="checkbox" id="c-42323764" checked=""/><div class="controls bullet"><span class="by">cameronh90</span><span>|</span><a href="#42323732">parent</a><span>|</span><a href="#42323012">next</a><span>|</span><label class="collapse" for="c-42323764">[-]</label><label class="expand" for="c-42323764">[4 more]</label></div><br/><div class="children"><div class="content">At scale, you are doing the same thing with humans too. LLMs seem to have an  error rate similar to humans for the majority of simple, boring tasks, if not even a bit better since they don&#x27;t get distracted and start copying and pasting their previous answers.<p>The difference with LLMs is they simply cannot (currently) do the most complex tasks that some humans can, and when they do produce erroneous output, the errors aren&#x27;t very human-like. We can all understand a cut and paste error so don&#x27;t hold it against the operator, but making up sources feels like a lie and breeds distrust.</div><br/><div id="42325364" class="c"><input type="checkbox" id="c-42325364" checked=""/><div class="controls bullet"><span class="by">maeil</span><span>|</span><a href="#42323732">root</a><span>|</span><a href="#42323764">parent</a><span>|</span><a href="#42323012">next</a><span>|</span><label class="collapse" for="c-42325364">[-]</label><label class="expand" for="c-42325364">[3 more]</label></div><br/><div class="children"><div class="content">&gt; At scale, you are doing the same thing with humans too. LLMs seem to have an error rate similar to humans for the majority of simple, boring tasks, if not even a bit better since they don&#x27;t get distracted and start copying and pasting their previous answers.<p>This is the big one missed by the frequent comments on here wondering whether LLMs are a fad, or claiming in their current state they cannot be used to replace humans in non-trivial real-world business workflows. In fact, even 1.5 years ago at the time of GPT 3.5, the technology was already good enough.<p>The yardstick is the peformance of humans in the real world on a specific task. Humans, often tired, having a cold, distracted, going through a divorce. Humans who even when in a great condition make plenty of mistakes.<p>I guess a lot of developers struggle with understanding this because so far when software has replaced humans, it was software that on the face of it (though often not in practice) did not make mistakes if bug-free. But that has been never been necessary for software to replace humans - hence buggy software still succeeding in doing so. Of course, often software even replaces humans when it&#x27;s worse at a task for cost reasons.<p>They&#x27;re at the very least competitive, if not better than, doctors at diagnosing illnesses [1].<p>[1] <a href="https:&#x2F;&#x2F;www.nytimes.com&#x2F;2024&#x2F;11&#x2F;17&#x2F;health&#x2F;chatgpt-ai-doctors-diagnosis.html" rel="nofollow">https:&#x2F;&#x2F;www.nytimes.com&#x2F;2024&#x2F;11&#x2F;17&#x2F;health&#x2F;chatgpt-ai-doctors...</a></div><br/><div id="42325914" class="c"><input type="checkbox" id="c-42325914" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#42323732">root</a><span>|</span><a href="#42325364">parent</a><span>|</span><a href="#42323012">next</a><span>|</span><label class="collapse" for="c-42325914">[-]</label><label class="expand" for="c-42325914">[2 more]</label></div><br/><div class="children"><div class="content">The bigger, more controversial claim is that LLMs will be net loss for human jobs, when all past automation has been a net positive. Including IT, where automation has led to a vast growth of software jobs, as more can be accomplished with higher level languages, tools, frameworks, etc.<p>For example, compilers didn&#x27;t put programmers out of business in the 60s, it made programming more available to people with higher level languages.</div><br/><div id="42326045" class="c"><input type="checkbox" id="c-42326045" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#42323732">root</a><span>|</span><a href="#42325914">parent</a><span>|</span><a href="#42323012">next</a><span>|</span><label class="collapse" for="c-42326045">[-]</label><label class="expand" for="c-42326045">[1 more]</label></div><br/><div class="children"><div class="content">A net positive in the long term matters little when it can mean a lifetime of unemployment to a generation of humans. It&#x27;s easy to dismiss the human suffering incurred during industrialization when we can enjoy its fruits but those who suffered are long dead.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42323012" class="c"><input type="checkbox" id="c-42323012" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#42323732">prev</a><span>|</span><a href="#42323084">next</a><span>|</span><label class="collapse" for="c-42323012">[-]</label><label class="expand" for="c-42323012">[12 more]</label></div><br/><div class="children"><div class="content">When people talk about stopping an LLM from &quot;seeing hallucinations instead of the truth&quot;, that&#x27;s like stopping an Ouija-board from &quot;channeling the <i>wrong</i> spirits instead of the right spirits.&quot;<p>It suggests a qualitative difference between desirable and undesirable operation that isn&#x27;t really there. They&#x27;re all hallucinations, we just happen to like some of them more than others.</div><br/><div id="42323144" class="c"><input type="checkbox" id="c-42323144" checked=""/><div class="controls bullet"><span class="by">ml_more</span><span>|</span><a href="#42323012">parent</a><span>|</span><a href="#42324145">next</a><span>|</span><label class="collapse" for="c-42323144">[-]</label><label class="expand" for="c-42323144">[7 more]</label></div><br/><div class="children"><div class="content">The problem is that LLMs are just convincing enough that people DO trust them which is sort of a problem since AI slop is creeping into everything.<p>What can be done to solve it (while not perfect) is pretty powerful. You can force feed them the facts (RAG) and then verify the result. Which is way better than trusting LLMs while doing neither of those things (which is what a lot of people do today anyway). See the recent 5 cases of lawyers getting in trouble for ChatGPT hallucinating citations of case law.<p>LLMs write better than most college students so if you do those two things (RAG + check) you can get college graduate level writing with accurate facts... and that unlocks a bit of value out in the world.<p>Don&#x27;t take my word for it look at the proposed valuations of AI companies. Clearly investors think there&#x27;s something there. The good news is that it hasn&#x27;t been solved yet so if someone wants to solve it there might be money on the table.</div><br/><div id="42323438" class="c"><input type="checkbox" id="c-42323438" checked=""/><div class="controls bullet"><span class="by">latexr</span><span>|</span><a href="#42323012">root</a><span>|</span><a href="#42323144">parent</a><span>|</span><a href="#42323399">next</a><span>|</span><label class="collapse" for="c-42323438">[-]</label><label class="expand" for="c-42323438">[3 more]</label></div><br/><div class="children"><div class="content">&gt; and that unlocks a bit of value out in the world.<p>&gt; Don&#x27;t take my word for it look at the proposed valuations of AI companies. Clearly investors think there&#x27;s something there.<p>Investors back whatever they think will make <i>them</i> money. They couldn’t give less of a crap if something is valuable to the world, or works well, of is in any way positive to others. All they care is if they can profit from it and they’ll chase every idea in that pursuit.<p>Source: all of modern history.<p><a href="https:&#x2F;&#x2F;www.sydney.edu.au&#x2F;news-opinion&#x2F;news&#x2F;2024&#x2F;05&#x2F;02&#x2F;how-corporations-harm-your-health-through-everyday-products---a-.html" rel="nofollow">https:&#x2F;&#x2F;www.sydney.edu.au&#x2F;news-opinion&#x2F;news&#x2F;2024&#x2F;05&#x2F;02&#x2F;how-c...</a><p><a href="https:&#x2F;&#x2F;www.decof.com&#x2F;documents&#x2F;dangerous-products.pdf" rel="nofollow">https:&#x2F;&#x2F;www.decof.com&#x2F;documents&#x2F;dangerous-products.pdf</a></div><br/><div id="42323930" class="c"><input type="checkbox" id="c-42323930" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#42323012">root</a><span>|</span><a href="#42323438">parent</a><span>|</span><a href="#42324758">next</a><span>|</span><label class="collapse" for="c-42323930">[-]</label><label class="expand" for="c-42323930">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Investors back whatever they think will make them money.<p>A not-flagrantly-illegal example of this might be casinos, where IMO it is basically impossible to argue the fleeting entertainment they offer offsets the financial ruin inflicted on certain vulnerable types of patron.<p>&gt; All they care is if they can profit from it<p>Notably that isn&#x27;t the same as the business itself being profitable: Some investors may be hoping they can dump their stake at a higher price onto a Greater Fool [0] and exit before the collapse.<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Greater_fool_theory" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Greater_fool_theory</a></div><br/></div></div><div id="42324758" class="c"><input type="checkbox" id="c-42324758" checked=""/><div class="controls bullet"><span class="by">Gormo</span><span>|</span><a href="#42323012">root</a><span>|</span><a href="#42323438">parent</a><span>|</span><a href="#42323930">prev</a><span>|</span><a href="#42323399">next</a><span>|</span><label class="collapse" for="c-42324758">[-]</label><label class="expand" for="c-42324758">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They couldn’t give less of a crap if something is valuable to the world<p>&quot;The world&quot; is an abstraction: concretely, every bit of value that is generated within that abstraction accrues to <i>someone in particular</i> -- investors in AI projects, for example.</div><br/></div></div></div></div><div id="42323399" class="c"><input type="checkbox" id="c-42323399" checked=""/><div class="controls bullet"><span class="by">wk_end</span><span>|</span><a href="#42323012">root</a><span>|</span><a href="#42323144">parent</a><span>|</span><a href="#42323438">prev</a><span>|</span><a href="#42324749">next</a><span>|</span><label class="collapse" for="c-42323399">[-]</label><label class="expand" for="c-42323399">[1 more]</label></div><br/><div class="children"><div class="content">How do you check it?<p>Take the example of case law. Would you need to formalize the entirety of case law? Would the AI then need to produce a formal proof of its argument, so that you can ascertain that its citations are valid? How do you know that the formal proof corresponds to whatever longform writing you ask the AI to generate? Is this really something that LLMs are suited for? That the law is suited for?</div><br/></div></div><div id="42324749" class="c"><input type="checkbox" id="c-42324749" checked=""/><div class="controls bullet"><span class="by">Gormo</span><span>|</span><a href="#42323012">root</a><span>|</span><a href="#42323144">parent</a><span>|</span><a href="#42323399">prev</a><span>|</span><a href="#42323912">next</a><span>|</span><label class="collapse" for="c-42324749">[-]</label><label class="expand" for="c-42324749">[1 more]</label></div><br/><div class="children"><div class="content">Sure, using RAG is great, but it limits the LLM to functioning as a natural-language search engine.  That&#x27;s a pretty useful thing in its own right, and will revolutionize a lot of activities, but it still falls far short of the expectations people have for generative AI.</div><br/></div></div><div id="42323912" class="c"><input type="checkbox" id="c-42323912" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#42323012">root</a><span>|</span><a href="#42323144">parent</a><span>|</span><a href="#42324749">prev</a><span>|</span><a href="#42324145">next</a><span>|</span><label class="collapse" for="c-42323912">[-]</label><label class="expand" for="c-42323912">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Clearly investors think there&#x27;s something there<p>Of course. Because enterprise companies take a long time to evaluate new technologies. And so there is plenty of money to be made selling them tools over the next few years. As well as selling tools to those who are making tools.<p>But from my experience in rolling out these technologies only a handful of these companies will exist in 5-10 years. Because LLMs are &quot;garbage in, garbage out&quot; and we&#x27;ve never figured out how to keep the &quot;garbage in&quot; to a minimum.</div><br/></div></div></div></div><div id="42324145" class="c"><input type="checkbox" id="c-42324145" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42323012">parent</a><span>|</span><a href="#42323144">prev</a><span>|</span><a href="#42323276">next</a><span>|</span><label class="collapse" for="c-42324145">[-]</label><label class="expand" for="c-42324145">[3 more]</label></div><br/><div class="children"><div class="content">I disagree with this take, Stallman has expressed it recently by linking some &quot;scientific article&quot;.<p>While I get that LLMs generate text in some way that does not guarantee correctness. There is a correlation between generated text and correctness, which is why millions of people use it...<p>You can judge the correctness of a sentence generated by an LLM. In the same way you can judge the correctness of a human generated sentence.<p>Now whether the truthness or correlation with reality of an LLM sentence can be judged on its own or whether it requires a human to interpret it is not very relevant, as sentences produced by the LLM are still correct most of the time. Just because it is not perfect doesn&#x27;t make the correctness in the other cases useless, albeit perhaps less useful of course.<p>This is nothing surprising of a statistical model, it tends to produce true results.</div><br/><div id="42324776" class="c"><input type="checkbox" id="c-42324776" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#42323012">root</a><span>|</span><a href="#42324145">parent</a><span>|</span><a href="#42323276">next</a><span>|</span><label class="collapse" for="c-42324776">[-]</label><label class="expand" for="c-42324776">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I disagree with this take, Stallman has expressed it recently by linking some &quot;scientific article&quot;.<p>I don&#x27;t know how to parse this. What article did Stallman &quot;link&quot;, and what are you saying Stallman &quot;expressed&quot; by linking&#x2F;using it?<p>&gt; whether the truthness or correlation with reality of an LLM sentence can be judged on its own or whether it requires a human to interpret it is not very relevant<p>It&#x27;s <i>incredibly</i> relevant. We wouldn&#x27;t even be having these debates if complex LLM judgements could always be verified without a human checking the logic.<p>&gt; sentences produced by the LLM are still correct most of the time<p>At least half the problem here is that humans are accustomed to using certain cues as an indirect sign of time-investment, attentiveness, intelligence, truth, etc... and now those cues can be <i>cheaply and quickly counterfeited.</i> It breaks all those old correlations faster than we are adapting.</div><br/><div id="42325053" class="c"><input type="checkbox" id="c-42325053" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42323012">root</a><span>|</span><a href="#42324776">parent</a><span>|</span><a href="#42323276">next</a><span>|</span><label class="collapse" for="c-42325053">[-]</label><label class="expand" for="c-42325053">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;stallman.org&#x2F;chatgpt.html" rel="nofollow">https:&#x2F;&#x2F;stallman.org&#x2F;chatgpt.html</a><p><a href="https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s10676-024-09775-5" rel="nofollow">https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s10676-024-09775-5</a></div><br/></div></div></div></div></div></div><div id="42323276" class="c"><input type="checkbox" id="c-42323276" checked=""/><div class="controls bullet"><span class="by">Nehnehneh</span><span>|</span><a href="#42323012">parent</a><span>|</span><a href="#42324145">prev</a><span>|</span><a href="#42323084">next</a><span>|</span><label class="collapse" for="c-42323276">[-]</label><label class="expand" for="c-42323276">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s just not true.<p>The training data is the underlying truth and that&#x27;s not nothing but a lot.<p>And hallucinations are pathes inside this space which are there for yet unknown reason.<p>We like answers from LLMs which walk through this space reasonable.</div><br/></div></div></div></div><div id="42323084" class="c"><input type="checkbox" id="c-42323084" checked=""/><div class="controls bullet"><span class="by">Loughla</span><span>|</span><a href="#42323012">prev</a><span>|</span><a href="#42322997">next</a><span>|</span><label class="collapse" for="c-42323084">[-]</label><label class="expand" for="c-42323084">[19 more]</label></div><br/><div class="children"><div class="content">I just recently showed a group of college students how and why using AI in school is a bad idea. Telling them it&#x27;s plagiarism doesn&#x27;t have an impact, but showing them how it gets even simple things wrong had a HUGE impact.<p>The first problem was a simple numbers problem. It&#x27;s 2 digit numbers in a series of boxes. You have to add numbers together to make a trail to get from left to right moving only horizontally or vertically. The numbers must add up to 1000 when you get to the exit. For people it takes about 5 minutes to figure out. The AI couldn&#x27;t get it after all 50 students each spent a full 30 minutes changing the prompt to try to get it done. The AI would just randomly add numbers and either add extra at the end to make 1000, or just say the numbers added to 1000 even if it didn&#x27;t.<p>The second problem was writing a basic one paragraph essay with one citation. The humans got it done, when with researching for a source, in about 10 minutes. After an additional 30 minutes none of the students could get AI to produce the paragraph without logic or citation errors. It would either make up fake sources, or would just flat out lie about what the sources said. My favorite was a citation related to dairy farming in an essay that was supposed to be about the dangers of smoking tobacco.<p>This isn&#x27;t necessarily relevant to the article above, but if there are any teachers here, this is something to do with your students to teach them exactly why not to just use AI for their homework.</div><br/><div id="42323137" class="c"><input type="checkbox" id="c-42323137" checked=""/><div class="controls bullet"><span class="by">ryanmcbride</span><span>|</span><a href="#42323084">parent</a><span>|</span><a href="#42323466">next</a><span>|</span><label class="collapse" for="c-42323137">[-]</label><label class="expand" for="c-42323137">[10 more]</label></div><br/><div class="children"><div class="content">My go-to to show people who don&#x27;t understand its limitations used to be the old &quot;how many Ms are there in the word &#x27;minimum&#x27; or something along those lines, but looks like it&#x27;s gotten a bit better at that. I just tried it with GPT4o and it gave me the right number, but the wrong placement. In the past it&#x27;s given it completely wrong:<p>&gt;how many instances of the letter L are in the word parallel<p>The word parallel contains 3 instances of the letter &quot;L&quot;:<p><pre><code>    The first &quot;L&quot; appears as the fourth letter.
    The second &quot;L&quot; appears as the sixth letter.
    The third &quot;L&quot; appears as the seventh letter.</code></pre></div><br/><div id="42326054" class="c"><input type="checkbox" id="c-42326054" checked=""/><div class="controls bullet"><span class="by">sugarkjube</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323137">parent</a><span>|</span><a href="#42323763">next</a><span>|</span><label class="collapse" for="c-42326054">[-]</label><label class="expand" for="c-42326054">[1 more]</label></div><br/><div class="children"><div class="content">Shows nicely what&#x27;s going on.<p>If you ask a human, they will answer 3. Sometimes they say 4. Or 2. That&#x27;s it.<p>An LLM produces a text using an example it was trained on. They were trained with these elaborate responses, so that&#x27;s what they produce.<p>Whenever chatgpt gets something wrong, someone at openai will analyse it, create a few correct examples, and put these on the pile for retraining. Thats why it gets better - not because it is smarter, but it&#x27;s retrained on your specific test cases.</div><br/></div></div><div id="42323763" class="c"><input type="checkbox" id="c-42323763" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323137">parent</a><span>|</span><a href="#42326054">prev</a><span>|</span><a href="#42323215">next</a><span>|</span><label class="collapse" for="c-42323763">[-]</label><label class="expand" for="c-42323763">[3 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s QwQ-32 answering the same question. I&#x27;m including the entire answer here including the CoT:<p>&gt; Let&#x27;s see. The word is &quot;parallel.&quot; I need to count how many times the letter &quot;L&quot; appears in it. Okay, spelling it out: P-A-R-A-L-L-E-L. So, starting from the beginning, first letter is P, not L. Then A, not L. R, not L. Another A, still not L. Then L, that&#x27;s one L. Next letter is L again, so that&#x27;s two L&#x27;s. Then E, not L, and another L, making it three L&#x27;s. Wait, but when I spelled it again, I saw P-A-R-A-L-L-E-L, and I counted three L&#x27;s. But let me check again. Parallel: P-A-R-A-L-L-E-L. Yes, L appears three times. So, the answer is three.<p>&gt; But, maybe there&#x27;s a trick here. Is there any special consideration for the letter L in this word or in general? Sometimes questions like this might be testing for homophones or something, but I don&#x27;t think so in this case. It&#x27;s straightforward: count the L&#x27;s in &quot;parallel.&quot;<p>&gt; Alternatively, perhaps it&#x27;s a trick with the word itself. Is &quot;parallel&quot; spelled differently sometimes? I don&#x27;t think so. It&#x27;s always P-A-R-A-L-L-E-L. So, three L&#x27;s.<p>&gt; Wait a minute, is there a possibility that the word is misspelled in the question? If the word was &quot;paralell,&quot; that would be four L&#x27;s, but that&#x27;s not the correct spelling. So, sticking with three L&#x27;s.<p>&gt; Another angle: is the question referring to uppercase or lowercase L? But in the word &quot;parallel,&quot; all letters are lowercase unless it&#x27;s at the beginning of a sentence, but the question just says &quot;the word parallel,&quot; which is typically in lowercase. So, doesn&#x27;t matter.<p>&gt; Also, considering that &quot;l&quot; and &quot;I&quot; can sometimes be confused, but in this word, it&#x27;s L&#x27;s only, no I&#x27;s.<p>&gt; So, my final answer is three instances of the letter L in the word &quot;parallel.&quot;<p>&gt; Final Answer: 3</div><br/><div id="42326153" class="c"><input type="checkbox" id="c-42326153" checked=""/><div class="controls bullet"><span class="by">Freak_NL</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323763">parent</a><span>|</span><a href="#42324046">next</a><span>|</span><label class="collapse" for="c-42326153">[-]</label><label class="expand" for="c-42326153">[1 more]</label></div><br/><div class="children"><div class="content">This AI is talking as if it is in the final stages of a multi-day interrogation by a very hush-hush and deniable branch of government. I give it another five minutes before it breaks down and will sell out its mother just to be allowed to sleep with the lights off.<p>I hope that this is not something I ever have to deal with. It&#x27;s exhausting.</div><br/></div></div><div id="42324046" class="c"><input type="checkbox" id="c-42324046" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323763">parent</a><span>|</span><a href="#42326153">prev</a><span>|</span><a href="#42323215">next</a><span>|</span><label class="collapse" for="c-42324046">[-]</label><label class="expand" for="c-42324046">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If the word was &quot;paralell,&quot; that would be four L&#x27;s, but that&#x27;s not the correct spelling.<p>Better but this is still a hallucination.</div><br/></div></div></div></div><div id="42323215" class="c"><input type="checkbox" id="c-42323215" checked=""/><div class="controls bullet"><span class="by">lawlessone</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323137">parent</a><span>|</span><a href="#42323763">prev</a><span>|</span><a href="#42323238">next</a><span>|</span><label class="collapse" for="c-42323215">[-]</label><label class="expand" for="c-42323215">[4 more]</label></div><br/><div class="children"><div class="content">They probably have a letter counting tool added to it now. that it just knows to call when asked to do this.<p>you ask it the number of letters and it sends those words off to another tool to count instances of L, but they didn&#x27;t add a placement one so it&#x27;s still guessing those.<p>edit: corrected some typos and phrasing.<p>Maybe we&#x27;ll reach a point where the LLM&#x27;s are just tool calling models and not really giver their own reply.</div><br/><div id="42323384" class="c"><input type="checkbox" id="c-42323384" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323215">parent</a><span>|</span><a href="#42323378">next</a><span>|</span><label class="collapse" for="c-42323384">[-]</label><label class="expand" for="c-42323384">[2 more]</label></div><br/><div class="children"><div class="content">There are only 5 tools it has available to call, and that isn&#x27;t one of them. A GitHub (forgot the url) stays up to date with the latest dumped system instructions.</div><br/><div id="42324816" class="c"><input type="checkbox" id="c-42324816" checked=""/><div class="controls bullet"><span class="by">jrmg</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323384">parent</a><span>|</span><a href="#42323378">next</a><span>|</span><label class="collapse" for="c-42324816">[-]</label><label class="expand" for="c-42324816">[1 more]</label></div><br/><div class="children"><div class="content">How do we know they’re the real system instructions? If they’re determined by interrogating the LLM hallucination is a very real possibility.</div><br/></div></div></div></div><div id="42323378" class="c"><input type="checkbox" id="c-42323378" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323215">parent</a><span>|</span><a href="#42323384">prev</a><span>|</span><a href="#42323238">next</a><span>|</span><label class="collapse" for="c-42323378">[-]</label><label class="expand" for="c-42323378">[1 more]</label></div><br/><div class="children"><div class="content">they probably just forgot to tell it humans are 1 indexed and to do the friendly conversion for them.</div><br/></div></div></div></div><div id="42323238" class="c"><input type="checkbox" id="c-42323238" checked=""/><div class="controls bullet"><span class="by">cruffle_duffle</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323137">parent</a><span>|</span><a href="#42323215">prev</a><span>|</span><a href="#42323466">next</a><span>|</span><label class="collapse" for="c-42323238">[-]</label><label class="expand" for="c-42323238">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if that is simply a reflection of there being more training data out there about this &quot;problem&quot; and the model hoovering all that up and regurgitating it?</div><br/></div></div></div></div><div id="42323466" class="c"><input type="checkbox" id="c-42323466" checked=""/><div class="controls bullet"><span class="by">quuxplusone</span><span>|</span><a href="#42323084">parent</a><span>|</span><a href="#42323137">prev</a><span>|</span><a href="#42323556">next</a><span>|</span><label class="collapse" for="c-42323466">[-]</label><label class="expand" for="c-42323466">[5 more]</label></div><br/><div class="children"><div class="content">Do you have a link to (or can you put here) that &quot;numbers in boxes&quot; problem?</div><br/><div id="42323611" class="c"><input type="checkbox" id="c-42323611" checked=""/><div class="controls bullet"><span class="by">mdaniel</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323466">parent</a><span>|</span><a href="#42323556">next</a><span>|</span><label class="collapse" for="c-42323611">[-]</label><label class="expand" for="c-42323611">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not them, but I think it&#x27;s a variation of the subset-sum problem<p>If we modify the question to be &quot;sum to 100&quot; (to just seriously reduce the number of example boxes required) then given:<p><pre><code>  | 50 | 20 | 24 |
  |  7 |  5 |  1 |
  | 51 | 51 | 51 |
</code></pre>
the solution would be<p><pre><code>  | [50] | [20] | [24] |
  |   7  | [ 5] | [ 1] |
  |  51  | 51   |  51  |

  | right | down  | win
  | X     | right | up
  | X     | X     | X</code></pre></div><br/><div id="42323655" class="c"><input type="checkbox" id="c-42323655" checked=""/><div class="controls bullet"><span class="by">Loughla</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323611">parent</a><span>|</span><a href="#42323556">next</a><span>|</span><label class="collapse" for="c-42323655">[-]</label><label class="expand" for="c-42323655">[3 more]</label></div><br/><div class="children"><div class="content">Correct. That but for 1000. You can build your own with any number of online tools.<p>I don&#x27;t have a link because it&#x27;s part of a lesson plan set behind a payment on teachers pay teachers.</div><br/><div id="42323855" class="c"><input type="checkbox" id="c-42323855" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323655">parent</a><span>|</span><a href="#42323556">next</a><span>|</span><label class="collapse" for="c-42323855">[-]</label><label class="expand" for="c-42323855">[2 more]</label></div><br/><div class="children"><div class="content">Do you have a link to any of those online tools that can be used to generate a puzzle of equivalent complexity to the one that you&#x27;ve tested on?</div><br/><div id="42324054" class="c"><input type="checkbox" id="c-42324054" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323855">parent</a><span>|</span><a href="#42323556">next</a><span>|</span><label class="collapse" for="c-42324054">[-]</label><label class="expand" for="c-42324054">[1 more]</label></div><br/><div class="children"><div class="content">The sum doesn’t even have to be 1000, just create a grid of random numbers, create a random path and find the sum. It’s an equivalent problem.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42323556" class="c"><input type="checkbox" id="c-42323556" checked=""/><div class="controls bullet"><span class="by">empath75</span><span>|</span><a href="#42323084">parent</a><span>|</span><a href="#42323466">prev</a><span>|</span><a href="#42322997">next</a><span>|</span><label class="collapse" for="c-42323556">[-]</label><label class="expand" for="c-42323556">[3 more]</label></div><br/><div class="children"><div class="content">I refuse to believe that you did any of this with any of the latest models.  Gemini and Chat GPT with search are both perfectly capable of producing decent essays with accurate citations.  And the 4o model is extremely good at writing python code that can accurately solve math and logic problems.<p>I asked 4o with search to write an essay about the dangers of smoking, along with citations and quotes from the relevant sources.  NotebookLM is even better if you drop in your sources and don&#x27;t rely on web search.  Whatever you think you know about what AI is capable of, it&#x27;s probably wrong.<p>---
Smoking remains a leading cause of preventable disease and death worldwide, adversely affecting nearly every organ in the human body. The National Cancer Institute (NCI) reports that &quot;cigarette smoking and exposure to tobacco smoke cause about 480,000 premature deaths each year in the United States.&quot;<p>The respiratory system is particularly vulnerable to the detrimental effects of smoking. The American Lung Association (ALA) states that smoking is the primary cause of lung cancer and chronic obstructive pulmonary disease (COPD), which includes emphysema and chronic bronchitis. 
 The inhalation of tobacco smoke introduces carcinogens and toxins that damage lung tissue, leading to reduced lung function and increased susceptibility to infections.<p>Cardiovascular health is also significantly compromised by smoking. The ALA notes that smoking &quot;harms nearly every organ in the body&quot; and is a major cause of coronary heart disease and stroke. 
 The chemicals in tobacco smoke damage blood vessels and the heart, increasing the risk of atherosclerosis and other cardiovascular conditions.<p>Beyond respiratory and cardiovascular diseases, smoking is linked to various cancers, including those of the mouth, throat, esophagus, pancreas, bladder, kidney, cervix, and stomach. The American Cancer Society (ACS) emphasizes that smoking and the use of other tobacco products &quot;harms nearly every organ in your body.&quot; 
 The carcinogens in tobacco smoke cause DNA damage, leading to uncontrolled cell growth and tumor formation.<p>Reproductive health is adversely affected by smoking as well. In women, smoking can lead to reduced fertility, complications during pregnancy, and increased risks of preterm delivery and low birth weight. In men, it can cause erectile dysfunction and reduced sperm quality, affecting fertility.<p>The immune system is not spared from the harmful effects of smoking. The ACS notes that smoking can affect your health in many ways, including &quot;lowered immune system function.&quot; 
 A weakened immune system makes the body more susceptible to infections and diseases.<p>Secondhand smoke poses significant health risks to non-smokers. The ALA reports that secondhand smoke exposure causes more than 41,000 deaths each year. 
 Children exposed to secondhand smoke are more likely to suffer from respiratory infections, asthma, and sudden infant death syndrome (SIDS).<p>Quitting smoking at any age can significantly reduce the risk of developing these diseases and improve overall health. The ACS highlights that &quot;people who quit smoking can also add as much as 10 years to their life, compared to people who continue to smoke.&quot; 
 Resources and support are available to assist individuals in their journey to quit smoking, leading to longer and healthier lives.<p>References<p>American Cancer Society. (n.d.). Health Risks of Smoking Tobacco. Retrieved from <a href="https:&#x2F;&#x2F;www.cancer.org&#x2F;cancer&#x2F;risk-prevention&#x2F;tobacco&#x2F;health-risks-of-tobacco&#x2F;health-risks-of-smoking-tobacco.html" rel="nofollow">https:&#x2F;&#x2F;www.cancer.org&#x2F;cancer&#x2F;risk-prevention&#x2F;tobacco&#x2F;health...</a><p>National Cancer Institute. (n.d.). Harms of Cigarette Smoking and Health Benefits of Quitting. Retrieved from <a href="https:&#x2F;&#x2F;www.cancer.gov&#x2F;about-cancer&#x2F;causes-prevention&#x2F;risk&#x2F;tobacco&#x2F;cessation-fact-sheet" rel="nofollow">https:&#x2F;&#x2F;www.cancer.gov&#x2F;about-cancer&#x2F;causes-prevention&#x2F;risk&#x2F;t...</a><p>American Lung Association. (n.d.). Health Effects of Smoking. Retrieved from <a href="https:&#x2F;&#x2F;www.lung.org&#x2F;quit-smoking&#x2F;smoking-facts&#x2F;health-effects&#x2F;smoking" rel="nofollow">https:&#x2F;&#x2F;www.lung.org&#x2F;quit-smoking&#x2F;smoking-facts&#x2F;health-effec...</a><p>Cleveland Clinic. (2023, April 28). Smoking: Effects, Risks, Diseases, Quitting &amp; Solutions. Retrieved from <a href="https:&#x2F;&#x2F;my.clevelandclinic.org&#x2F;health&#x2F;articles&#x2F;17488-smoking" rel="nofollow">https:&#x2F;&#x2F;my.clevelandclinic.org&#x2F;health&#x2F;articles&#x2F;17488-smoking</a><p>American Cancer Society. (n.d.). Health Risks of Using Tobacco Products. Retrieved from <a href="https:&#x2F;&#x2F;www.cancer.org&#x2F;cancer&#x2F;risk-prevention&#x2F;tobacco&#x2F;health-risks-of-tobacco.html" rel="nofollow">https:&#x2F;&#x2F;www.cancer.org&#x2F;cancer&#x2F;risk-prevention&#x2F;tobacco&#x2F;health...</a><p>American Cancer Society. (n.d.). Health Benefits of Quitting Smoking Over Time. Retrieved from <a href="https:&#x2F;&#x2F;www.cancer.org&#x2F;cancer&#x2F;risk-prevention&#x2F;tobacco&#x2F;benefits-of-quitting-smoking-over-time.html" rel="nofollow">https:&#x2F;&#x2F;www.cancer.org&#x2F;cancer&#x2F;risk-prevention&#x2F;tobacco&#x2F;benefi...</a></div><br/><div id="42326081" class="c"><input type="checkbox" id="c-42326081" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323556">parent</a><span>|</span><a href="#42323677">next</a><span>|</span><label class="collapse" for="c-42326081">[-]</label><label class="expand" for="c-42326081">[1 more]</label></div><br/><div class="children"><div class="content">You exemplify well a big problem with LLMs: When people see accurate enough output on some test question and take it as evidence that they can trust the output to any extent on areas they don&#x27;t dominate.</div><br/></div></div><div id="42323677" class="c"><input type="checkbox" id="c-42323677" checked=""/><div class="controls bullet"><span class="by">Loughla</span><span>|</span><a href="#42323084">root</a><span>|</span><a href="#42323556">parent</a><span>|</span><a href="#42326081">prev</a><span>|</span><a href="#42322997">next</a><span>|</span><label class="collapse" for="c-42323677">[-]</label><label class="expand" for="c-42323677">[1 more]</label></div><br/><div class="children"><div class="content">It was chat gpt, and it was two days ago. Believe it or not, that doesn&#x27;t really change anything for me. I absolutely understand what AI is capable of, and a lot of it is really quite something.<p>But you can&#x27;t trust it to be accurate. You just can&#x27;t. Every model will absolutely make shit up at some point.<p>I liken it to working with a very bright 7 year old. It may sound like it knows what it&#x27;s saying, and it may be able to spit out facts, but it&#x27;s very ignorant about most of the world.</div><br/></div></div></div></div></div></div><div id="42322997" class="c"><input type="checkbox" id="c-42322997" checked=""/><div class="controls bullet"><span class="by">mdaniel</span><span>|</span><a href="#42323084">prev</a><span>|</span><a href="#42324127">next</a><span>|</span><label class="collapse" for="c-42322997">[-]</label><label class="expand" for="c-42322997">[2 more]</label></div><br/><div class="children"><div class="content">the only comment on the prior submission 3 days ago summarizes the whole thing: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42285149">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42285149</a><p>Also, I saw any such blog title as &quot;how to make money in the stock market:&quot; friend, if you knew the answer you wouldn&#x27;t blog about it you&#x27;d be infinitely rich</div><br/><div id="42323275" class="c"><input type="checkbox" id="c-42323275" checked=""/><div class="controls bullet"><span class="by">rtsil</span><span>|</span><a href="#42322997">parent</a><span>|</span><a href="#42324127">next</a><span>|</span><label class="collapse" for="c-42323275">[-]</label><label class="expand" for="c-42323275">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t say how to make big money, don&#x27;t they? Also, the tl;dr is index funds and patience.</div><br/></div></div></div></div><div id="42324127" class="c"><input type="checkbox" id="c-42324127" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42322997">prev</a><span>|</span><a href="#42323608">next</a><span>|</span><label class="collapse" for="c-42324127">[-]</label><label class="expand" for="c-42324127">[1 more]</label></div><br/><div class="children"><div class="content">The debate around &quot;fixing&quot; hallucinations reminds me of the debate around schizophrenia.<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=nEnklxGAmak" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=nEnklxGAmak</a><p>It&#x27;s not a single thing, a specific defect, but rather a failure mode, an absence of cohesive intelligence.<p>Any attempt to fix a non-specific ailment (schizophrenia, death, old age, hallucinations) will run into useless panaceas.</div><br/></div></div><div id="42323608" class="c"><input type="checkbox" id="c-42323608" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#42324127">prev</a><span>|</span><a href="#42323252">next</a><span>|</span><label class="collapse" for="c-42323608">[-]</label><label class="expand" for="c-42323608">[4 more]</label></div><br/><div class="children"><div class="content">Completely misses the fact that a big part of the reason why llms hallucinate sp much is because there&#x27;s a huge innate bias towards producing more tokens over just stopping.</div><br/><div id="42324182" class="c"><input type="checkbox" id="c-42324182" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42323608">parent</a><span>|</span><a href="#42323252">next</a><span>|</span><label class="collapse" for="c-42324182">[-]</label><label class="expand" for="c-42324182">[3 more]</label></div><br/><div class="children"><div class="content">The less tokens produced at inference the lower the quality of the response will be.<p>The process of thinking for an LLM involves the use of words, which is why prompts that ask the LLM to only return the answer will cause lower quality.</div><br/><div id="42325596" class="c"><input type="checkbox" id="c-42325596" checked=""/><div class="controls bullet"><span class="by">ausbah</span><span>|</span><a href="#42323608">root</a><span>|</span><a href="#42324182">parent</a><span>|</span><a href="#42323252">next</a><span>|</span><label class="collapse" for="c-42325596">[-]</label><label class="expand" for="c-42325596">[2 more]</label></div><br/><div class="children"><div class="content">do you know if prompting without regards for length then asking for a summarization of the previous out out works?</div><br/><div id="42326247" class="c"><input type="checkbox" id="c-42326247" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42323608">root</a><span>|</span><a href="#42325596">parent</a><span>|</span><a href="#42323252">next</a><span>|</span><label class="collapse" for="c-42326247">[-]</label><label class="expand" for="c-42326247">[1 more]</label></div><br/><div class="children"><div class="content">It does. I think this was used in a gpt4 version, they called it Chain of Thought.</div><br/></div></div></div></div></div></div></div></div><div id="42323252" class="c"><input type="checkbox" id="c-42323252" checked=""/><div class="controls bullet"><span class="by">PLenz</span><span>|</span><a href="#42323608">prev</a><span>|</span><a href="#42325209">next</a><span>|</span><label class="collapse" for="c-42323252">[-]</label><label class="expand" for="c-42323252">[13 more]</label></div><br/><div class="children"><div class="content">Everything an LLM returns is an hallucination, it&#x27;s just that some of those hallucinations line up with reality</div><br/><div id="42323514" class="c"><input type="checkbox" id="c-42323514" checked=""/><div class="controls bullet"><span class="by">__MatrixMan__</span><span>|</span><a href="#42323252">parent</a><span>|</span><a href="#42324158">next</a><span>|</span><label class="collapse" for="c-42323514">[-]</label><label class="expand" for="c-42323514">[5 more]</label></div><br/><div class="children"><div class="content">There&#x27;s room for splitting hairs in there though.  Even fiction, for instance, can succeed or fail at being internally consistent, is or is not grammatically correct...<p>Calling everything an AI does a hallucination isn&#x27;t incorrect, but it reduces the term to meaninglessness. I&#x27;m not sure that&#x27;s most useful thing we can be doing.<p>Atoms are not indivisible, yet we use the term because it works. I anticipate hallucination will be the same.</div><br/><div id="42326099" class="c"><input type="checkbox" id="c-42326099" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#42323252">root</a><span>|</span><a href="#42323514">parent</a><span>|</span><a href="#42324793">next</a><span>|</span><label class="collapse" for="c-42326099">[-]</label><label class="expand" for="c-42326099">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Atoms are not indivisible<p>They are the smallest unit of a substance that cannot be broken down into smaller units of the same substance. They are, in a sense, indivisible.</div><br/><div id="42326253" class="c"><input type="checkbox" id="c-42326253" checked=""/><div class="controls bullet"><span class="by">Timwi</span><span>|</span><a href="#42323252">root</a><span>|</span><a href="#42326099">parent</a><span>|</span><a href="#42324793">next</a><span>|</span><label class="collapse" for="c-42326253">[-]</label><label class="expand" for="c-42326253">[2 more]</label></div><br/><div class="children"><div class="content">Of course they can. Carbon dioxide consists of quarks and electrons. I can divide it into units smaller than atoms and it&#x27;s still quarks and electrons. All you did was a word trick by assuming a specific meaning of “substance”.</div><br/><div id="42326297" class="c"><input type="checkbox" id="c-42326297" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#42323252">root</a><span>|</span><a href="#42326253">parent</a><span>|</span><a href="#42324793">next</a><span>|</span><label class="collapse" for="c-42326297">[-]</label><label class="expand" for="c-42326297">[1 more]</label></div><br/><div class="children"><div class="content">No word trick. Just pointing out that there&#x27;s some nuance to it.<p>&gt; Carbon dioxide consists of quarks and electrons.<p>But this is just plain wrong. Carbon dioxide consists of carbon dioxide molecules. There is no &quot;carbon-dioxidity&quot; to the quarks and electrons (which are also made of quarks) that the atoms that make the molecules can be broken down into.</div><br/></div></div></div></div></div></div><div id="42324793" class="c"><input type="checkbox" id="c-42324793" checked=""/><div class="controls bullet"><span class="by">Gormo</span><span>|</span><a href="#42323252">root</a><span>|</span><a href="#42323514">parent</a><span>|</span><a href="#42326099">prev</a><span>|</span><a href="#42324158">next</a><span>|</span><label class="collapse" for="c-42324793">[-]</label><label class="expand" for="c-42324793">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Calling everything an AI does a hallucination isn&#x27;t incorrect, but it reduces the term to meaninglessness.<p>I don&#x27;t think it does.  In this case, &quot;hallucination&quot; refers to claims generated entirely within a closed system, but which pertain to a reality external to it.<p>That&#x27;s not meaningless, and makes &quot;hallucinations&quot; distinguishable from claims verified against direct observation of the reality they are meant to represent.</div><br/></div></div></div></div><div id="42324158" class="c"><input type="checkbox" id="c-42324158" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42323252">parent</a><span>|</span><a href="#42323514">prev</a><span>|</span><a href="#42323900">next</a><span>|</span><label class="collapse" for="c-42324158">[-]</label><label class="expand" for="c-42324158">[3 more]</label></div><br/><div class="children"><div class="content">How are you defining hallucination then? In some pretty useless way inevitably.<p>Hallucinations are precisely the generated expressions that don&#x27;t correlate with reality or are not truthful.</div><br/><div id="42324840" class="c"><input type="checkbox" id="c-42324840" checked=""/><div class="controls bullet"><span class="by">Gormo</span><span>|</span><a href="#42323252">root</a><span>|</span><a href="#42324158">parent</a><span>|</span><a href="#42323900">next</a><span>|</span><label class="collapse" for="c-42324840">[-]</label><label class="expand" for="c-42324840">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that definition works: it&#x27;s attempting to categorize statements according to criteria completely external to them rather than according to any inherent property of the statement.<p>A better definition is that a hallucination is an expression that is generated within a closed system without direct input from the reality it is meant to represent.  The point is that an expression about reality that doesn&#x27;t come from observing reality can only be true coincidentally.<p>By way of analogy, if I have a dream about a future event, and then that event actually happens, it was still just a dream and not a clairvoyant vision of the future.  Sure, my dreams are influenced by past experiences I&#x27;ve had (in the same way that verified facts are included in the training data for LLMs), which makes them likely to include things that frequently do happen in real life and might be likely to happen again -- but the dream an the LLM alike are effectively just &quot;remixing&quot; prior input, and not generating any new observations of reality.</div><br/><div id="42325048" class="c"><input type="checkbox" id="c-42325048" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42323252">root</a><span>|</span><a href="#42324840">parent</a><span>|</span><a href="#42323900">next</a><span>|</span><label class="collapse" for="c-42325048">[-]</label><label class="expand" for="c-42325048">[1 more]</label></div><br/><div class="children"><div class="content">&quot;I don&#x27;t think that definition works: it&#x27;s attempting to categorize statements according to criteria completely external to them rather than according to any inherent property of the statement.&quot;<p>Correct. The basic concept of truth in logic relies on an objective reality, an expression a priori holds truth even in the absence or indistinct of such a reality. But the truthfulness or correctness of a posteriori statements can depend on the reality.  Examples of the former would be &quot;If A is B, then B is C. A is B, then B is C&quot; Example of the latter would be &quot;It is raining outside.&quot;<p>&quot;A better definition is that a hallucination is an expression that is generated within a closed system without direct input from the reality it is meant to represent. The point is that an expression about reality that doesn&#x27;t come from observing reality can only be true coincidentally.&quot;<p>Absolutely incorrect, you are talking about a concept of the state of the art of science and tech but you are failing basic philosophy and epistemology concepts.  The LLM has inputs from the reality (is it possible not to?), it is trained on a huge corpus of text written by humans that themselves perceive reality. The perception of reality can be indirect. We can measure something by observing it, or by observing an instrument that in turn observes it.<p>&quot;but the dream an the LLM alike are effectively just &quot;remixing&quot; prior input, and not generating any new observations of reality.&quot;<p>Again incorrect for three reasons:<p>1- Novel observations can occur purely from remixing. Einstein locked himself during a pandemic and developed the theory of relativity without additional experimental output.<p>2- LLMs combine their existing data with human input, which is an external source.<p>3- LLMs can interact with other sources of data whether by injection of data into the prompt, by function calling, RAG, etc..<p>So yeah. Try to go back to basics and study simpler systems, ideally with source code. This might be out of your league.</div><br/></div></div></div></div></div></div><div id="42323900" class="c"><input type="checkbox" id="c-42323900" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#42323252">parent</a><span>|</span><a href="#42324158">prev</a><span>|</span><a href="#42323266">next</a><span>|</span><label class="collapse" for="c-42323900">[-]</label><label class="expand" for="c-42323900">[2 more]</label></div><br/><div class="children"><div class="content">Subjectively, we operate the same way.</div><br/><div id="42326032" class="c"><input type="checkbox" id="c-42326032" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#42323252">root</a><span>|</span><a href="#42323900">parent</a><span>|</span><a href="#42323266">next</a><span>|</span><label class="collapse" for="c-42326032">[-]</label><label class="expand" for="c-42326032">[1 more]</label></div><br/><div class="children"><div class="content">Look at things and fix it then.</div><br/></div></div></div></div><div id="42323266" class="c"><input type="checkbox" id="c-42323266" checked=""/><div class="controls bullet"><span class="by">mountainriver</span><span>|</span><a href="#42323252">parent</a><span>|</span><a href="#42323900">prev</a><span>|</span><a href="#42325209">next</a><span>|</span><label class="collapse" for="c-42323266">[-]</label><label class="expand" for="c-42323266">[2 more]</label></div><br/><div class="children"><div class="content">The same is true for humans</div><br/><div id="42326070" class="c"><input type="checkbox" id="c-42326070" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#42323252">root</a><span>|</span><a href="#42323266">parent</a><span>|</span><a href="#42325209">next</a><span>|</span><label class="collapse" for="c-42326070">[-]</label><label class="expand" for="c-42326070">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a good thing that, as you state, humans hold &quot;togetherness&quot; as a &quot;true&quot; value.<p>But in this context, the value is in how much they have pondered to actually see and evaluate what is eventually seen as &quot;true&quot;.</div><br/></div></div></div></div></div></div><div id="42325209" class="c"><input type="checkbox" id="c-42325209" checked=""/><div class="controls bullet"><span class="by">madiator</span><span>|</span><a href="#42323252">prev</a><span>|</span><a href="#42323746">next</a><span>|</span><label class="collapse" for="c-42325209">[-]</label><label class="expand" for="c-42325209">[1 more]</label></div><br/><div class="children"><div class="content">For the specific form of hallucination, which is called grounded factuality, we have trained a pretty good model that can detect if a claim is supported by a context. This is super useful for RAG.
More info at <a href="https:&#x2F;&#x2F;bespokelabs.ai&#x2F;bespoke-minicheck" rel="nofollow">https:&#x2F;&#x2F;bespokelabs.ai&#x2F;bespoke-minicheck</a>.</div><br/></div></div><div id="42323746" class="c"><input type="checkbox" id="c-42323746" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#42325209">prev</a><span>|</span><a href="#42323375">next</a><span>|</span><label class="collapse" for="c-42323746">[-]</label><label class="expand" for="c-42323746">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been playing with Qwen&#x27;s QwQ-32b, and watching this thing&#x27;s chain of thought is really interesting. In particular, it&#x27;s pretty good at catching its own mistakes, and at the same time, gives off a &quot;feeling&quot; of someone very uncertain about themselves, trying to verify their answer again and again. Which seems to be the main reason why it can correctly solve puzzles that some much larger models fail. You can still see it occasionally hallucinate things in the CoT, but they are usually quickly caught and discarded.<p>The only downsides of this approach is that it requires a lot of tokens before the model can ascertain the correctness of its answer, and also that sometimes it just gives up and concludes that the puzzle is unsolvable (although that second part can be mitigated by adding something like &quot;There is definitely a solution, keep trying until you solve it&quot; to the prompt).</div><br/><div id="42326242" class="c"><input type="checkbox" id="c-42326242" checked=""/><div class="controls bullet"><span class="by">emil_sorensen</span><span>|</span><a href="#42323746">parent</a><span>|</span><a href="#42323375">next</a><span>|</span><label class="collapse" for="c-42326242">[-]</label><label class="expand" for="c-42326242">[1 more]</label></div><br/><div class="children"><div class="content">I find it so interesting that it&#x27;s possible to develop a &quot;feeling&quot; of a new model.</div><br/></div></div></div></div><div id="42323375" class="c"><input type="checkbox" id="c-42323375" checked=""/><div class="controls bullet"><span class="by">tshadley</span><span>|</span><a href="#42323746">prev</a><span>|</span><a href="#42324922">next</a><span>|</span><label class="collapse" for="c-42323375">[-]</label><label class="expand" for="c-42323375">[3 more]</label></div><br/><div class="children"><div class="content">The article referenced the Oxford semantic entropy study but failed to clarify that the issue greatly simplifies LLM hallucination (making most of the article outdated).<p>When we are not sure of an answer we have two choices: say the first thing that comes to mind (like an LLM), or say &quot;I&#x27;m not sure&quot;.<p>LLMs aren&#x27;t easily trained to say &quot;I&#x27;m not sure&quot; because that requires additional reasoning and introspection (which is why CoT models do better); hence hallucinations occur when training data is vague.<p>So why not just measure uncertainty in the tokens themselves?  Because there are many ways to say the same thing, so a high entropy answer may only reflect uncertainty in synonyms-- many ways to say the same thing.<p>The paper referenced works to eliminate semantic similarity from entropy measurements, leaving much more useful results, proving that hallucination is conceptually a simple problem.<p><a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-024-07421-0" rel="nofollow">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-024-07421-0</a></div><br/><div id="42323887" class="c"><input type="checkbox" id="c-42323887" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#42323375">parent</a><span>|</span><a href="#42323702">next</a><span>|</span><label class="collapse" for="c-42323887">[-]</label><label class="expand" for="c-42323887">[1 more]</label></div><br/><div class="children"><div class="content">QwQ is really good at saying &quot;I&#x27;m not sure&quot;, to the point where it will sometimes check the correct and obviously trivial answer a dozen times before concluding that it is, indeed, correct. And it does punch way above its weight for its size.<p>So, basically, the answer seems to be to give models extreme anxiety and doubt in their own abilities.</div><br/></div></div><div id="42323702" class="c"><input type="checkbox" id="c-42323702" checked=""/><div class="controls bullet"><span class="by">sfink</span><span>|</span><a href="#42323375">parent</a><span>|</span><a href="#42323887">prev</a><span>|</span><a href="#42324922">next</a><span>|</span><label class="collapse" for="c-42323702">[-]</label><label class="expand" for="c-42323702">[1 more]</label></div><br/><div class="children"><div class="content">&gt; proving that hallucination is conceptually a simple problem.<p>...proving that this one particular piece of the hallucination problem may be conceptually simple.<p>FTFY</div><br/></div></div></div></div><div id="42324922" class="c"><input type="checkbox" id="c-42324922" checked=""/><div class="controls bullet"><span class="by">fsckboy</span><span>|</span><a href="#42323375">prev</a><span>|</span><a href="#42325090">next</a><span>|</span><label class="collapse" for="c-42324922">[-]</label><label class="expand" for="c-42324922">[2 more]</label></div><br/><div class="children"><div class="content">it&#x27;s superficially counterintuitive to people that an AI that will sometimes spit out verbatim copies of written texts, also will just make other things up. It&#x27;s like &quot;choose one, please&quot;.<p>MetaAI makes up stuff reliably. You&#x27;d think it would be an ace at baseball stats for example, but &quot;what teams did so-and-so play for&quot;, you absolutely must check the results yourself.</div><br/><div id="42326201" class="c"><input type="checkbox" id="c-42326201" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#42324922">parent</a><span>|</span><a href="#42325090">next</a><span>|</span><label class="collapse" for="c-42326201">[-]</label><label class="expand" for="c-42326201">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;counterintuitive&quot;<p>It is consistent with the topic that the reply would be &quot;Tell them that sequences of words that were verbatim in a past input have high probability, and gaps in sequences compete in probability&quot;. Which fixes intuition, as duly. In fact, things are not supposed to reply through intuition, but through vetted intuition (and &quot;vetted mature intuition&quot;, in a loop).<p>&gt; <i>you absolutely must check the results yourself</i><p>So, consistently with the above, things are supposed to reply through a sort of &quot;&quot;&quot;RAG&quot;&quot;&quot; of the vetted (dynamically built through iterations of checks).</div><br/></div></div></div></div><div id="42325090" class="c"><input type="checkbox" id="c-42325090" checked=""/><div class="controls bullet"><span class="by">mwkaufma</span><span>|</span><a href="#42324922">prev</a><span>|</span><a href="#42323735">next</a><span>|</span><label class="collapse" for="c-42325090">[-]</label><label class="expand" for="c-42325090">[1 more]</label></div><br/><div class="children"><div class="content">How do we discriminate when a response is correct, vs. when it&#x27;s &quot;hallucinating&quot; an accurate fact, by coincidence? Are all responses hallucinations, independent of correspondence to ground-truth?</div><br/></div></div><div id="42323735" class="c"><input type="checkbox" id="c-42323735" checked=""/><div class="controls bullet"><span class="by">chefandy</span><span>|</span><a href="#42325090">prev</a><span>|</span><a href="#42323201">next</a><span>|</span><label class="collapse" for="c-42323735">[-]</label><label class="expand" for="c-42323735">[1 more]</label></div><br/><div class="children"><div class="content">Lots of folks in these conversations fail to distinguish between LLMs as a technology and &quot;AI Chatbots&quot; as commercial question answering services. Whether false information was expected or not matters to LLM product developers, but in the context of a commercial question-answering tool, it&#x27;s irrelevant. Hallucinations are bugs that creates= time-wasting zero-value output, at best, and downright harmful output at worst. If you&#x27;re selling people <i>LLM pattern generator output</i>, they should expect a lot of bullshit. If you&#x27;re selling people answers to questions, they should expect accurate answers to their questions. If paying users are <i>really</i> expected to assume every answer is bullshit and vet it themselves, that should probably move from the little print to the big print because a lot of people clearly don&#x27;t get it.</div><br/></div></div><div id="42323201" class="c"><input type="checkbox" id="c-42323201" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#42323735">prev</a><span>|</span><a href="#42324039">next</a><span>|</span><label class="collapse" for="c-42323201">[-]</label><label class="expand" for="c-42323201">[1 more]</label></div><br/><div class="children"><div class="content">Maybe don&#x27;t make things up in a blog post about LLMs making things up.<p>Because you don&#x27;t know how to fix it. Only how to mitigate it.</div><br/></div></div><div id="42324039" class="c"><input type="checkbox" id="c-42324039" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#42323201">prev</a><span>|</span><a href="#42323951">next</a><span>|</span><label class="collapse" for="c-42324039">[-]</label><label class="expand" for="c-42324039">[1 more]</label></div><br/><div class="children"><div class="content">Wow, a whole article that didn&#x27;t mention the word &quot;sampler&quot; once. There&#x27;s pretty strong evidence coming out that truncation samplers like min_p and entropix are strictly superior to previous samplers (which everyone uses like top_p) to prevent hallucinations and that LLMs usually &quot;know&quot; when they are &quot;hallucinating&quot; based on their logprobs.<p><a href="https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=FBkpCyujtS" rel="nofollow">https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=FBkpCyujtS</a> (min_p sampling, note extremely high review scores)<p><a href="https:&#x2F;&#x2F;github.com&#x2F;xjdr-alt&#x2F;entropix">https:&#x2F;&#x2F;github.com&#x2F;xjdr-alt&#x2F;entropix</a> (Entropix)<p><a href="https:&#x2F;&#x2F;artefact2.github.io&#x2F;llm-sampling&#x2F;index.xhtml" rel="nofollow">https:&#x2F;&#x2F;artefact2.github.io&#x2F;llm-sampling&#x2F;index.xhtml</a></div><br/></div></div><div id="42323951" class="c"><input type="checkbox" id="c-42323951" checked=""/><div class="controls bullet"><span class="by">prollyjethi</span><span>|</span><a href="#42324039">prev</a><span>|</span><a href="#42323269">next</a><span>|</span><label class="collapse" for="c-42323951">[-]</label><label class="expand" for="c-42323951">[2 more]</label></div><br/><div class="children"><div class="content">I am honestly very skeptical of articles like these. Hallucinations are a feature of LLMs. The only ways to &quot;FIX&quot; it is to either stop using LLMs. Or use a super bias some how.</div><br/><div id="42324019" class="c"><input type="checkbox" id="c-42324019" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#42323951">parent</a><span>|</span><a href="#42323269">next</a><span>|</span><label class="collapse" for="c-42324019">[-]</label><label class="expand" for="c-42324019">[1 more]</label></div><br/><div class="children"><div class="content">You should be. I don’t know anything about kepa.ai but before even clicking the article I assume they’re trying to sell me something. And “how to fix it” makes me think this is some kind of SEO written for people who think it can be fixed, meaning the article is written for robots and amateurs.</div><br/></div></div></div></div><div id="42323269" class="c"><input type="checkbox" id="c-42323269" checked=""/><div class="controls bullet"><span class="by">sollewitt</span><span>|</span><a href="#42323951">prev</a><span>|</span><a href="#42323681">next</a><span>|</span><label class="collapse" for="c-42323269">[-]</label><label class="expand" for="c-42323269">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Why LLMs do the one and only thing they do (and how to fix it)&quot;</div><br/></div></div><div id="42323681" class="c"><input type="checkbox" id="c-42323681" checked=""/><div class="controls bullet"><span class="by">pfisch</span><span>|</span><a href="#42323269">prev</a><span>|</span><a href="#42323638">next</a><span>|</span><label class="collapse" for="c-42323681">[-]</label><label class="expand" for="c-42323681">[1 more]</label></div><br/><div class="children"><div class="content">Anyone who has raised a child knows they hallucinate constantly when they are young because they are just doing probabilistic output of things they have heard people say in similar situations and saying words they don&#x27;t actually understand.<p>LLMs likely have a similar problem.</div><br/></div></div><div id="42323638" class="c"><input type="checkbox" id="c-42323638" checked=""/><div class="controls bullet"><span class="by">Mistletoe</span><span>|</span><a href="#42323681">prev</a><span>|</span><a href="#42323401">next</a><span>|</span><label class="collapse" for="c-42323638">[-]</label><label class="expand" for="c-42323638">[9 more]</label></div><br/><div class="children"><div class="content">Is there a way to code an LLM to just say &quot;I don&#x27;t know&quot; when it is uncertain or reaching some sort of edge?</div><br/><div id="42323869" class="c"><input type="checkbox" id="c-42323869" checked=""/><div class="controls bullet"><span class="by">gerdesj</span><span>|</span><a href="#42323638">parent</a><span>|</span><a href="#42323896">next</a><span>|</span><label class="collapse" for="c-42323869">[-]</label><label class="expand" for="c-42323869">[6 more]</label></div><br/><div class="children"><div class="content">&quot;It&quot; does not know when it does not know.  A LLM is a funny old beast that basically outputs words one after another based on probabilities.  There is no reasoning as we would know it involved.<p>However, I&#x27;ll tentatively allow that you do get a sort of &quot;emergent behaviour&quot; from them.  You do seem to get some form of intelligent output from a prompt but correctness is not built in, nor is any sort of reasoning.<p>The examples around here of how to trip up a LLM are cool.  There&#x27;s: &quot;How many letter &quot;m&quot;s in the word minimum&quot; howler which is probably optimised for by now and hence held up as a counterpoint by a fan.  The one about boxes adding up to 1000 will leave a relative of mine for lost for ever but they can still walk and catch a ball, negotiate stairs and recall facts from 50 years ago with clarity.<p>Intelligence is a slippery concept to even define, let alone ask what an artificial one might look like.  LLMs are a part of the puzzle and certainly not a solution.<p>You mention the word &quot;edge&quot; and I suppose you might be riffing on how neurons seem to work.  LLMs don&#x27;t have a sort of trigger threshold, they simply output the most likely answers based on their input.<p>If you keep your model tightly ie domain focussed and curate all of the input then you have more chance of avoiding &quot;hallucinations&quot; than if you don&#x27;t.  Trying to cover the entirety of everything is Quixotic nonsense.<p>Garbage in; garbage out.</div><br/><div id="42324193" class="c"><input type="checkbox" id="c-42324193" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42323638">root</a><span>|</span><a href="#42323869">parent</a><span>|</span><a href="#42323896">next</a><span>|</span><label class="collapse" for="c-42324193">[-]</label><label class="expand" for="c-42324193">[5 more]</label></div><br/><div class="children"><div class="content">&quot;It&quot; does not know when it does not know.<p>But it does know when it has uncertainty.<p>In the chatgpt api this is logprobs, each generated token has a level of uncertainty, so:<p>&quot;2+2=&quot;<p>The next token is with almost 100% certainty 4.<p>&quot;Today I am feeling&quot;<p>The next token will be very uncertain, it might be &quot;happy&quot;, it might be &quot;sad&quot;, it might be all sorts of things.</div><br/><div id="42324327" class="c"><input type="checkbox" id="c-42324327" checked=""/><div class="controls bullet"><span class="by">gerdesj</span><span>|</span><a href="#42323638">root</a><span>|</span><a href="#42324193">parent</a><span>|</span><a href="#42323896">next</a><span>|</span><label class="collapse" for="c-42324327">[-]</label><label class="expand" for="c-42324327">[4 more]</label></div><br/><div class="children"><div class="content">&quot;The next token is with almost 100% certainty 4.&quot;<p>By using the word &quot;almost&quot; with regards 2 + 2 = 4, you have not exactly dispelled LLM &quot;nonsense&quot;.<p>A human (with a modicum of maths knowledge) will know that 2 + 2 = 4 (pure integers - a fact by assertion).  A maths worrier will get slightly uncomfortable about 2.0 + 2.0 = 4.0 unless they are ensured that decimal places and accuracy are the same thing and a few other things.<p>A LLM will almost certainly &quot;know&quot; something that is certain, if its training set is conclusive about that.  However, it does not know why and if enough of the training set is suitably ambiguous then it (LLM) will drift off course and seem to spout bollocks - &quot;hallucinate&quot;.</div><br/><div id="42325013" class="c"><input type="checkbox" id="c-42325013" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42323638">root</a><span>|</span><a href="#42324327">parent</a><span>|</span><a href="#42323896">next</a><span>|</span><label class="collapse" for="c-42325013">[-]</label><label class="expand" for="c-42325013">[3 more]</label></div><br/><div class="children"><div class="content">You might be in the wrong thread. This is merely a comment about whether LLMs hold a concept of uncertainty, they do.<p>Also, the next token might be 2 and the next token might be ², the next token could also have been x, these are all valid statements and the LLM might have been uncertain because of them.<p>2+2=4<p>2+2=x<p>2+2=2x<p>2+2=2x2<p>2+2=2²<p>Are all valid statements.</div><br/><div id="42325218" class="c"><input type="checkbox" id="c-42325218" checked=""/><div class="controls bullet"><span class="by">glandium</span><span>|</span><a href="#42323638">root</a><span>|</span><a href="#42325013">parent</a><span>|</span><a href="#42323896">next</a><span>|</span><label class="collapse" for="c-42325218">[-]</label><label class="expand" for="c-42325218">[2 more]</label></div><br/><div class="children"><div class="content">And somewhere in its training data, you can be sure there&#x27;s also 2+2=5.</div><br/><div id="42325344" class="c"><input type="checkbox" id="c-42325344" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#42323638">root</a><span>|</span><a href="#42325218">parent</a><span>|</span><a href="#42323896">next</a><span>|</span><label class="collapse" for="c-42325344">[-]</label><label class="expand" for="c-42325344">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but most likely it&#x27;s marked as false or incorrect through fine tuning or some form of reinforcement.<p>The idea that the logprobs of any token is proportional to the amount of times it comes up in training data is not true.<p>For example, suppose that A is a common misconception and is repeated often in Reddit, but B appears in scholarly textbooks and papers, and higher reputation data sources. Then through reinforcement the logprobs of B can increase, and they can increase consistently when surrounded by contexts like &quot;This is true&quot; and conversely decrease in contexts of &quot;this is not true&quot;.<p>So the presumptions and values of its trainers are also embedded into the LLM in addition to those of the authors of the text corpus.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="42323896" class="c"><input type="checkbox" id="c-42323896" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#42323638">parent</a><span>|</span><a href="#42323869">prev</a><span>|</span><a href="#42323736">next</a><span>|</span><label class="collapse" for="c-42323896">[-]</label><label class="expand" for="c-42323896">[1 more]</label></div><br/><div class="children"><div class="content">It is. The new crop of models specifically trained to do CoT (as opposed to just forcing them into it via prompting) is partly about forcing them to continue to think and not just hastily conclude that they have the answer.</div><br/></div></div><div id="42323736" class="c"><input type="checkbox" id="c-42323736" checked=""/><div class="controls bullet"><span class="by">sfink</span><span>|</span><a href="#42323638">parent</a><span>|</span><a href="#42323896">prev</a><span>|</span><a href="#42323401">next</a><span>|</span><label class="collapse" for="c-42323736">[-]</label><label class="expand" for="c-42323736">[1 more]</label></div><br/><div class="children"><div class="content">If it works properly, it would need to say that it doesn&#x27;t know that it doesn&#x27;t know, and then where are you?<p>(Short answer is yes, but it only works for a limited set of things, and that set can be expanded with effort but will always remain limited.)</div><br/></div></div></div></div></div></div></div></div></div></body></html>