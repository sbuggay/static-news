<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1705136466852" as="style"/><link rel="stylesheet" href="styles.css?v=1705136466852"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/derrickburns/generalized-kmeans-clustering">Generalized K-Means Clustering</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>derrickrburns</span> | <span>73 comments</span></div><br/><div><div id="38976773" class="c"><input type="checkbox" id="c-38976773" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38976255">next</a><span>|</span><label class="collapse" for="c-38976773">[-]</label><label class="expand" for="c-38976773">[25 more]</label></div><br/><div class="children"><div class="content">I built a pipeline to automatically cluster and visualize large amounts of text documents in a completely unsupervised manner:<p>- Embed all the text documents.<p>- Project to 2D using UMAP which also creates its own emergent &quot;clusters&quot;.<p>- Use k-means clustering with a high cluster count depending on dataset size.<p>- Feed the ChatGPT API ~10 examples from each cluster and ask it to provide a concise label for the cluster.<p>- Bonus: Use DBSCAN to identify arbitrary subclusters within each cluster.<p>It is <i>extremely</i> effective and I have a theoetical implementation of a more practical use case to use said UMAP dimensionality reduction for better inference. There is evidence that current popular text embedding models (e.g. OpenAI ada, which outputs 1536D embeddings) are <i>way</i> too big for most use cases and could be giving poorly specified results for embedding similarity as a result, in addition to higher costs for the entire pipeline.</div><br/><div id="38977592" class="c"><input type="checkbox" id="c-38977592" checked=""/><div class="controls bullet"><span class="by">pseudonom-</span><span>|</span><a href="#38976773">parent</a><span>|</span><a href="#38977327">next</a><span>|</span><label class="collapse" for="c-38977592">[-]</label><label class="expand" for="c-38977592">[1 more]</label></div><br/><div class="children"><div class="content">Funny, I did almost the exact same thing: <a href="https:&#x2F;&#x2F;github.com&#x2F;colehaus&#x2F;hammock-public">https:&#x2F;&#x2F;github.com&#x2F;colehaus&#x2F;hammock-public</a>. Though I project to 3D and then put them in an interactive 3D plot. The other fun little thing the interactive plotting enables is stepping through a variety of clustering granularities.</div><br/></div></div><div id="38977327" class="c"><input type="checkbox" id="c-38977327" checked=""/><div class="controls bullet"><span class="by">dleeftink</span><span>|</span><a href="#38976773">parent</a><span>|</span><a href="#38977592">prev</a><span>|</span><a href="#38977004">next</a><span>|</span><label class="collapse" for="c-38977327">[-]</label><label class="expand" for="c-38977327">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing. I &#x27;d like to know what the (re)compute time might be when adding, say, another million documents using this pipeline. The cluster embedding approach in my view, while streamlined, still adds a (sometimes significant) timebump when high throughput is required.<p>I see some significant speedups can be achieved when discretising dimensions into buckets, and doing a simple frequency count of associated buckets -- leaving only highly related buckets per document. These &#x27;signatures&#x27; can then be indexed LSH style and a graph construed from documents with similar hashes.<p>When the input set is sufficiently large, this graph contains &#x27;natural&#x27; clusters, without any UMAP or k-means parameter tuning required. When implemented in BQ, I achieve sub minute performance for 5-10 million documents, from indexing to clustering.</div><br/></div></div><div id="38977004" class="c"><input type="checkbox" id="c-38977004" checked=""/><div class="controls bullet"><span class="by">sonium</span><span>|</span><a href="#38976773">parent</a><span>|</span><a href="#38977327">prev</a><span>|</span><a href="#38977718">next</a><span>|</span><label class="collapse" for="c-38977004">[-]</label><label class="expand" for="c-38977004">[3 more]</label></div><br/><div class="children"><div class="content">I did something similar (but not for documents) but I’m struggling with selecting the optimal number of clusters.</div><br/><div id="38977674" class="c"><input type="checkbox" id="c-38977674" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#38976773">root</a><span>|</span><a href="#38977004">parent</a><span>|</span><a href="#38977429">next</a><span>|</span><label class="collapse" for="c-38977674">[-]</label><label class="expand" for="c-38977674">[1 more]</label></div><br/><div class="children"><div class="content">Cluster stability is a good heuristic that should be more well-known:<p>For a given k:<p><pre><code>  for n=30 or 100 or 300 trials:
    subsample 80% of the points
    cluster them
    compute Fowlkes-Mallow score (available in sklearn) of the subset to the original, restricting only to the instances in the subset (otherwise you can&#x27;t compute it)
  output the average f-m score
</code></pre>
This essentially measure how &quot;stable&quot; the clusters are. The Fowlkes-Mallow score decreases when instances pop over to other clusters in the subset.<p>If you do this and plot the average score versus k, you&#x27;ll see a sharp dropoff at some point. That&#x27;s the maximal plausible k.<p>edit: Here&#x27;s code<p><pre><code>  def stability(Z, k):
    kmeans = KMeans(n_clusters=k, n_init=&quot;auto&quot;)
    kmeans.fit(Z)
    scores = []
    for i in range(100):
        # Randomly select 80% of the data, with replacement
        # TODO: without
        idx = np.random.choice(Z.shape[0], int(Z.shape[0]*0.8))
        kmeans2 = KMeans(n_clusters=k, n_init=&quot;auto&quot;)
        kmeans2.fit(Z[idx])

        # Compare the two clusterings
        score = fowlkes_mallows_score(kmeans.labels_[idx], kmeans2.labels_)
        scores.append(score)
    scores = np.array(scores) 
    return np.mean(scores), np.std(scores)</code></pre></div><br/></div></div><div id="38977429" class="c"><input type="checkbox" id="c-38977429" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#38976773">root</a><span>|</span><a href="#38977004">parent</a><span>|</span><a href="#38977674">prev</a><span>|</span><a href="#38977718">next</a><span>|</span><label class="collapse" for="c-38977429">[-]</label><label class="expand" for="c-38977429">[1 more]</label></div><br/><div class="children"><div class="content">Checkout hdbscan</div><br/></div></div></div></div><div id="38977718" class="c"><input type="checkbox" id="c-38977718" checked=""/><div class="controls bullet"><span class="by">bart_spoon</span><span>|</span><a href="#38976773">parent</a><span>|</span><a href="#38977004">prev</a><span>|</span><a href="#38976856">next</a><span>|</span><label class="collapse" for="c-38977718">[-]</label><label class="expand" for="c-38977718">[4 more]</label></div><br/><div class="children"><div class="content">When doing DBSCAN on the subclusters, do you cluster on the 2-D projected space? Do you use the original 2-D projection you used prior to k-means, or does each subcluster get its own UMAP projection?</div><br/><div id="38977748" class="c"><input type="checkbox" id="c-38977748" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38976773">root</a><span>|</span><a href="#38977718">parent</a><span>|</span><a href="#38976856">next</a><span>|</span><label class="collapse" for="c-38977748">[-]</label><label class="expand" for="c-38977748">[3 more]</label></div><br/><div class="children"><div class="content">I DBSCAN in the 2D projected space.<p>These aren&#x27;t visualized: I use identified clusters to look at manually to find trends.</div><br/><div id="38977906" class="c"><input type="checkbox" id="c-38977906" checked=""/><div class="controls bullet"><span class="by">appplication</span><span>|</span><a href="#38976773">root</a><span>|</span><a href="#38977748">parent</a><span>|</span><a href="#38976856">next</a><span>|</span><label class="collapse" for="c-38977906">[-]</label><label class="expand" for="c-38977906">[2 more]</label></div><br/><div class="children"><div class="content">Is it possible to dbscan on the unprojected space or does that lead to poor effectiveness? Also what led you to choose dbscan vs another technique?</div><br/><div id="38977939" class="c"><input type="checkbox" id="c-38977939" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38976773">root</a><span>|</span><a href="#38977906">parent</a><span>|</span><a href="#38976856">next</a><span>|</span><label class="collapse" for="c-38977939">[-]</label><label class="expand" for="c-38977939">[1 more]</label></div><br/><div class="children"><div class="content">Poor effectiveness. (again another hint why working in high dimensional space may not be ideal)<p>I was not aware of a robust clustering technique that&#x27;s better&#x2F;as easy to use other than DBSCAN.</div><br/></div></div></div></div></div></div></div></div><div id="38976856" class="c"><input type="checkbox" id="c-38976856" checked=""/><div class="controls bullet"><span class="by">sshumaker</span><span>|</span><a href="#38976773">parent</a><span>|</span><a href="#38977718">prev</a><span>|</span><a href="#38977923">next</a><span>|</span><label class="collapse" for="c-38976856">[-]</label><label class="expand" for="c-38976856">[1 more]</label></div><br/><div class="children"><div class="content">You can also look at Bertopic which has this functionality as an open source library:<p><a href="https:&#x2F;&#x2F;maartengr.github.io&#x2F;BERTopic&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;maartengr.github.io&#x2F;BERTopic&#x2F;index.html</a></div><br/></div></div><div id="38977923" class="c"><input type="checkbox" id="c-38977923" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38976773">parent</a><span>|</span><a href="#38976856">prev</a><span>|</span><a href="#38976805">next</a><span>|</span><label class="collapse" for="c-38977923">[-]</label><label class="expand" for="c-38977923">[1 more]</label></div><br/><div class="children"><div class="content">cluster naming was still an open problem pre-LLM</div><br/></div></div><div id="38976805" class="c"><input type="checkbox" id="c-38976805" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#38976773">parent</a><span>|</span><a href="#38977923">prev</a><span>|</span><a href="#38976818">next</a><span>|</span><label class="collapse" for="c-38976805">[-]</label><label class="expand" for="c-38976805">[2 more]</label></div><br/><div class="children"><div class="content">Interesting. What do you use the visualization for? Looking at trends in the documents?</div><br/><div id="38976825" class="c"><input type="checkbox" id="c-38976825" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38976773">root</a><span>|</span><a href="#38976805">parent</a><span>|</span><a href="#38976818">next</a><span>|</span><label class="collapse" for="c-38976825">[-]</label><label class="expand" for="c-38976825">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s say you want to look at a large dataset of user-submitted reviews for you app. User reviews are written extremely idiosyncratic so all traditional NLP methods will likely fail.<p>With the pipeline mentioned, it&#x27;s much easier to look at cluster density to identify patterns and high-level trends.</div><br/></div></div></div></div><div id="38976818" class="c"><input type="checkbox" id="c-38976818" checked=""/><div class="controls bullet"><span class="by">jncfhnb</span><span>|</span><a href="#38976773">parent</a><span>|</span><a href="#38976805">prev</a><span>|</span><a href="#38977057">next</a><span>|</span><label class="collapse" for="c-38976818">[-]</label><label class="expand" for="c-38976818">[4 more]</label></div><br/><div class="children"><div class="content">Why not just use DBSCAN though</div><br/><div id="38976844" class="c"><input type="checkbox" id="c-38976844" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38976773">root</a><span>|</span><a href="#38976818">parent</a><span>|</span><a href="#38977057">next</a><span>|</span><label class="collapse" for="c-38976844">[-]</label><label class="expand" for="c-38976844">[3 more]</label></div><br/><div class="children"><div class="content">You can use DBSCAN instead of k-means, but DBSCAN has a worst-case memory complexity of O(n^2) so things can get spicy with large datasets, which is why I opt it to only use it for subclusters. k-means also fixes the number of clusters, which is good for visualization sanity.<p><a href="https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;generated&#x2F;sklearn.cluster.DBSCAN.html" rel="nofollow">https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;generated&#x2F;sklearn.cl...</a></div><br/><div id="38977700" class="c"><input type="checkbox" id="c-38977700" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#38976773">root</a><span>|</span><a href="#38976844">parent</a><span>|</span><a href="#38977057">next</a><span>|</span><label class="collapse" for="c-38977700">[-]</label><label class="expand" for="c-38977700">[2 more]</label></div><br/><div class="children"><div class="content">Isn’t the embedding step much slower than clustering? How many documents are you dealing with?<p>For I news aggregator I worked on I disregarded k-means because you have to know the number of clusters in advance, and I think it will cluster every document, which is bad for the actual outliers in a dataset.<p>Agglomerative clustering yielded the best results for us. HDBSCAN was promising but doing weird things with some docs.</div><br/><div id="38977873" class="c"><input type="checkbox" id="c-38977873" checked=""/><div class="controls bullet"><span class="by">whakim</span><span>|</span><a href="#38976773">root</a><span>|</span><a href="#38977700">parent</a><span>|</span><a href="#38977057">next</a><span>|</span><label class="collapse" for="c-38977873">[-]</label><label class="expand" for="c-38977873">[1 more]</label></div><br/><div class="children"><div class="content">The embedding step is certainly slower than clustering, but the memory requirements blow up pretty fast when you&#x27;re doing density-based clustering on a dataset of even, say, 100k embeddings.</div><br/></div></div></div></div></div></div></div></div><div id="38977057" class="c"><input type="checkbox" id="c-38977057" checked=""/><div class="controls bullet"><span class="by">mr_mitm</span><span>|</span><a href="#38976773">parent</a><span>|</span><a href="#38976818">prev</a><span>|</span><a href="#38976865">next</a><span>|</span><label class="collapse" for="c-38977057">[-]</label><label class="expand" for="c-38977057">[3 more]</label></div><br/><div class="children"><div class="content">Which libraries are you using, in particular for the first step?</div><br/><div id="38977097" class="c"><input type="checkbox" id="c-38977097" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38976773">root</a><span>|</span><a href="#38977057">parent</a><span>|</span><a href="#38976865">next</a><span>|</span><label class="collapse" for="c-38977097">[-]</label><label class="expand" for="c-38977097">[2 more]</label></div><br/><div class="children"><div class="content">Embeddings is just SentenceTransformers: <a href="https:&#x2F;&#x2F;www.sbert.net&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.sbert.net&#x2F;</a><p>I used the bge-large-en-v1.5 model (<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;BAAI&#x2F;bge-large-en-v1.5" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;BAAI&#x2F;bge-large-en-v1.5</a>) because I could, but the common all-MiniLM-L6-v2 model is sufficient. The trick is to batch generate the embeddings on a GPU, which SentenceTransformers mostly does by default.<p>Other libraries are the typical ones (umap for UMAP, scikit-learn for k-means&#x2F;DBSCAN, chatgpt-python for ChatGPT interfacing, plotly for viz, pandas for some ETL). You don&#x27;t need to use a bespoke AI&#x2F;ML package for these workflows and they aren&#x27;t too complicated.</div><br/><div id="38977119" class="c"><input type="checkbox" id="c-38977119" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38976773">root</a><span>|</span><a href="#38977097">parent</a><span>|</span><a href="#38976865">next</a><span>|</span><label class="collapse" for="c-38977119">[-]</label><label class="expand" for="c-38977119">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just SentenceTransformers, but: <i>the wrong model is common because no one read SentenceTransformers</i>. MiniLM-L6-V2 is for symmetric search (target document has same wording as source document) MiniLM-L6-V3 is for asymmetric search (target document is likely to contain material matching query in source document)</div><br/></div></div></div></div></div></div><div id="38976865" class="c"><input type="checkbox" id="c-38976865" checked=""/><div class="controls bullet"><span class="by">mike_ivanov</span><span>|</span><a href="#38976773">parent</a><span>|</span><a href="#38977057">prev</a><span>|</span><a href="#38976255">next</a><span>|</span><label class="collapse" for="c-38976865">[-]</label><label class="expand" for="c-38976865">[4 more]</label></div><br/><div class="children"><div class="content">Why 2D? (edit: just the vis or there is some other reason?)</div><br/><div id="38976894" class="c"><input type="checkbox" id="c-38976894" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38976773">root</a><span>|</span><a href="#38976865">parent</a><span>|</span><a href="#38976255">next</a><span>|</span><label class="collapse" for="c-38976894">[-]</label><label class="expand" for="c-38976894">[3 more]</label></div><br/><div class="children"><div class="content">Both the viz, and that the 2D UMAP projection is actually enough to get accurately delineated topics.<p>Hence why I think the typical embedding dimensionality is way way too high.</div><br/><div id="38977550" class="c"><input type="checkbox" id="c-38977550" checked=""/><div class="controls bullet"><span class="by">markisus</span><span>|</span><a href="#38976773">root</a><span>|</span><a href="#38976894">parent</a><span>|</span><a href="#38976255">next</a><span>|</span><label class="collapse" for="c-38977550">[-]</label><label class="expand" for="c-38977550">[2 more]</label></div><br/><div class="children"><div class="content">Do you think 1D could work? Maybe topic-space is some sort of tree-shaped structure where documents live in the thin strands.</div><br/><div id="38977945" class="c"><input type="checkbox" id="c-38977945" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38976773">root</a><span>|</span><a href="#38977550">parent</a><span>|</span><a href="#38976255">next</a><span>|</span><label class="collapse" for="c-38977945">[-]</label><label class="expand" for="c-38977945">[1 more]</label></div><br/><div class="children"><div class="content">1D could work on certain datasets but it wouldn&#x27;t be ideal.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38976255" class="c"><input type="checkbox" id="c-38976255" checked=""/><div class="controls bullet"><span class="by">derrickrburns</span><span>|</span><a href="#38976773">prev</a><span>|</span><a href="#38977110">next</a><span>|</span><label class="collapse" for="c-38976255">[-]</label><label class="expand" for="c-38976255">[2 more]</label></div><br/><div class="children"><div class="content">AI has sparked new interest in high dimensional embeddings for approximate nearest neighbor search.  Here is a highly scalable, implementation of a companion technique, k-means clustering that uses Spark 1.1 written in Scala.<p>Please let me know if you fork this library and update it to the latter versions of Spark.</div><br/><div id="38977689" class="c"><input type="checkbox" id="c-38977689" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#38976255">parent</a><span>|</span><a href="#38977110">next</a><span>|</span><label class="collapse" for="c-38977689">[-]</label><label class="expand" for="c-38977689">[1 more]</label></div><br/><div class="children"><div class="content">Just curious, have you actually profiled this against running on a single large-memory machine?</div><br/></div></div></div></div><div id="38977110" class="c"><input type="checkbox" id="c-38977110" checked=""/><div class="controls bullet"><span class="by">zoogeny</span><span>|</span><a href="#38976255">prev</a><span>|</span><a href="#38976513">next</a><span>|</span><label class="collapse" for="c-38977110">[-]</label><label class="expand" for="c-38977110">[1 more]</label></div><br/><div class="children"><div class="content">There is a Twitch streamer Tsoding who posted a video of himself implementing K-means clustering in C recently [1]. He also does a follow up 3d visualization of the algorithm in progress using raylib [2].<p>1. <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=kH-hqG34ylA&amp;t=4788s&amp;ab_channel=TsodingDaily" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=kH-hqG34ylA&amp;t=4788s&amp;ab_chann...</a><p>2. <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=K7hWqxC_7Mw&amp;ab_channel=TsodingDaily" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=K7hWqxC_7Mw&amp;ab_channel=Tsodi...</a></div><br/></div></div><div id="38976513" class="c"><input type="checkbox" id="c-38976513" checked=""/><div class="controls bullet"><span class="by">staticautomatic</span><span>|</span><a href="#38977110">prev</a><span>|</span><a href="#38977485">next</a><span>|</span><label class="collapse" for="c-38976513">[-]</label><label class="expand" for="c-38976513">[30 more]</label></div><br/><div class="children"><div class="content">What are people using k-means for? I can count on one hand the number of times I’ve had a good <i>a priori</i> rationale for the value of k.</div><br/><div id="38977325" class="c"><input type="checkbox" id="c-38977325" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38977884">next</a><span>|</span><label class="collapse" for="c-38977325">[-]</label><label class="expand" for="c-38977325">[1 more]</label></div><br/><div class="children"><div class="content">The kmeans metric is exactly the metric you would want to optimize the performance of an algorithm like [bolt](<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1706.10283" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1706.10283</a>). In that and other discretization routines, the value of k is a parameter related to compression ratio, efficiency, and other metrics more predictable than some ethereal notion of how many clusters the data &quot;naturally&quot; has.</div><br/></div></div><div id="38977884" class="c"><input type="checkbox" id="c-38977884" checked=""/><div class="controls bullet"><span class="by">patcon</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38977325">prev</a><span>|</span><a href="#38976965">next</a><span>|</span><label class="collapse" for="c-38977884">[-]</label><label class="expand" for="c-38977884">[1 more]</label></div><br/><div class="children"><div class="content">Polis (and Twitter&#x27;s community notes, I believe)<p>Participation At Scale Can Repair The Public Square
<a href="https:&#x2F;&#x2F;www.noemamag.com&#x2F;participation-at-scale-can-repair-the-public-square&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.noemamag.com&#x2F;participation-at-scale-can-repair-t...</a><p>Polis: Scaling deliberation by mapping high dimensional opinion spaces
<a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;scholar?q=Polis:+Scaling+deliberation+by+mapping+high+dimensional+opinion+spaces&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart" rel="nofollow">https:&#x2F;&#x2F;scholar.google.com&#x2F;scholar?q=Polis:+Scaling+delibera...</a><p>Restricting clustering to 2-5 groups impacts group aware&#x2F;informed consensus and comment routing
<a href="https:&#x2F;&#x2F;github.com&#x2F;compdemocracy&#x2F;polis&#x2F;issues&#x2F;1289">https:&#x2F;&#x2F;github.com&#x2F;compdemocracy&#x2F;polis&#x2F;issues&#x2F;1289</a></div><br/></div></div><div id="38976965" class="c"><input type="checkbox" id="c-38976965" checked=""/><div class="controls bullet"><span class="by">web9ed</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38977884">prev</a><span>|</span><a href="#38977083">next</a><span>|</span><label class="collapse" for="c-38976965">[-]</label><label class="expand" for="c-38976965">[1 more]</label></div><br/><div class="children"><div class="content">I would recommend checking out DBSCAN as it is similar without having to provide a number k
<a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;DBSCAN" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;DBSCAN</a></div><br/></div></div><div id="38977083" class="c"><input type="checkbox" id="c-38977083" checked=""/><div class="controls bullet"><span class="by">Konnstann</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38976965">prev</a><span>|</span><a href="#38977554">next</a><span>|</span><label class="collapse" for="c-38977083">[-]</label><label class="expand" for="c-38977083">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve used something similar for tissue segmentation from hyperspectral images of animals where I know there should be K different tissue types I care about.</div><br/></div></div><div id="38977554" class="c"><input type="checkbox" id="c-38977554" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38977083">prev</a><span>|</span><a href="#38976677">next</a><span>|</span><label class="collapse" for="c-38977554">[-]</label><label class="expand" for="c-38977554">[1 more]</label></div><br/><div class="children"><div class="content">I implemented an algorithm which used k-means to reduce noise in a path tracer.<p>For each pixel instead of a single color value it generated k mean color values, using an online algorithm. These were then combined to produce the final pixel color.<p>The idea was that a pixel might have several distinct contributions (ie from different light sources for example), but due to the random sampling used in path tracing the variance of sample values is usually large.<p>The value k then was chosen based on scene complexity. There was also a memory trade-off of course, as memory usage was linear in k.</div><br/></div></div><div id="38976677" class="c"><input type="checkbox" id="c-38976677" checked=""/><div class="controls bullet"><span class="by">sshumaker</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38977554">prev</a><span>|</span><a href="#38976642">next</a><span>|</span><label class="collapse" for="c-38976677">[-]</label><label class="expand" for="c-38976677">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes you can use a heuristic to estimate K, or use a variant that terminates at some distance threshold.<p>That said, something like hdbscan doesn’t suffer from this problem.</div><br/></div></div><div id="38976642" class="c"><input type="checkbox" id="c-38976642" checked=""/><div class="controls bullet"><span class="by">derwiki</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38976677">prev</a><span>|</span><a href="#38976928">next</a><span>|</span><label class="collapse" for="c-38976642">[-]</label><label class="expand" for="c-38976642">[3 more]</label></div><br/><div class="children"><div class="content">Used it in college to downscale an X color image to Y number of colors. Sure, Photoshop does it, but it was informative to do it manually.</div><br/><div id="38976728" class="c"><input type="checkbox" id="c-38976728" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38976513">root</a><span>|</span><a href="#38976642">parent</a><span>|</span><a href="#38976928">next</a><span>|</span><label class="collapse" for="c-38976728">[-]</label><label class="expand" for="c-38976728">[2 more]</label></div><br/><div class="children"><div class="content">Google&#x27;s Material You uses this to initiate color theming<p>(n.b. Celebi&#x27;s,  note usage of Lab &#x2F; notHSL, respect Cartesian &#x2F; polar nature of inputs &#x2F; outputs, and ask for high-K, 128 is what we went with but it&#x27;s arbitrary. Can get away with as few as 32 if you&#x27;re ex. Doing brand color from favicon)</div><br/><div id="38976882" class="c"><input type="checkbox" id="c-38976882" checked=""/><div class="controls bullet"><span class="by">girvo</span><span>|</span><a href="#38976513">root</a><span>|</span><a href="#38976728">parent</a><span>|</span><a href="#38976928">next</a><span>|</span><label class="collapse" for="c-38976882">[-]</label><label class="expand" for="c-38976882">[1 more]</label></div><br/><div class="children"><div class="content">Ooh this is a much nicer approach than the kind of brute force approach we took at work for theme gen</div><br/></div></div></div></div></div></div><div id="38976928" class="c"><input type="checkbox" id="c-38976928" checked=""/><div class="controls bullet"><span class="by">dhosek</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38976642">prev</a><span>|</span><a href="#38976669">next</a><span>|</span><label class="collapse" for="c-38976928">[-]</label><label class="expand" for="c-38976928">[1 more]</label></div><br/><div class="children"><div class="content">I did a modified version of this once for a map of auto dealerships, although rather than working with a fixed <i>k</i>, I used a fix threshold for cluster distance. The algorithm I was working with had <i>O</i>(<i>n</i>³) complexity so to keep the pregeneration of clusters manageable, I partitioned data by state. The other fun part was finding the right metric formula for measuring distances. Because clusters needed to correspond to the rectangular view window on the map, rather than a standard Euclidean distance, I used <i>d</i> = max(Δ<i>x</i>,Δ<i>y</i>) which gives square neighborhoods rather than round ones.</div><br/></div></div><div id="38976669" class="c"><input type="checkbox" id="c-38976669" checked=""/><div class="controls bullet"><span class="by">bagels</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38976928">prev</a><span>|</span><a href="#38976723">next</a><span>|</span><label class="collapse" for="c-38976669">[-]</label><label class="expand" for="c-38976669">[1 more]</label></div><br/><div class="children"><div class="content">Measuring multiple physical objects with the same sensor, you can use k-means to separate the measurements from each object, given that you know how many objects are being sensed. I can&#x27;t get more specific than that.</div><br/></div></div><div id="38976723" class="c"><input type="checkbox" id="c-38976723" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38976669">prev</a><span>|</span><a href="#38976758">next</a><span>|</span><label class="collapse" for="c-38976723">[-]</label><label class="expand" for="c-38976723">[1 more]</label></div><br/><div class="children"><div class="content">k-means is good for <i>fast</i> unsupervised clustering on an unknown low-dimensional dataset. It&#x27;s helpful for EDA.<p>If you want accuracy at an order of magnitude more compute, you can use something like DBSCAN.</div><br/></div></div><div id="38976758" class="c"><input type="checkbox" id="c-38976758" checked=""/><div class="controls bullet"><span class="by">icelancer</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38976723">prev</a><span>|</span><a href="#38977200">next</a><span>|</span><label class="collapse" for="c-38976758">[-]</label><label class="expand" for="c-38976758">[1 more]</label></div><br/><div class="children"><div class="content">Baseball pitch types based on physics profiles.</div><br/></div></div><div id="38977200" class="c"><input type="checkbox" id="c-38977200" checked=""/><div class="controls bullet"><span class="by">missingrib</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38976758">prev</a><span>|</span><a href="#38976522">next</a><span>|</span><label class="collapse" for="c-38977200">[-]</label><label class="expand" for="c-38977200">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve used it for identifying dominant colors in images.</div><br/></div></div><div id="38976522" class="c"><input type="checkbox" id="c-38976522" checked=""/><div class="controls bullet"><span class="by">trc001</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38977200">prev</a><span>|</span><a href="#38976610">next</a><span>|</span><label class="collapse" for="c-38976522">[-]</label><label class="expand" for="c-38976522">[1 more]</label></div><br/><div class="children"><div class="content">Real estate price estimates would be the classic, and frankly still common, example</div><br/></div></div><div id="38976610" class="c"><input type="checkbox" id="c-38976610" checked=""/><div class="controls bullet"><span class="by">theophrastus</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38976522">prev</a><span>|</span><a href="#38977818">next</a><span>|</span><label class="collapse" for="c-38976610">[-]</label><label class="expand" for="c-38976610">[1 more]</label></div><br/><div class="children"><div class="content">Additional small molecule pharmaceutical candidates via molecular descriptors</div><br/></div></div><div id="38977818" class="c"><input type="checkbox" id="c-38977818" checked=""/><div class="controls bullet"><span class="by">tomnipotent</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38976610">prev</a><span>|</span><a href="#38976628">next</a><span>|</span><label class="collapse" for="c-38977818">[-]</label><label class="expand" for="c-38977818">[1 more]</label></div><br/><div class="children"><div class="content">Frequently used in e-commerce, such as RFM clustering for targeted marketing.</div><br/></div></div><div id="38976628" class="c"><input type="checkbox" id="c-38976628" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38977818">prev</a><span>|</span><a href="#38976939">next</a><span>|</span><label class="collapse" for="c-38976628">[-]</label><label class="expand" for="c-38976628">[1 more]</label></div><br/><div class="children"><div class="content">data segmentation (e.g Voronoi), grouping (like search query results), anomaly detection. lots of different things</div><br/></div></div><div id="38976939" class="c"><input type="checkbox" id="c-38976939" checked=""/><div class="controls bullet"><span class="by">calvinmorrison</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38976628">prev</a><span>|</span><a href="#38976656">next</a><span>|</span><label class="collapse" for="c-38976939">[-]</label><label class="expand" for="c-38976939">[1 more]</label></div><br/><div class="children"><div class="content">We used k means clustering on a project used to track fruit fly memory and learning behaviors<p><a href="http:&#x2F;&#x2F;git.ceux.org&#x2F;FlyTracking.git&#x2F;" rel="nofollow">http:&#x2F;&#x2F;git.ceux.org&#x2F;FlyTracking.git&#x2F;</a></div><br/></div></div><div id="38976656" class="c"><input type="checkbox" id="c-38976656" checked=""/><div class="controls bullet"><span class="by">Waterluvian</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38976939">prev</a><span>|</span><a href="#38976639">next</a><span>|</span><label class="collapse" for="c-38976656">[-]</label><label class="expand" for="c-38976656">[1 more]</label></div><br/><div class="children"><div class="content">Constantly in remote sensing and GIS work.</div><br/></div></div><div id="38976639" class="c"><input type="checkbox" id="c-38976639" checked=""/><div class="controls bullet"><span class="by">brianleb</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38976656">prev</a><span>|</span><a href="#38976814">next</a><span>|</span><label class="collapse" for="c-38976639">[-]</label><label class="expand" for="c-38976639">[8 more]</label></div><br/><div class="children"><div class="content">So after re-reading your comment a few times, I am left with this thought: either you don&#x27;t understand what k-means clustering is, or I don&#x27;t understand what k-means clustering is. I wouldn&#x27;t describe myself as a machine learning expert, but I have taken some grad level classes in statistics, analytics, and methods like this&#x2F;related to this.<p>So my question is... could you elaborate?</div><br/><div id="38976670" class="c"><input type="checkbox" id="c-38976670" checked=""/><div class="controls bullet"><span class="by">dilippkumar</span><span>|</span><a href="#38976513">root</a><span>|</span><a href="#38976639">parent</a><span>|</span><a href="#38976848">next</a><span>|</span><label class="collapse" for="c-38976670">[-]</label><label class="expand" for="c-38976670">[2 more]</label></div><br/><div class="children"><div class="content">Not GP, but I understood their question as follows.<p>Assume you collect some kindergartners and top NBA players into a room and collect their heights. Now say you pass these to two hapless grad students and ask them to
perform K-means clustering.<p>Suppose one of the grad students knew the composition of the people you measured and can guess these height should clump into 2 nice clusters. The other student who doesn&#x27;t know the composition of the class - what should they guess K to be?<p>I understood the GP&#x27;s comment to refer to the state of the second grad student. How useful is K-means clustering without knowing K in advance?</div><br/><div id="38978201" class="c"><input type="checkbox" id="c-38978201" checked=""/><div class="controls bullet"><span class="by">bjornbsm</span><span>|</span><a href="#38976513">root</a><span>|</span><a href="#38976670">parent</a><span>|</span><a href="#38976848">next</a><span>|</span><label class="collapse" for="c-38978201">[-]</label><label class="expand" for="c-38978201">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I understood the GP&#x27;s comment to refer to the state of the second grad student. How useful is K-means clustering without knowing K in advance?<p>There are several heuristics for this. Googling I see that the elbow method, average sillhouette method and gap statistic method is the most used.<p>I think you could play around with your own heuristics as well. Simple KDE plots showing the amount of peaks. Maybe, say the variance between clusters should be greater than the variance inside any cluster could maybe work. (Edit: this seems to be the main point of the average sillhouette method).</div><br/></div></div></div></div><div id="38976848" class="c"><input type="checkbox" id="c-38976848" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#38976513">root</a><span>|</span><a href="#38976639">parent</a><span>|</span><a href="#38976670">prev</a><span>|</span><a href="#38977020">next</a><span>|</span><label class="collapse" for="c-38976848">[-]</label><label class="expand" for="c-38976848">[1 more]</label></div><br/><div class="children"><div class="content">The first problem is picking k. The second problem is the definition of distance or, equivalently, the uniformity of the space. Naively using the Euclidean distance in an embedding where similarity is non-uniform leads to bad outcomes. This problem is solved by learning a uniform embedding, and this is much harder than running k-means.<p>k-means assumes these hard parts of the problem are taken care of and offers a trivial solution to the rest. Thanks for the help, I&#x27;ll cluster it myself.</div><br/></div></div><div id="38976660" class="c"><input type="checkbox" id="c-38976660" checked=""/><div class="controls bullet"><span class="by">bagels</span><span>|</span><a href="#38976513">root</a><span>|</span><a href="#38976639">parent</a><span>|</span><a href="#38977020">prev</a><span>|</span><a href="#38976814">next</a><span>|</span><label class="collapse" for="c-38976660">[-]</label><label class="expand" for="c-38976660">[3 more]</label></div><br/><div class="children"><div class="content">You have to choose the number of clusters, before using k-means.<p>Imagine that you have a dataset, where you think there are likely meaningful clusters, but you don&#x27;t know how many, especially where it&#x27;s many-dimensioned.<p>If you pick a k that is too small, you lump unrelated points together.<p>If k is too large, your meaningful clusters will be fragmented&#x2F;overfitted.<p>There are some algorithms that try to estimate the number of clusters or try to find the k with the best fit to the data to make up for this.</div><br/><div id="38976739" class="c"><input type="checkbox" id="c-38976739" checked=""/><div class="controls bullet"><span class="by">keenmaster</span><span>|</span><a href="#38976513">root</a><span>|</span><a href="#38976660">parent</a><span>|</span><a href="#38976814">next</a><span>|</span><label class="collapse" for="c-38976739">[-]</label><label class="expand" for="c-38976739">[2 more]</label></div><br/><div class="children"><div class="content">Couldn’t you make some educated guesses and then stop when you arrive at a K that gives you meaningful clusters that are neither too high level nor too atomized.</div><br/><div id="38977455" class="c"><input type="checkbox" id="c-38977455" checked=""/><div class="controls bullet"><span class="by">yarg</span><span>|</span><a href="#38976513">root</a><span>|</span><a href="#38976739">parent</a><span>|</span><a href="#38976814">next</a><span>|</span><label class="collapse" for="c-38977455">[-]</label><label class="expand" for="c-38977455">[1 more]</label></div><br/><div class="children"><div class="content">Probably not the best in terms of efficiency.<p>Easier just to deliberately overshoot (with a too high k) and then merge any clusters with too much overlap.</div><br/></div></div></div></div></div></div></div></div><div id="38976814" class="c"><input type="checkbox" id="c-38976814" checked=""/><div class="controls bullet"><span class="by">jncfhnb</span><span>|</span><a href="#38976513">parent</a><span>|</span><a href="#38976639">prev</a><span>|</span><a href="#38977485">next</a><span>|</span><label class="collapse" for="c-38976814">[-]</label><label class="expand" for="c-38976814">[1 more]</label></div><br/><div class="children"><div class="content">K is 3.<p>It’s honestly fine for just finding key differences like a principal component for light storytelling. They don’t need to be distinct clusters</div><br/></div></div></div></div><div id="38977485" class="c"><input type="checkbox" id="c-38977485" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#38976513">prev</a><span>|</span><a href="#38977163">next</a><span>|</span><label class="collapse" for="c-38977485">[-]</label><label class="expand" for="c-38977485">[3 more]</label></div><br/><div class="children"><div class="content">Although K-means clustering is often the correct approach given time crunch and code complexity constraints, I don&#x27;t like how it&#x27;s hard to extend and how it&#x27;s not principled. By not principled, I mean that it feels more like an algorithm (that happens to optimize) rather than an explicit optimization with an explicit loss function. And I found that in practice, modifying the distance function to anything more interesting doesn&#x27;t work.</div><br/><div id="38977591" class="c"><input type="checkbox" id="c-38977591" checked=""/><div class="controls bullet"><span class="by">blackbear_</span><span>|</span><a href="#38977485">parent</a><span>|</span><a href="#38977927">next</a><span>|</span><label class="collapse" for="c-38977591">[-]</label><label class="expand" for="c-38977591">[1 more]</label></div><br/><div class="children"><div class="content">K-means clustering is very well principled actually as an instance of the expectation maximization algorithm with &quot;hard&quot; cluster assignment. Turns out it&#x27;s just good old maximum likelihood:<p><a href="https:&#x2F;&#x2F;alliance.seas.upenn.edu&#x2F;~cis520&#x2F;dynamic&#x2F;2022&#x2F;wiki&#x2F;index.php?n=Lectures.EM" rel="nofollow">https:&#x2F;&#x2F;alliance.seas.upenn.edu&#x2F;~cis520&#x2F;dynamic&#x2F;2022&#x2F;wiki&#x2F;in...</a></div><br/></div></div><div id="38977927" class="c"><input type="checkbox" id="c-38977927" checked=""/><div class="controls bullet"><span class="by">selimthegrim</span><span>|</span><a href="#38977485">parent</a><span>|</span><a href="#38977591">prev</a><span>|</span><a href="#38977163">next</a><span>|</span><label class="collapse" for="c-38977927">[-]</label><label class="expand" for="c-38977927">[1 more]</label></div><br/><div class="children"><div class="content">Isn’t it also formally equivalent to a Gaussian mixture model?<p><a href="https:&#x2F;&#x2F;timydaley.github.io&#x2F;kmeans_gmm&#x2F;gmm_vs_kmeans.html" rel="nofollow">https:&#x2F;&#x2F;timydaley.github.io&#x2F;kmeans_gmm&#x2F;gmm_vs_kmeans.html</a></div><br/></div></div></div></div><div id="38977163" class="c"><input type="checkbox" id="c-38977163" checked=""/><div class="controls bullet"><span class="by">namuol</span><span>|</span><a href="#38977485">prev</a><span>|</span><a href="#38977466">next</a><span>|</span><label class="collapse" for="c-38977163">[-]</label><label class="expand" for="c-38977163">[1 more]</label></div><br/><div class="children"><div class="content">Here’s a very simple toy demonstration of how K-Means works that I made for fun years ago while studying machine learning: <a href="https:&#x2F;&#x2F;k-means.stackblitz.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;k-means.stackblitz.io&#x2F;</a><p>Essentially K-Means is a way of “learning” categories or other kinds of groupings within an unlabeled dataset, without any fancy deep learning. It’s handy for its simplicity and speed.<p>The demo works with simple 2D coordinates for illustrative purposes but the technique works with any number of dimensions.<p>Note that there may be some things I got wrong with the implementation and that there are other variations of the algorithm surely, but it still captures the basic idea well enough for an intro.</div><br/></div></div><div id="38977466" class="c"><input type="checkbox" id="c-38977466" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#38977163">prev</a><span>|</span><a href="#38977145">next</a><span>|</span><label class="collapse" for="c-38977466">[-]</label><label class="expand" for="c-38977466">[2 more]</label></div><br/><div class="children"><div class="content">Check out sampling with lightweight coresets if your data is big - it&#x27;s a principled approach with theoretical guarantees, and it&#x27;s only a couple of lines of numpy. Do check if the assumptions hold for your data though, as they are stronger than with regular coresets.</div><br/><div id="38978168" class="c"><input type="checkbox" id="c-38978168" checked=""/><div class="controls bullet"><span class="by">tomtom1337</span><span>|</span><a href="#38977466">parent</a><span>|</span><a href="#38977145">next</a><span>|</span><label class="collapse" for="c-38978168">[-]</label><label class="expand" for="c-38978168">[1 more]</label></div><br/><div class="children"><div class="content">Do you have a link to any implementations for this?</div><br/></div></div></div></div><div id="38977145" class="c"><input type="checkbox" id="c-38977145" checked=""/><div class="controls bullet"><span class="by">atum47</span><span>|</span><a href="#38977466">prev</a><span>|</span><a href="#38977449">next</a><span>|</span><label class="collapse" for="c-38977145">[-]</label><label class="expand" for="c-38977145">[1 more]</label></div><br/><div class="children"><div class="content">I remember when i first learned k-means, it opened the door for so many projects. Two that are on my GitHub to this day are a python script that groups your images by similarly (histogram) and one that classify your expenses based on previous data. I had so much fun working on those.</div><br/></div></div><div id="38977449" class="c"><input type="checkbox" id="c-38977449" checked=""/><div class="controls bullet"><span class="by">anArbitraryOne</span><span>|</span><a href="#38977145">prev</a><span>|</span><a href="#38977459">next</a><span>|</span><label class="collapse" for="c-38977449">[-]</label><label class="expand" for="c-38977449">[2 more]</label></div><br/><div class="children"><div class="content">Fun fact: K-Means is the least interesting clustering algorithm known to humans, but is quite fast and therefore useful in certain applications</div><br/><div id="38978036" class="c"><input type="checkbox" id="c-38978036" checked=""/><div class="controls bullet"><span class="by">snovv_crash</span><span>|</span><a href="#38977449">parent</a><span>|</span><a href="#38977459">next</a><span>|</span><label class="collapse" for="c-38978036">[-]</label><label class="expand" for="c-38978036">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s boring, in a sense that it always gives reasonable results and is easy to implement. It also scales well in N, D and K, and from my experience converges in just a few iterations from anything better than a pure random initialisation strategy.<p>IMO it is very good as a final clustering algorithm once you&#x27;ve already applied some more complex transformations on your data to linearise it and account for deeper knowledge of the problem. This might be a spectral space transformation (you care about connectedness), or an embedding (you care about whatever the network was trained on) or descriptor (you care about the algorithm&#x27;s similarity).<p>But once you&#x27;ve applied the transform, you then have a minimalist fast scalable clustering that just does clustering and doesn&#x27;t need to know anything more about the problem being solved. Very unix-y feeling.</div><br/></div></div></div></div><div id="38977459" class="c"><input type="checkbox" id="c-38977459" checked=""/><div class="controls bullet"><span class="by">fiddlerwoaroof</span><span>|</span><a href="#38977449">prev</a><span>|</span><a href="#38976824">next</a><span>|</span><label class="collapse" for="c-38977459">[-]</label><label class="expand" for="c-38977459">[1 more]</label></div><br/><div class="children"><div class="content">Does the “curse of dimensionality” affect the usefulness of k-means?</div><br/></div></div><div id="38976824" class="c"><input type="checkbox" id="c-38976824" checked=""/><div class="controls bullet"><span class="by">milofeynman</span><span>|</span><a href="#38977459">prev</a><span>|</span><a href="#38977117">next</a><span>|</span><label class="collapse" for="c-38976824">[-]</label><label class="expand" for="c-38976824">[2 more]</label></div><br/><div class="children"><div class="content">I applied to a certain scraping fintech in the Bay Area around 5 years ago and was asked to open the Wikipedia page to k-means squared clustering and implement the algorithm with tests from scratch. I was applying for an android position. I still laugh thinking about how they paid to fly me out and ask such a stupid interview question.</div><br/><div id="38977441" class="c"><input type="checkbox" id="c-38977441" checked=""/><div class="controls bullet"><span class="by">alluro2</span><span>|</span><a href="#38976824">parent</a><span>|</span><a href="#38977117">next</a><span>|</span><label class="collapse" for="c-38977441">[-]</label><label class="expand" for="c-38977441">[1 more]</label></div><br/><div class="children"><div class="content">I see how it might not have anything to do with usual Android development, but why do you consider it a stupid question?<p>K-means is not that complicated and naive implementation with e.g. Euclidean distance is a couple of dozens of lines of code, so should be practical enough for an interview.</div><br/></div></div></div></div></div></div></div></div></div></body></html>