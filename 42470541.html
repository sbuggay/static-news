<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1734771666194" as="style"/><link rel="stylesheet" href="styles.css?v=1734771666194"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.anthropic.com/research/building-effective-agents">Building Effective &quot;Agents&quot;</a>Â <span class="domain">(<a href="https://www.anthropic.com">www.anthropic.com</a>)</span></div><div class="subtext"><span>jascha_eng</span> | <span>58 comments</span></div><br/><div><div id="42475700" class="c"><input type="checkbox" id="c-42475700" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42478237">next</a><span>|</span><label class="collapse" for="c-42475700">[-]</label><label class="expand" for="c-42475700">[10 more]</label></div><br/><div class="children"><div class="content">This is by far the most practical piece of writing I&#x27;ve seen on the subject of &quot;agents&quot; - it includes actionable definitions, then splits most of the value out into &quot;workflows&quot; and describes those in depth with example applications.<p>There&#x27;s also a cookbook with useful code examples: <a href="https:&#x2F;&#x2F;github.com&#x2F;anthropics&#x2F;anthropic-cookbook&#x2F;tree&#x2F;main&#x2F;patterns&#x2F;agents">https:&#x2F;&#x2F;github.com&#x2F;anthropics&#x2F;anthropic-cookbook&#x2F;tree&#x2F;main&#x2F;p...</a><p>Blogged about this here: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Dec&#x2F;20&#x2F;building-effective-agents&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2024&#x2F;Dec&#x2F;20&#x2F;building-effective-age...</a></div><br/><div id="42476486" class="c"><input type="checkbox" id="c-42476486" checked=""/><div class="controls bullet"><span class="by">NeutralForest</span><span>|</span><a href="#42475700">parent</a><span>|</span><a href="#42478039">next</a><span>|</span><label class="collapse" for="c-42476486">[-]</label><label class="expand" for="c-42476486">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for all the write-ups on LLMs, you&#x27;re on top of the news and it makes it way easier to follow what&#x27;s happening and the existing implementations by following your blog instead.</div><br/></div></div><div id="42478039" class="c"><input type="checkbox" id="c-42478039" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#42475700">parent</a><span>|</span><a href="#42476486">prev</a><span>|</span><a href="#42475903">next</a><span>|</span><label class="collapse" for="c-42478039">[-]</label><label class="expand" for="c-42478039">[4 more]</label></div><br/><div class="children"><div class="content">Yes, they have actionable definitions, but they are defining something quite different than the normal definition of an &quot;agent&quot;. An agent is a party who acts for another.
Often this comes from an employer-employee relationship.<p>This matters mostly when things go wrong. Who&#x27;s responsible? 
The airline whose AI agent gave out wrong info about airline policies found, in court, that their &quot;intelligent agent&quot; was considered an agent in legal terms. Which meant the airline was stuck paying for their mistake.<p>Anthropic&#x27;s definition: Some customers define agents as fully autonomous systems that operate independently over extended periods, using various tools to accomplish complex tasks.<p>That&#x27;s an autonomous system, not an agent. Autonomy is about how much something can do without outside help. Agency is about who&#x27;s doing what for whom, and for whose benefit and with what authority. Those are independent concepts.</div><br/><div id="42478201" class="c"><input type="checkbox" id="c-42478201" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42475700">root</a><span>|</span><a href="#42478039">parent</a><span>|</span><a href="#42478093">next</a><span>|</span><label class="collapse" for="c-42478201">[-]</label><label class="expand" for="c-42478201">[1 more]</label></div><br/><div class="children"><div class="content">Where did you get the idea that your definition there is the &quot;normal&quot; definition of agent, especially in the context of AI?<p>I ask because you seem <i>very</i> confident in it - and my biggest frustration about the term &quot;agent&quot; is that so many people are confident that their personal definition is clearly the one everyone else should be using.</div><br/></div></div><div id="42478093" class="c"><input type="checkbox" id="c-42478093" checked=""/><div class="controls bullet"><span class="by">solidasparagus</span><span>|</span><a href="#42475700">root</a><span>|</span><a href="#42478039">parent</a><span>|</span><a href="#42478201">prev</a><span>|</span><a href="#42475903">next</a><span>|</span><label class="collapse" for="c-42478093">[-]</label><label class="expand" for="c-42478093">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s only one of many definitions for the word agent outside of the context of AI. Another is something produces effects on the world. Another is something that has agency.<p>Sort of interesting that we&#x27;ve coalesced on this term that has many definitions, sometimes conflicting, but where many of the definitions vaguely fit into what an &quot;AI Agent&quot; could be for a given person.<p>But in the context of AI, Agent as Anthropic defines it is an appropriate word because it is a thing that has agency.</div><br/><div id="42478308" class="c"><input type="checkbox" id="c-42478308" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#42475700">root</a><span>|</span><a href="#42478093">parent</a><span>|</span><a href="#42475903">next</a><span>|</span><label class="collapse" for="c-42478308">[-]</label><label class="expand" for="c-42478308">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But in the context of AI, Agent as Anthropic defines it is an appropriate word because it is a thing that has agency.<p>That seems circular.</div><br/></div></div></div></div></div></div><div id="42475903" class="c"><input type="checkbox" id="c-42475903" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#42475700">parent</a><span>|</span><a href="#42478039">prev</a><span>|</span><a href="#42477016">next</a><span>|</span><label class="collapse" for="c-42475903">[-]</label><label class="expand" for="c-42475903">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad they are publishing their cookbooks recipes on github too. Openai used to be more active there.</div><br/></div></div><div id="42477016" class="c"><input type="checkbox" id="c-42477016" checked=""/><div class="controls bullet"><span class="by">th0ma5</span><span>|</span><a href="#42475700">parent</a><span>|</span><a href="#42475903">prev</a><span>|</span><a href="#42478237">next</a><span>|</span><label class="collapse" for="c-42477016">[-]</label><label class="expand" for="c-42477016">[1 more]</label></div><br/><div class="children"><div class="content">How do you protect from compounding errors?</div><br/></div></div></div></div><div id="42478237" class="c"><input type="checkbox" id="c-42478237" checked=""/><div class="controls bullet"><span class="by">zby</span><span>|</span><a href="#42475700">prev</a><span>|</span><a href="#42478289">next</a><span>|</span><label class="collapse" for="c-42478237">[-]</label><label class="expand" for="c-42478237">[1 more]</label></div><br/><div class="children"><div class="content">My wish list for LLM APIs to make them more useful for &#x27;agentic&#x27; workflows:<p>Finer grained control over the tools the LLM is supposed to use. The &#x27;tool_choice&#x27; should allow giving a list of tools to choose. The point is that the list of all available tools is needed to interpret the past tool calls - so you cannot use it to also limit the LLM choice at a particular step. See also: <a href="https:&#x2F;&#x2F;zzbbyy.substack.com&#x2F;p&#x2F;two-roles-of-tool-schemas" rel="nofollow">https:&#x2F;&#x2F;zzbbyy.substack.com&#x2F;p&#x2F;two-roles-of-tool-schemas</a><p>Control over how many tool calls can go in one request. For stateful tools multiple tool calls in one request leads to confusion.<p>By the way - is anyone working with stateful tools? Often they seem very natural and you would think that the LLM at training should encounter lots of stateful interactions and be skilled in using them. But there aren&#x27;t many examples and the libraries are not really geared towards that.</div><br/></div></div><div id="42478289" class="c"><input type="checkbox" id="c-42478289" checked=""/><div class="controls bullet"><span class="by">zaran</span><span>|</span><a href="#42478237">prev</a><span>|</span><a href="#42470556">next</a><span>|</span><label class="collapse" for="c-42478289">[-]</label><label class="expand" for="c-42478289">[1 more]</label></div><br/><div class="children"><div class="content">This was an excellent writeup - felt a bit surprised at how much they considered &quot;workflow&quot; instead of agent but I think it&#x27;s good to start to narrow down the terminology<p>I think these days the main value of the LLM &quot;agent&quot; frameworks is being able to trivially switch between model providers, though even that breaks down when you start to use more esoteric features that may not be implemented in cleanly overlapping ways</div><br/></div></div><div id="42470556" class="c"><input type="checkbox" id="c-42470556" checked=""/><div class="controls bullet"><span class="by">jascha_eng</span><span>|</span><a href="#42478289">prev</a><span>|</span><a href="#42475299">next</a><span>|</span><label class="collapse" for="c-42470556">[-]</label><label class="expand" for="c-42470556">[5 more]</label></div><br/><div class="children"><div class="content">I put the agents in quotes because anthropic actually talks more about what they call &quot;workflows&quot;. And imo this is where the real value of LLMs currently lies, workflow automation.<p>They also say that using LangChain and other frameworks is mostly unnecessary and does more harm than good. They instead argue to use some simple patterns, directly on the API level. Not dis-similar to the old-school Gang of Four software engineering patterns.<p>Really like this post as a guidance for how to actually build useful tools with LLMs. Keep it simple, stupid.</div><br/><div id="42475996" class="c"><input type="checkbox" id="c-42475996" checked=""/><div class="controls bullet"><span class="by">rybosome</span><span>|</span><a href="#42470556">parent</a><span>|</span><a href="#42472766">next</a><span>|</span><label class="collapse" for="c-42475996">[-]</label><label class="expand" for="c-42475996">[1 more]</label></div><br/><div class="children"><div class="content">I felt deeply vindicated by their assessment of these frameworks, in particular LangChain.<p>I&#x27;ve built and&#x2F;or worked on a few different LLM-based workflows, and LangChain definitely makes things worse in my opinion.<p>What it boils down to is that we are still coming to understand the right patterns of development for how to develop agents and agentic workflows. LangChain made choices about how to abstract things that are not general or universal enough to be useful.</div><br/></div></div><div id="42472766" class="c"><input type="checkbox" id="c-42472766" checked=""/><div class="controls bullet"><span class="by">curious_cat_163</span><span>|</span><a href="#42470556">parent</a><span>|</span><a href="#42475996">prev</a><span>|</span><a href="#42475610">next</a><span>|</span><label class="collapse" for="c-42472766">[-]</label><label class="expand" for="c-42472766">[1 more]</label></div><br/><div class="children"><div class="content">Indeed. Very clarifying.<p>I would just posit that they do make a distinction between workflows and agents</div><br/></div></div><div id="42475610" class="c"><input type="checkbox" id="c-42475610" checked=""/><div class="controls bullet"><span class="by">Philpax</span><span>|</span><a href="#42470556">parent</a><span>|</span><a href="#42472766">prev</a><span>|</span><a href="#42475299">next</a><span>|</span><label class="collapse" for="c-42475610">[-]</label><label class="expand" for="c-42475610">[2 more]</label></div><br/><div class="children"><div class="content">Aren&#x27;t you editorialising by doing so?</div><br/><div id="42475730" class="c"><input type="checkbox" id="c-42475730" checked=""/><div class="controls bullet"><span class="by">jascha_eng</span><span>|</span><a href="#42470556">root</a><span>|</span><a href="#42475610">parent</a><span>|</span><a href="#42475299">next</a><span>|</span><label class="collapse" for="c-42475730">[-]</label><label class="expand" for="c-42475730">[1 more]</label></div><br/><div class="children"><div class="content">I guess a little. I really liked the read though, it put in words what I couldn&#x27;t and I was curious if others felt the same.<p>However the post was posted here yesterday and didn&#x27;t really have a lot of traction.
I thought this was partially because of the term agentic, which the community seems a bit fatigued by. So I put it in quotes to highlight that Anthropic themselves deems it a little vague and hopefully spark more interest.
I don&#x27;t think it messes with their message too much?<p>Honestly it didn&#x27;t matter anyways, without second chance pooling this post would have been lost again (so thanks Daniel!)</div><br/></div></div></div></div></div></div><div id="42475299" class="c"><input type="checkbox" id="c-42475299" checked=""/><div class="controls bullet"><span class="by">timdellinger</span><span>|</span><a href="#42470556">prev</a><span>|</span><a href="#42478081">next</a><span>|</span><label class="collapse" for="c-42475299">[-]</label><label class="expand" for="c-42475299">[9 more]</label></div><br/><div class="children"><div class="content">My personal view is that the roadmap to AGI requires an LLM acting as a prefrontal cortex: something designed to think about thinking.<p>It would decide what circumstances call for double-checking facts for accuracy, which would hopefully catch hallucinations.  It would write its own acceptance criteria for its answers, etc.<p>It&#x27;s not clear to me how to train each of the sub-models required, or how big (or small!) they need to be, or what architecture works best. But I think that complex architectures are going to win out over the &quot;just scale up with more data and more compute&quot; approach.</div><br/><div id="42475678" class="c"><input type="checkbox" id="c-42475678" checked=""/><div class="controls bullet"><span class="by">zby</span><span>|</span><a href="#42475299">parent</a><span>|</span><a href="#42476783">next</a><span>|</span><label class="collapse" for="c-42475678">[-]</label><label class="expand" for="c-42475678">[5 more]</label></div><br/><div class="children"><div class="content">IMHO with a simple loop LLMs are already capable of some meta thinking, even without any internal new architectures. For me where it still fails is that LLMs cannot catch their own mistakes even some obvious ones. Like with GPT 3.5 I had a persistent problem with the following question: &quot;Who is older, Annie Morton or Terry Richardson?&quot;. I was giving it Wikipedia and it was correctly finding out the birth dates of the most popular people with the names - but then instead of comparing ages it was comparing birth years. And once it did that it was impossible to it to spot the error.<p>Now with 4o-mini I have a similar even if not so obvious problem.<p>Just writing this down convinced me that there are some ideas to try here - taking a &#x27;report&#x27; of the thought process out of context and judging it there, or changing the temperature or even maybe doing cross-checking with a different model?</div><br/><div id="42478196" class="c"><input type="checkbox" id="c-42478196" checked=""/><div class="controls bullet"><span class="by">zby</span><span>|</span><a href="#42475299">root</a><span>|</span><a href="#42475678">parent</a><span>|</span><a href="#42477630">next</a><span>|</span><label class="collapse" for="c-42478196">[-]</label><label class="expand" for="c-42478196">[1 more]</label></div><br/><div class="children"><div class="content">Ah yeah - actually I tested that taking out of context. This is the thing that surprised me - I thought it is about &#x27;writing itself into a corner - but even in a completely different context the LLM is consistently doing an obvious mistake. 
Here is the example: <a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;67667827-dd88-8008-952b-242a40c2ac1d" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;67667827-dd88-8008-952b-242a40c2ac...</a><p>Janet Waldo was playing Corliss Archer on radio - and the quote the LLM found in Wikipedia was confirming it. But the question was about film - and the LLM cannot spot the gap in its reasoning - even if I try to warn it by telling it the report came from a junior researcher.</div><br/></div></div><div id="42477630" class="c"><input type="checkbox" id="c-42477630" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#42475299">root</a><span>|</span><a href="#42475678">parent</a><span>|</span><a href="#42478196">prev</a><span>|</span><a href="#42476783">next</a><span>|</span><label class="collapse" for="c-42477630">[-]</label><label class="expand" for="c-42477630">[3 more]</label></div><br/><div class="children"><div class="content">Brains are split internally, with each having their own monologue. One happens to have command.</div><br/><div id="42477878" class="c"><input type="checkbox" id="c-42477878" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#42475299">root</a><span>|</span><a href="#42477630">parent</a><span>|</span><a href="#42476783">next</a><span>|</span><label class="collapse" for="c-42477878">[-]</label><label class="expand" for="c-42477878">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think there&#x27;s reason to believe both halves have a monologue, is there? Experience, yes, but doesn&#x27;t only one half do language?</div><br/><div id="42477981" class="c"><input type="checkbox" id="c-42477981" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#42475299">root</a><span>|</span><a href="#42477878">parent</a><span>|</span><a href="#42476783">next</a><span>|</span><label class="collapse" for="c-42477981">[-]</label><label class="expand" for="c-42477981">[1 more]</label></div><br/><div class="children"><div class="content">So if like me you have an interior dialogue, which is speaking and which is listening or is it the same one? I do not ascribe the speaker or listener to a lobe, but whatever the language and comprehension centre(s) is(are), it can do both at the same time.</div><br/></div></div></div></div></div></div></div></div><div id="42476783" class="c"><input type="checkbox" id="c-42476783" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#42475299">parent</a><span>|</span><a href="#42475678">prev</a><span>|</span><a href="#42476257">next</a><span>|</span><label class="collapse" for="c-42476783">[-]</label><label class="expand" for="c-42476783">[1 more]</label></div><br/><div class="children"><div class="content">Interesting, because I almost think of it the opposite way. LLMs are like system 1 thinking, fast, intuitive, based on what you consider most probable based on what you know&#x2F;have experienced&#x2F;have been trained on. System 2 thinking is different, more careful, slower, logical, deductive, more like symbolic reasoning. And then some metasystem to tie these two together and make them work cohesively.</div><br/></div></div><div id="42476257" class="c"><input type="checkbox" id="c-42476257" checked=""/><div class="controls bullet"><span class="by">neom</span><span>|</span><a href="#42475299">parent</a><span>|</span><a href="#42476783">prev</a><span>|</span><a href="#42475914">next</a><span>|</span><label class="collapse" for="c-42476257">[-]</label><label class="expand" for="c-42476257">[1 more]</label></div><br/><div class="children"><div class="content">After I read attention is all you need, my first thought was: &quot;Orchestration is all you need&quot;. When 4o came out I published this: <a href="https:&#x2F;&#x2F;b.h4x.zip&#x2F;agi&#x2F;" rel="nofollow">https:&#x2F;&#x2F;b.h4x.zip&#x2F;agi&#x2F;</a></div><br/></div></div></div></div><div id="42478081" class="c"><input type="checkbox" id="c-42478081" checked=""/><div class="controls bullet"><span class="by">melvinmelih</span><span>|</span><a href="#42475299">prev</a><span>|</span><a href="#42472796">next</a><span>|</span><label class="collapse" for="c-42478081">[-]</label><label class="expand" for="c-42478081">[1 more]</label></div><br/><div class="children"><div class="content">While I agree with the premise of keeping it simple (especially when it comes to using opaque and overcomplicated frameworks like LangChain&#x2F;LangGraph!) I do believe thereâs a lot more to building agentic systems than this article covers.<p>I recently wrote[1] about the 4 main components of autonomous AI agents (Profile, Memory, Planning &amp; Action) and all of that can still be accomplished with simple LLM calls, but thereâs simply a lot more to think about than simple workflow orchestration if you are thinking of building production-ready autonomous agentic systems.<p>[1] <a href="https:&#x2F;&#x2F;melvintercan.com&#x2F;p&#x2F;anatomy-of-an-autonomous-ai-agent" rel="nofollow">https:&#x2F;&#x2F;melvintercan.com&#x2F;p&#x2F;anatomy-of-an-autonomous-ai-agent</a></div><br/></div></div><div id="42472796" class="c"><input type="checkbox" id="c-42472796" checked=""/><div class="controls bullet"><span class="by">curious_cat_163</span><span>|</span><a href="#42478081">prev</a><span>|</span><a href="#42478109">next</a><span>|</span><label class="collapse" for="c-42472796">[-]</label><label class="expand" for="c-42472796">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Agents can be used for open-ended problems where itâs difficult or impossible to predict the required number of steps, and where you canât hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents&#x27; autonomy makes them ideal for scaling tasks in trusted environments.<p>The questions then become:<p>1. When can you (i.e. a person who wants to build systems with them) trust them to make decisions on their own?<p>2. What type of trusted environments are we talking about? (Sandboxing?)<p>So, that all requires more thought -- perhaps by some folks who hang out at this site. :)<p>I suspect that someone will come up with a &quot;real-world&quot; application at a non-tech-first enterprise company and let us know.</div><br/><div id="42477100" class="c"><input type="checkbox" id="c-42477100" checked=""/><div class="controls bullet"><span class="by">ripped_britches</span><span>|</span><a href="#42472796">parent</a><span>|</span><a href="#42478109">next</a><span>|</span><label class="collapse" for="c-42477100">[-]</label><label class="expand" for="c-42477100">[2 more]</label></div><br/><div class="children"><div class="content">Just take any example and think how a human would break it down with decision trees.<p>You are building an AI system to respond to your email.<p>The first agent decides whether the new email should be responded to, yes or no.<p>If no, it can send it to another LLM call that decides to archive it or leave it in the inbox for the human.<p>If yes, it sends it to classifier that decides what type of response is required.<p>Maybe there are some emails like for your work that require something brief like âcongrats!â to all those new feature launch emails you get internally.<p>Or others that are inbound sales emails that need to go out to another system that fetches product related knowledge to craft a response with the right context. Followed by a checker call that makes sure the response follows brand guidelines.<p>The point is all of these steps are completely hypothetical but you can imagine how loosely providing some set of instructions and function calls and procedural limits can easily classify things and minimize error rate.<p>You can do this for any workflow by creatively combining different function calls, recursion, procedural limits, etc. And if you build multiple different decision trees&#x2F;workflows, you can A&#x2F;B test those and use LLM-as-a-judge to score the performance. Especially if youâre working on a task with lots of example outputs.<p>As for trusted environments, assume every single LLM call has been hijacked and donât trust its input&#x2F;output and youâll be good. I put mine in their own cloudflare workers where they canât do any damage beyond giving an odd response to the user.</div><br/><div id="42477598" class="c"><input type="checkbox" id="c-42477598" checked=""/><div class="controls bullet"><span class="by">skydhash</span><span>|</span><a href="#42472796">root</a><span>|</span><a href="#42477100">parent</a><span>|</span><a href="#42478109">next</a><span>|</span><label class="collapse" for="c-42477598">[-]</label><label class="expand" for="c-42477598">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>The first agent decides whether the new email should be responded to, yes or no.</i><p>How would you trust that the agent is following the criteria, and how sure that the criteria is specific enough. Like someone you just meet told you they going to send you something via email, but then the agent misinterpret it due to missing context and decided to respond in a generic manner leading to misunderstanding.<p>&gt; <i>assume every single LLM call has been hijacked and donât trust its input&#x2F;output and youâll be good.</i><p>Which is not new. But with formal languages, you have a more precise definition of what acceptable inputs are (the whole point of formalism is precise definitions). With LLM workflows, the whole environment should be assumed to be public information. And you should probably add a fine point that the output does not engage you in anything.</div><br/></div></div></div></div></div></div><div id="42478109" class="c"><input type="checkbox" id="c-42478109" checked=""/><div class="controls bullet"><span class="by">debois</span><span>|</span><a href="#42472796">prev</a><span>|</span><a href="#42475509">next</a><span>|</span><label class="collapse" for="c-42478109">[-]</label><label class="expand" for="c-42478109">[1 more]</label></div><br/><div class="children"><div class="content">Note how much the principles here resemble general programming principles: keep complexity down, avoid frameworks if you can, avoid unnecessary layers, make debugging easy, document, and test.<p>Itâs as if AI took over the writing-the-program part of software engineering, but sort of left all the rest.</div><br/></div></div><div id="42475509" class="c"><input type="checkbox" id="c-42475509" checked=""/><div class="controls bullet"><span class="by">serjester</span><span>|</span><a href="#42478109">prev</a><span>|</span><a href="#42476305">next</a><span>|</span><label class="collapse" for="c-42475509">[-]</label><label class="expand" for="c-42475509">[1 more]</label></div><br/><div class="children"><div class="content">Couldnât agree more with this - too many people rush to build autonomous agents when their problem could easily be defined as a DAG workflow. Agents increase the degrees of freedom in your system exponentially making it so much more challenging to evaluate systematically.</div><br/></div></div><div id="42476305" class="c"><input type="checkbox" id="c-42476305" checked=""/><div class="controls bullet"><span class="by">jsemrau</span><span>|</span><a href="#42475509">prev</a><span>|</span><a href="#42477185">next</a><span>|</span><label class="collapse" for="c-42476305">[-]</label><label class="expand" for="c-42476305">[4 more]</label></div><br/><div class="children"><div class="content">Agents are still a misaligned concept in AI. While this article offers a lot in orchestration, memory (only mentioned once in the post) and governance are not really mentioned. The latter is important to increase reliability -- something Ilya Sutskever mentioned to be important as agents can be less deterministic in their responses. 
Interestingly, &quot;agency&quot; i.e., the ability of the agent to make own decisions is not mentioned once.<p>I work on CAAs and document my journey on my substack (<a href="https:&#x2F;&#x2F;jdsmerau.substack.com" rel="nofollow">https:&#x2F;&#x2F;jdsmerau.substack.com</a>)</div><br/><div id="42476687" class="c"><input type="checkbox" id="c-42476687" checked=""/><div class="controls bullet"><span class="by">handfuloflight</span><span>|</span><a href="#42476305">parent</a><span>|</span><a href="#42477185">next</a><span>|</span><label class="collapse" for="c-42476687">[-]</label><label class="expand" for="c-42476687">[3 more]</label></div><br/><div class="children"><div class="content">That URL says Not Found.</div><br/><div id="42477094" class="c"><input type="checkbox" id="c-42477094" checked=""/><div class="controls bullet"><span class="by">vacuity</span><span>|</span><a href="#42476305">root</a><span>|</span><a href="#42476687">parent</a><span>|</span><a href="#42477185">next</a><span>|</span><label class="collapse" for="c-42477094">[-]</label><label class="expand" for="c-42477094">[2 more]</label></div><br/><div class="children"><div class="content">Seems to be <a href="https:&#x2F;&#x2F;jdsemrau.substack.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jdsemrau.substack.com&#x2F;</a>, also in their bio.</div><br/><div id="42477557" class="c"><input type="checkbox" id="c-42477557" checked=""/><div class="controls bullet"><span class="by">jsemrau</span><span>|</span><a href="#42476305">root</a><span>|</span><a href="#42477094">parent</a><span>|</span><a href="#42477185">next</a><span>|</span><label class="collapse" for="c-42477557">[-]</label><label class="expand" for="c-42477557">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. I was rushing out to the gym.</div><br/></div></div></div></div></div></div></div></div><div id="42476104" class="c"><input type="checkbox" id="c-42476104" checked=""/><div class="controls bullet"><span class="by">thoughtlede</span><span>|</span><a href="#42477185">prev</a><span>|</span><a href="#42475651">next</a><span>|</span><label class="collapse" for="c-42476104">[-]</label><label class="expand" for="c-42476104">[1 more]</label></div><br/><div class="children"><div class="content">When thinking about AI agents, there is still conflation between how to decide the next step to take vs what information is needed to decide the next step.<p>If runtime information is insufficient, we can use AI&#x2F;ML models to fill that information. But deciding the next step could be done ahead of time assuming complete information.<p>Most AI agent examples short circuit these two steps. When faced with unstructured or insufficient information, the program asks the LLM&#x2F;AI model to decide the next step. Instead, we could ask the LLM&#x2F;AI model to structure&#x2F;predict necessary information and use pre-defined rules to drive the process.<p>This approach will translate most [1] &quot;Agent&quot; examples into &quot;Workflow&quot; examples. The quotes here are meant to imply Anthropic&#x27;s definition of these terms.<p>[1] I said &quot;most&quot; because there might be continuous world systems (such as real world simulacrum) that will require a very large number of rules and is probably impractical to define each of them. I believe those systems are an exception, not a rule.</div><br/></div></div><div id="42475651" class="c"><input type="checkbox" id="c-42475651" checked=""/><div class="controls bullet"><span class="by">tonyhb</span><span>|</span><a href="#42476104">prev</a><span>|</span><a href="#42476484">next</a><span>|</span><label class="collapse" for="c-42475651">[-]</label><label class="expand" for="c-42475651">[1 more]</label></div><br/><div class="children"><div class="content">It looks like Agents are less about DAG workflows and fully autonomous &quot;networks of agents&quot;, but more of a stateful network:<p>* A &quot;network of agents&quot; is a system of agents and tools<p>* That run and build up state (both &quot;memory&quot; and actual state via tool use)<p>* Which is then inspected when routing as a kind of &quot;state machine&quot;.<p>* Routing should specify which agent (or agents, in parallel) to run next, via that state.<p>* Routing can also use other agents (routing agents) to figure out what to do next, instead of code.<p>We&#x27;re codifying this with durable workflows in a prototypical library â AgentKit: <a href="https:&#x2F;&#x2F;github.com&#x2F;inngest&#x2F;agent-kit&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;inngest&#x2F;agent-kit&#x2F;</a> (docs: <a href="https:&#x2F;&#x2F;agentkit.inngest.com&#x2F;overview" rel="nofollow">https:&#x2F;&#x2F;agentkit.inngest.com&#x2F;overview</a>).<p>It took less than a day to get a network of agents to correctly fix swebench-lite examples.  It&#x27;s super early, but very fun.  One of the cool things is that this uses Inngest under the hood, so you get all of the classic durable execution&#x2F;step function&#x2F;tracing&#x2F;o11y for free, but it&#x27;s just regular code that you write.</div><br/></div></div><div id="42476484" class="c"><input type="checkbox" id="c-42476484" checked=""/><div class="controls bullet"><span class="by">qianli_cs</span><span>|</span><a href="#42475651">prev</a><span>|</span><a href="#42478002">next</a><span>|</span><label class="collapse" for="c-42476484">[-]</label><label class="expand" for="c-42476484">[1 more]</label></div><br/><div class="children"><div class="content">Good article. I think it can emphasize a bit more on supporting human interactions in agentic workflows. While composing workflows isn&#x27;t new, involving human-in-the-loop introduces huge complexity, especially for long-running, async processes. Waiting for human input (which could take days), managing retries, and avoiding errors like duplicate refunds or missed updates require careful orchestration.<p>I think this is where durable execution shines. By ensuring every step in an async processing workflow is fault-tolerant and durable, even interruptions won&#x27;t lose progress. For example, in a refund workflow, a durable system can resume exactly where it left offâno duplicate refunds, no lost state.</div><br/></div></div><div id="42478002" class="c"><input type="checkbox" id="c-42478002" checked=""/><div class="controls bullet"><span class="by">mi_lk</span><span>|</span><a href="#42476484">prev</a><span>|</span><a href="#42477555">next</a><span>|</span><label class="collapse" for="c-42478002">[-]</label><label class="expand" for="c-42478002">[2 more]</label></div><br/><div class="children"><div class="content">Tangent but anyone know what software is used to draw those workflow diagrams?</div><br/><div id="42478038" class="c"><input type="checkbox" id="c-42478038" checked=""/><div class="controls bullet"><span class="by">huydotnet</span><span>|</span><a href="#42478002">parent</a><span>|</span><a href="#42477555">next</a><span>|</span><label class="collapse" for="c-42478038">[-]</label><label class="expand" for="c-42478038">[1 more]</label></div><br/><div class="children"><div class="content">they probably use their own designers. jk. the arrows looks a lot like Figma&#x2F;Figjam.</div><br/></div></div></div></div><div id="42477555" class="c"><input type="checkbox" id="c-42477555" checked=""/><div class="controls bullet"><span class="by">mediumsmart</span><span>|</span><a href="#42478002">prev</a><span>|</span><a href="#42476147">next</a><span>|</span><label class="collapse" for="c-42477555">[-]</label><label class="expand" for="c-42477555">[1 more]</label></div><br/><div class="children"><div class="content">I have always voted for the <i>Unix style multiple do one thing good blackboxes</i> as the plumbing in the ruling agent.<p>Divide and conquer me hearties.</div><br/></div></div><div id="42476147" class="c"><input type="checkbox" id="c-42476147" checked=""/><div class="controls bullet"><span class="by">elpalek</span><span>|</span><a href="#42477555">prev</a><span>|</span><a href="#42475699">next</a><span>|</span><label class="collapse" for="c-42476147">[-]</label><label class="expand" for="c-42476147">[1 more]</label></div><br/><div class="children"><div class="content">Claude api lacks structured output, without uniformity in output, it&#x27;s not useful as agent. I&#x27;ve had agents system broke down suddenly due to degradation in output, which leads to the previous suggested json output hacks (from official cookbook) stopped working.</div><br/></div></div><div id="42475699" class="c"><input type="checkbox" id="c-42475699" checked=""/><div class="controls bullet"><span class="by">brotchie</span><span>|</span><a href="#42476147">prev</a><span>|</span><a href="#42477927">next</a><span>|</span><label class="collapse" for="c-42475699">[-]</label><label class="expand" for="c-42475699">[5 more]</label></div><br/><div class="children"><div class="content">Have been building agents for past 2 years, my tl;dr is that:<p><i>Agents are Interfaces, Not Implementations</i><p>The current zeitgeist seems to think of agents as passthrough agents: e.g. a lite wrapper around a core that&#x27;s almost 100% a LLM.<p>The most effective agents I&#x27;ve seen, and have built, are largely traditional software engineering with a sprinkling of LLM calls for &quot;LLM hard&quot; problems. LLM hard problems are problems that can ONLY be solved by application of an LLM (creative writing, text synthesis, intelligent decision making). Leave all the problems that are amenable to decades of software engineering best practice to good old deterministic code.<p>I&#x27;ve been calling system like this <i>&quot;Transitional Software Design.&quot;</i> That is, they&#x27;re mostly a traditional software application under the hood (deterministic, well structured code, separation of concerns) with judicious use of LLMs where required.<p>Ultimately, users care about what the agent does, not how it does it.<p>The biggest differentiator I&#x27;ve seen between agents that work and get adoption, and those that are eternally in a demo phase, is related to the cardinality of the state space the agent is operating in. Too many folks try and &quot;boil the ocean&quot; and try and implement a generic purpose capability: e.g. Generate Python code to do something, or synthesizing SQL based on natural language.<p>The projects I&#x27;ve seen that work really focus on reducing the state space of agent decision making down to the smallest possible set that delivers user value.<p>e.g. Rather than generating arbitrary SQL, work out a set of ~20 SQL templates that are hyper-specific to the business problem you&#x27;re solving. Parameterize them with the options for select, filter, group by, order by, and the subset of aggregate operations that are relevant. Then let the agent chose the right template + parameters from a relatively small finite set of options.<p>^^^ the delta in agent quality between &quot;boiling the ocean&quot; vs &quot;agent&#x27;s free choice over a small state space&quot; is night and day. It lets you deploy early, deliver value, and start getting user feedback.<p>Building Transitional Software Systems:<p><pre><code>  1. Deeply understand the domain and CUJs,
  2. Segment out the system into &quot;problems that traditional software is good at solving&quot; and &quot;LLM-hard problems&quot;,
  3. For the LLM hard problems, work out the smallest possible state space of decision making,
  4. Build the system, and get users using it,
  5. Gradually expand the state space as feedback flows in from users.</code></pre></div><br/><div id="42476199" class="c"><input type="checkbox" id="c-42476199" checked=""/><div class="controls bullet"><span class="by">CharlieDigital</span><span>|</span><a href="#42475699">parent</a><span>|</span><a href="#42475906">next</a><span>|</span><label class="collapse" for="c-42476199">[-]</label><label class="expand" for="c-42476199">[1 more]</label></div><br/><div class="children"><div class="content">Same experience.<p>The smaller and more focused the context, the higher the consistency of output, and the lower the chance of jank.<p>Fundamentally no different than giving instructions to a junior dev.  Be more specific -- point them to the right docs, distill the requirements, identify the relevant areas of the source -- to get good output.<p>My last attempt at a workflow of agents was at the 3.5 to 4 transition and OpenAI wasn&#x27;t good enough at that point to produce consistently good output and was slow to boot.<p>My team has taken the stance that getting consistently good output from LLMs is really an ETL exercise: acquire, aggregate, and transform the minimum relevant data for the output to reach the desired level of quality and depth and let the LLM do it&#x27;s thing.</div><br/></div></div><div id="42475906" class="c"><input type="checkbox" id="c-42475906" checked=""/><div class="controls bullet"><span class="by">samdjstephens</span><span>|</span><a href="#42475699">parent</a><span>|</span><a href="#42476199">prev</a><span>|</span><a href="#42476710">next</a><span>|</span><label class="collapse" for="c-42475906">[-]</label><label class="expand" for="c-42475906">[2 more]</label></div><br/><div class="children"><div class="content">Thereâll always be an advantage for those who understand the problem theyâre solving for sure.<p>The balance of traditional software components and LLM driven components in a system is an interesting topic - I wonder how the capabilities of future generations of foundation model will change that?</div><br/><div id="42476349" class="c"><input type="checkbox" id="c-42476349" checked=""/><div class="controls bullet"><span class="by">brotchie</span><span>|</span><a href="#42475699">root</a><span>|</span><a href="#42475906">parent</a><span>|</span><a href="#42476710">next</a><span>|</span><label class="collapse" for="c-42476349">[-]</label><label class="expand" for="c-42476349">[1 more]</label></div><br/><div class="children"><div class="content">Certain the end state is &quot;one model to rule them all&quot; hence the &quot;transitional.&quot;<p>Just that the pragmatic approach, today, given current LLM capabilities, is to minimize the surface area &#x2F; state space that the LLM is actuating. And then gradually expand that until the whole system is just a passthrough. But starting with a passthrough kinda doesn&#x27;t lead to great products in December 2024.</div><br/></div></div></div></div><div id="42476710" class="c"><input type="checkbox" id="c-42476710" checked=""/><div class="controls bullet"><span class="by">handfuloflight</span><span>|</span><a href="#42475699">parent</a><span>|</span><a href="#42475906">prev</a><span>|</span><a href="#42477927">next</a><span>|</span><label class="collapse" for="c-42476710">[-]</label><label class="expand" for="c-42476710">[1 more]</label></div><br/><div class="children"><div class="content">When trying to do everything, they end up doing nothing.</div><br/></div></div></div></div><div id="42477927" class="c"><input type="checkbox" id="c-42477927" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#42475699">prev</a><span>|</span><a href="#42475908">next</a><span>|</span><label class="collapse" for="c-42477927">[-]</label><label class="expand" for="c-42477927">[1 more]</label></div><br/><div class="children"><div class="content">Does any one have a solid examples of a real agent, deployed in production?</div><br/></div></div><div id="42475908" class="c"><input type="checkbox" id="c-42475908" checked=""/><div class="controls bullet"><span class="by">websku</span><span>|</span><a href="#42477927">prev</a><span>|</span><a href="#42476023">next</a><span>|</span><label class="collapse" for="c-42475908">[-]</label><label class="expand" for="c-42475908">[1 more]</label></div><br/><div class="children"><div class="content">indeed, we&#x27;ve seen this approach as well. All these &quot;frameworks&quot; in real business cases become too complicated.</div><br/></div></div><div id="42476023" class="c"><input type="checkbox" id="c-42476023" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#42475908">prev</a><span>|</span><a href="#42475768">next</a><span>|</span><label class="collapse" for="c-42476023">[-]</label><label class="expand" for="c-42476023">[1 more]</label></div><br/><div class="children"><div class="content">Anthropic keeps advertising its MCP (Model Context Protocol), but to the extent it doesn&#x27;t support other LLMs, e.g. GPT, it couldn&#x27;t possibly gain adoption. I have yet to see any example of MCP that can be extended to use a random LLM.</div><br/></div></div><div id="42475768" class="c"><input type="checkbox" id="c-42475768" checked=""/><div class="controls bullet"><span class="by">ramesh31</span><span>|</span><a href="#42476023">prev</a><span>|</span><a href="#42476605">next</a><span>|</span><label class="collapse" for="c-42475768">[-]</label><label class="expand" for="c-42475768">[4 more]</label></div><br/><div class="children"><div class="content">Key to understanding the power of agentic workflows is tool usage. You don&#x27;t have to write logic anymore, you simply give an agent the tools it needs to accomplish a task and ask it to do so. Models like the latest Sonnet have gotten so advanced now that coding abilities are reaching superhuman levels. All the hallucinations and &quot;jitter&quot; of models from 1-2 years ago has gone away. They can be reasoned on now and you can build reliable systems with them.</div><br/><div id="42475778" class="c"><input type="checkbox" id="c-42475778" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#42475768">parent</a><span>|</span><a href="#42476605">next</a><span>|</span><label class="collapse" for="c-42475778">[-]</label><label class="expand" for="c-42475778">[3 more]</label></div><br/><div class="children"><div class="content">&gt; you simply give an agent the tools<p>That isnât simple. There is a lot of nuance in tool definition.</div><br/><div id="42477267" class="c"><input type="checkbox" id="c-42477267" checked=""/><div class="controls bullet"><span class="by">ripped_britches</span><span>|</span><a href="#42475768">root</a><span>|</span><a href="#42475778">parent</a><span>|</span><a href="#42475833">next</a><span>|</span><label class="collapse" for="c-42477267">[-]</label><label class="expand" for="c-42477267">[1 more]</label></div><br/><div class="children"><div class="content">Depends on what youâre building. A general assistant is going to have a lot of nuance. A well defined agent like a tutor only has so many tools to call upon.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>