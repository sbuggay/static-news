<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1695632463156" as="style"/><link rel="stylesheet" href="styles.css?v=1695632463156"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://barryzhang.substack.com/p/our-humble-attempt-at-fine-tuning">Our humble attempt at “how much data do you need to fine-tune”</a> <span class="domain">(<a href="https://barryzhang.substack.com">barryzhang.substack.com</a>)</span></div><div class="subtext"><span>gnahzby</span> | <span>31 comments</span></div><br/><div><div id="37637104" class="c"><input type="checkbox" id="c-37637104" checked=""/><div class="controls bullet"><span class="by">joewferrara</span><span>|</span><a href="#37637768">next</a><span>|</span><label class="collapse" for="c-37637104">[-]</label><label class="expand" for="c-37637104">[18 more]</label></div><br/><div class="children"><div class="content">They test two fine tuning tasks in the article - reliable output formatting and custom tone. These are two tasks (reliable output formatting in particular) that are advertised regularly as areas where fine tuning an LLM should work. The goal is not to change what the LLM knows, but to change how the LLM communicates what it knows. In theory the user wants to leverage the LLMs knowledge base and using the different output format is more useful to the user.<p>The hard question IMO is the question of when does it make sense to fine tune an LLM to update it&#x27;s knowledge and how much data is needed in this case? I have not seen anyone show a real example of succeeded in this case and wonder if it&#x27;s close to as difficult as training the LLM from scratch or if it&#x27;s a feasible fine tuning use case.</div><br/><div id="37637795" class="c"><input type="checkbox" id="c-37637795" checked=""/><div class="controls bullet"><span class="by">dnnssl2</span><span>|</span><a href="#37637104">parent</a><span>|</span><a href="#37637345">next</a><span>|</span><label class="collapse" for="c-37637795">[-]</label><label class="expand" for="c-37637795">[11 more]</label></div><br/><div class="children"><div class="content">Knowledge instillation is probably the holy grail of fine tuning. The hard part is:<p>1. Generalizing new facts. You can create a question answer pair of: “what is the population of the world in 2023?” “8 billion”, but it may not be able to pick up alternate phrasing or “does the world have 8 billion people on it?”<p>2. Catastrophic and behavioral forgetting. Continued fine tuning after RLHF and instruction fine tuning may result in the loss of the alignment and instruction following capabilities trained by OpenAI. At worst, it will start spewing random tokens like the example in the post.<p>I have not yet seen it successfully done, and I suspect that updating fractions (~.1%) of the original weights with PEFT methods won’t help.</div><br/><div id="37640105" class="c"><input type="checkbox" id="c-37640105" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37637104">root</a><span>|</span><a href="#37637795">parent</a><span>|</span><a href="#37637845">next</a><span>|</span><label class="collapse" for="c-37640105">[-]</label><label class="expand" for="c-37640105">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I have not yet seen it successfully done, and I suspect that updating fractions (~.1%) of the original weights with PEFT methods won’t help.<p>Nitpick, but although when training LoRAs you&#x27;re only training 1% or less (depending on rank) of the <i>number</i> of parameters of the entire model, the adapters affect the entire model and after merging the LoRA all of the weights of the model are updated.</div><br/></div></div><div id="37637845" class="c"><input type="checkbox" id="c-37637845" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37637104">root</a><span>|</span><a href="#37637795">parent</a><span>|</span><a href="#37640105">prev</a><span>|</span><a href="#37637345">next</a><span>|</span><label class="collapse" for="c-37637845">[-]</label><label class="expand" for="c-37637845">[9 more]</label></div><br/><div class="children"><div class="content">Your answer is not really answering and is liable to confuse someone asking the question this person asked... the answer to their question is a simple: No.<p>Current fine tuning techniques can only contribute to knowledge indirectly (getting better queries for an external data source for example), you cannot directly embed new facts in the model is any generally efficient&#x2F;effective manner.<p>There are toy examples of fine tuning in facts that are not of use outside of academic considerations at this point, and I sense it&#x27;s contributing to the widespread confusion about fine-tuning&#x27;s value proposition</div><br/><div id="37637971" class="c"><input type="checkbox" id="c-37637971" checked=""/><div class="controls bullet"><span class="by">dnnssl2</span><span>|</span><a href="#37637104">root</a><span>|</span><a href="#37637845">parent</a><span>|</span><a href="#37638457">next</a><span>|</span><label class="collapse" for="c-37637971">[-]</label><label class="expand" for="c-37637971">[4 more]</label></div><br/><div class="children"><div class="content">There are a few reputable academic examples of factual editing, such as: <a href="https:&#x2F;&#x2F;rome.baulab.info&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;rome.baulab.info&#x2F;</a><p>I don’t believe that the answer is strictly no. There are still many questions around the fine tuning method and the scale of data, as well as expectations of task accuracy from the perspective of an end user.</div><br/><div id="37638047" class="c"><input type="checkbox" id="c-37638047" checked=""/><div class="controls bullet"><span class="by">mikeagb</span><span>|</span><a href="#37637104">root</a><span>|</span><a href="#37637971">parent</a><span>|</span><a href="#37638057">next</a><span>|</span><label class="collapse" for="c-37638047">[-]</label><label class="expand" for="c-37638047">[1 more]</label></div><br/><div class="children"><div class="content">I agree that stating outright that the answer is no is a bit too strong of a statement. The general consensus has definitely been that fine-tuning (especially instruction fine-tuning) is primarily to pick up style over facts, but that doesn&#x27;t mean it&#x27;s not doable. Continuous pre-training is used to instill new knowledge, and the line where it becomes &quot;fine-tuning&quot; rather from &quot;continuous pre-training&quot; is not obvious to me.</div><br/></div></div><div id="37638057" class="c"><input type="checkbox" id="c-37638057" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37637104">root</a><span>|</span><a href="#37637971">parent</a><span>|</span><a href="#37638047">prev</a><span>|</span><a href="#37638457">next</a><span>|</span><label class="collapse" for="c-37638057">[-]</label><label class="expand" for="c-37638057">[2 more]</label></div><br/><div class="children"><div class="content">&gt; There are toy examples of fine tuning in facts that are not of use outside of academic considerations at this point, and I sense it&#x27;s contributing to the widespread confusion about fine-tuning&#x27;s value proposition<p>The answer for someone <i>asking that question</i> is a strict no. Many people asking this stuff only have access to SFT, so it&#x27;s a <i>super</i> no for them.<p>Honestly I don&#x27;t get this weird obsession right now with LLMs and throwing random roadblocks in any sort of common knowledge of the subject. If someone in CS 101 asked if they could write a game engine in CSS you wouldn&#x27;t get people lining up to tell them the answer isn&#x27;t &quot;No.&quot; despite it technically being possible (<a href="https:&#x2F;&#x2F;github.com&#x2F;brookjordan&#x2F;css-game-engine">https:&#x2F;&#x2F;github.com&#x2F;brookjordan&#x2F;css-game-engine</a>) because we understand that sometimes to enable understanding of a subject you need to setup some solid ground for new entrants to stand on.<p>Fine-tuning is not for knowledge. If you get comfortable enough to start experimenting with that application, you&#x27;ll understand that there&#x27;s some nuance to that statement either way and get to research&#x2F;tinker&#x2F;push boundaries armed with enough knowledge to not accept the simple no.</div><br/><div id="37640599" class="c"><input type="checkbox" id="c-37640599" checked=""/><div class="controls bullet"><span class="by">Phemist</span><span>|</span><a href="#37637104">root</a><span>|</span><a href="#37638057">parent</a><span>|</span><a href="#37638457">next</a><span>|</span><label class="collapse" for="c-37640599">[-]</label><label class="expand" for="c-37640599">[1 more]</label></div><br/><div class="children"><div class="content">Funny, as a corollary to how impractical a game engine would be in CSS: my first pass over the text had me reading CSS as something like C Sharp Sharp. A non-existent language that, according to my brain, still seems like a more likely language to build the game engine in</div><br/></div></div></div></div></div></div><div id="37638457" class="c"><input type="checkbox" id="c-37638457" checked=""/><div class="controls bullet"><span class="by">ozr</span><span>|</span><a href="#37637104">root</a><span>|</span><a href="#37637845">parent</a><span>|</span><a href="#37637971">prev</a><span>|</span><a href="#37637345">next</a><span>|</span><label class="collapse" for="c-37638457">[-]</label><label class="expand" for="c-37638457">[4 more]</label></div><br/><div class="children"><div class="content">This is simply wrong.  Information that did not exist in the model can be added to a model by finetuning, both full and PEFT.  It has been repeatedly demonstrated in practice and in multiple papers.</div><br/><div id="37638509" class="c"><input type="checkbox" id="c-37638509" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37637104">root</a><span>|</span><a href="#37638457">parent</a><span>|</span><a href="#37637345">next</a><span>|</span><label class="collapse" for="c-37638509">[-]</label><label class="expand" for="c-37638509">[3 more]</label></div><br/><div class="children"><div class="content">That is simply noise.<p>Surely you saw the sibling comment that tries to make the exact same point and did so hours ago, the reply is the same:<p>&gt; There are toy examples of fine tuning in facts that are not of use outside of academic considerations at this point, and I sense it&#x27;s contributing to the widespread confusion about fine-tuning&#x27;s value proposition<p>The answer for someone asking that question is a strict no. Many people asking this stuff only have access to SFT, so it&#x27;s a super no for them.<p>Honestly I don&#x27;t get this weird obsession right now with LLMs and throwing random roadblocks in any sort of common knowledge of the subject. If someone in CS 101 asked if they could write a game engine in CSS you wouldn&#x27;t get people lining up to tell them the answer isn&#x27;t &quot;No.&quot; despite it technically being possible (<a href="https:&#x2F;&#x2F;github.com&#x2F;brookjordan&#x2F;css-game-engine">https:&#x2F;&#x2F;github.com&#x2F;brookjordan&#x2F;css-game-engine</a>) because we understand that sometimes to enable understanding of a subject you need to setup some solid ground for new entrants to stand on.<p>Fine-tuning is not for knowledge. If you get comfortable enough to start experimenting with that application, you&#x27;ll understand that there&#x27;s some nuance to that statement either way and get to research&#x2F;tinker&#x2F;push boundaries armed with enough knowledge to not accept the simple no.<p>_<p>It&#x27;s no different than teaching the Bohr model of the atom: we know it doesn&#x27;t hold up to discoveries that you&#x27;ll come across after it is established, but it doesn&#x27;t matter because by the time you know enough to revisit the topic, you understand <i>why</i> the answer was a flat no then and can move past it on your own.<p>OP could have googled the topic but they asked human beings the question. They likely presumed they&#x27;d use their human sensibilities to understand the underlying intent of the question instead of parroting a list of toy experiments that would have zero benefit to them.</div><br/><div id="37639835" class="c"><input type="checkbox" id="c-37639835" checked=""/><div class="controls bullet"><span class="by">zwaps</span><span>|</span><a href="#37637104">root</a><span>|</span><a href="#37638509">parent</a><span>|</span><a href="#37637345">next</a><span>|</span><label class="collapse" for="c-37639835">[-]</label><label class="expand" for="c-37639835">[2 more]</label></div><br/><div class="children"><div class="content">Having literally done it in an enterprise setting (and participated in experiments for some of the largest companies in the world in their respective domain fields), I have to say: your lack of nuance and abundance of arrogance does not come across very well.<p>It is important to distinguish between something being impossible, infeasible and not well understood. Fine-tuning &quot;for effect&quot; is mostly the latter.<p>You say &quot;current fine-tuning techniques can only contribute to knowledge indirectly&quot; and then in the next post row back to &quot;except in toy examples&quot; because the former is - literally - not correct.<p>This is HN. We are not advising clients on how &quot;to get their data into their AI best&quot;. We can discuss here the actual technical detail of a thing. An intellectually honest discussion begins with saying: &quot;From a scientific standpoint, and even from a practical standpoint, we are not sure yet, however...&quot;</div><br/><div id="37640085" class="c"><input type="checkbox" id="c-37640085" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37637104">root</a><span>|</span><a href="#37639835">parent</a><span>|</span><a href="#37637345">next</a><span>|</span><label class="collapse" for="c-37640085">[-]</label><label class="expand" for="c-37640085">[1 more]</label></div><br/><div class="children"><div class="content">&quot;advising clients&quot; is such an odd way of describing &quot;making a complex topic approachable&quot;<p>But you&#x27;re correct, this <i>is</i> HN: so much pontificating without producing a single counterfactual implies you should speak for yourself and not the collective.<p>They said &quot;LLM&quot;, but given the context it&#x27;s an RLHF LLM, and presumably they want a generalized way to add factual information in a way that doesn&#x27;t cripple the model&#x27;s general performance (yes, I am being so arrogant as to draw obvious conclusions to give them a useful answer)<p>No paper on the subject has achieved this, the ones that come close (and by close I mean very far) fall back to BERT sized models which I already addressed below: so please petition your &quot;enterprise&quot; to share their secrets<p>(wrong crowd to get any gravitas out of the word enterprise btw, we understand it means &quot;constrained usecase with minimal external validation&quot;)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="37637345" class="c"><input type="checkbox" id="c-37637345" checked=""/><div class="controls bullet"><span class="by">ozr</span><span>|</span><a href="#37637104">parent</a><span>|</span><a href="#37637795">prev</a><span>|</span><a href="#37640082">next</a><span>|</span><label class="collapse" for="c-37637345">[-]</label><label class="expand" for="c-37637345">[2 more]</label></div><br/><div class="children"><div class="content">Fwiw, unpublished testing on LLaMA-1 13B showed that it was able to learn a new word and it&#x27;s meaning via PEFT with &lt;50 examples.  Finetuning can unquestionably add new data to a model.<p>Jeremy Howard has written a bit about how quickly LLMs can pick up new concepts as well:<p><a href="https:&#x2F;&#x2F;www.fast.ai&#x2F;posts&#x2F;2023-09-04-learning-jumps&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.fast.ai&#x2F;posts&#x2F;2023-09-04-learning-jumps&#x2F;</a></div><br/><div id="37637652" class="c"><input type="checkbox" id="c-37637652" checked=""/><div class="controls bullet"><span class="by">mikeagb</span><span>|</span><a href="#37637104">root</a><span>|</span><a href="#37637345">parent</a><span>|</span><a href="#37640082">next</a><span>|</span><label class="collapse" for="c-37637652">[-]</label><label class="expand" for="c-37637652">[1 more]</label></div><br/><div class="children"><div class="content">The question of how to fine-tune to teach LLMs facts&#x2F;knowledge is definitely something we&#x27;re interested in exploring more in future work. The common opinion seems to me at least to be that fine-tuning is more to teach the model how to use the knowledge it already has to complete a specific task rather than to instill new knowledge, and that RAG should be sued to provide more specific context. However, I personally believe there is potential in fine-tuning for &quot;memorization&quot; or learning, and am excited to see new developments in the field.</div><br/></div></div></div></div><div id="37640082" class="c"><input type="checkbox" id="c-37640082" checked=""/><div class="controls bullet"><span class="by">jawerty</span><span>|</span><a href="#37637104">parent</a><span>|</span><a href="#37637345">prev</a><span>|</span><a href="#37639103">next</a><span>|</span><label class="collapse" for="c-37640082">[-]</label><label class="expand" for="c-37640082">[1 more]</label></div><br/><div class="children"><div class="content">A lot of it is based on what you’re aiming to generalize. Typically for data that is fairly “conceptual” I find a DPR type solution to be better for most people with a similarity search. But if you want to generalize on style&#x2F;data extraction&#x2F;basic pattern matching fine tuning is incredible. I talk through some of these concepts on a fine tuning llama 2 live stream I did a couple months ago  <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;live&#x2F;TYgtG2Th6fI?si=7VVv0Qooe2o1wuzD">https:&#x2F;&#x2F;www.youtube.com&#x2F;live&#x2F;TYgtG2Th6fI?si=7VVv0Qooe2o1wuzD</a></div><br/></div></div><div id="37639103" class="c"><input type="checkbox" id="c-37639103" checked=""/><div class="controls bullet"><span class="by">KRAKRISMOTT</span><span>|</span><a href="#37637104">parent</a><span>|</span><a href="#37640082">prev</a><span>|</span><a href="#37637768">next</a><span>|</span><label class="collapse" for="c-37639103">[-]</label><label class="expand" for="c-37639103">[3 more]</label></div><br/><div class="children"><div class="content">Why not use the finite state machine logits filter trick for controlling output format? It seems to work pretty well for JSON</div><br/><div id="37639943" class="c"><input type="checkbox" id="c-37639943" checked=""/><div class="controls bullet"><span class="by">antman</span><span>|</span><a href="#37637104">root</a><span>|</span><a href="#37639103">parent</a><span>|</span><a href="#37637768">next</a><span>|</span><label class="collapse" for="c-37639943">[-]</label><label class="expand" for="c-37639943">[2 more]</label></div><br/><div class="children"><div class="content">What is that? Sounds useful</div><br/><div id="37640772" class="c"><input type="checkbox" id="c-37640772" checked=""/><div class="controls bullet"><span class="by">evrimoztamur</span><span>|</span><a href="#37637104">root</a><span>|</span><a href="#37639943">parent</a><span>|</span><a href="#37637768">next</a><span>|</span><label class="collapse" for="c-37640772">[-]</label><label class="expand" for="c-37640772">[1 more]</label></div><br/><div class="children"><div class="content">I think he&#x27;s referring to <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37125118">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37125118</a><p>Controlling token outputs to match a regex generates valid JSON 100% of the time!</div><br/></div></div></div></div></div></div></div></div><div id="37637768" class="c"><input type="checkbox" id="c-37637768" checked=""/><div class="controls bullet"><span class="by">tomohelix</span><span>|</span><a href="#37637104">prev</a><span>|</span><a href="#37639400">next</a><span>|</span><label class="collapse" for="c-37637768">[-]</label><label class="expand" for="c-37637768">[5 more]</label></div><br/><div class="children"><div class="content">Is this something like short term vs long term memory? The context window for LLMs is its short term memory where you can tell it to do things or quickly define something and the LLM can learn very quickly even with just 1 example or a sentence. But it forgets immediately once the work is done. But for finetuning, it commits the knowledge into its weight network and have a &quot;deeper&quot; understanding? The cost is it takes more effort and energy to do so?<p>If so, let say in the future, we have an LLM with 100K token context windows but with a subsystem where it notices some knowledge keeps being repeated in the context and then store that knowledge for finetuning when the LLM is not doing inference. Basically a mirror of the way we human work? Is that possible? An LLM that constantly improved and can adapt to new knowledge?</div><br/><div id="37637815" class="c"><input type="checkbox" id="c-37637815" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37637768">parent</a><span>|</span><a href="#37639400">next</a><span>|</span><label class="collapse" for="c-37637815">[-]</label><label class="expand" for="c-37637815">[4 more]</label></div><br/><div class="children"><div class="content">Fine tuning is mostly useless for direct addition of knowledge.<p>You can use it to improve knowledge in indirect ways:<p>- get the model better at crafting queries for an external data source<p>- get the model better a tool usage to do computation with an external system<p>- get more useful embeddings from BERT&#x2F;SBERT<p>- tell the model what it cannot answer accurately<p>But in general, fine tuning is noise right now because 99% of the people chasing it actually don&#x27;t need it.<p>If you want to change how the model presents text, use fine tuning. If you want to change what the model can present, fine tuning is a hopeless way of doing it.</div><br/><div id="37638541" class="c"><input type="checkbox" id="c-37638541" checked=""/><div class="controls bullet"><span class="by">CMCDragonkai</span><span>|</span><a href="#37637768">root</a><span>|</span><a href="#37637815">parent</a><span>|</span><a href="#37639400">next</a><span>|</span><label class="collapse" for="c-37638541">[-]</label><label class="expand" for="c-37638541">[3 more]</label></div><br/><div class="children"><div class="content">&gt; If you want to change what the model can present, fine tuning is a hopeless way of doing it.<p>If this is what I want, then what is the right way to do it?</div><br/><div id="37639185" class="c"><input type="checkbox" id="c-37639185" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37637768">root</a><span>|</span><a href="#37638541">parent</a><span>|</span><a href="#37639400">next</a><span>|</span><label class="collapse" for="c-37639185">[-]</label><label class="expand" for="c-37639185">[2 more]</label></div><br/><div class="children"><div class="content">Put the information in the context window of the model.<p>If you&#x27;re answering questions on current events, hook it up to a news API and let it take in a user query, return a search to you, and give the result back to the LLM before it responds.<p>Right now LLMs perform best for factual information if you treat them like flexible glue between the user and your data sources. Any knowledge you leverage from the LLM itself should &quot;static&quot; knowledge that can hold true across all user queries.<p>For example, you can use the LLM&#x27;s knowledge of marketing to produce marketing copy via chain of thought. Trends in marketing copy can change, but the actual act of making marketing copy takes a static set of knowledge.<p>_<p>Don&#x27;t use the LLM to surface competitors for the product, because one person will ask about &#x27;Coke&#x27; and it&#x27;ll say &#x27;Pepsi&#x27;, and another will name their founded in 2023 company and it&#x27;ll hallucinate: <i>especially</i> if you&#x27;re asking for structured output<p>Instead let the LLM form some searches, take the results and let the LLM use chain of thought on what defines a competitor, and surface that result</div><br/><div id="37640623" class="c"><input type="checkbox" id="c-37640623" checked=""/><div class="controls bullet"><span class="by">CMCDragonkai</span><span>|</span><a href="#37637768">root</a><span>|</span><a href="#37639185">parent</a><span>|</span><a href="#37639400">next</a><span>|</span><label class="collapse" for="c-37640623">[-]</label><label class="expand" for="c-37640623">[1 more]</label></div><br/><div class="children"><div class="content">Ok but what about transfer learning the LLM, are there practical ways of building more static knowledge into the LLM? Long term memory and all that?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37639400" class="c"><input type="checkbox" id="c-37639400" checked=""/><div class="controls bullet"><span class="by">bearjaws</span><span>|</span><a href="#37637768">prev</a><span>|</span><a href="#37638770">next</a><span>|</span><label class="collapse" for="c-37639400">[-]</label><label class="expand" for="c-37639400">[5 more]</label></div><br/><div class="children"><div class="content">One thing I love to see here, mainly because I was going to make it myself, is the chart on latency.<p>GPT4 is practically unusable unless you spend $10k+ a month and have an enterprise account.<p>No real end user wants to wait 20-40 seconds for a response, only to find it was 80% of what they wanted.</div><br/><div id="37639730" class="c"><input type="checkbox" id="c-37639730" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#37639400">parent</a><span>|</span><a href="#37638770">next</a><span>|</span><label class="collapse" for="c-37639730">[-]</label><label class="expand" for="c-37639730">[4 more]</label></div><br/><div class="children"><div class="content">&gt; ...our experiment showed that fine-tuned GPT-3.5 models were significantly faster than [base GPT-3.5] by 3.6 to 3.76 times.<p>does this pass the sniff test? under what situations could a finetuned single tenant model be faster than a batchable multitenant base model?</div><br/><div id="37640091" class="c"><input type="checkbox" id="c-37640091" checked=""/><div class="controls bullet"><span class="by">eqian99</span><span>|</span><a href="#37639400">root</a><span>|</span><a href="#37639730">parent</a><span>|</span><a href="#37639810">next</a><span>|</span><label class="collapse" for="c-37640091">[-]</label><label class="expand" for="c-37640091">[1 more]</label></div><br/><div class="children"><div class="content">Yeah we were definitely surprised! This was a small scale experiment; we uploaded our notebook for latency tests here if anyone is interested taking a look at the code &#x2F; trying it their own finetuned models: <a href="https:&#x2F;&#x2F;github.com&#x2F;eqian99&#x2F;finetuning-experiment">https:&#x2F;&#x2F;github.com&#x2F;eqian99&#x2F;finetuning-experiment</a></div><br/></div></div><div id="37639810" class="c"><input type="checkbox" id="c-37639810" checked=""/><div class="controls bullet"><span class="by">gnahzby</span><span>|</span><a href="#37639400">root</a><span>|</span><a href="#37639730">parent</a><span>|</span><a href="#37640091">prev</a><span>|</span><a href="#37640226">next</a><span>|</span><label class="collapse" for="c-37639810">[-]</label><label class="expand" for="c-37639810">[1 more]</label></div><br/><div class="children"><div class="content">This result really surprised us too, but it seems like we were not alone in observing this: <a href="https:&#x2F;&#x2F;medium.com&#x2F;@gabrielgrinberg&#x2F;openai-api-fine-tuned-gpt-3-5-vs-base-gpt-3-5-a-case-study-f3619b4f8cd8" rel="nofollow noreferrer">https:&#x2F;&#x2F;medium.com&#x2F;@gabrielgrinberg&#x2F;openai-api-fine-tuned-gp...</a><p>That being said, both accounts that we experimented on were personal accounts ( with a very limited budget) so maybe there are some special behaviors (e.g. some rate limit that only exists for base models)</div><br/></div></div><div id="37640226" class="c"><input type="checkbox" id="c-37640226" checked=""/><div class="controls bullet"><span class="by">dnnssl2</span><span>|</span><a href="#37639400">root</a><span>|</span><a href="#37639730">parent</a><span>|</span><a href="#37639810">prev</a><span>|</span><a href="#37638770">next</a><span>|</span><label class="collapse" for="c-37640226">[-]</label><label class="expand" for="c-37640226">[1 more]</label></div><br/><div class="children"><div class="content">Under the same conditions where enterprise versions of the API have significantly less latency and better reliability than personal. OpenAI can change anything about the underlying infrastructure.</div><br/></div></div></div></div></div></div><div id="37638770" class="c"><input type="checkbox" id="c-37638770" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#37639400">prev</a><span>|</span><a href="#37639100">next</a><span>|</span><label class="collapse" for="c-37638770">[-]</label><label class="expand" for="c-37638770">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m surprised that this few samples made such a big difference. Perhaps the learning rate is jacked up? Otherwise, the last couple of batches during regular training would set the overall model tone.</div><br/></div></div><div id="37639100" class="c"><input type="checkbox" id="c-37639100" checked=""/><div class="controls bullet"><span class="by">TheCaptain4815</span><span>|</span><a href="#37638770">prev</a><span>|</span><label class="collapse" for="c-37639100">[-]</label><label class="expand" for="c-37639100">[1 more]</label></div><br/><div class="children"><div class="content">Does the majority of the &#x27;power&#x27; of these finetunes lay in the dataset? So once GPT4 finetuning is out (this fall), could they in theory just use the exact same datasets for GPT4 finetune?</div><br/></div></div></div></div></div></div></div></body></html>