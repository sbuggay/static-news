<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1727168471114" as="style"/><link rel="stylesheet" href="styles.css?v=1727168471114"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://publish.obsidian.md/felafax/pages/Tune+Llama3+405B+on+AMD+MI300x+(our+journey)">We fine-tuned Llama 405B on AMD GPUs</a> <span class="domain">(<a href="https://publish.obsidian.md">publish.obsidian.md</a>)</span></div><div class="subtext"><span>felarof</span> | <span>70 comments</span></div><br/><div><div id="41631152" class="c"><input type="checkbox" id="c-41631152" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41634433">next</a><span>|</span><label class="collapse" for="c-41631152">[-]</label><label class="expand" for="c-41631152">[36 more]</label></div><br/><div class="children"><div class="content">Hey HN, we recently fine-tuned the llama3.1 405B model on 8xAMD MI300x GPUs using JAX instead of PyTorch. JAX&#x27;s advanced sharding APIs allowed us to achieve great performance. Check out our blog post to learn about the cool sharding tricks we used. We&#x27;ve also open-sourced the code: <a href="https:&#x2F;&#x2F;github.com&#x2F;felafax&#x2F;felafax">https:&#x2F;&#x2F;github.com&#x2F;felafax&#x2F;felafax</a><p>We&#x27;re a small startup building AI infra for fine-tuning and serving LLMs on non-NVIDIA hardware (TPUs, AMD, Trainium).<p>Problem: Many companies are trying to get PyTorch working on AMD GPUs, but we believe this is a treacherous path. PyTorch is deeply intertwined with the NVIDIA ecosystem in a lot of ways (e.g., `torch.cuda` or scaled_dot_product_attention is an NVIDIA CUDA kernel exposed as a PyTorch function). So, to get PyTorch code running on non-NVIDIA hardware, there&#x27;s a lot of &quot;de-NVIDIAfying&quot; that needs to be done.<p>Solution: We believe JAX is a better fit for non-NVIDIA hardware. In JAX, ML model code compiles to hardware-independent HLO graphs, which are then optimized by the XLA compiler before hardware-specific optimization. This clean separation allowed us to run the same LLaMA3 JAX code both on Google TPUs and AMD GPUs with no changes.<p>Our strategy as a company is to invest upfront in porting models to JAX, then leverage its framework and XLA kernels to extract maximum performance from non-NVIDIA backends. This is why we first ported Llama 3.1 from PyTorch to JAX, and now the same JAX model works great on TPUs and runs perfectly on AMD GPUs.<p>We&#x27;d love to hear your thoughts on our vision and repo!</div><br/><div id="41631842" class="c"><input type="checkbox" id="c-41631842" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#41631152">parent</a><span>|</span><a href="#41631816">next</a><span>|</span><label class="collapse" for="c-41631842">[-]</label><label class="expand" for="c-41631842">[10 more]</label></div><br/><div class="children"><div class="content">I, and several others, had no problem running on PyTorch on AMD GPUs, with no code changes from CUDA. Check out MosaicML&#x27;s blog posts: <a href="https:&#x2F;&#x2F;www.databricks.com&#x2F;blog&#x2F;training-llms-scale-amd-mi250-gpus" rel="nofollow">https:&#x2F;&#x2F;www.databricks.com&#x2F;blog&#x2F;training-llms-scale-amd-mi25...</a></div><br/><div id="41632295" class="c"><input type="checkbox" id="c-41632295" checked=""/><div class="controls bullet"><span class="by">mistymountains</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41631842">parent</a><span>|</span><a href="#41632003">next</a><span>|</span><label class="collapse" for="c-41632295">[-]</label><label class="expand" for="c-41632295">[1 more]</label></div><br/><div class="children"><div class="content">Again, the problem is custom kernels in CUDA. It’s not straightforward for many applications (LLMs are probably the most straightforward).</div><br/></div></div><div id="41632003" class="c"><input type="checkbox" id="c-41632003" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41631842">parent</a><span>|</span><a href="#41632295">prev</a><span>|</span><a href="#41631816">next</a><span>|</span><label class="collapse" for="c-41632003">[-]</label><label class="expand" for="c-41632003">[8 more]</label></div><br/><div class="children"><div class="content">Ahh, interesting, will take a look!<p>Curious what are the steps to run PyTorch on AMD (does it work out-of-box with PyTorch+rocm docker image)? Does torch.compile work smoothly?</div><br/><div id="41633839" class="c"><input type="checkbox" id="c-41633839" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41632003">parent</a><span>|</span><a href="#41632072">next</a><span>|</span><label class="collapse" for="c-41633839">[-]</label><label class="expand" for="c-41633839">[1 more]</label></div><br/><div class="children"><div class="content">While your project is neat and I&#x27;d like to see how the performance compares, for LLM training, PyTorch, including torch.compile works completely OOTB on AMD.<p>All you have to do is pip install the ROCm version of PyTorch (or run the docker image) and it&#x27;s seamless (the ROCm version just treats torch.cuda as calling ROCm).<p>I&#x27;ve used axolotl (trl&#x2F;accelerate based), torchtune, and LLaMA-Factory, which are all PyTorch-based without any issues for training.</div><br/></div></div><div id="41632072" class="c"><input type="checkbox" id="c-41632072" checked=""/><div class="controls bullet"><span class="by">anthonix1</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41632003">parent</a><span>|</span><a href="#41633839">prev</a><span>|</span><a href="#41631816">next</a><span>|</span><label class="collapse" for="c-41632072">[-]</label><label class="expand" for="c-41632072">[6 more]</label></div><br/><div class="children"><div class="content">Yeah I would suggest taking a look at PyTorch on AMD before saying stuff like &quot;scaled_dot_product_attention is an NVIDIA CUDA kernel exposed as a PyTorch function&quot;, because that is demonstrably false.<p>Also, FWIW, I would suggest getting a small Llama 3.1 model training fast before trying to do a big 405B model -- faster to iterate and almost everything you&#x27;ll learn on the small models will scale to the 405B.</div><br/><div id="41632252" class="c"><input type="checkbox" id="c-41632252" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41632072">parent</a><span>|</span><a href="#41631816">next</a><span>|</span><label class="collapse" for="c-41632252">[-]</label><label class="expand" for="c-41632252">[5 more]</label></div><br/><div class="children"><div class="content">Thanks for the feedback! I appreciate you pointing that out. My understanding was based on the PyTorch documentation for scaled_dot_product_attention (<a href="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;generated&#x2F;torch.nn.functional.scaled_dot_product_attention.html" rel="nofollow">https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;generated&#x2F;torch.nn.functiona...</a>). 
- &quot;The function may call optimized kernels for improved performance when using the CUDA backend. For all other backends, the PyTorch implementation will be used.&quot;<p>And was trying to make a broader point about the lack of transparency (in performance, lower-level impl) in PyTorch when running on NVIDIA vs. non-NVIDIA hardware.</div><br/><div id="41634396" class="c"><input type="checkbox" id="c-41634396" checked=""/><div class="controls bullet"><span class="by">chillee</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41632252">parent</a><span>|</span><a href="#41632620">next</a><span>|</span><label class="collapse" for="c-41634396">[-]</label><label class="expand" for="c-41634396">[1 more]</label></div><br/><div class="children"><div class="content">&gt; And was trying to make a broader point about the lack of transparency (in performance, lower-level impl) in PyTorch when running on NVIDIA vs. non-NVIDIA hardware.<p>I don&#x27;t quite understand this argument. Lack of transparency from running PyTorch so instead we&#x27;re gonna leave it all to XLA? How does this solve the &quot;transparency&quot; issue?</div><br/></div></div><div id="41632620" class="c"><input type="checkbox" id="c-41632620" checked=""/><div class="controls bullet"><span class="by">anthonix1</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41632252">parent</a><span>|</span><a href="#41634396">prev</a><span>|</span><a href="#41631816">next</a><span>|</span><label class="collapse" for="c-41632620">[-]</label><label class="expand" for="c-41632620">[3 more]</label></div><br/><div class="children"><div class="content">I just asked an instance of Hermes 3 Llama 3.1 405B running on AMD GPUs: &quot;does pytorch scaled dot product attention run on AMD GPUs?&quot;:<p>&quot;Yes, PyTorch&#x27;s scaled dot product attention can run on AMD GPUs. PyTorch supports AMD GPUs through the ROCm (Radeon Open Compute) platform. To use PyTorch with an AMD GPU, you need to install the ROCm version of PyTorch, which is specifically built for AMD GPUs [...]&quot;<p>And it proceeded to give the steps to follow to install and run, with example Python code to demonstrate it. One slight nitpick is that it referred to an older URL with the --index-url to install torch with pip, but otherwise it was correct.</div><br/><div id="41633572" class="c"><input type="checkbox" id="c-41633572" checked=""/><div class="controls bullet"><span class="by">unlikelymordant</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41632620">parent</a><span>|</span><a href="#41633690">next</a><span>|</span><label class="collapse" for="c-41633572">[-]</label><label class="expand" for="c-41633572">[1 more]</label></div><br/><div class="children"><div class="content">are you at all confident that this isn&#x27;t hallucinated? I&#x27;d never trust an answer like this from an LLM</div><br/></div></div><div id="41633690" class="c"><input type="checkbox" id="c-41633690" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41632620">parent</a><span>|</span><a href="#41633572">prev</a><span>|</span><a href="#41631816">next</a><span>|</span><label class="collapse" for="c-41633690">[-]</label><label class="expand" for="c-41633690">[1 more]</label></div><br/><div class="children"><div class="content">Did you verify everything else it said is true?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41631816" class="c"><input type="checkbox" id="c-41631816" checked=""/><div class="controls bullet"><span class="by">anthonix1</span><span>|</span><a href="#41631152">parent</a><span>|</span><a href="#41631842">prev</a><span>|</span><a href="#41631662">next</a><span>|</span><label class="collapse" for="c-41631816">[-]</label><label class="expand" for="c-41631816">[1 more]</label></div><br/><div class="children"><div class="content">Does JAX have its own implementations of matmul, flash attention etc? Or does it use the ROCm implementations like PyTorch does? (e.g,. hipblaslt, Composable Kernel FA etc)<p>Not too familiar with JAX, but the abysmal PyTorch training perf on MI300x is in large part attributable to the slow perf of the ROCm libraries it is using under the hood.</div><br/></div></div><div id="41631662" class="c"><input type="checkbox" id="c-41631662" checked=""/><div class="controls bullet"><span class="by">germanjoey</span><span>|</span><a href="#41631152">parent</a><span>|</span><a href="#41631816">prev</a><span>|</span><a href="#41631646">next</a><span>|</span><label class="collapse" for="c-41631662">[-]</label><label class="expand" for="c-41631662">[4 more]</label></div><br/><div class="children"><div class="content">How are you verifying accuracy for your JAX port of Llama 3.1?<p>IMHO, the main reason to use pytorch is actually that the original model used pytorch. What can seem to be identical logic between different model versions may actually cause model drift when infinitesimal floating point errors accumulate due to the huge scale of the data. My experience is that debugging an accuracy mismatches like this in a big model is a torturous ordeal beyond the 10th circle of hell.</div><br/><div id="41632498" class="c"><input type="checkbox" id="c-41632498" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41631662">parent</a><span>|</span><a href="#41631893">next</a><span>|</span><label class="collapse" for="c-41632498">[-]</label><label class="expand" for="c-41632498">[1 more]</label></div><br/><div class="children"><div class="content">Good question. We used a new AI+math-based testing tool (benchify.com) to run comparison tests, but we are working on building more robust infrastructure for this. Translating models from PyTorch to JAX is core to our strategy.<p>That said, this path is not uncommon (translating from one framework to another). HuggingFace translates Google&#x27;s Gemma family models from JAX to PyTorch, and a ton of people use it.</div><br/></div></div><div id="41631893" class="c"><input type="checkbox" id="c-41631893" checked=""/><div class="controls bullet"><span class="by">credit_guy</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41631662">parent</a><span>|</span><a href="#41632498">prev</a><span>|</span><a href="#41631769">next</a><span>|</span><label class="collapse" for="c-41631893">[-]</label><label class="expand" for="c-41631893">[1 more]</label></div><br/><div class="children"><div class="content">When you say &quot;model versions&quot;, do you mean different quantizations of the model? Then it&#x27;s not floating point errors that accumulate. Different quantizations of the model are different models. People will call such a model something like Meta-Llama-3.1-8B-Instruct--q4_0, claiming that it&#x27;s just a &quot;version&quot; of the Meta-Llama-3.1-8B-Instruct. But it&#x27;s just a lie. It&#x27;s not the same model, and you should not expect the same results. There is no reason to debug the differences, what exactly would you expect to find, and what action would you envision to take once you find what you are looking for? However, is the quantized version still a useful LLM? Absolutely. Most people don&#x27;t have an A100 to run the original model, so a quantized version is better than nothing.</div><br/></div></div><div id="41631769" class="c"><input type="checkbox" id="c-41631769" checked=""/><div class="controls bullet"><span class="by">srcreigh</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41631662">parent</a><span>|</span><a href="#41631893">prev</a><span>|</span><a href="#41631646">next</a><span>|</span><label class="collapse" for="c-41631769">[-]</label><label class="expand" for="c-41631769">[1 more]</label></div><br/><div class="children"><div class="content">Very fascinating, can you explain more about a time when this happened?<p>Like what area was affected by fp errors, why were they introduced (was it like refactoring of pytorch code?), how was this determined to be the cause?</div><br/></div></div></div></div><div id="41631646" class="c"><input type="checkbox" id="c-41631646" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#41631152">parent</a><span>|</span><a href="#41631662">prev</a><span>|</span><a href="#41634271">next</a><span>|</span><label class="collapse" for="c-41631646">[-]</label><label class="expand" for="c-41631646">[2 more]</label></div><br/><div class="children"><div class="content">Does this work on the consumer grade cards like the 7090 XTX?<p>And by work I don&#x27;t mean: spend two weeks trying to get the drivers set up and never update the server again.</div><br/><div id="41633868" class="c"><input type="checkbox" id="c-41633868" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41631646">parent</a><span>|</span><a href="#41634271">next</a><span>|</span><label class="collapse" for="c-41633868">[-]</label><label class="expand" for="c-41633868">[1 more]</label></div><br/><div class="children"><div class="content">A couple months ago I did some testing on some consumer cards. [1] I think you should be able to use torchtune or axolotl without anything besides installing the ROCm version of PyTorch.<p>[1] <a href="https:&#x2F;&#x2F;wandb.ai&#x2F;augmxnt&#x2F;train-bench&#x2F;reports&#x2F;Trainer-performance-comparison-torchtune-vs-axolotl-vs-Unsloth---Vmlldzo4MzU3NTAx" rel="nofollow">https:&#x2F;&#x2F;wandb.ai&#x2F;augmxnt&#x2F;train-bench&#x2F;reports&#x2F;Trainer-perform...</a></div><br/></div></div></div></div><div id="41634271" class="c"><input type="checkbox" id="c-41634271" checked=""/><div class="controls bullet"><span class="by">upbeat_general</span><span>|</span><a href="#41631152">parent</a><span>|</span><a href="#41631646">prev</a><span>|</span><a href="#41631611">next</a><span>|</span><label class="collapse" for="c-41634271">[-]</label><label class="expand" for="c-41634271">[1 more]</label></div><br/><div class="children"><div class="content">scaled_dot_product_attention isn’t CUDA specific, it even works on TPUs.</div><br/></div></div><div id="41631611" class="c"><input type="checkbox" id="c-41631611" checked=""/><div class="controls bullet"><span class="by">cameron_b</span><span>|</span><a href="#41631152">parent</a><span>|</span><a href="#41634271">prev</a><span>|</span><a href="#41631926">next</a><span>|</span><label class="collapse" for="c-41631611">[-]</label><label class="expand" for="c-41631611">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad to see a full implementation on AMD hardware.<p>I&#x27;m not familiar with JAX, but the idea of providing an abstraction layer to more easily get to work on what hardware is available seems really valuable. Bringing back some competitiveness to the ecosystem will be a big win for workload mobility.<p>I suspect that price&#x2F;performance across implementations will be highly dependent on contract details, but do you intend to publish some comparisons in the future?</div><br/></div></div><div id="41631926" class="c"><input type="checkbox" id="c-41631926" checked=""/><div class="controls bullet"><span class="by">anthonix1</span><span>|</span><a href="#41631152">parent</a><span>|</span><a href="#41631611">prev</a><span>|</span><a href="#41633824">next</a><span>|</span><label class="collapse" for="c-41631926">[-]</label><label class="expand" for="c-41631926">[8 more]</label></div><br/><div class="children"><div class="content">Any direct comparisons to 8xH100? 2 toks&#x2F;sec seems <i>very</i> slow!<p>I haven&#x27;t done any LoRA training on MI300x myself, but I have done LLama 3.1 full training on 8xMI300x and got pretty close to 8xH100 performance with my own kernels (ROCm is just too slow).</div><br/><div id="41632530" class="c"><input type="checkbox" id="c-41632530" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41631926">parent</a><span>|</span><a href="#41633824">next</a><span>|</span><label class="collapse" for="c-41632530">[-]</label><label class="expand" for="c-41632530">[7 more]</label></div><br/><div class="children"><div class="content">Oops, my calculation was wrong. Let me add an edit to the blog, thanks for pointing it out!<p>My train step was taking 30s.<p>And I was using a batch size of 16 and seq length of 64, making the training speed as (16*64&#x2F;30) tokens per sec == 35 tokens per second (for fine-tuning in JAX eager mode).<p>(I haven&#x27;t done comparison with 8XH100)</div><br/><div id="41633758" class="c"><input type="checkbox" id="c-41633758" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41632530">parent</a><span>|</span><a href="#41633824">next</a><span>|</span><label class="collapse" for="c-41633758">[-]</label><label class="expand" for="c-41633758">[6 more]</label></div><br/><div class="children"><div class="content">That’s approximately 0.8% MFU - h100 would get more like 30% or 40% MFU if well tuned<p>405e9 parameters<p>2 flops per matrix multiply per parameter<p>3 matrix multiplies for (forward, backward param, and backward activation) passes<p>batch size 16<p>seq length 64<p>1.3 petaflops per second per GPU in bfloat16<p>8 GPUs<p>30 seconds<p>So that’s 0.8% = (405e9 * 2 * 3 * 16 * 64 &#x2F; 30) &#x2F; (1.3e15 * 8)<p>Note that I’m ignoring the attention flops in this simplified calculation, but they would be a second order effect at this sequence length<p>Also note that I’m assuming full weight training, not LoRA .  The result would be lower MFU if using LoRA<p>These MI300X results are promising functionally (it&#x27;s tough to get any model this big running) but they have a long way to go on perf. It&#x27;s also single node. The biggest issues I&#x27;ve seen on MI300X are related to scaling to multiple nodes.<p>EDIT: The blog seems to indicate it is using LoRA. So we should remove the backward param pass from the equation above.  Backward param only applies to adaptor weights, which are much more than 10x smaller, so we set it to 0 in the approximation. So we get<p>0.53% = (405e9 * 2 * 2 * 16 * 64 &#x2F; 30) &#x2F; (1.3e15 * 8)</div><br/><div id="41633980" class="c"><input type="checkbox" id="c-41633980" checked=""/><div class="controls bullet"><span class="by">heyoni</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41633758">parent</a><span>|</span><a href="#41633986">next</a><span>|</span><label class="collapse" for="c-41633980">[-]</label><label class="expand" for="c-41633980">[3 more]</label></div><br/><div class="children"><div class="content">If I feed your reply to an llm will it be able to tell me what you’re talking about?<p>&#x2F;edit I think it did alright…</div><br/><div id="41634028" class="c"><input type="checkbox" id="c-41634028" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41633980">parent</a><span>|</span><a href="#41633986">next</a><span>|</span><label class="collapse" for="c-41634028">[-]</label><label class="expand" for="c-41634028">[2 more]</label></div><br/><div class="children"><div class="content">I tried with Llama 3.1 and it hallucinated MFU.  It also completely misinterpreted multi-node scaling.<p>==========================================================<p>Let&#x27;s break down the results described in the post.<p>Context: The post is discussing the performance of a large language model on a MI300X GPU, which is a high-performance computing (HPC) system. The model has approximately 405 billion parameters and is trained using a batch size of 16 and sequence length of 64.<p>Key metrics:<p>MFU (Million Floating-Point Operations per Second): This is a measure of the model&#x27;s performance, specifically the number of floating-point operations (FLOPs) it can perform per second.<p>FLOPs: The number of floating-point operations required to perform a matrix multiplication, which is a fundamental operation in deep learning.<p>GPU performance: The MI300X GPU is capable of 1.3 petaflops (1.3 x 10^15 FLOPs) per second in bfloat16 (a 16-bit floating-point format).<p>Calculations:<p>The author provides two calculations to estimate the MFU of the model:<p>Initial calculation: Assuming full weight training (not LoRA), the author estimates the MFU as:<p>405 billion parameters<p>2 FLOPs per matrix multiply per parameter<p>3 matrix multiplies (forward, backward parameter, and backward activation)<p>Batch size 16<p>Sequence length 64<p>30 seconds to complete the calculation<p>1.3 petaflops per second per GPU<p>8 GPUs<p>The calculation yields an MFU of approximately 0.8%.<p>Revised calculation: After correcting the assumption to use LoRA (a technique that reduces the number of FLOPs), the author revises the calculation by removing the backward parameter pass, which is only applied to adaptor weights (much smaller than the main weights). This yields an MFU of approximately 0.53%.<p>Interpretation:<p>The results indicate that the MI300X GPU is not yet optimized for this large language model, with an MFU of only 0.53% (or 0.8% in the initial calculation). This is a relatively low performance compared to the theoretical maximum of 1.3 petaflops per second per GPU. The author notes that the biggest issues are related to scaling to multiple nodes, suggesting that the performance may improve when running on a larger cluster.<p>The revised calculation using LoRA reduces the MFU by about 33%, indicating that using this technique can lead to a more efficient use of the GPU resources.<p>==========================================================<p>MFU means model flops utilitization.  it is a measure of efficiency from 0% to 100% .  100% means that the model is running at maximum possible efficiency, i.e. 1.3 petaflops per gpu.<p>==========================================================<p>In that case, the results indicate that the MI300X GPU is running the large language model at a relatively low efficiency, with an MFU of 0.53% (or 0.8% in the initial calculation).<p>This means that the model is only utilizing a tiny fraction of the GPU&#x27;s maximum theoretical performance of 1.3 petaflops per second. In other words, the model is not fully utilizing the GPU&#x27;s capabilities, and there is a significant amount of headroom for optimization.<p>To put this into perspective, an MFU of 100% would mean that the model is running at the maximum possible efficiency, using 1.3 petaflops per second per GPU. An MFU of 0.53% or 0.8% is extremely low, indicating that the model is running at a tiny fraction of its potential performance.<p>The author&#x27;s comment that the MI300X results are &quot;promising functionally&quot; suggests that the model is able to run, but the low MFU indicates that there are significant opportunities for optimization and performance improvement.</div><br/><div id="41634438" class="c"><input type="checkbox" id="c-41634438" checked=""/><div class="controls bullet"><span class="by">heyoni</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41634028">parent</a><span>|</span><a href="#41633986">next</a><span>|</span><label class="collapse" for="c-41634438">[-]</label><label class="expand" for="c-41634438">[1 more]</label></div><br/><div class="children"><div class="content">MFU being maximal floating point utilization right? I used chatgpt 4o and it otherwise looks very similar to your llama result</div><br/></div></div></div></div></div></div><div id="41633986" class="c"><input type="checkbox" id="c-41633986" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41633758">parent</a><span>|</span><a href="#41633980">prev</a><span>|</span><a href="#41633824">next</a><span>|</span><label class="collapse" for="c-41633986">[-]</label><label class="expand" for="c-41633986">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The blog seems to indicate it is using LoRA. So we should remove the backward param pass from the equation above. Backward param only applies to adaptor weights<p>Backward pass still runs on the non adapter weights. But yeah 10 TFlops&#x2F;GPU specially on tiny sequence size is very bad compared to what you can get on Nvidia. And I believe the difference would be even higher with large sequence length.</div><br/><div id="41634049" class="c"><input type="checkbox" id="c-41634049" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41633986">parent</a><span>|</span><a href="#41633824">next</a><span>|</span><label class="collapse" for="c-41634049">[-]</label><label class="expand" for="c-41634049">[1 more]</label></div><br/><div class="children"><div class="content">backward activations does but typically not backwards weight gradients.<p>Why compute gradients with regards to weights that aren&#x27;t going to be updated?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41633824" class="c"><input type="checkbox" id="c-41633824" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#41631152">parent</a><span>|</span><a href="#41631926">prev</a><span>|</span><a href="#41631362">next</a><span>|</span><label class="collapse" for="c-41633824">[-]</label><label class="expand" for="c-41633824">[1 more]</label></div><br/><div class="children"><div class="content">Did you consider using <a href="https:&#x2F;&#x2F;github.com&#x2F;AI-Hypercomputer&#x2F;maxtext">https:&#x2F;&#x2F;github.com&#x2F;AI-Hypercomputer&#x2F;maxtext</a> ? It has a Jax llama implementation, and gets decent MFU on TPU and GPU (I&#x27;ve only tried it on NVidia GPU, not AMD).</div><br/></div></div><div id="41631362" class="c"><input type="checkbox" id="c-41631362" checked=""/><div class="controls bullet"><span class="by">jgalt212</span><span>|</span><a href="#41631152">parent</a><span>|</span><a href="#41633824">prev</a><span>|</span><a href="#41631278">next</a><span>|</span><label class="collapse" for="c-41631362">[-]</label><label class="expand" for="c-41631362">[4 more]</label></div><br/><div class="children"><div class="content">Is there some cost rule of thumb to compare Nvidia, AMD, and Google TPU?</div><br/><div id="41631984" class="c"><input type="checkbox" id="c-41631984" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41631362">parent</a><span>|</span><a href="#41631587">next</a><span>|</span><label class="collapse" for="c-41631984">[-]</label><label class="expand" for="c-41631984">[1 more]</label></div><br/><div class="children"><div class="content">Good question. No good metric give performance depends on software stack (JAX vs PyTorch) + optimizations.<p>But my take performance per dollar of TPU &gt; AMD &gt; NVIDIA.</div><br/></div></div><div id="41631587" class="c"><input type="checkbox" id="c-41631587" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41631362">parent</a><span>|</span><a href="#41631984">prev</a><span>|</span><a href="#41631278">next</a><span>|</span><label class="collapse" for="c-41631587">[-]</label><label class="expand" for="c-41631587">[2 more]</label></div><br/><div class="children"><div class="content">TPUs are slow but efficient and AMD has bugs but for some things works quite well.  Nvidia is obviously the gold standard.</div><br/><div id="41631978" class="c"><input type="checkbox" id="c-41631978" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41631587">parent</a><span>|</span><a href="#41631278">next</a><span>|</span><label class="collapse" for="c-41631978">[-]</label><label class="expand" for="c-41631978">[1 more]</label></div><br/><div class="children"><div class="content">Haha, TPUs are not slow :) All of Google&#x27;s training (including Gemini models) is done on TPUs.<p>There are good 1p [a] and 3p [b] benchmarks comparing TPUs vs NVIDIA GPUs.<p>[a] - <a href="https:&#x2F;&#x2F;github.com&#x2F;GoogleCloudPlatform&#x2F;vertex-ai-samples&#x2F;blob&#x2F;main&#x2F;community-content&#x2F;vertex_model_garden&#x2F;benchmarking_reports&#x2F;jax_vit_benchmarking_report.md">https:&#x2F;&#x2F;github.com&#x2F;GoogleCloudPlatform&#x2F;vertex-ai-samples&#x2F;blo...</a><p>[b] - <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.07181" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.07181</a></div><br/></div></div></div></div></div></div><div id="41631278" class="c"><input type="checkbox" id="c-41631278" checked=""/><div class="controls bullet"><span class="by">ngcc_hk</span><span>|</span><a href="#41631152">parent</a><span>|</span><a href="#41631362">prev</a><span>|</span><a href="#41634433">next</a><span>|</span><label class="collapse" for="c-41631278">[-]</label><label class="expand" for="c-41631278">[3 more]</label></div><br/><div class="children"><div class="content">Given it is a migration, is there actual comparison of the same model on PyTorch vs your version.  The comparison table there seems to be on technical side.<p>Also any technical issues encountered?</div><br/><div id="41632574" class="c"><input type="checkbox" id="c-41632574" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41631278">parent</a><span>|</span><a href="#41634433">next</a><span>|</span><label class="collapse" for="c-41632574">[-]</label><label class="expand" for="c-41632574">[2 more]</label></div><br/><div class="children"><div class="content">We have a few technical issues that we still need to address:<p>1) This entire fine-tuning run was done in JAX eager mode. I kept running out of memory (OOM) when trying to `jax.jit` the entire training step. Even gradual `jax.jit` didn&#x27;t work.<p>2) The current version doesn&#x27;t have gradient accumulation, and with a batch size of just 16, that’s not ideal. I&#x27;m working on implementing gradient accumulation next.<p>3) We still haven&#x27;t found a good way to load large sequence-length data (like 32k sequence length). Currently, before sharding the training batch across GPUs, it ends up loading the entire batch onto a single GPU’s VRAM and causes OOM issues.</div><br/><div id="41633835" class="c"><input type="checkbox" id="c-41633835" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#41631152">root</a><span>|</span><a href="#41632574">parent</a><span>|</span><a href="#41634433">next</a><span>|</span><label class="collapse" for="c-41633835">[-]</label><label class="expand" for="c-41633835">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I kept running out of memory (OOM) when trying to `jax.jit` the entire training step. Even gradual `jax.jit` didn&#x27;t work.<p>Were you using activation checkpointing? <a href="https:&#x2F;&#x2F;jax.readthedocs.io&#x2F;en&#x2F;latest&#x2F;_autosummary&#x2F;jax.checkpoint.html" rel="nofollow">https:&#x2F;&#x2F;jax.readthedocs.io&#x2F;en&#x2F;latest&#x2F;_autosummary&#x2F;jax.checkp...</a> is very important for keeping memory usage reasonable when training large models.</div><br/></div></div></div></div></div></div></div></div><div id="41634433" class="c"><input type="checkbox" id="c-41634433" checked=""/><div class="controls bullet"><span class="by">chillee</span><span>|</span><a href="#41631152">prev</a><span>|</span><a href="#41634249">next</a><span>|</span><label class="collapse" for="c-41634433">[-]</label><label class="expand" for="c-41634433">[1 more]</label></div><br/><div class="children"><div class="content">To be clear, this performance is quite bad (presumably because you didn&#x27;t manage to get compilation working).<p>You&#x27;re getting 35 tokens&#x2F;s for a 405B model, which comes out to about 85 Teraflops. 8 MI300x GPUs comes out to 10.4 <i>Peta</i>flops, so you&#x27;re getting about 0.8% MFU (which is about 40-50x worse than decent training performance of 30-40% MFU).<p>For AMD&#x27;s sake, I hope that it&#x27;s your software stack that&#x27;s limiting perf.</div><br/></div></div><div id="41634249" class="c"><input type="checkbox" id="c-41634249" checked=""/><div class="controls bullet"><span class="by">steeve</span><span>|</span><a href="#41634433">prev</a><span>|</span><a href="#41631325">next</a><span>|</span><label class="collapse" for="c-41634249">[-]</label><label class="expand" for="c-41634249">[1 more]</label></div><br/><div class="children"><div class="content">We (ZML) measured MI300X at 30% faster than H100. These are great chips!</div><br/></div></div><div id="41631325" class="c"><input type="checkbox" id="c-41631325" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#41634249">prev</a><span>|</span><a href="#41633720">next</a><span>|</span><label class="collapse" for="c-41631325">[-]</label><label class="expand" for="c-41631325">[12 more]</label></div><br/><div class="children"><div class="content">Firstly great work! I dabbled with AMD GPUs and ROCm support a year ago, and it was obvious AMD still a long way from catch ling up with Nvidia. While opting for JAX is in an interesting approach, what were the challenges for you deviating from pytorch (being the standard library for ML)?</div><br/><div id="41631961" class="c"><input type="checkbox" id="c-41631961" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41631325">parent</a><span>|</span><a href="#41631410">next</a><span>|</span><label class="collapse" for="c-41631961">[-]</label><label class="expand" for="c-41631961">[1 more]</label></div><br/><div class="children"><div class="content">A few weeks ago, I did a Show HN explaining our journey: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41512142">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41512142</a>.<p>We initially started with the goal of fine-tuning LLaMA 3 on TPUs, but PyTorch XLA was clunky, so we decided to rewrite the model in JAX. That said, as mentioned earlier in the thread, we also believe JAX is a better platform for non-NVIDIA GPUs and want to build on JAX+openXLA for building infra for non-NVIDIA GPUs.</div><br/></div></div><div id="41631410" class="c"><input type="checkbox" id="c-41631410" checked=""/><div class="controls bullet"><span class="by">6y56h56</span><span>|</span><a href="#41631325">parent</a><span>|</span><a href="#41631961">prev</a><span>|</span><a href="#41633720">next</a><span>|</span><label class="collapse" for="c-41631410">[-]</label><label class="expand" for="c-41631410">[10 more]</label></div><br/><div class="children"><div class="content">I cannot get AMD ROCm running on my debian 12 system which is what I think is causing Ollama to use CPU instead of GPU. So I guess there is still a long way to go.</div><br/><div id="41632108" class="c"><input type="checkbox" id="c-41632108" checked=""/><div class="controls bullet"><span class="by">jchw</span><span>|</span><a href="#41631325">root</a><span>|</span><a href="#41631410">parent</a><span>|</span><a href="#41632163">next</a><span>|</span><label class="collapse" for="c-41632108">[-]</label><label class="expand" for="c-41632108">[2 more]</label></div><br/><div class="children"><div class="content">At the risk of pissing people off, I think you may be better served by a distribution that provides a more up-to-date kernel. Debian 12 will give you Linux 6.1 LTS, which is probably OK if you&#x27;re using an older Radeon card, but I&#x27;ve heard support for the 7900 XT&#x2F;X series is a bit dicey and beyond that (e.g. Radeon 890M) non-existent.<p>If there were improvements on the AMDGPU DRM driver side, you would not see them in Debian any time soon, as the 6.1 LTS kernel will be stuck with roughly whatever shipped January of last year. This is just a shortcoming in the Linux kernel, due to its lack of any kind of stable ABI for drivers.<p>Of course it is possible this would help nothing or even hurt. My experience running stable (or even newer) kernels has been quite good, though. I run stable or newer across a few devices and run into hiccups not more than once every few years, which is definitely worth it to be able to get new driver improvements years in advance.<p>(FWIW Debian is not even supported by ROCm[1]... although distros with even older kernels are. But, even if ROCm works, I can&#x27;t imagine you will get ideal hardware support when running older kernels. I am not sure if ROCm has some workaround for enterprise Linux distributions specifically, but it feels like they must, given how many of their customers in the datacenter are likely to want to use them.)<p>[1]: <a href="https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;en&#x2F;latest&#x2F;compatibility&#x2F;compatibility-matrix.html" rel="nofollow">https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;en&#x2F;latest&#x2F;compatibility&#x2F;compatibil...</a></div><br/><div id="41633047" class="c"><input type="checkbox" id="c-41633047" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#41631325">root</a><span>|</span><a href="#41632108">parent</a><span>|</span><a href="#41632163">next</a><span>|</span><label class="collapse" for="c-41633047">[-]</label><label class="expand" for="c-41633047">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ve heard support for the 7900 XT&#x2F;X series is a bit dicey<p>The firmware-amd-graphics package in stable is too old to properly support RDNA 3. It kind of works, but it is quite buggy. All RDNA 3 users on Debian 12 should be sure to install the kernel and firmware from bookworm-backports.<p>There is full support for RDNA 3 hardware enabled on Debian Testing (both in the drivers and runtime libraries). The Debian ROCm Team intended to backport all the ROCm packages from Testing into Bookworm, but have been held up as LLVM 17 is not available in bookworm-backports (yet?).<p>&gt; FWIW Debian is not even supported by ROCm<p>ROCm does not support Debian, but Debian supports ROCm. Most of the libraries that comprise ROCm have been directly packaged by the distribution.</div><br/></div></div></div></div><div id="41632163" class="c"><input type="checkbox" id="c-41632163" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#41631325">root</a><span>|</span><a href="#41631410">parent</a><span>|</span><a href="#41632108">prev</a><span>|</span><a href="#41631457">next</a><span>|</span><label class="collapse" for="c-41632163">[-]</label><label class="expand" for="c-41632163">[1 more]</label></div><br/><div class="children"><div class="content">Like everything in machine learning it only really runs on Ubuntu 22.04. Anything else is unsupported and you need to spend weeks tinkering to get it to work, then never upgrade.</div><br/></div></div><div id="41631457" class="c"><input type="checkbox" id="c-41631457" checked=""/><div class="controls bullet"><span class="by">ants_everywhere</span><span>|</span><a href="#41631325">root</a><span>|</span><a href="#41631410">parent</a><span>|</span><a href="#41632163">prev</a><span>|</span><a href="#41631680">next</a><span>|</span><label class="collapse" for="c-41631457">[-]</label><label class="expand" for="c-41631457">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had more luck with the ROCm docker container. I run it via k8s. It was pretty painless to set up and has been mostly painless since. Prior to that it was nearly impossible to get Jax running reliably on ROCm.<p>Even with the container, you have to be careful installing Python libraries because they can still break things.</div><br/><div id="41631636" class="c"><input type="checkbox" id="c-41631636" checked=""/><div class="controls bullet"><span class="by">lenova</span><span>|</span><a href="#41631325">root</a><span>|</span><a href="#41631457">parent</a><span>|</span><a href="#41631680">next</a><span>|</span><label class="collapse" for="c-41631636">[-]</label><label class="expand" for="c-41631636">[2 more]</label></div><br/><div class="children"><div class="content">I just recently went down the AMD GPU + ROCm rabbit hole as well. ROCm 6.2 was just released in August of this year and introduces a lot better support, though as the above poster mentioned, isn&#x27;t merged into most recent OSes.<p>This Github repo is good for tracking the latest Ubuntu + ROCm install process:
<a href="https:&#x2F;&#x2F;github.com&#x2F;nktice&#x2F;AMD-AI">https:&#x2F;&#x2F;github.com&#x2F;nktice&#x2F;AMD-AI</a></div><br/><div id="41632613" class="c"><input type="checkbox" id="c-41632613" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#41631325">root</a><span>|</span><a href="#41631636">parent</a><span>|</span><a href="#41631680">next</a><span>|</span><label class="collapse" for="c-41632613">[-]</label><label class="expand" for="c-41632613">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a nice repo of random installation notes. Very helpful, thanks!</div><br/></div></div></div></div></div></div><div id="41631680" class="c"><input type="checkbox" id="c-41631680" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#41631325">root</a><span>|</span><a href="#41631410">parent</a><span>|</span><a href="#41631457">prev</a><span>|</span><a href="#41633720">next</a><span>|</span><label class="collapse" for="c-41631680">[-]</label><label class="expand" for="c-41631680">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;d probably have a lot better luck using Vulkan acceleration (not ROCm) of llama.cpp as backend to ollama. It is incomparibly easier to set up and maintain compared to ROCm. You can actually do it on your computer&#x27;s normal OS instead of inside a bunch of container&#x2F;vms where the system libs are entirely customized to running just that one application.<p>AMD&#x27;s support of consumer cards is very, very short. By the time it&#x27;s stable enough for a new card to run the card is no longer supported. In 2021 I bought an AMD GPU that came out 3 years before and 1 year after I bought it (4 years since release) they dropped ROCm support.</div><br/><div id="41633204" class="c"><input type="checkbox" id="c-41633204" checked=""/><div class="controls bullet"><span class="by">coppsilgold</span><span>|</span><a href="#41631325">root</a><span>|</span><a href="#41631680">parent</a><span>|</span><a href="#41633720">next</a><span>|</span><label class="collapse" for="c-41633204">[-]</label><label class="expand" for="c-41633204">[2 more]</label></div><br/><div class="children"><div class="content">ROCm is not even worth the effort for inference workloads. Vulkan is much more convenient and performs fine.<p>llama.cpp and stable-diffusion.cpp offer Vulkan backends but generally you can run most models on Vulkan if you use IREE[1].<p>[1] &lt;<a href="https:&#x2F;&#x2F;iree.dev&#x2F;guides&#x2F;ml-frameworks&#x2F;" rel="nofollow">https:&#x2F;&#x2F;iree.dev&#x2F;guides&#x2F;ml-frameworks&#x2F;</a>&gt;</div><br/><div id="41634232" class="c"><input type="checkbox" id="c-41634232" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#41631325">root</a><span>|</span><a href="#41633204">parent</a><span>|</span><a href="#41633720">next</a><span>|</span><label class="collapse" for="c-41634232">[-]</label><label class="expand" for="c-41634232">[1 more]</label></div><br/><div class="children"><div class="content">While Vulkan can be a good fallback, for LLM inference at least, the performance difference is not as insignificant as you believe. I just ran a test on the latest pull just to make sure this is still the case on llama.cpp HEAD, but text generation is +44% faster and prompt processing is +202% (~3X) faster with ROCm vs Vulkan.<p>Note: if you&#x27;re building llama.cpp, all you have to do is swap GGML_HIPBLAS=1 and GGML_VULKAN=1 so the extra effort is just installing ROCm? (vs the Vulkan devtools)<p>ROCm:<p><pre><code>  CUDA_VISIBLE_DEVICES=1 .&#x2F;llama-bench -m &#x2F;models&#x2F;gguf&#x2F;llama-2-7b.Q4_0.gguf
  ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
  ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
  ggml_cuda_init: found 1 ROCm devices:
  Device 0: Radeon RX 7900 XTX, compute capability 11.0, VMM: no
  | model                          |       size |     params | backend    | ngl |          test |                  t&#x2F;s |
  | ------------------------------ | ---------: | ---------: | ---------- | --: | ------------: | -------------------: |
  | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | ROCm       |  99 |         pp512 |      3258.67 ± 29.23 |
  | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | ROCm       |  99 |         tg128 |        103.31 ± 0.03 |

  build: 31ac5834 (3818)
</code></pre>
Vulkan:<p><pre><code>  GGML_VK_VISIBLE_DEVICES=1 .&#x2F;llama-bench -m &#x2F;models&#x2F;gguf&#x2F;llama-2-7b.Q4_0.gguf
  | model                          |       size |     params | backend    | ngl |          test |                  t&#x2F;s |
  | ------------------------------ | ---------: | ---------: | ---------- | --: | ------------: | -------------------: |
  ggml_vulkan: Found 1 Vulkan devices:
  Vulkan0: Radeon RX 7900 XTX (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | warp size: 64
  | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Vulkan     |  99 |         pp512 |       1077.49 ± 2.00 |
  | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Vulkan     |  99 |         tg128 |         71.83 ± 0.06 |

  build: 31ac5834 (3818)

</code></pre>
EDIT: HN should really support markdown...</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41633720" class="c"><input type="checkbox" id="c-41633720" checked=""/><div class="controls bullet"><span class="by">Stem0037</span><span>|</span><a href="#41631325">prev</a><span>|</span><a href="#41631284">next</a><span>|</span><label class="collapse" for="c-41633720">[-]</label><label class="expand" for="c-41633720">[1 more]</label></div><br/><div class="children"><div class="content">If possible, it would be interesting to explore ways to overcome the memory constraints and run a JIT-compiled version. This could potentially lead to further performance improvements.</div><br/></div></div><div id="41631284" class="c"><input type="checkbox" id="c-41631284" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#41633720">prev</a><span>|</span><a href="#41631345">next</a><span>|</span><label class="collapse" for="c-41631284">[-]</label><label class="expand" for="c-41631284">[3 more]</label></div><br/><div class="children"><div class="content">Nice work! I was just playing with the inference side of things with 405B myself this weekend [0].<p>I&#x27;m not convinced that &#x27;torch.cuda&#x27; is really that bad since the AMD version of PyTorch just translates that for you. More like a naming problem, than anything. Fact is that it is just as easy to grab the rocm:pytorch container, as it is the rocm:jax container.<p>I don&#x27;t see very many numbers posted. What MFU did you get?<p>[0] <a href="https:&#x2F;&#x2F;x.com&#x2F;HotAisle&#x2F;status&#x2F;1837580046732874026" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;HotAisle&#x2F;status&#x2F;1837580046732874026</a></div><br/><div id="41631947" class="c"><input type="checkbox" id="c-41631947" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41631284">parent</a><span>|</span><a href="#41631345">next</a><span>|</span><label class="collapse" for="c-41631947">[-]</label><label class="expand" for="c-41631947">[2 more]</label></div><br/><div class="children"><div class="content">Nice!<p>I need to calculate MFU. GPU, VRAM details can be found in the repo: <a href="https:&#x2F;&#x2F;dub.sh&#x2F;amd-405b-res" rel="nofollow">https:&#x2F;&#x2F;dub.sh&#x2F;amd-405b-res</a>.<p>I plan to reattempt the training run next weekend and JIT the entire training step to calculate MFU then</div><br/></div></div></div></div><div id="41631345" class="c"><input type="checkbox" id="c-41631345" checked=""/><div class="controls bullet"><span class="by">yeahwhatever10</span><span>|</span><a href="#41631284">prev</a><span>|</span><a href="#41633568">next</a><span>|</span><label class="collapse" for="c-41631345">[-]</label><label class="expand" for="c-41631345">[2 more]</label></div><br/><div class="children"><div class="content">Where is the performance data?</div><br/><div id="41631931" class="c"><input type="checkbox" id="c-41631931" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41631345">parent</a><span>|</span><a href="#41633568">next</a><span>|</span><label class="collapse" for="c-41631931">[-]</label><label class="expand" for="c-41631931">[1 more]</label></div><br/><div class="children"><div class="content">(author here, sorry for the delay in replying, was stuck in back-to-back meetings)<p>I updated our github repo to include GPU, VRAM utilization data (<a href="https:&#x2F;&#x2F;github.com&#x2F;felafax&#x2F;felafax?tab=readme-ov-file#amd-405b-fine-tuning-run">https:&#x2F;&#x2F;github.com&#x2F;felafax&#x2F;felafax?tab=readme-ov-file#amd-40...</a>)<p>Note: we couldn&#x27;t run the JIT-compiled version of the 405B model due to our code&#x2F;VRAM constraints (we need to investigate this further). The entire training run was executed in JAX eager mode, so there is significant potential for performance improvements.<p>GPU utilization across the board was still ~30-40% even with eager mode, which is quite good! With JIT, I think the GPU util can easily shoot up to ~50-60%.</div><br/></div></div></div></div><div id="41633568" class="c"><input type="checkbox" id="c-41633568" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#41631345">prev</a><span>|</span><a href="#41633541">next</a><span>|</span><label class="collapse" for="c-41633568">[-]</label><label class="expand" for="c-41633568">[1 more]</label></div><br/><div class="children"><div class="content">Is AMD any closer to extracting value from this with large orders of their GPUs causing a shortage?<p>I’m getting the impression of “no”</div><br/></div></div><div id="41633541" class="c"><input type="checkbox" id="c-41633541" checked=""/><div class="controls bullet"><span class="by">system2</span><span>|</span><a href="#41633568">prev</a><span>|</span><a href="#41631447">next</a><span>|</span><label class="collapse" for="c-41633541">[-]</label><label class="expand" for="c-41633541">[2 more]</label></div><br/><div class="children"><div class="content">Why is obsidian (a note-taking app) doing this?</div><br/><div id="41633555" class="c"><input type="checkbox" id="c-41633555" checked=""/><div class="controls bullet"><span class="by">Zerot</span><span>|</span><a href="#41633541">parent</a><span>|</span><a href="#41631447">next</a><span>|</span><label class="collapse" for="c-41633555">[-]</label><label class="expand" for="c-41633555">[1 more]</label></div><br/><div class="children"><div class="content">They aren&#x27;t. This company is using obsidian publish to publish documents.</div><br/></div></div></div></div><div id="41631447" class="c"><input type="checkbox" id="c-41631447" checked=""/><div class="controls bullet"><span class="by">manojlds</span><span>|</span><a href="#41633541">prev</a><span>|</span><a href="#41631286">next</a><span>|</span><label class="collapse" for="c-41631447">[-]</label><label class="expand" for="c-41631447">[4 more]</label></div><br/><div class="children"><div class="content">Thought this was a post from Obsidian at first. Why haven&#x27;t they done the GitHub.com vs GitHub.io thing yet.</div><br/><div id="41631802" class="c"><input type="checkbox" id="c-41631802" checked=""/><div class="controls bullet"><span class="by">codetrotter</span><span>|</span><a href="#41631447">parent</a><span>|</span><a href="#41631619">next</a><span>|</span><label class="collapse" for="c-41631802">[-]</label><label class="expand" for="c-41631802">[1 more]</label></div><br/><div class="children"><div class="content">Looking at the URL has me thinking that this confusion would be resolved if HN adds a small piece of logic to treat the domain publish.obsidian.md specially, just like how HN already does for pages served under forbes.com&#x2F;sites which is not written by the Forbes staff themselves.<p>So instead of showing the domain as obsidian.md, HN would show the domain for this link as publish.obsidian.md<p>Maybe something for dang to consider if he sees this comment?</div><br/></div></div><div id="41631619" class="c"><input type="checkbox" id="c-41631619" checked=""/><div class="controls bullet"><span class="by">gbraad</span><span>|</span><a href="#41631447">parent</a><span>|</span><a href="#41631802">prev</a><span>|</span><a href="#41631286">next</a><span>|</span><label class="collapse" for="c-41631619">[-]</label><label class="expand" for="c-41631619">[2 more]</label></div><br/><div class="children"><div class="content">Same thought here. Why would Obsidian bother with AI? Oh wait, this is publish? So this is what $8 per month gets you? I am amazed, as I would have at least expected a subhost: [username].publish.obsidian.md</div><br/><div id="41631966" class="c"><input type="checkbox" id="c-41631966" checked=""/><div class="controls bullet"><span class="by">felarof</span><span>|</span><a href="#41631447">root</a><span>|</span><a href="#41631619">parent</a><span>|</span><a href="#41631286">next</a><span>|</span><label class="collapse" for="c-41631966">[-]</label><label class="expand" for="c-41631966">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, used Obsidian Publish.<p>But struggling to get custom domain to work with it (have emailed support).</div><br/></div></div></div></div></div></div><div id="41631286" class="c"><input type="checkbox" id="c-41631286" checked=""/><div class="controls bullet"><span class="by">abalaji</span><span>|</span><a href="#41631447">prev</a><span>|</span><label class="collapse" for="c-41631286">[-]</label><label class="expand" for="c-41631286">[6 more]</label></div><br/><div class="children"><div class="content">@dang: could we get url to include the username since this isn&#x27;t about Obsidian itself, but rather a user generated blog?</div><br/><div id="41631756" class="c"><input type="checkbox" id="c-41631756" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#41631286">parent</a><span>|</span><a href="#41631642">next</a><span>|</span><label class="collapse" for="c-41631756">[-]</label><label class="expand" for="c-41631756">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s strange that HN didn&#x27;t include the full domain &quot;publish.obsidian.cmd&quot;.</div><br/><div id="41632942" class="c"><input type="checkbox" id="c-41632942" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#41631286">root</a><span>|</span><a href="#41631756">parent</a><span>|</span><a href="#41631642">next</a><span>|</span><label class="collapse" for="c-41632942">[-]</label><label class="expand" for="c-41632942">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not turned on by default but I&#x27;ve done it for this domain now.</div><br/></div></div></div></div><div id="41631642" class="c"><input type="checkbox" id="c-41631642" checked=""/><div class="controls bullet"><span class="by">meiraleal</span><span>|</span><a href="#41631286">parent</a><span>|</span><a href="#41631756">prev</a><span>|</span><a href="#41631546">next</a><span>|</span><label class="collapse" for="c-41631642">[-]</label><label class="expand" for="c-41631642">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s something obsidian should fix if they care about not looking like they are being impersonated on HN.</div><br/><div id="41632412" class="c"><input type="checkbox" id="c-41632412" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#41631286">root</a><span>|</span><a href="#41631642">parent</a><span>|</span><a href="#41631546">next</a><span>|</span><label class="collapse" for="c-41632412">[-]</label><label class="expand" for="c-41632412">[1 more]</label></div><br/><div class="children"><div class="content">Obsidian can&#x27;t do anything about it. It&#x27;s HN chopping up the url</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>