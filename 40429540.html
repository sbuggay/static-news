<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1716368468768" as="style"/><link rel="stylesheet" href="styles.css?v=1716368468768"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a> <span class="domain">(<a href="https://transformer-circuits.pub">transformer-circuits.pub</a>)</span></div><div class="subtext"><span>1wheel</span> | <span>20 comments</span></div><br/><div><div id="40436757" class="c"><input type="checkbox" id="c-40436757" checked=""/><div class="controls bullet"><span class="by">e63f67dd-065b</span><span>|</span><a href="#40437406">next</a><span>|</span><label class="collapse" for="c-40436757">[-]</label><label class="expand" for="c-40436757">[5 more]</label></div><br/><div class="children"><div class="content">I find Anthorpic&#x27;s work on mech interp fascinating in general. Their initial towards monosemanticity paper was highly surprising, and so is this with the ability to scale to a real production-scale LLM.<p>My observation is, and this may be more philosophical than technical: this process of &quot;decomposing&quot; middle-layer activations with a sparse autoencoder -- is it capturing accurately underlying features in the latent space of the network, or are we drawing order from chaos, imposing monosemanticity where there aren&#x27;t any? Or to put it another way, were the features always there, learnt by training, or are we doing post-hoc rationalisations -- where the features exist because that&#x27;s how we defined the autoencoders&#x27; dictionaries, and we learn only what we wanted to learn? Are the alien minds of LLMs truly also operating on a similar semantic space as ours, or are we reading tea leaves and seeing what we want to see?<p>Maybe this distinction doesn&#x27;t even make sense to begin with; concepts are made by man, if clamping one of these features modifies outputs in a way that is understandable to humans, it doesn&#x27;t matter if it&#x27;s capturing some kind of underlying cluster in the latent space of the model. But I do think it&#x27;s an interesting idea to ponder.</div><br/><div id="40438098" class="c"><input type="checkbox" id="c-40438098" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#40436757">parent</a><span>|</span><a href="#40437384">next</a><span>|</span><label class="collapse" for="c-40438098">[-]</label><label class="expand" for="c-40438098">[1 more]</label></div><br/><div class="children"><div class="content">&gt; concepts are made by man<p>I find this statement... controversial?<p>The canonical example would be mathemathics - are they discovered or invented? Does the idea of &#x27;3&#x27; or an empty set or a straight line exist without any humans thinking about it or even if it is necessary to have any kind of an universe at all for these concepts to be valid? I think the answers here are &#x27;yes&#x27; and &#x27;no&#x27;.<p>Of course, there are still concepts which require grounding in the universe or humanity, but if you can think these up first (...somehow), you should need neither.</div><br/></div></div><div id="40437384" class="c"><input type="checkbox" id="c-40437384" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#40436757">parent</a><span>|</span><a href="#40438098">prev</a><span>|</span><a href="#40436897">next</a><span>|</span><label class="collapse" for="c-40437384">[-]</label><label class="expand" for="c-40437384">[1 more]</label></div><br/><div class="children"><div class="content">Their manipulation of the vectors and the effects produced would suggest that it isn&#x27;t that the SAE is just finding phantom representations that aren&#x27;t really there.</div><br/></div></div><div id="40436897" class="c"><input type="checkbox" id="c-40436897" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40436757">parent</a><span>|</span><a href="#40437384">prev</a><span>|</span><a href="#40437406">next</a><span>|</span><label class="collapse" for="c-40436897">[-]</label><label class="expand" for="c-40436897">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m allergic to latent space because I&#x27;ve yet to find any meaning to it beyond poetics, I develop an acute allergy when it&#x27;s explicitly related to visually dimensional ideas like clustering.<p>I&#x27;ll make a probably bad analogy: does your mindmap place things near each other like my mindmap?<p>To which I&#x27;d say, probably not, mindmaps are very personal, and the more complex we put on ours, the more personal and arbitrary they would be, and the less import the visuals would have<p>ex. if we have 3 million things on both our mindmaps, it&#x27;s peering too closely to wonder why you put mcdonalds closer to kids food than restaurants, and you have restaurants in the top left, whereas I put it closer to kids foods, in the top mid left.</div><br/><div id="40438743" class="c"><input type="checkbox" id="c-40438743" checked=""/><div class="controls bullet"><span class="by">anentropic</span><span>|</span><a href="#40436757">root</a><span>|</span><a href="#40436897">parent</a><span>|</span><a href="#40437406">next</a><span>|</span><label class="collapse" for="c-40438743">[-]</label><label class="expand" for="c-40438743">[1 more]</label></div><br/><div class="children"><div class="content">what if you averaged over millions of peoples&#x27; mindmaps?</div><br/></div></div></div></div></div></div><div id="40437406" class="c"><input type="checkbox" id="c-40437406" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#40436757">prev</a><span>|</span><a href="#40438615">next</a><span>|</span><label class="collapse" for="c-40437406">[-]</label><label class="expand" for="c-40437406">[3 more]</label></div><br/><div class="children"><div class="content">Great work as usual.<p>I was pretty upset seeing the superalignment team dissolve at OpenAI, but as is typical for the AI space, the news of one day was quickly eclipsed by the next day.<p>Anthropic are really killing it right now, and it&#x27;s very refreshing seeing their commitment to publishing novel findings.<p>I hope this finally serves as the nail in the coffin on the &quot;it&#x27;s just fancy autocomplete&quot; and &quot;it doesn&#x27;t understand what it&#x27;s saying, bro&quot; rhetoric.</div><br/><div id="40437912" class="c"><input type="checkbox" id="c-40437912" checked=""/><div class="controls bullet"><span class="by">jwilber</span><span>|</span><a href="#40437406">parent</a><span>|</span><a href="#40438615">next</a><span>|</span><label class="collapse" for="c-40437912">[-]</label><label class="expand" for="c-40437912">[2 more]</label></div><br/><div class="children"><div class="content">Love Anthropic research. Great visuals between Olah, Carter, and Pearce, as well.<p>I don’t think this paper does much in the way of your final point, “it doesn’t understand what it’s saying”, though our understanding certainly has improved.</div><br/><div id="40438375" class="c"><input type="checkbox" id="c-40438375" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#40437406">root</a><span>|</span><a href="#40437912">parent</a><span>|</span><a href="#40438615">next</a><span>|</span><label class="collapse" for="c-40438375">[-]</label><label class="expand" for="c-40438375">[1 more]</label></div><br/><div class="children"><div class="content">They were able to demonstrate conceptual vectors that were consistent across different languages and different mediums (text vs images) and that when manipulated were able to represent the abstract concept in the output regardless of prompt.<p>What kind of evidentiary threshold would you want if that&#x27;s not sufficient?</div><br/></div></div></div></div></div></div><div id="40438615" class="c"><input type="checkbox" id="c-40438615" checked=""/><div class="controls bullet"><span class="by">maciejgryka</span><span>|</span><a href="#40437406">prev</a><span>|</span><a href="#40434403">next</a><span>|</span><label class="collapse" for="c-40438615">[-]</label><label class="expand" for="c-40438615">[1 more]</label></div><br/><div class="children"><div class="content">I recorded myself trying to read through and understand the high-level of this if anyone&#x27;s interested in following along: <a href="https:&#x2F;&#x2F;maciej.gryka.net&#x2F;papers-in-public&#x2F;#scaling-monosemanticity" rel="nofollow">https:&#x2F;&#x2F;maciej.gryka.net&#x2F;papers-in-public&#x2F;#scaling-monoseman...</a></div><br/></div></div><div id="40434403" class="c"><input type="checkbox" id="c-40434403" checked=""/><div class="controls bullet"><span class="by">bjterry</span><span>|</span><a href="#40438615">prev</a><span>|</span><a href="#40438502">next</a><span>|</span><label class="collapse" for="c-40434403">[-]</label><label class="expand" for="c-40434403">[3 more]</label></div><br/><div class="children"><div class="content">It would be interesting to allow users of models to customize inference by tweaking these features, sort of like a semantic equalizer for LLMs. My guess is that this wouldn&#x27;t work as well as fine-tuning, since that would tweak all the features at once toward your use case, but the equalizer would require zero training data.<p>The prompt itself can trigger the features, so if you say &quot;Try to weave in mentions of San Francisco&quot; the San Francisco feature will be more activated in the response. But having a global equalizer could reduce drift as the conversation continued, perhaps?</div><br/><div id="40436786" class="c"><input type="checkbox" id="c-40436786" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#40434403">parent</a><span>|</span><a href="#40435401">next</a><span>|</span><label class="collapse" for="c-40436786">[-]</label><label class="expand" for="c-40436786">[1 more]</label></div><br/><div class="children"><div class="content">At least for right now this approach would in most cases still be like using a shotgun instead of a scalpel.<p>Over the next year or so I&#x27;m sure it will refine enough to be able to be more like a vector multiplier on activation, but simply flipping it on in general is going to create a very &#x27;obsessed&#x27; model as stated.</div><br/></div></div><div id="40435401" class="c"><input type="checkbox" id="c-40435401" checked=""/><div class="controls bullet"><span class="by">ericflo</span><span>|</span><a href="#40434403">parent</a><span>|</span><a href="#40436786">prev</a><span>|</span><a href="#40438502">next</a><span>|</span><label class="collapse" for="c-40435401">[-]</label><label class="expand" for="c-40435401">[1 more]</label></div><br/><div class="children"><div class="content">Related: <a href="https:&#x2F;&#x2F;vgel.me&#x2F;posts&#x2F;representation-engineering&#x2F;" rel="nofollow">https:&#x2F;&#x2F;vgel.me&#x2F;posts&#x2F;representation-engineering&#x2F;</a></div><br/></div></div></div></div><div id="40438502" class="c"><input type="checkbox" id="c-40438502" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#40434403">prev</a><span>|</span><a href="#40436795">next</a><span>|</span><label class="collapse" for="c-40438502">[-]</label><label class="expand" for="c-40438502">[1 more]</label></div><br/><div class="children"><div class="content">It looks like Anthropic is now leading the charge on safety</div><br/></div></div><div id="40436795" class="c"><input type="checkbox" id="c-40436795" checked=""/><div class="controls bullet"><span class="by">gautomdas</span><span>|</span><a href="#40438502">prev</a><span>|</span><a href="#40434535">next</a><span>|</span><label class="collapse" for="c-40436795">[-]</label><label class="expand" for="c-40436795">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve really been enjoying their series on mech interp, does anyone have any other good recs?</div><br/><div id="40437371" class="c"><input type="checkbox" id="c-40437371" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#40436795">parent</a><span>|</span><a href="#40434535">next</a><span>|</span><label class="collapse" for="c-40437371">[-]</label><label class="expand" for="c-40437371">[1 more]</label></div><br/><div class="children"><div class="content">The Othello-GPT and Chess-GPT lines of work.<p>Was the first research work that clued me into what Anthropic&#x27;s work today ended up demonstrating.</div><br/></div></div></div></div><div id="40434535" class="c"><input type="checkbox" id="c-40434535" checked=""/><div class="controls bullet"><span class="by">pagekicker</span><span>|</span><a href="#40436795">prev</a><span>|</span><a href="#40435879">next</a><span>|</span><label class="collapse" for="c-40434535">[-]</label><label class="expand" for="c-40434535">[2 more]</label></div><br/><div class="children"><div class="content">The article doesn&#x27;t explain how users can exploit these features in UI or prompt.  Does anyone have any insight on how to do so?</div><br/><div id="40435715" class="c"><input type="checkbox" id="c-40435715" checked=""/><div class="controls bullet"><span class="by">CephalopodMD</span><span>|</span><a href="#40434535">parent</a><span>|</span><a href="#40435879">next</a><span>|</span><label class="collapse" for="c-40435715">[-]</label><label class="expand" for="c-40435715">[1 more]</label></div><br/><div class="children"><div class="content">They explicitly aren&#x27;t releasing any tools to do this with their models for safety reasons. But you could probably do it from scratch with one of the open models by following their methodology.</div><br/></div></div></div></div><div id="40435879" class="c"><input type="checkbox" id="c-40435879" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#40434535">prev</a><span>|</span><a href="#40432727">next</a><span>|</span><label class="collapse" for="c-40435879">[-]</label><label class="expand" for="c-40435879">[1 more]</label></div><br/><div class="children"><div class="content">Someone should do this for Llama 3.</div><br/></div></div><div id="40432727" class="c"><input type="checkbox" id="c-40432727" checked=""/><div class="controls bullet"><span class="by">quotemstr</span><span>|</span><a href="#40435879">prev</a><span>|</span><label class="collapse" for="c-40432727">[-]</label><label class="expand" for="c-40432727">[1 more]</label></div><br/><div class="children"><div class="content">At this risk of anthropomorphizing too much, I can&#x27;t help but see parallels between the &quot;my physical form is the Golden Gate Bridge&quot; screenshot and the <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;God_helmet" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;God_helmet</a> in humans --- both cognitive distortions caused by targeted exogenous neural activation.</div><br/></div></div></div></div></div></div></div></body></html>