<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1689152467644" as="style"/><link rel="stylesheet" href="styles.css?v=1689152467644"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://zilliz.com/blog/vector-similarity-search">Introduction to vector similarity search (2022)</a>Â <span class="domain">(<a href="https://zilliz.com">zilliz.com</a>)</span></div><div class="subtext"><span>fzliu</span> | <span>26 comments</span></div><br/><div><div id="36691779" class="c"><input type="checkbox" id="c-36691779" checked=""/><div class="controls bullet"><span class="by">weinzierl</span><span>|</span><a href="#36690402">next</a><span>|</span><label class="collapse" for="c-36691779">[-]</label><label class="expand" for="c-36691779">[3 more]</label></div><br/><div class="children"><div class="content">How would I decide on which granularity level I create my vectors? For example, if I create word level embeddings with word2vec and store one vector per word I probably will only get good results  for keyword based queries, right?<p>If I create more fancy vectors, for example on the sentence level, it is not really clear to me how that is supposed to work.<p>Why is the vector embedding of a query, which typically has the sentence structure of a question, near the vector embedding of a propositional sentence that answers that query? Sure, it will probably not be completely off, just for the fact that query and answer will contain similar words, but how can that be better than just a fuzzy word search?<p>And finally: Should I decide on on particular level (like sentences) and store that or should I store word2vec and sentence and paragraph vectors in the same collection?</div><br/><div id="36691877" class="c"><input type="checkbox" id="c-36691877" checked=""/><div class="controls bullet"><span class="by">fzliu</span><span>|</span><a href="#36691779">parent</a><span>|</span><a href="#36690402">next</a><span>|</span><label class="collapse" for="c-36691877">[-]</label><label class="expand" for="c-36691877">[2 more]</label></div><br/><div class="children"><div class="content">I would not use word2vec in any application today - I used it in the blog post because it&#x27;s a well-known model and because the embeddings it generates are static, i.e. one token always maps to the same embedding regardless of context.<p>If you want to create embeddings at the sentence level, a good place to start is SBERT: <a href="https:&#x2F;&#x2F;www.sbert.net&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.sbert.net&#x2F;</a></div><br/><div id="36691996" class="c"><input type="checkbox" id="c-36691996" checked=""/><div class="controls bullet"><span class="by">weinzierl</span><span>|</span><a href="#36691779">root</a><span>|</span><a href="#36691877">parent</a><span>|</span><a href="#36690402">next</a><span>|</span><label class="collapse" for="c-36691996">[-]</label><label class="expand" for="c-36691996">[1 more]</label></div><br/><div class="children"><div class="content">I understood that it was just an example and it is a good choice for an introductory text. My question is more into advice how to go on from that. For example l, is SBERT a good choice if most of my queries are in fact multi-sentence paragraphs?<p>I guess what still doesn&#x27;t add up in my mental model is how people seem to assume that the query embedding vector must somehow automatically be near good answer embedding vectors. I would have assumed that some conditions have to be met to make this true and I would be interested into advice in that regard.</div><br/></div></div></div></div></div></div><div id="36690402" class="c"><input type="checkbox" id="c-36690402" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#36691779">prev</a><span>|</span><a href="#36690638">next</a><span>|</span><label class="collapse" for="c-36690402">[-]</label><label class="expand" for="c-36690402">[19 more]</label></div><br/><div class="children"><div class="content">Can anyone please suggest a good stack for the following:<p>- calculating text embeddings using open-source&#x2F;local methods (not OpenAI)<p>- storing them in a vector database. I&#x27;m confused by the myriad of options like Chromadb, Pinecone, etc.<p>- running vector similarity search using open-source&#x2F;local methods.<p>Also, how granular should the text chunks be? Too short and we&#x27;ll end up with a huge database, too long and we&#x27;ll probably miss some relevant information in some chunks.<p>Has anyone been able to achieve reliable results from these? Preferably w&#x2F;o using Langchain.</div><br/><div id="36690570" class="c"><input type="checkbox" id="c-36690570" checked=""/><div class="controls bullet"><span class="by">computerex</span><span>|</span><a href="#36690402">parent</a><span>|</span><a href="#36691205">next</a><span>|</span><label class="collapse" for="c-36690570">[-]</label><label class="expand" for="c-36690570">[3 more]</label></div><br/><div class="children"><div class="content">To calculate embeddings for free, use this very popular model:
<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;sentence-transformers&#x2F;all-MiniLM-L6-v2" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;sentence-transformers&#x2F;all-MiniLM-L6-v...</a><p>For storing the vectors and doing the vector search:
<a href="https:&#x2F;&#x2F;github.com&#x2F;pgvector&#x2F;pgvector">https:&#x2F;&#x2F;github.com&#x2F;pgvector&#x2F;pgvector</a><p>`ankane&#x2F;pgvector` docker image is a drop in replacement for the postgres image, so you can fire this up with docker very quickly.<p>It&#x27;s a normal postgres db with a vector datatype. It can index the vectors and allows efficient retrieval. Both AWS RDS and Google Cloud now support this in their managed Postgres offerings, so postgres+pgvector is a viable managed production vectordb solution.<p>&gt; Also, how granular should the text chunks be?<p>That depends on the use case, the size of your corpus, the context of the model you are using, how much money you are willing to spend.<p>&gt; Has anyone been able to achieve reliable results from these? Preferably w&#x2F;o using Langchain.<p>Definitely. We use postgres+pgvector with php.</div><br/><div id="36690861" class="c"><input type="checkbox" id="c-36690861" checked=""/><div class="controls bullet"><span class="by">ClassyJacket</span><span>|</span><a href="#36690402">root</a><span>|</span><a href="#36690570">parent</a><span>|</span><a href="#36691205">next</a><span>|</span><label class="collapse" for="c-36690861">[-]</label><label class="expand" for="c-36690861">[2 more]</label></div><br/><div class="children"><div class="content">&gt;This is a sentence-transformers model: It maps sentences &amp; paragraphs to a 384 dimensional dense vector space<p>Interesting. Can someone explain to me why 384 specifically? How did they arrive at that number?</div><br/><div id="36691024" class="c"><input type="checkbox" id="c-36691024" checked=""/><div class="controls bullet"><span class="by">computerex</span><span>|</span><a href="#36690402">root</a><span>|</span><a href="#36690861">parent</a><span>|</span><a href="#36691205">next</a><span>|</span><label class="collapse" for="c-36691024">[-]</label><label class="expand" for="c-36691024">[1 more]</label></div><br/><div class="children"><div class="content">It has to do with the architecture of the network used to create the embeddings. The embeddings is actually the output of the final layer of the model. The dimensionality is a function of the number of parameters in that layer.<p>Different models&#x2F;architectures will produce different dimension embeddings.</div><br/></div></div></div></div></div></div><div id="36691205" class="c"><input type="checkbox" id="c-36691205" checked=""/><div class="controls bullet"><span class="by">YossarianFrPrez</span><span>|</span><a href="#36690402">parent</a><span>|</span><a href="#36690570">prev</a><span>|</span><a href="#36690451">next</a><span>|</span><label class="collapse" for="c-36691205">[-]</label><label class="expand" for="c-36691205">[1 more]</label></div><br/><div class="children"><div class="content">Depending on your use case (particularly if it is research-oriented),  &quot;scipy.spatial.distance.cdist&quot; and &quot;scipy.spatial.distance.pdist&quot; are your friends. If you are doing something in production, the PG extension seems like a good bet.<p>One way to potentially answer your question about text-chunk-granularity is to take a random sample of 500 pieces of chunked text and look at several &quot;most similar pairs.&quot; Do this for a few different chunk-lengths and you&#x27;ll see how much information is lost...</div><br/></div></div><div id="36690451" class="c"><input type="checkbox" id="c-36690451" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#36690402">parent</a><span>|</span><a href="#36691205">prev</a><span>|</span><a href="#36691267">next</a><span>|</span><label class="collapse" for="c-36690451">[-]</label><label class="expand" for="c-36690451">[3 more]</label></div><br/><div class="children"><div class="content">Calculating the embeddings is probably going to be an application-specific thing. Either your application has reasonable pre-trained encoders or you train one off a mountain of matching pairs of data.<p>Once you have the embeddings in some space, for PoC Iâve mostly seen people shove them into faiss, which handles most of the rest very well for small&#x2F;medium datasets:
<a href="https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;faiss">https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;faiss</a></div><br/><div id="36690488" class="c"><input type="checkbox" id="c-36690488" checked=""/><div class="controls bullet"><span class="by">NhanH</span><span>|</span><a href="#36690402">root</a><span>|</span><a href="#36690451">parent</a><span>|</span><a href="#36691267">next</a><span>|</span><label class="collapse" for="c-36690488">[-]</label><label class="expand" for="c-36690488">[2 more]</label></div><br/><div class="children"><div class="content">Could you please point to some materials to understand the data needed to train the embedding model for a specific domain?</div><br/><div id="36690693" class="c"><input type="checkbox" id="c-36690693" checked=""/><div class="controls bullet"><span class="by">anon7725</span><span>|</span><a href="#36690402">root</a><span>|</span><a href="#36690488">parent</a><span>|</span><a href="#36691267">next</a><span>|</span><label class="collapse" for="c-36690693">[-]</label><label class="expand" for="c-36690693">[1 more]</label></div><br/><div class="children"><div class="content">You donât need to train anything if you just need embeddings. The data is text. You apply the pretrained model to your text and it returns the embedding. You save it in a vector database if youâre fancy, or a big numpy array if youâre like me. Then run your similarity search (cosine, Euclidean, etc).</div><br/></div></div></div></div></div></div><div id="36691267" class="c"><input type="checkbox" id="c-36691267" checked=""/><div class="controls bullet"><span class="by">owen-elliott</span><span>|</span><a href="#36690402">parent</a><span>|</span><a href="#36690451">prev</a><span>|</span><a href="#36690628">next</a><span>|</span><label class="collapse" for="c-36691267">[-]</label><label class="expand" for="c-36691267">[1 more]</label></div><br/><div class="children"><div class="content">You could use Marqo, it is a vector search engine that includes the text chunking, inference for calculating embeddings, vector storage, and vector search. You can pick from a heap of open-source models or bring your own fine-tuned ones. It all runs locally in docker <a href="https:&#x2F;&#x2F;github.com&#x2F;marqo-ai&#x2F;marqo">https:&#x2F;&#x2F;github.com&#x2F;marqo-ai&#x2F;marqo</a></div><br/></div></div><div id="36690628" class="c"><input type="checkbox" id="c-36690628" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#36690402">parent</a><span>|</span><a href="#36691267">prev</a><span>|</span><a href="#36690677">next</a><span>|</span><label class="collapse" for="c-36690628">[-]</label><label class="expand" for="c-36690628">[4 more]</label></div><br/><div class="children"><div class="content">This is what we use: BERT sentence transformers to generate the embeddings (we used Universal Sentence Encoder before that and it was good too), and ElasticSearch for storage, which has a dense vector data type. It also has a cosineSimilarity function to run searches.</div><br/><div id="36690687" class="c"><input type="checkbox" id="c-36690687" checked=""/><div class="controls bullet"><span class="by">fzliu</span><span>|</span><a href="#36690402">root</a><span>|</span><a href="#36690628">parent</a><span>|</span><a href="#36690677">next</a><span>|</span><label class="collapse" for="c-36690687">[-]</label><label class="expand" for="c-36690687">[3 more]</label></div><br/><div class="children"><div class="content">I would not use Elastic for vector search due to its architectural limitations and poor performance when conducting vector search. <a href="https:&#x2F;&#x2F;zilliz.com&#x2F;benchmark" rel="nofollow noreferrer">https:&#x2F;&#x2F;zilliz.com&#x2F;benchmark</a></div><br/><div id="36690970" class="c"><input type="checkbox" id="c-36690970" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#36690402">root</a><span>|</span><a href="#36690687">parent</a><span>|</span><a href="#36691478">next</a><span>|</span><label class="collapse" for="c-36690970">[-]</label><label class="expand" for="c-36690970">[1 more]</label></div><br/><div class="children"><div class="content">I shouldâve said that we were already using it for actual search where embeddings similarity is just one component of the overall score. For pure vector stuff a dedicated solution will be faster.</div><br/></div></div><div id="36691478" class="c"><input type="checkbox" id="c-36691478" checked=""/><div class="controls bullet"><span class="by">dunefox</span><span>|</span><a href="#36690402">root</a><span>|</span><a href="#36690687">parent</a><span>|</span><a href="#36690970">prev</a><span>|</span><a href="#36690677">next</a><span>|</span><label class="collapse" for="c-36691478">[-]</label><label class="expand" for="c-36691478">[1 more]</label></div><br/><div class="children"><div class="content">What about Solr?</div><br/></div></div></div></div></div></div><div id="36690677" class="c"><input type="checkbox" id="c-36690677" checked=""/><div class="controls bullet"><span class="by">fzliu</span><span>|</span><a href="#36690402">parent</a><span>|</span><a href="#36690628">prev</a><span>|</span><a href="#36690575">next</a><span>|</span><label class="collapse" for="c-36690677">[-]</label><label class="expand" for="c-36690677">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re just starting out, I&#x27;d use sentence-transformers for calculating embeddings. You&#x27;ll want a bi-encoder model since they produce embeddings. As the author of the blog, I&#x27;m partial towards Milvus (<a href="https:&#x2F;&#x2F;github.com&#x2F;milvus-io&#x2F;milvus">https:&#x2F;&#x2F;github.com&#x2F;milvus-io&#x2F;milvus</a>) due to its enterprise and scale, but FAISS is a great option too if you&#x27;re just looking for something more local and contained.<p>Milvus will perform vector similarity search for you - all you need to do is give it a query vector.</div><br/></div></div><div id="36690575" class="c"><input type="checkbox" id="c-36690575" checked=""/><div class="controls bullet"><span class="by">jeffchuber</span><span>|</span><a href="#36690402">parent</a><span>|</span><a href="#36690677">prev</a><span>|</span><a href="#36690638">next</a><span>|</span><label class="collapse" for="c-36690575">[-]</label><label class="expand" for="c-36690575">[5 more]</label></div><br/><div class="children"><div class="content">Really just chroma + openai is all you need. Chroma makes this easy <a href="https:&#x2F;&#x2F;docs.trychroma.com&#x2F;embeddings" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.trychroma.com&#x2F;embeddings</a>.<p>All you have to do is chunk... I&#x27;d start at a paragraph and experiment.</div><br/><div id="36690644" class="c"><input type="checkbox" id="c-36690644" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#36690402">root</a><span>|</span><a href="#36690575">parent</a><span>|</span><a href="#36691270">next</a><span>|</span><label class="collapse" for="c-36690644">[-]</label><label class="expand" for="c-36690644">[3 more]</label></div><br/><div class="children"><div class="content">This is not always good advice. Many people require to not use off premise models, due to data ownership issues.<p>I would therefore suggest a better default for this, such as BERT+Qdrant.<p>It would be wonderful if there were a simpler (single file, SQLite or DuckDB like) database for vectors than the complex (and in some cases, unfortunately cloud-based) ones available now.</div><br/><div id="36690884" class="c"><input type="checkbox" id="c-36690884" checked=""/><div class="controls bullet"><span class="by">fzliu</span><span>|</span><a href="#36690402">root</a><span>|</span><a href="#36690644">parent</a><span>|</span><a href="#36690744">next</a><span>|</span><label class="collapse" for="c-36690884">[-]</label><label class="expand" for="c-36690884">[1 more]</label></div><br/><div class="children"><div class="content">You might enjoy milvus-lite: <a href="https:&#x2F;&#x2F;zilliz.com&#x2F;blog&#x2F;exploring-magic-vector-databases-jupyter-notebooks" rel="nofollow noreferrer">https:&#x2F;&#x2F;zilliz.com&#x2F;blog&#x2F;exploring-magic-vector-databases-jup...</a><p><pre><code>   pip install milvus</code></pre></div><br/></div></div><div id="36690744" class="c"><input type="checkbox" id="c-36690744" checked=""/><div class="controls bullet"><span class="by">jeffchuber</span><span>|</span><a href="#36690402">root</a><span>|</span><a href="#36690644">parent</a><span>|</span><a href="#36690884">prev</a><span>|</span><a href="#36691270">next</a><span>|</span><label class="collapse" for="c-36690744">[-]</label><label class="expand" for="c-36690744">[1 more]</label></div><br/><div class="children"><div class="content">ah sorry, i should read OP better - chroma&#x27;s default embedding model is sentence transformers - and we have many other integrated - <a href="https:&#x2F;&#x2F;github.com&#x2F;chroma-core&#x2F;chroma&#x2F;blob&#x2F;main&#x2F;chromadb&#x2F;utils&#x2F;embedding_functions.py">https:&#x2F;&#x2F;github.com&#x2F;chroma-core&#x2F;chroma&#x2F;blob&#x2F;main&#x2F;chromadb&#x2F;uti...</a><p>&gt; It would be wonderful if there were a simpler (single file, SQLite or DuckDB like) database for vectors than the complex (and in some cases, unfortunately cloud-based) ones available now.<p>This is literally chroma!</div><br/></div></div></div></div><div id="36691270" class="c"><input type="checkbox" id="c-36691270" checked=""/><div class="controls bullet"><span class="by">drewcperkins</span><span>|</span><a href="#36690402">root</a><span>|</span><a href="#36690575">parent</a><span>|</span><a href="#36690644">prev</a><span>|</span><a href="#36690638">next</a><span>|</span><label class="collapse" for="c-36691270">[-]</label><label class="expand" for="c-36691270">[1 more]</label></div><br/><div class="children"><div class="content">Chroma is just a wrapper for Clickhouse. Iâd go with something like Weaviate or Qdrant that is a lot more mature as far as vector databases.</div><br/></div></div></div></div></div></div><div id="36690638" class="c"><input type="checkbox" id="c-36690638" checked=""/><div class="controls bullet"><span class="by">Xenoamorphous</span><span>|</span><a href="#36690402">prev</a><span>|</span><label class="collapse" for="c-36690638">[-]</label><label class="expand" for="c-36690638">[3 more]</label></div><br/><div class="children"><div class="content">Iâm confused by the choice of word2vec for the embeddings, for âsimplicityâ. Using a transformer based model like Universal Sentence Encoder or SBERT is just as easy and the results will be considerably better.</div><br/><div id="36690665" class="c"><input type="checkbox" id="c-36690665" checked=""/><div class="controls bullet"><span class="by">fzliu</span><span>|</span><a href="#36690638">parent</a><span>|</span><a href="#36691394">next</a><span>|</span><label class="collapse" for="c-36690665">[-]</label><label class="expand" for="c-36690665">[1 more]</label></div><br/><div class="children"><div class="content">word2vec is seminal and outputs static embeddings - fairly easy for beginners to understand. The embeddings for each token change relative to context in attention-based MLMs and I figured it might be confusing for an introductory blog.</div><br/></div></div><div id="36691394" class="c"><input type="checkbox" id="c-36691394" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#36690638">parent</a><span>|</span><a href="#36690665">prev</a><span>|</span><label class="collapse" for="c-36691394">[-]</label><label class="expand" for="c-36691394">[1 more]</label></div><br/><div class="children"><div class="content">Good enough for Google:<p><a href="https:&#x2F;&#x2F;www.google.com&#x2F;search?q=book+of+megadrive" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.google.com&#x2F;search?q=book+of+megadrive</a><p>In the TPUv4 paper they say they are doing word based embeddings for search, maybe explaining why Google search has gotten so bad.</div><br/></div></div></div></div></div></div></div></div></div></body></html>