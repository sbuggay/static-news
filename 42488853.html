<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1734944451746" as="style"/><link rel="stylesheet" href="styles.css?v=1734944451746"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2412.15210">Tokenisation Is NP-Complete</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>belter</span> | <span>12 comments</span></div><br/><div><div id="42491558" class="c"><input type="checkbox" id="c-42491558" checked=""/><div class="controls bullet"><span class="by">bnjmn</span><span>|</span><a href="#42490515">next</a><span>|</span><label class="collapse" for="c-42491558">[-]</label><label class="expand" for="c-42491558">[3 more]</label></div><br/><div class="children"><div class="content">&gt; We still do not know, for instance, what makes a good tokeniser (Gowda and May, 2020; Cognetta et al., 2024): which characteristics should its produced subwords `s` have to be a good starting point for language modelling? If we knew this, then we could define an <i>objective function</i> which we could evaluate tokenisers with.<p>I don&#x27;t see how the authors get past this true general statement from the first paragraph of the introduction. Finding a good tokenizer is not just NP-hard; we have no idea how hard it might be because we don&#x27;t have theoretical agreement on what &quot;good&quot; means.<p>In order to have something to prove, the authors decide (somewhat arbitrarily):<p>&gt; Specifically, we focus on finding tokenisers that maximise the compression of a text. Given this objective, we then define the tokenisation problem as the task of finding a tokeniser which compresses a dataset to at most δ symbols.<p>Is a tokenizer that maximizes the compression of text (e.g. by identifying longer tokens that tend to be used whole) necessarily a better tokenizer, in terms of overall model performance? Compression might be a useful property for an objective function to consider... but then again maybe not, if it makes the problem NP-hard.<p>I&#x27;m also not sure how realistic the limitation to &quot;at most δ symbols&quot; is. I mean, that limit is undeniably useful to make the proof of NP-completeness go through, because it&#x27;s a similar mechanism to the minimum number of satisfied clauses in the MAX-2-SAT definition. But why not just keep adding tokens as needed, rather than imposing any preordained limit? IIRC OpenAI&#x27;s tokenizer has a vocabulary of around 52k subword strings. When that tokenizer was being designed, I don&#x27;t imagine they worried much if the final number had been 60k or even 100k. How could you possibly choose a meaningful δ from first principles?<p>To put that point a different way, imagine the authors had proven NP-completeness by reduction from the Knapsack Problem, where the knapsack you&#x27;re packing has some maximum capacity. If you can easily swap your knapsack out for a larger knapsack whenever it gets (close to) full, then the problem becomes trivial.<p>If the authors managed to prove that any arbitrary objective function would lead to NP-hard tokenizer optimization problem, then their result would be more general. If the paper proves that somehow, I missed it.<p>I suppose this paper suggests &quot;here be dragons&quot; in an interesting if incomplete way, but I would also say there&#x27;s no need to hurt yourself with an expensive optimization problem when you&#x27;re not even sure it delivers the results you want.</div><br/><div id="42492632" class="c"><input type="checkbox" id="c-42492632" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#42491558">parent</a><span>|</span><a href="#42492635">next</a><span>|</span><label class="collapse" for="c-42492632">[-]</label><label class="expand" for="c-42492632">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not optimal, but if you want to play it safe you can use 1 byte = 1 token, plus a few control tokens like stop, user turn, model turn, etc.</div><br/></div></div><div id="42492635" class="c"><input type="checkbox" id="c-42492635" checked=""/><div class="controls bullet"><span class="by">mcyc</span><span>|</span><a href="#42491558">parent</a><span>|</span><a href="#42492632">prev</a><span>|</span><a href="#42490515">next</a><span>|</span><label class="collapse" for="c-42492635">[-]</label><label class="expand" for="c-42492635">[1 more]</label></div><br/><div class="children"><div class="content">Hi, I&#x27;m Cognetta from the above Cognetta et al. I can&#x27;t answer all of your questions (and I can&#x27;t speak for the authors of this paper ofc), but I will try to answer some.<p>&gt; Is a tokenizer that maximizes the compression of text (e.g. by identifying longer tokens that tend to be used whole) necessarily a better tokenizer, in terms of overall model performance? Compression might be a useful property for an objective function to consider... but then again maybe not, if it makes the problem NP-hard.<p>Compression isn&#x27;t necessarily the best metric for language modeling quality [1][2][3], but there are some papers that find a correlation between it and quality [4] and also it has one important benefit: it reduces inference time by making the input sequences shorter (this is particularly important for transformers, because the runtime is quadratic in the sequence length).<p>If you imagine that with enough data, basically any reasonable tokenization algorithm would be ok (I think this is mostly true; there are definitely bad and &quot;better&quot; tokenizers and you see this very clearly in small data settings, but once you get into the trillions-of-tokens and 10s-of-billion-of-parameters setting, other things are going to matter more), then optimizing the tokenizer for compression is a good choice as it will provide tangible, practical benefits in the sense of reduced inference time.<p>&gt; I&#x27;m also not sure how realistic the limitation to &quot;at most δ symbols&quot; is. [...] But why not just keep adding tokens as needed, rather than imposing any preordained limit?<p>This is a pretty realistic limitation imo. Of course you can arbitrarily increase the vocabulary size, but there is a tradeoff between modeling quality, parameter count, and inference time. If you increase the vocabulary a bunch, your inference speed will probably improve (although now you have a much larger softmax at the end of your model, which isn&#x27;t usually a bottleneck anymore, but still not great), parameter count will increase (due to the larger embedding table), and your modeling quality will go down (in that you have tokens which are so rare in the corpus that they are massively undertrained; this can cause big problems [5]).<p>So by constraining it to δ, you are basically setting a parameter budget for the vocabulary, and this is a pretty reasonable thing to do.<p>&gt; IIRC OpenAI&#x27;s tokenizer has a vocabulary of around 52k subword strings.<p>Yeah, the size of the vocabulary varies a lot across models, but it isn&#x27;t unusual to see significantly larger vocabularies these days (e.g., gemma has ~256k). However, these are still finite and very small compared to the corpus size.<p>&gt; How could you possibly choose a meaningful δ from first principles?<p>This is a really great question, and something that we don&#x27;t know how to answer. A lot of work has tried to answer it [6][7], but it is very much an open question.<p>[1]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.08754" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.08754</a><p>[2]: <a href="https:&#x2F;&#x2F;aclanthology.org&#x2F;2023.acl-long.284&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aclanthology.org&#x2F;2023.acl-long.284&#x2F;</a><p>[3]: <a href="https:&#x2F;&#x2F;aclanthology.org&#x2F;2024.emnlp-main.40&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aclanthology.org&#x2F;2024.emnlp-main.40&#x2F;</a><p>[4]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2403.06265" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2403.06265</a><p>[5]: <a href="https:&#x2F;&#x2F;aclanthology.org&#x2F;2024.emnlp-main.649&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aclanthology.org&#x2F;2024.emnlp-main.649&#x2F;</a><p>[6]: <a href="https:&#x2F;&#x2F;aclanthology.org&#x2F;2023.acl-long.284&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aclanthology.org&#x2F;2023.acl-long.284&#x2F;</a><p>[7]: <a href="https:&#x2F;&#x2F;aclanthology.org&#x2F;2020.findings-emnlp.352&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aclanthology.org&#x2F;2020.findings-emnlp.352&#x2F;</a></div><br/></div></div></div></div><div id="42490515" class="c"><input type="checkbox" id="c-42490515" checked=""/><div class="controls bullet"><span class="by">amanda99</span><span>|</span><a href="#42491558">prev</a><span>|</span><a href="#42491191">next</a><span>|</span><label class="collapse" for="c-42490515">[-]</label><label class="expand" for="c-42490515">[7 more]</label></div><br/><div class="children"><div class="content">Is this not kind of trivial, at least the way they&#x27;ve defined it? It&#x27;s kind of very obviously NP complete to me. Any of these text problems where you&#x27;re tying to optimize over a whole corpus is kind of not hard to see to be NP-complete, similar to longest common subsequence.</div><br/><div id="42490553" class="c"><input type="checkbox" id="c-42490553" checked=""/><div class="controls bullet"><span class="by">HeliumHydride</span><span>|</span><a href="#42490515">parent</a><span>|</span><a href="#42490834">next</a><span>|</span><label class="collapse" for="c-42490553">[-]</label><label class="expand" for="c-42490553">[3 more]</label></div><br/><div class="children"><div class="content">Longest common substring is linear time.</div><br/><div id="42491070" class="c"><input type="checkbox" id="c-42491070" checked=""/><div class="controls bullet"><span class="by">terrabiped</span><span>|</span><a href="#42490515">root</a><span>|</span><a href="#42490553">parent</a><span>|</span><a href="#42490834">next</a><span>|</span><label class="collapse" for="c-42491070">[-]</label><label class="expand" for="c-42491070">[2 more]</label></div><br/><div class="children"><div class="content">You may have misread the parent comment. Longest common substring is not the same type of problem as longest common <i>subsequence</i>.</div><br/><div id="42491882" class="c"><input type="checkbox" id="c-42491882" checked=""/><div class="controls bullet"><span class="by">Xmd5a</span><span>|</span><a href="#42490515">root</a><span>|</span><a href="#42491070">parent</a><span>|</span><a href="#42490834">next</a><span>|</span><label class="collapse" for="c-42491882">[-]</label><label class="expand" for="c-42491882">[1 more]</label></div><br/><div class="children"><div class="content">For those who were wondering what this means:<p><pre><code>    common substring: contiguous
    common subsequence: not necessarily contiguous but in order</code></pre></div><br/></div></div></div></div></div></div><div id="42490834" class="c"><input type="checkbox" id="c-42490834" checked=""/><div class="controls bullet"><span class="by">saiajc</span><span>|</span><a href="#42490515">parent</a><span>|</span><a href="#42490553">prev</a><span>|</span><a href="#42491191">next</a><span>|</span><label class="collapse" for="c-42490834">[-]</label><label class="expand" for="c-42490834">[3 more]</label></div><br/><div class="children"><div class="content">Longest common subsequence can be solved in at most quadratic time via dynamic programming.</div><br/><div id="42491065" class="c"><input type="checkbox" id="c-42491065" checked=""/><div class="controls bullet"><span class="by">terrabiped</span><span>|</span><a href="#42490515">root</a><span>|</span><a href="#42490834">parent</a><span>|</span><a href="#42491191">next</a><span>|</span><label class="collapse" for="c-42491065">[-]</label><label class="expand" for="c-42491065">[2 more]</label></div><br/><div class="children"><div class="content">Longest common subsequence for an arbitrary input is NP-hard: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Longest_common_subsequence" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Longest_common_subsequence</a><p>Some narrow versions of it could be optimally solved with DP, but when the constraints are lifted, you will probably have to pay for it with either exponential memory or time.<p>Same applies to the Knapsack problem. You can solve some variants of it with DP, but it won&#x27;t generalize.</div><br/><div id="42492749" class="c"><input type="checkbox" id="c-42492749" checked=""/><div class="controls bullet"><span class="by">hexomancer</span><span>|</span><a href="#42490515">root</a><span>|</span><a href="#42491065">parent</a><span>|</span><a href="#42491191">next</a><span>|</span><label class="collapse" for="c-42492749">[-]</label><label class="expand" for="c-42492749">[1 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t call the case with two sequence &quot;some narrow version&quot;. It is by far the most common instance of the problem and honestly the first thing that pops into my mind when I hear LCS, that&#x27;s why I was very surprised to hear it was NP-hard but it does make sense for an arbitrary number of sequences.</div><br/></div></div></div></div></div></div></div></div><div id="42491191" class="c"><input type="checkbox" id="c-42491191" checked=""/><div class="controls bullet"><span class="by">glitchc</span><span>|</span><a href="#42490515">prev</a><span>|</span><label class="collapse" for="c-42491191">[-]</label><label class="expand" for="c-42491191">[1 more]</label></div><br/><div class="children"><div class="content">For a second I thought this was related to DeFi... and then I read the abstract.</div><br/></div></div></div></div></div></div></div></body></html>