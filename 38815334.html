<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1704013264123" as="style"/><link rel="stylesheet" href="styles.css?v=1704013264123"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://theartofhpc.com/">The art of high performance computing</a> <span class="domain">(<a href="https://theartofhpc.com">theartofhpc.com</a>)</span></div><div class="subtext"><span>rramadass</span> | <span>87 comments</span></div><br/><div><div id="38818575" class="c"><input type="checkbox" id="c-38818575" checked=""/><div class="controls bullet"><span class="by">LASR</span><span>|</span><a href="#38815996">next</a><span>|</span><label class="collapse" for="c-38818575">[-]</label><label class="expand" for="c-38818575">[19 more]</label></div><br/><div class="children"><div class="content">The hardware &#x2F; datacenter side of this is equally fascinating.<p>I used to work in AWS, but on the software &#x2F; services side of things. But now and then, we would crash some talks from the datacenter folks.<p>One key relevation for me was that increasing compute power in DCs is primarily a thermodynamics problem than actual computing. The nodes have become so dense that shipping power in and shipping heat out, with all kinds of redundancies is an extremely hard problem. And it&#x27;s not like you can perform a software update if you&#x27;ve discovered some inefficiencies.<p>This was ~10 years ago, so probably some things have changed.<p>What blows me away is that Amazon, starting out as an internet bookstore is at the cutting edge of solving thermodynamics problems.</div><br/><div id="38819766" class="c"><input type="checkbox" id="c-38819766" checked=""/><div class="controls bullet"><span class="by">projectileboy</span><span>|</span><a href="#38818575">parent</a><span>|</span><a href="#38820098">next</a><span>|</span><label class="collapse" for="c-38819766">[-]</label><label class="expand" for="c-38819766">[2 more]</label></div><br/><div class="children"><div class="content">Seymour Cray used to say this all the way back in the 1970s: his biggest problems were associated with dissipating heat. For the Cray 2 he took an even more dramatic approach: &quot;The Cray-2&#x27;s unusual cooling scheme immersed dense stacks of circuit boards in a special non-conductive liquid called Fluorinert™&quot; (<a href="https:&#x2F;&#x2F;www.computerhistory.org&#x2F;revolution&#x2F;supercomputers&#x2F;10&#x2F;68" rel="nofollow">https:&#x2F;&#x2F;www.computerhistory.org&#x2F;revolution&#x2F;supercomputers&#x2F;10...</a>)</div><br/><div id="38820922" class="c"><input type="checkbox" id="c-38820922" checked=""/><div class="controls bullet"><span class="by">logtempo</span><span>|</span><a href="#38818575">root</a><span>|</span><a href="#38819766">parent</a><span>|</span><a href="#38820098">next</a><span>|</span><label class="collapse" for="c-38820922">[-]</label><label class="expand" for="c-38820922">[1 more]</label></div><br/><div class="children"><div class="content">Few days ago I saw an article passing by,about chips hiting the kw floor.</div><br/></div></div></div></div><div id="38820098" class="c"><input type="checkbox" id="c-38820098" checked=""/><div class="controls bullet"><span class="by">cogman10</span><span>|</span><a href="#38818575">parent</a><span>|</span><a href="#38819766">prev</a><span>|</span><a href="#38821049">next</a><span>|</span><label class="collapse" for="c-38820098">[-]</label><label class="expand" for="c-38820098">[15 more]</label></div><br/><div class="children"><div class="content">It always made me wonder why liquid cooling wasn&#x27;t more of a thing for datacenters.<p>Water has a massive amount of thermal capacity and can quickly and in bulk be cooled to optimal temperatures.  You&#x27;d probably still need fans and AC to dissipate heat of non-liquid cooled parts, but for the big energy items like CPUs and GPUs&#x2F;compute engines, you could ship out huge amounts of heat fairly quickly and directly.<p>I guess the complexity and risk of a leak would be a problem, but for amazon sized data centers that doesn&#x27;t seem like a major concern.</div><br/><div id="38820339" class="c"><input type="checkbox" id="c-38820339" checked=""/><div class="controls bullet"><span class="by">adev_</span><span>|</span><a href="#38818575">root</a><span>|</span><a href="#38820098">parent</a><span>|</span><a href="#38820363">next</a><span>|</span><label class="collapse" for="c-38820339">[-]</label><label class="expand" for="c-38820339">[3 more]</label></div><br/><div class="children"><div class="content">&gt; It always made me wonder why liquid cooling wasn&#x27;t more of a thing for datacenters.<p>Liquid cooling is almost a defacto-standard in data centers in the HPC world. The Top of the TOP500 machines are all liquid cooled. Not by choice, but due to physics constraints.<p>There is a big gap in power density between the HPC world and the usual datacenter-commodity-hardware world.<p>Commodity DS are designed with the assumption that the average machine will run with a fraction of it&#x27;s maximum load. HPC systems at the opposite are designed to operate safely at 100% load all the time.<p>In a previous company where I worked, we attempted to install a medium size HPC cluster in a well-known commerical datacenter and network provider. The commercial of the DS almost felt from his chair when we announced the power requirements.</div><br/><div id="38820375" class="c"><input type="checkbox" id="c-38820375" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#38818575">root</a><span>|</span><a href="#38820339">parent</a><span>|</span><a href="#38821162">next</a><span>|</span><label class="collapse" for="c-38820375">[-]</label><label class="expand" for="c-38820375">[1 more]</label></div><br/><div class="children"><div class="content">&gt; we attempted to install a medium size HPC cluster in a well-known commerical Datacenter and network provider. The commercial of the DS almost fall from his chair when we announced the power requirements.<p>Heh. We tried it too. They didn’t believe that a single node used their entire rack’s budget at first.</div><br/></div></div></div></div><div id="38820363" class="c"><input type="checkbox" id="c-38820363" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#38818575">root</a><span>|</span><a href="#38820098">parent</a><span>|</span><a href="#38820339">prev</a><span>|</span><a href="#38820335">next</a><span>|</span><label class="collapse" for="c-38820363">[-]</label><label class="expand" for="c-38820363">[9 more]</label></div><br/><div class="children"><div class="content">Because it’s complex. Even more complex than “engineered” air.<p>You need two circuits, and a CDU between them. Coolants needs maintaining. You add antifreeze, biocides, etc.<p>Air is brute force. It cools everything it touches. Liquid cooling is serialized in a node. Two sockets? Second will be hotter. HBA not making good contact? It’ll overheat.<p>You add extensive leak detection subsystems, the amount of coolant moving in your primary circuit becomes massive.<p>Currently you can remove 97% of the heat via liquid (including the PSUs), and it’s cheaper to do so than air, but it’s not “rails, screws, cables, power on”. Air cooled systems can be turned on in a week. Liquid cooled ones take a month.<p>However, using liquid is mandatory after some point. You can’t cool systems that dense and under that load with air. They’ll melt.</div><br/><div id="38822126" class="c"><input type="checkbox" id="c-38822126" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#38818575">root</a><span>|</span><a href="#38820363">parent</a><span>|</span><a href="#38820916">next</a><span>|</span><label class="collapse" for="c-38822126">[-]</label><label class="expand" for="c-38822126">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Liquid cooling is serialized in a node. Two sockets?<p>I&#x27;ve seen tests done on heavy PC loops (ie multi-GPU) both high-flow and low-flow, as well as on car engines, in different coolant flow configurations. The results from all of those are that the water doesn&#x27;t rise meaningfully in temperature between components.<p>Unless I did my back-of-the-napkin math wrong, this seems reasonable. If you have a single 10mm ID pipe going through a 1U server and up to the next, then for a full 42U rack you have about 1.7kg of water going through the servers. If the flow rate is about 1s per server (so 42 seconds for the full rack) and each 1U server dumps 500W of energy into the water, there should be just a 3 degree C difference in the water temperature between the first and the last server.</div><br/><div id="38822289" class="c"><input type="checkbox" id="c-38822289" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#38818575">root</a><span>|</span><a href="#38822126">parent</a><span>|</span><a href="#38820916">next</a><span>|</span><label class="collapse" for="c-38822289">[-]</label><label class="expand" for="c-38822289">[3 more]</label></div><br/><div class="children"><div class="content">In our system every node gets inlet water at the same temperature via parallel piping, but when it’s in node, it goes through processors first, then RAM, then PCIe and disks. Delta T between two sockets is 5 degrees C, and the delta T between input and output is around 15-18 C depending on load.</div><br/><div id="38822675" class="c"><input type="checkbox" id="c-38822675" checked=""/><div class="controls bullet"><span class="by">menaerus</span><span>|</span><a href="#38818575">root</a><span>|</span><a href="#38822289">parent</a><span>|</span><a href="#38820916">next</a><span>|</span><label class="collapse" for="c-38822675">[-]</label><label class="expand" for="c-38822675">[2 more]</label></div><br/><div class="children"><div class="content">First, thanks for sharing these details, I find them fascinating because they are not so common to be read or heard about.<p>&gt; Delta T between two sockets is 5 degrees C<p>And secondly, ~5-10 degrees is what I see on my dual-socket workstation, and have been wondering about this delta ever since the first day I started monitoring the temperatures. At first, I thought that the heat sink wasn&#x27;t installed properly but after reinstalling it the delta remained. Since I didn&#x27;t notice any CPU throttling or whatsoever I figured it&#x27;s &quot;normal&quot; and ignored it.</div><br/><div id="38822692" class="c"><input type="checkbox" id="c-38822692" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#38818575">root</a><span>|</span><a href="#38822675">parent</a><span>|</span><a href="#38820916">next</a><span>|</span><label class="collapse" for="c-38822692">[-]</label><label class="expand" for="c-38822692">[1 more]</label></div><br/><div class="children"><div class="content">Hey, no worries. Using one is equally fascinating as much as reading about it. It feels like a space shuttle, so different, yet so enjoyable.<p>I mean, water travels from one socket to another, so one processor adds heat equal to 5 degrees C under nominal load. The second socket doesn’t complain much, but this is enormous amounts of heat transferred in a such quick pace.</div><br/></div></div></div></div></div></div></div></div><div id="38820916" class="c"><input type="checkbox" id="c-38820916" checked=""/><div class="controls bullet"><span class="by">gopher_space</span><span>|</span><a href="#38818575">root</a><span>|</span><a href="#38820363">parent</a><span>|</span><a href="#38822126">prev</a><span>|</span><a href="#38820335">next</a><span>|</span><label class="collapse" for="c-38820916">[-]</label><label class="expand" for="c-38820916">[4 more]</label></div><br/><div class="children"><div class="content">What&#x27;s this all look like without an atmosphere?</div><br/><div id="38821048" class="c"><input type="checkbox" id="c-38821048" checked=""/><div class="controls bullet"><span class="by">eutropia</span><span>|</span><a href="#38818575">root</a><span>|</span><a href="#38820916">parent</a><span>|</span><a href="#38820335">next</a><span>|</span><label class="collapse" for="c-38821048">[-]</label><label class="expand" for="c-38821048">[3 more]</label></div><br/><div class="children"><div class="content">Worse, heat dissipation is a major constraint for spacecraft and satellites because you can only radiate heat away as infrared photons.</div><br/><div id="38821759" class="c"><input type="checkbox" id="c-38821759" checked=""/><div class="controls bullet"><span class="by">uticus</span><span>|</span><a href="#38818575">root</a><span>|</span><a href="#38821048">parent</a><span>|</span><a href="#38821350">next</a><span>|</span><label class="collapse" for="c-38821759">[-]</label><label class="expand" for="c-38821759">[1 more]</label></div><br/><div class="children"><div class="content">Amazing considering how much heat travels from Sun (and punches through atmosphere) to Earth surface. Didn’t realize there was that much of an insulation property.</div><br/></div></div><div id="38821350" class="c"><input type="checkbox" id="c-38821350" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#38818575">root</a><span>|</span><a href="#38821048">parent</a><span>|</span><a href="#38821759">prev</a><span>|</span><a href="#38820335">next</a><span>|</span><label class="collapse" for="c-38821350">[-]</label><label class="expand" for="c-38821350">[1 more]</label></div><br/><div class="children"><div class="content">Doesn’t have to be infrared but yeah, space isn’t “cold” so much as it’s an insulator.</div><br/></div></div></div></div></div></div></div></div><div id="38820335" class="c"><input type="checkbox" id="c-38820335" checked=""/><div class="controls bullet"><span class="by">lub</span><span>|</span><a href="#38818575">root</a><span>|</span><a href="#38820098">parent</a><span>|</span><a href="#38820363">prev</a><span>|</span><a href="#38820358">next</a><span>|</span><label class="collapse" for="c-38820335">[-]</label><label class="expand" for="c-38820335">[1 more]</label></div><br/><div class="children"><div class="content">OVH prominently uses water cooling including custom components with their own design.<p><a href="https:&#x2F;&#x2F;blog.ovhcloud.com&#x2F;water-cooling-from-innovation-to-disruption-part-i&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.ovhcloud.com&#x2F;water-cooling-from-innovation-to-d...</a><p><a href="https:&#x2F;&#x2F;blog.ovhcloud.com&#x2F;water-cooling-from-innovation-to-disruption-part-ii&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.ovhcloud.com&#x2F;water-cooling-from-innovation-to-d...</a><p><a href="https:&#x2F;&#x2F;blog.ovhcloud.com&#x2F;new-hybrid-immersion-liquid-cooling-developments-at-ovhcloud&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.ovhcloud.com&#x2F;new-hybrid-immersion-liquid-coolin...</a></div><br/></div></div><div id="38820358" class="c"><input type="checkbox" id="c-38820358" checked=""/><div class="controls bullet"><span class="by">victotronics</span><span>|</span><a href="#38818575">root</a><span>|</span><a href="#38820098">parent</a><span>|</span><a href="#38820335">prev</a><span>|</span><a href="#38821049">next</a><span>|</span><label class="collapse" for="c-38820358">[-]</label><label class="expand" for="c-38820358">[1 more]</label></div><br/><div class="children"><div class="content">Immersion cooling is getting big. At the last Supercomputing conference I probably saw at least a dozen vendors of immersion cooling equipment. My datacenter has one cluster with liquid cooling caps over the sockets, and two immersed clusters. The latter two have basins of various degrees of sophistication under them for when they do spring a leak.</div><br/></div></div></div></div><div id="38821049" class="c"><input type="checkbox" id="c-38821049" checked=""/><div class="controls bullet"><span class="by">cyrillite</span><span>|</span><a href="#38818575">parent</a><span>|</span><a href="#38820098">prev</a><span>|</span><a href="#38815996">next</a><span>|</span><label class="collapse" for="c-38821049">[-]</label><label class="expand" for="c-38821049">[1 more]</label></div><br/><div class="children"><div class="content">Is there any good data on the scale of this problem or that can be used to visualise it?<p>What is the cutting edge of cooling tech like?</div><br/></div></div></div></div><div id="38815996" class="c"><input type="checkbox" id="c-38815996" checked=""/><div class="controls bullet"><span class="by">dist1ll</span><span>|</span><a href="#38818575">prev</a><span>|</span><a href="#38817574">next</a><span>|</span><label class="collapse" for="c-38815996">[-]</label><label class="expand" for="c-38815996">[33 more]</label></div><br/><div class="children"><div class="content">It&#x27;s very interesting how abtracted away HPC sometimes looks from hardware. The books seem to revolve a lot around SPMD programming, algo &amp; DS, task parallelism, synchronization etc, but very little about computer architecture details like supercomputer memory subsystems, high-bandwidth interconnects like CXL, GPU architecture and so on. Are the abstractions and tooling already good enough that you don&#x27;t need to worry about these details? I&#x27;m also curious if HPC practitioners have to fiddle a lot of black-box knobs to squeeze out performance?</div><br/><div id="38816453" class="c"><input type="checkbox" id="c-38816453" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#38815996">parent</a><span>|</span><a href="#38817723">next</a><span>|</span><label class="collapse" for="c-38816453">[-]</label><label class="expand" for="c-38816453">[1 more]</label></div><br/><div class="children"><div class="content">For most HPC, you will not be able to maximize parallelism and throughput without intimate knowledge of the hardware architecture and its behavior. As a general principle, you want the topology of the software to match the topology of the hardware as closely as possible for optimal scaling behavior. Efficient HPC software is strongly influenced by the nature of the hardware.<p>When I wrote code for new HPC hardware, people were always surprised when I asked for the system hardware and architecture docs instead of the programming docs. But if you understood the hardware design, the correct way of designing software for it became obvious from first principles. The programming docs typically contained quite a few half-truths intended to make things seem misleadingly easier for developers than a proper understanding would suggest. In fact, some HPC platforms failed in large part because they consistently misrepresented what was required from developers to achieve maximum performance in order to appear &quot;easy to use&quot;, and then failing to deliver the performance the silicon was capable of if you actually wrote software the way the marketing implied would be effective.<p>You can write HPC code on top of abstractions, and many people do, but the performance and scaling losses are often unavoidably integer factor. As with most software, this was considered an acceptable loss in many cases if it allowed less capable software devs to design the code. HPC is like any other type of software in that most developers that notionally specialize in it struggle to produce consistently good results. Much of the expensive hardware used in HPC is there to mitigate the performance losses of worse software designs.<p>In HPC there are no shortcuts to actually understanding how the hardware works if you want maximum performance. Which is no different than regular software, in HPC the hardware systems are just bigger and more complex.</div><br/></div></div><div id="38817723" class="c"><input type="checkbox" id="c-38817723" checked=""/><div class="controls bullet"><span class="by">bluedino</span><span>|</span><a href="#38815996">parent</a><span>|</span><a href="#38816453">prev</a><span>|</span><a href="#38816651">next</a><span>|</span><label class="collapse" for="c-38817723">[-]</label><label class="expand" for="c-38817723">[7 more]</label></div><br/><div class="children"><div class="content">I started in HPC about 2 years ago on a ~500 node cluster at a Fortune 100 company. I was really just looking for a job where I was doing Linux 100% of the time, and it&#x27;s been fun so far.<p>But it wasn&#x27;t what I thought it would be. I guess I expected to be doing more performance oriented work, analyzing numbers and trying to get every last bit of performance out of the cluster. To be honest, they didn&#x27;t even have any kind of monitoring running. I set some up, and it doesn&#x27;t really get used. Once in a while we get questions from management about &quot;how busy is the cluster&quot;, to justify budgets and that sort of thing.<p>Most of my &#x27;optimization&#x27; work ends up being things like making sure people aren&#x27;t (usually unknowingly) requesting 384 CPUs when their script only uses 16, testing software to see what # of CPU&#x27;s it works with before you see a degradation, etc. I&#x27;ve only had the Intel profiler open twice.<p>And I&#x27;ve found that most of the job is really just helping researchers and such with their work. Typically running either a commercial or open-source program, troubleshooting it, or getting some code written by another team on another cluster and getting it built and running on yours. Slogging through terrible Python code. Trying to get a C++ project built on a more modern cluster in a CentOS 7 environment.<p>It can be fun in a way. I&#x27;ve worked with different languages over the years so I enjoy trying to get things working, digging through crashes and stack traces. And working with such large machines, your sense of normal gets twisted when you&#x27;re on a server with &#x27;only&#x27; 128GB of RAM or 20TB of disk.<p>It&#x27;s a little scary when you know the results of some of this stuff are being used in the real world, and the people running the simulations aren&#x27;t even doing things right. Incorrect code, mixed up source code, not using the data they thing they are, I once found a huge bug that had existed for 3 years. Doesn&#x27;t this invalidate all the work you&#x27;ve done on this subject?<p>The one drawback I find is that a lot of HPC jobs want you do have a masters degree. Even to just run the cluster. Doesn&#x27;t make sense to me, I&#x27;m not writing the software you&#x27;re running, we aren&#x27;t running some state of the art, TOP500 cluster. We&#x27;re just getting a bunch of machines networked together and running some code.</div><br/><div id="38818664" class="c"><input type="checkbox" id="c-38818664" checked=""/><div class="controls bullet"><span class="by">justin66</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38817723">parent</a><span>|</span><a href="#38817994">next</a><span>|</span><label class="collapse" for="c-38818664">[-]</label><label class="expand" for="c-38818664">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The one drawback I find is that a lot of HPC jobs want you do have a masters degree.<p>Is it possible that pretty much any specialization, outside of the most common ones, engages in a lot of gatekeeping? I remember how difficult it appeared to be after I graduated to break into embedded systems (I never did). I persisted until I realized it doesn&#x27;t even pay very well, comparatively.</div><br/></div></div><div id="38817994" class="c"><input type="checkbox" id="c-38817994" checked=""/><div class="controls bullet"><span class="by">throwawaaarrgh</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38817723">parent</a><span>|</span><a href="#38818664">prev</a><span>|</span><a href="#38816651">next</a><span>|</span><label class="collapse" for="c-38817994">[-]</label><label class="expand" for="c-38817994">[5 more]</label></div><br/><div class="children"><div class="content">I always found that funny too. A business who needs a powerful computing solution can come up with some amazingly robust stuff, whereas science&#x2F;research just buys a big mainframe and hopes it works.</div><br/><div id="38820532" class="c"><input type="checkbox" id="c-38820532" checked=""/><div class="controls bullet"><span class="by">s_Hogg</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38817994">parent</a><span>|</span><a href="#38816651">next</a><span>|</span><label class="collapse" for="c-38820532">[-]</label><label class="expand" for="c-38820532">[4 more]</label></div><br/><div class="children"><div class="content">I was working in a company that had been spun out of a university until recently and it was shocking how hopeless the researchers were. I&#x27;ve always been critical of how poor the job security in academia is but you&#x27;d think it&#x27;s still too much given how slapdash some of the crap you see is. We basically had to reinvent their product from the ground up, awful.</div><br/><div id="38821182" class="c"><input type="checkbox" id="c-38821182" checked=""/><div class="controls bullet"><span class="by">danparsonson</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38820532">parent</a><span>|</span><a href="#38821544">next</a><span>|</span><label class="collapse" for="c-38821182">[-]</label><label class="expand" for="c-38821182">[2 more]</label></div><br/><div class="children"><div class="content">This is probably a naive question but isn&#x27;t that the point of having developers on staff? The researchers aren&#x27;t coders and vice versa, so having researchers produce prototypes that are productized by engineers makes sense to me.</div><br/><div id="38821679" class="c"><input type="checkbox" id="c-38821679" checked=""/><div class="controls bullet"><span class="by">rramadass</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38821182">parent</a><span>|</span><a href="#38821544">next</a><span>|</span><label class="collapse" for="c-38821679">[-]</label><label class="expand" for="c-38821679">[1 more]</label></div><br/><div class="children"><div class="content">Exactly! This is how it should be.<p>Researchers&#x2F;Scientists with their hard earned PhDs should only concentrate on doing cutting-edge &quot;researchy&quot; stuff. It is hard enough that they should not be asked to learn all the intricacies&#x2F;problems inherent in Software Development. That is the domain of a &quot;Professional Software Engineer&quot;.<p>There is now in fact a new class called &quot;Research Software Engineer&quot; who are Software Developers working in Research developing code specific to their needs - <a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;d41586-022-01516-2" rel="nofollow">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;d41586-022-01516-2</a> and <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Research_software_engineering" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Research_software_engineering</a></div><br/></div></div></div></div><div id="38821544" class="c"><input type="checkbox" id="c-38821544" checked=""/><div class="controls bullet"><span class="by">m-ee</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38820532">parent</a><span>|</span><a href="#38821182">prev</a><span>|</span><a href="#38816651">next</a><span>|</span><label class="collapse" for="c-38821544">[-]</label><label class="expand" for="c-38821544">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had very similar experiences working with former researchers including at a university spinout. Mechanical rather than CS. It was perplexing how they still carried the elitism that industry was mostly for people who can&#x27;t hack it in academia given the quality of their work. Would be unacceptable coming from a new hire PD engineer at Apple yet you&#x27;re demanding respect because you used to lead a whole lab apparently producing rubbish?</div><br/></div></div></div></div></div></div></div></div><div id="38816651" class="c"><input type="checkbox" id="c-38816651" checked=""/><div class="controls bullet"><span class="by">dahart</span><span>|</span><a href="#38815996">parent</a><span>|</span><a href="#38817723">prev</a><span>|</span><a href="#38816158">next</a><span>|</span><label class="collapse" for="c-38816651">[-]</label><label class="expand" for="c-38816651">[3 more]</label></div><br/><div class="children"><div class="content">There is a lot of abstraction, but knowing which abstraction to use still takes knowing a lot about the hardware.<p>&gt; I’m also curious if HPC practitioners have to fiddle a lot of black-box knobs to squeeze out performance?<p>In my experience with CUDA developers, yes the Shmoo Plot (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Shmoo_plot" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Shmoo_plot</a>, sometimes called a ‘wedge’ in some industries) is one of the workhorses of every day optimization. I’m not sure I’d call it black-box, though maybe the net effect is the same. It’s really common to have educated guesses and to know what the knobs do and how they work, and still find big surprises when you measure. The first rule of optimization is measure. I always think of Michael Abrash’s first chapter in the “Black Book”: “The Best Optimizer is Between Your Ears” <a href="http:&#x2F;&#x2F;twimgs.com&#x2F;ddj&#x2F;abrashblackbook&#x2F;gpbb1.pdf" rel="nofollow">http:&#x2F;&#x2F;twimgs.com&#x2F;ddj&#x2F;abrashblackbook&#x2F;gpbb1.pdf</a>. This is a fabulous snippet of the philosophy of high performance (even though it’s PC game centric and not about modern HPC.)<p>Related to your point about abstraction, the heaviest knob-tuning should get done at the end of the optimization process, because as soon as you refactor or change anything, you have to do the knob tuning again. A minor change in register spills or cache access patterns can completely reset any fine-tuning of thread configuration or cache or shared memory size, etc.. Despite this, some healthy amount of knob tuning is still done along the way to check &amp; balance &amp; get an intuitive sense of the local perf space of the code. (Just noticed Abrash talks a little about why this is a good idea.)</div><br/><div id="38818857" class="c"><input type="checkbox" id="c-38818857" checked=""/><div class="controls bullet"><span class="by">squidgyhead</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38816651">parent</a><span>|</span><a href="#38816158">next</a><span>|</span><label class="collapse" for="c-38818857">[-]</label><label class="expand" for="c-38818857">[2 more]</label></div><br/><div class="children"><div class="content">Could you explain how you use a shmoo plot for optimization?  Do you just have a performance metric at each point in parameter space?</div><br/><div id="38820316" class="c"><input type="checkbox" id="c-38820316" checked=""/><div class="controls bullet"><span class="by">dahart</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38818857">parent</a><span>|</span><a href="#38816158">next</a><span>|</span><label class="collapse" for="c-38820316">[-]</label><label class="expand" for="c-38820316">[1 more]</label></div><br/><div class="children"><div class="content">The shmoo plot is just the name for measuring something (such as perf) over a range of parameter space. The simplest and most straightforward application is to pick a parameter or two that you don’t know what value they should be using, do the shmoo over the range of parameter space, and then set the knobs at whatever values give you the optimal measurement.<p>Usually though, you have to iterate. Doing shmoos along the way can help with understanding the effects of code changes, help understand how the hardware works, and it can sometimes help identify what code changes you might need to make. A simple abstract example might be I know what my theoretical peak bandwidth is, but my program only gets 30% of peak. I suspect it has to do with how many registers are used, and I have a knob to control it, so I turn the knob and plot all possible register settings, and find out that I can get 45% of peak with a different value. Now I know it was partially registers I was limited by, but I also know to look for something else too. Then I profile, examine the code, maybe refactor or adjust some things, hypothesize, test, and then shmoo again on a different knob or two if I suspect something else is the bottleneck.</div><br/></div></div></div></div></div></div><div id="38816158" class="c"><input type="checkbox" id="c-38816158" checked=""/><div class="controls bullet"><span class="by">atrettel</span><span>|</span><a href="#38815996">parent</a><span>|</span><a href="#38816651">prev</a><span>|</span><a href="#38819484">next</a><span>|</span><label class="collapse" for="c-38816158">[-]</label><label class="expand" for="c-38816158">[3 more]</label></div><br/><div class="children"><div class="content">Yes and no.<p>MPI and OpenMP are the primary abstractions from the hardware in HPC, with MPI being an abstracted form of distributed-memory parallel computing and OpenMP being an abstracted form of shared-memory parallel computing.  Many researchers write their codes purely using those, often both in the same code.  When using those, you really do not need to worry about the architectural details most of the time.<p>Still, some researchers who like to further optimize things do in fact fiddle with a lot of small architectural details to increase performance further.  For example, loop unrolling is pretty common and can get quite confusing in my opinion.  I vaguely recall some stuff about trying to vectorize operations by preferring addition over multiplication due to the particular CPU architecture, but I do not think I&#x27;ve seen that in practice.<p>Preventing cache misses is another major one, where some codes are written so that the most needed information is stored in the CPU&#x27;s cache rather than memory.  Most codes only handle this by ensuring column-major order loops for array operations in Fortran or row-major order loops in C, but the concept can be extended further.  If you know the cache size for your processors, you could hypothetically optimize some operations to keep all of the needed information inside the cache to minimize cache misses.  I&#x27;ve never seen this in practice but it was actively discussed in the scientific computing course I took in 2013.<p>The use of particular GPUs depends heavily on the problem being solved, with some being great on GPUs and others being too difficult.  I&#x27;m not too knowledgeable about that, unfortunately.</div><br/><div id="38816337" class="c"><input type="checkbox" id="c-38816337" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38816158">parent</a><span>|</span><a href="#38819484">next</a><span>|</span><label class="collapse" for="c-38816337">[-]</label><label class="expand" for="c-38816337">[2 more]</label></div><br/><div class="children"><div class="content">Of course, not every problem can be solved by BLAS, but if you are doing linear algebra, the cache stuff should be mostly handled by BLAS.<p>I’m not sure how much multiplication vs addition matters on a modern chip. You can have a bazillion instructions in flight after all, as long as they don’t have any dependencies, so I’d go with whichever option shortens the data dependencies on the critical path. The computer will figure out where to park longer instruction if it needs to.</div><br/><div id="38816463" class="c"><input type="checkbox" id="c-38816463" checked=""/><div class="controls bullet"><span class="by">atrettel</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38816337">parent</a><span>|</span><a href="#38819484">next</a><span>|</span><label class="collapse" for="c-38816463">[-]</label><label class="expand" for="c-38816463">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right that the addition vs. multiplication issue likely does not matter on a modern chip.  I just gave the example because it shows how the CPU architecture can affect how you write the code.  I do not recall precisely when or where I heard the idea, but it was about a decade ago --- ages ago by computing standards.</div><br/></div></div></div></div></div></div><div id="38819484" class="c"><input type="checkbox" id="c-38819484" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#38815996">parent</a><span>|</span><a href="#38816158">prev</a><span>|</span><a href="#38819821">next</a><span>|</span><label class="collapse" for="c-38819484">[-]</label><label class="expand" for="c-38819484">[2 more]</label></div><br/><div class="children"><div class="content">HPC admin here, generally serving &quot;long tail of science&quot; researchers.<p>In today&#x27;s x86_64 hardware, there&#x27;s no &quot;supercomputer memory subsystem&quot;. It&#x27;s just a glorified NUMA system, and the biggest problem is putting the memory close to your core, i.e. keeping data local in your NUMA node to reduce latencies.<p>Your resource mapping is handled by your scheduler. It knows your hardware, hence it creates a cgroup which satisfies your needs and as optimized as possible, and stuffs your application into that cgroup and runs it.<p>Currently king of high performance interconnects is Infiniband, and it accelerates MPI at the fabric level. You can send messages, broadcasts and reduce results like there&#x27;s no tomorrow. Because when the message arrives you, it&#x27;s already reduced. When you broadcast, you only send a single message which is broadcasted at fabric layer. Multiple Context IB cards have many queues and more than one MPI job can run on the same node&#x2F;card with queue&#x2F;context isolation.<p>If you&#x27;re using a framework for GPU work, the architecture &amp; optimization is done at that level automatically (the framework developers do the hard work generally). NVIDIA&#x27;s drivers are pure black magic, too. They handle some parts of the optimization, too. InterGPU connection is handled by a physical fabric, managed by drivers and its own daemon.<p>If you&#x27;re CPU bound, your libraries are generally hand tuned by its vendor (Intel MKL, BLAS, Eigen, etc.). I personally used Eigen, and it has processor specific hints and optimizations baked in.<p>The things you have to worry is to compile your code for the correct architecture, make sure that the hardware you run on can satisfy your demands (i.e.: do not make too many random memory accesses, keep the prefetcher and branch predictor happy if you&#x27;re trying to go &quot;all-out fast&quot; on the node, do not abuse disk access, etc.).<p>On the number crunching side, keeping things independent (so they can be instruction level parallelized&#x2F;vectorized), making sure you&#x27;re not doing unnecessary calculations, and not abusing MPI (reducing inter-node talk to only necessary chatter) is the key.<p>It&#x27;s way easier said than done, but when you get the hang of it, it becomes like a second nature to think about these things, if these kinds of things are your cup of tea.</div><br/><div id="38820512" class="c"><input type="checkbox" id="c-38820512" checked=""/><div class="controls bullet"><span class="by">dist1ll</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38819484">parent</a><span>|</span><a href="#38819821">next</a><span>|</span><label class="collapse" for="c-38820512">[-]</label><label class="expand" for="c-38820512">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the thoughtful comment, pretty fascinating stuff.<p>&gt; In today&#x27;s x86_64 hardware, there&#x27;s no &quot;supercomputer memory subsystem&quot;. It&#x27;s just a glorified NUMA system, and the biggest problem is putting the memory close to your core, i.e. keeping data local in your NUMA node to reduce latencies.<p>I mean, memory topology varies greatly by uarch (doubly so between vendors). I can&#x27;t take a routine tuned to Nehalem, run it on Haswell or Skylake and expect it to stay competitive. More generally, different hardware has different bandwidth and latency ratios, which affects software design (e.g. software written for commodity Dell w&#x2F; PCIe cards probably won&#x27;t translate to Cray accelerator grid connected by HPE slingshot). And then there&#x27;s hardware-specific features like RNICs bypassing DRAM and writing RDMA messages directly into the receiver&#x27;s cache. So I think that ccNUMA and data locality is not sufficient to reason about memory perf.</div><br/></div></div></div></div><div id="38819821" class="c"><input type="checkbox" id="c-38819821" checked=""/><div class="controls bullet"><span class="by">efxhoy</span><span>|</span><a href="#38815996">parent</a><span>|</span><a href="#38819484">prev</a><span>|</span><a href="#38816199">next</a><span>|</span><label class="collapse" for="c-38819821">[-]</label><label class="expand" for="c-38819821">[1 more]</label></div><br/><div class="children"><div class="content">I wrote scientific simulation software in academia for a few years. None of us writing the software had any formal software engineering training above what we’d pieced together ourselves from statistics courses. We wrote our simulations to run independently on many nodes and aggregated the results at the end, no use of any HPC features other than “run these 100 scripts on a node each please, thank you slurm”. That approach worked very well for our problem.<p>I’d bet a significant part of compute work on HPC clusters in academia works the same way. The only thing we paid attention to was number of cores on the node and preferring node local storage over the shared volumes for caching. No MPI.<p>There are of course problems requiring “genuine” HPC clusters but ours could have run on any pile of workers with a job queue.</div><br/></div></div><div id="38816199" class="c"><input type="checkbox" id="c-38816199" checked=""/><div class="controls bullet"><span class="by">eslaught</span><span>|</span><a href="#38815996">parent</a><span>|</span><a href="#38819821">prev</a><span>|</span><a href="#38816409">next</a><span>|</span><label class="collapse" for="c-38816199">[-]</label><label class="expand" for="c-38816199">[1 more]</label></div><br/><div class="children"><div class="content">No, the abstractions are not sufficient. We do care about these details, a lot.<p>Of course, not every application is optimized to the hilt. But if you <i>want</i> to so optimize an application, exactly things you&#x27;re talking about are what come into play.<p>So yes, I would expect every competent HPC practitioner to have a solid (if not necessarily intimate) grasp of hardware architecture.</div><br/></div></div><div id="38816409" class="c"><input type="checkbox" id="c-38816409" checked=""/><div class="controls bullet"><span class="by">mgaunard</span><span>|</span><a href="#38815996">parent</a><span>|</span><a href="#38816199">prev</a><span>|</span><a href="#38816668">next</a><span>|</span><label class="collapse" for="c-38816409">[-]</label><label class="expand" for="c-38816409">[1 more]</label></div><br/><div class="children"><div class="content">Regardless of what you do, domain knowledge tends to be more valuable than purely technical skills.<p>Knowing more numerical analysis will get probably get you further in HPC than knowledge of specific hardware architectures.<p>Ideally you want both, of course.</div><br/></div></div><div id="38816668" class="c"><input type="checkbox" id="c-38816668" checked=""/><div class="controls bullet"><span class="by">marcosdumay</span><span>|</span><a href="#38815996">parent</a><span>|</span><a href="#38816409">prev</a><span>|</span><a href="#38816156">next</a><span>|</span><label class="collapse" for="c-38816668">[-]</label><label class="expand" for="c-38816668">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not intuitive, but for HPC is more about scalability than performance.<p>You won&#x27;t be able to use a supercomputer at all without scalability, and it&#x27;s the one topic that is specific to it. But, of course, those computers time is quite expensive so you&#x27;ll want to optimize for performance too. It&#x27;s just secondary.</div><br/></div></div><div id="38816156" class="c"><input type="checkbox" id="c-38816156" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#38815996">parent</a><span>|</span><a href="#38816668">prev</a><span>|</span><a href="#38816164">next</a><span>|</span><label class="collapse" for="c-38816156">[-]</label><label class="expand" for="c-38816156">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think I do HPC (I only will use up to, say, 8 nodes at a time), but the impression I get is that they are already working on quite hard problems at the high-level, so they need to lean on good libraries for the low-level stuff, otherwise it is just too much.</div><br/></div></div><div id="38816164" class="c"><input type="checkbox" id="c-38816164" checked=""/><div class="controls bullet"><span class="by">MichaelZuo</span><span>|</span><a href="#38815996">parent</a><span>|</span><a href="#38816156">prev</a><span>|</span><a href="#38816565">next</a><span>|</span><label class="collapse" for="c-38816164">[-]</label><label class="expand" for="c-38816164">[1 more]</label></div><br/><div class="children"><div class="content">Memory architecture and bandwidth are still very important, most of IBM&#x27;s latest performance gains for both mainframes and POWER are reliant on some novel innovations there.</div><br/></div></div><div id="38816565" class="c"><input type="checkbox" id="c-38816565" checked=""/><div class="controls bullet"><span class="by">crabbone</span><span>|</span><a href="#38815996">parent</a><span>|</span><a href="#38816164">prev</a><span>|</span><a href="#38817574">next</a><span>|</span><label class="collapse" for="c-38816565">[-]</label><label class="expand" for="c-38816565">[10 more]</label></div><br/><div class="children"><div class="content">You&#x27;d be surprised how actually backwards and primitive are the tools used in HPC.<p>Take for instance the so-called workload managers, of which the most popular ones are Slurm, PBS, UGE, LSF.  Only Slurm is really open-source, PBS has a community edition, the rest is proprietary stuff executed in the best traditions of enterprise software which locks you into using pathetically bad tools, ancient and backwards tech with crappy &#x2F; nonexistent documentation and inept tech support.<p>The interface between WLMs and the user who wants to use some resources is through submitting &quot;jobs&quot;.  These jobs can be interactive, but most often they are the so-called &quot;batch jobs&quot;.  A batch job is usually defined as... a Unix Shell script, where the comments are parsed to interpret those as instructions to the WLM.  In the world with dozens of configuration formats... they chose to do this: embed configuration into Shell comments.<p>Debugging job failures is a nightmare, mostly because WLM software has really poor quality of execution.  Pathetic error reporting.  Idiotic defaults.  Everything is so fragile it falls apart if you just as much as look at it in the wrong way.  Working with it reminds me the very early days of Linux, when sometimes things just won&#x27;t build, or would segfault right after you&#x27;ve tried running them, and there wasn&#x27;t much you could do beside spending days or weeks trying to debug it just to get some basic functionality going.<p>When I have to deal with it, I feel kind of like in a steam-punk movie.  Some stuff is really advanced, and then you find out that this advanced stuff is propped by some DIY retro nonsense you thought have died off decades ago.  The advanced stuff is usually more on the side of hardware, while software is not keeping up with it for the most part.</div><br/><div id="38817532" class="c"><input type="checkbox" id="c-38817532" checked=""/><div class="controls bullet"><span class="by">victotronics</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38816565">parent</a><span>|</span><a href="#38817791">next</a><span>|</span><label class="collapse" for="c-38817532">[-]</label><label class="expand" for="c-38817532">[1 more]</label></div><br/><div class="children"><div class="content">You do a lot of scare quotes. Do you have any suggestions on how things could be different? You need batch jobs because the scheduler has to wait for resources to be available. It&#x27;s kinda like Tetris in processor&#x2F;time space. (In fact, that&#x27;s my personal &quot;proof&quot; that workload scheduling is NP-complete: it&#x27;s isomorphic to Tetris.)<p>And what&#x27;s wrong with shell scripts? It&#x27;s a lingua franca, generally accepted across scientific disciplines, cluster vendors, workload managers, .... Considering the complexity of some setups (copy data to node-local file systems; run multiple programs, post-process results, ... ) I don&#x27;t see how you could set  up things other than in some scripting language. And then unix shell scripts are not the worst idea.<p>Debugging failures: yeah. Too many levels where something can go wrong, and it can be a pain to debug. Still, your average cluster processes a few million jobs in its lifetime. If more than a microscopic portion of that would fail, computing centers would need way more personnel than they have.</div><br/></div></div><div id="38817791" class="c"><input type="checkbox" id="c-38817791" checked=""/><div class="controls bullet"><span class="by">romanows</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38816565">parent</a><span>|</span><a href="#38817532">prev</a><span>|</span><a href="#38816918">next</a><span>|</span><label class="collapse" for="c-38817791">[-]</label><label class="expand" for="c-38817791">[1 more]</label></div><br/><div class="children"><div class="content">I really like using Slurm, the documentation is great (<a href="https:&#x2F;&#x2F;slurm.schedmd.com" rel="nofollow">https:&#x2F;&#x2F;slurm.schedmd.com</a>) and the model is pretty straightforward, at least for the mostly-single-node jobs I used it for.<p>You can launch a job(s) via command-line, config in Bash comments, REST APIs, linking to their library, and I think a few more ways.<p>I found it pretty easy to setup and admin.  Scaling in the cloud was way less developed when I used it, so I just hacked in a simple script that allowed scaling up and down based on the job queue size.<p>What do you like better and for what use-case?  Mine was for a group of researchers training models, and the feature <i>I</i> desired most was an approximately fair distribution of resources (cores, GPU hours, etc.).</div><br/></div></div><div id="38816918" class="c"><input type="checkbox" id="c-38816918" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38816565">parent</a><span>|</span><a href="#38817791">prev</a><span>|</span><a href="#38817404">next</a><span>|</span><label class="collapse" for="c-38816918">[-]</label><label class="expand" for="c-38816918">[2 more]</label></div><br/><div class="children"><div class="content">Having switched from LSF to slurm, I have to appreciate that the ecosystem is so bash-centric. Lots of re-use in the conversion. If I’d had to learn some kind of slurm-markup-language or slurmScript or find buttons in some SlurmWizard, it would have been a nightmare.</div><br/><div id="38817091" class="c"><input type="checkbox" id="c-38817091" checked=""/><div class="controls bullet"><span class="by">crabbone</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38816918">parent</a><span>|</span><a href="#38817404">next</a><span>|</span><label class="collapse" for="c-38817091">[-]</label><label class="expand" for="c-38817091">[1 more]</label></div><br/><div class="children"><div class="content">Oh LSF... I don&#x27;t know if you know this.  LSF is perhaps the only system alive today that I know of that uses literal patches as a means of software distribution.<p>Fist time I saw it, I had a flashback to the times when I worked for HP, and they were making some huge SAP knock-off, and that system was so labor-intensive to deploy that their QA process involved actual patches.  As in pre-release QA cycle involved installing the system, validating it (which could take a few weeks) and if it&#x27;s not considered DoD, then the developers are given the final list of things they need to fix and those fixes would have to be submitted as patches (sometimes, literal diffs that need to be applied to the deployed system with the patch tool).<p>This is, I guess, how the &quot;patch version component&quot; came to be in SemVer spec.  It&#x27;s kind of funny how lots of tools are using this component today for completely unrelated purposes... but yeah, LSF feels like the time is ticking there at a different pace :)</div><br/></div></div></div></div><div id="38817404" class="c"><input type="checkbox" id="c-38817404" checked=""/><div class="controls bullet"><span class="by">OPA100</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38816565">parent</a><span>|</span><a href="#38816918">prev</a><span>|</span><a href="#38816855">next</a><span>|</span><label class="collapse" for="c-38817404">[-]</label><label class="expand" for="c-38817404">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve dug deeply into LSF in the last few years and it&#x27;s like a car crash - you can&#x27;t look away. It feels like something that started in the early unix days but was developed into perhaps the late 90s, but in reality LSF was only started in the 90s (in academia). As far as I can tell development all but stopped when IBM acquired it some ten years ago.</div><br/></div></div><div id="38816855" class="c"><input type="checkbox" id="c-38816855" checked=""/><div class="controls bullet"><span class="by">StableAlkyne</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38816565">parent</a><span>|</span><a href="#38817404">prev</a><span>|</span><a href="#38817421">next</a><span>|</span><label class="collapse" for="c-38816855">[-]</label><label class="expand" for="c-38816855">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  Working with it reminds me the very early days of Linux<p>The other cool thing about HPC is it is one of the last areas where multi-user Unix is used! At least, if you&#x27;re using a university or NSF cluster that is!<p>Only other place I really see multiple humans using the same machine is SDF or the Tildes</div><br/><div id="38817585" class="c"><input type="checkbox" id="c-38817585" checked=""/><div class="controls bullet"><span class="by">victotronics</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38816855">parent</a><span>|</span><a href="#38817421">next</a><span>|</span><label class="collapse" for="c-38817585">[-]</label><label class="expand" for="c-38817585">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s saturday afternoon.<p><pre><code>  [login1 ~:3] who | cut -d &#x27; &#x27; -f 1 | sort -u | wc -l
  41</code></pre></div><br/></div></div></div></div><div id="38817421" class="c"><input type="checkbox" id="c-38817421" checked=""/><div class="controls bullet"><span class="by">convolvatron</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38816565">parent</a><span>|</span><a href="#38816855">prev</a><span>|</span><a href="#38817574">next</a><span>|</span><label class="collapse" for="c-38817421">[-]</label><label class="expand" for="c-38817421">[2 more]</label></div><br/><div class="children"><div class="content">HPC software is one area where we have arguably regressed in the last 30 years. Chapel is the only light I see in the darkness</div><br/><div id="38821053" class="c"><input type="checkbox" id="c-38821053" checked=""/><div class="controls bullet"><span class="by">trentnelson</span><span>|</span><a href="#38815996">root</a><span>|</span><a href="#38817421">parent</a><span>|</span><a href="#38817574">next</a><span>|</span><label class="collapse" for="c-38821053">[-]</label><label class="expand" for="c-38821053">[1 more]</label></div><br/><div class="children"><div class="content">Want to elaborate more on Chapel?  I’ve recently being tasked with integrating Chapel into our system and it’s quite interesting.</div><br/></div></div></div></div></div></div></div></div><div id="38817574" class="c"><input type="checkbox" id="c-38817574" checked=""/><div class="controls bullet"><span class="by">toddm</span><span>|</span><a href="#38815996">prev</a><span>|</span><a href="#38819261">next</a><span>|</span><label class="collapse" for="c-38817574">[-]</label><label class="expand" for="c-38817574">[3 more]</label></div><br/><div class="children"><div class="content">Kudos to Victor for assembling such a wonderful resource!<p>While I am not acquainted with him personally, I did my doctoral work at UT Austin the the 1990&#x27;s and had the privilege of working with the resources (Cray Y-MP, IBM SP&#x2F;2 Winterhawk, and mostly on Lonestar, a host name which pointed to a Cray T3E at the time) maintained by TACC (one of my Ph.D. committee members is still on staff!) to complete my work (TACC was called HPCC and&#x2F;or CHPC if I recall the acronyms correctly).<p>Back then, it was incumbent on the programmer to parallelize their code (in my case, using MPI on the Cray T3E in the UNICOS environment) and have some understanding of the hardware, if only because the field was still emergent and problems were solved by reading the gray Cray ring-binder and whichever copies of Gropp et al. we had on-hand. That and having a very knowledgeable contact as mentioned above :) of course helped...</div><br/><div id="38817638" class="c"><input type="checkbox" id="c-38817638" checked=""/><div class="controls bullet"><span class="by">victotronics</span><span>|</span><a href="#38817574">parent</a><span>|</span><a href="#38821291">next</a><span>|</span><label class="collapse" for="c-38817638">[-]</label><label class="expand" for="c-38817638">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Lonestar, a host name which pointed to a Cray T3E<p>Lonestar5 was a Cray again. Currently Lonestar6 is an oil-immersion AMD Milan cluster with A100 GPUs. The times, they never stand still.</div><br/></div></div><div id="38821291" class="c"><input type="checkbox" id="c-38821291" checked=""/><div class="controls bullet"><span class="by">huitzitziltzin</span><span>|</span><a href="#38817574">parent</a><span>|</span><a href="#38817638">prev</a><span>|</span><a href="#38819261">next</a><span>|</span><label class="collapse" for="c-38821291">[-]</label><label class="expand" for="c-38821291">[1 more]</label></div><br/><div class="children"><div class="content">Dealt with him via TACC for a big simulation I did and was grateful enough for his help to buy a paper copy of the first volume in the series.  Very interesting though a bit outside of my area.  I will look at the others and encourage anyone interested to check them out.</div><br/></div></div></div></div><div id="38819261" class="c"><input type="checkbox" id="c-38819261" checked=""/><div class="controls bullet"><span class="by">jebarker</span><span>|</span><a href="#38817574">prev</a><span>|</span><a href="#38815933">next</a><span>|</span><label class="collapse" for="c-38819261">[-]</label><label class="expand" for="c-38819261">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m interested in what people think of the approach to teaching C++ used here. Any particular drawbacks?<p>I&#x27;m a very experienced Python programmer with some C, C++ and CUDA doing application level research in HPC environments (ML&#x2F;DL). I&#x27;d really like to level up my C++ skills and looking through book 3 it seems aimed exactly at the right level for me - doesn&#x27;t move too slowly and teaches best practices (per the author) rather than trying to be comprehensive.</div><br/><div id="38820438" class="c"><input type="checkbox" id="c-38820438" checked=""/><div class="controls bullet"><span class="by">leopoldj</span><span>|</span><a href="#38819261">parent</a><span>|</span><a href="#38815933">next</a><span>|</span><label class="collapse" for="c-38820438">[-]</label><label class="expand" for="c-38820438">[3 more]</label></div><br/><div class="children"><div class="content">C++ programmer and educator here. This (volume 3) is well organized good beginner level teaching material. You probably know most of it already.<p>I was looking for range-based for loop, std::array and std::span and happy to see that they are all there.<p>Because this book relates to HPC, I&#x27;d add a few things: Return Value Optimization,  move semantics, and in the recursive function section a note about Tail Call Optimization.<p>As a beginner level material I can highly recommend it.</div><br/><div id="38820940" class="c"><input type="checkbox" id="c-38820940" checked=""/><div class="controls bullet"><span class="by">jebarker</span><span>|</span><a href="#38819261">root</a><span>|</span><a href="#38820438">parent</a><span>|</span><a href="#38815933">next</a><span>|</span><label class="collapse" for="c-38820940">[-]</label><label class="expand" for="c-38820940">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s great - thank-you. Assuming I work through this quickly, what resources would you recommend as a follow-on?</div><br/><div id="38821410" class="c"><input type="checkbox" id="c-38821410" checked=""/><div class="controls bullet"><span class="by">rramadass</span><span>|</span><a href="#38819261">root</a><span>|</span><a href="#38820940">parent</a><span>|</span><a href="#38815933">next</a><span>|</span><label class="collapse" for="c-38821410">[-]</label><label class="expand" for="c-38821410">[1 more]</label></div><br/><div class="children"><div class="content">I am not the person you asked the question to, but my recommendation would be;<p>1) <i>Discovering Modern C++: An Intensive Course for Scientists, Engineers, and Programmers</i> by Peter Gottschling - Not too thick and focuses on how to program in the language.<p>2) <i>Software Architecture with C++: Design modern systems using effective architecture concepts, design patterns, and techniques with C++20</i> by Adrian Ostrowski et al. - Shows how to use C++ in the modern way&#x2F;ecosystems i.e. with CI&#x2F;CD, Microservices etc.<p>Optional but highly recommended;<p>a) <i>Scientific and Engineering C++: An Introduction with Advanced Techniques and Examples</i> by Barton &amp; Nackman - Old pre-Modern C++ book which pioneered many of the techniques which have now become common. One of the best for learning C++ design.</div><br/></div></div></div></div></div></div></div></div><div id="38815933" class="c"><input type="checkbox" id="c-38815933" checked=""/><div class="controls bullet"><span class="by">atrettel</span><span>|</span><a href="#38819261">prev</a><span>|</span><a href="#38816676">next</a><span>|</span><label class="collapse" for="c-38815933">[-]</label><label class="expand" for="c-38815933">[1 more]</label></div><br/><div class="children"><div class="content">I took a course on scientific computing in 2013.  It was cross-listed under both the computer science and applied math departments.  The issue is that the field is pretty broad overall and a lot of topics were covered in a cursory manner, including anything related to HPC and parallel programming in particular.  I don&#x27;t regret taking the course, but it was too broad for the applications I was pursuing.<p>I haven&#x27;t looked at what courses are being offered in several years, but when I was a graduate student, I really would have benefited from a dedicated semester-long course on parallel computing, especially going into the weeds about particular algorithms and data structures in parallel and distributed computing.  Those were handled in a super cursory manner in the scientific computing course I took, as if somehow you&#x27;d know precisely how to parallelize things the first time you try.  I&#x27;ve since learned a lot of this stuff on my own and from colleagues over the years, as many people do in HPC, but books like these would have been invaluable as part of a dedicated semester-long course.</div><br/></div></div><div id="38816676" class="c"><input type="checkbox" id="c-38816676" checked=""/><div class="controls bullet"><span class="by">rlupi</span><span>|</span><a href="#38815933">prev</a><span>|</span><a href="#38816514">next</a><span>|</span><label class="collapse" for="c-38816676">[-]</label><label class="expand" for="c-38816676">[10 more]</label></div><br/><div class="children"><div class="content">I am interested in the more hardware management side of HPC (how problems are detected, diagnosed, mapped into actions such as reboot&#x2F;reinstall&#x2F;repairs, how these are scheduled and how that is optimized to provide the best level of service, how this is done if there are multiple objectives to optimize at once e.g. node availability vs overall throughput, how different topologies affect the above, how other constraints affect the above, and in general a system dynamics approach to these problems).<p>I haven&#x27;t found many good sources for this kind of information. If you are aware of any, please cite them in a comment below.</div><br/><div id="38822002" class="c"><input type="checkbox" id="c-38822002" checked=""/><div class="controls bullet"><span class="by">mackid</span><span>|</span><a href="#38816676">parent</a><span>|</span><a href="#38817825">next</a><span>|</span><label class="collapse" for="c-38822002">[-]</label><label class="expand" for="c-38822002">[1 more]</label></div><br/><div class="children"><div class="content">Mark Russinovich gives a good talk most years on the internals of Azure and the systems that run it.  [1] is an example.  Look for talks from other years as well.<p>Meta also publishes a number of papers&#x2F;blogs&#x2F;OSS projects on their engineering site [2]<p>James Hamilton of AWS gives a talk most years on their infrastructure.  Worth watching multiple years [3].<p>[1] <a href="https:&#x2F;&#x2F;youtu.be&#x2F;69PrhWQorEM?si=u7vh_Um6SQNoyeFH" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;69PrhWQorEM?si=u7vh_Um6SQNoyeFH</a><p>[2] <a href="https:&#x2F;&#x2F;engineering.fb.com&#x2F;category&#x2F;data-center-engineering&#x2F;" rel="nofollow">https:&#x2F;&#x2F;engineering.fb.com&#x2F;category&#x2F;data-center-engineering&#x2F;</a><p>[3] <a href="https:&#x2F;&#x2F;youtu.be&#x2F;AyOAjFNPAbA?si=nFRJVcQI4EiamC-O" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;AyOAjFNPAbA?si=nFRJVcQI4EiamC-O</a></div><br/></div></div><div id="38817825" class="c"><input type="checkbox" id="c-38817825" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#38816676">parent</a><span>|</span><a href="#38822002">prev</a><span>|</span><a href="#38817010">next</a><span>|</span><label class="collapse" for="c-38817825">[-]</label><label class="expand" for="c-38817825">[1 more]</label></div><br/><div class="children"><div class="content">Assuming you are moving past just the typical nonblocking folded-Clos networksor Little&#x27;s Law; and want to have a more engineering focus, &quot;Queuing theory&quot; is one discipline you want to dig into.<p>Queuing theory seems trivial and easy how it is introduced, but it has many open questions.<p>Performance metrics for a system with random arrival times, independent service times, with k servers (M&#x2F;G&#x2F;k) is still an open question as an example.<p><a href="https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S0895717704905341" rel="nofollow">https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S089571770...</a><p>There are actually lots of open problems in queuing theory that one wouldn&#x27;t expect.</div><br/></div></div><div id="38817010" class="c"><input type="checkbox" id="c-38817010" checked=""/><div class="controls bullet"><span class="by">CoastalCoder</span><span>|</span><a href="#38816676">parent</a><span>|</span><a href="#38817825">prev</a><span>|</span><a href="#38818674">next</a><span>|</span><label class="collapse" for="c-38817010">[-]</label><label class="expand" for="c-38817010">[4 more]</label></div><br/><div class="children"><div class="content">This seemed like a big topic when I was interviewing with Meta and nVidia some months ago.<p>Meta had a few good YouTube videos about the problems of dealing with this many GPUs at scale.</div><br/><div id="38817885" class="c"><input type="checkbox" id="c-38817885" checked=""/><div class="controls bullet"><span class="by">keefle</span><span>|</span><a href="#38816676">root</a><span>|</span><a href="#38817010">parent</a><span>|</span><a href="#38818674">next</a><span>|</span><label class="collapse" for="c-38817885">[-]</label><label class="expand" for="c-38817885">[3 more]</label></div><br/><div class="children"><div class="content">Could you link me the YouTube videos&#x2F;articles in question? It happens to be my research area and I&#x27;m interested in knowing how big companies such as meta deal with multi-GPU systems</div><br/><div id="38818106" class="c"><input type="checkbox" id="c-38818106" checked=""/><div class="controls bullet"><span class="by">CoastalCoder</span><span>|</span><a href="#38816676">root</a><span>|</span><a href="#38817885">parent</a><span>|</span><a href="#38818674">next</a><span>|</span><label class="collapse" for="c-38818106">[-]</label><label class="expand" for="c-38818106">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have them bookmarked anymore, but they may have been from this playlist: [0]<p>[0] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;playlist?list=PLBnLThDtSXOw_kePWy3CSG7LLf3KdGqQz" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;playlist?list=PLBnLThDtSXOw_kePWy3CS...</a></div><br/><div id="38822612" class="c"><input type="checkbox" id="c-38822612" checked=""/><div class="controls bullet"><span class="by">keefle</span><span>|</span><a href="#38816676">root</a><span>|</span><a href="#38818106">parent</a><span>|</span><a href="#38818674">next</a><span>|</span><label class="collapse" for="c-38822612">[-]</label><label class="expand" for="c-38822612">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for sharing! I&#x27;ll hunt it down</div><br/></div></div></div></div></div></div></div></div><div id="38818674" class="c"><input type="checkbox" id="c-38818674" checked=""/><div class="controls bullet"><span class="by">cavisne</span><span>|</span><a href="#38816676">parent</a><span>|</span><a href="#38817010">prev</a><span>|</span><a href="#38816927">next</a><span>|</span><label class="collapse" for="c-38818674">[-]</label><label class="expand" for="c-38818674">[1 more]</label></div><br/><div class="children"><div class="content">This paper from Microsoft [1] is the coolest thing I&#x27;ve seen in this space. Basically workload (deep learning in this case) level optimization to allow jobs to be resized and preempted.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2202.07848.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2202.07848.pdf</a></div><br/></div></div><div id="38816927" class="c"><input type="checkbox" id="c-38816927" checked=""/><div class="controls bullet"><span class="by">synergy20</span><span>|</span><a href="#38816676">parent</a><span>|</span><a href="#38818674">prev</a><span>|</span><a href="#38816514">next</a><span>|</span><label class="collapse" for="c-38816927">[-]</label><label class="expand" for="c-38816927">[2 more]</label></div><br/><div class="children"><div class="content">check out openbmc project and DTMF association</div><br/><div id="38818109" class="c"><input type="checkbox" id="c-38818109" checked=""/><div class="controls bullet"><span class="by">timoteostewart</span><span>|</span><a href="#38816676">root</a><span>|</span><a href="#38816927">parent</a><span>|</span><a href="#38816514">next</a><span>|</span><label class="collapse" for="c-38818109">[-]</label><label class="expand" for="c-38818109">[1 more]</label></div><br/><div class="children"><div class="content">DMTF (not DTMF)<p><a href="https:&#x2F;&#x2F;www.dmtf.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.dmtf.org&#x2F;</a></div><br/></div></div></div></div></div></div><div id="38816514" class="c"><input type="checkbox" id="c-38816514" checked=""/><div class="controls bullet"><span class="by">rramadass</span><span>|</span><a href="#38816676">prev</a><span>|</span><a href="#38815780">next</a><span>|</span><label class="collapse" for="c-38816514">[-]</label><label class="expand" for="c-38816514">[1 more]</label></div><br/><div class="children"><div class="content">Just amazed at how the author has created (and shared for free) such a comprehensive set of books including teaching C++ and Unix tools! There is something to learn for all Programmers (HPC specific or not) here.<p>Related: Jorg Arndt&#x27;s &quot;Matters Computational&quot; book and FXT library - <a href="https:&#x2F;&#x2F;www.jjj.de&#x2F;fxt&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.jjj.de&#x2F;fxt&#x2F;</a></div><br/></div></div><div id="38815780" class="c"><input type="checkbox" id="c-38815780" checked=""/><div class="controls bullet"><span class="by">davidthewatson</span><span>|</span><a href="#38816514">prev</a><span>|</span><a href="#38815684">next</a><span>|</span><label class="collapse" for="c-38815780">[-]</label><label class="expand" for="c-38815780">[1 more]</label></div><br/><div class="children"><div class="content">I was asked to share a TA role on a graduate course in HPC a decade ago. I turned down the offer.<p>After a cursory glance, I can honestly say that if this book were available then, I&#x27;d have taken the opportunity.<p>The combination of what I perceive to be Knuth&#x27;s framing of art, along with carpentry and the need to be a better devops person than your devops person is compelling.<p>Kudos to the author for such an achievement. UT Austin seems to have achieved in computer science what North Texas State did in music.</div><br/></div></div><div id="38815684" class="c"><input type="checkbox" id="c-38815684" checked=""/><div class="controls bullet"><span class="by">mkoubaa</span><span>|</span><a href="#38815780">prev</a><span>|</span><a href="#38816007">next</a><span>|</span><label class="collapse" for="c-38815684">[-]</label><label class="expand" for="c-38815684">[8 more]</label></div><br/><div class="children"><div class="content">UT Austin really is a fantastic institution for HPC and computational methods.</div><br/><div id="38816198" class="c"><input type="checkbox" id="c-38816198" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#38815684">parent</a><span>|</span><a href="#38816007">next</a><span>|</span><label class="collapse" for="c-38816198">[-]</label><label class="expand" for="c-38816198">[7 more]</label></div><br/><div class="children"><div class="content">Every BLAS you want to use has at least some connection to UT Austin’s TACC.</div><br/><div id="38817930" class="c"><input type="checkbox" id="c-38817930" checked=""/><div class="controls bullet"><span class="by">victotronics</span><span>|</span><a href="#38815684">root</a><span>|</span><a href="#38816198">parent</a><span>|</span><a href="#38816385">next</a><span>|</span><label class="collapse" for="c-38817930">[-]</label><label class="expand" for="c-38817930">[3 more]</label></div><br/><div class="children"><div class="content">Not quite. Every modern BLAS is (likely) based on Kazushige Goto&#x27;s implementation, and he was indeed at TACC for a while. But probably the best open source implementation &quot;BLIS&quot; is from UT Austin, but not connected to TACC.</div><br/><div id="38818021" class="c"><input type="checkbox" id="c-38818021" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#38815684">root</a><span>|</span><a href="#38817930">parent</a><span>|</span><a href="#38816385">next</a><span>|</span><label class="collapse" for="c-38818021">[-]</label><label class="expand" for="c-38818021">[2 more]</label></div><br/><div class="children"><div class="content">Oh really? I thought BLIS was from TACC. Oops, mea culpa.</div><br/><div id="38818261" class="c"><input type="checkbox" id="c-38818261" checked=""/><div class="controls bullet"><span class="by">RhysU</span><span>|</span><a href="#38815684">root</a><span>|</span><a href="#38818021">parent</a><span>|</span><a href="#38816385">next</a><span>|</span><label class="collapse" for="c-38818261">[-]</label><label class="expand" for="c-38818261">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;flame&#x2F;blis&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;flame&#x2F;blis&#x2F;</a><p>Field et al, recent winners of the James H. Wilkinson Prize for Numerical Software.<p>Field and Goto both collaborated with Robert van de Geijn. Lots of TACC interaction in that broader team.</div><br/></div></div></div></div></div></div><div id="38816385" class="c"><input type="checkbox" id="c-38816385" checked=""/><div class="controls bullet"><span class="by">mgaunard</span><span>|</span><a href="#38815684">root</a><span>|</span><a href="#38816198">parent</a><span>|</span><a href="#38817930">prev</a><span>|</span><a href="#38816007">next</a><span>|</span><label class="collapse" for="c-38816385">[-]</label><label class="expand" for="c-38816385">[3 more]</label></div><br/><div class="children"><div class="content">aren&#x27;t the lapack people in tennessee?</div><br/><div id="38816610" class="c"><input type="checkbox" id="c-38816610" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#38815684">root</a><span>|</span><a href="#38816385">parent</a><span>|</span><a href="#38816007">next</a><span>|</span><label class="collapse" for="c-38816610">[-]</label><label class="expand" for="c-38816610">[2 more]</label></div><br/><div class="children"><div class="content">Sort of like BLAS, LAPACK is more than just one implementation. Dongarra described what everybody should do from Tennesse, but other places implemented it elsewhere.</div><br/><div id="38820255" class="c"><input type="checkbox" id="c-38820255" checked=""/><div class="controls bullet"><span class="by">mgaunard</span><span>|</span><a href="#38815684">root</a><span>|</span><a href="#38816610">parent</a><span>|</span><a href="#38816007">next</a><span>|</span><label class="collapse" for="c-38820255">[-]</label><label class="expand" for="c-38820255">[1 more]</label></div><br/><div class="children"><div class="content">plasma and magma are also from there.<p>I&#x27;m not aware of any other significant lapack-related developments, but I might just not know about them.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38816007" class="c"><input type="checkbox" id="c-38816007" checked=""/><div class="controls bullet"><span class="by">teleforce</span><span>|</span><a href="#38815684">prev</a><span>|</span><a href="#38817467">next</a><span>|</span><label class="collapse" for="c-38816007">[-]</label><label class="expand" for="c-38816007">[3 more]</label></div><br/><div class="children"><div class="content">Is there something wrong with the GitHub files since I cannot render any of the textbooks PDF files?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;VictorEijkhout&#x2F;TheArtofHPC_pdfs&#x2F;blob&#x2F;main&#x2F;vol1&#x2F;EijkhoutIntroToHPC.pdf">https:&#x2F;&#x2F;github.com&#x2F;VictorEijkhout&#x2F;TheArtofHPC_pdfs&#x2F;blob&#x2F;main...</a></div><br/><div id="38816106" class="c"><input type="checkbox" id="c-38816106" checked=""/><div class="controls bullet"><span class="by">npalli</span><span>|</span><a href="#38816007">parent</a><span>|</span><a href="#38817467">next</a><span>|</span><label class="collapse" for="c-38816106">[-]</label><label class="expand" for="c-38816106">[2 more]</label></div><br/><div class="children"><div class="content">I think the files are too large to render in the github browser and they give an error. You can pick the &#x27;download raw&#x27; option to download locally and read the file. Worked for me.</div><br/><div id="38817724" class="c"><input type="checkbox" id="c-38817724" checked=""/><div class="controls bullet"><span class="by">TimMeade</span><span>|</span><a href="#38816007">root</a><span>|</span><a href="#38816106">parent</a><span>|</span><a href="#38817467">next</a><span>|</span><label class="collapse" for="c-38817724">[-]</label><label class="expand" for="c-38817724">[1 more]</label></div><br/><div class="children"><div class="content">I just &quot;git clone <a href="https:&#x2F;&#x2F;github.com&#x2F;VictorEijkhout&#x2F;TheArtofHPC_pdfs.git">https:&#x2F;&#x2F;github.com&#x2F;VictorEijkhout&#x2F;TheArtofHPC_pdfs.git</a>&quot;  on my local drive.   Had it all in under a minute.</div><br/></div></div></div></div></div></div><div id="38817467" class="c"><input type="checkbox" id="c-38817467" checked=""/><div class="controls bullet"><span class="by">justin66</span><span>|</span><a href="#38816007">prev</a><span>|</span><a href="#38815907">next</a><span>|</span><label class="collapse" for="c-38817467">[-]</label><label class="expand" for="c-38817467">[1 more]</label></div><br/><div class="children"><div class="content">There is some really good content here for any programmer.<p>And with volume 3, such a contrast: the author teaches C++17 and... Fortran2008.</div><br/></div></div></div></div></div></div></div></body></html>