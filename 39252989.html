<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1707123658250" as="style"/><link rel="stylesheet" href="styles.css?v=1707123658250"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.lesswrong.com/posts/eDicGjD9yte6FLSie/interpreting-neural-networks-through-the-polytope-lens">Interpreting neural networks through the polytope lens (2022)</a> <span class="domain">(<a href="https://www.lesswrong.com">www.lesswrong.com</a>)</span></div><div class="subtext"><span>sva_</span> | <span>7 comments</span></div><br/><div><div id="39257536" class="c"><input type="checkbox" id="c-39257536" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#39256132">next</a><span>|</span><label class="collapse" for="c-39257536">[-]</label><label class="expand" for="c-39257536">[1 more]</label></div><br/><div class="children"><div class="content">Very interesting read and a rather &quot;obvious&quot; one. I can&#x27;t believe I didn&#x27;t see this before. Obviously... A perceptron layer is a bunch of dot products followed by comparison. Every graphics programmer knows this is a check of which side of a plane you&#x27;re on.<p>Of course, the relu unit is also a
passing on information when the result is on one side of the plane, making this a spline.<p>As others have said... Can we learn the separating planes without the backward gradient propagation? I don&#x27;t know but seeing it in this new way may help.</div><br/></div></div><div id="39256132" class="c"><input type="checkbox" id="c-39256132" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#39257536">prev</a><span>|</span><a href="#39255465">next</a><span>|</span><label class="collapse" for="c-39256132">[-]</label><label class="expand" for="c-39256132">[2 more]</label></div><br/><div class="children"><div class="content">I wonder if there&#x27;s a way to learn polytopes directly with a non-MLP formulation.</div><br/><div id="39256462" class="c"><input type="checkbox" id="c-39256462" checked=""/><div class="controls bullet"><span class="by">jtanderson</span><span>|</span><a href="#39256132">parent</a><span>|</span><a href="#39255465">next</a><span>|</span><label class="collapse" for="c-39256462">[-]</label><label class="expand" for="c-39256462">[1 more]</label></div><br/><div class="children"><div class="content">There are many interesting efforts — going back quite a few years —- to this goal, many of which in the PAC setting (which automatically means MLP is out, for theoretical guarantees). E.g [0]and its related references come to mind as an interesting place to look into it!<p>[0]: <a href="https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper&#x2F;2018&#x2F;file&#x2F;22b1f2e0983160db6f7bb9f62f4dbb39-Paper.pdf" rel="nofollow">https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper&#x2F;2018&#x2F;file&#x2F;22b1f2e098316...</a> pro<p>(Edited for some clarity)</div><br/></div></div></div></div><div id="39255465" class="c"><input type="checkbox" id="c-39255465" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#39256132">prev</a><span>|</span><a href="#39253709">next</a><span>|</span><label class="collapse" for="c-39255465">[-]</label><label class="expand" for="c-39255465">[1 more]</label></div><br/><div class="children"><div class="content">I’m very much exploring this idea. Hopf algebras provide a really nice wrapper around this. I have a discord to discuss this<p><a href="https:&#x2F;&#x2F;discord.cofunctional.ai" rel="nofollow">https:&#x2F;&#x2F;discord.cofunctional.ai</a></div><br/></div></div><div id="39253709" class="c"><input type="checkbox" id="c-39253709" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#39255465">prev</a><span>|</span><label class="collapse" for="c-39253709">[-]</label><label class="expand" for="c-39253709">[2 more]</label></div><br/><div class="children"><div class="content">I wonder if these two things from the HN post below this one:<p><a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;d41586-024-00288-1" rel="nofollow">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;d41586-024-00288-1</a><p>I would love to see a cross section of these two ideas....<p>I have been surprised that in the past few weeks, I have seen several posts on HN where, while separate, unrelated posts here - there have been related characteristics and if you look at them for a sec, you can see how having AIs GPT both studies&#x2F;papers - immediate connections worth looking at further are revealed.<p>If even for the sake of just a more informed tapestry of knowledge in a particular area...<p>Its really enjoyable reading and TIL&#x27;ing.</div><br/><div id="39255407" class="c"><input type="checkbox" id="c-39255407" checked=""/><div class="controls bullet"><span class="by">data_maan</span><span>|</span><a href="#39253709">parent</a><span>|</span><label class="collapse" for="c-39255407">[-]</label><label class="expand" for="c-39255407">[1 more]</label></div><br/><div class="children"><div class="content">These ideas are too far apart...</div><br/></div></div></div></div></div></div></div></div></div></body></html>