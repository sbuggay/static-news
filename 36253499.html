<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1686474046306" as="style"/><link rel="stylesheet" href="styles.css?v=1686474046306"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.anandtech.com/show/18901/big-leap-for-hdds-32-tb-hamr-drive-is-coming-40tb-on-horizon">Big leap for hard drive capacities: 32 TB HAMR drives due soon, 40tb on horizon</a> <span class="domain">(<a href="https://www.anandtech.com">www.anandtech.com</a>)</span></div><div class="subtext"><span>walterbell</span> | <span>95 comments</span></div><br/><div><div id="36253899" class="c"><input type="checkbox" id="c-36253899" checked=""/><div class="controls bullet"><span class="by">techdragon</span><span>|</span><a href="#36276910">next</a><span>|</span><label class="collapse" for="c-36253899">[-]</label><label class="expand" for="c-36253899">[19 more]</label></div><br/><div class="children"><div class="content">This is cool, but I really need to hear more about how I can effectively use these kinds of drives in storage pools and backup applications without issue.<p>I didn’t have time to learn back when the first disaster hit with SMR drives got snuck in without warning and we had a small outrage event.<p>I’m happy for new drive tech but I’ve not seen much about the pros and cons of HAMR in real world use yet.  Which could be because there’s no difference from traditional Perpendicular Magnetic Recording (PMR) hard drives, but then again it could have its own subtle set of trade offs entirely different to the ones that Shingled Magnetic Recording (SMR) drives do.</div><br/><div id="36254587" class="c"><input type="checkbox" id="c-36254587" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36253899">parent</a><span>|</span><a href="#36276832">next</a><span>|</span><label class="collapse" for="c-36254587">[-]</label><label class="expand" for="c-36254587">[13 more]</label></div><br/><div class="children"><div class="content">From what I’ve heard, these are primarily sold to the hyperscalers like the public clouds, Drop Box, etc…<p>At their scale it’s easy to utilise these efficiently despite the limitations.<p>They’re not very good in a small NAS or disk array. You’d need a big write cache in front of them and an OS that natively understands the sector groupings.</div><br/><div id="36279461" class="c"><input type="checkbox" id="c-36279461" checked=""/><div class="controls bullet"><span class="by">crabbone</span><span>|</span><a href="#36253899">root</a><span>|</span><a href="#36254587">parent</a><span>|</span><a href="#36256749">next</a><span>|</span><label class="collapse" for="c-36279461">[-]</label><label class="expand" for="c-36279461">[1 more]</label></div><br/><div class="children"><div class="content">Big disks present a problem in the cloud provider environment: users typically want to partition them into smaller logical disks... but this means that larger disks must also have lower latency &#x2F; higher bandwidth to service multiple users at the same time.<p>Most likely, cloud vendors have a large choice of sizes to try to match requested sizes of logical disks to physical disks, but if most users don&#x27;t want &#x2F; need disks that are this big (or bigger), then the provider will have a problem.</div><br/></div></div><div id="36256749" class="c"><input type="checkbox" id="c-36256749" checked=""/><div class="controls bullet"><span class="by">techdragon</span><span>|</span><a href="#36253899">root</a><span>|</span><a href="#36254587">parent</a><span>|</span><a href="#36279461">prev</a><span>|</span><a href="#36276832">next</a><span>|</span><label class="collapse" for="c-36256749">[-]</label><label class="expand" for="c-36256749">[11 more]</label></div><br/><div class="children"><div class="content">This is why I’m looking for more info. This is talking about how they are already sending these out to Hyperscale customers, and they will eventually be in individual customers hands… so it would be good to know what file systems and cache setups are needed in order to benefit from this.<p>I used to just rely on FreeNAS but it’s not as straightforward anymore. I’m having to consider Linux and looking at bCache FS and ZFS vs BTRFS and how all this compares on an all PCIe (m.2) flash drive setup… where a drive (or two) at this sort of size (~50TB) would make a great second backup copy of the flash array that can be started and stopped to periodically make the backups to save power… but then you have to think about copy efficiency since I don’t want to have them wasting read bandwidth from the flash drives… and the best is usually like to like file system copy ZFS -&gt; ZFS and BTRFS -&gt; BTRFS … so it adds another complication into a mix that is already far from simple.<p>So it’s become something I’m keeping an eye out for… hopefully before it becomes something I may purchase, someone will have already done a good writeup.</div><br/><div id="36278762" class="c"><input type="checkbox" id="c-36278762" checked=""/><div class="controls bullet"><span class="by">klodolph</span><span>|</span><a href="#36253899">root</a><span>|</span><a href="#36256749">parent</a><span>|</span><a href="#36277027">next</a><span>|</span><label class="collapse" for="c-36278762">[-]</label><label class="expand" for="c-36278762">[1 more]</label></div><br/><div class="children"><div class="content">&gt; … so it would be good to know what file systems and cache setups are needed in order to benefit from this.<p>I’ve worked on these systems. You might as well be asking F1 drivers for tips to help you commute to work.<p>The hyperscale stuff is not built on top of ordinary filesystems. It’s all clusters of machines, and error correction is handled at the level of clusters. If you’re evaluating systems like ZFS and BTRFS, then you’re already working with a radically different tech stack.<p>At scale, your file metadata is stored in a distributed database of some kind, and the file contents are stored with forward error correction across multiple machines in a cluster. Or something similar, but stored across multiple clusters. The pricing models for cloud storage are designed around the usage patterns—like, if you know that some data is going to stick around for 90 days, then SMR is a win. If a file could get deleted at any time, then SMR is a loss.</div><br/></div></div><div id="36277027" class="c"><input type="checkbox" id="c-36277027" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#36253899">root</a><span>|</span><a href="#36256749">parent</a><span>|</span><a href="#36278762">prev</a><span>|</span><a href="#36276832">next</a><span>|</span><label class="collapse" for="c-36277027">[-]</label><label class="expand" for="c-36277027">[9 more]</label></div><br/><div class="children"><div class="content">The simple answer is that the software stack necessary for consumers to effectively use these storage devices does not yet exist. Hyperscalers are pretty much defined as the ones who are big enough to do their own software stack. For the rest of us, we have some good components to work with but also some major gaps that won&#x27;t be filled anytime soon.<p>ZFS is a great improvement over traditional hardware RAID systems, but is still in many ways clearly a descendant of them. BTRFS has a slightly different mix of features from ZFS that make it a bit more flexible and a better choice for consumers who don&#x27;t buy drives by the dozen. Neither has a great solution for caching&#x2F;tiering with SSDs and hard drives. Ceph has a lot of features that would otherwise be almost exclusive to the hyperscalers, but is too complicated for something like a turnkey NAS. bcachefs aspires to <i>eventually</i> have most of the features you would want for a non-clustered storage system.<p>Zoned storage is something many of the above already have some degree of support for, but as a paradigm it has not even started showing up in the consumer computing ecosystem so many of the issues with adopting zoned storage for consumer systems aren&#x27;t even being worked on.</div><br/><div id="36277647" class="c"><input type="checkbox" id="c-36277647" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#36253899">root</a><span>|</span><a href="#36277027">parent</a><span>|</span><a href="#36277223">next</a><span>|</span><label class="collapse" for="c-36277647">[-]</label><label class="expand" for="c-36277647">[5 more]</label></div><br/><div class="children"><div class="content">Y&#x27;all keep saying &quot;these&quot; but SMR and HAMR are unrelated.</div><br/><div id="36277785" class="c"><input type="checkbox" id="c-36277785" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#36253899">root</a><span>|</span><a href="#36277647">parent</a><span>|</span><a href="#36277223">next</a><span>|</span><label class="collapse" for="c-36277785">[-]</label><label class="expand" for="c-36277785">[4 more]</label></div><br/><div class="children"><div class="content">In theory, yes, but the bottom of the article explains that Seagate currently plans for their upcoming 24TB drive to be the last new non-SMR drive and all larger drives (28+ TB) are planned to be SMR. So in practice SMR and HAMR will be going hand in hand, at least from this vendor.</div><br/><div id="36277810" class="c"><input type="checkbox" id="c-36277810" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#36253899">root</a><span>|</span><a href="#36277785">parent</a><span>|</span><a href="#36277223">next</a><span>|</span><label class="collapse" for="c-36277810">[-]</label><label class="expand" for="c-36277810">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not my interpretation but we&#x27;ll see.</div><br/><div id="36278122" class="c"><input type="checkbox" id="c-36278122" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#36253899">root</a><span>|</span><a href="#36277810">parent</a><span>|</span><a href="#36279120">next</a><span>|</span><label class="collapse" for="c-36278122">[-]</label><label class="expand" for="c-36278122">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;So, we have a 24TB coming out soon, next few months, you will see it,&quot; said Romano. &quot;That is the last PMR product. So I would say [higher] capacity point above 24TB PMR, that is probably 28TB SMR.&quot;<p>Sounds like the only uncertainty is in what the next capacity past 24TB will be, but it&#x27;ll definitely be SMR rather than PMR.</div><br/></div></div></div></div></div></div></div></div><div id="36277223" class="c"><input type="checkbox" id="c-36277223" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#36253899">root</a><span>|</span><a href="#36277027">parent</a><span>|</span><a href="#36277647">prev</a><span>|</span><a href="#36276832">next</a><span>|</span><label class="collapse" for="c-36277223">[-]</label><label class="expand" for="c-36277223">[3 more]</label></div><br/><div class="children"><div class="content">ZFS has the concept of a separate intent log device though, I wonder if that would be enough to make these types of disks work with it? I have no problem putting a couple of terabytes of flash in front of a ZFS array if it means I can have 40TB of redundant storage in 3 disks.</div><br/><div id="36277324" class="c"><input type="checkbox" id="c-36277324" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#36253899">root</a><span>|</span><a href="#36277223">parent</a><span>|</span><a href="#36279464">next</a><span>|</span><label class="collapse" for="c-36277324">[-]</label><label class="expand" for="c-36277324">[1 more]</label></div><br/><div class="children"><div class="content">The ZFS intent log is an example of a supremely disappointing, underwhelming way to integrate SSD storage into your system. Fundamentally, it&#x27;s just a workaround for the fact that most of the write caches preexisting in the storage stack are volatile caches and thus not safe. Putting the ZIL on an SSD allows you to get the safety without sacrificing the performance benefits of the write caching (performance that everyone has come to rely on). But the ZIL doesn&#x27;t help with read performance, and the data you write to it is never even read unless you have a crash or power failure. And then for the read caching, ZFS has L2ARC as an <i>entirely separate feature</i>. So ZFS does technically have ways to take advantage of SSDs to improve a hard drive storage array—but I don&#x27;t think anyone would consider it to be an ideal solution, more of a pragmatic minimum viable feature.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36276832" class="c"><input type="checkbox" id="c-36276832" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#36253899">parent</a><span>|</span><a href="#36254587">prev</a><span>|</span><a href="#36277086">next</a><span>|</span><label class="collapse" for="c-36276832">[-]</label><label class="expand" for="c-36276832">[4 more]</label></div><br/><div class="children"><div class="content">I am not entirely sure that HAMR will be too different from CMR&#x2F;PMR drives.  The premise of HAMR is that they heat the area under the head, expanding it, for reads and writes, and otherwise they are essentially CMR devices.  This isn&#x27;t like SMR, which had huge implications for drivers (due to the overlap in the bit lines).  There must be a significant durability cost to HAMR, but I don&#x27;t think the difference in drivers will be nearly as big for HAMR as it was for SMR.</div><br/><div id="36277867" class="c"><input type="checkbox" id="c-36277867" checked=""/><div class="controls bullet"><span class="by">rainbowzootsuit</span><span>|</span><a href="#36253899">root</a><span>|</span><a href="#36276832">parent</a><span>|</span><a href="#36276981">next</a><span>|</span><label class="collapse" for="c-36277867">[-]</label><label class="expand" for="c-36277867">[1 more]</label></div><br/><div class="children"><div class="content">The nanoscale heating manipulates the magnetic coercivity of the platter coating to allow the magnetic polarization to be manipulated. It seems somewhat analogous to modern day Magneto Optical but much finer scale and the readout is via a magnetic signal vs MO using an optical readout.</div><br/></div></div><div id="36276981" class="c"><input type="checkbox" id="c-36276981" checked=""/><div class="controls bullet"><span class="by">robotnikman</span><span>|</span><a href="#36253899">root</a><span>|</span><a href="#36276832">parent</a><span>|</span><a href="#36277867">prev</a><span>|</span><a href="#36277086">next</a><span>|</span><label class="collapse" for="c-36276981">[-]</label><label class="expand" for="c-36276981">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m guessing then that they will probably use a little bit more power then, but otherwise similar to CMR&#x2F;PMR in performance. That heated head element though seems like a massive point of failure, though I have no idea how durable the technology it.</div><br/><div id="36277157" class="c"><input type="checkbox" id="c-36277157" checked=""/><div class="controls bullet"><span class="by">dgacmu</span><span>|</span><a href="#36253899">root</a><span>|</span><a href="#36276981">parent</a><span>|</span><a href="#36277086">next</a><span>|</span><label class="collapse" for="c-36277157">[-]</label><label class="expand" for="c-36277157">[1 more]</label></div><br/><div class="children"><div class="content">The heating is done by a solid state laser diode. It should be very reliable other than adding one more thing to an already complex drive head. (Which undoubtedly means the first generation or two will have some kinks to work out, but that&#x27;s not a fundamental decrease, just engineering).<p>And there shouldn&#x27;t be much of an impact on durability. The heating is very brief - a nanosecond or so. The idea is not to expand the material physically, it&#x27;s to increase its magnetic permeability temporarily so that you can write the bit to it and then it becomes stable again.</div><br/></div></div></div></div></div></div></div></div><div id="36276910" class="c"><input type="checkbox" id="c-36276910" checked=""/><div class="controls bullet"><span class="by">metadat</span><span>|</span><a href="#36253899">prev</a><span>|</span><a href="#36279302">next</a><span>|</span><label class="collapse" for="c-36276910">[-]</label><label class="expand" for="c-36276910">[21 more]</label></div><br/><div class="children"><div class="content">I hadn&#x27;t read all the details behind HAMR before.  Fancy tech.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Heat-assisted_magnetic_recording" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Heat-assisted_magnetic_recordi...</a></div><br/><div id="36277453" class="c"><input type="checkbox" id="c-36277453" checked=""/><div class="controls bullet"><span class="by">lmpdev</span><span>|</span><a href="#36276910">parent</a><span>|</span><a href="#36279302">next</a><span>|</span><label class="collapse" for="c-36277453">[-]</label><label class="expand" for="c-36277453">[20 more]</label></div><br/><div class="children"><div class="content">I remember reading about HAMR a few years ago<p>Why does memory and other hardware tech always seem to magically come to fruition? At least compared to software<p>Is it because they&#x27;re more conservative in their announcements? More certainty in a path forward? More $$$?</div><br/><div id="36278042" class="c"><input type="checkbox" id="c-36278042" checked=""/><div class="controls bullet"><span class="by">scheme271</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36277453">parent</a><span>|</span><a href="#36278997">next</a><span>|</span><label class="collapse" for="c-36278042">[-]</label><label class="expand" for="c-36278042">[1 more]</label></div><br/><div class="children"><div class="content">Might be a selective bias effect?  A lot of hardware stuff has crashed and burned.  Most famously fusion has been 20 years away for 60+ years, and there&#x27;s always new rechargable battery tech that&#x27;s going to replace lithium ion in a few years.  For computer hardware, mram, feram, and optane&#x2F;crosspoint memory has all been pretty disappointing.</div><br/></div></div><div id="36278997" class="c"><input type="checkbox" id="c-36278997" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36277453">parent</a><span>|</span><a href="#36278042">prev</a><span>|</span><a href="#36278403">next</a><span>|</span><label class="collapse" for="c-36278997">[-]</label><label class="expand" for="c-36278997">[1 more]</label></div><br/><div class="children"><div class="content">In storage there are standards and abstractions that commodify the product. There are lots of vendors   and lots of projects at said vendors that try to deliver improved parts and they can count on customer demand &amp; competitive advantage if they succeed.<p>Most software (and hardware) is not like this.</div><br/></div></div><div id="36278403" class="c"><input type="checkbox" id="c-36278403" checked=""/><div class="controls bullet"><span class="by">omeysalvi</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36277453">parent</a><span>|</span><a href="#36278997">prev</a><span>|</span><a href="#36278599">next</a><span>|</span><label class="collapse" for="c-36278403">[-]</label><label class="expand" for="c-36278403">[6 more]</label></div><br/><div class="children"><div class="content">Most software innovations seem to be made to make the developer&#x27;s life easier instead of creating more performant software. I&#x27;m not complaining but user focused software innovations like LLMs do seem to come very few and far between compared to hardware.</div><br/><div id="36278582" class="c"><input type="checkbox" id="c-36278582" checked=""/><div class="controls bullet"><span class="by">wahahah</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36278403">parent</a><span>|</span><a href="#36278599">next</a><span>|</span><label class="collapse" for="c-36278582">[-]</label><label class="expand" for="c-36278582">[5 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t consider LLMs a software innovation.</div><br/><div id="36278773" class="c"><input type="checkbox" id="c-36278773" checked=""/><div class="controls bullet"><span class="by">karlshea</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36278582">parent</a><span>|</span><a href="#36278599">next</a><span>|</span><label class="collapse" for="c-36278773">[-]</label><label class="expand" for="c-36278773">[4 more]</label></div><br/><div class="children"><div class="content">I feel like if you&#x27;re gonna drop a take like that you should have at least one more sentence of justification to follow it up with.</div><br/><div id="36279147" class="c"><input type="checkbox" id="c-36279147" checked=""/><div class="controls bullet"><span class="by">Joeri</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36278773">parent</a><span>|</span><a href="#36278904">next</a><span>|</span><label class="collapse" for="c-36279147">[-]</label><label class="expand" for="c-36279147">[2 more]</label></div><br/><div class="children"><div class="content">I’m not sure I agree, but I’ll do my best to argue the position.<p>The neural net software algorithms have been around for decades. What made LLM’s feasible are the hardware advances to achieve unprecedented scale, just barely providing the ability (at great cost) to train today’s LLM models. Transformer architecture might be called a software innovation, but RWKV Raven gets similar performance to transformers and is built on decades-old RNN technology. So it is the hardware that was far more instrumental than the software in achieving LLM’s.<p>Counter to that argument: had google not done neural net research for google translate and proved the transformer approach scaled and performed well in their “attention is all you need” paper, people wouldn’t have spent the money to train foundation LLM models and we would not be having this discussion, so the software really mattered more than the hardware.<p>In reality I think it’s a little bit of both.</div><br/><div id="36279320" class="c"><input type="checkbox" id="c-36279320" checked=""/><div class="controls bullet"><span class="by">Shorel</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36279147">parent</a><span>|</span><a href="#36278904">next</a><span>|</span><label class="collapse" for="c-36279320">[-]</label><label class="expand" for="c-36279320">[1 more]</label></div><br/><div class="children"><div class="content">Great take. It is really a mix of several factors, each one leveraging the other, and your arguments are great.</div><br/></div></div></div></div><div id="36278904" class="c"><input type="checkbox" id="c-36278904" checked=""/><div class="controls bullet"><span class="by">hkt</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36278773">parent</a><span>|</span><a href="#36279147">prev</a><span>|</span><a href="#36278599">next</a><span>|</span><label class="collapse" for="c-36278904">[-]</label><label class="expand" for="c-36278904">[1 more]</label></div><br/><div class="children"><div class="content">I kind of admire the gumption, personally. Like writing them off as fancy markov bots.</div><br/></div></div></div></div></div></div></div></div><div id="36278599" class="c"><input type="checkbox" id="c-36278599" checked=""/><div class="controls bullet"><span class="by">physicsguy</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36277453">parent</a><span>|</span><a href="#36278403">prev</a><span>|</span><a href="#36277638">next</a><span>|</span><label class="collapse" for="c-36278599">[-]</label><label class="expand" for="c-36278599">[1 more]</label></div><br/><div class="children"><div class="content">I worked in this area of magnetics, and it’s been in research for at least 15 years. Companies had made devices by about 10 years ago but the storage capacities were low so they weren’t that useful for consumer applications at that point. The hard problem with HAMR is dissipating the heat.</div><br/></div></div><div id="36277638" class="c"><input type="checkbox" id="c-36277638" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36277453">parent</a><span>|</span><a href="#36278599">prev</a><span>|</span><a href="#36277566">next</a><span>|</span><label class="collapse" for="c-36277638">[-]</label><label class="expand" for="c-36277638">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure what you mean by magically. For every tech that makes it to production there are one or two others that don&#x27;t and people just forget about them. For example, storage roadmaps used to show bit-patterned media. Phase-change memory would have been nice to have too.</div><br/><div id="36277659" class="c"><input type="checkbox" id="c-36277659" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36277638">parent</a><span>|</span><a href="#36277837">next</a><span>|</span><label class="collapse" for="c-36277659">[-]</label><label class="expand" for="c-36277659">[3 more]</label></div><br/><div class="children"><div class="content">I recall reading about holographic storage[1] back in the 90s, was supposed to become a big thing &quot;real soon&quot;.<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Holographic_data_storage" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Holographic_data_storage</a></div><br/><div id="36277860" class="c"><input type="checkbox" id="c-36277860" checked=""/><div class="controls bullet"><span class="by">OldGuyInTheClub</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36277659">parent</a><span>|</span><a href="#36277770">next</a><span>|</span><label class="collapse" for="c-36277860">[-]</label><label class="expand" for="c-36277860">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not dead yet[1] but, yes, it&#x27;s never quite caught on the way so many of us thought it would back in the 1980s and 90s.  The 1987 movie &quot;Innerspace&quot; even referred to &quot;photon echo memory chips.&quot;  Friends of mine worked for years at one of the companies mentioned in the Wiki page but came up just a little short.<p>FWIW, gallium arsenide was also supposed to take over from silicon for CPUs once speeds went above 25-MHz (or so).  A senior colleague of mine said, &quot;There are a lot of smart people whose kids&#x27;s college tuition depend on making silicon a little better every year.&quot;  And so, it came to pass.  GaAs is definitely important but we&#x27;ve got multi-GHz silicon CPUs now.<p>[1] <a href="https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;project&#x2F;hsd&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;project&#x2F;hsd&#x2F;</a>
&quot;Project HSD: Holographic Storage Device for the Cloud&quot;</div><br/></div></div><div id="36277770" class="c"><input type="checkbox" id="c-36277770" checked=""/><div class="controls bullet"><span class="by">Twirrim</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36277659">parent</a><span>|</span><a href="#36277860">prev</a><span>|</span><a href="#36277837">next</a><span>|</span><label class="collapse" for="c-36277770">[-]</label><label class="expand" for="c-36277770">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s whitepapers and articles every few years on Holographic storage, it&#x27;s one of those perpetually just around the corner things.<p>See also memristors, that HP in particular have been saying is coming in the next couple of years, fundamentally changing the entire computing industry, for decades.</div><br/></div></div></div></div><div id="36277837" class="c"><input type="checkbox" id="c-36277837" checked=""/><div class="controls bullet"><span class="by">Clamchop</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36277638">parent</a><span>|</span><a href="#36277659">prev</a><span>|</span><a href="#36277566">next</a><span>|</span><label class="collapse" for="c-36277837">[-]</label><label class="expand" for="c-36277837">[1 more]</label></div><br/><div class="children"><div class="content">If we extend that to medical, the graveyard is bananas big.</div><br/></div></div></div></div><div id="36277566" class="c"><input type="checkbox" id="c-36277566" checked=""/><div class="controls bullet"><span class="by">celeritascelery</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36277453">parent</a><span>|</span><a href="#36277638">prev</a><span>|</span><a href="#36278356">next</a><span>|</span><label class="collapse" for="c-36277566">[-]</label><label class="expand" for="c-36277566">[1 more]</label></div><br/><div class="children"><div class="content">It seems like it was doubtful this would come to fruition, at least in 2013. There are many promising technologies that that never become economically viable. But necessity is the mother of invention.</div><br/></div></div><div id="36278356" class="c"><input type="checkbox" id="c-36278356" checked=""/><div class="controls bullet"><span class="by">gibolt</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36277453">parent</a><span>|</span><a href="#36277566">prev</a><span>|</span><a href="#36277747">next</a><span>|</span><label class="collapse" for="c-36278356">[-]</label><label class="expand" for="c-36278356">[1 more]</label></div><br/><div class="children"><div class="content">Lots of things don&#x27;t work until they do, and then iteration can be rapid. In addition, solutions for the same problem tend to have drastically different limitations and bottlenecks. Any new breakthrough can move the boundaries forward in leaps.</div><br/></div></div><div id="36277747" class="c"><input type="checkbox" id="c-36277747" checked=""/><div class="controls bullet"><span class="by">pyuser583</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36277453">parent</a><span>|</span><a href="#36278356">prev</a><span>|</span><a href="#36278594">next</a><span>|</span><label class="collapse" for="c-36277747">[-]</label><label class="expand" for="c-36277747">[2 more]</label></div><br/><div class="children"><div class="content">It’s the new Moores law. Instead of processing power doubling, it’s storage.</div><br/><div id="36278262" class="c"><input type="checkbox" id="c-36278262" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36277747">parent</a><span>|</span><a href="#36278594">next</a><span>|</span><label class="collapse" for="c-36278262">[-]</label><label class="expand" for="c-36278262">[1 more]</label></div><br/><div class="children"><div class="content">Kryder&#x27;s law is the storage equivalent of Moore&#x27;s law. It&#x27;s also coming to an end.</div><br/></div></div></div></div><div id="36278594" class="c"><input type="checkbox" id="c-36278594" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#36276910">root</a><span>|</span><a href="#36277453">parent</a><span>|</span><a href="#36277747">prev</a><span>|</span><a href="#36279302">next</a><span>|</span><label class="collapse" for="c-36278594">[-]</label><label class="expand" for="c-36278594">[1 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t be surprised if they have all this tech developed for 256TB hard drives already but they&#x27;re deliberately announcing it slowly over the years to buffer their time to do more research, lest they announce 256TB too fast and shareholders are like &quot;why don&#x27;t we have 1PB already damnit&quot; a quarter later and short the stock to oblivion</div><br/></div></div></div></div></div></div><div id="36279302" class="c"><input type="checkbox" id="c-36279302" checked=""/><div class="controls bullet"><span class="by">Shorel</span><span>|</span><a href="#36276910">prev</a><span>|</span><a href="#36277537">next</a><span>|</span><label class="collapse" for="c-36279302">[-]</label><label class="expand" for="c-36279302">[1 more]</label></div><br/><div class="children"><div class="content">Finally!?<p>I have a 5 TB external drive I bought many years ago, and despite its big physical footprint, the new drives still don&#x27;t feel like a justifiable upgrade, given their elevated price for a small increment in storage.</div><br/></div></div><div id="36277537" class="c"><input type="checkbox" id="c-36277537" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#36279302">prev</a><span>|</span><a href="#36277656">next</a><span>|</span><label class="collapse" for="c-36277537">[-]</label><label class="expand" for="c-36277537">[11 more]</label></div><br/><div class="children"><div class="content">As my NAS expands, I get more and more cold data. Meanwhile large SSD&#x27;s are getting cheap-ish (even TLC variants).<p>Are there any good options right now for multi-tiered storage for the home lab?<p>LVM has writeback cache as an option, but I couldn&#x27;t quite figure out how reliable that is, found some old posts with disturbing issues but not a lot of recent talk. Would also need to run ZFS on top of it for the ZFS features I rely on like snapshots and such, so feels like a Jenga tower solution.<p>I know Ceph has this option, but Ceph performance is abysmal for small installations from what I could benchmark.<p>Bcachefs looks like it&#x27;ll be a winner, but it&#x27;s still WIP so won&#x27;t trust it with my precious data quite yet I think.</div><br/><div id="36279306" class="c"><input type="checkbox" id="c-36279306" checked=""/><div class="controls bullet"><span class="by">matheusmoreira</span><span>|</span><a href="#36277537">parent</a><span>|</span><a href="#36277828">next</a><span>|</span><label class="collapse" for="c-36279306">[-]</label><label class="expand" for="c-36279306">[2 more]</label></div><br/><div class="children"><div class="content">&gt; As my NAS expands<p>How are you expanding capacity? With traditional software raid I&#x27;d need to fail every drive and replace them one by one with identical higher capacity drives which requires several rebuilds from parity which means a long time and massive I&#x2F;O loads and high risk of unrecoverable read errors just destroying the whole array in the process. It&#x27;s easier to make a new array and move data over...</div><br/><div id="36279431" class="c"><input type="checkbox" id="c-36279431" checked=""/><div class="controls bullet"><span class="by">KingMachiavelli</span><span>|</span><a href="#36277537">root</a><span>|</span><a href="#36279306">parent</a><span>|</span><a href="#36277828">next</a><span>|</span><label class="collapse" for="c-36279431">[-]</label><label class="expand" for="c-36279431">[1 more]</label></div><br/><div class="children"><div class="content">I this it&#x27;s just ZFS that has that &quot;limitation&quot;, LVM and Btrfs just let you mix drives of arbitrary sizes.</div><br/></div></div></div></div><div id="36277828" class="c"><input type="checkbox" id="c-36277828" checked=""/><div class="controls bullet"><span class="by">codemac</span><span>|</span><a href="#36277537">parent</a><span>|</span><a href="#36279306">prev</a><span>|</span><a href="#36278856">next</a><span>|</span><label class="collapse" for="c-36277828">[-]</label><label class="expand" for="c-36277828">[2 more]</label></div><br/><div class="children"><div class="content">The idea you want tiering... at home.. just seems so unlikely to save you any serious money, or gain you significant performance.<p>&quot;tiering&quot; in the traditional sense isn&#x27;t even popular in the DC with things like Vast, Pure, etc all doing well.<p>Is there any reason you don&#x27;t know the data set that is generally cold? The vast majority of home users basically have data that is trivially super cold that needs reasonable read performance - this is just HDDs. Then have a second mount point with things that you need greater performance, and have that be just SSDs.</div><br/><div id="36278929" class="c"><input type="checkbox" id="c-36278929" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#36277537">root</a><span>|</span><a href="#36277828">parent</a><span>|</span><a href="#36278856">next</a><span>|</span><label class="collapse" for="c-36278929">[-]</label><label class="expand" for="c-36278929">[1 more]</label></div><br/><div class="children"><div class="content">Having frequently used file metadata on SSDs while the cold parts are on HDDs would be noticeable even at home; the moment you start browsing the directory tree.</div><br/></div></div></div></div><div id="36278856" class="c"><input type="checkbox" id="c-36278856" checked=""/><div class="controls bullet"><span class="by">sacnoradhq</span><span>|</span><a href="#36277537">parent</a><span>|</span><a href="#36277828">prev</a><span>|</span><a href="#36279088">next</a><span>|</span><label class="collapse" for="c-36278856">[-]</label><label class="expand" for="c-36278856">[1 more]</label></div><br/><div class="children"><div class="content">I have more storage than I know what to do with. mdadm + LVM + XFS is reliable and works.</div><br/></div></div><div id="36279088" class="c"><input type="checkbox" id="c-36279088" checked=""/><div class="controls bullet"><span class="by">maccam94</span><span>|</span><a href="#36277537">parent</a><span>|</span><a href="#36278856">prev</a><span>|</span><a href="#36278305">next</a><span>|</span><label class="collapse" for="c-36279088">[-]</label><label class="expand" for="c-36279088">[1 more]</label></div><br/><div class="children"><div class="content">Why would you use LVM with ZFS? ZFS should handle all of the features LVM has (as far as I can think of, at least).</div><br/></div></div><div id="36278305" class="c"><input type="checkbox" id="c-36278305" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#36277537">parent</a><span>|</span><a href="#36279088">prev</a><span>|</span><a href="#36278532">next</a><span>|</span><label class="collapse" for="c-36278305">[-]</label><label class="expand" for="c-36278305">[1 more]</label></div><br/><div class="children"><div class="content">You need a buy a copy of StorNext.<p><a href="https:&#x2F;&#x2F;www.quantum.com&#x2F;en&#x2F;products&#x2F;file-system&#x2F;stornext&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.quantum.com&#x2F;en&#x2F;products&#x2F;file-system&#x2F;stornext&#x2F;</a></div><br/></div></div><div id="36278532" class="c"><input type="checkbox" id="c-36278532" checked=""/><div class="controls bullet"><span class="by">leptons</span><span>|</span><a href="#36277537">parent</a><span>|</span><a href="#36278305">prev</a><span>|</span><a href="#36277656">next</a><span>|</span><label class="collapse" for="c-36278532">[-]</label><label class="expand" for="c-36278532">[3 more]</label></div><br/><div class="children"><div class="content">I love my LTO-5 tape drive. 1.5TB on a tape, encryption, they have a write protect notch, and I add parity files to each tape to combat bitrot. The price per TB is pretty low. I keep a backup of important stuff off-site. As datacenters upgrade, I&#x27;ll gladly take their hand-me-downs, when the price is right.</div><br/><div id="36278798" class="c"><input type="checkbox" id="c-36278798" checked=""/><div class="controls bullet"><span class="by">sacnoradhq</span><span>|</span><a href="#36277537">root</a><span>|</span><a href="#36278532">parent</a><span>|</span><a href="#36277656">next</a><span>|</span><label class="collapse" for="c-36278798">[-]</label><label class="expand" for="c-36278798">[2 more]</label></div><br/><div class="children"><div class="content">Why? Hard drives are as cheap as tape now: $10&#x2F;TB. Tapes are sequential access media, wear out, and malfunction, whereas hard drives are random access media. I know all about StorageTek silos and 4U tape robots but still don&#x27;t bother with them.</div><br/><div id="36279449" class="c"><input type="checkbox" id="c-36279449" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#36277537">root</a><span>|</span><a href="#36278798">parent</a><span>|</span><a href="#36277656">next</a><span>|</span><label class="collapse" for="c-36279449">[-]</label><label class="expand" for="c-36279449">[1 more]</label></div><br/><div class="children"><div class="content">Tape is $5 per TB at retail and you can safely store data on it for at least 10 years, after which you may need to make a new copy not due to the risk of aging, but due to the risk of no longer finding new tape drives compatible with the tape format.<p>Your price of $10&#x2F;TB might apply when HDDs are purchased in bulk, because at retail I see prices between $15&#x2F;TB and $20&#x2F;TB.<p>No HDD may be trusted to store data for more than 5 years and this duration is valid only for the more expensive models.<p>Assuming your price of $10&#x2F;TB, one must buy at least a double HDD capacity than tape capacity, due to the short lifetime, so tape is at least 4 times cheaper. At the prices that I see at retail the difference is even greater.<p>The sequential transfer speed of tape is greater than that of HDDs, so archiving or retrieving many GB of data takes less time with tapes.<p>HDDs wear out and malfunction more frequently than tapes.<p>The only real disadvantage of tapes is the high cost of the tape drives, which makes tapes preferable only when more than 100 or 200 TB of data have to be stored.</div><br/></div></div></div></div></div></div></div></div><div id="36277656" class="c"><input type="checkbox" id="c-36277656" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#36277537">prev</a><span>|</span><a href="#36277179">next</a><span>|</span><label class="collapse" for="c-36277656">[-]</label><label class="expand" for="c-36277656">[5 more]</label></div><br/><div class="children"><div class="content">Somewhat related: 1TB optical discs for $3 from Folio Photonics (possible vaporware alert): <a href="https:&#x2F;&#x2F;www.techradar.com&#x2F;news&#x2F;exclusive-blu-ray-successor-will-cost-dollar1-per-1tb-disc-but-youll-need-a-dollar3000-drive-as-well" rel="nofollow">https:&#x2F;&#x2F;www.techradar.com&#x2F;news&#x2F;exclusive-blu-ray-successor-w...</a></div><br/><div id="36277902" class="c"><input type="checkbox" id="c-36277902" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#36277656">parent</a><span>|</span><a href="#36279351">next</a><span>|</span><label class="collapse" for="c-36277902">[-]</label><label class="expand" for="c-36277902">[3 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t exactly new, goes back to as far as [1] 2007 on Ars, and they reported something similar from the same startup [2]. I do wish them success though. Even a Single TB disc would be 10 to 40 times the current Blu-Ray.<p>It would be nice if they could do a mini cartridge version ( 6 Folio Mini Disc inside a Cartridge ) like Zip drive with 2TB for consumer. Unless you are a Data Hoarder 99.9% of the consumer could have their own backup down with a few of these cartridge. And in theory they should last a lot longer than HDD or even SSD.<p>Really wish this isn&#x27;t a pipe dream.<p>[1] <a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2007&#x2F;08&#x2F;new-dvd-sized-disc-to-hold-terabytes-of-data&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2007&#x2F;08&#x2F;new-dvd-sized-disc-t...</a><p>[2] <a href="https:&#x2F;&#x2F;www.storagereview.com&#x2F;news&#x2F;folio-photonics-working-on-optical-discs-of-the-future" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.storagereview.com&#x2F;news&#x2F;folio-photonics-working-o...</a></div><br/><div id="36278922" class="c"><input type="checkbox" id="c-36278922" checked=""/><div class="controls bullet"><span class="by">bunga-bunga</span><span>|</span><a href="#36277656">root</a><span>|</span><a href="#36277902">parent</a><span>|</span><a href="#36279351">next</a><span>|</span><label class="collapse" for="c-36278922">[-]</label><label class="expand" for="c-36278922">[2 more]</label></div><br/><div class="children"><div class="content">&gt; in theory they should last a lot longer than HDD or even SSD.<p>The etched plastic yes, but I think the reflective layer of CDs and DVDs is highly sensitive to humidity, shortening the lifespan to 10-20 years. You could probably recover it professionally, but not cheaply.</div><br/><div id="36278944" class="c"><input type="checkbox" id="c-36278944" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#36277656">root</a><span>|</span><a href="#36278922">parent</a><span>|</span><a href="#36279351">next</a><span>|</span><label class="collapse" for="c-36278944">[-]</label><label class="expand" for="c-36278944">[1 more]</label></div><br/><div class="children"><div class="content">&gt;the reflective layer of CDs and DVDs is highly sensitive to humidity, shortening the lifespan to 10-20 years.<p>this is actually solved for HTL BD-R Blurays as they use inorganic material for storing data. Only the outer plastic is organic there. See <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Blu-ray_Disc_recordable" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Blu-ray_Disc_recordable</a></div><br/></div></div></div></div></div></div><div id="36279351" class="c"><input type="checkbox" id="c-36279351" checked=""/><div class="controls bullet"><span class="by">Hamuko</span><span>|</span><a href="#36277656">parent</a><span>|</span><a href="#36277902">prev</a><span>|</span><a href="#36277179">next</a><span>|</span><label class="collapse" for="c-36279351">[-]</label><label class="expand" for="c-36279351">[1 more]</label></div><br/><div class="children"><div class="content">A $3000 drive sounds surprisingly tempting if it gives me access to 1 TB for only $3 afterwards, assuming that the drives have a decent amount of life to them.</div><br/></div></div></div></div><div id="36277179" class="c"><input type="checkbox" id="c-36277179" checked=""/><div class="controls bullet"><span class="by">mikerg87</span><span>|</span><a href="#36277656">prev</a><span>|</span><a href="#36277468">next</a><span>|</span><label class="collapse" for="c-36277179">[-]</label><label class="expand" for="c-36277179">[8 more]</label></div><br/><div class="children"><div class="content">Is it going to be hamstrung by SATA III?  It would take roughly 13 hours to fill the drive at 6 Gbps…</div><br/><div id="36277245" class="c"><input type="checkbox" id="c-36277245" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#36277179">parent</a><span>|</span><a href="#36277851">next</a><span>|</span><label class="collapse" for="c-36277245">[-]</label><label class="expand" for="c-36277245">[1 more]</label></div><br/><div class="children"><div class="content">Only if it&#x27;s a multi-actuator drive. Single-actuator drives are still barely hitting SATA II speeds, so this density boost on its own probably won&#x27;t be enough to increase performance to the point where SATA III&#x27;s 6Gbps is a real problem.</div><br/></div></div><div id="36277851" class="c"><input type="checkbox" id="c-36277851" checked=""/><div class="controls bullet"><span class="by">codemac</span><span>|</span><a href="#36277179">parent</a><span>|</span><a href="#36277245">prev</a><span>|</span><a href="#36277269">next</a><span>|</span><label class="collapse" for="c-36277851">[-]</label><label class="expand" for="c-36277851">[2 more]</label></div><br/><div class="children"><div class="content">You are hitting on the largest problem with high density HDDs - the IO dosn&#x27;t scale with the density, and thus the data needs to be colder and colder that you put on the drive.<p>The cold data thesis hasn&#x27;t worked out as well in the market as many storage vendors would have liked.</div><br/><div id="36278770" class="c"><input type="checkbox" id="c-36278770" checked=""/><div class="controls bullet"><span class="by">nayuki</span><span>|</span><a href="#36277179">root</a><span>|</span><a href="#36277851">parent</a><span>|</span><a href="#36277269">next</a><span>|</span><label class="collapse" for="c-36278770">[-]</label><label class="expand" for="c-36278770">[1 more]</label></div><br/><div class="children"><div class="content">There is a physical reason why I&#x2F;O speed doesn&#x27;t scale with density.<p>When the density of data on disks gets denser, it does so on both axes - x and y, or radial and tangential. But read&#x2F;write heads only access one track at a time, so the speed-up is only along one axis.<p>Think about CDs, DVDs, and BDs. The discs can only be spun at a finite rate (somewhere around 10k RPM) before they shatter. The R&#x2F;W head can read the data within any single track at full speed, but only one track at a time. From CDs to BDs, the amount of data per track increased (this doesn&#x27;t affect the amount of time needed to read all the data from a disc), but also the number of tracks increased (this does affect the total time).</div><br/></div></div></div></div><div id="36277269" class="c"><input type="checkbox" id="c-36277269" checked=""/><div class="controls bullet"><span class="by">dsr_</span><span>|</span><a href="#36277179">parent</a><span>|</span><a href="#36277851">prev</a><span>|</span><a href="#36277511">next</a><span>|</span><label class="collapse" for="c-36277269">[-]</label><label class="expand" for="c-36277269">[1 more]</label></div><br/><div class="children"><div class="content">You get some data throughput increases by increasing areal density and number of platters -- the days of counting on 100-120MB&#x2F;s from a spinner are now replaced by checking to see whether you get 220-280MB&#x2F;s -- but you should not expect the rotating disk technology to push the SATA III standard just yet.<p>It will be more than 13 hours.</div><br/></div></div><div id="36277511" class="c"><input type="checkbox" id="c-36277511" checked=""/><div class="controls bullet"><span class="by">lostlogin</span><span>|</span><a href="#36277179">parent</a><span>|</span><a href="#36277269">prev</a><span>|</span><a href="#36277835">next</a><span>|</span><label class="collapse" for="c-36277511">[-]</label><label class="expand" for="c-36277511">[2 more]</label></div><br/><div class="children"><div class="content">Adding an 18TB to a small Synology array takes days as it is. Adding a drive this size would take at least a week I think.</div><br/><div id="36279368" class="c"><input type="checkbox" id="c-36279368" checked=""/><div class="controls bullet"><span class="by">Hamuko</span><span>|</span><a href="#36277179">root</a><span>|</span><a href="#36277511">parent</a><span>|</span><a href="#36277835">next</a><span>|</span><label class="collapse" for="c-36279368">[-]</label><label class="expand" for="c-36279368">[1 more]</label></div><br/><div class="children"><div class="content">The biggest problem that I have with adding big drives to my NAS is the amount of prep work. badblocks takes fucking forever against bigger drives. It was like, 5 days to run three passes of badblocks against a 12 TB drive or something?</div><br/></div></div></div></div><div id="36277835" class="c"><input type="checkbox" id="c-36277835" checked=""/><div class="controls bullet"><span class="by">gnicholas</span><span>|</span><a href="#36277179">parent</a><span>|</span><a href="#36277511">prev</a><span>|</span><a href="#36277468">next</a><span>|</span><label class="collapse" for="c-36277835">[-]</label><label class="expand" for="c-36277835">[1 more]</label></div><br/><div class="children"><div class="content">Ah yes, &quot;mo TB, mo problems&quot;, as the Notorious B.I.G would lament.</div><br/></div></div></div></div><div id="36277468" class="c"><input type="checkbox" id="c-36277468" checked=""/><div class="controls bullet"><span class="by">xbmcuser</span><span>|</span><a href="#36277179">prev</a><span>|</span><a href="#36277562">next</a><span>|</span><label class="collapse" for="c-36277468">[-]</label><label class="expand" for="c-36277468">[1 more]</label></div><br/><div class="children"><div class="content">SSD prices at last seem to have an effect on hdd prices and capacities. We have almost had a decade of stagnant hdd prices and capacities. But it started to change as soon as ssd price per tb got close to hdd prices. I think we are back to a point where every 2-3 years the price of hhd&#x2F;tb drops regularly as larger hhd drives are added.</div><br/></div></div><div id="36277562" class="c"><input type="checkbox" id="c-36277562" checked=""/><div class="controls bullet"><span class="by">lyu07282</span><span>|</span><a href="#36277468">prev</a><span>|</span><a href="#36277852">next</a><span>|</span><label class="collapse" for="c-36277562">[-]</label><label class="expand" for="c-36277562">[1 more]</label></div><br/><div class="children"><div class="content">This is really exciting news, there hasn&#x27;t really been any big leaps in HDD capacities in a long time, +12TB perhaps even +20TB in one generation is huge. I hope they are going to be reasonably priced, but given the competition has nothing, 1k+? I really hope not.</div><br/></div></div><div id="36277852" class="c"><input type="checkbox" id="c-36277852" checked=""/><div class="controls bullet"><span class="by">iamgopal</span><span>|</span><a href="#36277562">prev</a><span>|</span><a href="#36277258">next</a><span>|</span><label class="collapse" for="c-36277852">[-]</label><label class="expand" for="c-36277852">[2 more]</label></div><br/><div class="children"><div class="content">If we go by surface area &#x2F; storage, how many atoms per bit this is ?</div><br/><div id="36278054" class="c"><input type="checkbox" id="c-36278054" checked=""/><div class="controls bullet"><span class="by">runlevel1</span><span>|</span><a href="#36277852">parent</a><span>|</span><a href="#36277258">next</a><span>|</span><label class="collapse" for="c-36278054">[-]</label><label class="expand" for="c-36278054">[1 more]</label></div><br/><div class="children"><div class="content">Seeing as how 3D NAND is already using quantum tunneling for charge trap flash, it&#x27;s gotta be hard to beat that, right?[^1]<p>[1]: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=5f2xOxRGKqk">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=5f2xOxRGKqk</a></div><br/></div></div></div></div><div id="36277258" class="c"><input type="checkbox" id="c-36277258" checked=""/><div class="controls bullet"><span class="by">WalterBright</span><span>|</span><a href="#36277852">prev</a><span>|</span><a href="#36276932">next</a><span>|</span><label class="collapse" for="c-36277258">[-]</label><label class="expand" for="c-36277258">[3 more]</label></div><br/><div class="children"><div class="content">Now the NSA will have enough storage to keep all that surveillance video of me watching The Munsters.</div><br/><div id="36277349" class="c"><input type="checkbox" id="c-36277349" checked=""/><div class="controls bullet"><span class="by">ww520</span><span>|</span><a href="#36277258">parent</a><span>|</span><a href="#36276932">next</a><span>|</span><label class="collapse" for="c-36277349">[-]</label><label class="expand" for="c-36277349">[2 more]</label></div><br/><div class="children"><div class="content">Enough for 18 surrounding cameras in 3D for 360 angles recorded in 120fps for 8k VR videos?  Times 7 billion people times 24&#x2F;7.</div><br/><div id="36279096" class="c"><input type="checkbox" id="c-36279096" checked=""/><div class="controls bullet"><span class="by">detrites</span><span>|</span><a href="#36277258">root</a><span>|</span><a href="#36277349">parent</a><span>|</span><a href="#36276932">next</a><span>|</span><label class="collapse" for="c-36279096">[-]</label><label class="expand" for="c-36279096">[1 more]</label></div><br/><div class="children"><div class="content">I admit I wasn&#x27;t particularly careful calculating this, but something like a quadrillion petabytes per year. I guess the datacenter would also need to be quite large, depending how vertical it could go. Say, a continent per year?<p>Your move, NSA.</div><br/></div></div></div></div></div></div><div id="36276932" class="c"><input type="checkbox" id="c-36276932" checked=""/><div class="controls bullet"><span class="by">fb03</span><span>|</span><a href="#36277258">prev</a><span>|</span><a href="#36277676">next</a><span>|</span><label class="collapse" for="c-36276932">[-]</label><label class="expand" for="c-36276932">[11 more]</label></div><br/><div class="children"><div class="content">Reminder: Have backups. Imagine losing 32TB of data in one go.</div><br/><div id="36277494" class="c"><input type="checkbox" id="c-36277494" checked=""/><div class="controls bullet"><span class="by">mmwelt</span><span>|</span><a href="#36276932">parent</a><span>|</span><a href="#36277156">next</a><span>|</span><label class="collapse" for="c-36277494">[-]</label><label class="expand" for="c-36277494">[2 more]</label></div><br/><div class="children"><div class="content">Restoring backups on a 32TB drive would also take a very long time!  Other comments have mentioned that it would probably take more than 13 hours to fill the drive.<p>On a related note, remirroring or rebuilding a 32TB RAID drive would probably take days.</div><br/><div id="36277791" class="c"><input type="checkbox" id="c-36277791" checked=""/><div class="controls bullet"><span class="by">toomuchtodo</span><span>|</span><a href="#36276932">root</a><span>|</span><a href="#36277494">parent</a><span>|</span><a href="#36277156">next</a><span>|</span><label class="collapse" for="c-36277791">[-]</label><label class="expand" for="c-36277791">[1 more]</label></div><br/><div class="children"><div class="content">Backup to cloud provider, and then have them ship you a replacement drive with the data preloaded when your local drive goes bad (Backblaze, for example, does this).<p>Local drives are more fast cache for data that should be stored reliably elsewhere (maybe hot, maybe cold, depending on provider and cost concerns).</div><br/></div></div></div></div><div id="36277156" class="c"><input type="checkbox" id="c-36277156" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#36276932">parent</a><span>|</span><a href="#36277494">prev</a><span>|</span><a href="#36277676">next</a><span>|</span><label class="collapse" for="c-36277156">[-]</label><label class="expand" for="c-36277156">[8 more]</label></div><br/><div class="children"><div class="content">Reminder: RAID is not a backup. You&#x27;re going to have more drives die while you&#x27;re resilvering 32TB of data.</div><br/><div id="36277271" class="c"><input type="checkbox" id="c-36277271" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#36276932">root</a><span>|</span><a href="#36277156">parent</a><span>|</span><a href="#36277539">next</a><span>|</span><label class="collapse" for="c-36277271">[-]</label><label class="expand" for="c-36277271">[1 more]</label></div><br/><div class="children"><div class="content">But &quot;RAID is not a backup&quot; is <i>not</i> related to this problem though. &quot;RAID is not a backup&quot; s a reflection of the fact that your RAID array is in one location, and instantly mirrors all data copied to it. A disaster to your storage array outside of a disk failure, will annihilate all your data (and this used to include &quot;RAID controller dies&quot;).<p>The practical issues of RAID rebuilds also don&#x27;t quite work that way - the &quot;drives die while rebuilding problem&quot; has two components:<p>(1) is that beyond a certain size, the risk of bit-error while resilvering becomes a certainty due to all the data you&#x27;re reading - ZFS fixes this problem with it&#x27;s checksums basically.<p>(2) If all your disks were installed at the same time, then the chance of another disk failure is high because they&#x27;re likely all in the same part of the mean-time-to-failure distribution.<p>(3) the risk of an error is higher because you&#x27;re doing a lot more activity then normal, and so the additional stress is increasing the chance of triggering a failure.</div><br/></div></div><div id="36277539" class="c"><input type="checkbox" id="c-36277539" checked=""/><div class="controls bullet"><span class="by">HumanOstrich</span><span>|</span><a href="#36276932">root</a><span>|</span><a href="#36277156">parent</a><span>|</span><a href="#36277271">prev</a><span>|</span><a href="#36277676">next</a><span>|</span><label class="collapse" for="c-36277539">[-]</label><label class="expand" for="c-36277539">[6 more]</label></div><br/><div class="children"><div class="content">Do you not regularly scrub your arrays to surface these issues promptly?</div><br/><div id="36277702" class="c"><input type="checkbox" id="c-36277702" checked=""/><div class="controls bullet"><span class="by">mbreese</span><span>|</span><a href="#36276932">root</a><span>|</span><a href="#36277539">parent</a><span>|</span><a href="#36277676">next</a><span>|</span><label class="collapse" for="c-36277702">[-]</label><label class="expand" for="c-36277702">[5 more]</label></div><br/><div class="children"><div class="content">With 32TB, nothing would be done “promptly”. Can you imagine how long resilvering this drive would take? These aren’t feasible for RAID setups…</div><br/><div id="36277821" class="c"><input type="checkbox" id="c-36277821" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#36276932">root</a><span>|</span><a href="#36277702">parent</a><span>|</span><a href="#36277676">next</a><span>|</span><label class="collapse" for="c-36277821">[-]</label><label class="expand" for="c-36277821">[4 more]</label></div><br/><div class="children"><div class="content">If you go from double to triple parity it should far more than make up for the extra resilvering time.<p>Though if you want specific numbers, what would you estimate as the per-drive odds of 8TB and 40TB drives dying during a resilver?  Let&#x27;s say they take 2 and 7 days respectively.</div><br/><div id="36278143" class="c"><input type="checkbox" id="c-36278143" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#36276932">root</a><span>|</span><a href="#36277821">parent</a><span>|</span><a href="#36277676">next</a><span>|</span><label class="collapse" for="c-36278143">[-]</label><label class="expand" for="c-36278143">[3 more]</label></div><br/><div class="children"><div class="content">Maybe, but then these have to be a lot cheaper for a triple redundant setup to be cost effective.</div><br/><div id="36278529" class="c"><input type="checkbox" id="c-36278529" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#36276932">root</a><span>|</span><a href="#36278143">parent</a><span>|</span><a href="#36277676">next</a><span>|</span><label class="collapse" for="c-36278529">[-]</label><label class="expand" for="c-36278529">[2 more]</label></div><br/><div class="children"><div class="content">Not really.  Let&#x27;s say you were doing 8+2 before (times five) with 8TB drives, and switch to 8+3 with 40TB drives.  You go from 400TB of raw space to 440TB of raw space, but you also go from 50 drive slots down to 11.  Even if both drive models are the same cost per terabyte, the latter setup should have a lower total cost.<p>But I would also expect these drives to be cheaper per TB, at least by the time HAMR is 2-3 generations old.</div><br/><div id="36279487" class="c"><input type="checkbox" id="c-36279487" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#36276932">root</a><span>|</span><a href="#36278529">parent</a><span>|</span><a href="#36277676">next</a><span>|</span><label class="collapse" for="c-36279487">[-]</label><label class="expand" for="c-36279487">[1 more]</label></div><br/><div class="children"><div class="content">I think if you&#x27;re in the market for 32TB drives, you&#x27;re already using 20TB ones instead of 8. With 8+2 at 20TB you pay for 200TB and get 160TB usable. With 7+3 at 32TB you pay for 320TB and have 224TB available. That&#x27;s 160% of TB paid for and only 140% of TB usable. HMR will need to be a sizable 12.5% discount per terabyte just to break even there.<p>If you only do 5+3 to keep the same 160T available, it&#x27;s 256TB or 128% paid for and the same 100% usable, so an even steeper 22% discount to break even.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="36277676" class="c"><input type="checkbox" id="c-36277676" checked=""/><div class="controls bullet"><span class="by">codemac</span><span>|</span><a href="#36276932">prev</a><span>|</span><a href="#36277877">next</a><span>|</span><label class="collapse" for="c-36277676">[-]</label><label class="expand" for="c-36277676">[3 more]</label></div><br/><div class="children"><div class="content">HAMR has some serious faults with it&#x27;s io heads (could rant about this forever), especially given the &quot;reman&quot; logic they use to qualify the durability of the drives.<p>I&#x27;d wait until these actually ship, and we get some data on them. These end up basically being flaky new-age tape systems rather than HDD given SMR is required for this density.</div><br/><div id="36277784" class="c"><input type="checkbox" id="c-36277784" checked=""/><div class="controls bullet"><span class="by">codemac</span><span>|</span><a href="#36277676">parent</a><span>|</span><a href="#36277877">next</a><span>|</span><label class="collapse" for="c-36277784">[-]</label><label class="expand" for="c-36277784">[2 more]</label></div><br/><div class="children"><div class="content">Ok, I&#x27;ll rant now.<p>Tape drives have multiple heads for IO, which allows some to fail along the way and you can still read&#x2F;write your data. This sounds good - but it actually just means these tape drive heads are flaky. What do you do when too many fail? You call IBM or whoever and get them to replace your tape drive, or &quot;reman&quot; it by replacing the failed heads. This is the only way they actually achieve their warranties around lifetime read&#x2F;writes.. they assume you&#x27;ll fix the hardware along the way.<p>These HAMR drives have the same problem. The &quot;heat assisted&quot; just means they&#x27;re using a laser to heat up a piece of gold, and sometimes this means the gold kinda drips around, and the head can be ruined. So their read&#x2F;write lifetime numbers are pretty loose compared to PMR, and there is an assumption you&#x27;ll &quot;reman&quot; these drives if they start to have failed heads for IO. However, the gold can even drip onto the platter, giving you permanent data loss anyways.<p>Lastly, they use SMR to get this density. SMR is not like PMR. PMR is what you think of with an HDD with many small blocks either 512B or 4KiB which you can read&#x2F;write to. SMR has 256MiB (or 128) &quot;zones&quot; that you can only append, or reset. This means instead of being able to write randomly across the drive&#x27;s capacity, you need to plan out your writes across appendable regions. This complicates your GC, compaction, and reduces your total system IO. Your random read performance is still better than Tape, but this basically turns your HDD solutions into something that looks a lot more like Tape. This is incredibly unpopular technology for this reason.<p>The market for these things is much smaller than most HDD vendors would like. You have a few hyperscalars that have figured out how to write out backups efficiently, but they want a lot of read IO, and reducing the spindle to byte ratio means you have less total system bandwidth to your data.<p>This means that the price per byte would actually need to be <i>lower</i> than their PMR drives, let alone wildly better warranty agreements, for these to be better TCO than current generation drives.</div><br/><div id="36278322" class="c"><input type="checkbox" id="c-36278322" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#36277676">root</a><span>|</span><a href="#36277784">parent</a><span>|</span><a href="#36277877">next</a><span>|</span><label class="collapse" for="c-36278322">[-]</label><label class="expand" for="c-36278322">[1 more]</label></div><br/><div class="children"><div class="content">These don&#x27;t sound like insurmountable issues.<p>&gt; The &quot;heat assisted&quot; just means they&#x27;re using a laser to heat up a piece of gold, and sometimes this means the gold kinda drips around, and the head can be ruined.<p>It sounds like this tech is related to MO tech, which has been used for decades without issues (see: minidisc).<p>The laser is said to heat the spot on the platter to a bit over 400C, while the melting point of gold is over 1000C, so that doesn&#x27;t jive. Did you meant something else?<p>&gt; Lastly, they use SMR to get this density.<p>Why do they have to? SMR is just a way of fitting more data on a disc by overlapping tracks. I don&#x27;t know why it is necessary to use this method on this technology.</div><br/></div></div></div></div></div></div><div id="36277877" class="c"><input type="checkbox" id="c-36277877" checked=""/><div class="controls bullet"><span class="by">civilitty</span><span>|</span><a href="#36277676">prev</a><span>|</span><a href="#36277310">next</a><span>|</span><label class="collapse" for="c-36277877">[-]</label><label class="expand" for="c-36277877">[1 more]</label></div><br/><div class="children"><div class="content">Finally, something dense enough to fit my entire porn collection in a single 2U server!</div><br/></div></div><div id="36277310" class="c"><input type="checkbox" id="c-36277310" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#36277877">prev</a><span>|</span><a href="#36277385">next</a><span>|</span><label class="collapse" for="c-36277310">[-]</label><label class="expand" for="c-36277310">[2 more]</label></div><br/><div class="children"><div class="content">Do HAMR drives have the same access pattern behaviors as normal spinning disks (eg sequential seek preferred to random)?</div><br/><div id="36277395" class="c"><input type="checkbox" id="c-36277395" checked=""/><div class="controls bullet"><span class="by">winrid</span><span>|</span><a href="#36277310">parent</a><span>|</span><a href="#36277385">next</a><span>|</span><label class="collapse" for="c-36277395">[-]</label><label class="expand" for="c-36277395">[1 more]</label></div><br/><div class="children"><div class="content">I suspect so, they&#x27;re still an arm that moves across the disk. It just also has to heat the surface for reads, evidently. It will be neat to know if there&#x27;s a warmup time for writes.</div><br/></div></div></div></div><div id="36278231" class="c"><input type="checkbox" id="c-36278231" checked=""/><div class="controls bullet"><span class="by">at_a_remove</span><span>|</span><a href="#36277385">prev</a><span>|</span><a href="#36277286">next</a><span>|</span><label class="collapse" for="c-36278231">[-]</label><label class="expand" for="c-36278231">[1 more]</label></div><br/><div class="children"><div class="content">Thinking &quot;quantity has a quality all its own,&quot; I&#x27;ve started to wonder more about the black swans on the horizon, should you look far enough: cosmic rays bitflipping your memory, unusual collisions, ridiculous rebuild times, the LTO-machine syndicate ...<p>Maybe file fixity is in the future, where the concept is more foreground than before.</div><br/></div></div><div id="36277286" class="c"><input type="checkbox" id="c-36277286" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#36278231">prev</a><span>|</span><label class="collapse" for="c-36277286">[-]</label><label class="expand" for="c-36277286">[3 more]</label></div><br/><div class="children"><div class="content">Since it’s heat assisted I wonder how much the drives will heat up server racks?</div><br/><div id="36277327" class="c"><input type="checkbox" id="c-36277327" checked=""/><div class="controls bullet"><span class="by">Scaevolus</span><span>|</span><a href="#36277286">parent</a><span>|</span><a href="#36277309">next</a><span>|</span><label class="collapse" for="c-36277327">[-]</label><label class="expand" for="c-36277327">[1 more]</label></div><br/><div class="children"><div class="content">Incredibly tiny areas are being heated for tiny lengths of time, so it has negligible effect.<p><a href="https:&#x2F;&#x2F;blog.seagate.com&#x2F;craftsman-ship&#x2F;hamr-next-leap-forward-now&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.seagate.com&#x2F;craftsman-ship&#x2F;hamr-next-leap-forwa...</a> 
&gt; Power, heat, and the reliability of related systems is equally nominal. HAMR heads integrated in customer systems consume under 200mW power while writing — a tiny percentage of the total 8W power a drive uses during random write, and easily maintaining a total power consumption equivalent to standard drives.</div><br/></div></div><div id="36277309" class="c"><input type="checkbox" id="c-36277309" checked=""/><div class="controls bullet"><span class="by">dannyw</span><span>|</span><a href="#36277286">parent</a><span>|</span><a href="#36277327">prev</a><span>|</span><label class="collapse" for="c-36277309">[-]</label><label class="expand" for="c-36277309">[1 more]</label></div><br/><div class="children"><div class="content">The heating is to a tiny area. It’s 0.2 watts or so. So, it won’t heat up server racks as normal HDDs use 8-15 watts.</div><br/></div></div></div></div></div></div></div></div></div></body></html>