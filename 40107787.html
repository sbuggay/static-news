<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1713776468192" as="style"/><link rel="stylesheet" href="styles.css?v=1713776468192"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2404.08698">Lossless Acceleration of LLM via Adaptive N-Gram Parallel Decoding</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>PaulHoule</span> | <span>23 comments</span></div><br/><div><div id="40108806" class="c"><input type="checkbox" id="c-40108806" checked=""/><div class="controls bullet"><span class="by">kristjansson</span><span>|</span><a href="#40108308">next</a><span>|</span><label class="collapse" for="c-40108806">[-]</label><label class="expand" for="c-40108806">[2 more]</label></div><br/><div class="children"><div class="content">This is speculative decoding with a n-gram Markov chain instead of a weaker transformer model in the “speculating” position.</div><br/><div id="40109937" class="c"><input type="checkbox" id="c-40109937" checked=""/><div class="controls bullet"><span class="by">huevosabio</span><span>|</span><a href="#40108806">parent</a><span>|</span><a href="#40108308">next</a><span>|</span><label class="collapse" for="c-40109937">[-]</label><label class="expand" for="c-40109937">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, this is a great, succinct way of summarizing the paper.</div><br/></div></div></div></div><div id="40108308" class="c"><input type="checkbox" id="c-40108308" checked=""/><div class="controls bullet"><span class="by">SonOfLilit</span><span>|</span><a href="#40108806">prev</a><span>|</span><a href="#40108647">next</a><span>|</span><label class="collapse" for="c-40108308">[-]</label><label class="expand" for="c-40108308">[9 more]</label></div><br/><div class="children"><div class="content">Good idea. Generate 8 tokens with a small model, then give them to a large model as one batch (much faster) and if tokens 1..3 are in agreement but 4..8 nut, take only 1..4 and start again. Most tokens are easy to guess so you&#x27;ll get x3.5 gains.<p>I do have a feeling of dejavú, like I&#x27;ve seen this before on hn.</div><br/><div id="40108605" class="c"><input type="checkbox" id="c-40108605" checked=""/><div class="controls bullet"><span class="by">milkey_mouse</span><span>|</span><a href="#40108308">parent</a><span>|</span><a href="#40108632">next</a><span>|</span><label class="collapse" for="c-40108605">[-]</label><label class="expand" for="c-40108605">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I do have a feeling of dejavú, like I&#x27;ve seen this before on hn.<p>You&#x27;re either thinking of speculative encoding more generally, or Medusa: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.10774" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.10774</a></div><br/></div></div><div id="40108632" class="c"><input type="checkbox" id="c-40108632" checked=""/><div class="controls bullet"><span class="by">johntb86</span><span>|</span><a href="#40108308">parent</a><span>|</span><a href="#40108605">prev</a><span>|</span><a href="#40109959">next</a><span>|</span><label class="collapse" for="c-40108632">[-]</label><label class="expand" for="c-40108632">[1 more]</label></div><br/><div class="children"><div class="content">They mention previous work on speculative decoding using similar techniques, but &quot;ANPD dynamically generates draft outputs
via an adaptive N-gram module using real-time
statistics, after which the drafts are verified by the
LLM. This characteristic is exactly the difference
between ANPD and the previous speculative decoding methods.&quot;</div><br/></div></div><div id="40109959" class="c"><input type="checkbox" id="c-40109959" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#40108308">parent</a><span>|</span><a href="#40108632">prev</a><span>|</span><a href="#40108607">next</a><span>|</span><label class="collapse" for="c-40109959">[-]</label><label class="expand" for="c-40109959">[3 more]</label></div><br/><div class="children"><div class="content">Just so I understand, this is essentially faster because reading all the model weights takes more time than calculating the model output?</div><br/><div id="40110461" class="c"><input type="checkbox" id="c-40110461" checked=""/><div class="controls bullet"><span class="by">SonOfLilit</span><span>|</span><a href="#40108308">root</a><span>|</span><a href="#40109959">parent</a><span>|</span><a href="#40108607">next</a><span>|</span><label class="collapse" for="c-40110461">[-]</label><label class="expand" for="c-40110461">[2 more]</label></div><br/><div class="children"><div class="content">GPUs are great at doing the same math on every item of a large multidimensional array.<p>Therefore, unsurprisingly, the cost per item of inference on a batch of items is significantly lower when the batch is e.g. 8 than 1 (in the case of Transformers there are further gains to be made because roughly half of the attention calculations in token k+1 are identical to the calculations of token k and can be easily reused by writing the formulas a certain way, the keyword to look for is causal attention mask).<p>In any reasonable GPU inference setup the weights would be preloaded.</div><br/><div id="40112290" class="c"><input type="checkbox" id="c-40112290" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#40108308">root</a><span>|</span><a href="#40110461">parent</a><span>|</span><a href="#40108607">next</a><span>|</span><label class="collapse" for="c-40112290">[-]</label><label class="expand" for="c-40112290">[1 more]</label></div><br/><div class="children"><div class="content">Indeed GPUs are great at doing the same calculation in parallel. But if it was just that there should be enough opportunity to parallelise even without doing the <i>exact</i> same calculation multiple times.<p>The main reason I can come up with why doing the same calculation 8 times in parallel instead of 8 times sequentially is that you benefit from better locality of reference.</div><br/></div></div></div></div></div></div><div id="40108607" class="c"><input type="checkbox" id="c-40108607" checked=""/><div class="controls bullet"><span class="by">outofpaper</span><span>|</span><a href="#40108308">parent</a><span>|</span><a href="#40109959">prev</a><span>|</span><a href="#40108369">next</a><span>|</span><label class="collapse" for="c-40108607">[-]</label><label class="expand" for="c-40108607">[2 more]</label></div><br/><div class="children"><div class="content">I think a number of us are using the method  and these folks decided to do a paper and release their findings.</div><br/><div id="40112110" class="c"><input type="checkbox" id="c-40112110" checked=""/><div class="controls bullet"><span class="by">jimmySixDOF</span><span>|</span><a href="#40108308">root</a><span>|</span><a href="#40108607">parent</a><span>|</span><a href="#40108369">next</a><span>|</span><label class="collapse" for="c-40112110">[-]</label><label class="expand" for="c-40112110">[1 more]</label></div><br/><div class="children"><div class="content">Lots of speedups already on Llama3 like Groq (hardware arch) and Modal [1] getting 200 t&#x2F;s<p>From the below Modal link they use:
&gt;continuous batching, so multiple generations can take place at the same time on a single container<p>&gt;PagedAttention, which applies memory paging to the attention mechanism’s key-value cache, increasing throughput<p>[1]<a href="https:&#x2F;&#x2F;modal.com&#x2F;docs&#x2F;examples&#x2F;text_generation_inference" rel="nofollow">https:&#x2F;&#x2F;modal.com&#x2F;docs&#x2F;examples&#x2F;text_generation_inference</a></div><br/></div></div></div></div><div id="40108369" class="c"><input type="checkbox" id="c-40108369" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#40108308">parent</a><span>|</span><a href="#40108607">prev</a><span>|</span><a href="#40108647">next</a><span>|</span><label class="collapse" for="c-40108369">[-]</label><label class="expand" for="c-40108369">[1 more]</label></div><br/><div class="children"><div class="content">Very reminiscent of parallel wavenet, which speed things up by generating multiple audio samples at a time.</div><br/></div></div></div></div><div id="40108647" class="c"><input type="checkbox" id="c-40108647" checked=""/><div class="controls bullet"><span class="by">nsagent</span><span>|</span><a href="#40108308">prev</a><span>|</span><a href="#40109481">next</a><span>|</span><label class="collapse" for="c-40108647">[-]</label><label class="expand" for="c-40108647">[2 more]</label></div><br/><div class="children"><div class="content">How does this differ from the 2018 NeurIPS paper, Blockwise Parallel Decoding for Deep Autoregressive Models?<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1811.03115" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1811.03115</a></div><br/><div id="40110679" class="c"><input type="checkbox" id="c-40110679" checked=""/><div class="controls bullet"><span class="by">tripplyons</span><span>|</span><a href="#40108647">parent</a><span>|</span><a href="#40109481">next</a><span>|</span><label class="collapse" for="c-40110679">[-]</label><label class="expand" for="c-40110679">[1 more]</label></div><br/><div class="children"><div class="content">They use a separate ngram model to generate the proposed sequence instead of extra heads on top of the main model. The process of verifying the proposed sequence appears to be the same.</div><br/></div></div></div></div><div id="40109481" class="c"><input type="checkbox" id="c-40109481" checked=""/><div class="controls bullet"><span class="by">fspeech</span><span>|</span><a href="#40108647">prev</a><span>|</span><a href="#40108563">next</a><span>|</span><label class="collapse" for="c-40109481">[-]</label><label class="expand" for="c-40109481">[1 more]</label></div><br/><div class="children"><div class="content">The speedup here would be very dependent on the context -- the kind of texts that the models are working with, as it proposes a rather naive n-gram generator (maybe I should say it does not provide any details on this critical component, instead simply refers to Jurafsky textbook). It might not be  robust. Instead Apple&#x27;s work on using the same model to produce n-gram lookahead is robust -- the n-gram generator works as well as the model itself: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.11131" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.11131</a></div><br/></div></div><div id="40108563" class="c"><input type="checkbox" id="c-40108563" checked=""/><div class="controls bullet"><span class="by">MyFirstSass</span><span>|</span><a href="#40109481">prev</a><span>|</span><a href="#40111470">next</a><span>|</span><label class="collapse" for="c-40108563">[-]</label><label class="expand" for="c-40108563">[5 more]</label></div><br/><div class="children"><div class="content">If this is &quot;plug and play&quot; can it be added to say llama.cpp and give ~3.67 speedup to existing models or is there some complication?</div><br/><div id="40108740" class="c"><input type="checkbox" id="c-40108740" checked=""/><div class="controls bullet"><span class="by">jncraton</span><span>|</span><a href="#40108563">parent</a><span>|</span><a href="#40110696">next</a><span>|</span><label class="collapse" for="c-40108740">[-]</label><label class="expand" for="c-40108740">[3 more]</label></div><br/><div class="children"><div class="content">The speedup would not be that high in practice for folks already using speculative decoding[1]. ANPD is similar but uses a simpler and faster drafting approach. These two enhancements can&#x27;t be meaningfully stacked. Here&#x27;s how the paper describes it:<p>&gt; ANPD dynamically generates draft outputs via an adaptive N-gram module using real-time statistics, after which the drafts are verified by the LLM. This characteristic is exactly the difference between ANPD and the previous speculative decoding methods.<p>ANPD does provide a more general-purpose solution to drafting that does not require training, loading, and running draft LLMs.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;2926">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;2926</a></div><br/><div id="40110972" class="c"><input type="checkbox" id="c-40110972" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#40108563">root</a><span>|</span><a href="#40108740">parent</a><span>|</span><a href="#40110696">next</a><span>|</span><label class="collapse" for="c-40110972">[-]</label><label class="expand" for="c-40110972">[2 more]</label></div><br/><div class="children"><div class="content">Who is already using speculative decoding? I haven&#x27;t seen anything about it in the llama.cpp or ollama docs.</div><br/><div id="40111102" class="c"><input type="checkbox" id="c-40111102" checked=""/><div class="controls bullet"><span class="by">eshoyuan</span><span>|</span><a href="#40108563">root</a><span>|</span><a href="#40110972">parent</a><span>|</span><a href="#40110696">next</a><span>|</span><label class="collapse" for="c-40111102">[-]</label><label class="expand" for="c-40111102">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;master&#x2F;examples&#x2F;speculative">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;master&#x2F;examples&#x2F;...</a></div><br/></div></div></div></div></div></div><div id="40110696" class="c"><input type="checkbox" id="c-40110696" checked=""/><div class="controls bullet"><span class="by">tripplyons</span><span>|</span><a href="#40108563">parent</a><span>|</span><a href="#40108740">prev</a><span>|</span><a href="#40111470">next</a><span>|</span><label class="collapse" for="c-40110696">[-]</label><label class="expand" for="c-40110696">[1 more]</label></div><br/><div class="children"><div class="content">The HuggingFace transformers library already has support for a similar method called prompt lookup decoding that uses the existing context to generate an ngram model: <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;27722">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;27722</a><p>I don&#x27;t think it would be that hard to switch it out for a pretrained ngram model.</div><br/></div></div></div></div><div id="40111470" class="c"><input type="checkbox" id="c-40111470" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#40108563">prev</a><span>|</span><a href="#40110612">next</a><span>|</span><label class="collapse" for="c-40111470">[-]</label><label class="expand" for="c-40111470">[1 more]</label></div><br/><div class="children"><div class="content">They should support another layer of pre-generation that is controlled on client side&#x2F;through grammar instead of n-gram markov chain &#x2F; small network.<p>Especially when it comes to programming languages or data formats like json - a lot of compute is spent on ie. silly whitespaces.</div><br/></div></div><div id="40110612" class="c"><input type="checkbox" id="c-40110612" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#40111470">prev</a><span>|</span><a href="#40111155">next</a><span>|</span><label class="collapse" for="c-40110612">[-]</label><label class="expand" for="c-40110612">[1 more]</label></div><br/><div class="children"><div class="content">I might be naive comes to classic ML &#x2F; NLP: how do you keep the prefix table for N = 5? Naively, that look-up table would be 100k^5 (assuming 100k vocabulary size). Is it very sparse? How large that usually is?</div><br/></div></div><div id="40111155" class="c"><input type="checkbox" id="c-40111155" checked=""/><div class="controls bullet"><span class="by">eshoyuan</span><span>|</span><a href="#40110612">prev</a><span>|</span><label class="collapse" for="c-40111155">[-]</label><label class="expand" for="c-40111155">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think this will be better than weaker transformer.</div><br/></div></div></div></div></div></div></div></body></html>