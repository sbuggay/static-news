<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1724490066257" as="style"/><link rel="stylesheet" href="styles.css?v=1724490066257"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://lmstudio.ai/blog/lmstudio-v0.3.0">LM Studio 0.3 – Discover, download, and run local LLMs</a> <span class="domain">(<a href="https://lmstudio.ai">lmstudio.ai</a>)</span></div><div class="subtext"><span>fdb</span> | <span>17 comments</span></div><br/><div><div id="41336699" class="c"><input type="checkbox" id="c-41336699" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#41336631">next</a><span>|</span><label class="collapse" for="c-41336699">[-]</label><label class="expand" for="c-41336699">[1 more]</label></div><br/><div class="children"><div class="content">Nice, it’s a solid product! It’s just a shame it’s not open source and its license doesn’t permit work use.</div><br/></div></div><div id="41336631" class="c"><input type="checkbox" id="c-41336631" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#41336699">prev</a><span>|</span><a href="#41336458">next</a><span>|</span><label class="collapse" for="c-41336631">[-]</label><label class="expand" for="c-41336631">[3 more]</label></div><br/><div class="children"><div class="content">Yesterday I wanted to find a conversation snippet in ChatGPT of a conversation I had maybe 1 or 2 weeks ago. Searching for a single keyword would have been enough to find it.<p>How is it possible that there&#x27;s still no way to search through your conversations?</div><br/><div id="41336701" class="c"><input type="checkbox" id="c-41336701" checked=""/><div class="controls bullet"><span class="by">code51</span><span>|</span><a href="#41336631">parent</a><span>|</span><a href="#41336643">next</a><span>|</span><label class="collapse" for="c-41336701">[-]</label><label class="expand" for="c-41336701">[1 more]</label></div><br/><div class="children"><div class="content">For Mac and iOS, you can install ChatGPT app.<p>Why they won&#x27;t enable search for their main web user crowd is beyond me.<p>Perhaps they are just afraid of scale. With all their might, it&#x27;s still possible that they can&#x27;t estimate the scale and complexity of queries they might receive.</div><br/></div></div><div id="41336643" class="c"><input type="checkbox" id="c-41336643" checked=""/><div class="controls bullet"><span class="by">Jedd</span><span>|</span><a href="#41336631">parent</a><span>|</span><a href="#41336701">prev</a><span>|</span><a href="#41336458">next</a><span>|</span><label class="collapse" for="c-41336643">[-]</label><label class="expand" for="c-41336643">[1 more]</label></div><br/><div class="children"><div class="content">Are you complaining about OpenAI&#x27;s ChatGPT&#x27;s web UI interface?</div><br/></div></div></div></div><div id="41336458" class="c"><input type="checkbox" id="c-41336458" checked=""/><div class="controls bullet"><span class="by">pornlover</span><span>|</span><a href="#41336631">prev</a><span>|</span><a href="#41336185">next</a><span>|</span><label class="collapse" for="c-41336458">[-]</label><label class="expand" for="c-41336458">[1 more]</label></div><br/><div class="children"><div class="content">LM Studio is great, although I wish recommended prompts were part of the data of each LLM. I probably just don&#x27;t know enough but I feel like I get hunk of magic data and then I&#x27;m mostly on my own.<p>Similarly with images, LLMs and ML in general feel like DOS and config.sys and autoexec.bat and qemm days.</div><br/></div></div><div id="41336185" class="c"><input type="checkbox" id="c-41336185" checked=""/><div class="controls bullet"><span class="by">pcf</span><span>|</span><a href="#41336458">prev</a><span>|</span><a href="#41336575">next</a><span>|</span><label class="collapse" for="c-41336185">[-]</label><label class="expand" for="c-41336185">[3 more]</label></div><br/><div class="children"><div class="content">In some brief testing, I discovered that the same models (Llama 3 7B and one more I can&#x27;t remember) are running MUCH slower in LM Studio than in Ollama on my MacBook Air M1 2020.<p>Has anyone found the same thing, or was that a fluke and I should try LM Studio again?</div><br/><div id="41336702" class="c"><input type="checkbox" id="c-41336702" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#41336185">parent</a><span>|</span><a href="#41336382">next</a><span>|</span><label class="collapse" for="c-41336702">[-]</label><label class="expand" for="c-41336702">[1 more]</label></div><br/><div class="children"><div class="content">Don’t forget to tune your num_batch</div><br/></div></div><div id="41336382" class="c"><input type="checkbox" id="c-41336382" checked=""/><div class="controls bullet"><span class="by">christkv</span><span>|</span><a href="#41336185">parent</a><span>|</span><a href="#41336702">prev</a><span>|</span><a href="#41336575">next</a><span>|</span><label class="collapse" for="c-41336382">[-]</label><label class="expand" for="c-41336382">[1 more]</label></div><br/><div class="children"><div class="content">Make sure you turn on the use of the GPU using the slider. By default it does not leverage the full speed.</div><br/></div></div></div></div><div id="41336575" class="c"><input type="checkbox" id="c-41336575" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#41336185">prev</a><span>|</span><a href="#41335876">next</a><span>|</span><label class="collapse" for="c-41336575">[-]</label><label class="expand" for="c-41336575">[1 more]</label></div><br/><div class="children"><div class="content">Neat! Can i use it with Brave browser‘s local LLM festure?</div><br/></div></div><div id="41335876" class="c"><input type="checkbox" id="c-41335876" checked=""/><div class="controls bullet"><span class="by">webprofusion</span><span>|</span><a href="#41336575">prev</a><span>|</span><a href="#41336413">next</a><span>|</span><label class="collapse" for="c-41335876">[-]</label><label class="expand" for="c-41335876">[2 more]</label></div><br/><div class="children"><div class="content">Cool, it&#x27;s a bit weird that the Windows download is 32-bit, it should be 64-bit by default and there&#x27;s no need for a 32-bit windows version at all.</div><br/><div id="41335884" class="c"><input type="checkbox" id="c-41335884" checked=""/><div class="controls bullet"><span class="by">webprofusion</span><span>|</span><a href="#41335876">parent</a><span>|</span><a href="#41336413">next</a><span>|</span><label class="collapse" for="c-41335884">[-]</label><label class="expand" for="c-41335884">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s probably 64-bit and they just call it x86 on their website. Needs an option to choose where models get downloaded to as your typically C: drive is an SSD with limited space.</div><br/></div></div></div></div><div id="41336413" class="c"><input type="checkbox" id="c-41336413" checked=""/><div class="controls bullet"><span class="by">grigio</span><span>|</span><a href="#41335876">prev</a><span>|</span><a href="#41336307">next</a><span>|</span><label class="collapse" for="c-41336413">[-]</label><label class="expand" for="c-41336413">[2 more]</label></div><br/><div class="children"><div class="content">can somebody share benchmarks on AMD ryzen AI with and without NPU ?</div><br/><div id="41336635" class="c"><input type="checkbox" id="c-41336635" checked=""/><div class="controls bullet"><span class="by">Jedd</span><span>|</span><a href="#41336413">parent</a><span>|</span><a href="#41336307">next</a><span>|</span><label class="collapse" for="c-41336635">[-]</label><label class="expand" for="c-41336635">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s using llama.cpp, so it&#x27;s going to be the same benchmarks as almost all other apps (given almost everything uses llama.cpp under the hood).</div><br/></div></div></div></div><div id="41336307" class="c"><input type="checkbox" id="c-41336307" checked=""/><div class="controls bullet"><span class="by">alok-g</span><span>|</span><a href="#41336413">prev</a><span>|</span><a href="#41336056">next</a><span>|</span><label class="collapse" for="c-41336307">[-]</label><label class="expand" for="c-41336307">[1 more]</label></div><br/><div class="children"><div class="content">See also:  Msty.app<p>It allows both local and cloud models.<p>* Not associated with them in any way.  Am a happy user.</div><br/></div></div><div id="41325540" class="c"><input type="checkbox" id="c-41325540" checked=""/><div class="controls bullet"><span class="by">navaed01</span><span>|</span><a href="#41336056">prev</a><span>|</span><label class="collapse" for="c-41325540">[-]</label><label class="expand" for="c-41325540">[1 more]</label></div><br/><div class="children"><div class="content">Congrats! I’m a big fan of the existing product and the are some great updates to make the app even more accessible and powerful</div><br/></div></div></div></div></div></div></div></body></html>