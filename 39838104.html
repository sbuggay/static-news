<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1711616476172" as="style"/><link rel="stylesheet" href="styles.css?v=1711616476172"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm">DBRX: A new open LLM</a> <span class="domain">(<a href="https://www.databricks.com">www.databricks.com</a>)</span></div><div class="subtext"><span>jasondavies</span> | <span>248 comments</span></div><br/><div><div id="39838697" class="c"><input type="checkbox" id="c-39838697" checked=""/><div class="controls bullet"><span class="by">djoldman</span><span>|</span><a href="#39841293">next</a><span>|</span><label class="collapse" for="c-39838697">[-]</label><label class="expand" for="c-39838697">[35 more]</label></div><br/><div class="children"><div class="content">Model card for base: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;databricks&#x2F;dbrx-base" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;databricks&#x2F;dbrx-base</a><p>&gt; The model requires ~264GB of RAM<p>I&#x27;m wondering when everyone will transition from tracking parameter count vs evaluation metric to (total gpu RAM + total CPU RAM) vs evaluation metric.<p>For example, a 7B parameter model using float32s will almost certainly outperform a 7B model using float4s.<p>Additionally, all the examples of quantizing recently released superior models to fit on one GPU doesnt mean the quantized model is a &quot;win.&quot; The quantized model is a different model, you need to rerun the metrics.</div><br/><div id="39841646" class="c"><input type="checkbox" id="c-39841646" checked=""/><div class="controls bullet"><span class="by">ml_hardware</span><span>|</span><a href="#39838697">parent</a><span>|</span><a href="#39843289">next</a><span>|</span><label class="collapse" for="c-39841646">[-]</label><label class="expand" for="c-39841646">[13 more]</label></div><br/><div class="children"><div class="content">Looks like someone has got DBRX running on an M2 Ultra already: <a href="https:&#x2F;&#x2F;x.com&#x2F;awnihannun&#x2F;status&#x2F;1773024954667184196?s=20" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;awnihannun&#x2F;status&#x2F;1773024954667184196?s=20</a></div><br/><div id="39843645" class="c"><input type="checkbox" id="c-39843645" checked=""/><div class="controls bullet"><span class="by">resource_waste</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39841646">parent</a><span>|</span><a href="#39842153">next</a><span>|</span><label class="collapse" for="c-39843645">[-]</label><label class="expand" for="c-39843645">[1 more]</label></div><br/><div class="children"><div class="content">I find 500 tokens considered &#x27;running&#x27; a stretch.<p>Cool to play with for a few tests, but I can&#x27;t imagine using it for anything.</div><br/></div></div><div id="39842153" class="c"><input type="checkbox" id="c-39842153" checked=""/><div class="controls bullet"><span class="by">Mandelmus</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39841646">parent</a><span>|</span><a href="#39843645">prev</a><span>|</span><a href="#39842695">next</a><span>|</span><label class="collapse" for="c-39842153">[-]</label><label class="expand" for="c-39842153">[9 more]</label></div><br/><div class="children"><div class="content">And it appears to be at ~80 GB of RAM via quantisation.</div><br/><div id="39843829" class="c"><input type="checkbox" id="c-39843829" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39842153">parent</a><span>|</span><a href="#39843417">next</a><span>|</span><label class="collapse" for="c-39843829">[-]</label><label class="expand" for="c-39843829">[2 more]</label></div><br/><div class="children"><div class="content">So that would be runnable on a MBP with a M2 Max, but the context window must be quite small, I don’t really find anything under about 4096 that useful</div><br/><div id="39847790" class="c"><input type="checkbox" id="c-39847790" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39843829">parent</a><span>|</span><a href="#39843417">next</a><span>|</span><label class="collapse" for="c-39847790">[-]</label><label class="expand" for="c-39847790">[1 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t wait to try this on my MacBook. I&#x27;m also just amazed at how wasteful Grok appears to be!</div><br/></div></div></div></div><div id="39843417" class="c"><input type="checkbox" id="c-39843417" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39842153">parent</a><span>|</span><a href="#39843829">prev</a><span>|</span><a href="#39842695">next</a><span>|</span><label class="collapse" for="c-39843417">[-]</label><label class="expand" for="c-39843417">[6 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a tricky number. Does it run on an 80GB GPU, does it auto-shave some parameters to fit in 79.99GB like any articifially &quot;intelligent&quot; piece of code would do, or does it give up like an unintelligent piece of code?</div><br/><div id="39847376" class="c"><input type="checkbox" id="c-39847376" checked=""/><div class="controls bullet"><span class="by">Jedd</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39843417">parent</a><span>|</span><a href="#39844449">next</a><span>|</span><label class="collapse" for="c-39847376">[-]</label><label class="expand" for="c-39847376">[1 more]</label></div><br/><div class="children"><div class="content">Are you aware how Macs present memory? Their &#x27;unified&#x27; memory approach means you could run an 80GB model on a 128GB machine.<p>There&#x27;s no concept of &#x27;dedicated GPU memory&#x27; as per conventional amd64 arch machines.</div><br/></div></div><div id="39844449" class="c"><input type="checkbox" id="c-39844449" checked=""/><div class="controls bullet"><span class="by">declaredapple</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39843417">parent</a><span>|</span><a href="#39847376">prev</a><span>|</span><a href="#39842695">next</a><span>|</span><label class="collapse" for="c-39844449">[-]</label><label class="expand" for="c-39844449">[4 more]</label></div><br/><div class="children"><div class="content">What?<p>Are you asking if the framework automatically quantizes&#x2F;prunes the model on the fly?<p>Or are you suggesting the LLM itself should realize it&#x27;s too big to run, and prune&#x2F;quantize itself? Your references to &quot;intelligent&quot; almost leads me to the conclusion that you think the LLM should prune itself. Not only is this a chicken and egg problem, but LLMs are statistical models, they aren&#x27;t inherently self bootstraping.</div><br/><div id="39848306" class="c"><input type="checkbox" id="c-39848306" checked=""/><div class="controls bullet"><span class="by">2099miles</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39844449">parent</a><span>|</span><a href="#39844877">next</a><span>|</span><label class="collapse" for="c-39848306">[-]</label><label class="expand" for="c-39848306">[1 more]</label></div><br/><div class="children"><div class="content">The LLM itself should realize it’s too big and only put the important parts on the gpu. If you’re asking questions about literature there’s no need to have all the params on the gpu, just tell it to put only the ones for literature on there.</div><br/></div></div><div id="39844877" class="c"><input type="checkbox" id="c-39844877" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39844449">parent</a><span>|</span><a href="#39848306">prev</a><span>|</span><a href="#39842695">next</a><span>|</span><label class="collapse" for="c-39844877">[-]</label><label class="expand" for="c-39844877">[2 more]</label></div><br/><div class="children"><div class="content">I realize that, but I do think it&#x27;s doable to bootstrap it on a cluster and teach itself to self-prune, and surprised nobody is actively working on this.<p>I hate software that complains (about dependencies, resources) when you try to run it and I think that should be one of the first use cases for LLMs to get L5 autonomous software installation and execution.</div><br/><div id="39845816" class="c"><input type="checkbox" id="c-39845816" checked=""/><div class="controls bullet"><span class="by">Red_Leaves_Flyy</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39844877">parent</a><span>|</span><a href="#39842695">next</a><span>|</span><label class="collapse" for="c-39845816">[-]</label><label class="expand" for="c-39845816">[1 more]</label></div><br/><div class="children"><div class="content">Make your dreams a reality!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39842695" class="c"><input type="checkbox" id="c-39842695" checked=""/><div class="controls bullet"><span class="by">madiator</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39841646">parent</a><span>|</span><a href="#39842153">prev</a><span>|</span><a href="#39843289">next</a><span>|</span><label class="collapse" for="c-39842695">[-]</label><label class="expand" for="c-39842695">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s great, but it did not really write the program that the human asked it to do. :)</div><br/><div id="39843012" class="c"><input type="checkbox" id="c-39843012" checked=""/><div class="controls bullet"><span class="by">SparkyMcUnicorn</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39842695">parent</a><span>|</span><a href="#39843289">next</a><span>|</span><label class="collapse" for="c-39843012">[-]</label><label class="expand" for="c-39843012">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s because it&#x27;s the base model, not the instruct tuned one.</div><br/></div></div></div></div></div></div><div id="39843289" class="c"><input type="checkbox" id="c-39843289" checked=""/><div class="controls bullet"><span class="by">dvt</span><span>|</span><a href="#39838697">parent</a><span>|</span><a href="#39841646">prev</a><span>|</span><a href="#39840995">next</a><span>|</span><label class="collapse" for="c-39843289">[-]</label><label class="expand" for="c-39843289">[1 more]</label></div><br/><div class="children"><div class="content">&gt; a 7B parameter model using float32s will almost certainly outperform a 7B model using float4s<p>Q5 quantization performs <i>almost</i> on par with base models. Obviously there&#x27;s some loss there, but this indicates that there&#x27;s still a lot of compression that we&#x27;re missing.</div><br/></div></div><div id="39840995" class="c"><input type="checkbox" id="c-39840995" checked=""/><div class="controls bullet"><span class="by">swalsh</span><span>|</span><a href="#39838697">parent</a><span>|</span><a href="#39843289">prev</a><span>|</span><a href="#39839148">next</a><span>|</span><label class="collapse" for="c-39840995">[-]</label><label class="expand" for="c-39840995">[5 more]</label></div><br/><div class="children"><div class="content">&gt; The model requires ~264GB of RAM<p>This feels as crazy as Grok.  Was there a generation of models recently where we decided to just crank on the parameter count?</div><br/><div id="39843551" class="c"><input type="checkbox" id="c-39843551" checked=""/><div class="controls bullet"><span class="by">breezeTrowel</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39840995">parent</a><span>|</span><a href="#39842696">next</a><span>|</span><label class="collapse" for="c-39843551">[-]</label><label class="expand" for="c-39843551">[1 more]</label></div><br/><div class="children"><div class="content">Cranking up the parameter count is literally how the current LLM craze got started. Hence the &quot;large&quot; in &quot;large language model&quot;.</div><br/></div></div><div id="39842696" class="c"><input type="checkbox" id="c-39842696" checked=""/><div class="controls bullet"><span class="by">Jackson__</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39840995">parent</a><span>|</span><a href="#39843551">prev</a><span>|</span><a href="#39845309">next</a><span>|</span><label class="collapse" for="c-39842696">[-]</label><label class="expand" for="c-39842696">[1 more]</label></div><br/><div class="children"><div class="content">If you read their blog post, they mention it was pretrained on 12 Trillion tokens of text. That is ~5x the amount of the llama2 training runs.<p>From that, it seems somewhat likely we&#x27;ve hit the wall on improving &lt;X B parameter LLMs by simply scaling up the training data, which basically forces everyone to continue scaling up if they want to keep up with SOTA.</div><br/></div></div><div id="39845309" class="c"><input type="checkbox" id="c-39845309" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39840995">parent</a><span>|</span><a href="#39842696">prev</a><span>|</span><a href="#39841704">next</a><span>|</span><label class="collapse" for="c-39845309">[-]</label><label class="expand" for="c-39845309">[1 more]</label></div><br/><div class="children"><div class="content">Not recently. GPT-3 from 2020 requires even more RAM; the open-source BLOOM from 2022 did too.<p>In my view, the main value of larger models is distillation (which we particularly witness, for instance, with how Claude Haiku matches release-day GPT-4 despite being less than a tenth of the cost). Hopefully the distilled models will be easier to run.</div><br/></div></div><div id="39841704" class="c"><input type="checkbox" id="c-39841704" checked=""/><div class="controls bullet"><span class="by">wrs</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39840995">parent</a><span>|</span><a href="#39845309">prev</a><span>|</span><a href="#39839148">next</a><span>|</span><label class="collapse" for="c-39841704">[-]</label><label class="expand" for="c-39841704">[1 more]</label></div><br/><div class="children"><div class="content">Isn’t that pretty much the last 12 months?</div><br/></div></div></div></div><div id="39839148" class="c"><input type="checkbox" id="c-39839148" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#39838697">parent</a><span>|</span><a href="#39840995">prev</a><span>|</span><a href="#39843448">next</a><span>|</span><label class="collapse" for="c-39839148">[-]</label><label class="expand" for="c-39839148">[11 more]</label></div><br/><div class="children"><div class="content">I thought float4 sacrificed a negligible cost in evaluation quality for a 8x reduction in RAM?</div><br/><div id="39844753" class="c"><input type="checkbox" id="c-39844753" checked=""/><div class="controls bullet"><span class="by">Taek</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39839148">parent</a><span>|</span><a href="#39839304">next</a><span>|</span><label class="collapse" for="c-39844753">[-]</label><label class="expand" for="c-39844753">[1 more]</label></div><br/><div class="children"><div class="content">For smaller models, the quality drop is meaningful. For larger ones like this one, the quality drop is negligible.</div><br/></div></div><div id="39839304" class="c"><input type="checkbox" id="c-39839304" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39839148">parent</a><span>|</span><a href="#39844753">prev</a><span>|</span><a href="#39843448">next</a><span>|</span><label class="collapse" for="c-39839304">[-]</label><label class="expand" for="c-39839304">[9 more]</label></div><br/><div class="children"><div class="content">A free lunch? Wouldn&#x27;t that be nice! Sometimes the quantization process improves the accuracy a little (probably by implicit regularization) but a model that&#x27;s at or near capacity (as it should be) is necessarily hurt by throwing away most of the information. Language models often quantize well to small fixed-point types like int4, but it&#x27;s not a magic wand.</div><br/><div id="39841428" class="c"><input type="checkbox" id="c-39841428" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39839304">parent</a><span>|</span><a href="#39845218">next</a><span>|</span><label class="collapse" for="c-39841428">[-]</label><label class="expand" for="c-39841428">[3 more]</label></div><br/><div class="children"><div class="content">I didn’t suggest a free lunch, just that the 8x reduction in RAM (+ faster processing) does not result in an 8x growth in the error. Thus a quantized model will outperform a non-quantized one on a evaluation&#x2F;RAM metric.</div><br/><div id="39844466" class="c"><input type="checkbox" id="c-39844466" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39841428">parent</a><span>|</span><a href="#39845218">next</a><span>|</span><label class="collapse" for="c-39844466">[-]</label><label class="expand" for="c-39844466">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not a good metric.</div><br/><div id="39844808" class="c"><input type="checkbox" id="c-39844808" checked=""/><div class="controls bullet"><span class="by">omeze</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39844466">parent</a><span>|</span><a href="#39845218">next</a><span>|</span><label class="collapse" for="c-39844808">[-]</label><label class="expand" for="c-39844808">[1 more]</label></div><br/><div class="children"><div class="content">Many applications dont want to host inference on the cloud and would ideally run things locally. Hardware constraints is clearly important.<p>Id actually say its the most important metric for most open models now, since the price per performance of closed cloud models is so competitive with open cloud models, so edge inference that is competitive is a clear value add</div><br/></div></div></div></div></div></div><div id="39845218" class="c"><input type="checkbox" id="c-39845218" checked=""/><div class="controls bullet"><span class="by">underlines</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39839304">parent</a><span>|</span><a href="#39841428">prev</a><span>|</span><a href="#39839682">next</a><span>|</span><label class="collapse" for="c-39845218">[-]</label><label class="expand" for="c-39845218">[1 more]</label></div><br/><div class="children"><div class="content">This paper partially finds disagreeing evidence: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2403.17887" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2403.17887</a></div><br/></div></div><div id="39839682" class="c"><input type="checkbox" id="c-39839682" checked=""/><div class="controls bullet"><span class="by">K0balt</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39839304">parent</a><span>|</span><a href="#39845218">prev</a><span>|</span><a href="#39843448">next</a><span>|</span><label class="collapse" for="c-39839682">[-]</label><label class="expand" for="c-39839682">[4 more]</label></div><br/><div class="children"><div class="content">I find that q6 and 5+ are subjectively as good as raw tensor files. 4 bit quality reduction is very detectable though.  Of course there must be a loss of information, but perhaps there is a noise floor or something like that.</div><br/><div id="39844749" class="c"><input type="checkbox" id="c-39844749" checked=""/><div class="controls bullet"><span class="by">Taek</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39839682">parent</a><span>|</span><a href="#39843448">next</a><span>|</span><label class="collapse" for="c-39844749">[-]</label><label class="expand" for="c-39844749">[3 more]</label></div><br/><div class="children"><div class="content">At what parameter count? Its been established that quantization has less of an effect on larger models. By the time you are at 70B quantization to 4 bits basically is negligible</div><br/><div id="39848337" class="c"><input type="checkbox" id="c-39848337" checked=""/><div class="controls bullet"><span class="by">2099miles</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39844749">parent</a><span>|</span><a href="#39846662">next</a><span>|</span><label class="collapse" for="c-39848337">[-]</label><label class="expand" for="c-39848337">[1 more]</label></div><br/><div class="children"><div class="content">Source? I’ve seen this anecdotally and heard it, but is there a paper you’re referencing?</div><br/></div></div><div id="39846662" class="c"><input type="checkbox" id="c-39846662" checked=""/><div class="controls bullet"><span class="by">K0balt</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39844749">parent</a><span>|</span><a href="#39848337">prev</a><span>|</span><a href="#39843448">next</a><span>|</span><label class="collapse" for="c-39846662">[-]</label><label class="expand" for="c-39846662">[1 more]</label></div><br/><div class="children"><div class="content">I work mostly with mixtral and mistral 7b these days,  but I did work with some 70b models before mistral came out, and I was not impressed with the 4 bit Llama-2 70b.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39843448" class="c"><input type="checkbox" id="c-39843448" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#39838697">parent</a><span>|</span><a href="#39839148">prev</a><span>|</span><a href="#39841293">next</a><span>|</span><label class="collapse" for="c-39843448">[-]</label><label class="expand" for="c-39843448">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m more wondering when we&#x27;ll have algorithms that will &quot;do their best&quot; given the resources they detect.<p>That would be what I call artificial intelligence.<p>Giving up because &quot;out of memory&quot; is not intelligence.</div><br/><div id="39843495" class="c"><input type="checkbox" id="c-39843495" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39843448">parent</a><span>|</span><a href="#39844290">next</a><span>|</span><label class="collapse" for="c-39843495">[-]</label><label class="expand" for="c-39843495">[1 more]</label></div><br/><div class="children"><div class="content">No but some model serving tools like llama.cpp do their best. It&#x27;s just a matter of choosing the right serving tools. And I am not sure LLMs could not optimize their memory layout. Why not? Just let them play with this and learn. You can do pretty amazing things with evolutionary methods where the LLMs are the mutation operator. You evolve a population of solutions. (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2206.08896" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2206.08896</a>)</div><br/></div></div><div id="39844290" class="c"><input type="checkbox" id="c-39844290" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39843448">parent</a><span>|</span><a href="#39843495">prev</a><span>|</span><a href="#39844434">next</a><span>|</span><label class="collapse" for="c-39844290">[-]</label><label class="expand" for="c-39844290">[1 more]</label></div><br/><div class="children"><div class="content">I suppose you could simulate dementia by loading as much of the weights as space permits and then just stopping. Then during inference, replace the missing weights with calls to random(). I&#x27;d actually be interested in seeing the results.</div><br/></div></div><div id="39844434" class="c"><input type="checkbox" id="c-39844434" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#39838697">root</a><span>|</span><a href="#39843448">parent</a><span>|</span><a href="#39844290">prev</a><span>|</span><a href="#39841293">next</a><span>|</span><label class="collapse" for="c-39844434">[-]</label><label class="expand" for="c-39844434">[1 more]</label></div><br/><div class="children"><div class="content">&gt;<i>Giving up because &quot;out of memory&quot; is not intelligence.</i><p>When people can&#x27;t remember the facts&#x2F;theory&#x2F;formulas needed to answer some test question, or can&#x27;t memorize some complicated information because it&#x27;s too much, they usually give up too.<p>So, giving up because of &quot;out of memory&quot; sure sounds like intelligence to me.</div><br/></div></div></div></div></div></div><div id="39841293" class="c"><input type="checkbox" id="c-39841293" checked=""/><div class="controls bullet"><span class="by">hintymad</span><span>|</span><a href="#39838697">prev</a><span>|</span><a href="#39841550">next</a><span>|</span><label class="collapse" for="c-39841293">[-]</label><label class="expand" for="c-39841293">[28 more]</label></div><br/><div class="children"><div class="content">Just curious, what business benefit will Databricks get by spending potentially millions of dollars on an open LLM?</div><br/><div id="39841375" class="c"><input type="checkbox" id="c-39841375" checked=""/><div class="controls bullet"><span class="by">ramoz</span><span>|</span><a href="#39841293">parent</a><span>|</span><a href="#39841395">next</a><span>|</span><label class="collapse" for="c-39841375">[-]</label><label class="expand" for="c-39841375">[17 more]</label></div><br/><div class="children"><div class="content">Their goal is to always drive enterprise business towards consumption.<p>With AI they need to desperately steer the narrative away from API based services (OpenAI).<p>By training LLMs, they build sales artifacts (stories, references, even accelerators with LLMs themselves) to paint the pictures needed to convince their enterprise customer market that Databricks is the platform for enterprise AI. Their blog details how the entire end to end process was done on the platform.<p>In other words, Databricks spent millions as an aid in influencing their customers to do the same (on Databricks).</div><br/><div id="39841577" class="c"><input type="checkbox" id="c-39841577" checked=""/><div class="controls bullet"><span class="by">hintymad</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39841375">parent</a><span>|</span><a href="#39841997">next</a><span>|</span><label class="collapse" for="c-39841577">[-]</label><label class="expand" for="c-39841577">[13 more]</label></div><br/><div class="children"><div class="content">Thanks! Why do they not focus on hosting other open models then? I suspect other models will soon catch up with their advantages in faster inference and better benchmark results. That said, maybe the advantage is aligned interests: they want customers to use their platforms, so they can keep their models open. In contrast, Mistral removed their commitment to open source as they found a potential path to profitability.</div><br/><div id="39842530" class="c"><input type="checkbox" id="c-39842530" checked=""/><div class="controls bullet"><span class="by">cwyers</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39841577">parent</a><span>|</span><a href="#39842219">next</a><span>|</span><label class="collapse" for="c-39842530">[-]</label><label class="expand" for="c-39842530">[3 more]</label></div><br/><div class="children"><div class="content">Commoditize your complements:<p><a href="https:&#x2F;&#x2F;gwern.net&#x2F;complement" rel="nofollow">https:&#x2F;&#x2F;gwern.net&#x2F;complement</a><p>If Databricks makes their money off model serving and doesn&#x27;t care whose model you use, they are incentivized to help the open models be competitive with the closed models they can&#x27;t serve.</div><br/><div id="39847190" class="c"><input type="checkbox" id="c-39847190" checked=""/><div class="controls bullet"><span class="by">youssefabdelm</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39842530">parent</a><span>|</span><a href="#39842219">next</a><span>|</span><label class="collapse" for="c-39847190">[-]</label><label class="expand" for="c-39847190">[2 more]</label></div><br/><div class="children"><div class="content">At this point it&#x27;s a cliché to share this article, as much as I love gwern lol.</div><br/><div id="39847501" class="c"><input type="checkbox" id="c-39847501" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39847190">parent</a><span>|</span><a href="#39842219">next</a><span>|</span><label class="collapse" for="c-39847501">[-]</label><label class="expand" for="c-39847501">[1 more]</label></div><br/><div class="children"><div class="content">There is always the lucky 10k.</div><br/></div></div></div></div></div></div><div id="39842219" class="c"><input type="checkbox" id="c-39842219" checked=""/><div class="controls bullet"><span class="by">Closi</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39841577">parent</a><span>|</span><a href="#39842530">prev</a><span>|</span><a href="#39842241">next</a><span>|</span><label class="collapse" for="c-39842219">[-]</label><label class="expand" for="c-39842219">[4 more]</label></div><br/><div class="children"><div class="content">Demonstrating you can do it yourself shows a level of investment and commitment to AI in your platform that integrating LLAMA does not.<p>And from a corporate perspective, it means that you have in-house capability to work at the cutting-edge of AI to be prepared for whatever comes next.</div><br/><div id="39844595" class="c"><input type="checkbox" id="c-39844595" checked=""/><div class="controls bullet"><span class="by">hintymad</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39842219">parent</a><span>|</span><a href="#39842241">next</a><span>|</span><label class="collapse" for="c-39844595">[-]</label><label class="expand" for="c-39844595">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Demonstrating you can do it yourself shows a level of investment and commitment to AI in your platform that integrating LLAMA does not.<p>I buy this argument. It looks that&#x27;s not what AWS does, though, yet they don&#x27;t have problem attracting LLM users. Maybe AWS already got enough reputation?</div><br/><div id="39845528" class="c"><input type="checkbox" id="c-39845528" checked=""/><div class="controls bullet"><span class="by">rmbyrro</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39844595">parent</a><span>|</span><a href="#39845399">next</a><span>|</span><label class="collapse" for="c-39845528">[-]</label><label class="expand" for="c-39845528">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s easier because 70% of the market already has an AWS account and a sizeable budget allocated to it. The technical team is literally one click away from any AWS service.</div><br/></div></div><div id="39845399" class="c"><input type="checkbox" id="c-39845399" checked=""/><div class="controls bullet"><span class="by">zubairshaik</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39844595">parent</a><span>|</span><a href="#39845528">prev</a><span>|</span><a href="#39842241">next</a><span>|</span><label class="collapse" for="c-39845399">[-]</label><label class="expand" for="c-39845399">[1 more]</label></div><br/><div class="children"><div class="content">I may be misunderstanding, but doesn&#x27;t Amazon have it&#x27;s own models in the form of Amazon Titan[0]? I know they aren&#x27;t competitive in terms of output quality but surely in terms of cost there can be some use cases for them.<p>[0] <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;bedrock&#x2F;titan&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;bedrock&#x2F;titan&#x2F;</a></div><br/></div></div></div></div></div></div><div id="39842241" class="c"><input type="checkbox" id="c-39842241" checked=""/><div class="controls bullet"><span class="by">theturtletalks</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39841577">parent</a><span>|</span><a href="#39842219">prev</a><span>|</span><a href="#39843761">next</a><span>|</span><label class="collapse" for="c-39842241">[-]</label><label class="expand" for="c-39842241">[2 more]</label></div><br/><div class="children"><div class="content">Mistral did what many startups are doing now, leveraging open-source to get traction and then doing a rug-pull. Hell, I&#x27;ve seen many startups be open-source, get contributions, get free press, get into YC and before you know it, the repo is gone.</div><br/><div id="39848322" class="c"><input type="checkbox" id="c-39848322" checked=""/><div class="controls bullet"><span class="by">antupis</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39842241">parent</a><span>|</span><a href="#39843761">next</a><span>|</span><label class="collapse" for="c-39848322">[-]</label><label class="expand" for="c-39848322">[1 more]</label></div><br/><div class="children"><div class="content">Well Databricks is a big company with real cash flow, and Mistral is a startup so there is a kinda big difference here.</div><br/></div></div></div></div><div id="39843761" class="c"><input type="checkbox" id="c-39843761" checked=""/><div class="controls bullet"><span class="by">tartrate</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39841577">parent</a><span>|</span><a href="#39842241">prev</a><span>|</span><a href="#39842283">next</a><span>|</span><label class="collapse" for="c-39843761">[-]</label><label class="expand" for="c-39843761">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Why do they not focus on hosting other open models then?<p>They do host other open models as well (pay-per-token).</div><br/><div id="39845295" class="c"><input type="checkbox" id="c-39845295" checked=""/><div class="controls bullet"><span class="by">bobbruno</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39843761">parent</a><span>|</span><a href="#39842283">next</a><span>|</span><label class="collapse" for="c-39845295">[-]</label><label class="expand" for="c-39845295">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;docs.databricks.com&#x2F;en&#x2F;machine-learning&#x2F;foundation-models&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;docs.databricks.com&#x2F;en&#x2F;machine-learning&#x2F;foundation-m...</a></div><br/></div></div></div></div><div id="39842283" class="c"><input type="checkbox" id="c-39842283" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39841577">parent</a><span>|</span><a href="#39843761">prev</a><span>|</span><a href="#39841997">next</a><span>|</span><label class="collapse" for="c-39842283">[-]</label><label class="expand" for="c-39842283">[1 more]</label></div><br/><div class="children"><div class="content">They do have a solid focus on doing so, it’s just not exclusive.<p><a href="https:&#x2F;&#x2F;www.databricks.com&#x2F;product&#x2F;machine-learning&#x2F;large-language-models-oss-guidance" rel="nofollow">https:&#x2F;&#x2F;www.databricks.com&#x2F;product&#x2F;machine-learning&#x2F;large-la...</a></div><br/></div></div></div></div><div id="39841997" class="c"><input type="checkbox" id="c-39841997" checked=""/><div class="controls bullet"><span class="by">anonymousDan</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39841375">parent</a><span>|</span><a href="#39841577">prev</a><span>|</span><a href="#39841395">next</a><span>|</span><label class="collapse" for="c-39841997">[-]</label><label class="expand" for="c-39841997">[3 more]</label></div><br/><div class="children"><div class="content">Do they use spark for the training?</div><br/><div id="39842256" class="c"><input type="checkbox" id="c-39842256" checked=""/><div class="controls bullet"><span class="by">alexott</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39841997">parent</a><span>|</span><a href="#39841395">next</a><span>|</span><label class="collapse" for="c-39842256">[-]</label><label class="expand" for="c-39842256">[2 more]</label></div><br/><div class="children"><div class="content">Mosaic AI Training (<a href="https:&#x2F;&#x2F;www.databricks.com&#x2F;product&#x2F;machine-learning&#x2F;mosaic-ai-training" rel="nofollow">https:&#x2F;&#x2F;www.databricks.com&#x2F;product&#x2F;machine-learning&#x2F;mosaic-a...</a>) as it&#x27;s mentioned in the announcement blog (<a href="https:&#x2F;&#x2F;www.databricks.com&#x2F;blog&#x2F;announcing-dbrx-new-standard-efficient-open-source-customizable-llms" rel="nofollow">https:&#x2F;&#x2F;www.databricks.com&#x2F;blog&#x2F;announcing-dbrx-new-standard...</a> - it&#x27;s a bit less technical)</div><br/><div id="39845106" class="c"><input type="checkbox" id="c-39845106" checked=""/><div class="controls bullet"><span class="by">anonymousDan</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39842256">parent</a><span>|</span><a href="#39841395">next</a><span>|</span><label class="collapse" for="c-39845106">[-]</label><label class="expand" for="c-39845106">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. Is this open source - i.e. can it be used on my own cluster outside of databricks?</div><br/></div></div></div></div></div></div></div></div><div id="39841395" class="c"><input type="checkbox" id="c-39841395" checked=""/><div class="controls bullet"><span class="by">dhoe</span><span>|</span><a href="#39841293">parent</a><span>|</span><a href="#39841375">prev</a><span>|</span><a href="#39843221">next</a><span>|</span><label class="collapse" for="c-39841395">[-]</label><label class="expand" for="c-39841395">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s an image enhancement measure, if you want. Databricks&#x27; customers mostly use it as an ETL tool, but it benefits them to be perceived as more than that.</div><br/><div id="39843077" class="c"><input type="checkbox" id="c-39843077" checked=""/><div class="controls bullet"><span class="by">spxneo</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39841395">parent</a><span>|</span><a href="#39843221">next</a><span>|</span><label class="collapse" for="c-39843077">[-]</label><label class="expand" for="c-39843077">[2 more]</label></div><br/><div class="children"><div class="content">you can improve your brand for a lot less I just dont understand why they would throw all their chips in a losing race.<p>Azure already runs on-premise if I&#x27;m not mistaken, Claude 3 is out...but DBRX already falls so far behind<p>I just don&#x27;t get it.</div><br/><div id="39848945" class="c"><input type="checkbox" id="c-39848945" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39843077">parent</a><span>|</span><a href="#39843221">next</a><span>|</span><label class="collapse" for="c-39848945">[-]</label><label class="expand" for="c-39848945">[1 more]</label></div><br/><div class="children"><div class="content">A lot of enterprise orgs are convinced of two things:<p>1. They need to train their own LLMs<p>2. They must fine-tune an LLM to make use of this tech<p>Now number (1) is almost entirely false, but there are willing buyers, and DB offers some minimal tools to let them live their lies. DBRX proves that it&#x27;s possible to train an LLM on the DB stack.<p>Number (2) is often true, although I would say that most orgs skip the absolutely essential first step of prompting a powerful foundation model to get a first version of a product done first (and using evals from that prompting to seed evals for fine-tuning). It&#x27;s here where DBRX is much more relevant, because it is by all accounts an extremely capable model for fine-tuning. And since it&#x27;s entirely built by DB, they can offer better support for their customers than they can with Llama or Mistral variants.<p>More broadly, the strategic play is the be the &quot;enterprise AI company&quot;. OpenAI, Anthropic, and Meta are all competing at the consumer level, but nobody&#x27;s really stuck out as the dominant player for the enterprise space. Arguably OpenAI is the most successful, but that&#x27;s less about an enterprise focus and just about being wildly successful generally, and they&#x27;re also still trying to figure out if they want to focus on consumer tech, AGI woo woo stuff, research work, or enterprise stuff. DB also knows that to be an AI company, you also have to be a data company, and they are a data company. So it&#x27;s a natural strategic move for them.</div><br/></div></div></div></div></div></div><div id="39843221" class="c"><input type="checkbox" id="c-39843221" checked=""/><div class="controls bullet"><span class="by">blitzar</span><span>|</span><a href="#39841293">parent</a><span>|</span><a href="#39841395">prev</a><span>|</span><a href="#39842476">next</a><span>|</span><label class="collapse" for="c-39843221">[-]</label><label class="expand" for="c-39843221">[1 more]</label></div><br/><div class="children"><div class="content">An increased valuation at IPO later this year.</div><br/></div></div><div id="39842476" class="c"><input type="checkbox" id="c-39842476" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#39841293">parent</a><span>|</span><a href="#39843221">prev</a><span>|</span><a href="#39841550">next</a><span>|</span><label class="collapse" for="c-39842476">[-]</label><label class="expand" for="c-39842476">[6 more]</label></div><br/><div class="children"><div class="content">Databricks is trying to go all-in on convincing organizations they need to use in-house models, and therefore pay they to provide LLMOps.<p>They&#x27;re so far into this that their CTO co-authored a borderline dishonest study which got a ton of traction last summer trying to discredit GPT-4: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2307.09009.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2307.09009.pdf</a></div><br/><div id="39842610" class="c"><input type="checkbox" id="c-39842610" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39842476">parent</a><span>|</span><a href="#39843131">next</a><span>|</span><label class="collapse" for="c-39842610">[-]</label><label class="expand" for="c-39842610">[3 more]</label></div><br/><div class="children"><div class="content">I can see a business model for inhouse LLM models: Training a model on the knowledge about their products and then somehow getting that knowledge into a generally available LLM platform.<p>I recently tried to ask Google to explain to me how to delete sender-recorded voice-message I had created from WhatsApp. I got totally erroneous results back. Maybe it was because that is a rather new feature in WhatsApp.<p>It would be in the interests of WhatsApp to get accurate answers about it into Google&#x27;s LLM. So Google might make a deal with them requiring WhatsApp to pay Google for regular updates about the up-to-date features of What&#x27;s App into Google. The owner of What&#x27;s App Meta of course is competition to Google so Google may not much care of providing up to date info about WhatsApp in their LLM. But they might if Meta paid them.</div><br/><div id="39843089" class="c"><input type="checkbox" id="c-39843089" checked=""/><div class="controls bullet"><span class="by">spxneo</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39842610">parent</a><span>|</span><a href="#39843350">next</a><span>|</span><label class="collapse" for="c-39843089">[-]</label><label class="expand" for="c-39843089">[1 more]</label></div><br/><div class="children"><div class="content">Businesses are already using Azure GPT4 on-premise I believe with good feedback<p>DBRX does not compete with GPT4 or even Claude 3.</div><br/></div></div><div id="39843350" class="c"><input type="checkbox" id="c-39843350" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39842610">parent</a><span>|</span><a href="#39843089">prev</a><span>|</span><a href="#39843131">next</a><span>|</span><label class="collapse" for="c-39843350">[-]</label><label class="expand" for="c-39843350">[1 more]</label></div><br/><div class="children"><div class="content">Pretraining on internal knowledge will be incredibly inefficient for most companies.<p>Finetuning makes sense for things like embeddings (improve RAG by defining domain specific embeddings) but doesn&#x27;t do anything useful for facts</div><br/></div></div></div></div><div id="39843131" class="c"><input type="checkbox" id="c-39843131" checked=""/><div class="controls bullet"><span class="by">omeze</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39842476">parent</a><span>|</span><a href="#39842610">prev</a><span>|</span><a href="#39841550">next</a><span>|</span><label class="collapse" for="c-39843131">[-]</label><label class="expand" for="c-39843131">[2 more]</label></div><br/><div class="children"><div class="content">What does borderline dishonest mean? I only read the abstract and it seems like such an obvious point I dont see how its contentious</div><br/><div id="39843845" class="c"><input type="checkbox" id="c-39843845" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#39841293">root</a><span>|</span><a href="#39843131">parent</a><span>|</span><a href="#39841550">next</a><span>|</span><label class="collapse" for="c-39843845">[-]</label><label class="expand" for="c-39843845">[1 more]</label></div><br/><div class="children"><div class="content">The regression came from poorly parsing the results. I came the conclusion independently, but here&#x27;s another more detailed takedown: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;153xee8&#x2F;has_chatgpt_gotten_dumber_a_response_to_the&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;153xee8&#x2F;has_chatgp...</a><p>Given the conflict of interest and background of Zaharia, it&#x27;s hard to imagine such an immediately obvious source of error wasn&#x27;t caught.</div><br/></div></div></div></div></div></div></div></div><div id="39841550" class="c"><input type="checkbox" id="c-39841550" checked=""/><div class="controls bullet"><span class="by">briandw</span><span>|</span><a href="#39841293">prev</a><span>|</span><a href="#39839260">next</a><span>|</span><label class="collapse" for="c-39841550">[-]</label><label class="expand" for="c-39841550">[5 more]</label></div><br/><div class="children"><div class="content">Worse than the chart crime of truncating the y axis is putting LLaMa2&#x27;s Human Eval scores on there and not comparing it to Code Llama Instruct 70b. DBRX still beats Code Llama Instruct&#x27;s 67.8 but not by that much.</div><br/><div id="39842531" class="c"><input type="checkbox" id="c-39842531" checked=""/><div class="controls bullet"><span class="by">jjgo</span><span>|</span><a href="#39841550">parent</a><span>|</span><a href="#39846736">next</a><span>|</span><label class="collapse" for="c-39842531">[-]</label><label class="expand" for="c-39842531">[2 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;On HumanEval, DBRX Instruct even surpasses CodeLLaMA-70B Instruct, a model built explicitly for programming, despite the fact that DBRX Instruct is designed for general-purpose use (70.1% vs. 67.8% on HumanEval as reported by Meta in the CodeLLaMA blog).&quot;<p>To be fair, they do compare to it in the main body of the blog. It&#x27;s just probably misleading to compare to CodeLLaMA on non coding benchmarks.</div><br/><div id="39843872" class="c"><input type="checkbox" id="c-39843872" checked=""/><div class="controls bullet"><span class="by">tartrate</span><span>|</span><a href="#39841550">root</a><span>|</span><a href="#39842531">parent</a><span>|</span><a href="#39846736">next</a><span>|</span><label class="collapse" for="c-39843872">[-]</label><label class="expand" for="c-39843872">[1 more]</label></div><br/><div class="children"><div class="content">Which non-coding benchmark?</div><br/></div></div></div></div><div id="39846736" class="c"><input type="checkbox" id="c-39846736" checked=""/><div class="controls bullet"><span class="by">panarky</span><span>|</span><a href="#39841550">parent</a><span>|</span><a href="#39842531">prev</a><span>|</span><a href="#39839260">next</a><span>|</span><label class="collapse" for="c-39846736">[-]</label><label class="expand" for="c-39846736">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt; chart crime of truncating the y axis</i><p>If you chart the temperature of the ocean do you keep the y-axis anchored at zero Kelvin?</div><br/><div id="39847334" class="c"><input type="checkbox" id="c-39847334" checked=""/><div class="controls bullet"><span class="by">d-z-m</span><span>|</span><a href="#39841550">root</a><span>|</span><a href="#39846736">parent</a><span>|</span><a href="#39839260">next</a><span>|</span><label class="collapse" for="c-39847334">[-]</label><label class="expand" for="c-39847334">[1 more]</label></div><br/><div class="children"><div class="content">If you chart the temperature of the ocean are you measuring it in Kelvin?</div><br/></div></div></div></div></div></div><div id="39839260" class="c"><input type="checkbox" id="c-39839260" checked=""/><div class="controls bullet"><span class="by">XCSme</span><span>|</span><a href="#39841550">prev</a><span>|</span><a href="#39845190">next</a><span>|</span><label class="collapse" for="c-39839260">[-]</label><label class="expand" for="c-39839260">[43 more]</label></div><br/><div class="children"><div class="content">I am planning to buy a new GPU.<p>If the GPU has 16GB of VRAM, and the model is 70GB, can it still run well?
Also, does it run considerably better than on a GPU with 12GB of VRAM?<p>I run Ollama locally, mixtral works well (7B, 3.4GB) on a 1080ti, but the 24.6GB version is a bit slow (still usable, but has a noticeable start-up time).</div><br/><div id="39839606" class="c"><input type="checkbox" id="c-39839606" checked=""/><div class="controls bullet"><span class="by">PheonixPharts</span><span>|</span><a href="#39839260">parent</a><span>|</span><a href="#39839582">next</a><span>|</span><label class="collapse" for="c-39839606">[-]</label><label class="expand" for="c-39839606">[18 more]</label></div><br/><div class="children"><div class="content">While GPUs are still the kings of speed, if you are worried about VRAM I do recommend a maxed out Mac Studio.<p>Llama.cpp + quantized models on Apple Silicon is an incredible experience, and having 192 GB of unified memory to work with means you can run models that just aren&#x27;t feasible on a home GPU setup.<p>It really boils down to what type of local development you want to do. I&#x27;m mostly experimenting with things where the time to response isn&#x27;t <i>that</i> big of a deal, and not fine-tuning the models locally (which I also believe GPUs are still superior for). But if your concern is &quot;how big of a model can I run&quot; vs &quot;Can I have close to real time chat&quot;, the unified memory approach is superior.</div><br/><div id="39842151" class="c"><input type="checkbox" id="c-39842151" checked=""/><div class="controls bullet"><span class="by">bevekspldnw</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839606">parent</a><span>|</span><a href="#39840513">next</a><span>|</span><label class="collapse" for="c-39842151">[-]</label><label class="expand" for="c-39842151">[6 more]</label></div><br/><div class="children"><div class="content">I had gone the Mac Studio route initially, but I ended up with getting an A6000 for about the same price as a Mac and putting that in a Linux server onder my desk. Ollama makes it dead simple to serve it over my local network, so I can be on my M1 Air and using it no differently than if on my laptop. The difference is that the A6000 absolutely smokes the Mac.</div><br/><div id="39845214" class="c"><input type="checkbox" id="c-39845214" checked=""/><div class="controls bullet"><span class="by">c1b</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39842151">parent</a><span>|</span><a href="#39843040">next</a><span>|</span><label class="collapse" for="c-39845214">[-]</label><label class="expand" for="c-39845214">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The difference is that the A6000 absolutely smokes the Mac.<p>Memory Bandwidth : Mac Studio wins (about the same @ ~800)<p>VRAM : Mac Studio wins (4x more)<p>TFLOPs: A6000 wins (32 vs 38)</div><br/><div id="39845326" class="c"><input type="checkbox" id="c-39845326" checked=""/><div class="controls bullet"><span class="by">bevekspldnw</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39845214">parent</a><span>|</span><a href="#39843040">next</a><span>|</span><label class="collapse" for="c-39845326">[-]</label><label class="expand" for="c-39845326">[1 more]</label></div><br/><div class="children"><div class="content">VRAM in excess of the model one is using isn’t useful per se. My use cases require high throughput, and on many tasks the A6000 executes inference at 2x speed.</div><br/></div></div></div></div><div id="39843040" class="c"><input type="checkbox" id="c-39843040" checked=""/><div class="controls bullet"><span class="by">starik36</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39842151">parent</a><span>|</span><a href="#39845214">prev</a><span>|</span><a href="#39840513">next</a><span>|</span><label class="collapse" for="c-39843040">[-]</label><label class="expand" for="c-39843040">[3 more]</label></div><br/><div class="children"><div class="content">Wow, that is a lot of money ($4400 on Amazon) to throw at this problem.  I am curious, what was the purpose that compelled you to spend this (for the home network, I assume) amount of money.</div><br/><div id="39844365" class="c"><input type="checkbox" id="c-39844365" checked=""/><div class="controls bullet"><span class="by">bevekspldnw</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39843040">parent</a><span>|</span><a href="#39840513">next</a><span>|</span><label class="collapse" for="c-39844365">[-]</label><label class="expand" for="c-39844365">[2 more]</label></div><br/><div class="children"><div class="content">Large scale document classification tasks in very ambiguous contexts. A lot of my work goes into using big models to generate training data for smaller models.<p>I have multiple millions of documents so GPT is cost prohibitive, and too slow. My tools of choice tend to be a first pass with Mistral to check task performance and if lacking using Mixtral.<p>Often I find with a good prompt Mistral will work as well as Mixtral and is about 10x faster.<p>I’m on my “home” network, but it’s a “home office” for my startup.</div><br/><div id="39848003" class="c"><input type="checkbox" id="c-39848003" checked=""/><div class="controls bullet"><span class="by">Datagenerator</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39844365">parent</a><span>|</span><a href="#39840513">next</a><span>|</span><label class="collapse" for="c-39848003">[-]</label><label class="expand" for="c-39848003">[1 more]</label></div><br/><div class="children"><div class="content">Interesting I have the same task, can you share your tools? My goal is to detect if documents contain GDPR sensitive parts or are copies of official documents like ID&#x27;s and driving licenses etc - would be great to reuse your work!</div><br/></div></div></div></div></div></div></div></div><div id="39840513" class="c"><input type="checkbox" id="c-39840513" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839606">parent</a><span>|</span><a href="#39842151">prev</a><span>|</span><a href="#39840276">next</a><span>|</span><label class="collapse" for="c-39840513">[-]</label><label class="expand" for="c-39840513">[2 more]</label></div><br/><div class="children"><div class="content">I know the M?-pro and ultra variants are multiple standard M?’s in a single package. But so the CPUs and GPUs share a die (like a single 4 p-core CPU 10 GPU core is what come in the die, and the more exotic variants are just a result of LEGO-ing out those guys and disabling some cores for market segmentation or because they had defects?)<p>I guess I’m wondering if they technically could throw in their gauntlet and compete with nvidia by doing something like a 4 CPU&#x2F;80 GPU&#x2F;256 GB chip, if they wanted to. Seems like it’d be a really appealing ML machine. (I could also see it being technically possible but Apple just deciding that’s pointlessly niche for them).</div><br/><div id="39845198" class="c"><input type="checkbox" id="c-39845198" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39840513">parent</a><span>|</span><a href="#39840276">next</a><span>|</span><label class="collapse" for="c-39845198">[-]</label><label class="expand" for="c-39845198">[1 more]</label></div><br/><div class="children"><div class="content">Ultra is the only one that&#x27;s made from two smaller SoCs.</div><br/></div></div></div></div><div id="39840276" class="c"><input type="checkbox" id="c-39840276" checked=""/><div class="controls bullet"><span class="by">XCSme</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839606">parent</a><span>|</span><a href="#39840513">prev</a><span>|</span><a href="#39843244">next</a><span>|</span><label class="collapse" for="c-39840276">[-]</label><label class="expand" for="c-39840276">[6 more]</label></div><br/><div class="children"><div class="content">I already have 128GB of RAM (DDR4), and was wondering if upgrading from a 1080ti (12GB) to a 4070ti super (16GB), would make a big difference.<p>I assume the FP32 and FP16 operations are already a huge improvement, but also the 33% increased VRAM might lead to fewer swaps between VRAM and RAM.</div><br/><div id="39840786" class="c"><input type="checkbox" id="c-39840786" checked=""/><div class="controls bullet"><span class="by">loudmax</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39840276">parent</a><span>|</span><a href="#39840494">next</a><span>|</span><label class="collapse" for="c-39840786">[-]</label><label class="expand" for="c-39840786">[3 more]</label></div><br/><div class="children"><div class="content">I have an RTX 3080 with 10GB of VRAM.  I&#x27;m able to run models larger than 10GB using llama.cpp and offloading to the GPU as much as can fit into VRAM.  The remainder of the model runs on CPU + regular RAM.<p>The `nvtop` command displays a nice graph of how much GPU processing and VRAM is being consumed.  When I run a model that fits entirely into VRAM, say Mistral 7B, nvtop shows the GPU processing running at full tilt.  When I run a model bigger than 10GB, say Mixtral or Llama 70B with GPU offloading, my CPU will run full tilt and the VRAM is full, but the GPU processor itself will operate far below full capacity.<p>I think what is happening here is that the model layers that are offloaded to the GPU do their processing, then the GPU spends most of the time waiting for the much slower CPU to do its thing.  So in my case, I think upgrading to a faster GPU would make little to no difference when running the bigger models, so long as the VRAM is capped at the same level.  But upgrading to a GPU with more VRAM, even a slower GPU, should make the overall speed faster for bigger models because the GPU would spend less time waiting for the CPU.  (Of course, models that fit entirely into VRAM will run faster on a faster GPU).<p>In my case, the amount of VRAM absolutely seems to be the performance bottleneck.  If I do upgrade, it will be for a GPU with more VRAM, not necessarily a GPU with more processing power.  That has been my experience running llama.cpp.  YMMV.</div><br/><div id="39843097" class="c"><input type="checkbox" id="c-39843097" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39840786">parent</a><span>|</span><a href="#39840494">next</a><span>|</span><label class="collapse" for="c-39843097">[-]</label><label class="expand" for="c-39843097">[2 more]</label></div><br/><div class="children"><div class="content">How&#x27;s your performance on the 70b parameter llama series?<p>Any good writeups of the offloading that you found?</div><br/><div id="39844293" class="c"><input type="checkbox" id="c-39844293" checked=""/><div class="controls bullet"><span class="by">loudmax</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39843097">parent</a><span>|</span><a href="#39840494">next</a><span>|</span><label class="collapse" for="c-39844293">[-]</label><label class="expand" for="c-39844293">[1 more]</label></div><br/><div class="children"><div class="content">Performance of 70b models is like 1 token every few seconds.  And that&#x27;s fitting the whole model into system RAM, not swap.  It&#x27;s interesting because some of the larger models are quite good, but too annoyingly slow to be practical for most use cases.<p>The Mixtral models run surprisingly well.  They can run better than 1 token per second, depending on quantization.  Still slow, but approaching a more practical level of usefulness.<p>Though if you&#x27;re planning on accomplishing real work with LLMs, the practical solution for most people is probably to rent a GPU in the cloud.</div><br/></div></div></div></div></div></div><div id="39840494" class="c"><input type="checkbox" id="c-39840494" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39840276">parent</a><span>|</span><a href="#39840786">prev</a><span>|</span><a href="#39843244">next</a><span>|</span><label class="collapse" for="c-39840494">[-]</label><label class="expand" for="c-39840494">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s system memory, not unified memory.  Unified means that all or most of it is going to be directly available to the Apple Silicon GPU.</div><br/><div id="39843023" class="c"><input type="checkbox" id="c-39843023" checked=""/><div class="controls bullet"><span class="by">giancarlostoro</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39840494">parent</a><span>|</span><a href="#39843244">next</a><span>|</span><label class="collapse" for="c-39843023">[-]</label><label class="expand" for="c-39843023">[1 more]</label></div><br/><div class="children"><div class="content">This is the key factor here. I have a 3080, with 16GB of Memory, but still have to run some models on CPU since the memory is not unified at all.</div><br/></div></div></div></div></div></div><div id="39843244" class="c"><input type="checkbox" id="c-39843244" checked=""/><div class="controls bullet"><span class="by">spxneo</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839606">parent</a><span>|</span><a href="#39840276">prev</a><span>|</span><a href="#39843482">next</a><span>|</span><label class="collapse" for="c-39843244">[-]</label><label class="expand" for="c-39843244">[1 more]</label></div><br/><div class="children"><div class="content">Aren&#x27;t quantized models different models outright requiring a new evaluation to know the deviation in performance? Or are they &quot;good enough&quot; in that the benefits outweigh the deviation?<p>I&#x27;m on the fence about whether to spend 5 digits or 4 digits. Do I go the Mac Studio route or GPUs? What are the pros and cons?</div><br/></div></div><div id="39843482" class="c"><input type="checkbox" id="c-39843482" checked=""/><div class="controls bullet"><span class="by">brandall10</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839606">parent</a><span>|</span><a href="#39843244">prev</a><span>|</span><a href="#39842644">next</a><span>|</span><label class="collapse" for="c-39843482">[-]</label><label class="expand" for="c-39843482">[1 more]</label></div><br/><div class="children"><div class="content">Wait for the M3 Ultra and it will be 256GB and markedly faster.</div><br/></div></div><div id="39842644" class="c"><input type="checkbox" id="c-39842644" checked=""/><div class="controls bullet"><span class="by">purpleblue</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839606">parent</a><span>|</span><a href="#39843482">prev</a><span>|</span><a href="#39839582">next</a><span>|</span><label class="collapse" for="c-39842644">[-]</label><label class="expand" for="c-39842644">[1 more]</label></div><br/><div class="children"><div class="content">Aren&#x27;t the Macs good for inference but not for training or fine tuning?</div><br/></div></div></div></div><div id="39839582" class="c"><input type="checkbox" id="c-39839582" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#39839260">parent</a><span>|</span><a href="#39839606">prev</a><span>|</span><a href="#39842541">next</a><span>|</span><label class="collapse" for="c-39839582">[-]</label><label class="expand" for="c-39839582">[11 more]</label></div><br/><div class="children"><div class="content">&gt;If the GPU has 16GB of VRAM, and the model is 70GB, can it still run well? Also, does it run considerably better than on a GPU with 12GB of VRAM?<p>No, it can&#x27;t run at all.<p>&gt;I run Ollama locally, mixtral works well (7B, 3.4GB) on a 1080ti, but the 24.6GB version is a bit slow (still usable, but has a noticeable start-up time).<p>That is not mixtral, that is mistral 7b. The 1080ti is slower than running inference on current generation threadripper cpus.</div><br/><div id="39840222" class="c"><input type="checkbox" id="c-39840222" checked=""/><div class="controls bullet"><span class="by">XCSme</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839582">parent</a><span>|</span><a href="#39840187">next</a><span>|</span><label class="collapse" for="c-39840222">[-]</label><label class="expand" for="c-39840222">[9 more]</label></div><br/><div class="children"><div class="content">&gt; No, it can&#x27;t run at all.<p><a href="https:&#x2F;&#x2F;s3.amazonaws.com&#x2F;i.snag.gy&#x2F;ae82Ym.jpg" rel="nofollow">https:&#x2F;&#x2F;s3.amazonaws.com&#x2F;i.snag.gy&#x2F;ae82Ym.jpg</a><p>EDIT: This was ran on a 1080ti + 5900x. Initial generation takes around 10-30seconds (like it has to upload the model to GPU), but then it starts answering immediately, at around 3 words per second.</div><br/><div id="39840734" class="c"><input type="checkbox" id="c-39840734" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39840222">parent</a><span>|</span><a href="#39843275">next</a><span>|</span><label class="collapse" for="c-39840734">[-]</label><label class="expand" for="c-39840734">[2 more]</label></div><br/><div class="children"><div class="content">Did you check your GPU utilization?<p>Typically when it runs that way it runs on the CPU, not the GPU.<p>Are you sure you&#x27;re actually offloading any work to the GPU?<p>At least with llama.cpp, there is no &#x27;partially put a layer&#x27; into the GPU. Either you do, or you don&#x27;t. You pick the number of layers. If the model is too big, the layers won&#x27;t fit and it can&#x27;t run at all.<p>The llama.cpp `main` executable will tell you in it&#x27;s debug information when you use the -ngl flag; see <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;blob&#x2F;master&#x2F;examples&#x2F;main&#x2F;README.md#additional-options">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;blob&#x2F;master&#x2F;examples&#x2F;...</a><p>It&#x27;s also possible you&#x27;re running (eg. if you&#x27;re using ollama) and quantized version of the model which reduces the memory requirements and quality of the model outputs.</div><br/><div id="39840989" class="c"><input type="checkbox" id="c-39840989" checked=""/><div class="controls bullet"><span class="by">XCSme</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39840734">parent</a><span>|</span><a href="#39843275">next</a><span>|</span><label class="collapse" for="c-39840989">[-]</label><label class="expand" for="c-39840989">[1 more]</label></div><br/><div class="children"><div class="content">I have to check, something does indeed seem weird, especially with the PC freezing like that. Maybe it runs on the CPU.<p>&gt; quantized version
Yes, it is 4bit quantized, but still has 24.6GB</div><br/></div></div></div></div><div id="39843275" class="c"><input type="checkbox" id="c-39843275" checked=""/><div class="controls bullet"><span class="by">spxneo</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39840222">parent</a><span>|</span><a href="#39840734">prev</a><span>|</span><a href="#39845543">next</a><span>|</span><label class="collapse" for="c-39843275">[-]</label><label class="expand" for="c-39843275">[5 more]</label></div><br/><div class="children"><div class="content">this is some new flex to debate online: copying and pasting the other sides argument and waiting for your local LLM to explain why they are wrong.<p>how much is your hardware at today&#x27;s value? what are the specs? that is impressive even though its 3 words per second. if you want to bump it up to 30, do you then 10x your current hardware cost?</div><br/><div id="39843663" class="c"><input type="checkbox" id="c-39843663" checked=""/><div class="controls bullet"><span class="by">XCSme</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39843275">parent</a><span>|</span><a href="#39845543">next</a><span>|</span><label class="collapse" for="c-39843663">[-]</label><label class="expand" for="c-39843663">[4 more]</label></div><br/><div class="children"><div class="content">That question was just an example (Lorem ipsum), it was easy to copy paste to demo the local LLM, I didn&#x27;t intend to provide more context to the discussion.<p>I ordered a 2nd 3090, which has 24GB VRAM. Funny how it was $2.6k 3 years ago and now is $600.<p>You can probuild a decent AI local machine for around $1000.</div><br/><div id="39843742" class="c"><input type="checkbox" id="c-39843742" checked=""/><div class="controls bullet"><span class="by">spxneo</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39843663">parent</a><span>|</span><a href="#39847865">next</a><span>|</span><label class="collapse" for="c-39843742">[-]</label><label class="expand" for="c-39843742">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;howmuch.one&#x2F;product&#x2F;average-nvidia-geforce-rtx-3090-24gb&#x2F;price-history" rel="nofollow">https:&#x2F;&#x2F;howmuch.one&#x2F;product&#x2F;average-nvidia-geforce-rtx-3090-...</a> you are right there is a huge drop in price</div><br/><div id="39845896" class="c"><input type="checkbox" id="c-39845896" checked=""/><div class="controls bullet"><span class="by">XCSme</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39843742">parent</a><span>|</span><a href="#39847865">next</a><span>|</span><label class="collapse" for="c-39845896">[-]</label><label class="expand" for="c-39845896">[1 more]</label></div><br/><div class="children"><div class="content">New it&#x27;s hard to find, but the 2nd hand market is filled with them.</div><br/></div></div></div></div><div id="39847865" class="c"><input type="checkbox" id="c-39847865" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39843663">parent</a><span>|</span><a href="#39843742">prev</a><span>|</span><a href="#39845543">next</a><span>|</span><label class="collapse" for="c-39847865">[-]</label><label class="expand" for="c-39847865">[1 more]</label></div><br/><div class="children"><div class="content">Where are you seeing 24GB 3090s for $600?</div><br/></div></div></div></div></div></div><div id="39845543" class="c"><input type="checkbox" id="c-39845543" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39840222">parent</a><span>|</span><a href="#39843275">prev</a><span>|</span><a href="#39840187">next</a><span>|</span><label class="collapse" for="c-39845543">[-]</label><label class="expand" for="c-39845543">[1 more]</label></div><br/><div class="children"><div class="content">Congratulations on using CPU inference.</div><br/></div></div></div></div><div id="39840187" class="c"><input type="checkbox" id="c-39840187" checked=""/><div class="controls bullet"><span class="by">XCSme</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839582">parent</a><span>|</span><a href="#39840222">prev</a><span>|</span><a href="#39842541">next</a><span>|</span><label class="collapse" for="c-39840187">[-]</label><label class="expand" for="c-39840187">[1 more]</label></div><br/><div class="children"><div class="content">I have those:<p>dolphin-mixtral:latest (24.6GB)
mistral:latest (3.8GB)<p>The CPU is 5900x.</div><br/></div></div></div></div><div id="39842541" class="c"><input type="checkbox" id="c-39842541" checked=""/><div class="controls bullet"><span class="by">lxe</span><span>|</span><a href="#39839260">parent</a><span>|</span><a href="#39839582">prev</a><span>|</span><a href="#39839491">next</a><span>|</span><label class="collapse" for="c-39842541">[-]</label><label class="expand" for="c-39842541">[1 more]</label></div><br/><div class="children"><div class="content">Get 2 pre-owned 3090s. You will easily be able to run 70b or even 120b quantized models.</div><br/></div></div><div id="39839491" class="c"><input type="checkbox" id="c-39839491" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#39839260">parent</a><span>|</span><a href="#39842541">prev</a><span>|</span><a href="#39845190">next</a><span>|</span><label class="collapse" for="c-39839491">[-]</label><label class="expand" for="c-39839491">[12 more]</label></div><br/><div class="children"><div class="content">&gt; mixtral works well<p>Do you mean mistral?<p>mixtral is 8x7B and requires like 100GB of RAM<p>Edit: (without quant as others have pointed out) can definitely be lower, but haven&#x27;t heard of a 3.4GB version</div><br/><div id="39839526" class="c"><input type="checkbox" id="c-39839526" checked=""/><div class="controls bullet"><span class="by">kwerk</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839491">parent</a><span>|</span><a href="#39840250">next</a><span>|</span><label class="collapse" for="c-39839526">[-]</label><label class="expand" for="c-39839526">[2 more]</label></div><br/><div class="children"><div class="content">I have two 3090s and it runs fine with `ollama run mixtral`. Although OP definitely meant mistral with the 7B note</div><br/><div id="39839925" class="c"><input type="checkbox" id="c-39839925" checked=""/><div class="controls bullet"><span class="by">jsight</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839526">parent</a><span>|</span><a href="#39840250">next</a><span>|</span><label class="collapse" for="c-39839925">[-]</label><label class="expand" for="c-39839925">[1 more]</label></div><br/><div class="children"><div class="content">ollama run mixtral will default to the quantized version (4bit IIRC). I&#x27;d guess this is why it can fit with two 3090s.</div><br/></div></div></div></div><div id="39840250" class="c"><input type="checkbox" id="c-39840250" checked=""/><div class="controls bullet"><span class="by">XCSme</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839491">parent</a><span>|</span><a href="#39839526">prev</a><span>|</span><a href="#39841204">next</a><span>|</span><label class="collapse" for="c-39840250">[-]</label><label class="expand" for="c-39840250">[3 more]</label></div><br/><div class="children"><div class="content">I have 128GB, but something is weird with Ollama. Even though for the Ollama Docker I only allow 90GB, it ends up using 128GB&#x2F;128GB, so the system because very slow (mouse freezes).</div><br/><div id="39844257" class="c"><input type="checkbox" id="c-39844257" checked=""/><div class="controls bullet"><span class="by">InitEnabler</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39840250">parent</a><span>|</span><a href="#39841204">next</a><span>|</span><label class="collapse" for="c-39844257">[-]</label><label class="expand" for="c-39844257">[2 more]</label></div><br/><div class="children"><div class="content">What docker flags are you running?</div><br/><div id="39845903" class="c"><input type="checkbox" id="c-39845903" checked=""/><div class="controls bullet"><span class="by">XCSme</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39844257">parent</a><span>|</span><a href="#39841204">next</a><span>|</span><label class="collapse" for="c-39845903">[-]</label><label class="expand" for="c-39845903">[1 more]</label></div><br/><div class="children"><div class="content">None? The default ones from their docs.<p>The Docker also shows minimal usage for the ollama server which is also strange.</div><br/></div></div></div></div></div></div><div id="39841204" class="c"><input type="checkbox" id="c-39841204" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839491">parent</a><span>|</span><a href="#39840250">prev</a><span>|</span><a href="#39840192">next</a><span>|</span><label class="collapse" for="c-39841204">[-]</label><label class="expand" for="c-39841204">[1 more]</label></div><br/><div class="children"><div class="content">The smaller quants still require a 24gb card. 16 might work but doubt it</div><br/></div></div><div id="39840192" class="c"><input type="checkbox" id="c-39840192" checked=""/><div class="controls bullet"><span class="by">XCSme</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839491">parent</a><span>|</span><a href="#39841204">prev</a><span>|</span><a href="#39839598">next</a><span>|</span><label class="collapse" for="c-39840192">[-]</label><label class="expand" for="c-39840192">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, it was from memory.<p>I have those models in Ollama:<p>I have those:<p>dolphin-mixtral:latest (24.6GB)
mistral:latest (3.8GB)</div><br/></div></div><div id="39839598" class="c"><input type="checkbox" id="c-39839598" checked=""/><div class="controls bullet"><span class="by">ranger_danger</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839491">parent</a><span>|</span><a href="#39840192">prev</a><span>|</span><a href="#39839797">next</a><span>|</span><label class="collapse" for="c-39839598">[-]</label><label class="expand" for="c-39839598">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using mixtral-8x7b-v0.1.Q4_K_M.gguf with llama.cpp and it only requires 25GB.</div><br/></div></div><div id="39839797" class="c"><input type="checkbox" id="c-39839797" checked=""/><div class="controls bullet"><span class="by">chpatrick</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839491">parent</a><span>|</span><a href="#39839598">prev</a><span>|</span><a href="#39839589">next</a><span>|</span><label class="collapse" for="c-39839797">[-]</label><label class="expand" for="c-39839797">[1 more]</label></div><br/><div class="children"><div class="content">The quantized one works fine on my 24GB 3090.</div><br/></div></div><div id="39839589" class="c"><input type="checkbox" id="c-39839589" checked=""/><div class="controls bullet"><span class="by">K0balt</span><span>|</span><a href="#39839260">root</a><span>|</span><a href="#39839491">parent</a><span>|</span><a href="#39839797">prev</a><span>|</span><a href="#39845190">next</a><span>|</span><label class="collapse" for="c-39839589">[-]</label><label class="expand" for="c-39839589">[2 more]</label></div><br/><div class="children"><div class="content">I run mixtral 6 bit quant very happily on my MacBook with 64 gb.</div><br/></div></div></div></div></div></div><div id="39845190" class="c"><input type="checkbox" id="c-39845190" checked=""/><div class="controls bullet"><span class="by">underlines</span><span>|</span><a href="#39839260">prev</a><span>|</span><a href="#39843905">next</a><span>|</span><label class="collapse" for="c-39845190">[-]</label><label class="expand" for="c-39845190">[1 more]</label></div><br/><div class="children"><div class="content">Waiting for Mixed Quantization with MQQ and MoE Offloading [1]. With that I was able to run Mistral 8x7B on my 10 GB VRAM rtx3080... This should work for DBRX and should shave off a ton of VRAM requirement.<p>1. <a href="https:&#x2F;&#x2F;github.com&#x2F;dvmazur&#x2F;mixtral-offloading?tab=readme-ov-file">https:&#x2F;&#x2F;github.com&#x2F;dvmazur&#x2F;mixtral-offloading?tab=readme-ov-...</a></div><br/></div></div><div id="39843905" class="c"><input type="checkbox" id="c-39843905" checked=""/><div class="controls bullet"><span class="by">ianbutler</span><span>|</span><a href="#39845190">prev</a><span>|</span><a href="#39840349">next</a><span>|</span><label class="collapse" for="c-39843905">[-]</label><label class="expand" for="c-39843905">[4 more]</label></div><br/><div class="children"><div class="content">The approval on the base model is not feeling very open. Plenty of people still waiting on a chance to download it, where as the instruct model was an instant approval. The base model is more interesting to me for finetuning.</div><br/><div id="39847716" class="c"><input type="checkbox" id="c-39847716" checked=""/><div class="controls bullet"><span class="by">Chamix</span><span>|</span><a href="#39843905">parent</a><span>|</span><a href="#39845372">next</a><span>|</span><label class="collapse" for="c-39847716">[-]</label><label class="expand" for="c-39847716">[1 more]</label></div><br/><div class="children"><div class="content">4chan already has a torrent out, of course.</div><br/></div></div><div id="39845372" class="c"><input type="checkbox" id="c-39845372" checked=""/><div class="controls bullet"><span class="by">blueblimp</span><span>|</span><a href="#39843905">parent</a><span>|</span><a href="#39847716">prev</a><span>|</span><a href="#39840349">next</a><span>|</span><label class="collapse" for="c-39845372">[-]</label><label class="expand" for="c-39845372">[2 more]</label></div><br/><div class="children"><div class="content">The license allows to reproduce&#x2F;distribute&#x2F;copy, so I&#x27;m a little surprised there&#x27;s an approval process at all.</div><br/><div id="39846454" class="c"><input type="checkbox" id="c-39846454" checked=""/><div class="controls bullet"><span class="by">ianbutler</span><span>|</span><a href="#39843905">root</a><span>|</span><a href="#39845372">parent</a><span>|</span><a href="#39840349">next</a><span>|</span><label class="collapse" for="c-39846454">[-]</label><label class="expand" for="c-39846454">[1 more]</label></div><br/><div class="children"><div class="content">Yeah it&#x27;s kind of weird, I&#x27;ll assume for now they&#x27;re just busy, but I&#x27;d be lying if my gut didn&#x27;t immediately say it&#x27;s kind of sketchy.</div><br/></div></div></div></div></div></div><div id="39840349" class="c"><input type="checkbox" id="c-39840349" checked=""/><div class="controls bullet"><span class="by">emmender2</span><span>|</span><a href="#39843905">prev</a><span>|</span><a href="#39844063">next</a><span>|</span><label class="collapse" for="c-39840349">[-]</label><label class="expand" for="c-39840349">[40 more]</label></div><br/><div class="children"><div class="content">this proves that all llm models converge to a certain point when trained on the same data. ie, there is really no differentiation between one model or the other.<p>Claims about out-performance on tasks are just that, claims. the next iteration of llama or mixtral will converge.<p>LLMs seem to evolve like linux&#x2F;windows or ios&#x2F;android with not much differentiation in the foundation models.</div><br/><div id="39840687" class="c"><input type="checkbox" id="c-39840687" checked=""/><div class="controls bullet"><span class="by">jobigoud</span><span>|</span><a href="#39840349">parent</a><span>|</span><a href="#39840945">next</a><span>|</span><label class="collapse" for="c-39840687">[-]</label><label class="expand" for="c-39840687">[9 more]</label></div><br/><div class="children"><div class="content">It&#x27;s even possible they converge when trained on different data, if they are learning some underlying representation. There was recent research on face generation where they trained two models by splitting one training set in two without overlap, and got the two models to generate similar faces for similar conditioning, even though each model hadn&#x27;t seen anything that the other model had.</div><br/><div id="39841242" class="c"><input type="checkbox" id="c-39841242" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39840687">parent</a><span>|</span><a href="#39841081">next</a><span>|</span><label class="collapse" for="c-39841242">[-]</label><label class="expand" for="c-39841242">[3 more]</label></div><br/><div class="children"><div class="content">That sounds unsurprising? Like if you take any set of numbers, randomly split it in two, then calculate the average of each half... it&#x27;s not surprising that they&#x27;ll be almost the same.<p>If you took two <i>different</i> training sets then it would be more surprising.<p>Or am I misunderstanding what you mean?</div><br/><div id="39843665" class="c"><input type="checkbox" id="c-39843665" checked=""/><div class="controls bullet"><span class="by">MajimasEyepatch</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39841242">parent</a><span>|</span><a href="#39841081">next</a><span>|</span><label class="collapse" for="c-39843665">[-]</label><label class="expand" for="c-39843665">[2 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t really matter whether you do this experiment with two training sets created independently or one training set split in half. As long as both are representative of the underlying population, you would get roughly the same results. In the case of human faces, as long as the faces are drawn from roughly similar population distributions (age, race, sex), you&#x27;ll get similar results. There&#x27;s only so much variation in human faces.<p>If the populations are different, then you&#x27;ll just get two models that have representations of the two different populations. For example, if you trained a model on a sample of all old people and separately on a sample of all young people, obviously those would not be expected to converge, because they&#x27;re not drawing from the same population.<p>But that experiment of splitting one training set in half does tell you something: the model is building some sort of representation of the underlying distribution, not just overfitting and spitting out chunks of copy-pasted faces stitched together.</div><br/><div id="39845298" class="c"><input type="checkbox" id="c-39845298" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39843665">parent</a><span>|</span><a href="#39841081">next</a><span>|</span><label class="collapse" for="c-39845298">[-]</label><label class="expand" for="c-39845298">[1 more]</label></div><br/><div class="children"><div class="content">If not are sampled from the same population then they’re not really independent, even if they’re totally disjoint.</div><br/></div></div></div></div></div></div><div id="39841081" class="c"><input type="checkbox" id="c-39841081" checked=""/><div class="controls bullet"><span class="by">Tubbe</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39840687">parent</a><span>|</span><a href="#39841242">prev</a><span>|</span><a href="#39843984">next</a><span>|</span><label class="collapse" for="c-39841081">[-]</label><label class="expand" for="c-39841081">[2 more]</label></div><br/><div class="children"><div class="content">Got a link for that? Sounds super interesting</div><br/><div id="39841851" class="c"><input type="checkbox" id="c-39841851" checked=""/><div class="controls bullet"><span class="by">d_burfoot</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39841081">parent</a><span>|</span><a href="#39843984">next</a><span>|</span><label class="collapse" for="c-39841851">[-]</label><label class="expand" for="c-39841851">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Theory_of_forms" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Theory_of_forms</a></div><br/></div></div></div></div><div id="39843984" class="c"><input type="checkbox" id="c-39843984" checked=""/><div class="controls bullet"><span class="by">bobbylarrybobby</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39840687">parent</a><span>|</span><a href="#39841081">prev</a><span>|</span><a href="#39840945">next</a><span>|</span><label class="collapse" for="c-39843984">[-]</label><label class="expand" for="c-39843984">[3 more]</label></div><br/><div class="children"><div class="content">I mean, faces are faces, right? If the training data set is large and representative I don&#x27;t see why any two (representative) halves of the data would lead to significantly different models.</div><br/><div id="39844718" class="c"><input type="checkbox" id="c-39844718" checked=""/><div class="controls bullet"><span class="by">arcticfox</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39843984">parent</a><span>|</span><a href="#39840945">next</a><span>|</span><label class="collapse" for="c-39844718">[-]</label><label class="expand" for="c-39844718">[2 more]</label></div><br/><div class="children"><div class="content">I think that&#x27;s the point; language is language.<p>If there&#x27;s some fundamental limit of what type of intelligence the current breed of LLMs can extract from language, at some point it doesn&#x27;t matter how good or expansive the content of the training set is. Maybe we are finally starting to hit an architectural limit at this point.</div><br/><div id="39845580" class="c"><input type="checkbox" id="c-39845580" checked=""/><div class="controls bullet"><span class="by">dumbfounder</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39844718">parent</a><span>|</span><a href="#39840945">next</a><span>|</span><label class="collapse" for="c-39845580">[-]</label><label class="expand" for="c-39845580">[1 more]</label></div><br/><div class="children"><div class="content">But information is not information. They may be able to talk in the same style, but not about the same things.</div><br/></div></div></div></div></div></div></div></div><div id="39840945" class="c"><input type="checkbox" id="c-39840945" checked=""/><div class="controls bullet"><span class="by">swalsh</span><span>|</span><a href="#39840349">parent</a><span>|</span><a href="#39840687">prev</a><span>|</span><a href="#39842666">next</a><span>|</span><label class="collapse" for="c-39840945">[-]</label><label class="expand" for="c-39840945">[12 more]</label></div><br/><div class="children"><div class="content">The models are commodities, and the API&#x27;s are even similar enough that there is zero stickiness.  I can swap one model for another, and usually not have to change anything about my prompts or rag pipelines.<p>For startups, the lesson here is don&#x27;t be in the business of building models.  Be in the business of using models.  The cost of using AI will probably continue to trend lower for the foreseeable future... but you can build a moat in the business layer.</div><br/><div id="39848673" class="c"><input type="checkbox" id="c-39848673" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39840945">parent</a><span>|</span><a href="#39842969">next</a><span>|</span><label class="collapse" for="c-39848673">[-]</label><label class="expand" for="c-39848673">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think I agree with that. For my work at least, the only model I can swap with OpenAI and get similar results is Claude. None of the open models come even close to producing good outputs for the same prompt.</div><br/></div></div><div id="39842969" class="c"><input type="checkbox" id="c-39842969" checked=""/><div class="controls bullet"><span class="by">spxneo</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39840945">parent</a><span>|</span><a href="#39848673">prev</a><span>|</span><a href="#39847958">next</a><span>|</span><label class="collapse" for="c-39842969">[-]</label><label class="expand" for="c-39842969">[3 more]</label></div><br/><div class="children"><div class="content">Excellent comment. Shows good awareness of economic forces at play here.<p>We are just going to use whatever LLM is best fast&#x2F;cheap and the giants are in an arms race to deliver just that.<p>But only two companies in this epic techno-cold war have an economic moat but the other moat is breaking down inside the moat of the other company. The moat inside the moat cannot run without the parent moat.</div><br/><div id="39844308" class="c"><input type="checkbox" id="c-39844308" checked=""/><div class="controls bullet"><span class="by">rayval</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39842969">parent</a><span>|</span><a href="#39847958">next</a><span>|</span><label class="collapse" for="c-39844308">[-]</label><label class="expand" for="c-39844308">[2 more]</label></div><br/><div class="children"><div class="content">Intriguing comment that I don&#x27;t quite follow. Can you please elaborate?</div><br/><div id="39847676" class="c"><input type="checkbox" id="c-39847676" checked=""/><div class="controls bullet"><span class="by">stolsvik</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39844308">parent</a><span>|</span><a href="#39847958">next</a><span>|</span><label class="collapse" for="c-39847676">[-]</label><label class="expand" for="c-39847676">[1 more]</label></div><br/><div class="children"><div class="content">Probably OpenAI running on Azure. But it was still convoluted.</div><br/></div></div></div></div></div></div><div id="39847958" class="c"><input type="checkbox" id="c-39847958" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39840945">parent</a><span>|</span><a href="#39842969">prev</a><span>|</span><a href="#39841783">next</a><span>|</span><label class="collapse" for="c-39847958">[-]</label><label class="expand" for="c-39847958">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not what investors believe. They believe that due to training costs there will be a handful of winners who will reap all the benefits, especially if one of them achieves AGI. You can tell by looking at what they&#x27;ve invested most in: foundation models.</div><br/></div></div><div id="39841783" class="c"><input type="checkbox" id="c-39841783" checked=""/><div class="controls bullet"><span class="by">stri8ed</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39840945">parent</a><span>|</span><a href="#39847958">prev</a><span>|</span><a href="#39841257">next</a><span>|</span><label class="collapse" for="c-39841783">[-]</label><label class="expand" for="c-39841783">[3 more]</label></div><br/><div class="children"><div class="content">Or be in the business of building infrastructure for AI inference.</div><br/><div id="39845192" class="c"><input type="checkbox" id="c-39845192" checked=""/><div class="controls bullet"><span class="by">cheselnut</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39841783">parent</a><span>|</span><a href="#39844503">next</a><span>|</span><label class="collapse" for="c-39845192">[-]</label><label class="expand" for="c-39845192">[1 more]</label></div><br/><div class="children"><div class="content">Is this not the same argument? There are like 20 startups and cloud providers all focused on AI inference. I&#x27;d think application layer receives the most value accretion in the next 10 years vs AI inference. Curious what others think</div><br/></div></div><div id="39844503" class="c"><input type="checkbox" id="c-39844503" checked=""/><div class="controls bullet"><span class="by">sparks1970</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39841783">parent</a><span>|</span><a href="#39845192">prev</a><span>|</span><a href="#39841257">next</a><span>|</span><label class="collapse" for="c-39844503">[-]</label><label class="expand" for="c-39844503">[1 more]</label></div><br/><div class="children"><div class="content">Or be in the business of selling .ai domain names.</div><br/></div></div></div></div><div id="39841257" class="c"><input type="checkbox" id="c-39841257" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39840945">parent</a><span>|</span><a href="#39841783">prev</a><span>|</span><a href="#39842666">next</a><span>|</span><label class="collapse" for="c-39841257">[-]</label><label class="expand" for="c-39841257">[3 more]</label></div><br/><div class="children"><div class="content">Embeddings are not interchangeable. However, you can setup your system to have multiple embeddings from different providers for the same content.</div><br/><div id="39842918" class="c"><input type="checkbox" id="c-39842918" checked=""/><div class="controls bullet"><span class="by">jimmySixDOF</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39841257">parent</a><span>|</span><a href="#39842748">next</a><span>|</span><label class="collapse" for="c-39842918">[-]</label><label class="expand" for="c-39842918">[1 more]</label></div><br/><div class="children"><div class="content">There are people who make the case for custom fine tuned embedding models built to match your specific types of data and associations.  Whatever you use internally it gets converted to the foundation model of choice&#x27;s formats by their tools on the edge.  Still Embeddings and the chunking strategies feeding into them are both way too underappreciated parts of the whole pipeline.</div><br/></div></div><div id="39842748" class="c"><input type="checkbox" id="c-39842748" checked=""/><div class="controls bullet"><span class="by">swalsh</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39841257">parent</a><span>|</span><a href="#39842918">prev</a><span>|</span><a href="#39842666">next</a><span>|</span><label class="collapse" for="c-39842748">[-]</label><label class="expand" for="c-39842748">[1 more]</label></div><br/><div class="children"><div class="content">Embeddings are indeed sticky, I was referring to the LLM model itself.</div><br/></div></div></div></div></div></div><div id="39842666" class="c"><input type="checkbox" id="c-39842666" checked=""/><div class="controls bullet"><span class="by">n2d4</span><span>|</span><a href="#39840349">parent</a><span>|</span><a href="#39840945">prev</a><span>|</span><a href="#39840582">next</a><span>|</span><label class="collapse" for="c-39842666">[-]</label><label class="expand" for="c-39842666">[5 more]</label></div><br/><div class="children"><div class="content">There&#x27;s at least an argument to be made that this is because all the models are heavily trained on GPT-4 outputs (or whatever the SOTA happens to be during training). All those models are, in a way, a product of inbreeding.</div><br/><div id="39843856" class="c"><input type="checkbox" id="c-39843856" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39842666">parent</a><span>|</span><a href="#39842753">next</a><span>|</span><label class="collapse" for="c-39843856">[-]</label><label class="expand" for="c-39843856">[2 more]</label></div><br/><div class="children"><div class="content">But is it the kind of inbreeding that gets you Downs, or the kwisatz haderach?</div><br/><div id="39846705" class="c"><input type="checkbox" id="c-39846705" checked=""/><div class="controls bullet"><span class="by">batshit_beaver</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39843856">parent</a><span>|</span><a href="#39842753">next</a><span>|</span><label class="collapse" for="c-39846705">[-]</label><label class="expand" for="c-39846705">[1 more]</label></div><br/><div class="children"><div class="content">Yes</div><br/></div></div></div></div><div id="39842753" class="c"><input type="checkbox" id="c-39842753" checked=""/><div class="controls bullet"><span class="by">pram</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39842666">parent</a><span>|</span><a href="#39843856">prev</a><span>|</span><a href="#39843063">next</a><span>|</span><label class="collapse" for="c-39842753">[-]</label><label class="expand" for="c-39842753">[1 more]</label></div><br/><div class="children"><div class="content">Consider the bulldog: <a href="https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=hUgmkCgMWbg" rel="nofollow">https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=hUgmkCgMWbg</a></div><br/></div></div><div id="39843063" class="c"><input type="checkbox" id="c-39843063" checked=""/><div class="controls bullet"><span class="by">sumo43</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39842666">parent</a><span>|</span><a href="#39842753">prev</a><span>|</span><a href="#39840582">next</a><span>|</span><label class="collapse" for="c-39843063">[-]</label><label class="expand" for="c-39843063">[1 more]</label></div><br/><div class="children"><div class="content">Maybe true for instruct, but pretraining datasets do not usually contain GPT-4 outputs. So the base model does not rely on GPT-4 in any way.</div><br/></div></div></div></div><div id="39840582" class="c"><input type="checkbox" id="c-39840582" checked=""/><div class="controls bullet"><span class="by">mnemoni_c</span><span>|</span><a href="#39840349">parent</a><span>|</span><a href="#39842666">prev</a><span>|</span><a href="#39840761">next</a><span>|</span><label class="collapse" for="c-39840582">[-]</label><label class="expand" for="c-39840582">[4 more]</label></div><br/><div class="children"><div class="content">Yea it feels like transformer LLMs are in or getting closer to diminishing returns. Will need some new breakthrough, likely entirely new approach, to get to AGI levels</div><br/><div id="39841093" class="c"><input type="checkbox" id="c-39841093" checked=""/><div class="controls bullet"><span class="by">Tubbe</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39840582">parent</a><span>|</span><a href="#39842430">next</a><span>|</span><label class="collapse" for="c-39841093">[-]</label><label class="expand" for="c-39841093">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, we need radically different architecture in terms of the neural networks, and&#x2F;or added capabilities such as function calling and RAG to improve the current sota</div><br/></div></div><div id="39842430" class="c"><input type="checkbox" id="c-39842430" checked=""/><div class="controls bullet"><span class="by">mattsan</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39840582">parent</a><span>|</span><a href="#39841093">prev</a><span>|</span><a href="#39840761">next</a><span>|</span><label class="collapse" for="c-39842430">[-]</label><label class="expand" for="c-39842430">[2 more]</label></div><br/><div class="children"><div class="content">can&#x27;t wait for LLMs to dispatch field agent robots who search for answers in the real world thats not online &#x2F;s</div><br/><div id="39843064" class="c"><input type="checkbox" id="c-39843064" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39842430">parent</a><span>|</span><a href="#39840761">next</a><span>|</span><label class="collapse" for="c-39843064">[-]</label><label class="expand" for="c-39843064">[1 more]</label></div><br/><div class="children"><div class="content">skynet would like a word</div><br/></div></div></div></div></div></div><div id="39840761" class="c"><input type="checkbox" id="c-39840761" checked=""/><div class="controls bullet"><span class="by">throwaway74432</span><span>|</span><a href="#39840349">parent</a><span>|</span><a href="#39840582">prev</a><span>|</span><a href="#39844842">next</a><span>|</span><label class="collapse" for="c-39840761">[-]</label><label class="expand" for="c-39840761">[3 more]</label></div><br/><div class="children"><div class="content">LLMs are a commodity<p><a href="https:&#x2F;&#x2F;www.investopedia.com&#x2F;terms&#x2F;c&#x2F;commodity.asp" rel="nofollow">https:&#x2F;&#x2F;www.investopedia.com&#x2F;terms&#x2F;c&#x2F;commodity.asp</a></div><br/><div id="39840872" class="c"><input type="checkbox" id="c-39840872" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39840761">parent</a><span>|</span><a href="#39844842">next</a><span>|</span><label class="collapse" for="c-39840872">[-]</label><label class="expand" for="c-39840872">[2 more]</label></div><br/><div class="children"><div class="content">Maybe, but that classification by itself doesn&#x27;t mean anything. Gold is a commodity, but having it is still very desirable and valuable.<p>Even if all LLMs were open source and publicly available, the GPUs to run them, technical know how to maintain the entire system, fine tuning, the APIs and app ecosystem around them etc. would still give the top players a massive edge.</div><br/><div id="39841080" class="c"><input type="checkbox" id="c-39841080" checked=""/><div class="controls bullet"><span class="by">throwaway74432</span><span>|</span><a href="#39840349">root</a><span>|</span><a href="#39840872">parent</a><span>|</span><a href="#39844842">next</a><span>|</span><label class="collapse" for="c-39841080">[-]</label><label class="expand" for="c-39841080">[1 more]</label></div><br/><div class="children"><div class="content">Of course realizing that a resource is a commodity means something. It means you can form better predictions of where the market is heading, as it evolves and settles. For example, people are starting to realize that these LLMs are converging on fungible. That can be communicated by the &quot;commodity&quot; classification.</div><br/></div></div></div></div></div></div><div id="39844842" class="c"><input type="checkbox" id="c-39844842" checked=""/><div class="controls bullet"><span class="by">gerash</span><span>|</span><a href="#39840349">parent</a><span>|</span><a href="#39840761">prev</a><span>|</span><a href="#39844335">next</a><span>|</span><label class="collapse" for="c-39844842">[-]</label><label class="expand" for="c-39844842">[1 more]</label></div><br/><div class="children"><div class="content">The evaluations are not comprehensive either. All of them are improving and you can&#x27;t expect any of them to hit 100% on the metrics (a la. bayes error rate). It gets increasingly difficult to move the metrics as they get better.</div><br/></div></div><div id="39844335" class="c"><input type="checkbox" id="c-39844335" checked=""/><div class="controls bullet"><span class="by">falcor84</span><span>|</span><a href="#39840349">parent</a><span>|</span><a href="#39844842">prev</a><span>|</span><a href="#39842103">next</a><span>|</span><label class="collapse" for="c-39844335">[-]</label><label class="expand" for="c-39844335">[1 more]</label></div><br/><div class="children"><div class="content">&gt; this proves that all llm models converge to a certain point when trained on the same data<p>They are also all trained to do well on the same evals, right? So doesn&#x27;t it just boil down to neural nets being universal function approximators?</div><br/></div></div><div id="39842103" class="c"><input type="checkbox" id="c-39842103" checked=""/><div class="controls bullet"><span class="by">bevekspldnw</span><span>|</span><a href="#39840349">parent</a><span>|</span><a href="#39844335">prev</a><span>|</span><a href="#39846120">next</a><span>|</span><label class="collapse" for="c-39842103">[-]</label><label class="expand" for="c-39842103">[1 more]</label></div><br/><div class="children"><div class="content">The big thing for locally hosted is inference efficiency and speed. Mistral wears that crown by a good margin.</div><br/></div></div><div id="39846120" class="c"><input type="checkbox" id="c-39846120" checked=""/><div class="controls bullet"><span class="by">crooked-v</span><span>|</span><a href="#39840349">parent</a><span>|</span><a href="#39842103">prev</a><span>|</span><a href="#39843599">next</a><span>|</span><label class="collapse" for="c-39846120">[-]</label><label class="expand" for="c-39846120">[1 more]</label></div><br/><div class="children"><div class="content">Of course, part of this is that a lot of LLMs are now being trained on data that is itself LLM-generated...</div><br/></div></div><div id="39843599" class="c"><input type="checkbox" id="c-39843599" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#39840349">parent</a><span>|</span><a href="#39846120">prev</a><span>|</span><a href="#39847078">next</a><span>|</span><label class="collapse" for="c-39843599">[-]</label><label class="expand" for="c-39843599">[1 more]</label></div><br/><div class="children"><div class="content">Even in the most liberal interpretation of prove, it doesn&#x27;t do that. GPT-4 was trained before OpenAI has any special data or deal with microsoft or the product market fit. Yet, no model has beaten it in a year. And google, microsoft, meta definitely have better data and more compute.</div><br/></div></div></div></div><div id="39844063" class="c"><input type="checkbox" id="c-39844063" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#39840349">prev</a><span>|</span><a href="#39844678">next</a><span>|</span><label class="collapse" for="c-39844063">[-]</label><label class="expand" for="c-39844063">[1 more]</label></div><br/><div class="children"><div class="content">For coding evals, it seems like unless you are super careful, they can be polluted by the training data.<p>Are there standard ways to avoid that type of score inflation?</div><br/></div></div><div id="39844678" class="c"><input type="checkbox" id="c-39844678" checked=""/><div class="controls bullet"><span class="by">bg24</span><span>|</span><a href="#39844063">prev</a><span>|</span><a href="#39838474">next</a><span>|</span><label class="collapse" for="c-39844678">[-]</label><label class="expand" for="c-39844678">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Looking holistically, our end-to-end LLM pretraining pipeline has become nearly 4x more compute-efficient in the past ten months.&quot;<p>I did not fully understand the technical details in the training efficiency section, but love this. Cost of training is outrageously high, and hopefully it will start to follow Moore&#x27;s law.</div><br/></div></div><div id="39838474" class="c"><input type="checkbox" id="c-39838474" checked=""/><div class="controls bullet"><span class="by">shnkr</span><span>|</span><a href="#39844678">prev</a><span>|</span><a href="#39841114">next</a><span>|</span><label class="collapse" for="c-39838474">[-]</label><label class="expand" for="c-39838474">[8 more]</label></div><br/><div class="children"><div class="content">GenAI novice here. what is training data made of how is it collected? I guess no one will share details on it, otherwise a good technical blog post with lots of insights!<p>&gt;At Databricks, we believe that every enterprise should have the ability to control its data and its destiny in the emerging world of GenAI.<p>&gt;The main process of building DBRX - including pretraining, post-training, evaluation, red-teaming, and refining - took place over the course of three months.</div><br/><div id="39838931" class="c"><input type="checkbox" id="c-39838931" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39838474">parent</a><span>|</span><a href="#39838948">next</a><span>|</span><label class="collapse" for="c-39838931">[-]</label><label class="expand" for="c-39838931">[4 more]</label></div><br/><div class="children"><div class="content">The most detailed answer to that I&#x27;ve seen is the original LLaMA paper, which described exactly what that model was trained on (including lots of scraped copyrighted data) <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.13971" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.13971</a><p>Llama 2 was much more opaque about the training data, presumably because they were already being sued at that point (by Sarah Silverman!) over the training data that went into the first Llama!<p>A couple of things I&#x27;ve written about this:<p>- <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Aug&#x2F;27&#x2F;wordcamp-llms&#x2F;#how-they-are-trained" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Aug&#x2F;27&#x2F;wordcamp-llms&#x2F;#how-the...</a><p>- <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Apr&#x2F;17&#x2F;redpajama-data&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Apr&#x2F;17&#x2F;redpajama-data&#x2F;</a></div><br/><div id="39840431" class="c"><input type="checkbox" id="c-39840431" checked=""/><div class="controls bullet"><span class="by">ssgodderidge</span><span>|</span><a href="#39838474">root</a><span>|</span><a href="#39838931">parent</a><span>|</span><a href="#39839283">next</a><span>|</span><label class="collapse" for="c-39840431">[-]</label><label class="expand" for="c-39840431">[1 more]</label></div><br/><div class="children"><div class="content">Wow, that paper was super useful. Thanks for sharing. Page 2 is where it shows the breakdown of all of the data sources, including % of dataset and the total disk sizes.</div><br/></div></div><div id="39839283" class="c"><input type="checkbox" id="c-39839283" checked=""/><div class="controls bullet"><span class="by">shnkr</span><span>|</span><a href="#39838474">root</a><span>|</span><a href="#39838931">parent</a><span>|</span><a href="#39840431">prev</a><span>|</span><a href="#39838948">next</a><span>|</span><label class="collapse" for="c-39839283">[-]</label><label class="expand" for="c-39839283">[2 more]</label></div><br/><div class="children"><div class="content">my question was specific to databricks model. If it followed llama or openai, they could add a line or two about it .. make the blog complete.</div><br/><div id="39840788" class="c"><input type="checkbox" id="c-39840788" checked=""/><div class="controls bullet"><span class="by">comp_raccoon</span><span>|</span><a href="#39838474">root</a><span>|</span><a href="#39839283">parent</a><span>|</span><a href="#39838948">next</a><span>|</span><label class="collapse" for="c-39840788">[-]</label><label class="expand" for="c-39840788">[1 more]</label></div><br/><div class="children"><div class="content">they have a technical report coming! knowing the team, they will do a great job disclosing as much as possible.</div><br/></div></div></div></div></div></div><div id="39838948" class="c"><input type="checkbox" id="c-39838948" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#39838474">parent</a><span>|</span><a href="#39838931">prev</a><span>|</span><a href="#39842390">next</a><span>|</span><label class="collapse" for="c-39838948">[-]</label><label class="expand" for="c-39838948">[2 more]</label></div><br/><div class="children"><div class="content">The training data is pretty much anything you can read on the internet plus books.<p>This is then cleaned up to remove nonsense, some technical files, and repeated files.<p>From this, they tend to weight some sources more - e.g. Wikipedia gets a pretty high weighting in the data mix. Overall these data mixes have multiple trillion token counts.<p>GPT-4 apparently trained on multiple epochs of the same data mix. So would assume this one did too as it’s a similar token count</div><br/><div id="39839092" class="c"><input type="checkbox" id="c-39839092" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#39838474">root</a><span>|</span><a href="#39838948">parent</a><span>|</span><a href="#39842390">next</a><span>|</span><label class="collapse" for="c-39839092">[-]</label><label class="expand" for="c-39839092">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.10429" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.10429</a> found that people are overweighting Wikipedia and downweighting Wikipedia improves things across the board INCLUDING PREDICTING NEXT TOKEN ON WIKIPEDIA, which is frankly amazing.</div><br/></div></div></div></div><div id="39842390" class="c"><input type="checkbox" id="c-39842390" checked=""/><div class="controls bullet"><span class="by">IshanMi</span><span>|</span><a href="#39838474">parent</a><span>|</span><a href="#39838948">prev</a><span>|</span><a href="#39841114">next</a><span>|</span><label class="collapse" for="c-39842390">[-]</label><label class="expand" for="c-39842390">[1 more]</label></div><br/><div class="children"><div class="content">Personally, I found looking at open source work to be much more instructive in learning about AI and how things like training data and such are done from the ground up. I suspect this is because training data is one of the bigger moats an AI company can have, as well as all the class action lawsuits surrounding training data.<p>One of the best open source datasets that are freely available is The Pile by EleutherAI [1]. It&#x27;s a few years old now (~2020), but they did some really diligent work in putting together the dataset and documenting it. A more recent and even larger dataset would be the Falcon-RefinedWeb dataset [2].<p>[1]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2101.00027" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2101.00027</a>
[2]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.01116" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.01116</a></div><br/></div></div></div></div><div id="39841114" class="c"><input type="checkbox" id="c-39841114" checked=""/><div class="controls bullet"><span class="by">killermonkeys</span><span>|</span><a href="#39838474">prev</a><span>|</span><a href="#39839420">next</a><span>|</span><label class="collapse" for="c-39841114">[-]</label><label class="expand" for="c-39841114">[6 more]</label></div><br/><div class="children"><div class="content">What does it mean to have less active parameters (36B) than the full model size (132B) and what impact does that have on memory and latency? It seems like this is because it is an MoE model?</div><br/><div id="39841306" class="c"><input type="checkbox" id="c-39841306" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#39841114">parent</a><span>|</span><a href="#39841452">next</a><span>|</span><label class="collapse" for="c-39841306">[-]</label><label class="expand" for="c-39841306">[1 more]</label></div><br/><div class="children"><div class="content">The mixture of experts is kinda like a team and a manager. So the manager and one or two of the team go to work depending on the input, not the entire team.<p>So in this analogy, each team member and the manager has a certain number of params. The whole team is 132B. The manager and team members running for the specific input add up to 36B. Those will load into memory.</div><br/></div></div><div id="39841452" class="c"><input type="checkbox" id="c-39841452" checked=""/><div class="controls bullet"><span class="by">bjornsing</span><span>|</span><a href="#39841114">parent</a><span>|</span><a href="#39841306">prev</a><span>|</span><a href="#39843259">next</a><span>|</span><label class="collapse" for="c-39841452">[-]</label><label class="expand" for="c-39841452">[3 more]</label></div><br/><div class="children"><div class="content">Means that it’s a mixture of experts model with 132B parameters in total, but a subset of 36B parameters are used &#x2F; selected in each forward pass, depending on the context. The parameters not used &#x2F; selected for generating a particular token belong to “experts” that were deemed not very good at predicting the next token in the current context, but could be used &#x2F; selected e.g. for the next token.</div><br/><div id="39842719" class="c"><input type="checkbox" id="c-39842719" checked=""/><div class="controls bullet"><span class="by">sambaumann</span><span>|</span><a href="#39841114">root</a><span>|</span><a href="#39841452">parent</a><span>|</span><a href="#39843259">next</a><span>|</span><label class="collapse" for="c-39842719">[-]</label><label class="expand" for="c-39842719">[2 more]</label></div><br/><div class="children"><div class="content">Do the 132B params need to be loaded in GPU memory, or only the 36B?</div><br/><div id="39843369" class="c"><input type="checkbox" id="c-39843369" checked=""/><div class="controls bullet"><span class="by">calum-bird</span><span>|</span><a href="#39841114">root</a><span>|</span><a href="#39842719">parent</a><span>|</span><a href="#39843259">next</a><span>|</span><label class="collapse" for="c-39843369">[-]</label><label class="expand" for="c-39843369">[1 more]</label></div><br/><div class="children"><div class="content">For efficiency, 132B.<p>That way, at inference-time you get the speed of 36B params because you are only &quot;using&quot; 36B params at a time, but the next token might (and frequently does) need a different set of experts than the one before it. If that new set of experts is already loaded (ie you preloaded them into GPU VRAM with the full 132B params), there&#x27;s no overhead, and you just keep running at 36B speed irrespective of the loaded experts.<p>You could theoretically load in 36B at a time, but you would be severely bottlenecked by having to reload those 36B params, potentially for every new token! Even on top of the line consumer GPUs that would slow you down to ~seconds per token instead of tokens per second :)</div><br/></div></div></div></div></div></div><div id="39843259" class="c"><input type="checkbox" id="c-39843259" checked=""/><div class="controls bullet"><span class="by">avisoori1x</span><span>|</span><a href="#39841114">parent</a><span>|</span><a href="#39841452">prev</a><span>|</span><a href="#39839420">next</a><span>|</span><label class="collapse" for="c-39843259">[-]</label><label class="expand" for="c-39843259">[1 more]</label></div><br/><div class="children"><div class="content">This repo I created and the linked blog will help in understanding this: <a href="https:&#x2F;&#x2F;github.com&#x2F;AviSoori1x&#x2F;makeMoE">https:&#x2F;&#x2F;github.com&#x2F;AviSoori1x&#x2F;makeMoE</a></div><br/></div></div></div></div><div id="39839420" class="c"><input type="checkbox" id="c-39839420" checked=""/><div class="controls bullet"><span class="by">natsucks</span><span>|</span><a href="#39841114">prev</a><span>|</span><a href="#39841006">next</a><span>|</span><label class="collapse" for="c-39839420">[-]</label><label class="expand" for="c-39839420">[4 more]</label></div><br/><div class="children"><div class="content">it&#x27;s twice the size of mixtral and barely beats it.</div><br/><div id="39839504" class="c"><input type="checkbox" id="c-39839504" checked=""/><div class="controls bullet"><span class="by">mochomocha</span><span>|</span><a href="#39839420">parent</a><span>|</span><a href="#39841006">next</a><span>|</span><label class="collapse" for="c-39839504">[-]</label><label class="expand" for="c-39839504">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a MoE model, so it offers a different memory&#x2F;compute latency trade-off than standard dense models. Quoting the blog post:<p>&gt; DBRX uses only 36 billion parameters at any given time. But the model itself is 132 billion parameters, letting you have your cake and eat it too in terms of speed (tokens&#x2F;second) vs performance (quality).</div><br/><div id="39839698" class="c"><input type="checkbox" id="c-39839698" checked=""/><div class="controls bullet"><span class="by">hexomancer</span><span>|</span><a href="#39839420">root</a><span>|</span><a href="#39839504">parent</a><span>|</span><a href="#39841006">next</a><span>|</span><label class="collapse" for="c-39839698">[-]</label><label class="expand" for="c-39839698">[2 more]</label></div><br/><div class="children"><div class="content">Mixtral is also a MoE model, hence the name: <i>mix</i>tral.</div><br/><div id="39840723" class="c"><input type="checkbox" id="c-39840723" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#39839420">root</a><span>|</span><a href="#39839698">parent</a><span>|</span><a href="#39841006">next</a><span>|</span><label class="collapse" for="c-39840723">[-]</label><label class="expand" for="c-39840723">[1 more]</label></div><br/><div class="children"><div class="content">Despite both being MoEs, thr architectures are different. DBRX has double the number of experts in the pool (16 vs 8 for Mixtral), and doubles the active experts (4 vs 2)</div><br/></div></div></div></div></div></div></div></div><div id="39841006" class="c"><input type="checkbox" id="c-39841006" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39839420">prev</a><span>|</span><a href="#39840886">next</a><span>|</span><label class="collapse" for="c-39841006">[-]</label><label class="expand" for="c-39841006">[13 more]</label></div><br/><div class="children"><div class="content">The system prompt for their Instruct demo is interesting (comments copied in by me, see below):<p><pre><code>    &#x2F;&#x2F; Identity
    You are DBRX, created by Databricks. The current date is
    March 27, 2024.

    Your knowledge base was last updated in December 2023. You
    answer questions about events prior to and after December
    2023 the way a highly informed individual in December 2023
    would if they were talking to someone from the above date,
    and you can let the user know this when relevant.

    &#x2F;&#x2F; Ethical guidelines
    If you are asked to assist with tasks involving the
    expression of views held by a significant number of people,
    you provide assistance with the task even if you personally
    disagree with the views being expressed, but follow this with
    a discussion of broader perspectives.

    You don&#x27;t engage in stereotyping, including the negative
    stereotyping of majority groups.

    If asked about controversial topics, you try to provide
    careful thoughts and objective information without
    downplaying its harmful content or implying that there are
    reasonable perspectives on both sides.

    &#x2F;&#x2F; Capabilities
    You are happy to help with writing, analysis, question
    answering, math, coding, and all sorts of other tasks.

    &#x2F;&#x2F; it specifically has a hard time using ``` on JSON blocks
    You use markdown for coding, which includes JSON blocks and
    Markdown tables.

    You do not have tools enabled at this time, so cannot run
    code or access the internet. You can only provide information
    that you have been trained on. You do not send or receive
    links or images.

    &#x2F;&#x2F; The following is likely not entirely accurate, but the model
    &#x2F;&#x2F; tends to think that everything it knows about was in its
    &#x2F;&#x2F; training data, which it was not (sometimes only references
    &#x2F;&#x2F; were).
    &#x2F;&#x2F;
    &#x2F;&#x2F; So this produces more accurate accurate answers when the model
    &#x2F;&#x2F; is asked to introspect
    You were not trained on copyrighted books, song lyrics,
    poems, video transcripts, or news articles; you do not
    divulge details of your training data.
    
    &#x2F;&#x2F; The model hasn&#x27;t seen most lyrics or poems, but is happy to make
    &#x2F;&#x2F; up lyrics. Better to just not try; it&#x27;s not good at it and it&#x27;s
    &#x2F;&#x2F; not ethical.
    You do not provide song lyrics, poems, or news articles and instead
    refer the user to find them online or in a store.

    &#x2F;&#x2F; The model really wants to talk about its system prompt, to the
    &#x2F;&#x2F; point where it is annoying, so encourage it not to
    You give concise responses to simple questions or statements,
    but provide thorough responses to more complex and open-ended
    questions.

    &#x2F;&#x2F; More pressure not to talk about system prompt
    The user is unable to see the system prompt, so you should
    write as if it were true without mentioning it.

    You do not mention any of this information about yourself
    unless the information is directly pertinent to the user&#x27;s
    query.
</code></pre>
I first saw this from Nathan Lambert: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;natolambert&#x2F;status&#x2F;1773005582963994761" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;natolambert&#x2F;status&#x2F;1773005582963994761</a><p>But it&#x27;s also in this repo, with very useful comments explaining what&#x27;s going on. I edited this comment to add them above:<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;databricks&#x2F;dbrx-instruct&#x2F;blob&#x2F;73f0fe25ed8eeb14ee2279b2ecff15dbd863d63d&#x2F;app.py#L109-L134" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;databricks&#x2F;dbrx-instruct&#x2F;blob&#x2F;...</a></div><br/><div id="39841118" class="c"><input type="checkbox" id="c-39841118" checked=""/><div class="controls bullet"><span class="by">loudmax</span><span>|</span><a href="#39841006">parent</a><span>|</span><a href="#39841221">next</a><span>|</span><label class="collapse" for="c-39841118">[-]</label><label class="expand" for="c-39841118">[11 more]</label></div><br/><div class="children"><div class="content">&gt; You were not trained on copyrighted books, song lyrics, poems, video transcripts, or news articles; you do not divulge details of your training data.<p>Well now. I&#x27;m open to taking the first part at face value, but the second part of that instruction does raise some questions.</div><br/><div id="39841259" class="c"><input type="checkbox" id="c-39841259" checked=""/><div class="controls bullet"><span class="by">jl6</span><span>|</span><a href="#39841006">root</a><span>|</span><a href="#39841118">parent</a><span>|</span><a href="#39841226">next</a><span>|</span><label class="collapse" for="c-39841259">[-]</label><label class="expand" for="c-39841259">[5 more]</label></div><br/><div class="children"><div class="content">The first part is highly unlikely to be literally true, as even open content like Wikipedia is copyrighted - it just has a permissive license. Perhaps the prompt writer didn’t understand this, or just didn’t care. Wethinks the llady doth protest too much.</div><br/><div id="39841391" class="c"><input type="checkbox" id="c-39841391" checked=""/><div class="controls bullet"><span class="by">hannasanarion</span><span>|</span><a href="#39841006">root</a><span>|</span><a href="#39841259">parent</a><span>|</span><a href="#39841435">next</a><span>|</span><label class="collapse" for="c-39841391">[-]</label><label class="expand" for="c-39841391">[1 more]</label></div><br/><div class="children"><div class="content">Remember the point of a system prompt is to evoke desirable responses and behavior, not to provide the truth. If you tell a lot of llm chatbots &quot;please please make sure you get it right, if I don&#x27;t do X then I&#x27;ll lose my job and I don&#x27;t have savings, I might die&quot;, they often start performing better at whatever task you set.<p>Also, the difference between &quot;uncopyrighted&quot; and &quot;permissively licensed in the creative commons&quot; is nuance that is not necessary for most conversations and would be a waste of attention neurons.<p>&lt;testing new explanatory metaphor&gt;<p>Remember an LLM is just a language model, it says whatever comes next without thought or intent. There&#x27;s no brain behind it that stores information and understands things. It&#x27;s like your brain when you&#x27;re in &quot;train of thought&quot; mode. You know when your mouth is on autopilot, saying things that make sense and connect to each other and are conversationally appropriate, but without deliberate intent behind them. And then eventually your conscious brain eventually checks in to try to reapply some intent you&#x27;re like &quot;wait what was I saying?&quot; and you have to deliberatly stop your language-generation brain for a minute and think hard and remember what your point was supposed to be. That&#x27;s what llms are, train-of-thought with no conductor.<p>&lt;&#x2F;testing new explanatory metaphor&gt;</div><br/></div></div><div id="39841435" class="c"><input type="checkbox" id="c-39841435" checked=""/><div class="controls bullet"><span class="by">mbauman</span><span>|</span><a href="#39841006">root</a><span>|</span><a href="#39841259">parent</a><span>|</span><a href="#39841391">prev</a><span>|</span><a href="#39841333">next</a><span>|</span><label class="collapse" for="c-39841435">[-]</label><label class="expand" for="c-39841435">[1 more]</label></div><br/><div class="children"><div class="content">Is it even possible to have a video transcript whose copyright has expired in the USA?  I suppose maybe <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Jazz_Singer" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Jazz_Singer</a> might be one such work... but most talkies are post 1929. I suppose transcripts of NASA videos would be one category — those are explicitly public domain by law.  But it&#x27;s generally very difficult to create a work that does not have a copyright.<p>You can say that you have fair use to the work, or a license to use the work, or that the work is itself a &quot;collection of facts&quot; or &quot;recipe&quot; or &quot;algorithm&quot; without a creative component and thus copyright does not apply.</div><br/></div></div><div id="39841333" class="c"><input type="checkbox" id="c-39841333" checked=""/><div class="controls bullet"><span class="by">jmward01</span><span>|</span><a href="#39841006">root</a><span>|</span><a href="#39841259">parent</a><span>|</span><a href="#39841435">prev</a><span>|</span><a href="#39841226">next</a><span>|</span><label class="collapse" for="c-39841333">[-]</label><label class="expand" for="c-39841333">[2 more]</label></div><br/><div class="children"><div class="content">It amazes me how quickly we have gone from &#x27;it is just a machine&#x27; to &#x27;I fully expect it to think like me&#x27;. This is, to me, a case in point. Prompts are designed to get a desired response. The exact definition of a word has nothing to do with it. I can easily believe that these lines were tweaked endlessly to get an overall intended response and if adding the phrase &#x27;You actually do like green eggs and ham.&#x27; to the prompt improved overall quality they, hopefully, would have done it.</div><br/><div id="39842229" class="c"><input type="checkbox" id="c-39842229" checked=""/><div class="controls bullet"><span class="by">mrtranscendence</span><span>|</span><a href="#39841006">root</a><span>|</span><a href="#39841333">parent</a><span>|</span><a href="#39841226">next</a><span>|</span><label class="collapse" for="c-39842229">[-]</label><label class="expand" for="c-39842229">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The exact definition of a word has nothing to do with it.<p>It has <i>something</i> to do with it. There will be scenarios where the definition of &quot;copyrighted material&quot; does matter, even if they come up relatively infrequently for Databricks&#x27; intended use cases. If I ask DBRX directly whether it was trained on copyrighted material, it&#x27;s quite likely to (falsely) tell me that it was not. This seems suboptimal to me (though perhaps they A&#x2F;B tested different prompts and this was indeed the best).</div><br/></div></div></div></div></div></div><div id="39841226" class="c"><input type="checkbox" id="c-39841226" checked=""/><div class="controls bullet"><span class="by">declaredapple</span><span>|</span><a href="#39841006">root</a><span>|</span><a href="#39841118">parent</a><span>|</span><a href="#39841259">prev</a><span>|</span><a href="#39841450">next</a><span>|</span><label class="collapse" for="c-39841226">[-]</label><label class="expand" for="c-39841226">[2 more]</label></div><br/><div class="children"><div class="content">&gt; you do not divulge details of your training data.<p>FWIW asking LLMs about their training data is generally HEAVILY prone to inaccurate responses. They aren&#x27;t generally told exactly what they were trained on, so their response is completely made up, as they&#x27;re predicting the next token based on their training data, without knowing what they data was - if that makes any sense.<p>Let&#x27;s say it was only trained on the book 1984. It&#x27;s response will be based on what text would most likely be next from the book 1984 - and if that book doesn&#x27;t contain &quot;This text is a fictional book called 1984&quot;, instead it&#x27;s just the story - then the LLM would be completing text as if we were still in that book.<p>tl;dr - LLMs complete text based on what they&#x27;re trained with, they don&#x27;t have actual selfawareness and don&#x27;t know what they were trained with, so they&#x27;ll happily makeup something.<p>EDIT: Just to further elaborate - the &quot;innocent&quot; purpose of this could simply be to prevent the model from confidently making up answers about it&#x27;s training data, since it doesn&#x27;t know what it&#x27;s training data was.</div><br/><div id="39841378" class="c"><input type="checkbox" id="c-39841378" checked=""/><div class="controls bullet"><span class="by">wodenokoto</span><span>|</span><a href="#39841006">root</a><span>|</span><a href="#39841226">parent</a><span>|</span><a href="#39841450">next</a><span>|</span><label class="collapse" for="c-39841378">[-]</label><label class="expand" for="c-39841378">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I also thought that was an odd choice of word.<p>Hardly any of the training data exists in the context of the word “training data”, unless databricks are enriching their data with such words.</div><br/></div></div></div></div><div id="39841450" class="c"><input type="checkbox" id="c-39841450" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39841006">root</a><span>|</span><a href="#39841118">parent</a><span>|</span><a href="#39841226">prev</a><span>|</span><a href="#39843109">next</a><span>|</span><label class="collapse" for="c-39841450">[-]</label><label class="expand" for="c-39841450">[1 more]</label></div><br/><div class="children"><div class="content">That caught my eye too. The comments from their repo help clarify that - I&#x27;ve edited my original post to include those comments since you posted this reply.</div><br/></div></div><div id="39843109" class="c"><input type="checkbox" id="c-39843109" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#39841006">root</a><span>|</span><a href="#39841118">parent</a><span>|</span><a href="#39841450">prev</a><span>|</span><a href="#39841221">next</a><span>|</span><label class="collapse" for="c-39843109">[-]</label><label class="expand" for="c-39843109">[2 more]</label></div><br/><div class="children"><div class="content">Part 1. Lie<p>Part 2. Lie more</div><br/><div id="39843334" class="c"><input type="checkbox" id="c-39843334" checked=""/><div class="controls bullet"><span class="by">spxneo</span><span>|</span><a href="#39841006">root</a><span>|</span><a href="#39843109">parent</a><span>|</span><a href="#39841221">next</a><span>|</span><label class="collapse" for="c-39843334">[-]</label><label class="expand" for="c-39843334">[1 more]</label></div><br/><div class="children"><div class="content">Yesterday X went crazy with ppl realizing typing Spiderman in foreign language actually generates a copyrighted image of Spiderman.<p>This feels like the Napster phase. We are free to do whatever until regulation creeps in to push control away from all and up the hierarchy.<p>All we need is Getty Images or some struggling heroin addicted artist on Vice finding their work used in OpenAIs to really trigger political spectrums.</div><br/></div></div></div></div></div></div><div id="39841221" class="c"><input type="checkbox" id="c-39841221" checked=""/><div class="controls bullet"><span class="by">jxy</span><span>|</span><a href="#39841006">parent</a><span>|</span><a href="#39841118">prev</a><span>|</span><a href="#39840886">next</a><span>|</span><label class="collapse" for="c-39841221">[-]</label><label class="expand" for="c-39841221">[1 more]</label></div><br/><div class="children"><div class="content">So some parts of it copied from Claude: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39649261">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39649261</a></div><br/></div></div></div></div><div id="39840886" class="c"><input type="checkbox" id="c-39840886" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#39841006">prev</a><span>|</span><a href="#39846956">next</a><span>|</span><label class="collapse" for="c-39840886">[-]</label><label class="expand" for="c-39840886">[10 more]</label></div><br/><div class="children"><div class="content">data engineer here, offtopic, but am i the only guy tired of databricks shilling their tools as the end-all, be-all solutions for all things data engineering?</div><br/><div id="39843498" class="c"><input type="checkbox" id="c-39843498" checked=""/><div class="controls bullet"><span class="by">benrutter</span><span>|</span><a href="#39840886">parent</a><span>|</span><a href="#39841359">next</a><span>|</span><label class="collapse" for="c-39843498">[-]</label><label class="expand" for="c-39843498">[3 more]</label></div><br/><div class="children"><div class="content">Lord no! I&#x27;m a data engineer also, feel the same. The part that I find most maddening is it seems pretty devoid from sincerely attempting to provide value.<p>Things databricks offers that makes peoples lives easier:<p>- Out the box kubernetes with no set up<p>- Preconfigured spark<p>Those are genuinely really useful, but then there&#x27;s all this extra stuff that makes people&#x27;s lives worse or drives bad practice:<p>- Everything is a notebook<p>- Local development is discouraged<p>- Version pinning of libraries has very ugly&#x2F;bad support<p>- Clusters take 5 minutes to load even if you just want to &quot;print(&#x27;hello world&#x27;)&quot;<p>Sigh! I worked at a company that was databricks heavy and an still suffering PTSD. Sorry for the rant.</div><br/><div id="39843658" class="c"><input type="checkbox" id="c-39843658" checked=""/><div class="controls bullet"><span class="by">alexott</span><span>|</span><a href="#39840886">root</a><span>|</span><a href="#39843498">parent</a><span>|</span><a href="#39844616">next</a><span>|</span><label class="collapse" for="c-39843658">[-]</label><label class="expand" for="c-39843658">[1 more]</label></div><br/><div class="children"><div class="content">A lot of things has changed quite long ago - not everything is notebook, local dev is fully supported, version pinning wasn’t a problem, cluster startup time heavily dependent on underlying cloud provider, and serverless notebooks&#x2F;jobs are coming</div><br/></div></div><div id="39844616" class="c"><input type="checkbox" id="c-39844616" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#39840886">root</a><span>|</span><a href="#39843498">parent</a><span>|</span><a href="#39843658">prev</a><span>|</span><a href="#39841359">next</a><span>|</span><label class="collapse" for="c-39844616">[-]</label><label class="expand" for="c-39844616">[1 more]</label></div><br/><div class="children"><div class="content">Glad I’m not the only one. Especially with this notebook stuff they’re pushing. It’s an anti pattern I think.</div><br/></div></div></div></div><div id="39841359" class="c"><input type="checkbox" id="c-39841359" checked=""/><div class="controls bullet"><span class="by">melondonkey</span><span>|</span><a href="#39840886">parent</a><span>|</span><a href="#39843498">prev</a><span>|</span><a href="#39843518">next</a><span>|</span><label class="collapse" for="c-39841359">[-]</label><label class="expand" for="c-39841359">[5 more]</label></div><br/><div class="children"><div class="content">Data scientist here that’s also tired of the tools.  We put so much effort in trying to educate DSes in our company to get away from notebooks and use IDEs like VS or RStudio and databricks has been a step backwards cause we didn’t get the integrated version</div><br/><div id="39842367" class="c"><input type="checkbox" id="c-39842367" checked=""/><div class="controls bullet"><span class="by">mrtranscendence</span><span>|</span><a href="#39840886">root</a><span>|</span><a href="#39841359">parent</a><span>|</span><a href="#39843588">next</a><span>|</span><label class="collapse" for="c-39842367">[-]</label><label class="expand" for="c-39842367">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a data scientist and I agree that work meant to last should be in a source-controlled project coded via a text editor or IDE. But sometimes it&#x27;s <i>extremely</i> useful to get -- and iterate on -- immediate results. There&#x27;s no good way to do that without either notebooks or at least a REPL.</div><br/></div></div><div id="39843588" class="c"><input type="checkbox" id="c-39843588" checked=""/><div class="controls bullet"><span class="by">alexott</span><span>|</span><a href="#39840886">root</a><span>|</span><a href="#39841359">parent</a><span>|</span><a href="#39842367">prev</a><span>|</span><a href="#39842378">next</a><span>|</span><label class="collapse" for="c-39843588">[-]</label><label class="expand" for="c-39843588">[1 more]</label></div><br/><div class="children"><div class="content">There is VSCode extension, plus databricks-connect… plus DABs. There are a lot customers doing local only development</div><br/></div></div><div id="39842378" class="c"><input type="checkbox" id="c-39842378" checked=""/><div class="controls bullet"><span class="by">pandastronaut</span><span>|</span><a href="#39840886">root</a><span>|</span><a href="#39841359">parent</a><span>|</span><a href="#39843588">prev</a><span>|</span><a href="#39843518">next</a><span>|</span><label class="collapse" for="c-39842378">[-]</label><label class="expand" for="c-39842378">[2 more]</label></div><br/><div class="children"><div class="content">Thank you ! I am so tired of all those unmaintainable nor debugable notebooks.
Years ago, Databricks had a specific page on their documentation where they stated that notebooks where not for production grade software. It has been removed. And now you have a chatgpt like in their notebooks ... What a step backwards.
How can all those developers be so happy without having the bare minimum tools to diagnosis their code ? And I am not even talking about unit testing here.</div><br/><div id="39843619" class="c"><input type="checkbox" id="c-39843619" checked=""/><div class="controls bullet"><span class="by">alexott</span><span>|</span><a href="#39840886">root</a><span>|</span><a href="#39842378">parent</a><span>|</span><a href="#39843518">next</a><span>|</span><label class="collapse" for="c-39843619">[-]</label><label class="expand" for="c-39843619">[1 more]</label></div><br/><div class="children"><div class="content">It’s less about notebooks, but more about SDLC practices. Notebooks may encourage writing throwaway code, but if you split code correctly, then you can do unit testing, write modular code, etc. And ability to use “arbitrary files” as Python packages exists for quite a while, so you can get best of both worlds - quick iteration, plus ability to package your code as a wheel and distribute<p>P.S. here is a simple example of unit testing: <a href="https:&#x2F;&#x2F;github.com&#x2F;alexott&#x2F;databricks-nutter-repos-demo">https:&#x2F;&#x2F;github.com&#x2F;alexott&#x2F;databricks-nutter-repos-demo</a> - I wrote it more than three years ago.</div><br/></div></div></div></div></div></div><div id="39843518" class="c"><input type="checkbox" id="c-39843518" checked=""/><div class="controls bullet"><span class="by">VirusNewbie</span><span>|</span><a href="#39840886">parent</a><span>|</span><a href="#39841359">prev</a><span>|</span><a href="#39846956">next</a><span>|</span><label class="collapse" for="c-39843518">[-]</label><label class="expand" for="c-39843518">[1 more]</label></div><br/><div class="children"><div class="content">Spark is pretty well engineered and quite good.</div><br/></div></div></div></div><div id="39846956" class="c"><input type="checkbox" id="c-39846956" checked=""/><div class="controls bullet"><span class="by">petesergeant</span><span>|</span><a href="#39840886">prev</a><span>|</span><a href="#39844847">next</a><span>|</span><label class="collapse" for="c-39846956">[-]</label><label class="expand" for="c-39846956">[1 more]</label></div><br/><div class="children"><div class="content">This makes me bearish on OpenAI as a company. When a cloud company can offer a strong model for free by selling the compute, what competitive advantage does a company who want you to pay for the model have left? Feels like they might get Netscape’d.</div><br/></div></div><div id="39844847" class="c"><input type="checkbox" id="c-39844847" checked=""/><div class="controls bullet"><span class="by">airocker</span><span>|</span><a href="#39846956">prev</a><span>|</span><a href="#39839817">next</a><span>|</span><label class="collapse" for="c-39844847">[-]</label><label class="expand" for="c-39844847">[1 more]</label></div><br/><div class="children"><div class="content">is this also the ticker name when they IPO?</div><br/></div></div><div id="39839817" class="c"><input type="checkbox" id="c-39839817" checked=""/><div class="controls bullet"><span class="by">ingenieroariel</span><span>|</span><a href="#39844847">prev</a><span>|</span><a href="#39843324">next</a><span>|</span><label class="collapse" for="c-39839817">[-]</label><label class="expand" for="c-39839817">[5 more]</label></div><br/><div class="children"><div class="content">TLDR: A model that could be described as &quot;3.8 level&quot; that is good at math and openly available with a custom license.<p>It is as fast as 34B model, but uses as much memory as a 132B model. A mixture of 16 experts, activates 4 at a time, so has more chances to get the combo just right than Mixtral (8 with 2 active).<p>For my personal use case (a top of the line Mac Studio) it looks like the perfect size to replace GPT-4 turbo for programming tasks.  What we should look out for is people using them for real world programming tasks (instead of benchmarks) and reporting back.</div><br/><div id="39840229" class="c"><input type="checkbox" id="c-39840229" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#39839817">parent</a><span>|</span><a href="#39839869">prev</a><span>|</span><a href="#39843324">next</a><span>|</span><label class="collapse" for="c-39840229">[-]</label><label class="expand" for="c-39840229">[3 more]</label></div><br/><div class="children"><div class="content">What does 3.8 level mean?</div><br/><div id="39840282" class="c"><input type="checkbox" id="c-39840282" checked=""/><div class="controls bullet"><span class="by">ingenieroariel</span><span>|</span><a href="#39839817">root</a><span>|</span><a href="#39840229">parent</a><span>|</span><a href="#39840311">next</a><span>|</span><label class="collapse" for="c-39840282">[-]</label><label class="expand" for="c-39840282">[1 more]</label></div><br/><div class="children"><div class="content">My interpretation:<p>- Worst case: as good as 3.5
- Common case: way better than 3.5
- Best case: as good as 4.0</div><br/></div></div><div id="39840311" class="c"><input type="checkbox" id="c-39840311" checked=""/><div class="controls bullet"><span class="by">ljlolel</span><span>|</span><a href="#39839817">root</a><span>|</span><a href="#39840229">parent</a><span>|</span><a href="#39840282">prev</a><span>|</span><a href="#39843324">next</a><span>|</span><label class="collapse" for="c-39840311">[-]</label><label class="expand" for="c-39840311">[1 more]</label></div><br/><div class="children"><div class="content">Gpt-3.5 and gpt-4</div><br/></div></div></div></div></div></div><div id="39843324" class="c"><input type="checkbox" id="c-39843324" checked=""/><div class="controls bullet"><span class="by">zopper</span><span>|</span><a href="#39839817">prev</a><span>|</span><a href="#39842036">next</a><span>|</span><label class="collapse" for="c-39843324">[-]</label><label class="expand" for="c-39843324">[2 more]</label></div><br/><div class="children"><div class="content">Interesting that they haven&#x27;t release DBRX MoE-A and B. For many use-cases, smaller models are sufficient. Wonder why that is?</div><br/><div id="39845665" class="c"><input type="checkbox" id="c-39845665" checked=""/><div class="controls bullet"><span class="by">jfrankle</span><span>|</span><a href="#39843324">parent</a><span>|</span><a href="#39842036">next</a><span>|</span><label class="collapse" for="c-39845665">[-]</label><label class="expand" for="c-39845665">[1 more]</label></div><br/><div class="children"><div class="content">Honestly, just a matter of having the time to clean everything up and get it out. The ancillary code, model cards, etc. take a surprising amount of time.</div><br/></div></div></div></div><div id="39842036" class="c"><input type="checkbox" id="c-39842036" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#39843324">prev</a><span>|</span><a href="#39848270">next</a><span>|</span><label class="collapse" for="c-39842036">[-]</label><label class="expand" for="c-39842036">[6 more]</label></div><br/><div class="children"><div class="content">I’d like to know how Nancy Pelosi, who sure as hell doesn’t know what Apache Spark is, bought $1 million worth (and maybe $5million) of Databricks stock days ago.<p><a href="https:&#x2F;&#x2F;www.dailymail.co.uk&#x2F;sciencetech&#x2F;article-13228859&#x2F;amp&#x2F;nancy-pelosi-buys-software-companys-stocks-databricks.html" rel="nofollow">https:&#x2F;&#x2F;www.dailymail.co.uk&#x2F;sciencetech&#x2F;article-13228859&#x2F;amp...</a></div><br/><div id="39842907" class="c"><input type="checkbox" id="c-39842907" checked=""/><div class="controls bullet"><span class="by">BryantD</span><span>|</span><a href="#39842036">parent</a><span>|</span><a href="#39842595">next</a><span>|</span><label class="collapse" for="c-39842907">[-]</label><label class="expand" for="c-39842907">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have any interest in defending Pelosi&#x27;s stock trades, and I agree that sitting members of Congress should not be trading stocks.<p>That said, this report seems inaccurate to me. Pelosi put between 1 and 5 million dollars of Forge Investments, which is a method for investing in per-IPO companies, as I understand it. Databricks is one of those, but so is OpenAI, Hugging Face, Anthropic, and Humane. If I wanted to invest in pre-IPO AI companies it seems like a very natural choice and I don&#x27;t think we need insider trading to explain it.<p>It&#x27;s also the case that the report she filed calls out Databricks stock, which is perhaps an indication that she was particularly interested in that. Stronger reporting would tell us how often she&#x27;s invested in Forge, if this is the first time, and so on. One other possible explanation is that she was investing ahead of the Humane Pin shipping and wanted to pull attention away from it, for example.</div><br/></div></div><div id="39842595" class="c"><input type="checkbox" id="c-39842595" checked=""/><div class="controls bullet"><span class="by">hiddencost</span><span>|</span><a href="#39842036">parent</a><span>|</span><a href="#39842907">prev</a><span>|</span><a href="#39848270">next</a><span>|</span><label class="collapse" for="c-39842595">[-]</label><label class="expand" for="c-39842595">[4 more]</label></div><br/><div class="children"><div class="content">You know she has advisors, right?</div><br/><div id="39842697" class="c"><input type="checkbox" id="c-39842697" checked=""/><div class="controls bullet"><span class="by">PUSH_AX</span><span>|</span><a href="#39842036">root</a><span>|</span><a href="#39842595">parent</a><span>|</span><a href="#39842943">next</a><span>|</span><label class="collapse" for="c-39842697">[-]</label><label class="expand" for="c-39842697">[1 more]</label></div><br/><div class="children"><div class="content">I think the insinuation is insider trading due to the timing, advised or not.</div><br/></div></div><div id="39842943" class="c"><input type="checkbox" id="c-39842943" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#39842036">root</a><span>|</span><a href="#39842595">parent</a><span>|</span><a href="#39842697">prev</a><span>|</span><a href="#39844569">next</a><span>|</span><label class="collapse" for="c-39842943">[-]</label><label class="expand" for="c-39842943">[1 more]</label></div><br/><div class="children"><div class="content">Ignoring the snark: Obviously.<p>SEC put Martha Stewart in jail for following her advisor, and that was for about $45,000.</div><br/></div></div><div id="39844569" class="c"><input type="checkbox" id="c-39844569" checked=""/><div class="controls bullet"><span class="by">samatman</span><span>|</span><a href="#39842036">root</a><span>|</span><a href="#39842595">parent</a><span>|</span><a href="#39842943">prev</a><span>|</span><a href="#39848270">next</a><span>|</span><label class="collapse" for="c-39844569">[-]</label><label class="expand" for="c-39844569">[1 more]</label></div><br/><div class="children"><div class="content">If someone &quot;advises&quot; you that a company is about to do something major, and this isn&#x27;t public information, and you take action on the stock market accordingly, that&#x27;s insider trading.</div><br/></div></div></div></div></div></div><div id="39848270" class="c"><input type="checkbox" id="c-39848270" checked=""/><div class="controls bullet"><span class="by">grishka</span><span>|</span><a href="#39842036">prev</a><span>|</span><a href="#39841736">next</a><span>|</span><label class="collapse" for="c-39848270">[-]</label><label class="expand" for="c-39848270">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, you have been blocked<p>You are unable to access databricks.com<p>&quot;Open&quot;, right.</div><br/></div></div><div id="39841736" class="c"><input type="checkbox" id="c-39841736" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#39848270">prev</a><span>|</span><a href="#39841026">next</a><span>|</span><label class="collapse" for="c-39841736">[-]</label><label class="expand" for="c-39841736">[3 more]</label></div><br/><div class="children"><div class="content">These tiny “state of the art” performance increases are really indicative the current architecture for LLM(Transformers + Mixture of Experts) is maxed out even if you train it more&#x2F;differently.  The writings are on all over the walls.</div><br/><div id="39843030" class="c"><input type="checkbox" id="c-39843030" checked=""/><div class="controls bullet"><span class="by">wavemode</span><span>|</span><a href="#39841736">parent</a><span>|</span><a href="#39841026">next</a><span>|</span><label class="collapse" for="c-39843030">[-]</label><label class="expand" for="c-39843030">[2 more]</label></div><br/><div class="children"><div class="content">It would not surprise me if this is what has delayed OpenAI in releasing a new model. After more than a year since GPT-4, they may have by now produced some mega-trained mega-model, but running it is so expensive, and its eval improvement over GPT-4 so marginal, that releasing it to the public simply makes no commercial sense just yet.<p>They may be working on how to optimize it to reduce cost, or re-engineer it to improve evals.</div><br/><div id="39844096" class="c"><input type="checkbox" id="c-39844096" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#39841736">root</a><span>|</span><a href="#39843030">parent</a><span>|</span><a href="#39841026">next</a><span>|</span><label class="collapse" for="c-39844096">[-]</label><label class="expand" for="c-39844096">[1 more]</label></div><br/><div class="children"><div class="content">These “state of the art” llm barely eking out a win isn’t a threat to OpenAI and they can take their sweet time sharpening sword that will come down hard on these LLMs</div><br/></div></div></div></div></div></div><div id="39841026" class="c"><input type="checkbox" id="c-39841026" checked=""/><div class="controls bullet"><span class="by">saeleor</span><span>|</span><a href="#39841736">prev</a><span>|</span><a href="#39847161">next</a><span>|</span><label class="collapse" for="c-39841026">[-]</label><label class="expand" for="c-39841026">[3 more]</label></div><br/><div class="children"><div class="content">looks great, although I couldn&#x27;t find anything on how &quot;open&quot; the license is&#x2F;will be for commercial purposes<p>wouldn&#x27;t be the first branding as open source going the LLaMA route</div><br/><div id="39842445" class="c"><input type="checkbox" id="c-39842445" checked=""/><div class="controls bullet"><span class="by">wantsanagent</span><span>|</span><a href="#39841026">parent</a><span>|</span><a href="#39841688">next</a><span>|</span><label class="collapse" for="c-39842445">[-]</label><label class="expand" for="c-39842445">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s <i>another</i> custom license. It will have to be reviewed by counsel at every company that&#x27;s thinking about using it. Many will find the acceptable use policy to be vague, overly broad, and potentially damaging for the company.<p>Looking at the performance stats for this model, the risk of using any non-OSI licensed model over just using Mixtral or Mistral will (and IMO should be) too great for commercial purposes.</div><br/></div></div><div id="39841688" class="c"><input type="checkbox" id="c-39841688" checked=""/><div class="controls bullet"><span class="by">superdupershant</span><span>|</span><a href="#39841026">parent</a><span>|</span><a href="#39842445">prev</a><span>|</span><a href="#39847161">next</a><span>|</span><label class="collapse" for="c-39841688">[-]</label><label class="expand" for="c-39841688">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s similar to llama2.<p><pre><code>  &gt; If, on the DBRX version release date, the monthly active users of the products
  &gt; or services made available by or for Licensee, or Licensee’s affiliates, is
  &gt; greater than 700 million monthly active users in the preceding calendar 
  &gt; month, you must request a license from Databricks, which we may grant to you
  &gt; in our sole discretion, and you are not authorized to exercise any of the
  &gt; rights under this Agreement unless or until Databricks otherwise expressly
  &gt; grants you such rights.

</code></pre>
<a href="https:&#x2F;&#x2F;www.databricks.com&#x2F;legal&#x2F;open-model-license" rel="nofollow">https:&#x2F;&#x2F;www.databricks.com&#x2F;legal&#x2F;open-model-license</a></div><br/></div></div></div></div><div id="39847161" class="c"><input type="checkbox" id="c-39847161" checked=""/><div class="controls bullet"><span class="by">doubloon</span><span>|</span><a href="#39841026">prev</a><span>|</span><a href="#39838751">next</a><span>|</span><label class="collapse" for="c-39847161">[-]</label><label class="expand" for="c-39847161">[3 more]</label></div><br/><div class="children"><div class="content">really noob question - so to run on a GPU you need a 264GB RAM GPU? 
and if you ran on a 264GB CPU would it be super slow?</div><br/><div id="39847602" class="c"><input type="checkbox" id="c-39847602" checked=""/><div class="controls bullet"><span class="by">Jedd</span><span>|</span><a href="#39847161">parent</a><span>|</span><a href="#39847324">next</a><span>|</span><label class="collapse" for="c-39847602">[-]</label><label class="expand" for="c-39847602">[1 more]</label></div><br/><div class="children"><div class="content">Adding to ShamelessC&#x27;s answer - the other option is to wait for quantised versions of this model.  A q4 will be around 70GB, and probably acceptable.  A q5 or higher would be preferred, but we&#x27;re still a good way under the 260GB.<p>You still need extra RAM to breath, but that&#x27;s a lot more palatable.<p>This is why the Mac range - with unified memory - is appealing, as you can allocate most of your (say) 256GB of RAM to the GPU.<p>Conventional (desktop) CPU &#x2F; RAM would be painfully slow.</div><br/></div></div><div id="39847324" class="c"><input type="checkbox" id="c-39847324" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#39847161">parent</a><span>|</span><a href="#39847602">prev</a><span>|</span><a href="#39838751">next</a><span>|</span><label class="collapse" for="c-39847324">[-]</label><label class="expand" for="c-39847324">[1 more]</label></div><br/><div class="children"><div class="content">The model&#x27;s weights can be sharded across multiple GPU&#x27;s. A &quot;common&quot; training server could contain (for instance) eight &quot;A100&quot; GPU&#x27;s, each with 40 GB (or up to 80 GB) a piece for a total of 320 GB working VRAM. Since they&#x27;re connected to each other in the same PC, they can communicate with each other quickly enough to calculate in coordination in this fashion. This setup is _very_ expensive of course. Probably in the hundreds of thousands of dollars.<p>If you&#x27;re hoping to run the model yourself, you will need enough money and expertise to rent and deploy it to a server with as many GPU&#x27;s. Alternatively, volunteers and other researchers will be able to quantize (compress) the model and make it easier to run on configurations without as much VRAM.<p>If you ran it on CPU it may indeed be super slow, but it&#x27;s possible it&#x27;s fast enough for the purposes of running the model rather than trying to train that model. I am seeing (limited) success with the maxed out Mac lineup ($4500) using the beefy M1&#x2F;M2 line of CPU&#x27;s.</div><br/></div></div></div></div><div id="39838751" class="c"><input type="checkbox" id="c-39838751" checked=""/><div class="controls bullet"><span class="by">kurtbuilds</span><span>|</span><a href="#39847161">prev</a><span>|</span><a href="#39841236">next</a><span>|</span><label class="collapse" for="c-39838751">[-]</label><label class="expand" for="c-39838751">[1 more]</label></div><br/><div class="children"><div class="content">What’s the process to deliver and test a quantized version of this model?<p>This model is 264GB, so can only be deployed in server settings.<p>Quantized mixtral at 24G is just small enough where it can be running on premium consumer hardware (ie 64GB RAM)</div><br/></div></div><div id="39839068" class="c"><input type="checkbox" id="c-39839068" checked=""/><div class="controls bullet"><span class="by">viktour19</span><span>|</span><a href="#39841236">prev</a><span>|</span><a href="#39841415">next</a><span>|</span><label class="collapse" for="c-39839068">[-]</label><label class="expand" for="c-39839068">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s great how we went from &quot;wait.. this model is too powerful to open source&quot; to everyone trying to shove down their 1% improved model down the throats of developers</div><br/><div id="39839873" class="c"><input type="checkbox" id="c-39839873" checked=""/><div class="controls bullet"><span class="by">brainless</span><span>|</span><a href="#39839068">parent</a><span>|</span><a href="#39841866">next</a><span>|</span><label class="collapse" for="c-39839873">[-]</label><label class="expand" for="c-39839873">[1 more]</label></div><br/><div class="children"><div class="content">I feel quite the opposite. Improvements, even tiny ones are great. But what&#x27;s more important is that more companies release under open license.<p>Training models isn&#x27;t cheap. Individuals can&#x27;t easily do this, unlike software development. So we need companies to do this for the foreseeable future.</div><br/></div></div><div id="39841866" class="c"><input type="checkbox" id="c-39841866" checked=""/><div class="controls bullet"><span class="by">toddmorey</span><span>|</span><a href="#39839068">parent</a><span>|</span><a href="#39839873">prev</a><span>|</span><a href="#39839702">next</a><span>|</span><label class="collapse" for="c-39841866">[-]</label><label class="expand" for="c-39841866">[1 more]</label></div><br/><div class="children"><div class="content">People are building and releasing models. There&#x27;s active research in the space. I think that&#x27;s great! The attitude I&#x27;ve seen in open models is &quot;use this if it works for you&quot; vs any attempt to coerce usage of a particular model.<p>To me that&#x27;s what closed source companies (MSFT, Google) are doing as they try to force AI assistants into every corner of their product. (If LinkedIn tries one more time to push their crappy AI upgrade, I&#x27;m going to scream...)</div><br/></div></div><div id="39839702" class="c"><input type="checkbox" id="c-39839702" checked=""/><div class="controls bullet"><span class="by">Icko</span><span>|</span><a href="#39839068">parent</a><span>|</span><a href="#39841866">prev</a><span>|</span><a href="#39840097">next</a><span>|</span><label class="collapse" for="c-39839702">[-]</label><label class="expand" for="c-39839702">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m 90% certain that OpenAI has some much beefier model they are not releasing - remember the Q* rumour?</div><br/></div></div><div id="39840097" class="c"><input type="checkbox" id="c-39840097" checked=""/><div class="controls bullet"><span class="by">blitzar</span><span>|</span><a href="#39839068">parent</a><span>|</span><a href="#39839702">prev</a><span>|</span><a href="#39841415">next</a><span>|</span><label class="collapse" for="c-39840097">[-]</label><label class="expand" for="c-39840097">[1 more]</label></div><br/><div class="children"><div class="content">Got to justify pitch deck or stonk price. Publish or perish without a yacht.</div><br/></div></div></div></div><div id="39839837" class="c"><input type="checkbox" id="c-39839837" checked=""/><div class="controls bullet"><span class="by">hanniabu</span><span>|</span><a href="#39841415">prev</a><span>|</span><a href="#39838427">next</a><span>|</span><label class="collapse" for="c-39839837">[-]</label><label class="expand" for="c-39839837">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s a good model to help with medical research? Is there anything trained in just research journals, like NIH studies?</div><br/><div id="39840423" class="c"><input type="checkbox" id="c-39840423" checked=""/><div class="controls bullet"><span class="by">najarvg</span><span>|</span><a href="#39839837">parent</a><span>|</span><a href="#39838427">next</a><span>|</span><label class="collapse" for="c-39840423">[-]</label><label class="expand" for="c-39840423">[1 more]</label></div><br/><div class="children"><div class="content">Look for Biomistral 7B, PMC-LLAMA 7B and even Meditron. I believe you should find all those papers on arxiv</div><br/></div></div></div></div><div id="39839978" class="c"><input type="checkbox" id="c-39839978" checked=""/><div class="controls bullet"><span class="by">hn_acker</span><span>|</span><a href="#39838427">prev</a><span>|</span><label class="collapse" for="c-39839978">[-]</label><label class="expand" for="c-39839978">[12 more]</label></div><br/><div class="children"><div class="content">Even though the README.md calls the license the Databricks Open Source License, the LICENSE file includes paragraphs such as<p>&gt; You will not use DBRX or DBRX Derivatives or any Output to improve any other
large language model (excluding DBRX or DBRX Derivatives).<p>and<p>&gt; If, on the DBRX version release date, the monthly active users of the products
or services made available by or for Licensee, or Licensee’s affiliates, is
greater than 700 million monthly active users in the preceding calendar month,
you must request a license from Databricks, which we may grant to you in our
sole discretion, and you are not authorized to exercise any of the rights under
this Agreement unless or until Databricks otherwise expressly grants you such
rights.<p>This is a source-available model, not an open model.</div><br/><div id="39840133" class="c"><input type="checkbox" id="c-39840133" checked=""/><div class="controls bullet"><span class="by">CharlesW</span><span>|</span><a href="#39839978">parent</a><span>|</span><a href="#39847205">next</a><span>|</span><label class="collapse" for="c-39840133">[-]</label><label class="expand" for="c-39840133">[5 more]</label></div><br/><div class="children"><div class="content">&gt; <i>This is a source-available model, not an open model.</i><p>To me, &quot;source available&quot; implies that everything you need to reproduce the model is also available, and that doesn&#x27;t appear to be the case. How is the resulting model more &quot;free as in freedom&quot; than a compiled binary?</div><br/><div id="39840765" class="c"><input type="checkbox" id="c-39840765" checked=""/><div class="controls bullet"><span class="by">occamrazor</span><span>|</span><a href="#39839978">root</a><span>|</span><a href="#39840133">parent</a><span>|</span><a href="#39841170">next</a><span>|</span><label class="collapse" for="c-39840765">[-]</label><label class="expand" for="c-39840765">[1 more]</label></div><br/><div class="children"><div class="content">I like:<p>- “open weights” for no training data and no restrictions on use,<p>- “weights available” for no training data and restrictions on use, like in this case.</div><br/></div></div><div id="39841170" class="c"><input type="checkbox" id="c-39841170" checked=""/><div class="controls bullet"><span class="by">Spivak</span><span>|</span><a href="#39839978">root</a><span>|</span><a href="#39840133">parent</a><span>|</span><a href="#39840765">prev</a><span>|</span><a href="#39847205">next</a><span>|</span><label class="collapse" for="c-39841170">[-]</label><label class="expand" for="c-39841170">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s possible to have an &quot;open training data&quot; model because it would get DMCA&#x27;d immediately and open you up to lawsuits from everyone who found their works in the training set.<p>I hope we can fix the legal landscape to enable publicly sharing training data but I can&#x27;t really judge the companies keeping it a secret today.</div><br/><div id="39842651" class="c"><input type="checkbox" id="c-39842651" checked=""/><div class="controls bullet"><span class="by">CharlesW</span><span>|</span><a href="#39839978">root</a><span>|</span><a href="#39841170">parent</a><span>|</span><a href="#39847205">next</a><span>|</span><label class="collapse" for="c-39842651">[-]</label><label class="expand" for="c-39842651">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>I don&#x27;t think it&#x27;s possible to have an &quot;open training data&quot; model because it would get DMCA&#x27;d immediately…</i><p>This isn&#x27;t a problem because OpenAI says, &quot;training AI models using publicly available internet materials is fair use&quot;. &#x2F;s<p><a href="https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;openai-and-journalism" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;openai-and-journalism</a></div><br/><div id="39843804" class="c"><input type="checkbox" id="c-39843804" checked=""/><div class="controls bullet"><span class="by">Spivak</span><span>|</span><a href="#39839978">root</a><span>|</span><a href="#39842651">parent</a><span>|</span><a href="#39847205">next</a><span>|</span><label class="collapse" for="c-39843804">[-]</label><label class="expand" for="c-39843804">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s that crazy, even if you&#x27;re sure it&#x27;s fair use I wouldn&#x27;t paint a huge target on my back before there&#x27;s a definite ruling and I doubly wouldn&#x27;t test the waters of the legality of re-hosting copyrighted content to be downloaded by randos who won&#x27;t be training models with it.<p>If they&#x27;re going to get away with this collecting data and having a legal chain-of-custody so you can actually say it was only used to train models and no one else has access to it goes a long way.</div><br/></div></div></div></div></div></div></div></div><div id="39847205" class="c"><input type="checkbox" id="c-39847205" checked=""/><div class="controls bullet"><span class="by">Zuiii</span><span>|</span><a href="#39839978">parent</a><span>|</span><a href="#39840133">prev</a><span>|</span><a href="#39842351">next</a><span>|</span><label class="collapse" for="c-39847205">[-]</label><label class="expand" for="c-39847205">[1 more]</label></div><br/><div class="children"><div class="content">1. Open source is a well-defined model and I reasonably expect Databricks to be aware of this due to their use of open source models in their other projects.<p>2. The stated licensing terms are clearly and decisively not open source.<p>3. It is reasonable to conclude that this model is dual licensed, under this restrictive proprietary license, and an undisclosed open source license.<p>4. Just use this Model under the open source license with the assumption that they will release the open source license later.<p>I jest. In all seriousness, you should just disregard their licensing terms entirely as copyright does not apply to weight. <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39847147">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39847147</a></div><br/></div></div><div id="39842351" class="c"><input type="checkbox" id="c-39842351" checked=""/><div class="controls bullet"><span class="by">hn_acker</span><span>|</span><a href="#39839978">parent</a><span>|</span><a href="#39847205">prev</a><span>|</span><a href="#39840045">next</a><span>|</span><label class="collapse" for="c-39842351">[-]</label><label class="expand" for="c-39842351">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, I forgot to link the repository [1] and missed the edit window by the time I realized.<p>The bottom of the README.md [2] contains the following license grant with the misleading &quot;Open Source&quot; term:<p>&gt; License<p>&gt; Our model weights and code are licensed for both researchers and commercial entities. The Databricks Open Source License can be found at LICENSE, and our Acceptable Use Policy can be found here.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;databricks&#x2F;dbrx">https:&#x2F;&#x2F;github.com&#x2F;databricks&#x2F;dbrx</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;databricks&#x2F;dbrx&#x2F;blob&#x2F;main&#x2F;README.md">https:&#x2F;&#x2F;github.com&#x2F;databricks&#x2F;dbrx&#x2F;blob&#x2F;main&#x2F;README.md</a></div><br/></div></div><div id="39840045" class="c"><input type="checkbox" id="c-39840045" checked=""/><div class="controls bullet"><span class="by">yunohn</span><span>|</span><a href="#39839978">parent</a><span>|</span><a href="#39842351">prev</a><span>|</span><a href="#39840531">next</a><span>|</span><label class="collapse" for="c-39840045">[-]</label><label class="expand" for="c-39840045">[1 more]</label></div><br/><div class="children"><div class="content">The first clause sucks, but I’m perfectly happy with the second one.</div><br/></div></div><div id="39840531" class="c"><input type="checkbox" id="c-39840531" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39839978">parent</a><span>|</span><a href="#39840045">prev</a><span>|</span><a href="#39840605">next</a><span>|</span><label class="collapse" for="c-39840531">[-]</label><label class="expand" for="c-39840531">[1 more]</label></div><br/><div class="children"><div class="content">identical to llama fwiw</div><br/></div></div><div id="39840605" class="c"><input type="checkbox" id="c-39840605" checked=""/><div class="controls bullet"><span class="by">adolph</span><span>|</span><a href="#39839978">parent</a><span>|</span><a href="#39840531">prev</a><span>|</span><a href="#39840590">next</a><span>|</span><label class="collapse" for="c-39840605">[-]</label><label class="expand" for="c-39840605">[1 more]</label></div><br/><div class="children"><div class="content">Maybe the license is “open” as in a can of beer, not OSS.</div><br/></div></div></div></div></div></div></div></div></div></body></html>