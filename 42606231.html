<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1736154058433" as="style"/><link rel="stylesheet" href="styles.css?v=1736154058433"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://r0bk.github.io/killedbyllm/">Killed by LLM</a> <span class="domain">(<a href="https://r0bk.github.io">r0bk.github.io</a>)</span></div><div class="subtext"><span>yz-exodao</span> | <span>65 comments</span></div><br/><div><div id="42607821" class="c"><input type="checkbox" id="c-42607821" checked=""/><div class="controls bullet"><span class="by">Ukv</span><span>|</span><a href="#42607078">next</a><span>|</span><label class="collapse" for="c-42607821">[-]</label><label class="expand" for="c-42607821">[11 more]</label></div><br/><div class="children"><div class="content">IMO a critical feature of the Turing test&#x2F;imitation game, which many modern implementations including this site&#x27;s linked paper ignore, is that the interrogator talks to both a human and a bot and must decide that one xor the other is a human. So fooling an interrogator means having them choose the bot as human <i>over</i> an actual human, not just judging the bot to be human (while probably judging humans to be human even more frequently).<p>When the interrogator is only answering &quot;do you think your conversation partner was a human?&quot; individually, bots can score fairly highly simply by giving little information in either direction - like pretending to be a non-english-speaking child, or sending very few messages.<p>Whereas when pitted against a human, the bot is forced to give stronger or equally strong evidence of being human as the average human (over enough tests). To be chosen as human, giving 0 evidence becomes a bad strategy when the opponent (the real human) is likely giving some positive non-zero evidence towards their personhood.</div><br/><div id="42608106" class="c"><input type="checkbox" id="c-42608106" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#42607821">parent</a><span>|</span><a href="#42607855">next</a><span>|</span><label class="collapse" for="c-42608106">[-]</label><label class="expand" for="c-42608106">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not the original Turing test either. The original imitation game as proposed by Turing involves reading a text transcript of a human and a computer and having the evaluator determine which is which. The evaluator does not interact directly with the conversing parties.</div><br/><div id="42608547" class="c"><input type="checkbox" id="c-42608547" checked=""/><div class="controls bullet"><span class="by">tripletao</span><span>|</span><a href="#42607821">root</a><span>|</span><a href="#42608106">parent</a><span>|</span><a href="#42607855">next</a><span>|</span><label class="collapse" for="c-42608547">[-]</label><label class="expand" for="c-42608547">[1 more]</label></div><br/><div class="children"><div class="content">Where are you getting that? Turing&#x27;s most famous paper is just as Ukv describes. The link on that site doesn&#x27;t work for me, but the reference is buried in their source:<p><a href="https:&#x2F;&#x2F;courses.cs.umbc.edu&#x2F;471&#x2F;papers&#x2F;turing.pdf" rel="nofollow">https:&#x2F;&#x2F;courses.cs.umbc.edu&#x2F;471&#x2F;papers&#x2F;turing.pdf</a><p>In Turing&#x27;s test, the forced binary choice means P(human-judged-human) + P(machine-judged-human) is necessarily equal to 100%. This gives the 50% threshold clear intuitive and mathematical significance.<p>In the bastardized test that GPT-4 &quot;passed&quot;, that sum can be (and actually was) &gt;100%. This makes the result practically impossible to interpret, since it depends on the interrogators&#x27; prior. The correct prior seems to be that it was human with p = 25%, though the paper doesn&#x27;t say that explicitly, or say anything about what the interrogators were told. If the interrogators guessed mistakenly that it was 50% then that would lead them to systematically misjudge machines as humans, perhaps as observed.<p>The bastardized test is pretty bad, but treating the 50% threshold as meaningful there is inexcusable. I see the preprint hasn&#x27;t yet passed peer review, and I&#x27;ll regain some faith in social science professors if it never does. Of course the credulous media coverage is everywhere already, including the LLM training sets--so regardless of whether LLMs can pass the Turing test, they now believe they do.</div><br/></div></div></div></div><div id="42607855" class="c"><input type="checkbox" id="c-42607855" checked=""/><div class="controls bullet"><span class="by">silisili</span><span>|</span><a href="#42607821">parent</a><span>|</span><a href="#42608106">prev</a><span>|</span><a href="#42607078">next</a><span>|</span><label class="collapse" for="c-42607855">[-]</label><label class="expand" for="c-42607855">[8 more]</label></div><br/><div class="children"><div class="content">I&#x27;m skeptical on the claim.  I think most folks, given the test you describe, would be able to pick out which is human.  I think it can get there, but I&#x27;m not sure anyone has made one yet.  ChatGPT responses are heavily downvoted and mocked because they&#x27;re easy to spot.<p>Does there exist a public LLM that isn&#x27;t so...wordy, excited, and guardrailed all the time?<p>You can pretty much spot the bot today by prompting something horribly offensive.  Their response is always very inhuman, probably due to lack of emotional energy.</div><br/><div id="42607953" class="c"><input type="checkbox" id="c-42607953" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#42607821">root</a><span>|</span><a href="#42607855">parent</a><span>|</span><a href="#42608395">next</a><span>|</span><label class="collapse" for="c-42607953">[-]</label><label class="expand" for="c-42607953">[1 more]</label></div><br/><div class="children"><div class="content">I agree but that&#x27;s not really a <i>scientific</i> limitation though, right? As I understand it in the early days of GPT 4, before it was publicly released and RLHF&#x27;d for brand safety, it would have offered convincing text completions for just about any context, whether an academic discussion of philosophy or a steamy crossover fanfiction or a reddit trash-talk exchange. It took a deliberate bit of lobotomizing to make them so bland, conservative, and cheery-helpful.<p>The required investment probably means it will be a while before any less brand- and legal-action-conscious actors offer up unrestrained foundation models of comparable quality, but it&#x27;s only a matter of time, isn&#x27;t it?</div><br/></div></div><div id="42608395" class="c"><input type="checkbox" id="c-42608395" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#42607821">root</a><span>|</span><a href="#42607855">parent</a><span>|</span><a href="#42607953">prev</a><span>|</span><a href="#42607878">next</a><span>|</span><label class="collapse" for="c-42608395">[-]</label><label class="expand" for="c-42608395">[1 more]</label></div><br/><div class="children"><div class="content">&quot;pretending to be a non-english-speaking child&quot; isn&#x27;t a hypothetical, it&#x27;s a real tactic that was annoyingly effective a while back.<p>Being uncooperative makes it really hard to tell anything about you, including whether you&#x27;re real.</div><br/></div></div><div id="42607878" class="c"><input type="checkbox" id="c-42607878" checked=""/><div class="controls bullet"><span class="by">seanmcdirmid</span><span>|</span><a href="#42607821">root</a><span>|</span><a href="#42607855">parent</a><span>|</span><a href="#42608395">prev</a><span>|</span><a href="#42608722">next</a><span>|</span><label class="collapse" for="c-42607878">[-]</label><label class="expand" for="c-42607878">[2 more]</label></div><br/><div class="children"><div class="content">Aren&#x27;t you just describing those emails in a big corp that are supposedly still written by humans? Yes, they are wordy, excited, and guardrailed, but I don&#x27;t think they are written by AI yet.<p>I guess this is why LLMs are so feared by high school English teachers. Yes, they don&#x27;t write well, but neither do their students.</div><br/><div id="42608464" class="c"><input type="checkbox" id="c-42608464" checked=""/><div class="controls bullet"><span class="by">brabel</span><span>|</span><a href="#42607821">root</a><span>|</span><a href="#42607878">parent</a><span>|</span><a href="#42608722">next</a><span>|</span><label class="collapse" for="c-42608464">[-]</label><label class="expand" for="c-42608464">[1 more]</label></div><br/><div class="children"><div class="content">Nowadays, it&#x27;s usually the opposite: if the text is too good, everyone starts accusing it of being written by AI.</div><br/></div></div></div></div><div id="42608722" class="c"><input type="checkbox" id="c-42608722" checked=""/><div class="controls bullet"><span class="by">dathinab</span><span>|</span><a href="#42607821">root</a><span>|</span><a href="#42607855">parent</a><span>|</span><a href="#42607878">prev</a><span>|</span><a href="#42608121">next</a><span>|</span><label class="collapse" for="c-42608722">[-]</label><label class="expand" for="c-42608722">[1 more]</label></div><br/><div class="children"><div class="content">easy to spot by you and other people involved in tech<p>but the test subjects should be randomly samples from society at which point the skill availability&#x2F;level of spotting it goes majorly down</div><br/></div></div><div id="42608121" class="c"><input type="checkbox" id="c-42608121" checked=""/><div class="controls bullet"><span class="by">chriscappuccio</span><span>|</span><a href="#42607821">root</a><span>|</span><a href="#42607855">parent</a><span>|</span><a href="#42608722">prev</a><span>|</span><a href="#42608122">next</a><span>|</span><label class="collapse" for="c-42608121">[-]</label><label class="expand" for="c-42608121">[1 more]</label></div><br/><div class="children"><div class="content">You can get rid of OpenAI&#x27;s wordy, excited and guardrailed responses with the eigenrobot prompt, for instance.<p><a href="https:&#x2F;&#x2F;x.com&#x2F;eigenrobot&#x2F;status&#x2F;1870696676819640348" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;eigenrobot&#x2F;status&#x2F;1870696676819640348</a><p>I generally prefer it to the default. It doesn&#x27;t work as well on Claude or Grok for various reasons. I think it really shines on GPT o1-mini and GPT 4o.</div><br/></div></div><div id="42608122" class="c"><input type="checkbox" id="c-42608122" checked=""/><div class="controls bullet"><span class="by">fastball</span><span>|</span><a href="#42607821">root</a><span>|</span><a href="#42607855">parent</a><span>|</span><a href="#42608121">prev</a><span>|</span><a href="#42607078">next</a><span>|</span><label class="collapse" for="c-42608122">[-]</label><label class="expand" for="c-42608122">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a lack of emotional energy, it is the guardrails you point out. All of the SotA models are heavily fine-tuned <i>to be botlike</i>, and even then they are fooling people. If you had an LLM fine-tuned with RLHF to deliberately confuse humans in a Turing test it seems clear it would do a good job.</div><br/></div></div></div></div></div></div><div id="42607078" class="c"><input type="checkbox" id="c-42607078" checked=""/><div class="controls bullet"><span class="by">lamename</span><span>|</span><a href="#42607821">prev</a><span>|</span><a href="#42607080">next</a><span>|</span><label class="collapse" for="c-42607078">[-]</label><label class="expand" for="c-42607078">[5 more]</label></div><br/><div class="children"><div class="content">Posted by Chollet himself:<p>&gt; I don&#x27;t think people really appreciate how simple ARC-AGI-1 was, and what solving it really means.
It was designed as the simplest, most basic assessment of fluid intelligence possible. Failure to pass signifies a near-total inability to adapt or problem-solve in unfamiliar situations.<p>&gt; Passing it means your system exhibits non-zero fluid intelligence -- you&#x27;re finally looking at something that isn&#x27;t pure memorized skill. But it says rather little about how intelligent your system is, or how close to human intelligence it is.<p><a href="https:&#x2F;&#x2F;bsky.app&#x2F;profile&#x2F;fchollet.bsky.social&#x2F;post&#x2F;3les3izgdj22j" rel="nofollow">https:&#x2F;&#x2F;bsky.app&#x2F;profile&#x2F;fchollet.bsky.social&#x2F;post&#x2F;3les3izgd...</a></div><br/><div id="42607323" class="c"><input type="checkbox" id="c-42607323" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#42607078">parent</a><span>|</span><a href="#42607861">next</a><span>|</span><label class="collapse" for="c-42607323">[-]</label><label class="expand" for="c-42607323">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Failure to pass signifies a near-total inability to adapt or problem-solve in unfamiliar situations.<p>Not necessarily. Get a human to solve ARC-AGI if the problems are shown as a string. They&#x27;ll perform badly. But that doesn&#x27;t mean that humans can&#x27;t reason. It means that human reasoning doesn&#x27;t have access to the non-reasoning building blocks it needs (things like concepts, words, or in this case: spatially local and useful visual representations).<p>Humans have good resolution-invariant visual perception. For example, take an ARC-AGI problem, and for each square, duplicate it a few times, increasing its resolution from X*X to 2X*2X. To a human, the problem will be almost exactly equally difficulty. Not for LLMs that have to deal with 4x as much context. Maybe for an LLM if it can somehow reason over the output of a CNN, and if it was trained to do that like how humans are built to do that.</div><br/><div id="42607863" class="c"><input type="checkbox" id="c-42607863" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42607078">root</a><span>|</span><a href="#42607323">parent</a><span>|</span><a href="#42608239">next</a><span>|</span><label class="collapse" for="c-42607863">[-]</label><label class="expand" for="c-42607863">[1 more]</label></div><br/><div class="children"><div class="content">Excellent point, I&#x27;m not sure people are aware, but these are straight-up lifted from standard IQ tests<i>, so they&#x27;re definitely not all trivially humanly solvable.<p></i> I needed an official one for medical reasons a few years back</div><br/></div></div><div id="42608239" class="c"><input type="checkbox" id="c-42608239" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#42607078">root</a><span>|</span><a href="#42607323">parent</a><span>|</span><a href="#42607863">prev</a><span>|</span><a href="#42607861">next</a><span>|</span><label class="collapse" for="c-42608239">[-]</label><label class="expand" for="c-42608239">[1 more]</label></div><br/><div class="children"><div class="content">ARC-AGI feels like it would fall to a higher dimensional convolution rather than reasoning.</div><br/></div></div></div></div><div id="42607861" class="c"><input type="checkbox" id="c-42607861" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#42607078">parent</a><span>|</span><a href="#42607323">prev</a><span>|</span><a href="#42607080">next</a><span>|</span><label class="collapse" for="c-42607861">[-]</label><label class="expand" for="c-42607861">[1 more]</label></div><br/><div class="children"><div class="content">Honestly, after that, I&#x27;m tuned out completely on him and ARC-AGI. Nice minor sidestory at one point in time.<p>He&#x27;s right that this isn&#x27;t solving all human-intelligence domain level problems.<p>But the whole stunt, this whole time, was that <i>this</i> was the ARC-<i>AGI</i> benchmark.<p>The conceit was the fact LLMs couldn&#x27;t do well on it proved they weren&#x27;t intelligent. And real researchers would step up to bench well on that, avoiding the ideological tarpit of LLMs, which could never be intelligent.<p>It&#x27;s fine to turn around and say &quot;My AGI benchmark says little about intelligence&quot;, but, the level of conversation is decidedly more that of punters at the local stables than rigorous analysis.</div><br/></div></div></div></div><div id="42607080" class="c"><input type="checkbox" id="c-42607080" checked=""/><div class="controls bullet"><span class="by">0xDEAFBEAD</span><span>|</span><a href="#42607078">prev</a><span>|</span><a href="#42607527">next</a><span>|</span><label class="collapse" for="c-42607080">[-]</label><label class="expand" for="c-42607080">[5 more]</label></div><br/><div class="children"><div class="content">I assumed this was about chatbot users committing suicide in order to &quot;join&quot; the bot they are chatting with.  It&#x27;s already happened a couple of times, apparently:<p><a href="https:&#x2F;&#x2F;futurism.com&#x2F;teen-suicide-obsessed-ai-chatbot" rel="nofollow">https:&#x2F;&#x2F;futurism.com&#x2F;teen-suicide-obsessed-ai-chatbot</a><p><a href="https:&#x2F;&#x2F;garymarcus.substack.com&#x2F;p&#x2F;the-first-known-chatbot-associated" rel="nofollow">https:&#x2F;&#x2F;garymarcus.substack.com&#x2F;p&#x2F;the-first-known-chatbot-as...</a></div><br/><div id="42608113" class="c"><input type="checkbox" id="c-42608113" checked=""/><div class="controls bullet"><span class="by">cdev_gl</span><span>|</span><a href="#42607080">parent</a><span>|</span><a href="#42607438">next</a><span>|</span><label class="collapse" for="c-42608113">[-]</label><label class="expand" for="c-42608113">[1 more]</label></div><br/><div class="children"><div class="content">Yea, I too was not expecting a list of past benchmarks. If not the aforementioned actual human deaths, I had expected either a list of companies whose pivot to AI&#x2F;LLMs led to their downfall (but I guess we&#x27;re going to need to wait a year or two for that) or a list of industries (such as audio transcription) that are being killed by AI as we speak.<p>We really do live in interesting times. Usually I feel pretty confident about predicting how a trend will continue, but as it is the only prediction I can make with confidence for this latest AI research is that it is and will be used by militaries to kill a lot of people. Oh, hey, that&#x27;s another thing this article could have listed!<p>Outside of that, all bets are open. Possible wagers include: &quot;Turns out to be mostly useful in specific niche applications and only seemingly useful anywhere else&quot;, &quot;Extremely useful for businesses looking to offset responsibility for unpopular decisions&quot;, &quot;Ushers in an end to work and a golden age for all mankind&quot;, &quot;Ushers in an end to work and a dark age for most of the world&quot;, &quot;Combines with profit motives to damage all art, culture, and community&quot;, etc etc.<p>I know many folk have strong opinions one way or the other, but I think it&#x27;s literally anyone&#x27;s game at this point, though I will say I&#x27;m not leaning optimistic.</div><br/></div></div><div id="42607438" class="c"><input type="checkbox" id="c-42607438" checked=""/><div class="controls bullet"><span class="by">sam0x17</span><span>|</span><a href="#42607080">parent</a><span>|</span><a href="#42608113">prev</a><span>|</span><a href="#42607862">next</a><span>|</span><label class="collapse" for="c-42607438">[-]</label><label class="expand" for="c-42607438">[1 more]</label></div><br/><div class="children"><div class="content">Similarly I thought it would be about ML and data projects that have become defunct due to the advent of LLMs.</div><br/></div></div><div id="42607862" class="c"><input type="checkbox" id="c-42607862" checked=""/><div class="controls bullet"><span class="by">nayuki</span><span>|</span><a href="#42607080">parent</a><span>|</span><a href="#42607438">prev</a><span>|</span><a href="#42608852">next</a><span>|</span><label class="collapse" for="c-42607862">[-]</label><label class="expand" for="c-42607862">[1 more]</label></div><br/><div class="children"><div class="content">I thought the title meant that a chatbot gave bad medical, engineering, and&#x2F;or safety-critical advice that a human ended up following.</div><br/></div></div><div id="42608852" class="c"><input type="checkbox" id="c-42608852" checked=""/><div class="controls bullet"><span class="by">rasz</span><span>|</span><a href="#42607080">parent</a><span>|</span><a href="#42607862">prev</a><span>|</span><a href="#42607527">next</a><span>|</span><label class="collapse" for="c-42608852">[-]</label><label class="expand" for="c-42608852">[1 more]</label></div><br/><div class="children"><div class="content">Using people with severe mental health problems might be a poor benchmark of performance.</div><br/></div></div></div></div><div id="42607527" class="c"><input type="checkbox" id="c-42607527" checked=""/><div class="controls bullet"><span class="by">ultrablack</span><span>|</span><a href="#42607080">prev</a><span>|</span><a href="#42608862">next</a><span>|</span><label class="collapse" for="c-42607527">[-]</label><label class="expand" for="c-42607527">[4 more]</label></div><br/><div class="children"><div class="content">The tortoise lays on its back, its belly baking in the hot sun, beating its legs trying to turn itself over, but it can&#x27;t. Not without your help. But you&#x27;re not helping.</div><br/><div id="42607548" class="c"><input type="checkbox" id="c-42607548" checked=""/><div class="controls bullet"><span class="by">mindcrime</span><span>|</span><a href="#42607527">parent</a><span>|</span><a href="#42608862">next</a><span>|</span><label class="collapse" for="c-42607548">[-]</label><label class="expand" for="c-42607548">[3 more]</label></div><br/><div class="children"><div class="content">Describe in single words, only the good things that come into your mind about your mother.</div><br/><div id="42607840" class="c"><input type="checkbox" id="c-42607840" checked=""/><div class="controls bullet"><span class="by">mmustapic</span><span>|</span><a href="#42607527">root</a><span>|</span><a href="#42607548">parent</a><span>|</span><a href="#42607756">next</a><span>|</span><label class="collapse" for="c-42607840">[-]</label><label class="expand" for="c-42607840">[1 more]</label></div><br/><div class="children"><div class="content">Let me tell you about my mother</div><br/></div></div></div></div></div></div><div id="42608862" class="c"><input type="checkbox" id="c-42608862" checked=""/><div class="controls bullet"><span class="by">yamrzou</span><span>|</span><a href="#42607527">prev</a><span>|</span><a href="#42606663">next</a><span>|</span><label class="collapse" for="c-42608862">[-]</label><label class="expand" for="c-42608862">[1 more]</label></div><br/><div class="children"><div class="content"><i>ARC-AGI</i> is not yet killed by LLM. O3 achieved a breakthrough only on <i>ARC-AGI-PUB</i>, which is semi-private. Nothing guarantees that the test data wasn&#x27;t leaked to OpenAI in previous testing rounds, because the model is not running offline.<p>See: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42478098">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42478098</a></div><br/></div></div><div id="42606663" class="c"><input type="checkbox" id="c-42606663" checked=""/><div class="controls bullet"><span class="by">matt3210</span><span>|</span><a href="#42608862">prev</a><span>|</span><a href="#42607671">next</a><span>|</span><label class="collapse" for="c-42606663">[-]</label><label class="expand" for="c-42606663">[5 more]</label></div><br/><div class="children"><div class="content">I read recently that small variations in the tests cause failures by large margins.<p>If this doesn’t show over fitting in don’t know what would.</div><br/><div id="42607260" class="c"><input type="checkbox" id="c-42607260" checked=""/><div class="controls bullet"><span class="by">friend_Fernando</span><span>|</span><a href="#42606663">parent</a><span>|</span><a href="#42606698">next</a><span>|</span><label class="collapse" for="c-42607260">[-]</label><label class="expand" for="c-42607260">[1 more]</label></div><br/><div class="children"><div class="content">Eventually, all the better AGI tests should have large private evaluation datasets with no possible cheating or feedback loops. We&#x27;re getting there.</div><br/></div></div><div id="42606698" class="c"><input type="checkbox" id="c-42606698" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#42606663">parent</a><span>|</span><a href="#42607260">prev</a><span>|</span><a href="#42607671">next</a><span>|</span><label class="collapse" for="c-42606698">[-]</label><label class="expand" for="c-42606698">[3 more]</label></div><br/><div class="children"><div class="content">Wasn’t that for human tests, i.e. not specifically AI benchmarks? Benchmarks should generally not be game-able by overfitting.</div><br/><div id="42606907" class="c"><input type="checkbox" id="c-42606907" checked=""/><div class="controls bullet"><span class="by">matt3210</span><span>|</span><a href="#42606663">root</a><span>|</span><a href="#42606698">parent</a><span>|</span><a href="#42607671">next</a><span>|</span><label class="collapse" for="c-42606907">[-]</label><label class="expand" for="c-42606907">[2 more]</label></div><br/><div class="children"><div class="content">The article shows all the tests against human performance.<p>The math one in particular is the one where small variations reduce the success rate significantly. I can’t find the source but it was pasted here in the last 2 weeks.</div><br/><div id="42606982" class="c"><input type="checkbox" id="c-42606982" checked=""/><div class="controls bullet"><span class="by">CrazyStat</span><span>|</span><a href="#42606663">root</a><span>|</span><a href="#42606907">parent</a><span>|</span><a href="#42607671">next</a><span>|</span><label class="collapse" for="c-42606982">[-]</label><label class="expand" for="c-42606982">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re probably remembering this: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42565606">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42565606</a></div><br/></div></div></div></div></div></div></div></div><div id="42607671" class="c"><input type="checkbox" id="c-42607671" checked=""/><div class="controls bullet"><span class="by">sinuhe69</span><span>|</span><a href="#42606663">prev</a><span>|</span><a href="#42607243">next</a><span>|</span><label class="collapse" for="c-42607671">[-]</label><label class="expand" for="c-42607671">[5 more]</label></div><br/><div class="children"><div class="content">I find that MATH challenge “solved” by AI hard to believe. The reason given was “saturation”. Could anybody help explain it a bit?
Also in my daily encounter, I stop find a lot of simple math problems all the frontier models could not solve: long logic puzzle, many cases reasoning, and particularly geometry problems. I don’t know where the 97% number for o1 does come from, but in my experience they are much lower than that and math, even elementary maths certainly can not be considered to be “solved”.
As far as I can see, OpenAI has been trained their models on all these public problems, so testing on them to record a benchmark is tainted as best when not outright cheating.</div><br/><div id="42607747" class="c"><input type="checkbox" id="c-42607747" checked=""/><div class="controls bullet"><span class="by">Taek</span><span>|</span><a href="#42607671">parent</a><span>|</span><a href="#42607801">next</a><span>|</span><label class="collapse" for="c-42607747">[-]</label><label class="expand" for="c-42607747">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve found o1 to be entirely useful at math problems that are beyond my own (admittedly modest) skills. I&#x27;ve had it write full proofs of correctness for me (one shot, verified), I&#x27;ve had it optimize equations to reduce necessary precision, I&#x27;ve had it optimize equations to remove specific expensive operations (making them computationally more efficient), and finally I&#x27;ve had it prove a handful of my conjectures, which was helpful for taking algorithmic shortcuts in a security sensitive environment.<p>Mostly all algebra and calculus, but definitely all problems that most undergrads would struggle with.<p>It&#x27;s most useful because it has deep knowledge of related and adjacent conjectures that are well understood, even if you&#x27;ve never heard of them. So it can mix and match things with a lot more ease than a tinkering mathematician</div><br/><div id="42608572" class="c"><input type="checkbox" id="c-42608572" checked=""/><div class="controls bullet"><span class="by">hgomersall</span><span>|</span><a href="#42607671">root</a><span>|</span><a href="#42607747">parent</a><span>|</span><a href="#42608753">next</a><span>|</span><label class="collapse" for="c-42608572">[-]</label><label class="expand" for="c-42608572">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I had (so far apparent, but still be verified) success with o1 teaching me the necessary physics and maths I need to solve my specific problem. This is definitely grad level stuff but well understood. My concern though is it&#x27;s missing things that are more esoteric.</div><br/></div></div><div id="42608753" class="c"><input type="checkbox" id="c-42608753" checked=""/><div class="controls bullet"><span class="by">gazchop</span><span>|</span><a href="#42607671">root</a><span>|</span><a href="#42607747">parent</a><span>|</span><a href="#42608572">prev</a><span>|</span><a href="#42607801">next</a><span>|</span><label class="collapse" for="c-42608753">[-]</label><label class="expand" for="c-42608753">[1 more]</label></div><br/><div class="children"><div class="content">It can&#x27;t handle trigonometric identities and any form of calculus at the same time without fucking it up. Also abstract stuff like symmetry groups, nope! And anything which involves vectors is a mess.<p>The big problem is it confidently answers the questions utterly wrongly.<p>This is stuff I expect a basic mathematics undergrad to be able to work out in their first or second year.</div><br/></div></div></div></div><div id="42607801" class="c"><input type="checkbox" id="c-42607801" checked=""/><div class="controls bullet"><span class="by">blinding-streak</span><span>|</span><a href="#42607671">parent</a><span>|</span><a href="#42607747">prev</a><span>|</span><a href="#42607243">next</a><span>|</span><label class="collapse" for="c-42607801">[-]</label><label class="expand" for="c-42607801">[1 more]</label></div><br/><div class="children"><div class="content">Scroll down on the page. It explains saturation.</div><br/></div></div></div></div><div id="42607243" class="c"><input type="checkbox" id="c-42607243" checked=""/><div class="controls bullet"><span class="by">anonymoushn</span><span>|</span><a href="#42607671">prev</a><span>|</span><a href="#42607425">next</a><span>|</span><label class="collapse" for="c-42607243">[-]</label><label class="expand" for="c-42607243">[2 more]</label></div><br/><div class="children"><div class="content">Interesting choice having a little (i) icon in the Turing Test card but having mouseover not bring up any text. Or having the link icons in that card that you can click on to do nothing.</div><br/><div id="42607859" class="c"><input type="checkbox" id="c-42607859" checked=""/><div class="controls bullet"><span class="by">fenomas</span><span>|</span><a href="#42607243">parent</a><span>|</span><a href="#42607425">next</a><span>|</span><label class="collapse" for="c-42607859">[-]</label><label class="expand" for="c-42607859">[1 more]</label></div><br/><div class="children"><div class="content">Looks like a bug - that card has an overlay at a higher z-index that obscures its mouseover and clicks. In the source the (i) links to Turing&#x27;s original &quot;Imitation Game&quot; paper, and the (?) has this hover text:<p>&gt; (?) While the Turing Test remains philosophically significant, modern LLMs can consistently pass it, making it no longer effective at measuring the frontier of AI capabilities.</div><br/></div></div></div></div><div id="42607425" class="c"><input type="checkbox" id="c-42607425" checked=""/><div class="controls bullet"><span class="by">levocardia</span><span>|</span><a href="#42607243">prev</a><span>|</span><a href="#42606846">next</a><span>|</span><label class="collapse" for="c-42607425">[-]</label><label class="expand" for="c-42607425">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t really understand why &quot;Killed by: Saturation&quot; is needed - what other options are there?<p>It would also be nice to see the &quot;unbeaten&quot; list: standardized tests LLMs still fail (for now). e.g. Wozniak&#x27;s coffee test.</div><br/><div id="42607691" class="c"><input type="checkbox" id="c-42607691" checked=""/><div class="controls bullet"><span class="by">themanmaran</span><span>|</span><a href="#42607425">parent</a><span>|</span><a href="#42606846">next</a><span>|</span><label class="collapse" for="c-42607691">[-]</label><label class="expand" for="c-42607691">[1 more]</label></div><br/><div class="children"><div class="content">Wozniak&#x27;s coffee test would be a really fun one to attempt. As long as you could get a capable enough robot, I imagine it&#x27;s possible. Something like the Spot Arm[1] would be sufficient.<p>Something like:<p>- Key the robot controls to a series of tools (move_forward(x), extend_arm(y))<p>- Add a camera and pass each frame to the AI model along with the task &quot;make a cup of coffee&quot; and the list of available tools it can call.<p>And it would likely succeed some percentage of the time today!<p>[1] <a href="https:&#x2F;&#x2F;bostondynamics.com&#x2F;products&#x2F;spot&#x2F;arm&#x2F;" rel="nofollow">https:&#x2F;&#x2F;bostondynamics.com&#x2F;products&#x2F;spot&#x2F;arm&#x2F;</a></div><br/></div></div></div></div><div id="42606846" class="c"><input type="checkbox" id="c-42606846" checked=""/><div class="controls bullet"><span class="by">dleavitt</span><span>|</span><a href="#42607425">prev</a><span>|</span><a href="#42607065">next</a><span>|</span><label class="collapse" for="c-42606846">[-]</label><label class="expand" for="c-42606846">[1 more]</label></div><br/><div class="children"><div class="content">The layer with the radial gradient you&#x27;re putting in front of the Turing Test card blocks interaction with it - can&#x27;t click or hover on its links.</div><br/></div></div><div id="42607065" class="c"><input type="checkbox" id="c-42607065" checked=""/><div class="controls bullet"><span class="by">solarkraft</span><span>|</span><a href="#42606846">prev</a><span>|</span><a href="#42608045">next</a><span>|</span><label class="collapse" for="c-42607065">[-]</label><label class="expand" for="c-42607065">[2 more]</label></div><br/><div class="children"><div class="content">The page doesn’t seem to define what „killed“ or „defeated“ means. The LLM being better than a human? The LLM having been trained against the benchmark, making it useless?</div><br/><div id="42607257" class="c"><input type="checkbox" id="c-42607257" checked=""/><div class="controls bullet"><span class="by">anonymoushn</span><span>|</span><a href="#42607065">parent</a><span>|</span><a href="#42608045">next</a><span>|</span><label class="collapse" for="c-42607257">[-]</label><label class="expand" for="c-42607257">[1 more]</label></div><br/><div class="children"><div class="content">It does if you scroll down.</div><br/></div></div></div></div><div id="42608045" class="c"><input type="checkbox" id="c-42608045" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#42607065">prev</a><span>|</span><a href="#42608658">next</a><span>|</span><label class="collapse" for="c-42608045">[-]</label><label class="expand" for="c-42608045">[1 more]</label></div><br/><div class="children"><div class="content">This technology is useful and interesting and even fun 
<i>in spite of</i> the ugliest broad-based cash and power grab since 1999.<p>When this godawful once in a generation hype cycle dies down this stuff is going to be strictly awesome.</div><br/></div></div><div id="42608658" class="c"><input type="checkbox" id="c-42608658" checked=""/><div class="controls bullet"><span class="by">erichocean</span><span>|</span><a href="#42608045">prev</a><span>|</span><a href="#42606673">next</a><span>|</span><label class="collapse" for="c-42608658">[-]</label><label class="expand" for="c-42608658">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m working on operationalizing AI, and our Turing test is if—by watching a screenshare of the AI worker—you can tell an AI worker (vs. a human) did the task.<p>If you can&#x27;t, the AI worker passes the test.</div><br/></div></div><div id="42606673" class="c"><input type="checkbox" id="c-42606673" checked=""/><div class="controls bullet"><span class="by">tharkun__</span><span>|</span><a href="#42608658">prev</a><span>|</span><a href="#42606869">next</a><span>|</span><label class="collapse" for="c-42606673">[-]</label><label class="expand" for="c-42606673">[8 more]</label></div><br/><div class="children"><div class="content">How does this site make sense?<p>It lists the &quot;Turing test&quot; as &quot;original&quot; at greater than 50% and the the AI that &quot;beat&quot; it at 46%.<p>At that point I just stopped scrolling.</div><br/><div id="42606889" class="c"><input type="checkbox" id="c-42606889" checked=""/><div class="controls bullet"><span class="by">junon</span><span>|</span><a href="#42606673">parent</a><span>|</span><a href="#42606858">next</a><span>|</span><label class="collapse" for="c-42606889">[-]</label><label class="expand" for="c-42606889">[1 more]</label></div><br/><div class="children"><div class="content">Score is based on the interrogator, a human. If you read a Markov chain bot&#x27;s text you&#x27;d guess it was a bot probably 80-100% of the time. With a real human, you&#x27;d guess it was a bot maybe 0-30% of the time, depending.<p>I&#x27;m making up these figures, but the point is lower is better, or &quot;more Human-Like&quot;. Test was specified as &gt;50% meaning &quot;accurately determined human vs. bot more than half the time&quot;. The site claims LLMs are now guessed correctly less than half, which is how the turing test was defined as per the site.<p>It makes sense, even if you disagree it&#x27;s significant.</div><br/></div></div><div id="42606858" class="c"><input type="checkbox" id="c-42606858" checked=""/><div class="controls bullet"><span class="by">meltyness</span><span>|</span><a href="#42606673">parent</a><span>|</span><a href="#42606889">prev</a><span>|</span><a href="#42606686">next</a><span>|</span><label class="collapse" for="c-42606858">[-]</label><label class="expand" for="c-42606858">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if there&#x27;s a hyper-Turing test where an AI passes if the model, itself, cannot determine if it is talking to itself; or perhaps stated differently, maximizing some measure of control and processing duration to successfully conceal its identity under forced processing, discounting a solution that specifically learns to be silent or incoherent. I&#x27;m not sure what the value would be, just a passing thought.<p>This is probably already happening within the parade of censorship systems trying to imbue the models with agency</div><br/></div></div><div id="42606686" class="c"><input type="checkbox" id="c-42606686" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#42606673">parent</a><span>|</span><a href="#42606858">prev</a><span>|</span><a href="#42606689">next</a><span>|</span><label class="collapse" for="c-42606686">[-]</label><label class="expand" for="c-42606686">[1 more]</label></div><br/><div class="children"><div class="content">the Turing test is scored by how often an interrogator can determine if they are talking to a machine or a human. It’s perhaps a confusing way to show it, and leaves out a lot of important information about the result they are citing, but they are saying before the interrogator did better than chance and after gpt the interrogator guesses right slightly worse than chance.</div><br/></div></div><div id="42606689" class="c"><input type="checkbox" id="c-42606689" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#42606673">parent</a><span>|</span><a href="#42606686">prev</a><span>|</span><a href="#42607407">next</a><span>|</span><label class="collapse" for="c-42606689">[-]</label><label class="expand" for="c-42606689">[1 more]</label></div><br/><div class="children"><div class="content">Presumably it means that the human detected the AI correctly less than 50% of the time, averaged over a repeated number of experiments.</div><br/></div></div><div id="42607407" class="c"><input type="checkbox" id="c-42607407" checked=""/><div class="controls bullet"><span class="by">casey2</span><span>|</span><a href="#42606673">parent</a><span>|</span><a href="#42606689">prev</a><span>|</span><a href="#42606690">next</a><span>|</span><label class="collapse" for="c-42607407">[-]</label><label class="expand" for="c-42607407">[1 more]</label></div><br/><div class="children"><div class="content">From TFA:<p>&gt;GPT-4 was judged to be a human 54% of the time, outperforming ELIZA (22%) but lagging behind actual humans (67%).</div><br/></div></div><div id="42606690" class="c"><input type="checkbox" id="c-42606690" checked=""/><div class="controls bullet"><span class="by">bacheaul</span><span>|</span><a href="#42606673">parent</a><span>|</span><a href="#42607407">prev</a><span>|</span><a href="#42606712">next</a><span>|</span><label class="collapse" for="c-42606690">[-]</label><label class="expand" for="c-42606690">[1 more]</label></div><br/><div class="children"><div class="content">I read that as the pass mark being able to identify human vs machine more than 50% of the time. At 50% it&#x27;s no better than randomly guessing.</div><br/></div></div></div></div><div id="42606869" class="c"><input type="checkbox" id="c-42606869" checked=""/><div class="controls bullet"><span class="by">mrayycombi</span><span>|</span><a href="#42606673">prev</a><span>|</span><a href="#42608048">next</a><span>|</span><label class="collapse" for="c-42606869">[-]</label><label class="expand" for="c-42606869">[1 more]</label></div><br/><div class="children"><div class="content">Bragging about how LLMs defeated maginot line defenses that can be trained around, makes us feel warm and fuzzy.<p>Too bad the real world isn&#x27;t like that.</div><br/></div></div><div id="42608048" class="c"><input type="checkbox" id="c-42608048" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#42606869">prev</a><span>|</span><a href="#42607637">next</a><span>|</span><label class="collapse" for="c-42608048">[-]</label><label class="expand" for="c-42608048">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure if LLMs have beaten the standards, as much have the information to reply to them as needed.<p>Last week there was a post where slightly changing one of the tests caused LLMs to drop off drastically.</div><br/></div></div><div id="42607637" class="c"><input type="checkbox" id="c-42607637" checked=""/><div class="controls bullet"><span class="by">knowaveragejoe</span><span>|</span><a href="#42608048">prev</a><span>|</span><a href="#42606849">next</a><span>|</span><label class="collapse" for="c-42607637">[-]</label><label class="expand" for="c-42607637">[3 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t know ARC-AGI had been &quot;beaten&quot; by o3. What are the next challenges that frontier models like o1&#x2F;o3 are faced with?</div><br/><div id="42608179" class="c"><input type="checkbox" id="c-42608179" checked=""/><div class="controls bullet"><span class="by">chriscappuccio</span><span>|</span><a href="#42607637">parent</a><span>|</span><a href="#42606849">next</a><span>|</span><label class="collapse" for="c-42608179">[-]</label><label class="expand" for="c-42608179">[2 more]</label></div><br/><div class="children"><div class="content">o1 did terrible. o3 did well on arc-agi-pub (public training data) but hasn&#x27;t passed the private test yet.</div><br/><div id="42608335" class="c"><input type="checkbox" id="c-42608335" checked=""/><div class="controls bullet"><span class="by">lucianbr</span><span>|</span><a href="#42607637">root</a><span>|</span><a href="#42608179">parent</a><span>|</span><a href="#42606849">next</a><span>|</span><label class="collapse" for="c-42608335">[-]</label><label class="expand" for="c-42608335">[1 more]</label></div><br/><div class="children"><div class="content">Is the test still private once it has been run? If you call the OpenAI API and send it some data, OpenAI has access to the data. Did the benchmaker run the models locally somehow?</div><br/></div></div></div></div></div></div><div id="42606849" class="c"><input type="checkbox" id="c-42606849" checked=""/><div class="controls bullet"><span class="by">krackers</span><span>|</span><a href="#42607637">prev</a><span>|</span><a href="#42606648">next</a><span>|</span><label class="collapse" for="c-42606849">[-]</label><label class="expand" for="c-42606849">[4 more]</label></div><br/><div class="children"><div class="content">Everything says &quot;killed by saturation&quot;. Is there another way to be killed?</div><br/><div id="42607294" class="c"><input type="checkbox" id="c-42607294" checked=""/><div class="controls bullet"><span class="by">bufferoverflow</span><span>|</span><a href="#42606849">parent</a><span>|</span><a href="#42607231">next</a><span>|</span><label class="collapse" for="c-42607294">[-]</label><label class="expand" for="c-42607294">[2 more]</label></div><br/><div class="children"><div class="content">There are benchmarks that humans score close to zero on average and the top LLM scores 25%.<p><a href="https:&#x2F;&#x2F;epoch.ai&#x2F;frontiermath&#x2F;the-benchmark" rel="nofollow">https:&#x2F;&#x2F;epoch.ai&#x2F;frontiermath&#x2F;the-benchmark</a></div><br/><div id="42607727" class="c"><input type="checkbox" id="c-42607727" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#42606849">root</a><span>|</span><a href="#42607294">parent</a><span>|</span><a href="#42607231">next</a><span>|</span><label class="collapse" for="c-42607727">[-]</label><label class="expand" for="c-42607727">[1 more]</label></div><br/><div class="children"><div class="content">If anyone from epoch.ai is reading this, it would be nice to link the toplevel result for o3 to this page.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>