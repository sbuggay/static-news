<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1691571664234" as="style"/><link rel="stylesheet" href="styles.css?v=1691571664234"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://christianjmills.com/posts/arc-a770-testing/part-2/">Testing Intel’s Arc A770 GPU for Deep Learning</a> <span class="domain">(<a href="https://christianjmills.com">christianjmills.com</a>)</span></div><div class="subtext"><span>T-A</span> | <span>48 comments</span></div><br/><div><div id="37057840" class="c"><input type="checkbox" id="c-37057840" checked=""/><div class="controls bullet"><span class="by">ndneighbor</span><span>|</span><a href="#37058792">next</a><span>|</span><label class="collapse" for="c-37057840">[-]</label><label class="expand" for="c-37057840">[12 more]</label></div><br/><div class="children"><div class="content">I know it&#x27;s not ideal, but I am really rooting for Intel&#x27;s effort for graphics. From launch nearly a year ago to today, the driver team has been knocking out driver issues and slowly but surely trying to achieve parity with AMD and Nvidia. From what I hear, the team really going for broke on trying to get ML workloads fully supported. When Intel shipped a 16GB card at that price point, you knew that they were really trying, it would be cool if Battlemage (the next gen.) shipped with 32GB for larger models.</div><br/><div id="37060181" class="c"><input type="checkbox" id="c-37060181" checked=""/><div class="controls bullet"><span class="by">fransje26</span><span>|</span><a href="#37057840">parent</a><span>|</span><a href="#37058553">next</a><span>|</span><label class="collapse" for="c-37060181">[-]</label><label class="expand" for="c-37060181">[1 more]</label></div><br/><div class="children"><div class="content">&gt; trying to achieve parity with AMD and Nvidia<p>Interesting to put AMD and Nvidia at the same level. From experience developing GPGPU applications they are not even close..</div><br/></div></div><div id="37058553" class="c"><input type="checkbox" id="c-37058553" checked=""/><div class="controls bullet"><span class="by">meragrin_</span><span>|</span><a href="#37057840">parent</a><span>|</span><a href="#37060181">prev</a><span>|</span><a href="#37057851">next</a><span>|</span><label class="collapse" for="c-37058553">[-]</label><label class="expand" for="c-37058553">[4 more]</label></div><br/><div class="children"><div class="content">&gt; From what I hear, the team really going for broke on trying to get ML workloads fully supported.<p>I&#x27;ve been waiting on AMD to do that.  Embarrassingly, it looks like Intel will get there faster than AMD.</div><br/><div id="37058970" class="c"><input type="checkbox" id="c-37058970" checked=""/><div class="controls bullet"><span class="by">RF_Savage</span><span>|</span><a href="#37057840">root</a><span>|</span><a href="#37058553">parent</a><span>|</span><a href="#37057851">next</a><span>|</span><label class="collapse" for="c-37058970">[-]</label><label class="expand" for="c-37058970">[3 more]</label></div><br/><div class="children"><div class="content">Maybe Intel has less baggage and can target what they know people actually use?</div><br/><div id="37058998" class="c"><input type="checkbox" id="c-37058998" checked=""/><div class="controls bullet"><span class="by">vbezhenar</span><span>|</span><a href="#37057840">root</a><span>|</span><a href="#37058970">parent</a><span>|</span><a href="#37057851">next</a><span>|</span><label class="collapse" for="c-37058998">[-]</label><label class="expand" for="c-37058998">[2 more]</label></div><br/><div class="children"><div class="content">My opinion is that Intel has excellent software culture compared to other hardware companies. It might be one reason.</div><br/><div id="37059082" class="c"><input type="checkbox" id="c-37059082" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#37057840">root</a><span>|</span><a href="#37058998">parent</a><span>|</span><a href="#37057851">next</a><span>|</span><label class="collapse" for="c-37059082">[-]</label><label class="expand" for="c-37059082">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a loyal Intel customer because their stuff is, in general, straight up less janky and more practically reliable than AMD&#x27;s stuff. I agree that Intel knows how to do software in ways that AMD simply doesn&#x27;t.</div><br/></div></div></div></div></div></div></div></div><div id="37057851" class="c"><input type="checkbox" id="c-37057851" checked=""/><div class="controls bullet"><span class="by">Conscat</span><span>|</span><a href="#37057840">parent</a><span>|</span><a href="#37058553">prev</a><span>|</span><a href="#37057926">next</a><span>|</span><label class="collapse" for="c-37057851">[-]</label><label class="expand" for="c-37057851">[1 more]</label></div><br/><div class="children"><div class="content">Do you have any thoughts on how OneAPI compares to the Nsight tools?</div><br/></div></div><div id="37057926" class="c"><input type="checkbox" id="c-37057926" checked=""/><div class="controls bullet"><span class="by">sergiotapia</span><span>|</span><a href="#37057840">parent</a><span>|</span><a href="#37057851">prev</a><span>|</span><a href="#37058792">next</a><span>|</span><label class="collapse" for="c-37057926">[-]</label><label class="expand" for="c-37057926">[5 more]</label></div><br/><div class="children"><div class="content">If anything to get a new competitor to shake things up. It&#x27;s not good to have a literal family duopoly. Both CEOs are blood relatives.</div><br/><div id="37059357" class="c"><input type="checkbox" id="c-37059357" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#37057840">root</a><span>|</span><a href="#37057926">parent</a><span>|</span><a href="#37059123">next</a><span>|</span><label class="collapse" for="c-37059357">[-]</label><label class="expand" for="c-37059357">[1 more]</label></div><br/><div class="children"><div class="content">Lisa Su is CEO not owner of AMD.  It&#x27;s a public company.  if you&#x27;re implying she&#x27;s not competing with Nvidia cause it&#x27;s all in the family, well..... that&#x27;s kinda out there.</div><br/></div></div><div id="37059123" class="c"><input type="checkbox" id="c-37059123" checked=""/><div class="controls bullet"><span class="by">speakspokespok</span><span>|</span><a href="#37057840">root</a><span>|</span><a href="#37057926">parent</a><span>|</span><a href="#37059357">prev</a><span>|</span><a href="#37058792">next</a><span>|</span><label class="collapse" for="c-37059123">[-]</label><label class="expand" for="c-37059123">[3 more]</label></div><br/><div class="children"><div class="content">They&#x27;re distant cousins. Both from the same city, both moved to the US and crushed it, but both families were large and then had lots of kids.</div><br/><div id="37059527" class="c"><input type="checkbox" id="c-37059527" checked=""/><div class="controls bullet"><span class="by">newqer</span><span>|</span><a href="#37057840">root</a><span>|</span><a href="#37059123">parent</a><span>|</span><a href="#37058792">next</a><span>|</span><label class="collapse" for="c-37059527">[-]</label><label class="expand" for="c-37059527">[2 more]</label></div><br/><div class="children"><div class="content">This can be molded into a generational power grab thriller. Think Godfather trilogy, but both Su and Huang are sort of Sith lords.<p>The family started with nothing, multiplied and played the strategic long game to conquer the AI marked to enable Skynet.<p>But Intel are the Jedi hacker good guys.</div><br/><div id="37059850" class="c"><input type="checkbox" id="c-37059850" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#37057840">root</a><span>|</span><a href="#37059527">parent</a><span>|</span><a href="#37058792">next</a><span>|</span><label class="collapse" for="c-37059850">[-]</label><label class="expand" for="c-37059850">[1 more]</label></div><br/><div class="children"><div class="content">Yes. This is exactly right.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37058792" class="c"><input type="checkbox" id="c-37058792" checked=""/><div class="controls bullet"><span class="by">mastax</span><span>|</span><a href="#37057840">prev</a><span>|</span><a href="#37057404">next</a><span>|</span><label class="collapse" for="c-37058792">[-]</label><label class="expand" for="c-37058792">[4 more]</label></div><br/><div class="children"><div class="content">That software experience does not look as bad as I was expecting.<p>That performance is shockingly good, am I missing something? Comparable to a Titan RTX? Yes, that card is 4 years older but it cost an order of magnitude more, has almost twice the die area, and has twice the GDDR6 bus width. The A770 theoretical FP16 is apparently a bit higher (39.2T vs 32.6T) but in gaming workloads the Arc cards far under-perform their theoretical performance compared to competitors. I guess ML is less demanding on the drivers, or perhaps some micro-architectural thing. In gaming performance the Titan RTX is about 70% faster than the A770.<p>I&#x27;m really looking forward to the next generation now.<p><a href="https:&#x2F;&#x2F;www.techpowerup.com&#x2F;gpu-specs&#x2F;titan-rtx.c3311" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.techpowerup.com&#x2F;gpu-specs&#x2F;titan-rtx.c3311</a><p><a href="https:&#x2F;&#x2F;www.techpowerup.com&#x2F;gpu-specs&#x2F;arc-a770.c3914" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.techpowerup.com&#x2F;gpu-specs&#x2F;arc-a770.c3914</a></div><br/><div id="37059403" class="c"><input type="checkbox" id="c-37059403" checked=""/><div class="controls bullet"><span class="by">kookamamie</span><span>|</span><a href="#37058792">parent</a><span>|</span><a href="#37059110">next</a><span>|</span><label class="collapse" for="c-37059403">[-]</label><label class="expand" for="c-37059403">[1 more]</label></div><br/><div class="children"><div class="content">As you mentioned, Titan RTX is ancient when it comes to DL&#x2F;ML use - if that has a 4-fold perf compared to Arc, it&#x27;s not looking great. It&#x27;s not like you can cram 10x  Arc into a computer to make up for the gap.</div><br/></div></div><div id="37059110" class="c"><input type="checkbox" id="c-37059110" checked=""/><div class="controls bullet"><span class="by">7speter</span><span>|</span><a href="#37058792">parent</a><span>|</span><a href="#37059403">prev</a><span>|</span><a href="#37057404">next</a><span>|</span><label class="collapse" for="c-37059110">[-]</label><label class="expand" for="c-37059110">[2 more]</label></div><br/><div class="children"><div class="content">From what I can understand, gaming is a strange workload that Nvida and AMD have a 20+ year head start on Intel. Deep learning and AI workloads are probably pretty straight forward as far as how it&#x27;s implemented and run, but game developers do all sorts of hacky things to make their games work for the sake of meeting a given deadline (and ultimately, release date). The 20 year advantage of AMD and Nvidia comes from them optimizing their graphics drivers for all of the edgecase games as the games came out (this is why there may be a &quot;The Last of Us&quot; exclusive driver for Nvidia cards, for example). Intel has a massive backlog of games to work through to optimize for, and they also have to figure out what games are the ones to prioritize on their optimization todo list, I imagine.</div><br/><div id="37059628" class="c"><input type="checkbox" id="c-37059628" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#37058792">root</a><span>|</span><a href="#37059110">parent</a><span>|</span><a href="#37057404">next</a><span>|</span><label class="collapse" for="c-37059628">[-]</label><label class="expand" for="c-37059628">[1 more]</label></div><br/><div class="children"><div class="content">Many of the DirectX issues on these cards can be almost entirely mitigated with DXVK. It&#x27;s pretty well-documented at this point, but the Vulkan translation of older DX titles is better than using Intel&#x27;s native DirectX driver.<p>For Linux gamers, DXVK is how you&#x27;d be playing the game regardless. It&#x27;s a first-gen card that&#x27;s not a terrible prospect for the right sort of tinkerer.</div><br/></div></div></div></div></div></div><div id="37057404" class="c"><input type="checkbox" id="c-37057404" checked=""/><div class="controls bullet"><span class="by">mrbuttons454</span><span>|</span><a href="#37058792">prev</a><span>|</span><a href="#37059702">next</a><span>|</span><label class="collapse" for="c-37057404">[-]</label><label class="expand" for="c-37057404">[22 more]</label></div><br/><div class="children"><div class="content">Ship something reasonably priced with 32gb+ and I&#x27;d be interested!</div><br/><div id="37057547" class="c"><input type="checkbox" id="c-37057547" checked=""/><div class="controls bullet"><span class="by">lukevp</span><span>|</span><a href="#37057404">parent</a><span>|</span><a href="#37058439">next</a><span>|</span><label class="collapse" for="c-37057547">[-]</label><label class="expand" for="c-37057547">[17 more]</label></div><br/><div class="children"><div class="content">I was in the market for a new GPU and I debated between a 4090 and a 4070. 4070 is fine for the type of work and gaming I do, but I was thinking about the additional RAM. Then I realized that the 4070 has more vram than most consumers do, so many models will be optimized for that, and at some point someone will come out with a high-ram neural network only card at some point, and that would end up being better than the 4090 anyway because 24 gigs of vram is still quite small. I don’t know a ton about this stuff so who knows if that was the right decision… but I’m thinking along the same lines as you.</div><br/><div id="37057625" class="c"><input type="checkbox" id="c-37057625" checked=""/><div class="controls bullet"><span class="by">squeaky-clean</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37057547">parent</a><span>|</span><a href="#37057721">next</a><span>|</span><label class="collapse" for="c-37057625">[-]</label><label class="expand" for="c-37057625">[6 more]</label></div><br/><div class="children"><div class="content">&gt; someone will come out with a high-ram neural network only card at some point, and that would end up being better than the 4090 anyway because 24 gigs of vram is still quite small<p>These already exist, they&#x27;re just disproportionately expensive compared to consumer cards. You can get a Nvidia A100 with 80GB VRAM on eBay for about $14,000.<p>3.33x the memory for 9x the price.</div><br/><div id="37058419" class="c"><input type="checkbox" id="c-37058419" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37057625">parent</a><span>|</span><a href="#37059133">next</a><span>|</span><label class="collapse" for="c-37058419">[-]</label><label class="expand" for="c-37058419">[3 more]</label></div><br/><div class="children"><div class="content">Or you could get A6000 for $4k with 48 GB VRAM.</div><br/><div id="37059117" class="c"><input type="checkbox" id="c-37059117" checked=""/><div class="controls bullet"><span class="by">moonchrome</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37058419">parent</a><span>|</span><a href="#37059133">next</a><span>|</span><label class="collapse" for="c-37059117">[-]</label><label class="expand" for="c-37059117">[2 more]</label></div><br/><div class="children"><div class="content">Is 48GB enough to run 70B models ?</div><br/><div id="37059513" class="c"><input type="checkbox" id="c-37059513" checked=""/><div class="controls bullet"><span class="by">sacred_numbers</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37059117">parent</a><span>|</span><a href="#37059133">next</a><span>|</span><label class="collapse" for="c-37059513">[-]</label><label class="expand" for="c-37059513">[1 more]</label></div><br/><div class="children"><div class="content">Yes, with 4 bit quantization.</div><br/></div></div></div></div></div></div><div id="37059133" class="c"><input type="checkbox" id="c-37059133" checked=""/><div class="controls bullet"><span class="by">7speter</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37057625">parent</a><span>|</span><a href="#37058419">prev</a><span>|</span><a href="#37057721">next</a><span>|</span><label class="collapse" for="c-37059133">[-]</label><label class="expand" for="c-37059133">[2 more]</label></div><br/><div class="children"><div class="content">You can also buy used enterprise cards on ebay. There may not be many 32gb+ cards up yet, but they&#x27;ll be on there eventually. The big issue is how to cool these cards though, both amd and nvidia make the cards with fan-less heat sinks because the servers they get packed into provide cooling via their loud, ultra fast, potentially digit severing fans, but people who are salvaging these cards have to find their own solution to cool them, and fit them in a given case.</div><br/><div id="37060003" class="c"><input type="checkbox" id="c-37060003" checked=""/><div class="controls bullet"><span class="by">yrro</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37059133">parent</a><span>|</span><a href="#37057721">next</a><span>|</span><label class="collapse" for="c-37060003">[-]</label><label class="expand" for="c-37060003">[1 more]</label></div><br/><div class="children"><div class="content">How purchasers get drivers, don&#x27;t they require a subscription, or are they pirated?</div><br/></div></div></div></div></div></div><div id="37057721" class="c"><input type="checkbox" id="c-37057721" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37057547">parent</a><span>|</span><a href="#37057625">prev</a><span>|</span><a href="#37057613">next</a><span>|</span><label class="collapse" for="c-37057721">[-]</label><label class="expand" for="c-37057721">[6 more]</label></div><br/><div class="children"><div class="content">Also, 24GB is good because it&#x27;s <i>just barely</i> big enough for the (currently unreleased) Llama V2 34B.<p>In Llama V1, 7B was clever but kinda dumb, 13B was still short sighted, but 33B was a big jump to &quot;disturbingly good&quot; territory. 7B&#x2F;13B finetunes required very specific prompting, but 33B would still give good responses even outside the finetuning format.<p>16GB is probably perfect for a ~13B model with a very long context and better (5 bit?) quantization, or just partially offloading a ~33B model.</div><br/><div id="37058482" class="c"><input type="checkbox" id="c-37058482" checked=""/><div class="controls bullet"><span class="by">zacmps</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37057721">parent</a><span>|</span><a href="#37059325">next</a><span>|</span><label class="collapse" for="c-37058482">[-]</label><label class="expand" for="c-37058482">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know, maybe I&#x27;m desensitized by GPT4 but I&#x27;m running Llama2 70B and I certainly wouldn&#x27;t call it &#x27;disturbingly good&#x27;.</div><br/><div id="37058499" class="c"><input type="checkbox" id="c-37058499" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37058482">parent</a><span>|</span><a href="#37059325">next</a><span>|</span><label class="collapse" for="c-37058499">[-]</label><label class="expand" for="c-37058499">[1 more]</label></div><br/><div class="children"><div class="content">Base Llama is not good at all, but the finetunes are quite good at their niches.<p>Llama in general is not great for code completion and fact retrieval, from what I have tried</div><br/></div></div></div></div><div id="37059325" class="c"><input type="checkbox" id="c-37059325" checked=""/><div class="controls bullet"><span class="by">ripvanwinkle</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37057721">parent</a><span>|</span><a href="#37058482">prev</a><span>|</span><a href="#37057613">next</a><span>|</span><label class="collapse" for="c-37059325">[-]</label><label class="expand" for="c-37059325">[3 more]</label></div><br/><div class="children"><div class="content">Does it have to be exclusively vram to be useful or is shared ram also useful here. I have a system that reports 8gb dedicated to GPU with another 15 that is shared and thus a GPU memory of 23 GB</div><br/><div id="37059437" class="c"><input type="checkbox" id="c-37059437" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37059325">parent</a><span>|</span><a href="#37057613">next</a><span>|</span><label class="collapse" for="c-37059437">[-]</label><label class="expand" for="c-37059437">[2 more]</label></div><br/><div class="children"><div class="content">Driver shared memory is effectively useless. The way it accesses CPU memory is (for now) extremely slow.<p>First some background: llama is divided into prompt ingestion code, and &quot;layers&quot; for actually generating the tokens.<p>There are different offloading schemes, but what llama.cpp specifically does is map the layers to different devices. For example, ~7GB of the model could reside on the 8GB GPU, and the other ~9GB would live on the CPU.<p>During runtime, the prompt is ingested by the GPU all at once (which doesn&#x27;t take much VRAM), and then for each word, the layers are run sequentially. So the first ~half of a word would run on your GPU, and the last half would run on your CPU, and the alternation repeats till all the words are generated.<p>The beauty of llama.cpp is that its cpu token generation is (compared to other llama runtimes) extremely fast. So offloading even half or two thirds of a model to a decent CPU is not so bad.</div><br/><div id="37059605" class="c"><input type="checkbox" id="c-37059605" checked=""/><div class="controls bullet"><span class="by">ripvanwinkle</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37059437">parent</a><span>|</span><a href="#37057613">next</a><span>|</span><label class="collapse" for="c-37059605">[-]</label><label class="expand" for="c-37059605">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for the explanation</div><br/></div></div></div></div></div></div></div></div><div id="37057613" class="c"><input type="checkbox" id="c-37057613" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37057547">parent</a><span>|</span><a href="#37057721">prev</a><span>|</span><a href="#37058224">next</a><span>|</span><label class="collapse" for="c-37057613">[-]</label><label class="expand" for="c-37057613">[1 more]</label></div><br/><div class="children"><div class="content">My compromise:<p>A 3090.<p>Ampere will be well supported until its <i>very</i> obsolete because of the A100.</div><br/></div></div><div id="37058224" class="c"><input type="checkbox" id="c-37058224" checked=""/><div class="controls bullet"><span class="by">h11h</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37057547">parent</a><span>|</span><a href="#37057613">prev</a><span>|</span><a href="#37057871">next</a><span>|</span><label class="collapse" for="c-37058224">[-]</label><label class="expand" for="c-37058224">[1 more]</label></div><br/><div class="children"><div class="content">If anything, 24GB is probably the sweet spot for what&#x27;s optimised for as many local LLM enthusiasts are running 3090s and 4090s (and all want more VRAM).</div><br/></div></div><div id="37057871" class="c"><input type="checkbox" id="c-37057871" checked=""/><div class="controls bullet"><span class="by">RcouF1uZ4gsC</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37057547">parent</a><span>|</span><a href="#37058224">prev</a><span>|</span><a href="#37058439">next</a><span>|</span><label class="collapse" for="c-37057871">[-]</label><label class="expand" for="c-37057871">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Then I realized that the 4070 has more vram than most consumers do<p>The M1 and M2 Apple processors might change that equation.<p>The 32gb and 64gb MacBooks can run inference on a lot of these big models.<p>They might be quite a bit slower in TFLOPS than a 4070, but if they can run the model while the 4070 can&#x27;t run it due to limited RAM, then they come out the winner</div><br/><div id="37058001" class="c"><input type="checkbox" id="c-37058001" checked=""/><div class="controls bullet"><span class="by">pram</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37057871">parent</a><span>|</span><a href="#37058439">next</a><span>|</span><label class="collapse" for="c-37058001">[-]</label><label class="expand" for="c-37058001">[1 more]</label></div><br/><div class="children"><div class="content">Yes I could run llama 65b on my M1 Max with 64GB of ram.</div><br/></div></div></div></div></div></div><div id="37058439" class="c"><input type="checkbox" id="c-37058439" checked=""/><div class="controls bullet"><span class="by">lumost</span><span>|</span><a href="#37057404">parent</a><span>|</span><a href="#37057547">prev</a><span>|</span><a href="#37057667">next</a><span>|</span><label class="collapse" for="c-37058439">[-]</label><label class="expand" for="c-37058439">[3 more]</label></div><br/><div class="children"><div class="content">Very hopeful for unified&#x2F;configurable memory.<p>Why are gpu vendors deciding how much ram the gpu gets? This feels so 1980s. GenAI workloads have radically variable memory footprints, some folks want 256GB per card so they have a spitting shot at trying something out - and other folks want 8GB per card to maximize throughput.<p>To add complication, I really don’t want to deal with copying 256GB models between cpu&#x2F;card. We should have unified memory.</div><br/><div id="37058485" class="c"><input type="checkbox" id="c-37058485" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37058439">parent</a><span>|</span><a href="#37059668">next</a><span>|</span><label class="collapse" for="c-37058485">[-]</label><label class="expand" for="c-37058485">[1 more]</label></div><br/><div class="children"><div class="content">CXL is that, but a compromise. Practically, that means an APU.<p>Intel&#x2F;AMD are reportedly coming out with wide M1-Pro like chips.</div><br/></div></div><div id="37059668" class="c"><input type="checkbox" id="c-37059668" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#37057404">root</a><span>|</span><a href="#37058439">parent</a><span>|</span><a href="#37058485">prev</a><span>|</span><a href="#37057667">next</a><span>|</span><label class="collapse" for="c-37059668">[-]</label><label class="expand" for="c-37059668">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why are gpu vendors deciding how much ram the gpu gets?<p>...because they make them?<p>You can go buy 256gb systems if you want, at extreme cost. You can also use an 8gb card for whatever you want too. What are they doing wrong?<p>&gt; To add complication, I really don’t want to deal with copying 256GB models between cpu&#x2F;card.<p>For deployment, it doesn&#x27;t really matter. You just memory-map it to the CPU or keep it hot-loaded in GPU memory. With a unified memory model you&#x27;re still limited by your disk speed, so the performance difference probably wouldn&#x27;t come out great either way.</div><br/></div></div></div></div><div id="37057667" class="c"><input type="checkbox" id="c-37057667" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37057404">parent</a><span>|</span><a href="#37058439">prev</a><span>|</span><a href="#37059702">next</a><span>|</span><label class="collapse" for="c-37057667">[-]</label><label class="expand" for="c-37057667">[1 more]</label></div><br/><div class="children"><div class="content">I just ordered a 3090, but I would have bought a 32GB A770 instead without even blinking.<p>The card is 256 bit... Intel could have done this, just like the 16GB RTX 4060 with a 128 bit bus.</div><br/></div></div></div></div><div id="37059702" class="c"><input type="checkbox" id="c-37059702" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#37057404">prev</a><span>|</span><a href="#37058551">next</a><span>|</span><label class="collapse" for="c-37059702">[-]</label><label class="expand" for="c-37059702">[1 more]</label></div><br/><div class="children"><div class="content">For the price being within spitting range of a Titan seems pretty damn good</div><br/></div></div><div id="37058551" class="c"><input type="checkbox" id="c-37058551" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#37059702">prev</a><span>|</span><a href="#37057475">next</a><span>|</span><label class="collapse" for="c-37058551">[-]</label><label class="expand" for="c-37058551">[1 more]</label></div><br/><div class="children"><div class="content">Intel ARC is not a particularly big worry on training with mixed fp32&#x2F;16.<p>On LLM inference we’ve got a baseball game.</div><br/></div></div><div id="37057475" class="c"><input type="checkbox" id="c-37057475" checked=""/><div class="controls bullet"><span class="by">mcbuilder</span><span>|</span><a href="#37058551">prev</a><span>|</span><label class="collapse" for="c-37057475">[-]</label><label class="expand" for="c-37057475">[7 more]</label></div><br/><div class="children"><div class="content">At least Intel has some non x86 hardware for deep learning now. Between killing Nervana on the verge of release and Habana not making a splash, they are looking for a win.</div><br/><div id="37059189" class="c"><input type="checkbox" id="c-37059189" checked=""/><div class="controls bullet"><span class="by">ac29</span><span>|</span><a href="#37057475">parent</a><span>|</span><a href="#37057687">next</a><span>|</span><label class="collapse" for="c-37059189">[-]</label><label class="expand" for="c-37059189">[1 more]</label></div><br/><div class="children"><div class="content">Latest earnings call Intel said they have over $1B in the &quot;pipeline of opportunities&quot; for AI, with Gaudi being the &quot;lion&#x27;s share&quot; of that. Certainly not bad.</div><br/></div></div><div id="37057687" class="c"><input type="checkbox" id="c-37057687" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37057475">parent</a><span>|</span><a href="#37059189">prev</a><span>|</span><a href="#37057495">next</a><span>|</span><label class="collapse" for="c-37057687">[-]</label><label class="expand" for="c-37057687">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Habana not making a splash<p>Yeah this is sad.<p>Intel is supposedly turning it into a tensor core-ish accelerator on Falcon Shores... And, hopefully, lower end Arc cards.</div><br/></div></div><div id="37057495" class="c"><input type="checkbox" id="c-37057495" checked=""/><div class="controls bullet"><span class="by">meepmorp</span><span>|</span><a href="#37057475">parent</a><span>|</span><a href="#37057687">prev</a><span>|</span><label class="collapse" for="c-37057495">[-]</label><label class="expand" for="c-37057495">[4 more]</label></div><br/><div class="children"><div class="content">They also bought (and killed) Movidius, iirc</div><br/><div id="37059976" class="c"><input type="checkbox" id="c-37059976" checked=""/><div class="controls bullet"><span class="by">my123</span><span>|</span><a href="#37057475">root</a><span>|</span><a href="#37057495">parent</a><span>|</span><a href="#37059006">next</a><span>|</span><label class="collapse" for="c-37059976">[-]</label><label class="expand" for="c-37059976">[1 more]</label></div><br/><div class="children"><div class="content">Movidius&#x27;s death is greatly exaggerated.<p>Movidius AI accelerators are going to be there as part of every single Meteor Lake chip.</div><br/></div></div><div id="37059006" class="c"><input type="checkbox" id="c-37059006" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#37057475">root</a><span>|</span><a href="#37057495">parent</a><span>|</span><a href="#37059976">prev</a><span>|</span><label class="collapse" for="c-37059006">[-]</label><label class="expand" for="c-37059006">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know what&#x27;s in development but they do have a new chip that stated shipping this year and until recently they were the only ML-oriented product Intel had that was competitive in it&#x27;s segment.</div><br/><div id="37059435" class="c"><input type="checkbox" id="c-37059435" checked=""/><div class="controls bullet"><span class="by">touisteur</span><span>|</span><a href="#37057475">root</a><span>|</span><a href="#37059006">parent</a><span>|</span><label class="collapse" for="c-37059435">[-]</label><label class="expand" for="c-37059435">[1 more]</label></div><br/><div class="children"><div class="content">Been waiting on Keem Bay for so long it&#x27;s now a running joke.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>