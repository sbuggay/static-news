<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1733562053692" as="style"/><link rel="stylesheet" href="styles.css?v=1733562053692"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ollama.com/blog/structured-outputs">Structured Outputs with Ollama</a> <span class="domain">(<a href="https://ollama.com">ollama.com</a>)</span></div><div class="subtext"><span>Patrick_Devine</span> | <span>37 comments</span></div><br/><div><div id="42347131" class="c"><input type="checkbox" id="c-42347131" checked=""/><div class="controls bullet"><span class="by">rdescartes</span><span>|</span><a href="#42346837">next</a><span>|</span><label class="collapse" for="c-42347131">[-]</label><label class="expand" for="c-42347131">[2 more]</label></div><br/><div class="children"><div class="content">If anyone needs a more powerful constrain outputs, llama.cpp support gbnf:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;blob&#x2F;master&#x2F;grammars&#x2F;README.md">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;blob&#x2F;master&#x2F;grammars&#x2F;...</a></div><br/><div id="42348234" class="c"><input type="checkbox" id="c-42348234" checked=""/><div class="controls bullet"><span class="by">jimmySixDOF</span><span>|</span><a href="#42347131">parent</a><span>|</span><a href="#42346837">next</a><span>|</span><label class="collapse" for="c-42348234">[-]</label><label class="expand" for="c-42348234">[1 more]</label></div><br/><div class="children"><div class="content">Thats is exactly what they are using</div><br/></div></div></div></div><div id="42346837" class="c"><input type="checkbox" id="c-42346837" checked=""/><div class="controls bullet"><span class="by">chirau</span><span>|</span><a href="#42347131">prev</a><span>|</span><a href="#42347096">next</a><span>|</span><label class="collapse" for="c-42346837">[-]</label><label class="expand" for="c-42346837">[6 more]</label></div><br/><div class="children"><div class="content">This is wonderful news.<p>I was actually scratching my head on how to structure a regular prompt to produce csv data without extra nonsense like &quot;Here is your data&quot; and &quot;Please note blah blah&quot; at the beginning and end, so this is much welcome as I can define exactly what I want returned then just push structured output to csv.</div><br/><div id="42346983" class="c"><input type="checkbox" id="c-42346983" checked=""/><div class="controls bullet"><span class="by">firejake308</span><span>|</span><a href="#42346837">parent</a><span>|</span><a href="#42347297">next</a><span>|</span><label class="collapse" for="c-42346983">[-]</label><label class="expand" for="c-42346983">[2 more]</label></div><br/><div class="children"><div class="content">Remember that you still need to include an instruction to produce a CSV to get the prompt into the right context to generate a CSV that makes sense. Otherwise, you may get output that is technically in the CSV format but doesn&#x27;t make any sense because the model was actually trying to write a paragraph response and the token sampler just selected really low-probability tokens that the model didn&#x27;t really want to say.</div><br/><div id="42347118" class="c"><input type="checkbox" id="c-42347118" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#42346837">root</a><span>|</span><a href="#42346983">parent</a><span>|</span><a href="#42347297">next</a><span>|</span><label class="collapse" for="c-42347118">[-]</label><label class="expand" for="c-42347118">[1 more]</label></div><br/><div class="children"><div class="content">It seems ollama only supports JSON Schema.<p>Interestingly, JSON Schema has much less of this problem than say CSV - when the model is forced to produce `{&quot;first_key&quot;:` it will generally understand it&#x27;s supposed to continue in JSON. It still helps to tell it the schema though, especially due to weird tokenization issues you can get otherwise.</div><br/></div></div></div></div><div id="42347297" class="c"><input type="checkbox" id="c-42347297" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#42346837">parent</a><span>|</span><a href="#42346983">prev</a><span>|</span><a href="#42347096">next</a><span>|</span><label class="collapse" for="c-42347297">[-]</label><label class="expand" for="c-42347297">[3 more]</label></div><br/><div class="children"><div class="content">A lot of the time you can prevent this by prefilling the output with ```\n and stopping at ```.</div><br/><div id="42347411" class="c"><input type="checkbox" id="c-42347411" checked=""/><div class="controls bullet"><span class="by">chirau</span><span>|</span><a href="#42346837">root</a><span>|</span><a href="#42347297">parent</a><span>|</span><a href="#42347096">next</a><span>|</span><label class="collapse" for="c-42347411">[-]</label><label class="expand" for="c-42347411">[2 more]</label></div><br/><div class="children"><div class="content">care to explain further? I am not sure I understand you fully</div><br/><div id="42347916" class="c"><input type="checkbox" id="c-42347916" checked=""/><div class="controls bullet"><span class="by">bonzini</span><span>|</span><a href="#42346837">root</a><span>|</span><a href="#42347411">parent</a><span>|</span><a href="#42347096">next</a><span>|</span><label class="collapse" for="c-42347916">[-]</label><label class="expand" for="c-42347916">[1 more]</label></div><br/><div class="children"><div class="content">``` is markdown for pre formatted text. It puts the LLM in the mood of generating machine readable data instead of prose.</div><br/></div></div></div></div></div></div></div></div><div id="42347096" class="c"><input type="checkbox" id="c-42347096" checked=""/><div class="controls bullet"><span class="by">quaintdev</span><span>|</span><a href="#42346837">prev</a><span>|</span><a href="#42346507">next</a><span>|</span><label class="collapse" for="c-42347096">[-]</label><label class="expand" for="c-42347096">[1 more]</label></div><br/><div class="children"><div class="content">Yay! It works. I used gemma2:2b and gave it below text<p><pre><code>   You have spent 190 at Fresh Mart. Current balance: 5098
</code></pre>
and it gave below output<p><pre><code>   {\n\&quot;amount\&quot;: 190,\n\&quot;balance\&quot;: 5098 ,\&quot;category\&quot;: \&quot;Shopping\&quot;,\n\&quot;place\&quot;:\&quot;Fresh Mart\&quot;\n}</code></pre></div><br/></div></div><div id="42346507" class="c"><input type="checkbox" id="c-42346507" checked=""/><div class="controls bullet"><span class="by">bluechair</span><span>|</span><a href="#42347096">prev</a><span>|</span><a href="#42346509">next</a><span>|</span><label class="collapse" for="c-42346507">[-]</label><label class="expand" for="c-42346507">[13 more]</label></div><br/><div class="children"><div class="content">Has anyone seen how these constraints affect the quality of the output out of the LLM?<p>In some instances, I&#x27;d rather parse Markdown or plain text if it means the quality of the output is higher.</div><br/><div id="42346734" class="c"><input type="checkbox" id="c-42346734" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#42346507">parent</a><span>|</span><a href="#42346535">next</a><span>|</span><label class="collapse" for="c-42346734">[-]</label><label class="expand" for="c-42346734">[1 more]</label></div><br/><div class="children"><div class="content">YMMV, it&#x27;s a negative effect in terms of &quot;reasoning&quot; but the delta isn&#x27;t super significant in most cases. It really depends on the LLM and whether your prompt is likely to generate a JSON response to begin with, the more you have to coerce the LLM the less likely it is to generate sane input. With smaller models you more quickly end up at the edge of space where the LLM has meaningful predictive power and so the outputs start getting closer to random noise.<p>FWIW measured by me using a vibes based method, nothing rigorous just a lot of hours spent on various LLM projects. I have not used these particular tools yet but ollama was previously able to guarantee json output through what I assume is similar techniques and my partner and I worked previously on a jsonformer-like thing for oobabooga, another LLM runtime tool.</div><br/></div></div><div id="42346535" class="c"><input type="checkbox" id="c-42346535" checked=""/><div class="controls bullet"><span class="by">parthsareen</span><span>|</span><a href="#42346507">parent</a><span>|</span><a href="#42346734">prev</a><span>|</span><a href="#42346823">next</a><span>|</span><label class="collapse" for="c-42346535">[-]</label><label class="expand" for="c-42346535">[6 more]</label></div><br/><div class="children"><div class="content">We’ve been keeping a close eye on this as well as research is coming out. We’re looking into improving sampling as a whole on both speed and accuracy.<p>Hopefully with those changes we might also enable general structure generation not only limited to JSON.</div><br/><div id="42346613" class="c"><input type="checkbox" id="c-42346613" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#42346507">root</a><span>|</span><a href="#42346535">parent</a><span>|</span><a href="#42346823">next</a><span>|</span><label class="collapse" for="c-42346613">[-]</label><label class="expand" for="c-42346613">[5 more]</label></div><br/><div class="children"><div class="content">Who is &quot;we&quot;?</div><br/><div id="42346713" class="c"><input type="checkbox" id="c-42346713" checked=""/><div class="controls bullet"><span class="by">parthsareen</span><span>|</span><a href="#42346507">root</a><span>|</span><a href="#42346613">parent</a><span>|</span><a href="#42346823">next</a><span>|</span><label class="collapse" for="c-42346713">[-]</label><label class="expand" for="c-42346713">[4 more]</label></div><br/><div class="children"><div class="content">I authored the blog with some other contributors and worked on the feature (PR: <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;pull&#x2F;7900">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;pull&#x2F;7900</a>).<p>The current implementation uses llama.cpp GBNF grammars. The more recent research (Outlines, XGrammar) points to potentially speeding up the sampling process through FSTs and GPU parallelism.</div><br/><div id="42347142" class="c"><input type="checkbox" id="c-42347142" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#42346507">root</a><span>|</span><a href="#42346713">parent</a><span>|</span><a href="#42346990">next</a><span>|</span><label class="collapse" for="c-42347142">[-]</label><label class="expand" for="c-42347142">[2 more]</label></div><br/><div class="children"><div class="content">If you want avoid startup cost, llguidance [0] has no compilation phase and by far the fullest JSON support [1] of any library. I did a PoC llama.cpp integration [2] though our focus is mostly server-side [3].<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;guidance-ai&#x2F;llguidance">https:&#x2F;&#x2F;github.com&#x2F;guidance-ai&#x2F;llguidance</a>
[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;guidance-ai&#x2F;llguidance&#x2F;blob&#x2F;main&#x2F;parser&#x2F;src&#x2F;json&#x2F;README.md">https:&#x2F;&#x2F;github.com&#x2F;guidance-ai&#x2F;llguidance&#x2F;blob&#x2F;main&#x2F;parser&#x2F;s...</a>
[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;10224">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;10224</a>
[3] <a href="https:&#x2F;&#x2F;github.com&#x2F;guidance-ai&#x2F;llgtrt">https:&#x2F;&#x2F;github.com&#x2F;guidance-ai&#x2F;llgtrt</a></div><br/><div id="42347160" class="c"><input type="checkbox" id="c-42347160" checked=""/><div class="controls bullet"><span class="by">parthsareen</span><span>|</span><a href="#42346507">root</a><span>|</span><a href="#42347142">parent</a><span>|</span><a href="#42346990">next</a><span>|</span><label class="collapse" for="c-42347160">[-]</label><label class="expand" for="c-42347160">[1 more]</label></div><br/><div class="children"><div class="content">This looks really useful. Thank you!</div><br/></div></div></div></div><div id="42346990" class="c"><input type="checkbox" id="c-42346990" checked=""/><div class="controls bullet"><span class="by">netghost</span><span>|</span><a href="#42346507">root</a><span>|</span><a href="#42346713">parent</a><span>|</span><a href="#42347142">prev</a><span>|</span><a href="#42346823">next</a><span>|</span><label class="collapse" for="c-42346990">[-]</label><label class="expand" for="c-42346990">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for the details!</div><br/></div></div></div></div></div></div></div></div><div id="42346823" class="c"><input type="checkbox" id="c-42346823" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#42346507">parent</a><span>|</span><a href="#42346535">prev</a><span>|</span><a href="#42347215">next</a><span>|</span><label class="collapse" for="c-42346823">[-]</label><label class="expand" for="c-42346823">[1 more]</label></div><br/><div class="children"><div class="content">Working with OpenAI&#x27;s models I&#x27;ve found a very good strategy is to have two passes if you can afford the extra tokens: one pass uses a heavy model and natural language with markdown sections discussing the reasoning and providing a final natural language answer (ideally labeled clearly with a markdown header). The second pass can use a cheaper and faster model to put the answer into a structured output format for consumption by the non-LLM parts of the pipeline.<p>You basically use JSON schema mode to draw a clean boundary around the wishy-washy language bits, using the LLM as a preprocessor to capture its own output in a useful format.</div><br/></div></div><div id="42347215" class="c"><input type="checkbox" id="c-42347215" checked=""/><div class="controls bullet"><span class="by">crystal_revenge</span><span>|</span><a href="#42346507">parent</a><span>|</span><a href="#42346823">prev</a><span>|</span><a href="#42346664">next</a><span>|</span><label class="collapse" for="c-42347215">[-]</label><label class="expand" for="c-42347215">[1 more]</label></div><br/><div class="children"><div class="content">There was a paper going around claiming that structured outputs <i>did</i> hurt the quality of the output, but it turns out their experiment setup was laughably bad [0].<p>It looks like, so long as you&#x27;re reasonable with the prompting, you tend to get <i>better</i> outputs when using structure.<p>0. <a href="https:&#x2F;&#x2F;blog.dottxt.co&#x2F;say-what-you-mean.html" rel="nofollow">https:&#x2F;&#x2F;blog.dottxt.co&#x2F;say-what-you-mean.html</a></div><br/></div></div><div id="42346664" class="c"><input type="checkbox" id="c-42346664" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#42346507">parent</a><span>|</span><a href="#42347215">prev</a><span>|</span><a href="#42347170">next</a><span>|</span><label class="collapse" for="c-42346664">[-]</label><label class="expand" for="c-42346664">[1 more]</label></div><br/><div class="children"><div class="content">I can say that I was categorically wrong about the utility of things like instructor.<p>It’s easy to burn a lot of tokens but if the thing you’re doing merits the cost? You can be a bully with it and while its never the best, 95% as good for zero effort is a tool in one’s kit.</div><br/></div></div><div id="42347170" class="c"><input type="checkbox" id="c-42347170" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#42346507">parent</a><span>|</span><a href="#42346664">prev</a><span>|</span><a href="#42346650">next</a><span>|</span><label class="collapse" for="c-42347170">[-]</label><label class="expand" for="c-42347170">[1 more]</label></div><br/><div class="children"><div class="content">It depends how fine-tuned the model is to JSON output.<p>Also, you need to tell the model the schema. If you don&#x27;t you will get more weird tokenization issues.<p>For example, if the schema expects a JSON key &quot;foobarbaz&quot; and the canonical BPE tokenization is [&quot;foobar&quot;, &quot;baz&quot;], the token mask generated by all current constrained output libraries will let the model choose from &quot;f&quot;, &quot;foo&quot;, &quot;foobar&quot; (assuming these are all valid tokens). The model might then choose &quot;foo&quot;, and then the constraint will force eg. &quot;bar&quot; and &quot;baz&quot; as next tokens. Now the model will see [&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;] instead of [&quot;foobar&quot;, &quot;baz&quot;] and will get confused [0]<p>If the model knows from the prompt &quot;foobarbaz&quot; is one of the schema keys, it will generally prefer &quot;foobar&quot; over &quot;foo&quot;.<p>[0] In modern models these tokens are related, because of regularization but they are not the same.</div><br/></div></div><div id="42346650" class="c"><input type="checkbox" id="c-42346650" checked=""/><div class="controls bullet"><span class="by">nikolayasdf123</span><span>|</span><a href="#42346507">parent</a><span>|</span><a href="#42347170">prev</a><span>|</span><a href="#42346509">next</a><span>|</span><label class="collapse" for="c-42346650">[-]</label><label class="expand" for="c-42346650">[1 more]</label></div><br/><div class="children"><div class="content">same here. I noticed that when you ask model to generate elaborate responses in natural text, and then come up with an answer, quality is orders of magnitude better, and something in line you would expect human-like reasoning.<p>asking LLM to directly generate JSON gives much worser results, similar to either random guess or intuition.</div><br/></div></div></div></div><div id="42346509" class="c"><input type="checkbox" id="c-42346509" checked=""/><div class="controls bullet"><span class="by">quaintdev</span><span>|</span><a href="#42346507">prev</a><span>|</span><a href="#42347927">next</a><span>|</span><label class="collapse" for="c-42346509">[-]</label><label class="expand" for="c-42346509">[3 more]</label></div><br/><div class="children"><div class="content">So I can use this with any supported models? The reason I&#x27;m asking is because I can only run 1b-3b models reliably on my hardware.</div><br/><div id="42346526" class="c"><input type="checkbox" id="c-42346526" checked=""/><div class="controls bullet"><span class="by">parthsareen</span><span>|</span><a href="#42346509">parent</a><span>|</span><a href="#42347927">next</a><span>|</span><label class="collapse" for="c-42346526">[-]</label><label class="expand" for="c-42346526">[2 more]</label></div><br/><div class="children"><div class="content">Hey! Author of the blog post here. Yes you should be able to use any model. Your mileage may vary with the smaller models but asking them to “return x in json” tends to help with accuracy (anecdotally).</div><br/><div id="42347427" class="c"><input type="checkbox" id="c-42347427" checked=""/><div class="controls bullet"><span class="by">pamelafox</span><span>|</span><a href="#42346509">root</a><span>|</span><a href="#42346526">parent</a><span>|</span><a href="#42347927">next</a><span>|</span><label class="collapse" for="c-42347427">[-]</label><label class="expand" for="c-42347427">[1 more]</label></div><br/><div class="children"><div class="content">Do you happen to know if got-4o would be negatively affected by the addition of “return x in json”? I’m debating whether I could use the same prompt across all models, hosted and ollama.</div><br/></div></div></div></div></div></div><div id="42347927" class="c"><input type="checkbox" id="c-42347927" checked=""/><div class="controls bullet"><span class="by">lormayna</span><span>|</span><a href="#42346509">prev</a><span>|</span><a href="#42347605">next</a><span>|</span><label class="collapse" for="c-42347927">[-]</label><label class="expand" for="c-42347927">[1 more]</label></div><br/><div class="children"><div class="content">This is a fantastic news! 
I spent hours on fine tuning my prompt to summarise text and output in JSON and still have some issues sometimes.
Is this feature available also with Go?</div><br/></div></div><div id="42347605" class="c"><input type="checkbox" id="c-42347605" checked=""/><div class="controls bullet"><span class="by">JackYoustra</span><span>|</span><a href="#42347927">prev</a><span>|</span><a href="#42346699">next</a><span>|</span><label class="collapse" for="c-42347605">[-]</label><label class="expand" for="c-42347605">[1 more]</label></div><br/><div class="children"><div class="content">PRs on this have been open for something like a year! I&#x27;m a bit sad about how quiet the maintainers have been on this.</div><br/></div></div><div id="42346699" class="c"><input type="checkbox" id="c-42346699" checked=""/><div class="controls bullet"><span class="by">lxe</span><span>|</span><a href="#42347605">prev</a><span>|</span><a href="#42346878">next</a><span>|</span><label class="collapse" for="c-42346699">[-]</label><label class="expand" for="c-42346699">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m still running oobabooga because of its exlv2 support which does much more efficient inference on dual 3090s</div><br/><div id="42347090" class="c"><input type="checkbox" id="c-42347090" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#42346699">parent</a><span>|</span><a href="#42346878">next</a><span>|</span><label class="collapse" for="c-42347090">[-]</label><label class="expand" for="c-42347090">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t touched ooba in a while, what&#x27;s the situation like with exl2 vs the non-homogeneous quantization methods people are using like q3k_s or whatever. IIRC while exl2 is faster the gptq quants were outperforming it in terms of accuracy esp at lower bit depths.</div><br/></div></div></div></div><div id="42346878" class="c"><input type="checkbox" id="c-42346878" checked=""/><div class="controls bullet"><span class="by">xnx</span><span>|</span><a href="#42346699">prev</a><span>|</span><a href="#42346676">next</a><span>|</span><label class="collapse" for="c-42346878">[-]</label><label class="expand" for="c-42346878">[2 more]</label></div><br/><div class="children"><div class="content">Is there a best approach for providing structured input to LLMs? Example: feed in 100 sentences and get each one classified in different ways. It&#x27;s easy to get structured data out, but my approach of prefixing line numbers seems clumsy.</div><br/><div id="42347218" class="c"><input type="checkbox" id="c-42347218" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#42346878">parent</a><span>|</span><a href="#42346676">next</a><span>|</span><label class="collapse" for="c-42347218">[-]</label><label class="expand" for="c-42347218">[1 more]</label></div><br/><div class="children"><div class="content">Models are trained on Markdown, JSON and various programming languages, so either one of these should work.<p>However, in this case, you&#x27;re best of giving the model sentences one by one to avoid it being confused. If you structure the prompt like &quot;Classify the following sentence, here are the rules ....&quot; + sentence, then you should be hitting prefix cache and get even better performance than when doing a single query. Of course, this only works if you have the prefix cache and are not paying per input token (though most providers now let you indicate you want to use prefix cache and pay less).</div><br/></div></div></div></div><div id="42346676" class="c"><input type="checkbox" id="c-42346676" checked=""/><div class="controls bullet"><span class="by">vincentpants</span><span>|</span><a href="#42346878">prev</a><span>|</span><a href="#42347000">next</a><span>|</span><label class="collapse" for="c-42346676">[-]</label><label class="expand" for="c-42346676">[1 more]</label></div><br/><div class="children"><div class="content">Wow neat! The first step to format ambivalence! Curious to see how well does this perform on the edge, our overhead is always so scarce!<p>Amazing work as always, looking forward to taking this for a spin!</div><br/></div></div><div id="42347874" class="c"><input type="checkbox" id="c-42347874" checked=""/><div class="controls bullet"><span class="by">guerrilla</span><span>|</span><a href="#42347000">prev</a><span>|</span><a href="#42347766">next</a><span>|</span><label class="collapse" for="c-42347874">[-]</label><label class="expand" for="c-42347874">[1 more]</label></div><br/><div class="children"><div class="content">No way. This is amazing and one of the things I actually wanted. I love ollama be because it makes using an LLM feel like using any other UNIX program. It makes LLMs feel like they belong on UNIX.<p>Question though. Has anyone had luck running it on AMD GPUs? I&#x27;ve heard it&#x27;s harder but I really want to support the competition when I get cards next year.</div><br/></div></div><div id="42347766" class="c"><input type="checkbox" id="c-42347766" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#42347874">prev</a><span>|</span><a href="#42346948">next</a><span>|</span><label class="collapse" for="c-42347766">[-]</label><label class="expand" for="c-42347766">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s very useful. To see why, try to get an LLM _reliably_ generate JSON output without this. Sometimes it will, but sometimes it&#x27;ll just YOLO and produce something you didn&#x27;t ask for, that can&#x27;t be parsed.</div><br/></div></div></div></div></div></div></div></body></html>