<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1699693265330" as="style"/><link rel="stylesheet" href="styles.css?v=1699693265330"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e">Google Cloud TPU Multislice Training</a> <span class="domain">(<a href="https://cloud.google.com">cloud.google.com</a>)</span></div><div class="subtext"><span>infixed</span> | <span>44 comments</span></div><br/><div><div id="38222863" class="c"><input type="checkbox" id="c-38222863" checked=""/><div class="controls bullet"><span class="by">xnx</span><span>|</span><a href="#38223442">next</a><span>|</span><label class="collapse" for="c-38222863">[-]</label><label class="expand" for="c-38222863">[1 more]</label></div><br/><div class="children"><div class="content">Full title: &quot;Google Cloud demonstrates the world’s largest distributed training job for large language models across 50000+ TPU v5e chips&quot;<p>Summary from Bard: &quot;This article is about training large language models (LLMs) on Google Cloud TPUs. It discusses the challenges of training LLMs at scale, and how Google Cloud TPU Multislice Training addresses these challenges. The article also details the results of a recent experiment in which Google trained a 128B parameter LLM on 50,944 TPU v5e chips. This experiment is the largest publicly disclosed LLM distributed training job to date.&quot;</div><br/></div></div><div id="38223442" class="c"><input type="checkbox" id="c-38223442" checked=""/><div class="controls bullet"><span class="by">jrk</span><span>|</span><a href="#38222863">prev</a><span>|</span><a href="#38223570">next</a><span>|</span><label class="collapse" for="c-38223442">[-]</label><label class="expand" for="c-38223442">[5 more]</label></div><br/><div class="children"><div class="content">As far as I can tell, the article notably never defines what &quot;slices&quot; are or what &quot;multi-slice&quot; means.</div><br/><div id="38224140" class="c"><input type="checkbox" id="c-38224140" checked=""/><div class="controls bullet"><span class="by">rwitten</span><span>|</span><a href="#38223442">parent</a><span>|</span><a href="#38223570">next</a><span>|</span><label class="collapse" for="c-38224140">[-]</label><label class="expand" for="c-38224140">[4 more]</label></div><br/><div class="children"><div class="content">Great questions! Slices are a set of TPU chips that share a fast, private inter-chip-interconnect. Unlike the current GPU generation in clouds, the TPUs on different machines can communicate through this private network. Multislice means that we&#x27;re using a hierarchical network, where there is both inter-chip-interconnect and normal data-center netowrking.<p>More details:
<a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;tpu&#x2F;docs&#x2F;multislice-introduction" rel="nofollow noreferrer">https:&#x2F;&#x2F;cloud.google.com&#x2F;tpu&#x2F;docs&#x2F;multislice-introduction</a><p>(P.S. - contributor on blog post, Google employee, all thoughts my own)</div><br/><div id="38224503" class="c"><input type="checkbox" id="c-38224503" checked=""/><div class="controls bullet"><span class="by">smarterclayton</span><span>|</span><a href="#38223442">root</a><span>|</span><a href="#38224140">parent</a><span>|</span><a href="#38223570">next</a><span>|</span><label class="collapse" for="c-38224503">[-]</label><label class="expand" for="c-38224503">[3 more]</label></div><br/><div class="children"><div class="content">Also, I should point out that a set of machines hosting TPUs is referred to as a &quot;pod&quot;, which is not the same thing as a Kubernetes pod (also referenced in this doc).<p>The term &quot;pod&quot; originated in early data center design and occasionally crosses over from HPC to broad use - i.e. nVidia calls the set of DGX machines a &quot;pod&quot; <a href="https:&#x2F;&#x2F;blogs.nvidia.com&#x2F;blog&#x2F;2021&#x2F;03&#x2F;05&#x2F;what-is-a-cluster-pod&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;blogs.nvidia.com&#x2F;blog&#x2F;2021&#x2F;03&#x2F;05&#x2F;what-is-a-cluster-p...</a>.<p>Kubernetes chose &quot;pod&quot; to represent a set of co-scheduled containers, like a &quot;pod of whales&quot;.  Other systems like Mesos and Google&#x27;s Borg <a href="https:&#x2F;&#x2F;storage.googleapis.com&#x2F;pub-tools-public-publication-data&#x2F;pdf&#x2F;43438.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;storage.googleapis.com&#x2F;pub-tools-public-publication-...</a> use &quot;task&quot; to refer to a single container but didn&#x27;t have a concept for heterogenous co-scheduled tasks at the time.<p>Somewhat ironically, it now means TPUs on GKE are confusing because we have TPUs hosts organized into &quot;pods&quot;, and &quot;pods&quot; for the software using the TPUs.<p>A Kubernetes pod using a TPU lands on a host which is part of a slice of a TPU pod.</div><br/><div id="38225100" class="c"><input type="checkbox" id="c-38225100" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38223442">root</a><span>|</span><a href="#38224503">parent</a><span>|</span><a href="#38223570">next</a><span>|</span><label class="collapse" for="c-38225100">[-]</label><label class="expand" for="c-38225100">[2 more]</label></div><br/><div class="children"><div class="content">As your second link mentions in section 2.4, Borg has &quot;allocs&quot; which are basically pods.</div><br/><div id="38226042" class="c"><input type="checkbox" id="c-38226042" checked=""/><div class="controls bullet"><span class="by">smarterclayton</span><span>|</span><a href="#38223442">root</a><span>|</span><a href="#38225100">parent</a><span>|</span><a href="#38223570">next</a><span>|</span><label class="collapse" for="c-38226042">[-]</label><label class="expand" for="c-38226042">[1 more]</label></div><br/><div class="children"><div class="content">True. The pod’s monotonic and atomic lifecycle across containers is a significant difference, but you can broadly accomplish similar behaviors with an alloc for sharing resources.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38223570" class="c"><input type="checkbox" id="c-38223570" checked=""/><div class="controls bullet"><span class="by">leumassuehtam</span><span>|</span><a href="#38223442">prev</a><span>|</span><a href="#38224724">next</a><span>|</span><label class="collapse" for="c-38223570">[-]</label><label class="expand" for="c-38223570">[9 more]</label></div><br/><div class="children"><div class="content">Did they use this to train Gemini? Which raises the question, where is Gemini?</div><br/><div id="38227460" class="c"><input type="checkbox" id="c-38227460" checked=""/><div class="controls bullet"><span class="by">antifa</span><span>|</span><a href="#38223570">parent</a><span>|</span><a href="#38224480">next</a><span>|</span><label class="collapse" for="c-38227460">[-]</label><label class="expand" for="c-38227460">[1 more]</label></div><br/><div class="children"><div class="content">They also announced Claude 2 back in August, but AFAICT there&#x27;s currently no way to get API access to Claude 2.</div><br/></div></div><div id="38224480" class="c"><input type="checkbox" id="c-38224480" checked=""/><div class="controls bullet"><span class="by">lern_too_spel</span><span>|</span><a href="#38223570">parent</a><span>|</span><a href="#38227460">prev</a><span>|</span><a href="#38224724">next</a><span>|</span><label class="collapse" for="c-38224480">[-]</label><label class="expand" for="c-38224480">[7 more]</label></div><br/><div class="children"><div class="content">Unlikely. One reason Google Cloud is so terrible is that nobody in Google actually uses Google Cloud. It used to be that every time I mentioned this, somebody would jump in and say, &quot;Well actually, Google Domains runs on Google Cloud,&quot; and we&#x27;d discuss whether Google Domains was a business critical part of Google. <a href="https:&#x2F;&#x2F;support.google.com&#x2F;domains&#x2F;answer&#x2F;13689670?hl=en" rel="nofollow noreferrer">https:&#x2F;&#x2F;support.google.com&#x2F;domains&#x2F;answer&#x2F;13689670?hl=en</a></div><br/><div id="38225091" class="c"><input type="checkbox" id="c-38225091" checked=""/><div class="controls bullet"><span class="by">amf12</span><span>|</span><a href="#38223570">root</a><span>|</span><a href="#38224480">parent</a><span>|</span><a href="#38225732">next</a><span>|</span><label class="collapse" for="c-38225091">[-]</label><label class="expand" for="c-38225091">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Unlikely. One reason Google Cloud is so terrible is that nobody in Google actually uses Google Cloud.<p>Well, actually, Google Cloud is just an abstraction on top of internal Google infra, so this isn&#x27;t the right question. So, it depends on what you want to infer&#x2F;compare.</div><br/><div id="38228021" class="c"><input type="checkbox" id="c-38228021" checked=""/><div class="controls bullet"><span class="by">tdullien</span><span>|</span><a href="#38223570">root</a><span>|</span><a href="#38225091">parent</a><span>|</span><a href="#38227638">next</a><span>|</span><label class="collapse" for="c-38228021">[-]</label><label class="expand" for="c-38228021">[1 more]</label></div><br/><div class="children"><div class="content">Xoogler here. GCP was not an abstraction on Borg when I was there. GKE isn&#x27;t either.<p>So up until late 2018 when I left, very little of Cloud ran on &quot;proper&quot; Google3 infra. This may have shifted slightly (cloud has been fishing for good Google infra to externalize a lot), but in general cloud!=google3 infra.</div><br/></div></div><div id="38227638" class="c"><input type="checkbox" id="c-38227638" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#38223570">root</a><span>|</span><a href="#38225091">parent</a><span>|</span><a href="#38228021">prev</a><span>|</span><a href="#38227497">next</a><span>|</span><label class="collapse" for="c-38227638">[-]</label><label class="expand" for="c-38227638">[1 more]</label></div><br/><div class="children"><div class="content">It is the right question. It&#x27;s the right question because Google doesn&#x27;t dogfood Google Cloud like they should&#x2F;could. Dogfooding a bunch of stuff at a lower level of abstraction isn&#x27;t the same thing.</div><br/></div></div><div id="38227497" class="c"><input type="checkbox" id="c-38227497" checked=""/><div class="controls bullet"><span class="by">lern_too_spel</span><span>|</span><a href="#38223570">root</a><span>|</span><a href="#38225091">parent</a><span>|</span><a href="#38227638">prev</a><span>|</span><a href="#38225732">next</a><span>|</span><label class="collapse" for="c-38227497">[-]</label><label class="expand" for="c-38227497">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Well, actually, Google Cloud is just an abstraction on top of internal Google infra<p>I didn&#x27;t say otherwise. Of course Google Cloud runs on internal Google infrastructure. They wouldn&#x27;t have an entirely different stack to build Google Cloud on. The problem is that Googlers don&#x27;t use Google Cloud.<p>Amazonians use AWS. <a href="https:&#x2F;&#x2F;courses.cs.washington.edu&#x2F;courses&#x2F;cse452&#x2F;23wi&#x2F;papers&#x2F;yegge-platform-rant.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;courses.cs.washington.edu&#x2F;courses&#x2F;cse452&#x2F;23wi&#x2F;papers...</a><p>Microsofties use Azure. <a href="https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;microsoft-moves-closer-to-running-all-of-its-own-services-on-azure&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;microsoft-moves-closer-to-runn...</a></div><br/></div></div></div></div><div id="38225732" class="c"><input type="checkbox" id="c-38225732" checked=""/><div class="controls bullet"><span class="by">milesward</span><span>|</span><a href="#38223570">root</a><span>|</span><a href="#38224480">parent</a><span>|</span><a href="#38225091">prev</a><span>|</span><a href="#38224724">next</a><span>|</span><label class="collapse" for="c-38225732">[-]</label><label class="expand" for="c-38225732">[2 more]</label></div><br/><div class="children"><div class="content">Lots of stuff inside Alphabet runs on GCP. And yes, Google Domains pisses me off.</div><br/><div id="38227500" class="c"><input type="checkbox" id="c-38227500" checked=""/><div class="controls bullet"><span class="by">lern_too_spel</span><span>|</span><a href="#38223570">root</a><span>|</span><a href="#38225732">parent</a><span>|</span><a href="#38224724">next</a><span>|</span><label class="collapse" for="c-38227500">[-]</label><label class="expand" for="c-38227500">[1 more]</label></div><br/><div class="children"><div class="content">If anything important ran on Google Cloud, you can bet we&#x27;d see a blog post from Google Cloud marketing about that. Yes, many of the money losing side bets from the non-Google companies under the Alphabet umbrella use Google Cloud. That&#x27;s only because they want the optionality to spin them off if by some miracle any of them are ever worth anything. If they were part of Google, they would use internal infrastructure. If they weren&#x27;t under Alphabet, they would use AWS or Azure like everyone else.</div><br/></div></div></div></div></div></div></div></div><div id="38224724" class="c"><input type="checkbox" id="c-38224724" checked=""/><div class="controls bullet"><span class="by">DavidSJ</span><span>|</span><a href="#38223570">prev</a><span>|</span><a href="#38223719">next</a><span>|</span><label class="collapse" for="c-38224724">[-]</label><label class="expand" for="c-38224724">[8 more]</label></div><br/><div class="children"><div class="content">Question for rwitten or anyone else involved in this project:<p>I see a per-device batch size of 6 for the 16B model. With 256x199 = 50944 TPUs and a sequence length of 2048, this works out to 104M tokens per batch. This is much larger than typical for training runs of dense LMs of this size, which are usually closer to ~4M tokens per batch.<p>Was your critical batch size really this large? In other words, did you really see a benefit as compared to a much smaller batch size (and probably many fewer TPUs)? Did you use some special learning rate schedule or optimizer to achieve this?</div><br/><div id="38225870" class="c"><input type="checkbox" id="c-38225870" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#38224724">parent</a><span>|</span><a href="#38223719">next</a><span>|</span><label class="collapse" for="c-38225870">[-]</label><label class="expand" for="c-38225870">[7 more]</label></div><br/><div class="children"><div class="content">I’m confused - why do you multiply number of chips by sequence length? Shouldn’t the total batch size be 50k x6?</div><br/><div id="38226381" class="c"><input type="checkbox" id="c-38226381" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#38224724">root</a><span>|</span><a href="#38225870">parent</a><span>|</span><a href="#38227255">next</a><span>|</span><label class="collapse" for="c-38226381">[-]</label><label class="expand" for="c-38226381">[5 more]</label></div><br/><div class="children"><div class="content">Token count. But tokens per batch is a bad metric. I learned this the hard way through experience.<p>It turns out that what matters is step count. Increasing batch size makes the model train faster, but increasing seq length from 1024 to 2048 doesn’t make it train twice as fast. So saying 104M tokens rather than batch size 50k x 6 is misleading to yourself. (One of the most surprising aspects of learning ML was how easy it is for me to trick myself in various silly ways like that.)<p>The mental model to make this easy to remember: progress happens in discrete quantities called steps. Training on 104M tokens per step means it’s embedding 104M tokens of knowledge every step. This isn’t the same thing as requiring an special case optimizer due to large batch sizes — sequence length adds &quot;knowledge bandwidth&quot;, but otherwise doesn’t mess with the training dynamics.<p>As far as large batch optimizers, there’s LARS, which google used for their MLPerf results. I imagine they stuck with that. It creates a per-layer confidence metric, so that when the massive batch size makes a massive change, it dampens the change to smooth out the effect across the network. And since it’s a multiply, the shape of the gradient (by &quot;shape&quot; I mean in 3D space, where the Z axis is the intensity of the gradient) remains the same, so it doesn’t harm any knowledge transfer. It’s purely a stabilization aid.<p>Kind of weird I remember that after three years.</div><br/><div id="38227270" class="c"><input type="checkbox" id="c-38227270" checked=""/><div class="controls bullet"><span class="by">DavidSJ</span><span>|</span><a href="#38224724">root</a><span>|</span><a href="#38226381">parent</a><span>|</span><a href="#38227255">next</a><span>|</span><label class="collapse" for="c-38227270">[-]</label><label class="expand" for="c-38227270">[4 more]</label></div><br/><div class="children"><div class="content">Whether you count tokens or sequences, it’s about 25x the usual batch size. My guess is it makes for a fancy benchmark but isn’t actually useful. Would be interested in being proven otherwise.</div><br/><div id="38227738" class="c"><input type="checkbox" id="c-38227738" checked=""/><div class="controls bullet"><span class="by">rwitten</span><span>|</span><a href="#38224724">root</a><span>|</span><a href="#38227270">parent</a><span>|</span><a href="#38227498">next</a><span>|</span><label class="collapse" for="c-38227738">[-]</label><label class="expand" for="c-38227738">[2 more]</label></div><br/><div class="children"><div class="content">This is a great series of questions and it isn&#x27;t our goal to prove you otherwise!<p>We work with customers interested in training models who run their own ablations, including batch size and learning rates.<p>Based on that, we demonstrate workloads that we think will be interesting to potential customers! Absolutely agreed that this workload has a larger batch size than the public literature suggests.</div><br/><div id="38227836" class="c"><input type="checkbox" id="c-38227836" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#38224724">root</a><span>|</span><a href="#38227738">parent</a><span>|</span><a href="#38227498">next</a><span>|</span><label class="collapse" for="c-38227836">[-]</label><label class="expand" for="c-38227836">[1 more]</label></div><br/><div class="children"><div class="content">Wait, this actually proves their point. It sounds like you didn’t train a model, but rather ran a bunch of ops. That’s fine, but the answer to their question would be &quot;we didn’t actually measure loss or try to get a useful result, because the goal was to demonstrate raw throughput.&quot;<p>I agree that raw throughput is the metric to aim for, since figuring out how to put it to use is an exercise for the user. But it’s probably best to be straightforward about that. The reason MLPerf measures &quot;time until loss reaches X for a resnet classifier on imagenet&quot; is precisely because it gives information about performance at scale —- if you didn’t train anything, you haven’t actually achieved &quot;fastest training run&quot;. You’ve achieved largest throughput, which is similar but not the same.<p>And I don’t think this is a pedantic distinction. Just throw LARS on it (the MLPerf code you used in 2019 is at <a href="https:&#x2F;&#x2F;github.com&#x2F;shawwn&#x2F;daxx-lightning">https:&#x2F;&#x2F;github.com&#x2F;shawwn&#x2F;daxx-lightning</a> fwiw, and it runs on pods last time I tried) and see how it performs in practice.<p>EDIT: reading over <a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;maxtext">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;maxtext</a>, it looks pretty delightful. I was in the TPU scene back in 2020, and there was no way to do ahead of time compilation. Restarting training runs was a major pain point once LLMs became the focus, and I kept pestering James Bradbury to please add it. Happy to see that it finally made its way in.<p>It sounds like MaxText is the right approach, but until you try to actually train a model —- to achieve a low loss on a specific dataset —- you can’t know whether the code works. This isn’t theoretical. I spent over a year debugging google’s public BigGAN code (compare_gan) and discovered why it never worked: the batch norm gamma parameter was initialized to zero instead of one, so everything was being multiplied by zero to start off, which severely crippled the model.<p>A bug like that could easily be lurking in MaxText. You can’t know until you try to train a useful LLM. Note that compare_gan <i>seemed to work</i>; the authors noted that they couldn’t replicate the performance of the official BigGAN paper, but the samples looked sort of reasonable. But the model was screwy, and no one knew why until the rigorous debugging process.<p>If you need help with this, let me know. There are challenges when training an actual LLM that aren’t present in theoretical runs like these. For example, you need a big dataset. The Pile is a good starting point for that, and it gives a nice comparison baseline, e.g. to GPT-J.<p>Alternatively, post a link to a tensorboard.dev showing the loss curves for your training runs. I suspect the reason you didn’t is because you didn’t have a real dataset. That’s ok, but it doesn’t prove that MaxText works until there’s empirical evidence.<p>In other words, DavidSJ was precisely right: it’s an impressive-looking benchmark, which doesn’t actually help your customers train LLMs in practice. They’ll need to solve this problem eventually, and the optimizer is certainly one aspect. The other is the quantized INT8 training. It may sound impressive to say it gives a 1.4x step count speedup, but that’s useless if it harms loss convergence. How do you know it doesn’t? This isn’t an easy question to answer unless you run MLPerf or some other known stable baseline, which I’m a little shocked no one has done yet.</div><br/></div></div></div></div><div id="38227498" class="c"><input type="checkbox" id="c-38227498" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#38224724">root</a><span>|</span><a href="#38227270">parent</a><span>|</span><a href="#38227738">prev</a><span>|</span><a href="#38227255">next</a><span>|</span><label class="collapse" for="c-38227498">[-]</label><label class="expand" for="c-38227498">[1 more]</label></div><br/><div class="children"><div class="content">Proof by contradiction: they used batch size 6 because 7 was past the point of diminishing returns.</div><br/></div></div></div></div></div></div><div id="38227255" class="c"><input type="checkbox" id="c-38227255" checked=""/><div class="controls bullet"><span class="by">DavidSJ</span><span>|</span><a href="#38224724">root</a><span>|</span><a href="#38225870">parent</a><span>|</span><a href="#38226381">prev</a><span>|</span><a href="#38223719">next</a><span>|</span><label class="collapse" for="c-38227255">[-]</label><label class="expand" for="c-38227255">[1 more]</label></div><br/><div class="children"><div class="content">You can measure it either way, and you’ll see it both ways in the literature. In this case it doesn’t matter much how you measure since 2048 is a typical pretraining sequence length. 300k sequences per batch is huge compared to typical batch sizes in the literature, which are closer to 2048, for about 4M tokens total.</div><br/></div></div></div></div></div></div><div id="38223719" class="c"><input type="checkbox" id="c-38223719" checked=""/><div class="controls bullet"><span class="by">sashank_1509</span><span>|</span><a href="#38224724">prev</a><span>|</span><a href="#38223375">next</a><span>|</span><label class="collapse" for="c-38223719">[-]</label><label class="expand" for="c-38223719">[12 more]</label></div><br/><div class="children"><div class="content">Ok so they claim in the article, 50000 TPU’s is equivalent to 10 exaflop floating point computations. That is equivalent to ~2,512 NVIDIA H100’s, which is like really small. Just shows the difference between TPU’s and GPU’s I guess. Inflection, a new LLM company created a 20,000 H100 cluster, I’m positive OpenAI, Tesla, Meta etc have orchestrated a job on more than 2500 H100 GPU’s.</div><br/><div id="38224065" class="c"><input type="checkbox" id="c-38224065" checked=""/><div class="controls bullet"><span class="by">rwitten</span><span>|</span><a href="#38223719">parent</a><span>|</span><a href="#38223859">next</a><span>|</span><label class="collapse" for="c-38224065">[-]</label><label class="expand" for="c-38224065">[5 more]</label></div><br/><div class="children"><div class="content">Hey! I&#x27;m an contributor on this (Rafi Witten), all opinions my own.<p>You&#x27;re asking the right question but I think the math is off by a bit. The equivalent number on the H100&#x27;s is 989 TFLOP&#x2F;s&#x2F;chip so the equivalent job is ~10K H100&#x27;s = (10 * 10^18) &#x2F; (989 * 10^12). (Both chips also have 8-bit acceleration!)<p>I believe this is the largest ML job both by exaflops and number of chips every demonstrated. Other companies own more chips or exaflops than we show in this job but getting all the hardware working at once on a single job is a different matter! :-)</div><br/><div id="38224397" class="c"><input type="checkbox" id="c-38224397" checked=""/><div class="controls bullet"><span class="by">sashank_1509</span><span>|</span><a href="#38223719">root</a><span>|</span><a href="#38224065">parent</a><span>|</span><a href="#38224319">next</a><span>|</span><label class="collapse" for="c-38224397">[-]</label><label class="expand" for="c-38224397">[3 more]</label></div><br/><div class="children"><div class="content">I think your math is also slightly off, in the Google article, it claims 
“that is capable of achieving 10 exa-FLOPs (16-bit).” , so you should be comparing with 16 bit operations from a H100.<p>989 is TF32 core, for 16 bit it is 1979, so I guess around 5000 H100’s in a single training job would be equivalent to the training job mentioned in this article.<p>Either way I actually would not be surprised if OpenAI has launched a single job on more than 10k GPU’s, but I also am not very knowledgeable on practical scaling. Congrats on the feat!</div><br/><div id="38224446" class="c"><input type="checkbox" id="c-38224446" checked=""/><div class="controls bullet"><span class="by">aschleck</span><span>|</span><a href="#38223719">root</a><span>|</span><a href="#38224397">parent</a><span>|</span><a href="#38224319">next</a><span>|</span><label class="collapse" for="c-38224446">[-]</label><label class="expand" for="c-38224446">[2 more]</label></div><br/><div class="children"><div class="content">1979 16 bit flops on an H100 is with sparsity. See footnote 2 on <a href="https:&#x2F;&#x2F;www.nvidia.com&#x2F;en-us&#x2F;data-center&#x2F;h100&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nvidia.com&#x2F;en-us&#x2F;data-center&#x2F;h100&#x2F;</a>. You should be halving it for non-sparse flops.</div><br/><div id="38228450" class="c"><input type="checkbox" id="c-38228450" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#38223719">root</a><span>|</span><a href="#38224446">parent</a><span>|</span><a href="#38224319">next</a><span>|</span><label class="collapse" for="c-38228450">[-]</label><label class="expand" for="c-38228450">[1 more]</label></div><br/><div class="children"><div class="content">GP is correct. With sparsity it is 3958. 1979 Tflop&#x2F;s is without sparsity.</div><br/></div></div></div></div></div></div><div id="38224319" class="c"><input type="checkbox" id="c-38224319" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38223719">root</a><span>|</span><a href="#38224065">parent</a><span>|</span><a href="#38224397">prev</a><span>|</span><a href="#38223859">next</a><span>|</span><label class="collapse" for="c-38224319">[-]</label><label class="expand" for="c-38224319">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to hear more about the challenges of getting the hardware working.</div><br/></div></div></div></div><div id="38223859" class="c"><input type="checkbox" id="c-38223859" checked=""/><div class="controls bullet"><span class="by">aschleck</span><span>|</span><a href="#38223719">parent</a><span>|</span><a href="#38224065">prev</a><span>|</span><a href="#38224024">next</a><span>|</span><label class="collapse" for="c-38223859">[-]</label><label class="expand" for="c-38223859">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s worth noting that just because an H100 has a higher flops number doesn&#x27;t mean your program is actually hitting that number of flops. Modern TPUs are surprisingly competitive with Nvidia on a perf&#x2F;$ metric, if you&#x27;re doing cloud ML they are absolutely worth a look. We have been keeping costs down by racking our own GPUs but TPUs are so cost effective that we need to do some thinking about changing our approach.<p>I&#x27;m not certain but I think part of this is that XLA (for example) is a mountain of chip-specific optimizations between your code and the actual operations. So comparing your throughput between GPU and TPU is not just flops-to-flops.</div><br/></div></div><div id="38224024" class="c"><input type="checkbox" id="c-38224024" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38223719">parent</a><span>|</span><a href="#38223859">prev</a><span>|</span><a href="#38224168">next</a><span>|</span><label class="collapse" for="c-38224024">[-]</label><label class="expand" for="c-38224024">[3 more]</label></div><br/><div class="children"><div class="content">It sounds like they partnered with CoreWeave to use their equipment and it is &quot;only&quot; 3500 gpus so far.<p><a href="https:&#x2F;&#x2F;inflection.ai&#x2F;inflection-ai-announces-1-3-billion-of-funding" rel="nofollow noreferrer">https:&#x2F;&#x2F;inflection.ai&#x2F;inflection-ai-announces-1-3-billion-of...</a><p><a href="https:&#x2F;&#x2F;inflection.ai&#x2F;nvidia-coreweave-mlperf" rel="nofollow noreferrer">https:&#x2F;&#x2F;inflection.ai&#x2F;nvidia-coreweave-mlperf</a></div><br/><div id="38227386" class="c"><input type="checkbox" id="c-38227386" checked=""/><div class="controls bullet"><span class="by">abatilo</span><span>|</span><a href="#38223719">root</a><span>|</span><a href="#38224024">parent</a><span>|</span><a href="#38224168">next</a><span>|</span><label class="collapse" for="c-38227386">[-]</label><label class="expand" for="c-38227386">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;youtu.be&#x2F;z3hmfSVmyqg?si=eLPZ0D6ug3D6PreI" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;z3hmfSVmyqg?si=eLPZ0D6ug3D6PreI</a><p>As of 2 months ago, they had at least 7000 up and running, fwiw</div><br/><div id="38227825" class="c"><input type="checkbox" id="c-38227825" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38223719">root</a><span>|</span><a href="#38227386">parent</a><span>|</span><a href="#38224168">next</a><span>|</span><label class="collapse" for="c-38227825">[-]</label><label class="expand" for="c-38227825">[1 more]</label></div><br/><div class="children"><div class="content">The meat starts here:<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;z3hmfSVmyqg?feature=shared&amp;t=3328" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;z3hmfSVmyqg?feature=shared&amp;t=3328</a><p>I&#x27;m curious why it is so hard for them to deploy the compute. They seem to be fairly behind schedule.</div><br/></div></div></div></div></div></div><div id="38224168" class="c"><input type="checkbox" id="c-38224168" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#38223719">parent</a><span>|</span><a href="#38224024">prev</a><span>|</span><a href="#38224387">next</a><span>|</span><label class="collapse" for="c-38224168">[-]</label><label class="expand" for="c-38224168">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That is equivalent<p>On what number or op for the h100?</div><br/></div></div><div id="38224387" class="c"><input type="checkbox" id="c-38224387" checked=""/><div class="controls bullet"><span class="by">lern_too_spel</span><span>|</span><a href="#38223719">parent</a><span>|</span><a href="#38224168">prev</a><span>|</span><a href="#38223375">next</a><span>|</span><label class="collapse" for="c-38224387">[-]</label><label class="expand" for="c-38224387">[1 more]</label></div><br/><div class="children"><div class="content">This is a blog post from Google Cloud marketing. It&#x27;s saying that you, too, could train an LLM on Google Cloud if you hand them enough money. You can&#x27;t do that on Inflection&#x27;s or Tesla&#x27;s clusters. Similar marketing blog post from last year: <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;compute&#x2F;calculating-100-trillion-digits-of-pi-on-google-cloud" rel="nofollow noreferrer">https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;compute&#x2F;calculating-1...</a><p>The PaLM paper linked in the blog post is about how to get something actually useful out of that compute.</div><br/></div></div></div></div><div id="38223375" class="c"><input type="checkbox" id="c-38223375" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#38223719">prev</a><span>|</span><a href="#38223604">next</a><span>|</span><label class="collapse" for="c-38223375">[-]</label><label class="expand" for="c-38223375">[3 more]</label></div><br/><div class="children"><div class="content">Can someone ELI5 this?</div><br/><div id="38223574" class="c"><input type="checkbox" id="c-38223574" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38223375">parent</a><span>|</span><a href="#38223586">next</a><span>|</span><label class="collapse" for="c-38223574">[-]</label><label class="expand" for="c-38223574">[1 more]</label></div><br/><div class="children"><div class="content">Google has a megawatt-scale TPUv5 cluster that they can spare for stunts, and they are flexing it.</div><br/></div></div><div id="38223586" class="c"><input type="checkbox" id="c-38223586" checked=""/><div class="controls bullet"><span class="by">filterfiber</span><span>|</span><a href="#38223375">parent</a><span>|</span><a href="#38223574">prev</a><span>|</span><a href="#38223604">next</a><span>|</span><label class="collapse" for="c-38223586">[-]</label><label class="expand" for="c-38223586">[1 more]</label></div><br/><div class="children"><div class="content">tl;dr - google has big computer (a lot of TPUs)<p>they just showed they could indeed make 50k TPUs do some flops.<p>With no paper this is just a marketing press release - the only takeaway is that existing tech stacks can utilize it probably.</div><br/></div></div></div></div><div id="38223604" class="c"><input type="checkbox" id="c-38223604" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38223375">prev</a><span>|</span><label class="collapse" for="c-38223604">[-]</label><label class="expand" for="c-38223604">[5 more]</label></div><br/><div class="children"><div class="content">Something that doesn&#x27;t seem worth bragging about is that the startup time increases linearly with the cluster size. Wouldn&#x27;t you want it to be constant? What&#x27;s the issue there?</div><br/><div id="38223979" class="c"><input type="checkbox" id="c-38223979" checked=""/><div class="controls bullet"><span class="by">smarterclayton</span><span>|</span><a href="#38223604">parent</a><span>|</span><a href="#38224104">next</a><span>|</span><label class="collapse" for="c-38223979">[-]</label><label class="expand" for="c-38223979">[3 more]</label></div><br/><div class="children"><div class="content">Disclaimer: work associated with this team, didn&#x27;t write or review the blog post<p>Article stated that it was throughput scheduling the pods on the clusters (from unrelated benchmarks that&#x27;s usually ~300 pods&#x2F;sec throughput for kube scheduler today) and then doing XLA compilation at pod launch, rather than amortizing once for all jobs.<p>Optimizing throughput of kube scheduler is a good general opportunity and something I believe we would like to see.<p>I believe AOT compilation just not a critical optimization for the test, we <i>would</i> recommend it when running large and long training jobs to AOT compile to keep pod start latency low for hardware failures and job restarts (from checkpoints).<p>&gt; The start times we observed were impressive, but we believe we can improve these even further. We are working on areas such as optimizing scheduling in GKE to increase throughput and enabling ahead-of-time compilation in MaxText to avoid just-in-time compilations on the full cluster.</div><br/><div id="38224432" class="c"><input type="checkbox" id="c-38224432" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38223604">root</a><span>|</span><a href="#38223979">parent</a><span>|</span><a href="#38224104">next</a><span>|</span><label class="collapse" for="c-38224432">[-]</label><label class="expand" for="c-38224432">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for the context. I remember recently reading a paper from I think Baidu where they claimed to have a container arrival rate in the millions per second, consequently it was practical to operate their whole site in the style of lambda&#x2F;cloud functions.<p>Actually now that I am searching for that it seems Baidu has a number of papers on workload orchestration at scale specifically for learning.</div><br/><div id="38224552" class="c"><input type="checkbox" id="c-38224552" checked=""/><div class="controls bullet"><span class="by">smarterclayton</span><span>|</span><a href="#38223604">root</a><span>|</span><a href="#38224432">parent</a><span>|</span><a href="#38224104">next</a><span>|</span><label class="collapse" for="c-38224552">[-]</label><label class="expand" for="c-38224552">[1 more]</label></div><br/><div class="children"><div class="content">I will note that a trend I have observed with recent ML - as we increasingly use accelerators and models correspondingly grow in size, we are returning to a &quot;one machine, one workload&quot; paradigm for the biggest training and inference jobs.  You might have 8k accelerators, but only 1000 machines, and if you have one container per host 300 schedules &#x2F; second is fast.<p>While at the same time as you note we have functional models for container execution that are approaching millions of dispatches for highly partitionable work, especially in data engineering and ETL.</div><br/></div></div></div></div></div></div><div id="38224104" class="c"><input type="checkbox" id="c-38224104" checked=""/><div class="controls bullet"><span class="by">rwitten</span><span>|</span><a href="#38223604">parent</a><span>|</span><a href="#38223979">prev</a><span>|</span><label class="collapse" for="c-38224104">[-]</label><label class="expand" for="c-38224104">[1 more]</label></div><br/><div class="children"><div class="content">(Contributor on the blog post, all opinions my own)<p>Agreed with you and we definitely weren&#x27;t trying to brag! This is fast compared to people&#x27;s expectations in the space but slow compared to what we should be able to accomplish and will accomplish in the future.</div><br/></div></div></div></div></div></div></div></div></div></body></html>