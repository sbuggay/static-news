<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1721984466086" as="style"/><link rel="stylesheet" href="styles.css?v=1721984466086"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://rcoh.me/posts/linear-time-median-finding/">My Favorite Algorithm: Linear Time Median Finding (2018)</a>Â <span class="domain">(<a href="https://rcoh.me">rcoh.me</a>)</span></div><div class="subtext"><span>skanderbm</span> | <span>143 comments</span></div><br/><div><div id="41068098" class="c"><input type="checkbox" id="c-41068098" checked=""/><div class="controls bullet"><span class="by">danlark</span><span>|</span><a href="#41071372">next</a><span>|</span><label class="collapse" for="c-41068098">[-]</label><label class="expand" for="c-41068098">[2 more]</label></div><br/><div class="children"><div class="content">Around 4 years ago I compared lots of different median algorithms and the article turned out to be much longer than I anticipated :)<p><a href="https:&#x2F;&#x2F;danlark.org&#x2F;2020&#x2F;11&#x2F;11&#x2F;miniselect-practical-and-generic-selection-algorithms&#x2F;" rel="nofollow">https:&#x2F;&#x2F;danlark.org&#x2F;2020&#x2F;11&#x2F;11&#x2F;miniselect-practical-and-gene...</a></div><br/><div id="41068529" class="c"><input type="checkbox" id="c-41068529" checked=""/><div class="controls bullet"><span class="by">cfors</span><span>|</span><a href="#41068098">parent</a><span>|</span><a href="#41071372">next</a><span>|</span><label class="collapse" for="c-41068529">[-]</label><label class="expand" for="c-41068529">[1 more]</label></div><br/><div class="children"><div class="content">Just wanted to say thank you for this article - I&#x27;ve read and shared this a few times over the years!</div><br/></div></div></div></div><div id="41071372" class="c"><input type="checkbox" id="c-41071372" checked=""/><div class="controls bullet"><span class="by">rented_mule</span><span>|</span><a href="#41068098">prev</a><span>|</span><a href="#41076554">next</a><span>|</span><label class="collapse" for="c-41071372">[-]</label><label class="expand" for="c-41071372">[17 more]</label></div><br/><div class="children"><div class="content">10-15 years ago, I found myself needing to regularly find the median of many billions of values, each parsed out of a multi-kilobyte log entry. MapReduce was what we were using for processing large amounts of data at the time. With MapReduce over that much data, you don&#x27;t just want linear time, but ideally single pass, distributed across machines. Subsequent passes over much smaller amounts of data are fine.<p>It was a struggle until I figured out that knowledge of the precision and range of our data helped. These were timings, expressed in integer milliseconds. So they were non-negative, and I knew the 90th percentile was well under a second.<p>As the article mentions, finding a median typically involves something akin to sorting. With the above knowledge, bucket sort becomes available, with a slight tweak in my case. Even if the samples were floating point, the same approach could be used as long as an integer (or even fixed point) approximation that is very close to the true median is good enough, again assuming a known, relatively small range.<p>The idea is to build a dictionary where the keys are the timings in integer milliseconds and the values are a count of the keys&#x27; appearance in the data, i.e., a histogram of timings. The maximum timing isn&#x27;t known, so to ensure the size of the dictionary doesn&#x27;t get out of control, use the knowledge that the 90th percentile is well under a second and count everything over, say, 999ms in the 999ms bin. Then the dictionary will be limited to 2000 integers (keys in the range 0-999 and corresponding values) - this is the part that is different from an ordinary bucket sort. All of that is trivial to do in a single pass, even when distributed with MapReduce. Then it&#x27;s easy to get the median from that dictionary &#x2F; histogram.</div><br/><div id="41071519" class="c"><input type="checkbox" id="c-41071519" checked=""/><div class="controls bullet"><span class="by">justinpombrio</span><span>|</span><a href="#41071372">parent</a><span>|</span><a href="#41076347">next</a><span>|</span><label class="collapse" for="c-41071519">[-]</label><label class="expand" for="c-41071519">[10 more]</label></div><br/><div class="children"><div class="content">Did you actually need to find the true median of billions of values? Or would finding a value between 49.9% and 50.1% suffice? Because the latter is much easier: sample 10,000 elements uniformly at random and take their median.<p>(I made the number 10,000 up, but you could do some statistics to figure out how many samples would be needed for a given level of confidence, and I don&#x27;t think it would be prohibitively large.)</div><br/><div id="41072762" class="c"><input type="checkbox" id="c-41072762" checked=""/><div class="controls bullet"><span class="by">rented_mule</span><span>|</span><a href="#41071372">root</a><span>|</span><a href="#41071519">parent</a><span>|</span><a href="#41072828">next</a><span>|</span><label class="collapse" for="c-41072762">[-]</label><label class="expand" for="c-41072762">[4 more]</label></div><br/><div class="children"><div class="content">The kind of margin you indicate would have been plenty for our use cases. But, we were already processing all these log entries for multiple other purposes in a single pass (not one pass per thing computed). With this single pass approach, the median calculation could happen with the same single-pass parsing of the logs (they were JSON and that parsing was most of our cost), roughly for free.<p>Uniform sampling also wasn&#x27;t obviously simple, at least to me. There were thousands of log files involved, coming from hundreds of computers. Any single log file only had timings from a single computer. What kind of bias would be introduced by different approaches to distributing those log files to a cluster for the median calculation? Once the solution outlined in the previous comment was identified, that seemed simpler that trying to understand if we were talking about 49-51% or 40-50%. And if it was too big a margin, restructuring our infra to allow different log file distribution algorithms would have been far more complicated.</div><br/><div id="41076771" class="c"><input type="checkbox" id="c-41076771" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#41071372">root</a><span>|</span><a href="#41072762">parent</a><span>|</span><a href="#41075641">next</a><span>|</span><label class="collapse" for="c-41076771">[-]</label><label class="expand" for="c-41076771">[1 more]</label></div><br/><div class="children"><div class="content">Speaking of &quot;single pass&quot;, one of the criticisms I have of the &quot;enumerator&quot; patterns in modern programming languages is that they encourage multiple passes.<p>As an example: computing the .min() and .max() of an enumerable is <i>two passes</i> even though it could be done with one pass.<p>I&#x27;d love to see a language embrace a more efficient style similar to how a SQL does it, where you can elegantly request this as a single pass over the data: &quot;SELECT min(x), max(x) FROM y&quot;</div><br/></div></div><div id="41075641" class="c"><input type="checkbox" id="c-41075641" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#41071372">root</a><span>|</span><a href="#41072762">parent</a><span>|</span><a href="#41076771">prev</a><span>|</span><a href="#41072828">next</a><span>|</span><label class="collapse" for="c-41075641">[-]</label><label class="expand" for="c-41075641">[2 more]</label></div><br/><div class="children"><div class="content">Actually, seeking the bias numbers can be quite illuminating.</div><br/><div id="41076509" class="c"><input type="checkbox" id="c-41076509" checked=""/><div class="controls bullet"><span class="by">rented_mule</span><span>|</span><a href="#41071372">root</a><span>|</span><a href="#41075641">parent</a><span>|</span><a href="#41072828">next</a><span>|</span><label class="collapse" for="c-41076509">[-]</label><label class="expand" for="c-41076509">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re absolutely right! Some learning that we came to later that isn&#x27;t unrelated to what you&#x27;re saying... don&#x27;t just look at metrics (in the case I&#x27;ve described above, it was timings of operations in a large system), but look at histograms for them. You should be able to explain why they have the shape they do. The distributions are so often multimodal, and understanding the modes helps you understand a lot more nuance about your system.<p>We were an infra-focused team building up a scalable data handling &#x2F; computation platform in preparation to apply ML at a non-trivial scale. When we finally hired some people with deep stats knowledge 10 or 12 years ago, there was a lot of great learning for everyone about how each of our areas of expertise helped the others. I wish we&#x27;d found the right stats people earlier to help us understand things like this more deeply, more quickly.</div><br/></div></div></div></div></div></div><div id="41072828" class="c"><input type="checkbox" id="c-41072828" checked=""/><div class="controls bullet"><span class="by">enriquto</span><span>|</span><a href="#41071372">root</a><span>|</span><a href="#41071519">parent</a><span>|</span><a href="#41072762">prev</a><span>|</span><a href="#41071773">next</a><span>|</span><label class="collapse" for="c-41072828">[-]</label><label class="expand" for="c-41072828">[4 more]</label></div><br/><div class="children"><div class="content">&gt; the latter is much easier: sample 10,000 elements uniformly at random and take their median<p>Do you have a source for that claim?<p>I don&#x27;t see how could that possibly be true... For example, if your original points are sampled from two gaussians of centers -100 and 100, of small but slightly different variance, then the true median can be anywhere between  the two centers, and you may need a humungous number of samples to get anywhere close to it.<p>True, in that case any point between say -90 and 90 would be equally good as a median in most applications.  But this does not mean that the median can be found accurately by your method.</div><br/><div id="41075751" class="c"><input type="checkbox" id="c-41075751" checked=""/><div class="controls bullet"><span class="by">rhymer</span><span>|</span><a href="#41071372">root</a><span>|</span><a href="#41072828">parent</a><span>|</span><a href="#41073628">next</a><span>|</span><label class="collapse" for="c-41075751">[-]</label><label class="expand" for="c-41075751">[1 more]</label></div><br/><div class="children"><div class="content">Asymptotic properties of quantile estimators are widely studied [1]. The key is to have a sufficiently large sample size.<p>[1] Bahadur, R. R. (1966). A note on quantiles in large samples. Annals of Mathematical Statistics, 37, 577â580.</div><br/></div></div><div id="41073628" class="c"><input type="checkbox" id="c-41073628" checked=""/><div class="controls bullet"><span class="by">maronato</span><span>|</span><a href="#41071372">root</a><span>|</span><a href="#41072828">parent</a><span>|</span><a href="#41075751">prev</a><span>|</span><a href="#41075026">next</a><span>|</span><label class="collapse" for="c-41073628">[-]</label><label class="expand" for="c-41073628">[1 more]</label></div><br/><div class="children"><div class="content">the key word is âuniformlyâ. If your data is not uniformly distributed, then you just have to pick random values non-uniformly. There are many ways to do that, and once you have your algo youâll be able to reliably find an approximation of the median much faster than you would find the actual median.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Non-uniform_random_variate_generation" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Non-uniform_random_variate_g...</a></div><br/></div></div><div id="41075026" class="c"><input type="checkbox" id="c-41075026" checked=""/><div class="controls bullet"><span class="by">B1FF_PSUVM</span><span>|</span><a href="#41071372">root</a><span>|</span><a href="#41072828">parent</a><span>|</span><a href="#41073628">prev</a><span>|</span><a href="#41071773">next</a><span>|</span><label class="collapse" for="c-41075026">[-]</label><label class="expand" for="c-41075026">[1 more]</label></div><br/><div class="children"><div class="content">&gt; this does not mean that the median can be found accurately by your method.<p>You can do dynamic sampling: e.g. take double the samples, see what decimal in your result budges. Adjust.</div><br/></div></div></div></div><div id="41071773" class="c"><input type="checkbox" id="c-41071773" checked=""/><div class="controls bullet"><span class="by">andruby</span><span>|</span><a href="#41071372">root</a><span>|</span><a href="#41071519">parent</a><span>|</span><a href="#41072828">prev</a><span>|</span><a href="#41076347">next</a><span>|</span><label class="collapse" for="c-41071773">[-]</label><label class="expand" for="c-41071773">[1 more]</label></div><br/><div class="children"><div class="content">I was thinking the same thing.<p>In all use-cases I&#x27;ve seen a close estimate of the median was enough.</div><br/></div></div></div></div><div id="41076347" class="c"><input type="checkbox" id="c-41076347" checked=""/><div class="controls bullet"><span class="by">digaozao</span><span>|</span><a href="#41071372">parent</a><span>|</span><a href="#41071519">prev</a><span>|</span><a href="#41075434">next</a><span>|</span><label class="collapse" for="c-41076347">[-]</label><label class="expand" for="c-41076347">[1 more]</label></div><br/><div class="children"><div class="content">I am not sure. But from the outside, it looks like what Prometheus does behind the scenes.
It seems to me that Prometheus works like that because it has a limit on latency time around 10s on some systems I worked.
So when we had requests above that limit it got all on 10s, even though it could be higher than that.
Interesting.</div><br/></div></div><div id="41075434" class="c"><input type="checkbox" id="c-41075434" checked=""/><div class="controls bullet"><span class="by">ashton314</span><span>|</span><a href="#41071372">parent</a><span>|</span><a href="#41076347">prev</a><span>|</span><a href="#41072891">next</a><span>|</span><label class="collapse" for="c-41075434">[-]</label><label class="expand" for="c-41075434">[2 more]</label></div><br/><div class="children"><div class="content">Where were you working? Sounds like you got lucky to work on some fun problems!</div><br/><div id="41076406" class="c"><input type="checkbox" id="c-41076406" checked=""/><div class="controls bullet"><span class="by">rented_mule</span><span>|</span><a href="#41071372">root</a><span>|</span><a href="#41075434">parent</a><span>|</span><a href="#41072891">next</a><span>|</span><label class="collapse" for="c-41076406">[-]</label><label class="expand" for="c-41076406">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, but I&#x27;m trying to keep this account relatively anonymous to sidestep some of my issues with being shy.<p>But, you&#x27;re right, I was lucky to work on a bunch of fun problems. That period, in particular, was pretty amazing. I was part of a fun, collaborative team working on hard problems. And management showed a lot of trust in us. We came up with some very interesting solutions, some by skill and some by luck, that set the foundation for years of growth that came after that (both revenue growth and technical platform growth).</div><br/></div></div></div></div><div id="41072891" class="c"><input type="checkbox" id="c-41072891" checked=""/><div class="controls bullet"><span class="by">ant6n</span><span>|</span><a href="#41071372">parent</a><span>|</span><a href="#41075434">prev</a><span>|</span><a href="#41076554">next</a><span>|</span><label class="collapse" for="c-41072891">[-]</label><label class="expand" for="c-41072891">[3 more]</label></div><br/><div class="children"><div class="content">Iâm not sure why you use a dictionary with keys 0â¦999, instead of an array indexed 0â¦999.</div><br/><div id="41075104" class="c"><input type="checkbox" id="c-41075104" checked=""/><div class="controls bullet"><span class="by">rented_mule</span><span>|</span><a href="#41071372">root</a><span>|</span><a href="#41072891">parent</a><span>|</span><a href="#41074711">next</a><span>|</span><label class="collapse" for="c-41075104">[-]</label><label class="expand" for="c-41075104">[1 more]</label></div><br/><div class="children"><div class="content">I was using the term dictionary for illustration purposes. Remember, this was all in the context of MapReduce. Computation within MapReduce is built around grouping values by keys, which makes dictionaries a natural way to think about many MapReduce oriented algorithms, at least for me. The key&#x2F;value pairs appear as streams of two-tuples, not as dictionaries or arrays.</div><br/></div></div><div id="41074711" class="c"><input type="checkbox" id="c-41074711" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#41071372">root</a><span>|</span><a href="#41072891">parent</a><span>|</span><a href="#41075104">prev</a><span>|</span><a href="#41076554">next</a><span>|</span><label class="collapse" for="c-41074711">[-]</label><label class="expand" for="c-41074711">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s just a dict&#x2F;map with less flexibility on the keys :D</div><br/></div></div></div></div></div></div><div id="41076554" class="c"><input type="checkbox" id="c-41076554" checked=""/><div class="controls bullet"><span class="by">kwantam</span><span>|</span><a href="#41071372">prev</a><span>|</span><a href="#41068928">next</a><span>|</span><label class="collapse" for="c-41076554">[-]</label><label class="expand" for="c-41076554">[3 more]</label></div><br/><div class="children"><div class="content">One of the fun things about the median-of-medians algorithm is its completely star-studded author list.<p>Manuel Blum - Turing award winner in 1995<p>Robert Floyd - Turing award winner in 1978<p>Ron Rivest - Turing award winner in 2002<p>Bob Tarjan - Turing award winner in 1986 (oh and also the inaugural Nevanlinna prizewinner in 1982)<p>Vaughan Pratt - oh no, the only non-Turing award winner in the list. Oh right but he&#x27;s emeritus faculty at Stanford, directed the SUN project before it became Sun Microsystems, was instrumental in Sun&#x27;s early days (director of research and designer of the Sun logo!), and is responsible for all kinds of other awesome stuff (near and dear to me: Pratt certificates of primality).<p>Four independent Turing awards! SPARCstations! This paper has it all.</div><br/><div id="41076601" class="c"><input type="checkbox" id="c-41076601" checked=""/><div class="controls bullet"><span class="by">Munksgaard</span><span>|</span><a href="#41076554">parent</a><span>|</span><a href="#41076753">next</a><span>|</span><label class="collapse" for="c-41076601">[-]</label><label class="expand" for="c-41076601">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s a direct link for anyone who, like me, would be interested in reading the original article: <a href="https:&#x2F;&#x2F;people.csail.mit.edu&#x2F;rivest&#x2F;pubs&#x2F;BFPRT73.pdf" rel="nofollow">https:&#x2F;&#x2F;people.csail.mit.edu&#x2F;rivest&#x2F;pubs&#x2F;BFPRT73.pdf</a><p>That&#x27;s an impressive list of authors, for sure.</div><br/></div></div><div id="41076753" class="c"><input type="checkbox" id="c-41076753" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#41076554">parent</a><span>|</span><a href="#41076601">prev</a><span>|</span><a href="#41068928">next</a><span>|</span><label class="collapse" for="c-41076753">[-]</label><label class="expand" for="c-41076753">[1 more]</label></div><br/><div class="children"><div class="content">Job interview question for an entry-level front end developer: &quot;Reproduce the work of four Turing award winners in the next thirty minutes. You have a dirty whiteboard and a dry pen. Your time begins... <i>now</i>.&quot;</div><br/></div></div></div></div><div id="41068928" class="c"><input type="checkbox" id="c-41068928" checked=""/><div class="controls bullet"><span class="by">xinok</span><span>|</span><a href="#41076554">prev</a><span>|</span><a href="#41066860">next</a><span>|</span><label class="collapse" for="c-41068928">[-]</label><label class="expand" for="c-41068928">[3 more]</label></div><br/><div class="children"><div class="content">&gt; P.S: In 2017 a new paper came out that actually makes the median-of-medians approach competitive with other selection algorithms. Thanks to the paperâs author, Andrei Alexandrescu for bringing it to my attention!<p>He also gave a talk about his algorithm in 2016. He&#x27;s an entertaining presenter, I highly recommended!<p>There&#x27;s Treasure Everywhere - Andrei Alexandrescu<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=fd1_Miy1Clg" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=fd1_Miy1Clg</a></div><br/><div id="41075463" class="c"><input type="checkbox" id="c-41075463" checked=""/><div class="controls bullet"><span class="by">fasa99</span><span>|</span><a href="#41068928">parent</a><span>|</span><a href="#41072110">next</a><span>|</span><label class="collapse" for="c-41075463">[-]</label><label class="expand" for="c-41075463">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s wild, a bit of a polymath by computer science standards.  I know him from template metaprogramming fame and here he is shifting from programming languages to algorithms</div><br/></div></div><div id="41072110" class="c"><input type="checkbox" id="c-41072110" checked=""/><div class="controls bullet"><span class="by">pjkundert</span><span>|</span><a href="#41068928">parent</a><span>|</span><a href="#41075463">prev</a><span>|</span><a href="#41066860">next</a><span>|</span><label class="collapse" for="c-41072110">[-]</label><label class="expand" for="c-41072110">[1 more]</label></div><br/><div class="children"><div class="content">Andrei Alexandrescu is awesome; around 2000 he gave on talk on lock-free wait-free algorithms that I immediately applied to a huge C++ industrial control networking project at the time.<p>I&#x27;d recommend anyone who writes software listening and reading anything of Andrei&#x27;s you can find; this one is indeed a Treasure!</div><br/></div></div></div></div><div id="41066860" class="c"><input type="checkbox" id="c-41066860" checked=""/><div class="controls bullet"><span class="by">furstenheim</span><span>|</span><a href="#41068928">prev</a><span>|</span><a href="#41067123">next</a><span>|</span><label class="collapse" for="c-41066860">[-]</label><label class="expand" for="c-41066860">[1 more]</label></div><br/><div class="children"><div class="content">Floyd Ryvest also does the job . A bit more efficient IIRC.<p>However I never managed to understand how it works.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Floyd%E2%80%93Rivest_algorithm" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Floyd%E2%80%93Rivest_algorit...</a></div><br/></div></div><div id="41067123" class="c"><input type="checkbox" id="c-41067123" checked=""/><div class="controls bullet"><span class="by">mgaunard</span><span>|</span><a href="#41066860">prev</a><span>|</span><a href="#41076051">next</a><span>|</span><label class="collapse" for="c-41067123">[-]</label><label class="expand" for="c-41067123">[18 more]</label></div><br/><div class="children"><div class="content">You could also use one of the streaming algorithms which allow you to compute approximations for arbitrary quantiles without ever needing to store the whole data in memory.</div><br/><div id="41067292" class="c"><input type="checkbox" id="c-41067292" checked=""/><div class="controls bullet"><span class="by">cosmic_quanta</span><span>|</span><a href="#41067123">parent</a><span>|</span><a href="#41067634">next</a><span>|</span><label class="collapse" for="c-41067292">[-]</label><label class="expand" for="c-41067292">[6 more]</label></div><br/><div class="children"><div class="content">That is cool if you can tolerate approximations. But the uncomfortable questions soon arise: 
Can I tolerate an approximate calculation? 
What assumptions about my data do I to determine an error bound?
How to verify the validity of my assumptions on an ongoing basis?<p>Personally I would gravitate towards the quickselect algorithm described in the OP until I was forced to consider a streaming median approximation method.</div><br/><div id="41069778" class="c"><input type="checkbox" id="c-41069778" checked=""/><div class="controls bullet"><span class="by">skrtskrt</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41067292">parent</a><span>|</span><a href="#41067439">next</a><span>|</span><label class="collapse" for="c-41069778">[-]</label><label class="expand" for="c-41069778">[1 more]</label></div><br/><div class="children"><div class="content">A use case for something like this, and not just for medians, is where you have a querying&#x2F;investigation UI like Grafana or Datadog or whatever.<p>You write the query and the UI knows you&#x27;re querying metric xyz_inflight_requests, it runs a preflight check to get the cardinality of that metric, and gives you a prompt: &quot;xyz_inflight_requests is a high-cardinality metric, this query may take some time - consider using estimated_median instead of median&quot;.</div><br/></div></div><div id="41067439" class="c"><input type="checkbox" id="c-41067439" checked=""/><div class="controls bullet"><span class="by">conradludgate</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41067292">parent</a><span>|</span><a href="#41069778">prev</a><span>|</span><a href="#41072184">next</a><span>|</span><label class="collapse" for="c-41067439">[-]</label><label class="expand" for="c-41067439">[3 more]</label></div><br/><div class="children"><div class="content">Well, I believe you could use the streaming algorithms to pick the likely median, so help choose the pivot for the real quickselect. quickselect can be done inplace too which is O(1) memory if you can afford to rearrange the data.</div><br/><div id="41069069" class="c"><input type="checkbox" id="c-41069069" checked=""/><div class="controls bullet"><span class="by">SkiFire13</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41067439">parent</a><span>|</span><a href="#41072215">next</a><span>|</span><label class="collapse" for="c-41069069">[-]</label><label class="expand" for="c-41069069">[1 more]</label></div><br/><div class="children"><div class="content">Then you don&#x27;t get a guaranteed O(n) complexity if the approximated algorithm happen to make bad choices</div><br/></div></div><div id="41072215" class="c"><input type="checkbox" id="c-41072215" checked=""/><div class="controls bullet"><span class="by">mgaunard</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41067439">parent</a><span>|</span><a href="#41069069">prev</a><span>|</span><a href="#41072184">next</a><span>|</span><label class="collapse" for="c-41072215">[-]</label><label class="expand" for="c-41072215">[1 more]</label></div><br/><div class="children"><div class="content">say you want the median value since the beginning of the day for a time series that has 1000 entries per second, that&#x27;s potentially hundreds of gigabytes in RAM, or just a few variables if you use a p-square estimator.</div><br/></div></div></div></div><div id="41072184" class="c"><input type="checkbox" id="c-41072184" checked=""/><div class="controls bullet"><span class="by">mgaunard</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41067292">parent</a><span>|</span><a href="#41067439">prev</a><span>|</span><a href="#41067634">next</a><span>|</span><label class="collapse" for="c-41072184">[-]</label><label class="expand" for="c-41072184">[1 more]</label></div><br/><div class="children"><div class="content">any approximation gives a bound on the error.</div><br/></div></div></div></div><div id="41067634" class="c"><input type="checkbox" id="c-41067634" checked=""/><div class="controls bullet"><span class="by">sevensor</span><span>|</span><a href="#41067123">parent</a><span>|</span><a href="#41067292">prev</a><span>|</span><a href="#41076051">next</a><span>|</span><label class="collapse" for="c-41067634">[-]</label><label class="expand" for="c-41067634">[11 more]</label></div><br/><div class="children"><div class="content">Iâve definitely had situations where a streaming quantile algorithm would have been useful, do you have any references?</div><br/><div id="41068365" class="c"><input type="checkbox" id="c-41068365" checked=""/><div class="controls bullet"><span class="by">fanf2</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41067634">parent</a><span>|</span><a href="#41070776">next</a><span>|</span><label class="collapse" for="c-41068365">[-]</label><label class="expand" for="c-41068365">[6 more]</label></div><br/><div class="children"><div class="content">There are two kinds:<p>- quantile sketches, such as t-digest, which aim to control the quantile error or rank error. Apache DataSketches has several examples, <a href="https:&#x2F;&#x2F;datasketches.apache.org&#x2F;docs&#x2F;Quantiles&#x2F;QuantilesOverview.html" rel="nofollow">https:&#x2F;&#x2F;datasketches.apache.org&#x2F;docs&#x2F;Quantiles&#x2F;QuantilesOver...</a><p>- histograms, such as my hg64, or hdr histograms, or ddsketch. These control the value error, and are generally easier to understand and faster than quantile sketches. <a href="https:&#x2F;&#x2F;dotat.at&#x2F;@&#x2F;2022-10-12-histogram.html" rel="nofollow">https:&#x2F;&#x2F;dotat.at&#x2F;@&#x2F;2022-10-12-histogram.html</a></div><br/><div id="41075301" class="c"><input type="checkbox" id="c-41075301" checked=""/><div class="controls bullet"><span class="by">ssfrr</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41068365">parent</a><span>|</span><a href="#41070619">next</a><span>|</span><label class="collapse" for="c-41075301">[-]</label><label class="expand" for="c-41075301">[1 more]</label></div><br/><div class="children"><div class="content">Do these both assume the quantile is stationary, or are they also applicable in tracking a rolling quantile (aka quantile filtering)? Below I gave an algorithm Iâve used for quantile filtering, but thatâs a somewhat different problem than streaming single-pass estimation of a stationary quantile.</div><br/></div></div><div id="41070619" class="c"><input type="checkbox" id="c-41070619" checked=""/><div class="controls bullet"><span class="by">throwaway_2494</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41068365">parent</a><span>|</span><a href="#41075301">prev</a><span>|</span><a href="#41075251">next</a><span>|</span><label class="collapse" for="c-41070619">[-]</label><label class="expand" for="c-41070619">[3 more]</label></div><br/><div class="children"><div class="content">See also the Greenwald Khanna quantile estimator, an online algorithm which can compute any quantile within a given Ïµ.<p><a href="https:&#x2F;&#x2F;aakinshin.net&#x2F;posts&#x2F;greenwald-khanna-quantile-estimator&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aakinshin.net&#x2F;posts&#x2F;greenwald-khanna-quantile-estima...</a></div><br/><div id="41075271" class="c"><input type="checkbox" id="c-41075271" checked=""/><div class="controls bullet"><span class="by">sevensor</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41070619">parent</a><span>|</span><a href="#41075251">next</a><span>|</span><label class="collapse" for="c-41075271">[-]</label><label class="expand" for="c-41075271">[2 more]</label></div><br/><div class="children"><div class="content">I am so glad I asked. This is a wheel Iâve been reinventing in my head for eighteen years now. Iâve even asked in other venues, why are there no online median algorithms? Nobody knew of even one. Turns out, I was asking the wrong people!</div><br/><div id="41075642" class="c"><input type="checkbox" id="c-41075642" checked=""/><div class="controls bullet"><span class="by">throwaway_2494</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41075271">parent</a><span>|</span><a href="#41075251">next</a><span>|</span><label class="collapse" for="c-41075642">[-]</label><label class="expand" for="c-41075642">[1 more]</label></div><br/><div class="children"><div class="content">Glad it helped.<p>I&#x27;ve used online quantile estimators (GK in particular) to very effectively look for anomalies in streaming data.<p>It worked much better than the usual mean&#x2F;stddev threshold approach (which was embedded in competing producsts), because it made no assumptions about the distribution of the data.<p>One thing to note is that GK is online, but not windowed, so it looks back to the very first value.<p>However this can be overcome by using multiple, possibly overlapping, summaries, to allow old values to eventually drop off.</div><br/></div></div></div></div></div></div><div id="41075251" class="c"><input type="checkbox" id="c-41075251" checked=""/><div class="controls bullet"><span class="by">sevensor</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41068365">parent</a><span>|</span><a href="#41070619">prev</a><span>|</span><a href="#41070776">next</a><span>|</span><label class="collapse" for="c-41075251">[-]</label><label class="expand" for="c-41075251">[1 more]</label></div><br/><div class="children"><div class="content">Awesome, you did one! Iâll give it a read.</div><br/></div></div></div></div><div id="41070776" class="c"><input type="checkbox" id="c-41070776" checked=""/><div class="controls bullet"><span class="by">ssfrr</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41067634">parent</a><span>|</span><a href="#41068365">prev</a><span>|</span><a href="#41067661">next</a><span>|</span><label class="collapse" for="c-41070776">[-]</label><label class="expand" for="c-41070776">[2 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s a simple one I&#x27;ve used before. It&#x27;s a variation on FAME (Fast Algorithm for Median Estimation) [1].<p>You keep an estimate for the current quantile value, and then for each element in your stream, you either increment (if the element is greater than your estimate) or decrement (if the element is less than your estimate) by fixed &quot;up -step&quot; and &quot;down-step&quot; amounts. If your increment and decrement steps are equal, you should converge to the median. If you shift the ratio of increment and decrement steps you can estimate any quantile.<p>For example, say that your increment step is 0.05 and your decrement step is 0.95. When your estimate converges to a steady state, then you must be hitting greater values 95% of the time and lesser values 5% of the time, hence you&#x27;ve estimated the 95th percentile.<p>The tricky bit is choosing the overall scale of your steps. If your steps are very small relative to the scale of your values, it will converge very slowly and not track changes in the stream. You don&#x27;t want your steps to be too large because they determine the precision. The FAME algorithm has a step where you shrink your step size when your data value is near your estimate (causing the step size to auto-scale down).<p>[1]: <a href="http:&#x2F;&#x2F;www.eng.tau.ac.il&#x2F;~shavitt&#x2F;courses&#x2F;LargeG&#x2F;streaming-median.pdf" rel="nofollow">http:&#x2F;&#x2F;www.eng.tau.ac.il&#x2F;~shavitt&#x2F;courses&#x2F;LargeG&#x2F;streaming-m...</a><p>[2]: <a href="https:&#x2F;&#x2F;stats.stackexchange.com&#x2F;a&#x2F;445714" rel="nofollow">https:&#x2F;&#x2F;stats.stackexchange.com&#x2F;a&#x2F;445714</a></div><br/><div id="41075366" class="c"><input type="checkbox" id="c-41075366" checked=""/><div class="controls bullet"><span class="by">sevensor</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41070776">parent</a><span>|</span><a href="#41067661">next</a><span>|</span><label class="collapse" for="c-41075366">[-]</label><label class="expand" for="c-41075366">[1 more]</label></div><br/><div class="children"><div class="content">I like how straightforward this one is! Itâs fast, itâs obvious, and itâs good enough, if you know something about the data ahead of time. I would have reached for this about four years ago, if Iâd known about it.</div><br/></div></div></div></div><div id="41067661" class="c"><input type="checkbox" id="c-41067661" checked=""/><div class="controls bullet"><span class="by">fzy95</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41067634">parent</a><span>|</span><a href="#41070776">prev</a><span>|</span><a href="#41076051">next</a><span>|</span><label class="collapse" for="c-41067661">[-]</label><label class="expand" for="c-41067661">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Further analysis of the remedian algorithm&quot;
<a href="https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S0304397513004519" rel="nofollow">https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S030439751...</a><p>This one has a streaming variant.</div><br/><div id="41068250" class="c"><input type="checkbox" id="c-41068250" checked=""/><div class="controls bullet"><span class="by">sevensor</span><span>|</span><a href="#41067123">root</a><span>|</span><a href="#41067661">parent</a><span>|</span><a href="#41076051">next</a><span>|</span><label class="collapse" for="c-41068250">[-]</label><label class="expand" for="c-41068250">[1 more]</label></div><br/><div class="children"><div class="content">Thanks!!</div><br/></div></div></div></div></div></div></div></div><div id="41076051" class="c"><input type="checkbox" id="c-41076051" checked=""/><div class="controls bullet"><span class="by">teo_zero</span><span>|</span><a href="#41067123">prev</a><span>|</span><a href="#41070083">next</a><span>|</span><label class="collapse" for="c-41076051">[-]</label><label class="expand" for="c-41076051">[2 more]</label></div><br/><div class="children"><div class="content">&gt; On average, the pivot will split the list into 2 approximately equal-sized pieces.<p>Where does this come from?<p>Even assuming a perfect random function, this would be true only for distributions that show some symmetry. But if the input is all 10s and one 5, each step will generate quite different-sized pieces!</div><br/><div id="41076391" class="c"><input type="checkbox" id="c-41076391" checked=""/><div class="controls bullet"><span class="by">paldepind2</span><span>|</span><a href="#41076051">parent</a><span>|</span><a href="#41070083">next</a><span>|</span><label class="collapse" for="c-41076391">[-]</label><label class="expand" for="c-41076391">[1 more]</label></div><br/><div class="children"><div class="content">I think you answered your own question. It&#x27;s the standard average-time analysis of Quicksort and the (unmentioned) assumption is that the numbers are from some uniform distribution.<p>Why would the distribution have to be symmetric? My intuition is that if you sample n numbers from some distribution (even if it&#x27;s skewed) and pick a random number among the n numbers, then on average that number would be separate the number into two equal-sized sets. Are you saying that is wrong?</div><br/></div></div></div></div><div id="41070083" class="c"><input type="checkbox" id="c-41070083" checked=""/><div class="controls bullet"><span class="by">jagged-chisel</span><span>|</span><a href="#41076051">prev</a><span>|</span><a href="#41070117">next</a><span>|</span><label class="collapse" for="c-41070083">[-]</label><label class="expand" for="c-41070083">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s quicksort with a modification to select the median during the process. I feel like this is a good way to approach lots of &quot;find $THING in list&quot; questions.</div><br/></div></div><div id="41070117" class="c"><input type="checkbox" id="c-41070117" checked=""/><div class="controls bullet"><span class="by">ncruces</span><span>|</span><a href="#41070083">prev</a><span>|</span><a href="#41073840">next</a><span>|</span><label class="collapse" for="c-41070117">[-]</label><label class="expand" for="c-41070117">[1 more]</label></div><br/><div class="children"><div class="content">An implementation in Go, that&#x27;s (hopefully) simple enough to be understandable, yet minimally practical:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ncruces&#x2F;sort&#x2F;blob&#x2F;main&#x2F;quick&#x2F;quick.go">https:&#x2F;&#x2F;github.com&#x2F;ncruces&#x2F;sort&#x2F;blob&#x2F;main&#x2F;quick&#x2F;quick.go</a></div><br/></div></div><div id="41073840" class="c"><input type="checkbox" id="c-41073840" checked=""/><div class="controls bullet"><span class="by">TacticalCoder</span><span>|</span><a href="#41070117">prev</a><span>|</span><a href="#41067245">next</a><span>|</span><label class="collapse" for="c-41073840">[-]</label><label class="expand" for="c-41073840">[3 more]</label></div><br/><div class="children"><div class="content">I really enjoyed TFA but this:<p>&gt; Technically, you could get extremely unlucky: at each step, you could pick the largest element as your pivot. Each step would only remove one element from the list and youâd actually have O(n2) performance instead of O(n)<p>If adversarial input is a concern, doing a O(n) shuffle of the data first guarantees this cannot happen. If the data is really too big to shuffle, then only shuffle once a bucket is small enough to be shuffled.<p>If you do shuffle, probabilities are here to guarantee that that worst case cannot happen. If anyone says that &quot;technically&quot; it can happen, I&#x27;ll answer that then &quot;technically&quot; an attacker could also guess correctly every bit of your 256 bits private key.<p>Our world is build on probabilities: all our private keys are protected by the mathematical improbability that someone shall guess them correctly.<p>From what I read, a shuffle followed by quickselect is O(n) for all practical purposes.</div><br/><div id="41076305" class="c"><input type="checkbox" id="c-41076305" checked=""/><div class="controls bullet"><span class="by">Reubend</span><span>|</span><a href="#41073840">parent</a><span>|</span><a href="#41073897">next</a><span>|</span><label class="collapse" for="c-41076305">[-]</label><label class="expand" for="c-41076305">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If adversarial input is a concern, doing a O(n) shuffle of the data first guarantees this cannot happen.<p>It doesn&#x27;t guarantee that you avoid the worst case, it just removes the possibility of forcing the worst case.</div><br/></div></div><div id="41073897" class="c"><input type="checkbox" id="c-41073897" checked=""/><div class="controls bullet"><span class="by">bo1024</span><span>|</span><a href="#41073840">parent</a><span>|</span><a href="#41076305">prev</a><span>|</span><a href="#41067245">next</a><span>|</span><label class="collapse" for="c-41073897">[-]</label><label class="expand" for="c-41073897">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re already using your own randomness to pick the pivot at random, so I don&#x27;t see why the shuffle helps more. But yes, if your randomness is trustworthy, the probability of more than O(n) runtime is very low.</div><br/></div></div></div></div><div id="41067245" class="c"><input type="checkbox" id="c-41067245" checked=""/><div class="controls bullet"><span class="by">Xcelerate</span><span>|</span><a href="#41073840">prev</a><span>|</span><a href="#41066882">next</a><span>|</span><label class="collapse" for="c-41067245">[-]</label><label class="expand" for="c-41067245">[19 more]</label></div><br/><div class="children"><div class="content">I received a variant of this problem as an interview question a few months ago. Except the linear time approach would not have worked here, since the list contains trillions of numbers, you only have sequential read access, and the list cannot be loaded into memory. 30 minutes â go.<p>First I asked if anything could be assumed about the statistics on the distribution of the numbers. Nope, could be anything, except the numbers are 32-bit ints. After fiddling around for a bit I finally decided on a scheme that creates a bounding interval for the unknown median value (one variable contains the upper bound and one contains the lower bound based on 2^32 possible values) and then adjusts this interval on each successive pass through the data. The last step is to average the upper and lower bound in case there are an odd number of integers. Worst case, this approach requires O(log n) passes through the data, so even for trillions of numbers itâs fairly quick.<p>I wrapped up the solution right at the time limit, and my code ran fine on the test cases. Was decently proud of myself for getting a solution in the allotted time.<p>Well, the interview feedback arrived, and it turns out my solution was rejected for being suboptimal. Apparently there is a more efficient approach that utilizes priority heaps. After looking up and reading about the priority heap approach, all I can say is that I didnât realize the interview task was to re-implement someoneâs PhD thesis in 30 minutes...<p>I had never used leetcode before because I never had difficulty with prior coding interviews (my last job search was many years before the 2022 layoffs), but after this interview, I immediately signed up for a subscription. And of course the âmedian file integerâ question I received is one of the most asked questions on the list of âhardâ problems.</div><br/><div id="41067470" class="c"><input type="checkbox" id="c-41067470" checked=""/><div class="controls bullet"><span class="by">pfdietz</span><span>|</span><a href="#41067245">parent</a><span>|</span><a href="#41075547">prev</a><span>|</span><a href="#41067259">next</a><span>|</span><label class="collapse" for="c-41067470">[-]</label><label class="expand" for="c-41067470">[8 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need to load trillions of numbers into memory, you just need to count how many of each number there are.  This requires 2^32 words of memory, not trillions of words.  After doing that just scan down the array of counts, summing, until you find the midpoint.</div><br/><div id="41067585" class="c"><input type="checkbox" id="c-41067585" checked=""/><div class="controls bullet"><span class="by">Xcelerate</span><span>|</span><a href="#41067245">root</a><span>|</span><a href="#41067470">parent</a><span>|</span><a href="#41067259">next</a><span>|</span><label class="collapse" for="c-41067585">[-]</label><label class="expand" for="c-41067585">[7 more]</label></div><br/><div class="children"><div class="content">Yeah, I thought of that actually, but the interviewer said âvery little memoryâ at one point which gave me the impression that perhaps I only had some registers available to work with. Was this an algorithm for an embedded system?<p>The whole problem was kind of miscommunicated, because the interviewer showed up 10 minutes late, picked a problem from a list, and the requirements for the problem were only revealed when I started going a direction the interviewer wasnât looking for (âOh, the file is actually read-only.â âOh, each number in the file is an integer, not a float.â)</div><br/><div id="41067731" class="c"><input type="checkbox" id="c-41067731" checked=""/><div class="controls bullet"><span class="by">jagged-chisel</span><span>|</span><a href="#41067245">root</a><span>|</span><a href="#41067585">parent</a><span>|</span><a href="#41068364">next</a><span>|</span><label class="collapse" for="c-41067731">[-]</label><label class="expand" for="c-41067731">[4 more]</label></div><br/><div class="children"><div class="content">That âmiscommunicationâ you mention has been used against me in several interviews, because I was expected to ask questions (and sometimes a specific question they had in mind) before making assumptions. Well, then the 30min becomes an exercise in requirements gathering and not algorithmic implementation.</div><br/><div id="41070666" class="c"><input type="checkbox" id="c-41070666" checked=""/><div class="controls bullet"><span class="by">NoToP</span><span>|</span><a href="#41067245">root</a><span>|</span><a href="#41067731">parent</a><span>|</span><a href="#41068364">next</a><span>|</span><label class="collapse" for="c-41070666">[-]</label><label class="expand" for="c-41070666">[3 more]</label></div><br/><div class="children"><div class="content">Which in fairness, is a reasonable competency to test for in an interview</div><br/><div id="41071752" class="c"><input type="checkbox" id="c-41071752" checked=""/><div class="controls bullet"><span class="by">jagged-chisel</span><span>|</span><a href="#41067245">root</a><span>|</span><a href="#41070666">parent</a><span>|</span><a href="#41068364">next</a><span>|</span><label class="collapse" for="c-41071752">[-]</label><label class="expand" for="c-41071752">[2 more]</label></div><br/><div class="children"><div class="content">Indeed. But I need clarity on which skill they want to test in thirty minutes.</div><br/><div id="41073803" class="c"><input type="checkbox" id="c-41073803" checked=""/><div class="controls bullet"><span class="by">klyrs</span><span>|</span><a href="#41067245">root</a><span>|</span><a href="#41071752">parent</a><span>|</span><a href="#41068364">next</a><span>|</span><label class="collapse" for="c-41073803">[-]</label><label class="expand" for="c-41073803">[1 more]</label></div><br/><div class="children"><div class="content">Speaking as an interviewer: nope, you&#x27;re not being tested on a single skill.</div><br/></div></div></div></div></div></div></div></div><div id="41068364" class="c"><input type="checkbox" id="c-41068364" checked=""/><div class="controls bullet"><span class="by">creata</span><span>|</span><a href="#41067245">root</a><span>|</span><a href="#41067585">parent</a><span>|</span><a href="#41067731">prev</a><span>|</span><a href="#41067259">next</a><span>|</span><label class="collapse" for="c-41068364">[-]</label><label class="expand" for="c-41068364">[2 more]</label></div><br/><div class="children"><div class="content">With 256 counters, you could use the same approach with four passes: pass i bins the numbers by byte i (0 = most sig., 3 = least sig.) and then identifies the bin that contains the median.<p>I really want to know what a one-pass, low-memory solution looks like, lol.</div><br/><div id="41070081" class="c"><input type="checkbox" id="c-41070081" checked=""/><div class="controls bullet"><span class="by">pfdietz</span><span>|</span><a href="#41067245">root</a><span>|</span><a href="#41068364">parent</a><span>|</span><a href="#41067259">next</a><span>|</span><label class="collapse" for="c-41070081">[-]</label><label class="expand" for="c-41070081">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps the interviewer was looking for an argument that no such solution can exist?  The counterargument would look like this:  divide the N numbers into two halves, each N&#x2F;2 numbers.  Now, suppose you don&#x27;t have enough memory to represent the first N&#x2F;2 numbers (ignoring changes in ordering); in that case, two different bags of numbers will have the same representation in memory.  One can now construct a second half of numbers for which the algorithm will get the wrong answer for at least one of the two colliding cases.<p>This is assuming a deterministic algorithm; maybe a random algorithm could work with high probability and less memory?</div><br/></div></div></div></div></div></div></div></div><div id="41067259" class="c"><input type="checkbox" id="c-41067259" checked=""/><div class="controls bullet"><span class="by">anonymoushn</span><span>|</span><a href="#41067245">parent</a><span>|</span><a href="#41067470">prev</a><span>|</span><a href="#41067399">next</a><span>|</span><label class="collapse" for="c-41067259">[-]</label><label class="expand" for="c-41067259">[5 more]</label></div><br/><div class="children"><div class="content">Do you mean topK rather than median, for K small? You certainly cannot build a heap with trillions of items in it.</div><br/><div id="41067368" class="c"><input type="checkbox" id="c-41067368" checked=""/><div class="controls bullet"><span class="by">Xcelerate</span><span>|</span><a href="#41067245">root</a><span>|</span><a href="#41067259">parent</a><span>|</span><a href="#41067399">next</a><span>|</span><label class="collapse" for="c-41067368">[-]</label><label class="expand" for="c-41067368">[4 more]</label></div><br/><div class="children"><div class="content">No, I mean median. Here is an article describing a very similar problem since I canât link to the leetcode version: <a href="https:&#x2F;&#x2F;www.geeksforgeeks.org&#x2F;median-of-stream-of-running-integers-using-stl&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.geeksforgeeks.org&#x2F;median-of-stream-of-running-in...</a></div><br/><div id="41069030" class="c"><input type="checkbox" id="c-41069030" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#41067245">root</a><span>|</span><a href="#41067368">parent</a><span>|</span><a href="#41069934">next</a><span>|</span><label class="collapse" for="c-41069030">[-]</label><label class="expand" for="c-41069030">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Auxiliary Space : O(n).<p>&gt; The Space required to store the elements in Heap is O(n).<p>I don&#x27;t think this algorithm is suitable for trillions of items.</div><br/></div></div><div id="41069934" class="c"><input type="checkbox" id="c-41069934" checked=""/><div class="controls bullet"><span class="by">osti</span><span>|</span><a href="#41067245">root</a><span>|</span><a href="#41067368">parent</a><span>|</span><a href="#41069030">prev</a><span>|</span><a href="#41067809">next</a><span>|</span><label class="collapse" for="c-41069934">[-]</label><label class="expand" for="c-41069934">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m wondering what heap approach can solve that problem, as I can&#x27;t think of any. Hopefully OP got a link to the thesis.<p>The n log n approach definitely works though.</div><br/></div></div><div id="41067809" class="c"><input type="checkbox" id="c-41067809" checked=""/><div class="controls bullet"><span class="by">Tarean</span><span>|</span><a href="#41067245">root</a><span>|</span><a href="#41067368">parent</a><span>|</span><a href="#41069934">prev</a><span>|</span><a href="#41067399">next</a><span>|</span><label class="collapse" for="c-41067809">[-]</label><label class="expand" for="c-41067809">[1 more]</label></div><br/><div class="children"><div class="content">But that stores all elements into memory?</div><br/></div></div></div></div></div></div><div id="41067399" class="c"><input type="checkbox" id="c-41067399" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#41067245">parent</a><span>|</span><a href="#41067259">prev</a><span>|</span><a href="#41067712">next</a><span>|</span><label class="collapse" for="c-41067399">[-]</label><label class="expand" for="c-41067399">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve heard of senior people applying for jobs like this simply turning the interview question around and demanding that the person asking it solve it in the allotted time. A surprisingly high percentage of the time they can&#x27;t.</div><br/><div id="41067494" class="c"><input type="checkbox" id="c-41067494" checked=""/><div class="controls bullet"><span class="by">Xcelerate</span><span>|</span><a href="#41067245">root</a><span>|</span><a href="#41067399">parent</a><span>|</span><a href="#41067712">next</a><span>|</span><label class="collapse" for="c-41067494">[-]</label><label class="expand" for="c-41067494">[2 more]</label></div><br/><div class="children"><div class="content">This company receives so many candidates that the interviewer would have just ended the call and moved on to the next candidate.<p>I get the notion of making the point out of principle, but itâs sort of like arguing on the phone with someone at a call centerâitâs better to just cut your losses quickly and move on to the next option in the current market.</div><br/><div id="41068470" class="c"><input type="checkbox" id="c-41068470" checked=""/><div class="controls bullet"><span class="by">jagged-chisel</span><span>|</span><a href="#41067245">root</a><span>|</span><a href="#41067494">parent</a><span>|</span><a href="#41067712">next</a><span>|</span><label class="collapse" for="c-41068470">[-]</label><label class="expand" for="c-41068470">[1 more]</label></div><br/><div class="children"><div class="content">And we, as software engineers, should also take that advice: it&#x27;s better to just cut your losses quickly and move on to the next option in the current market.</div><br/></div></div></div></div></div></div><div id="41067712" class="c"><input type="checkbox" id="c-41067712" checked=""/><div class="controls bullet"><span class="by">jagged-chisel</span><span>|</span><a href="#41067245">parent</a><span>|</span><a href="#41067399">prev</a><span>|</span><a href="#41066882">next</a><span>|</span><label class="collapse" for="c-41067712">[-]</label><label class="expand" for="c-41067712">[1 more]</label></div><br/><div class="children"><div class="content">&gt; â¦ I didnât realize the interview task was to re-implement someoneâs PhD thesis in 30 minutes...<p>What a bullshit task. Iâm beginning to think this kind of interviewing should be banned. Seems to me itâs just an easy escape hatch for the interviewer&#x2F;hiring manager when they want to discriminate based on prejudice.</div><br/></div></div></div></div><div id="41066882" class="c"><input type="checkbox" id="c-41066882" checked=""/><div class="controls bullet"><span class="by">anonymoushn</span><span>|</span><a href="#41067245">prev</a><span>|</span><a href="#41068110">next</a><span>|</span><label class="collapse" for="c-41066882">[-]</label><label class="expand" for="c-41066882">[16 more]</label></div><br/><div class="children"><div class="content">One commonly sees the implication that radix sort cannot be used for data types other than integers, or for composite data types, or for large data types. For example, TFA says you could use radix sort if your input is 32-bit integers. But you can use it on anything. You can use radix sort to sort strings in O(n) time.</div><br/><div id="41066934" class="c"><input type="checkbox" id="c-41066934" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#41066882">parent</a><span>|</span><a href="#41066958">next</a><span>|</span><label class="collapse" for="c-41066934">[-]</label><label class="expand" for="c-41066934">[4 more]</label></div><br/><div class="children"><div class="content">It should also be noted that radix sort is ridiculously fast because it just scans linearly through the list each time.<p>It&#x27;s actually hard to come up with something that cannot be sorted lexicographically. The best example I was able to find was big fractions. Though even then you could write them as continued fractions and sort those lexicographically (would be a bit trickier than strings).</div><br/><div id="41068075" class="c"><input type="checkbox" id="c-41068075" checked=""/><div class="controls bullet"><span class="by">anonymoushn</span><span>|</span><a href="#41066882">root</a><span>|</span><a href="#41066934">parent</a><span>|</span><a href="#41066958">next</a><span>|</span><label class="collapse" for="c-41068075">[-]</label><label class="expand" for="c-41068075">[3 more]</label></div><br/><div class="children"><div class="content">Sorting fractions by numerical value is a good example. Previously I&#x27;ve heard that there are some standard collation schemes for some human languages that resist radix sort, but when I asked about which ones in specific I didn&#x27;t hear back :(</div><br/><div id="41069228" class="c"><input type="checkbox" id="c-41069228" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#41066882">root</a><span>|</span><a href="#41068075">parent</a><span>|</span><a href="#41068382">next</a><span>|</span><label class="collapse" for="c-41069228">[-]</label><label class="expand" for="c-41069228">[1 more]</label></div><br/><div class="children"><div class="content">The Unicode Collation algorithm doesn&#x27;t look fun to implement in radix sort, but not <i>entirely</i> impossible either. They do note that some characters are contextual, an example they give is that CH can be treated as a single character that sorts <i>after</i> C (so also after CZ). Technically that is still lexicographical, but not byte-for-byte.</div><br/></div></div></div></div></div></div><div id="41066958" class="c"><input type="checkbox" id="c-41066958" checked=""/><div class="controls bullet"><span class="by">exDM69</span><span>|</span><a href="#41066882">parent</a><span>|</span><a href="#41066934">prev</a><span>|</span><a href="#41068110">next</a><span>|</span><label class="collapse" for="c-41066958">[-]</label><label class="expand" for="c-41066958">[11 more]</label></div><br/><div class="children"><div class="content">Problem with radix sorting strings is that it is O(k*N) where k is length of key, in this case it&#x27;s the second longest string&#x27;s length. Additional problems arise if you are dealing with null terminated strings and do not have the length stored.<p>Radix sort is awesome if k is small, N is huge and&#x2F;or you are using a GPU. On a CPU, comparison based sorting is faster in most cases.</div><br/><div id="41067125" class="c"><input type="checkbox" id="c-41067125" checked=""/><div class="controls bullet"><span class="by">anonymoushn</span><span>|</span><a href="#41066882">root</a><span>|</span><a href="#41066958">parent</a><span>|</span><a href="#41068110">next</a><span>|</span><label class="collapse" for="c-41067125">[-]</label><label class="expand" for="c-41067125">[10 more]</label></div><br/><div class="children"><div class="content">No, it&#x27;s O(N+M) where N is the number of strings and M is the sum of the lengths of the strings. Maybe your radix sort has some problems?<p>I evaluated various sorts for strings as part of my winning submission to <a href="https:&#x2F;&#x2F;easyperf.net&#x2F;blog&#x2F;2022&#x2F;05&#x2F;28&#x2F;Performance-analysis-and-tuning-contest-6" rel="nofollow">https:&#x2F;&#x2F;easyperf.net&#x2F;blog&#x2F;2022&#x2F;05&#x2F;28&#x2F;Performance-analysis-an...</a> and found <a href="https:&#x2F;&#x2F;github.com&#x2F;bingmann&#x2F;parallel-string-sorting">https:&#x2F;&#x2F;github.com&#x2F;bingmann&#x2F;parallel-string-sorting</a> to be helpful. For a single core, the fastest implementation among those in parallel-string-sorting was a radix sort, so my submission included a radix sort based on that one.<p>The only other contender was multi-key quicksort, which is notably not a comparison sort (i.e. a general-purpose string comparison function is not used as a subroutine of multi-key quicksort). In either case, you end up operating on something like an array of structs containing a pointer to the string, an integer offset into the string, and a few cached bytes from the string, and in either case I don&#x27;t really know what problems you expect to have if you&#x27;re dealing with null-terminated strings.<p>A very similar similar radix sort is included in <a href="https:&#x2F;&#x2F;github.com&#x2F;alichraghi&#x2F;zort">https:&#x2F;&#x2F;github.com&#x2F;alichraghi&#x2F;zort</a> which includes some benchmarks, but I haven&#x27;t done the work to have it work on strings or arbitrary structs.</div><br/><div id="41067642" class="c"><input type="checkbox" id="c-41067642" checked=""/><div class="controls bullet"><span class="by">Aardwolf</span><span>|</span><a href="#41066882">root</a><span>|</span><a href="#41067125">parent</a><span>|</span><a href="#41068430">next</a><span>|</span><label class="collapse" for="c-41067642">[-]</label><label class="expand" for="c-41067642">[4 more]</label></div><br/><div class="children"><div class="content">&gt; No, it&#x27;s O(N+M) where N is the number of strings and M is the sum of the lengths of the strings.<p>That would mean it&#x27;s possible to sort N random 64-bit integers in O(N+M) which is just O(N) with a constant factor of 9 (if taking the length in bytes) or 65 (if taking the length in bits), so sort billions of random integers in linear time, is that truly right?<p>EDIT: I think it does make sense, M is length*N, and in scenarios where this matters this length will be in the order of log(N) anyway so it&#x27;s still NlogN-sh.</div><br/><div id="41067824" class="c"><input type="checkbox" id="c-41067824" checked=""/><div class="controls bullet"><span class="by">anonymoushn</span><span>|</span><a href="#41066882">root</a><span>|</span><a href="#41067642">parent</a><span>|</span><a href="#41068256">next</a><span>|</span><label class="collapse" for="c-41067824">[-]</label><label class="expand" for="c-41067824">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That would mean it&#x27;s possible to sort N random 64-bit integers in O(N+M) which is just O(N) with a constant factor of 9 (if taking the length in bytes) or 65 (if taking the length in bits), so sort billions of random integers in linear time, is that truly right?<p>Yes. You can sort just about anything in linear time.<p>&gt; EDIT: I think it does make sense, M is length*N, and in scenarios where this matters this length will be in the order of log(N) anyway so it&#x27;s still NlogN-sh.<p>I mainly wrote N+M rather than M to be pedantically correct for degenerate inputs that consist of mostly empty strings. Regardless, if you consider the size of the whole input, it&#x27;s linear in that.</div><br/></div></div><div id="41068256" class="c"><input type="checkbox" id="c-41068256" checked=""/><div class="controls bullet"><span class="by">exDM69</span><span>|</span><a href="#41066882">root</a><span>|</span><a href="#41067642">parent</a><span>|</span><a href="#41067824">prev</a><span>|</span><a href="#41068430">next</a><span>|</span><label class="collapse" for="c-41068256">[-]</label><label class="expand" for="c-41068256">[2 more]</label></div><br/><div class="children"><div class="content">Yes, radix sort can sort integers in linear O(N) time.</div><br/></div></div></div></div><div id="41068430" class="c"><input type="checkbox" id="c-41068430" checked=""/><div class="controls bullet"><span class="by">dataflow</span><span>|</span><a href="#41066882">root</a><span>|</span><a href="#41067125">parent</a><span>|</span><a href="#41067642">prev</a><span>|</span><a href="#41068110">next</a><span>|</span><label class="collapse" for="c-41068430">[-]</label><label class="expand" for="c-41068430">[5 more]</label></div><br/><div class="children"><div class="content">If you have a million 1-character strings and one string of length 1 million, how many steps would your LSD radix sort take? And (if it&#x27;s indeed linear in the total input size like you say) how do you make it jump over the empty slots without losing real-world efficiency in other cases?</div><br/><div id="41068495" class="c"><input type="checkbox" id="c-41068495" checked=""/><div class="controls bullet"><span class="by">anonymoushn</span><span>|</span><a href="#41066882">root</a><span>|</span><a href="#41068430">parent</a><span>|</span><a href="#41068110">next</a><span>|</span><label class="collapse" for="c-41068495">[-]</label><label class="expand" for="c-41068495">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s MSB radix sort. I think LSB radix sort is not generally as useful because even for fixed-size inputs it often makes more passes over most of the input than MSB radix sort.<p>Your comment makes me think it would be swell to add a fast path for when the input range compares equal for the current byte or bytes though. In general radix sorts have some computation to spare (on commonly used CPUs, more computation may be performed per element during the histogramming pass without spending any additional time). Some cutting-edge radix sorts spend this spare computation to look for sorted subsequences, etc.</div><br/><div id="41068702" class="c"><input type="checkbox" id="c-41068702" checked=""/><div class="controls bullet"><span class="by">dataflow</span><span>|</span><a href="#41066882">root</a><span>|</span><a href="#41068495">parent</a><span>|</span><a href="#41068110">next</a><span>|</span><label class="collapse" for="c-41068702">[-]</label><label class="expand" for="c-41068702">[3 more]</label></div><br/><div class="children"><div class="content">Oh I see. I&#x27;ve found LSD better in some cases, but maybe it&#x27;s just my implementation. For MSD, do you get a performance hit from putting your stack on the heap (to avoid overflow on recursion)? I don&#x27;t just mean the minor register vs. memory differences in the codegen, but also the cache misses &amp; memory pressure due to potentially long input elements (and thus correspondingly large stack to manage).<p>&gt; Some cutting-edge radix sorts<p>Where do you find these? :-)</div><br/><div id="41068853" class="c"><input type="checkbox" id="c-41068853" checked=""/><div class="controls bullet"><span class="by">anonymoushn</span><span>|</span><a href="#41066882">root</a><span>|</span><a href="#41068702">parent</a><span>|</span><a href="#41068110">next</a><span>|</span><label class="collapse" for="c-41068853">[-]</label><label class="expand" for="c-41068853">[2 more]</label></div><br/><div class="children"><div class="content">&gt; For MSD, do you get a performance hit from putting your stack on the heap (to avoid overflow on recursion)?<p>I&#x27;m not sure. My original C++ implementation which is for non-adversarial inputs puts the array containing histograms and indices on the heap but uses the stack for a handful of locals, so it would explode if you passed the wrong thing. The sort it&#x27;s based on in the parallel-string-sorting repo works the same way. Oops!<p>&gt; cache misses &amp; memory pressure due to potentially long input elements (and thus correspondingly large stack)<p>I think this should be okay? The best of these sorts try to visit the actual strings infrequently. You&#x27;d achieve worst-case cache performance by passing a pretty large number of strings with a long, equal prefix. Ideally these strings would share the bottom ~16 bits of their address, so that they could evict each other from cache when you access them. See <a href="https:&#x2F;&#x2F;danluu.com&#x2F;3c-conflict&#x2F;" rel="nofollow">https:&#x2F;&#x2F;danluu.com&#x2F;3c-conflict&#x2F;</a><p>&gt; Where do you find these? :-)<p>There&#x27;s a list of cool sorts here <a href="https:&#x2F;&#x2F;github.com&#x2F;scandum&#x2F;quadsort?tab=readme-ov-file#variants">https:&#x2F;&#x2F;github.com&#x2F;scandum&#x2F;quadsort?tab=readme-ov-file#varia...</a> and they are often submitted to HN.</div><br/><div id="41069574" class="c"><input type="checkbox" id="c-41069574" checked=""/><div class="controls bullet"><span class="by">dataflow</span><span>|</span><a href="#41066882">root</a><span>|</span><a href="#41068853">parent</a><span>|</span><a href="#41068110">next</a><span>|</span><label class="collapse" for="c-41069574">[-]</label><label class="expand" for="c-41069574">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, what I hate about MSD is the stack explosion.<p>Otherwise - cool, thanks!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="41068110" class="c"><input type="checkbox" id="c-41068110" checked=""/><div class="controls bullet"><span class="by">chpatrick</span><span>|</span><a href="#41066882">prev</a><span>|</span><a href="#41067138">next</a><span>|</span><label class="collapse" for="c-41068110">[-]</label><label class="expand" for="c-41068110">[1 more]</label></div><br/><div class="children"><div class="content">Another nice one is O(1) weighted sampling (after O(n) preprocessing).<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Alias_method" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Alias_method</a></div><br/></div></div><div id="41067138" class="c"><input type="checkbox" id="c-41067138" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#41068110">prev</a><span>|</span><a href="#41067043">next</a><span>|</span><label class="collapse" for="c-41067138">[-]</label><label class="expand" for="c-41067138">[1 more]</label></div><br/><div class="children"><div class="content">If an approximation is enough, the p2 quantile estimator (<i>O(1)</i> memory) is pretty neat: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25201093">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25201093</a></div><br/></div></div><div id="41067043" class="c"><input type="checkbox" id="c-41067043" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41067138">prev</a><span>|</span><a href="#41068749">next</a><span>|</span><label class="collapse" for="c-41067043">[-]</label><label class="expand" for="c-41067043">[2 more]</label></div><br/><div class="children"><div class="content">This is hinted at in the post but if you&#x27;re using C++ you will typically have access to quickselect via std::nth_element. I&#x27;ve replaced many a sort with that in code review :) (Well, not many. But at least a handful.)</div><br/><div id="41067507" class="c"><input type="checkbox" id="c-41067507" checked=""/><div class="controls bullet"><span class="by">conradludgate</span><span>|</span><a href="#41067043">parent</a><span>|</span><a href="#41068749">next</a><span>|</span><label class="collapse" for="c-41067507">[-]</label><label class="expand" for="c-41067507">[1 more]</label></div><br/><div class="children"><div class="content">Same with rust, there&#x27;s the `select_nth_unstable` family on slices that will do this for you. It uses a more fancy pivot choosing algorithm but will fall back to median-of-medians if it detects it&#x27;s taking too long</div><br/></div></div></div></div><div id="41068749" class="c"><input type="checkbox" id="c-41068749" checked=""/><div class="controls bullet"><span class="by">hammeiam</span><span>|</span><a href="#41067043">prev</a><span>|</span><a href="#41070108">next</a><span>|</span><label class="collapse" for="c-41068749">[-]</label><label class="expand" for="c-41068749">[9 more]</label></div><br/><div class="children"><div class="content">The &quot;Split the array into subarrays of length 5, now sorting all of the arrays is O(n) instead of O(n log n)&quot; feels like cheating to me</div><br/><div id="41069003" class="c"><input type="checkbox" id="c-41069003" checked=""/><div class="controls bullet"><span class="by">marcosdumay</span><span>|</span><a href="#41068749">parent</a><span>|</span><a href="#41068793">next</a><span>|</span><label class="collapse" for="c-41069003">[-]</label><label class="expand" for="c-41069003">[1 more]</label></div><br/><div class="children"><div class="content">O(n log 5) is O(n). There&#x27;s no cheating, sorting small arrays in a list is a completely different problem from sorting a large array.</div><br/></div></div><div id="41068793" class="c"><input type="checkbox" id="c-41068793" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41068749">parent</a><span>|</span><a href="#41069003">prev</a><span>|</span><a href="#41070565">next</a><span>|</span><label class="collapse" for="c-41068793">[-]</label><label class="expand" for="c-41068793">[2 more]</label></div><br/><div class="children"><div class="content">They&#x27;re not sorting all the arrays?<p><i>Later</i><p>(i was going to delete this comment, but for posterity, i misread --- sorting <i>the lists</i>, not the contents of the list, sure)</div><br/></div></div><div id="41069740" class="c"><input type="checkbox" id="c-41069740" checked=""/><div class="controls bullet"><span class="by">IncreasePosts</span><span>|</span><a href="#41068749">parent</a><span>|</span><a href="#41070549">prev</a><span>|</span><a href="#41069700">next</a><span>|</span><label class="collapse" for="c-41069740">[-]</label><label class="expand" for="c-41069740">[1 more]</label></div><br/><div class="children"><div class="content">It would only be cheating if you could merge the arrays in O(1), which you can&#x27;t.</div><br/></div></div><div id="41069700" class="c"><input type="checkbox" id="c-41069700" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#41068749">parent</a><span>|</span><a href="#41069740">prev</a><span>|</span><a href="#41069054">next</a><span>|</span><label class="collapse" for="c-41069700">[-]</label><label class="expand" for="c-41069700">[1 more]</label></div><br/><div class="children"><div class="content">Itâs unambiguously O(n), thereâs no lg n anywhere to be seen. It may be O(n) with a bit larger constant factor, but the whole point of big-O analysis is that those donât matter.</div><br/></div></div><div id="41069054" class="c"><input type="checkbox" id="c-41069054" checked=""/><div class="controls bullet"><span class="by">pfortuny</span><span>|</span><a href="#41068749">parent</a><span>|</span><a href="#41069700">prev</a><span>|</span><a href="#41070108">next</a><span>|</span><label class="collapse" for="c-41069054">[-]</label><label class="expand" for="c-41069054">[1 more]</label></div><br/><div class="children"><div class="content">Actually lots of algorithms &quot;feel&quot; like cheating until you understand what you were not looking at (fast matrix multiplication, fast fourier transforms...).</div><br/></div></div></div></div><div id="41070108" class="c"><input type="checkbox" id="c-41070108" checked=""/><div class="controls bullet"><span class="by">throwaway294531</span><span>|</span><a href="#41068749">prev</a><span>|</span><a href="#41068309">next</a><span>|</span><label class="collapse" for="c-41070108">[-]</label><label class="expand" for="c-41070108">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re selecting the n:th element, where n is very small (or large), using median-of-medians may not be the best choice.<p>Instead, you can use a biased pivot as in [1] or something I call &quot;j:th of k:th&quot;. Floyd-Rivest can also speed things up. I have a hobby project that gets 1.2-2.0x throughput when compared to a well implemented quickselect, see: <a href="https:&#x2F;&#x2F;github.com&#x2F;koskinev&#x2F;turboselect">https:&#x2F;&#x2F;github.com&#x2F;koskinev&#x2F;turboselect</a><p>If anyone has pointers to fast generic &amp; in-place selection algorithms, I&#x27;m interested.<p>[1] <a href="https:&#x2F;&#x2F;doi.org&#x2F;10.4230&#x2F;LIPIcs.SEA.2017.24" rel="nofollow">https:&#x2F;&#x2F;doi.org&#x2F;10.4230&#x2F;LIPIcs.SEA.2017.24</a></div><br/></div></div><div id="41068309" class="c"><input type="checkbox" id="c-41068309" checked=""/><div class="controls bullet"><span class="by">Someone</span><span>|</span><a href="#41070108">prev</a><span>|</span><a href="#41069173">next</a><span>|</span><label class="collapse" for="c-41068309">[-]</label><label class="expand" for="c-41068309">[2 more]</label></div><br/><div class="children"><div class="content">FTA:<p><i>âProof of Average O(n)<p>On average, the pivot will split the list into 2 approximately equal-sized pieces. Therefore, each subsequent recursion operates on 1â2 the data of the previous step.â</i><p>That âthereforeâ doesnât follow, so this is more an intuition than a proof. The problem with it is that the medium is more likely to end up in the larger of the two pieces, so you more likely have to recurse on the larger part than on the smaller part.<p>What saves you is that <i>O(n)</i> doesnât say anything about constants.<p>Also, I would think you can improve things a bit for real world data by, on subsequent iterations, using the average of the set as pivot (You can compute that for both pieces on the fly while doing the splitting. The average may not be in the set of items, but that doesnât matter for this algorithm). Is that true?</div><br/><div id="41072112" class="c"><input type="checkbox" id="c-41072112" checked=""/><div class="controls bullet"><span class="by">meatmanek</span><span>|</span><a href="#41068309">parent</a><span>|</span><a href="#41069173">next</a><span>|</span><label class="collapse" for="c-41072112">[-]</label><label class="expand" for="c-41072112">[1 more]</label></div><br/><div class="children"><div class="content">If I&#x27;m understanding correctly, the median is actually guaranteed to be in the larger of the two pieces of the array after partitioning. That means on average you&#x27;d only discard 25% of the array after each partition. Your selected pivot is either below the median, above the median, or exactly the median. If it&#x27;s below the median, it could be anywhere in the range [p0, p50) for an average of around p25; if it&#x27;s above the median, it could be anywhere in the range (p50, p100] for an average of around p75.<p>Since these remaining fractions combine multiplicatively, we actually care about the geometric mean of the remaining fraction of the array, which is e^[(integral of ln(x) dx from x=0.5 to x=1) &#x2F; (1 - 0.5)], or about 73.5%.<p>Regardless, it forms a geometric series, which should converge to 1&#x2F;(1-0.735) or about 3.77.<p>Regarding using the average as the pivot: the question is really what quantile would be equal to the mean for your distribution. Heavily skewed distributions would perform pretty badly. It would perform particularly badly on 0.01*np.arange(1, 100) -- for each partitioning step, the mean would land between the first element and the second element.</div><br/></div></div></div></div><div id="41069173" class="c"><input type="checkbox" id="c-41069173" checked=""/><div class="controls bullet"><span class="by">sfpotter</span><span>|</span><a href="#41068309">prev</a><span>|</span><a href="#41067716">next</a><span>|</span><label class="collapse" for="c-41069173">[-]</label><label class="expand" for="c-41069173">[1 more]</label></div><br/><div class="children"><div class="content">A nice way to approximate the median: <a href="https:&#x2F;&#x2F;www.stat.berkeley.edu&#x2F;~ryantibs&#x2F;papers&#x2F;median.pdf" rel="nofollow">https:&#x2F;&#x2F;www.stat.berkeley.edu&#x2F;~ryantibs&#x2F;papers&#x2F;median.pdf</a></div><br/></div></div><div id="41067716" class="c"><input type="checkbox" id="c-41067716" checked=""/><div class="controls bullet"><span class="by">Tarean</span><span>|</span><a href="#41069173">prev</a><span>|</span><a href="#41067853">next</a><span>|</span><label class="collapse" for="c-41067716">[-]</label><label class="expand" for="c-41067716">[1 more]</label></div><br/><div class="children"><div class="content">Love this algorithm. It feels like magic, and then it feels obvious and basically like binary search.<p>Similar to the algorithm to parallelize the merge step of merge sort.  
Split the two sorted sequences into four sequences so that `merge(left[0:leftSplit], right[0:rightSplit])+merge(left[leftSplit:], right[rightSplit:])` is sorted. leftSplit+rightSplit should be halve the total length, and the elements in the left partition must be &lt;= the elements in the right partition.<p>Seems impossible, and then you think about it and it&#x27;s just binary search.</div><br/></div></div><div id="41067853" class="c"><input type="checkbox" id="c-41067853" checked=""/><div class="controls bullet"><span class="by">runiq</span><span>|</span><a href="#41067716">prev</a><span>|</span><a href="#41070389">next</a><span>|</span><label class="collapse" for="c-41067853">[-]</label><label class="expand" for="c-41067853">[1 more]</label></div><br/><div class="children"><div class="content">Why is it okay to drop not-full chunks? The article doesn&#x27;t explain that and I&#x27;m stupid.<p>Edit: I just realized that the function where non-full chunks are dropped is just the one for finding the pivot, not the one for finding the median. I understand now.</div><br/></div></div><div id="41070389" class="c"><input type="checkbox" id="c-41070389" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#41067853">prev</a><span>|</span><a href="#41067982">next</a><span>|</span><label class="collapse" for="c-41070389">[-]</label><label class="expand" for="c-41070389">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Quickselect gets us linear performance, but only in the average case. What if we arenât happy to be average, but instead want to guarantee that our algorithm is linear time, no matter what?<p>I don&#x27;t agree with the need for this guarantee. Note that the article already says the selection of the pivot is by random. You can simply choose a very good random function to avoid an attacker crafting an input that needs quadratic time. You&#x27;ll never be unlucky enough for this to be a problem. This is basically the same kind of mindset that leads people into thinking, what if I use SHA256 to hash these two different strings to get the same hash?</div><br/><div id="41070676" class="c"><input type="checkbox" id="c-41070676" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#41070389">parent</a><span>|</span><a href="#41070735">next</a><span>|</span><label class="collapse" for="c-41070676">[-]</label><label class="expand" for="c-41070676">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a very important guarantee for use in real-time signal processing applications.</div><br/></div></div><div id="41070735" class="c"><input type="checkbox" id="c-41070735" checked=""/><div class="controls bullet"><span class="by">forrestthewoods</span><span>|</span><a href="#41070389">parent</a><span>|</span><a href="#41070676">prev</a><span>|</span><a href="#41067982">next</a><span>|</span><label class="collapse" for="c-41070735">[-]</label><label class="expand" for="c-41070735">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t agree with the need for this guarantee.<p>You donât get to agree with it or not. It depends on the project! Clearly there exist some projects in the world where itâs important.<p>But honestly it doesnât matter. Because as the article shows with random data that median-of-medians is strictly better than random pivot. So even if you donât need the requirement there is zero loss to achieve it.</div><br/><div id="41071130" class="c"><input type="checkbox" id="c-41071130" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#41070389">root</a><span>|</span><a href="#41070735">parent</a><span>|</span><a href="#41067982">next</a><span>|</span><label class="collapse" for="c-41071130">[-]</label><label class="expand" for="c-41071130">[2 more]</label></div><br/><div class="children"><div class="content">The median-of-median comes at a cost for execution time. Chances are, sorting each five-element chunk is a lot slower than even running a sophisticated random number generator.</div><br/><div id="41076356" class="c"><input type="checkbox" id="c-41076356" checked=""/><div class="controls bullet"><span class="by">Quekid5</span><span>|</span><a href="#41070389">root</a><span>|</span><a href="#41071130">parent</a><span>|</span><a href="#41067982">next</a><span>|</span><label class="collapse" for="c-41076356">[-]</label><label class="expand" for="c-41076356">[1 more]</label></div><br/><div class="children"><div class="content">Slowness (lower throughput) is often the tradeoff for more predictable run time.</div><br/></div></div></div></div></div></div></div></div><div id="41067982" class="c"><input type="checkbox" id="c-41067982" checked=""/><div class="controls bullet"><span class="by">ValleZ</span><span>|</span><a href="#41070389">prev</a><span>|</span><a href="#41068416">next</a><span>|</span><label class="collapse" for="c-41067982">[-]</label><label class="expand" for="c-41067982">[1 more]</label></div><br/><div class="children"><div class="content">I was asked to invent this algorithm on a whiteboard in 30 minutes. Loved it.</div><br/></div></div><div id="41068416" class="c"><input type="checkbox" id="c-41068416" checked=""/><div class="controls bullet"><span class="by">mabbo</span><span>|</span><a href="#41067982">prev</a><span>|</span><a href="#41069132">next</a><span>|</span><label class="collapse" for="c-41068416">[-]</label><label class="expand" for="c-41068416">[5 more]</label></div><br/><div class="children"><div class="content">I learned about the median-of-medians quickselect algorithm when I was an undergrad and was really impressed by it. I implemented it, and it was terribly slow. It&#x27;s runtime grew linearly, but that only really mattered if you had at least a few billion items in your list.<p>I was chatting about this with a grad student friend who casually said something like &quot;Sure, it&#x27;s slow, but what really matters is that it proves that it&#x27;s <i>possible</i> to do selection of an unsorted list in O(n) time. At one point, we didn&#x27;t know whether that was even possible. Now that we do, we know there might an even faster linear algorithm.&quot; Really got into the philosophy of what Computer Science is about in the first place.<p>The lesson was so simple yet so profound that I nearly applied to grad school because of it. I have no idea if they even recall the conversation, but it was a pivotal moment of my education.</div><br/><div id="41068514" class="c"><input type="checkbox" id="c-41068514" checked=""/><div class="controls bullet"><span class="by">zelphirkalt</span><span>|</span><a href="#41068416">parent</a><span>|</span><a href="#41073817">next</a><span>|</span><label class="collapse" for="c-41068514">[-]</label><label class="expand" for="c-41068514">[3 more]</label></div><br/><div class="children"><div class="content">Does the fact, that any linear time algorithm exist, indicate, that a faster linear time algorithm exists? Otherwise, what is the gain from that bit of knowledge? You could also think: &quot;We already know, that some &lt;arbitrary O(...)&gt; algorithm exists, there might be an even faster &lt;other O(...)&gt; algorithm!&quot;
What makes the existence of an O(n) algo give more indication, than the existence of an O(n log(n)) algorithm?</div><br/><div id="41070356" class="c"><input type="checkbox" id="c-41070356" checked=""/><div class="controls bullet"><span class="by">blt</span><span>|</span><a href="#41068416">root</a><span>|</span><a href="#41068514">parent</a><span>|</span><a href="#41068648">next</a><span>|</span><label class="collapse" for="c-41070356">[-]</label><label class="expand" for="c-41070356">[1 more]</label></div><br/><div class="children"><div class="content">I am not the original commenter, but I (and probably many CS students) have had similar moments of clarity. The key part for me isn&#x27;t<p>&gt; there might be an even faster linear algorithm,<p>but<p>&gt; it&#x27;s possible to do selection of an unsorted list in O(n) time. At one point, we didn&#x27;t know whether that was even possible.<p>For me, the moment of clarity was understanding that theoretical CS mainly cares about <i>problems</i>, not <i>algorithms</i>. Algorithms are tools to prove upper bounds on the complexity of problems. Lower bounds are equally important and cannot be proved by designing algorithms. We even see theorems of the form &quot;there exists an O(whatever) algorithm for &lt;problem&gt;&quot;: the algorithm&#x27;s existence can sometimes be proven non-constructively.<p>So if the median problem sat for a long time with a linear lower bound and superlinear upper bound, we might start to wonder if the problem has a superlinear lower bound, and spend our effort working on that instead. The existence of a linear-time algorithm immediately closes that path. The only remaining work is to tighten the constant factor. The community&#x27;s effort can be focused.<p>A famous example is the linear programming problem. Klee and Minty proved an exponential worst case for <i>the simplex algorithm</i>, but not for <i>linear programming itself</i>. Later, Khachiyan proved that the ellipsoid algorithm was polynomial-time, but it had huge constant factors and was useless in practice. However, a few years later, Karmarkar gave an efficient polynomial-time algorithm. One can imagine how Khachiyan&#x27;s work, although inefficient, could motivate a more intense focus on polynomial-time LP algorithms leading to Karmarkar&#x27;s breakthrough.</div><br/></div></div><div id="41068648" class="c"><input type="checkbox" id="c-41068648" checked=""/><div class="controls bullet"><span class="by">anonymoushn</span><span>|</span><a href="#41068416">root</a><span>|</span><a href="#41068514">parent</a><span>|</span><a href="#41070356">prev</a><span>|</span><a href="#41073817">next</a><span>|</span><label class="collapse" for="c-41068648">[-]</label><label class="expand" for="c-41068648">[1 more]</label></div><br/><div class="children"><div class="content">If you had two problems, and a linear time solution was known to exist for only one of them, I think it would be reasonable to say that it&#x27;s more likely that a practical linear time solution exists for that one than for the other one.</div><br/></div></div></div></div><div id="41073817" class="c"><input type="checkbox" id="c-41073817" checked=""/><div class="controls bullet"><span class="by">mrguyorama</span><span>|</span><a href="#41068416">parent</a><span>|</span><a href="#41068514">prev</a><span>|</span><a href="#41069132">next</a><span>|</span><label class="collapse" for="c-41073817">[-]</label><label class="expand" for="c-41073817">[1 more]</label></div><br/><div class="children"><div class="content">We studied (I believe) this algorithm in my senior year of Computer Science. We talked about the theory side of it that you mention, but this algorithm was also used to demonstrate that &quot;slow linear algorithm&quot; is not faster than &quot;Fast nlogn algorithm&quot; in most real life cases.<p>I think we got a constant factor of 22 for this algorithm so maybe it was a related one or something.</div><br/></div></div></div></div><div id="41069132" class="c"><input type="checkbox" id="c-41069132" checked=""/><div class="controls bullet"><span class="by">beyondCritics</span><span>|</span><a href="#41068416">prev</a><span>|</span><a href="#41068443">next</a><span>|</span><label class="collapse" for="c-41069132">[-]</label><label class="expand" for="c-41069132">[1 more]</label></div><br/><div class="children"><div class="content">&lt;Itâs not straightforward to prove why this is O(n).<p>Replace T(n&#x2F;5) with T(floor(n&#x2F;5)) and T(7n&#x2F;10) with T(floor(7n&#x2F;10)) and show by induction that T(n) &lt;= 10n for all n.</div><br/></div></div><div id="41068443" class="c"><input type="checkbox" id="c-41068443" checked=""/><div class="controls bullet"><span class="by">zelphirkalt</span><span>|</span><a href="#41069132">prev</a><span>|</span><a href="#41068962">next</a><span>|</span><label class="collapse" for="c-41068443">[-]</label><label class="expand" for="c-41068443">[4 more]</label></div><br/><div class="children"><div class="content">You can simply pass once over the data, and while you do that, count occurrences of the elements, memorizing the last maximum. Whenever an element is counted, you check, if that count is now higher than the previous maximum. If it is, you memorize the element and its count as the maximum, of course. Very simple approach and linear in time, with minimal book keeping on the way (only the median element and the count (previous max)).<p>I don&#x27;t find it surprising or special at all, that finding the median works in linear time, since even this ad-hoc thought of way is in linear time.<p>EDIT: Ah right, I mixed up mode and median. My bad.</div><br/><div id="41068483" class="c"><input type="checkbox" id="c-41068483" checked=""/><div class="controls bullet"><span class="by">gcr</span><span>|</span><a href="#41068443">parent</a><span>|</span><a href="#41068962">next</a><span>|</span><label class="collapse" for="c-41068483">[-]</label><label class="expand" for="c-41068483">[3 more]</label></div><br/><div class="children"><div class="content">This finds the mode (most common element), not the median.<p>Wouldn&#x27;t you also need to keep track of all element counts with your approach? You can&#x27;t keep the count of only the second-most-common element because you don&#x27;t know what that is yet.</div><br/><div id="41068883" class="c"><input type="checkbox" id="c-41068883" checked=""/><div class="controls bullet"><span class="by">zelphirkalt</span><span>|</span><a href="#41068443">root</a><span>|</span><a href="#41068483">parent</a><span>|</span><a href="#41068962">next</a><span>|</span><label class="collapse" for="c-41068883">[-]</label><label class="expand" for="c-41068883">[2 more]</label></div><br/><div class="children"><div class="content">Yes, you are right. I mixed up mode and median.<p>And yes, one would need to keep track of at least a key for each element (not a huge element, if they are somehow huge). But that would be about space complexity.</div><br/><div id="41069064" class="c"><input type="checkbox" id="c-41069064" checked=""/><div class="controls bullet"><span class="by">gcr</span><span>|</span><a href="#41068443">root</a><span>|</span><a href="#41068883">parent</a><span>|</span><a href="#41068962">next</a><span>|</span><label class="collapse" for="c-41069064">[-]</label><label class="expand" for="c-41069064">[1 more]</label></div><br/><div class="children"><div class="content">pardon! it&#x27;s fun to think about though!</div><br/></div></div></div></div></div></div></div></div><div id="41068962" class="c"><input type="checkbox" id="c-41068962" checked=""/><div class="controls bullet"><span class="by">SkiFire13</span><span>|</span><a href="#41068443">prev</a><span>|</span><a href="#41068754">next</a><span>|</span><label class="collapse" for="c-41068962">[-]</label><label class="expand" for="c-41068962">[3 more]</label></div><br/><div class="children"><div class="content">I wonder what&#x27;s the reason of picking groups of 5 elements instead of 2 or 8.</div><br/><div id="41069150" class="c"><input type="checkbox" id="c-41069150" checked=""/><div class="controls bullet"><span class="by">danlark</span><span>|</span><a href="#41068962">parent</a><span>|</span><a href="#41069018">next</a><span>|</span><label class="collapse" for="c-41069150">[-]</label><label class="expand" for="c-41069150">[1 more]</label></div><br/><div class="children"><div class="content">3 and 4 elements will fail to prove the complexity is linear<p>You still can do 3 or 4 but with slight modifications<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.3600" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.3600</a><p>For example, for 4 elements, it&#x27;s advised to take lower median for the first half and upper median for the second half. Then the complexity will be linear</div><br/></div></div><div id="41069018" class="c"><input type="checkbox" id="c-41069018" checked=""/><div class="controls bullet"><span class="by">lalaland1125</span><span>|</span><a href="#41068962">parent</a><span>|</span><a href="#41069150">prev</a><span>|</span><a href="#41068754">next</a><span>|</span><label class="collapse" for="c-41069018">[-]</label><label class="expand" for="c-41069018">[1 more]</label></div><br/><div class="children"><div class="content">1. You want an odd number so the median is the middle element of the sublist.<p>2. One and three are probably too small</div><br/></div></div></div></div><div id="41068754" class="c"><input type="checkbox" id="c-41068754" checked=""/><div class="controls bullet"><span class="by">someplaceguy</span><span>|</span><a href="#41068962">prev</a><span>|</span><a href="#41068004">next</a><span>|</span><label class="collapse" for="c-41068754">[-]</label><label class="expand" for="c-41068754">[4 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>    return l[len(l) &#x2F; 2]
</code></pre>
I&#x27;m not a Python expert, but doesn&#x27;t the `&#x2F;` operator return a float in Python? Why would you use a float as an array index instead of doing integer division (with `&#x2F;&#x2F;`)?<p>I know this probably won&#x27;t matter until you have extremely large arrays, but this is still quite a code smell.<p>Perhaps this could be forgiven if you&#x27;re a Python novice and hadn&#x27;t realized that the two different operators exist, but this is not the case here, as the article contains this even more baffling code which uses integer division in one branch but float division in the other:<p><pre><code>    def quickselect_median(l, pivot_fn=random.choice):
        if len(l) % 2 == 1:
            return quickselect(l, len(l) &#x2F;&#x2F; 2, pivot_fn)
        else:
            return 0.5 * (quickselect(l, len(l) &#x2F; 2 - 1, pivot_fn) +
                           quickselect(l, len(l) &#x2F; 2, pivot_fn))
</code></pre>
That we&#x27;re 50 comments in and nobody seems to have noticed this only serves to reinforce my existing prejudice against the average Python code quality.</div><br/><div id="41071615" class="c"><input type="checkbox" id="c-41071615" checked=""/><div class="controls bullet"><span class="by">jononor</span><span>|</span><a href="#41068754">parent</a><span>|</span><a href="#41070110">next</a><span>|</span><label class="collapse" for="c-41071615">[-]</label><label class="expand" for="c-41071615">[1 more]</label></div><br/><div class="children"><div class="content">Well spotted! In Python 2 there was only one operator, but in Python 3 they are distinct.
Indexing an array with a float raises an exception, I believe.</div><br/></div></div><div id="41070110" class="c"><input type="checkbox" id="c-41070110" checked=""/><div class="controls bullet"><span class="by">runeblaze</span><span>|</span><a href="#41068754">parent</a><span>|</span><a href="#41071615">prev</a><span>|</span><a href="#41069058">next</a><span>|</span><label class="collapse" for="c-41070110">[-]</label><label class="expand" for="c-41070110">[1 more]</label></div><br/><div class="children"><div class="content">I do agree that it is a code smell. However given that this is an algorithms article I don&#x27;t think it is exactly that fair to judge it based on code quality. I think of it as: instead of writing it in pseudocode the author chose a real pseudocode-like programming language, and it (presumably) runs well for illustrative purposes.</div><br/></div></div></div></div><div id="41068004" class="c"><input type="checkbox" id="c-41068004" checked=""/><div class="controls bullet"><span class="by">nilslindemann</span><span>|</span><a href="#41068754">prev</a><span>|</span><a href="#41068973">next</a><span>|</span><label class="collapse" for="c-41068004">[-]</label><label class="expand" for="c-41068004">[3 more]</label></div><br/><div class="children"><div class="content">&quot;ns&quot; instead of &quot;l&quot; and &quot;n&quot; instead of &quot;el&quot; would have been my choice (seen in Haskell code).</div><br/><div id="41068241" class="c"><input type="checkbox" id="c-41068241" checked=""/><div class="controls bullet"><span class="by">robinhouston</span><span>|</span><a href="#41068004">parent</a><span>|</span><a href="#41068973">next</a><span>|</span><label class="collapse" for="c-41068241">[-]</label><label class="expand" for="c-41068241">[2 more]</label></div><br/><div class="children"><div class="content">The trouble with using this convention (which I also like) in Python code is that sooner or later one wants to name a pair of lists &#x27;as&#x27; and &#x27;bs&#x27;, which then causes a syntax error because &#x27;as&#x27; is a keyword in Python. There is a similar problem with &#x27;is&#x27; and &#x27;js&#x27;.</div><br/><div id="41069868" class="c"><input type="checkbox" id="c-41069868" checked=""/><div class="controls bullet"><span class="by">nilslindemann</span><span>|</span><a href="#41068004">root</a><span>|</span><a href="#41068241">parent</a><span>|</span><a href="#41068973">next</a><span>|</span><label class="collapse" for="c-41069868">[-]</label><label class="expand" for="c-41069868">[1 more]</label></div><br/><div class="children"><div class="content">Sure, naming is hard, but avoid &quot;l&quot;, &quot;I&quot;, &quot;O&quot;, &quot;o&quot;.<p>Very short variable names (including &quot;ns&quot; and &quot;n&quot;) are always some kind of disturbance when reading code, especially when the variable lasts longer than one screen of code â one has to memorize the meaning. They sometimes have a point, e.g. in mathematical code like this one. But variables like &quot;l&quot; and &quot;O&quot; are bad for a further reason, as they can not easily be distinguished from the numbers. See also the Python style guide: <a href="https:&#x2F;&#x2F;peps.python.org&#x2F;pep-0008&#x2F;#names-to-avoid" rel="nofollow">https:&#x2F;&#x2F;peps.python.org&#x2F;pep-0008&#x2F;#names-to-avoid</a></div><br/></div></div></div></div></div></div><div id="41068973" class="c"><input type="checkbox" id="c-41068973" checked=""/><div class="controls bullet"><span class="by">someplaceguy</span><span>|</span><a href="#41068004">prev</a><span>|</span><label class="collapse" for="c-41068973">[-]</label><label class="expand" for="c-41068973">[11 more]</label></div><br/><div class="children"><div class="content">I found this part of the code quite funny:<p><pre><code>    # If there are &lt; 5 items, just return the median
    if len(l) &lt; 5:
        # In this case, we fall back on the first median function we wrote.
        # Since we only run this on a list of 5 or fewer items, it doesn&#x27;t
        # depend on the length of the input and can be considered constant
        # time.
        return nlogn_median(l)
</code></pre>
Hell, why not just use 2^140 instead of 5 as the cut-off point, then? This way you&#x27;d have <i>constant time</i> median finding for all arrays that can be represented in any real-world computer! :) [1]<p>[1] According to <a href="https:&#x2F;&#x2F;hbfs.wordpress.com&#x2F;2009&#x2F;02&#x2F;10&#x2F;to-boil-the-oceans&#x2F;" rel="nofollow">https:&#x2F;&#x2F;hbfs.wordpress.com&#x2F;2009&#x2F;02&#x2F;10&#x2F;to-boil-the-oceans&#x2F;</a></div><br/><div id="41069146" class="c"><input type="checkbox" id="c-41069146" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#41068973">parent</a><span>|</span><a href="#41069010">next</a><span>|</span><label class="collapse" for="c-41069146">[-]</label><label class="expand" for="c-41069146">[6 more]</label></div><br/><div class="children"><div class="content">Big-O notation and &quot;real-world computer&quot; don&#x27;t belong to the same sentence. The whole point of big-O notation is to abstract the algorithm out of real-world limitations so we can talk about arbitrarily large input.<p>Any halting program that runs on a real world computer is O(1), by definition.</div><br/><div id="41069302" class="c"><input type="checkbox" id="c-41069302" checked=""/><div class="controls bullet"><span class="by">someplaceguy</span><span>|</span><a href="#41068973">root</a><span>|</span><a href="#41069146">parent</a><span>|</span><a href="#41069010">next</a><span>|</span><label class="collapse" for="c-41069302">[-]</label><label class="expand" for="c-41069302">[5 more]</label></div><br/><div class="children"><div class="content">&gt; The whole point of big-O notation is to abstract the algorithm out of real-world limitations so we can talk about arbitrarily large input.<p>Except that there is no such thing as &quot;arbitrarily large storage&quot;, as my link in the parent comment explained: <a href="https:&#x2F;&#x2F;hbfs.wordpress.com&#x2F;2009&#x2F;02&#x2F;10&#x2F;to-boil-the-oceans&#x2F;" rel="nofollow">https:&#x2F;&#x2F;hbfs.wordpress.com&#x2F;2009&#x2F;02&#x2F;10&#x2F;to-boil-the-oceans&#x2F;</a><p>So why would you want to talk about arbitrarily large input (where the input is an array that is stored in memory)?<p>As I understood, this big-O notation is intended to have some real-world usefulness, is it not? Care to elaborate what that usefulness is, exactly? Or is it just a purely fictional notion in the realm of ideas with no real-world application?<p>And if so, why bother studying it at all, except as a mathematical curiosity written in some mathematical pseudo-code rather than a programming or engineering challenge written in a real-world programming language?<p>Edit: s&#x2F;pretending&#x2F;intended&#x2F;</div><br/><div id="41070039" class="c"><input type="checkbox" id="c-41070039" checked=""/><div class="controls bullet"><span class="by">fenomas</span><span>|</span><a href="#41068973">root</a><span>|</span><a href="#41069302">parent</a><span>|</span><a href="#41069755">next</a><span>|</span><label class="collapse" for="c-41070039">[-]</label><label class="expand" for="c-41070039">[3 more]</label></div><br/><div class="children"><div class="content">Big-O analysis is about <i>scaling</i> behavior - its real-world implications lie in what it tells you about <i>relative</i> sizes, not absolute sizes.<p>E.g., if you need to run a task on 10M inputs, then knowing that your algorithm is O(N) doesn&#x27;t tell you anything at all about how long your task will take. It also doesn&#x27;t tell you whether that algorithm will be faster than some other algorithm that&#x27;s O(N^2).<p>But it <i>does</i> tell you that if your task size doubles to 20M inputs, you can expect the time required for the first algorithm to double, and the second to quadruple. And that knowledge isn&#x27;t arcane or theoretical, it works on real-world hardware and is really useful for modeling how your code will run as inputs scale up.</div><br/><div id="41075552" class="c"><input type="checkbox" id="c-41075552" checked=""/><div class="controls bullet"><span class="by">laweijfmvo</span><span>|</span><a href="#41068973">root</a><span>|</span><a href="#41070039">parent</a><span>|</span><a href="#41069755">next</a><span>|</span><label class="collapse" for="c-41075552">[-]</label><label class="expand" for="c-41075552">[2 more]</label></div><br/><div class="children"><div class="content">thank you for this explanation! to me it looks like the algo sorts the whole array but in groups of 5; the number of chunks should scale O(N&#x2F;5) = O(N), no? so how can you claim just by chunking you can ignore the fact that you still sorted N elements e.g. a selection sort would still perform N^2 comparisons total.</div><br/><div id="41075717" class="c"><input type="checkbox" id="c-41075717" checked=""/><div class="controls bullet"><span class="by">fenomas</span><span>|</span><a href="#41068973">root</a><span>|</span><a href="#41075552">parent</a><span>|</span><a href="#41069755">next</a><span>|</span><label class="collapse" for="c-41075717">[-]</label><label class="expand" for="c-41075717">[1 more]</label></div><br/><div class="children"><div class="content">Ah, so the issue here is the difference between quickSort and quickSelect. In both cases you pick a pivot and divide the data into two partitions - but in quickSort you then recurse on both partitions, and in quickSelect you determine which partition your target is in and recurse only on that one.<p>So you&#x27;re right that dividing into chunks of 5 and iterating over them doesn&#x27;t affect the scaling - you wind up with an array of (N&#x2F;5) medians, and it&#x27;s still O(N) to iterate over that array. But the next step isn&#x27;t to sort that array, you only need to quickSelect on it, and that&#x27;s why the final step is O(N) rather than O(NlogN).</div><br/></div></div></div></div></div></div><div id="41069755" class="c"><input type="checkbox" id="c-41069755" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#41068973">root</a><span>|</span><a href="#41069302">parent</a><span>|</span><a href="#41070039">prev</a><span>|</span><a href="#41069010">next</a><span>|</span><label class="collapse" for="c-41069755">[-]</label><label class="expand" for="c-41069755">[1 more]</label></div><br/><div class="children"><div class="content">&gt; (where the input is an array that is stored in memory)?<p>If the input is an array that is stored in a piece of real-world memory, then the only possible complexity is O(1). It&#x27;s just how big-O works. Big-O notation is an abstraction that is much much closer to mathematics than to engineering.<p>&gt; this big-O notation is pretending to have some real-world usefulness...<p>Big-O notation is not a person so I&#x27;m not sure whether it can pretend something. CS professors might exaggerate big-O notation&#x27;s real-world usefulness so their students don&#x27;t fall asleep too fast.<p>&gt; fictional<p>Theoretical. Just like the other theoretical ideas, <i>at best</i> big-O notation gives some vague insights that help people solve real problems. It gives a <i>very</i> rough feeling about whether an algorithm is fast or not.<p>By the way, Turing machine is in this category as well.</div><br/></div></div></div></div></div></div><div id="41069010" class="c"><input type="checkbox" id="c-41069010" checked=""/><div class="controls bullet"><span class="by">SkiFire13</span><span>|</span><a href="#41068973">parent</a><span>|</span><a href="#41069146">prev</a><span>|</span><label class="collapse" for="c-41069010">[-]</label><label class="expand" for="c-41069010">[4 more]</label></div><br/><div class="children"><div class="content">Ultimately when an algorithm has worse complexity than another it might still be faster up to a certain point. In this case 5 is likely under that point, though I doubt 2^256 will.<p>In practice you might also want to use a O(n^2) algorithm like insertion sort under some threshold.</div><br/><div id="41069089" class="c"><input type="checkbox" id="c-41069089" checked=""/><div class="controls bullet"><span class="by">someplaceguy</span><span>|</span><a href="#41068973">root</a><span>|</span><a href="#41069010">parent</a><span>|</span><label class="collapse" for="c-41069089">[-]</label><label class="expand" for="c-41069089">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Ultimately when an algorithm has worse complexity than another it might still be faster up to a certain point.<p>Sure, but the author didn&#x27;t argue that the simpler algorithm would be faster for 5 items, which would indeed make sense.<p>Instead, the author argued that it&#x27;s OK to use the simpler algorithm for less than 5 items because 5 is a constant and therefore the simpler algorithm runs in constant time, hence my point that you could use the same argument to say that 2^140 (or 2^256) could just as well be used as the cut-off point and similarly argue that the simpler algorithm runs in <i>constant time</i> for all arrays than can be represented on a real-world computer, therefore obviating the need for the more complex algorithm (which obviously makes no sense).</div><br/><div id="41069502" class="c"><input type="checkbox" id="c-41069502" checked=""/><div class="controls bullet"><span class="by">thrw2486776</span><span>|</span><a href="#41068973">root</a><span>|</span><a href="#41069089">parent</a><span>|</span><label class="collapse" for="c-41069502">[-]</label><label class="expand" for="c-41069502">[2 more]</label></div><br/><div class="children"><div class="content">If you set n=2^140, then sure, itâs constant. If instead you only have n&lt;=2^140, then n varies across a large set and is practically indistinguishable from n&lt;=infinity (since we get into the territory of the number of atoms in the universe), therefore you can perform limit calculations on it, in particular big O stuff.<p>In the article n was set to 5. All of those arrays (except maybe 1) have exactly 5 elements. There is no variance (and even if there was, it would be tiny, there is no point in talking about limits of 5-element sequences).</div><br/><div id="41069771" class="c"><input type="checkbox" id="c-41069771" checked=""/><div class="controls bullet"><span class="by">someplaceguy</span><span>|</span><a href="#41068973">root</a><span>|</span><a href="#41069502">parent</a><span>|</span><label class="collapse" for="c-41069771">[-]</label><label class="expand" for="c-41069771">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In the article n was set to 5. All of those arrays (except maybe 1) have exactly 5 elements. There is no variance<p>No, the code was:<p><pre><code>    # If there are &lt; 5 items, just return the median
    if len(l) &lt; 5:
        return nlogn_median(l)
</code></pre>
&gt; and even if there was, it would be tiny, there is no point in talking about limits of 5-element sequences<p>So your point is: not all constants are created equal. Which circles all the way back to my original point that this argument is pretty funny :)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>