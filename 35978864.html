<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1684400451676" as="style"/><link rel="stylesheet" href="styles.css?v=1684400451676"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/ray-project/llm-numbers">Numbers every LLM developer should know</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>richardliaw</span> | <span>87 comments</span></div><br/><div><div id="35980190" class="c"><input type="checkbox" id="c-35980190" checked=""/><div class="controls bullet"><span class="by">abetlen</span><span>|</span><a href="#35985225">next</a><span>|</span><label class="collapse" for="c-35980190">[-]</label><label class="expand" for="c-35980190">[9 more]</label></div><br/><div class="children"><div class="content">I would add the following two numbers if you&#x27;re generating realtime text or speech for human consumption:<p>- Human Reading Speed (English): ~250 words per minute<p>- Human Speaking Speed (English): ~150 words per minute<p>Should be treated like the Doherty Threshold [1] for generative content.<p>[1] <a href="https:&#x2F;&#x2F;lawsofux.com&#x2F;doherty-threshold&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lawsofux.com&#x2F;doherty-threshold&#x2F;</a></div><br/><div id="35982546" class="c"><input type="checkbox" id="c-35982546" checked=""/><div class="controls bullet"><span class="by">skykooler</span><span>|</span><a href="#35980190">parent</a><span>|</span><a href="#35980604">next</a><span>|</span><label class="collapse" for="c-35982546">[-]</label><label class="expand" for="c-35982546">[3 more]</label></div><br/><div class="children"><div class="content">Human reading speed varies by a factor of 10 or more between individuals, while speaking speed is much more consistent.</div><br/><div id="35982645" class="c"><input type="checkbox" id="c-35982645" checked=""/><div class="controls bullet"><span class="by">fritzo</span><span>|</span><a href="#35980190">root</a><span>|</span><a href="#35982546">parent</a><span>|</span><a href="#35980604">next</a><span>|</span><label class="collapse" for="c-35982645">[-]</label><label class="expand" for="c-35982645">[2 more]</label></div><br/><div class="children"><div class="content">Even my own reading speed even varies by a factor of 5 day to day, depending on how much reading I&#x27;ve been doing, sleep I&#x27;ve gotten, etc.</div><br/><div id="35984137" class="c"><input type="checkbox" id="c-35984137" checked=""/><div class="controls bullet"><span class="by">irrational</span><span>|</span><a href="#35980190">root</a><span>|</span><a href="#35982645">parent</a><span>|</span><a href="#35980604">next</a><span>|</span><label class="collapse" for="c-35984137">[-]</label><label class="expand" for="c-35984137">[1 more]</label></div><br/><div class="children"><div class="content">Plus, whether I am reading light fiction versus technical documentation.</div><br/></div></div></div></div></div></div><div id="35980604" class="c"><input type="checkbox" id="c-35980604" checked=""/><div class="controls bullet"><span class="by">armchairhacker</span><span>|</span><a href="#35980190">parent</a><span>|</span><a href="#35982546">prev</a><span>|</span><a href="#35985225">next</a><span>|</span><label class="collapse" for="c-35980604">[-]</label><label class="expand" for="c-35980604">[5 more]</label></div><br/><div class="children"><div class="content">But I&#x27;d say LLMs produce content faster than I can read or write it, because they can produce content which is really dense.<p>Ask GPT-4 a question and then answer it yourself. Maybe your answer will be as good or better than GPT-4&#x27;s but GPT-4 writes its answer a lot faster.</div><br/><div id="35982442" class="c"><input type="checkbox" id="c-35982442" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#35980190">root</a><span>|</span><a href="#35980604">parent</a><span>|</span><a href="#35984694">next</a><span>|</span><label class="collapse" for="c-35982442">[-]</label><label class="expand" for="c-35982442">[3 more]</label></div><br/><div class="children"><div class="content">It certainly doesn&#x27;t produce content as fast as I can read it.</div><br/><div id="35984394" class="c"><input type="checkbox" id="c-35984394" checked=""/><div class="controls bullet"><span class="by">renonce</span><span>|</span><a href="#35980190">root</a><span>|</span><a href="#35982442">parent</a><span>|</span><a href="#35984694">next</a><span>|</span><label class="collapse" for="c-35984394">[-]</label><label class="expand" for="c-35984394">[2 more]</label></div><br/><div class="children"><div class="content">Only if you use gpt-4. gpt-3.5-turbo is much faster, and gpt-4 is only going to get faster as GPUs get faster.</div><br/><div id="35985259" class="c"><input type="checkbox" id="c-35985259" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#35980190">root</a><span>|</span><a href="#35984394">parent</a><span>|</span><a href="#35984694">next</a><span>|</span><label class="collapse" for="c-35985259">[-]</label><label class="expand" for="c-35985259">[1 more]</label></div><br/><div class="children"><div class="content">Bing also uses GPT-4 and it is very fast. Microsoft spends more ok compute.</div><br/></div></div></div></div></div></div><div id="35984694" class="c"><input type="checkbox" id="c-35984694" checked=""/><div class="controls bullet"><span class="by">bombolo</span><span>|</span><a href="#35980190">root</a><span>|</span><a href="#35980604">parent</a><span>|</span><a href="#35982442">prev</a><span>|</span><a href="#35985225">next</a><span>|</span><label class="collapse" for="c-35984694">[-]</label><label class="expand" for="c-35984694">[1 more]</label></div><br/><div class="children"><div class="content">dense content? Not in my experience. It seems to be really overly verbose for me.</div><br/></div></div></div></div></div></div><div id="35985225" class="c"><input type="checkbox" id="c-35985225" checked=""/><div class="controls bullet"><span class="by">cornfutes</span><span>|</span><a href="#35980190">prev</a><span>|</span><a href="#35979985">next</a><span>|</span><label class="collapse" for="c-35985225">[-]</label><label class="expand" for="c-35985225">[3 more]</label></div><br/><div class="children"><div class="content">&gt; LLM developer<p>This is the first time I heard this term, and when I Google search &quot;LLM developer&quot; in an incognito tab, different device, this article is one of the first results.<p>Seems like we should first establish what exactly is an LLM developer.<p>&gt; When I was at Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know.<p>The personal plug and appeal to authority of &quot;When I was a Google&quot; is unnecessary. &quot;Numbers every Engineer should know&quot; is public and literally linked there. It&#x27;s a weird way to start a engineering blog post and makes it feel like marketing of one&#x27;s resume. Then again, I guess that&#x27;s what most of these engineering blog posts are nowadays.<p>Indeed Jeff Dean is a legend and needing to add the &quot;legendary engineer&quot; qualifier detracts from this point. Let these things speak for themselves.</div><br/><div id="35985341" class="c"><input type="checkbox" id="c-35985341" checked=""/><div class="controls bullet"><span class="by">mrtranscendence</span><span>|</span><a href="#35985225">parent</a><span>|</span><a href="#35985445">next</a><span>|</span><label class="collapse" for="c-35985341">[-]</label><label class="expand" for="c-35985341">[1 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re being somewhat uncharitable here. There&#x27;s nothing wrong with adding a personal detail here or there, and nothing wrong with giving credit to those who deserve it. I don&#x27;t see any reason to bikeshed the short, inessential details included in the blogger&#x27;s prose.</div><br/></div></div><div id="35985445" class="c"><input type="checkbox" id="c-35985445" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#35985225">parent</a><span>|</span><a href="#35985341">prev</a><span>|</span><a href="#35979985">next</a><span>|</span><label class="collapse" for="c-35985445">[-]</label><label class="expand" for="c-35985445">[1 more]</label></div><br/><div class="children"><div class="content">The term &quot;LLM developer&quot; is clear enough from context.</div><br/></div></div></div></div><div id="35979985" class="c"><input type="checkbox" id="c-35979985" checked=""/><div class="controls bullet"><span class="by">jncraton</span><span>|</span><a href="#35985225">prev</a><span>|</span><a href="#35985184">next</a><span>|</span><label class="collapse" for="c-35979985">[-]</label><label class="expand" for="c-35979985">[9 more]</label></div><br/><div class="children"><div class="content">&gt; There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy there is too much loss of resolution.<p>I&#x27;m not sure this is accurate. From what I have seen, 8-bit quantization is usually fine, and even 4-bit is a viable tradeoff. Here are some benchmarks from TextSynth showing no significant degradation between 16 and 8 bit:<p><a href="https:&#x2F;&#x2F;textsynth.com&#x2F;technology.html" rel="nofollow">https:&#x2F;&#x2F;textsynth.com&#x2F;technology.html</a><p>8-bit uses half as much memory and doubles the throughput for limited quality loss.</div><br/><div id="35981091" class="c"><input type="checkbox" id="c-35981091" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#35979985">parent</a><span>|</span><a href="#35982042">next</a><span>|</span><label class="collapse" for="c-35981091">[-]</label><label class="expand" for="c-35981091">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s true if you&#x27;re doing training. But for inference severe quantization is mostly okay. And there are some internal parts of a transformer running inference with a quantized model where you might want the x-bit inputs to do calculations with 16 bits like the dot product similarity between vectors.</div><br/><div id="35981155" class="c"><input type="checkbox" id="c-35981155" checked=""/><div class="controls bullet"><span class="by">Jackson__</span><span>|</span><a href="#35979985">root</a><span>|</span><a href="#35981091">parent</a><span>|</span><a href="#35982042">next</a><span>|</span><label class="collapse" for="c-35981155">[-]</label><label class="expand" for="c-35981155">[2 more]</label></div><br/><div class="children"><div class="content">Even that is being tackled by newer GPU architectures. For example, novelai is currently training an LLM in fp8 precision, using H100 GPUs.[1]<p>[1]
<a href="https:&#x2F;&#x2F;blog.novelai.net&#x2F;anlatan-acquires-hgx-h100-cluster-4b7a2e6a631e" rel="nofollow">https:&#x2F;&#x2F;blog.novelai.net&#x2F;anlatan-acquires-hgx-h100-cluster-4...</a><p><a href="https:&#x2F;&#x2F;blog.novelai.net&#x2F;text-model-progress-is-going-good-82a94855445e" rel="nofollow">https:&#x2F;&#x2F;blog.novelai.net&#x2F;text-model-progress-is-going-good-8...</a></div><br/><div id="35981257" class="c"><input type="checkbox" id="c-35981257" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#35979985">root</a><span>|</span><a href="#35981155">parent</a><span>|</span><a href="#35982042">next</a><span>|</span><label class="collapse" for="c-35981257">[-]</label><label class="expand" for="c-35981257">[1 more]</label></div><br/><div class="children"><div class="content">Cool stuff. I looked at <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hopper_%28microarchitecture%29" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hopper_%28microarchitecture%29</a> and I noticed that that the fp8 support is only for the tensor cores and not the CUDA side. Does that mean training with H100 GPU in fp8 mode would use some software ecosystem that&#x27;s not the existing vast existing CUDA one? Or am I just misunderstanding CUDA cores vs tensor cores?<p>PS, as a joke, they should implement GPU fluint8 and get baked in non-linearity for the activation function without even using a non-linear function, <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Ae9EKCyI1xU">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Ae9EKCyI1xU</a> (&quot;GradIEEEnt half decent: The hidden power of imprecise lines&quot; by suckerpinch)</div><br/></div></div></div></div></div></div><div id="35982042" class="c"><input type="checkbox" id="c-35982042" checked=""/><div class="controls bullet"><span class="by">fzliu</span><span>|</span><a href="#35979985">parent</a><span>|</span><a href="#35981091">prev</a><span>|</span><a href="#35980176">next</a><span>|</span><label class="collapse" for="c-35982042">[-]</label><label class="expand" for="c-35982042">[1 more]</label></div><br/><div class="children"><div class="content">AFAIK for over-parameterized models, performing quantization or any other form of compression won&#x27;t reduce accuracy by much (don&#x27;t quote me on this though).</div><br/></div></div><div id="35980176" class="c"><input type="checkbox" id="c-35980176" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#35979985">parent</a><span>|</span><a href="#35982042">prev</a><span>|</span><a href="#35980197">next</a><span>|</span><label class="collapse" for="c-35980176">[-]</label><label class="expand" for="c-35980176">[1 more]</label></div><br/><div class="children"><div class="content">The problem with 8bit at the moment is massive performance degradation with bitsandbytes. Recent improvements in 4bit inference mean that 8bit is now a massive laggard (although there’s no reason not to expect this to resolve).</div><br/></div></div><div id="35980197" class="c"><input type="checkbox" id="c-35980197" checked=""/><div class="controls bullet"><span class="by">waleedk</span><span>|</span><a href="#35979985">parent</a><span>|</span><a href="#35980176">prev</a><span>|</span><a href="#35980196">next</a><span>|</span><label class="collapse" for="c-35980197">[-]</label><label class="expand" for="c-35980197">[1 more]</label></div><br/><div class="children"><div class="content">[Author] Fair point. Adjusted the language.<p>Nonetheless people do tend to use 16 bit huggingface models, and if you do go to 8 bits and it&#x27;s wrong, you&#x27;re never quite sure if it&#x27;s the quant or the model.</div><br/></div></div><div id="35980196" class="c"><input type="checkbox" id="c-35980196" checked=""/><div class="controls bullet"><span class="by">f_devd</span><span>|</span><a href="#35979985">parent</a><span>|</span><a href="#35980197">prev</a><span>|</span><a href="#35985184">next</a><span>|</span><label class="collapse" for="c-35980196">[-]</label><label class="expand" for="c-35980196">[2 more]</label></div><br/><div class="children"><div class="content">The article is right, 8-bit (and especially 4-bit) is atypical for deep learning models and highly depends on the amount of parameters (larger model can handle more quantization) and can even depend on specific training hyperparameters (mainly dropout &amp; weight decay which can induce sparsity)</div><br/><div id="35980884" class="c"><input type="checkbox" id="c-35980884" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35979985">root</a><span>|</span><a href="#35980196">parent</a><span>|</span><a href="#35985184">next</a><span>|</span><label class="collapse" for="c-35980884">[-]</label><label class="expand" for="c-35980884">[1 more]</label></div><br/><div class="children"><div class="content">Thing is, even when the impact from 4-bit is substantial, the larger parameter count it allows on the same hardware more than makes up for it. E.g. llama-30b is better at 4-bit than <i>any</i> derivative of llama-13b, no matter how fine-tuned or quantized.</div><br/></div></div></div></div></div></div><div id="35985184" class="c"><input type="checkbox" id="c-35985184" checked=""/><div class="controls bullet"><span class="by">zenogantner</span><span>|</span><a href="#35979985">prev</a><span>|</span><a href="#35980109">next</a><span>|</span><label class="collapse" for="c-35985184">[-]</label><label class="expand" for="c-35985184">[1 more]</label></div><br/><div class="children"><div class="content">&gt; 40-90%: Amount saved by appending “Be Concise” to your prompt<p>Looks to me like &quot;numbers every LLM <i>user</i> needs to know&quot;.</div><br/></div></div><div id="35980109" class="c"><input type="checkbox" id="c-35980109" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#35985184">prev</a><span>|</span><a href="#35980461">next</a><span>|</span><label class="collapse" for="c-35980109">[-]</label><label class="expand" for="c-35980109">[19 more]</label></div><br/><div class="children"><div class="content">&gt; Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical.<p>No, 4bit quantization is the typical case.<p>At 4bit you can fit twice the parameters of 8bit in the same space for far better performance&#x2F;perplexity&#x2F;quality.<p>Running LLMs higher than 4bit is atypical and almost always sub-optimal (compared to running a model half the size in 8bit).<p>Even pretraining and finetuning in 4bit is likely to become the norm soon as fp4 becomes more well understood.</div><br/><div id="35980236" class="c"><input type="checkbox" id="c-35980236" checked=""/><div class="controls bullet"><span class="by">waleedk</span><span>|</span><a href="#35980109">parent</a><span>|</span><a href="#35980242">next</a><span>|</span><label class="collapse" for="c-35980236">[-]</label><label class="expand" for="c-35980236">[7 more]</label></div><br/><div class="children"><div class="content">[Author] Completely disagree. Any analysis shows that you see perplexity reduction at 4 bits. Have a look at llama.cpp&#x27;s results here:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp#quantization">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp#quantization</a><p>4 bit has a perplexity score 0.13 or so higher.</div><br/><div id="35980848" class="c"><input type="checkbox" id="c-35980848" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#35980109">root</a><span>|</span><a href="#35980236">parent</a><span>|</span><a href="#35981483">next</a><span>|</span><label class="collapse" for="c-35980848">[-]</label><label class="expand" for="c-35980848">[4 more]</label></div><br/><div class="children"><div class="content">You&#x27;re just wrong. You&#x27;re looking at the wrong numbers. The perplexity score of a model with twice the parameters in half the bits (4bit) is FAR LOWER (ie better).<p>If you are limited to X RAM and have two 16bit models of size 4X and 2X then the 4X model in 4bit will always be far superior to the 2X model in 8bit, with far lower perplexity.<p>Compare 13B&#x27;s 4bit perplexity of 5.3607 to 7B&#x27;s 8bit perplexity of 5.9069. That is over 0.54 lower perplexity for the same RAM amount by using 4bit! That is MASSIVE!</div><br/><div id="35982750" class="c"><input type="checkbox" id="c-35982750" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#35980109">root</a><span>|</span><a href="#35980848">parent</a><span>|</span><a href="#35983946">next</a><span>|</span><label class="collapse" for="c-35982750">[-]</label><label class="expand" for="c-35982750">[1 more]</label></div><br/><div class="children"><div class="content">Another factor is that larger models degrade less when quantized.<p>You have to wonder if running a huge model, say, 300B parameters at 2-bit quantization might be &quot;optimal&quot; in that it would fit into a single A100 or H100 GPU and likely outperform an 80B parameter 8-bit model...</div><br/></div></div><div id="35983946" class="c"><input type="checkbox" id="c-35983946" checked=""/><div class="controls bullet"><span class="by">oh_sigh</span><span>|</span><a href="#35980109">root</a><span>|</span><a href="#35980848">parent</a><span>|</span><a href="#35982750">prev</a><span>|</span><a href="#35981483">next</a><span>|</span><label class="collapse" for="c-35983946">[-]</label><label class="expand" for="c-35983946">[2 more]</label></div><br/><div class="children"><div class="content">A stupid question but...what about a 16x model in 1bit?</div><br/><div id="35984419" class="c"><input type="checkbox" id="c-35984419" checked=""/><div class="controls bullet"><span class="by">renonce</span><span>|</span><a href="#35980109">root</a><span>|</span><a href="#35983946">parent</a><span>|</span><a href="#35981483">next</a><span>|</span><label class="collapse" for="c-35984419">[-]</label><label class="expand" for="c-35984419">[1 more]</label></div><br/><div class="children"><div class="content">Has binary neural network been implemented for Transformers yet?</div><br/></div></div></div></div></div></div><div id="35981483" class="c"><input type="checkbox" id="c-35981483" checked=""/><div class="controls bullet"><span class="by">Taek</span><span>|</span><a href="#35980109">root</a><span>|</span><a href="#35980236">parent</a><span>|</span><a href="#35980848">prev</a><span>|</span><a href="#35980737">next</a><span>|</span><label class="collapse" for="c-35981483">[-]</label><label class="expand" for="c-35981483">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s also research showing that the perplexity reduction is less at higher parameter counts. E.g. a 65b parameter model barely has any impact at all when reducing from 16bit to 4bit</div><br/></div></div><div id="35980737" class="c"><input type="checkbox" id="c-35980737" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#35980109">root</a><span>|</span><a href="#35980236">parent</a><span>|</span><a href="#35981483">prev</a><span>|</span><a href="#35980242">next</a><span>|</span><label class="collapse" for="c-35980737">[-]</label><label class="expand" for="c-35980737">[1 more]</label></div><br/><div class="children"><div class="content">Well, if you have a fixed RAM size, you&#x27;re better off with the largest model you can fit at 4 bits (13B 4b is way better than 7B 16b despite being twice smaller).</div><br/></div></div></div></div><div id="35980242" class="c"><input type="checkbox" id="c-35980242" checked=""/><div class="controls bullet"><span class="by">kherud</span><span>|</span><a href="#35980109">parent</a><span>|</span><a href="#35980236">prev</a><span>|</span><a href="#35980875">next</a><span>|</span><label class="collapse" for="c-35980242">[-]</label><label class="expand" for="c-35980242">[6 more]</label></div><br/><div class="children"><div class="content">Can somebody please explain how quantization below 8 bit works? Since a byte is the smallest addressable unit I think, is the dimensionality of the weights somehow reduced?</div><br/><div id="35980372" class="c"><input type="checkbox" id="c-35980372" checked=""/><div class="controls bullet"><span class="by">waleedk</span><span>|</span><a href="#35980109">root</a><span>|</span><a href="#35980242">parent</a><span>|</span><a href="#35983119">next</a><span>|</span><label class="collapse" for="c-35980372">[-]</label><label class="expand" for="c-35980372">[2 more]</label></div><br/><div class="children"><div class="content">[Author] You approximate the weights using fewer bits. You also switch to ints instead of floats and then do some fancy stuff when multiplying to make it all work together.<p>More detail than you probably wanted: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;hf-bitsandbytes-integration" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;hf-bitsandbytes-integration</a></div><br/><div id="35980924" class="c"><input type="checkbox" id="c-35980924" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#35980109">root</a><span>|</span><a href="#35980372">parent</a><span>|</span><a href="#35983119">next</a><span>|</span><label class="collapse" for="c-35980924">[-]</label><label class="expand" for="c-35980924">[1 more]</label></div><br/><div class="children"><div class="content">The latest release of bitsandbytes uses a new fp4 format. 4bit floating point scailing results in much lower perplexity than int4.<p>Also note that for a fixed memory (RAM) size, 4bit (even int4) is always superior, resulting in lower perplexity than 8bit.<p>E.g. LLaMA-13B int4 is far better&#x2F;lower perplexity than LLaMA-7B fp8 while using the same amount of RAM.</div><br/></div></div></div></div><div id="35983119" class="c"><input type="checkbox" id="c-35983119" checked=""/><div class="controls bullet"><span class="by">dahart</span><span>|</span><a href="#35980109">root</a><span>|</span><a href="#35980242">parent</a><span>|</span><a href="#35980372">prev</a><span>|</span><a href="#35984225">next</a><span>|</span><label class="collapse" for="c-35983119">[-]</label><label class="expand" for="c-35983119">[1 more]</label></div><br/><div class="children"><div class="content">Software can address units of any size, by packing and unpacking bits from bytes (or more likely words) in the underlying implementation. I don’t know about any specific NN implementation here, just commenting in general that the size of the addressable unit and the size of your reads can writes can be completely independent. I routinely use bit-packing data compression techniques in CUDA, for example.</div><br/></div></div><div id="35984225" class="c"><input type="checkbox" id="c-35984225" checked=""/><div class="controls bullet"><span class="by">sifar</span><span>|</span><a href="#35980109">root</a><span>|</span><a href="#35980242">parent</a><span>|</span><a href="#35983119">prev</a><span>|</span><a href="#35980359">next</a><span>|</span><label class="collapse" for="c-35984225">[-]</label><label class="expand" for="c-35984225">[1 more]</label></div><br/><div class="children"><div class="content">Generally, since the memory is byte addressable, you load data which is packed into bytes. It is the compute instructions that use the specified bits needed.<p>So in this case one would load a byte which would have 2 4b data, and then you would have a 4b ADD or MAC which would operate on them.<p>If you don&#x27;t have them then you need to sign&#x2F;zero extend or convert the smaller bit-widths to 8&#x2F;16&#x2F;32b whichever is available.</div><br/></div></div><div id="35980359" class="c"><input type="checkbox" id="c-35980359" checked=""/><div class="controls bullet"><span class="by">f_devd</span><span>|</span><a href="#35980109">root</a><span>|</span><a href="#35980242">parent</a><span>|</span><a href="#35984225">prev</a><span>|</span><a href="#35980875">next</a><span>|</span><label class="collapse" for="c-35980359">[-]</label><label class="expand" for="c-35980359">[1 more]</label></div><br/><div class="children"><div class="content">I believe it&#x27;s locally (inner-loop or simd op) up-cast to float8&#x2F;float16&#x2F;int8, but I haven&#x27;t looked at the internals of llama.cpp myself</div><br/></div></div></div></div><div id="35980875" class="c"><input type="checkbox" id="c-35980875" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#35980109">parent</a><span>|</span><a href="#35980242">prev</a><span>|</span><a href="#35980368">next</a><span>|</span><label class="collapse" for="c-35980875">[-]</label><label class="expand" for="c-35980875">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  llama.cpp which runs a 13 billion parameter model on a 6GB GPU<p>I think that&#x27;s a typo there too, the 13B model needs like 10G of memory for 4 bits, it&#x27;s the 7B one that fits into 6G. Well unless you do the split thing with some layers on the CPU I guess.</div><br/><div id="35983453" class="c"><input type="checkbox" id="c-35983453" checked=""/><div class="controls bullet"><span class="by">DANmode</span><span>|</span><a href="#35980109">root</a><span>|</span><a href="#35980875">parent</a><span>|</span><a href="#35980368">next</a><span>|</span><label class="collapse" for="c-35983453">[-]</label><label class="expand" for="c-35983453">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35937505" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35937505</a></div><br/></div></div></div></div><div id="35980368" class="c"><input type="checkbox" id="c-35980368" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#35980109">parent</a><span>|</span><a href="#35980875">prev</a><span>|</span><a href="#35980461">next</a><span>|</span><label class="collapse" for="c-35980368">[-]</label><label class="expand" for="c-35980368">[3 more]</label></div><br/><div class="children"><div class="content">No it isn&#x27;t, quantization is not free. You lose a significant amount of performance that you are not measuring properly in automated benchmarks when you quantize to that level.<p>You can see it in real time when you take most LLMs and compare them at different quantization levels. I can see the degradation even in the largest llama quite badly even at 8 bits.</div><br/><div id="35980972" class="c"><input type="checkbox" id="c-35980972" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#35980109">root</a><span>|</span><a href="#35980368">parent</a><span>|</span><a href="#35980861">next</a><span>|</span><label class="collapse" for="c-35980972">[-]</label><label class="expand" for="c-35980972">[1 more]</label></div><br/><div class="children"><div class="content">Quantization is not free, but VRAM is even less free.<p>If you have X amount of VRAM and can fit a 16bit model of size 2X in 8bit or a model of size 4X in 4bit then the 4X model in 4bit is ALWAYS superior with lower perplexity and better performance.<p>You LOSE performance by using a smaller model in 8bit vs a larger model in 4bit.</div><br/></div></div><div id="35980861" class="c"><input type="checkbox" id="c-35980861" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#35980109">root</a><span>|</span><a href="#35980368">parent</a><span>|</span><a href="#35980972">prev</a><span>|</span><a href="#35980461">next</a><span>|</span><label class="collapse" for="c-35980861">[-]</label><label class="expand" for="c-35980861">[1 more]</label></div><br/><div class="children"><div class="content">If you take a model and quantize it it&#x27;s obviously going to get worse, but what if you train it again after that?</div><br/></div></div></div></div></div></div><div id="35980461" class="c"><input type="checkbox" id="c-35980461" checked=""/><div class="controls bullet"><span class="by">PoignardAzur</span><span>|</span><a href="#35980109">prev</a><span>|</span><a href="#35981007">next</a><span>|</span><label class="collapse" for="c-35980461">[-]</label><label class="expand" for="c-35980461">[5 more]</label></div><br/><div class="children"><div class="content">&gt; <i>~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens</i><p>MosaicML claims they trained a 7 billion parameter on 1 trillion tokens with a budget of $200k.<p><a href="https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;mpt-7b" rel="nofollow">https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;mpt-7b</a><p>Does training cost scale linearly with model size and token count? If so, that suggests a lower bound of $600k to train the 13 billion params model. (Still roughly the same magnitude)</div><br/><div id="35980506" class="c"><input type="checkbox" id="c-35980506" checked=""/><div class="controls bullet"><span class="by">waleedk</span><span>|</span><a href="#35980461">parent</a><span>|</span><a href="#35981007">next</a><span>|</span><label class="collapse" for="c-35980506">[-]</label><label class="expand" for="c-35980506">[4 more]</label></div><br/><div class="children"><div class="content">[Author] Mosaic must be getting some kind of sweetheart deals on A100 80GB and A100 40GB. The prices they are quoting are not what say the AWS on-demand prices are. They quote $2 per GPU for A100 40GB and $2.50 for A100 80GB. That&#x27;s literally half the AWS on-demand rate for A100s here: <a href="https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;p4&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aws.amazon.com&#x2F;ec2&#x2F;instance-types&#x2F;p4&#x2F;</a><p>And these are impossible to get. We tried to get some for Anyscale, and we were told there were no on-demand available and lead time for reserved (ouchie on the price! You&#x27;re talking a quarter of a million dollars a year for one machine at list) was in weeks.<p>Once you take the model size and hefty sweetheart deals into account, you&#x27;re within 10%. Mosaic does have some nice whitebox optimizations, but nothing that radically changes the equation.</div><br/><div id="35983696" class="c"><input type="checkbox" id="c-35983696" checked=""/><div class="controls bullet"><span class="by">EgoIncarnate</span><span>|</span><a href="#35980461">root</a><span>|</span><a href="#35980506">parent</a><span>|</span><a href="#35981587">next</a><span>|</span><label class="collapse" for="c-35983696">[-]</label><label class="expand" for="c-35983696">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for putting to this together.<p>I have a suggested modification. You are mixing references in your document.<p>Re: &#x27;~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens
The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs.&#x27;<p>The LLaMA-13B model took 2.75 days of 2048xA100 (135,168 GPU-hours) with 1 trillion tokens. The 21 days for 1.4 trillion was for LLaMA-65B.<p>I would suggest using the LLaMa-13B numbers since those are the most relevant for this section, or at least modify &quot;21 days to train LLaMa&quot; to &quot;21 days to train LLaMa-65B&quot; for clarity.</div><br/></div></div><div id="35981587" class="c"><input type="checkbox" id="c-35981587" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#35980461">root</a><span>|</span><a href="#35980506">parent</a><span>|</span><a href="#35983696">prev</a><span>|</span><a href="#35983884">next</a><span>|</span><label class="collapse" for="c-35981587">[-]</label><label class="expand" for="c-35981587">[1 more]</label></div><br/><div class="children"><div class="content">A100-40GB is like $1.10 on LambdaLabs, on demand.  Their availability is horrific on singles, but I&#x27;ve seen 8x instances pop up more often than not.  And you can rent A100s for a buck a pop interruptible from other clouds, plenty of availability.  $2 doesn&#x27;t seem like much of a sweetheart deal.</div><br/></div></div><div id="35983884" class="c"><input type="checkbox" id="c-35983884" checked=""/><div class="controls bullet"><span class="by">Zuiii</span><span>|</span><a href="#35980461">root</a><span>|</span><a href="#35980506">parent</a><span>|</span><a href="#35981587">prev</a><span>|</span><a href="#35981007">next</a><span>|</span><label class="collapse" for="c-35983884">[-]</label><label class="expand" for="c-35983884">[1 more]</label></div><br/><div class="children"><div class="content">There is no possible way for anyone buying 1M worth of compute to get list pricing.</div><br/></div></div></div></div></div></div><div id="35981007" class="c"><input type="checkbox" id="c-35981007" checked=""/><div class="controls bullet"><span class="by">born-jre</span><span>|</span><a href="#35980461">prev</a><span>|</span><a href="#35985100">next</a><span>|</span><label class="collapse" for="c-35981007">[-]</label><label class="expand" for="c-35981007">[3 more]</label></div><br/><div class="children"><div class="content">RANDOM THOUGHT:<p>i wonder when we are getting docker for llm ... a Modelfile ?<p>FROM &quot;PAAMA&#x2F;16b&quot;<p>APPLY &quot;MNO&#x2F;DATASET&quot;<p>each layer could be lora adapter like thing maybe.<p>maybe when AI chips are finally here.</div><br/><div id="35981240" class="c"><input type="checkbox" id="c-35981240" checked=""/><div class="controls bullet"><span class="by">kristjansson</span><span>|</span><a href="#35981007">parent</a><span>|</span><a href="#35981327">next</a><span>|</span><label class="collapse" for="c-35981240">[-]</label><label class="expand" for="c-35981240">[1 more]</label></div><br/><div class="children"><div class="content">SQLFlow[0] looks sort of like that:<p><pre><code>    SELECT * FROM iris.train
    TO TRAIN DNNClassifier
    WITH model.hidden_units = [10, 10], model.n_classes = 3, train.epoch= 10
    COLUMN sepal_length, sepal_width, petal_length, petal_width
    LABEL class
    INTO sqlflow_models.my_dnn_model;
</code></pre>
No idea how well it works.<p>[0]: <a href="https:&#x2F;&#x2F;sql-machine-learning.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;sql-machine-learning.github.io&#x2F;</a></div><br/></div></div><div id="35981327" class="c"><input type="checkbox" id="c-35981327" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#35981007">parent</a><span>|</span><a href="#35981240">prev</a><span>|</span><a href="#35985100">next</a><span>|</span><label class="collapse" for="c-35981327">[-]</label><label class="expand" for="c-35981327">[1 more]</label></div><br/><div class="children"><div class="content">PyTorch tutorial looks similar (lower on the page)<p><a href="https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;pytorch_with_examples.html" rel="nofollow">https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;pytorch_with_examples...</a></div><br/></div></div></div></div><div id="35985100" class="c"><input type="checkbox" id="c-35985100" checked=""/><div class="controls bullet"><span class="by">diatone</span><span>|</span><a href="#35981007">prev</a><span>|</span><a href="#35981180">next</a><span>|</span><label class="collapse" for="c-35985100">[-]</label><label class="expand" for="c-35985100">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second.<p>Why?</div><br/></div></div><div id="35981180" class="c"><input type="checkbox" id="c-35981180" checked=""/><div class="controls bullet"><span class="by">ramesh1994</span><span>|</span><a href="#35985100">prev</a><span>|</span><a href="#35979944">next</a><span>|</span><label class="collapse" for="c-35981180">[-]</label><label class="expand" for="c-35981180">[2 more]</label></div><br/><div class="children"><div class="content">I think parts of the write-up are great.<p>There are some unique assumptions being made in parts of the gist<p>&gt; 10: Cost Ratio of OpenAI embedding to Self-Hosted embedding<p>&gt; 1: Cost Ratio of Self-Hosted base vs fine-tuned model queries<p>I don&#x27;t know how useful these numbers are if you take away the assumptions that self-hosted will work as well as API.<p>&gt; 10x: Throughput improvement from batching LLM requests<p>I see that the write up mentions memory being a caveat to this, but it also depends on the card specs as well. Memory Bandwidth &#x2F; TFLOPs offered by say 4090 is superior while having the same amount of VRAM as 3090. The caveat mentioned with token length in the gist itself makes the 10x claim not a useful rule of thumb.</div><br/><div id="35981337" class="c"><input type="checkbox" id="c-35981337" checked=""/><div class="controls bullet"><span class="by">ramesh1994</span><span>|</span><a href="#35981180">parent</a><span>|</span><a href="#35979944">next</a><span>|</span><label class="collapse" for="c-35981337">[-]</label><label class="expand" for="c-35981337">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x!<p>In a narrow use-case of a strict look-up. This seems to exaggerate the cost difference while having completely different trade-offs.</div><br/></div></div></div></div><div id="35979944" class="c"><input type="checkbox" id="c-35979944" checked=""/><div class="controls bullet"><span class="by">Flux159</span><span>|</span><a href="#35981180">prev</a><span>|</span><a href="#35982315">next</a><span>|</span><label class="collapse" for="c-35979944">[-]</label><label class="expand" for="c-35979944">[2 more]</label></div><br/><div class="children"><div class="content">I think that it would be helpful to add a fine-tuning costs for an open source model (think LLaMA to Alpaca).<p>From the phrasing around fine tuning right now it seems like it&#x27;s using openai&#x27;s fine tuning api to determine that cost, but it&#x27;s not very clear.<p>Also this would be helpful for other foundation models if that doesn&#x27;t already exist - how much VRAM to run Stable Diffusion v2.1 at different resolutions, running Whisper or Bark for audio, etc.</div><br/><div id="35980046" class="c"><input type="checkbox" id="c-35980046" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35979944">parent</a><span>|</span><a href="#35982315">next</a><span>|</span><label class="collapse" for="c-35980046">[-]</label><label class="expand" for="c-35980046">[1 more]</label></div><br/><div class="children"><div class="content">They mention that they could finetune a 6B model for $7. Obviously the number depends on the amount of data and the model size but it&#x27;s probably not going to be a significant expense in practice.</div><br/></div></div></div></div><div id="35982315" class="c"><input type="checkbox" id="c-35982315" checked=""/><div class="controls bullet"><span class="by">crosen99</span><span>|</span><a href="#35979944">prev</a><span>|</span><a href="#35979893">next</a><span>|</span><label class="collapse" for="c-35982315">[-]</label><label class="expand" for="c-35982315">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m surprised not to see anything about data-to-parameter ratios for optimal scaling.  My superficial understanding per the Chinchilla paper is to target 20 to 1.<p>I&#x27;m also confused about this:<p>&gt; ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens<p>This is apparently related to the LLaMa paper, but that paper seems to cite 1.0T tokens (rather than 1.4T tokens) for the 13B model.  Also, if 20 to 1 is in fact optimal for the data-to-parameter ratio, then using a 100 to 1 ratio doesn&#x27;t seem like an appropriate way to arrive at a magic number for training costs.  The magic number should really be based on an optimal configuration.  Or, perhaps, my superficial understanding here leads me to miss some important distinctions.</div><br/><div id="35982516" class="c"><input type="checkbox" id="c-35982516" checked=""/><div class="controls bullet"><span class="by">llambada</span><span>|</span><a href="#35982315">parent</a><span>|</span><a href="#35979893">next</a><span>|</span><label class="collapse" for="c-35982516">[-]</label><label class="expand" for="c-35982516">[1 more]</label></div><br/><div class="children"><div class="content">The Chinchilla paper only addresses the contrived use case of a model that is trained once and never used for inference. Since most of the real world compute cost will be in inference, Chinchilla seems to offer little practical guidance.</div><br/></div></div></div></div><div id="35979893" class="c"><input type="checkbox" id="c-35979893" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#35982315">prev</a><span>|</span><a href="#35981827">next</a><span>|</span><label class="collapse" for="c-35979893">[-]</label><label class="expand" for="c-35979893">[9 more]</label></div><br/><div class="children"><div class="content">How come the token to word ratio is smaller than 1 if tokens are either words or part of words? Shouldn&#x27;t you expect <i>more</i> tokens than words?</div><br/><div id="35979933" class="c"><input type="checkbox" id="c-35979933" checked=""/><div class="controls bullet"><span class="by">yonixw</span><span>|</span><a href="#35979893">parent</a><span>|</span><a href="#35980313">next</a><span>|</span><label class="collapse" for="c-35979933">[-]</label><label class="expand" for="c-35979933">[1 more]</label></div><br/><div class="children"><div class="content">That is how I understood it, a token is on average a 3&#x2F;4 of a word. &quot;Token to word&quot;. So if you want to buy 1000 tokens you would get effectively 750 words.</div><br/></div></div><div id="35980313" class="c"><input type="checkbox" id="c-35980313" checked=""/><div class="controls bullet"><span class="by">waleedk</span><span>|</span><a href="#35979893">parent</a><span>|</span><a href="#35979933">prev</a><span>|</span><a href="#35979947">next</a><span>|</span><label class="collapse" for="c-35980313">[-]</label><label class="expand" for="c-35980313">[1 more]</label></div><br/><div class="children"><div class="content">[Author] Fair point -- I clarified the language and gave a concrete example. Hope that helps!</div><br/></div></div><div id="35979991" class="c"><input type="checkbox" id="c-35979991" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#35979893">parent</a><span>|</span><a href="#35979947">prev</a><span>|</span><a href="#35979949">next</a><span>|</span><label class="collapse" for="c-35979991">[-]</label><label class="expand" for="c-35979991">[4 more]</label></div><br/><div class="children"><div class="content">I think all the ratios given are x:1 and they tell you x.</div><br/><div id="35980245" class="c"><input type="checkbox" id="c-35980245" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#35979893">root</a><span>|</span><a href="#35979991">parent</a><span>|</span><a href="#35980029">next</a><span>|</span><label class="collapse" for="c-35980245">[-]</label><label class="expand" for="c-35980245">[1 more]</label></div><br/><div class="children"><div class="content">It’s the other way around.<p>1 GPT4 token is equivalent to 50 GPT3.5 tokens.<p>1 token is equivalent to 0.75 words.</div><br/></div></div><div id="35980029" class="c"><input type="checkbox" id="c-35980029" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#35979893">root</a><span>|</span><a href="#35979991">parent</a><span>|</span><a href="#35980245">prev</a><span>|</span><a href="#35979949">next</a><span>|</span><label class="collapse" for="c-35980029">[-]</label><label class="expand" for="c-35980029">[2 more]</label></div><br/><div class="children"><div class="content">That would make it 0.75 tokens to 1 word right?</div><br/><div id="35982466" class="c"><input type="checkbox" id="c-35982466" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#35979893">root</a><span>|</span><a href="#35980029">parent</a><span>|</span><a href="#35979949">next</a><span>|</span><label class="collapse" for="c-35982466">[-]</label><label class="expand" for="c-35982466">[1 more]</label></div><br/><div class="children"><div class="content">lol, yes, I&#x27;m glad they clarified because I understood it correctly then made the mistake GP did when I replied to them.</div><br/></div></div></div></div></div></div><div id="35979949" class="c"><input type="checkbox" id="c-35979949" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#35979893">parent</a><span>|</span><a href="#35979991">prev</a><span>|</span><a href="#35981827">next</a><span>|</span><label class="collapse" for="c-35979949">[-]</label><label class="expand" for="c-35979949">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the token to word multiplier, yeah. i.e. x tokens = 0.75x words.</div><br/></div></div></div></div><div id="35981827" class="c"><input type="checkbox" id="c-35981827" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#35979893">prev</a><span>|</span><a href="#35984121">next</a><span>|</span><label class="collapse" for="c-35981827">[-]</label><label class="expand" for="c-35981827">[3 more]</label></div><br/><div class="children"><div class="content">&gt; ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens<p>Llama paper mentioned  135,168 A100 hours for training 13 billion model on 1 trillion tokens, which means ~$150k for lambdalabs on demand instance.</div><br/><div id="35981885" class="c"><input type="checkbox" id="c-35981885" checked=""/><div class="controls bullet"><span class="by">waleedk</span><span>|</span><a href="#35981827">parent</a><span>|</span><a href="#35984121">next</a><span>|</span><label class="collapse" for="c-35981885">[-]</label><label class="expand" for="c-35981885">[2 more]</label></div><br/><div class="children"><div class="content">[Author] Good luck trying to use clusters of Lambda machines. Lambda labs are cheap for a reason: their API is not very featureful (we looked at them and we saw they didn&#x27;t even support machine tagging). If you&#x27;re looking for a box or two, lambda labs is fine. If you&#x27;re looking for 1,000, not so much.<p>Plus they don&#x27;t actually have any actually A100s available at the moment (2022-05-17).<p>CoreWeave is a nice middle ground. You can at least get the A100 machines into a k8s cluster.</div><br/><div id="35983312" class="c"><input type="checkbox" id="c-35983312" checked=""/><div class="controls bullet"><span class="by">pseg134</span><span>|</span><a href="#35981827">root</a><span>|</span><a href="#35981885">parent</a><span>|</span><a href="#35984121">next</a><span>|</span><label class="collapse" for="c-35983312">[-]</label><label class="expand" for="c-35983312">[1 more]</label></div><br/><div class="children"><div class="content">Okay well that is just your experience. If you are brand new in this industry that is undergoing absolutely massive shortages of GPUs right now you probably will not be able to easily source GPUs. It might not seem fair but why would Lamda help someone they never heard of who will move for the next fad as quick as possible versus their long term existing customers?</div><br/></div></div></div></div></div></div><div id="35984121" class="c"><input type="checkbox" id="c-35984121" checked=""/><div class="controls bullet"><span class="by">EvgeniyZh</span><span>|</span><a href="#35981827">prev</a><span>|</span><a href="#35980078">next</a><span>|</span><label class="collapse" for="c-35984121">[-]</label><label class="expand" for="c-35984121">[1 more]</label></div><br/><div class="children"><div class="content">Talks about throughput but doesn&#x27;t mention memory I&#x2F;O speed, which should be a bottleneck for LLMs</div><br/></div></div><div id="35980078" class="c"><input type="checkbox" id="c-35980078" checked=""/><div class="controls bullet"><span class="by">throwaway888abc</span><span>|</span><a href="#35984121">prev</a><span>|</span><a href="#35980325">next</a><span>|</span><label class="collapse" for="c-35980078">[-]</label><label class="expand" for="c-35980078">[2 more]</label></div><br/><div class="children"><div class="content">Excellent!  Thank you so much for making&#x2F;posting this</div><br/><div id="35980256" class="c"><input type="checkbox" id="c-35980256" checked=""/><div class="controls bullet"><span class="by">waleedk</span><span>|</span><a href="#35980078">parent</a><span>|</span><a href="#35980325">next</a><span>|</span><label class="collapse" for="c-35980256">[-]</label><label class="expand" for="c-35980256">[1 more]</label></div><br/><div class="children"><div class="content">[Author] You&#x27;re welcome -- glad it was useful!</div><br/></div></div></div></div><div id="35980325" class="c"><input type="checkbox" id="c-35980325" checked=""/><div class="controls bullet"><span class="by">cwkoss</span><span>|</span><a href="#35980078">prev</a><span>|</span><a href="#35980045">next</a><span>|</span><label class="collapse" for="c-35980325">[-]</label><label class="expand" for="c-35980325">[8 more]</label></div><br/><div class="children"><div class="content">Are there any open source host-your-own LLMs that have licensing that allows for commercial use?</div><br/><div id="35980930" class="c"><input type="checkbox" id="c-35980930" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35980325">parent</a><span>|</span><a href="#35980473">next</a><span>|</span><label class="collapse" for="c-35980930">[-]</label><label class="expand" for="c-35980930">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;BlinkDL&#x2F;RWKV-LM">https:&#x2F;&#x2F;github.com&#x2F;BlinkDL&#x2F;RWKV-LM</a></div><br/></div></div><div id="35980473" class="c"><input type="checkbox" id="c-35980473" checked=""/><div class="controls bullet"><span class="by">elorant</span><span>|</span><a href="#35980325">parent</a><span>|</span><a href="#35980930">prev</a><span>|</span><a href="#35980345">next</a><span>|</span><label class="collapse" for="c-35980473">[-]</label><label class="expand" for="c-35980473">[4 more]</label></div><br/><div class="children"><div class="content">Vicuna-13b is on Apache License 2.0.</div><br/><div id="35980716" class="c"><input type="checkbox" id="c-35980716" checked=""/><div class="controls bullet"><span class="by">twbarr</span><span>|</span><a href="#35980325">root</a><span>|</span><a href="#35980473">parent</a><span>|</span><a href="#35980345">next</a><span>|</span><label class="collapse" for="c-35980716">[-]</label><label class="expand" for="c-35980716">[3 more]</label></div><br/><div class="children"><div class="content">Vicuna is a delta model that you have to apply on top of LLaMA.</div><br/><div id="35984767" class="c"><input type="checkbox" id="c-35984767" checked=""/><div class="controls bullet"><span class="by">ripvanwinkle</span><span>|</span><a href="#35980325">root</a><span>|</span><a href="#35980716">parent</a><span>|</span><a href="#35980345">next</a><span>|</span><label class="collapse" for="c-35984767">[-]</label><label class="expand" for="c-35984767">[2 more]</label></div><br/><div class="children"><div class="content">how does one get the original LLaMA weights. I tried the form that Meta has no dice. Also tried some torrents no luck there either</div><br/><div id="35985208" class="c"><input type="checkbox" id="c-35985208" checked=""/><div class="controls bullet"><span class="by">speedgoose</span><span>|</span><a href="#35980325">root</a><span>|</span><a href="#35984767">parent</a><span>|</span><a href="#35980345">next</a><span>|</span><label class="collapse" for="c-35985208">[-]</label><label class="expand" for="c-35985208">[1 more]</label></div><br/><div class="children"><div class="content">I filled the form and got the weights a few weeks later, but I work for a research organisation.<p>I thought the torrents were super active.</div><br/></div></div></div></div></div></div></div></div><div id="35980345" class="c"><input type="checkbox" id="c-35980345" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#35980325">parent</a><span>|</span><a href="#35980473">prev</a><span>|</span><a href="#35980045">next</a><span>|</span><label class="collapse" for="c-35980345">[-]</label><label class="expand" for="c-35980345">[2 more]</label></div><br/><div class="children"><div class="content">Dolly from Databricks is one at least</div><br/><div id="35980442" class="c"><input type="checkbox" id="c-35980442" checked=""/><div class="controls bullet"><span class="by">waleedk</span><span>|</span><a href="#35980325">root</a><span>|</span><a href="#35980345">parent</a><span>|</span><a href="#35980045">next</a><span>|</span><label class="collapse" for="c-35980442">[-]</label><label class="expand" for="c-35980442">[1 more]</label></div><br/><div class="children"><div class="content">[Author] TL;DR OS LLM models are coming.<p>Dolly&#x27;s not that great -- I&#x27;ve hit lots of issues using it to be honest .<p>MosaicML has a nice commercially usable model here: <a href="https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;mpt-7b" rel="nofollow">https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;mpt-7b</a><p>I think they&#x27;re one of the leading ones (bias: they&#x27;re kinda competitors to my employer Anyscale, but you gotta say something&#x27;s good when it is).<p>Red Pajama are leading an effort to build a fully open source model similar to LLaMa. <a href="https:&#x2F;&#x2F;www.together.xyz&#x2F;blog&#x2F;redpajama" rel="nofollow">https:&#x2F;&#x2F;www.together.xyz&#x2F;blog&#x2F;redpajama</a></div><br/></div></div></div></div></div></div><div id="35980045" class="c"><input type="checkbox" id="c-35980045" checked=""/><div class="controls bullet"><span class="by">curiousgal</span><span>|</span><a href="#35980325">prev</a><span>|</span><a href="#35984330">next</a><span>|</span><label class="collapse" for="c-35980045">[-]</label><label class="expand" for="c-35980045">[6 more]</label></div><br/><div class="children"><div class="content">&gt; <i>LLM Developer</i><p>This is the fastest I&#x27;ve rolled my eyes in a long time!</div><br/><div id="35980308" class="c"><input type="checkbox" id="c-35980308" checked=""/><div class="controls bullet"><span class="by">ryanklee</span><span>|</span><a href="#35980045">parent</a><span>|</span><a href="#35984330">next</a><span>|</span><label class="collapse" for="c-35980308">[-]</label><label class="expand" for="c-35980308">[5 more]</label></div><br/><div class="children"><div class="content">The amount of get-off-my-lawn grognardness that LLM activity inspires is really ridiculous.<p>I really would ask you to take a second look at the spirit of your comment and think carefully about how much you really understand about the work being done on top of LLMs and if it justifies this kind of response.</div><br/><div id="35981013" class="c"><input type="checkbox" id="c-35981013" checked=""/><div class="controls bullet"><span class="by">astrea</span><span>|</span><a href="#35980045">root</a><span>|</span><a href="#35980308">parent</a><span>|</span><a href="#35984330">next</a><span>|</span><label class="collapse" for="c-35981013">[-]</label><label class="expand" for="c-35981013">[4 more]</label></div><br/><div class="children"><div class="content">I had the same reaction as the OP. I’m not a data scientist by trade or title, but I would personally be a little offended. If you designed the Porsche 911, would you not be offended by the shade tree mechanic who simply knows how to change the oil calling himself a Porsche designer&#x2F;engineer?</div><br/><div id="35981439" class="c"><input type="checkbox" id="c-35981439" checked=""/><div class="controls bullet"><span class="by">ryanklee</span><span>|</span><a href="#35980045">root</a><span>|</span><a href="#35981013">parent</a><span>|</span><a href="#35981097">next</a><span>|</span><label class="collapse" for="c-35981439">[-]</label><label class="expand" for="c-35981439">[2 more]</label></div><br/><div class="children"><div class="content">There are people making applications based on LLMs. You may quibble with the term LLM Developer, but to sneer or roll your eyes at it as if it were prima facie inaccurate or laughable is unjustified.</div><br/><div id="35983322" class="c"><input type="checkbox" id="c-35983322" checked=""/><div class="controls bullet"><span class="by">pseg134</span><span>|</span><a href="#35980045">root</a><span>|</span><a href="#35981439">parent</a><span>|</span><a href="#35981097">next</a><span>|</span><label class="collapse" for="c-35983322">[-]</label><label class="expand" for="c-35983322">[1 more]</label></div><br/><div class="children"><div class="content">Well he was a web3 developer 6 months ago and a nft dev 12 months ago so forgive us for not taking this weeks flavor as being all that serious.</div><br/></div></div></div></div><div id="35981097" class="c"><input type="checkbox" id="c-35981097" checked=""/><div class="controls bullet"><span class="by">RyanCavanaugh</span><span>|</span><a href="#35980045">root</a><span>|</span><a href="#35981013">parent</a><span>|</span><a href="#35981439">prev</a><span>|</span><a href="#35984330">next</a><span>|</span><label class="collapse" for="c-35981097">[-]</label><label class="expand" for="c-35981097">[1 more]</label></div><br/><div class="children"><div class="content">Context matters. Is a &quot;web developer&quot; someone who makes web pages, or works on a browser rendering engine?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>