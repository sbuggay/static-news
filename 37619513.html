<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1695459660752" as="style"/><link rel="stylesheet" href="styles.css?v=1695459660752"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2309.06979">Auto-Regressive Next-Token Predictors Are Universal Learners</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>dataminer</span> | <span>5 comments</span></div><br/><div><div id="37619634" class="c"><input type="checkbox" id="c-37619634" checked=""/><div class="controls bullet"><span class="by">version_five</span><span>|</span><a href="#37619755">next</a><span>|</span><label class="collapse" for="c-37619634">[-]</label><label class="expand" for="c-37619634">[2 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  Our results demonstrate that the power of language models can be attributed, to a great extent, to the auto-regressive next-token training scheme, and not necessarily to a particular choice of architecture.
</code></pre>
I think that&#x27;s obvious isn&#x27;t it? Neural networks are universal function approximates, the question is how to make the efficient, either in parameters or computation or whatever, as well as all the usual stuff like encouraging convergence, avoiding big gradients, etc. That&#x27;s why transformers are popular, nobody thinks they especially can compute a function that other models can&#x27;t.</div><br/><div id="37621613" class="c"><input type="checkbox" id="c-37621613" checked=""/><div class="controls bullet"><span class="by">nonrandomstring</span><span>|</span><a href="#37619634">parent</a><span>|</span><a href="#37619755">next</a><span>|</span><label class="collapse" for="c-37621613">[-]</label><label class="expand" for="c-37621613">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I immediately thought, isn&#x27;t this congruent to a statement about
Turing machines. Sure there are classes of many things that are
computationally equivalent, including computers made of paper-tape,
tin cans and string. Just most of them are horrendously inefficient
and useful only as thought experiments.<p>I saw this again in audio synthesis but with more nuance. Most methods
are equivalent in some crazy limit, but all have a &quot;special&quot; area of
most useful effectiveness. For example in theory you can predict a
signal of many minutes or hours just using linear prediction (LPC),
but only at the cost of a gargantuan parameter space that&#x27;s less
efficient than just sampling the signal.<p>Nonetheless it is nice to see that researchers are connecting up these
dots, even if the pure maths behind it isn&#x27;t saying anything obviously
useful right away. Who knows what insights this might lead to for
discovering other new methods of computation.</div><br/></div></div></div></div></div></div></div></div></div></body></html>