<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1690275648550" as="style"/><link rel="stylesheet" href="styles.css?v=1690275648550"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://brev.dev/blog/fine-tuning-llama-2">A simple guide to fine-tuning Llama 2</a>Â <span class="domain">(<a href="https://brev.dev">brev.dev</a>)</span></div><div class="subtext"><span>samlhuillier</span> | <span>45 comments</span></div><br/><div><div id="36853995" class="c"><input type="checkbox" id="c-36853995" checked=""/><div class="controls bullet"><span class="by">nmitchko</span><span>|</span><a href="#36855789">next</a><span>|</span><label class="collapse" for="c-36853995">[-]</label><label class="expand" for="c-36853995">[11 more]</label></div><br/><div class="children"><div class="content">This is a pretty useless post. You could also follow the same 1000x tutorials about llama and use the already uploaded hugging face formats that are on hugging face...<p>Here are some actually useful links<p><a href="https:&#x2F;&#x2F;blog.ovhcloud.com&#x2F;fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;blog.ovhcloud.com&#x2F;fine-tuning-llama-2-models-using-a...</a><p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;meta-llama&#x2F;Llama-2-70b-hf" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;meta-llama&#x2F;Llama-2-70b-hf</a><p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;meta-llama&#x2F;Llama-2-7b-hf" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;meta-llama&#x2F;Llama-2-7b-hf</a></div><br/><div id="36854599" class="c"><input type="checkbox" id="c-36854599" checked=""/><div class="controls bullet"><span class="by">onlypositive</span><span>|</span><a href="#36853995">parent</a><span>|</span><a href="#36859243">next</a><span>|</span><label class="collapse" for="c-36854599">[-]</label><label class="expand" for="c-36854599">[4 more]</label></div><br/><div class="children"><div class="content">Is it really &quot;useless&quot; if I didn&#x27;t even know about llama? And look, now I have 3 more links to dive into.<p>This is the opposite of useless.</div><br/><div id="36855400" class="c"><input type="checkbox" id="c-36855400" checked=""/><div class="controls bullet"><span class="by">mciancia</span><span>|</span><a href="#36853995">root</a><span>|</span><a href="#36854599">parent</a><span>|</span><a href="#36859243">next</a><span>|</span><label class="collapse" for="c-36855400">[-]</label><label class="expand" for="c-36855400">[3 more]</label></div><br/><div class="children"><div class="content">Well it&#x27;s quite possible that it is useless for you since you didn&#x27;t hear about llama by now ;)</div><br/><div id="36855678" class="c"><input type="checkbox" id="c-36855678" checked=""/><div class="controls bullet"><span class="by">hoten</span><span>|</span><a href="#36853995">root</a><span>|</span><a href="#36855400">parent</a><span>|</span><a href="#36859243">next</a><span>|</span><label class="collapse" for="c-36855678">[-]</label><label class="expand" for="c-36855678">[2 more]</label></div><br/><div class="children"><div class="content">I guess this was meant as a tongue-in-cheek joke comment, but it comes off as needless gatekeeping.</div><br/></div></div></div></div></div></div><div id="36859243" class="c"><input type="checkbox" id="c-36859243" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#36853995">parent</a><span>|</span><a href="#36854599">prev</a><span>|</span><a href="#36856324">next</a><span>|</span><label class="collapse" for="c-36859243">[-]</label><label class="expand" for="c-36859243">[1 more]</label></div><br/><div class="children"><div class="content">I still don&#x27;t get why it&#x27;s useless. Maybe less useful, but useless seems to be a hyperbole.</div><br/></div></div><div id="36856324" class="c"><input type="checkbox" id="c-36856324" checked=""/><div class="controls bullet"><span class="by">alfalfasprout</span><span>|</span><a href="#36853995">parent</a><span>|</span><a href="#36859243">prev</a><span>|</span><a href="#36855838">next</a><span>|</span><label class="collapse" for="c-36856324">[-]</label><label class="expand" for="c-36856324">[3 more]</label></div><br/><div class="children"><div class="content">Honestly HN has turned into reposting random how-tos about the latest hype LLMs.</div><br/><div id="36857630" class="c"><input type="checkbox" id="c-36857630" checked=""/><div class="controls bullet"><span class="by">g42gregory</span><span>|</span><a href="#36853995">root</a><span>|</span><a href="#36856324">parent</a><span>|</span><a href="#36857692">next</a><span>|</span><label class="collapse" for="c-36857630">[-]</label><label class="expand" for="c-36857630">[1 more]</label></div><br/><div class="children"><div class="content">If that&#x27;s what people like, why is that a concern?</div><br/></div></div></div></div><div id="36855838" class="c"><input type="checkbox" id="c-36855838" checked=""/><div class="controls bullet"><span class="by">IAmNotACellist</span><span>|</span><a href="#36853995">parent</a><span>|</span><a href="#36856324">prev</a><span>|</span><a href="#36854384">next</a><span>|</span><label class="collapse" for="c-36855838">[-]</label><label class="expand" for="c-36855838">[1 more]</label></div><br/><div class="children"><div class="content">And it&#x27;s probably already in TextGen and better UIs for serious stuff than yet another Jupyter notebook.</div><br/></div></div><div id="36854384" class="c"><input type="checkbox" id="c-36854384" checked=""/><div class="controls bullet"><span class="by">jeremycarter</span><span>|</span><a href="#36853995">parent</a><span>|</span><a href="#36855838">prev</a><span>|</span><a href="#36855789">next</a><span>|</span><label class="collapse" for="c-36854384">[-]</label><label class="expand" for="c-36854384">[1 more]</label></div><br/><div class="children"><div class="content">Thanks!</div><br/></div></div></div></div><div id="36855789" class="c"><input type="checkbox" id="c-36855789" checked=""/><div class="controls bullet"><span class="by">zoogeny</span><span>|</span><a href="#36853995">prev</a><span>|</span><a href="#36859340">next</a><span>|</span><label class="collapse" for="c-36855789">[-]</label><label class="expand" for="c-36855789">[8 more]</label></div><br/><div class="children"><div class="content">What I&#x27;d like to do is create a website where:<p>1. There is a list of open source fine-tuning datasets on millions of topics. Like, anime, lord of the rings, dnd, customer service responses, finance, code in many programming languages, children&#x27;s books, religions, philosophies, etc. I mean, on every topic imaginable sort of like a Wikipedia or Reddit of fine-tuning data sets.<p>2. Users can select one or more available datasets as well as upload their own private datasets<p>3. Users can turn-key fine-tune llama 2 or other pre-trained models<p>Right now, doing this kind of thing is way beyond the capability of the common user.</div><br/><div id="36855831" class="c"><input type="checkbox" id="c-36855831" checked=""/><div class="controls bullet"><span class="by">IAmNotACellist</span><span>|</span><a href="#36855789">parent</a><span>|</span><a href="#36855821">next</a><span>|</span><label class="collapse" for="c-36855831">[-]</label><label class="expand" for="c-36855831">[3 more]</label></div><br/><div class="children"><div class="content">I personally don&#x27;t see a future where common users will ever have to know the phrase &quot;fine-tuning&quot; or worry about it. The most I can see is &quot;Do you consent to share your information with Apple&#x2F;Meta&#x2F;X&#x2F;Microsoft&#x2F;OpenAI&#x27;s knowledge engine?&quot; and if you agree, everything they have on you will power an extremely powerful all-encompassing knowledge engine. Probably with some daily recommendations to integrate a new domain into it, like, &quot;We noticed you&#x27;re into Lord of the Rings, so we went ahead and made your knowledge engine familiar with the collected works of Tolkein, all historical academic and modern interpretations and criticisms, transcripts of the movies, and generative AI fan fiction capabilities.&quot;</div><br/><div id="36855974" class="c"><input type="checkbox" id="c-36855974" checked=""/><div class="controls bullet"><span class="by">zoogeny</span><span>|</span><a href="#36855789">root</a><span>|</span><a href="#36855831">parent</a><span>|</span><a href="#36856895">next</a><span>|</span><label class="collapse" for="c-36855974">[-]</label><label class="expand" for="c-36855974">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think the major barrier to the idea would be consumer awareness. For the near-term the major barrier will be cost. Just as one example, together.ai offers fine-tuning service at an advertised cost of $0.001 per 1k tokens used [1]. That will get pricey for even small datasets. No doubt this will come down, but I don&#x27;t see consumers paying $1000 for a customized AI model that they then have to pay inference costs to run. Maybe once we get consumer devices that have sufficiently capable AI accelerators (e.g. Apple Neural Engine) to run sufficiently capable llm models, then customers would be willing to customize and run local.<p>The second point is, we don&#x27;t know if fine-tuned models, vector search or more-massive general purpose llm models is the right way to go.<p>But for business-to-business, I think this might be a viable business. If you had a whole bunch of ready-to-go open-source fine-tune datasets for commercial applications you might find a market of businesses that want to run their own models for a variety of reasons.<p>1. <a href="https:&#x2F;&#x2F;together.ai&#x2F;pricing" rel="nofollow noreferrer">https:&#x2F;&#x2F;together.ai&#x2F;pricing</a></div><br/></div></div><div id="36856895" class="c"><input type="checkbox" id="c-36856895" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#36855789">root</a><span>|</span><a href="#36855831">parent</a><span>|</span><a href="#36855974">prev</a><span>|</span><a href="#36855821">next</a><span>|</span><label class="collapse" for="c-36856895">[-]</label><label class="expand" for="c-36856895">[1 more]</label></div><br/><div class="children"><div class="content">The year is 2149, previously, we thought <i>time</i> was the real commoditiy, <i>water</i> before that, and <i>money</i> even prior....<p>But now. Now. Its DNAX... Cloning fraudulant DNA to make BIO-chips to unlock credits for &quot;yee ol&#x27; goods an&#x27; services gub&#x27;nah&quot;<p>Basically every transaction is bio-tracked, so if you want an off grid you have to have false clones...<p>DNA from old embryoes that allow you to build identities in their names and wear them like sleaves to navigate the systems.<p>This is how you manipulate the engines.</div><br/></div></div></div></div><div id="36855821" class="c"><input type="checkbox" id="c-36855821" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36855789">parent</a><span>|</span><a href="#36855831">prev</a><span>|</span><a href="#36856835">next</a><span>|</span><label class="collapse" for="c-36855821">[-]</label><label class="expand" for="c-36855821">[3 more]</label></div><br/><div class="children"><div class="content">This sounds like a great fit for Cerebras, if they can set up the text database front end.<p>They could host the text database for free, and then offer a &quot;oh look, you can train llama on this text <i>right now</i> for cheaper than a Nvidia box&quot; button on every listing.<p>Then charge through the nose for private business training (kinds like they do now, but charging more.)</div><br/><div id="36855994" class="c"><input type="checkbox" id="c-36855994" checked=""/><div class="controls bullet"><span class="by">zoogeny</span><span>|</span><a href="#36855789">root</a><span>|</span><a href="#36855821">parent</a><span>|</span><a href="#36856835">next</a><span>|</span><label class="collapse" for="c-36855994">[-]</label><label class="expand" for="c-36855994">[2 more]</label></div><br/><div class="children"><div class="content">I agree that it would be almost impossible to defend this kind of business, especially if you stayed committed to open-source datasets. It would come down to the UX and the community if you hoped to survive. Probably long-term you would either have to get into your own pre-trained models, fight the commodity hosting business or aim to get acquired.</div><br/><div id="36857135" class="c"><input type="checkbox" id="c-36857135" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36855789">root</a><span>|</span><a href="#36855994">parent</a><span>|</span><a href="#36856835">next</a><span>|</span><label class="collapse" for="c-36857135">[-]</label><label class="expand" for="c-36857135">[1 more]</label></div><br/><div class="children"><div class="content">Well civitai is basically what you are describing. Its very doable.<p>But a big difference is that (for now) Stable Diffusion finetuning is much easier than LLaMA.</div><br/></div></div></div></div></div></div><div id="36856835" class="c"><input type="checkbox" id="c-36856835" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#36855789">parent</a><span>|</span><a href="#36855821">prev</a><span>|</span><a href="#36859340">next</a><span>|</span><label class="collapse" for="c-36856835">[-]</label><label class="expand" for="c-36856835">[1 more]</label></div><br/><div class="children"><div class="content">ELI5 - who exactly makes the open datasets you refer to? [SERIOUS Q]</div><br/></div></div></div></div><div id="36859340" class="c"><input type="checkbox" id="c-36859340" checked=""/><div class="controls bullet"><span class="by">marcopicentini</span><span>|</span><a href="#36855789">prev</a><span>|</span><a href="#36857493">next</a><span>|</span><label class="collapse" for="c-36859340">[-]</label><label class="expand" for="c-36859340">[1 more]</label></div><br/><div class="children"><div class="content">Anyone has calculate the break even point (as number of token per month) between self-hosted LLAMA and OpenAI GPT-3.5 API?</div><br/></div></div><div id="36857493" class="c"><input type="checkbox" id="c-36857493" checked=""/><div class="controls bullet"><span class="by">moneywoes</span><span>|</span><a href="#36859340">prev</a><span>|</span><a href="#36853906">next</a><span>|</span><label class="collapse" for="c-36857493">[-]</label><label class="expand" for="c-36857493">[2 more]</label></div><br/><div class="children"><div class="content">Any fine tuning success stories? Or real world use cases</div><br/><div id="36858362" class="c"><input type="checkbox" id="c-36858362" checked=""/><div class="controls bullet"><span class="by">bvm</span><span>|</span><a href="#36857493">parent</a><span>|</span><a href="#36853906">next</a><span>|</span><label class="collapse" for="c-36858362">[-]</label><label class="expand" for="c-36858362">[1 more]</label></div><br/><div class="children"><div class="content">Sure. I worked at a company that produced tens of thousands of human written summaries of news data a year. This was costly and slow but our clients really valued them. Back in 2019 we fine tuned an LLM to help, we put a lot of effort into creating a human-in-the-loop experience, highlighting parts of speech that were commonly hallucinated and ensuring that we were allowing humans to focus on things that humans are good at.<p>We also released some of the data as a free dataset with a commercial option for all of it. This was more successful than I thought it would be and was hoovered up by the kind of people that buy these datasets.<p>It will have been surpassed by recent developments now but it was an incredibly enjoyable project.</div><br/></div></div></div></div><div id="36853906" class="c"><input type="checkbox" id="c-36853906" checked=""/><div class="controls bullet"><span class="by">eachro</span><span>|</span><a href="#36857493">prev</a><span>|</span><a href="#36853313">next</a><span>|</span><label class="collapse" for="c-36853906">[-]</label><label class="expand" for="c-36853906">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve veen a bit out of the loop on this area but would like to get back into it given how much has changed in the LLM landscape in the last 1-2 yrs. What models are small enough to play with on Collab? Or am I going to have to spin up my own gpu box on aws to be able to mess around with these models?</div><br/><div id="36854346" class="c"><input type="checkbox" id="c-36854346" checked=""/><div class="controls bullet"><span class="by">naderkhalil</span><span>|</span><a href="#36853906">parent</a><span>|</span><a href="#36855162">next</a><span>|</span><label class="collapse" for="c-36854346">[-]</label><label class="expand" for="c-36854346">[1 more]</label></div><br/><div class="children"><div class="content">Hey, you could use a template on brev.dev to spin up a gpu box with the model and Jupyter notebook. Alternatively, the falcon 7b model should be small enough for colab</div><br/></div></div></div></div><div id="36853313" class="c"><input type="checkbox" id="c-36853313" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#36853906">prev</a><span>|</span><a href="#36854073">next</a><span>|</span><label class="collapse" for="c-36853313">[-]</label><label class="expand" for="c-36853313">[9 more]</label></div><br/><div class="children"><div class="content">Can someone share a good tutorial how to prepare the data? And for fine tuning, does a 3090 have enough VRAM? I want to do what the author mentioned by fine tuning the model on my personal data but Iâm not sure how to prepare the data. I tried using vector search + LLM but I find the results very subpar when using a local LLM.</div><br/><div id="36853686" class="c"><input type="checkbox" id="c-36853686" checked=""/><div class="controls bullet"><span class="by">jawerty</span><span>|</span><a href="#36853313">parent</a><span>|</span><a href="#36854887">next</a><span>|</span><label class="collapse" for="c-36853686">[-]</label><label class="expand" for="c-36853686">[3 more]</label></div><br/><div class="children"><div class="content">I just streamed this last night <a href="https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=TYgtG2Th6fI&amp;t=3998s">https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=TYgtG2Th6fI&amp;t=3998s</a><p>Iâve been live streaming myself fine tuning llama on my GitHub data (to code like me)</div><br/><div id="36854411" class="c"><input type="checkbox" id="c-36854411" checked=""/><div class="controls bullet"><span class="by">jeremycarter</span><span>|</span><a href="#36853313">root</a><span>|</span><a href="#36853686">parent</a><span>|</span><a href="#36854887">next</a><span>|</span><label class="collapse" for="c-36854411">[-]</label><label class="expand" for="c-36854411">[2 more]</label></div><br/><div class="children"><div class="content">Fantastic job! Very easy to follow</div><br/><div id="36854646" class="c"><input type="checkbox" id="c-36854646" checked=""/><div class="controls bullet"><span class="by">jawerty</span><span>|</span><a href="#36853313">root</a><span>|</span><a href="#36854411">parent</a><span>|</span><a href="#36854887">next</a><span>|</span><label class="collapse" for="c-36854646">[-]</label><label class="expand" for="c-36854646">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! I have some other streams where I do little projects like these check them out</div><br/></div></div></div></div></div></div><div id="36854887" class="c"><input type="checkbox" id="c-36854887" checked=""/><div class="controls bullet"><span class="by">notpublic</span><span>|</span><a href="#36853313">parent</a><span>|</span><a href="#36853686">prev</a><span>|</span><a href="#36853393">next</a><span>|</span><label class="collapse" for="c-36854887">[-]</label><label class="expand" for="c-36854887">[1 more]</label></div><br/><div class="children"><div class="content">As mentioned in the OP&#x27;s blog post, checkout <a href="https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;llama-recipes.git">https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;llama-recipes.git</a>. specifically files in ft_datasets directory.<p>I am able to finetune meta-llama&#x2F;Llama-2-13b-chat-hf on a 3090 using instructions from quickstart.ipynb.</div><br/></div></div><div id="36853393" class="c"><input type="checkbox" id="c-36853393" checked=""/><div class="controls bullet"><span class="by">samlhuillier</span><span>|</span><a href="#36853313">parent</a><span>|</span><a href="#36854887">prev</a><span>|</span><a href="#36854073">next</a><span>|</span><label class="collapse" for="c-36853393">[-]</label><label class="expand" for="c-36853393">[4 more]</label></div><br/><div class="children"><div class="content">Working on this now!</div><br/><div id="36853488" class="c"><input type="checkbox" id="c-36853488" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#36853313">root</a><span>|</span><a href="#36853393">parent</a><span>|</span><a href="#36854073">next</a><span>|</span><label class="collapse" for="c-36853488">[-]</label><label class="expand" for="c-36853488">[3 more]</label></div><br/><div class="children"><div class="content">Iâm looking forward to this! Are you using an adapter (I donât see it mentioned in your article)? I was under the impression you cannot fit 7B at 4 bit since itâll take 25GB of VRAM or so.</div><br/><div id="36854328" class="c"><input type="checkbox" id="c-36854328" checked=""/><div class="controls bullet"><span class="by">samlhuillier</span><span>|</span><a href="#36853313">root</a><span>|</span><a href="#36853488">parent</a><span>|</span><a href="#36854073">next</a><span>|</span><label class="collapse" for="c-36854328">[-]</label><label class="expand" for="c-36854328">[2 more]</label></div><br/><div class="children"><div class="content">Yes using the qlora adapter that hugging face provides with peft</div><br/><div id="36856763" class="c"><input type="checkbox" id="c-36856763" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#36853313">root</a><span>|</span><a href="#36854328">parent</a><span>|</span><a href="#36854073">next</a><span>|</span><label class="collapse" for="c-36856763">[-]</label><label class="expand" for="c-36856763">[1 more]</label></div><br/><div class="children"><div class="content">ahh, I was on my phone before so I must of glimpsed over it, I see it on the last section. Thanks!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36854073" class="c"><input type="checkbox" id="c-36854073" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#36853313">prev</a><span>|</span><a href="#36854007">next</a><span>|</span><label class="collapse" for="c-36854073">[-]</label><label class="expand" for="c-36854073">[7 more]</label></div><br/><div class="children"><div class="content">Is there any tutorial on how to use HuggingFace LLaMA 2-derived models? They don&#x27;t have checkpoint files of the original LLaMA and can&#x27;t be used by the Meta&#x27;s provided inference code, instead they use .bin files. I am only interested in Python code so no llama.cpp.</div><br/><div id="36854743" class="c"><input type="checkbox" id="c-36854743" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#36854073">parent</a><span>|</span><a href="#36857893">next</a><span>|</span><label class="collapse" for="c-36854743">[-]</label><label class="expand" for="c-36854743">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;d reconsider your rejection of llama.cpp if I were you. You can always call out to it from Python, but llama.cpp is by far the most active project in this space, and they&#x27;ve gotten the UX to the point where it&#x27;s extremely simple to use.<p>This user on HuggingFace has all the models ready to go in GGML format and quantized at various sizes, which saves a lot of bandwidth:<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke</a></div><br/><div id="36855302" class="c"><input type="checkbox" id="c-36855302" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#36854073">root</a><span>|</span><a href="#36854743">parent</a><span>|</span><a href="#36857893">next</a><span>|</span><label class="collapse" for="c-36855302">[-]</label><label class="expand" for="c-36855302">[3 more]</label></div><br/><div class="children"><div class="content">I understand, I use llama.cpp for my own personal stuff but can&#x27;t override the policy on the project I want to plug it in, which is python-only.</div><br/><div id="36855984" class="c"><input type="checkbox" id="c-36855984" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#36854073">root</a><span>|</span><a href="#36855302">parent</a><span>|</span><a href="#36857893">next</a><span>|</span><label class="collapse" for="c-36855984">[-]</label><label class="expand" for="c-36855984">[2 more]</label></div><br/><div class="children"><div class="content">There was a post yesterday about a 500 line single-file C implmenetation of llama2 with no dependencies. The llama2 architecture is hard coded. It shouldn&#x27;t be too hard to port to python.<p>Found the repo, couldn&#x27;t easily find the HN thread.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;llama2.c">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;llama2.c</a></div><br/><div id="36856421" class="c"><input type="checkbox" id="c-36856421" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#36854073">root</a><span>|</span><a href="#36855984">parent</a><span>|</span><a href="#36857893">next</a><span>|</span><label class="collapse" for="c-36856421">[-]</label><label class="expand" for="c-36856421">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s Andrej&#x27;s toy project, it won&#x27;t run 7B LLaMA.</div><br/></div></div></div></div></div></div></div></div><div id="36857893" class="c"><input type="checkbox" id="c-36857893" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#36854073">parent</a><span>|</span><a href="#36854743">prev</a><span>|</span><a href="#36855178">next</a><span>|</span><label class="collapse" for="c-36857893">[-]</label><label class="expand" for="c-36857893">[1 more]</label></div><br/><div class="children"><div class="content">If you want high performance inference us this: <a href="https:&#x2F;&#x2F;vllm.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;vllm.ai&#x2F;</a><p>For local batch stuff: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;main_classes&#x2F;pipelines" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;main_classes&#x2F;pipeli...</a></div><br/></div></div><div id="36855178" class="c"><input type="checkbox" id="c-36855178" checked=""/><div class="controls bullet"><span class="by">ramesh31</span><span>|</span><a href="#36854073">parent</a><span>|</span><a href="#36857893">prev</a><span>|</span><a href="#36854007">next</a><span>|</span><label class="collapse" for="c-36855178">[-]</label><label class="expand" for="c-36855178">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I am only interested in Python code so no llama.cpp.<p>llama cpp has python bindings: <a href="https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;llama-cpp-python&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;llama-cpp-python&#x2F;</a><p>Here&#x27;s using it with langchain: <a href="https:&#x2F;&#x2F;python.langchain.com&#x2F;docs&#x2F;integrations&#x2F;llms&#x2F;llamacpp" rel="nofollow noreferrer">https:&#x2F;&#x2F;python.langchain.com&#x2F;docs&#x2F;integrations&#x2F;llms&#x2F;llamacpp</a></div><br/></div></div></div></div><div id="36854007" class="c"><input type="checkbox" id="c-36854007" checked=""/><div class="controls bullet"><span class="by">m00dy</span><span>|</span><a href="#36854073">prev</a><span>|</span><label class="collapse" for="c-36854007">[-]</label><label class="expand" for="c-36854007">[3 more]</label></div><br/><div class="children"><div class="content">Which dataset would be good to fine-tune for developing sales assistant like chatbot ?</div><br/><div id="36855097" class="c"><input type="checkbox" id="c-36855097" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#36854007">parent</a><span>|</span><label class="collapse" for="c-36855097">[-]</label><label class="expand" for="c-36855097">[2 more]</label></div><br/><div class="children"><div class="content">You could try using a transcript of The Wolf of Wall Street, maybe throw in Glengarry Glen Ross for good measure?<p>&#x2F;s</div><br/><div id="36855591" class="c"><input type="checkbox" id="c-36855591" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#36854007">root</a><span>|</span><a href="#36855097">parent</a><span>|</span><label class="collapse" for="c-36855591">[-]</label><label class="expand" for="c-36855591">[1 more]</label></div><br/><div class="children"><div class="content">First prize is an 80 GB H100.  Second prize is a 4090.  Third prize is a PIP.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>