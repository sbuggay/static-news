<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1728464455038" as="style"/><link rel="stylesheet" href="styles.css?v=1728464455038"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2410.00907">Addition Is All You Need for Energy-Efficient Language Models</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>InvisibleUp</span> | <span>18 comments</span></div><br/><div><div id="41784968" class="c"><input type="checkbox" id="c-41784968" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41785674">next</a><span>|</span><label class="collapse" for="c-41784968">[-]</label><label class="expand" for="c-41784968">[12 more]</label></div><br/><div class="children"><div class="content">&gt; can potentially reduce 95% energy cost by elementwise floating point tensor multiplications and 80% energy cost of dot products<p>It this were about convolutional nets then optimizing compute would be a much bigger deal. Transformers are lightweight on compute and heavy on memory. The weakest link in the chain is fetching the model weights into the cores. The 95% and 80% energy reductions cited are for the multiplication operations in isolation, not for the entire inference process.</div><br/><div id="41785373" class="c"><input type="checkbox" id="c-41785373" checked=""/><div class="controls bullet"><span class="by">SuchAnonMuchWow</span><span>|</span><a href="#41784968">parent</a><span>|</span><a href="#41785043">next</a><span>|</span><label class="collapse" for="c-41785373">[-]</label><label class="expand" for="c-41785373">[1 more]</label></div><br/><div class="children"><div class="content">Its worse than that: the energy gains are when comparing computations made with fp32, but for fp8 the multipliers are really tiny and the adder&#x2F;shifters represent a largest part of the operators (energy-wise and area-wise) and this paper will only have small gains.<p>On fp8, the estimated gate count of fp8 multipliers is 296 vs. 157 with their technique, so the power gain on the multipliers will be much lower (50% would be a more reasonable estimation), but again for fp8 the additions in the dot products are a large part of the operations.<p>Overall, its really disingenuous to claim 80% power gain and small drop in accuracy, when the power gain is only for fp32 operations and the small drop in accuracy is only for fp8 operators. They don&#x27;t analyze the accuracy drop in fp32, and don&#x27;t present the power saved for fp8 dot product.</div><br/></div></div><div id="41785043" class="c"><input type="checkbox" id="c-41785043" checked=""/><div class="controls bullet"><span class="by">lifthrasiir</span><span>|</span><a href="#41784968">parent</a><span>|</span><a href="#41785373">prev</a><span>|</span><a href="#41785383">next</a><span>|</span><label class="collapse" for="c-41785043">[-]</label><label class="expand" for="c-41785043">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;m also sure that fp8 is small enough that multiplication can really be done in a much simpler circuit than larger fp formats. Even smaller formats like fp4 would be able to just use a lookup table, and that makes them more like sort-of-standardized quantization schemes.</div><br/><div id="41785195" class="c"><input type="checkbox" id="c-41785195" checked=""/><div class="controls bullet"><span class="by">tankenmate</span><span>|</span><a href="#41784968">root</a><span>|</span><a href="#41785043">parent</a><span>|</span><a href="#41785293">next</a><span>|</span><label class="collapse" for="c-41785195">[-]</label><label class="expand" for="c-41785195">[2 more]</label></div><br/><div class="children"><div class="content">i suspect that you could do fp8 with log tables and interpolation if you really wanted to (compared to the memory required for the model it&#x27;s peanuts), it just turns into a LUT (log table look up) and bit shift (interpolation). so again, memory bandwidth is the limiting factor for transformers (as far as energy is concerned).</div><br/><div id="41785267" class="c"><input type="checkbox" id="c-41785267" checked=""/><div class="controls bullet"><span class="by">lifthrasiir</span><span>|</span><a href="#41784968">root</a><span>|</span><a href="#41785195">parent</a><span>|</span><a href="#41785293">next</a><span>|</span><label class="collapse" for="c-41785267">[-]</label><label class="expand" for="c-41785267">[1 more]</label></div><br/><div class="children"><div class="content">This time though LUT exists in a circuit, which is much more efficient than typical memory lookup. Such LUT would have to exist per each ALU though, so it can&#x27;t be too large.</div><br/></div></div></div></div><div id="41785293" class="c"><input type="checkbox" id="c-41785293" checked=""/><div class="controls bullet"><span class="by">brilee</span><span>|</span><a href="#41784968">root</a><span>|</span><a href="#41785043">parent</a><span>|</span><a href="#41785195">prev</a><span>|</span><a href="#41785383">next</a><span>|</span><label class="collapse" for="c-41785293">[-]</label><label class="expand" for="c-41785293">[4 more]</label></div><br/><div class="children"><div class="content">fp4&#x2F;fp8 for neural networks don&#x27;t work the way you think they do - they are merely compression formats - a set of, say, 256 fp32 weights from 1 neuron are lossily turned into 1 max value (stored in fp32 precision) and 256 fp4&#x2F;fp8 numbers. Those compressed numbers are multiplied by the fp32 number at runtime to restore the original weights and full fp32 multiplication + additions are executed.</div><br/><div id="41785382" class="c"><input type="checkbox" id="c-41785382" checked=""/><div class="controls bullet"><span class="by">lifthrasiir</span><span>|</span><a href="#41784968">root</a><span>|</span><a href="#41785293">parent</a><span>|</span><a href="#41785422">next</a><span>|</span><label class="collapse" for="c-41785382">[-]</label><label class="expand" for="c-41785382">[1 more]</label></div><br/><div class="children"><div class="content">You are correct that the accumulation (i.e. additions in dot products) has to be done in a higher precision, however the multiplication can still be done via LUT. (Source: I currently work at a hardware-accelerated ML hardware startup.)</div><br/></div></div><div id="41785422" class="c"><input type="checkbox" id="c-41785422" checked=""/><div class="controls bullet"><span class="by">SuchAnonMuchWow</span><span>|</span><a href="#41784968">root</a><span>|</span><a href="#41785293">parent</a><span>|</span><a href="#41785382">prev</a><span>|</span><a href="#41785419">next</a><span>|</span><label class="collapse" for="c-41785422">[-]</label><label class="expand" for="c-41785422">[1 more]</label></div><br/><div class="children"><div class="content">The goal of this type of quantization is to move the multiplication by the fp32 rescale factor outside of the dot-product accumulation.<p>So the multiplications+additions are done on fp8&#x2F;int8&#x2F;int4&#x2F;whatever (when the hardware support those operators of course) and accumulated in a fp32 or similar, and only the final accumulator is multiplied by the rescale factor in fp32.</div><br/></div></div><div id="41785419" class="c"><input type="checkbox" id="c-41785419" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#41784968">root</a><span>|</span><a href="#41785293">parent</a><span>|</span><a href="#41785422">prev</a><span>|</span><a href="#41785383">next</a><span>|</span><label class="collapse" for="c-41785419">[-]</label><label class="expand" for="c-41785419">[1 more]</label></div><br/><div class="children"><div class="content">With w8a8 quantization the hw (&gt;= hopper) can do the heavy math in fp8 twice as fast as fp16.</div><br/></div></div></div></div></div></div><div id="41785383" class="c"><input type="checkbox" id="c-41785383" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#41784968">parent</a><span>|</span><a href="#41785043">prev</a><span>|</span><a href="#41785032">next</a><span>|</span><label class="collapse" for="c-41785383">[-]</label><label class="expand" for="c-41785383">[2 more]</label></div><br/><div class="children"><div class="content">That is true for single user&#x2F;light inference only. For training and batch inference you can get compute bound fast enough.</div><br/><div id="41785535" class="c"><input type="checkbox" id="c-41785535" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41784968">root</a><span>|</span><a href="#41785383">parent</a><span>|</span><a href="#41785032">next</a><span>|</span><label class="collapse" for="c-41785535">[-]</label><label class="expand" for="c-41785535">[1 more]</label></div><br/><div class="children"><div class="content">That really depends on what you&#x27;re doing. Trying to feed a tensor core is pretty hard–they&#x27;re really fast.</div><br/></div></div></div></div><div id="41785032" class="c"><input type="checkbox" id="c-41785032" checked=""/><div class="controls bullet"><span class="by">kendalf89</span><span>|</span><a href="#41784968">parent</a><span>|</span><a href="#41785383">prev</a><span>|</span><a href="#41785674">next</a><span>|</span><label class="collapse" for="c-41785032">[-]</label><label class="expand" for="c-41785032">[1 more]</label></div><br/><div class="children"><div class="content">Maybe this technique can be used for training then since that is a lot more compute intensive?</div><br/></div></div></div></div><div id="41785674" class="c"><input type="checkbox" id="c-41785674" checked=""/><div class="controls bullet"><span class="by">js8</span><span>|</span><a href="#41784968">prev</a><span>|</span><a href="#41785030">next</a><span>|</span><label class="collapse" for="c-41785674">[-]</label><label class="expand" for="c-41785674">[1 more]</label></div><br/><div class="children"><div class="content">Haven&#x27;t read it, but isn&#x27;t this just logarithmic tables in some form?<p>I am asking not to dismiss it, I genuinely feel I don&#x27;t understand logarithms on a fundamental level (of logic gates etc.). If multiplication can be replaced with table lookup and addition, then there has to be a circuit that gives you difficult addition and easy multiplication, or any combination of those tradeoffs.</div><br/></div></div><div id="41785030" class="c"><input type="checkbox" id="c-41785030" checked=""/><div class="controls bullet"><span class="by">CGamesPlay</span><span>|</span><a href="#41785674">prev</a><span>|</span><a href="#41785016">next</a><span>|</span><label class="collapse" for="c-41785030">[-]</label><label class="expand" for="c-41785030">[1 more]</label></div><br/><div class="children"><div class="content">I believe this reduces the compute required, but still uses 8 bits per value, so it does not reduce the memory requirements required to run inference, so it doesn’t particularly make the models more accessible for inference. Is this storage method suitable for training? That could potentially be an interesting application.</div><br/></div></div><div id="41785016" class="c"><input type="checkbox" id="c-41785016" checked=""/><div class="controls bullet"><span class="by">cpldcpu</span><span>|</span><a href="#41785030">prev</a><span>|</span><a href="#41784898">next</a><span>|</span><label class="collapse" for="c-41785016">[-]</label><label class="expand" for="c-41785016">[1 more]</label></div><br/><div class="children"><div class="content">It puzzles me that there does not seem to be a proper derivation and discussion of the error term in the paper. It&#x27;s all treated indirectly way inference results.</div><br/></div></div><div id="41784898" class="c"><input type="checkbox" id="c-41784898" checked=""/><div class="controls bullet"><span class="by">md_rumpf</span><span>|</span><a href="#41785016">prev</a><span>|</span><a href="#41785658">next</a><span>|</span><label class="collapse" for="c-41784898">[-]</label><label class="expand" for="c-41784898">[1 more]</label></div><br/><div class="children"><div class="content">The return of the CPU?!</div><br/></div></div><div id="41785658" class="c"><input type="checkbox" id="c-41785658" checked=""/><div class="controls bullet"><span class="by">scotty79</span><span>|</span><a href="#41784898">prev</a><span>|</span><label class="collapse" for="c-41785658">[-]</label><label class="expand" for="c-41785658">[1 more]</label></div><br/><div class="children"><div class="content">All You Need is Considered Harmful.</div><br/></div></div></div></div></div></div></div></body></html>