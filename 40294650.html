<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1715158874819" as="style"/><link rel="stylesheet" href="styles.css?v=1715158874819"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2405.04517">XLSTM: Extended Long Short-Term Memory</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>mauricesvp</span> | <span>18 comments</span></div><br/><div><div id="40295633" class="c"><input type="checkbox" id="c-40295633" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#40295189">next</a><span>|</span><label class="collapse" for="c-40295633">[-]</label><label class="expand" for="c-40295633">[1 more]</label></div><br/><div class="children"><div class="content">I like the color coded equations, I wish they would become a thing. We have syntax highlighting for programming languages, it&#x27;s time we have it for math too.</div><br/></div></div><div id="40295189" class="c"><input type="checkbox" id="c-40295189" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#40295633">prev</a><span>|</span><a href="#40295782">next</a><span>|</span><label class="collapse" for="c-40295189">[-]</label><label class="expand" for="c-40295189">[3 more]</label></div><br/><div class="children"><div class="content">For those who don&#x27;t know, the senior author on this paper (Sepp Hochreiter) was the first author on the original paper with Schmidhuber introducing LSTMs in 1997.</div><br/><div id="40295783" class="c"><input type="checkbox" id="c-40295783" checked=""/><div class="controls bullet"><span class="by">ramraj07</span><span>|</span><a href="#40295189">parent</a><span>|</span><a href="#40295393">next</a><span>|</span><label class="collapse" for="c-40295783">[-]</label><label class="expand" for="c-40295783">[1 more]</label></div><br/><div class="children"><div class="content">At least in biology, the first author of a paper is more often than not just a pair of gifted hands who did the experiments and plotted the graphs. Doesn’t always translate that they become good PIs later (though they get their chances from these papers).</div><br/></div></div></div></div><div id="40295782" class="c"><input type="checkbox" id="c-40295782" checked=""/><div class="controls bullet"><span class="by">beAbU</span><span>|</span><a href="#40295189">prev</a><span>|</span><a href="#40295210">next</a><span>|</span><label class="collapse" for="c-40295782">[-]</label><label class="expand" for="c-40295782">[1 more]</label></div><br/><div class="children"><div class="content">I thought this was some extension or enhancement to XSLT.</div><br/></div></div><div id="40295210" class="c"><input type="checkbox" id="c-40295210" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#40295782">prev</a><span>|</span><a href="#40295686">next</a><span>|</span><label class="collapse" for="c-40295210">[-]</label><label class="expand" for="c-40295210">[8 more]</label></div><br/><div class="children"><div class="content">It seems Sepp Hochreiter has talked already about this model since Oct 2023: <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;27011">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;27011</a><p>In the scaling law comparison, I wonder if it is reasonable to compare number of parameters between Llama, Mamba, RWKV, xLSTM? Isn&#x27;t compute time more relevant? E.g. in the figure about scaling laws, replace num of params by compute time.<p>Specifically, the sLSTM has still recurrence (memory mixing) in it, i.e. you cannot fully parallelize the computation. So scaling up Transformer could still look better when you look at compute time.<p>It seems neither the code nor the model params are released. I wonder if that will follow.</div><br/><div id="40295657" class="c"><input type="checkbox" id="c-40295657" checked=""/><div class="controls bullet"><span class="by">korbip</span><span>|</span><a href="#40295210">parent</a><span>|</span><a href="#40295288">next</a><span>|</span><label class="collapse" for="c-40295657">[-]</label><label class="expand" for="c-40295657">[3 more]</label></div><br/><div class="children"><div class="content">Disclaimer: I&#x27;m shared first author of this paper.<p>As a clarification: The speed for training will be on par with FlashAttention-2, when fully optimized and only including the mLSTM. For decoding&#x2F;inference both are very close to Mamba as xLSTM is a recurrent architecture. The sLSTM has memory mixing, that is state tracking capabilities, for problems Transformers and State Space Models (and any other sequence-parallelizable architecture) cannot solve fundamentally.</div><br/><div id="40295681" class="c"><input type="checkbox" id="c-40295681" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#40295210">root</a><span>|</span><a href="#40295657">parent</a><span>|</span><a href="#40295288">next</a><span>|</span><label class="collapse" for="c-40295681">[-]</label><label class="expand" for="c-40295681">[2 more]</label></div><br/><div class="children"><div class="content">Congratulations on the paper. That&#x27;s some very interesting work!<p>But you would want to include sLSTM as well to get the best performance, right? How does the speed compares in that case? Specifically when scaling up.</div><br/><div id="40295734" class="c"><input type="checkbox" id="c-40295734" checked=""/><div class="controls bullet"><span class="by">korbip</span><span>|</span><a href="#40295210">root</a><span>|</span><a href="#40295681">parent</a><span>|</span><a href="#40295288">next</a><span>|</span><label class="collapse" for="c-40295734">[-]</label><label class="expand" for="c-40295734">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! I can say that it is not really a diminishing factor at the scales reported in the paper. So, xLSTM[7:1] is pretty much on par with xLSTM[1:0] in speed.
We show that it is helpful on toy tasks, and it shows even better sequence extrapolation performance, so yes.</div><br/></div></div></div></div></div></div><div id="40295288" class="c"><input type="checkbox" id="c-40295288" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#40295210">parent</a><span>|</span><a href="#40295657">prev</a><span>|</span><a href="#40295686">next</a><span>|</span><label class="collapse" for="c-40295288">[-]</label><label class="expand" for="c-40295288">[4 more]</label></div><br/><div class="children"><div class="content">Recurrence is less of issue with really large models training than it is with medium sized models. Medium sized transformer models are generally not trained with sequence parallelism, but sequence parallelism is getting more common with transformer training. And sequence parallelism is same for transformer or recurrent model.<p>For really large models, it is in fact easier to achieve peak flops because computation required scales faster than memory bandwidth required(square vs cube).</div><br/><div id="40295331" class="c"><input type="checkbox" id="c-40295331" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#40295210">root</a><span>|</span><a href="#40295288">parent</a><span>|</span><a href="#40295686">next</a><span>|</span><label class="collapse" for="c-40295331">[-]</label><label class="expand" for="c-40295331">[3 more]</label></div><br/><div class="children"><div class="content">With sequence parallelism, you mean to increase the batch size, i.e. number of sequences in a batch?<p>&gt; Medium sized transformer models are generally not trained with sequence parallelism, but sequence parallelism is getting more common with transformer training<p>Is there some word missing? You mean it&#x27;s more common for large-sized Transformers?<p>&gt; computation required scales faster than memory bandwidth required (square vs cube)<p>That is an interesting thought. I&#x27;m trying to understand what exactly you mean. You mean, computation time is in O(N^2) where N is the sequence length, while required memory bandwidth is in O(N^3)? Why is that?</div><br/><div id="40295452" class="c"><input type="checkbox" id="c-40295452" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#40295210">root</a><span>|</span><a href="#40295331">parent</a><span>|</span><a href="#40295686">next</a><span>|</span><label class="collapse" for="c-40295452">[-]</label><label class="expand" for="c-40295452">[2 more]</label></div><br/><div class="children"><div class="content">No, it means dividing the sequence into multiple chunks and processing them one by one, very similar to recurrence. See [1]. Sequence parallelism is needed when the sequence can&#x27;t fit in a single GPU. Sequence parallelism is the hardest parallelism, but it is required for longer sequence. Many models just trains for smaller sequence length for majority of the training and switch to sequence parallelism for last few percentage of training.<p>[1]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2105.13120" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2105.13120</a></div><br/><div id="40295568" class="c"><input type="checkbox" id="c-40295568" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#40295210">root</a><span>|</span><a href="#40295452">parent</a><span>|</span><a href="#40295686">next</a><span>|</span><label class="collapse" for="c-40295568">[-]</label><label class="expand" for="c-40295568">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Sequence parallelism is the hardest parallelism, but it is required for longer sequence<p>In terms of difficulty of implementation it&#x27;s arguably much easier than pipeline parallelism, which I&#x27;d argue is the hardest kind (at least to implement it efficiently without bubbles), and takes the most lines of code to implement (especially in Jax, where sequence parallelism is almost trivial).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40295686" class="c"><input type="checkbox" id="c-40295686" checked=""/><div class="controls bullet"><span class="by">elygre</span><span>|</span><a href="#40295210">prev</a><span>|</span><a href="#40295529">next</a><span>|</span><label class="collapse" for="c-40295686">[-]</label><label class="expand" for="c-40295686">[1 more]</label></div><br/><div class="children"><div class="content">I have no idea about what this is, so going off topic:<p>The name XLSTM reminds me of the time in the late eighties when my university professor got accepted to hold a presentation on WOM: write-only memory.</div><br/></div></div><div id="40295255" class="c"><input type="checkbox" id="c-40295255" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#40295529">prev</a><span>|</span><label class="collapse" for="c-40295255">[-]</label><label class="expand" for="c-40295255">[2 more]</label></div><br/><div class="children"><div class="content">Another week, another paper that thinks they can revive recurrent networks. Although this time the father of LSTM is a co-author, so this paper should not come as a surprise. Sadly, the results seem to indicate that even by employing literally all tricks of the trade, their architecture can&#x27;t beat the throughput of flash-attention (not by a long shot, but that is not surprising for recurrent designs) and, on top of that, it is even slower than Mamba, which offers similar accuracy at lower cost. So my money is on this being another DOA architecture, like all the others we&#x27;ve seen this year already.</div><br/><div id="40295784" class="c"><input type="checkbox" id="c-40295784" checked=""/><div class="controls bullet"><span class="by">l33tman</span><span>|</span><a href="#40295255">parent</a><span>|</span><label class="collapse" for="c-40295784">[-]</label><label class="expand" for="c-40295784">[1 more]</label></div><br/><div class="children"><div class="content">To put another perspective on this, lots of modern advancements in both ML&#x2F;AI and especially computer graphics has come from ideas already from the 70-80s that were published, forgotten, and revived. Because underlying dependencies change, like the profile of the HW of the day. So just let the ideas flow, not every paper has to have an immediate payoff.</div><br/></div></div></div></div></div></div></div></div></div></body></html>