<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1703926864451" as="style"/><link rel="stylesheet" href="styles.css?v=1703926864451"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://davidgomes.com/understanding-parquet-iceberg-and-data-lakehouses-at-broad/">Understanding Parquet, Iceberg and Data Lakehouses</a> <span class="domain">(<a href="https://davidgomes.com">davidgomes.com</a>)</span></div><div class="subtext"><span>munchor</span> | <span>31 comments</span></div><br/><div><div id="38812825" class="c"><input type="checkbox" id="c-38812825" checked=""/><div class="controls bullet"><span class="by">twoodfin</span><span>|</span><a href="#38813199">next</a><span>|</span><label class="collapse" for="c-38812825">[-]</label><label class="expand" for="c-38812825">[10 more]</label></div><br/><div class="children"><div class="content">I often hear references to Apache Iceberg and Delta Lake as if they’re two peas in the Open Table Formats pod. Yet…<p>Here’s the Apache Iceberg table format specification:<p><a href="https:&#x2F;&#x2F;iceberg.apache.org&#x2F;spec&#x2F;" rel="nofollow">https:&#x2F;&#x2F;iceberg.apache.org&#x2F;spec&#x2F;</a><p>As they like to say in patent law, anyone “skilled in the art” of database systems could use this to build and query Iceberg tables without too much difficulty.<p>This is nominally the Delta Lake equivalent:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;delta-io&#x2F;delta&#x2F;blob&#x2F;master&#x2F;PROTOCOL.md">https:&#x2F;&#x2F;github.com&#x2F;delta-io&#x2F;delta&#x2F;blob&#x2F;master&#x2F;PROTOCOL.md</a><p>I defy anyone to even scope out what level of effort would be required to fully implement the current spec, let alone what would be involved in keeping up to date as this beast evolves.<p>Frankly, the Delta Lake spec reads like a reverse engineering of whatever implementation tradeoffs Databricks is making as they race to build out a lakehouse for every Fortune 1000 company burned by Hadoop (which is to say, most of them).<p>My point is that I’ve yet to be convinced that buying into Delta Lake is actually buying into an open ecosystem. Would appreciate any reassurance on this front!<p>Editing to append this GitHub history, which is unfortunately not reassuring:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;delta-io&#x2F;delta&#x2F;commits&#x2F;master&#x2F;PROTOCOL.md">https:&#x2F;&#x2F;github.com&#x2F;delta-io&#x2F;delta&#x2F;commits&#x2F;master&#x2F;PROTOCOL.md</a><p>Random features and tweaks just popping up, PR’d by Databricks engineers and promptly approved by Databricks senior engineers…</div><br/><div id="38813725" class="c"><input type="checkbox" id="c-38813725" checked=""/><div class="controls bullet"><span class="by">FridgeSeal</span><span>|</span><a href="#38812825">parent</a><span>|</span><a href="#38813077">next</a><span>|</span><label class="collapse" for="c-38813725">[-]</label><label class="expand" for="c-38813725">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I’ve yet to be convinced that buying into Delta Lake is actually buying into an open ecosystem. Would appreciate any reassurance on this front!<p>Sentiment echoed.<p>I’m ultra cautious of stuff offered by databricks in general. I think they’re only nominally open source, and shouldn’t be trusted.<p>I’ve also used Delta lake before, there were some really frustrating shortcomings and a lot of “sharp edges” in its usage. We ended up dropping that project entirely, but did investigate iceberg at the time as well. Iceberg and hudi had more coherently designed feature sets, but were less supported. Really hoping this changes more in future.</div><br/></div></div><div id="38813077" class="c"><input type="checkbox" id="c-38813077" checked=""/><div class="controls bullet"><span class="by">jamesblonde</span><span>|</span><a href="#38812825">parent</a><span>|</span><a href="#38813725">prev</a><span>|</span><a href="#38813071">next</a><span>|</span><label class="collapse" for="c-38813077">[-]</label><label class="expand" for="c-38813077">[2 more]</label></div><br/><div class="children"><div class="content">I agree with all of this. Databricks are also holding back features from open source Delta (like bloom filters), which is their right. But then you can&#x27;t claim it is a community-driven open format, unless it is an animal farm version of that, where one of the versions is the Pig (some are more equal than others).</div><br/><div id="38813806" class="c"><input type="checkbox" id="c-38813806" checked=""/><div class="controls bullet"><span class="by">hacful-tonteg</span><span>|</span><a href="#38812825">root</a><span>|</span><a href="#38813077">parent</a><span>|</span><a href="#38813071">next</a><span>|</span><label class="collapse" for="c-38813806">[-]</label><label class="expand" for="c-38813806">[1 more]</label></div><br/><div class="children"><div class="content">Databricks has a lot of nice closed-sourced components, e.g., Unity Catalog, Delta Live Tables and Photon (a C++ implementation of Spark).<p>Delta itself seems fairly open-source: <a href="https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;delta-io&#x2F;projects&#x2F;10&#x2F;views&#x2F;1">https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;delta-io&#x2F;projects&#x2F;10&#x2F;views&#x2F;1</a> and hopefully someone will implement Liquid Clustering!</div><br/></div></div></div></div><div id="38813071" class="c"><input type="checkbox" id="c-38813071" checked=""/><div class="controls bullet"><span class="by">snthpy</span><span>|</span><a href="#38812825">parent</a><span>|</span><a href="#38813077">prev</a><span>|</span><a href="#38813199">next</a><span>|</span><label class="collapse" for="c-38813071">[-]</label><label class="expand" for="c-38813071">[6 more]</label></div><br/><div class="children"><div class="content">Thanks for this. I&#x27;ve been following this space for about a year or two and was wondering why Iceberg was more popular in open source.<p>Over the past six months I got the impression that Delta is pulling ahead in the race as Iceberg is struggling to provide tools for people not in the JVM ecosystem. Delta is a lot more accessible in that way.</div><br/><div id="38813679" class="c"><input type="checkbox" id="c-38813679" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#38812825">root</a><span>|</span><a href="#38813071">parent</a><span>|</span><a href="#38813084">next</a><span>|</span><label class="collapse" for="c-38813679">[-]</label><label class="expand" for="c-38813679">[1 more]</label></div><br/><div class="children"><div class="content">Snowflake is rolling out Iceberg support and not Delta support, I think that says a lot.</div><br/></div></div><div id="38813084" class="c"><input type="checkbox" id="c-38813084" checked=""/><div class="controls bullet"><span class="by">jamesblonde</span><span>|</span><a href="#38812825">root</a><span>|</span><a href="#38813071">parent</a><span>|</span><a href="#38813679">prev</a><span>|</span><a href="#38813090">next</a><span>|</span><label class="collapse" for="c-38813084">[-]</label><label class="expand" for="c-38813084">[3 more]</label></div><br/><div class="children"><div class="content">I guess you are referring to delta-rs (for Python in particular). An interesting factoid here is that Databricks started delta-rs, and other companies are now driving it forward - not Databricks. I guess it is not in Databricks interest to push the non JVM ecosystem.
PyIceberg is catching up. Write support is almost there -
<a href="https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;iceberg-python&#x2F;pull&#x2F;41">https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;iceberg-python&#x2F;pull&#x2F;41</a></div><br/><div id="38813270" class="c"><input type="checkbox" id="c-38813270" checked=""/><div class="controls bullet"><span class="by">alexott</span><span>|</span><a href="#38812825">root</a><span>|</span><a href="#38813084">parent</a><span>|</span><a href="#38813090">next</a><span>|</span><label class="collapse" for="c-38813270">[-]</label><label class="expand" for="c-38813270">[2 more]</label></div><br/><div class="children"><div class="content">As I remember, delta-rs was started by Scribd, not by Databricks: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;2jgfpJD5D6U" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;2jgfpJD5D6U</a>, <a href="https:&#x2F;&#x2F;youtu.be&#x2F;scYz12UK-OY" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;scYz12UK-OY</a></div><br/><div id="38813460" class="c"><input type="checkbox" id="c-38813460" checked=""/><div class="controls bullet"><span class="by">jamesblonde</span><span>|</span><a href="#38812825">root</a><span>|</span><a href="#38813270">parent</a><span>|</span><a href="#38813090">next</a><span>|</span><label class="collapse" for="c-38813460">[-]</label><label class="expand" for="c-38813460">[1 more]</label></div><br/><div class="children"><div class="content">I stand corrected, then.</div><br/></div></div></div></div></div></div><div id="38813090" class="c"><input type="checkbox" id="c-38813090" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#38812825">root</a><span>|</span><a href="#38813071">parent</a><span>|</span><a href="#38813084">prev</a><span>|</span><a href="#38813199">next</a><span>|</span><label class="collapse" for="c-38813090">[-]</label><label class="expand" for="c-38813090">[1 more]</label></div><br/><div class="children"><div class="content">DuckDB (lightweight, non-JVM, many language bindings) supports querying from Iceberg now.<p><a href="https:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;extensions&#x2F;iceberg.html" rel="nofollow">https:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;extensions&#x2F;iceberg.html</a><p>You still need Spark to generate the Iceberg metadata though.</div><br/></div></div></div></div></div></div><div id="38813199" class="c"><input type="checkbox" id="c-38813199" checked=""/><div class="controls bullet"><span class="by">benjaminwootton</span><span>|</span><a href="#38812825">prev</a><span>|</span><a href="#38813680">next</a><span>|</span><label class="collapse" for="c-38813199">[-]</label><label class="expand" for="c-38813199">[2 more]</label></div><br/><div class="children"><div class="content">This is a big deal in the database world as delta, iceberg and hudi mean that data is being stored in an open source format, often on S3.<p>It means that the storage and much of the processing is being standrdised so that you can move between databases easily and almost all tools will eventually be able to work with the same set of files in a transactionally sound way.<p>For instance, Snowflake could be writing to a file, a data scientist could be querying the data live from a Jupyter notebook, and ClickHouse could be serving user facing analytics against the same data with consistency guarantees.<p>If the business then decide to switch Snowflake to Databricks then it isn’t such a big deal.<p>Right now it isn’t quite as fast to query these formats on S3 as a native ingestion would be, but every database vendor will be forced by the market to optimise for performance such that they tend towards the performance of natively ingested data.<p>It’s a great win for openness and open source and for businesses to have their data in open and portable formats.<p>Lakehouse has the same implications.  Lots of companies have data lakes and data warehouses and end up copying data between the two.  To query the same set of data and have just one system to manage is equally impactful.<p>It’s a very interesting time to be in the data engineering world.</div><br/><div id="38813615" class="c"><input type="checkbox" id="c-38813615" checked=""/><div class="controls bullet"><span class="by">pjot</span><span>|</span><a href="#38813199">parent</a><span>|</span><a href="#38813680">next</a><span>|</span><label class="collapse" for="c-38813615">[-]</label><label class="expand" for="c-38813615">[1 more]</label></div><br/><div class="children"><div class="content">Apache Arrow and Substrait have been working towards making this a reality. 
I see a future where executing a query can&#x2F;will send plans to many different engines distributed across the cloud, but also locally on your on machine.</div><br/></div></div></div></div><div id="38813680" class="c"><input type="checkbox" id="c-38813680" checked=""/><div class="controls bullet"><span class="by">alentred</span><span>|</span><a href="#38813199">prev</a><span>|</span><a href="#38812687">next</a><span>|</span><label class="collapse" for="c-38813680">[-]</label><label class="expand" for="c-38813680">[1 more]</label></div><br/><div class="children"><div class="content">I am very excited about Iceberg specifically (because open-source), but the last time I looked into it the only implementation was a Spark library, and Trino&#x27;s (formerly Presto, an SQL engine) Iceberg connector had a hard dependency on Hive! It is like the entire industry had a hard time divorcing its MapReduce, Hive, and dare I to say Spark, legacy.<p>I didn&#x27;t look into Iceberg since, but plan to, and I am really looking forward for this to develop. We have the tools and the compute power today to deal with data without legacy tech, and not all data is big data either. Consequently &quot;data engineering&quot;, thankfully, resembles the regular back-end development more and more, with its regular development practices being put in place.<p>So, here is to the hope of having a pure Python Iceberg lib some day very soon!</div><br/></div></div><div id="38812687" class="c"><input type="checkbox" id="c-38812687" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#38813680">prev</a><span>|</span><a href="#38813101">next</a><span>|</span><label class="collapse" for="c-38812687">[-]</label><label class="expand" for="c-38812687">[1 more]</label></div><br/><div class="children"><div class="content">Great article. I&#x27;ve worked with Parquet files on S3 for years, but I didn&#x27;t quite understand what Iceberg was, but the article explained it well. It&#x27;s a database metadata format for an underlying set of data which describes its schema, partitioning etc.<p>Most people use Hive partitioning convention (i.e. directory names like &#x2F;key3=000&#x2F;key2=002&#x2F;) but Iceberg goes farther than this by exposing even more structure to the query engine.<p>In a traditional DBMS like Postgres, the schema, the query engine and the storage format come  as a single package.<p>But with big data, we&#x27;re building database components from scratch, and we can mix and match. We can use Iceberg as a metadata format, DuckDB as the query engine, Parquet as the storage format, and S3 as the storage medium.</div><br/></div></div><div id="38813101" class="c"><input type="checkbox" id="c-38813101" checked=""/><div class="controls bullet"><span class="by">jamesblonde</span><span>|</span><a href="#38812687">prev</a><span>|</span><a href="#38812891">next</a><span>|</span><label class="collapse" for="c-38813101">[-]</label><label class="expand" for="c-38813101">[1 more]</label></div><br/><div class="children"><div class="content">I disagree with this strongly - 
&quot;The best way to store Apache Arrow dataframes in files on disk is with Feather. However, it’s also possible to convert to Apache Parquet format and others.&quot;<p>The best way to build your own non-JVM lakehouse is to use Iceberg for metadata, Parquet for the Data, Query with DuckDB using Arrow tables (read Parquet directly into Arrow is very low cost), and then use Arrow-&gt;Pandas or Polars (either directly or via a service with Arrow Flight).<p>If you put Feather in the mix, the whole Python lakehouse stack doesn&#x27;t currently work.</div><br/></div></div><div id="38812891" class="c"><input type="checkbox" id="c-38812891" checked=""/><div class="controls bullet"><span class="by">debo_</span><span>|</span><a href="#38813101">prev</a><span>|</span><a href="#38813470">next</a><span>|</span><label class="collapse" for="c-38812891">[-]</label><label class="expand" for="c-38812891">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve heard of data lakes, but &quot;data lakehouse&quot; sounds like where upper class data goes in the summer to take their data-boats data-fishing.</div><br/><div id="38813252" class="c"><input type="checkbox" id="c-38813252" checked=""/><div class="controls bullet"><span class="by">benjaminwootton</span><span>|</span><a href="#38812891">parent</a><span>|</span><a href="#38813470">next</a><span>|</span><label class="collapse" for="c-38813252">[-]</label><label class="expand" for="c-38813252">[2 more]</label></div><br/><div class="children"><div class="content">The name is easy to poke fun at, but I think it’s a real problem.  A lot of companies use data lakes to store data and warehouses to serve BI to tools like Tableau or PowerBI.  They then up copying data between the two.<p>Querying a lake directly and having transactions, governance etc against one set of data (a data Lakehouse) can really simplify the stack and take out cost.</div><br/></div></div></div></div><div id="38813470" class="c"><input type="checkbox" id="c-38813470" checked=""/><div class="controls bullet"><span class="by">lmeyerov</span><span>|</span><a href="#38812891">prev</a><span>|</span><a href="#38812764">next</a><span>|</span><label class="collapse" for="c-38813470">[-]</label><label class="expand" for="c-38813470">[2 more]</label></div><br/><div class="children"><div class="content">We have been excited to dig into the Iceberg era of more managed parquet storage... But they are still years behind on supporting fast GPU IO (GPUDirect&#x2F;cuFile). So every time we look at bringing them to a customer for powering AI workloads... We hit that wall.<p>It seems inevitable, more of a when vs if. Being able to have our cake &amp; eat it too will be very cool :)</div><br/><div id="38813509" class="c"><input type="checkbox" id="c-38813509" checked=""/><div class="controls bullet"><span class="by">jamesblonde</span><span>|</span><a href="#38813470">parent</a><span>|</span><a href="#38812764">next</a><span>|</span><label class="collapse" for="c-38813509">[-]</label><label class="expand" for="c-38813509">[1 more]</label></div><br/><div class="children"><div class="content">For what use case? Image data storage?
For text storage, Parquet is good enough today.
PyTorch Data Loader and TF Data provide multi-threaded clients that read ahead in parallel and fill up an in-memory buffer that is then transferred in&#x2F;out from GPUs. I agree that S3 can be a bottleneck here. That&#x27;s why we have HopsFS as a global distributed coherent NVMe cache over S3. Anyscale have been doing something similar with a local NVMe cache for S3.
Another interesting file format is Lance - it&#x27;s like Parquet, but for image data. It has an additional index for fast random I&#x2F;O within a file (to find images).</div><br/></div></div></div></div><div id="38812764" class="c"><input type="checkbox" id="c-38812764" checked=""/><div class="controls bullet"><span class="by">albert_e</span><span>|</span><a href="#38813470">prev</a><span>|</span><a href="#38813373">next</a><span>|</span><label class="collapse" for="c-38812764">[-]</label><label class="expand" for="c-38812764">[2 more]</label></div><br/><div class="children"><div class="content">Sorry genuine question -- what does the phrase &quot;at Broad&quot; at the end of the blog  post&#x27;s title mean or refer to? Maybe a phrase that I am unfamiliar with? I first wondered if it is the name of an organization or team -- and this post is describing what they did in that team, but that doesn&#x27;t seem to be the case?<p>&gt;&gt; Understanding Parquet, Iceberg and Data Lakehouses at Broad</div><br/><div id="38812810" class="c"><input type="checkbox" id="c-38812810" checked=""/><div class="controls bullet"><span class="by">chomp5977</span><span>|</span><a href="#38812764">parent</a><span>|</span><a href="#38813373">next</a><span>|</span><label class="collapse" for="c-38812810">[-]</label><label class="expand" for="c-38812810">[1 more]</label></div><br/><div class="children"><div class="content">Not in depth, general understanding</div><br/></div></div></div></div><div id="38813373" class="c"><input type="checkbox" id="c-38813373" checked=""/><div class="controls bullet"><span class="by">mulmen</span><span>|</span><a href="#38812764">prev</a><span>|</span><a href="#38813052">next</a><span>|</span><label class="collapse" for="c-38813373">[-]</label><label class="expand" for="c-38813373">[1 more]</label></div><br/><div class="children"><div class="content">How do dependencies work in this type of data lakehouse?  Does the orchestration layer handle that or is there metadata within the data lake that provides completeness information?</div><br/></div></div><div id="38813052" class="c"><input type="checkbox" id="c-38813052" checked=""/><div class="controls bullet"><span class="by">alexott</span><span>|</span><a href="#38813373">prev</a><span>|</span><a href="#38813197">next</a><span>|</span><label class="collapse" for="c-38813052">[-]</label><label class="expand" for="c-38813052">[1 more]</label></div><br/><div class="children"><div class="content">Unity Catalog isn’t comparable with Iceberg Catalogs. It’s not required for Delta to function…<p>There was a paper at VLDB about Delta Lake: <a href="https:&#x2F;&#x2F;www.vldb.org&#x2F;pvldb&#x2F;vol13&#x2F;p3411-armbrust.pdf" rel="nofollow">https:&#x2F;&#x2F;www.vldb.org&#x2F;pvldb&#x2F;vol13&#x2F;p3411-armbrust.pdf</a> - it describes why it was created, plus details of implementation.</div><br/></div></div><div id="38813197" class="c"><input type="checkbox" id="c-38813197" checked=""/><div class="controls bullet"><span class="by">Boxxed</span><span>|</span><a href="#38813052">prev</a><span>|</span><a href="#38813703">next</a><span>|</span><label class="collapse" for="c-38813197">[-]</label><label class="expand" for="c-38813197">[3 more]</label></div><br/><div class="children"><div class="content">One thing I&#x27;m confused about is why does Iceberg need a spark deployment to function? Or am I wrong about that? I would rather avoid that ecosystem if I can.</div><br/><div id="38813479" class="c"><input type="checkbox" id="c-38813479" checked=""/><div class="controls bullet"><span class="by">jamesblonde</span><span>|</span><a href="#38813197">parent</a><span>|</span><a href="#38813238">next</a><span>|</span><label class="collapse" for="c-38813479">[-]</label><label class="expand" for="c-38813479">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need a Spark deployment. The first reference implementations for reading and writing were in Spark.<p>Now, with PyIceberg, there is read support in Python. Write support should be merged very soon - <a href="https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;iceberg-python&#x2F;pull&#x2F;41">https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;iceberg-python&#x2F;pull&#x2F;41</a>
So, very soon, you will be able to read&#x2F;write Iceberg tables in Python.
I look forward to doing data transformations in Polars for data of reasonable scale (up to 100GB or so) and writing to Iceberg tables with PyIceberg. No Spark.</div><br/></div></div><div id="38813238" class="c"><input type="checkbox" id="c-38813238" checked=""/><div class="controls bullet"><span class="by">benjaminwootton</span><span>|</span><a href="#38813197">parent</a><span>|</span><a href="#38813479">prev</a><span>|</span><a href="#38813703">next</a><span>|</span><label class="collapse" for="c-38813238">[-]</label><label class="expand" for="c-38813238">[1 more]</label></div><br/><div class="children"><div class="content">It tends to be more library dependencies than live clusters.<p>A lot of data lakes are managed using Hadoop and Spark so I think it’s just an artefact of that.<p>In the end I can’t see why you wouldn’t just be able to create and manage Iceberg files directly from a standard Python&#x2F;JS&#x2F;Java without that legacy.</div><br/></div></div></div></div><div id="38813703" class="c"><input type="checkbox" id="c-38813703" checked=""/><div class="controls bullet"><span class="by">meehai</span><span>|</span><a href="#38813197">prev</a><span>|</span><a href="#38813623">next</a><span>|</span><label class="collapse" for="c-38813703">[-]</label><label class="expand" for="c-38813703">[1 more]</label></div><br/><div class="children"><div class="content">can confirm that it is a nice thing to work with parquet files. Before this, we&#x27;ve worked for ~1 year with CSVs (I know the horror) and we made an effort to port all the &#x27;legacy&#x27; code to Parquet files<p>We interface with BigQuery (via Airflow) mostly, and except one very annoying situation it&#x27;s a big improvement in terms of speed (parsing floats after querying the DB is NEVER a good option).<p>---<p>In case anyone&#x27;s wondering, it&#x27;s basically storing and loading native numpy arrays in BigQuery via the python client(s).<p>You have a bunch of options (assume you have one or more cols with float32 numpy arrays):<p>- dataframe -&gt; to_parquet -&gt; upload to GCS -&gt; GCSToBigQueryOperator (<a href="https:&#x2F;&#x2F;airflow.apache.org&#x2F;docs&#x2F;apache-airflow-providers-google&#x2F;stable&#x2F;_api&#x2F;airflow&#x2F;providers&#x2F;google&#x2F;cloud&#x2F;transfers&#x2F;gcs_to_bigquery&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;airflow.apache.org&#x2F;docs&#x2F;apache-airflow-providers-goo...</a>)<p><pre><code>  -&gt; instead of storing as a `FLOAT, REPEATED` it will be stored as a STRUCT with a structure of `list&gt;item` OR `list&gt;element` (pyarrow==11 OR pyarrow==13).This requires a manual parsing from this &#x27;json structure&#x27; that you get when querying the DB back to np.array -&gt; slow and basically you are using CSVs again.

  -&gt; Read more: https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;68303327&#x2F;unnecessary-list-item-nesting-in-bigquery-schemas-from-pyarrow-upload-dataframe

  -&gt; set the schema before uploading? Nope, all values will uploaded as null in BQ.
</code></pre>
- dataframe -&gt; bigquery.Client -&gt; upload the dataframe from python<p><pre><code>  - very slow, you need to batch your data (imagine 24h vs 5 minutes kind of slow as dataframe sizes increase + necessity to keep all data in memory or batch it so extra save&#x2F;load of each batch before uploading)

  - arrays are stored properly
</code></pre>
- solution: you must do 2 things, one on the pyarrow side and one on the BigQuery side<p><pre><code>  - `df.to_parquet(..., use_compliant_nested_type=True)` (in pyarrow==14 it&#x27;s True by default, but airflow needs pyarrow==11, where it&#x27;s False by default)

  - use `enable_list_inference=True` (link: https:&#x2F;&#x2F;cloud.google.com&#x2F;bigquery&#x2F;docs&#x2F;loading-data-cloud-storage-parquet#list_logical_type)

  - when both of this are true (i.e. save parquet files [to GCS] using that flag and load parquet files [from GCS to BQ] using the other flag arrays can be stored as (FLOAT, REPEATED) and queried as numpy arrays out of the box without any manual management.
</code></pre>
This took me like 1 week of debugging and reading source code, obscure SO comments and GH issues etc.</div><br/></div></div><div id="38813623" class="c"><input type="checkbox" id="c-38813623" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#38813703">prev</a><span>|</span><label class="collapse" for="c-38813623">[-]</label><label class="expand" for="c-38813623">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s really easy to get lost in the technical jargon that the <i>vendors</i> who are <i>selling products</i> throw around, but this article has missed the important part, and spent all the time talking about the relatively unimportant part (data formats).<p>You need to step back and look from a broader perspective to understand this domain.<p>Talking about arrow&#x2F;parquet&#x2F;iceberg is like talking about InnoDB vs MyISAM when you&#x27;re talking about databases; yes, those are technically storage engines for mysql&#x2F;mariadb, but no, you probably do not care about them until you need them, and you most certainly do not care about them when you want to understand what a relational DB vs. an no-SQL db are.<p>They are <i>technical details</i>.<p>...<p>So, if you step back, what you need to read about is <i>STAR SCHEMAS</i>. Here are some links (1), (2).<p>This is what people used to be before data lakes.<p>So the tldr: you have a big database which contains <i>condensed and annotated</i> versions of your data, which is easy to query, and structured in a way that is suitable for visualization tools such as PowerBI, Tableau, MicroStrategy (ugh, but people do use it), etc. to use.<p>This means you can generate <i>reports and insights</i> from your data.<p>Great.<p>...the problem is that generating this structured data from absolutely massive amounts of unstructured data involves a truly colossal amount of engineering work; and it&#x27;s never realtime.<p>That&#x27;s because the process of turning <i>raw data</i> into <i>a star schema</i> was traditionally done via ETL tools that were slow and terrible. &#x27;Were&#x27;. These tools are still slow and terrible.<p>Basically, the output you get is very valuable, but <i>getting it</i> is very difficult, very expensive and both of those problems scale as the data size scales.<p>So...<p>Datalakes.<p>Datalakes are the solution to this problem; you don&#x27;t transform the data. You just injest it and store it, basically raw, and <i>on the fly</i> when you need the data for something, you can process it.<p>The idea was something like a dependency graph; what if, instead of processing all your data every day&#x2F;hour&#x2F;whatever, you defined what data you needed, and then when you need it, you rebuild just that part of the database.<p>Certainly you don&#x27;t get the nice star schema, but... you can handle a lot of data, and what you need to do process it &#x27;adhoc&#x27; is pretty trivial mostly, so you don&#x27;t need a huge engineering effort to support it; you just need some smart <i>table formats</i>, a <i>lot of storage</i> and on-demand compute.<p>...Great?<p>No. Totally rubbish.<p>Turn out this is a stupid idea, and what you get is a lot of data you can&#x27;t get any insights from.<p>So, along come the &#x27;nextgen&#x27; batch of BI companies like databricks so they invent this idea of a &#x27;lake house&#x27; (3), (4).<p>What is it? Take a wild guess. I&#x27;ll give you a hint: having no tables was a stupid idea.<p>Yes! Correct, they&#x27;ve invented a layer that sits on top of a data lake that presents a &#x27;virtual database&#x27; with ACID transactions that you then build a star schema in&#x2F;on.<p>Since the underlying implementation is (magic here, etc. etc. technical details) this approach supports output in the form we originally had (structured data suitable for analytics tools), but it has some nice features like streaming, etc. that make it capable of handling very large volumes of data; but it&#x27;s not a &#x27;real&#x27; database, so it does have some limitations which are difficult to resolve (like security and RBAC).<p>...<p>Of course, the promise, that you just pour all your data in and &#x27;magic!&#x27; you have insights, is still just as much nonsense as it ever was.<p>If you use any of these tools now, you&#x27;ll see that they require you to transform your data; usually as some kind of batch process.<p>If you closed your eyes and said &quot;ETL?&quot;, you&#x27;d win a cookie.<p>All a &#x27;lake house&#x27; is, is a traditional BI data warehouse built on a different type of database.<p>Almost without exception, everything else is marketing fluff.<p>* exception: kafka and streaming is actually fundamentally different for real time aggregated metrics, but its also fabulously difficult to do well, so most people still don&#x27;t, as far as I&#x27;m aware.<p>...and I&#x27;ll go out on a limb here and say really, you probably do not care if your implementation uses delta tables or iceberg; that&#x27;s an implementation detail.<p>I <i>guarantee</i> that correctly understanding your domain data and modelling a form of it suitable for reporting and insights is more important and more valuable than what storage engine you use.<p>[1] - <a href="https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;power-bi&#x2F;guidance&#x2F;star-schema" rel="nofollow">https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;power-bi&#x2F;guidance&#x2F;star-sch...</a>
[2] - <a href="https:&#x2F;&#x2F;www.kimballgroup.com&#x2F;data-warehouse-business-intelligence-resources&#x2F;books&#x2F;data-warehouse-dw-toolkit&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.kimballgroup.com&#x2F;data-warehouse-business-intelli...</a><p>[3] - <a href="https:&#x2F;&#x2F;www.snowflake.com&#x2F;guides&#x2F;what-data-lakehouse" rel="nofollow">https:&#x2F;&#x2F;www.snowflake.com&#x2F;guides&#x2F;what-data-lakehouse</a>
[4] - <a href="https:&#x2F;&#x2F;www.databricks.com&#x2F;glossary&#x2F;data-lakehouse" rel="nofollow">https:&#x2F;&#x2F;www.databricks.com&#x2F;glossary&#x2F;data-lakehouse</a></div><br/><div id="38813650" class="c"><input type="checkbox" id="c-38813650" checked=""/><div class="controls bullet"><span class="by">xvector</span><span>|</span><a href="#38813623">parent</a><span>|</span><label class="collapse" for="c-38813650">[-]</label><label class="expand" for="c-38813650">[1 more]</label></div><br/><div class="children"><div class="content">As someone that came away confused after reading the article, this was hugely helpful - thanks.</div><br/></div></div></div></div></div></div></div></div></div></body></html>