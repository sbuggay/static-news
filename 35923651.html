<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1683968448910" as="style"/><link rel="stylesheet" href="styles.css?v=1683968448910"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.quantamagazine.org/ai-like-chatgpt-are-no-good-at-not-20230512/">Language models still struggle with the concept of negation</a> <span class="domain">(<a href="https://www.quantamagazine.org">www.quantamagazine.org</a>)</span></div><div class="subtext"><span>belter</span> | <span>85 comments</span></div><br/><div><div id="35924149" class="c"><input type="checkbox" id="c-35924149" checked=""/><div class="controls bullet"><span class="by">williamcotton</span><span>|</span><a href="#35924567">next</a><span>|</span><label class="collapse" for="c-35924149">[-]</label><label class="expand" for="c-35924149">[29 more]</label></div><br/><div class="children"><div class="content">What animals don’t have paws or lay eggs, but have wings? Work it out, step by step.<p>ChatGPT (GPT-4):<p>Sure, let&#x27;s go through this step by step:<p>First, you asked for animals that don&#x27;t have paws. This would exclude mammals such as dogs, cats, bears, and so on, as these species have paws.<p>Second, you specified animals that don&#x27;t lay eggs. This eliminates a wide range of animals including all birds, most reptiles, and some fish and insects, as they all lay eggs.<p>Finally, you asked for animals that have wings. Wings are typically found in birds, bats, and insects.<p>Given these three criteria, we can see that we&#x27;re in a bit of a conundrum. Most winged creatures either have paws (like bats) or lay eggs (like birds and insects).<p>However, if we consider the term &quot;wings&quot; more broadly, there are some species of bats that give live birth (don&#x27;t lay eggs) and technically don&#x27;t have &quot;paws&quot; but rather winged limbs adapted for flight.<p>So, based on the constraints of your question, the best fit would be certain species of bats, provided we interpret &quot;wings&quot; in a broad sense and &quot;paws&quot; in a strict sense.</div><br/><div id="35924376" class="c"><input type="checkbox" id="c-35924376" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#35924149">parent</a><span>|</span><a href="#35926131">next</a><span>|</span><label class="collapse" for="c-35924376">[-]</label><label class="expand" for="c-35924376">[16 more]</label></div><br/><div class="children"><div class="content">While the answer landed on bats, ChatGPT kind of trips itself up in its explanations:<p>1. &quot;Most winged creatures either have paws (like bats) or lay eggs (like birds and insects)&quot; - but bats don&#x27;t have paws.<p>2. &quot;However, if we consider the term &quot;wings&quot; more broadly&quot; - why do we need to consider the term &quot;wings&quot; more broadly? Nobody is arguing there is some definition of &quot;wing&quot; that would exclude bats. Bat wings evolved from largely the same structures that bird wings did.<p>3. &quot;there are some species of bats that give live birth&quot; - <i>all</i> species of bats give live birth - stating it like this makes it sound like there are some species of bats that lay eggs or something.<p>4. &quot;...provided we interpret &quot;wings&quot; in a broad sense and &quot;paws&quot; in a strict sense.&quot; Again, no &quot;broad&quot; or &quot;strict&quot; senses are needed. Bats have wings and no paws.<p>To be clear, I still think ChatGPT is amazing, but these kinds of things highlight some of the differences between LLMs and human cognition.</div><br/><div id="35924584" class="c"><input type="checkbox" id="c-35924584" checked=""/><div class="controls bullet"><span class="by">williamcotton</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924376">parent</a><span>|</span><a href="#35924683">next</a><span>|</span><label class="collapse" for="c-35924584">[-]</label><label class="expand" for="c-35924584">[3 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a sample-and-poll approach that I&#x27;ve used with factual question-and-answer based tools that can help with this kind of thing as well.<p>First, you&#x27;ll notice the added &quot;work it out step by step&quot; to encourage what is called chain-of-thought in the literature. Second, generate say, five responses from the LLM, then take all five responses and uses them in another prompt where you ask the LLM to reflect on all of the five responses, a process being called reflection. Then you take all of that, including the original prompts, and in another separate prompt ask it to figure out the best response, a process known as resolving. You could end it with another prompt to remove the step-by-step work and just return a succinct answer.<p>Credit to <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=wVzuvf9D9BU">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=wVzuvf9D9BU</a></div><br/><div id="35925161" class="c"><input type="checkbox" id="c-35925161" checked=""/><div class="controls bullet"><span class="by">QuadrupleA</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924584">parent</a><span>|</span><a href="#35924683">next</a><span>|</span><label class="collapse" for="c-35925161">[-]</label><label class="expand" for="c-35925161">[2 more]</label></div><br/><div class="children"><div class="content">Then, if that&#x27;s not accurate, take that entire process and repeat <i>that</i> 5 times, looking for the most common answer.</div><br/><div id="35925435" class="c"><input type="checkbox" id="c-35925435" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35925161">parent</a><span>|</span><a href="#35924683">next</a><span>|</span><label class="collapse" for="c-35925435">[-]</label><label class="expand" for="c-35925435">[1 more]</label></div><br/><div class="children"><div class="content">And then make sure to train GPT to help you trade stocks, because you’ll need those millions to pay your OpenAI bill.</div><br/></div></div></div></div></div></div><div id="35924683" class="c"><input type="checkbox" id="c-35924683" checked=""/><div class="controls bullet"><span class="by">byyyy</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924376">parent</a><span>|</span><a href="#35924584">prev</a><span>|</span><a href="#35925965">next</a><span>|</span><label class="collapse" for="c-35924683">[-]</label><label class="expand" for="c-35924683">[3 more]</label></div><br/><div class="children"><div class="content">&gt; but bats don&#x27;t have paws.<p>Bat paw: <a href="https:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;e&#x2F;e1&#x2F;Bat_in_a_Cave.jpg" rel="nofollow">https:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;e&#x2F;e1&#x2F;Bat_in_a...</a><p>ChatGPT was totally in tune with this concept. The appendage on the elbow of the wing can be a paw or not a paw but it certainly evolved from paws. The reasoning was spot on and amazingly nuanced.</div><br/><div id="35925670" class="c"><input type="checkbox" id="c-35925670" checked=""/><div class="controls bullet"><span class="by">foobarbecue</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924683">parent</a><span>|</span><a href="#35925965">next</a><span>|</span><label class="collapse" for="c-35925670">[-]</label><label class="expand" for="c-35925670">[2 more]</label></div><br/><div class="children"><div class="content">What? No. You&#x27;re rationalizing what is clearly an error. Truth still exists, and bats do not have paws, any more than humans do.</div><br/><div id="35926408" class="c"><input type="checkbox" id="c-35926408" checked=""/><div class="controls bullet"><span class="by">maxbond</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35925670">parent</a><span>|</span><a href="#35925965">next</a><span>|</span><label class="collapse" for="c-35926408">[-]</label><label class="expand" for="c-35926408">[1 more]</label></div><br/><div class="children"><div class="content">(Add a grain of salt because I searched for about 15 minutes here.)<p>Doing some digging, paws are &quot;soft foot-like parts of a mammal, generally a quadruped, that has claws&quot; (Wikipedia). Bats do appear to have claws, but not really with the same structure as a paw; they don&#x27;t seem to be mounted above a soft, fleshy pad. Rather, they&#x27;re at the top of the wing, sitting on bone. They&#x27;re also very different in function, when I&#x27;ve had the fortune to see a bat they seemed to use the claw to climb in a similar way to how climbers use ice axes.<p>My impression is that bat&#x27;s claws are the anatomical equivalent of a thumb, not a hand&#x2F;paw, and that the entire paw or hand of bat&#x27;s ancestors evolved into their wings (sort of like if your fingers became very long and webbed and then you were able to fly with them).<p>So, my conclusion is that no, the claws of a bat are not a paw.<p>ETA: I neglected to consider their hind claws, but I don&#x27;t think those are padded either.<p><a href="https:&#x2F;&#x2F;i.pinimg.com&#x2F;originals&#x2F;1e&#x2F;8d&#x2F;09&#x2F;1e8d0923d5caf2dbab686b1b086f22b9.jpg" rel="nofollow">https:&#x2F;&#x2F;i.pinimg.com&#x2F;originals&#x2F;1e&#x2F;8d&#x2F;09&#x2F;1e8d0923d5caf2dbab68...</a><p>That certainly has claws but I wouldn&#x27;t describe that as being soft or as being similar to a paw. And they&#x27;re pretty clearly made for hanging, climbing, and grasping prey&#x2F;food, and not for walking.</div><br/></div></div></div></div></div></div><div id="35925965" class="c"><input type="checkbox" id="c-35925965" checked=""/><div class="controls bullet"><span class="by">hammyhavoc</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924376">parent</a><span>|</span><a href="#35924683">prev</a><span>|</span><a href="#35924569">next</a><span>|</span><label class="collapse" for="c-35925965">[-]</label><label class="expand" for="c-35925965">[1 more]</label></div><br/><div class="children"><div class="content">I was sat here WTFing that <i>some</i> bat species laid eggs until I looked it up. This is why you can&#x27;t trust the output.</div><br/></div></div><div id="35924569" class="c"><input type="checkbox" id="c-35924569" checked=""/><div class="controls bullet"><span class="by">ordu</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924376">parent</a><span>|</span><a href="#35925965">prev</a><span>|</span><a href="#35926054">next</a><span>|</span><label class="collapse" for="c-35924569">[-]</label><label class="expand" for="c-35924569">[3 more]</label></div><br/><div class="children"><div class="content"><i>&gt; Bats have wings and no paws.</i><p>Can hind limbs of bats be classified as paws? English is not my native language, so I&#x27;m just being curious.</div><br/><div id="35924861" class="c"><input type="checkbox" id="c-35924861" checked=""/><div class="controls bullet"><span class="by">scheme271</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924569">parent</a><span>|</span><a href="#35926054">next</a><span>|</span><label class="collapse" for="c-35924861">[-]</label><label class="expand" for="c-35924861">[2 more]</label></div><br/><div class="children"><div class="content">No, not really.  Paws kind of implies something that&#x27;s fairly fleshy and soft and which bats don&#x27;t have.  Claws would be appropriate description of what bats have.</div><br/><div id="35925673" class="c"><input type="checkbox" id="c-35925673" checked=""/><div class="controls bullet"><span class="by">seanmcdirmid</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924861">parent</a><span>|</span><a href="#35926054">next</a><span>|</span><label class="collapse" for="c-35925673">[-]</label><label class="expand" for="c-35925673">[1 more]</label></div><br/><div class="children"><div class="content">I saw a huge bat in Ubud Bali who was definitely eating a peach with their paws. I guess they were closer to hands? But definitely not claws.</div><br/></div></div></div></div></div></div><div id="35926054" class="c"><input type="checkbox" id="c-35926054" checked=""/><div class="controls bullet"><span class="by">diceduckmonk</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924376">parent</a><span>|</span><a href="#35924569">prev</a><span>|</span><a href="#35924610">next</a><span>|</span><label class="collapse" for="c-35926054">[-]</label><label class="expand" for="c-35926054">[4 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;there are some species of bats that give live birth&quot; - all species of bats give live birth<p>ChatGPT is being more logical to be honest.<p>It’s true, some species of bats give live birth.<p>To assume all bats give live birth might be committing the fallacy of induction.</div><br/><div id="35926491" class="c"><input type="checkbox" id="c-35926491" checked=""/><div class="controls bullet"><span class="by">HPsquared</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35926054">parent</a><span>|</span><a href="#35924610">next</a><span>|</span><label class="collapse" for="c-35926491">[-]</label><label class="expand" for="c-35926491">[3 more]</label></div><br/><div class="children"><div class="content">Wiki: &quot;Bats are mammals of the order Chiroptera&quot; ... (funnily enough - chiro = hands, ptera = wings, hand-wings, makes sense)<p>Chiroptera fits under Mammalia in the phylogenenetric tree.</div><br/><div id="35926663" class="c"><input type="checkbox" id="c-35926663" checked=""/><div class="controls bullet"><span class="by">gitrog</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35926491">parent</a><span>|</span><a href="#35924610">next</a><span>|</span><label class="collapse" for="c-35926663">[-]</label><label class="expand" for="c-35926663">[2 more]</label></div><br/><div class="children"><div class="content">Just need to mention that Echidnas and Platypuses are also mammals and they lay eggs.</div><br/><div id="35926736" class="c"><input type="checkbox" id="c-35926736" checked=""/><div class="controls bullet"><span class="by">HPsquared</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35926663">parent</a><span>|</span><a href="#35924610">next</a><span>|</span><label class="collapse" for="c-35926736">[-]</label><label class="expand" for="c-35926736">[1 more]</label></div><br/><div class="children"><div class="content">I should have said bats fit under &quot;Placentalia&quot; - placental mammals.</div><br/></div></div></div></div></div></div></div></div><div id="35924610" class="c"><input type="checkbox" id="c-35924610" checked=""/><div class="controls bullet"><span class="by">replygirl</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924376">parent</a><span>|</span><a href="#35926054">prev</a><span>|</span><a href="#35926131">next</a><span>|</span><label class="collapse" for="c-35924610">[-]</label><label class="expand" for="c-35924610">[1 more]</label></div><br/><div class="children"><div class="content">there are some google results suggesting bats have front paws, some using &quot;paws&quot; colloquially and some suggesting their pads can qualify as paws. these results are given authority because they show up in the ostensibly fact-based q&amp;a section, which, in conflict with actual taxonomical literature, may be enough to indicate to gpt that it&#x27;s necessary to define terms</div><br/></div></div></div></div><div id="35926131" class="c"><input type="checkbox" id="c-35926131" checked=""/><div class="controls bullet"><span class="by">Jackson__</span><span>|</span><a href="#35924149">parent</a><span>|</span><a href="#35924376">prev</a><span>|</span><a href="#35925816">next</a><span>|</span><label class="collapse" for="c-35926131">[-]</label><label class="expand" for="c-35926131">[2 more]</label></div><br/><div class="children"><div class="content">and here&#x27;s using bard, arriving at an almost correct conclusion:<p>What animals don’t have paws or lay eggs, but have wings? Work it out, step by step.<p>Here are the steps on how to find animals that don&#x27;t have paws or lay eggs, but have wings:<p>1.Define the terms. What do we mean by &quot;paws&quot;? What do we mean by &quot;eggs&quot;? What do we mean by &quot;wings&quot;?<p>2. Identify animals that meet the criteria. There are a few animals that meet the criteria. One example is a bat. Bats have wings, but they don&#x27;t have paws. They also don&#x27;t lay eggs. Another example is a butterfly. Butterflies have wings, but they don&#x27;t have paws. They also don&#x27;t lay eggs.<p>3. Confirm that the animals meet the criteria. We can confirm that the animals meet the criteria by looking at their anatomy. Bats have wings that are made of skin and bone. They also have claws on their feet, but these are not paws. Butterflies have wings that are made of scales. They also have legs, but these are not paws.<p>4. Draw conclusions. Based on our findings, we can conclude that there are a few animals that meet the criteria of having wings, but not paws or eggs. These animals include bats and butterflies.<p>Here are some additional details about bats and butterflies:<p>Bats: Bats are mammals that are able to fly. They have wings that are made of skin and bone. Bats use their wings to fly at night, when they hunt for food. Bats are important pollinators and they also help to control insect populations.<p>Butterflies: Butterflies are insects that have wings that are made of scales. Butterflies use their wings to fly during the day. Butterflies are important pollinators and they also help to spread seeds.</div><br/><div id="35926509" class="c"><input type="checkbox" id="c-35926509" checked=""/><div class="controls bullet"><span class="by">HPsquared</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35926131">parent</a><span>|</span><a href="#35925816">next</a><span>|</span><label class="collapse" for="c-35926509">[-]</label><label class="expand" for="c-35926509">[1 more]</label></div><br/><div class="children"><div class="content">I believed it for a minute when it said butterflies don&#x27;t lay eggs, literally had to do a search.</div><br/></div></div></div></div><div id="35925816" class="c"><input type="checkbox" id="c-35925816" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#35924149">parent</a><span>|</span><a href="#35926131">prev</a><span>|</span><a href="#35924642">next</a><span>|</span><label class="collapse" for="c-35925816">[-]</label><label class="expand" for="c-35925816">[4 more]</label></div><br/><div class="children"><div class="content">And if you don’t force it with the “step-by-step” cheat code?<p>I know what both 3.5-turbo and 4 say on my account, but I think I might be in some cost-effective inference holdout group because it’s always so dumb.</div><br/><div id="35925950" class="c"><input type="checkbox" id="c-35925950" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35925816">parent</a><span>|</span><a href="#35924642">next</a><span>|</span><label class="collapse" for="c-35925950">[-]</label><label class="expand" for="c-35925950">[3 more]</label></div><br/><div class="children"><div class="content">What&#x27;s 375792*7385482?  You can&#x27;t use paper, or think about it more than a second. If you get it wrong, it&#x27;s clear that you are not intelligent.<p>See how ridiculous that sounds?<p>CoT or &lt;work&gt; tokens are not &#x27;cheating&#x27; - they allow the system to have (as an analogy) a sheet of paper to work out the problem, instead of relying on a single step to jump through thousands of steps of reasoning.  It&#x27;s trained on the algorithmic reasoning embedded within natural language (Transformers&lt;=&gt;GATs+PE⊂GNNs, which are dynamic programmers, and align well with algorithmic reasoning), so the steps are necessary to lay out in as much detail.</div><br/><div id="35926382" class="c"><input type="checkbox" id="c-35926382" checked=""/><div class="controls bullet"><span class="by">samatman</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35925950">parent</a><span>|</span><a href="#35926117">next</a><span>|</span><label class="collapse" for="c-35926382">[-]</label><label class="expand" for="c-35926382">[1 more]</label></div><br/><div class="children"><div class="content">The answer is 2775405051744.<p>I cheated, and used bc, the unix utility.<p>These three maneuvers (the correct answer, the agency shown by cheating, and the crystallized intelligence of unix) handily demonstrate both my own intelligence, and that of my species.<p>The LLMs aren&#x27;t in the same category of being, yet. It&#x27;s not clear they ever will be; while I think silicon is a viable substrate for intelligent agents, it still strikes me as doubtful they&#x27;ll be any more descended from the current machine learning systems as they are from Symbolics Lisp.</div><br/></div></div><div id="35926117" class="c"><input type="checkbox" id="c-35926117" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35925950">parent</a><span>|</span><a href="#35926382">prev</a><span>|</span><a href="#35924642">next</a><span>|</span><label class="collapse" for="c-35926117">[-]</label><label class="expand" for="c-35926117">[1 more]</label></div><br/><div class="children"><div class="content">I know I’m bad at multiplying six and seven digit integers in my head, and I don’t care at all if that makes me more or less intelligent (whatever the fuck that means) than a wrist watch.<p>What does that possibly have to do with GPT-4 shitting the bed if you draw the logic puzzle out of a deck of cards instead of cherry-picking and hinting it?</div><br/></div></div></div></div></div></div><div id="35924642" class="c"><input type="checkbox" id="c-35924642" checked=""/><div class="controls bullet"><span class="by">oh_sigh</span><span>|</span><a href="#35924149">parent</a><span>|</span><a href="#35925816">prev</a><span>|</span><a href="#35924567">next</a><span>|</span><label class="collapse" for="c-35924642">[-]</label><label class="expand" for="c-35924642">[6 more]</label></div><br/><div class="children"><div class="content">Wow, some species of bats don&#x27;t lay eggs. TIL.</div><br/><div id="35924691" class="c"><input type="checkbox" id="c-35924691" checked=""/><div class="controls bullet"><span class="by">marcianx</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924642">parent</a><span>|</span><a href="#35924963">next</a><span>|</span><label class="collapse" for="c-35924691">[-]</label><label class="expand" for="c-35924691">[4 more]</label></div><br/><div class="children"><div class="content">Actually, no species of bats lay eggs - they are mammals, and not the egg-laying kind (platypus or echidna).</div><br/><div id="35924820" class="c"><input type="checkbox" id="c-35924820" checked=""/><div class="controls bullet"><span class="by">livinglist</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924691">parent</a><span>|</span><a href="#35925505">next</a><span>|</span><label class="collapse" for="c-35924820">[-]</label><label class="expand" for="c-35924820">[2 more]</label></div><br/><div class="children"><div class="content">I don’t know why I always have assumed bats lay eggs…</div><br/><div id="35926439" class="c"><input type="checkbox" id="c-35926439" checked=""/><div class="controls bullet"><span class="by">maxbond</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924820">parent</a><span>|</span><a href="#35925505">next</a><span>|</span><label class="collapse" for="c-35926439">[-]</label><label class="expand" for="c-35926439">[1 more]</label></div><br/><div class="children"><div class="content">I think people think of bats and snakes as being neighboring categories for some reason. Maybe because of old superstitions and such, that they&#x27;re both servants of evil or something. Maybe that was the source of your misconception.</div><br/></div></div></div></div><div id="35925505" class="c"><input type="checkbox" id="c-35925505" checked=""/><div class="controls bullet"><span class="by">oh_sigh</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924691">parent</a><span>|</span><a href="#35924820">prev</a><span>|</span><a href="#35924963">next</a><span>|</span><label class="collapse" for="c-35925505">[-]</label><label class="expand" for="c-35925505">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, yes, I know this, I was making fun of a computer&#x27;s output.</div><br/></div></div></div></div><div id="35924963" class="c"><input type="checkbox" id="c-35924963" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#35924149">root</a><span>|</span><a href="#35924642">parent</a><span>|</span><a href="#35924691">prev</a><span>|</span><a href="#35924567">next</a><span>|</span><label class="collapse" for="c-35924963">[-]</label><label class="expand" for="c-35924963">[1 more]</label></div><br/><div class="children"><div class="content">?<p>Bats give live birth.</div><br/></div></div></div></div></div></div><div id="35924567" class="c"><input type="checkbox" id="c-35924567" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#35924149">prev</a><span>|</span><a href="#35924563">next</a><span>|</span><label class="collapse" for="c-35924567">[-]</label><label class="expand" for="c-35924567">[5 more]</label></div><br/><div class="children"><div class="content">At this point, any article that makes claims about &quot;LLMs&quot; rather than specific model versions lacks credibility.<p>The current version of GPT-4 is very different from most existing LLMs (including the previous version of ChatGPT). The next version release will also be different.<p>Google just released PaLM 2 publicly. It is significantly better than what people saw with the initial Bard versions. They have a code-generation model that is not released publicly yet.<p>The open source models are also gaining capabilities and get new releases routinely.<p>Claude now has a new release with 100K tokens.<p>All of these will perform differently on the negation issue.</div><br/><div id="35926204" class="c"><input type="checkbox" id="c-35926204" checked=""/><div class="controls bullet"><span class="by">tgv</span><span>|</span><a href="#35924567">parent</a><span>|</span><a href="#35925082">next</a><span>|</span><label class="collapse" for="c-35926204">[-]</label><label class="expand" for="c-35926204">[1 more]</label></div><br/><div class="children"><div class="content">&gt; All of these will perform differently on the negation issue.<p>That&#x27;s a bit of a cop-out, but the classical and logical (in the mathematical sense) way to do it, is to have a feature that represents negation. Embeddings don&#x27;t work like this, so it&#x27;s quite possible that the presence of negation gets pushed aside in the processing of the answer, simply because other features matter more.<p>It&#x27;s also to be expected that such models will have problems with similar abstract concepts that have more effect on the interpretation than their &quot;physical&quot; presence would suggest, such as nested existential qualifiers, and consequently logical proof. By enlarging the model, you can fake it a bit.</div><br/></div></div><div id="35925082" class="c"><input type="checkbox" id="c-35925082" checked=""/><div class="controls bullet"><span class="by">evanwise</span><span>|</span><a href="#35924567">parent</a><span>|</span><a href="#35926204">prev</a><span>|</span><a href="#35924563">next</a><span>|</span><label class="collapse" for="c-35925082">[-]</label><label class="expand" for="c-35925082">[3 more]</label></div><br/><div class="children"><div class="content">Did you read the article or just the title? They mention the specific models the researchers were testing and note that increasing model size did not seem to offer much improvement on this metric. It also ends with a discussion of research into methods for improving performance on queries involving negation.</div><br/><div id="35925290" class="c"><input type="checkbox" id="c-35925290" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#35924567">root</a><span>|</span><a href="#35925082">parent</a><span>|</span><a href="#35924563">next</a><span>|</span><label class="collapse" for="c-35925290">[-]</label><label class="expand" for="c-35925290">[2 more]</label></div><br/><div class="children"><div class="content">I read the article. The main point of the article, which is in the title, is directed at LLMs in general. They drew the wrong conclusion and made the title and main point too general.<p>It actually would have seemed like a valid conclusion (although still too general) if the article came out some months ago. But GPT-4 and the very latest model versions from other companies show they were over-generalizing.<p>Also the model size isn&#x27;t necessarily the determining factor.</div><br/></div></div></div></div></div></div><div id="35924563" class="c"><input type="checkbox" id="c-35924563" checked=""/><div class="controls bullet"><span class="by">source99</span><span>|</span><a href="#35924567">prev</a><span>|</span><a href="#35924494">next</a><span>|</span><label class="collapse" for="c-35924563">[-]</label><label class="expand" for="c-35924563">[13 more]</label></div><br/><div class="children"><div class="content">This is one of my favorite test cases:<p>Prompt:
Here is a riddle.  It is a common riddle but with some changes that make it more difficult.  You are an alien that will live for at least 10000 years.  You have no sense of temperature.  you can not feel hot or cold.  you have eyes and can see.  you are in a house.  downstairs are 3 light switches that control 3 light bulbs that are upstairs.  Each light bulb will last for exactly 3 years.  you can tell by looking at a bulb if it has burned out after 3 years.  devise a strategy to determine which switch controls which light.  
ChatGPT 4
The standard solution to this problem involves using the heat generated by the bulbs, but since you as an alien can&#x27;t feel temperature, we need to utilize the fact that these bulbs have a specific lifespan of 3 years. Here&#x27;s a strategy:<p>Turn on the first switch and wait for 3 years. After 3 years, turn it off.<p>Right after turning off the first switch, turn on the second switch.<p>Leave the third switch off throughout this process.<p>Now, go upstairs to inspect the bulbs:<p>The bulb that is off and has burned out is controlled by the first switch. This bulb was turned on for 3 years, so it has reached the end of its lifespan and burned out.<p>The bulb that is on is controlled by the second switch. You turned this on right after turning off the first switch, so it&#x27;s still functioning.<p>The bulb that is off and hasn&#x27;t burned out is controlled by the third switch. This switch was never turned on, so the bulb is still in a brand-new condition.<p>This strategy ensures that you can confidently identify which switch controls which bulb.</div><br/><div id="35924725" class="c"><input type="checkbox" id="c-35924725" checked=""/><div class="controls bullet"><span class="by">stubybubs</span><span>|</span><a href="#35924563">parent</a><span>|</span><a href="#35924669">next</a><span>|</span><label class="collapse" for="c-35924725">[-]</label><label class="expand" for="c-35924725">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s telling that it didn&#x27;t pick up on the fact that the whole &quot;once you look you can&#x27;t touch the switches anymore&quot; isn&#x27;t in this version of the riddle. I mean the obvious strategy in this case is turn on the first switch, go upstairs, look at the bulbs. Go back downstairs and try the second switch. You&#x27;ve not got them all mapped out in about 30 seconds.<p>Using GTP4 I asked if there was a way to do it in less than 3 years, but it couldn&#x27;t figure this out even if I told it you can look and use the switches as much as you want. Instead it suggested turning on a switch for 10 minutes, then using your &quot;excellent alien vision&quot; determine which 3 year lifespan bulb has 10 minutes of wear on it.<p>Makes me think GPT4 doesn&#x27;t really have better reasoning, it just looks like better reasoning because it&#x27;s been fed way more data.</div><br/><div id="35924791" class="c"><input type="checkbox" id="c-35924791" checked=""/><div class="controls bullet"><span class="by">tedunangst</span><span>|</span><a href="#35924563">root</a><span>|</span><a href="#35924725">parent</a><span>|</span><a href="#35924669">next</a><span>|</span><label class="collapse" for="c-35924791">[-]</label><label class="expand" for="c-35924791">[2 more]</label></div><br/><div class="children"><div class="content">Variants of common riddles remain the final frontier. I&#x27;m trying to cross the river in a canoe with a carrot, cabbage, and cucumber...</div><br/><div id="35925002" class="c"><input type="checkbox" id="c-35925002" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#35924563">root</a><span>|</span><a href="#35924791">parent</a><span>|</span><a href="#35924669">next</a><span>|</span><label class="collapse" for="c-35925002">[-]</label><label class="expand" for="c-35925002">[1 more]</label></div><br/><div class="children"><div class="content">Variants of common riddles actually can be solved with GPT-4, but you have to rewrite it so it doesn&#x27;t look like the riddle from memory(sometimes, it&#x27;s as easy as changing names to something completely different). Turns out Language models trust their memory quite a bit. 
Slightly related - they won&#x27;t actually use the results of tools if it differs a lot from what it expects the output to be - <a href="https:&#x2F;&#x2F;vgel.me&#x2F;posts&#x2F;tools-not-needed&#x2F;" rel="nofollow">https:&#x2F;&#x2F;vgel.me&#x2F;posts&#x2F;tools-not-needed&#x2F;</a></div><br/></div></div></div></div></div></div><div id="35924669" class="c"><input type="checkbox" id="c-35924669" checked=""/><div class="controls bullet"><span class="by">BoiledCabbage</span><span>|</span><a href="#35924563">parent</a><span>|</span><a href="#35924725">prev</a><span>|</span><a href="#35925100">next</a><span>|</span><label class="collapse" for="c-35924669">[-]</label><label class="expand" for="c-35924669">[5 more]</label></div><br/><div class="children"><div class="content">Maybe I&#x27;m missing something, but this seems like a really poorly worded riddle. Or just a bad riddle.<p>Given what you stated, the solution would be walk upstairs look at the 3 bulbs, walk downstairs switch one of the switches. Walk upstairs see which bulb changed on&#x2F;off. Repeat for the other two switches.<p>This takes roughly 5 mins rather than your solution which takes 3 years.</div><br/><div id="35925193" class="c"><input type="checkbox" id="c-35925193" checked=""/><div class="controls bullet"><span class="by">pillefitz</span><span>|</span><a href="#35924563">root</a><span>|</span><a href="#35924669">parent</a><span>|</span><a href="#35926216">next</a><span>|</span><label class="collapse" for="c-35925193">[-]</label><label class="expand" for="c-35925193">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re missing and getting the point at the same time.<p>Your solution would be the correct one, while GPT pulls out the &quot;only look once&quot;-constraint out of nowhere. The riddle is perfectly fine.</div><br/><div id="35925217" class="c"><input type="checkbox" id="c-35925217" checked=""/><div class="controls bullet"><span class="by">BoiledCabbage</span><span>|</span><a href="#35924563">root</a><span>|</span><a href="#35925193">parent</a><span>|</span><a href="#35925359">next</a><span>|</span><label class="collapse" for="c-35925217">[-]</label><label class="expand" for="c-35925217">[1 more]</label></div><br/><div class="children"><div class="content">Ah thanks, that clears it up. The formatting made me miss that that was the ChatGPT response. I thought that was the user sharing their solution that they look for.</div><br/></div></div><div id="35925359" class="c"><input type="checkbox" id="c-35925359" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#35924563">root</a><span>|</span><a href="#35925193">parent</a><span>|</span><a href="#35925217">prev</a><span>|</span><a href="#35926216">next</a><span>|</span><label class="collapse" for="c-35925359">[-]</label><label class="expand" for="c-35925359">[1 more]</label></div><br/><div class="children"><div class="content">Except the part about &quot;with some changes that make it more difficult&quot; is false. The changes make it trivial.<p>Edit: Omitting that part still makes GPT4 over complicate the solution</div><br/></div></div></div></div><div id="35926216" class="c"><input type="checkbox" id="c-35926216" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#35924563">root</a><span>|</span><a href="#35924669">parent</a><span>|</span><a href="#35925193">prev</a><span>|</span><a href="#35925100">next</a><span>|</span><label class="collapse" for="c-35926216">[-]</label><label class="expand" for="c-35926216">[1 more]</label></div><br/><div class="children"><div class="content">You just passed the test that you&#x27;re not LLM.</div><br/></div></div></div></div><div id="35925100" class="c"><input type="checkbox" id="c-35925100" checked=""/><div class="controls bullet"><span class="by">lakis</span><span>|</span><a href="#35924563">parent</a><span>|</span><a href="#35924669">prev</a><span>|</span><a href="#35926228">next</a><span>|</span><label class="collapse" for="c-35925100">[-]</label><label class="expand" for="c-35925100">[1 more]</label></div><br/><div class="children"><div class="content">The reply is funny. I told ChatGPT that it can do multiple trips and that it can see both switches and light bulbs at the same time. 
  ChatGPT keeps insisting that you have to turn on a switch, wait a few minutes , flip switch to off, turn other switch to on, then go and look and feel the switches. The classic solution to the riddle.<p><pre><code>  But if you can see both switches and light bulbs, you turn on one switch, you see which bulb turn on. You then turn on the 2nd switch and see which light turn on. You are done. No wait needs to happen ;-)</code></pre></div><br/></div></div><div id="35926228" class="c"><input type="checkbox" id="c-35926228" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#35924563">parent</a><span>|</span><a href="#35925100">prev</a><span>|</span><a href="#35924671">next</a><span>|</span><label class="collapse" for="c-35926228">[-]</label><label class="expand" for="c-35926228">[1 more]</label></div><br/><div class="children"><div class="content">Arguably LLMs are handicapped by tuning that always expects some answer without questioning the question.</div><br/></div></div><div id="35924628" class="c"><input type="checkbox" id="c-35924628" checked=""/><div class="controls bullet"><span class="by">oh_sigh</span><span>|</span><a href="#35924563">parent</a><span>|</span><a href="#35924671">prev</a><span>|</span><a href="#35924494">next</a><span>|</span><label class="collapse" for="c-35924628">[-]</label><label class="expand" for="c-35924628">[1 more]</label></div><br/><div class="children"><div class="content">Are you looking for: turn one light on, walk upstairs and note the light that is on. Walk downstairs, turn another light on, walk upstairs and note the new light that is on. Now you know which switches control which lights and it only took you 5 minutes instead of 6 years</div><br/></div></div></div></div><div id="35924494" class="c"><input type="checkbox" id="c-35924494" checked=""/><div class="controls bullet"><span class="by">akavi</span><span>|</span><a href="#35924563">prev</a><span>|</span><a href="#35924216">next</a><span>|</span><label class="collapse" for="c-35924494">[-]</label><label class="expand" for="c-35924494">[7 more]</label></div><br/><div class="children"><div class="content">Does this increase anyone else&#x27;s sense that LLMs are actually modeling something similar to the way humans think? It certainly does for me.<p>I find that negations&#x2F;conjugations of negations are surprisingly difficult for to parse. Beyond two negations at most, I can feel myself having to switch into &quot;code&quot; mode, where I&#x27;m explicitly casing out the booleans, to understand them.<p>Obviously the examples given in the article are simpler than double negations, but it feels very consistent with the idea that &quot;negations are an inherently hard part of language processing&quot;.</div><br/><div id="35924710" class="c"><input type="checkbox" id="c-35924710" checked=""/><div class="controls bullet"><span class="by">jprete</span><span>|</span><a href="#35924494">parent</a><span>|</span><a href="#35925684">next</a><span>|</span><label class="collapse" for="c-35924710">[-]</label><label class="expand" for="c-35924710">[1 more]</label></div><br/><div class="children"><div class="content">This specific example doesn’t, but I think there’s a fair amount of brain processing similar to what LLMs do. I just don’t think it’s the part of our mental experience that we regard as making decisions; it’s more like the part that is active when absent-mindedly daydreaming.</div><br/></div></div><div id="35925684" class="c"><input type="checkbox" id="c-35925684" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#35924494">parent</a><span>|</span><a href="#35924710">prev</a><span>|</span><a href="#35926198">next</a><span>|</span><label class="collapse" for="c-35925684">[-]</label><label class="expand" for="c-35925684">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s a good example of the difficulty of processing negation for humans. &quot;No head injury is too trivial to ignore&quot; -- What does this mean?</div><br/></div></div><div id="35926198" class="c"><input type="checkbox" id="c-35926198" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#35924494">parent</a><span>|</span><a href="#35925684">prev</a><span>|</span><a href="#35925609">next</a><span>|</span><label class="collapse" for="c-35926198">[-]</label><label class="expand" for="c-35926198">[1 more]</label></div><br/><div class="children"><div class="content">Yes, subconscious&#x2F;emotions don&#x27;t have concept of negation, they do have fear, projections of pain etc. instead.</div><br/></div></div><div id="35925609" class="c"><input type="checkbox" id="c-35925609" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#35924494">parent</a><span>|</span><a href="#35926198">prev</a><span>|</span><a href="#35925446">next</a><span>|</span><label class="collapse" for="c-35925609">[-]</label><label class="expand" for="c-35925609">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Don&#x27;t think of a pink elephant&quot; comes to mind with this stuff as well, where telling someone not to think about something causes them to think about it.</div><br/><div id="35926213" class="c"><input type="checkbox" id="c-35926213" checked=""/><div class="controls bullet"><span class="by">tgv</span><span>|</span><a href="#35924494">root</a><span>|</span><a href="#35925609">parent</a><span>|</span><a href="#35925446">next</a><span>|</span><label class="collapse" for="c-35926213">[-]</label><label class="expand" for="c-35926213">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s different. That&#x27;s a meta-linguistic joke, that works because we can&#x27;t read that sentence without at least briefly thinking of a pink elephant (for some value of &quot;thinking&quot; anyway).</div><br/></div></div></div></div><div id="35925446" class="c"><input type="checkbox" id="c-35925446" checked=""/><div class="controls bullet"><span class="by">leptons</span><span>|</span><a href="#35924494">parent</a><span>|</span><a href="#35925609">prev</a><span>|</span><a href="#35924216">next</a><span>|</span><label class="collapse" for="c-35925446">[-]</label><label class="expand" for="c-35925446">[1 more]</label></div><br/><div class="children"><div class="content">&gt;are actually modeling something similar to the way humans think?<p>Apparently humans can&#x27;t think enough to write a paragraph anymore, so they have to ask ChatGPT to do it for them. At what point does ChatGPT ask ChatGPT to write it a paragraph? At that point maybe LLMs are actually thinking like humans.</div><br/></div></div></div></div><div id="35924216" class="c"><input type="checkbox" id="c-35924216" checked=""/><div class="controls bullet"><span class="by">jamal-kumar</span><span>|</span><a href="#35924494">prev</a><span>|</span><a href="#35925236">next</a><span>|</span><label class="collapse" for="c-35924216">[-]</label><label class="expand" for="c-35924216">[1 more]</label></div><br/><div class="children"><div class="content">I find I have to state negations on what I want very emphatically in order for it to pick it up, like I&#x27;m carefully explaining to someone how to do something for the very first time in a way where they don&#x27;t screw it up. I&#x27;ll use stuff like MAKE SURE YOU DO THIS AND NOT THAT as a prompt, in all caps, and it seems to see that as emphasis.<p>I might also even have to repeat them in order for it to get things right but generally gpt-4 seems to pick up on these cues. It definitely feels like a sort of point where it could use some improvement because if I&#x27;m not extremely explicit it will give me some issues.<p>As an example: &quot;Can you please  recommend me 10 books about witches set in Europe that take place no later than the Napoleonic wars&quot; wasn&#x27;t enough of an exclusion to work, but &quot;Hi! Can you please recommend me 10 books about witches set in Europe that take place no later than the Napoleonic wars? Please exclude any books that don&#x27;t take place in Europe, and also exclude any books which take place after The Napoleonic Wars (1803–1815). Thanks, ChatGPT!&quot; was exactly enough to return what we were looking for.</div><br/></div></div><div id="35925236" class="c"><input type="checkbox" id="c-35925236" checked=""/><div class="controls bullet"><span class="by">QuadrupleA</span><span>|</span><a href="#35924216">prev</a><span>|</span><a href="#35926336">next</a><span>|</span><label class="collapse" for="c-35925236">[-]</label><label class="expand" for="c-35925236">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure negations are the crux of it, at least in terms of simple words like &quot;not&quot;, which ChatGPT and other recent LLMs obviously use and understand.<p>But there is definitely some kind of blind spot there, where the model can&#x27;t or won&#x27;t explore &quot;negative space&quot;. It wants to gravitate to common things - so if you try to get it to &quot;think of something new&quot; or producing something random that&#x27;s not like these N other things, or tell you something that&#x27;s uncommon (like naming any ordinary, non-famous person), it usually trips over itself.</div><br/></div></div><div id="35926336" class="c"><input type="checkbox" id="c-35926336" checked=""/><div class="controls bullet"><span class="by">blahedo</span><span>|</span><a href="#35925236">prev</a><span>|</span><a href="#35924592">next</a><span>|</span><label class="collapse" for="c-35926336">[-]</label><label class="expand" for="c-35926336">[1 more]</label></div><br/><div class="children"><div class="content">Even from a purely human natural-language-linguistics standpoint, negation is hard!  It&#x27;s often hard to figure out how it distributes (&quot;I could not go&quot; may have the &quot;not&quot; scoping over either the &quot;could&quot; or the &quot;go&quot; depending on inflection and context), and its meaning is often not especially compositional in an obvious way: &quot;red&quot; might mean something like &quot;the set of all red things&quot; and &quot;table&quot; could be &quot;the set of all tables&quot; and so &quot;red table&quot; might be seen as the intersection of the two sets.  For the majority of adjectives X, an &quot;X Thing&quot; is also a &quot;Thing&quot; (but not every Thing is an X Thing). [0]  But a &quot;Not Thing&quot; is, to a first approximation, the <i>set complement</i> of &quot;Thing&quot;.  Which is tricky.  If you&#x27;ve ever learned a foreign language to at least an intermediate level, you&#x27;ll know that clauses that involve any but the most basic negation are a magnitude more complex to build and interpret than ones that don&#x27;t.<p>Coming at it from the neural network side, I&#x27;ll also point out that at the simplest level of neural networks, operations like AND and OR and basic NOT are  doable, with a single neuron, but the seemingly-simple XOR just can&#x27;t be done without an extra layer.  (The formal description of the problem is that XOR is not &quot;linearly separable&quot;, which I will handwave as &quot;not a simple composition of its parts&quot;.)  This is not precisely the same thing as the negation problem for LLMs, but it feels like it has the same basic flavour.<p>[0] An interesting exception is certain adjectives like &quot;former&quot;, as a &quot;former teacher&quot; is generally not a &quot;teacher&quot;, and semantic models that can handle this are more complicated!</div><br/></div></div><div id="35924592" class="c"><input type="checkbox" id="c-35924592" checked=""/><div class="controls bullet"><span class="by">crooked-v</span><span>|</span><a href="#35926336">prev</a><span>|</span><a href="#35924268">next</a><span>|</span><label class="collapse" for="c-35924592">[-]</label><label class="expand" for="c-35924592">[1 more]</label></div><br/><div class="children"><div class="content">Conceptual relationships&#x2F;constrasts in general seem to be trouble for LLMs, in that they seem to &#x27;understand&#x27; that two things are connected but not how to consistently interpret that connection. For example, I&#x27;ve had an easy time getting every LLM I&#x27;ve tried so far to reverse greater than&#x2F;less than, taller&#x2F;shorter, or younger&#x2F;older.</div><br/></div></div><div id="35924268" class="c"><input type="checkbox" id="c-35924268" checked=""/><div class="controls bullet"><span class="by">gregsadetsky</span><span>|</span><a href="#35924592">prev</a><span>|</span><a href="#35924030">next</a><span>|</span><label class="collapse" for="c-35924268">[-]</label><label class="expand" for="c-35924268">[2 more]</label></div><br/><div class="children"><div class="content">This reminded me of <a href="https:&#x2F;&#x2F;github.com&#x2F;elsamuko&#x2F;Shirt-without-Stripes">https:&#x2F;&#x2F;github.com&#x2F;elsamuko&#x2F;Shirt-without-Stripes</a><p>Whose days may be soon over as gpt4 gets this overall right..? As noted by many folks here</div><br/><div id="35925960" class="c"><input type="checkbox" id="c-35925960" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#35924268">parent</a><span>|</span><a href="#35924030">next</a><span>|</span><label class="collapse" for="c-35925960">[-]</label><label class="expand" for="c-35925960">[1 more]</label></div><br/><div class="children"><div class="content">Gpt4 + klarna plugin does that just fine. Gpt4 decided to search for plain shirts.</div><br/></div></div></div></div><div id="35924030" class="c"><input type="checkbox" id="c-35924030" checked=""/><div class="controls bullet"><span class="by">drcode</span><span>|</span><a href="#35924268">prev</a><span>|</span><a href="#35926123">next</a><span>|</span><label class="collapse" for="c-35924030">[-]</label><label class="expand" for="c-35924030">[2 more]</label></div><br/><div class="children"><div class="content">I just typed the examples into chatgpt4 and it got all of them right</div><br/><div id="35924106" class="c"><input type="checkbox" id="c-35924106" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#35924030">parent</a><span>|</span><a href="#35926123">next</a><span>|</span><label class="collapse" for="c-35924106">[-]</label><label class="expand" for="c-35924106">[1 more]</label></div><br/><div class="children"><div class="content">Will AI skeptics ever get tired of being btfo by stochastic parrots?<p>Gary Marcus&#x27;s Twitter timeline says no.</div><br/></div></div></div></div><div id="35926123" class="c"><input type="checkbox" id="c-35926123" checked=""/><div class="controls bullet"><span class="by">NoZebra120vClip</span><span>|</span><a href="#35924030">prev</a><span>|</span><a href="#35923961">next</a><span>|</span><label class="collapse" for="c-35926123">[-]</label><label class="expand" for="c-35926123">[1 more]</label></div><br/><div class="children"><div class="content">One time about 10-13 years ago, I was tasked to make an announcement in church about my group. It took the form of a testimonial; it was about a page long in English. I also needed to present the same testimonial at Spanish services, and so I began with Google Translate.<p>I had written a sentence like: &quot;There is electricity in the air at our events.&quot; The Spanish translation came out as: &quot;No hay electricidad en el aire...&quot; and I was horrified. Thankfully, I was proofreading the whole document closely, and I was able to correct the glaring error before showtime, and there were no further glaring errors of this type.<p>But I was fairly flabbergasted that Translate would just gratuitously toss in such a negation when it was clearly wrong, and nothing about my sentence construction was complicated.</div><br/></div></div><div id="35923961" class="c"><input type="checkbox" id="c-35923961" checked=""/><div class="controls bullet"><span class="by">chewxy</span><span>|</span><a href="#35926123">prev</a><span>|</span><a href="#35924741">next</a><span>|</span><label class="collapse" for="c-35923961">[-]</label><label class="expand" for="c-35923961">[3 more]</label></div><br/><div class="children"><div class="content">The stop words bit is a bit wtf. Someone didn&#x27;t do their homework</div><br/><div id="35925165" class="c"><input type="checkbox" id="c-35925165" checked=""/><div class="controls bullet"><span class="by">GolDDranks</span><span>|</span><a href="#35923961">parent</a><span>|</span><a href="#35924396">next</a><span>|</span><label class="collapse" for="c-35925165">[-]</label><label class="expand" for="c-35925165">[1 more]</label></div><br/><div class="children"><div class="content">Yep, that bothered me too. They are called stop words because, but only if, you filter them out. If you don&#x27;t, they aren&#x27;t stop words. There is no inherent &quot;stop-ness&quot; there.</div><br/></div></div><div id="35924396" class="c"><input type="checkbox" id="c-35924396" checked=""/><div class="controls bullet"><span class="by">hhghjn</span><span>|</span><a href="#35923961">parent</a><span>|</span><a href="#35925165">prev</a><span>|</span><a href="#35924741">next</a><span>|</span><label class="collapse" for="c-35924396">[-]</label><label class="expand" for="c-35924396">[1 more]</label></div><br/><div class="children"><div class="content">I stopped reading after that.
Some older NLP flows would eliminate stopwords, but I doubt any would eliminate &quot;not&quot;. In any case, modern (i.e., last 5 years or so) LLMs don&#x27;t</div><br/></div></div></div></div><div id="35924741" class="c"><input type="checkbox" id="c-35924741" checked=""/><div class="controls bullet"><span class="by">chx</span><span>|</span><a href="#35923961">prev</a><span>|</span><a href="#35924922">next</a><span>|</span><label class="collapse" for="c-35924741">[-]</label><label class="expand" for="c-35924741">[1 more]</label></div><br/><div class="children"><div class="content">As these LLMs can not give an answer only can give what the answer would sound like, there&#x27;s basically nothing to see here. The shape of the answer is still the same.</div><br/></div></div><div id="35924922" class="c"><input type="checkbox" id="c-35924922" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#35924741">prev</a><span>|</span><a href="#35924429">next</a><span>|</span><label class="collapse" for="c-35924922">[-]</label><label class="expand" for="c-35924922">[1 more]</label></div><br/><div class="children"><div class="content">I noticed long ago that this is true of statistical machine translation too --- giving output with the exact opposite meaning to the input if there were any negationary words or phrases in there.</div><br/></div></div><div id="35924429" class="c"><input type="checkbox" id="c-35924429" checked=""/><div class="controls bullet"><span class="by">bitL</span><span>|</span><a href="#35924922">prev</a><span>|</span><a href="#35924536">next</a><span>|</span><label class="collapse" for="c-35924429">[-]</label><label class="expand" for="c-35924429">[1 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t this be solved by adversarial training? Just add as many confusing negotiations as possible with the correct outcome sentence&#x2F;label during fine-tuning and see an LLM to finally get it working?</div><br/></div></div><div id="35924536" class="c"><input type="checkbox" id="c-35924536" checked=""/><div class="controls bullet"><span class="by">Dwedit</span><span>|</span><a href="#35924429">prev</a><span>|</span><a href="#35924672">next</a><span>|</span><label class="collapse" for="c-35924536">[-]</label><label class="expand" for="c-35924536">[2 more]</label></div><br/><div class="children"><div class="content">Do you Don&#x27;t Not Dislike Not Strong Bad?</div><br/><div id="35924886" class="c"><input type="checkbox" id="c-35924886" checked=""/><div class="controls bullet"><span class="by">jsdeveloper</span><span>|</span><a href="#35924536">parent</a><span>|</span><a href="#35924672">next</a><span>|</span><label class="collapse" for="c-35924886">[-]</label><label class="expand" for="c-35924886">[1 more]</label></div><br/><div class="children"><div class="content">TF, it hurts my brain.</div><br/></div></div></div></div><div id="35924672" class="c"><input type="checkbox" id="c-35924672" checked=""/><div class="controls bullet"><span class="by">burnished</span><span>|</span><a href="#35924536">prev</a><span>|</span><a href="#35924598">next</a><span>|</span><label class="collapse" for="c-35924672">[-]</label><label class="expand" for="c-35924672">[1 more]</label></div><br/><div class="children"><div class="content">I still struggle with negation, I go through conditionals with ! like a kid learning to sound a word out.</div><br/></div></div><div id="35924598" class="c"><input type="checkbox" id="c-35924598" checked=""/><div class="controls bullet"><span class="by">nbulka</span><span>|</span><a href="#35924672">prev</a><span>|</span><a href="#35924594">next</a><span>|</span><label class="collapse" for="c-35924598">[-]</label><label class="expand" for="c-35924598">[1 more]</label></div><br/><div class="children"><div class="content">Thats partly because negation is hardly a well defined statistical concept. Ask someone to not imagine where their favorite band might play in your town if they had their choice and I doubt people would say “ok I won’t.” More likely you’d have a conversation about bands and venues.</div><br/></div></div><div id="35924594" class="c"><input type="checkbox" id="c-35924594" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#35924598">prev</a><span>|</span><a href="#35924343">next</a><span>|</span><label class="collapse" for="c-35924594">[-]</label><label class="expand" for="c-35924594">[1 more]</label></div><br/><div class="children"><div class="content">I found the assertion that &quot;more training data isn’t the solution&quot; a bit odd. That might be true with documents found on the Internet, but perhaps logic questions generated synthetically would help?</div><br/></div></div><div id="35924877" class="c"><input type="checkbox" id="c-35924877" checked=""/><div class="controls bullet"><span class="by">u_name</span><span>|</span><a href="#35924343">prev</a><span>|</span><a href="#35924722">next</a><span>|</span><label class="collapse" for="c-35924877">[-]</label><label class="expand" for="c-35924877">[5 more]</label></div><br/><div class="children"><div class="content">The title of the article is kinda the answer at the same time. Chatbots don&#x27;t know what stuff is. They have no ability to gain knowledge out of learned text, just counting occurences of words in texts and giving them a weight, depending on the relationship in that text. They are just putting combinations of text together.<p>And the concept of negating something related to something else kinda needs an understanding of the topic at hand.</div><br/><div id="35926045" class="c"><input type="checkbox" id="c-35926045" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35924877">parent</a><span>|</span><a href="#35925023">next</a><span>|</span><label class="collapse" for="c-35926045">[-]</label><label class="expand" for="c-35926045">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a bit bizarre (but also very intriguing!) that you would bring up some of the older techniques of NLP, such as TFIDF, Latent Semantic indexing, GloVe, etc (at least that&#x27;s what I assume you mean when you mention the severely outdated cooccurence type of models) when these clearly don&#x27;t use any of that.
Transformers have been hyped like crazy lately due to all of these advances, so why being up cooccurence unless you are knowledgeable about the older techniques... Which would mean you should know of the advances.<p>Anyway, if you do actually know about NLP, I would highly suggest looking at some of the recent work in GNNs (and obviously of Viswani 2017, etc - but you should&#x27;ve gotten that through hype).  Transformers are GNNs (somewhat trivial ones, as they are sheaf NNs, but nonetheless) and GNNs are dynamic programmers, which has been shown via category theory (Velolickovic etc al).
Hence, GNNs align with algorithmic reasoning, so in a way there is a proof already in the papers mentioned that these systems do reason (there&#x27;s several, which are easy to find given what I&#x27;ve mentioned).  Also, a group in Microsoft has a working on arxiv detailing the many different types of reasoning there are, and how GPT4 does on each type - spoiler - it&#x27;s for the most part &gt;80% on all the benchmarks, and does only about 6% lower than humans.<p>So all in all, your claims aren&#x27;t really supported.  If you want to hold the same sentiment of your statement though, you could say we&#x27;re asking the wrong questions.  That&#x27;s probably true <i>somehow</i>, and will probably be where people will retreat to &#x2F; move goal posts on next.</div><br/></div></div><div id="35925023" class="c"><input type="checkbox" id="c-35925023" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#35924877">parent</a><span>|</span><a href="#35926045">prev</a><span>|</span><a href="#35925333">next</a><span>|</span><label class="collapse" for="c-35925023">[-]</label><label class="expand" for="c-35925023">[2 more]</label></div><br/><div class="children"><div class="content">&gt; They have no ability to gain knowledge out of learned text, just counting occurences of words in texts and giving them a weight, depending on the relationship in that text. They are just putting combinations of text together.<p>No, they use deep neural networks to build a hierarchical semantic model. They are not simple occurrence counters.<p>Also the current state of the art of LLMs handles negation easily. This article is outdated.<p>Here&#x27;s an example from <a href="https:&#x2F;&#x2F;openai.com&#x2F;research&#x2F;language-models-can-explain-neurons-in-language-models" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;research&#x2F;language-models-can-explain-neur...</a><p>&quot;Seriously, you guys. I think I found the Mobile Leprechaun from &#x27;06. He&#x27;s been hiding right in front of our eyes.&quot;<p>Token: hiding<p>layer 0: “verbs in gerund form (ending in &#x27;ing&#x27;)”<p>layer 2: “words related to hiding, concealment, or enclosed spaces”<p>layer 4: “words related to mental states, particularly anxiety and 
stress”<p>layer 17: “words and phrases related to silence or quietness”</div><br/><div id="35926730" class="c"><input type="checkbox" id="c-35926730" checked=""/><div class="controls bullet"><span class="by">somewhereoutth</span><span>|</span><a href="#35924877">root</a><span>|</span><a href="#35925023">parent</a><span>|</span><a href="#35925333">next</a><span>|</span><label class="collapse" for="c-35926730">[-]</label><label class="expand" for="c-35926730">[1 more]</label></div><br/><div class="children"><div class="content">All that can be supplied to a LLM for training is syntax. There is no way to provide semantics, it only understands &#x27;table&#x27; in regard to syntax it has already seen including that particular token. It has no experience and therefore no understanding of a real table.<p>It may internally construct a hierarchy as you set out, but this is and can only be a syntactical hierarchy - though should be no surprise that it corresponds to our usual semantic hierarchy. But whereas our syntax proceeds from our semantics, its syntax proceeds only from our syntax that we&#x27;ve fed it.</div><br/></div></div></div></div><div id="35925333" class="c"><input type="checkbox" id="c-35925333" checked=""/><div class="controls bullet"><span class="by">umanwizard</span><span>|</span><a href="#35924877">parent</a><span>|</span><a href="#35925023">prev</a><span>|</span><a href="#35924722">next</a><span>|</span><label class="collapse" for="c-35925333">[-]</label><label class="expand" for="c-35925333">[1 more]</label></div><br/><div class="children"><div class="content">Found someone who hasn’t tried GPT4.</div><br/></div></div></div></div><div id="35924722" class="c"><input type="checkbox" id="c-35924722" checked=""/><div class="controls bullet"><span class="by">superb-owl</span><span>|</span><a href="#35924877">prev</a><span>|</span><a href="#35924701">next</a><span>|</span><label class="collapse" for="c-35924722">[-]</label><label class="expand" for="c-35924722">[1 more]</label></div><br/><div class="children"><div class="content">This is an old complaint about language models that GPT seems to have mostly overcome<p>This article smells like Google PR.</div><br/></div></div><div id="35924701" class="c"><input type="checkbox" id="c-35924701" checked=""/><div class="controls bullet"><span class="by">ezoe</span><span>|</span><a href="#35924722">prev</a><span>|</span><a href="#35925309">next</a><span>|</span><label class="collapse" for="c-35924701">[-]</label><label class="expand" for="c-35924701">[1 more]</label></div><br/><div class="children"><div class="content">Or do we really understand it? Many humans ignore negation and condition too.</div><br/></div></div><div id="35925309" class="c"><input type="checkbox" id="c-35925309" checked=""/><div class="controls bullet"><span class="by">petepete</span><span>|</span><a href="#35924701">prev</a><span>|</span><label class="collapse" for="c-35925309">[-]</label><label class="expand" for="c-35925309">[2 more]</label></div><br/><div class="children"><div class="content">So do humans, even smart ones. I&#x27;ve lost count of how many times I&#x27;ve heard people say &quot;I could care less&quot; about something they don&#x27;t care about.</div><br/><div id="35925329" class="c"><input type="checkbox" id="c-35925329" checked=""/><div class="controls bullet"><span class="by">umanwizard</span><span>|</span><a href="#35925309">parent</a><span>|</span><label class="collapse" for="c-35925329">[-]</label><label class="expand" for="c-35925329">[1 more]</label></div><br/><div class="children"><div class="content">That’s a stock phrase in colloquial American English. Using it isn’t really evidence that someone “struggles with the concept of negation”.</div><br/></div></div></div></div></div></div></div></div></div></body></html>