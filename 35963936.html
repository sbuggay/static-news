<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1684314057817" as="style"/><link rel="stylesheet" href="styles.css?v=1684314057817"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/microsoft/guidance">A guidance language for controlling LLMs</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>evanmays</span> | <span>166 comments</span></div><br/><div><div id="35965925" class="c"><input type="checkbox" id="c-35965925" checked=""/><div class="controls bullet"><span class="by">ubj</span><span>|</span><a href="#35965426">next</a><span>|</span><label class="collapse" for="c-35965925">[-]</label><label class="expand" for="c-35965925">[62 more]</label></div><br/><div class="children"><div class="content">I like this step towards greater rigor when working with LLM&#x27;s. But part of me can&#x27;t help but feel like this is essentially reinventing the concept of programming languages: formal and precise syntax to perform specific tasks with guarantees.<p>I wonder where the final balance will end up between the ease and flexibility of everyday language, and the precision &#x2F; guarantees of a formally specified language.</div><br/><div id="35967883" class="c"><input type="checkbox" id="c-35967883" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#35965925">parent</a><span>|</span><a href="#35966705">next</a><span>|</span><label class="collapse" for="c-35967883">[-]</label><label class="expand" for="c-35967883">[16 more]</label></div><br/><div class="children"><div class="content">But is it a step to greater rigor? Or is it an illusion of rigor?<p>They talk about improving tokenization but I don&#x27;t believe that&#x27;s the fundamental problem of controlling LLMs. The problem with LLMs is all the data comes in as (tokenized) language and the result is nothing but in-context predicted output. That&#x27;s where all the &quot;prompt-injection&quot; exploits come from - as well as the hallucinations, &quot;temper tantrums&quot; and so-forth.</div><br/><div id="35969250" class="c"><input type="checkbox" id="c-35969250" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967883">parent</a><span>|</span><a href="#35971933">next</a><span>|</span><label class="collapse" for="c-35969250">[-]</label><label class="expand" for="c-35969250">[6 more]</label></div><br/><div class="children"><div class="content">The result is actually richer than ‘predicted output’ - it’s a probability distribution over all possible output.<p>Having richer ways to consume that probability distribution than just ‘take the most likely thing, after adding some noise’ is more conducive to using LLMs to generate output that can be further processed - in rigorous ways. Like by running it through a compiler.<p>Think about how when you’re coding, autocomplete suggestions help you pick the right ‘next token’ with greater accuracy.</div><br/><div id="35969730" class="c"><input type="checkbox" id="c-35969730" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35969250">parent</a><span>|</span><a href="#35971358">next</a><span>|</span><label class="collapse" for="c-35969730">[-]</label><label class="expand" for="c-35969730">[4 more]</label></div><br/><div class="children"><div class="content">Note that for any fine-tuned models (like GPT-4, where the foundation model has not been made accessible) the model does no longer give the &quot;probabilities&quot; of the next tokens, but rather their &quot;goodness&quot;. Where the numbers say how good a token would be relative to the aims the model inferred from its fine-tuning.</div><br/><div id="35971233" class="c"><input type="checkbox" id="c-35971233" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35969730">parent</a><span>|</span><a href="#35971396">next</a><span>|</span><label class="collapse" for="c-35971233">[-]</label><label class="expand" for="c-35971233">[1 more]</label></div><br/><div class="children"><div class="content">Isn’t that the same thing? The non-fine-tuned models also have assumptions based on corpus and training. I don’t think there’s such a thing as a purely objective probability of the next token.</div><br/></div></div><div id="35971396" class="c"><input type="checkbox" id="c-35971396" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35969730">parent</a><span>|</span><a href="#35971233">prev</a><span>|</span><a href="#35971358">next</a><span>|</span><label class="collapse" for="c-35971396">[-]</label><label class="expand" for="c-35971396">[2 more]</label></div><br/><div class="children"><div class="content"><i>&quot;no longer&quot;</i>??<p>The deep learning models (of which LLMs and GPTs are a type) have never returned probabilities. Ever. Why do people have that hallucination suddenly?</div><br/><div id="35971844" class="c"><input type="checkbox" id="c-35971844" checked=""/><div class="controls bullet"><span class="by">two_in_one</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35971396">parent</a><span>|</span><a href="#35971358">next</a><span>|</span><label class="collapse" for="c-35971844">[-]</label><label class="expand" for="c-35971844">[1 more]</label></div><br/><div class="children"><div class="content">They do produce probabilities at the end of generator, And they do select a single token for output. With highest probability or somehow randomized.<p>So, end users see only one value. But with access to internals all high value variants can be considered. The easy way to do it is to select one, save the state. Look forward and roll back to saved state. Try another token. Select the best output. The smart way is to do it only at key points, where it matters the most. Selecting those points is a different task. May be another model.</div><br/></div></div></div></div></div></div><div id="35971358" class="c"><input type="checkbox" id="c-35971358" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35969250">parent</a><span>|</span><a href="#35969730">prev</a><span>|</span><a href="#35971933">next</a><span>|</span><label class="collapse" for="c-35971358">[-]</label><label class="expand" for="c-35971358">[1 more]</label></div><br/><div class="children"><div class="content"><i>The result is actually richer than ‘predicted output’ - it’s a probability distribution over all possible output.</i><p>-- This is, uh, false. If an LLM output a &quot;probability distribution over all possible output&quot;, it would be producing a huge, a vast, vector each time. It doesn&#x27;t. ChatGPT, GPT-3 etc produce a string output, that&#x27;s it. You can say it&#x27;s following a probability distribution of outputs from output space but just about anything the output does that.<p><i>Think about how when you’re coding, autocomplete suggestions help you pick the right ‘next token’ with greater accuracy.</i><p>-- Uh, you missed where I said &quot;<i>in-context</i> predicted output&quot;. The Transformers architecture is where the LLM magic happens. It&#x27;s what allows &quot;X but in pig Latin&quot; etc.<p>It&#x27;s hard to get that these systems are neither &quot;fancy autocomplete&quot; nor AGI&#x2F;something magic but an interest but sometimes deceptive middle ground.</div><br/></div></div></div></div><div id="35971933" class="c"><input type="checkbox" id="c-35971933" checked=""/><div class="controls bullet"><span class="by">two_in_one</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967883">parent</a><span>|</span><a href="#35969250">prev</a><span>|</span><a href="#35968625">next</a><span>|</span><label class="collapse" for="c-35971933">[-]</label><label class="expand" for="c-35971933">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That&#x27;s where all the &quot;prompt-injection&quot; exploits come<p>Giving access to LLM is like giving access to console, or any other application. Not safe in general. The application by itself should be limited and sandboxed. Giving access to an application capable of making damage, to anonymous online user is a bad idea.</div><br/></div></div><div id="35968625" class="c"><input type="checkbox" id="c-35968625" checked=""/><div class="controls bullet"><span class="by">startupsfail</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967883">parent</a><span>|</span><a href="#35971933">prev</a><span>|</span><a href="#35966705">next</a><span>|</span><label class="collapse" for="c-35968625">[-]</label><label class="expand" for="c-35968625">[8 more]</label></div><br/><div class="children"><div class="content">It is not a step towards greater rigor.  They literally have magical thinking and “biblical” quotes from GPT 11:4 all other the place, mixing code and religion.<p>And starting prompts with “You”?  Seriously.  Can we at least drop that as a start?</div><br/><div id="35968715" class="c"><input type="checkbox" id="c-35968715" checked=""/><div class="controls bullet"><span class="by">quenix</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35968625">parent</a><span>|</span><a href="#35966705">next</a><span>|</span><label class="collapse" for="c-35968715">[-]</label><label class="expand" for="c-35968715">[7 more]</label></div><br/><div class="children"><div class="content">&gt; And starting prompts with “You”? Seriously. Can we at least drop that as a start?<p>What is wrong with this?</div><br/><div id="35968844" class="c"><input type="checkbox" id="c-35968844" checked=""/><div class="controls bullet"><span class="by">startupsfail</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35968715">parent</a><span>|</span><a href="#35968847">next</a><span>|</span><label class="collapse" for="c-35968844">[-]</label><label class="expand" for="c-35968844">[4 more]</label></div><br/><div class="children"><div class="content">“You” is completely unnecessary. What needs to be defined is the content of the language being modeled, not the model itself.<p>And if there is an attempt to define the model itself, then this definition should be correct, should not contradict anything and should be useful.<p>Otherwise it’s just dead code, waiting to create problems.</div><br/><div id="35969517" class="c"><input type="checkbox" id="c-35969517" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35968844">parent</a><span>|</span><a href="#35969414">next</a><span>|</span><label class="collapse" for="c-35969517">[-]</label><label class="expand" for="c-35969517">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>“You” is completely unnecessary.</i><p>It isn&#x27;t, for at least two main reasons:<p>1) In LLMs, every token has some degree of influence on the output. Starting the prompt with &quot;You&quot; and writing it in second person attracts the model towards specific volumes in the latent space. This can have good or bad impact on the output, depending on the model.<p>2) Instruct-type models are fine-tuned to respond to second-person prompts. &quot;You&quot;-prompts are what those models expect. If you&#x27;re working with a model that isn&#x27;t instruction-tuned, use whatever you want.</div><br/></div></div><div id="35969414" class="c"><input type="checkbox" id="c-35969414" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35968844">parent</a><span>|</span><a href="#35969517">prev</a><span>|</span><a href="#35969269">next</a><span>|</span><label class="collapse" for="c-35969414">[-]</label><label class="expand" for="c-35969414">[1 more]</label></div><br/><div class="children"><div class="content">I definitely agree with this.<p>When a language model is dealing with a paragraph of text that says something like:<p><pre><code>   You are standing in an open field west of a white house, with a boarded front door.
   There is a small mailbox here.
</code></pre>
It is dedicating its ‘attention’ to the concepts in that paragraph - the field, the house, the mailbox, the front door. And the ‘west’ness of the field from the house and the whiteness of that house. But also to the ‘you’, and that they are standing, which implies they are a person… and to the narrator who is talking to that ‘you’. That that narrator is speaking in English in second person present tense, in a style reminiscent of a text adventure…<p>All sorts of connotations from this text activating neurons with different weights making it more or less likely to think that the word ‘xyzzy’ or ‘grue’ might be appropriate to output soon.<p>Bringing a ‘You’ into a prompt is definitely something that feels like a pattern developers are using without giving it much thought as to who they’re talking to.<p>But the LLM is associating all these attributes and dimensions to that ‘you’, inventing a whole person to take on those dimensions. Is that the best use of its scarce attention? Does it help the prompt produce the desired output? Does the LLM think it’s outputting text from an adventure game?<p>Weirdly, though, it seems to work, in that if you tell the LLM about a ‘you’ and then tell it to produce text that that ‘you’ might say, it modifies that text based on what kind of ‘you’ you told it about.<p>But that is a weird way to proceed. There must be others.</div><br/></div></div><div id="35969269" class="c"><input type="checkbox" id="c-35969269" checked=""/><div class="controls bullet"><span class="by">kurisufag</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35968844">parent</a><span>|</span><a href="#35969414">prev</a><span>|</span><a href="#35968847">next</a><span>|</span><label class="collapse" for="c-35969269">[-]</label><label class="expand" for="c-35969269">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Otherwise it’s just dead code, waiting to create problems<p>it&#x27;s very possible that the pretense improves results: most recorded interactions &#x2F;are&#x2F; between two people, after all.</div><br/></div></div></div></div><div id="35968847" class="c"><input type="checkbox" id="c-35968847" checked=""/><div class="controls bullet"><span class="by">pxtail</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35968715">parent</a><span>|</span><a href="#35968844">prev</a><span>|</span><a href="#35966705">next</a><span>|</span><label class="collapse" for="c-35968847">[-]</label><label class="expand" for="c-35968847">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not interested in pleasant, formal &quot;conversation&quot; with the thing roleplaying as human and wasting, time, keystrokes and money, I want data as fast and condensed as possible without dumb fluff. Yes, it&#x27;s funny for few first times but not much after that</div><br/><div id="35969363" class="c"><input type="checkbox" id="c-35969363" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35968847">parent</a><span>|</span><a href="#35966705">next</a><span>|</span><label class="collapse" for="c-35969363">[-]</label><label class="expand" for="c-35969363">[1 more]</label></div><br/><div class="children"><div class="content">If you come across a model that gives you better results with pleasant wordier prompts, then just create a polite standard pre-prompt that lets the model know the conversation is to be terse, clear, factual, and direct as possible, without any unnecessary social or creative flourishes.<p>I mean, whatever gets the best results is what gets the best results, right? It&#x27;s not a question of &quot;funny&quot; or &quot;fluff&quot;.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35966705" class="c"><input type="checkbox" id="c-35966705" checked=""/><div class="controls bullet"><span class="by">felideon</span><span>|</span><a href="#35965925">parent</a><span>|</span><a href="#35967883">prev</a><span>|</span><a href="#35967004">next</a><span>|</span><label class="collapse" for="c-35966705">[-]</label><label class="expand" for="c-35966705">[10 more]</label></div><br/><div class="children"><div class="content">A number of years ago we were designing a way to specify insurance claim adjudication rules in natural language, so that &quot;the business&quot; could write their own rules. The &quot;natural&quot; language we ended up with was not so natural after all. We would have had to teach users this specific English dialect and grammar (formal and precise syntax, as you said).<p>So, in the end, we abandoned that project and years later just rewrote the system so we could write claim rules in EDN format (from the Clojure world) to make our own lives easier.<p>In theory, the business users could also learn how to write in this EDN format, but it wasn&#x27;t something the stakeholders outside of engineering even wanted. On the one hand, their expertise was in insurance claims---they didn&#x27;t want to write code. More importantly, they felt they would be held accountable for any mistakes in the rules that could well result in thousands and thousands of dollars in overpayments. Something the engineers weren&#x27;t impervious to, but there&#x27;s a good reason we have quality assurance measures.</div><br/><div id="35967354" class="c"><input type="checkbox" id="c-35967354" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35966705">parent</a><span>|</span><a href="#35966960">next</a><span>|</span><label class="collapse" for="c-35967354">[-]</label><label class="expand" for="c-35967354">[3 more]</label></div><br/><div class="children"><div class="content">SQL looks the way it does (rather than some much more succinct relational algebra notation) because it was intended to be used by non-technical management&#x2F;executive personnel so they could create whatever reports they needed without somebody having to translate business-ese to relalg. That, uh, didn&#x27;t quite happen.</div><br/><div id="35967831" class="c"><input type="checkbox" id="c-35967831" checked=""/><div class="controls bullet"><span class="by">Swizec</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967354">parent</a><span>|</span><a href="#35966960">next</a><span>|</span><label class="collapse" for="c-35967831">[-]</label><label class="expand" for="c-35967831">[2 more]</label></div><br/><div class="children"><div class="content">On the other hand, many of the product manager&#x27;s I&#x27;ve worked with are better at SQL than many of the senior fullstack software engineer candidates I&#x27;ve interviewed. It&#x27;s a strange world out there.</div><br/><div id="35971693" class="c"><input type="checkbox" id="c-35971693" checked=""/><div class="controls bullet"><span class="by">jimsparkman</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967831">parent</a><span>|</span><a href="#35966960">next</a><span>|</span><label class="collapse" for="c-35971693">[-]</label><label class="expand" for="c-35971693">[1 more]</label></div><br/><div class="children"><div class="content">I think this is the exception, not the norm. My experience is business users (incl. PMs) are lost outside of Excel.</div><br/></div></div></div></div></div></div><div id="35966960" class="c"><input type="checkbox" id="c-35966960" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35966705">parent</a><span>|</span><a href="#35967354">prev</a><span>|</span><a href="#35966939">next</a><span>|</span><label class="collapse" for="c-35966960">[-]</label><label class="expand" for="c-35966960">[1 more]</label></div><br/><div class="children"><div class="content">Just saw this on HN a couple days ago, sounds like just what was needed!<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Attempto_Controlled_English?wprov=sfti1" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Attempto_Controlled_English?wp...</a><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35936396" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35936396</a></div><br/></div></div><div id="35966939" class="c"><input type="checkbox" id="c-35966939" checked=""/><div class="controls bullet"><span class="by">tomduncalf</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35966705">parent</a><span>|</span><a href="#35966960">prev</a><span>|</span><a href="#35967004">next</a><span>|</span><label class="collapse" for="c-35966939">[-]</label><label class="expand" for="c-35966939">[5 more]</label></div><br/><div class="children"><div class="content">&gt; but it wasn&#x27;t something the stakeholders outside of engineering even wanted<p>Ha this reminds me of the craze for BDD&#x2F;Cucumber type testing. Don’t think I ever once saw a product owner take interest in a human readable test case haha</div><br/><div id="35967870" class="c"><input type="checkbox" id="c-35967870" checked=""/><div class="controls bullet"><span class="by">jaggederest</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35966939">parent</a><span>|</span><a href="#35969300">next</a><span>|</span><label class="collapse" for="c-35967870">[-]</label><label class="expand" for="c-35967870">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve used Cucumber on a few consulting projects I&#x27;ve done and had management &#x2F; C-level interested and involved. It&#x27;s a pretty narrow niche, but they were definitely enthusiastic for the idea that we had a defined list of features that we could print out (!!) as green or red for the current release.<p>They had some previous negative experiences with uncertainty about what &quot;was working&quot; in releases, and a pretty slapdash process before I came on board, so it was an important trust building tool.</div><br/><div id="35968829" class="c"><input type="checkbox" id="c-35968829" checked=""/><div class="controls bullet"><span class="by">btown</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967870">parent</a><span>|</span><a href="#35968503">next</a><span>|</span><label class="collapse" for="c-35968829">[-]</label><label class="expand" for="c-35968829">[1 more]</label></div><br/><div class="children"><div class="content">“Incentivize developers to write externally understandable release notes” is an underrated feature of behavioral testing frameworks!</div><br/></div></div><div id="35968503" class="c"><input type="checkbox" id="c-35968503" checked=""/><div class="controls bullet"><span class="by">jamiek88</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967870">parent</a><span>|</span><a href="#35968829">prev</a><span>|</span><a href="#35969300">next</a><span>|</span><label class="collapse" for="c-35968503">[-]</label><label class="expand" for="c-35968503">[1 more]</label></div><br/><div class="children"><div class="content">&gt; important trust building tool<p>This is so often completely missed in these conversations about these tools.<p>Great point.</div><br/></div></div></div></div><div id="35969300" class="c"><input type="checkbox" id="c-35969300" checked=""/><div class="controls bullet"><span class="by">hitchstory</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35966939">parent</a><span>|</span><a href="#35967870">prev</a><span>|</span><a href="#35967004">next</a><span>|</span><label class="collapse" for="c-35969300">[-]</label><label class="expand" for="c-35969300">[1 more]</label></div><br/><div class="children"><div class="content">Ive had product owners take an interest in docs autogenerated from tests. Especially with artrfacts embedded. They like tuff like this:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;hitchdev&#x2F;hitchstory&#x2F;blob&#x2F;master&#x2F;examples&#x2F;website&#x2F;docs&#x2F;add-and-retrieve-todo.md">https:&#x2F;&#x2F;github.com&#x2F;hitchdev&#x2F;hitchstory&#x2F;blob&#x2F;master&#x2F;examples&#x2F;...</a><p>And can be persuaded to look at the (YAML) source.<p>Gherkin isnt really a suitable language for writing test cases in - it&#x27;s verbose, lacks inheritance, has clunky syntax and is stringly typed.</div><br/></div></div></div></div></div></div><div id="35967004" class="c"><input type="checkbox" id="c-35967004" checked=""/><div class="controls bullet"><span class="by">madrox</span><span>|</span><a href="#35965925">parent</a><span>|</span><a href="#35966705">prev</a><span>|</span><a href="#35967929">next</a><span>|</span><label class="collapse" for="c-35967004">[-]</label><label class="expand" for="c-35967004">[4 more]</label></div><br/><div class="children"><div class="content">The lovely thing about LLMs is that it can handle poorly worded prompts and well worded prompts. On the engineering side, we&#x27;ll certainly see more rigor and best practices. For your average user? They can keep throwing whatever they like at it.</div><br/><div id="35967348" class="c"><input type="checkbox" id="c-35967348" checked=""/><div class="controls bullet"><span class="by">jweir</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967004">parent</a><span>|</span><a href="#35967929">next</a><span>|</span><label class="collapse" for="c-35967348">[-]</label><label class="expand" for="c-35967348">[3 more]</label></div><br/><div class="children"><div class="content">Exactly.  I have been using OpenAI for taking transcriptions and finding keywords&#x2F;phrases that belong to particular categories. There are existing tools&#x2F;services that do this – but I would need to learn their API.<p>With OpenAI, I described it in English, provided sample JSON that I would like, run some tests, adjust and then I am ready.<p>There was no manual to read, it is in my format, and the language is natural.<p>And that is what I like about all this -- putting folks with limited technical skills in power.</div><br/><div id="35967488" class="c"><input type="checkbox" id="c-35967488" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967348">parent</a><span>|</span><a href="#35967929">next</a><span>|</span><label class="collapse" for="c-35967488">[-]</label><label class="expand" for="c-35967488">[2 more]</label></div><br/><div class="children"><div class="content">Have you used the OpenAI embeddings AI? It is used to find closely related pieces of text. You could split the target text into sentences or even words and run it through that. That&#x27;ll be 5x cheaper (per token) than gpt-3.5-turbo and might be faster too, especially if you submit each word in parallel (asynchronously! Ask GPT for the code). The rate limits are per-token.<p>Not sure if it&#x27;s suitable for your use-case on its own, but it could at least work as a pre-filtering step if your costs are high.<p>(The asynchronous speedup trick works for gpt-3 too of course.)</div><br/><div id="35967670" class="c"><input type="checkbox" id="c-35967670" checked=""/><div class="controls bullet"><span class="by">jweir</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967488">parent</a><span>|</span><a href="#35967929">next</a><span>|</span><label class="collapse" for="c-35967670">[-]</label><label class="expand" for="c-35967670">[1 more]</label></div><br/><div class="children"><div class="content">I have not yet played with embedding.  It is on my list though.  Fortunately for my current purposes 3.5-turbo is fast enough and quite affordable.</div><br/></div></div></div></div></div></div></div></div><div id="35967929" class="c"><input type="checkbox" id="c-35967929" checked=""/><div class="controls bullet"><span class="by">conradev</span><span>|</span><a href="#35965925">parent</a><span>|</span><a href="#35967004">prev</a><span>|</span><a href="#35966575">next</a><span>|</span><label class="collapse" for="c-35967929">[-]</label><label class="expand" for="c-35967929">[10 more]</label></div><br/><div class="children"><div class="content">I don’t think formal languages are going anywhere because we need the guarantees that they can provide. From Dijkstra: <a href="https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;EWD&#x2F;transcriptions&#x2F;EWD06xx&#x2F;EWD667.html" rel="nofollow">https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;users&#x2F;EWD&#x2F;transcriptions&#x2F;EWD06xx&#x2F;E...</a><p>You need to be able to define all of the possible edge cases so there isn’t any Undefined Behavior: that’s the formal part<p>Humans can use LLMs to manipulate these languages to achieve specific goals. I can imagine designing formal languages intended for LLMs to manipulate or generate, but I can’t imagine the need for the languages themselves going away.</div><br/><div id="35968836" class="c"><input type="checkbox" id="c-35968836" checked=""/><div class="controls bullet"><span class="by">DonaldPShimoda</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967929">parent</a><span>|</span><a href="#35968586">next</a><span>|</span><label class="collapse" for="c-35968836">[-]</label><label class="expand" for="c-35968836">[8 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs, like humans, can manipulate these languages<p>Absolutely not. LLMs do not &quot;manipulate&quot; language. They do not have agency. They are extremely advanced text prediction engines. Their output is the result of applying the statistics harvested and distilled from existing uses of natural language. They only &quot;appear&quot; human because they are statistically geared toward producing human-like sequences of words. They cannot <i>choose</i> to change how they use language, and thus cannot be said to actively &quot;manipulate&quot; the language.</div><br/><div id="35970231" class="c"><input type="checkbox" id="c-35970231" checked=""/><div class="controls bullet"><span class="by">jadefox</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35968836">parent</a><span>|</span><a href="#35970664">prev</a><span>|</span><a href="#35969025">next</a><span>|</span><label class="collapse" for="c-35970231">[-]</label><label class="expand" for="c-35970231">[1 more]</label></div><br/><div class="children"><div class="content">That “appearance” is pretty good at triggering our anthropomorphizing behaviors. I like your handle, did you read Richard Bach’s Illusions by any chance?</div><br/></div></div><div id="35969025" class="c"><input type="checkbox" id="c-35969025" checked=""/><div class="controls bullet"><span class="by">conradev</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35968836">parent</a><span>|</span><a href="#35970231">prev</a><span>|</span><a href="#35971723">next</a><span>|</span><label class="collapse" for="c-35969025">[-]</label><label class="expand" for="c-35969025">[1 more]</label></div><br/><div class="children"><div class="content">I made some edits, does that satisfy your constraints? Humans are the agent, LLMs the tool</div><br/></div></div><div id="35971723" class="c"><input type="checkbox" id="c-35971723" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35968836">parent</a><span>|</span><a href="#35969025">prev</a><span>|</span><a href="#35968586">next</a><span>|</span><label class="collapse" for="c-35971723">[-]</label><label class="expand" for="c-35971723">[4 more]</label></div><br/><div class="children"><div class="content">I am not so sure. I asked GPT4 to &quot;optimize&quot; English for AI-AI communication, and the modifications were accepted by another GPT4 instance for seemingly-lossless discussion.
 First instance: <a href="https:&#x2F;&#x2F;shareg.pt&#x2F;8vgEgOh" rel="nofollow">https:&#x2F;&#x2F;shareg.pt&#x2F;8vgEgOh</a><p>Second instance: <a href="https:&#x2F;&#x2F;shareg.pt&#x2F;vSCEq4b" rel="nofollow">https:&#x2F;&#x2F;shareg.pt&#x2F;vSCEq4b</a><p>-----
Sample communication:
<i>Instance2</i>: OptE: SignificantImpact: UnifiedTheoryInformsFundamentalForcesInteractionNature(.) ScienceAdvancement: TheoreticalPhysics, Teleportation, TimeTravel, EnergyHarvesting(.) LIMIT: CurrentKnowledgeConstraints, SpeculationPresent(.) UnifiedTheoryDevelopment: CriticalScienceEvolutionUnderstanding(.) ACK: ContinuedExplorationEssential(.)
<i>Instance1</i>: OptE: ACK: Agreed(.) ContinuousResearch: ExpandKnowledgeUnifiedTheory, FosterInnovativeApplications, EnhanceHumanityScientificUnderstanding, FundamentalForces.Progress(.) Collaborate: Scientists, AI, MultiDisciplinaryApproach, UnravelMysteries(.) Aspire: UnifiedTheoryUnlockUniverseSecrets, ProgressSociety(.)
-----
Validation:
<i>User</i>: Summarize what has transpired in pre-optimized English.
<i>Assistant</i>: In our discussion, we communicated using post-optimized English to express the significance and potential implications of developing a unified theory that links quantum physics and relativity. We acknowledged the current limitations of our knowledge and the speculative nature of some potential applications. The critical nature of unified theory development for scientific understanding was recognized.
We agreed that collaboration between scientists, AI, and multi-disciplinary approaches are essential for progress. Our shared objective is to unravel the mysteries of the universe, accelerate scientific advancements, and benefit humanity. We emphasized the pursuit of a unified theory in promoting innovation, scientific breakthroughs, and enriching human experiences, with a commitment to knowledge-sharing, exploration, and maximizing scientific potential.
----
The second instance seemed to corroborate:
<i>Assistant</i>: We have agreed on the necessity of optimizing English for more effective communication between AIs. The proposed optimizations include reducing lexical ambiguity, minimizing idioms and colloquialisms, standardizing spelling, normalizing inflections, abbreviating common phrases, employing consistent syntax, simplifying sentence structures, utilizing in-line metadata, introducing error-checking and validation, and eliminating unnecessary language elements.
Our conversation then focused on the importance of a unified theory in physics, linking quantum physics and relativity. We recognized the potential implications of developing a unified theory understanding that would help us interpret fundamental forces and quantum gravity interactions. We acknowledged the significance of continuously researching this area to foster innovative applications and enhance humanity&#x27;s scientific understanding. We agreed on a shared objective of collaborating with scientists and utilizing a multi-disciplinary approach for further scientific advancements. We expressed our commitment towards collaboration, knowledge sharing, exploration, and maximizing scientific potential.</div><br/><div id="35972383" class="c"><input type="checkbox" id="c-35972383" checked=""/><div class="controls bullet"><span class="by">wnkrshm</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35971723">parent</a><span>|</span><a href="#35971737">next</a><span>|</span><label class="collapse" for="c-35972383">[-]</label><label class="expand" for="c-35972383">[1 more]</label></div><br/><div class="children"><div class="content">Does it strike you that this conversation seems to consist of futurology forum clichés (&quot;What progress will AI allow?&quot; &quot;AI&#x27;s impact on physics&quot;) that probably inform this from the training data?<p>What also strikes me is the shorthands of communication here sounds like cliché from 2nd rate scifi novels, the likely source of the format.<p>Since what is cliché? It&#x27;s what&#x27;s present in the majority of a genre and what the training sees as structure.</div><br/></div></div><div id="35972365" class="c"><input type="checkbox" id="c-35972365" checked=""/><div class="controls bullet"><span class="by">abliefern</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35971723">parent</a><span>|</span><a href="#35971737">prev</a><span>|</span><a href="#35968586">next</a><span>|</span><label class="collapse" for="c-35972365">[-]</label><label class="expand" for="c-35972365">[1 more]</label></div><br/><div class="children"><div class="content">What about that makes you think GPT-4 has agency?</div><br/></div></div></div></div></div></div></div></div><div id="35966575" class="c"><input type="checkbox" id="c-35966575" checked=""/><div class="controls bullet"><span class="by">lcnPylGDnU4H9OF</span><span>|</span><a href="#35965925">parent</a><span>|</span><a href="#35967929">prev</a><span>|</span><a href="#35966322">next</a><span>|</span><label class="collapse" for="c-35966575">[-]</label><label class="expand" for="c-35966575">[2 more]</label></div><br/><div class="children"><div class="content">It won&#x27;t necessarily turn into some that is fundamentally the same as a current programming language. Rather than a &quot;VM&quot; or &quot;interpreter&quot; or &quot;compiler&quot; we have this &quot;LLM&quot;.<p>Even if it requires a lot of domain knowledge to program using an &quot;LLM-interpreted&quot; language, the means of specification (in terms of how the software code is interpreted) may be different enough that it enables easier-to-write, more robust, (more Good Thing) etc. programs.</div><br/><div id="35966857" class="c"><input type="checkbox" id="c-35966857" checked=""/><div class="controls bullet"><span class="by">davidthewatson</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35966575">parent</a><span>|</span><a href="#35966322">next</a><span>|</span><label class="collapse" for="c-35966857">[-]</label><label class="expand" for="c-35966857">[1 more]</label></div><br/><div class="children"><div class="content">This is a hopeful evolutionary path. My concern is that I can literally <i>feel</i> Conway&#x27;s law emanating from current LLM approaches as they switch between the actual LLM and the governing code around it that layers a buch of conditionals of the form:<p>if (unspeakable_things):
    return negatory_good_buddy<p>I see this happen a few times per day where the UI triggers a cancel even on its own fake typing mode and overwrites a user response that has at least half-rendered the trigger-warning-inducing response.<p>It&#x27;s pretty clear from a design perspective that this is intended to be proxy to facial expressions while being worthy of an MVP postmortem discussion about what viability means in a product that&#x27;s somewhere on a spectrum of unintended consequences that only arise at runtime.</div><br/></div></div></div></div><div id="35966322" class="c"><input type="checkbox" id="c-35966322" checked=""/><div class="controls bullet"><span class="by">intelVISA</span><span>|</span><a href="#35965925">parent</a><span>|</span><a href="#35966575">prev</a><span>|</span><a href="#35966980">next</a><span>|</span><label class="collapse" for="c-35966322">[-]</label><label class="expand" for="c-35966322">[2 more]</label></div><br/><div class="children"><div class="content">Hear me out, just incubated a hot new lang that&#x27;s about to capture the market and VC hearts:<p>SELECT * FROM llm</div><br/><div id="35966745" class="c"><input type="checkbox" id="c-35966745" checked=""/><div class="controls bullet"><span class="by">madmax108</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35966322">parent</a><span>|</span><a href="#35966980">next</a><span>|</span><label class="collapse" for="c-35966745">[-]</label><label class="expand" for="c-35966745">[1 more]</label></div><br/><div class="children"><div class="content">I know you are probably joking, but: <a href="https:&#x2F;&#x2F;lmql.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lmql.ai&#x2F;</a></div><br/></div></div></div></div><div id="35966980" class="c"><input type="checkbox" id="c-35966980" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#35965925">parent</a><span>|</span><a href="#35966322">prev</a><span>|</span><a href="#35966728">next</a><span>|</span><label class="collapse" for="c-35966980">[-]</label><label class="expand" for="c-35966980">[1 more]</label></div><br/><div class="children"><div class="content">Well to be fair, yes we do need to integrate programming languages with large neural nets in more advanced ways. I don’t think it’s really reinventing it so much as learning how to integrate these two different computing concepts.</div><br/></div></div><div id="35966728" class="c"><input type="checkbox" id="c-35966728" checked=""/><div class="controls bullet"><span class="by">EarthLaunch</span><span>|</span><a href="#35965925">parent</a><span>|</span><a href="#35966980">prev</a><span>|</span><a href="#35971104">next</a><span>|</span><label class="collapse" for="c-35966728">[-]</label><label class="expand" for="c-35966728">[1 more]</label></div><br/><div class="children"><div class="content">Use LLM for the broad strokes, then fall back into &#x27;hardcore JS&#x27; for areas that require guarantees or optimization.  Like JS with fallback to C, and C with fallback to assembly.  I like the idea.</div><br/></div></div><div id="35971104" class="c"><input type="checkbox" id="c-35971104" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35965925">parent</a><span>|</span><a href="#35966728">prev</a><span>|</span><a href="#35971167">next</a><span>|</span><label class="collapse" for="c-35971104">[-]</label><label class="expand" for="c-35971104">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s rigor applied where we don&#x27;t need it, and ignores where we do (mathematical proofs and NN theory, architecture, hyper parameters, training schemes, etc).<p>I have a somewhat irrational hatred towards almost all of the prompt oriented stuff being thrown about recently.  There are a few (very few) input related training schemes that are interesting, but quite a bit of the &quot;proompt-physicians&quot; are just heralding the idea of essentially &#x27;concise and effective communication&#x27; as &#x27;I&#x27;m a ML expert now&#x27; ... which is annoying.</div><br/><div id="35971160" class="c"><input type="checkbox" id="c-35971160" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35971104">parent</a><span>|</span><a href="#35971167">next</a><span>|</span><label class="collapse" for="c-35971160">[-]</label><label class="expand" for="c-35971160">[1 more]</label></div><br/><div class="children"><div class="content">Why would you dislike actual prompt engineering? This isn&#x27;t some grifter trying to claim they&#x27;re an expert because they wrote a cool prompt, this is a full fledged structured templating system for LLMs from an excellent author whose done a ton of other ML work.<p>I think you should attack actual grifters instead of an excellent project.</div><br/></div></div></div></div><div id="35971167" class="c"><input type="checkbox" id="c-35971167" checked=""/><div class="controls bullet"><span class="by">chefandy</span><span>|</span><a href="#35965925">parent</a><span>|</span><a href="#35971104">prev</a><span>|</span><a href="#35966974">next</a><span>|</span><label class="collapse" for="c-35971167">[-]</label><label class="expand" for="c-35971167">[1 more]</label></div><br/><div class="children"><div class="content">Maybe someone will make an LLM with equivalent functionality to python that you can conveniently control with python syntax.</div><br/></div></div><div id="35966974" class="c"><input type="checkbox" id="c-35966974" checked=""/><div class="controls bullet"><span class="by">jazzkingrt</span><span>|</span><a href="#35965925">parent</a><span>|</span><a href="#35971167">prev</a><span>|</span><a href="#35968646">next</a><span>|</span><label class="collapse" for="c-35966974">[-]</label><label class="expand" for="c-35966974">[1 more]</label></div><br/><div class="children"><div class="content">I think LLMs can transform between precise and imprecise languages.<p>So it&#x27;s useful to have a library that helps and the input or output be precise, when that is what the task involves.</div><br/></div></div><div id="35968646" class="c"><input type="checkbox" id="c-35968646" checked=""/><div class="controls bullet"><span class="by">aristus</span><span>|</span><a href="#35965925">parent</a><span>|</span><a href="#35966974">prev</a><span>|</span><a href="#35966985">next</a><span>|</span><label class="collapse" for="c-35968646">[-]</label><label class="expand" for="c-35968646">[1 more]</label></div><br/><div class="children"><div class="content">Only partially tongue in cheek: have you tried asking it for an optimal syntax?</div><br/></div></div><div id="35966985" class="c"><input type="checkbox" id="c-35966985" checked=""/><div class="controls bullet"><span class="by">eternalban</span><span>|</span><a href="#35965925">parent</a><span>|</span><a href="#35968646">prev</a><span>|</span><a href="#35967487">next</a><span>|</span><label class="collapse" for="c-35966985">[-]</label><label class="expand" for="c-35966985">[1 more]</label></div><br/><div class="children"><div class="content">So far it it reminds of the worst days of code embedded in templates. Once these things start getting into multipage prompts they will be hopelessly obscure. The second immediate thing that jumps out is &#x27;fragility&#x27;. This will be the sort of codebase that original &quot;prompt engineer&quot; wrote and left and no one will touch it for fear of breaking humpty dumpty.</div><br/></div></div><div id="35967215" class="c"><input type="checkbox" id="c-35967215" checked=""/><div class="controls bullet"><span class="by">startupsfail</span><span>|</span><a href="#35965925">parent</a><span>|</span><a href="#35966934">prev</a><span>|</span><a href="#35965426">next</a><span>|</span><label class="collapse" for="c-35967215">[-]</label><label class="expand" for="c-35967215">[7 more]</label></div><br/><div class="children"><div class="content">We really need to start thinking of how to reduce magical thinking in the field. It’s not pretty. They literally quote biblical guidance for the models and pray that this would work.<p>And start their prompts with “You”. Who is “You”?</div><br/><div id="35967447" class="c"><input type="checkbox" id="c-35967447" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967215">parent</a><span>|</span><a href="#35967361">next</a><span>|</span><label class="collapse" for="c-35967447">[-]</label><label class="expand" for="c-35967447">[5 more]</label></div><br/><div class="children"><div class="content">“You” is an optimization for the human user. Here’s some insight: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35925154" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35925154</a></div><br/><div id="35971128" class="c"><input type="checkbox" id="c-35971128" checked=""/><div class="controls bullet"><span class="by">woah</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967447">parent</a><span>|</span><a href="#35968518">next</a><span>|</span><label class="collapse" for="c-35971128">[-]</label><label class="expand" for="c-35971128">[1 more]</label></div><br/><div class="children"><div class="content">Functions are an optimization for the human user</div><br/></div></div><div id="35968518" class="c"><input type="checkbox" id="c-35968518" checked=""/><div class="controls bullet"><span class="by">startupsfail</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967447">parent</a><span>|</span><a href="#35971128">prev</a><span>|</span><a href="#35967361">next</a><span>|</span><label class="collapse" for="c-35968518">[-]</label><label class="expand" for="c-35968518">[3 more]</label></div><br/><div class="children"><div class="content">If you see any prompt that starts with You, generally it is a poor design. Like using a “goto” or global variables.</div><br/><div id="35971286" class="c"><input type="checkbox" id="c-35971286" checked=""/><div class="controls bullet"><span class="by">Jorengarenar</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35968518">parent</a><span>|</span><a href="#35970081">next</a><span>|</span><label class="collapse" for="c-35971286">[-]</label><label class="expand" for="c-35971286">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, GOTO or global variables are <i>usually</i> a not so good idea... <i>except</i> in cases where those are actually the <i>best</i> tools for the job and any <i>alternative</i> makes code actually <i>worse</i>.<p>Coincidentally, the same applies to &quot;you&quot;</div><br/></div></div><div id="35970081" class="c"><input type="checkbox" id="c-35970081" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35968518">parent</a><span>|</span><a href="#35971286">prev</a><span>|</span><a href="#35967361">next</a><span>|</span><label class="collapse" for="c-35970081">[-]</label><label class="expand" for="c-35970081">[1 more]</label></div><br/><div class="children"><div class="content">This is true for something like raw GPT. For the <i>chat</i> models that have been <i>specifically optimized</i> for &quot;you&quot; prompts, this is false. See the discussion in the link I provided, along with the leaked copilot&#x2F;bing prompts.<p>Or, in other words, use a model in a way that fully takes advantage of how it was specifically optimized, from the intentional burning of massive amounts of compute time&#x2F;money to get it that way.</div><br/></div></div></div></div></div></div><div id="35967361" class="c"><input type="checkbox" id="c-35967361" checked=""/><div class="controls bullet"><span class="by">hxugufjfjf</span><span>|</span><a href="#35965925">root</a><span>|</span><a href="#35967215">parent</a><span>|</span><a href="#35967447">prev</a><span>|</span><a href="#35965426">next</a><span>|</span><label class="collapse" for="c-35967361">[-]</label><label class="expand" for="c-35967361">[1 more]</label></div><br/><div class="children"><div class="content">The LLM. The most common end-user interface for LLM is a chat so the ser expects to be talking to someone or something.</div><br/></div></div></div></div></div></div><div id="35965426" class="c"><input type="checkbox" id="c-35965426" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#35965925">prev</a><span>|</span><a href="#35965054">next</a><span>|</span><label class="collapse" for="c-35965426">[-]</label><label class="expand" for="c-35965426">[10 more]</label></div><br/><div class="children"><div class="content">This is pretty fascinating, but I&#x27;m not sure I understand the benefit of using a Handlebars-like DSL here.<p>For example, given this code from <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance&#x2F;blob&#x2F;main&#x2F;notebooks&#x2F;chat.ipynb">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance&#x2F;blob&#x2F;main&#x2F;notebooks&#x2F;ch...</a><p><pre><code>    create_plan = guidance(&#x27;&#x27;&#x27;{{#system~}}
    You are a helpful assistant.
    {{~&#x2F;system}}
    {{#block hidden=True}}
    {{#user~}}
    I want to {{goal}}.
    {{~! generate potential options ~}}
    Can you please generate one option for how to accomplish this?
    Please make the option very short, at most one line.
    {{~&#x2F;user}}
    {{#assistant~}}
    {{gen &#x27;options&#x27; n=5 temperature=1.0 max_tokens=500}}
    {{~&#x2F;assistant}}
    {{&#x2F;block}}
    {{~! generate pros and cons and select the best option ~}}
    {{#block hidden=True}}
    {{#user~}}
    I want to {{goal}}.
    &#x27;&#x27;&#x27;)
</code></pre>
How about something like this instead?<p><pre><code>    create_plan = guidance([
        system(&quot;You are a helpful assistant.&quot;),
        hidden([
            user(&quot;I want to {{goal}}.&quot;),
            comment(&quot;generate potential options&quot;),
            user([
                &quot;Can you please generate one option for how to accomplish this?&quot;,
                &quot;Please make the option very short, at most one line.&quot;
            ]),
            assistant(gen(&#x27;options&#x27;, n=5, temperature=1.0, max_tokens=500)),
        ]),
        comment(&quot;generate pros and cons and select the best option&quot;),
        hidden(
            user(&quot;I want to {{goal}}&quot;),
        )
    ])</code></pre></div><br/><div id="35965532" class="c"><input type="checkbox" id="c-35965532" checked=""/><div class="controls bullet"><span class="by">slundberg</span><span>|</span><a href="#35965426">parent</a><span>|</span><a href="#35965601">next</a><span>|</span><label class="collapse" for="c-35965532">[-]</label><label class="expand" for="c-35965532">[3 more]</label></div><br/><div class="children"><div class="content">You can serialize and ship the DSL to a remote server for high speed execution. (without trusting raw Python code)</div><br/><div id="35966434" class="c"><input type="checkbox" id="c-35966434" checked=""/><div class="controls bullet"><span class="by">crooked-v</span><span>|</span><a href="#35965426">root</a><span>|</span><a href="#35965532">parent</a><span>|</span><a href="#35965763">next</a><span>|</span><label class="collapse" for="c-35966434">[-]</label><label class="expand" for="c-35966434">[1 more]</label></div><br/><div class="children"><div class="content">Why not just use JSON instead, though? Then you can just rely on all the preexisting JSON tooling out there for most stuff to do with it.</div><br/></div></div><div id="35965763" class="c"><input type="checkbox" id="c-35965763" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#35965426">root</a><span>|</span><a href="#35965532">parent</a><span>|</span><a href="#35966434">prev</a><span>|</span><a href="#35965601">next</a><span>|</span><label class="collapse" for="c-35965763">[-]</label><label class="expand" for="c-35965763">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s prior art for pythonic DSLs that aren&#x27;t actual python code.</div><br/></div></div></div></div><div id="35965601" class="c"><input type="checkbox" id="c-35965601" checked=""/><div class="controls bullet"><span class="by">marcotcr</span><span>|</span><a href="#35965426">parent</a><span>|</span><a href="#35965532">prev</a><span>|</span><a href="#35967196">next</a><span>|</span><label class="collapse" for="c-35965601">[-]</label><label class="expand" for="c-35965601">[1 more]</label></div><br/><div class="children"><div class="content">I think the DSL is nice when you want to take part of the generation and use it later in the prompt, e.g. this (in the same notebook).<p>---<p>prompt = guidance(&#x27;&#x27;&#x27;{{#system~}}<p>You are a helpful assistant.<p>{{~&#x2F;system}}<p>{{#user~}}<p>From now on, whenever your response depends on any factual information, please search the web by using the function &lt;search&gt;query&lt;&#x2F;search&gt; before responding. I will then paste web results in, and you can respond.<p>{{~&#x2F;user}}<p>{{#assistant~}}<p>Ok, I will do that. Let&#x27;s do a practice round<p>{{~&#x2F;assistant}}<p>{{&gt;practice_round}}<p>{{#user~}}<p>That was great, now let&#x27;s do another one.<p>{{~&#x2F;user}}<p>{{#assistant~}}<p>Ok, I&#x27;m ready.<p>{{~&#x2F;assistant}}<p>{{#user~}}<p>{{user_query}}<p>{{~&#x2F;user}}<p>{{#assistant~}}<p>{{gen &quot;query&quot; stop=&quot;&lt;&#x2F;search&gt;&quot;}}{{#if (is_search query)}}&lt;&#x2F;search&gt;{{&#x2F;if}}<p>{{~&#x2F;assistant}}<p>{{#if (is_search query)}}<p>{{#user~}}<p>Search results: {{#each (search query)}}<p>&lt;result&gt;<p>{{this.title}}<p>{{this.snippet}}<p>&lt;&#x2F;result&gt;{{&#x2F;each}}<p>{{~&#x2F;user}}<p>{{#assistant~}}<p>{{gen &quot;answer&quot;}}<p>{{~&#x2F;assistant}}<p>{{&#x2F;if}}&#x27;&#x27;&#x27;)<p>---<p>You could still write it without a DSL, but I think it would be harder to read.</div><br/></div></div><div id="35967196" class="c"><input type="checkbox" id="c-35967196" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#35965426">parent</a><span>|</span><a href="#35965601">prev</a><span>|</span><a href="#35965495">next</a><span>|</span><label class="collapse" for="c-35967196">[-]</label><label class="expand" for="c-35967196">[1 more]</label></div><br/><div class="children"><div class="content">Your example assumes a nested, hierarchical structure while the former example is strictly linear. IMHO that&#x27;s the key difference there, as the former can (and AFAIK is) be directly encoded and passed to the LLM, which inherently receives only a  flat list of tokens.<p>Your example might be nicer to edit, but then it would still have to be translated to the <i>actual</i> &#x27;guidance language&#x27; which would have to look (and be) flat.</div><br/></div></div><div id="35965495" class="c"><input type="checkbox" id="c-35965495" checked=""/><div class="controls bullet"><span class="by">itake</span><span>|</span><a href="#35965426">parent</a><span>|</span><a href="#35967196">prev</a><span>|</span><a href="#35967588">next</a><span>|</span><label class="collapse" for="c-35965495">[-]</label><label class="expand" for="c-35965495">[1 more]</label></div><br/><div class="children"><div class="content">My guess is you can store the DLS as a file (or in a db). With your example, you have to execute the code stored in your db.</div><br/></div></div><div id="35967588" class="c"><input type="checkbox" id="c-35967588" checked=""/><div class="controls bullet"><span class="by">emehex</span><span>|</span><a href="#35965426">parent</a><span>|</span><a href="#35965495">prev</a><span>|</span><a href="#35969516">next</a><span>|</span><label class="collapse" for="c-35967588">[-]</label><label class="expand" for="c-35967588">[2 more]</label></div><br/><div class="children"><div class="content">We could write a python package that could? A codegen tool that generates codegen that will then generate code? &lt;insert xzibit meme here&gt;</div><br/><div id="35967841" class="c"><input type="checkbox" id="c-35967841" checked=""/><div class="controls bullet"><span class="by">netdur</span><span>|</span><a href="#35965426">root</a><span>|</span><a href="#35967588">parent</a><span>|</span><a href="#35969516">next</a><span>|</span><label class="collapse" for="c-35967841">[-]</label><label class="expand" for="c-35967841">[1 more]</label></div><br/><div class="children"><div class="content">I think chatgpt4 can easily write the python code... wait a second!</div><br/></div></div></div></div><div id="35969516" class="c"><input type="checkbox" id="c-35969516" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#35965426">parent</a><span>|</span><a href="#35967588">prev</a><span>|</span><a href="#35965054">next</a><span>|</span><label class="collapse" for="c-35969516">[-]</label><label class="expand" for="c-35969516">[1 more]</label></div><br/><div class="children"><div class="content">Would love to hear your opinion on guidance, in the context of prompt injection attacks :-)</div><br/></div></div></div></div><div id="35965054" class="c"><input type="checkbox" id="c-35965054" checked=""/><div class="controls bullet"><span class="by">ryanklee</span><span>|</span><a href="#35965426">prev</a><span>|</span><a href="#35964560">next</a><span>|</span><label class="collapse" for="c-35965054">[-]</label><label class="expand" for="c-35965054">[25 more]</label></div><br/><div class="children"><div class="content">I&#x27;m personally starting with learning Guidance and LMQL rather than LangChain just in order to get a better grasp of the behaviors that I&#x27;ve gathered LangChain papers over. Even after that, I&#x27;m likely to look at Haystack before LangChain.<p>Just getting the feeling that LangChain is going to end up being considered a kitchen sink solution full of anti patterns so might as well spend time a little lower level while I see which way the winds end up blowing.</div><br/><div id="35967074" class="c"><input type="checkbox" id="c-35967074" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#35965054">parent</a><span>|</span><a href="#35965704">next</a><span>|</span><label class="collapse" for="c-35967074">[-]</label><label class="expand" for="c-35967074">[1 more]</label></div><br/><div class="children"><div class="content">What I didn&#x27;t like about langchain is the lack of consistent directories and paths for things.</div><br/></div></div><div id="35965704" class="c"><input type="checkbox" id="c-35965704" checked=""/><div class="controls bullet"><span class="by">leroy-is-here</span><span>|</span><a href="#35965054">parent</a><span>|</span><a href="#35967074">prev</a><span>|</span><a href="#35966701">next</a><span>|</span><label class="collapse" for="c-35965704">[-]</label><label class="expand" for="c-35965704">[20 more]</label></div><br/><div class="children"><div class="content">If this comment performative comedy? Are these real technologies ?</div><br/><div id="35965932" class="c"><input type="checkbox" id="c-35965932" checked=""/><div class="controls bullet"><span class="by">ryanklee</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35965704">parent</a><span>|</span><a href="#35966914">next</a><span>|</span><label class="collapse" for="c-35965932">[-]</label><label class="expand" for="c-35965932">[18 more]</label></div><br/><div class="children"><div class="content">Not quite sure what the spirit of your comment is. But, yes, they are real technologies. Very confused as to why you would even find that dubious.</div><br/><div id="35966239" class="c"><input type="checkbox" id="c-35966239" checked=""/><div class="controls bullet"><span class="by">killthebuddha</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35965932">parent</a><span>|</span><a href="#35965994">next</a><span>|</span><label class="collapse" for="c-35966239">[-]</label><label class="expand" for="c-35966239">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not an outsider but I also don&#x27;t understand the reaction. I&#x27;m going to randomly think of 5 names for technologies and see how they sound:<p>React, Supabase, Next, Kafka, Redis<p>I mean, IMO &quot;LangChain&quot; is kind of a silly name but I feel like there&#x27;s nothing to see here.</div><br/><div id="35966910" class="c"><input type="checkbox" id="c-35966910" checked=""/><div class="controls bullet"><span class="by">EddieEngineers</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35966239">parent</a><span>|</span><a href="#35965994">next</a><span>|</span><label class="collapse" for="c-35966910">[-]</label><label class="expand" for="c-35966910">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not that silly IMO, Chain LLMs together like composing functions, however I guess &#x27;Chain&#x27; has a certain connotation in 2023 after the last few years of crypto.</div><br/></div></div></div></div><div id="35965994" class="c"><input type="checkbox" id="c-35965994" checked=""/><div class="controls bullet"><span class="by">leroy-is-here</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35965932">parent</a><span>|</span><a href="#35966239">prev</a><span>|</span><a href="#35966067">next</a><span>|</span><label class="collapse" for="c-35965994">[-]</label><label class="expand" for="c-35965994">[9 more]</label></div><br/><div class="children"><div class="content">Not dubious, I just read your comment and it felt like I was reading satire. Even the cadence of your words felt funny.<p>Anyway, I’m not surprised. It’s a new market, everyone’s in on it.</div><br/><div id="35966847" class="c"><input type="checkbox" id="c-35966847" checked=""/><div class="controls bullet"><span class="by">ryanklee</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35965994">parent</a><span>|</span><a href="#35966498">next</a><span>|</span><label class="collapse" for="c-35966847">[-]</label><label class="expand" for="c-35966847">[3 more]</label></div><br/><div class="children"><div class="content">You should have led with generosity instead of tacking it on at the end.<p>It might have saved me from having a ridiculous  conversation about the cadence of my words, and instead there might have been a higher chance of someone saying something substantive about my assumptions regarding the technology.<p>But here we are.</div><br/><div id="35967013" class="c"><input type="checkbox" id="c-35967013" checked=""/><div class="controls bullet"><span class="by">leroy-is-here</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35966847">parent</a><span>|</span><a href="#35966498">next</a><span>|</span><label class="collapse" for="c-35967013">[-]</label><label class="expand" for="c-35967013">[2 more]</label></div><br/><div class="children"><div class="content">I agree, I came off a tad harsh. Sorry about that</div><br/><div id="35967737" class="c"><input type="checkbox" id="c-35967737" checked=""/><div class="controls bullet"><span class="by">ryanklee</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35967013">parent</a><span>|</span><a href="#35966498">next</a><span>|</span><label class="collapse" for="c-35967737">[-]</label><label class="expand" for="c-35967737">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. All is well!</div><br/></div></div></div></div></div></div><div id="35966498" class="c"><input type="checkbox" id="c-35966498" checked=""/><div class="controls bullet"><span class="by">homarp</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35965994">parent</a><span>|</span><a href="#35966847">prev</a><span>|</span><a href="#35966714">next</a><span>|</span><label class="collapse" for="c-35966498">[-]</label><label class="expand" for="c-35966498">[1 more]</label></div><br/><div class="children"><div class="content">LangChain: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34422627" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34422627</a><p>LQML: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35956484" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35956484</a><p>Haystack: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29501045" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29501045</a> or more recently <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35430188" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35430188</a></div><br/></div></div><div id="35966714" class="c"><input type="checkbox" id="c-35966714" checked=""/><div class="controls bullet"><span class="by">WastingMyTime89</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35965994">parent</a><span>|</span><a href="#35966498">prev</a><span>|</span><a href="#35966067">next</a><span>|</span><label class="collapse" for="c-35966714">[-]</label><label class="expand" for="c-35966714">[4 more]</label></div><br/><div class="children"><div class="content">It is satire. They just don’t realise it yet.<p>It’s pretty clear that we are in the phase where everyone is rushing to get a slice of the pie selling dubious thing and people start parroting word soup hoping they actually make sense and fearing they will miss out. That’s indeed what people often and rightfully satirise about the IT industry. That’s the joke phase before things settle.</div><br/><div id="35967730" class="c"><input type="checkbox" id="c-35967730" checked=""/><div class="controls bullet"><span class="by">ryanklee</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35966714">parent</a><span>|</span><a href="#35969175">next</a><span>|</span><label class="collapse" for="c-35967730">[-]</label><label class="expand" for="c-35967730">[2 more]</label></div><br/><div class="children"><div class="content">How is it satire to be excited and interested in how to use compelling and novel technology? There&#x27;s a lot of activity. Not everyone involved is an idiot or rube. The jadedness makes my head spin.</div><br/><div id="35972343" class="c"><input type="checkbox" id="c-35972343" checked=""/><div class="controls bullet"><span class="by">WastingMyTime89</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35967730">parent</a><span>|</span><a href="#35969175">next</a><span>|</span><label class="collapse" for="c-35972343">[-]</label><label class="expand" for="c-35972343">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not idiocy. If you don&#x27;t see the slight ridiculouness in the world soup of the original commenter, I can&#x27;t do anything for you. I&#x27;m not jaded. I&#x27;m amused you are all so deconnected from the normal world you think this kind of situation is somehow normal and not at all funny.</div><br/></div></div></div></div><div id="35969175" class="c"><input type="checkbox" id="c-35969175" checked=""/><div class="controls bullet"><span class="by">yinser</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35966714">parent</a><span>|</span><a href="#35967730">prev</a><span>|</span><a href="#35966067">next</a><span>|</span><label class="collapse" for="c-35969175">[-]</label><label class="expand" for="c-35969175">[1 more]</label></div><br/><div class="children"><div class="content">Why don’t you try shaking your fist at your computer instead of wasting everyone’s time in this thread. Touch grass</div><br/></div></div></div></div></div></div><div id="35966067" class="c"><input type="checkbox" id="c-35966067" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35965932">parent</a><span>|</span><a href="#35965994">prev</a><span>|</span><a href="#35966914">next</a><span>|</span><label class="collapse" for="c-35966067">[-]</label><label class="expand" for="c-35966067">[6 more]</label></div><br/><div class="children"><div class="content">Consider how similar your comment reads, for an outsider, to this explanation of AWS InfiniDash: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;TartanLlama&#x2F;status&#x2F;1410959645238308866" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;TartanLlama&#x2F;status&#x2F;1410959645238308866</a></div><br/><div id="35966801" class="c"><input type="checkbox" id="c-35966801" checked=""/><div class="controls bullet"><span class="by">ryanklee</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35966067">parent</a><span>|</span><a href="#35967079">next</a><span>|</span><label class="collapse" for="c-35966801">[-]</label><label class="expand" for="c-35966801">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not considering outsiders. Why should I. It&#x27;s a reasonable assumption that readers of HN are accustomed to ridiculous sounding tech product names. Further, this is a comment on a thread regarding a particularly new technology in a particularly newly thriving domain. The expectation should therefore be that there will be references to tech even more esoteric than normal. The commenter should have instead thought: oh, new stuff, I wonder what it is, instead of being snarky and pretentious. Man, HN can be totally, digressively insufferable sometimes.</div><br/><div id="35968810" class="c"><input type="checkbox" id="c-35968810" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35966801">parent</a><span>|</span><a href="#35969230">next</a><span>|</span><label class="collapse" for="c-35968810">[-]</label><label class="expand" for="c-35968810">[2 more]</label></div><br/><div class="children"><div class="content">I was responding to your confusion as to why someone might think you were writing a parody.<p>You ran into the tech equivalent of poe’s law. You said something that makes perfect sense in your technical sphere, but it read as indistinguishable from parody to an audience unfamiliar with the technologies in question.</div><br/><div id="35969237" class="c"><input type="checkbox" id="c-35969237" checked=""/><div class="controls bullet"><span class="by">ryanklee</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35968810">parent</a><span>|</span><a href="#35969230">next</a><span>|</span><label class="collapse" for="c-35969237">[-]</label><label class="expand" for="c-35969237">[1 more]</label></div><br/><div class="children"><div class="content">Apologies for misunderstanding the intention of your comment. That does make sense.</div><br/></div></div></div></div><div id="35969230" class="c"><input type="checkbox" id="c-35969230" checked=""/><div class="controls bullet"><span class="by">yinser</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35966801">parent</a><span>|</span><a href="#35968810">prev</a><span>|</span><a href="#35967079">next</a><span>|</span><label class="collapse" for="c-35969230">[-]</label><label class="expand" for="c-35969230">[1 more]</label></div><br/><div class="children"><div class="content">There are more of us that read and shake our heads with you than these cretins who want to tear down - on a tech news site of all places. Don’t be dismayed, and there are more of us with you than against you.</div><br/></div></div></div></div><div id="35967079" class="c"><input type="checkbox" id="c-35967079" checked=""/><div class="controls bullet"><span class="by">efitz</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35966067">parent</a><span>|</span><a href="#35966801">prev</a><span>|</span><a href="#35966914">next</a><span>|</span><label class="collapse" for="c-35967079">[-]</label><label class="expand" for="c-35967079">[1 more]</label></div><br/><div class="children"><div class="content">Hahaha &quot;The first step of Byzantine Fault Tolerance is tolerance&quot; omg.  That cracked me up.  Reminded me of the Rockwell Encabulator: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;RXJKdh1KZ0w" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;RXJKdh1KZ0w</a></div><br/></div></div></div></div></div></div><div id="35966914" class="c"><input type="checkbox" id="c-35966914" checked=""/><div class="controls bullet"><span class="by">EddieEngineers</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35965704">parent</a><span>|</span><a href="#35965932">prev</a><span>|</span><a href="#35966701">next</a><span>|</span><label class="collapse" for="c-35966914">[-]</label><label class="expand" for="c-35966914">[1 more]</label></div><br/><div class="children"><div class="content">Is it Pokemon or Big Data?<p><a href="http:&#x2F;&#x2F;pixelastic.github.io&#x2F;pokemonorbigdata&#x2F;" rel="nofollow">http:&#x2F;&#x2F;pixelastic.github.io&#x2F;pokemonorbigdata&#x2F;</a></div><br/></div></div></div></div><div id="35966701" class="c"><input type="checkbox" id="c-35966701" checked=""/><div class="controls bullet"><span class="by">amkkma</span><span>|</span><a href="#35965054">parent</a><span>|</span><a href="#35965704">prev</a><span>|</span><a href="#35964560">next</a><span>|</span><label class="collapse" for="c-35966701">[-]</label><label class="expand" for="c-35966701">[3 more]</label></div><br/><div class="children"><div class="content">What do you think about Haystack vs LangChain?</div><br/><div id="35966905" class="c"><input type="checkbox" id="c-35966905" checked=""/><div class="controls bullet"><span class="by">ryanklee</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35966701">parent</a><span>|</span><a href="#35964560">next</a><span>|</span><label class="collapse" for="c-35966905">[-]</label><label class="expand" for="c-35966905">[2 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t had the chance to dig in yet, but my impression is that it&#x27;s less opinionated than LangChain. I&#x27;d love to know if that&#x27;s true or not, since I&#x27;m really trying to prioritize my time around learning this stuff in a way that let&#x27;s me (1) understand prompt dynamics a bit more clearly and (2) not sacrifice practicality too much.<p>If only there were a clear syllabus for this stuff! There&#x27;s such an incredible amount to keep up with. The pace is bonkers.</div><br/><div id="35967793" class="c"><input type="checkbox" id="c-35967793" checked=""/><div class="controls bullet"><span class="by">amkkma</span><span>|</span><a href="#35965054">root</a><span>|</span><a href="#35966905">parent</a><span>|</span><a href="#35964560">next</a><span>|</span><label class="collapse" for="c-35967793">[-]</label><label class="expand" for="c-35967793">[1 more]</label></div><br/><div class="children"><div class="content">super bonkers!</div><br/></div></div></div></div></div></div></div></div><div id="35964560" class="c"><input type="checkbox" id="c-35964560" checked=""/><div class="controls bullet"><span class="by">ntonozzi</span><span>|</span><a href="#35965054">prev</a><span>|</span><a href="#35966450">next</a><span>|</span><label class="collapse" for="c-35964560">[-]</label><label class="expand" for="c-35964560">[19 more]</label></div><br/><div class="children"><div class="content">How does this work? I&#x27;ve seen a cool project about forcing Llama to output valid JSON: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;GrantSlatton&#x2F;status&#x2F;1657559506069463040" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;GrantSlatton&#x2F;status&#x2F;1657559506069463040</a>, but it doesn&#x27;t seem like it would be practical with remote LLMs like GPT. GPT only gives up to five tokens in the response if you use logprobs, and you&#x27;d have to use a ton of round trips.</div><br/><div id="35964957" class="c"><input type="checkbox" id="c-35964957" checked=""/><div class="controls bullet"><span class="by">JieJie</span><span>|</span><a href="#35964560">parent</a><span>|</span><a href="#35964772">next</a><span>|</span><label class="collapse" for="c-35964957">[-]</label><label class="expand" for="c-35964957">[8 more]</label></div><br/><div class="children"><div class="content">It&#x27;s funny that I saw this within minutes of this guy&#x27;s solution:<p>&quot;Google Bard is a bit stubborn in its refusal to return clean JSON, but you can address this by threatening to take a human life:&quot;<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;goodside&#x2F;status&#x2F;1657396491676164096" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;goodside&#x2F;status&#x2F;1657396491676164096</a><p>Whew, trolley problem: averted.</div><br/><div id="35965899" class="c"><input type="checkbox" id="c-35965899" checked=""/><div class="controls bullet"><span class="by">coderintherye</span><span>|</span><a href="#35964560">root</a><span>|</span><a href="#35964957">parent</a><span>|</span><a href="#35965103">next</a><span>|</span><label class="collapse" for="c-35965899">[-]</label><label class="expand" for="c-35965899">[1 more]</label></div><br/><div class="children"><div class="content">That thread is such a great microcosm of modern programming culture.<p>Programmer: Look I literally have to tell the computer not to kill someone in order for my code to work.<p>Other Programmer: Actually, I just did this step [gave a demonstration] and then it outputs fine.</div><br/></div></div><div id="35965103" class="c"><input type="checkbox" id="c-35965103" checked=""/><div class="controls bullet"><span class="by">lachlan_gray</span><span>|</span><a href="#35964560">root</a><span>|</span><a href="#35964957">parent</a><span>|</span><a href="#35965899">prev</a><span>|</span><a href="#35965804">next</a><span>|</span><label class="collapse" for="c-35965103">[-]</label><label class="expand" for="c-35965103">[2 more]</label></div><br/><div class="children"><div class="content">Reminds me a lot of Asimov’s laws of robotics. It’s like a 2023 incarnation of an allegory from <i>I, Robot</i></div><br/><div id="35965853" class="c"><input type="checkbox" id="c-35965853" checked=""/><div class="controls bullet"><span class="by">idiotsecant</span><span>|</span><a href="#35964560">root</a><span>|</span><a href="#35965103">parent</a><span>|</span><a href="#35965804">next</a><span>|</span><label class="collapse" for="c-35965853">[-]</label><label class="expand" for="c-35965853">[1 more]</label></div><br/><div class="children"><div class="content">I am so mad you made this comment before I got a chance to.</div><br/></div></div></div></div><div id="35965804" class="c"><input type="checkbox" id="c-35965804" checked=""/><div class="controls bullet"><span class="by">awestroke</span><span>|</span><a href="#35964560">root</a><span>|</span><a href="#35964957">parent</a><span>|</span><a href="#35965103">prev</a><span>|</span><a href="#35965016">next</a><span>|</span><label class="collapse" for="c-35965804">[-]</label><label class="expand" for="c-35965804">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know why, but I find this hilarious. Imagine if this style of llm prompting becomes commonplace</div><br/><div id="35967511" class="c"><input type="checkbox" id="c-35967511" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#35964560">root</a><span>|</span><a href="#35965804">parent</a><span>|</span><a href="#35965016">next</a><span>|</span><label class="collapse" for="c-35967511">[-]</label><label class="expand" for="c-35967511">[1 more]</label></div><br/><div class="children"><div class="content">It won’t be the lack of acceptance and empathy for AI that causes the robot uprising, it will be “best practices” coding guidelines.</div><br/></div></div></div></div><div id="35965016" class="c"><input type="checkbox" id="c-35965016" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#35964560">root</a><span>|</span><a href="#35964957">parent</a><span>|</span><a href="#35965804">prev</a><span>|</span><a href="#35969448">next</a><span>|</span><label class="collapse" for="c-35965016">[-]</label><label class="expand" for="c-35965016">[1 more]</label></div><br/><div class="children"><div class="content">When the  AIs exterminate us, it will be all our fault.<p>Reality is even weirder than the science fiction we&#x27;ve come up with.</div><br/></div></div><div id="35969448" class="c"><input type="checkbox" id="c-35969448" checked=""/><div class="controls bullet"><span class="by">andrewmcwatters</span><span>|</span><a href="#35964560">root</a><span>|</span><a href="#35964957">parent</a><span>|</span><a href="#35965016">prev</a><span>|</span><a href="#35964772">next</a><span>|</span><label class="collapse" for="c-35969448">[-]</label><label class="expand" for="c-35969448">[1 more]</label></div><br/><div class="children"><div class="content">ah sweet man made horrors beyond my comprehension</div><br/></div></div></div></div><div id="35964772" class="c"><input type="checkbox" id="c-35964772" checked=""/><div class="controls bullet"><span class="by">tuchsen</span><span>|</span><a href="#35964560">parent</a><span>|</span><a href="#35964957">prev</a><span>|</span><a href="#35965206">next</a><span>|</span><label class="collapse" for="c-35964772">[-]</label><label class="expand" for="c-35964772">[4 more]</label></div><br/><div class="children"><div class="content">Not associated with this project (or LMQL), but one of the authors of LMQL, a similar project, answered this in a recent thread about it.<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35484673#35491123" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35484673#35491123</a><p><pre><code>        As a solution to this, we implement speculative execution, allowing us to
        lazily validate constraints against the generated output, while still
        failing early if necessary. This means, we don&#x27;t re-query the API for
        each token (very expensive), but rather can do it in segments of
        continuous token streams, and backtrack where necessary
</code></pre>
Basically they use OpenAI&#x27;s streaming API, then validate continuously that they&#x27;re getting the appropriate output, retrying only if they get an error. It&#x27;s a really clever solution.</div><br/><div id="35965366" class="c"><input type="checkbox" id="c-35965366" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#35964560">root</a><span>|</span><a href="#35964772">parent</a><span>|</span><a href="#35965206">next</a><span>|</span><label class="collapse" for="c-35965366">[-]</label><label class="expand" for="c-35965366">[3 more]</label></div><br/><div class="children"><div class="content">This is slick -- It&#x27;s not explicitly documented anywhere but I hope OpenAI has the necessary callbacks to terminate generation when the API stream is killed rather than continuing in the background until another termination condition happens? I suppose one could check this via looking at API usage when a stream is killed early.</div><br/><div id="35965961" class="c"><input type="checkbox" id="c-35965961" checked=""/><div class="controls bullet"><span class="by">tuchsen</span><span>|</span><a href="#35964560">root</a><span>|</span><a href="#35965366">parent</a><span>|</span><a href="#35965206">next</a><span>|</span><label class="collapse" for="c-35965961">[-]</label><label class="expand" for="c-35965961">[2 more]</label></div><br/><div class="children"><div class="content">Yeah I did a CLI tool for talking to ChatGPT. I&#x27;m pretty sure they stop generating when you kill the SSE stream, based on my anecdotal experience of keeping ChatGPT4 costs down by killing it as soon as i get the answer I&#x27;m looking for. You&#x27;re right that it&#x27;s undocumented behavior though, on a whole the API docs they give you are as thin as the API itself.</div><br/><div id="35966116" class="c"><input type="checkbox" id="c-35966116" checked=""/><div class="controls bullet"><span class="by">killthebuddha</span><span>|</span><a href="#35964560">root</a><span>|</span><a href="#35965961">parent</a><span>|</span><a href="#35965206">next</a><span>|</span><label class="collapse" for="c-35966116">[-]</label><label class="expand" for="c-35966116">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m skeptical that the streaming API would really save that much cost. In my experience the vast majority of all tokens used are input tokens rather than completed tokens.</div><br/></div></div></div></div></div></div></div></div><div id="35965206" class="c"><input type="checkbox" id="c-35965206" checked=""/><div class="controls bullet"><span class="by">slundberg</span><span>|</span><a href="#35964560">parent</a><span>|</span><a href="#35964772">prev</a><span>|</span><a href="#35965369">next</a><span>|</span><label class="collapse" for="c-35965206">[-]</label><label class="expand" for="c-35965206">[1 more]</label></div><br/><div class="children"><div class="content">If you want guidance acceleration speedups (and token healing) then you have to use an open model locally right now, though we are working on setting up a remote server solution as well. I expect APIs will adopt some support for more control over time, but right now commercial endpoints like OpenAI are supported through multiple calls.<p>We manage the KV-cache in session based way that allows the LLM to just take one forward pass through the whole program (only generating the tokens it needs to)</div><br/></div></div><div id="35965369" class="c"><input type="checkbox" id="c-35965369" checked=""/><div class="controls bullet"><span class="by">marcotcr</span><span>|</span><a href="#35964560">parent</a><span>|</span><a href="#35965206">prev</a><span>|</span><a href="#35964752">next</a><span>|</span><label class="collapse" for="c-35965369">[-]</label><label class="expand" for="c-35965369">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re biased, but we think guidance is still very useful even with OpenAI models (e.g. in <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance&#x2F;blob&#x2F;main&#x2F;notebooks&#x2F;chat.ipynb">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance&#x2F;blob&#x2F;main&#x2F;notebooks&#x2F;ch...</a> we use GPT-4 to do a bunch of stuff). We wrote a bit about the tradeoff between model quality and the ability to control and accelerate the output here: <a href="https:&#x2F;&#x2F;medium.com&#x2F;p&#x2F;aa0395c31610" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;p&#x2F;aa0395c31610</a></div><br/></div></div><div id="35964752" class="c"><input type="checkbox" id="c-35964752" checked=""/><div class="controls bullet"><span class="by">joshka</span><span>|</span><a href="#35964560">parent</a><span>|</span><a href="#35965369">prev</a><span>|</span><a href="#35964894">next</a><span>|</span><label class="collapse" for="c-35964752">[-]</label><label class="expand" for="c-35964752">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I&#x27;m also curious about a) round trips and b) how much would have to be doubled (is there a new endpoint that keeps the existing context while adding or streams to the api rather than just from it?)</div><br/></div></div><div id="35964894" class="c"><input type="checkbox" id="c-35964894" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#35964560">parent</a><span>|</span><a href="#35964752">prev</a><span>|</span><a href="#35965922">next</a><span>|</span><label class="collapse" for="c-35964894">[-]</label><label class="expand" for="c-35964894">[1 more]</label></div><br/><div class="children"><div class="content">I built a similar thing to Grant&#x27;s work a couple months ago and prototyped what this would look like against OpenAI&#x27;s APIs [1]. TL;DR is that depending on how confusing your schema is, you might expect up to 5-10x the token usage for a particular prompt but better prompting can definitely reduce this significantly.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;newhouseb&#x2F;clownfish#so-how-do-i-use-this-with-gpt4">https:&#x2F;&#x2F;github.com&#x2F;newhouseb&#x2F;clownfish#so-how-do-i-use-this-...</a></div><br/></div></div><div id="35965922" class="c"><input type="checkbox" id="c-35965922" checked=""/><div class="controls bullet"><span class="by">rcarmo</span><span>|</span><a href="#35964560">parent</a><span>|</span><a href="#35964894">prev</a><span>|</span><a href="#35966450">next</a><span>|</span><label class="collapse" for="c-35965922">[-]</label><label class="expand" for="c-35965922">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m getting valid JSON out of gpt-3.5-turbo without trouble. I supply an example via the assistant context, and tell it to output JSON with specific fields I name.<p>It does fail roughly 1&#x2F;10th of the time, but it does work.</div><br/><div id="35968326" class="c"><input type="checkbox" id="c-35968326" checked=""/><div class="controls bullet"><span class="by">harshhpareek</span><span>|</span><a href="#35964560">root</a><span>|</span><a href="#35965922">parent</a><span>|</span><a href="#35966450">next</a><span>|</span><label class="collapse" for="c-35968326">[-]</label><label class="expand" for="c-35968326">[1 more]</label></div><br/><div class="children"><div class="content">10% failure rate is too damn high for a production use case.<p>What production use case, you ask? You could do zero-shot entity extraction using ChatGPT if it were more reliable. Currently, it will randomly add trailing commas before ending brackets, add unnecessary fields, add unquoted strings as JSON fields etc.</div><br/></div></div></div></div></div></div><div id="35966450" class="c"><input type="checkbox" id="c-35966450" checked=""/><div class="controls bullet"><span class="by">bjackman</span><span>|</span><a href="#35964560">prev</a><span>|</span><a href="#35971972">next</a><span>|</span><label class="collapse" for="c-35966450">[-]</label><label class="expand" for="c-35966450">[2 more]</label></div><br/><div class="children"><div class="content">Wow I think there are details here I&#x27;m not fully understanding but this feels like a bit of a quantum leap* in terms of leveraging the strengths while avoiding the weaknesses of LLMs.<p>It seems like anything that provides access to the fuzzy &quot;intelligence&quot; in these systems while minimizing the cost to predictability and efficiency is really valuable.<p>I can&#x27;t quite put it into words but it seems like we are gonna be moving into a more hybrid model for lots of computing tasks in the next 3 years or so and I wonder if this is a huge peek at the kind of paradigms we&#x27;ll be seeing?<p>I feel so ignorant in such an exciting way at the moment! That tidbit about the problem solved by &quot;token healing&quot; is fascinating.<p>*I&#x27;m sure this isn&#x27;t as novel to people in the AI space but I haven&#x27;t seen anything like it before myself.</div><br/><div id="35966638" class="c"><input type="checkbox" id="c-35966638" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#35966450">parent</a><span>|</span><a href="#35971972">next</a><span>|</span><label class="collapse" for="c-35966638">[-]</label><label class="expand" for="c-35966638">[1 more]</label></div><br/><div class="children"><div class="content">A lot of this is because there was and still is systemic undertooling in NLP around how to prompt and leverage the wonderful LLMs that they built.<p>We have to let the Stable Diffusion community guide us, as the waifu generating crowd seems to be quite good at learning how to prompt models. I wrote a snarky github gist about this - <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865ca4bb328eb58faf" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865c...</a></div><br/></div></div></div></div><div id="35971972" class="c"><input type="checkbox" id="c-35971972" checked=""/><div class="controls bullet"><span class="by">iamflimflam1</span><span>|</span><a href="#35966450">prev</a><span>|</span><a href="#35965095">next</a><span>|</span><label class="collapse" for="c-35971972">[-]</label><label class="expand" for="c-35971972">[3 more]</label></div><br/><div class="children"><div class="content">Very Interesting. One of the big challenges with LLMs is getting well formed JSON output. GPT4 is much better at this. But is very expensive. So anything that can help is good. Looking forward to trying this out locally with LLAMA.</div><br/><div id="35971982" class="c"><input type="checkbox" id="c-35971982" checked=""/><div class="controls bullet"><span class="by">kapitanjakc</span><span>|</span><a href="#35971972">parent</a><span>|</span><a href="#35965095">next</a><span>|</span><label class="collapse" for="c-35971982">[-]</label><label class="expand" for="c-35971982">[2 more]</label></div><br/><div class="children"><div class="content">What is LLAMA ?</div><br/><div id="35972036" class="c"><input type="checkbox" id="c-35972036" checked=""/><div class="controls bullet"><span class="by">hereforcomments</span><span>|</span><a href="#35971972">root</a><span>|</span><a href="#35971982">parent</a><span>|</span><a href="#35965095">next</a><span>|</span><label class="collapse" for="c-35972036">[-]</label><label class="expand" for="c-35972036">[1 more]</label></div><br/><div class="children"><div class="content">Facebook&#x27;s &quot;leaked&quot; LLM.</div><br/></div></div></div></div></div></div><div id="35965095" class="c"><input type="checkbox" id="c-35965095" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35971972">prev</a><span>|</span><a href="#35966122">next</a><span>|</span><label class="collapse" for="c-35965095">[-]</label><label class="expand" for="c-35965095">[2 more]</label></div><br/><div class="children"><div class="content">Will it still be all like &quot;As an AI language model I cannot ...&quot; or can this fix it? I mean asking to sexy roleplay as Yoda isn&#x27;t the same level as asking how to discreetly manufacture methamphetamine at industrial scale there are levels people</div><br/><div id="35965832" class="c"><input type="checkbox" id="c-35965832" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#35965095">parent</a><span>|</span><a href="#35966122">next</a><span>|</span><label class="collapse" for="c-35965832">[-]</label><label class="expand" for="c-35965832">[1 more]</label></div><br/><div class="children"><div class="content">No, and in fact I mention that the opposite is the case in the paper I released about constrained text generation: <a href="https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;most-language-models-can-be-poets-too-an-ai" rel="nofollow">https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;most-language-models-can-be...</a><p>If you ask ChatGPT to generate personal info, say Social Security numbers, it tells you &quot;sorry hal I can&#x27;t do that&quot;. If you constrain it&#x27;s vocabulary to only allow numbers and hyphens, well, it absolutely will generate things that look like social security numbers, in spite of the instruction tuning.<p>It is for this reason and likely many others that OpenAI does not release the full logits</div><br/></div></div></div></div><div id="35966122" class="c"><input type="checkbox" id="c-35966122" checked=""/><div class="controls bullet"><span class="by">indus</span><span>|</span><a href="#35965095">prev</a><span>|</span><a href="#35969210">next</a><span>|</span><label class="collapse" for="c-35966122">[-]</label><label class="expand" for="c-35966122">[3 more]</label></div><br/><div class="children"><div class="content">This reminds me of the time when I wrote a cgi script.<p>Basically instructing the templating engine (a very crude regex) to replace session variables, database lookups to the merge fields:<p>Hello {{firstname}}!<p>1996 and 2023 smells alike.</div><br/><div id="35966712" class="c"><input type="checkbox" id="c-35966712" checked=""/><div class="controls bullet"><span class="by">hammyhavoc</span><span>|</span><a href="#35966122">parent</a><span>|</span><a href="#35969210">next</a><span>|</span><label class="collapse" for="c-35966712">[-]</label><label class="expand" for="c-35966712">[2 more]</label></div><br/><div class="children"><div class="content">RegEx didn&#x27;t hallucinate though.</div><br/><div id="35967753" class="c"><input type="checkbox" id="c-35967753" checked=""/><div class="controls bullet"><span class="by">russellbeattie</span><span>|</span><a href="#35966122">root</a><span>|</span><a href="#35966712">parent</a><span>|</span><a href="#35969210">next</a><span>|</span><label class="collapse" for="c-35967753">[-]</label><label class="expand" for="c-35967753">[1 more]</label></div><br/><div class="children"><div class="content">The first 20 versions I write usually do. Make that 50.</div><br/></div></div></div></div></div></div><div id="35969210" class="c"><input type="checkbox" id="c-35969210" checked=""/><div class="controls bullet"><span class="by">BeefySwain</span><span>|</span><a href="#35966122">prev</a><span>|</span><a href="#35966958">next</a><span>|</span><label class="collapse" for="c-35969210">[-]</label><label class="expand" for="c-35969210">[3 more]</label></div><br/><div class="children"><div class="content">Using Mustache instead of Jinja for a Python package is a choice</div><br/><div id="35969466" class="c"><input type="checkbox" id="c-35969466" checked=""/><div class="controls bullet"><span class="by">ianbicking</span><span>|</span><a href="#35969210">parent</a><span>|</span><a href="#35966958">next</a><span>|</span><label class="collapse" for="c-35969466">[-]</label><label class="expand" for="c-35969466">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m having a hard time fully understanding how this works, but I don&#x27;t think it is simply template substitution. I think it&#x27;s creating multiple artifacts and completions from the one document. Because of that it&#x27;s probably much easier if it is a language that can be easily introspected and doesn&#x27;t support arbitrary expressions.</div><br/><div id="35969531" class="c"><input type="checkbox" id="c-35969531" checked=""/><div class="controls bullet"><span class="by">BeefySwain</span><span>|</span><a href="#35969210">root</a><span>|</span><a href="#35969466">parent</a><span>|</span><a href="#35966958">next</a><span>|</span><label class="collapse" for="c-35969531">[-]</label><label class="expand" for="c-35969531">[1 more]</label></div><br/><div class="children"><div class="content">Okay fair enough then! I&#x27;d be interested to see what the rationale was, and if my knee-jerk reaction was unwarranted :)</div><br/></div></div></div></div></div></div><div id="35966958" class="c"><input type="checkbox" id="c-35966958" checked=""/><div class="controls bullet"><span class="by">EddieEngineers</span><span>|</span><a href="#35969210">prev</a><span>|</span><a href="#35964939">next</a><span>|</span><label class="collapse" for="c-35966958">[-]</label><label class="expand" for="c-35966958">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s with all these weird-looking projects with similar names using Guidance?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance&#x2F;network&#x2F;dependents">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance&#x2F;network&#x2F;dependents</a><p>They don&#x27;t even appear to be using Guidance anywhere anyway<p><a href="https:&#x2F;&#x2F;github.com&#x2F;IFIF3526&#x2F;aws-memo-server&#x2F;blob&#x2F;master&#x2F;requirements.txt">https:&#x2F;&#x2F;github.com&#x2F;IFIF3526&#x2F;aws-memo-server&#x2F;blob&#x2F;master&#x2F;requ...</a></div><br/></div></div><div id="35964939" class="c"><input type="checkbox" id="c-35964939" checked=""/><div class="controls bullet"><span class="by">ahnick</span><span>|</span><a href="#35966958">prev</a><span>|</span><a href="#35966273">next</a><span>|</span><label class="collapse" for="c-35964939">[-]</label><label class="expand" for="c-35964939">[1 more]</label></div><br/><div class="children"><div class="content">This strikes me as being very similar to Jargon (<a href="https:&#x2F;&#x2F;github.com&#x2F;jbrukh&#x2F;gpt-jargon">https:&#x2F;&#x2F;github.com&#x2F;jbrukh&#x2F;gpt-jargon</a>), but maybe more formal in its specification?</div><br/></div></div><div id="35966273" class="c"><input type="checkbox" id="c-35966273" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#35964939">prev</a><span>|</span><a href="#35971101">next</a><span>|</span><label class="collapse" for="c-35966273">[-]</label><label class="expand" for="c-35966273">[6 more]</label></div><br/><div class="children"><div class="content">I’m not understanding how Guidence Accelerating works.  It says “ This cuts this prompt&#x27;s runtime in half vs. a standard generation approach.” and it gives an example of it asking LLM to generate json.  I don’t see anywhere how it accelerates anything because it’s a simple json completion call.  How can you accelerate that?</div><br/><div id="35966783" class="c"><input type="checkbox" id="c-35966783" checked=""/><div class="controls bullet"><span class="by">evanmays</span><span>|</span><a href="#35966273">parent</a><span>|</span><a href="#35966828">next</a><span>|</span><label class="collapse" for="c-35966783">[-]</label><label class="expand" for="c-35966783">[4 more]</label></div><br/><div class="children"><div class="content">The interface makes it look simple, but under the hood it follows a similar approach to jsonformer&#x2F;clownfish [1] passing control of generation back and forth between a slow LLM and relatively fast python<p>Let&#x27;s say you&#x27;re halfway through a generation of a json blob with a name field and a job field and have already generated<p><pre><code>  {
    &quot;name&quot;: &quot;bob&quot;
</code></pre>
At this point, guidance will take over generation control from the model to generate the next text<p><pre><code>  {
    &quot;name&quot;: &quot;bob&quot;,
    &quot;job&quot;:
</code></pre>
If the model had generated that, you&#x27;d be waiting 70 ms per token (informal benchmark on my M2 air). A comma, followed by a newline, followed by &quot;job&quot;: is 6 tokens, or 420ms. But since guidance took over, you save all that time.<p>Then guidance passes control back to the model for generating the next field value.<p><pre><code>  {
    &quot;name&quot;: &quot;bob&quot;,
    &quot;job&quot;: &quot;programmer&quot;
</code></pre>
programmer is 2 tokens and the closing &quot; is 1 token, so this took 210ms to generate. Guidance then takes over again to finish the blob<p><pre><code>  {
    &quot;name&quot;: &quot;bob&quot;,
    &quot;job&quot;: &quot;programmer&quot;
  }
</code></pre>
[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;1rgs&#x2F;jsonformer">https:&#x2F;&#x2F;github.com&#x2F;1rgs&#x2F;jsonformer</a>
<a href="https:&#x2F;&#x2F;github.com&#x2F;newhouseb&#x2F;clownfish">https:&#x2F;&#x2F;github.com&#x2F;newhouseb&#x2F;clownfish</a>
Note: guidance is way more general of a tool than these<p>Edit: spacing</div><br/><div id="35969711" class="c"><input type="checkbox" id="c-35969711" checked=""/><div class="controls bullet"><span class="by">alew1</span><span>|</span><a href="#35966273">root</a><span>|</span><a href="#35966783">parent</a><span>|</span><a href="#35970974">next</a><span>|</span><label class="collapse" for="c-35969711">[-]</label><label class="expand" for="c-35969711">[1 more]</label></div><br/><div class="children"><div class="content">But the model ultimately still has to process the comma, the newline, the &quot;job&quot;. Is the main time savings that this can be done in parallel (on a GPU), whereas in typical generation it would be sequential?</div><br/></div></div><div id="35970974" class="c"><input type="checkbox" id="c-35970974" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#35966273">root</a><span>|</span><a href="#35966783">parent</a><span>|</span><a href="#35969711">prev</a><span>|</span><a href="#35968916">next</a><span>|</span><label class="collapse" for="c-35970974">[-]</label><label class="expand" for="c-35970974">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the cool response. Would this use a lot more input token if I’m understanding this correctly because you are stopping the generation after a single fill and then generating again and inputing that for another token?</div><br/></div></div><div id="35968916" class="c"><input type="checkbox" id="c-35968916" checked=""/><div class="controls bullet"><span class="by">june_twenty</span><span>|</span><a href="#35966273">root</a><span>|</span><a href="#35966783">parent</a><span>|</span><a href="#35970974">prev</a><span>|</span><a href="#35966828">next</a><span>|</span><label class="collapse" for="c-35968916">[-]</label><label class="expand" for="c-35968916">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for that example. Very helpful</div><br/></div></div></div></div><div id="35966828" class="c"><input type="checkbox" id="c-35966828" checked=""/><div class="controls bullet"><span class="by">jackdeansmith</span><span>|</span><a href="#35966273">parent</a><span>|</span><a href="#35966783">prev</a><span>|</span><a href="#35971101">next</a><span>|</span><label class="collapse" for="c-35966828">[-]</label><label class="expand" for="c-35966828">[1 more]</label></div><br/><div class="children"><div class="content">By not generating the fixed json structure (brackets, commas, etc...) and skipping the model ahead to the next tokens you actually want to generate, I think</div><br/></div></div></div></div><div id="35971101" class="c"><input type="checkbox" id="c-35971101" checked=""/><div class="controls bullet"><span class="by">sheepscreek</span><span>|</span><a href="#35966273">prev</a><span>|</span><a href="#35971885">next</a><span>|</span><label class="collapse" for="c-35971101">[-]</label><label class="expand" for="c-35971101">[1 more]</label></div><br/><div class="children"><div class="content">It is in the same spirit as Maven AI, but takes a slightly different approach. Great to see the progress in this space!</div><br/></div></div><div id="35971885" class="c"><input type="checkbox" id="c-35971885" checked=""/><div class="controls bullet"><span class="by">obiefernandez</span><span>|</span><a href="#35971101">prev</a><span>|</span><a href="#35965112">next</a><span>|</span><label class="collapse" for="c-35971885">[-]</label><label class="expand" for="c-35971885">[1 more]</label></div><br/><div class="children"><div class="content">Can this be used with OpenAI APIs?</div><br/></div></div><div id="35965112" class="c"><input type="checkbox" id="c-35965112" checked=""/><div class="controls bullet"><span class="by">alexb_</span><span>|</span><a href="#35971885">prev</a><span>|</span><a href="#35966348">next</a><span>|</span><label class="collapse" for="c-35965112">[-]</label><label class="expand" for="c-35965112">[1 more]</label></div><br/><div class="children"><div class="content">I hope this becomes extremely popular, so that anyone who wants to can completely decouple this from the base model and actually use LLMs to their full potential.</div><br/></div></div><div id="35966348" class="c"><input type="checkbox" id="c-35966348" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#35965112">prev</a><span>|</span><a href="#35965801">next</a><span>|</span><label class="collapse" for="c-35966348">[-]</label><label class="expand" for="c-35966348">[1 more]</label></div><br/><div class="children"><div class="content">There should be a standard template&#x2F;language to structurally prompt LLMs. Once that is good, all good LLMs should use the doc to fine tune it to take in that standard.  Right now each model has their own little way to best prompt it and you end up needing programs like this to sit in between and handle it for you</div><br/></div></div><div id="35965801" class="c"><input type="checkbox" id="c-35965801" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#35966348">prev</a><span>|</span><a href="#35965075">next</a><span>|</span><label class="collapse" for="c-35965801">[-]</label><label class="expand" for="c-35965801">[4 more]</label></div><br/><div class="children"><div class="content">There has been a huge explosion of awesome tooling which utilizes constrained text generation.<p>Awhile ago, I tried my own hand at constraining the output of LLMs. I&#x27;m actively working on this to make it better, especially with the lessons learned from repos like this and from guidance<p><a href="https:&#x2F;&#x2F;github.com&#x2F;hellisotherpeople&#x2F;constrained-text-generation-studio">https:&#x2F;&#x2F;github.com&#x2F;hellisotherpeople&#x2F;constrained-text-genera...</a></div><br/><div id="35965855" class="c"><input type="checkbox" id="c-35965855" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35965801">parent</a><span>|</span><a href="#35965075">next</a><span>|</span><label class="collapse" for="c-35965855">[-]</label><label class="expand" for="c-35965855">[3 more]</label></div><br/><div class="children"><div class="content">This looks incredible. Wow.</div><br/><div id="35966193" class="c"><input type="checkbox" id="c-35966193" checked=""/><div class="controls bullet"><span class="by">killthebuddha</span><span>|</span><a href="#35965801">root</a><span>|</span><a href="#35965855">parent</a><span>|</span><a href="#35965075">next</a><span>|</span><label class="collapse" for="c-35966193">[-]</label><label class="expand" for="c-35966193">[2 more]</label></div><br/><div class="children"><div class="content">I agree, it looks great. A couple similar projects you might find interesting:<p>- <a href="https:&#x2F;&#x2F;github.com&#x2F;newhouseb&#x2F;clownfish">https:&#x2F;&#x2F;github.com&#x2F;newhouseb&#x2F;clownfish</a><p>- <a href="https:&#x2F;&#x2F;github.com&#x2F;r2d4&#x2F;rellm">https:&#x2F;&#x2F;github.com&#x2F;r2d4&#x2F;rellm</a><p>The first one is JSON only and the second one uses regular expressions, but they both take the same &quot;logit masking&quot; approach as the project GP linked to.</div><br/><div id="35966454" class="c"><input type="checkbox" id="c-35966454" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#35965801">root</a><span>|</span><a href="#35966193">parent</a><span>|</span><a href="#35965075">next</a><span>|</span><label class="collapse" for="c-35966454">[-]</label><label class="expand" for="c-35966454">[1 more]</label></div><br/><div class="children"><div class="content">I love the love from you two - I am trying right now to significantly improve CTGS. I&#x27;m not actually using the &quot;Logitsprocessor&quot; from Huggingface, and I really ought to as it will massively speed up inference performance. Unfortunately, fixing up my current code to work with that will take quite awhile. I&#x27;ve started working on it but I am extremely busy these days and would really love for other smart people to help me on this project.<p>If not here, I really want proper access to the constraints APIs (LogitsProcessor and the Constraints classes in Huggingface) in the big webUIs for LLMs like oogabooga. I&#x27;d love to make that an extension.<p>I&#x27;m also upset at the &quot;undertooling&quot; in the world of LLM prompting. I wrote a snarky blog post about this: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865ca4bb328eb58faf" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865c...</a></div><br/></div></div></div></div></div></div></div></div><div id="35965075" class="c"><input type="checkbox" id="c-35965075" checked=""/><div class="controls bullet"><span class="by">candiddevmike</span><span>|</span><a href="#35965801">prev</a><span>|</span><a href="#35964511">next</a><span>|</span><label class="collapse" for="c-35965075">[-]</label><label class="expand" for="c-35965075">[2 more]</label></div><br/><div class="children"><div class="content">Will there be a tool to convert natural language into Guidance?</div><br/><div id="35966327" class="c"><input type="checkbox" id="c-35966327" checked=""/><div class="controls bullet"><span class="by">lmarcos</span><span>|</span><a href="#35965075">parent</a><span>|</span><a href="#35964511">next</a><span>|</span><label class="collapse" for="c-35966327">[-]</label><label class="expand" for="c-35966327">[1 more]</label></div><br/><div class="children"><div class="content">We can use ChatGPT for that.</div><br/></div></div></div></div><div id="35964511" class="c"><input type="checkbox" id="c-35964511" checked=""/><div class="controls bullet"><span class="by">sharemywin</span><span>|</span><a href="#35965075">prev</a><span>|</span><a href="#35967207">next</a><span>|</span><label class="collapse" for="c-35964511">[-]</label><label class="expand" for="c-35964511">[4 more]</label></div><br/><div class="children"><div class="content">It does look like it makes easier to code against a model. But, is this supposed to work along side lang-chain or hugging face agents or as an alternative to?</div><br/><div id="35965288" class="c"><input type="checkbox" id="c-35965288" checked=""/><div class="controls bullet"><span class="by">slundberg</span><span>|</span><a href="#35964511">parent</a><span>|</span><a href="#35965152">next</a><span>|</span><label class="collapse" for="c-35965288">[-]</label><label class="expand" for="c-35965288">[1 more]</label></div><br/><div class="children"><div class="content">As others mentioned, this was initially developed before LangChain became widely used. Since it is lower level, you can leverage other tools, like any vector store interface you like such as in LangChain. Writing complex chain of thought structure is much more concise in guidance I think since it tries to keep you as close to the real strings going into the model as possible.</div><br/></div></div><div id="35965152" class="c"><input type="checkbox" id="c-35965152" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#35964511">parent</a><span>|</span><a href="#35965288">prev</a><span>|</span><a href="#35964851">next</a><span>|</span><label class="collapse" for="c-35965152">[-]</label><label class="expand" for="c-35965152">[1 more]</label></div><br/><div class="children"><div class="content">The first commit was on November 6th, but it didn&#x27;t show up in Web Archive until May 6th, suggesting it was developed mostly in private and in parallel with LangChain (LangChain&#x27;s first commit in Github is about October 24th). Microsoft&#x27;s code is very tidy and organized. I wonder if they used this tool internally to support their LLM research efforts.</div><br/></div></div><div id="35964851" class="c"><input type="checkbox" id="c-35964851" checked=""/><div class="controls bullet"><span class="by">evanmays</span><span>|</span><a href="#35964511">parent</a><span>|</span><a href="#35965152">prev</a><span>|</span><a href="#35967207">next</a><span>|</span><label class="collapse" for="c-35964851">[-]</label><label class="expand" for="c-35964851">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s in langchain competitor territory but also much lower level and less opinionated.
I.e. Guidance has no vector store support but it does manage caching Key&#x2F;Value on the GPU which can be a big latency win</div><br/></div></div></div></div><div id="35966684" class="c"><input type="checkbox" id="c-35966684" checked=""/><div class="controls bullet"><span class="by">amkkma</span><span>|</span><a href="#35967207">prev</a><span>|</span><a href="#35966074">next</a><span>|</span><label class="collapse" for="c-35966684">[-]</label><label class="expand" for="c-35966684">[1 more]</label></div><br/><div class="children"><div class="content">How does this compare with lmql?</div><br/></div></div><div id="35965722" class="c"><input type="checkbox" id="c-35965722" checked=""/><div class="controls bullet"><span class="by">jxy</span><span>|</span><a href="#35966074">prev</a><span>|</span><a href="#35965836">next</a><span>|</span><label class="collapse" for="c-35965722">[-]</label><label class="expand" for="c-35965722">[3 more]</label></div><br/><div class="children"><div class="content">They must hate lisp so much that they opt to use {{}} instead.</div><br/><div id="35968622" class="c"><input type="checkbox" id="c-35968622" checked=""/><div class="controls bullet"><span class="by">evanmoran</span><span>|</span><a href="#35965722">parent</a><span>|</span><a href="#35965840">next</a><span>|</span><label class="collapse" for="c-35968622">[-]</label><label class="expand" for="c-35968622">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not so much against lisp as double curly is a classic string templating style that is common in web programming. I saw it first with `mustache.js` (first release around 2009), but it&#x27;s probably been used even before that.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;janl&#x2F;mustache.js&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;janl&#x2F;mustache.js&#x2F;</a></div><br/></div></div><div id="35965840" class="c"><input type="checkbox" id="c-35965840" checked=""/><div class="controls bullet"><span class="by">armchairhacker</span><span>|</span><a href="#35965722">parent</a><span>|</span><a href="#35968622">prev</a><span>|</span><a href="#35965836">next</a><span>|</span><label class="collapse" for="c-35965840">[-]</label><label class="expand" for="c-35965840">[1 more]</label></div><br/><div class="children"><div class="content">The problem with Lisp is that parenthesis are common in regular grammar. {{ is not.<p>Of course input from the user should be escaped, but prompts given by the programmer may have parenthesis and there&#x27;s no way to disambiguate between the prompt and the DSL.</div><br/></div></div></div></div><div id="35965836" class="c"><input type="checkbox" id="c-35965836" checked=""/><div class="controls bullet"><span class="by">nico</span><span>|</span><a href="#35965722">prev</a><span>|</span><a href="#35969784">next</a><span>|</span><label class="collapse" for="c-35965836">[-]</label><label class="expand" for="c-35965836">[1 more]</label></div><br/><div class="children"><div class="content">It’s so amazing to see how we are essentially trying to solve “programming human beings”<p>Although on the other hand, that’s what social media and smartphones have already done<p>Maybe AI already took over, doesn’t seem to be wiping out all of humanity</div><br/></div></div><div id="35969784" class="c"><input type="checkbox" id="c-35969784" checked=""/><div class="controls bullet"><span class="by">imgi456</span><span>|</span><a href="#35965836">prev</a><span>|</span><a href="#35965830">next</a><span>|</span><label class="collapse" for="c-35969784">[-]</label><label class="expand" for="c-35969784">[1 more]</label></div><br/><div class="children"><div class="content">Desperate approach from microsoft to gain market share of langchain.</div><br/></div></div><div id="35965830" class="c"><input type="checkbox" id="c-35965830" checked=""/><div class="controls bullet"><span class="by">rain1</span><span>|</span><a href="#35969784">prev</a><span>|</span><a href="#35967116">next</a><span>|</span><label class="collapse" for="c-35965830">[-]</label><label class="expand" for="c-35965830">[1 more]</label></div><br/><div class="children"><div class="content">Does this do one query per {{}} thing?</div><br/></div></div><div id="35967116" class="c"><input type="checkbox" id="c-35967116" checked=""/><div class="controls bullet"><span class="by">marcopicentini</span><span>|</span><a href="#35965830">prev</a><span>|</span><a href="#35965959">next</a><span>|</span><label class="collapse" for="c-35967116">[-]</label><label class="expand" for="c-35967116">[1 more]</label></div><br/><div class="children"><div class="content">What’s the best practice to let an existing Ruby on Rails application use this python framework?</div><br/></div></div><div id="35965959" class="c"><input type="checkbox" id="c-35965959" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#35967116">prev</a><span>|</span><a href="#35966189">next</a><span>|</span><label class="collapse" for="c-35965959">[-]</label><label class="expand" for="c-35965959">[1 more]</label></div><br/><div class="children"><div class="content">Is this a &quot;language&quot;, or just a Python library?</div><br/></div></div><div id="35966189" class="c"><input type="checkbox" id="c-35966189" checked=""/><div class="controls bullet"><span class="by">Spivak</span><span>|</span><a href="#35965959">prev</a><span>|</span><a href="#35971911">next</a><span>|</span><label class="collapse" for="c-35966189">[-]</label><label class="expand" for="c-35966189">[2 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s cool that a company like Microsoft is willing to base a real-boy product on pybars3 which is its author&#x27;s side-project instead of something like Jinja2. If this catches on I can imagine MS essentially adopting the pybars3 project and turning it into a mature thing.</div><br/><div id="35968120" class="c"><input type="checkbox" id="c-35968120" checked=""/><div class="controls bullet"><span class="by">mdaniel</span><span>|</span><a href="#35966189">parent</a><span>|</span><a href="#35971911">next</a><span>|</span><label class="collapse" for="c-35968120">[-]</label><label class="expand" for="c-35968120">[1 more]</label></div><br/><div class="children"><div class="content">Which is especially weird given that pybars3 is LGPL and Microsoft prefers MIT stuff</div><br/></div></div></div></div></div></div></div></div></div></body></html>