<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1692090071808" as="style"/><link rel="stylesheet" href="styles.css?v=1692090071808"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.anyscale.com/blog/continuous-batching-llm-inference">Continuous batch enables 23x throughput in LLM inference and reduce p50 latency</a>Â <span class="domain">(<a href="https://www.anyscale.com">www.anyscale.com</a>)</span></div><div class="subtext"><span>michellezzz</span> | <span>2 comments</span></div><br/><div><div id="37131665" class="c"><input type="checkbox" id="c-37131665" checked=""/><div class="controls bullet"><span class="by">okalldal</span><span>|</span><label class="collapse" for="c-37131665">[-]</label><label class="expand" for="c-37131665">[1 more]</label></div><br/><div class="children"><div class="content">As this article is from some weeks ago and Huggingface has now implemented Paged Attention in text-generation-inference[1], I would assume the benchmark results would be quite different if done today. Would be very interesting to see more recent benchmarks if anyone has done any!<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;text-generation-inference&#x2F;issues&#x2F;478">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;text-generation-inference&#x2F;iss...</a></div><br/></div></div></div></div></div></div></div></body></html>