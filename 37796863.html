<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1696669263483" as="style"/><link rel="stylesheet" href="styles.css?v=1696669263483"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/okuvshynov/slowllama">SlowLlama: Finetune llama2-70B and codellama on MacBook Air without quantization</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>behnamoh</span> | <span>41 comments</span></div><br/><div><div id="37797476" class="c"><input type="checkbox" id="c-37797476" checked=""/><div class="controls bullet"><span class="by">sbierwagen</span><span>|</span><a href="#37797453">next</a><span>|</span><label class="collapse" for="c-37797476">[-]</label><label class="expand" for="c-37797476">[11 more]</label></div><br/><div class="children"><div class="content">What’s the use case here? The author notes doing inference from disk is too slow. Fine tune a model and do nothing with it? Fine tune and then upload it to a cloud machine? Not sure how often I’d like to try uploading multi gigabyte files from a consumer internet connection with narrow upload bandwidth.</div><br/><div id="37797547" class="c"><input type="checkbox" id="c-37797547" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#37797476">parent</a><span>|</span><a href="#37797906">next</a><span>|</span><label class="collapse" for="c-37797547">[-]</label><label class="expand" for="c-37797547">[2 more]</label></div><br/><div class="children"><div class="content">Not sure about 70b, but this is a really useful repo for fine-tuning the 7b models on M1 machines—and all the examples in the README seem to use 7b.</div><br/><div id="37798953" class="c"><input type="checkbox" id="c-37798953" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37797476">root</a><span>|</span><a href="#37797547">parent</a><span>|</span><a href="#37797906">next</a><span>|</span><label class="collapse" for="c-37798953">[-]</label><label class="expand" for="c-37798953">[1 more]</label></div><br/><div class="children"><div class="content">Only LLaMA2, or does it also  work on Mistral-7B?</div><br/></div></div></div></div><div id="37797906" class="c"><input type="checkbox" id="c-37797906" checked=""/><div class="controls bullet"><span class="by">miloignis</span><span>|</span><a href="#37797476">parent</a><span>|</span><a href="#37797547">prev</a><span>|</span><a href="#37799343">next</a><span>|</span><label class="collapse" for="c-37797906">[-]</label><label class="expand" for="c-37797906">[1 more]</label></div><br/><div class="children"><div class="content">Depending on how much ram you have, fine-tune then quanitize &amp; run with llama.cpp, which works quite well.</div><br/></div></div><div id="37799343" class="c"><input type="checkbox" id="c-37799343" checked=""/><div class="controls bullet"><span class="by">moneywoes</span><span>|</span><a href="#37797476">parent</a><span>|</span><a href="#37797906">prev</a><span>|</span><a href="#37797979">next</a><span>|</span><label class="collapse" for="c-37799343">[-]</label><label class="expand" for="c-37799343">[1 more]</label></div><br/><div class="children"><div class="content">fine tune for local use?</div><br/></div></div><div id="37797979" class="c"><input type="checkbox" id="c-37797979" checked=""/><div class="controls bullet"><span class="by">thelastparadise</span><span>|</span><a href="#37797476">parent</a><span>|</span><a href="#37799343">prev</a><span>|</span><a href="#37797453">next</a><span>|</span><label class="collapse" for="c-37797979">[-]</label><label class="expand" for="c-37797979">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Not sure how often I’d like to try uploading multi gigabyte files from a consumer internet connection with narrow upload bandwidth<p>It&#x27;s 2023... this is a fairly standard use case (e.g. YouTubers uploading videos, etc).</div><br/><div id="37798545" class="c"><input type="checkbox" id="c-37798545" checked=""/><div class="controls bullet"><span class="by">askiiart</span><span>|</span><a href="#37797476">root</a><span>|</span><a href="#37797979">parent</a><span>|</span><a href="#37797453">next</a><span>|</span><label class="collapse" for="c-37798545">[-]</label><label class="expand" for="c-37798545">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m on the mid-tier plan available to me. 500 Mbps down, 20 Mbps up. It would take me, bare minimum, about 6.67 minutes to upload 1 gigabyte. I&#x27;ll be the first to admit that I don&#x27;t know much about AI&#x2F;ML, so I could be wrong, but assuming a fine-tuned model is the same size as the base model, it would take me 90 minutes to upload the llama2 7B model (13.5 GB).<p>If it were standard practice for many people, we might finally get symmetric cable. But most people aren&#x27;t YouTubers. In my experience, people who aren&#x27;t into tech usually have no idea their upload speeds are <i>any</i> slower than their download speeds, much less 25 times slower. They usually just let their phone upload their photos in the background, and it just works. Hell, I can&#x27;t even find my upload speed on the consumer section of my ISP&#x27;s site, only in a pdf for their business plans. Not even in legalese.</div><br/><div id="37799494" class="c"><input type="checkbox" id="c-37799494" checked=""/><div class="controls bullet"><span class="by">manmal</span><span>|</span><a href="#37797476">root</a><span>|</span><a href="#37798545">parent</a><span>|</span><a href="#37798804">next</a><span>|</span><label class="collapse" for="c-37799494">[-]</label><label class="expand" for="c-37799494">[2 more]</label></div><br/><div class="children"><div class="content">In my country there’s a tendency to provide max 50MBit&#x2F;s up on consumer connections, but a few do offer more. I‘m on a FTTH plan with 130 up now, and think this would suffice for most current use cases. There’s also the possibility to bundle multiple connections with enterprise grade hardware - last time I looked that was a 1k one time investment. I‘m dubious as to whether that would actually boost uploading speeds though.</div><br/><div id="37800047" class="c"><input type="checkbox" id="c-37800047" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#37797476">root</a><span>|</span><a href="#37799494">parent</a><span>|</span><a href="#37798804">next</a><span>|</span><label class="collapse" for="c-37800047">[-]</label><label class="expand" for="c-37800047">[1 more]</label></div><br/><div class="children"><div class="content">Same here, I&#x27;m on 600 Mbps up&#x2F;down and I enjoy the up more than the down.</div><br/></div></div></div></div><div id="37798804" class="c"><input type="checkbox" id="c-37798804" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#37797476">root</a><span>|</span><a href="#37798545">parent</a><span>|</span><a href="#37799494">prev</a><span>|</span><a href="#37799493">next</a><span>|</span><label class="collapse" for="c-37798804">[-]</label><label class="expand" for="c-37798804">[1 more]</label></div><br/><div class="children"><div class="content">Spectrum?  Lots of cable internet right now has upstream limitations, but fiber is making it to more homes and upstream limitations are (slowly) improving on cable.<p>As a society we have something like a 20% CAGR on upstream speeds over the past few years.  It may get better even faster, because DOCSIS 4 is coming soon and is full duplex over the same channels.<p>But even at 20mbps, uploading a 70B model might take you 12 hours... after you spent many hours fine-tuning it.  It&#x27;s annoying but not an unmanageable problem.</div><br/></div></div><div id="37799493" class="c"><input type="checkbox" id="c-37799493" checked=""/><div class="controls bullet"><span class="by">obscur</span><span>|</span><a href="#37797476">root</a><span>|</span><a href="#37798545">parent</a><span>|</span><a href="#37798804">prev</a><span>|</span><a href="#37797453">next</a><span>|</span><label class="collapse" for="c-37799493">[-]</label><label class="expand" for="c-37799493">[1 more]</label></div><br/><div class="children"><div class="content">It uses LoRA which is much smaller than full model</div><br/></div></div></div></div></div></div></div></div><div id="37797453" class="c"><input type="checkbox" id="c-37797453" checked=""/><div class="controls bullet"><span class="by">abroadwin</span><span>|</span><a href="#37797476">prev</a><span>|</span><a href="#37799501">next</a><span>|</span><label class="collapse" for="c-37797453">[-]</label><label class="expand" for="c-37797453">[4 more]</label></div><br/><div class="children"><div class="content">&gt; ...it offloads parts of model to SSD or main memory on both forward&#x2F;backward passes<p>So... does this murder SSDs?</div><br/><div id="37797502" class="c"><input type="checkbox" id="c-37797502" checked=""/><div class="controls bullet"><span class="by">sbierwagen</span><span>|</span><a href="#37797453">parent</a><span>|</span><a href="#37799501">next</a><span>|</span><label class="collapse" for="c-37797502">[-]</label><label class="expand" for="c-37797502">[3 more]</label></div><br/><div class="children"><div class="content">Having the SSD soldered to the mainboard usually doesn’t matter… except for this particular use case.<p>You could stick a SSD in a thunderbolt enclosure and use that, but at that point you might as well rent a cloud GPU instance for a buck an hour.</div><br/><div id="37798013" class="c"><input type="checkbox" id="c-37798013" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#37797453">root</a><span>|</span><a href="#37797502">parent</a><span>|</span><a href="#37799501">next</a><span>|</span><label class="collapse" for="c-37798013">[-]</label><label class="expand" for="c-37798013">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  for a buck an hour.<p>Every decent GPU cloud I&#x27;ve seen is around +$3&#x2F;hour. Where do you get ~$1&#x2F;hour?</div><br/><div id="37798190" class="c"><input type="checkbox" id="c-37798190" checked=""/><div class="controls bullet"><span class="by">Reubend</span><span>|</span><a href="#37797453">root</a><span>|</span><a href="#37798013">parent</a><span>|</span><a href="#37799501">next</a><span>|</span><label class="collapse" for="c-37798190">[-]</label><label class="expand" for="c-37798190">[1 more]</label></div><br/><div class="children"><div class="content">Take a look at this great comment, which provides a list of affordable services.
<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34358781">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34358781</a><p>Basically, it depends a lot of what you mean by &quot;decent&quot;. Your standards for performance might be a lot more than the person you&#x27;re replying to.</div><br/></div></div></div></div></div></div></div></div><div id="37799501" class="c"><input type="checkbox" id="c-37799501" checked=""/><div class="controls bullet"><span class="by">DarthNebo</span><span>|</span><a href="#37797453">prev</a><span>|</span><a href="#37797378">next</a><span>|</span><label class="collapse" for="c-37799501">[-]</label><label class="expand" for="c-37799501">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m hitting 3.9tok&#x2F;s with CTX of 300 tokens on Android&#x2F;778G via Userland &amp; this is with an older unoptimized build of llama.cpp</div><br/></div></div><div id="37797378" class="c"><input type="checkbox" id="c-37797378" checked=""/><div class="controls bullet"><span class="by">SillyUsername</span><span>|</span><a href="#37799501">prev</a><span>|</span><a href="#37799514">next</a><span>|</span><label class="collapse" for="c-37797378">[-]</label><label class="expand" for="c-37797378">[15 more]</label></div><br/><div class="children"><div class="content">So I keep getting told &quot;Macs don&#x27;t use memory like Windows&quot;, &quot;8GB is fine for everything on Mac&quot; (with quotes from people who use it to Photoshop) and &quot;the SSD is so fast if you use virtual memory anyway it&#x27;s not noticeable&quot;...
Is 8GB suitable for LLM use like this, and has anybody actually used it?</div><br/><div id="37797435" class="c"><input type="checkbox" id="c-37797435" checked=""/><div class="controls bullet"><span class="by">LeoPanthera</span><span>|</span><a href="#37797378">parent</a><span>|</span><a href="#37797430">next</a><span>|</span><label class="collapse" for="c-37797435">[-]</label><label class="expand" for="c-37797435">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are notoriously RAM-hungry. It’s not fair to judge what a MacBook comes with against what LLaMa2 needs. That is not normal use of a MacBook.</div><br/></div></div><div id="37797430" class="c"><input type="checkbox" id="c-37797430" checked=""/><div class="controls bullet"><span class="by">jzelinskie</span><span>|</span><a href="#37797378">parent</a><span>|</span><a href="#37797435">prev</a><span>|</span><a href="#37798490">next</a><span>|</span><label class="collapse" for="c-37797430">[-]</label><label class="expand" for="c-37797430">[4 more]</label></div><br/><div class="children"><div class="content">Apple really shouldn&#x27;t sell Macs with less than 16GiB of RAM. The first set of M1s got some bad press because so many folks were plagued by pop-ups saying they were out of memory -- from heavy browser usage not even intense workloads.</div><br/><div id="37798192" class="c"><input type="checkbox" id="c-37798192" checked=""/><div class="controls bullet"><span class="by">sgarland</span><span>|</span><a href="#37797378">root</a><span>|</span><a href="#37797430">parent</a><span>|</span><a href="#37799398">next</a><span>|</span><label class="collapse" for="c-37798192">[-]</label><label class="expand" for="c-37798192">[2 more]</label></div><br/><div class="children"><div class="content">As someone with a base model M1 Air, I’ve never experienced this issue from normal workloads. The ONLY times I’ve exceeded memory capacity were doing expensive computations without chunking, and running minikube.<p>I’m neither a Node dev nor an AI person, though, so that probably helps.</div><br/><div id="37798922" class="c"><input type="checkbox" id="c-37798922" checked=""/><div class="controls bullet"><span class="by">jsjohnst</span><span>|</span><a href="#37797378">root</a><span>|</span><a href="#37798192">parent</a><span>|</span><a href="#37799398">next</a><span>|</span><label class="collapse" for="c-37798922">[-]</label><label class="expand" for="c-37798922">[1 more]</label></div><br/><div class="children"><div class="content">Have a half dozen Google Docs open at once and you could force an 8gb machine to start swapping. Just saying.</div><br/></div></div></div></div><div id="37799398" class="c"><input type="checkbox" id="c-37799398" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37797378">root</a><span>|</span><a href="#37797430">parent</a><span>|</span><a href="#37798192">prev</a><span>|</span><a href="#37798490">next</a><span>|</span><label class="collapse" for="c-37799398">[-]</label><label class="expand" for="c-37799398">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s OS or app bugs. The way that dialog works isn&#x27;t just how much RAM you have.</div><br/></div></div></div></div><div id="37798490" class="c"><input type="checkbox" id="c-37798490" checked=""/><div class="controls bullet"><span class="by">wryanzimmerman</span><span>|</span><a href="#37797378">parent</a><span>|</span><a href="#37797430">prev</a><span>|</span><a href="#37797500">next</a><span>|</span><label class="collapse" for="c-37798490">[-]</label><label class="expand" for="c-37798490">[1 more]</label></div><br/><div class="children"><div class="content">Macs manage memory and swap in a way that makes them feel much more responsive when you have a lot of common applications open at once (or something like a lot of browser tabs at once), but if you’re literally doing a single task that uses a given amount of ram, you need that much ram or you’ll slow way down.</div><br/></div></div><div id="37797500" class="c"><input type="checkbox" id="c-37797500" checked=""/><div class="controls bullet"><span class="by">SparkyMcUnicorn</span><span>|</span><a href="#37797378">parent</a><span>|</span><a href="#37798490">prev</a><span>|</span><a href="#37798631">next</a><span>|</span><label class="collapse" for="c-37797500">[-]</label><label class="expand" for="c-37797500">[2 more]</label></div><br/><div class="children"><div class="content">8GB of memory is not currently suitable for LLM use.<p>A 7B model @ Q4 needs roughly 6.5GB of memory.<p>I have a 64GB M1. If my memory pressure is high and I start up a model that goes into swap, the machine becomes unbearably slow.</div><br/><div id="37798624" class="c"><input type="checkbox" id="c-37798624" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37797378">root</a><span>|</span><a href="#37797500">parent</a><span>|</span><a href="#37798631">next</a><span>|</span><label class="collapse" for="c-37798624">[-]</label><label class="expand" for="c-37798624">[1 more]</label></div><br/><div class="children"><div class="content">More than that for the 8K+ context people are running now.</div><br/></div></div></div></div><div id="37798631" class="c"><input type="checkbox" id="c-37798631" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37797378">parent</a><span>|</span><a href="#37797500">prev</a><span>|</span><a href="#37797382">next</a><span>|</span><label class="collapse" for="c-37798631">[-]</label><label class="expand" for="c-37798631">[1 more]</label></div><br/><div class="children"><div class="content">Before I got a desktop, I used to run my laptop (16GB RAM + a 6GB Nvidia GPU) headless to just <i>barely</i> squeeze the 33B models onto the GPU + RAM, and play with it on another device.<p>...I am not an OSX person, but if you can boot headless, it can squeeze 7B on there with full context</div><br/></div></div><div id="37797382" class="c"><input type="checkbox" id="c-37797382" checked=""/><div class="controls bullet"><span class="by">RockRobotRock</span><span>|</span><a href="#37797378">parent</a><span>|</span><a href="#37798631">prev</a><span>|</span><a href="#37797437">next</a><span>|</span><label class="collapse" for="c-37797382">[-]</label><label class="expand" for="c-37797382">[1 more]</label></div><br/><div class="children"><div class="content">8 GB on a MacBook is kinda rough. I regret it.</div><br/></div></div><div id="37797437" class="c"><input type="checkbox" id="c-37797437" checked=""/><div class="controls bullet"><span class="by">DrBenCarson</span><span>|</span><a href="#37797378">parent</a><span>|</span><a href="#37797382">prev</a><span>|</span><a href="#37798507">next</a><span>|</span><label class="collapse" for="c-37797437">[-]</label><label class="expand" for="c-37797437">[1 more]</label></div><br/><div class="children"><div class="content">8gb on a Mac is better than 8gb on Windows, but 16gb is a whole lot better</div><br/></div></div><div id="37798507" class="c"><input type="checkbox" id="c-37798507" checked=""/><div class="controls bullet"><span class="by">Geezus_42</span><span>|</span><a href="#37797378">parent</a><span>|</span><a href="#37797437">prev</a><span>|</span><a href="#37797481">next</a><span>|</span><label class="collapse" for="c-37798507">[-]</label><label class="expand" for="c-37798507">[1 more]</label></div><br/><div class="children"><div class="content">I use an M1 Pro every day for work, and while it runs fantastically, I call bullshit on the 8GB claim.</div><br/></div></div><div id="37797481" class="c"><input type="checkbox" id="c-37797481" checked=""/><div class="controls bullet"><span class="by">loxias</span><span>|</span><a href="#37797378">parent</a><span>|</span><a href="#37798507">prev</a><span>|</span><a href="#37797825">next</a><span>|</span><label class="collapse" for="c-37797481">[-]</label><label class="expand" for="c-37797481">[1 more]</label></div><br/><div class="children"><div class="content">I wonder the same thing, I have 64GB on a thinkpad from 3 years ago...</div><br/></div></div><div id="37797825" class="c"><input type="checkbox" id="c-37797825" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#37797378">parent</a><span>|</span><a href="#37797481">prev</a><span>|</span><a href="#37799514">next</a><span>|</span><label class="collapse" for="c-37797825">[-]</label><label class="expand" for="c-37797825">[1 more]</label></div><br/><div class="children"><div class="content">If you need a continuous block of addressable ram, there is nothing that a mac does differently and you will need more than 8gb if you need more than an 8gb block for a process</div><br/></div></div></div></div><div id="37799514" class="c"><input type="checkbox" id="c-37799514" checked=""/><div class="controls bullet"><span class="by">pk-protect-ai</span><span>|</span><a href="#37797378">prev</a><span>|</span><a href="#37797342">next</a><span>|</span><label class="collapse" for="c-37799514">[-]</label><label class="expand" for="c-37799514">[1 more]</label></div><br/><div class="children"><div class="content">From the article:
~25-30 min per iteration ....</div><br/></div></div><div id="37797342" class="c"><input type="checkbox" id="c-37797342" checked=""/><div class="controls bullet"><span class="by">narrator</span><span>|</span><a href="#37799514">prev</a><span>|</span><a href="#37797687">next</a><span>|</span><label class="collapse" for="c-37797342">[-]</label><label class="expand" for="c-37797342">[1 more]</label></div><br/><div class="children"><div class="content">I love that I can now do an LLM fine tune on my local Macbook. IMHO, this is why Facebook open sourced Llama.  Rockstar programmers like this guy are going to make it scale out on cheap hardware and give them an edge over even their well funded competitors.</div><br/></div></div><div id="37797687" class="c"><input type="checkbox" id="c-37797687" checked=""/><div class="controls bullet"><span class="by">foogazi</span><span>|</span><a href="#37797342">prev</a><span>|</span><label class="collapse" for="c-37797687">[-]</label><label class="expand" for="c-37797687">[7 more]</label></div><br/><div class="children"><div class="content">Side question: is there a benefit to doing this on an MacBook vs PC ?</div><br/><div id="37798705" class="c"><input type="checkbox" id="c-37798705" checked=""/><div class="controls bullet"><span class="by">duskwuff</span><span>|</span><a href="#37797687">parent</a><span>|</span><a href="#37797917">next</a><span>|</span><label class="collapse" for="c-37798705">[-]</label><label class="expand" for="c-37798705">[2 more]</label></div><br/><div class="children"><div class="content">Yes, and it revolves around memory.<p>If you&#x27;re running code on the CPU, Apple Silicon systems have significantly higher memory bandwidth than most x86 systems -- up to 800 GB&#x2F;sec on M2 Ultra.<p>If you&#x27;re running code on the GPU, an Apple Silicon system can be configured with up to 192 GB of unified memory, whereas most discrete graphics cards top out around 16-24 GB of VRAM. (There are a few larger compute-focused cards like Nvidia A100, but they&#x27;re incredibly expensive.)</div><br/><div id="37799316" class="c"><input type="checkbox" id="c-37799316" checked=""/><div class="controls bullet"><span class="by">omneity</span><span>|</span><a href="#37797687">root</a><span>|</span><a href="#37798705">parent</a><span>|</span><a href="#37797917">next</a><span>|</span><label class="collapse" for="c-37799316">[-]</label><label class="expand" for="c-37799316">[1 more]</label></div><br/><div class="children"><div class="content">There’s no _single_ consumer or datacenter GPU that is currently breaking 100GB VRAM. In this regard, the Apple offering is pretty unique, but their compute capability is lagging.<p>Also the 800GB&#x2F;s figure for memory bandwidth is impressive ... for a CPU. GPUs regularly hit 2TB&#x2F;s or more.<p>TL;DR on Apple for ML: Bigger models, slower calculations.</div><br/></div></div></div></div><div id="37797917" class="c"><input type="checkbox" id="c-37797917" checked=""/><div class="controls bullet"><span class="by">lagniappe</span><span>|</span><a href="#37797687">parent</a><span>|</span><a href="#37798705">prev</a><span>|</span><a href="#37798651">next</a><span>|</span><label class="collapse" for="c-37797917">[-]</label><label class="expand" for="c-37797917">[1 more]</label></div><br/><div class="children"><div class="content">Personally speaking I never realized how much fan noise and hot air coming from the machine distracted me until it wasn&#x27;t there anymore.</div><br/></div></div><div id="37798651" class="c"><input type="checkbox" id="c-37798651" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37797687">parent</a><span>|</span><a href="#37797917">prev</a><span>|</span><label class="collapse" for="c-37798651">[-]</label><label class="expand" for="c-37798651">[3 more]</label></div><br/><div class="children"><div class="content">Macs have a larger iGPU and faster RAM than pretty much any comparable PC, which are precisely the two things you want for LLMs.<p>Also, lots of people only own a Mac.<p>You&#x27;d have to be kinda crazy to buy a Mac <i>explicitly</i> for running&#x2F;training genai though.</div><br/><div id="37799714" class="c"><input type="checkbox" id="c-37799714" checked=""/><div class="controls bullet"><span class="by">lostmsu</span><span>|</span><a href="#37797687">root</a><span>|</span><a href="#37798651">parent</a><span>|</span><label class="collapse" for="c-37799714">[-]</label><label class="expand" for="c-37799714">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think you want iGPU for LLMs. 4090 + RAM offload would be many times faster.</div><br/><div id="37799888" class="c"><input type="checkbox" id="c-37799888" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37797687">root</a><span>|</span><a href="#37799714">parent</a><span>|</span><label class="collapse" for="c-37799888">[-]</label><label class="expand" for="c-37799888">[1 more]</label></div><br/><div class="children"><div class="content">3090 + RAM offload is not very fast on my desktop. Its <i>fine</i>, but the speed hit is large.<p>And I was thinking regular laptops as a baseline vs the base M1&#x2F;M2.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>