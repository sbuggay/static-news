<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1695027663760" as="style"/><link rel="stylesheet" href="styles.css?v=1695027663760"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://petals.dev/">Run LLMs at home, BitTorrent‑style</a> <span class="domain">(<a href="https://petals.dev">petals.dev</a>)</span></div><div class="subtext"><span>udev4096</span> | <span>93 comments</span></div><br/><div><div id="37549014" class="c"><input type="checkbox" id="c-37549014" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37549025">next</a><span>|</span><label class="collapse" for="c-37549014">[-]</label><label class="expand" for="c-37549014">[1 more]</label></div><br/><div class="children"><div class="content">This is neat. Model weights are split into their layers and distributed across several machines who then report themselves in a big hash table when they are ready to perform inference or fine tuning &quot;as a team&quot; over their subset of the layers.<p>It&#x27;s early but I&#x27;ve been working on hosting model weights in a Docker registry for <a href="https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama">https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama</a>. Mainly for the content addressability (Ollama will verify the correct weights are downloaded every time) and ultimately weights can be fetched by their content instead of by their name or url (which may change!). Perhaps a good next step might be to split the models by layers and store each layer independently  for use cases like this (or even just for downloading + running larger models over several &quot;local&quot; machines).</div><br/></div></div><div id="37549025" class="c"><input type="checkbox" id="c-37549025" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37549014">prev</a><span>|</span><a href="#37553180">next</a><span>|</span><label class="collapse" for="c-37549025">[-]</label><label class="expand" for="c-37549025">[30 more]</label></div><br/><div class="children"><div class="content">&gt; and fine‑tune them for your tasks<p>This is the part that raised my eyebrows.<p>Finetuning 70B is not just hard, its literally impossible without renting a very expensive cloud instance or buying a PC the price of a house, no matter how long you are willing to wait. I would absolutely contribute to a &quot;llama training horde&quot;</div><br/><div id="37549076" class="c"><input type="checkbox" id="c-37549076" checked=""/><div class="controls bullet"><span class="by">AaronFriel</span><span>|</span><a href="#37549025">parent</a><span>|</span><a href="#37549046">next</a><span>|</span><label class="collapse" for="c-37549076">[-]</label><label class="expand" for="c-37549076">[12 more]</label></div><br/><div class="children"><div class="content">That&#x27;s true for conventional fine-tuning, but is it the case for parameter efficient fine tuning  and qLORA? My understanding is that for a N billion parameter model, fine tuning can occur with a slightly-less-than-N gigabyte of VRAM GPU.<p>For that 70B parameter model: an A100?</div><br/><div id="37550185" class="c"><input type="checkbox" id="c-37550185" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549076">parent</a><span>|</span><a href="#37549967">next</a><span>|</span><label class="collapse" for="c-37550185">[-]</label><label class="expand" for="c-37550185">[10 more]</label></div><br/><div class="children"><div class="content">2x 40&#x2F;48GB GPUs would be the cheapest. But that&#x27;s still a very expensive system, especially if you don&#x27;t have a beefy workstation with 2x PCIe slots just lying around.</div><br/><div id="37552262" class="c"><input type="checkbox" id="c-37552262" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37550185">parent</a><span>|</span><a href="#37550994">next</a><span>|</span><label class="collapse" for="c-37552262">[-]</label><label class="expand" for="c-37552262">[8 more]</label></div><br/><div class="children"><div class="content">Even mATX boards tend to come with two (full-length) PCIe slots, and that&#x27;s easy sub-$1k territory. Not exactly a beefy workstation.<p>Source: have a $200 board in my computer right now with two full-length PCIe slots.</div><br/><div id="37552613" class="c"><input type="checkbox" id="c-37552613" checked=""/><div class="controls bullet"><span class="by">7speter</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37552262">parent</a><span>|</span><a href="#37550994">next</a><span>|</span><label class="collapse" for="c-37552613">[-]</label><label class="expand" for="c-37552613">[7 more]</label></div><br/><div class="children"><div class="content">Whats more difficult is trying to cool gpus with 24-48gb of RAM… they all seem to be passively cooled</div><br/><div id="37552679" class="c"><input type="checkbox" id="c-37552679" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37552613">parent</a><span>|</span><a href="#37550994">next</a><span>|</span><label class="collapse" for="c-37552679">[-]</label><label class="expand" for="c-37552679">[6 more]</label></div><br/><div class="children"><div class="content">Good point, I think most of them are designed for a high-airflow server chassis, with airflow in a direction that a desktop case wouldn&#x27;t necessarily facilitate (parallel to the card).</div><br/><div id="37552855" class="c"><input type="checkbox" id="c-37552855" checked=""/><div class="controls bullet"><span class="by">segfaultbuserr</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37552679">parent</a><span>|</span><a href="#37550994">next</a><span>|</span><label class="collapse" for="c-37552855">[-]</label><label class="expand" for="c-37552855">[5 more]</label></div><br/><div class="children"><div class="content">Waterblocks exist for some compute-only GPUs, including the Nvidia A100. Also, there are a few small vendors in China that offer mounting kits that allow you to mod these compute-only GPUs to use off-the-shelf AIO watercoolers. Certainly, not many people are going to take the risk to modify the expensive Nvidia A100, but these solutions are moderately popular among the DIY home lab developers to convert older server cards for home workstation use. Decommissioned Nvidia Tesla P100 or V100 can be purchased cheaply for several hundreds dollars.</div><br/><div id="37553070" class="c"><input type="checkbox" id="c-37553070" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37552855">parent</a><span>|</span><a href="#37550994">next</a><span>|</span><label class="collapse" for="c-37553070">[-]</label><label class="expand" for="c-37553070">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Decommissioned Nvidia Tesla P100 or V100 can be purchased cheaply for several hundreds dollars.<p>Meh. If you want 16GB of VRAM for several hundred dollars, can&#x27;t you just pull a brand new 30-series off the shelf and have ten times more computing power than those old pascal cards? You&#x27;ll even have more VRAM if you go for the 3080 or 3090. Admittedly, the 3090 is closer to $700 or so, but it should still make a P100 very sad in comparison.</div><br/><div id="37553236" class="c"><input type="checkbox" id="c-37553236" checked=""/><div class="controls bullet"><span class="by">segfaultbuserr</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37553070">parent</a><span>|</span><a href="#37550994">next</a><span>|</span><label class="collapse" for="c-37553236">[-]</label><label class="expand" for="c-37553236">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, these GPUs became less appealing after the prices of 30-series GPUs have dropped. The price of SXM cards are still somewhat unbeatable though if you have a compatible server motherboard [1]. Nvidia P100s are being sold for as low as $100 each, there are similar savings for the Nvidia V100s. But yeah, a saving around $100 to $200 is not really worthwhile...<p>Another curious contender is the decommissioned Nvidia CMP series GPUs from miners. For example, the Nvidia CMP 170HX basically uses the same Nvidia A100 PCB with its features downsized or disabled (8 GB VRAM, halved shaders, etc). But interestingly, it seems to preserve the full 1500 GB&#x2F;s memory bandwidth, making it potentially an interesting card for running memory-bound simulations.<p>[1] Prices are so low exactly because most people don&#x27;t. SXM-to-PCIe adapters also exist which cost $100-$200 - nearly as much as you have saved. It should be trivial to reverse-engineer the pinout to make a free and open source version.</div><br/><div id="37553281" class="c"><input type="checkbox" id="c-37553281" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37553236">parent</a><span>|</span><a href="#37550994">next</a><span>|</span><label class="collapse" for="c-37553281">[-]</label><label class="expand" for="c-37553281">[2 more]</label></div><br/><div class="children"><div class="content">Is it possible to take something like a CMP 170HX and do board-level work to add more memory chips? Or are they not connected to silicon?</div><br/><div id="37553530" class="c"><input type="checkbox" id="c-37553530" checked=""/><div class="controls bullet"><span class="by">segfaultbuserr</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37553281">parent</a><span>|</span><a href="#37550994">next</a><span>|</span><label class="collapse" for="c-37553530">[-]</label><label class="expand" for="c-37553530">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t believe it&#x27;s possible. The HBM2e chips are integrated onto the package of the GPU die, making them impossible to remove or modify in a non-destructive manner.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="37549967" class="c"><input type="checkbox" id="c-37549967" checked=""/><div class="controls bullet"><span class="by">zacmps</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549076">parent</a><span>|</span><a href="#37550185">prev</a><span>|</span><a href="#37549046">next</a><span>|</span><label class="collapse" for="c-37549967">[-]</label><label class="expand" for="c-37549967">[1 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;d need 2 80GB A100&#x27;s for unquantised.</div><br/></div></div></div></div><div id="37549046" class="c"><input type="checkbox" id="c-37549046" checked=""/><div class="controls bullet"><span class="by">Zetobal</span><span>|</span><a href="#37549025">parent</a><span>|</span><a href="#37549076">prev</a><span>|</span><a href="#37550472">next</a><span>|</span><label class="collapse" for="c-37549046">[-]</label><label class="expand" for="c-37549046">[6 more]</label></div><br/><div class="children"><div class="content">An H100 is maybe a car but not nearly close to a house...</div><br/><div id="37549240" class="c"><input type="checkbox" id="c-37549240" checked=""/><div class="controls bullet"><span class="by">KomoD</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549046">parent</a><span>|</span><a href="#37549726">next</a><span>|</span><label class="collapse" for="c-37549240">[-]</label><label class="expand" for="c-37549240">[2 more]</label></div><br/><div class="children"><div class="content">Maybe not in your area, but it&#x27;s very doable in other places, like where I live.</div><br/><div id="37550964" class="c"><input type="checkbox" id="c-37550964" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549240">parent</a><span>|</span><a href="#37549726">next</a><span>|</span><label class="collapse" for="c-37550964">[-]</label><label class="expand" for="c-37550964">[1 more]</label></div><br/><div class="children"><div class="content">You expect me to believe there are other places than where I live?!</div><br/></div></div></div></div><div id="37549726" class="c"><input type="checkbox" id="c-37549726" checked=""/><div class="controls bullet"><span class="by">ioedward</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549046">parent</a><span>|</span><a href="#37549240">prev</a><span>|</span><a href="#37549481">next</a><span>|</span><label class="collapse" for="c-37549726">[-]</label><label class="expand" for="c-37549726">[1 more]</label></div><br/><div class="children"><div class="content">8 H100s would have enough VRAM to finetune a 70B model.</div><br/></div></div><div id="37549481" class="c"><input type="checkbox" id="c-37549481" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549046">parent</a><span>|</span><a href="#37549726">prev</a><span>|</span><a href="#37550472">next</a><span>|</span><label class="collapse" for="c-37549481">[-]</label><label class="expand" for="c-37549481">[2 more]</label></div><br/><div class="children"><div class="content">Is a single H100 enough?</div><br/><div id="37551824" class="c"><input type="checkbox" id="c-37551824" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549481">parent</a><span>|</span><a href="#37550472">next</a><span>|</span><label class="collapse" for="c-37551824">[-]</label><label class="expand" for="c-37551824">[1 more]</label></div><br/><div class="children"><div class="content">80GB is enough, yeah.<p>I&#x27;m not sure what exact LORA&#x2F;quantization settings would be ideal, but check out <a href="https:&#x2F;&#x2F;github.com&#x2F;OpenAccess-AI-Collective&#x2F;axolotl#config">https:&#x2F;&#x2F;github.com&#x2F;OpenAccess-AI-Collective&#x2F;axolotl#config</a></div><br/></div></div></div></div></div></div><div id="37550472" class="c"><input type="checkbox" id="c-37550472" checked=""/><div class="controls bullet"><span class="by">pavelstoev</span><span>|</span><a href="#37549025">parent</a><span>|</span><a href="#37549046">prev</a><span>|</span><a href="#37549793">next</a><span>|</span><label class="collapse" for="c-37550472">[-]</label><label class="expand" for="c-37550472">[1 more]</label></div><br/><div class="children"><div class="content">You can finetune 40B falcon on 4 x A10 with compiler optimization technology from CentML. No changes to the model.</div><br/></div></div><div id="37549793" class="c"><input type="checkbox" id="c-37549793" checked=""/><div class="controls bullet"><span class="by">malwrar</span><span>|</span><a href="#37549025">parent</a><span>|</span><a href="#37550472">prev</a><span>|</span><a href="#37549275">next</a><span>|</span><label class="collapse" for="c-37549793">[-]</label><label class="expand" for="c-37549793">[2 more]</label></div><br/><div class="children"><div class="content">Impossible? It’s just a bunch of math, you don’t need to keep the entire network in memory the whole time.</div><br/><div id="37550211" class="c"><input type="checkbox" id="c-37550211" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549793">parent</a><span>|</span><a href="#37549275">next</a><span>|</span><label class="collapse" for="c-37550211">[-]</label><label class="expand" for="c-37550211">[1 more]</label></div><br/><div class="children"><div class="content">Well, any scheme where weights are dynamically loaded&#x2F;unloaded from memory enough to fit on a 48GB GPU are so slow that training is basically impractical. Your 70B model would be obsolete by the time the finetuning is done.<p>Some inference frameworks came up with schemes for just this, and it was horrifically slow.</div><br/></div></div></div></div><div id="37549275" class="c"><input type="checkbox" id="c-37549275" checked=""/><div class="controls bullet"><span class="by">akomtu</span><span>|</span><a href="#37549025">parent</a><span>|</span><a href="#37549793">prev</a><span>|</span><a href="#37553180">next</a><span>|</span><label class="collapse" for="c-37549275">[-]</label><label class="expand" for="c-37549275">[8 more]</label></div><br/><div class="children"><div class="content">What prevents parallel LLM training? If you read book 1 first and then book 2, the resulting update in your knowledge will be the same if you read the books in the reverse order. It seems reasonable to assume that LLM is trained on each book independently, the two deltas in the LLM weights can be just added up.</div><br/><div id="37549849" class="c"><input type="checkbox" id="c-37549849" checked=""/><div class="controls bullet"><span class="by">ctoth</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549275">parent</a><span>|</span><a href="#37551005">next</a><span>|</span><label class="collapse" for="c-37549849">[-]</label><label class="expand" for="c-37549849">[1 more]</label></div><br/><div class="children"><div class="content">This is not at all intuitive to me. It doesn&#x27;t make sense in a human perspective, as each book changes you. Consider the trivial case of a series, where nothing will make sense if you haven&#x27;t read the prior books (not that I think they feed it the book corpus in order maybe they should!), but even in a more philosophical sort of way, each book changes you. and the person who reads Harry Potter first and The Iliad second will have a different experience of each.
Then, with large language models, we have the concept of grokking something. If grokking happens in the middle of book 1, it is a different model which is reading book 2 and of course the inverse applies.</div><br/></div></div><div id="37551005" class="c"><input type="checkbox" id="c-37551005" checked=""/><div class="controls bullet"><span class="by">Grimblewald</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549275">parent</a><span>|</span><a href="#37549849">prev</a><span>|</span><a href="#37549864">next</a><span>|</span><label class="collapse" for="c-37551005">[-]</label><label class="expand" for="c-37551005">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t true. Set up even a simple ANN dense feed forward three layers you know the one. Then keep everything the same for two models you train with the exception of data order. You&#x27;ll end up with two different models even though you started with the same weights, etc.</div><br/></div></div><div id="37549864" class="c"><input type="checkbox" id="c-37549864" checked=""/><div class="controls bullet"><span class="by">eachro</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549275">parent</a><span>|</span><a href="#37551005">prev</a><span>|</span><a href="#37552297">next</a><span>|</span><label class="collapse" for="c-37549864">[-]</label><label class="expand" for="c-37549864">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure this is true. For instance, consider reading textbooks for linear algebra and functional analysis out of order. You might still grok the functional analysis if you read it first but you&#x27;d be better served by reading the linear algebra one first.</div><br/></div></div><div id="37552297" class="c"><input type="checkbox" id="c-37552297" checked=""/><div class="controls bullet"><span class="by">dTal</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549275">parent</a><span>|</span><a href="#37549864">prev</a><span>|</span><a href="#37549599">next</a><span>|</span><label class="collapse" for="c-37552297">[-]</label><label class="expand" for="c-37552297">[1 more]</label></div><br/><div class="children"><div class="content">The &quot;deltas&quot; are calculated by the error in how well the <i>current state</i> of the network predicts the output, backpropagated. Sequential runs are not commutative because the state <i>changes</i>.<p>Consider the trivial example of training a network to distinguish between sample A and sample B. Give it a hundred As in a row and it just learns &quot;everything is A&quot;. Give it a hundred Bs in a row and it relearns &quot;no, everything is B&quot;. To train it to distinguish, you must alternate As and Bs (and not too regularly, either!)</div><br/></div></div><div id="37549599" class="c"><input type="checkbox" id="c-37549599" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549275">parent</a><span>|</span><a href="#37552297">prev</a><span>|</span><a href="#37549328">next</a><span>|</span><label class="collapse" for="c-37549599">[-]</label><label class="expand" for="c-37549599">[1 more]</label></div><br/><div class="children"><div class="content">By the “delta in the LLM weights”, I am assuming you mean the gradients. You are effectively describing large batch training (data parallelism) which is part of the way you can scale up but there are quickly diminishing returns to large batch sizes.</div><br/></div></div><div id="37549328" class="c"><input type="checkbox" id="c-37549328" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549275">parent</a><span>|</span><a href="#37549599">prev</a><span>|</span><a href="#37550640">next</a><span>|</span><label class="collapse" for="c-37549328">[-]</label><label class="expand" for="c-37549328">[1 more]</label></div><br/><div class="children"><div class="content">In ordinary gradient descent the order does matter, since the position changes in between. I think stochastic gradient descent does sum a couple of gradients together sometimes, but I&#x27;m not sure what the trade-offs are and if LLMs do so as well.</div><br/></div></div><div id="37550640" class="c"><input type="checkbox" id="c-37550640" checked=""/><div class="controls bullet"><span class="by">necroforest</span><span>|</span><a href="#37549025">root</a><span>|</span><a href="#37549275">parent</a><span>|</span><a href="#37549328">prev</a><span>|</span><a href="#37553180">next</a><span>|</span><label class="collapse" for="c-37550640">[-]</label><label class="expand" for="c-37550640">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are trained in parallel. The model weights and optimizer state are split over a number (possibly thousands) of accelerators.<p>The main bottleneck to doing distributed training like this is the communication between nodes.</div><br/></div></div></div></div></div></div><div id="37553180" class="c"><input type="checkbox" id="c-37553180" checked=""/><div class="controls bullet"><span class="by">timost</span><span>|</span><a href="#37549025">prev</a><span>|</span><a href="#37551145">next</a><span>|</span><label class="collapse" for="c-37553180">[-]</label><label class="expand" for="c-37553180">[1 more]</label></div><br/><div class="children"><div class="content">You can host your own swarm of servers apparently [0].
I would be curious to have a ballpark estimate of the finetunning performance of a &quot;private&quot; petals cluster.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;bigscience-workshop&#x2F;petals&#x2F;wiki&#x2F;Launch-your-own-swarm">https:&#x2F;&#x2F;github.com&#x2F;bigscience-workshop&#x2F;petals&#x2F;wiki&#x2F;Launch-yo...</a></div><br/></div></div><div id="37551145" class="c"><input type="checkbox" id="c-37551145" checked=""/><div class="controls bullet"><span class="by">malwrar</span><span>|</span><a href="#37553180">prev</a><span>|</span><a href="#37546992">next</a><span>|</span><label class="collapse" for="c-37551145">[-]</label><label class="expand" for="c-37551145">[3 more]</label></div><br/><div class="children"><div class="content">How does this defend against a malicious participant altering the output of their share of the larger computation? Even without some kind of method for e.g. producing attacker-determined network output, this system seems vulnerable to lots of nodes joining and simply returning junk results, effectively DoSing the system.</div><br/><div id="37551349" class="c"><input type="checkbox" id="c-37551349" checked=""/><div class="controls bullet"><span class="by">borzunov</span><span>|</span><a href="#37551145">parent</a><span>|</span><a href="#37546992">next</a><span>|</span><label class="collapse" for="c-37551349">[-]</label><label class="expand" for="c-37551349">[2 more]</label></div><br/><div class="children"><div class="content">Hi, a Petals dev here. We&#x27;re developing validators that periodically go over all servers and ban the ones that return incorrect results. Additionally, clients can run data through multiple disjoint routes in the network and check that the results match.<p>This catches frequent attackers but doesn&#x27;t provide 100% protection - so we expect people to set up a _private_ swarm if they want full correctness guarantees. For example, if you don&#x27;t have enough GPUs to run an LLM yourself but have some hardware owners you trust to, you can set up a private Petals swarm and jointly run the LLM on geo-distributed hardware to process your data.</div><br/><div id="37551691" class="c"><input type="checkbox" id="c-37551691" checked=""/><div class="controls bullet"><span class="by">dleeftink</span><span>|</span><a href="#37551145">root</a><span>|</span><a href="#37551349">parent</a><span>|</span><a href="#37546992">next</a><span>|</span><label class="collapse" for="c-37551691">[-]</label><label class="expand" for="c-37551691">[1 more]</label></div><br/><div class="children"><div class="content">How about tried and tested reputation systems for GPUs&#x2F;providers to join certain swarms?<p>Yes, this can also be gamed (and I do not wish to bring yet another scoring system into this world), but it might just work for users wanting to choose between various levels of LLM security.<p>You might be able to even tie this into &#x27;energy per compute unit&#x27; spent, enticing users to opt for more energy efficient offerings. Potentially, an all-round metric (or multiple metrics) for the viability of a GPU provider.</div><br/></div></div></div></div></div></div><div id="37546992" class="c"><input type="checkbox" id="c-37546992" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#37551145">prev</a><span>|</span><a href="#37549115">next</a><span>|</span><label class="collapse" for="c-37546992">[-]</label><label class="expand" for="c-37546992">[15 more]</label></div><br/><div class="children"><div class="content">The first question I had was &quot;what are the economics?&quot; From the FAQ:<p><i>Will Petals incentives be based on crypto, blockchain, etc.?</i><p><pre><code>  No, we are working on a centralized incentive system similar to the AI Horde kudos, even though Petals is a fully decentralized system in all other aspects. We do not plan to provide a service to exchange these points for money, so you should see these incentives as &quot;game&quot; points designed to be spent inside our system.

  Petals is an ML-focused project designed for ML researchers and engineers, it does not have anything to do with finance. We decided to make the incentive system centralized because it is much easier to develop and maintain, so we can focus on developing features useful for ML researchers.
</code></pre>
<a href="https:&#x2F;&#x2F;github.com&#x2F;bigscience-workshop&#x2F;petals&#x2F;wiki&#x2F;FAQ:-Frequently-asked-questions#general">https:&#x2F;&#x2F;github.com&#x2F;bigscience-workshop&#x2F;petals&#x2F;wiki&#x2F;FAQ:-Freq...</a></div><br/><div id="37548212" class="c"><input type="checkbox" id="c-37548212" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37546992">parent</a><span>|</span><a href="#37548752">next</a><span>|</span><label class="collapse" for="c-37548212">[-]</label><label class="expand" for="c-37548212">[3 more]</label></div><br/><div class="children"><div class="content">&gt; similar to the AI Horde kudos<p>What they are referencing, which is super cool and (IMO) criminally underused:<p><a href="https:&#x2F;&#x2F;lite.koboldai.net&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;lite.koboldai.net&#x2F;</a><p><a href="https:&#x2F;&#x2F;tinybots.net&#x2F;artbot" rel="nofollow noreferrer">https:&#x2F;&#x2F;tinybots.net&#x2F;artbot</a><p><a href="https:&#x2F;&#x2F;aihorde.net&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;aihorde.net&#x2F;</a><p>In fact, I can host a 13B-70B finetune in the afternoon if anyone on HN wants to test a particular one out:<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;models?sort=modified&amp;search=70B+gguf" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;models?sort=modified&amp;search=70B+gguf</a></div><br/><div id="37548659" class="c"><input type="checkbox" id="c-37548659" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#37546992">root</a><span>|</span><a href="#37548212">parent</a><span>|</span><a href="#37548752">next</a><span>|</span><label class="collapse" for="c-37548659">[-]</label><label class="expand" for="c-37548659">[2 more]</label></div><br/><div class="children"><div class="content">&gt; GGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp. GGUF offers numerous advantages over GGML, such as better tokenisation, and support for special tokens. It is also supports metadata, and is designed to be extensible<p>is there a more canonical blogpost or link to learn more about the technical decisions here?</div><br/><div id="37548960" class="c"><input type="checkbox" id="c-37548960" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37546992">root</a><span>|</span><a href="#37548659">parent</a><span>|</span><a href="#37548752">next</a><span>|</span><label class="collapse" for="c-37548960">[-]</label><label class="expand" for="c-37548960">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;philpax&#x2F;ggml&#x2F;blob&#x2F;gguf-spec&#x2F;docs&#x2F;gguf.md#historical-state-of-affairs">https:&#x2F;&#x2F;github.com&#x2F;philpax&#x2F;ggml&#x2F;blob&#x2F;gguf-spec&#x2F;docs&#x2F;gguf.md#...</a><p>It is (IMO) a necessary and good change.<p>I just specified gguf because my 3090 cannot host a 70B model without offloading outside of exLlama&#x27;s very new ~2 bit quantization. And pre quantized gguf is a much smaller download than raw fp16 for conversion.</div><br/></div></div></div></div></div></div><div id="37548752" class="c"><input type="checkbox" id="c-37548752" checked=""/><div class="controls bullet"><span class="by">beardog</span><span>|</span><a href="#37546992">parent</a><span>|</span><a href="#37548212">prev</a><span>|</span><a href="#37548693">next</a><span>|</span><label class="collapse" for="c-37548752">[-]</label><label class="expand" for="c-37548752">[1 more]</label></div><br/><div class="children"><div class="content">&gt;What&#x27;s the motivation for people to host model layers in the public swarm?<p>&gt;People who run inference and fine-tuning themselves get a certain speedup if they host a part of the model locally. Some may be also motivated to &quot;give back&quot; to the community helping them to run the model (similarly to how BitTorrent users help others by sharing data they have already downloaded).<p>&gt;Since it may be not enough for everyone, we are also working on introducing explicit incentives (&quot;bloom points&quot;) for people donating their GPU time to the public swarm. Once this system is ready, we will display the top contributors on our website. People who earned these points will be able to spend them on inference&#x2F;fine-tuning with higher priority or increased security guarantees, or (maybe) exchange them for other rewards.<p>It does seem like they want a sort of centralized token however.</div><br/></div></div><div id="37548693" class="c"><input type="checkbox" id="c-37548693" checked=""/><div class="controls bullet"><span class="by">sn0wf1re</span><span>|</span><a href="#37546992">parent</a><span>|</span><a href="#37548752">prev</a><span>|</span><a href="#37548537">next</a><span>|</span><label class="collapse" for="c-37548693">[-]</label><label class="expand" for="c-37548693">[1 more]</label></div><br/><div class="children"><div class="content">Similarly there have been distributed render farms for graphic design for a long time. No incentives other than higher points means your jobs are prioritized.<p><a href="https:&#x2F;&#x2F;www.sheepit-renderfarm.com&#x2F;home" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.sheepit-renderfarm.com&#x2F;home</a></div><br/></div></div><div id="37548537" class="c"><input type="checkbox" id="c-37548537" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#37546992">parent</a><span>|</span><a href="#37548693">prev</a><span>|</span><a href="#37548603">next</a><span>|</span><label class="collapse" for="c-37548537">[-]</label><label class="expand" for="c-37548537">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a shame that every decentralized projects needs to be compared to cryptocoins now</div><br/><div id="37552754" class="c"><input type="checkbox" id="c-37552754" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#37546992">root</a><span>|</span><a href="#37548537">parent</a><span>|</span><a href="#37548603">next</a><span>|</span><label class="collapse" for="c-37552754">[-]</label><label class="expand" for="c-37552754">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not the comparison, it&#x27;s that it&#x27;s one of the things cryptocoins are actually useful for: You have people all over the world with GPUs, some of them want to pay the others for use of them, but their countries use different payment networks or the developers want to be able to automate it without forcing the users to all sign up with the same mercurial payment processor who could screw over any of the users at random.</div><br/><div id="37553088" class="c"><input type="checkbox" id="c-37553088" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#37546992">root</a><span>|</span><a href="#37552754">parent</a><span>|</span><a href="#37548603">next</a><span>|</span><label class="collapse" for="c-37553088">[-]</label><label class="expand" for="c-37553088">[1 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s that it&#x27;s one of the things cryptocoins are actually useful for<p>It&#x27;s what their proponent claim that they are useful for, yet there&#x27;s no single instance of a successful blockchain project actually achieving this kind of resource-sharing goal.<p>&gt; You have people all over the world with GPUs, some of them want to pay the others for use of them<p>The gigantic success of bitTorrent shows that humans as a group don&#x27;t need to have monetary incentives to share their spare hardware. In fact, it&#x27;s likely that trying to add money into the mix will just break the system instead of improving it: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Overjustification_effect" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Overjustification_effect</a></div><br/></div></div></div></div></div></div><div id="37548603" class="c"><input type="checkbox" id="c-37548603" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#37546992">parent</a><span>|</span><a href="#37548537">prev</a><span>|</span><a href="#37548395">next</a><span>|</span><label class="collapse" for="c-37548603">[-]</label><label class="expand" for="c-37548603">[3 more]</label></div><br/><div class="children"><div class="content">The logical conclusion is that they (the models) will eventually be linked to crypto payments though. This is where Lightning becomes important...<p>Edit: To clarify, I&#x27;m not suggesting linking these Petal &quot;tokens&quot; to any payment system. I&#x27;m talking about, in general, calls to clusters of machine learning models, decentralized or not, will likely use crypto payments because it gives you auth and a means of payment.<p>I do think Petal is a good implementation of using decentralized compute for model use and will likely be valuable long term.</div><br/><div id="37548673" class="c"><input type="checkbox" id="c-37548673" checked=""/><div class="controls bullet"><span class="by">vorpalhex</span><span>|</span><a href="#37546992">root</a><span>|</span><a href="#37548603">parent</a><span>|</span><a href="#37548395">next</a><span>|</span><label class="collapse" for="c-37548673">[-]</label><label class="expand" for="c-37548673">[2 more]</label></div><br/><div class="children"><div class="content">I mean, I can sell you Eve or Runescape currency but we don&#x27;t need any crypto to execute on it. &quot;Gold sellers&quot; existed well before crypto.</div><br/><div id="37552771" class="c"><input type="checkbox" id="c-37552771" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#37546992">root</a><span>|</span><a href="#37548673">parent</a><span>|</span><a href="#37548395">next</a><span>|</span><label class="collapse" for="c-37552771">[-]</label><label class="expand" for="c-37552771">[1 more]</label></div><br/><div class="children"><div class="content">Is there an API for that which doesn&#x27;t require each of the users to create a separate account on something else?</div><br/></div></div></div></div></div></div><div id="37548395" class="c"><input type="checkbox" id="c-37548395" checked=""/><div class="controls bullet"><span class="by">Szpadel</span><span>|</span><a href="#37546992">parent</a><span>|</span><a href="#37548603">prev</a><span>|</span><a href="#37549514">next</a><span>|</span><label class="collapse" for="c-37548395">[-]</label><label class="expand" for="c-37548395">[1 more]</label></div><br/><div class="children"><div class="content">if that part could be replaced with any third party server it would be a tracker in BitTorrent analogy.</div><br/></div></div><div id="37549514" class="c"><input type="checkbox" id="c-37549514" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#37546992">parent</a><span>|</span><a href="#37548395">prev</a><span>|</span><a href="#37548500">next</a><span>|</span><label class="collapse" for="c-37549514">[-]</label><label class="expand" for="c-37549514">[1 more]</label></div><br/><div class="children"><div class="content">Can they actually prevent people from trading petals for money though?</div><br/></div></div></div></div><div id="37549115" class="c"><input type="checkbox" id="c-37549115" checked=""/><div class="controls bullet"><span class="by">nico</span><span>|</span><a href="#37546992">prev</a><span>|</span><a href="#37550303">next</a><span>|</span><label class="collapse" for="c-37549115">[-]</label><label class="expand" for="c-37549115">[1 more]</label></div><br/><div class="children"><div class="content">This is so cool. Hopefully this will give access to thousands or millions more developers in the space</div><br/></div></div><div id="37550303" class="c"><input type="checkbox" id="c-37550303" checked=""/><div class="controls bullet"><span class="by">__rito__</span><span>|</span><a href="#37549115">prev</a><span>|</span><a href="#37548672">next</a><span>|</span><label class="collapse" for="c-37550303">[-]</label><label class="expand" for="c-37550303">[1 more]</label></div><br/><div class="children"><div class="content">I have used Petals at a past project. I share my GPU as well as wrote code for the project.<p>The Petals part was abstracted away from me. I had a normal experience writing code.<p>I don&#x27;t have the project listed anywhere. Don&#x27;t really know what happened to it. But, it was mainly some five or so guys spearheading the thing.</div><br/></div></div><div id="37548672" class="c"><input type="checkbox" id="c-37548672" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#37550303">prev</a><span>|</span><a href="#37551369">next</a><span>|</span><label class="collapse" for="c-37548672">[-]</label><label class="expand" for="c-37548672">[5 more]</label></div><br/><div class="children"><div class="content">so given that GGML can serve like 100 tok&#x2F;s on an M2 Max, and this thing advertises 6 tok&#x2F;s distributed, is this basically for people with lower end devices?</div><br/><div id="37548726" class="c"><input type="checkbox" id="c-37548726" checked=""/><div class="controls bullet"><span class="by">version_five</span><span>|</span><a href="#37548672">parent</a><span>|</span><a href="#37549013">next</a><span>|</span><label class="collapse" for="c-37548726">[-]</label><label class="expand" for="c-37548726">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s talking about 70B and 160B models. Even heavily quantized can ggml run those that fast? (I&#x27;m guessing possibly). So maybe this is for people that dont have a high end computer? I have a decent linux laptop a couple years old and there&#x27;s no way I could run those models that fast. I get a few tokens per second on a quantized 7B model.</div><br/><div id="37548999" class="c"><input type="checkbox" id="c-37548999" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37548672">root</a><span>|</span><a href="#37548726">parent</a><span>|</span><a href="#37549013">next</a><span>|</span><label class="collapse" for="c-37548999">[-]</label><label class="expand" for="c-37548999">[1 more]</label></div><br/><div class="children"><div class="content">Yeah. My 3090 gets like ~5 tokens&#x2F;s on 70B Q3KL.<p>This is a good idea, as splitting up llms is actually pretty efficient with pipelined requests.</div><br/></div></div></div></div><div id="37549013" class="c"><input type="checkbox" id="c-37549013" checked=""/><div class="controls bullet"><span class="by">russellbeattie</span><span>|</span><a href="#37548672">parent</a><span>|</span><a href="#37548726">prev</a><span>|</span><a href="#37549202">next</a><span>|</span><label class="collapse" for="c-37549013">[-]</label><label class="expand" for="c-37549013">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>...lower end devices</i><p>So, pretty much every other consumer PC available? Those losers.</div><br/></div></div></div></div><div id="37551369" class="c"><input type="checkbox" id="c-37551369" checked=""/><div class="controls bullet"><span class="by">sumo43</span><span>|</span><a href="#37548672">prev</a><span>|</span><a href="#37551908">next</a><span>|</span><label class="collapse" for="c-37551369">[-]</label><label class="expand" for="c-37551369">[3 more]</label></div><br/><div class="children"><div class="content">Cool service. It&#x27;s worth noting that, with quantization&#x2F;QLORA, models as big as llama2-70b can be run on consumer hardware (2xRTX 3090) at acceptable speeds (~20t&#x2F;s) using frameworks like llama.cpp. Doing this avoids the significant latency from parallelism schemes across different servers.<p>p.s. from experience instruct-finetuning falcon180b, it&#x27;s not worth using over llama2-70b as it&#x27;s significantly undertrained.</div><br/><div id="37551681" class="c"><input type="checkbox" id="c-37551681" checked=""/><div class="controls bullet"><span class="by">borzunov</span><span>|</span><a href="#37551369">parent</a><span>|</span><a href="#37551911">next</a><span>|</span><label class="collapse" for="c-37551681">[-]</label><label class="expand" for="c-37551681">[1 more]</label></div><br/><div class="children"><div class="content">Hi, a Petals dev here. You&#x27;re right, there&#x27;s no point in using Petals if your machine has enough GPU memory to fit the model and you&#x27;re okay with the quantization quality.<p>We developed Petals for people who have less GPU memory than needed. Also, there&#x27;s still a chance of larger open models being released in the future.</div><br/></div></div><div id="37551911" class="c"><input type="checkbox" id="c-37551911" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37551369">parent</a><span>|</span><a href="#37551681">prev</a><span>|</span><a href="#37551908">next</a><span>|</span><label class="collapse" for="c-37551911">[-]</label><label class="expand" for="c-37551911">[1 more]</label></div><br/><div class="children"><div class="content">AFAIK you cannot train 70B on 2x 3090, even with GPTQ&#x2F;qlora.<p>And the inference is pretty inefficient. Pooling the hardware would achieve much better GPU utilization and (theoretically) faster responses for the host&#x27;s requests</div><br/></div></div></div></div><div id="37551908" class="c"><input type="checkbox" id="c-37551908" checked=""/><div class="controls bullet"><span class="by">wwwtyro</span><span>|</span><a href="#37551369">prev</a><span>|</span><a href="#37552008">next</a><span>|</span><label class="collapse" for="c-37551908">[-]</label><label class="expand" for="c-37551908">[1 more]</label></div><br/><div class="children"><div class="content">I love this direction. I hope that WebGPU can be leveraged for this purpose in the future so that I can feel somewhat mollified about security and to promote adoption.</div><br/></div></div><div id="37552008" class="c"><input type="checkbox" id="c-37552008" checked=""/><div class="controls bullet"><span class="by">cphoover</span><span>|</span><a href="#37551908">prev</a><span>|</span><a href="#37549803">next</a><span>|</span><label class="collapse" for="c-37552008">[-]</label><label class="expand" for="c-37552008">[1 more]</label></div><br/><div class="children"><div class="content">Logo is both mesmerizing and distracting.</div><br/></div></div><div id="37549803" class="c"><input type="checkbox" id="c-37549803" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#37552008">prev</a><span>|</span><a href="#37551750">next</a><span>|</span><label class="collapse" for="c-37549803">[-]</label><label class="expand" for="c-37549803">[1 more]</label></div><br/><div class="children"><div class="content">Would love to share my 3080 Ti, but after running the commands in the getting started guide (<a href="https:&#x2F;&#x2F;github.com&#x2F;bigscience-workshop&#x2F;petals&#x2F;wiki&#x2F;Run-Petals-server-on-Windows">https:&#x2F;&#x2F;github.com&#x2F;bigscience-workshop&#x2F;petals&#x2F;wiki&#x2F;Run-Petal...</a>) it looks like there&#x27;s a dependency versioning issue:<p><pre><code>    ImportError: cannot import name &#x27;get_full_repo_name&#x27; from &#x27;huggingface_hub&#x27; (~&#x2F;.local&#x2F;lib&#x2F;python3.8&#x2F;site-packages&#x2F;huggingface_hub&#x2F;__init__.py)</code></pre></div><br/></div></div><div id="37551750" class="c"><input type="checkbox" id="c-37551750" checked=""/><div class="controls bullet"><span class="by">vanillax</span><span>|</span><a href="#37549803">prev</a><span>|</span><a href="#37550520">next</a><span>|</span><label class="collapse" for="c-37551750">[-]</label><label class="expand" for="c-37551750">[1 more]</label></div><br/><div class="children"><div class="content">Very cool.</div><br/></div></div><div id="37550520" class="c"><input type="checkbox" id="c-37550520" checked=""/><div class="controls bullet"><span class="by">senectus1</span><span>|</span><a href="#37551750">prev</a><span>|</span><a href="#37549932">next</a><span>|</span><label class="collapse" for="c-37550520">[-]</label><label class="expand" for="c-37550520">[1 more]</label></div><br/><div class="children"><div class="content">so how long until &quot;tokens&quot; are used to pay for GPU cycles.. people will stop &quot;mining&quot; and just donate their GPU cycles for distributed LLM usages....<p>in fact, if they did this so that it followed the sun so that the vast majority of it was powered by daylight Solar PV energy I wouldn&#x27;t even be upset by that.</div><br/></div></div><div id="37549932" class="c"><input type="checkbox" id="c-37549932" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#37550520">prev</a><span>|</span><a href="#37551070">next</a><span>|</span><label class="collapse" for="c-37549932">[-]</label><label class="expand" for="c-37549932">[21 more]</label></div><br/><div class="children"><div class="content">looking at the list of contributors, way more people need to donate their GPU time for the betterment of all. maybe we finally have a good use for decentralized computing that doesn&#x27;t calculate meaningless hashes for crypto, but helps the humanity by keeping these open source LLMs alive.</div><br/><div id="37550011" class="c"><input type="checkbox" id="c-37550011" checked=""/><div class="controls bullet"><span class="by">judge2020</span><span>|</span><a href="#37549932">parent</a><span>|</span><a href="#37550618">next</a><span>|</span><label class="collapse" for="c-37550011">[-]</label><label class="expand" for="c-37550011">[8 more]</label></div><br/><div class="children"><div class="content">It can cost a lot to run a GPU, especially at full load. The 4090 stock pulls 500 watts of power under full load[0], which is 12 kWh&#x2F;day or just under 4380 kWh a year, or over $450 in a year assuming $0.10-$0.11&#x2F;kWh for average residential rates. The only variable is whether or not training requires the same power draw as hitting it with furmark.<p>0: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;j9vC9NBL8zo?t=983" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;j9vC9NBL8zo?t=983</a></div><br/><div id="37550731" class="c"><input type="checkbox" id="c-37550731" checked=""/><div class="controls bullet"><span class="by">namtab00</span><span>|</span><a href="#37549932">root</a><span>|</span><a href="#37550011">parent</a><span>|</span><a href="#37550454">next</a><span>|</span><label class="collapse" for="c-37550731">[-]</label><label class="expand" for="c-37550731">[4 more]</label></div><br/><div class="children"><div class="content">&gt; $0.10-$0.11&#x2F;kWh for average residential rates<p>you Americans don&#x27;t know how good you have it...</div><br/><div id="37550906" class="c"><input type="checkbox" id="c-37550906" checked=""/><div class="controls bullet"><span class="by">throwaway20222</span><span>|</span><a href="#37549932">root</a><span>|</span><a href="#37550731">parent</a><span>|</span><a href="#37550454">next</a><span>|</span><label class="collapse" for="c-37550906">[-]</label><label class="expand" for="c-37550906">[3 more]</label></div><br/><div class="children"><div class="content">That’s a cheap rate for sure. Southern California is $.36&#x2F;.59&#x2F;.74 peak. Super expensive.</div><br/><div id="37551445" class="c"><input type="checkbox" id="c-37551445" checked=""/><div class="controls bullet"><span class="by">judge2020</span><span>|</span><a href="#37549932">root</a><span>|</span><a href="#37550906">parent</a><span>|</span><a href="#37551297">next</a><span>|</span><label class="collapse" for="c-37551445">[-]</label><label class="expand" for="c-37551445">[1 more]</label></div><br/><div class="children"><div class="content">Only Cali and the most northeastern states seem to have these high rates. Every other continental state is under $0.14 <a href="https:&#x2F;&#x2F;www.eia.gov&#x2F;electricity&#x2F;state&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.eia.gov&#x2F;electricity&#x2F;state&#x2F;</a></div><br/></div></div><div id="37551297" class="c"><input type="checkbox" id="c-37551297" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#37549932">root</a><span>|</span><a href="#37550906">parent</a><span>|</span><a href="#37551445">prev</a><span>|</span><a href="#37550454">next</a><span>|</span><label class="collapse" for="c-37551297">[-]</label><label class="expand" for="c-37551297">[1 more]</label></div><br/><div class="children"><div class="content">Southern California? Time to buy some solar panels!</div><br/></div></div></div></div></div></div><div id="37550454" class="c"><input type="checkbox" id="c-37550454" checked=""/><div class="controls bullet"><span class="by">pavelstoev</span><span>|</span><a href="#37549932">root</a><span>|</span><a href="#37550011">parent</a><span>|</span><a href="#37550731">prev</a><span>|</span><a href="#37550618">next</a><span>|</span><label class="collapse" for="c-37550454">[-]</label><label class="expand" for="c-37550454">[3 more]</label></div><br/><div class="children"><div class="content">Imagine someone paid you 25c&#x2F;hour for 4090 compute sharing.</div><br/><div id="37551760" class="c"><input type="checkbox" id="c-37551760" checked=""/><div class="controls bullet"><span class="by">judge2020</span><span>|</span><a href="#37549932">root</a><span>|</span><a href="#37550454">parent</a><span>|</span><a href="#37550976">prev</a><span>|</span><a href="#37550618">next</a><span>|</span><label class="collapse" for="c-37551760">[-]</label><label class="expand" for="c-37551760">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s pretty much what Nicehash does, but after you pay for that electricity it isn&#x27;t super profitable - especially if you use it for 1&#x2F;3 or more of the day for your own purposes (gaming&#x2F;etc).</div><br/></div></div></div></div></div></div><div id="37550618" class="c"><input type="checkbox" id="c-37550618" checked=""/><div class="controls bullet"><span class="by">corndoge</span><span>|</span><a href="#37549932">parent</a><span>|</span><a href="#37550011">prev</a><span>|</span><a href="#37549984">next</a><span>|</span><label class="collapse" for="c-37550618">[-]</label><label class="expand" for="c-37550618">[1 more]</label></div><br/><div class="children"><div class="content">I immediately wanted to contribute and it&#x27;s quite difficult to find the link on the homepage! The &quot;contribute&quot; button should not be a tiny text link that says &quot;help hosting&quot; in the footnote, it should be a big button next to the colab button.<p>Edit: Oh hey, they did it.</div><br/></div></div><div id="37549984" class="c"><input type="checkbox" id="c-37549984" checked=""/><div class="controls bullet"><span class="by">Obscurity4340</span><span>|</span><a href="#37549932">parent</a><span>|</span><a href="#37550618">prev</a><span>|</span><a href="#37552079">next</a><span>|</span><label class="collapse" for="c-37549984">[-]</label><label class="expand" for="c-37549984">[2 more]</label></div><br/><div class="children"><div class="content">This way too nobody can copyright-cancel the LLM like OpenAI or whatever</div><br/><div id="37550606" class="c"><input type="checkbox" id="c-37550606" checked=""/><div class="controls bullet"><span class="by">alextheparrot</span><span>|</span><a href="#37549932">root</a><span>|</span><a href="#37549984">parent</a><span>|</span><a href="#37552079">next</a><span>|</span><label class="collapse" for="c-37550606">[-]</label><label class="expand" for="c-37550606">[1 more]</label></div><br/><div class="children"><div class="content">Exactly, litigation has never been applied to content delivered over BitTorrent-style networks</div><br/></div></div></div></div><div id="37552079" class="c"><input type="checkbox" id="c-37552079" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#37549932">parent</a><span>|</span><a href="#37549984">prev</a><span>|</span><a href="#37551148">next</a><span>|</span><label class="collapse" for="c-37552079">[-]</label><label class="expand" for="c-37552079">[1 more]</label></div><br/><div class="children"><div class="content">For the most part, gpus are no longer used for hashing. Once ETH switched to PoS, it decimated the entire GPU mining market.</div><br/></div></div></div></div><div id="37551070" class="c"><input type="checkbox" id="c-37551070" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37549932">prev</a><span>|</span><label class="collapse" for="c-37551070">[-]</label><label class="expand" for="c-37551070">[6 more]</label></div><br/><div class="children"><div class="content">I got a lurid NSFW comment, just asking for the time (using the Colab), so I assume some people are trolling the network?<p>Human: what is the time?<p>The time is 12:30 PM.<p>Human: are you sure?<p>Yes, I am sure. The time is 12:30 PM.^&lt;&#x2F;s&gt;^&lt;s&gt; I&#x27;m a young {...}</div><br/><div id="37551232" class="c"><input type="checkbox" id="c-37551232" checked=""/><div class="controls bullet"><span class="by">borzunov</span><span>|</span><a href="#37551070">parent</a><span>|</span><a href="#37551904">next</a><span>|</span><label class="collapse" for="c-37551232">[-]</label><label class="expand" for="c-37551232">[2 more]</label></div><br/><div class="children"><div class="content">Hi, a Petals dev here. &lt;&#x2F;s&gt; means &quot;end of sequence&quot; for LLMs. If a model generates it, it forgets everything and continues with an unrelated random text (I&#x27;m sorry to hear that the model generated a disturbing text in this case). Still, I doubt that malicious actors are involved here.<p>Apparently, the Colab code snippet is just too simplified and does not handle &lt;&#x2F;s&gt; correctly. This is not the case with the full chatbot app at <a href="https:&#x2F;&#x2F;chat.petals.dev" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.petals.dev</a> - you can try it out instead.</div><br/><div id="37551755" class="c"><input type="checkbox" id="c-37551755" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37551070">root</a><span>|</span><a href="#37551232">parent</a><span>|</span><a href="#37551904">next</a><span>|</span><label class="collapse" for="c-37551755">[-]</label><label class="expand" for="c-37551755">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the reply. One way to guard against that would be if the LLM architecture refused to serve against just &lt;s&gt; as a token?</div><br/></div></div></div></div><div id="37551904" class="c"><input type="checkbox" id="c-37551904" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37551070">parent</a><span>|</span><a href="#37551232">prev</a><span>|</span><a href="#37551140">next</a><span>|</span><label class="collapse" for="c-37551904">[-]</label><label class="expand" for="c-37551904">[1 more]</label></div><br/><div class="children"><div class="content">Base llama has lots of lurid in it already.</div><br/></div></div></div></div></div></div></div></div></div></body></html>