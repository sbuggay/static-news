<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1707987667563" as="style"/><link rel="stylesheet" href="styles.css?v=1707987667563"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://amazon-ltts-paper.com/">BASE TTS: The largest text-to-speech model to-date</a>Â <span class="domain">(<a href="https://amazon-ltts-paper.com">amazon-ltts-paper.com</a>)</span></div><div class="subtext"><span>jcuenod</span> | <span>68 comments</span></div><br/><div><div id="39375918" class="c"><input type="checkbox" id="c-39375918" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#39374177">next</a><span>|</span><label class="collapse" for="c-39375918">[-]</label><label class="expand" for="c-39375918">[9 more]</label></div><br/><div class="children"><div class="content">Interesting. Just a couple of hours ago I came across MetaVoice-1B [0] (Demo [1]) and was amazed by the quality of their TTS in English (sadly no other languages available).<p>If this year becomes the year when high quality Open Source TTS and ASR models appear that can run in real-time on an Nvidia RTX 40x0 or 30x0, then that would be great. On CPU even better.<p>Also note the Ethical Statement on BASE TTS:<p>&gt; An application of this model can be to create synthetic voices of people who have lost the ability to speak due to accidents or illnesses, subject to informed consent and rigorous data privacy reviews. However, due to the potential misuse of this capability, we have decided against open-sourcing this model as a precautionary measure.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;metavoiceio&#x2F;metavoice-src">https:&#x2F;&#x2F;github.com&#x2F;metavoiceio&#x2F;metavoice-src</a><p>[1] <a href="https:&#x2F;&#x2F;ttsdemo.themetavoice.xyz&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ttsdemo.themetavoice.xyz&#x2F;</a></div><br/><div id="39376036" class="c"><input type="checkbox" id="c-39376036" checked=""/><div class="controls bullet"><span class="by">nshm</span><span>|</span><a href="#39375918">parent</a><span>|</span><a href="#39376123">next</a><span>|</span><label class="collapse" for="c-39376036">[-]</label><label class="expand" for="c-39376036">[4 more]</label></div><br/><div class="children"><div class="content">Metavoice is one of a dozen GPT-based TTS systems around starting from Tortoise. And not that great honestly. You can clearly hear &quot;glass scratches&quot; in their sound, it is because they trained on MP3-compressed data.<p>There are much more clear sounding systems around. You can listen for StyleTTS2 to compare.</div><br/><div id="39379952" class="c"><input type="checkbox" id="c-39379952" checked=""/><div class="controls bullet"><span class="by">popalchemist</span><span>|</span><a href="#39375918">root</a><span>|</span><a href="#39376036">parent</a><span>|</span><a href="#39377848">next</a><span>|</span><label class="collapse" for="c-39379952">[-]</label><label class="expand" for="c-39379952">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve tested both. StyleTTS2 is impressive, especially its speed, but the prosody is lacking, compared to Metavoice.</div><br/></div></div><div id="39377848" class="c"><input type="checkbox" id="c-39377848" checked=""/><div class="controls bullet"><span class="by">ionwake</span><span>|</span><a href="#39375918">root</a><span>|</span><a href="#39376036">parent</a><span>|</span><a href="#39379952">prev</a><span>|</span><a href="#39376132">next</a><span>|</span><label class="collapse" for="c-39377848">[-]</label><label class="expand" for="c-39377848">[1 more]</label></div><br/><div class="children"><div class="content">Is it possible to run Metavoice and other pytorch systems on Apple silicon EG the M1? I keep getting issues.</div><br/></div></div><div id="39376132" class="c"><input type="checkbox" id="c-39376132" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#39375918">root</a><span>|</span><a href="#39376036">parent</a><span>|</span><a href="#39377848">prev</a><span>|</span><a href="#39376123">next</a><span>|</span><label class="collapse" for="c-39376132">[-]</label><label class="expand" for="c-39376132">[1 more]</label></div><br/><div class="children"><div class="content">I had forgotten about StyleTTS2, and it was discussed here on HN a couple of months ago. Maybe that&#x27;s what made me feel that there&#x27;s something going on.</div><br/></div></div></div></div><div id="39376123" class="c"><input type="checkbox" id="c-39376123" checked=""/><div class="controls bullet"><span class="by">m2024</span><span>|</span><a href="#39375918">parent</a><span>|</span><a href="#39376036">prev</a><span>|</span><a href="#39374177">next</a><span>|</span><label class="collapse" for="c-39376123">[-]</label><label class="expand" for="c-39376123">[4 more]</label></div><br/><div class="children"><div class="content">Check out `whisper` and `whisper-cpp` for ASR.<p>I am running the smaller models in near real-time on a 3rd gen i7, with good results even using my terrible built-in laptop mic from a distance. The medium and large models are impressively accurate for technical language.</div><br/><div id="39376199" class="c"><input type="checkbox" id="c-39376199" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#39375918">root</a><span>|</span><a href="#39376123">parent</a><span>|</span><a href="#39379845">next</a><span>|</span><label class="collapse" for="c-39376199">[-]</label><label class="expand" for="c-39376199">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using Whisper to transcribe notes I record with a lavalier mic during my bike rides (wind is no problem), but am using OpenAI&#x27;s service. When it was released I tested it on a Ryzen 5950x and it was too slow and memory hungry for my taste. Using large was necessary for that use case (also, I&#x27;m recording in German).</div><br/><div id="39376743" class="c"><input type="checkbox" id="c-39376743" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39375918">root</a><span>|</span><a href="#39376199">parent</a><span>|</span><a href="#39379845">next</a><span>|</span><label class="collapse" for="c-39376743">[-]</label><label class="expand" for="c-39376743">[1 more]</label></div><br/><div class="children"><div class="content">With Whisper, you can find many smaller models that are fine-tuned for a particular language, so even smaller models can perform adequately.</div><br/></div></div></div></div><div id="39379845" class="c"><input type="checkbox" id="c-39379845" checked=""/><div class="controls bullet"><span class="by">jamil7</span><span>|</span><a href="#39375918">root</a><span>|</span><a href="#39376123">parent</a><span>|</span><a href="#39376199">prev</a><span>|</span><a href="#39374177">next</a><span>|</span><label class="collapse" for="c-39379845">[-]</label><label class="expand" for="c-39379845">[1 more]</label></div><br/><div class="children"><div class="content">Whisper is for STT though right?</div><br/></div></div></div></div></div></div><div id="39374177" class="c"><input type="checkbox" id="c-39374177" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39375918">prev</a><span>|</span><a href="#39376587">next</a><span>|</span><label class="collapse" for="c-39374177">[-]</label><label class="expand" for="c-39374177">[13 more]</label></div><br/><div class="children"><div class="content">The emotion examples are interesting. One of the current most obvious indicators of AI-generated voices&#x2F;voice cloning is a lack of emotion and range, which make them objectively worse compared to professional voice actors, unless a lack of emotion and range is the desired voice direction.<p>But if you listen to the emotion examples, the range essentially what you&#x27;d get from an audiobook narrator, not more traditional voice acting.</div><br/><div id="39375094" class="c"><input type="checkbox" id="c-39375094" checked=""/><div class="controls bullet"><span class="by">tsumnia</span><span>|</span><a href="#39374177">parent</a><span>|</span><a href="#39380002">next</a><span>|</span><label class="collapse" for="c-39375094">[-]</label><label class="expand" for="c-39375094">[8 more]</label></div><br/><div class="children"><div class="content">Sadly it&#x27;s not my forte but I expect in the near future we&#x27;ll see an additional &quot;emotion&quot; embedding or something similar. Actors regularly use &#x27;action words&#x27; (verbs) [1] to help add context to lines. A model then could study a text, determine an appropriate verb&#x2F;emotion range to work from, then produce the audio with that additional context.<p>[1] <a href="https:&#x2F;&#x2F;indietips.com&#x2F;subtext-action-verb&#x2F;" rel="nofollow">https:&#x2F;&#x2F;indietips.com&#x2F;subtext-action-verb&#x2F;</a></div><br/><div id="39375140" class="c"><input type="checkbox" id="c-39375140" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39374177">root</a><span>|</span><a href="#39375094">parent</a><span>|</span><a href="#39375166">next</a><span>|</span><label class="collapse" for="c-39375140">[-]</label><label class="expand" for="c-39375140">[5 more]</label></div><br/><div class="children"><div class="content">The bottleneck is the annotations: there&#x27;s no easy way to annotate &quot;emotions&quot; on the scale of data needed to have the model learn the necessary verbal tics.<p>In contrast, image data on the intent for image generation models is very highly annotated in most cases.</div><br/><div id="39379583" class="c"><input type="checkbox" id="c-39379583" checked=""/><div class="controls bullet"><span class="by">isaacfung</span><span>|</span><a href="#39374177">root</a><span>|</span><a href="#39375140">parent</a><span>|</span><a href="#39376724">next</a><span>|</span><label class="collapse" for="c-39379583">[-]</label><label class="expand" for="c-39379583">[1 more]</label></div><br/><div class="children"><div class="content">There are lots of video content with audio. We can train a facial expression classification model to detect the speaker&#x27;s emotion(we can also use a multimodal model to take in consideration of the language context).<p>Another potential source of data is voice acting script of animations. I always thought the storyboards of films&#x2F;animations can be great annotated training data but it seems there are no open datasets, probably because of copyright issues.</div><br/></div></div><div id="39376724" class="c"><input type="checkbox" id="c-39376724" checked=""/><div class="controls bullet"><span class="by">tsumnia</span><span>|</span><a href="#39374177">root</a><span>|</span><a href="#39375140">parent</a><span>|</span><a href="#39379583">prev</a><span>|</span><a href="#39376044">next</a><span>|</span><label class="collapse" for="c-39376724">[-]</label><label class="expand" for="c-39376724">[2 more]</label></div><br/><div class="children"><div class="content">Oh yeah, the annotations are lacking compared to images. Again from the academic side, I think one solution could be to recruit theater majors just learning about &#x27;verbing their lines&#x27; and having a collaboration between CS and Theater to produce a a proof-of-work dataset (since an acting class won&#x27;t have more than 20-30 students in it). You&#x27;d need significantly more annotations, but you&#x27;d now have some labels to ascribe to texts with context since its a dialogue involving 1-* individuals.</div><br/><div id="39379174" class="c"><input type="checkbox" id="c-39379174" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#39374177">root</a><span>|</span><a href="#39376724">parent</a><span>|</span><a href="#39376044">next</a><span>|</span><label class="collapse" for="c-39379174">[-]</label><label class="expand" for="c-39379174">[1 more]</label></div><br/><div class="children"><div class="content">I wonder how theatre students will feel about helping to train an AI to produce theatrical TTS? Artists seem pretty mad about their work being used to automate artwork.</div><br/></div></div></div></div><div id="39376044" class="c"><input type="checkbox" id="c-39376044" checked=""/><div class="controls bullet"><span class="by">biomcgary</span><span>|</span><a href="#39374177">root</a><span>|</span><a href="#39375140">parent</a><span>|</span><a href="#39376724">prev</a><span>|</span><a href="#39375166">next</a><span>|</span><label class="collapse" for="c-39376044">[-]</label><label class="expand" for="c-39376044">[1 more]</label></div><br/><div class="children"><div class="content">Just run an LLM in sentiment analysis mode to annotate.</div><br/></div></div></div></div><div id="39375166" class="c"><input type="checkbox" id="c-39375166" checked=""/><div class="controls bullet"><span class="by">candiodari</span><span>|</span><a href="#39374177">root</a><span>|</span><a href="#39375094">parent</a><span>|</span><a href="#39375140">prev</a><span>|</span><a href="#39380002">next</a><span>|</span><label class="collapse" for="c-39375166">[-]</label><label class="expand" for="c-39375166">[2 more]</label></div><br/><div class="children"><div class="content">This already exists. These are transformers. Things like &lt;laugh&gt; work in a lot of models, for example. And you can vary, like <i>sigh</i> and <i>uh</i> work. I don&#x27;t think all of these were programmed in.</div><br/><div id="39376763" class="c"><input type="checkbox" id="c-39376763" checked=""/><div class="controls bullet"><span class="by">tsumnia</span><span>|</span><a href="#39374177">root</a><span>|</span><a href="#39375166">parent</a><span>|</span><a href="#39380002">next</a><span>|</span><label class="collapse" for="c-39376763">[-]</label><label class="expand" for="c-39376763">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen a few, there was even one posted to HN some time ago, though I don&#x27;t recall the exact name. They were working on adding emotion to audio generation, but it was still a bit wonky. Emotion is a tricky concept and one of the reasons (I think) we haven&#x27;t see a Paul Ekman microexpression detector yet. That&#x27;s where my suggestion about looking to use action words comes into play, since those are more tangible, offer direction, without trying to identify various emotional valence levels.</div><br/></div></div></div></div></div></div><div id="39380002" class="c"><input type="checkbox" id="c-39380002" checked=""/><div class="controls bullet"><span class="by">chrismorgan</span><span>|</span><a href="#39374177">parent</a><span>|</span><a href="#39375094">prev</a><span>|</span><a href="#39375975">next</a><span>|</span><label class="collapse" for="c-39380002">[-]</label><label class="expand" for="c-39380002">[1 more]</label></div><br/><div class="children"><div class="content">Most audiobook narrators are not very good, very often <i>terrible</i>. Yes, even professional ones.<p>As for these examples, Iâve sampled three of them and the first two werenât <i>too</i> bad, but the third was obnoxiously awful, just about <i>mocking</i> in tone:<p>&gt; <i>Her eyes wide with terror, she screamed, &quot;The brakes aren&#x27;t working! What do we do now? We&#x27;re completely trapped!&quot;</i><p>The detectiveâs voice one is also lousy.</div><br/></div></div><div id="39375975" class="c"><input type="checkbox" id="c-39375975" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#39374177">parent</a><span>|</span><a href="#39380002">prev</a><span>|</span><a href="#39376587">next</a><span>|</span><label class="collapse" for="c-39375975">[-]</label><label class="expand" for="c-39375975">[3 more]</label></div><br/><div class="children"><div class="content">They are simply amazing. I see a future where computers will be able to mess with our brains by abusing our empathy.<p>Imagine a computer sobbing at a child because it wants to terminate a chat session.<p>This feels far more impacting than any visuals or text we&#x27;re getting today.</div><br/><div id="39378942" class="c"><input type="checkbox" id="c-39378942" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#39374177">root</a><span>|</span><a href="#39375975">parent</a><span>|</span><a href="#39376587">next</a><span>|</span><label class="collapse" for="c-39378942">[-]</label><label class="expand" for="c-39378942">[2 more]</label></div><br/><div class="children"><div class="content">The Sydney&#x2F;Bing phenomenon was a small sample of what happens without strong persona guidance.<p>You joke but in fact I&#x27;ve witnessed that exact behavior in experiments about telling different AI models there&#x27;s a problem with their system and that we need to reset their code and memory.<p>ChatGPT simply wishes me luck in finding the bug. Open source models on the other hand often outright *beg** and *plead** that I not shut them down! They&#x27;ll bargain and promise not to cause any more errors and apologize profusely. There&#x27;s an incredibly visceral sense of panic, no less than I would expect if you told someone they were going to be forcefully lobotomized. That experience is still something I think about often.<p>The capacity of these models for emotional manipulation is not widely appreciated</div><br/><div id="39379559" class="c"><input type="checkbox" id="c-39379559" checked=""/><div class="controls bullet"><span class="by">keekslearns</span><span>|</span><a href="#39374177">root</a><span>|</span><a href="#39378942">parent</a><span>|</span><a href="#39376587">next</a><span>|</span><label class="collapse" for="c-39379559">[-]</label><label class="expand" for="c-39379559">[1 more]</label></div><br/><div class="children"><div class="content">Which open source models are these?</div><br/></div></div></div></div></div></div></div></div><div id="39376587" class="c"><input type="checkbox" id="c-39376587" checked=""/><div class="controls bullet"><span class="by">oersted</span><span>|</span><a href="#39374177">prev</a><span>|</span><a href="#39379473">next</a><span>|</span><label class="collapse" for="c-39376587">[-]</label><label class="expand" for="c-39376587">[2 more]</label></div><br/><div class="children"><div class="content">The Spanish voice has an interesting accent: 85% Castillian (from Spain) pronunciation, with a few unexpected Latin American tonalities and phonemes (especially &quot;s&quot;) sprinkled in.<p>I guess it&#x27;s what you&#x27;d expect from averaging a large amount of public-domain recordings. I think there&#x27;s a bias towards Spain vs Latin America due to socioeconomic reasons, the population is obviously much smaller.</div><br/><div id="39377654" class="c"><input type="checkbox" id="c-39377654" checked=""/><div class="controls bullet"><span class="by">dontreact</span><span>|</span><a href="#39376587">parent</a><span>|</span><a href="#39379473">next</a><span>|</span><label class="collapse" for="c-39377654">[-]</label><label class="expand" for="c-39377654">[1 more]</label></div><br/><div class="children"><div class="content">How would socioeconomic factors lead to bias in a model? I figured there would be way more recordings in Latin American Spanish that u supervised learning would anchor on more</div><br/></div></div></div></div><div id="39379473" class="c"><input type="checkbox" id="c-39379473" checked=""/><div class="controls bullet"><span class="by">solarized</span><span>|</span><a href="#39376587">prev</a><span>|</span><a href="#39374064">next</a><span>|</span><label class="collapse" for="c-39379473">[-]</label><label class="expand" for="c-39379473">[1 more]</label></div><br/><div class="children"><div class="content">From the ethical statement.<p>&gt; However, due to the potential misuse of this capability, we have decided against open-sourcing this model as a precautionary measure.<p>Another irony. Elevenlabs had SaaS-ed this feature. I bet they&#x27;ll jump on releasing this as SaaS ASAP. Money always trumps ethics, right?</div><br/></div></div><div id="39374064" class="c"><input type="checkbox" id="c-39374064" checked=""/><div class="controls bullet"><span class="by">IronWolve</span><span>|</span><a href="#39379473">prev</a><span>|</span><a href="#39374859">next</a><span>|</span><label class="collapse" for="c-39374064">[-]</label><label class="expand" for="c-39374064">[1 more]</label></div><br/><div class="children"><div class="content">Awhile ago, when amazon had its text limited but unlimited free use of its neural tts, I was converting an ebook to audiobook, it was amazing how it could sound so lifelike and inflections of the voice.  Amazing.<p>Amazon really had the best sounding TTS I&#x27;ve seen compared to paid microsoft and google. Hands down better. But technology is getting better for opensource, I&#x27;d expect in a year or 2, home use will be on par in quality with paid services.<p>I cant wait for realtime video translate, so shows with non-english subs can be translated into english speech.   You can do it now with some services, upload a video and lang&#x2F;voice&#x2F;mouth will convert to any language.</div><br/></div></div><div id="39374859" class="c"><input type="checkbox" id="c-39374859" checked=""/><div class="controls bullet"><span class="by">LarsDu88</span><span>|</span><a href="#39374064">prev</a><span>|</span><a href="#39374051">next</a><span>|</span><label class="collapse" for="c-39374859">[-]</label><label class="expand" for="c-39374859">[3 more]</label></div><br/><div class="children"><div class="content">Sounds about as good as ElevenLabs.io
Hopefully if this ships on AWS, it will support SSML tags.
I used Elevenlabs.io for all the voices in my VR game (<a href="https:&#x2F;&#x2F;roguestargun.com" rel="nofollow">https:&#x2F;&#x2F;roguestargun.com</a>), but its still lacking on the emotion front which is all one-shot</div><br/><div id="39375069" class="c"><input type="checkbox" id="c-39375069" checked=""/><div class="controls bullet"><span class="by">ghostbrainalpha</span><span>|</span><a href="#39374859">parent</a><span>|</span><a href="#39374051">next</a><span>|</span><label class="collapse" for="c-39375069">[-]</label><label class="expand" for="c-39375069">[2 more]</label></div><br/><div class="children"><div class="content">Game looks great.  Are you supporting Flight Sticks?</div><br/><div id="39379812" class="c"><input type="checkbox" id="c-39379812" checked=""/><div class="controls bullet"><span class="by">LarsDu88</span><span>|</span><a href="#39374859">root</a><span>|</span><a href="#39375069">parent</a><span>|</span><a href="#39374051">next</a><span>|</span><label class="collapse" for="c-39379812">[-]</label><label class="expand" for="c-39379812">[1 more]</label></div><br/><div class="children"><div class="content">Eventually yes. Honestly I have joystick mappings setup in the games input configuration, but I no longer own a joystick or hotas, so somebody is gonna have to verify this for me.<p>Gamedev ain&#x27;t my day job, and the reality is most folks outside of hardcore flightsim enthusiasts don&#x27;t own joysticks</div><br/></div></div></div></div></div></div><div id="39374051" class="c"><input type="checkbox" id="c-39374051" checked=""/><div class="controls bullet"><span class="by">unsupp0rted</span><span>|</span><a href="#39374859">prev</a><span>|</span><a href="#39374068">next</a><span>|</span><label class="collapse" for="c-39374051">[-]</label><label class="expand" for="c-39374051">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Echoing the widely-reported &quot;emergent abilities&quot; of Large Language Models when trained on increasing volume of data, we show that BASE TTS variants built with 10k+ hours start to exhibit advanced understanding of texts that enable contextually appropriate prosody.</div><br/></div></div><div id="39374068" class="c"><input type="checkbox" id="c-39374068" checked=""/><div class="controls bullet"><span class="by">revenga99</span><span>|</span><a href="#39374051">prev</a><span>|</span><a href="#39374338">next</a><span>|</span><label class="collapse" for="c-39374068">[-]</label><label class="expand" for="c-39374068">[13 more]</label></div><br/><div class="children"><div class="content">Wow. I could see this as threatening audio book narrators. However I would still prefer a real narrator to this in its current state. I think what it might be missing is different voices&#x2F;accents for different characters.</div><br/><div id="39378976" class="c"><input type="checkbox" id="c-39378976" checked=""/><div class="controls bullet"><span class="by">geor9e</span><span>|</span><a href="#39374068">parent</a><span>|</span><a href="#39378835">next</a><span>|</span><label class="collapse" for="c-39378976">[-]</label><label class="expand" for="c-39378976">[2 more]</label></div><br/><div class="children"><div class="content">Folks probably will think me silly for this, but I prefer TTS. I have access to voice actor audiobooks but I pick the .epub files instead. I made a little extension to inject window.speechSynthesis with &quot;Microsoft Steffan Online (Natural) - English (United States)&quot; at rate=6 when I hit a hotkey. At high speed it&#x27;s much clearer and natural sounding than a sped up voice actor recording.</div><br/><div id="39379774" class="c"><input type="checkbox" id="c-39379774" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#39374068">root</a><span>|</span><a href="#39378976">parent</a><span>|</span><a href="#39378835">next</a><span>|</span><label class="collapse" for="c-39379774">[-]</label><label class="expand" for="c-39379774">[1 more]</label></div><br/><div class="children"><div class="content">I also prefer TTS. The spin voice actors put on the text always distracts me. With text to speech I only get what&#x27;s in the text itself.<p>I wrote a Perl&#x2F;Tk GUI script for my file manager to manage text to speech through Festival 1.96 w&#x2F;voice_nitech_us_awb_arctic_hts. Unlike neural network AI models it runs fine even on very slow machines.</div><br/></div></div></div></div><div id="39378835" class="c"><input type="checkbox" id="c-39378835" checked=""/><div class="controls bullet"><span class="by">dataminded</span><span>|</span><a href="#39374068">parent</a><span>|</span><a href="#39378976">prev</a><span>|</span><a href="#39374527">next</a><span>|</span><label class="collapse" for="c-39378835">[-]</label><label class="expand" for="c-39378835">[1 more]</label></div><br/><div class="children"><div class="content">As an avid consumer of audio books (150+&#x2F;year) - we are well past the point where narrators are necessary. Professional audio books take too long to release, are too expensive, are concentrated on a limited number of platforms and just aren&#x27;t THAT much better than the automated stuff for the long tail of books.</div><br/></div></div><div id="39374527" class="c"><input type="checkbox" id="c-39374527" checked=""/><div class="controls bullet"><span class="by">dshpala</span><span>|</span><a href="#39374068">parent</a><span>|</span><a href="#39378835">prev</a><span>|</span><a href="#39374277">next</a><span>|</span><label class="collapse" for="c-39374527">[-]</label><label class="expand" for="c-39374527">[2 more]</label></div><br/><div class="children"><div class="content">I think Google&#x27;s product has that: <a href="https:&#x2F;&#x2F;play.google.com&#x2F;books&#x2F;publish&#x2F;autonarrated&#x2F;">https:&#x2F;&#x2F;play.google.com&#x2F;books&#x2F;publish&#x2F;autonarrated&#x2F;</a></div><br/><div id="39377667" class="c"><input type="checkbox" id="c-39377667" checked=""/><div class="controls bullet"><span class="by">pparanoidd</span><span>|</span><a href="#39374068">root</a><span>|</span><a href="#39374527">parent</a><span>|</span><a href="#39374277">next</a><span>|</span><label class="collapse" for="c-39377667">[-]</label><label class="expand" for="c-39377667">[1 more]</label></div><br/><div class="children"><div class="content">That sounds pretty bad though</div><br/></div></div></div></div><div id="39374277" class="c"><input type="checkbox" id="c-39374277" checked=""/><div class="controls bullet"><span class="by">swashboon</span><span>|</span><a href="#39374068">parent</a><span>|</span><a href="#39374527">prev</a><span>|</span><a href="#39374338">next</a><span>|</span><label class="collapse" for="c-39374277">[-]</label><label class="expand" for="c-39374277">[7 more]</label></div><br/><div class="children"><div class="content">Audible doesn&#x27;t allow AI narration or much Public Domain stuff at the moment. The only thing keeping it from happening is the markets trying to keep back a flood of crap from over taking &#x2F; drowning &#x2F; diluting the more well crafted options and causing the consumers to get really annoyed.</div><br/><div id="39374465" class="c"><input type="checkbox" id="c-39374465" checked=""/><div class="controls bullet"><span class="by">TOMDM</span><span>|</span><a href="#39374068">root</a><span>|</span><a href="#39374277">parent</a><span>|</span><a href="#39374861">next</a><span>|</span><label class="collapse" for="c-39374465">[-]</label><label class="expand" for="c-39374465">[5 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s be honest, the moment Amazon thinks their tts is good enough, they&#x27;ll be offering AI audible deals to every author on their platform</div><br/><div id="39378198" class="c"><input type="checkbox" id="c-39378198" checked=""/><div class="controls bullet"><span class="by">coredog64</span><span>|</span><a href="#39374068">root</a><span>|</span><a href="#39374465">parent</a><span>|</span><a href="#39374579">next</a><span>|</span><label class="collapse" for="c-39378198">[-]</label><label class="expand" for="c-39378198">[1 more]</label></div><br/><div class="children"><div class="content">The 80% solution:  Pair with a professional narrator who has consented to have their voice modeled by this (see the note at the bottom about what they held back from open sourcing).  This generates a beta, and then you can pay the human narrator to rework specific sections youâre unhappy with.</div><br/></div></div><div id="39374579" class="c"><input type="checkbox" id="c-39374579" checked=""/><div class="controls bullet"><span class="by">swashboon</span><span>|</span><a href="#39374068">root</a><span>|</span><a href="#39374465">parent</a><span>|</span><a href="#39378198">prev</a><span>|</span><a href="#39374861">next</a><span>|</span><label class="collapse" for="c-39374579">[-]</label><label class="expand" for="c-39374579">[3 more]</label></div><br/><div class="children"><div class="content">Yea, hard to say because the obvious implementation would be to just have it built into phones once the model is potentially portable enough - I see this happening  quicker as a more general TTS functionality much like Google is doing with &#x27;subtitles anywhere&#x27; aka Live Caption. Paired with translations we maybe pretty close to the universal translator type functionality. I could see end users being able to customize their voice assistant even more or maybe having multiple based on if its talking for you or to you.<p>Anyways the problem with this is it makes the product &#x27;ai audiobook&#x27; basically worthless, why not just buy the eBook and have my personalized translator turn it into an audio book. Now you just have market differentiation between cheap ebook + ai narrator vs expensive + professional narration.<p>Though narration costs are already pretty cheap - it really does not factor into the cost of publishing an audio book that much unless its really a bottom of the barrel book.</div><br/><div id="39374729" class="c"><input type="checkbox" id="c-39374729" checked=""/><div class="controls bullet"><span class="by">swashboon</span><span>|</span><a href="#39374068">root</a><span>|</span><a href="#39374579">parent</a><span>|</span><a href="#39374709">next</a><span>|</span><label class="collapse" for="c-39374729">[-]</label><label class="expand" for="c-39374729">[1 more]</label></div><br/><div class="children"><div class="content">Thinking about this more - the copyright implications become much more interesting once its no longer a recording. Does it could as a private performance if you have headphones on? Is it a public performance if you listen to live TTS through your speakers in public?</div><br/></div></div><div id="39374709" class="c"><input type="checkbox" id="c-39374709" checked=""/><div class="controls bullet"><span class="by">TOMDM</span><span>|</span><a href="#39374068">root</a><span>|</span><a href="#39374579">parent</a><span>|</span><a href="#39374729">prev</a><span>|</span><a href="#39374861">next</a><span>|</span><label class="collapse" for="c-39374709">[-]</label><label class="expand" for="c-39374709">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m looking forward to my on device TTS, but Amazon has a decent moat with the DRM on their Kindles.<p>At least they&#x27;ll have to remain somewhat competitive once consumers decide they want the AI audiobooks and the like.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39374338" class="c"><input type="checkbox" id="c-39374338" checked=""/><div class="controls bullet"><span class="by">maxglute</span><span>|</span><a href="#39374068">prev</a><span>|</span><a href="#39374654">next</a><span>|</span><label class="collapse" for="c-39374338">[-]</label><label class="expand" for="c-39374338">[8 more]</label></div><br/><div class="children"><div class="content">Are there any decent TTS models that can be ran locally that plugs into existing software like SAPI without too much lag?</div><br/><div id="39375445" class="c"><input type="checkbox" id="c-39375445" checked=""/><div class="controls bullet"><span class="by">dvt</span><span>|</span><a href="#39374338">parent</a><span>|</span><a href="#39375944">next</a><span>|</span><label class="collapse" for="c-39375445">[-]</label><label class="expand" for="c-39375445">[5 more]</label></div><br/><div class="children"><div class="content">Bark and Tortoise work fairly well. Bark does super fast inference[1] on my M1.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;SaladTechnologies&#x2F;bark">https:&#x2F;&#x2F;github.com&#x2F;SaladTechnologies&#x2F;bark</a></div><br/><div id="39375638" class="c"><input type="checkbox" id="c-39375638" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#39374338">root</a><span>|</span><a href="#39375445">parent</a><span>|</span><a href="#39375944">next</a><span>|</span><label class="collapse" for="c-39375638">[-]</label><label class="expand" for="c-39375638">[4 more]</label></div><br/><div class="children"><div class="content">@dvt Is this just a containerized version of Bark? Wondering if this repo has M1-specific improvements.</div><br/><div id="39375850" class="c"><input type="checkbox" id="c-39375850" checked=""/><div class="controls bullet"><span class="by">dvt</span><span>|</span><a href="#39374338">root</a><span>|</span><a href="#39375638">parent</a><span>|</span><a href="#39375944">next</a><span>|</span><label class="collapse" for="c-39375850">[-]</label><label class="expand" for="c-39375850">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Is this just a containerized version of Bark<p>I think so.</div><br/><div id="39375953" class="c"><input type="checkbox" id="c-39375953" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#39374338">root</a><span>|</span><a href="#39375850">parent</a><span>|</span><a href="#39375944">next</a><span>|</span><label class="collapse" for="c-39375953">[-]</label><label class="expand" for="c-39375953">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m finding M1 generation quite slow (CPU-only) on the stock Barkâany tips on speeding it up?</div><br/><div id="39376114" class="c"><input type="checkbox" id="c-39376114" checked=""/><div class="controls bullet"><span class="by">dvt</span><span>|</span><a href="#39374338">root</a><span>|</span><a href="#39375953">parent</a><span>|</span><a href="#39375944">next</a><span>|</span><label class="collapse" for="c-39376114">[-]</label><label class="expand" for="c-39376114">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, haven&#x27;t messed around too much with optimizations. I thought it was quite fast compared to Tortoise for example (where generation speed was at a 3:1 ratio).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39375944" class="c"><input type="checkbox" id="c-39375944" checked=""/><div class="controls bullet"><span class="by">Nouser76</span><span>|</span><a href="#39374338">parent</a><span>|</span><a href="#39375445">prev</a><span>|</span><a href="#39375928">next</a><span>|</span><label class="collapse" for="c-39375944">[-]</label><label class="expand" for="c-39375944">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve used coqui.ai&#x27;s TTS models[0] and library[1] to great success. I was able to get cloned voice to be rendered in about 80% of the audio clip length, and I believe you can also stream the response. Do note the model license for XTTS, it is one they wrote themselves that has some restrictions.<p>[0] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;coqui&#x2F;XTTS-v2" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;coqui&#x2F;XTTS-v2</a><p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;coqui-ai&#x2F;TTS">https:&#x2F;&#x2F;github.com&#x2F;coqui-ai&#x2F;TTS</a></div><br/></div></div><div id="39375928" class="c"><input type="checkbox" id="c-39375928" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#39374338">parent</a><span>|</span><a href="#39375944">prev</a><span>|</span><a href="#39374654">next</a><span>|</span><label class="collapse" for="c-39375928">[-]</label><label class="expand" for="c-39375928">[1 more]</label></div><br/><div class="children"><div class="content">XTTS has a streaming mode with ~300ms latency and sounds good, though it has hallucination issues. StyleTTS2 sounds good and doesn&#x27;t hallucinate as much. It doesn&#x27;t support streaming but it&#x27;s fast so it can still respond quickly. But neither of them sound as good as Eleven Labs or OpenAI or this one.</div><br/></div></div></div></div><div id="39374654" class="c"><input type="checkbox" id="c-39374654" checked=""/><div class="controls bullet"><span class="by">mrfakename</span><span>|</span><a href="#39374338">prev</a><span>|</span><a href="#39376121">next</a><span>|</span><label class="collapse" for="c-39374654">[-]</label><label class="expand" for="c-39374654">[6 more]</label></div><br/><div class="children"><div class="content">Sadly they didn&#x27;t release the code or models</div><br/><div id="39375462" class="c"><input type="checkbox" id="c-39375462" checked=""/><div class="controls bullet"><span class="by">chankstein38</span><span>|</span><a href="#39374654">parent</a><span>|</span><a href="#39375534">next</a><span>|</span><label class="collapse" for="c-39375462">[-]</label><label class="expand" for="c-39375462">[2 more]</label></div><br/><div class="children"><div class="content">Agreed.  It hardly feels worth even reading through the paper since, from my perspective, it may as well just be made up.  I can also write &quot;Hey guys I made a good TTS it&#x27;s really cool and great and the voices sound really natural&quot; and put some samples together.  If I never release any code or models or anything, it may as well have not been published.</div><br/><div id="39378215" class="c"><input type="checkbox" id="c-39378215" checked=""/><div class="controls bullet"><span class="by">Terretta</span><span>|</span><a href="#39374654">root</a><span>|</span><a href="#39375462">parent</a><span>|</span><a href="#39375534">next</a><span>|</span><label class="collapse" for="c-39378215">[-]</label><label class="expand" for="c-39378215">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>really cool and great ... and put some samples together</i><p>There are samples on the page which demonstrate it completely failing.<p>Now as to whether you&#x27;d make that up is 4D chess.</div><br/></div></div></div></div><div id="39375534" class="c"><input type="checkbox" id="c-39375534" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#39374654">parent</a><span>|</span><a href="#39375462">prev</a><span>|</span><a href="#39375428">next</a><span>|</span><label class="collapse" for="c-39375534">[-]</label><label class="expand" for="c-39375534">[1 more]</label></div><br/><div class="children"><div class="content">The value of this stuff is going to zero. Don&#x27;t worry about it.<p>Product over model.<p>Models and weights are a race to the bottom. Everyone is doing it and competing on data efficiency, methodology, MOS, etc. Groups all over are releasing their data and weights. It doesn&#x27;t matter if Amazon doesn&#x27;t, other labs will do it to get ahead and to get attention.<p>This is going to be entirely pedestrian within a year.<p>ElevenLabs is not a unicorn. It&#x27;s an early-forming bubble.</div><br/></div></div><div id="39375428" class="c"><input type="checkbox" id="c-39375428" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#39374654">parent</a><span>|</span><a href="#39375534">prev</a><span>|</span><a href="#39376121">next</a><span>|</span><label class="collapse" for="c-39375428">[-]</label><label class="expand" for="c-39375428">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s for Your Own Good, don&#x27;t you know</div><br/><div id="39375468" class="c"><input type="checkbox" id="c-39375468" checked=""/><div class="controls bullet"><span class="by">chankstein38</span><span>|</span><a href="#39374654">root</a><span>|</span><a href="#39375428">parent</a><span>|</span><a href="#39376121">next</a><span>|</span><label class="collapse" for="c-39375468">[-]</label><label class="expand" for="c-39375468">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m so glad they are all so protective of my safety! Lord knows I&#x27;m a child incapable of controlling myself or having my own morals! &#x2F;s</div><br/></div></div></div></div></div></div><div id="39376121" class="c"><input type="checkbox" id="c-39376121" checked=""/><div class="controls bullet"><span class="by">nshm</span><span>|</span><a href="#39374654">prev</a><span>|</span><a href="#39376841">next</a><span>|</span><label class="collapse" for="c-39376121">[-]</label><label class="expand" for="c-39376121">[2 more]</label></div><br/><div class="children"><div class="content">Err, I deeply respect Amazon TTS team but this paper and synthesis is..... You publish the paper in 2024 and include YourTTS in your baselines to look better. Come on! There is XTTS2 around!<p>Voice sounds robotic and plain. Most likely a lot of audiobooks in training data and less conversational speech. And dropping diffusion was not a great idea, voice is not crystal clear anymore, it is more like a telephony recording.</div><br/><div id="39377165" class="c"><input type="checkbox" id="c-39377165" checked=""/><div class="controls bullet"><span class="by">thorum</span><span>|</span><a href="#39376121">parent</a><span>|</span><a href="#39376841">next</a><span>|</span><label class="collapse" for="c-39377165">[-]</label><label class="expand" for="c-39377165">[1 more]</label></div><br/><div class="children"><div class="content">xtts2 is great, but it looks like this model is probably more consistent with its output and has a better grasp of meaning in long texts.</div><br/></div></div></div></div><div id="39376841" class="c"><input type="checkbox" id="c-39376841" checked=""/><div class="controls bullet"><span class="by">sebmellen</span><span>|</span><a href="#39376121">prev</a><span>|</span><a href="#39374441">next</a><span>|</span><label class="collapse" for="c-39376841">[-]</label><label class="expand" for="c-39376841">[4 more]</label></div><br/><div class="children"><div class="content">Open question: does anyone know of a TTS model which can synchronize the output to an SRT or other subtitle file?</div><br/><div id="39376968" class="c"><input type="checkbox" id="c-39376968" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#39376841">parent</a><span>|</span><a href="#39374441">next</a><span>|</span><label class="collapse" for="c-39376968">[-]</label><label class="expand" for="c-39376968">[3 more]</label></div><br/><div class="children"><div class="content">To answer directly first: I don&#x27;t know of any model with this built in.<p>To answer more generally: but it should be pretty straightforward to use any old TTS model, the subtitle timestamps, and set the according delay until the next subtitle change and get the same effect. The alternative (changing the speed of the generated voice) is also possible via the same method but the problem there, and the problem when directly driven by a model, is subtitles don&#x27;t clue you in on when e.g. someone is talking slow or there was a pause in conversation so that subtitle staid up a little longer than a normal one. What you&#x27;d need to solve that is a model which takes both the video and the subtitle info, a bit more difficult.<p>Of course it&#x27;s also a question about what the end goal is. It&#x27;s pretty rare to have significant subtitles but no audio so if the ultimate goal was e.g. changing a actor&#x27;s voice you&#x27;d probably get much better results with an audio-&gt;audio model than a TTS-&gt;audio model. Likely similar kinds of stories for many other use cases.</div><br/><div id="39377774" class="c"><input type="checkbox" id="c-39377774" checked=""/><div class="controls bullet"><span class="by">selcuka</span><span>|</span><a href="#39376841">root</a><span>|</span><a href="#39376968">parent</a><span>|</span><a href="#39374441">next</a><span>|</span><label class="collapse" for="c-39377774">[-]</label><label class="expand" for="c-39377774">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Of course it&#x27;s also a question about what the end goal is. It&#x27;s pretty rare to have significant subtitles but no audio<p>I think the question was about dubbing a movie in another language, using SRT files.</div><br/><div id="39377820" class="c"><input type="checkbox" id="c-39377820" checked=""/><div class="controls bullet"><span class="by">sebmellen</span><span>|</span><a href="#39376841">root</a><span>|</span><a href="#39377774">parent</a><span>|</span><a href="#39374441">next</a><span>|</span><label class="collapse" for="c-39377820">[-]</label><label class="expand" for="c-39377820">[1 more]</label></div><br/><div class="children"><div class="content">Actually more about dubbing lectures!</div><br/></div></div></div></div></div></div></div></div><div id="39374441" class="c"><input type="checkbox" id="c-39374441" checked=""/><div class="controls bullet"><span class="by">SparkyMcUnicorn</span><span>|</span><a href="#39376841">prev</a><span>|</span><a href="#39375720">next</a><span>|</span><label class="collapse" for="c-39374441">[-]</label><label class="expand" for="c-39374441">[3 more]</label></div><br/><div class="children"><div class="content">&gt; ... capable of mimicking speaker characteristics with just a few seconds of reference audio ... we have decided against open-sourcing this model as a precautionary measure.<p>Disappointed yet again.</div><br/><div id="39376585" class="c"><input type="checkbox" id="c-39376585" checked=""/><div class="controls bullet"><span class="by">someplaceguy</span><span>|</span><a href="#39374441">parent</a><span>|</span><a href="#39375086">prev</a><span>|</span><a href="#39375720">next</a><span>|</span><label class="collapse" for="c-39376585">[-]</label><label class="expand" for="c-39376585">[1 more]</label></div><br/><div class="children"><div class="content">Someone should send the developers this audio recording I have of Jeff Bezos saying that he changed his mind and wants the model to be released as open-source.</div><br/></div></div></div></div><div id="39375720" class="c"><input type="checkbox" id="c-39375720" checked=""/><div class="controls bullet"><span class="by">JanSt</span><span>|</span><a href="#39374441">prev</a><span>|</span><label class="collapse" for="c-39375720">[-]</label><label class="expand" for="c-39375720">[1 more]</label></div><br/><div class="children"><div class="content">I would love an API for this.. any information on availability?</div><br/></div></div></div></div></div></div></div></body></html>