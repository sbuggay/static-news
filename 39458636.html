<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708592470782" as="style"/><link rel="stylesheet" href="styles.css?v=1708592470782"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.peerdb.io/moving-a-billion-postgres-rows-on-a-100-budget">Moving a billion Postgres rows on a $100 budget</a> <span class="domain">(<a href="https://blog.peerdb.io">blog.peerdb.io</a>)</span></div><div class="subtext"><span>samaysharma</span> | <span>56 comments</span></div><br/><div><div id="39464153" class="c"><input type="checkbox" id="c-39464153" checked=""/><div class="controls bullet"><span class="by">rthnbgrredf</span><span>|</span><a href="#39464316">next</a><span>|</span><label class="collapse" for="c-39464153">[-]</label><label class="expand" for="c-39464153">[1 more]</label></div><br/><div class="children"><div class="content">You can do something better for 0$, just install the TimescaleDB postgres extension and execute:<p>SELECT create_hypertable(&#x27;public.challenge_1br&#x27;, by_range(&#x27;time&#x27;));<p>Now, enjoy your better than Snowflake query performance performance at no extra cost.</div><br/></div></div><div id="39464316" class="c"><input type="checkbox" id="c-39464316" checked=""/><div class="controls bullet"><span class="by">mannyv</span><span>|</span><a href="#39464153">prev</a><span>|</span><a href="#39460181">next</a><span>|</span><label class="collapse" for="c-39464316">[-]</label><label class="expand" for="c-39464316">[1 more]</label></div><br/><div class="children"><div class="content">Why not just set up a replica pgsql, then break the connection and upgrade it to primary?<p>It&#x27;s amusing how much effort is put into ETL these days. I remember when ETL departments were filled with the sludge of the programming world. It took  ETL departments weeks to generate a CSV, and it would inevitably have massive numbers of errors because they didn&#x27;t actually follow the format that was specified on the form they forced everyone to use.</div><br/></div></div><div id="39460181" class="c"><input type="checkbox" id="c-39460181" checked=""/><div class="controls bullet"><span class="by">hipadev23</span><span>|</span><a href="#39464316">prev</a><span>|</span><a href="#39459824">next</a><span>|</span><label class="collapse" for="c-39460181">[-]</label><label class="expand" for="c-39460181">[13 more]</label></div><br/><div class="children"><div class="content">&gt; Moving 1 billion rows is no easy task<p>This isn&#x27;t an accurate premise. Modern OLAP databases make dealing with billions to trillions of rows manageable, including on a single server. Exporting &quot;select * from table&quot; from an OLTP such as Postgres or MySQL into an OLAP is trivial and quite fast, and if 100M rows&#x2F;sec on commodity servers isn&#x27;t fast enough, there&#x27;s always performance tuning [1].<p>[1] <a href="https:&#x2F;&#x2F;altinity.com&#x2F;blog&#x2F;loading-100b-rows-in-minutes-in-altinity-cloud" rel="nofollow">https:&#x2F;&#x2F;altinity.com&#x2F;blog&#x2F;loading-100b-rows-in-minutes-in-al...</a></div><br/><div id="39460301" class="c"><input type="checkbox" id="c-39460301" checked=""/><div class="controls bullet"><span class="by">saisrirampur</span><span>|</span><a href="#39460181">parent</a><span>|</span><a href="#39461755">next</a><span>|</span><label class="collapse" for="c-39460301">[-]</label><label class="expand" for="c-39460301">[8 more]</label></div><br/><div class="children"><div class="content">That sentence is more in the context of the blog of moving a billion rows across data-stores. We will edit it to make that more clear. Thanks for the feedback.</div><br/><div id="39460335" class="c"><input type="checkbox" id="c-39460335" checked=""/><div class="controls bullet"><span class="by">hipadev23</span><span>|</span><a href="#39460181">root</a><span>|</span><a href="#39460301">parent</a><span>|</span><a href="#39462276">next</a><span>|</span><label class="collapse" for="c-39460335">[-]</label><label class="expand" for="c-39460335">[2 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t speak for Snowflake, but on Clickhouse it&#x27;s quite literally [1]:<p><pre><code>    insert into new_table select * from postgresql(&#x27;postgres:5432&#x27;, &#x27;db&#x27;, &#x27;table&#x27;, &#x27;user&#x27;, &#x27;pass&#x27;);
</code></pre>
I assume it&#x27;s similarly easy on Snowflake, Databricks, SingleStore, and the rest.<p>[1] <a href="https:&#x2F;&#x2F;clickhouse.com&#x2F;docs&#x2F;en&#x2F;sql-reference&#x2F;table-functions&#x2F;postgresql" rel="nofollow">https:&#x2F;&#x2F;clickhouse.com&#x2F;docs&#x2F;en&#x2F;sql-reference&#x2F;table-functions...</a></div><br/><div id="39461519" class="c"><input type="checkbox" id="c-39461519" checked=""/><div class="controls bullet"><span class="by">notjoemama</span><span>|</span><a href="#39460181">root</a><span>|</span><a href="#39460335">parent</a><span>|</span><a href="#39462276">next</a><span>|</span><label class="collapse" for="c-39461519">[-]</label><label class="expand" for="c-39461519">[1 more]</label></div><br/><div class="children"><div class="content">Yes, and there’s loads of opportunity to over engineer every step of the process.</div><br/></div></div></div></div><div id="39462276" class="c"><input type="checkbox" id="c-39462276" checked=""/><div class="controls bullet"><span class="by">CyberDildonics</span><span>|</span><a href="#39460181">root</a><span>|</span><a href="#39460301">parent</a><span>|</span><a href="#39460335">prev</a><span>|</span><a href="#39461755">next</a><span>|</span><label class="collapse" for="c-39462276">[-]</label><label class="expand" for="c-39462276">[5 more]</label></div><br/><div class="children"><div class="content">data stores or data lakes with data rivers?</div><br/><div id="39462437" class="c"><input type="checkbox" id="c-39462437" checked=""/><div class="controls bullet"><span class="by">ssss11</span><span>|</span><a href="#39460181">root</a><span>|</span><a href="#39462276">parent</a><span>|</span><a href="#39463504">next</a><span>|</span><label class="collapse" for="c-39462437">[-]</label><label class="expand" for="c-39462437">[3 more]</label></div><br/><div class="children"><div class="content">You’re forgetting the data dams on those data rivers</div><br/><div id="39463353" class="c"><input type="checkbox" id="c-39463353" checked=""/><div class="controls bullet"><span class="by">collingreen</span><span>|</span><a href="#39460181">root</a><span>|</span><a href="#39462437">parent</a><span>|</span><a href="#39463504">next</a><span>|</span><label class="collapse" for="c-39463353">[-]</label><label class="expand" for="c-39463353">[2 more]</label></div><br/><div class="children"><div class="content">Please can we have data fish ladders next?</div><br/><div id="39463366" class="c"><input type="checkbox" id="c-39463366" checked=""/><div class="controls bullet"><span class="by">davidw</span><span>|</span><a href="#39460181">root</a><span>|</span><a href="#39463353">parent</a><span>|</span><a href="#39463504">next</a><span>|</span><label class="collapse" for="c-39463366">[-]</label><label class="expand" for="c-39463366">[1 more]</label></div><br/><div class="children"><div class="content">Here in Oregon we are removing the data dams to have more natural data flows.</div><br/></div></div></div></div></div></div><div id="39463504" class="c"><input type="checkbox" id="c-39463504" checked=""/><div class="controls bullet"><span class="by">forgingahead</span><span>|</span><a href="#39460181">root</a><span>|</span><a href="#39462276">parent</a><span>|</span><a href="#39462437">prev</a><span>|</span><a href="#39461755">next</a><span>|</span><label class="collapse" for="c-39463504">[-]</label><label class="expand" for="c-39463504">[1 more]</label></div><br/><div class="children"><div class="content">gotta stick to the rivers and lakes that you&#x27;re used to</div><br/></div></div></div></div></div></div><div id="39461755" class="c"><input type="checkbox" id="c-39461755" checked=""/><div class="controls bullet"><span class="by">vmfunction</span><span>|</span><a href="#39460181">parent</a><span>|</span><a href="#39460301">prev</a><span>|</span><a href="#39459824">next</a><span>|</span><label class="collapse" for="c-39461755">[-]</label><label class="expand" for="c-39461755">[4 more]</label></div><br/><div class="children"><div class="content">Or wouldn&#x27;t COPY from CSV be much faster?</div><br/><div id="39462864" class="c"><input type="checkbox" id="c-39462864" checked=""/><div class="controls bullet"><span class="by">killingtime74</span><span>|</span><a href="#39460181">root</a><span>|</span><a href="#39461755">parent</a><span>|</span><a href="#39459824">next</a><span>|</span><label class="collapse" for="c-39462864">[-]</label><label class="expand" for="c-39462864">[3 more]</label></div><br/><div class="children"><div class="content">CSV based export&#x2F;import would involve writing to disk, sending over network then reading back from file. Select is just one copy over network, no intermediate CSV.</div><br/><div id="39463037" class="c"><input type="checkbox" id="c-39463037" checked=""/><div class="controls bullet"><span class="by">subleq</span><span>|</span><a href="#39460181">root</a><span>|</span><a href="#39462864">parent</a><span>|</span><a href="#39459824">next</a><span>|</span><label class="collapse" for="c-39463037">[-]</label><label class="expand" for="c-39463037">[2 more]</label></div><br/><div class="children"><div class="content">You can stream CSV without writing it to a disk.</div><br/><div id="39463460" class="c"><input type="checkbox" id="c-39463460" checked=""/><div class="controls bullet"><span class="by">edgyquant</span><span>|</span><a href="#39460181">root</a><span>|</span><a href="#39463037">parent</a><span>|</span><a href="#39459824">next</a><span>|</span><label class="collapse" for="c-39463460">[-]</label><label class="expand" for="c-39463460">[1 more]</label></div><br/><div class="children"><div class="content">What would be the point in that?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39459824" class="c"><input type="checkbox" id="c-39459824" checked=""/><div class="controls bullet"><span class="by">_boffin_</span><span>|</span><a href="#39460181">prev</a><span>|</span><a href="#39460836">next</a><span>|</span><label class="collapse" for="c-39459824">[-]</label><label class="expand" for="c-39459824">[3 more]</label></div><br/><div class="children"><div class="content">How about 11b and a horrible python script over to parquet to your wee little NAS for your homelab?</div><br/><div id="39461357" class="c"><input type="checkbox" id="c-39461357" checked=""/><div class="controls bullet"><span class="by">MenhirMike</span><span>|</span><a href="#39459824">parent</a><span>|</span><a href="#39460135">next</a><span>|</span><label class="collapse" for="c-39461357">[-]</label><label class="expand" for="c-39461357">[1 more]</label></div><br/><div class="children"><div class="content">Your wee little NAS will probably still have 2x-3x better IO performance than cloud based services.</div><br/></div></div><div id="39460135" class="c"><input type="checkbox" id="c-39460135" checked=""/><div class="controls bullet"><span class="by">sss111</span><span>|</span><a href="#39459824">parent</a><span>|</span><a href="#39461357">prev</a><span>|</span><a href="#39460836">next</a><span>|</span><label class="collapse" for="c-39460135">[-]</label><label class="expand" for="c-39460135">[1 more]</label></div><br/><div class="children"><div class="content">now we&#x27;re talking about innovation</div><br/></div></div></div></div><div id="39460836" class="c"><input type="checkbox" id="c-39460836" checked=""/><div class="controls bullet"><span class="by">HermitX</span><span>|</span><a href="#39459824">prev</a><span>|</span><a href="#39460198">next</a><span>|</span><label class="collapse" for="c-39460836">[-]</label><label class="expand" for="c-39460836">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve already thought of the follow-up to this article, &#x27;Querying a billion rows on a $XX budget.&#x27; Let me give you my answer directly: switch from Snowflake to StarRocks. It&#x27;s an open-source project under the Linux Foundation, with speed that&#x27;s more than adequate, especially for queries involving multiple tables. If you&#x27;re interested, you might want to check it out, <a href="https:&#x2F;&#x2F;medium.com&#x2F;starrocks-engineering&#x2F;how-to-reduce-snowflake-costs-by-80-deb87aa69bc5" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;starrocks-engineering&#x2F;how-to-reduce-snowf...</a>.</div><br/><div id="39461818" class="c"><input type="checkbox" id="c-39461818" checked=""/><div class="controls bullet"><span class="by">ako</span><span>|</span><a href="#39460836">parent</a><span>|</span><a href="#39460198">next</a><span>|</span><label class="collapse" for="c-39461818">[-]</label><label class="expand" for="c-39461818">[2 more]</label></div><br/><div class="children"><div class="content">Starrocks sounds too good to be true, what are the cons?</div><br/><div id="39463630" class="c"><input type="checkbox" id="c-39463630" checked=""/><div class="controls bullet"><span class="by">392</span><span>|</span><a href="#39460836">root</a><span>|</span><a href="#39461818">parent</a><span>|</span><a href="#39460198">next</a><span>|</span><label class="collapse" for="c-39463630">[-]</label><label class="expand" for="c-39463630">[1 more]</label></div><br/><div class="children"><div class="content">Hard to set up in Kubernetes. Seems to want you to know too many specifics for how to shard your db ahead of time. Don&#x27;t remember how it handles schema evolution. I settled for Databend because it just worked easy and fast with nothing more than table definitions.</div><br/></div></div></div></div></div></div><div id="39460198" class="c"><input type="checkbox" id="c-39460198" checked=""/><div class="controls bullet"><span class="by">tharakam</span><span>|</span><a href="#39460836">prev</a><span>|</span><a href="#39459829">next</a><span>|</span><label class="collapse" for="c-39460198">[-]</label><label class="expand" for="c-39460198">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m confused. Everything sounds very expensive to me.<p>The last table which compares it with the other vendors is surprising. Even Stich Data (cheapest) costs $1 to move 240K records: (1B &#x2F; 4,166.67 = 240K). Is this real?<p>So, their solution costs $1 to process 13.6M records. Sounds like this is not very share-worthy.<p>What I&#x27;m missing here?</div><br/><div id="39461235" class="c"><input type="checkbox" id="c-39461235" checked=""/><div class="controls bullet"><span class="by">garciasn</span><span>|</span><a href="#39460198">parent</a><span>|</span><a href="#39461829">next</a><span>|</span><label class="collapse" for="c-39461235">[-]</label><label class="expand" for="c-39461235">[4 more]</label></div><br/><div class="children"><div class="content">What I want to know is why the  fuck it takes 8 days to load 700MM records—in 2024.<p>I couldn’t even continue reading the article because it must be from 2006.</div><br/><div id="39461282" class="c"><input type="checkbox" id="c-39461282" checked=""/><div class="controls bullet"><span class="by">saisrirampur</span><span>|</span><a href="#39460198">root</a><span>|</span><a href="#39461235">parent</a><span>|</span><a href="#39461836">next</a><span>|</span><label class="collapse" for="c-39461282">[-]</label><label class="expand" for="c-39461282">[2 more]</label></div><br/><div class="children"><div class="content">700M records in 8 days (1024 rps) is to mimic a real-world transactional (OLTP) workload. It doesn&#x27;t define limits on what throughput can be achieved.</div><br/><div id="39461556" class="c"><input type="checkbox" id="c-39461556" checked=""/><div class="controls bullet"><span class="by">garciasn</span><span>|</span><a href="#39460198">root</a><span>|</span><a href="#39461282">parent</a><span>|</span><a href="#39461836">next</a><span>|</span><label class="collapse" for="c-39461556">[-]</label><label class="expand" for="c-39461556">[1 more]</label></div><br/><div class="children"><div class="content">Ahhh. Thank you. That makes more sense.</div><br/></div></div></div></div></div></div><div id="39461829" class="c"><input type="checkbox" id="c-39461829" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#39460198">parent</a><span>|</span><a href="#39461235">prev</a><span>|</span><a href="#39459829">next</a><span>|</span><label class="collapse" for="c-39461829">[-]</label><label class="expand" for="c-39461829">[1 more]</label></div><br/><div class="children"><div class="content">Welcome to 2024 and the generation of developers raised in the cloud native world who think this is normal.<p>A billion rows is nothing and having $100 appear in conjunction with that is absurd unless you are doing some kind of really heavy compute or AI model training on that data.<p>By 2030 we’ll have those costs well up over a thousand dollars and it’ll take five or six separate SaaS systems wired together to do this. Progress!</div><br/></div></div></div></div><div id="39459829" class="c"><input type="checkbox" id="c-39459829" checked=""/><div class="controls bullet"><span class="by">twelfthnight</span><span>|</span><a href="#39460198">prev</a><span>|</span><a href="#39460017">next</a><span>|</span><label class="collapse" for="c-39459829">[-]</label><label class="expand" for="c-39459829">[14 more]</label></div><br/><div class="children"><div class="content">If someone is price conscious, why move from postgres to snowflake?</div><br/><div id="39459847" class="c"><input type="checkbox" id="c-39459847" checked=""/><div class="controls bullet"><span class="by">happytiger</span><span>|</span><a href="#39459829">parent</a><span>|</span><a href="#39459962">next</a><span>|</span><label class="collapse" for="c-39459847">[-]</label><label class="expand" for="c-39459847">[9 more]</label></div><br/><div class="children"><div class="content">That was my first thought as well — like who is the audience for this?<p>My first thought was that you could stay on Postgres <i>and save that $100</i> by using the secret power of Open Source.<p>Well said.</div><br/><div id="39460140" class="c"><input type="checkbox" id="c-39460140" checked=""/><div class="controls bullet"><span class="by">__s</span><span>|</span><a href="#39459829">root</a><span>|</span><a href="#39459847">parent</a><span>|</span><a href="#39460167">next</a><span>|</span><label class="collapse" for="c-39460140">[-]</label><label class="expand" for="c-39460140">[2 more]</label></div><br/><div class="children"><div class="content">Seems like the title would be more accurate specifying postgres to snowflake<p>In the end, there&#x27;s companies paying to use snowflake, &amp; despite what one may believe they aren&#x27;t price oblivious. Having their application in postgres is a cost optimization, but then still relying on snowflake for data warehouse integrations</div><br/><div id="39464304" class="c"><input type="checkbox" id="c-39464304" checked=""/><div class="controls bullet"><span class="by">foobiekr</span><span>|</span><a href="#39459829">root</a><span>|</span><a href="#39460140">parent</a><span>|</span><a href="#39460167">next</a><span>|</span><label class="collapse" for="c-39464304">[-]</label><label class="expand" for="c-39464304">[1 more]</label></div><br/><div class="children"><div class="content">Honestly I think most snowflake customers were w cheap money side effect.<p>I love snowflake but their pricing is, in fact, absurd.</div><br/></div></div></div></div><div id="39460167" class="c"><input type="checkbox" id="c-39460167" checked=""/><div class="controls bullet"><span class="by">saisrirampur</span><span>|</span><a href="#39459829">root</a><span>|</span><a href="#39459847">parent</a><span>|</span><a href="#39460140">prev</a><span>|</span><a href="#39459962">next</a><span>|</span><label class="collapse" for="c-39460167">[-]</label><label class="expand" for="c-39460167">[6 more]</label></div><br/><div class="children"><div class="content">Based on my experience working with Postgres users since a decade (ex-Citus&#x2F;Microsoft), I don&#x27;t think Postgres is there yet to support every possible workload - ex: medium to larger scale (ex:1TB+) workloads in Real Time analytics, Data Warehousing, Search etc. Sure at smaller scales, it is very versatile to support any workload. That is why it is super common for companies to complement Postgres with other data stores. Don&#x27;t mean to say that Postgres will not get there, I think it will! But I see it to be more in the long term.</div><br/><div id="39460624" class="c"><input type="checkbox" id="c-39460624" checked=""/><div class="controls bullet"><span class="by">xenator</span><span>|</span><a href="#39459829">root</a><span>|</span><a href="#39460167">parent</a><span>|</span><a href="#39462663">next</a><span>|</span><label class="collapse" for="c-39460624">[-]</label><label class="expand" for="c-39460624">[3 more]</label></div><br/><div class="children"><div class="content">We operate with 80 Tb of data ATM. It is laying in several nodes and meta nodes (this is our own terminology). All Postgres.<p>Recently we need to move data from one DB to another, about 600M records. It is not biggest chank of the data, but we need it on different server because we use FTS a lot. And don&#x27;t want to interrupt other operations on previous server. It took 3 days and costs 0.</div><br/><div id="39460747" class="c"><input type="checkbox" id="c-39460747" checked=""/><div class="controls bullet"><span class="by">saisrirampur</span><span>|</span><a href="#39459829">root</a><span>|</span><a href="#39460624">parent</a><span>|</span><a href="#39460910">next</a><span>|</span><label class="collapse" for="c-39460747">[-]</label><label class="expand" for="c-39460747">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for context! Totally understand where you are coming from. Postgres can be moulded to work for many use-cases. However it could take good amount of effort to make it happen. For example in your case building and managing a sharded Postgres environment isn&#x27;t straightforward. It requires quite a lot of time and expertise. Citus automated exactly this (sharding). However it wasn&#x27;t a fit for every workload. <a href="https:&#x2F;&#x2F;docs.citusdata.com&#x2F;en&#x2F;v12.1&#x2F;get_started&#x2F;what_is_citus.html#when-citus-is-inappropriate" rel="nofollow">https:&#x2F;&#x2F;docs.citusdata.com&#x2F;en&#x2F;v12.1&#x2F;get_started&#x2F;what_is_citu...</a></div><br/></div></div><div id="39460910" class="c"><input type="checkbox" id="c-39460910" checked=""/><div class="controls bullet"><span class="by">quadrature</span><span>|</span><a href="#39459829">root</a><span>|</span><a href="#39460624">parent</a><span>|</span><a href="#39460747">prev</a><span>|</span><a href="#39462663">next</a><span>|</span><label class="collapse" for="c-39460910">[-]</label><label class="expand" for="c-39460910">[1 more]</label></div><br/><div class="children"><div class="content">Is your workload an analytical or transactional workload ?.</div><br/></div></div></div></div><div id="39462663" class="c"><input type="checkbox" id="c-39462663" checked=""/><div class="controls bullet"><span class="by">aprdm</span><span>|</span><a href="#39459829">root</a><span>|</span><a href="#39460167">parent</a><span>|</span><a href="#39460624">prev</a><span>|</span><a href="#39459962">next</a><span>|</span><label class="collapse" for="c-39462663">[-]</label><label class="expand" for="c-39462663">[2 more]</label></div><br/><div class="children"><div class="content">since when is 1TB+ medium to large scale ? That easily fits on a single computer running Postgres, can even run almost all of it on memory depending on the server..</div><br/><div id="39462759" class="c"><input type="checkbox" id="c-39462759" checked=""/><div class="controls bullet"><span class="by">saisrirampur</span><span>|</span><a href="#39459829">root</a><span>|</span><a href="#39462663">parent</a><span>|</span><a href="#39459962">next</a><span>|</span><label class="collapse" for="c-39462759">[-]</label><label class="expand" for="c-39462759">[1 more]</label></div><br/><div class="children"><div class="content">Sure, the workload could be compute bound than memory. It also depends on what query workloads is the postgres planner and executor optimized for. TPC-H benchmark is an example where vanila Postgres is not as optimal as pure analytical stores.</div><br/></div></div></div></div></div></div></div></div><div id="39459962" class="c"><input type="checkbox" id="c-39459962" checked=""/><div class="controls bullet"><span class="by">quadrature</span><span>|</span><a href="#39459829">parent</a><span>|</span><a href="#39459847">prev</a><span>|</span><a href="#39460229">next</a><span>|</span><label class="collapse" for="c-39459962">[-]</label><label class="expand" for="c-39459962">[3 more]</label></div><br/><div class="children"><div class="content">Its not about being price conscious, its about finding an efficient way to replicate data into your data warehouse.</div><br/><div id="39460008" class="c"><input type="checkbox" id="c-39460008" checked=""/><div class="controls bullet"><span class="by">saisrirampur</span><span>|</span><a href="#39459829">root</a><span>|</span><a href="#39459962">parent</a><span>|</span><a href="#39463677">next</a><span>|</span><label class="collapse" for="c-39460008">[-]</label><label class="expand" for="c-39460008">[1 more]</label></div><br/><div class="children"><div class="content">OP here, thanks for chiming in. Yep, the blog talks about how efficient you can be while replicating data from Postgres to Data Warehouses. Also just for some context, the inspiration behind writing this blog is a common concern from customers that data-movement is expensive and also the insanely crazy margins in this space.</div><br/></div></div><div id="39463677" class="c"><input type="checkbox" id="c-39463677" checked=""/><div class="controls bullet"><span class="by">ies7</span><span>|</span><a href="#39459829">root</a><span>|</span><a href="#39459962">parent</a><span>|</span><a href="#39460008">prev</a><span>|</span><a href="#39460229">next</a><span>|</span><label class="collapse" for="c-39463677">[-]</label><label class="expand" for="c-39463677">[1 more]</label></div><br/><div class="children"><div class="content">Then they should take out the &quot;on a $100 budget&quot; from the title.</div><br/></div></div></div></div><div id="39460229" class="c"><input type="checkbox" id="c-39460229" checked=""/><div class="controls bullet"><span class="by">saisrirampur</span><span>|</span><a href="#39459829">parent</a><span>|</span><a href="#39459962">prev</a><span>|</span><a href="#39460017">next</a><span>|</span><label class="collapse" for="c-39460229">[-]</label><label class="expand" for="c-39460229">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for chiming in. The concepts&#x2F;idea of this blog can be extended to other target data-stores too, say ClickHouse.</div><br/></div></div></div></div><div id="39460017" class="c"><input type="checkbox" id="c-39460017" checked=""/><div class="controls bullet"><span class="by">rplnt</span><span>|</span><a href="#39459829">prev</a><span>|</span><a href="#39463732">next</a><span>|</span><label class="collapse" for="c-39460017">[-]</label><label class="expand" for="c-39460017">[3 more]</label></div><br/><div class="children"><div class="content">(edit: mostly offtopic observation follows)<p>I only knew Snowflake the id selection algorithm, so was a bit confused, but googling &quot;snowflake db&quot; showed me this blurb and now I&#x27;m even more confused.<p>&gt; Snowflake enables organizations to learn, build, and connect with their data-driven peers. Collaborate, build data apps &amp; power diverse workloads in the ...</div><br/><div id="39460588" class="c"><input type="checkbox" id="c-39460588" checked=""/><div class="controls bullet"><span class="by">quickslowdown</span><span>|</span><a href="#39460017">parent</a><span>|</span><a href="#39463732">next</a><span>|</span><label class="collapse" for="c-39460588">[-]</label><label class="expand" for="c-39460588">[2 more]</label></div><br/><div class="children"><div class="content">Snowflake &amp; Azure Synapse are competing products, if that helps.</div><br/><div id="39464199" class="c"><input type="checkbox" id="c-39464199" checked=""/><div class="controls bullet"><span class="by">dacryn</span><span>|</span><a href="#39460017">root</a><span>|</span><a href="#39460588">parent</a><span>|</span><a href="#39463732">next</a><span>|</span><label class="collapse" for="c-39464199">[-]</label><label class="expand" for="c-39464199">[1 more]</label></div><br/><div class="children"><div class="content">synapse is such a disaster. I don&#x27;t know any example of a decent size succesfull deployment that. It grinds to a halt and charges you a lot of money for it.<p>Snowflake is an amazing Agile database that has a great ecosystem.
Teradata still remains king if you want this type of cloud datawarehouse, and becomes price competitive with Snowflake if you actually use it intensively (transactions are cheaper on teradata than snowflake)<p>But in the end, a good managed postgresql is probably enough for 80% of the clients of Synaps&#x2F;Snowflake anyway. It&#x27;s just that CTO&#x27;s are starting to lose technical knowledge and are more politicians nowadays</div><br/></div></div></div></div></div></div><div id="39463732" class="c"><input type="checkbox" id="c-39463732" checked=""/><div class="controls bullet"><span class="by">guidedlight</span><span>|</span><a href="#39460017">prev</a><span>|</span><a href="#39460312">next</a><span>|</span><label class="collapse" for="c-39463732">[-]</label><label class="expand" for="c-39463732">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Moving a Billion Postgres Rows on a $100 Budget<p>In a magical universe where your time is free.</div><br/></div></div><div id="39460312" class="c"><input type="checkbox" id="c-39460312" checked=""/><div class="controls bullet"><span class="by">jmholla</span><span>|</span><a href="#39463732">prev</a><span>|</span><a href="#39460098">next</a><span>|</span><label class="collapse" for="c-39460312">[-]</label><label class="expand" for="c-39460312">[1 more]</label></div><br/><div class="children"><div class="content">This was a disappointing article. It was expecting it to explore validating the  integrity and consistency of the data, but that just seems to be handwaved away by a short section saying PeerDB handles it. This is especially disappointing since the article calls that out as one of the cruxes that makes this so difficult.</div><br/></div></div><div id="39460098" class="c"><input type="checkbox" id="c-39460098" checked=""/><div class="controls bullet"><span class="by">pryelluw</span><span>|</span><a href="#39460312">prev</a><span>|</span><a href="#39461147">next</a><span>|</span><label class="collapse" for="c-39460098">[-]</label><label class="expand" for="c-39460098">[2 more]</label></div><br/><div class="children"><div class="content">Does the elastic license fall under open source as defined  by OSI?</div><br/><div id="39462929" class="c"><input type="checkbox" id="c-39462929" checked=""/><div class="controls bullet"><span class="by">tristan957</span><span>|</span><a href="#39460098">parent</a><span>|</span><a href="#39461147">next</a><span>|</span><label class="collapse" for="c-39462929">[-]</label><label class="expand" for="c-39462929">[1 more]</label></div><br/><div class="children"><div class="content">No it does not.</div><br/></div></div></div></div><div id="39461147" class="c"><input type="checkbox" id="c-39461147" checked=""/><div class="controls bullet"><span class="by">nojvek</span><span>|</span><a href="#39460098">prev</a><span>|</span><a href="#39460116">next</a><span>|</span><label class="collapse" for="c-39461147">[-]</label><label class="expand" for="c-39461147">[6 more]</label></div><br/><div class="children"><div class="content">Postgres seriously needs a columnstore backed table instead of just a rowstore.<p>MSSQL has this and it is magic.
SingleStore has it, and it is wonderful.<p>I&#x27;m willing to give a bounty of $1000 to whoever adds that into main postgres tree.<p>Snowflake is great as a warehouse. it&#x27;s latency is shit when it comes to fast lookups and aggregates. If you can tolerate &gt;1s api calls, that is fine. It takes forever to insert a few rows in a large table.<p>If you want a proper live DB, snowflake is a rich man&#x27;s poor database.</div><br/><div id="39463750" class="c"><input type="checkbox" id="c-39463750" checked=""/><div class="controls bullet"><span class="by">speedgoose</span><span>|</span><a href="#39461147">parent</a><span>|</span><a href="#39464037">next</a><span>|</span><label class="collapse" for="c-39463750">[-]</label><label class="expand" for="c-39463750">[1 more]</label></div><br/><div class="children"><div class="content">Columnar store PostgreSQL extension exists, here are two but I think I’m missing at least another one:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;citusdata&#x2F;cstore_fdw">https:&#x2F;&#x2F;github.com&#x2F;citusdata&#x2F;cstore_fdw</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;hydradatabase&#x2F;hydra">https:&#x2F;&#x2F;github.com&#x2F;hydradatabase&#x2F;hydra</a><p>You can also connect other stores using the foreign data wrappers, like parquet files stored on an object store, duckdb, clickhouse… though the joins aren’t optimised as PostgreSQL would do full scan on the external table when joining.</div><br/></div></div><div id="39464037" class="c"><input type="checkbox" id="c-39464037" checked=""/><div class="controls bullet"><span class="by">Loic</span><span>|</span><a href="#39461147">parent</a><span>|</span><a href="#39463750">prev</a><span>|</span><a href="#39462810">next</a><span>|</span><label class="collapse" for="c-39464037">[-]</label><label class="expand" for="c-39464037">[1 more]</label></div><br/><div class="children"><div class="content">For time series, you have the TimescaleDB extension.<p><a href="https:&#x2F;&#x2F;www.timescale.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.timescale.com&#x2F;</a></div><br/></div></div><div id="39462810" class="c"><input type="checkbox" id="c-39462810" checked=""/><div class="controls bullet"><span class="by">CSSer</span><span>|</span><a href="#39461147">parent</a><span>|</span><a href="#39464037">prev</a><span>|</span><a href="#39464276">next</a><span>|</span><label class="collapse" for="c-39462810">[-]</label><label class="expand" for="c-39462810">[1 more]</label></div><br/><div class="children"><div class="content">Know anybody who could do it in a weekend? That’s about what you’re paying for.</div><br/></div></div><div id="39464276" class="c"><input type="checkbox" id="c-39464276" checked=""/><div class="controls bullet"><span class="by">ddorian43</span><span>|</span><a href="#39461147">parent</a><span>|</span><a href="#39462810">prev</a><span>|</span><a href="#39461339">next</a><span>|</span><label class="collapse" for="c-39464276">[-]</label><label class="expand" for="c-39464276">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m willing to give a bounty of $1000 to whoever adds that into main postgres tree.<p>This is the most disrespectful thing I&#x27;ve read on HN.</div><br/></div></div><div id="39461339" class="c"><input type="checkbox" id="c-39461339" checked=""/><div class="controls bullet"><span class="by">dkatz23238</span><span>|</span><a href="#39461147">parent</a><span>|</span><a href="#39464276">prev</a><span>|</span><a href="#39460116">next</a><span>|</span><label class="collapse" for="c-39461339">[-]</label><label class="expand" for="c-39461339">[1 more]</label></div><br/><div class="children"><div class="content">Parade DB are working on this problem: <a href="https:&#x2F;&#x2F;github.com&#x2F;paradedb&#x2F;paradedb&#x2F;tree&#x2F;dev&#x2F;pg_analytics">https:&#x2F;&#x2F;github.com&#x2F;paradedb&#x2F;paradedb&#x2F;tree&#x2F;dev&#x2F;pg_analytics</a></div><br/></div></div></div></div><div id="39460116" class="c"><input type="checkbox" id="c-39460116" checked=""/><div class="controls bullet"><span class="by">ralusek</span><span>|</span><a href="#39461147">prev</a><span>|</span><label class="collapse" for="c-39460116">[-]</label><label class="expand" for="c-39460116">[1 more]</label></div><br/><div class="children"><div class="content">If someone asked me how much it would cost to move a billion Postgres rows, I would say &quot;probably under $100.&quot;<p>I just had to move 500 million &quot;rows&quot; into S3, and it came in at about $100. I would expect S3 to be more expensive.</div><br/></div></div></div></div></div></div></div></body></html>