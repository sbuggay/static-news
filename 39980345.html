<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1712912459811" as="style"/><link rel="stylesheet" href="styles.css?v=1712912459811"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://clemenswinter.com/2024/04/07/the-simple-beauty-of-xor-floating-point-compression/">The simple beauty of XOR floating point compression</a> <span class="domain">(<a href="https://clemenswinter.com">clemenswinter.com</a>)</span></div><div class="subtext"><span>cwinter</span> | <span>59 comments</span></div><br/><div><div id="40008650" class="c"><input type="checkbox" id="c-40008650" checked=""/><div class="controls bullet"><span class="by">bcrl</span><span>|</span><a href="#40006699">next</a><span>|</span><label class="collapse" for="c-40008650">[-]</label><label class="expand" for="c-40008650">[2 more]</label></div><br/><div class="children"><div class="content">I did something similar in a messaging application...  We had a data structure consisting of several tables describing various message queues in 4GB of persistent storage.  The initialization of those tables was quite regular, as they were just pointers into blocks in the persistent memory.  When the application was ported to run on AWS instances, people decided to try to run the application on instances with the crappiest 20MB&#x2F;s storage.  I had specified during implementation that the journalling layer would not work well on crappy storage, as the actual persistent memory the system was originally designed for had gobs of bandwidth (it was supercap backed DRAM on a PCIe card built using an FPGA and a pair of 10Gbps SFP+ ports to mirror to another host -- you had a couple of GB&#x2F;s of write throughput).   Customers ignored the requirement, and opened a bug saying the system couldn&#x27;t reinitialize the table within the 30 second limit for commands on the CLI.  To fix the &quot;bug&quot;, I ended up delta encoding the persistent memory tables to make the data more regular, ran it through libz at the lowest compression level, then wrote the compressed data out to disk.  4GB of data ended up taking less than 100MB in the journal.  gzip compression on its own was not able to get under the 600MB requirement.  It was a total hack, but it worked and took a few hours to implement.  Domain specific knowledge really helps improve compression ratios!</div><br/><div id="40008943" class="c"><input type="checkbox" id="c-40008943" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#40008650">parent</a><span>|</span><a href="#40006699">next</a><span>|</span><label class="collapse" for="c-40008943">[-]</label><label class="expand" for="c-40008943">[1 more]</label></div><br/><div class="children"><div class="content">If you zig zag encode the deltas you might be able to get it down to 10mb.<p>δzd is the best. <a href="https:&#x2F;&#x2F;justine.lol&#x2F;sizetricks&#x2F;#dzd" rel="nofollow">https:&#x2F;&#x2F;justine.lol&#x2F;sizetricks&#x2F;#dzd</a></div><br/></div></div></div></div><div id="40006699" class="c"><input type="checkbox" id="c-40006699" checked=""/><div class="controls bullet"><span class="by">duskwuff</span><span>|</span><a href="#40008650">prev</a><span>|</span><a href="#40007963">next</a><span>|</span><label class="collapse" for="c-40006699">[-]</label><label class="expand" for="c-40006699">[1 more]</label></div><br/><div class="children"><div class="content">The biggest issue with this approach is that it&#x27;s bit-aligned, and the alignment is determined by fields within the data. It&#x27;s hard to make this run fast on most CPUs; a typical implementation ends up having to maintain a bit buffer, and shifts bits in&#x2F;out of it a couple times for every codeword.<p>I bet you could improve the performance of this algorithm significantly, without harming its compression too badly, by one or both of:<p>A) Adjusting the sizes of some bit fields to make them evenly divide the system word size, e.g. requiring the run length of the 11-prefix to be a multiple of 4, so that it can be encoded in 4 bits instead of 6. (This is a net space savings of half a bit; you have to include an average of 1.5 more bits of nonzero data, but the length is two bits shorter.)<p>B) Writing the streams of bits for e.g. &quot;mode&quot;, &quot;number of leading zeroes&quot;, and &quot;bit subsequences&quot; separately for each block of compressed data, rather than combining them into a single stream. As a bonus, each of those streams will now be more amenable to compression by generic compressors.</div><br/></div></div><div id="40007963" class="c"><input type="checkbox" id="c-40007963" checked=""/><div class="controls bullet"><span class="by">StillBored</span><span>|</span><a href="#40006699">prev</a><span>|</span><a href="#40006752">next</a><span>|</span><label class="collapse" for="c-40007963">[-]</label><label class="expand" for="c-40007963">[3 more]</label></div><br/><div class="children"><div class="content">Its largely a variation of delta encoding (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Delta_encoding" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Delta_encoding</a>) which is using xor to determine the variation between values. Both are exploiting the fact that in certain kinds of series&#x2F;samples the range (relative entropy) between values is far less than the range of the underlying value representation.<p>Whether or not the resulting ranges are encoded in a space saving manner (as this algorithm is doing) or used as input into a BW transform&#x2F;Huffman&#x2F;etc sorta depends on the resulting value sequences.<p>Much of this though should apply the understanding that data being compressed can frequently lead to data specific preprocessing that can significantly increase the compression ratios over generic encoders. Just taking the output of this routine and running it through deflate might yield another couple percent if it were tuned. Or put another way, all the bit prefix&#x2F;shifting decisions might be redundant for certain cases because the 0&#x27;s would end up being single bit tokens in a Huffman encoder.</div><br/><div id="40008302" class="c"><input type="checkbox" id="c-40008302" checked=""/><div class="controls bullet"><span class="by">selcuka</span><span>|</span><a href="#40007963">parent</a><span>|</span><a href="#40006752">next</a><span>|</span><label class="collapse" for="c-40008302">[-]</label><label class="expand" for="c-40008302">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Its largely a variation of delta encoding<p>Amiga famously used a version of hardware backed delta encoding [1] to display more colours on the screen than normally possible within its memory limits.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hold-And-Modify" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hold-And-Modify</a></div><br/><div id="40008687" class="c"><input type="checkbox" id="c-40008687" checked=""/><div class="controls bullet"><span class="by">bcrl</span><span>|</span><a href="#40007963">root</a><span>|</span><a href="#40008302">parent</a><span>|</span><a href="#40006752">next</a><span>|</span><label class="collapse" for="c-40008687">[-]</label><label class="expand" for="c-40008687">[1 more]</label></div><br/><div class="children"><div class="content">HAM would have worked even better if it had been in the intended hue-saturation-value (HSV) colour space as Jay Miner originally intended.  Sadly, HSV support was dropped, but due to schedule constraints, there wasn&#x27;t enough time to rework the design and remove HAM.  I found it fascinating to learn these things long after I was no longer using my Amiga for day to day work.</div><br/></div></div></div></div></div></div><div id="40006752" class="c"><input type="checkbox" id="c-40006752" checked=""/><div class="controls bullet"><span class="by">jhj</span><span>|</span><a href="#40007963">prev</a><span>|</span><a href="#40006920">next</a><span>|</span><label class="collapse" for="c-40006752">[-]</label><label class="expand" for="c-40006752">[7 more]</label></div><br/><div class="children"><div class="content">People in the HPC&#x2F;classical supercomputing space have done this sort of thing for a while. There&#x27;s a fair amount of literature on lossless floating point compression, such as Martin Burtscher&#x27;s work or stuff out of LLNL (fpzip):<p><a href="https:&#x2F;&#x2F;userweb.cs.txstate.edu&#x2F;~burtscher&#x2F;" rel="nofollow">https:&#x2F;&#x2F;userweb.cs.txstate.edu&#x2F;~burtscher&#x2F;</a>
<a href="https:&#x2F;&#x2F;computing.llnl.gov&#x2F;projects&#x2F;floating-point-compression" rel="nofollow">https:&#x2F;&#x2F;computing.llnl.gov&#x2F;projects&#x2F;floating-point-compressi...</a><p>but it tends to be very application specific, where there tends to be high correlation &#x2F; small deltas between neighboring values in a 2d&#x2F;3d&#x2F;4d&#x2F;etc floating point array (e.g., you are compressing neighboring temperature grid points in a PDE weather simulation model; temperature differences in neighboring cells won&#x27;t differ by that much).<p>In a lot of other cases (e.g., machine learning) the floating point significand bits (and sometimes the sign bit) tends to be incompressible noise. The exponent is the only thing that is really compressible, and the xor trick does not help you as much because neighboring values could still vary a bit in terms of exponents. An entropy encoder instead works well for that (encode closer to the actual underlying data distribution&#x2F;entropy), and you also don&#x27;t depend upon neighboring floats having similar exponents as well.<p>In 2022, I created dietgpu, a library to losslessly compress&#x2F;decompress floating point data at up to 400 GB&#x2F;s on an A100. It uses a general-purpose asymmetric numeral system encoder&#x2F;decoder on GPU (the first such implementation of general ANS on GPU, predating nvCOMP) for exponent compression.<p>We have used this to losslessly compress floating point data between GPUs (e.g., over Infiniband&#x2F;NVLink&#x2F;ethernet&#x2F;etc) in training massive ML models to speed up overall wall clock time of training across 100s&#x2F;1000s of GPUs without changing anything about how the training works (it&#x27;s lossless compression, it computes the same thing that it did before).<p><a href="https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;dietgpu">https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;dietgpu</a></div><br/><div id="40008155" class="c"><input type="checkbox" id="c-40008155" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40006752">parent</a><span>|</span><a href="#40009532">next</a><span>|</span><label class="collapse" for="c-40008155">[-]</label><label class="expand" for="c-40008155">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve talked to several world class computer scientists&#x2F;bioinformaticians who  realized, on their own, that decompression is the fastest way to fill the cache with data (because the amount of data streaming in is smaller, and CPUs are VERY fast for decompression).<p>One of the key innovations in the AMBER MD engine that made it work OK on cheaper systems was lossless floating point compression.  It still impresses me that you can compress floats, send them over MPI, and decompress them, all faster&#x2F;lower latency than the transport can send the uncompressed data.</div><br/><div id="40008589" class="c"><input type="checkbox" id="c-40008589" checked=""/><div class="controls bullet"><span class="by">jhj</span><span>|</span><a href="#40006752">root</a><span>|</span><a href="#40008155">parent</a><span>|</span><a href="#40009532">next</a><span>|</span><label class="collapse" for="c-40008589">[-]</label><label class="expand" for="c-40008589">[1 more]</label></div><br/><div class="children"><div class="content">Not just MPI over a network. We can compress floats, send them over NVLink or PCIe to another GPU in the same host, and decompress and it can be faster than sending data raw between GPUs, that&#x27;s the premise behind dietgpu even (it&#x27;s cheap compression, not a great compression ratio, like 0.6-0.9x of original size, but it&#x27;s extremely fast, 100s of GB&#x2F;s throughput, with the idea that you&#x27;re trying to race something that is similarly as fast. General floating point data could be quite incompressible or highly compressible, it really just depends upon what is being passed around).<p>The interconnects are improving at a slower rate in general than compute on the CPU&#x2F;GPU is and it can be exploited.</div><br/></div></div></div></div><div id="40009532" class="c"><input type="checkbox" id="c-40009532" checked=""/><div class="controls bullet"><span class="by">throwiforgtnlzy</span><span>|</span><a href="#40006752">parent</a><span>|</span><a href="#40008155">prev</a><span>|</span><a href="#40006897">next</a><span>|</span><label class="collapse" for="c-40009532">[-]</label><label class="expand" for="c-40009532">[1 more]</label></div><br/><div class="children"><div class="content"><i>Waves across I-35 to Reality Labs.</i><p>Sounds like something Numba should include perhaps.<p>I retired from HPC around the era of iPython, Open MPI, Infiniband, OFED, GPFS&#x2F;Panasas&#x2F;Lustre, Rocks Cluster, and Condor&#x2F;PBS&#x2F;SGE.</div><br/></div></div><div id="40006897" class="c"><input type="checkbox" id="c-40006897" checked=""/><div class="controls bullet"><span class="by">tines</span><span>|</span><a href="#40006752">parent</a><span>|</span><a href="#40009532">prev</a><span>|</span><a href="#40006887">next</a><span>|</span><label class="collapse" for="c-40006897">[-]</label><label class="expand" for="c-40006897">[1 more]</label></div><br/><div class="children"><div class="content">Wow, cool to see Dr. Burtscher&#x27;s name here! I was a student in his undergraduate parallel computing course and was invited to work in his lab. Very smart guy.</div><br/></div></div><div id="40006887" class="c"><input type="checkbox" id="c-40006887" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#40006752">parent</a><span>|</span><a href="#40006897">prev</a><span>|</span><a href="#40006920">next</a><span>|</span><label class="collapse" for="c-40006887">[-]</label><label class="expand" for="c-40006887">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not exceptionally good with compression, but I did a few experiments.<p>I&#x27;ve noticed that array-of-structs form is harder to compress than struct-of-arrays form. This is relevant because a floating-point number is really a struct containing 3 values: 1-bit sign, 8-bit exponent, and 23-bit mantissa.<p>--------<p>If you have 1000 floats, I would expect 1000-sign bits in order to be more compressible (whatever compression algorithm you use, it would better determine the pattern of signs).<p>Then 8000-bits of exponent would also be more compressible, because all the exponents would likely follow its own pattern.<p>Finally, the 23-bits of mantissa would probably be difficult to impossible for compression. But &quot;extracting it out&quot; before running a compression algorithm will give the sign + exponents more opportunity for comrpession.<p>--------<p>So yeah, just think about your data. And favor structure-of-arrays form for better compression.</div><br/><div id="40009479" class="c"><input type="checkbox" id="c-40009479" checked=""/><div class="controls bullet"><span class="by">dahart</span><span>|</span><a href="#40006752">root</a><span>|</span><a href="#40006887">parent</a><span>|</span><a href="#40006920">next</a><span>|</span><label class="collapse" for="c-40009479">[-]</label><label class="expand" for="c-40009479">[1 more]</label></div><br/><div class="children"><div class="content">Totally, SoAs are often easier to compress. I hadn’t thought about floats as being AoSs, and then unpacking the sign, exponent and mantissas of a batch of float into separate arrays, that’s an interesting way to look at it. You could probably take that 1 step further with floats or ints, and treat an array of numbers as an array of structures of 32 or 64 fields where every field is 1 bit. So to compress a batch of 1000 32-bit numbers, take all bit 31s in a row, then all bit 30s, etc.. Or another way to look at it is to stack the numbers in a column, transpose it, and read out the bits.<p>It’s worth noting that the algorithm in the article, and it’s cousin, delta encoding, both do already take advantage of any cases where the sign &amp; exponent bits don’t change with every sample, and&#x2F;or where the mantissa ULPs are constant for a while at a time. It’d be interesting to compare the explicit SoA of floats compression idea to XOR or delta compression, but I think it’s also interesting in a meta kinda of way that these two seemingly different ideas have similar outcomes.</div><br/></div></div></div></div></div></div><div id="40006920" class="c"><input type="checkbox" id="c-40006920" checked=""/><div class="controls bullet"><span class="by">EdSchouten</span><span>|</span><a href="#40006752">prev</a><span>|</span><a href="#40006248">next</a><span>|</span><label class="collapse" for="c-40006920">[-]</label><label class="expand" for="c-40006920">[6 more]</label></div><br/><div class="children"><div class="content">As the article states, you need to apply some heuristics to choose between cases (2) and (3). I&#x27;m not a big fan of that, because it means the encoding is not deterministic (as in, it leaves room for interpretation&#x2F;optimization by encoders).<p>Just for fun I implemented this algorithm: <a href="https:&#x2F;&#x2F;go.dev&#x2F;play&#x2F;p&#x2F;72GyYmVnwyc" rel="nofollow">https:&#x2F;&#x2F;go.dev&#x2F;play&#x2F;p&#x2F;72GyYmVnwyc</a><p>The algorithm described in the article uses prefixes 0, 10, and 11. My algorithm uses different prefixes: 0, 10, 110, 1110, 11110, [...]. By default, the subsequence that is used is identical to that containing differing bits during the previous iteration. The prefixes describe previously can be used to widen the subsequence:<p>- 0: Keep the subsequence the same width.<p>- 10: Make the subsequence (1&lt;&lt;1)-1 = 1 bit wider, both on the left and right side.<p>- 110: Make the subsequence (1&lt;&lt;2)-1 = 3 bits wider, both on the left and right side.<p>- 1110: Make the subsequence (1&lt;&lt;3)-1 = 7 bits wider, both on the left and right side.<p>- ...<p>For the sequence of numbers presented under &quot;Tweaking the algorithm&quot;, the algorithm in the article produces 969 bits of data. My algorithm produces just 823.<p>Edit: Tiny bug in original code.<p>Edit 2: maybe it’s smarter to widen the subsequence using triangular numbers. Doing it exponentially is a bit too aggressive, it seems.</div><br/><div id="40008559" class="c"><input type="checkbox" id="c-40008559" checked=""/><div class="controls bullet"><span class="by">teo_zero</span><span>|</span><a href="#40006920">parent</a><span>|</span><a href="#40008133">next</a><span>|</span><label class="collapse" for="c-40008559">[-]</label><label class="expand" for="c-40008559">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The prefixes describe previously can be used to widen the subsequence<p>So you can only widen the subsequence, never shrink it. Shrinking would address the issue raised by TFA:<p>&gt; If a series contains an outlier value that requires a very large window to encode, and all subsequent values have nonzero values only in a much smaller subwindow, the inefficient larger window will get locked in because we never hit the condition that would trigger a reset of the window size.</div><br/><div id="40009174" class="c"><input type="checkbox" id="c-40009174" checked=""/><div class="controls bullet"><span class="by">EdSchouten</span><span>|</span><a href="#40006920">root</a><span>|</span><a href="#40008559">parent</a><span>|</span><a href="#40008133">next</a><span>|</span><label class="collapse" for="c-40009174">[-]</label><label class="expand" for="c-40009174">[1 more]</label></div><br/><div class="children"><div class="content">Shrinking happens automatically, because every iteration takes the actually meaningful width of the previous iteration.</div><br/></div></div></div></div><div id="40008133" class="c"><input type="checkbox" id="c-40008133" checked=""/><div class="controls bullet"><span class="by">duskwuff</span><span>|</span><a href="#40006920">parent</a><span>|</span><a href="#40008559">prev</a><span>|</span><a href="#40007191">next</a><span>|</span><label class="collapse" for="c-40008133">[-]</label><label class="expand" for="c-40008133">[1 more]</label></div><br/><div class="children"><div class="content">What you&#x27;re working towards here is essentially Golomb coding. The optimal divisor will depend on how the data is distributed.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Golomb_coding" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Golomb_coding</a></div><br/></div></div><div id="40007191" class="c"><input type="checkbox" id="c-40007191" checked=""/><div class="controls bullet"><span class="by">addaon</span><span>|</span><a href="#40006920">parent</a><span>|</span><a href="#40008133">prev</a><span>|</span><a href="#40006248">next</a><span>|</span><label class="collapse" for="c-40007191">[-]</label><label class="expand" for="c-40007191">[2 more]</label></div><br/><div class="children"><div class="content">&gt; By default, the subsequence that is used is identical to that containing differing bits during the previous iteration<p>If I&#x27;m understanding correctly, this means that if the first choice a sized subsection is oversized, this will carry through to future samples (unless the encoder chooses to re-size it explicitly). I wonder if, with your cheaper &quot;loosen&quot; encoding, it is worth automatically tightening the bit delta. For example, if I go to an initial window of &quot;5 zeros, the 7 bits 1001101, remainder zeros&quot; (which I&#x27;ll call 5;7), and the next encoding is &quot;same, 0101110&quot;, the leading and trailing zeros adjust the window to (in this example) 6;5, &quot;tightening&quot; around the non-zero bits. With your low-cost &quot;loosen&quot; doing this aggressively (instead of, say, if it happens for multiple samples in a row) seems sane. It also means that the fact that your loosen encoding is symmetric (which makes sense in terms of bit efficiency) is somewhat mitigated because the implicit tighten can break the symmetry.</div><br/><div id="40007220" class="c"><input type="checkbox" id="c-40007220" checked=""/><div class="controls bullet"><span class="by">EdSchouten</span><span>|</span><a href="#40006920">root</a><span>|</span><a href="#40007191">parent</a><span>|</span><a href="#40006248">next</a><span>|</span><label class="collapse" for="c-40007220">[-]</label><label class="expand" for="c-40007220">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &gt; By default, the subsequence that is used is identical to that containing differing bits during the previous iteration<p>&gt; If I&#x27;m understanding correctly, this means that if the first choice a sized subsection is oversized, this will carry through to future samples (unless the encoder chooses to re-size it explicitly).<p>And that&#x27;s exactly what I do: I resize it. The subsequence that is picked during round n is the one that <i>actually</i> contained the differing bits in round n-1.</div><br/></div></div></div></div></div></div><div id="40006248" class="c"><input type="checkbox" id="c-40006248" checked=""/><div class="controls bullet"><span class="by">theamk</span><span>|</span><a href="#40006920">prev</a><span>|</span><a href="#40006562">next</a><span>|</span><label class="collapse" for="c-40006248">[-]</label><label class="expand" for="c-40006248">[2 more]</label></div><br/><div class="children"><div class="content">I wonder how it compares to traditional compression?<p>I&#x27;ve tried their &quot;Integer that increments by 1 at every timestep.&quot; case on 10000 integer. The algorithm described said &quot;6.98x compression&quot;<p>bzip2 gave 80000&#x2F;8642 = 9.2x compression<p>zstd gave 80000&#x2F;11316 = 7.1x compression</div><br/><div id="40006639" class="c"><input type="checkbox" id="c-40006639" checked=""/><div class="controls bullet"><span class="by">Chabsff</span><span>|</span><a href="#40006248">parent</a><span>|</span><a href="#40006562">next</a><span>|</span><label class="collapse" for="c-40006639">[-]</label><label class="expand" for="c-40006639">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure that&#x27;s a very fair comparison, as this algorithm does point-wise streaming compression&#x2F;decompression. I could see myself using something like this in a FPGA context or within a tight acquisition inner loop for example.</div><br/></div></div></div></div><div id="40006562" class="c"><input type="checkbox" id="c-40006562" checked=""/><div class="controls bullet"><span class="by">mzs</span><span>|</span><a href="#40006248">prev</a><span>|</span><a href="#40009622">next</a><span>|</span><label class="collapse" for="c-40006562">[-]</label><label class="expand" for="c-40006562">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s discussion of an article from 2020 that explains this and other time series compression approaches: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31379344">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31379344</a></div><br/></div></div><div id="40009622" class="c"><input type="checkbox" id="c-40009622" checked=""/><div class="controls bullet"><span class="by">chrchang523</span><span>|</span><a href="#40006562">prev</a><span>|</span><a href="#40008334">next</a><span>|</span><label class="collapse" for="c-40009622">[-]</label><label class="expand" for="c-40009622">[1 more]</label></div><br/><div class="children"><div class="content">One question: it is possible for the XOR of two consecutive floating-point numbers to have 32-63 leading zeros; the numbers 32-63 do not fit in 5 bits.  I imagine this is treated by Gorilla like 31 leading zeros?</div><br/></div></div><div id="40008334" class="c"><input type="checkbox" id="c-40008334" checked=""/><div class="controls bullet"><span class="by">GuB-42</span><span>|</span><a href="#40009622">prev</a><span>|</span><a href="#40006666">next</a><span>|</span><label class="collapse" for="c-40008334">[-]</label><label class="expand" for="c-40008334">[1 more]</label></div><br/><div class="children"><div class="content">What if the floating point number is, for example, a number of seconds coming from a millisecond time source? Binary floating point numbers notoriously can&#x27;t represent decimals precisely.<p>You would obviously be better off timing in the proper unit so that you have an integer number of milliseconds but that&#x27;s the kind of situation where performance starts to drop, no one understands why until some dev starts digging deeply into the encoding and maybe even write a blog post about how he got a massive speedup by multiplying numbers by 1000.</div><br/></div></div><div id="40006666" class="c"><input type="checkbox" id="c-40006666" checked=""/><div class="controls bullet"><span class="by">markrages</span><span>|</span><a href="#40008334">prev</a><span>|</span><a href="#40009448">next</a><span>|</span><label class="collapse" for="c-40006666">[-]</label><label class="expand" for="c-40006666">[2 more]</label></div><br/><div class="children"><div class="content">For integer values, just subtracting bytes is enough to allow much better compression ratios.  See the table at <a href="https:&#x2F;&#x2F;opensource.quarq.com&#x2F;zfit&#x2F;zfit.pdf" rel="nofollow">https:&#x2F;&#x2F;opensource.quarq.com&#x2F;zfit&#x2F;zfit.pdf</a></div><br/><div id="40007089" class="c"><input type="checkbox" id="c-40007089" checked=""/><div class="controls bullet"><span class="by">hcs</span><span>|</span><a href="#40006666">parent</a><span>|</span><a href="#40009448">next</a><span>|</span><label class="collapse" for="c-40007089">[-]</label><label class="expand" for="c-40007089">[1 more]</label></div><br/><div class="children"><div class="content">Very handy to do with 7-Zip: -mf=Delta:4 (or just f=Delta:4 in the GUI config window). This is for 4 byte little endian integers, which works pretty well with 16-bit stereo PCM even if there&#x27;s some unwanted overflow between the channels, if you can&#x27;t use flac in some context (raw data, etc).</div><br/></div></div></div></div><div id="40009448" class="c"><input type="checkbox" id="c-40009448" checked=""/><div class="controls bullet"><span class="by">throwiforgtnlzy</span><span>|</span><a href="#40006666">prev</a><span>|</span><a href="#40006489">next</a><span>|</span><label class="collapse" for="c-40009448">[-]</label><label class="expand" for="c-40009448">[3 more]</label></div><br/><div class="children"><div class="content">IIRC, there was a WAV-era audio format that used PCM delta compression by recording differences rather than absolute values.</div><br/><div id="40009485" class="c"><input type="checkbox" id="c-40009485" checked=""/><div class="controls bullet"><span class="by">adzm</span><span>|</span><a href="#40009448">parent</a><span>|</span><a href="#40006489">next</a><span>|</span><label class="collapse" for="c-40009485">[-]</label><label class="expand" for="c-40009485">[2 more]</label></div><br/><div class="children"><div class="content">Differential PCM did this basically versus the predicted value. Delta modulation is the other scheme.</div><br/><div id="40009537" class="c"><input type="checkbox" id="c-40009537" checked=""/><div class="controls bullet"><span class="by">throwiforgtnlzy</span><span>|</span><a href="#40009448">root</a><span>|</span><a href="#40009485">parent</a><span>|</span><a href="#40006489">next</a><span>|</span><label class="collapse" for="c-40009537">[-]</label><label class="expand" for="c-40009537">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s probably it. Thanks for easing my perpetual forgetfulness!</div><br/></div></div></div></div></div></div><div id="40006489" class="c"><input type="checkbox" id="c-40006489" checked=""/><div class="controls bullet"><span class="by">porphyra</span><span>|</span><a href="#40009448">prev</a><span>|</span><a href="#40007347">next</a><span>|</span><label class="collapse" for="c-40006489">[-]</label><label class="expand" for="c-40006489">[3 more]</label></div><br/><div class="children"><div class="content">In the first several examples, the input seems to have a mantissa with all zeros beyond the first 15 bits. Why isn&#x27;t the author using float32 for this instead of float64?</div><br/><div id="40006564" class="c"><input type="checkbox" id="c-40006564" checked=""/><div class="controls bullet"><span class="by">Chabsff</span><span>|</span><a href="#40006489">parent</a><span>|</span><a href="#40007347">next</a><span>|</span><label class="collapse" for="c-40006564">[-]</label><label class="expand" for="c-40006564">[2 more]</label></div><br/><div class="children"><div class="content">Not having to make that kind of decision in the first place is inherently valuable in of itself, which I believe is what&#x27;s being showcased here.</div><br/><div id="40008760" class="c"><input type="checkbox" id="c-40008760" checked=""/><div class="controls bullet"><span class="by">porphyra</span><span>|</span><a href="#40006489">root</a><span>|</span><a href="#40006564">parent</a><span>|</span><a href="#40007347">next</a><span>|</span><label class="collapse" for="c-40008760">[-]</label><label class="expand" for="c-40008760">[1 more]</label></div><br/><div class="children"><div class="content">I guess, but most compression algorithms like DEFLATE or zstd would compress those big sequences of zeros basically to nothing.</div><br/></div></div></div></div></div></div><div id="40007347" class="c"><input type="checkbox" id="c-40007347" checked=""/><div class="controls bullet"><span class="by">Manabu-eo</span><span>|</span><a href="#40006489">prev</a><span>|</span><a href="#40005814">next</a><span>|</span><label class="collapse" for="c-40007347">[-]</label><label class="expand" for="c-40007347">[1 more]</label></div><br/><div class="children"><div class="content">For those interested in the topic, I highly recommend the series of posts from Charles Bloom:<p><a href="http:&#x2F;&#x2F;cbloomrants.blogspot.com&#x2F;2023&#x2F;07&#x2F;notes-on-float-and-multi-byte-delta.html" rel="nofollow">http:&#x2F;&#x2F;cbloomrants.blogspot.com&#x2F;2023&#x2F;07&#x2F;notes-on-float-and-m...</a><p>&gt; TLDR:<p>&gt; For S16 deltas, bias by 0x8080 , for S32 deltas, bias by 0x80808080.<p>&gt; De-interleaving multi-byte integers into homogenous streams can help, particularly with weaker back-end compressors.<p>&gt; TLDR:<p>&gt; For floats, just reinterpret F32 as U32.<p>&gt; If the floats have a mix of positive and negative, convert the sign bit to two&#x27;s-complement signed S32.<p>&gt; Consider lossy elimination of negative zero -0.f<p>&gt; If you didn&#x27;t actually want the huge precision for floats near zero, a simple lossy encoding is just to do float += constant, which works for non-negative floats where you don&#x27;t know the high end of the range so you can&#x27;t just use fixed point.<p>&gt; TLDR:<p>&gt; The best way to compress numeric data that is larger than bytes (F32,F16,S32,S16) is usually to delta them in their original size integer, then de-interleave after the delta.<p>&gt; Sometimes no filter or no deinterleave is best, particularly with stronger compressors, so being able to select filter on&#x2F;off per-file can give big wins.</div><br/></div></div><div id="40005814" class="c"><input type="checkbox" id="c-40005814" checked=""/><div class="controls bullet"><span class="by">sameoldtune</span><span>|</span><a href="#40007347">prev</a><span>|</span><a href="#40006318">next</a><span>|</span><label class="collapse" for="c-40005814">[-]</label><label class="expand" for="c-40005814">[3 more]</label></div><br/><div class="children"><div class="content">This algorithm is pretty neat. I wonder if for the use case outlined by the post they could get even better compression through the use of fewer significant digits. For the purposes of recording times, 0. 00768 is likely just as useful as 0.0076792240142822266</div><br/><div id="40006020" class="c"><input type="checkbox" id="c-40006020" checked=""/><div class="controls bullet"><span class="by">Chabsff</span><span>|</span><a href="#40005814">parent</a><span>|</span><a href="#40006370">next</a><span>|</span><label class="collapse" for="c-40006020">[-]</label><label class="expand" for="c-40006020">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s already (one of) the main property that the algorithm is exploiting: That f64 is an horribly overkill representation for the data since it was generated using a microsecond clock, leading to a lot of trailing zeros.<p>So yeah, dropping more significant digits should lead to even better compression.<p>But if we are going to be massaging the data based on what we know&#x2F;need, there are better ways to go about compressing it. e.g. I&#x27;d expect that simple fixed-point quantization with a delta encoding would compress better than this. What I find cool about this is that it dynamically figures things out without having to deal with any priors.</div><br/></div></div><div id="40006370" class="c"><input type="checkbox" id="c-40006370" checked=""/><div class="controls bullet"><span class="by">cwinter</span><span>|</span><a href="#40005814">parent</a><span>|</span><a href="#40006020">prev</a><span>|</span><a href="#40006318">next</a><span>|</span><label class="collapse" for="c-40006370">[-]</label><label class="expand" for="c-40006370">[1 more]</label></div><br/><div class="children"><div class="content">yes! the very last example in the post shows what happens if you truncate the mantissa to just the 4 most significant bits</div><br/></div></div></div></div><div id="40006318" class="c"><input type="checkbox" id="c-40006318" checked=""/><div class="controls bullet"><span class="by">shin_lao</span><span>|</span><a href="#40005814">prev</a><span>|</span><a href="#40006303">next</a><span>|</span><label class="collapse" for="c-40006318">[-]</label><label class="expand" for="c-40006318">[4 more]</label></div><br/><div class="children"><div class="content">How does this compare to regular dictionary compression?</div><br/><div id="40006621" class="c"><input type="checkbox" id="c-40006621" checked=""/><div class="controls bullet"><span class="by">duskwuff</span><span>|</span><a href="#40006318">parent</a><span>|</span><a href="#40006303">next</a><span>|</span><label class="collapse" for="c-40006621">[-]</label><label class="expand" for="c-40006621">[3 more]</label></div><br/><div class="children"><div class="content">Dictionary compression isn&#x27;t really applicable to floating-point data, because it isn&#x27;t made up of commonly repeated &quot;words&quot; like text is.</div><br/><div id="40008940" class="c"><input type="checkbox" id="c-40008940" checked=""/><div class="controls bullet"><span class="by">GuB-42</span><span>|</span><a href="#40006318">root</a><span>|</span><a href="#40006621">parent</a><span>|</span><a href="#40006303">next</a><span>|</span><label class="collapse" for="c-40008940">[-]</label><label class="expand" for="c-40008940">[2 more]</label></div><br/><div class="children"><div class="content">Dictionary compression, combined with a form of entropy coding can work. As shown in the article the exponents often repeat themselves, and the low order bits of the mantissa are often zero.<p>Advanced context modeling compression algorithms can work even better than the scheme presented in the article. These are like mini-AIs, and they can recognize patterns more complex than just a string of unchanged bits, and with the appropriate entropy coder, give excellent compression. Or course the processing cost is huge compare to a few bit manipulations, and wouldn&#x27;t make much sense unless you need extreme compression.</div><br/><div id="40010441" class="c"><input type="checkbox" id="c-40010441" checked=""/><div class="controls bullet"><span class="by">gliptic</span><span>|</span><a href="#40006318">root</a><span>|</span><a href="#40008940">parent</a><span>|</span><a href="#40006303">next</a><span>|</span><label class="collapse" for="c-40010441">[-]</label><label class="expand" for="c-40010441">[1 more]</label></div><br/><div class="children"><div class="content">But context modelling isn&#x27;t dictionary compression. Parent is correct that dictionary compression itself (e.g. LZ4) isn&#x27;t very useful for floating point.</div><br/></div></div></div></div></div></div></div></div><div id="40006303" class="c"><input type="checkbox" id="c-40006303" checked=""/><div class="controls bullet"><span class="by">mgaunard</span><span>|</span><a href="#40006318">prev</a><span>|</span><a href="#40007951">next</a><span>|</span><label class="collapse" for="c-40006303">[-]</label><label class="expand" for="c-40006303">[17 more]</label></div><br/><div class="children"><div class="content">A normal network runs at 10Gbps or 1.25GiB&#x2F;s.<p>200MiB&#x2F;s is too slow by a factor of  7. Though obviously some people have optimised it.</div><br/><div id="40006552" class="c"><input type="checkbox" id="c-40006552" checked=""/><div class="controls bullet"><span class="by">geon</span><span>|</span><a href="#40006303">parent</a><span>|</span><a href="#40006539">next</a><span>|</span><label class="collapse" for="c-40006552">[-]</label><label class="expand" for="c-40006552">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d expect an actually optimized implementation to be at least that much faster.</div><br/></div></div><div id="40006539" class="c"><input type="checkbox" id="c-40006539" checked=""/><div class="controls bullet"><span class="by">wizzwizz4</span><span>|</span><a href="#40006303">parent</a><span>|</span><a href="#40006552">prev</a><span>|</span><a href="#40006376">next</a><span>|</span><label class="collapse" for="c-40006539">[-]</label><label class="expand" for="c-40006539">[2 more]</label></div><br/><div class="children"><div class="content">According to <a href="https:&#x2F;&#x2F;www.speedtest.net&#x2F;global-index" rel="nofollow">https:&#x2F;&#x2F;www.speedtest.net&#x2F;global-index</a>, 200MiB&#x2F;s is over five times faster than Singapore&#x27;s broadband speed – and that&#x27;s the country with the fastest broadband in the world.<p>200Mbps puts you in the top 15 countries by broadband speed (above Japan) and the top 4 by mobile speed (above South Korea). Not that that&#x27;s relevant to anything. (I haven&#x27;t edited out <i>any</i> mistakes. The child comment is lying.)</div><br/><div id="40006820" class="c"><input type="checkbox" id="c-40006820" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#40006303">root</a><span>|</span><a href="#40006539">parent</a><span>|</span><a href="#40006376">next</a><span>|</span><label class="collapse" for="c-40006820">[-]</label><label class="expand" for="c-40006820">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re conflating MiB&#x2F;s with Mbps. Though I believe the GP was actually talking about local server networks, not internet speeds.<p>If you need to stream sequential floating point data out an internet link something more space efficient (and probably chunked instead) would probably be worthwhile over the simplicity.</div><br/></div></div></div></div><div id="40006376" class="c"><input type="checkbox" id="c-40006376" checked=""/><div class="controls bullet"><span class="by">helf</span><span>|</span><a href="#40006303">parent</a><span>|</span><a href="#40006539">prev</a><span>|</span><a href="#40007951">next</a><span>|</span><label class="collapse" for="c-40006376">[-]</label><label class="expand" for="c-40006376">[13 more]</label></div><br/><div class="children"><div class="content">Uh no? Most not-corporate networks are gigabit to 2.5gbit at best. Higher end home users may be doing 5gbit and 10gbit+.<p>Even top end wifi tends to only be around 1.2gbit.</div><br/><div id="40006448" class="c"><input type="checkbox" id="c-40006448" checked=""/><div class="controls bullet"><span class="by">vardump</span><span>|</span><a href="#40006303">root</a><span>|</span><a href="#40006376">parent</a><span>|</span><a href="#40007951">next</a><span>|</span><label class="collapse" for="c-40006448">[-]</label><label class="expand" for="c-40006448">[12 more]</label></div><br/><div class="children"><div class="content">Higher end home users are doing 25, 50 and 100 Gbps. Hardware is actually rather affordable now.<p>Although there&#x27;s just not enough PCIe bandwidth in current consumer hardware for 100 Gbps...</div><br/><div id="40006954" class="c"><input type="checkbox" id="c-40006954" checked=""/><div class="controls bullet"><span class="by">jdsully</span><span>|</span><a href="#40006303">root</a><span>|</span><a href="#40006448">parent</a><span>|</span><a href="#40006541">next</a><span>|</span><label class="collapse" for="c-40006954">[-]</label><label class="expand" for="c-40006954">[1 more]</label></div><br/><div class="children"><div class="content">How many homes have fibre runs everywhere.  Most homes don&#x27;t even have cat-5</div><br/></div></div><div id="40006541" class="c"><input type="checkbox" id="c-40006541" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#40006303">root</a><span>|</span><a href="#40006448">parent</a><span>|</span><a href="#40006954">prev</a><span>|</span><a href="#40006576">next</a><span>|</span><label class="collapse" for="c-40006541">[-]</label><label class="expand" for="c-40006541">[8 more]</label></div><br/><div class="children"><div class="content">Your high end is like the top 0.0001% of home users.</div><br/><div id="40007003" class="c"><input type="checkbox" id="c-40007003" checked=""/><div class="controls bullet"><span class="by">vardump</span><span>|</span><a href="#40006303">root</a><span>|</span><a href="#40006541">parent</a><span>|</span><a href="#40006576">next</a><span>|</span><label class="collapse" for="c-40007003">[-]</label><label class="expand" for="c-40007003">[7 more]</label></div><br/><div class="children"><div class="content">You can get to this under $1k nowadays. This Mikrotik switch changed everything: <a href="https:&#x2F;&#x2F;mikrotik.com&#x2F;product&#x2F;crs504_4xq_in" rel="nofollow">https:&#x2F;&#x2F;mikrotik.com&#x2F;product&#x2F;crs504_4xq_in</a><p>You can buy second hand 100g networking cards pretty cheaply. The caveat is that they only support older PCIe standards, so you need PCIe 16x to use them at full speed.<p>Please also don&#x27;t forget typical NVMe drives are pretty fast now. Just <i>two</i> of them can nearly saturate a 100g link.<p>Personally I know a <i>lot</i> of people in IT field who have single mode fiber SFP28 networks. I&#x27;d say 0.001% - 0.01% is closer to reality.<p>Remember we were talking about high end.</div><br/><div id="40007104" class="c"><input type="checkbox" id="c-40007104" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#40006303">root</a><span>|</span><a href="#40007003">parent</a><span>|</span><a href="#40008064">next</a><span>|</span><label class="collapse" for="c-40007104">[-]</label><label class="expand" for="c-40007104">[5 more]</label></div><br/><div class="children"><div class="content">Like 99% of the people I know who work in IT don’t have anything near what you’re mentioning. They spend $100 on <i>all</i> their networking equipment, not just a switch. They’re not laying fiber in their house. And outside of that group the number is basically 0.</div><br/><div id="40007165" class="c"><input type="checkbox" id="c-40007165" checked=""/><div class="controls bullet"><span class="by">vardump</span><span>|</span><a href="#40006303">root</a><span>|</span><a href="#40007104">parent</a><span>|</span><a href="#40008064">next</a><span>|</span><label class="collapse" for="c-40007165">[-]</label><label class="expand" for="c-40007165">[4 more]</label></div><br/><div class="children"><div class="content">I guess typical gateway drug is to get a 10g internet connection or 10g NAS at home and find out copper based cat6&#x2F;7 is a lousy solution to actually make use of it.<p>Surprisingly high latencies (compared to fiber, that is) and noisy fans due to a lot of heat generated.<p>Shrug. I guess your mileage may vary.</div><br/><div id="40008709" class="c"><input type="checkbox" id="c-40008709" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#40006303">root</a><span>|</span><a href="#40007165">parent</a><span>|</span><a href="#40008692">next</a><span>|</span><label class="collapse" for="c-40008709">[-]</label><label class="expand" for="c-40008709">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s hard for me to picture doing something with an internet&#x2F;NAS connection and caring about the extra percents of a millisecond copper gives you.  And all the 10g cards amazon is showing me are fanless.   Even if you need a fan, it should be enough at whisper-quiet speeds.</div><br/></div></div><div id="40008692" class="c"><input type="checkbox" id="c-40008692" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#40006303">root</a><span>|</span><a href="#40007165">parent</a><span>|</span><a href="#40008709">prev</a><span>|</span><a href="#40008064">next</a><span>|</span><label class="collapse" for="c-40008692">[-]</label><label class="expand" for="c-40008692">[2 more]</label></div><br/><div class="children"><div class="content">I say this because I am almost at this point (I run copper through my house) but was scared off on going full fiber. I know some who have, but it’s definitely really, really rare.</div><br/><div id="40010273" class="c"><input type="checkbox" id="c-40010273" checked=""/><div class="controls bullet"><span class="by">vardump</span><span>|</span><a href="#40006303">root</a><span>|</span><a href="#40008692">parent</a><span>|</span><a href="#40008064">next</a><span>|</span><label class="collapse" for="c-40010273">[-]</label><label class="expand" for="c-40010273">[1 more]</label></div><br/><div class="children"><div class="content">I had (well, still have) cat 7 in the house, while venturing into fiber as well. 10GBASE-T switches are noisy and surprisingly power hungry.<p>One big reason for fiber is a home lab. Transferring large VM images etc. is so much faster. Ability to run iSCSI <i>fast</i> is also amazing.<p>Single mode fiber is pretty nice. It&#x27;s fast, has low power requirements and no need to worry about electrical issues. Prices have declined significantly. The only minus is that you can&#x27;t have PoE, so the best option is to wire both.<p>(Ok, there&#x27;s actually Power over Fiber, but that&#x27;s just crazy stuff <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Power-over-fiber" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Power-over-fiber</a>. A huge fire risk.)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40008221" class="c"><input type="checkbox" id="c-40008221" checked=""/><div class="controls bullet"><span class="by">helf</span><span>|</span><a href="#40006303">root</a><span>|</span><a href="#40006448">parent</a><span>|</span><a href="#40006576">prev</a><span>|</span><a href="#40007951">next</a><span>|</span><label class="collapse" for="c-40008221">[-]</label><label class="expand" for="c-40008221">[1 more]</label></div><br/><div class="children"><div class="content">Yeah but that is the extreme minority. Claiming it isnt is absurd.</div><br/></div></div></div></div></div></div></div></div><div id="40007951" class="c"><input type="checkbox" id="c-40007951" checked=""/><div class="controls bullet"><span class="by">gafferongames</span><span>|</span><a href="#40006303">prev</a><span>|</span><label class="collapse" for="c-40007951">[-]</label><label class="expand" for="c-40007951">[1 more]</label></div><br/><div class="children"><div class="content">Excellent stuff</div><br/></div></div></div></div></div></div></div></body></html>