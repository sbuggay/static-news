<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1691139660200" as="style"/><link rel="stylesheet" href="styles.css?v=1691139660200"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/ErwannMillon/Color-diffusion">Color-Diffusion: using diffusion models to colorize black and white images</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>dvrp</span> | <span>123 comments</span></div><br/><div><div id="36992831" class="c"><input type="checkbox" id="c-36992831" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36992344">next</a><span>|</span><label class="collapse" for="c-36992831">[-]</label><label class="expand" for="c-36992831">[7 more]</label></div><br/><div class="children"><div class="content">Btw, I did this in pixel space for simplicity, cool animations, and compute costs. Would be really interesting to do this as an LDM (though of course you can&#x27;t really do the LAB color space thing, unless you maybe train an AE specifically for that color space. )<p>I was really interested in how color was represented in latent space and ran some experiments with VQGAN clip. You can actually do a (not great) colorization of an image by encoding it w&#x2F; VQGAN, and using a prompt like &quot;a colorful image of a woman&quot;.<p>Would be fun to experiment with if anyone wants to try, would love to see any results if someone wants to build</div><br/><div id="36993183" class="c"><input type="checkbox" id="c-36993183" checked=""/><div class="controls bullet"><span class="by">carbocation</span><span>|</span><a href="#36992831">parent</a><span>|</span><a href="#36993066">next</a><span>|</span><label class="collapse" for="c-36993183">[-]</label><label class="expand" for="c-36993183">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>I did this in pixel space for simplicity, cool animations, and compute costs</i><p>A slight nitpick, wouldn&#x27;t doing diffusion in the latent space be cheaper?</div><br/><div id="36994246" class="c"><input type="checkbox" id="c-36994246" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36992831">root</a><span>|</span><a href="#36993183">parent</a><span>|</span><a href="#36993335">next</a><span>|</span><label class="collapse" for="c-36994246">[-]</label><label class="expand" for="c-36994246">[1 more]</label></div><br/><div class="children"><div class="content">Depends, given the low res, the 3x64x64 pixel space image is smaller than the latents you would get from encoding a higher-res image with models like VQGAN or the stablediff VAE at their native resolutions.<p>It&#x27;s easier to get a sense of what&#x27;s going wrong with a pixel space model though. With latent space, there&#x27;s always the question of how color is represented in latent space &#x2F; how entangled it is with other structure &#x2F; semantics.<p>Starting in pixel space removed a lot of variables from the equation, but latent diffusion is the obvious next step</div><br/></div></div><div id="36993335" class="c"><input type="checkbox" id="c-36993335" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#36992831">root</a><span>|</span><a href="#36993183">parent</a><span>|</span><a href="#36994246">prev</a><span>|</span><a href="#36993066">next</a><span>|</span><label class="collapse" for="c-36993335">[-]</label><label class="expand" for="c-36993335">[1 more]</label></div><br/><div class="children"><div class="content">Not necessarily if you don’t already have a pretrained autoencoder.</div><br/></div></div></div></div><div id="36993066" class="c"><input type="checkbox" id="c-36993066" checked=""/><div class="controls bullet"><span class="by">xigency</span><span>|</span><a href="#36992831">parent</a><span>|</span><a href="#36993183">prev</a><span>|</span><a href="#36992344">next</a><span>|</span><label class="collapse" for="c-36993066">[-]</label><label class="expand" for="c-36993066">[3 more]</label></div><br/><div class="children"><div class="content">Question, how long did it take to train this model and what hardware did you use?</div><br/><div id="36994199" class="c"><input type="checkbox" id="c-36994199" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36992831">root</a><span>|</span><a href="#36993066">parent</a><span>|</span><a href="#36992344">next</a><span>|</span><label class="collapse" for="c-36994199">[-]</label><label class="expand" for="c-36994199">[2 more]</label></div><br/><div class="children"><div class="content">Took a lot of failed experiments, the model would keep converging to greyscale &#x2F; sepia images. Think one of the ways I fixed was by adding an greyscale encoder to the arch. Used its output embedding as additional conditioning. Can&#x27;t remember if I only added it to the Unet input or injected it during various stages of the unet down pass.</div><br/><div id="36994210" class="c"><input type="checkbox" id="c-36994210" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36992831">root</a><span>|</span><a href="#36994199">parent</a><span>|</span><a href="#36992344">next</a><span>|</span><label class="collapse" for="c-36994210">[-]</label><label class="expand" for="c-36994210">[1 more]</label></div><br/><div class="children"><div class="content">Think the final training run was only a couple hours on a Colab V100</div><br/></div></div></div></div></div></div></div></div><div id="36992344" class="c"><input type="checkbox" id="c-36992344" checked=""/><div class="controls bullet"><span class="by">data-ottawa</span><span>|</span><a href="#36992831">prev</a><span>|</span><a href="#36993045">next</a><span>|</span><label class="collapse" for="c-36992344">[-]</label><label class="expand" for="c-36992344">[5 more]</label></div><br/><div class="children"><div class="content">Off topic: this has an absolutely 90’s sci-fi movie effect watching the gifs, it’s funny how the tech just wound up looking like that.</div><br/><div id="36992464" class="c"><input type="checkbox" id="c-36992464" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36992344">parent</a><span>|</span><a href="#36993045">next</a><span>|</span><label class="collapse" for="c-36992464">[-]</label><label class="expand" for="c-36992464">[4 more]</label></div><br/><div class="children"><div class="content">hahaha it reminded me of some &quot;zoom and enhance&quot; stuff when I was making the animations</div><br/><div id="36993256" class="c"><input type="checkbox" id="c-36993256" checked=""/><div class="controls bullet"><span class="by">barrkel</span><span>|</span><a href="#36992344">root</a><span>|</span><a href="#36992464">parent</a><span>|</span><a href="#36992962">next</a><span>|</span><label class="collapse" for="c-36993256">[-]</label><label class="expand" for="c-36993256">[2 more]</label></div><br/><div class="children"><div class="content">It reminded me of the days of antenna pass-through VCR players, where you had to tune into your VCR&#x27;s broadcast signal when you couldn&#x27;t use SCART.</div><br/><div id="36996951" class="c"><input type="checkbox" id="c-36996951" checked=""/><div class="controls bullet"><span class="by">Cthulhu_</span><span>|</span><a href="#36992344">root</a><span>|</span><a href="#36993256">parent</a><span>|</span><a href="#36992962">next</a><span>|</span><label class="collapse" for="c-36996951">[-]</label><label class="expand" for="c-36996951">[1 more]</label></div><br/><div class="children"><div class="content">I never knew that was a thing, today I learned. I was spoiled with our first VCR having SCART already :p. And an IR remote! We could put the antenna cable into the VCR then use the remote to change channels (all three) instead of having to walk up to the TV. (this was late 80&#x27;s, maybe early 90&#x27;s; I wonder if we were late with things like that)</div><br/></div></div></div></div><div id="36992962" class="c"><input type="checkbox" id="c-36992962" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#36992344">root</a><span>|</span><a href="#36992464">parent</a><span>|</span><a href="#36993256">prev</a><span>|</span><a href="#36993045">next</a><span>|</span><label class="collapse" for="c-36992962">[-]</label><label class="expand" for="c-36992962">[1 more]</label></div><br/><div class="children"><div class="content">Looks like something you&#x27;d see in an X Files episode.</div><br/></div></div></div></div></div></div><div id="36993045" class="c"><input type="checkbox" id="c-36993045" checked=""/><div class="controls bullet"><span class="by">asciimov</span><span>|</span><a href="#36992344">prev</a><span>|</span><a href="#36992680">next</a><span>|</span><label class="collapse" for="c-36993045">[-]</label><label class="expand" for="c-36993045">[14 more]</label></div><br/><div class="children"><div class="content">I’m not a fan of b&amp;w colorization. Often the colors are wrong, either outright color errors (like choices for clothing or cars) or often not taking in to account lighting conditions (late in day shadows but midday brightness).<p>Then there is the issue of B&amp;W movies. Using this kind of tech might not give pleasing results as the colors used for sets and outfits were chosen to work well for film contrast and not for story accuracy. That “blue” dress might really be green. (Please, just leave B&amp;W movies the way they are.)</div><br/><div id="36993451" class="c"><input type="checkbox" id="c-36993451" checked=""/><div class="controls bullet"><span class="by">imoverclocked</span><span>|</span><a href="#36993045">parent</a><span>|</span><a href="#36995997">next</a><span>|</span><label class="collapse" for="c-36993451">[-]</label><label class="expand" for="c-36993451">[2 more]</label></div><br/><div class="children"><div class="content">I think keeping the art as it was produced is important but there is also a good history of modifying art to produce new art too. In the digital age, we aren’t losing the original art so it seems even stranger to be against modification of the “original.”<p>However, just applying a simple filter (or single transform without effort) definitely feels derivative to me.</div><br/><div id="36996842" class="c"><input type="checkbox" id="c-36996842" checked=""/><div class="controls bullet"><span class="by">SiempreViernes</span><span>|</span><a href="#36993045">root</a><span>|</span><a href="#36993451">parent</a><span>|</span><a href="#36995997">next</a><span>|</span><label class="collapse" for="c-36996842">[-]</label><label class="expand" for="c-36996842">[1 more]</label></div><br/><div class="children"><div class="content">Additionally, colorisations very commonly present themselves as showing a &quot;more true&quot; version of how things looked and not as creative art projects.</div><br/></div></div></div></div><div id="36995997" class="c"><input type="checkbox" id="c-36995997" checked=""/><div class="controls bullet"><span class="by">solumunus</span><span>|</span><a href="#36993045">parent</a><span>|</span><a href="#36993451">prev</a><span>|</span><a href="#36994021">next</a><span>|</span><label class="collapse" for="c-36995997">[-]</label><label class="expand" for="c-36995997">[2 more]</label></div><br/><div class="children"><div class="content">I don’t see why it matters if the blue dress was really green. The result is either an enhanced experience or not, if it is then minor inaccuracies don’t seem relevant.</div><br/><div id="36996920" class="c"><input type="checkbox" id="c-36996920" checked=""/><div class="controls bullet"><span class="by">Cthulhu_</span><span>|</span><a href="#36993045">root</a><span>|</span><a href="#36995997">parent</a><span>|</span><a href="#36994021">next</a><span>|</span><label class="collapse" for="c-36996920">[-]</label><label class="expand" for="c-36996920">[1 more]</label></div><br/><div class="children"><div class="content">If there&#x27;s a source that a blue dress was green, then that could be taken into consideration for recoloring, but as you said, it&#x27;s to enhance the experience, not to be 100% accurate.</div><br/></div></div></div></div><div id="36994021" class="c"><input type="checkbox" id="c-36994021" checked=""/><div class="controls bullet"><span class="by">qiqitori</span><span>|</span><a href="#36993045">parent</a><span>|</span><a href="#36995997">prev</a><span>|</span><a href="#36993860">next</a><span>|</span><label class="collapse" for="c-36994021">[-]</label><label class="expand" for="c-36994021">[2 more]</label></div><br/><div class="children"><div class="content">Maybe you&#x27;re used to looking at B&amp;W stuff and effortlessly figuring out what the scene is depicting, but for me at least it&#x27;s very hard. Adding a little color makes it much easier. In that regard, it doesn&#x27;t matter to me if the colors are wrong.<p>(Perhaps it just takes some getting used to. Back when I read a black and white comic for the first time (as a child), I had a hard time figuring out things at first but got used to it at some point.)</div><br/><div id="36994340" class="c"><input type="checkbox" id="c-36994340" checked=""/><div class="controls bullet"><span class="by">brianpan</span><span>|</span><a href="#36993045">root</a><span>|</span><a href="#36994021">parent</a><span>|</span><a href="#36993860">next</a><span>|</span><label class="collapse" for="c-36994340">[-]</label><label class="expand" for="c-36994340">[1 more]</label></div><br/><div class="children"><div class="content">I think the point being made is that movies were made for the B&amp;W end result, not just shooting color with B&amp;W film.<p>For instance, fake blood in B&amp;W was often produced with black liquid. Colorizing it correctly just doesn&#x27;t make sense. Or a green or blue dress can be chosen because of the way it looks on film, not because it&#x27;s supposed to BE a green or blue dress.</div><br/></div></div></div></div><div id="36993860" class="c"><input type="checkbox" id="c-36993860" checked=""/><div class="controls bullet"><span class="by">jojobas</span><span>|</span><a href="#36993045">parent</a><span>|</span><a href="#36994021">prev</a><span>|</span><a href="#36993211">next</a><span>|</span><label class="collapse" for="c-36993860">[-]</label><label class="expand" for="c-36993860">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not as if B&amp;W movies or pictures are taken away, it&#x27;s just a fun exercise for NN to play with.</div><br/><div id="36995248" class="c"><input type="checkbox" id="c-36995248" checked=""/><div class="controls bullet"><span class="by">hedora</span><span>|</span><a href="#36993045">root</a><span>|</span><a href="#36993860">parent</a><span>|</span><a href="#36993211">next</a><span>|</span><label class="collapse" for="c-36995248">[-]</label><label class="expand" for="c-36995248">[4 more]</label></div><br/><div class="children"><div class="content">Due to the magic of the DMCA’s anti-circumvention clauses, the B&amp;W movie can be taken away.<p>The last time I checked, “the source is public domain” is not a valid defense against the pro-DRM parts of that law.</div><br/><div id="36996926" class="c"><input type="checkbox" id="c-36996926" checked=""/><div class="controls bullet"><span class="by">Cthulhu_</span><span>|</span><a href="#36993045">root</a><span>|</span><a href="#36995248">parent</a><span>|</span><a href="#36995418">next</a><span>|</span><label class="collapse" for="c-36996926">[-]</label><label class="expand" for="c-36996926">[1 more]</label></div><br/><div class="children"><div class="content">Would a B&amp;W film republished in color cause the original film to have its copyright extended?</div><br/></div></div><div id="36995418" class="c"><input type="checkbox" id="c-36995418" checked=""/><div class="controls bullet"><span class="by">jojobas</span><span>|</span><a href="#36993045">root</a><span>|</span><a href="#36995248">parent</a><span>|</span><a href="#36996926">prev</a><span>|</span><a href="#36993211">next</a><span>|</span><label class="collapse" for="c-36995418">[-]</label><label class="expand" for="c-36995418">[2 more]</label></div><br/><div class="children"><div class="content">So, source is public domain somebody &quot;owns copyright&quot; on colorizing it, and you can no longer access the B&amp;W source due to DMCA?<p>That&#x27;s pretty perverse, got any links for a primer?</div><br/><div id="36996024" class="c"><input type="checkbox" id="c-36996024" checked=""/><div class="controls bullet"><span class="by">grncdr</span><span>|</span><a href="#36993045">root</a><span>|</span><a href="#36995418">parent</a><span>|</span><a href="#36993211">next</a><span>|</span><label class="collapse" for="c-36996024">[-]</label><label class="expand" for="c-36996024">[1 more]</label></div><br/><div class="children"><div class="content">Not specific to colorization, but didn’t something like this happen with the Star Wars trilogy? Lucas made a re-release with a few edits (that were not universally liked) and it’s now impossible to purchase a new copy of the original version (or something like that, I only remember hearing about it).</div><br/></div></div></div></div></div></div></div></div><div id="36993211" class="c"><input type="checkbox" id="c-36993211" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#36993045">parent</a><span>|</span><a href="#36993860">prev</a><span>|</span><a href="#36993464">next</a><span>|</span><label class="collapse" for="c-36993211">[-]</label><label class="expand" for="c-36993211">[1 more]</label></div><br/><div class="children"><div class="content">I think colorization with some effort put in can be pretty decent. E.g. I prefer the 2007 colorization of It&#x27;s a Wonderful Life to the original. It&#x27;s never perfect but I don&#x27;t think that&#x27;s a prerequisite to being better. Some will always disagree though.<p>About every completely automated colorized video tends to be pretty bad though. Particularly the YouTube &quot;8k colorized interpolated&quot; kind of low effort channels where they just let them pump out without caring if it&#x27;s actually any good.</div><br/></div></div><div id="36993464" class="c"><input type="checkbox" id="c-36993464" checked=""/><div class="controls bullet"><span class="by">blululu</span><span>|</span><a href="#36993045">parent</a><span>|</span><a href="#36993211">prev</a><span>|</span><a href="#36992680">next</a><span>|</span><label class="collapse" for="c-36993464">[-]</label><label class="expand" for="c-36993464">[1 more]</label></div><br/><div class="children"><div class="content">Yeah it&#x27;s cool tech but I really don&#x27;t appreciate how it is just straight up deceitful and spreading misinformation. A lot of hues are underdetermined and the result is more or less arbitrary in a historical context. If one were to research and fine-tune the model such that ambiguous shades are historically accurate I would be less annoyed by the sense that these images are just spreading misinformation. Compare this with Sergey Prokudin-Gorsky&#x27;s photos of the Russian Empire or autochromes of Paris in 1910 which are actual windows into a lost world.<p>*for works of fiction these issues vanish, but for any historical or documentary photographs&#x2F;films, I really hate that I am being lied to.</div><br/></div></div></div></div><div id="36992680" class="c"><input type="checkbox" id="c-36992680" checked=""/><div class="controls bullet"><span class="by">ChrisArchitect</span><span>|</span><a href="#36993045">prev</a><span>|</span><a href="#36991762">next</a><span>|</span><label class="collapse" for="c-36992680">[-]</label><label class="expand" for="c-36992680">[1 more]</label></div><br/><div class="children"><div class="content">Author&#x27;s writeup on this from May: <a href="https:&#x2F;&#x2F;medium.com&#x2F;@erwannmillon&#x2F;color-diffusion-colorizing-black-and-white-images-with-diffusion-models-269828f71c81" rel="nofollow noreferrer">https:&#x2F;&#x2F;medium.com&#x2F;@erwannmillon&#x2F;color-diffusion-colorizing-...</a></div><br/></div></div><div id="36991762" class="c"><input type="checkbox" id="c-36991762" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#36992680">prev</a><span>|</span><a href="#36991341">next</a><span>|</span><label class="collapse" for="c-36991762">[-]</label><label class="expand" for="c-36991762">[2 more]</label></div><br/><div class="children"><div class="content">Does it work on arbitrary image sizes?<p>One of the nice features of the somewhat old Deoldify colorizer is support for any resolution. It actually does better than photoshops colorization: <a href="https:&#x2F;&#x2F;blog.maxg.io&#x2F;colorizing-infrared-images-with-photoshop&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;blog.maxg.io&#x2F;colorizing-infrared-images-with-photosh...</a><p>Edit - technically, I suppose, the way Deoldify works is by rendering the color at a low resolution and then applying the filter to a higher resolution using OpenCV. I think the same sub-sampling approach could work here...</div><br/><div id="36992722" class="c"><input type="checkbox" id="c-36992722" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36991762">parent</a><span>|</span><a href="#36991341">next</a><span>|</span><label class="collapse" for="c-36992722">[-]</label><label class="expand" for="c-36992722">[1 more]</label></div><br/><div class="children"><div class="content">Technically yes, the encoder and unet are convolutional and support arbitrary input sizes, but the model was trained at 64x64px bc of compute limitations. You could probably resume the training from a 64x64 resolution checkpoint and train at a higher resolution.<p>But like most diffusion models, they don&#x27;t generalize very well to resolutions outside of their training dataset</div><br/></div></div></div></div><div id="36991341" class="c"><input type="checkbox" id="c-36991341" checked=""/><div class="controls bullet"><span class="by">snvzz</span><span>|</span><a href="#36991762">prev</a><span>|</span><a href="#36996062">next</a><span>|</span><label class="collapse" for="c-36991341">[-]</label><label class="expand" for="c-36991341">[33 more]</label></div><br/><div class="children"><div class="content">All the examples are portraits of people.<p>I have to wonder whether it works well with anything else.</div><br/><div id="36991406" class="c"><input type="checkbox" id="c-36991406" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36991341">parent</a><span>|</span><a href="#36996062">next</a><span>|</span><label class="collapse" for="c-36991406">[-]</label><label class="expand" for="c-36991406">[32 more]</label></div><br/><div class="children"><div class="content">trained on celebA, so no, but you could for sure train this on a more varied dataset</div><br/><div id="36991509" class="c"><input type="checkbox" id="c-36991509" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991406">parent</a><span>|</span><a href="#36996062">next</a><span>|</span><label class="collapse" for="c-36991509">[-]</label><label class="expand" for="c-36991509">[31 more]</label></div><br/><div class="children"><div class="content">Would it be as simple as feeding it a bunch of decolorized images along with the originals?</div><br/><div id="36991666" class="c"><input type="checkbox" id="c-36991666" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991509">parent</a><span>|</span><a href="#36991544">next</a><span>|</span><label class="collapse" for="c-36991666">[-]</label><label class="expand" for="c-36991666">[22 more]</label></div><br/><div class="children"><div class="content">basically the training works as follows:
Take a color image in RGB. Convert it to LAB. This is an alternative color space where the first channel is a greyscale image, and two channels that represent the color information.<p>In a traditional pixel-space (non latent) diffusion model, you noise all the RGB channels and train a Unet to predict the noise at a given timestep.<p>When colorizing an image, the Unet always &quot;knows&quot; the black and white image (i.e the L channel).<p>This implementation only adds noise to the color channels, while keeping the L channel constant.<p>So to train the model, you need a dataset of colored images. They would be converted to LAB, and the color channels would be noised.<p>You can&#x27;t train on decolorized images, because the neural network needs to learn how to predict color with a black and white image as context. Without color info, the model can&#x27;t learn.</div><br/><div id="36991895" class="c"><input type="checkbox" id="c-36991895" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991666">parent</a><span>|</span><a href="#36991793">next</a><span>|</span><label class="collapse" for="c-36991895">[-]</label><label class="expand" for="c-36991895">[14 more]</label></div><br/><div class="children"><div class="content">But since you do not have access to colour originals of historical photos in almost every instance, you cannot possibly train the network to have any instinct for the colour sensitivity of the medium, can you?<p>An extreme example:<p><a href="https:&#x2F;&#x2F;www.cabinetmagazine.org&#x2F;issues&#x2F;51&#x2F;archibald.php" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.cabinetmagazine.org&#x2F;issues&#x2F;51&#x2F;archibald.php</a><p><a href="https:&#x2F;&#x2F;www.messynessychic.com&#x2F;2016&#x2F;05&#x2F;05&#x2F;max-factors-clown-contouring-make-up-of-early-television&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.messynessychic.com&#x2F;2016&#x2F;05&#x2F;05&#x2F;max-factors-clown-...</a><p>Colourising old TV footage can <i>only</i> result in a misrepresentation, because the underlying colour is false to have any kind of usable representation on the medium itself.<p>And this caricatured example underpins the problem with colourisation: contemporary bias is unavoidable, and can be misleading. Can you take a black and white photo of an African-American woman in the 1930s and accurately colour her skin?<p>You cannot.</div><br/><div id="36992115" class="c"><input type="checkbox" id="c-36992115" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991895">parent</a><span>|</span><a href="#36991947">next</a><span>|</span><label class="collapse" for="c-36992115">[-]</label><label class="expand" for="c-36992115">[11 more]</label></div><br/><div class="children"><div class="content">&gt; Can you take a black and white photo of an African-American woman in the 1930s and accurately colour her skin?<p>AI colorization will, in general, be <i>plausible</i>, not <i>accurate</i>.</div><br/><div id="36992502" class="c"><input type="checkbox" id="c-36992502" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36992115">parent</a><span>|</span><a href="#36992131">next</a><span>|</span><label class="collapse" for="c-36992502">[-]</label><label class="expand" for="c-36992502">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, the model is racist for sure. That&#x27;s a limitation of the dataset though (celeb A is not known for its diversity, but it was easy for me to work with, I trained this model on Colab)<p>And plausibility is a feauture, not a bug.<p>There are always many plausibily correct colorizations of an image, which you want the model to be able to capture in order to be versatile.<p>Many colorization models introduce additional losses (such as discriminator losses) that avoid constraining the model to a single &quot;correct answer&quot; when the solution space is actually considerably larger.</div><br/></div></div><div id="36992131" class="c"><input type="checkbox" id="c-36992131" checked=""/><div class="controls bullet"><span class="by">morelisp</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36992115">parent</a><span>|</span><a href="#36992502">prev</a><span>|</span><a href="#36991947">next</a><span>|</span><label class="collapse" for="c-36992131">[-]</label><label class="expand" for="c-36992131">[9 more]</label></div><br/><div class="children"><div class="content">In other words, bullshit.</div><br/><div id="36992182" class="c"><input type="checkbox" id="c-36992182" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36992131">parent</a><span>|</span><a href="#36992163">next</a><span>|</span><label class="collapse" for="c-36992182">[-]</label><label class="expand" for="c-36992182">[3 more]</label></div><br/><div class="children"><div class="content">No more so than any other colorization method that isn’t dependent on out-of-band info about the particular image (and even that is just more constrained informed guesswork.)<p>That&#x27;s what happens when you are filling in missing info that isn&#x27;t in your source.<p>EDIT: 
Of course, color photography can be “bullshit” rather than accurate in relation to the actual colors of things in the image; as is the case with the red, blue, and <i>green</i> (actual colors of the physical items) uniforms in Star Trek: The Original Series. But, also fairly frequently, lots of not-intentionally-distortive reproductions of skin tones (often most politically sensitive in the US with racially non-White subjects, where there are also plenty of examples of <i>deliberate</i> manipulation.)</div><br/><div id="36992238" class="c"><input type="checkbox" id="c-36992238" checked=""/><div class="controls bullet"><span class="by">morelisp</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36992182">parent</a><span>|</span><a href="#36992163">next</a><span>|</span><label class="collapse" for="c-36992238">[-]</label><label class="expand" for="c-36992238">[2 more]</label></div><br/><div class="children"><div class="content">Showing color X on TVs by actually making the thing color Y in the studio, well, <i>filming</i>, not bullshit. It&#x27;s an intentional choice playing out as intended. It is meant to communicate a particular thing and does so.</div><br/><div id="36992293" class="c"><input type="checkbox" id="c-36992293" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36992238">parent</a><span>|</span><a href="#36992163">next</a><span>|</span><label class="collapse" for="c-36992293">[-]</label><label class="expand" for="c-36992293">[1 more]</label></div><br/><div class="children"><div class="content">That particular thing was <i>not</i> intentional, and is the reason why the (same color in person, different material) command wrap uniform that is supposed to be color-matched to the made-as-green uniforms isn’t on screen.<p>But, yes, in general inaccurate color reproduction can be intentionally manipulated with planning to intentionally create appearances in photos that do not exist in reality.</div><br/></div></div></div></div></div></div><div id="36992163" class="c"><input type="checkbox" id="c-36992163" checked=""/><div class="controls bullet"><span class="by">snvzz</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36992131">parent</a><span>|</span><a href="#36992182">prev</a><span>|</span><a href="#36992451">next</a><span>|</span><label class="collapse" for="c-36992163">[-]</label><label class="expand" for="c-36992163">[4 more]</label></div><br/><div class="children"><div class="content">The original color information just isn&#x27;t there.<p>So bullshit is the best you&#x27;re going to get.</div><br/><div id="36992247" class="c"><input type="checkbox" id="c-36992247" checked=""/><div class="controls bullet"><span class="by">morelisp</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36992163">parent</a><span>|</span><a href="#36992451">next</a><span>|</span><label class="collapse" for="c-36992247">[-]</label><label class="expand" for="c-36992247">[3 more]</label></div><br/><div class="children"><div class="content">Well, you could also <i>not put more bullshit in the world by not doing the thing.</i></div><br/><div id="36992766" class="c"><input type="checkbox" id="c-36992766" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36992247">parent</a><span>|</span><a href="#36992857">next</a><span>|</span><label class="collapse" for="c-36992766">[-]</label><label class="expand" for="c-36992766">[1 more]</label></div><br/><div class="children"><div class="content">Why are you so negative about it? Pretty sure many people would find it impressive to colorize old photos to look at them as if these were taken in color.<p>Should artists not put their bs in the world? Writers? Musicians? Most of it is made up but plausible to make you feel something subjective.</div><br/></div></div><div id="36992857" class="c"><input type="checkbox" id="c-36992857" checked=""/><div class="controls bullet"><span class="by">roywiggins</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36992247">parent</a><span>|</span><a href="#36992766">prev</a><span>|</span><a href="#36992451">next</a><span>|</span><label class="collapse" for="c-36992857">[-]</label><label class="expand" for="c-36992857">[1 more]</label></div><br/><div class="children"><div class="content">People have been colorizing photos as long as there have been photos.</div><br/></div></div></div></div></div></div><div id="36992451" class="c"><input type="checkbox" id="c-36992451" checked=""/><div class="controls bullet"><span class="by">jackpeterfletch</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36992131">parent</a><span>|</span><a href="#36992163">prev</a><span>|</span><a href="#36991947">next</a><span>|</span><label class="collapse" for="c-36992451">[-]</label><label class="expand" for="c-36992451">[1 more]</label></div><br/><div class="children"><div class="content"><i>shrug</i> people like looking at colorised photos because it helps root the image within the setting of the real world they occupy.<p>For some it’s more evocative, irregardless of the absolute accuracy.<p>Having a professional do it for that picture of your great grandad is expensive.<p>Having a colourisation subreddit do it is probably worse for accuracy.<p>I think there is a place for this bullshit.</div><br/></div></div></div></div></div></div><div id="36991947" class="c"><input type="checkbox" id="c-36991947" checked=""/><div class="controls bullet"><span class="by">atorodius</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991895">parent</a><span>|</span><a href="#36992115">prev</a><span>|</span><a href="#36992065">next</a><span>|</span><label class="collapse" for="c-36991947">[-]</label><label class="expand" for="c-36991947">[1 more]</label></div><br/><div class="children"><div class="content">This is true, but if you have some reference images, you can probably adapt some of the recent diffusion adaptation work such as DreamBooth, to tell the model „hey this period looked like this“, and finetune it.<p><a href="https:&#x2F;&#x2F;dreambooth.github.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;dreambooth.github.io&#x2F;</a></div><br/></div></div></div></div><div id="36991793" class="c"><input type="checkbox" id="c-36991793" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991666">parent</a><span>|</span><a href="#36991895">prev</a><span>|</span><a href="#36992055">next</a><span>|</span><label class="collapse" for="c-36991793">[-]</label><label class="expand" for="c-36991793">[1 more]</label></div><br/><div class="children"><div class="content">&gt;<i>You can&#x27;t train on decolorized images, because the neural network needs to learn how to predict color with a black and white image as context. Without color info, the model can&#x27;t learn.</i><p>I think the parent means with delocorized images used to test the success and guide the training (since they can be readily compared with the colored image they resulted from which would be the perfect result).<p>Not to use decolorized images alone to train for coloring (which doesn&#x27;t even make sense).</div><br/></div></div><div id="36992055" class="c"><input type="checkbox" id="c-36992055" checked=""/><div class="controls bullet"><span class="by">omoikane</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991666">parent</a><span>|</span><a href="#36991793">prev</a><span>|</span><a href="#36991882">next</a><span>|</span><label class="collapse" for="c-36992055">[-]</label><label class="expand" for="c-36992055">[3 more]</label></div><br/><div class="children"><div class="content">Is there a reason for using LAB as opposed to YCbCr?  My understanding is that YCbCr is another model that separates luma (Y) from chroma (Cb and Cr), but JPEG uses YCbCr natively, so I wonder if there would be any advantage in using that instead of LAB?</div><br/><div id="36992190" class="c"><input type="checkbox" id="c-36992190" checked=""/><div class="controls bullet"><span class="by">TylerE</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36992055">parent</a><span>|</span><a href="#36991882">next</a><span>|</span><label class="collapse" for="c-36992190">[-]</label><label class="expand" for="c-36992190">[2 more]</label></div><br/><div class="children"><div class="content">The Y in YCbCr is linear, and is just a grayscale image. The L channel in lab is non-linear (as are A and B), and is a complex transfer function designed to mimic the response of the human eye.<p>A YCbCr colorspace is directly mapped from RGB, and thus is limited to that gamut.<p>LAB can encode colors brighter than diffuse white (ala #ffffff), like an outdoor scene in direct sunlight.<p>Sorta HDR (LAB) vs non-HDR (YCbCr).<p>This image (<a href="https:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;thumb&#x2F;f&#x2F;f3&#x2F;Example_of_LAB_color_enhancement.jpg&#x2F;1920px-Example_of_LAB_color_enhancement.jpg" rel="nofollow noreferrer">https:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;thumb&#x2F;f&#x2F;f3&#x2F;Ex...</a>) is a good demo, left side was processed in LAB, right in YCbCr). Even reduced back down to a jpeg, the left side is obviously more lifelike, since the highlights and tones were preserved until much later in processing pipeline.</div><br/><div id="36995545" class="c"><input type="checkbox" id="c-36995545" checked=""/><div class="controls bullet"><span class="by">aendruk</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36992190">parent</a><span>|</span><a href="#36991882">next</a><span>|</span><label class="collapse" for="c-36995545">[-]</label><label class="expand" for="c-36995545">[1 more]</label></div><br/><div class="children"><div class="content">The description included with  that image conflicts with your account:<p>&gt; An example of color enhancement using LAB colorspace in Photoshop (CIELAB D50). Left side is enhanced, right side is not. Enhancement is &quot;overdone&quot; to show the effect better.<p>And per the original upload the “enhancement” demonstrated is linear compression of the a* and b* channels—<p><a href="https:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;archive&#x2F;f&#x2F;f3&#x2F;20120915215050%21Example_of_LAB_color_enhancement.jpg" rel="nofollow noreferrer">https:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;archive&#x2F;f&#x2F;f3&#x2F;...</a><p>—the effect a divergence from the likeness of life at least as I’ve experienced it.</div><br/></div></div></div></div></div></div><div id="36991882" class="c"><input type="checkbox" id="c-36991882" checked=""/><div class="controls bullet"><span class="by">atorodius</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991666">parent</a><span>|</span><a href="#36992055">prev</a><span>|</span><a href="#36991544">next</a><span>|</span><label class="collapse" for="c-36991882">[-]</label><label class="expand" for="c-36991882">[3 more]</label></div><br/><div class="children"><div class="content">You can take arbitrary images and convert them to grayscale for training, and do conditional diffusion</div><br/><div id="36992076" class="c"><input type="checkbox" id="c-36992076" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991882">parent</a><span>|</span><a href="#36991544">next</a><span>|</span><label class="collapse" for="c-36992076">[-]</label><label class="expand" for="c-36992076">[2 more]</label></div><br/><div class="children"><div class="content">But convert them to grayscale how?<p>Black and white film doesn&#x27;t have one single colour sensitivity. Play around with something like DxO FilmPack sometime (it has excellent measurement-based representations of black and white film stocks).<p>It&#x27;s a much more complex problem than it might seem on the surface.</div><br/><div id="36992180" class="c"><input type="checkbox" id="c-36992180" checked=""/><div class="controls bullet"><span class="by">atorodius</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36992076">parent</a><span>|</span><a href="#36991544">next</a><span>|</span><label class="collapse" for="c-36992180">[-]</label><label class="expand" for="c-36992180">[1 more]</label></div><br/><div class="children"><div class="content">fair, but can’t you just randomize the grayscale generation for training?</div><br/></div></div></div></div></div></div></div></div><div id="36991544" class="c"><input type="checkbox" id="c-36991544" checked=""/><div class="controls bullet"><span class="by">atorodius</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991509">parent</a><span>|</span><a href="#36991666">prev</a><span>|</span><a href="#36996062">next</a><span>|</span><label class="collapse" for="c-36991544">[-]</label><label class="expand" for="c-36991544">[8 more]</label></div><br/><div class="children"><div class="content">yes, so infinite training data. but the challenge will be scaling to large resolutions and getting global consistency</div><br/><div id="36991749" class="c"><input type="checkbox" id="c-36991749" checked=""/><div class="controls bullet"><span class="by">jrockway</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991544">parent</a><span>|</span><a href="#36991665">next</a><span>|</span><label class="collapse" for="c-36991749">[-]</label><label class="expand" for="c-36991749">[4 more]</label></div><br/><div class="children"><div class="content">Is that challenging?  Humans have awful color resolution perception, so even if you have a huge black-and-white image, people would think it looks right with even with very low-resolution color information.  Or, if the AI hallucinates a lot of high frequency color noise, it wouldn&#x27;t be noticable.<p>Wikipedia has a great example image here: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Chroma_subsampling" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Chroma_subsampling</a>.  Most people would say all of them looked fine at 1:1 resolution.</div><br/><div id="36991813" class="c"><input type="checkbox" id="c-36991813" checked=""/><div class="controls bullet"><span class="by">atorodius</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991749">parent</a><span>|</span><a href="#36991665">next</a><span>|</span><label class="collapse" for="c-36991813">[-]</label><label class="expand" for="c-36991813">[3 more]</label></div><br/><div class="children"><div class="content">I meant more from a comoute standpoint, the models are expensive to run full res</div><br/><div id="36992025" class="c"><input type="checkbox" id="c-36992025" checked=""/><div class="controls bullet"><span class="by">jrockway</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991813">parent</a><span>|</span><a href="#36991665">next</a><span>|</span><label class="collapse" for="c-36992025">[-]</label><label class="expand" for="c-36992025">[2 more]</label></div><br/><div class="children"><div class="content">I see what you mean.  I think that you can happily scale the B&amp;W image down, run the model, and then scale the chroma information back up.<p>Something I was thinking about after writing the comment is that the model is probably trained on chroma-subsampled images.  Digital cameras do it with the bayer filter, and video cameras add 4:2:0 subsampling or similar subsampling as they compress the image.  So the AI is probably biased towards &quot;look like this photo was taken with a digital camera&quot; versus &quot;actually reconstruct the colors of the image&quot;.  What effect this actually has, I don&#x27;t know!</div><br/><div id="36992167" class="c"><input type="checkbox" id="c-36992167" checked=""/><div class="controls bullet"><span class="by">atorodius</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36992025">parent</a><span>|</span><a href="#36991665">next</a><span>|</span><label class="collapse" for="c-36992167">[-]</label><label class="expand" for="c-36992167">[1 more]</label></div><br/><div class="children"><div class="content">good point, I hadn’t realized that you only need to predict chroma! That actully greatly simplifies things<p>re. chroma subsampling in training data: this is actually a big problem and a good generative model will absolutely learn to predict chroma subsampled values (or JPEG artifacts even!). you can get around it by applying random downscaling with antialiasing during training.</div><br/></div></div></div></div></div></div></div></div><div id="36991665" class="c"><input type="checkbox" id="c-36991665" checked=""/><div class="controls bullet"><span class="by">drapado</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991544">parent</a><span>|</span><a href="#36991749">prev</a><span>|</span><a href="#36996062">next</a><span>|</span><label class="collapse" for="c-36991665">[-]</label><label class="expand" for="c-36991665">[3 more]</label></div><br/><div class="children"><div class="content">I guess you can always use a two-stage process. First colorize, then upscale</div><br/><div id="36991826" class="c"><input type="checkbox" id="c-36991826" checked=""/><div class="controls bullet"><span class="by">atorodius</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991665">parent</a><span>|</span><a href="#36996062">next</a><span>|</span><label class="collapse" for="c-36991826">[-]</label><label class="expand" for="c-36991826">[2 more]</label></div><br/><div class="children"><div class="content">yeah, you can use SOTA super res, but that tends to be generative too (even diffusion based on its own, or more commonly based on GANs). it can be a challenge to synthesize the right high res details.<p>but that’s basically the stable diffusion paper (diffusion in latent space plus GAN superres)</div><br/><div id="36994332" class="c"><input type="checkbox" id="c-36994332" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36991341">root</a><span>|</span><a href="#36991826">parent</a><span>|</span><a href="#36996062">next</a><span>|</span><label class="collapse" for="c-36994332">[-]</label><label class="expand" for="c-36994332">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, if you have a high res image, you can get color info at super low-res and then regenerate the colors at high res with another model. (though this isn&#x27;t an efficient approach at all)<p><a href="https:&#x2F;&#x2F;github.com&#x2F;TencentARC&#x2F;T2I-Adapter">https:&#x2F;&#x2F;github.com&#x2F;TencentARC&#x2F;T2I-Adapter</a><p>i&#x27;ve also seen a controlnet do this.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="36996062" class="c"><input type="checkbox" id="c-36996062" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#36991341">prev</a><span>|</span><a href="#36996130">next</a><span>|</span><label class="collapse" for="c-36996062">[-]</label><label class="expand" for="c-36996062">[1 more]</label></div><br/><div class="children"><div class="content">That is so 2023 and so 1993 at the same time!</div><br/></div></div><div id="36996130" class="c"><input type="checkbox" id="c-36996130" checked=""/><div class="controls bullet"><span class="by">realusername</span><span>|</span><a href="#36996062">prev</a><span>|</span><a href="#36993056">next</a><span>|</span><label class="collapse" for="c-36996130">[-]</label><label class="expand" for="c-36996130">[1 more]</label></div><br/><div class="children"><div class="content">Is there anything that exists right now with diffusion models to improve poor VHS coloring? The coloring does exist so I would not want to replace a red shirt by a blue shirt for example but it&#x27;s just not very accurate.</div><br/></div></div><div id="36993056" class="c"><input type="checkbox" id="c-36993056" checked=""/><div class="controls bullet"><span class="by">iamflimflam1</span><span>|</span><a href="#36996130">prev</a><span>|</span><a href="#36991920">next</a><span>|</span><label class="collapse" for="c-36993056">[-]</label><label class="expand" for="c-36993056">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if this can be used for color correction in videos.</div><br/></div></div><div id="36991920" class="c"><input type="checkbox" id="c-36991920" checked=""/><div class="controls bullet"><span class="by">aziaziazi</span><span>|</span><a href="#36993056">prev</a><span>|</span><a href="#36992437">next</a><span>|</span><label class="collapse" for="c-36991920">[-]</label><label class="expand" for="c-36991920">[10 more]</label></div><br/><div class="children"><div class="content">How much would it cost to colorize a movie with a fork of this?</div><br/><div id="36992082" class="c"><input type="checkbox" id="c-36992082" checked=""/><div class="controls bullet"><span class="by">NBJack</span><span>|</span><a href="#36991920">parent</a><span>|</span><a href="#36992134">next</a><span>|</span><label class="collapse" for="c-36992082">[-]</label><label class="expand" for="c-36992082">[3 more]</label></div><br/><div class="children"><div class="content">I think the bigger question is would it be stable enough. Many SD like models struggle with consistency across multiple images (i.e. frames) even when content doesn&#x27;t change much. Would he a cool problem to see tackled.</div><br/><div id="36992737" class="c"><input type="checkbox" id="c-36992737" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36991920">root</a><span>|</span><a href="#36992082">parent</a><span>|</span><a href="#36992134">next</a><span>|</span><label class="collapse" for="c-36992737">[-]</label><label class="expand" for="c-36992737">[2 more]</label></div><br/><div class="children"><div class="content">temporal coherence is def an issue with these types of models, though I haven&#x27;t tested it out with ColorDiffusion. Assuming you&#x27;re not doing anything autoregressive (from frame to frame) to do temporal coherence, you can also parallelize the colorization of each frame, which would affect cost.<p>Tbh most cost effective would be a conditional GAN though</div><br/><div id="36992966" class="c"><input type="checkbox" id="c-36992966" checked=""/><div class="controls bullet"><span class="by">lajamerr</span><span>|</span><a href="#36991920">root</a><span>|</span><a href="#36992737">parent</a><span>|</span><a href="#36992134">next</a><span>|</span><label class="collapse" for="c-36992966">[-]</label><label class="expand" for="c-36992966">[1 more]</label></div><br/><div class="children"><div class="content">Change up the model. That allows it to see previous frames and 1-2 future frames.<p>Then train the model on movies that are color and then turn them black and white.<p>That way you can train temporal coherence.</div><br/></div></div></div></div></div></div><div id="36992134" class="c"><input type="checkbox" id="c-36992134" checked=""/><div class="controls bullet"><span class="by">leetharris</span><span>|</span><a href="#36991920">parent</a><span>|</span><a href="#36992082">prev</a><span>|</span><a href="#36992078">next</a><span>|</span><label class="collapse" for="c-36992134">[-]</label><label class="expand" for="c-36992134">[5 more]</label></div><br/><div class="children"><div class="content">Quick math:<p>24 frames per second * 60 seconds per minute * 90 minute movie length = 129600 frames<p>If you could get cost to a penny per frame, about $13k? But I&#x27;d bet you could easily get it an order of magnitude less in terms of cost. So $1500 or so?<p>And that&#x27;s assuming you do 100% of frames and don&#x27;t have any clever tricks there.</div><br/><div id="36992196" class="c"><input type="checkbox" id="c-36992196" checked=""/><div class="controls bullet"><span class="by">caturopath</span><span>|</span><a href="#36991920">root</a><span>|</span><a href="#36992134">parent</a><span>|</span><a href="#36992078">next</a><span>|</span><label class="collapse" for="c-36992196">[-]</label><label class="expand" for="c-36992196">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m willing to bet that if you just treated each frame as an image, it would result in some weird stuff when you played them as a movie.<p>&gt; penny per frame<p>Where did this come from?</div><br/><div id="36992381" class="c"><input type="checkbox" id="c-36992381" checked=""/><div class="controls bullet"><span class="by">leetharris</span><span>|</span><a href="#36991920">root</a><span>|</span><a href="#36992196">parent</a><span>|</span><a href="#36993316">next</a><span>|</span><label class="collapse" for="c-36992381">[-]</label><label class="expand" for="c-36992381">[1 more]</label></div><br/><div class="children"><div class="content">I do lots of large scale ML work, this was just sort of a random educated &quot;order of magnitude&quot; guess.</div><br/></div></div><div id="36993316" class="c"><input type="checkbox" id="c-36993316" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#36991920">root</a><span>|</span><a href="#36992196">parent</a><span>|</span><a href="#36992381">prev</a><span>|</span><a href="#36992078">next</a><span>|</span><label class="collapse" for="c-36993316">[-]</label><label class="expand" for="c-36993316">[2 more]</label></div><br/><div class="children"><div class="content">Seems like a pretty reasonable estimate, if it cost about $2 a hour to rent a decent GPU, that’s 18s per penny which sounds pretty doable to run one frame.</div><br/><div id="36994354" class="c"><input type="checkbox" id="c-36994354" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36991920">root</a><span>|</span><a href="#36993316">parent</a><span>|</span><a href="#36992078">next</a><span>|</span><label class="collapse" for="c-36994354">[-]</label><label class="expand" for="c-36994354">[1 more]</label></div><br/><div class="children"><div class="content">Think inference time was on the order of 4-5seconds per image on a v100, which you can rent for like .80 cents an hour, though you can get way better gpus like a100s for ~1.1 usd&#x2F;h now. But ofc this is at 64px res in pixel space.<p>If you wanted to do this at high res, you would definitely use a latent diffusion model. The autoencoder is almost free to run, and reduces the dimensionality of high res images significantly, which makes it a lot cheaper to run the autoregressive diffusion model for multiple steps.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36992437" class="c"><input type="checkbox" id="c-36992437" checked=""/><div class="controls bullet"><span class="by">jurassic</span><span>|</span><a href="#36991920">prev</a><span>|</span><a href="#36991688">next</a><span>|</span><label class="collapse" for="c-36992437">[-]</label><label class="expand" for="c-36992437">[11 more]</label></div><br/><div class="children"><div class="content">This is a cool party trick, but I don&#x27;t see a need for this in any real applications. Black and white is its own art form, and a lot of really great black and white images would look like absolute garbage if you could convert them to color. This is because the things that make a great black and white image (dramatic contrasts, emphasis on shape&#x2F;geometry, texture, etc) can lose a lot of their impact when you introduce color. Our aesthetic tolerance for contrast seems significantly reduced in color because our expectations for the image are more anchored in how things look in the real world. And colors which can be very pleasing in some images are just distracting in others.<p>So all this is to say.... I don&#x27;t think there would be commercial demand to, say, &quot;upgrade&quot; classic movies with color. Those films were shot by cinematographers who were steeped in the black &amp; white medium and made lighting and compositional choices that take greatest advantage of those creative limitations.</div><br/><div id="36992849" class="c"><input type="checkbox" id="c-36992849" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36992437">parent</a><span>|</span><a href="#36992504">next</a><span>|</span><label class="collapse" for="c-36992849">[-]</label><label class="expand" for="c-36992849">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve run colorization like this against historic photographs and it had a very real impact on me - I found myself able to imagine life when the photo or video was taken much more easily when it was no longer in black and white.<p>Here&#x27;s an example I really enjoyed, of a snowball fight in 1896: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;JoaquimCampa&#x2F;status&#x2F;1311391615425093634" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;JoaquimCampa&#x2F;status&#x2F;1311391615425093634</a></div><br/><div id="36993392" class="c"><input type="checkbox" id="c-36993392" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#36992437">root</a><span>|</span><a href="#36992849">parent</a><span>|</span><a href="#36992504">next</a><span>|</span><label class="collapse" for="c-36993392">[-]</label><label class="expand" for="c-36993392">[1 more]</label></div><br/><div class="children"><div class="content">The issue I have is that the examples appear to be color images that were converted to black and white. In other words, they are modern images with plenty of images acquired in color for training. Converting an archival shot from early 20th century is totally different. Totally lowers credibility in my eyes</div><br/></div></div></div></div><div id="36992504" class="c"><input type="checkbox" id="c-36992504" checked=""/><div class="controls bullet"><span class="by">pythonguython</span><span>|</span><a href="#36992437">parent</a><span>|</span><a href="#36992849">prev</a><span>|</span><a href="#36995982">next</a><span>|</span><label class="collapse" for="c-36992504">[-]</label><label class="expand" for="c-36992504">[1 more]</label></div><br/><div class="children"><div class="content">Counterexample: They Shall Not Grow Old, a WW1 documentary film with mostly colorized footage with recreated audio. The film was commercially successful and I found it to be a great watch.</div><br/></div></div><div id="36995982" class="c"><input type="checkbox" id="c-36995982" checked=""/><div class="controls bullet"><span class="by">112233</span><span>|</span><a href="#36992437">parent</a><span>|</span><a href="#36992504">prev</a><span>|</span><a href="#36992557">next</a><span>|</span><label class="collapse" for="c-36995982">[-]</label><label class="expand" for="c-36995982">[2 more]</label></div><br/><div class="children"><div class="content">I really really want something like this to actually reduce noise in images. Current raw photo denoising often is little more than a tuned up gaussian blur. Making it guess color based on larger context and external info would be step up.<p>Eztra happy if it would be possibe to tune denoising, using photos from the same series. Multiframe NLMeans right now is slow and mostly theoretical.</div><br/><div id="36996066" class="c"><input type="checkbox" id="c-36996066" checked=""/><div class="controls bullet"><span class="by">ragnarok451</span><span>|</span><a href="#36992437">root</a><span>|</span><a href="#36995982">parent</a><span>|</span><a href="#36992557">next</a><span>|</span><label class="collapse" for="c-36996066">[-]</label><label class="expand" for="c-36996066">[1 more]</label></div><br/><div class="children"><div class="content">Have you used Topaz Labs? That denoising is pretty good, definitely more advanced than a tuned-up gaussian.</div><br/></div></div></div></div><div id="36992557" class="c"><input type="checkbox" id="c-36992557" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#36992437">parent</a><span>|</span><a href="#36995982">prev</a><span>|</span><a href="#36992910">next</a><span>|</span><label class="collapse" for="c-36992557">[-]</label><label class="expand" for="c-36992557">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t think there would be commercial demand to, say, &quot;upgrade&quot; classic movies with color.<p>There was, and maybe there will be again once we get far enough from the consumer burnout from the absolute deluge of that in, mostly, the 1980s-1990s.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;List_of_black-and-white_films_that_have_been_colorized" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;List_of_black-and-white_film...</a></div><br/></div></div><div id="36992910" class="c"><input type="checkbox" id="c-36992910" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36992437">parent</a><span>|</span><a href="#36992557">prev</a><span>|</span><a href="#36992694">next</a><span>|</span><label class="collapse" for="c-36992910">[-]</label><label class="expand" for="c-36992910">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t think there would be commercial demand to, say, &quot;upgrade&quot; classic movies with color.<p>Alas there has been serious money in this in the past (VHS and as I understand it US cable TV).<p>I would not assume that we have more taste now than we did then. (The state of cinema suggests the opposite to me at least.)</div><br/></div></div><div id="36992694" class="c"><input type="checkbox" id="c-36992694" checked=""/><div class="controls bullet"><span class="by">MrVandemar</span><span>|</span><a href="#36992437">parent</a><span>|</span><a href="#36992910">prev</a><span>|</span><a href="#36992811">next</a><span>|</span><label class="collapse" for="c-36992694">[-]</label><label class="expand" for="c-36992694">[1 more]</label></div><br/><div class="children"><div class="content">Some of the old Doctor Who stories that were filmed in colour they only have black and white copies of. The colourisations have been ... very good, better than I would have thought, but not perfect. Could be an a good application.</div><br/></div></div><div id="36993792" class="c"><input type="checkbox" id="c-36993792" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#36992437">parent</a><span>|</span><a href="#36992811">prev</a><span>|</span><a href="#36991688">next</a><span>|</span><label class="collapse" for="c-36993792">[-]</label><label class="expand" for="c-36993792">[1 more]</label></div><br/><div class="children"><div class="content">&gt; but I don&#x27;t see a need for this in any real applications<p>when your mom asks you to make a black and white image in color<p>digital retouchers do this kind of work all day for decades</div><br/></div></div></div></div><div id="36991688" class="c"><input type="checkbox" id="c-36991688" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36992437">prev</a><span>|</span><label class="collapse" for="c-36991688">[-]</label><label class="expand" for="c-36991688">[36 more]</label></div><br/><div class="children"><div class="content">Colourising old photographs is the banal apotheosis application of diffusion AI.<p>It&#x27;s the pinnacle of the whole thing: &quot;imagine it for me in a way that conforms to my contemporary expectations&quot;.<p>If you&#x27;re going to colourise images, have the decency to do it by hand. If possible on a print with brushes.<p>Edit: didn&#x27;t think this would be popular. Maybe it&#x27;s the historical photography nerd in me, but colourising images without effort and thought is like smashing vintage glass windows for the fun of it: cultural vandalism.</div><br/><div id="36992333" class="c"><input type="checkbox" id="c-36992333" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#36991688">parent</a><span>|</span><a href="#36991770">next</a><span>|</span><label class="collapse" for="c-36992333">[-]</label><label class="expand" for="c-36992333">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But since you do not have access to colour originals of historical photos in almost every instance, you cannot possibly train the network to have any instinct for the colour sensitivity of the medium, can you?<p>Plenty of people say that about colorization period, which, while I disagree, seems more sensible than your position to me, which just seems to be fetishizing suffering.</div><br/></div></div><div id="36991770" class="c"><input type="checkbox" id="c-36991770" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#36991688">parent</a><span>|</span><a href="#36992333">prev</a><span>|</span><a href="#36992122">next</a><span>|</span><label class="collapse" for="c-36991770">[-]</label><label class="expand" for="c-36991770">[20 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re going to write code, have the decency to do it on punch cards. If possible by hand punching, rather than using a keypunch machine.</div><br/><div id="36991814" class="c"><input type="checkbox" id="c-36991814" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36991770">parent</a><span>|</span><a href="#36992122">next</a><span>|</span><label class="collapse" for="c-36991814">[-]</label><label class="expand" for="c-36991814">[19 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t the point I am making.<p>The point I am making is that colourisation is subjective art, and that alone.<p>Colourisation cannot fail to enforce contemporary biases based on poor understanding of the materials. It will darken or lighten skin inappropriately, and mislead in any number of ways.<p>Doing it by hand (in photoshop or on a print) acknowledges the inherent bias that is involved in colourisation.<p>Automating it is banal at best and dangerous at worst; colourised images risk distorting history.</div><br/><div id="36991962" class="c"><input type="checkbox" id="c-36991962" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36991814">parent</a><span>|</span><a href="#36991891">next</a><span>|</span><label class="collapse" for="c-36991962">[-]</label><label class="expand" for="c-36991962">[5 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Colourisation cannot fail to enforce contemporary biases based on poor understanding of the materials. It will darken or lighten skin inappropriately, and mislead in any number of ways.</i><p>If anything, an AI trained on a large and diverse dataset is probably going to wind up being much <i>more</i> accurate with regards to skin color than a human colorist would be in most cases.<p>The problem here isn&#x27;t whether colorization is done by man or machine; it&#x27;s just ensuring that colorized photos are identified as such. Which they usually are -- that&#x27;s not a new problem to be solved.</div><br/><div id="36992046" class="c"><input type="checkbox" id="c-36992046" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36991962">parent</a><span>|</span><a href="#36991891">next</a><span>|</span><label class="collapse" for="c-36992046">[-]</label><label class="expand" for="c-36992046">[4 more]</label></div><br/><div class="children"><div class="content">No it&#x27;s not, not really.<p>A diverse data set of black and white images doesn&#x27;t have any kind of knowledge of the colour sensitivity of the medium in that moment.<p>What film was it? How was it processed? Is it a scan of a negative or a print? What was the colour of the lighting? Was a particular colour tint filter used on the lens? Was the subject wearing makeup optimised for black and white photography?<p>The black and white image, standing alone, cannot tell you this, I think. Sure, it might get a bit better at, say, identifying a 1950s TV show. But what is the &quot;correct&quot; accurate colour representation of that scene, when televisual makeup was wildly unnatural in colour?</div><br/><div id="36992217" class="c"><input type="checkbox" id="c-36992217" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36992046">parent</a><span>|</span><a href="#36991891">next</a><span>|</span><label class="collapse" for="c-36992217">[-]</label><label class="expand" for="c-36992217">[3 more]</label></div><br/><div class="children"><div class="content">But do people have any of that knowledge either? Most of the time, I don&#x27;t think so -- they colorize stuff in a way that just &quot;looks right&quot; or &quot;looks natural&quot; or &quot;looks nice&quot; to their eye, that&#x27;s all.<p>And the dataset an AI is going to train on should be using original color photos that are then converted to B&amp;W across a wide variety of color curves. So it should be fairly robust to all sorts of film types. So again, I repeat that it&#x27;s probably going to wind up being <i>more</i> accurate with regard to skin tone than a human (with their aesthetic biases) usually would.</div><br/><div id="36992418" class="c"><input type="checkbox" id="c-36992418" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36992217">parent</a><span>|</span><a href="#36991891">next</a><span>|</span><label class="collapse" for="c-36992418">[-]</label><label class="expand" for="c-36992418">[2 more]</label></div><br/><div class="children"><div class="content">&gt; But do people have any of that knowledge either? Most of the time, I don&#x27;t think so -- they colorize stuff in a way that just &quot;looks right&quot; or &quot;looks natural&quot; or &quot;looks nice&quot; to their eye, that&#x27;s all.<p>No, indeed. Which is why doing it by hand is more respectful of the notion that it is subjective.<p>Automatic colourisation is and will be viewed differently, as more &quot;scientific&quot;, when it&#x27;s still absolutely beholden to the same biases and maybe misconceptions that we can&#x27;t unpick because they come from poor training data.<p>Finally: &quot;original colour photos&quot; are also a problem. Not only for the part of the history where they don&#x27;t exist. But also for the part of history (until the early 1960s) when the colour rendition of those photos was false or incomplete. You can get a little closer to understanding what that colour looked like, but it&#x27;s important to understand that colour emulsions vary in the way they work: it&#x27;s not black and white film with extra colour sensitivity.<p>So at best you will be colourising the black and white film to look like the colour film, which is not reality. And there are well-understood problems with correct representation of skin tones with colour film until the mid-eighties.<p>I can see your point; I just think there&#x27;s a bigger picture here (pun not intended) that you&#x27;re not seeing.</div><br/><div id="36992539" class="c"><input type="checkbox" id="c-36992539" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36992418">parent</a><span>|</span><a href="#36991891">next</a><span>|</span><label class="collapse" for="c-36992539">[-]</label><label class="expand" for="c-36992539">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Automatic colourisation is and will be viewed differently, as more &quot;scientific&quot;</i><p>Then the solution is to correct that misperception, not deny ourselves a useful tool.<p>&gt; <i>I can see your point; I just think there&#x27;s a bigger picture here (pun not intended) that you&#x27;re not seeing.</i><p>My overarching point is that this is a tool like any other. And the idea that &quot;doing it by hand is more respectful of the notion that it is subjective&quot; I will push back on 100%.<p>There is nothing disrespectful about colorizing a photo, automatically or by hand. But it should always be clearly communicated that it is subjective not objective, whether human or machine.<p>Again, if someone believes the colorization is somehow &quot;real&quot; or &quot;scientific&quot; because a computer did it, then correct their misbelief. Don&#x27;t stop using the tool. That&#x27;s the bigger picture here.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36991891" class="c"><input type="checkbox" id="c-36991891" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36991814">parent</a><span>|</span><a href="#36991962">prev</a><span>|</span><a href="#36992383">next</a><span>|</span><label class="collapse" for="c-36991891">[-]</label><label class="expand" for="c-36991891">[5 more]</label></div><br/><div class="children"><div class="content">&gt;<i>Automating it is banal at best and dangerous at worst; colourised images risk distorting history</i><p>Well, faces still have a certain tint, the sky is mostly blue, the grass green, water is blue, mud pools are brown, the ground too, a lot of historical fabrics are certain inherent colors, known flowers have known colors, brownstones have red&#x2F;brown color. A lot of it, is just not that subjective.<p>Besides different color film stock (or camera sensor &quot;color science&quot;) can already result in dozens of widely different colorings of the same exactly scene.</div><br/><div id="36992004" class="c"><input type="checkbox" id="c-36992004" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36991891">parent</a><span>|</span><a href="#36992383">next</a><span>|</span><label class="collapse" for="c-36992004">[-]</label><label class="expand" for="c-36992004">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Well, faces still have a certain tint<p>Do they? A <i>certain</i> tint?<p>You <i>cannot</i> accurately colourise skin from photographic film without an _enormous_ amount of knowledge of the taking and processing of the film, and of the lighting and subject.<p>An AI can&#x27;t do it any better than a painter. You can&#x27;t take a scan of a print or a negative and get skin tones right.<p>Think about how weird the skin tones are from scans of wet-plate photography plates compared to the same process used in antiquity with the aim of producing a carbon print.</div><br/><div id="36992375" class="c"><input type="checkbox" id="c-36992375" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36992004">parent</a><span>|</span><a href="#36992383">next</a><span>|</span><label class="collapse" for="c-36992375">[-]</label><label class="expand" for="c-36992375">[3 more]</label></div><br/><div class="children"><div class="content">&gt;<i>Do they? A certain tint?</i><p>Yes. There&#x27;s just not a single one across all faces - but I wasn&#x27;t meaning that.<p>What I mean is, we know the kind of tints a face will have. A face is not suddenly going to be blue or green or poppy red. And by how light a black and white face appears, we can tell quite well if it&#x27;s a darker one (oilish to brown) or lighter (pinkish towards more pale).<p>If we get it wrong within a range it&#x27;s no big deal. Color film stocks would also vary it widely.<p>Hell, even actual people who met the person we colourise in real life will remember (or even experience in real time) their face&#x27;s hue somewhat differently each.</div><br/><div id="36992465" class="c"><input type="checkbox" id="c-36992465" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36992375">parent</a><span>|</span><a href="#36992383">next</a><span>|</span><label class="collapse" for="c-36992465">[-]</label><label class="expand" for="c-36992465">[2 more]</label></div><br/><div class="children"><div class="content">But how brown? How pink? How light? How dark?<p>This is an enormously important issue.<p>Black and white films of different technologies and manufacturers and eras actually lighten or darken skin tones. Really <i>very</i> significantly.<p>And it&#x27;s not going to be obvious from the final positive, unless there&#x27;s _extensive_ data with those images about how the photography was done. And there never is.<p>Editing because I can no longer reply: the question of whether a skin tone is a dark one or a light one has had severe real life impacts on people whose lives are now only represented in photographs. You can&#x27;t write this off as micromanagement; it&#x27;s about the ethics of representation.</div><br/><div id="36992549" class="c"><input type="checkbox" id="c-36992549" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36992465">parent</a><span>|</span><a href="#36992383">next</a><span>|</span><label class="collapse" for="c-36992549">[-]</label><label class="expand" for="c-36992549">[1 more]</label></div><br/><div class="children"><div class="content">&gt;<i>But how brown? How pink? How light? How dark? This is an enormously important issue</i><p>Is it?<p>If 2 colour film stocks took the same image of them, it would show their hue a little (or a lot) different.<p>Even if two different people actually met the same person, they will probably describe their face as slightly different tones from memory. (And let&#x27;s not even get into different types of color-blindness they could have had).<p>Hell, a person&#x27;s hue will even look different to the same person looking at them, in real time, depending on the changes in lighting and the shade at the scene as they talk (e.g. sun behind clouds vs directly sun vs shade vs bulbs).<p>It&#x27;s not really &quot;enormously important&quot; to micromanage the (non-existent) exact right brown or right pink.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36992383" class="c"><input type="checkbox" id="c-36992383" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36991814">parent</a><span>|</span><a href="#36991891">prev</a><span>|</span><a href="#36992755">next</a><span>|</span><label class="collapse" for="c-36992383">[-]</label><label class="expand" for="c-36992383">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Doing it by hand (in photoshop or on a print) acknowledges the inherent bias that is involved in colourisation.<p>No, doing it by hand doesn&#x27;t acknowledge that your interpretation is a fallible interpretation shaped by bias, just like translating a written work (e.g., the Bible, for a noted example where this has been done often without any such acknowledgement being conveyed) by human effort doesn’t do that.<p>Acknowledging bias in translation of either kind is <i>an entirely separate action</i>, orthogonal to the method of the translation itself.</div><br/></div></div><div id="36992755" class="c"><input type="checkbox" id="c-36992755" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36991814">parent</a><span>|</span><a href="#36992383">prev</a><span>|</span><a href="#36992138">next</a><span>|</span><label class="collapse" for="c-36992755">[-]</label><label class="expand" for="c-36992755">[1 more]</label></div><br/><div class="children"><div class="content">Fair enough. Honestly this was just a fun side project. I actually coded this up last october when I was doing a deep dive to learn about diffusion models, and saw that no one had ever applied them to colorization. This was just a fun opportunity to build a project that no one had done before</div><br/></div></div><div id="36992138" class="c"><input type="checkbox" id="c-36992138" checked=""/><div class="controls bullet"><span class="by">geon</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36991814">parent</a><span>|</span><a href="#36992755">prev</a><span>|</span><a href="#36992935">next</a><span>|</span><label class="collapse" for="c-36992138">[-]</label><label class="expand" for="c-36992138">[3 more]</label></div><br/><div class="children"><div class="content">How can it affect the lightness channel when it is locked?</div><br/><div id="36994373" class="c"><input type="checkbox" id="c-36994373" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36992138">parent</a><span>|</span><a href="#36992529">next</a><span>|</span><label class="collapse" for="c-36994373">[-]</label><label class="expand" for="c-36994373">[1 more]</label></div><br/><div class="children"><div class="content">So we&#x27;re using a color space that has two channels dedicated entirely to color, which is the only thing the model needs to learn.<p>The model doesn&#x27;t need to touch the lightness channel at all, only predict the noised added to the color channels at train time.<p>At inference time, we start with a real lightness channel (b&#x2F;w image), and initialize the color channels to random noise. The model iteratively denoises the color channels while keeping the lightness channel locked.</div><br/></div></div><div id="36992529" class="c"><input type="checkbox" id="c-36992529" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36992138">parent</a><span>|</span><a href="#36994373">prev</a><span>|</span><a href="#36992935">next</a><span>|</span><label class="collapse" for="c-36992529">[-]</label><label class="expand" for="c-36992529">[1 more]</label></div><br/><div class="children"><div class="content">The point is that the source black and white image is not truthful about skin colour. The film locks in a level of lightness but that lightness may be very wrong (depending on the red and blue sensitivity of the film, the colour of the light, the time of day, the print, whether a filter was being sued).<p>So if you colourise an image of someone who appears to be a light-skinned 1930s African-American with colours that appear to conform to our contemporary understanding of light-skinned Black people of our era, you might be getting it right, of course.<p>But you might be getting it quite, quite wrong, in a way that matters.</div><br/></div></div></div></div><div id="36992935" class="c"><input type="checkbox" id="c-36992935" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36991814">parent</a><span>|</span><a href="#36992138">prev</a><span>|</span><a href="#36992122">next</a><span>|</span><label class="collapse" for="c-36992935">[-]</label><label class="expand" for="c-36992935">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Automating it is banal at best and dangerous at worst; colourised images risk distorting history.<p>There&#x27;s a lot of irony in acknowledging this but not acknowledging that each and everyone of us has their own biases inherent to our perception and experiences.<p>Like the blue and white dress; we all perceive things differently even on identical images, monitors, screens, etc.</div><br/><div id="36994063" class="c"><input type="checkbox" id="c-36994063" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36992935">parent</a><span>|</span><a href="#36992122">next</a><span>|</span><label class="collapse" for="c-36994063">[-]</label><label class="expand" for="c-36994063">[2 more]</label></div><br/><div class="children"><div class="content">My post you are replying to contains the sentence &quot;Doing it by hand (in photoshop or on a print) acknowledges the inherent bias that is involved in colourisation&quot;.</div><br/><div id="36996878" class="c"><input type="checkbox" id="c-36996878" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36994063">parent</a><span>|</span><a href="#36992122">next</a><span>|</span><label class="collapse" for="c-36996878">[-]</label><label class="expand" for="c-36996878">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the point; you acknowledge the inherent bias in constructing, but not the bias in observing.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36992122" class="c"><input type="checkbox" id="c-36992122" checked=""/><div class="controls bullet"><span class="by">geon</span><span>|</span><a href="#36991688">parent</a><span>|</span><a href="#36991770">prev</a><span>|</span><a href="#36992301">next</a><span>|</span><label class="collapse" for="c-36992122">[-]</label><label class="expand" for="c-36992122">[2 more]</label></div><br/><div class="children"><div class="content">How was anything destroyed? the original grayscale is still there.</div><br/><div id="36992340" class="c"><input type="checkbox" id="c-36992340" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36992122">parent</a><span>|</span><a href="#36992301">next</a><span>|</span><label class="collapse" for="c-36992340">[-]</label><label class="expand" for="c-36992340">[1 more]</label></div><br/><div class="children"><div class="content">Colourised images absolutely replace mono images in image searches, unfortunately; I&#x27;ve seen this again and again. It gets more difficult to find originals.<p>But also you have to consider that bias is being introduced in the colour rendition. That causes damage.<p>For example, you could see a photograph of an African American woman in the 20s or 30s, and your AI would say, this is an African American woman and colour her skin in some way.<p>But a lighter-skinned-looking African American woman in a pre&#x2F;early-post-war photo is a challenge. She may have had darker skin -- been unable to &quot;pass&quot; -- and the film simply didn&#x27;t get that across because of its colour sensitivity.<p>Or she may actually have been light-skinned and able to &quot;pass&quot; (or wearing makeup that helped).<p>Automatically colouring that image introduces risks to the reading of history; you can read that woman&#x27;s entire life completely wrong.<p>It&#x27;s also common with photos of men from that era who worked outdoors. Many of them will come across much darker-skinned in photos than they actually would have appeared in real life, because not-readily-visible sun damage can look odd in mono. But if you colourise all those sun-baked people the same way, what happens to those of mixed heritage among them? (A thing that is already rather &quot;airbrushed out&quot; of history.)<p>Without knowing about the lighting, the material, the processing and the source of the positive (is it a negative scan? was it a good one? or is it a scan of a print?) you cannot make accurate impressions of skin tone.<p>And given the power and importance of photography in the history of the USA in particular -- photography coincides with and actually helps define the modern unified US self-image -- this is not something to blaze through without care.<p>This is a far less tricky problem in more homogeneous societies, obviously. But even then, there is this perception from photographs that British women in the 1920s were all deathly pale; colourisation preserves that illusion that actually comes in part from photographic style.</div><br/></div></div></div></div><div id="36992301" class="c"><input type="checkbox" id="c-36992301" checked=""/><div class="controls bullet"><span class="by">pkoiralap</span><span>|</span><a href="#36991688">parent</a><span>|</span><a href="#36992122">prev</a><span>|</span><a href="#36991832">next</a><span>|</span><label class="collapse" for="c-36992301">[-]</label><label class="expand" for="c-36992301">[3 more]</label></div><br/><div class="children"><div class="content">Making music without actually knowing anything about it is the banal apotheosis application of Generative AI. - Music nerd in me<p>Creating art without actually knowing anything about it is the banal apotheosis application of Diffusion AI. - Artist in me<p>Using ChatGPT to write essays that are better than anyone could have ever written is the banal apotheosis application of LLMs - Teacher in me<p>It is already here. Better use, appreciate, and try to understand how it works rather than complaining about it doing a better job.
In this instance, for example, the model can be made to generate multiple outputs or even better, generate output based on precise user input.</div><br/><div id="36994415" class="c"><input type="checkbox" id="c-36994415" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36992301">parent</a><span>|</span><a href="#36992622">next</a><span>|</span><label class="collapse" for="c-36994415">[-]</label><label class="expand" for="c-36994415">[1 more]</label></div><br/><div class="children"><div class="content">I work at krea.ai. We are for sure making art extremely accessible, but we consider it enhancing creativity rather than replacing.<p>I fully agree that being able to generate an aesthetically pleasing image with an AI that has been optimized to do exactly that is a banal application of creativity.<p>I do think that AI has incredible potential to make (and become art).<p>The best AI artists don&#x27;t just throw art into midjourney, they experiment, create their own secret sauce.<p>Training models has become an art form in and of itself: ai artists curate incredible datasets and devise recipes for training stunning models. Their workflows span multiple companies &#x2F; tools &#x2F; models.<p>AI just means that the goalposts for creativity are shifting. Boring people will use AI to make boring art, artists will find completely unexpected ways to use the tools we build to create art forms we&#x27;ve never imagined before.</div><br/></div></div><div id="36992622" class="c"><input type="checkbox" id="c-36992622" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36992301">parent</a><span>|</span><a href="#36994415">prev</a><span>|</span><a href="#36991832">next</a><span>|</span><label class="collapse" for="c-36992622">[-]</label><label class="expand" for="c-36992622">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m actually concerned it is doing a <i>worse</i> job, in important ethical ways, than a hand colourist. But I&#x27;ve explained elsewhere.<p>Colourisation cannot be done accurately from a black and white image without context that is almost always lacking. Hand colouring is <i>less</i> dishonest.</div><br/></div></div></div></div><div id="36991832" class="c"><input type="checkbox" id="c-36991832" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#36991688">parent</a><span>|</span><a href="#36992301">prev</a><span>|</span><a href="#36991720">next</a><span>|</span><label class="collapse" for="c-36991832">[-]</label><label class="expand" for="c-36991832">[4 more]</label></div><br/><div class="children"><div class="content">When did colorizing images become an &quot;art&quot;?<p>What if the &quot;effort&quot; way is less accurate?</div><br/><div id="36991940" class="c"><input type="checkbox" id="c-36991940" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36991832">parent</a><span>|</span><a href="#36991862">next</a><span>|</span><label class="collapse" for="c-36991940">[-]</label><label class="expand" for="c-36991940">[2 more]</label></div><br/><div class="children"><div class="content">The effort is obviously going to be less accurate.<p>But it reflects the fact that an accurate colourisation of a black and white image without access to every possible detail about the scene and processing from the photographer&#x27;s perspective is impossible.<p>Black and white film is substantially more complex and varied than people understand. Its sensitivities are complex and vary from processing run to processing run, and people at the time knew of the weaknesses of black and white and often used false colour to get an acceptable rendition.<p>Colourisation is a form of expression, not a form of recovery.</div><br/><div id="36992457" class="c"><input type="checkbox" id="c-36992457" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36991940">parent</a><span>|</span><a href="#36991862">next</a><span>|</span><label class="collapse" for="c-36992457">[-]</label><label class="expand" for="c-36992457">[1 more]</label></div><br/><div class="children"><div class="content">&gt;<i>But it reflects the fact that an accurate colourisation of a black and white image without access to every possible detail about the scene and processing from the photographer&#x27;s perspective is impossible.</i><p>Accurate colourisation is impossible even in a color photograph. There is no &quot;canonical&quot; film stock that accurately represents all actual real-life colors.<p>The expectation from colourisation is not an accurate representation of the original colors, but a good application of color based on our knowledge (whether from historical facts a human colorist knows or from training with similar objects and materials a NN did) that matches a realistic representation of the scene.<p>If a human colourist draws a dress and doesn&#x27;t know the color of it, nor have they any historical information about what the person depicted wore that day, they&#x27;re going to take a guess. That&#x27;s kind of what the NN will do as well.</div><br/></div></div></div></div><div id="36991862" class="c"><input type="checkbox" id="c-36991862" checked=""/><div class="controls bullet"><span class="by">vorpalhex</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36991832">parent</a><span>|</span><a href="#36991940">prev</a><span>|</span><a href="#36991720">next</a><span>|</span><label class="collapse" for="c-36991862">[-]</label><label class="expand" for="c-36991862">[1 more]</label></div><br/><div class="children"><div class="content">There is a community of people who carefully recolor historical photos by hand. It&#x27;s really beautiful time consuming work and often they invest heavily to get the colors to be correct.</div><br/></div></div></div></div><div id="36991720" class="c"><input type="checkbox" id="c-36991720" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36991688">parent</a><span>|</span><a href="#36991832">prev</a><span>|</span><a href="#36992174">next</a><span>|</span><label class="collapse" for="c-36991720">[-]</label><label class="expand" for="c-36991720">[2 more]</label></div><br/><div class="children"><div class="content">touché, nevertheless, colors go brrrrrrrr</div><br/><div id="36991774" class="c"><input type="checkbox" id="c-36991774" checked=""/><div class="controls bullet"><span class="by">bemusedthrow75</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36991720">parent</a><span>|</span><a href="#36992174">next</a><span>|</span><label class="collapse" for="c-36991774">[-]</label><label class="expand" for="c-36991774">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t get me wrong. It&#x27;s impressive technology. I&#x27;m amazed at what it can do.<p>Also horrified.</div><br/></div></div></div></div><div id="36992174" class="c"><input type="checkbox" id="c-36992174" checked=""/><div class="controls bullet"><span class="by">mrkeen</span><span>|</span><a href="#36991688">parent</a><span>|</span><a href="#36991720">prev</a><span>|</span><a href="#36991849">next</a><span>|</span><label class="collapse" for="c-36992174">[-]</label><label class="expand" for="c-36992174">[1 more]</label></div><br/><div class="children"><div class="content">Nice, I&#x27;ll have to try smashing vintage glass windows.  Thanks for the tip!</div><br/></div></div><div id="36991849" class="c"><input type="checkbox" id="c-36991849" checked=""/><div class="controls bullet"><span class="by">vorpalhex</span><span>|</span><a href="#36991688">parent</a><span>|</span><a href="#36992174">prev</a><span>|</span><label class="collapse" for="c-36991849">[-]</label><label class="expand" for="c-36991849">[2 more]</label></div><br/><div class="children"><div class="content">I think a lot of it depends on what you are doing and why.<p>Yes, recolors can be inaccurate but they can make historical moments feel more alive and connected. At the same time one can imagine the issues of a recolor that is inaccurate and that is troubling with historical photographs.<p>At the same time I have a bunch of old family photos I&#x27;d love to recolorize. Maybe the colors won&#x27;t be quite right but that&#x27;s an OK failure mode for family photos!<p>I&#x27;d love to see a version where you can drop just a spot or two of the correct color and let the AI fill it out. My grandmother had stark red hair but most algorithms will color her as a blond. It&#x27;d be nice to fix that, using one of the color photos we do have.</div><br/><div id="36992454" class="c"><input type="checkbox" id="c-36992454" checked=""/><div class="controls bullet"><span class="by">erwannmillon</span><span>|</span><a href="#36991688">root</a><span>|</span><a href="#36991849">parent</a><span>|</span><label class="collapse" for="c-36992454">[-]</label><label class="expand" for="c-36992454">[1 more]</label></div><br/><div class="children"><div class="content">You can do this with spatial palette t2i or controlnet. Give a super lores spatial palette as conditioning like this:
<a href="https:&#x2F;&#x2F;camo.githubusercontent.com&#x2F;8e488996fd309165fb065b0cdef17e55d68b1a2e9ea9ecfaee91ebb7ee38ca58&#x2F;68747470733a2f2f68756767696e67666163652e636f2f54656e63656e744152432f5432492d416461707465722f7265736f6c76652f6d61696e2f6173736574732f636f6c6f722e706e67" rel="nofollow noreferrer">https:&#x2F;&#x2F;camo.githubusercontent.com&#x2F;8e488996fd309165fb065b0cd...</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;TencentARC&#x2F;T2I-Adapter">https:&#x2F;&#x2F;github.com&#x2F;TencentARC&#x2F;T2I-Adapter</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>