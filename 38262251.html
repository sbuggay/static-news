<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700038872302" as="style"/><link rel="stylesheet" href="styles.css?v=1700038872302"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://tratt.net/laurie/blog/2023/four_kinds_of_optimisation.html">Four Kinds of Optimisation</a> <span class="domain">(<a href="https://tratt.net">tratt.net</a>)</span></div><div class="subtext"><span>ltratt</span> | <span>38 comments</span></div><br/><div><div id="38271189" class="c"><input type="checkbox" id="c-38271189" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#38270895">next</a><span>|</span><label class="collapse" for="c-38271189">[-]</label><label class="expand" for="c-38271189">[12 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s difficult to measure the indirect impact of things like memory locality — I have heard such factors blamed for poor performance much more often than I have seen such factors proven as responsible for poor performance<p>This particular assertion does not seem to be well-founded. The importance of spatial and temporal locality to software performance on modern hardware is a singular obsession of data-intensive software architectures. Nor is it difficult to measure, these are some of the highest impact optimizations you can make assuming the code isn&#x27;t naive. There are perf counters and such but the majority of poor locality is visible via simple code inspection. You don&#x27;t need perf counters until you are doing serious performance engineering.<p>Locality optimization tends to be architectural in nature. If you do not design your software to have good locality characteristics then it will be complex to fix later. An argument can be made that it isn&#x27;t worth the cost to fix software designs with poor locality after the fact, but the centrality of locality to software optimization is not controversial.</div><br/><div id="38274011" class="c"><input type="checkbox" id="c-38274011" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#38271189">parent</a><span>|</span><a href="#38274219">next</a><span>|</span><label class="collapse" for="c-38274011">[-]</label><label class="expand" for="c-38274011">[1 more]</label></div><br/><div class="children"><div class="content">The first memorable locality problem I ever solved, I had clocked a function at 10% overall cpu time. The more I looked at it, the more I realized we were calling it way too often. It wasn’t allocating a lot of memory, it was just crunching a bit of data.<p>A bit of jiggling of the call structure above it and it goes from 2 invocations per call to one. So almost exactly half as many calls overall. I ran my end to end benchmark again, expecting 4-6% improvement in run time. I got 20%. Twice what I would have expected to get by commenting the function out entirely.<p>Few years later, similar situation. Operation taking about 5 times as long as we were allowed. Function A gets a list from the database. Function B gets the same list. I don’t recall what percent of the overall time this was, but it was a tall tent pole and obvious waste. Something like 30%. As a first course for a longer optimization engagement, I changed the method signatures to take the list and return a result, looked up the list once in the common parent function. Best case I figured a 20% reduction, which is still slow enough I can use the stopwatch function on my phone to estimate the improvement at the UI. I didn’t get 20%, I got 90%. So I went from 5x over the target to half, in one change. Unfortunately even this result didn’t convince some people they were barking up the wrong tree with respect to performance problems.<p>Right about the time of the first experience I had been learning an ugly truth: profilers lie. They aren’t the floodlight people think they are. They’re a book of matches in the dark. They reveal part of the picture, and cast as many shadows as they do light. You have to fill in a lot of gaps yourself, and that takes mechanical sympathy and more than a little stubbornness. Hardware counters narrow that gap a lot, but they still aren’t perfect, and a lot of languages seem not to use them.</div><br/></div></div><div id="38274219" class="c"><input type="checkbox" id="c-38274219" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#38271189">parent</a><span>|</span><a href="#38274011">prev</a><span>|</span><a href="#38271526">next</a><span>|</span><label class="collapse" for="c-38274219">[-]</label><label class="expand" for="c-38274219">[3 more]</label></div><br/><div class="children"><div class="content">Most code is naive. FWIW, in my experience most people who just look around for poor code locality focus on things that don&#x27;t actually matter, while the most expensive misses (when you actually profile and decide you care about them) end up in surprising places, or they were just missed in someone&#x27;s quest to reorder all the structs they see in the codebase.</div><br/><div id="38274266" class="c"><input type="checkbox" id="c-38274266" checked=""/><div class="controls bullet"><span class="by">meheleventyone</span><span>|</span><a href="#38271189">root</a><span>|</span><a href="#38274219">parent</a><span>|</span><a href="#38271526">next</a><span>|</span><label class="collapse" for="c-38274266">[-]</label><label class="expand" for="c-38274266">[2 more]</label></div><br/><div class="children"><div class="content">It’s memory not code locality (do you mean in the instruction cache or something else?) that matters. The problem for a naive solution with poor memory locality is that the memory locality issue is more akin to a rewrite to a faster language. The performance gap left on the table can be significant but the need for it not show up in profiles because everything is slow.</div><br/><div id="38274362" class="c"><input type="checkbox" id="c-38274362" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#38271189">root</a><span>|</span><a href="#38274266">parent</a><span>|</span><a href="#38271526">next</a><span>|</span><label class="collapse" for="c-38274362">[-]</label><label class="expand" for="c-38274362">[1 more]</label></div><br/><div class="children"><div class="content">That is very often the case, yes.</div><br/></div></div></div></div></div></div><div id="38271526" class="c"><input type="checkbox" id="c-38271526" checked=""/><div class="controls bullet"><span class="by">anonymoushn</span><span>|</span><a href="#38271189">parent</a><span>|</span><a href="#38274219">prev</a><span>|</span><a href="#38273311">next</a><span>|</span><label class="collapse" for="c-38271526">[-]</label><label class="expand" for="c-38271526">[2 more]</label></div><br/><div class="children"><div class="content">Well, it&#x27;s a lot of work to prove to someone who doesn&#x27;t do this sort of thing that they have this sort of problem. And it&#x27;s a lot less work to say that there is this sort of problem. So he sounds right to me.</div><br/><div id="38274319" class="c"><input type="checkbox" id="c-38274319" checked=""/><div class="controls bullet"><span class="by">meheleventyone</span><span>|</span><a href="#38271189">root</a><span>|</span><a href="#38271526">parent</a><span>|</span><a href="#38273311">next</a><span>|</span><label class="collapse" for="c-38274319">[-]</label><label class="expand" for="c-38274319">[1 more]</label></div><br/><div class="children"><div class="content">A nice easy one is to benchmark trivial examples for people. For example summing an array of numbers in JS. Have examples of four levels of indirection between objects down to one level, straight numbers in a standard JS array and the same in a TypedArray. This pretty clearly demonstrates the gains that can be had.</div><br/></div></div></div></div><div id="38273311" class="c"><input type="checkbox" id="c-38273311" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#38271189">parent</a><span>|</span><a href="#38271526">prev</a><span>|</span><a href="#38271296">next</a><span>|</span><label class="collapse" for="c-38273311">[-]</label><label class="expand" for="c-38273311">[2 more]</label></div><br/><div class="children"><div class="content">You need perf measurements to understand the impact. It sounds like you&#x27;re saying that it&#x27;s a massive time stick to fix the problems, so it had better be a measurable impact to understand the ROI.<p>I&#x27;ve definitely seen careful memory management make a huge difference in custom RNN code, so don&#x27;t doubt it can make a difference, but I expect it matters mainly when it&#x27;s part of a very tight loop.</div><br/><div id="38273808" class="c"><input type="checkbox" id="c-38273808" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#38271189">root</a><span>|</span><a href="#38273311">parent</a><span>|</span><a href="#38271296">next</a><span>|</span><label class="collapse" for="c-38273808">[-]</label><label class="expand" for="c-38273808">[1 more]</label></div><br/><div class="children"><div class="content">Unlike some other optimizations, the effects of locality optimization are usually not local. The reduction in cache and bandwidth pressure makes everything run better. These are common optimizations in e.g. databases, even shaving a few bytes off of key data structures can result in system-wide gains of 5-10% performance just from the cache pressure reduction resulting from a seemingly innocuous reduction in data size. You don’t need to measure it to know it will have impact, the relationship between reducing cache pressure and performance is pretty direct and repeatable. Spatial locality is straightforward, temporal locality is a bigger engineering commitment.<p>A canonical example of locality optimization is thread-per-core architectures versus classic multi-threading architectures. By random luck, I was around when thread-per-core architectures were first developed to improve efficiency on many-core processors (on supercomputers). Instant 10x performance improvement on the same hardware. By comparison, aggressive vectorization with AVX-512 might buy you 3x improvement. Locality optimizations are the highest leverage optimization we can do today. It does require disciplined software architecture.<p>There are limits to locality optimization in real systems. If you are doing a good job of it, you frequently run out of memory bandwidth at some point. That’s a pretty good outcome.</div><br/></div></div></div></div><div id="38271296" class="c"><input type="checkbox" id="c-38271296" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#38271189">parent</a><span>|</span><a href="#38273311">prev</a><span>|</span><a href="#38270895">next</a><span>|</span><label class="collapse" for="c-38271296">[-]</label><label class="expand" for="c-38271296">[3 more]</label></div><br/><div class="children"><div class="content">Anything that can&#x27;t be objectively measured will inherently be controversial. You claim you can see poor locality via code inspection; will other people who inspect the same code reach the same conclusions? Why should we believe you rather than anyone else?</div><br/><div id="38271573" class="c"><input type="checkbox" id="c-38271573" checked=""/><div class="controls bullet"><span class="by">jmoss20</span><span>|</span><a href="#38271189">root</a><span>|</span><a href="#38271296">parent</a><span>|</span><a href="#38270895">next</a><span>|</span><label class="collapse" for="c-38271573">[-]</label><label class="expand" for="c-38271573">[2 more]</label></div><br/><div class="children"><div class="content">Agreed with the general point, but it doesn&#x27;t apply here. Memory locality can be objectively measured (e.g. with last level cache miss counters), and parent comment is correct besides -- it&#x27;s usually plain to see in the code.<p>There are mysterious boogiemen in performance optimization, but this isn&#x27;t really one of them.</div><br/><div id="38274697" class="c"><input type="checkbox" id="c-38274697" checked=""/><div class="controls bullet"><span class="by">mejutoco</span><span>|</span><a href="#38271189">root</a><span>|</span><a href="#38271573">parent</a><span>|</span><a href="#38270895">next</a><span>|</span><label class="collapse" for="c-38274697">[-]</label><label class="expand" for="c-38274697">[1 more]</label></div><br/><div class="children"><div class="content">I am happy (good) science does take the is obvious claim as sufficient, and instead focuses on proving things with objective facts.<p>I am not saying these cannot be plain to see in the code, but the best standard IMO is still to measure before and after the optimization. IMHO, again, you can skip that step, but then other people might rightfully ask you what proof you have that the optimization is faster (I would).</div><br/></div></div></div></div></div></div></div></div><div id="38270895" class="c"><input type="checkbox" id="c-38270895" checked=""/><div class="controls bullet"><span class="by">webnrrd2k</span><span>|</span><a href="#38271189">prev</a><span>|</span><a href="#38274366">next</a><span>|</span><label class="collapse" for="c-38270895">[-]</label><label class="expand" for="c-38270895">[4 more]</label></div><br/><div class="children"><div class="content">Its a good post, but I&#x27;d add that there isvalso a different kind of optimization where you optimize by minimizing th overall cognitive load of any given code.<p>For example, it&#x27;s important to develop a feel for data structures. Often, if you choose the right data structure, the algorithm is fairly easy. In other words, when faced with a choice of complex algorithm vs complex data structures, it&#x27;s generally easier to work with a complex data structure and use a simpler algorithm.<p>Data is fairly static, and therefore easier to reason about, vs a complex and &quot;dynamic&quot; algorithm. Optimization for ease of reading and maintenance is also really important.</div><br/><div id="38270981" class="c"><input type="checkbox" id="c-38270981" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#38270895">parent</a><span>|</span><a href="#38274366">next</a><span>|</span><label class="collapse" for="c-38270981">[-]</label><label class="expand" for="c-38270981">[3 more]</label></div><br/><div class="children"><div class="content">A great example of this is Norvig’s sudoku solver. Arranging the data the right way makes solving the problem a mere matter of correct bookkeeping .</div><br/><div id="38271651" class="c"><input type="checkbox" id="c-38271651" checked=""/><div class="controls bullet"><span class="by">keyle</span><span>|</span><a href="#38270895">root</a><span>|</span><a href="#38270981">parent</a><span>|</span><a href="#38271304">next</a><span>|</span><label class="collapse" for="c-38271651">[-]</label><label class="expand" for="c-38271651">[1 more]</label></div><br/><div class="children"><div class="content">ref. <a href="https:&#x2F;&#x2F;norvig.com&#x2F;sudoku.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;norvig.com&#x2F;sudoku.html</a></div><br/></div></div><div id="38271304" class="c"><input type="checkbox" id="c-38271304" checked=""/><div class="controls bullet"><span class="by">webnrrd2k</span><span>|</span><a href="#38270895">root</a><span>|</span><a href="#38270981">parent</a><span>|</span><a href="#38271651">prev</a><span>|</span><a href="#38274366">next</a><span>|</span><label class="collapse" for="c-38271304">[-]</label><label class="expand" for="c-38271304">[1 more]</label></div><br/><div class="children"><div class="content">Yes, great example. I forgot about that one.</div><br/></div></div></div></div></div></div><div id="38274366" class="c"><input type="checkbox" id="c-38274366" checked=""/><div class="controls bullet"><span class="by">lifthrasiir</span><span>|</span><a href="#38270895">prev</a><span>|</span><a href="#38272434">next</a><span>|</span><label class="collapse" for="c-38274366">[-]</label><label class="expand" for="c-38274366">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Exactly what constitutes &quot;correct&quot; varies from one situation to another. For example, fast inverse square root approximates multiplicative inverse: for situations such as games, its fast nearly-correct answer is a better trade-off than a slow definitely-correct answer.<p>Amazingly in topic, the famous constant 0x5f3759df from this algorithm was also thought to be derived from an approximate search algorithm because you need ~2^31 calls to determine the maximal relative error for a <i>single</i> constant---a big deal back in 1980s. A better constant 0x5f375a86 was only found in 2003, and took a few more years to be proven optimal (Robertson 2012, Moroz et al. 2016).</div><br/></div></div><div id="38272434" class="c"><input type="checkbox" id="c-38272434" checked=""/><div class="controls bullet"><span class="by">from-nibly</span><span>|</span><a href="#38274366">prev</a><span>|</span><a href="#38273795">next</a><span>|</span><label class="collapse" for="c-38272434">[-]</label><label class="expand" for="c-38272434">[3 more]</label></div><br/><div class="children"><div class="content">He forgot the most powerful optimization of all. Redirecting to &#x2F;dev&#x2F;null. Or in other words just not doing it in the first place.<p>As dumb as it sounds it&#x27;s really easy to get caught up in optimizing stuff you just flat out don&#x27;t need to do.</div><br/><div id="38272698" class="c"><input type="checkbox" id="c-38272698" checked=""/><div class="controls bullet"><span class="by">howenterprisey</span><span>|</span><a href="#38272434">parent</a><span>|</span><a href="#38274062">next</a><span>|</span><label class="collapse" for="c-38272698">[-]</label><label class="expand" for="c-38272698">[1 more]</label></div><br/><div class="children"><div class="content">Very true. At work we have &quot;the first and most important thing to optimize is the requirements&quot; as a principle and it works well.</div><br/></div></div><div id="38274062" class="c"><input type="checkbox" id="c-38274062" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#38272434">parent</a><span>|</span><a href="#38272698">prev</a><span>|</span><a href="#38273795">next</a><span>|</span><label class="collapse" for="c-38274062">[-]</label><label class="expand" for="c-38274062">[1 more]</label></div><br/><div class="children"><div class="content">The fastest code is the code that never runs.</div><br/></div></div></div></div><div id="38273795" class="c"><input type="checkbox" id="c-38273795" checked=""/><div class="controls bullet"><span class="by">YoshiRulz</span><span>|</span><a href="#38272434">prev</a><span>|</span><a href="#38274029">next</a><span>|</span><label class="collapse" for="c-38273795">[-]</label><label class="expand" for="c-38273795">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I think that, in general, most programmers struggle to accept that correctness can sometimes be traded-off—personally, it offends a deep internal conviction of mine that programs should be correct. [...] possible incorrectness more often causes problems. I might be happy trading off a bit of image-quality for better compression, but if an ML system rewrites my code and leaves off a &quot;not&quot; I&#x27;m unhappy.<p>I&#x27;m torn on this point. On the one hand, I&#x27;m as fallible as any human, and reading this has made me aware of a bias I&#x27;ve always had but never realised was there.
But on the other hand, our field is young, and there are surely better, correct solutions to be found for things like compression and sorting. So with that assumption, I find it tempting to label programmers who trade correctness for speed or size (e.g. using JPEG when QOI and WebP exist) as &#x27;lazy&#x27;, since &quot;they haven&#x27;t exhausted all other options&quot;. Obviously that&#x27;s not fair—it ignores practicality and trivialises the discovery of novel algorithms—but I feel there&#x27;s still some truth in it.</div><br/></div></div><div id="38274029" class="c"><input type="checkbox" id="c-38274029" checked=""/><div class="controls bullet"><span class="by">matheusmoreira</span><span>|</span><a href="#38273795">prev</a><span>|</span><a href="#38272137">next</a><span>|</span><label class="collapse" for="c-38274029">[-]</label><label class="expand" for="c-38274029">[1 more]</label></div><br/><div class="children"><div class="content">The linked paper about PyPy&#x27;s homogeneous collections optimization is extremely interesting:<p><a href="https:&#x2F;&#x2F;tratt.net&#x2F;laurie&#x2F;research&#x2F;pubs&#x2F;html&#x2F;bolz_diekmann_tratt__storage_strategies_for_collections_in_dynamically_typed_languages&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;tratt.net&#x2F;laurie&#x2F;research&#x2F;pubs&#x2F;html&#x2F;bolz_diekmann_tr...</a><p>Would be nice to have a collection of such techniques for increasing performance sorted by effort required...</div><br/></div></div><div id="38272137" class="c"><input type="checkbox" id="c-38272137" checked=""/><div class="controls bullet"><span class="by">orf</span><span>|</span><a href="#38274029">prev</a><span>|</span><a href="#38263586">next</a><span>|</span><label class="collapse" for="c-38272137">[-]</label><label class="expand" for="c-38272137">[2 more]</label></div><br/><div class="children"><div class="content">&gt; For anything but the smallest lists [6], binary search is much quicker than the linear search above.<p>&gt; In my experience, binary search becomes faster than linear search after about 8-12 elements<p>It depends on your data, but CPUs are <i>extremely</i> good at scanning through contiguous vectors. Binary searching might make a difference at tens of thousands of elements.<p>If you’re using Python then it’s different as nothing is contiguous, or you have a complex equality function, but then a simple improvement is to put the data in a numpy array of dataframe.</div><br/><div id="38274148" class="c"><input type="checkbox" id="c-38274148" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#38272137">parent</a><span>|</span><a href="#38263586">next</a><span>|</span><label class="collapse" for="c-38274148">[-]</label><label class="expand" for="c-38274148">[1 more]</label></div><br/><div class="children"><div class="content">&gt; or you have a complex equality function<p>This is part of my thesis that Knuth today is wrong to the point of harm. That we need a new complexity theory built on top of Information Theory. We are steering kids wrong more often than right at this point. Data sets we work with are at least six orders of magnitude larger than they were in the eighties. There are almost no comparisons in Real Data that run in constant time, and for big enough n even addition and multiplication are not constant time. For large enough n, storage accesses are sqrt(n). Even in primary storage.<p>Imagine you have a list of a million distinct strings. Case sensitive, alphanumeric. Complexity theory tells us you can sort those in nlogn time. But that’s pure fantasy because n unique strings have an average length of logn, meaning the compares all take logn time. Mergesort for any real data type is n (logn)².  Now sort a quintillion unique strings. That takes 65-70 exabytes of space, just for starters, so it’s going to involve Ethernet cables and NVME RAID arrays. Your mergesort is now in the neighborhood of n^1.5 (log n)². Your piece of paper says this might run in hours, while the real implementation runs for weeks.<p>For most purposes, you should multiply every algorithm in TAOC by logn. In some cases, sqrt(n). And as n actually approaches infinity, multiply by both.<p>From another angle, I think the fact that it took 54 years to get from mergesort to Timsort has more to do with sacred cows than Tim’s brilliance. I’d like to see benchmarks on Pentium hardware, maybe even 486.<p>“If I have not seen further it is because giants are stepping on my toes.”</div><br/></div></div></div></div><div id="38263586" class="c"><input type="checkbox" id="c-38263586" checked=""/><div class="controls bullet"><span class="by">gavinhoward</span><span>|</span><a href="#38272137">prev</a><span>|</span><a href="#38267908">next</a><span>|</span><label class="collapse" for="c-38263586">[-]</label><label class="expand" for="c-38263586">[5 more]</label></div><br/><div class="children"><div class="content">This is a great article, and I have some notes.<p>I initially disagreed with the author when he said, &quot;we tend to overestimate how much we know about the software we&#x27;re working on.&quot;<p>I disagreed because that is <i>not</i> my experience; I do know a lot about my software.<p>However, then he said, &quot;We overemphasise the parts of the system we&#x27;ve personally worked on, particularly those we&#x27;ve most recently worked on. We downplay other parts of the system, including the impact of dependencies (e.g. libraries).&quot;<p>Oh. Um, well, this is <i>also</i> not my experience.<p>I work on software alone, and I do not have any dependencies.<p>So the author is right; I just have weird experience. :)<p>I agree with the four kinds of optimization, especially since he focuses on optimization humans do. I think you could even shoehorn compiler optimizations into those four. For example, strength reduction could be considered a &quot;better algorithm.&quot;<p>His observations about the differences between <i>best</i>, <i>average</i>, and <i>worst</i> cases are the things programmers need to learn most. At some point, our code will <i>always</i> hit bad cases, and we need to know if it will hit it <i>often</i>.<p>I love his example of Timsort because I&#x27;ve analyzed and implemented it myself. It ended being more comments than code. (Of course, that includes two license header comments, so slightly less comments than code actually.)<p>Timsort is pretty complicated, and Tim isn&#x27;t the best at naming things, so it took a bit for me to understand what was going on.<p>(The most prominent example is the galloping functions. He named them &quot;gallop_left&quot; and &quot;gallop_right,&quot; but they both gallop left <i>and</i> right. I renamed them to &quot;gallop2Leftmost&quot; and &quot;gallop2Rightmost,&quot; which is <i>actually</i> what they do.)<p>The author says he biases to writing algorithms and adopting pre-written data structures. This is a good thing. However, after having written data structures (because I work in C, which doesn&#x27;t have them), I would encourage every programmer to write some. Best, average, and worst cases are easier to learn on data structures than algorithms. Still use pre-written ones though; they are usually better optimized.<p>The trick about reducing memory is <i>crucial</i>. I recently spent an entire refactor just reducing the size of some of the most-used structs. I cut the size of one struct in <i>half</i>. `pahole` is great for this.<p>The part of PyPy is a great example too. A great book to read is <i>Is Parallel Programming Hard, And, If So, What Can You Do About It?</i> The entire point of the book is to make you think, &quot;Okay, I need some concurrency or parallelism; what is the easiest way to get it?&quot; The PyPy example is that same thing with optimization; what&#x27;s the easiest way to get the performance you need?<p>Every time you run into an optimization problem, ask that question first, and it will get faster to answer every time. And the author&#x27;s summary reinforces this.<p>[1]: <a href="https:&#x2F;&#x2F;mirrors.edge.kernel.org&#x2F;pub&#x2F;linux&#x2F;kernel&#x2F;people&#x2F;paulmck&#x2F;perfbook&#x2F;perfbook.2022.09.25a.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;mirrors.edge.kernel.org&#x2F;pub&#x2F;linux&#x2F;kernel&#x2F;people&#x2F;paul...</a></div><br/><div id="38270430" class="c"><input type="checkbox" id="c-38270430" checked=""/><div class="controls bullet"><span class="by">dj_mc_merlin</span><span>|</span><a href="#38263586">parent</a><span>|</span><a href="#38267908">next</a><span>|</span><label class="collapse" for="c-38270430">[-]</label><label class="expand" for="c-38270430">[4 more]</label></div><br/><div class="children"><div class="content">&gt; However, then he said, &quot;We overemphasise the parts of the system we&#x27;ve personally worked on, particularly those we&#x27;ve most recently worked on. We downplay other parts of the system, including the impact of dependencies (e.g. libraries).&quot;<p>&gt; Oh. Um, well, this is also not my experience.<p>&gt; I work on software alone, and I do not have any dependencies.<p>If you work in a company of 100+ engineers with multiple specialties it&#x27;s impossible to keep up with everything. At &lt;50 engineers if you collaborate and&#x2F;or switch teams it&#x27;s possible to know almost everything, but that&#x27;s the most I think the majority of us have the internal storage&#x2F;RAM for.</div><br/><div id="38271555" class="c"><input type="checkbox" id="c-38271555" checked=""/><div class="controls bullet"><span class="by">ska</span><span>|</span><a href="#38263586">root</a><span>|</span><a href="#38270430">parent</a><span>|</span><a href="#38270449">next</a><span>|</span><label class="collapse" for="c-38271555">[-]</label><label class="expand" for="c-38271555">[2 more]</label></div><br/><div class="children"><div class="content">I suspect your thresholds are far too high, and that some specialties are almost mutually incomprehensible, so rare for one person to properly understand what is going on.<p>Time is a real factor also.  How long has this code base been accumulating...</div><br/><div id="38272201" class="c"><input type="checkbox" id="c-38272201" checked=""/><div class="controls bullet"><span class="by">dj_mc_merlin</span><span>|</span><a href="#38263586">root</a><span>|</span><a href="#38271555">parent</a><span>|</span><a href="#38270449">next</a><span>|</span><label class="collapse" for="c-38272201">[-]</label><label class="expand" for="c-38272201">[1 more]</label></div><br/><div class="children"><div class="content">Early on in the company I work we had a practice of collaborating across teams, sharing members etc. so a lot of us knew a lot of the code base from &quot;different angles&quot;. If you implemented a backend feature you might write also write the terraform and the frontend that worked with it, with some input from the people experienced in those things. Kind of like random full-stack development? Slower than normal development at first but it makes future collaboration easier. It was quite great, but unsustainable as the amount of people grows.<p>edit: I think in general at the early stages of a company you hire a lot of people who are good at adapting and learning new skills since they might need to fill new roles quickly as new needs arise. So those people are good at this kind of full stack development since it&#x27;s their niche. As you grow, you hire more specialized or junior people, and they can&#x27;t do this process quickly enough for it to feel fluid anymore.</div><br/></div></div></div></div><div id="38270449" class="c"><input type="checkbox" id="c-38270449" checked=""/><div class="controls bullet"><span class="by">gavinhoward</span><span>|</span><a href="#38263586">root</a><span>|</span><a href="#38270430">parent</a><span>|</span><a href="#38271555">prev</a><span>|</span><a href="#38267908">next</a><span>|</span><label class="collapse" for="c-38270449">[-]</label><label class="expand" for="c-38270449">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I said as much in my comment.</div><br/></div></div></div></div></div></div><div id="38267908" class="c"><input type="checkbox" id="c-38267908" checked=""/><div class="controls bullet"><span class="by">llamajams</span><span>|</span><a href="#38263586">prev</a><span>|</span><a href="#38270508">next</a><span>|</span><label class="collapse" for="c-38267908">[-]</label><label class="expand" for="c-38267908">[4 more]</label></div><br/><div class="children"><div class="content">There needs to be more articles like this, far too often the answers on SO and the like is canned and dissimissive; &quot;you don&#x27;t need to optimize because who cares youre writing a crud app anyway&quot;.</div><br/><div id="38270245" class="c"><input type="checkbox" id="c-38270245" checked=""/><div class="controls bullet"><span class="by">armchairhacker</span><span>|</span><a href="#38267908">parent</a><span>|</span><a href="#38270731">next</a><span>|</span><label class="collapse" for="c-38270245">[-]</label><label class="expand" for="c-38270245">[2 more]</label></div><br/><div class="children"><div class="content">The very first CS class taught at my undergraduate, where many students get exposed to programming the very first time and solve the simplest problems in functional style (e.g. list map, fold, fibonacci), introduces problems which require optimizations in the later part.<p>No matter how powerful your computer and how small your computer program is, you have to optimize at least the exponential-time algorithms, and sometimes the high-degree-polynomial ones. When writing a real-time or production app you have to optimize many polynomial or even linear-time algorithms.<p>Micro-optimizations like allocation and boxed numbers? Unnecessary unless you need performance, only apply a constant multiplier to your program’s speed. But macro-optimizations which affect time complexity can’t be ignored.</div><br/><div id="38274316" class="c"><input type="checkbox" id="c-38274316" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#38267908">root</a><span>|</span><a href="#38270245">parent</a><span>|</span><a href="#38270731">next</a><span>|</span><label class="collapse" for="c-38274316">[-]</label><label class="expand" for="c-38274316">[1 more]</label></div><br/><div class="children"><div class="content">Algorithmic time complexity is a very important, but I honestly feel like a typical university education heavily underemphasizes exactly how much the constant factor can matter. Like, consider a simple example: someone&#x27;s hand-written memcpy in C might achieve, say, 500 MB&#x2F;s, maybe 1 GB&#x2F;s if the loop is particularly tight. The one that ships on your system likely does 20-100 GB&#x2F;s. Same big-O on both, of course. Most cases aren&#x27;t this easy but there is a <i>lot</i> of performance that comes from constant factor optimization, which can dwarf all sorts of clever algorithms that are theoretically more efficient. Everyone likes to go &quot;ok well if I scale this to a billion users your algorithm takes a month and mine takes a minute&quot; but it is very likely that between now and the billion users the code is probably not even going to look anywhere near the same. But it&#x27;s likely to be running on the same JVM or the same OS or the same allocator and all of those have been optimized to cut their constant factor down. After all, you don&#x27;t want to be the guy who has the same algorithmic complexity as your competitor but your cloud bill is twice as much.<p>(This isn&#x27;t to say you shouldn&#x27;t do algorithmic improvements, and most of the performance work I do is in fact along those lines, but I do want to clarify that the &quot;micro-optimizations&quot; you&#x27;re talking about are in fact the difference between a computer from today and the 1990s.)</div><br/></div></div></div></div></div></div><div id="38270508" class="c"><input type="checkbox" id="c-38270508" checked=""/><div class="controls bullet"><span class="by">morelisp</span><span>|</span><a href="#38267908">prev</a><span>|</span><a href="#38270274">next</a><span>|</span><label class="collapse" for="c-38270508">[-]</label><label class="expand" for="c-38270508">[2 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s difficult to measure the indirect impact of things like memory locality — I have heard such factors blamed for poor performance much more often than I have seen such factors proven as responsible for poor performance. In general, I only look to such factors when I&#x27;m getting desperate.<p>No thanks.<p>Cache-related perf counters are easily measurable, but the impacts are so big you rarely need them.</div><br/></div></div><div id="38270274" class="c"><input type="checkbox" id="c-38270274" checked=""/><div class="controls bullet"><span class="by">nerpderp82</span><span>|</span><a href="#38270508">prev</a><span>|</span><label class="collapse" for="c-38270274">[-]</label><label class="expand" for="c-38270274">[2 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>    v = [ random.randrange(0, 100) for _ in range(1000) ]
    %timeit sorted(v) ## include a free extra allocation
    71.2 µs ± 1.99 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)</code></pre></div><br/></div></div></div></div></div></div></div></body></html>