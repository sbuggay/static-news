<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1725699671405" as="style"/><link rel="stylesheet" href="styles.css?v=1725699671405"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2409.03384">Hardware Acceleration of LLMs: A comprehensive survey and comparison</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>matt_d</span> | <span>36 comments</span></div><br/><div><div id="41470771" class="c"><input type="checkbox" id="c-41470771" checked=""/><div class="controls bullet"><span class="by">refibrillator</span><span>|</span><a href="#41472079">next</a><span>|</span><label class="collapse" for="c-41470771">[-]</label><label class="expand" for="c-41470771">[5 more]</label></div><br/><div class="children"><div class="content">This paper is light on background so I’ll offer some additional context:<p>As early as the 90s it was observed that CPU speed (FLOPs) was improving faster than memory bandwidth. In 1995 William Wulf and Sally Mckee predicted this divergence would lead to a “memory wall”, where most computations would be bottlenecked by data access rather than arithmetic operations.<p>Over the past 20 years peak server hardware FLOPS has been scaling at 3x every 2 years, outpacing the growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and 1.4 times every 2 years, respectively.<p>Thus for training and inference of LLMs, the performance bottleneck is increasingly shifting toward memory bandwidth. Particularly for autoregressive Transformer decoder models, it can be the <i>dominant</i> bottleneck.<p>This is driving the need for new tech like Compute-in-memory (CIM), also known as processing-in-memory (PIM). Hardware in which operations are performed directly on the data in memory, rather than transferring data to CPU registers first. Thereby improving latency and power consumption, and possibly sidestepping the great “memory wall”.<p>Notably to compare ASIC and FPGA hardware across varying semiconductor process sizes, the paper uses a fitted polynomial to extrapolate to a common denominator of 16nm:<p><i>&gt; Based on the article by Aaron Stillmaker and B.Baas titled ”Scaling equations for the accurate prediction of CMOS device performance from 180 nm to 7nm,” we extrapolated the performance and the energy efficiency on a 16nm technology to make a fair comparison</i><p>But extrapolation for CIM&#x2F;PIM is not done because they claim:<p><i>&gt; As the in-memory accelerators the performance is not based only on the process technology, the extrapolation is performed only on the FPGA and ASIC accelerators where the process technology affects significantly the performance of the systems.</i><p>Which strikes me as an odd claim at face value, but perhaps others here could offer further insight on that decision.<p>Links below for further reading.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2403.14123" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2403.14123</a><p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;In-memory_processing" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;In-memory_processing</a><p><a href="http:&#x2F;&#x2F;vcl.ece.ucdavis.edu&#x2F;pubs&#x2F;2017.02.VLSIintegration.TechScale&#x2F;" rel="nofollow">http:&#x2F;&#x2F;vcl.ece.ucdavis.edu&#x2F;pubs&#x2F;2017.02.VLSIintegration.Tech...</a></div><br/><div id="41471221" class="c"><input type="checkbox" id="c-41471221" checked=""/><div class="controls bullet"><span class="by">bilsbie</span><span>|</span><a href="#41470771">parent</a><span>|</span><a href="#41472079">next</a><span>|</span><label class="collapse" for="c-41471221">[-]</label><label class="expand" for="c-41471221">[4 more]</label></div><br/><div class="children"><div class="content">Thanks for the background. Whatever happened to memristors and the promise of memory living alongside cpu?</div><br/><div id="41471423" class="c"><input type="checkbox" id="c-41471423" checked=""/><div class="controls bullet"><span class="by">dewarrn1</span><span>|</span><a href="#41470771">root</a><span>|</span><a href="#41471221">parent</a><span>|</span><a href="#41471509">next</a><span>|</span><label class="collapse" for="c-41471423">[-]</label><label class="expand" for="c-41471423">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s funny, I had thought that memristors were a solved problem based on this talk from a while back (2010!): <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=bKGhvKyjgLY" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=bKGhvKyjgLY</a>, but I gather HP never really commercialized the technology.  More recently, there does seem to be interest in and research on the topic for the reasons you and the GP post noted (e.g., <a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-023-05759-5" rel="nofollow">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-023-05759-5</a>).</div><br/></div></div><div id="41471509" class="c"><input type="checkbox" id="c-41471509" checked=""/><div class="controls bullet"><span class="by">tonetegeatinst</span><span>|</span><a href="#41470771">root</a><span>|</span><a href="#41471221">parent</a><span>|</span><a href="#41471423">prev</a><span>|</span><a href="#41471341">next</a><span>|</span><label class="collapse" for="c-41471509">[-]</label><label class="expand" for="c-41471509">[1 more]</label></div><br/><div class="children"><div class="content">I believe asianometry did a YouTube video on memristors....might be worth watching.</div><br/></div></div></div></div></div></div><div id="41472079" class="c"><input type="checkbox" id="c-41472079" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#41470771">prev</a><span>|</span><a href="#41471242">next</a><span>|</span><label class="collapse" for="c-41472079">[-]</label><label class="expand" for="c-41472079">[1 more]</label></div><br/><div class="children"><div class="content">Related:<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2406.08413" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2406.08413</a> Memory Is All You Need:
An Overview of Compute-in-Memory Architectures
for Accelerating Large Language Model Inference</div><br/></div></div><div id="41471242" class="c"><input type="checkbox" id="c-41471242" checked=""/><div class="controls bullet"><span class="by">koolala</span><span>|</span><a href="#41472079">prev</a><span>|</span><a href="#41470907">next</a><span>|</span><label class="collapse" for="c-41471242">[-]</label><label class="expand" for="c-41471242">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to watch a LLM run in WebGL where everything is Textures. Would be neat to visually see the difference in architectures.</div><br/><div id="41471729" class="c"><input type="checkbox" id="c-41471729" checked=""/><div class="controls bullet"><span class="by">vanviegen</span><span>|</span><a href="#41471242">parent</a><span>|</span><a href="#41470907">next</a><span>|</span><label class="collapse" for="c-41471729">[-]</label><label class="expand" for="c-41471729">[2 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t that be just like watching static noise?</div><br/><div id="41472501" class="c"><input type="checkbox" id="c-41472501" checked=""/><div class="controls bullet"><span class="by">archerx</span><span>|</span><a href="#41471242">root</a><span>|</span><a href="#41471729">parent</a><span>|</span><a href="#41470907">next</a><span>|</span><label class="collapse" for="c-41472501">[-]</label><label class="expand" for="c-41472501">[1 more]</label></div><br/><div class="children"><div class="content">I think some patterns would appear.</div><br/></div></div></div></div></div></div><div id="41470907" class="c"><input type="checkbox" id="c-41470907" checked=""/><div class="controls bullet"><span class="by">next_xibalba</span><span>|</span><a href="#41471242">prev</a><span>|</span><a href="#41471587">next</a><span>|</span><label class="collapse" for="c-41470907">[-]</label><label class="expand" for="c-41470907">[2 more]</label></div><br/><div class="children"><div class="content">Could a FPGA + ASICs + in-mem hybrid architecture have any role to play in scaling&#x2F;flexibility? Each one has its own benefits (e.g., FPGAs for flexibility, ASICs for performance, in-memory for energy efficiency), so could a hybrid approach integrating each to juice LLM perf even further?</div><br/><div id="41470947" class="c"><input type="checkbox" id="c-41470947" checked=""/><div class="controls bullet"><span class="by">synergy20</span><span>|</span><a href="#41470907">parent</a><span>|</span><a href="#41471587">next</a><span>|</span><label class="collapse" for="c-41470947">[-]</label><label class="expand" for="c-41470947">[1 more]</label></div><br/><div class="children"><div class="content">normally it&#x27;s FPGA + memory first, when it hits a sweet spot in the market with volume, you then turn FPGA to ASIC for performance and cost saving. For big companies they will go ASIC directly.</div><br/></div></div></div></div><div id="41471587" class="c"><input type="checkbox" id="c-41471587" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#41470907">prev</a><span>|</span><a href="#41470936">next</a><span>|</span><label class="collapse" for="c-41471587">[-]</label><label class="expand" for="c-41471587">[4 more]</label></div><br/><div class="children"><div class="content">Is there a &quot;nice&quot; way to read content on Arxiv?<p>Every time I land on that site I&#x27;m so confused &#x2F; lost in it&#x27;s interface (or lack there of) I usually end up leaving without getting to the content.</div><br/><div id="41472024" class="c"><input type="checkbox" id="c-41472024" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#41471587">parent</a><span>|</span><a href="#41471650">next</a><span>|</span><label class="collapse" for="c-41472024">[-]</label><label class="expand" for="c-41472024">[1 more]</label></div><br/><div class="children"><div class="content">Click on &quot;View PDF&quot; or &quot;HTML (experimental)&quot; on the top right to get to the content.</div><br/></div></div><div id="41471650" class="c"><input type="checkbox" id="c-41471650" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#41471587">parent</a><span>|</span><a href="#41472024">prev</a><span>|</span><a href="#41471934">next</a><span>|</span><label class="collapse" for="c-41471650">[-]</label><label class="expand" for="c-41471650">[1 more]</label></div><br/><div class="children"><div class="content">It’s a paper pre-publishing website, so everything is formatted in PDFs by default. They recently added html: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2409.03384v1" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2409.03384v1</a>
That’s the best way per paper. There’s a few arxiv frontends, like
 <a href="https:&#x2F;&#x2F;arxiv-sanity-lite.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;arxiv-sanity-lite.com&#x2F;</a></div><br/></div></div><div id="41471934" class="c"><input type="checkbox" id="c-41471934" checked=""/><div class="controls bullet"><span class="by">Noumenon72</span><span>|</span><a href="#41471587">parent</a><span>|</span><a href="#41471650">prev</a><span>|</span><a href="#41470936">next</a><span>|</span><label class="collapse" for="c-41471934">[-]</label><label class="expand" for="c-41471934">[1 more]</label></div><br/><div class="children"><div class="content">Same here -- I visited this link earlier today and thought &quot;Oh, it&#x27;s just an abstract, I&#x27;m out&quot;. I&#x27;ve read Arxiv papers before but the UI just doesn&#x27;t look like it offers any content.</div><br/></div></div></div></div><div id="41470936" class="c"><input type="checkbox" id="c-41470936" checked=""/><div class="controls bullet"><span class="by">synergy20</span><span>|</span><a href="#41471587">prev</a><span>|</span><a href="#41470711">next</a><span>|</span><label class="collapse" for="c-41470936">[-]</label><label class="expand" for="c-41470936">[1 more]</label></div><br/><div class="children"><div class="content">Memory move is the bottleneck these days, thus the expensive HBM, Nvidia&#x27;s design is also memory-optimized since it&#x27;s the true bottleneck chip wise and system wise.</div><br/></div></div><div id="41470711" class="c"><input type="checkbox" id="c-41470711" checked=""/><div class="controls bullet"><span class="by">jumploops</span><span>|</span><a href="#41470936">prev</a><span>|</span><a href="#41470425">next</a><span>|</span><label class="collapse" for="c-41470711">[-]</label><label class="expand" for="c-41470711">[1 more]</label></div><br/><div class="children"><div class="content">Curious if anyone is making AccelTran ASICs?</div><br/></div></div><div id="41470425" class="c"><input type="checkbox" id="c-41470425" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#41470711">prev</a><span>|</span><a href="#41470584">next</a><span>|</span><label class="collapse" for="c-41470425">[-]</label><label class="expand" for="c-41470425">[9 more]</label></div><br/><div class="children"><div class="content">I&#x27;m unfamiliar; in this context is &quot;in-memory&quot; specialized hardware that combines CPU+RAM?</div><br/><div id="41470494" class="c"><input type="checkbox" id="c-41470494" checked=""/><div class="controls bullet"><span class="by">kurthr</span><span>|</span><a href="#41470425">parent</a><span>|</span><a href="#41470466">next</a><span>|</span><label class="collapse" for="c-41470494">[-]</label><label class="expand" for="c-41470494">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d expect it to be MAC hardware embedded on the DRAM die (or in the case of stacked HBM, possibly on the substrate die).<p>To quote from an old article about such acceleration which sees 19x improvements over DRAM + GPU:<p><pre><code>   Since MAC operations consume the dominant part of most ML workload runtime,
   we propose in-subarray multiplication coupled with intra-bank accumulation.
   The multiplication operation is performed by performing AND operations and
   addition in column-based fashion while only adding less than 1% area overhead.
</code></pre>
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2105.03736" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2105.03736</a></div><br/></div></div><div id="41470466" class="c"><input type="checkbox" id="c-41470466" checked=""/><div class="controls bullet"><span class="by">limit499karma</span><span>|</span><a href="#41470425">parent</a><span>|</span><a href="#41470494">prev</a><span>|</span><a href="#41470584">next</a><span>|</span><label class="collapse" for="c-41470466">[-]</label><label class="expand" for="c-41470466">[7 more]</label></div><br/><div class="children"><div class="content">In-mem (generally) means no (re)loading of data from a storage device.</div><br/><div id="41472048" class="c"><input type="checkbox" id="c-41472048" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#41470425">root</a><span>|</span><a href="#41470466">parent</a><span>|</span><a href="#41470679">next</a><span>|</span><label class="collapse" for="c-41472048">[-]</label><label class="expand" for="c-41472048">[1 more]</label></div><br/><div class="children"><div class="content">Not in the context of discussing hardware architectures.<p>(Context in the abstract is &quot;First, we present the accelerators based on FPGAs, then we
present the accelerators targeting GPUs and finally accelerators
ported on ASICs and In-memory architectures&quot; and the section title in the paper body is &quot;V. In-Memory Hardware Accelerators&quot;)</div><br/></div></div><div id="41470679" class="c"><input type="checkbox" id="c-41470679" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#41470425">root</a><span>|</span><a href="#41470466">parent</a><span>|</span><a href="#41472048">prev</a><span>|</span><a href="#41470584">next</a><span>|</span><label class="collapse" for="c-41470679">[-]</label><label class="expand" for="c-41470679">[5 more]</label></div><br/><div class="children"><div class="content">Sure, but I don&#x27;t think that makes sense here; when I run an LLM on CPU, I load to memory and run it, when I run on GPU I load the model into the GPU&#x27;s memory and run it, and I don&#x27;t have anything like that much money to burn but I imagine if I used an FPGA then I would load the model into its memory and then run it from there. So the fact that they&#x27;re saying &quot;in-memory&quot; in <i>contrast</i> to ex. GPU makes me think that they&#x27;re talking about something different here.</div><br/><div id="41471056" class="c"><input type="checkbox" id="c-41471056" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#41470425">root</a><span>|</span><a href="#41470679">parent</a><span>|</span><a href="#41470584">next</a><span>|</span><label class="collapse" for="c-41471056">[-]</label><label class="expand" for="c-41471056">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a different kind of memory chip that also does some computation. See <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;In-memory_processing" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;In-memory_processing</a></div><br/><div id="41471602" class="c"><input type="checkbox" id="c-41471602" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#41470425">root</a><span>|</span><a href="#41471056">parent</a><span>|</span><a href="#41470584">next</a><span>|</span><label class="collapse" for="c-41471602">[-]</label><label class="expand" for="c-41471602">[3 more]</label></div><br/><div class="children"><div class="content">While this has been proposed repeatedly for many decades, I doubt that it will ever become useful.<p>Combining memory with computation seems good in theory, but it is difficult to do in practice.<p>The fabrication technologies for DRAM and for computational devices are very different. If you implement computational units on a DRAM chip, they will have a much worse performance than those implemented with a dedicated fabrication process, so for instance their performance per watt and per occupied area will be worse, leading to higher costs than for using separate memories and computational devices.<p>The higher cost might be acceptable in certain cases if a much higher performance is obtained. However it is unavoidable that unlike with a CPU&#x2F;GPU&#x2F;FPGA, where you can easily reprogram the device to implement a completely different algorithm, a device with in-memory computation would be much less flexible, so it either will implement extremely simple operations, like adding to memory or multiplying the memory, which would not increase much the performance due to communication overheads, or it would implement some more complex operations, which might implement some ML&#x2F;AI algorithm that is popular for the moment, but which would be hard to use to implement better algorithms when such algorithms are discovered.</div><br/><div id="41471706" class="c"><input type="checkbox" id="c-41471706" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#41470425">root</a><span>|</span><a href="#41471602">parent</a><span>|</span><a href="#41470584">next</a><span>|</span><label class="collapse" for="c-41471706">[-]</label><label class="expand" for="c-41471706">[2 more]</label></div><br/><div class="children"><div class="content">I suspect that the attempts to remove the DRAM controller and embedding it into the chips directly will succeed in meaningfully reducing the power per retrieval and increase the bandwidth by big enough that it’ll postpone these more esoteric architectures even though its pretty clear that bulk data processing like LLMs (and maybe even graphics) is better suited to this architecture since it’s cheaper to fan out the code than it is to shuffle all these bits back and forth.</div><br/><div id="41472202" class="c"><input type="checkbox" id="c-41472202" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#41470425">root</a><span>|</span><a href="#41471706">parent</a><span>|</span><a href="#41470584">next</a><span>|</span><label class="collapse" for="c-41472202">[-]</label><label class="expand" for="c-41472202">[1 more]</label></div><br/><div class="children"><div class="content">In-memory doesn’t mean in-DRAM.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2406.08413" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2406.08413</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="41470331" class="c"><input type="checkbox" id="c-41470331" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#41470510">prev</a><span>|</span><a href="#41470499">next</a><span>|</span><label class="collapse" for="c-41470331">[-]</label><label class="expand" for="c-41470331">[4 more]</label></div><br/><div class="children"><div class="content">In-memory sounds like the way to go not just in terms of performance, but in that it makes no sense to build an ASIC or program an FPGA for a model that will most likely be obsolete in a few months at best if you&#x27;re lucky.</div><br/><div id="41470470" class="c"><input type="checkbox" id="c-41470470" checked=""/><div class="controls bullet"><span class="by">limit499karma</span><span>|</span><a href="#41470331">parent</a><span>|</span><a href="#41470859">next</a><span>|</span><label class="collapse" for="c-41470470">[-]</label><label class="expand" for="c-41470470">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2402.09709" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2402.09709</a></div><br/></div></div><div id="41470859" class="c"><input type="checkbox" id="c-41470859" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#41470331">parent</a><span>|</span><a href="#41470470">prev</a><span>|</span><a href="#41470499">next</a><span>|</span><label class="collapse" for="c-41470859">[-]</label><label class="expand" for="c-41470859">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, it&#x27;s not like foundational models ever share compute kernels, or anything.</div><br/><div id="41472351" class="c"><input type="checkbox" id="c-41472351" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#41470331">root</a><span>|</span><a href="#41470859">parent</a><span>|</span><a href="#41470499">next</a><span>|</span><label class="collapse" for="c-41472351">[-]</label><label class="expand" for="c-41472351">[1 more]</label></div><br/><div class="children"><div class="content">Eh, there&#x27;s so much shenanigans these days even in fine tuning, people adding empty layers and pruning and whatnot, it&#x27;s unlikely that even models based on the same one will have the same architecture.<p>For new foundation models it&#x27;s even worse, because there&#x27;s some fancy experiment every time and the llama.cpp team needs two weeks to figure out how to implement it so the model can even run.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>