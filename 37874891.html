<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1697274059207" as="style"/><link rel="stylesheet" href="styles.css?v=1697274059207"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2310.03589">TimeGPT-1</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>PaulHoule</span> | <span>97 comments</span></div><br/><div><div id="37877443" class="c"><input type="checkbox" id="c-37877443" checked=""/><div class="controls bullet"><span class="by">dongobread</span><span>|</span><a href="#37876804">next</a><span>|</span><label class="collapse" for="c-37877443">[-]</label><label class="expand" for="c-37877443">[11 more]</label></div><br/><div class="children"><div class="content">As someone who&#x27;s worked in time series forecasting for a while, I haven&#x27;t yet found a use case for these &quot;time series&quot; focused deep learning models.<p>On extremely high dimensional data (I worked at a credit card processor company doing fraud modeling), deep learning dominates, but there&#x27;s simply no advantage in using a designated &quot;time series&quot; model that treats time differently than any other feature. We&#x27;ve tried most time series deep learning models that claim to be SoTA - N-BEATS, N-HiTS, every RNN variant that was popular pre-transformers, and they don&#x27;t beat an MLP that just uses lagged values as features. I&#x27;ve talked to several others in the forecasting space and they&#x27;ve found the same result.<p>On mid-dimensional data, LightGBM&#x2F;Xgboost is by far the best and generally performs at or better than any deep learning model, while requiring much less finetuning and a tiny fraction of the computation time.<p>And on low-dimensional data, (V)ARIMA&#x2F;ETS&#x2F;Factor models are still king, since without adequate data, the model needs to be structured with human intuition.<p>As a result I&#x27;m extremely skeptical of any of these claims about a generally high performing &quot;time series&quot; model. Training on time series gives a model very limited understanding of the fundamental structure of how the world works, unlike a language model, so the amount of generalization ability a model will gain is very limited.</div><br/><div id="37879057" class="c"><input type="checkbox" id="c-37879057" checked=""/><div class="controls bullet"><span class="by">pfalke</span><span>|</span><a href="#37877443">parent</a><span>|</span><a href="#37877553">next</a><span>|</span><label class="collapse" for="c-37879057">[-]</label><label class="expand" for="c-37879057">[1 more]</label></div><br/><div class="children"><div class="content">Foundational models can work where so far „needs human intuition“ was the state of things. I can picture a time series model with large enough Training corpus being able to deal quite well with typical quirks of seasonalities, shocks, outliers, etc.<p>I fully agree regarding how things have been so far, but I’m excited to see practitioners try out models such as the one presented here — it might just work.</div><br/></div></div><div id="37877553" class="c"><input type="checkbox" id="c-37877553" checked=""/><div class="controls bullet"><span class="by">yeahwhatever10</span><span>|</span><a href="#37877443">parent</a><span>|</span><a href="#37879057">prev</a><span>|</span><a href="#37878619">next</a><span>|</span><label class="collapse" for="c-37877553">[-]</label><label class="expand" for="c-37877553">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for this write up. Your comment clears up a lot of the confusion I&#x27;ve had around these time series transformers.<p>How does lagged features for an MLP compare to longer sequence lengths for attention in Transformers? Are you able to lag 128 time steps in a feed forward network and get good results?</div><br/></div></div><div id="37878619" class="c"><input type="checkbox" id="c-37878619" checked=""/><div class="controls bullet"><span class="by">waltherg</span><span>|</span><a href="#37877443">parent</a><span>|</span><a href="#37877553">prev</a><span>|</span><a href="#37877947">next</a><span>|</span><label class="collapse" for="c-37878619">[-]</label><label class="expand" for="c-37878619">[2 more]</label></div><br/><div class="children"><div class="content">Great write-up, thank you. Do you have rough measures for what constitutes high&#x2F;mid&#x2F;low- dimensional data? And how do you use XGBoost et al for multi-step forecasting, I.e. in scenarios where you want to predict multiple time steps in the future?</div><br/><div id="37878821" class="c"><input type="checkbox" id="c-37878821" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#37877443">root</a><span>|</span><a href="#37878619">parent</a><span>|</span><a href="#37877947">next</a><span>|</span><label class="collapse" for="c-37878821">[-]</label><label class="expand" for="c-37878821">[1 more]</label></div><br/><div class="children"><div class="content">Because they&#x27;re so cheap to train, you can just use n models if you want to predict n steps ahead.<p>In sklearn, if you have a single-output regressor, use this for ergonomics: <a href="https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;generated&#x2F;sklearn.multioutput.MultiOutputRegressor.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;generated&#x2F;sklearn.mu...</a><p>The added benefit is that you optimize each regressor towards its own target timestep t+1 ... t+n. A single loss on the aggregate of all timesteps is often problematic</div><br/></div></div></div></div><div id="37877947" class="c"><input type="checkbox" id="c-37877947" checked=""/><div class="controls bullet"><span class="by">teeray</span><span>|</span><a href="#37877443">parent</a><span>|</span><a href="#37878619">prev</a><span>|</span><a href="#37878165">next</a><span>|</span><label class="collapse" for="c-37877947">[-]</label><label class="expand" for="c-37877947">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I haven&#x27;t yet found a use case for these &quot;time series&quot; focused deep learning models.<p>I guarantee you there will be chartists hawking GPT-powered market forecasts.</div><br/><div id="37878506" class="c"><input type="checkbox" id="c-37878506" checked=""/><div class="controls bullet"><span class="by">kylebenzle</span><span>|</span><a href="#37877443">root</a><span>|</span><a href="#37877947">parent</a><span>|</span><a href="#37878165">next</a><span>|</span><label class="collapse" for="c-37878506">[-]</label><label class="expand" for="c-37878506">[1 more]</label></div><br/><div class="children"><div class="content">That is terrifying but inevitable. Lol, the back end will just be chatgpt API asking, which stock should I buy next?</div><br/></div></div></div></div><div id="37878165" class="c"><input type="checkbox" id="c-37878165" checked=""/><div class="controls bullet"><span class="by">recursive4</span><span>|</span><a href="#37877443">parent</a><span>|</span><a href="#37877947">prev</a><span>|</span><a href="#37876804">next</a><span>|</span><label class="collapse" for="c-37878165">[-]</label><label class="expand" for="c-37878165">[4 more]</label></div><br/><div class="children"><div class="content">So fraud is consistent with respect to time?</div><br/><div id="37878314" class="c"><input type="checkbox" id="c-37878314" checked=""/><div class="controls bullet"><span class="by">jldugger</span><span>|</span><a href="#37877443">root</a><span>|</span><a href="#37878165">parent</a><span>|</span><a href="#37878515">next</a><span>|</span><label class="collapse" for="c-37878314">[-]</label><label class="expand" for="c-37878314">[2 more]</label></div><br/><div class="children"><div class="content">My read on this was that you can just dump the lagged values as inputs and let the network figure it out just as well as the other, time series specific models do, not that time doesn&#x27;t matter.</div><br/></div></div><div id="37878515" class="c"><input type="checkbox" id="c-37878515" checked=""/><div class="controls bullet"><span class="by">cbeach</span><span>|</span><a href="#37877443">root</a><span>|</span><a href="#37878165">parent</a><span>|</span><a href="#37878314">prev</a><span>|</span><a href="#37876804">next</a><span>|</span><label class="collapse" for="c-37878515">[-]</label><label class="expand" for="c-37878515">[1 more]</label></div><br/><div class="children"><div class="content">I assume the time series modelling is used to predict normal non-fraud behaviour. And then simpler algorithms are able to highlight deviations from the norm?</div><br/></div></div></div></div></div></div><div id="37876804" class="c"><input type="checkbox" id="c-37876804" checked=""/><div class="controls bullet"><span class="by">screye</span><span>|</span><a href="#37877443">prev</a><span>|</span><a href="#37875565">next</a><span>|</span><label class="collapse" for="c-37876804">[-]</label><label class="expand" for="c-37876804">[17 more]</label></div><br/><div class="children"><div class="content">No, Transformers are not a silver bullet.<p>As much as Transformers feel like the state of the art universal function approximators, people need to realize why they work so well for language and vision.<p>Transformers parallelize incredibly well, and they learn sophisticated intermediate representations. We start seeing neat separation of different semantic concepts in space. We start seeing models do delimiter detection naturally. We start seeing models reason about lines, curves, colors, dog ears etc. The final layers of a Transformer are then putting these sophisticated concepts together to learn high level concepts like dog&#x2F;cat&#x2F;blog etc.<p>Transformers (and deep learning methods in general) do not work for time series data because they have yet to extract any novel intermediate representations from said data.<p>At face value, how do you even work with a &#x27;token window&#x27; ? At the simplest level, time series modelling is about identifying repeating patterns over very different lifecycles conditioned on certain observations about the world. You need a model to natively be able to reason over years, days and seconds all at the same time to even be able to reason about the problem in the first place. Hilariously, last week&#x27;s streaming LLM paper from MIT might actually help here.<p>Secondly, the improvements appear marginal at best. If you&#x27;re proposing a massive architecture change, removing observability &amp; and explainability .........then you better have some incredible results.<p>Truth is, if someone identifies a groundbreaking technique for timeseries forecasting, then they&#x27;d be an idiot to tell anyone about it before making their first $Billion$ on the market. Hell, I&#x27;d say they&#x27;d be an idiot for stopping at a billion. Time series forecasting is the most monetarily rewarding problem you could solve. If you publish a paper, then by implication, I expect it to be disappointing.</div><br/><div id="37876869" class="c"><input type="checkbox" id="c-37876869" checked=""/><div class="controls bullet"><span class="by">Xcelerate</span><span>|</span><a href="#37876804">parent</a><span>|</span><a href="#37876989">next</a><span>|</span><label class="collapse" for="c-37876869">[-]</label><label class="expand" for="c-37876869">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Truth is, if someone identifies a groundbreaking technique for timeseries forecasting<p>It’s really quite simple. Just iterate through all possible monotone universal Turing machines where the input tape consists of all data we can possibly collect concatenated with the time series of interest. Skip the programs that take too long to halt, keep the remaining ones that reproduce the input sequence, then form a probability distribution based on the next output bits, weighted by 2^-(program size).<p>What’s so hard about that?</div><br/><div id="37877127" class="c"><input type="checkbox" id="c-37877127" checked=""/><div class="controls bullet"><span class="by">justinjlynn</span><span>|</span><a href="#37876804">root</a><span>|</span><a href="#37876869">parent</a><span>|</span><a href="#37878807">next</a><span>|</span><label class="collapse" for="c-37877127">[-]</label><label class="expand" for="c-37877127">[2 more]</label></div><br/><div class="children"><div class="content">Oh, just the death of the planet through waste heat buildup. That&#x27;s all. Just a small matter of code should fix that.</div><br/><div id="37877704" class="c"><input type="checkbox" id="c-37877704" checked=""/><div class="controls bullet"><span class="by">pyinstallwoes</span><span>|</span><a href="#37876804">root</a><span>|</span><a href="#37877127">parent</a><span>|</span><a href="#37878807">next</a><span>|</span><label class="collapse" for="c-37877704">[-]</label><label class="expand" for="c-37877704">[1 more]</label></div><br/><div class="children"><div class="content">Just like Bitcoin</div><br/></div></div></div></div><div id="37878807" class="c"><input type="checkbox" id="c-37878807" checked=""/><div class="controls bullet"><span class="by">ganzuul</span><span>|</span><a href="#37876804">root</a><span>|</span><a href="#37876869">parent</a><span>|</span><a href="#37877127">prev</a><span>|</span><a href="#37877040">next</a><span>|</span><label class="collapse" for="c-37878807">[-]</label><label class="expand" for="c-37878807">[1 more]</label></div><br/><div class="children"><div class="content">Error: Fake news is prediction output returning to input.</div><br/></div></div></div></div><div id="37876989" class="c"><input type="checkbox" id="c-37876989" checked=""/><div class="controls bullet"><span class="by">chronic7490</span><span>|</span><a href="#37876804">parent</a><span>|</span><a href="#37876869">prev</a><span>|</span><a href="#37878407">next</a><span>|</span><label class="collapse" for="c-37876989">[-]</label><label class="expand" for="c-37876989">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Truth is, if someone identifies a groundbreaking technique for timeseries forecasting, then they&#x27;d be an idiot to tell anyone about it before making their first $Billion$ on the market.<p>This is correct.<p>I work in HFT and the industry has been successfully applying deep learning to market data for a while now. Everything from pcaps&#x2F;ticks to candles.<p>Why publish your method when it generates $1B+&#x2F;year in profit for a team of 50 quants&#x2F;SWEs&#x2F;traders?</div><br/><div id="37879035" class="c"><input type="checkbox" id="c-37879035" checked=""/><div class="controls bullet"><span class="by">akrymski</span><span>|</span><a href="#37876804">root</a><span>|</span><a href="#37876989">parent</a><span>|</span><a href="#37878880">next</a><span>|</span><label class="collapse" for="c-37879035">[-]</label><label class="expand" for="c-37879035">[1 more]</label></div><br/><div class="children"><div class="content">Re candles - even longer term, hourly&#x2F;daily?  Are there actually strategies out there that deliver great sharpe over many years with just time series forecasting?  Most hedge funds don&#x27;t beat the index afaik</div><br/></div></div><div id="37878880" class="c"><input type="checkbox" id="c-37878880" checked=""/><div class="controls bullet"><span class="by">ganzuul</span><span>|</span><a href="#37876804">root</a><span>|</span><a href="#37876989">parent</a><span>|</span><a href="#37879035">prev</a><span>|</span><a href="#37878781">next</a><span>|</span><label class="collapse" for="c-37878880">[-]</label><label class="expand" for="c-37878880">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why publish your method when it generates $1B+&#x2F;year in profit for a team of 50 quants&#x2F;SWEs&#x2F;traders?<p>Do you also believe that megacorporations as the custodians of superintelligence is bad?</div><br/></div></div><div id="37878781" class="c"><input type="checkbox" id="c-37878781" checked=""/><div class="controls bullet"><span class="by">ckastner</span><span>|</span><a href="#37876804">root</a><span>|</span><a href="#37876989">parent</a><span>|</span><a href="#37878880">prev</a><span>|</span><a href="#37878407">next</a><span>|</span><label class="collapse" for="c-37878781">[-]</label><label class="expand" for="c-37878781">[1 more]</label></div><br/><div class="children"><div class="content">Are you at liberty to say how high the frequency gets in connection with these models?<p>I assume the latency is comparably much higher but also wouldn&#x27;t be surprised if microseconds generally aren&#x27;t a problem, eg because the patterns detected are on a much larger scale.</div><br/></div></div></div></div><div id="37878407" class="c"><input type="checkbox" id="c-37878407" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#37876804">parent</a><span>|</span><a href="#37876989">prev</a><span>|</span><a href="#37877889">next</a><span>|</span><label class="collapse" for="c-37878407">[-]</label><label class="expand" for="c-37878407">[1 more]</label></div><br/><div class="children"><div class="content">Time series prediction is always about using the particular features of your distribution of time series. In standard time series prediction the features of the distribution are mostly things like &quot;periodic patterns are continued&quot; or &quot;growth patterns are continued&quot;. A transformer that is trained on language data essentially learns time series prediction where a large variety of complex feature appear that influence the continuation. Language data is so complex and diverse that continuing a text necessitates in-context learning: Being able to find some common features in any kind of string of symbols, and using those to continue the text. Just think that language data could contain huge excel tables of various data, like stock market prices, or weather recordings. It is therefore plausible that in-context learning can be very powerful, enough to perform zero-shot time series continuation. Moreover, I believe that due to in-context learning language data + transformer architecture has the potential to really obtain general intelligence like behaviour. General pattern recognition. Language data is complex enough that SGD must lead to general pattern recognition and continuation. We are only at the beginning, and right now we are focused on finetuning which destroys in-context learning. But we will soon train giant transformers on every modality, every string of symboly we can find.</div><br/></div></div><div id="37877889" class="c"><input type="checkbox" id="c-37877889" checked=""/><div class="controls bullet"><span class="by">tsumnia</span><span>|</span><a href="#37876804">parent</a><span>|</span><a href="#37878407">prev</a><span>|</span><a href="#37877601">next</a><span>|</span><label class="collapse" for="c-37877889">[-]</label><label class="expand" for="c-37877889">[2 more]</label></div><br/><div class="children"><div class="content">&gt; people need to realize why they work so well for language and vision.<p>I agree with your entire post, however this sentence made me think, well video is just layered vision. Why couldn&#x27;t frames of vision work similar to vision? We know the current answer is it doesn&#x27;t, but is it a matter of NNs can&#x27;t or we haven&#x27;t figured out the correct way to model it yet?</div><br/><div id="37877893" class="c"><input type="checkbox" id="c-37877893" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#37876804">root</a><span>|</span><a href="#37877889">parent</a><span>|</span><a href="#37877601">next</a><span>|</span><label class="collapse" for="c-37877893">[-]</label><label class="expand" for="c-37877893">[1 more]</label></div><br/><div class="children"><div class="content">Why do you say it doesn&#x27;t work well? I thought it did?</div><br/></div></div></div></div><div id="37877601" class="c"><input type="checkbox" id="c-37877601" checked=""/><div class="controls bullet"><span class="by">jmpeax</span><span>|</span><a href="#37876804">parent</a><span>|</span><a href="#37877889">prev</a><span>|</span><a href="#37877362">next</a><span>|</span><label class="collapse" for="c-37877601">[-]</label><label class="expand" for="c-37877601">[2 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t forecast the market, all you can do is present an &quot;awesome AI&quot; to suckers and take a slice of their profit with no exposure to their losses.</div><br/><div id="37877625" class="c"><input type="checkbox" id="c-37877625" checked=""/><div class="controls bullet"><span class="by">sheeshkebab</span><span>|</span><a href="#37876804">root</a><span>|</span><a href="#37877601">parent</a><span>|</span><a href="#37877362">next</a><span>|</span><label class="collapse" for="c-37877625">[-]</label><label class="expand" for="c-37877625">[1 more]</label></div><br/><div class="children"><div class="content">Oh yeah? I guess you never day traded..</div><br/></div></div></div></div><div id="37877362" class="c"><input type="checkbox" id="c-37877362" checked=""/><div class="controls bullet"><span class="by">peteradio</span><span>|</span><a href="#37876804">parent</a><span>|</span><a href="#37877601">prev</a><span>|</span><a href="#37875565">next</a><span>|</span><label class="collapse" for="c-37877362">[-]</label><label class="expand" for="c-37877362">[2 more]</label></div><br/><div class="children"><div class="content">Truth is not immutable through time.</div><br/><div id="37879014" class="c"><input type="checkbox" id="c-37879014" checked=""/><div class="controls bullet"><span class="by">ganzuul</span><span>|</span><a href="#37876804">root</a><span>|</span><a href="#37877362">parent</a><span>|</span><a href="#37875565">next</a><span>|</span><label class="collapse" for="c-37879014">[-]</label><label class="expand" for="c-37879014">[1 more]</label></div><br/><div class="children"><div class="content">Truth is not additive. In moral matters, truth is non-commutative. Truth is however associative.</div><br/></div></div></div></div></div></div><div id="37875565" class="c"><input type="checkbox" id="c-37875565" checked=""/><div class="controls bullet"><span class="by">bfung</span><span>|</span><a href="#37876804">prev</a><span>|</span><a href="#37876651">next</a><span>|</span><label class="collapse" for="c-37875565">[-]</label><label class="expand" for="c-37875565">[22 more]</label></div><br/><div class="children"><div class="content">“Some popular models like Prophet [Taylor and Letham, 2018] and ARIMA were excluded from the analysis due to their prohibitive computational requirements and extensive training times.”<p>Anyone who work a lot in time series forecasting can explain this in some more details?<p>I’ve def used ARIMA, but only for simple things. Not sure why this would be more expensive to train and run than a Transformer model, and even if true, ARIMA is so ubiquitous that comparing resources &amp; time would be enlightening.  Otherwise it just sounds like a sales pitch and throw more obscure acronyms for a bit of “I’m the expert, abc xyz industry letters” marketing.</div><br/><div id="37875698" class="c"><input type="checkbox" id="c-37875698" checked=""/><div class="controls bullet"><span class="by">mvanaltvorst</span><span>|</span><a href="#37875565">parent</a><span>|</span><a href="#37876816">next</a><span>|</span><label class="collapse" for="c-37875698">[-]</label><label class="expand" for="c-37875698">[3 more]</label></div><br/><div class="children"><div class="content">I immediately tried to find a comparison with ARIMA as well and was disappointed. It&#x27;s difficult to take this paper seriously when they dismiss a forecasting technique from the 70&#x27;s because of &quot;extensive training times&quot;.</div><br/><div id="37876453" class="c"><input type="checkbox" id="c-37876453" checked=""/><div class="controls bullet"><span class="by">a5seo</span><span>|</span><a href="#37875565">root</a><span>|</span><a href="#37875698">parent</a><span>|</span><a href="#37876816">next</a><span>|</span><label class="collapse" for="c-37876453">[-]</label><label class="expand" for="c-37876453">[2 more]</label></div><br/><div class="children"><div class="content">Maybe if your time interval is super short and you have hundreds of years of data? Otherwise, I’m not sure what they’re on about.</div><br/><div id="37877708" class="c"><input type="checkbox" id="c-37877708" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#37875565">root</a><span>|</span><a href="#37876453">parent</a><span>|</span><a href="#37876816">next</a><span>|</span><label class="collapse" for="c-37877708">[-]</label><label class="expand" for="c-37877708">[1 more]</label></div><br/><div class="children"><div class="content">Even then, 500 years of daily data is less than 200k observations, most of which are meaningless for predicting the future. Less than 16B seconds of data. Regression might not handle directly, but linear algebra tricks are still available.</div><br/></div></div></div></div></div></div><div id="37876816" class="c"><input type="checkbox" id="c-37876816" checked=""/><div class="controls bullet"><span class="by">maxmc</span><span>|</span><a href="#37875565">parent</a><span>|</span><a href="#37875698">prev</a><span>|</span><a href="#37875789">next</a><span>|</span><label class="collapse" for="c-37876816">[-]</label><label class="expand" for="c-37876816">[1 more]</label></div><br/><div class="children"><div class="content">We love ARIMAs. That is why we put so much effort into creating fast and scalable Arimas and AutoArima in Python [1].<p>Regarding your valid concern. There are several reasons for the high computational costs. First, ARIMA and other &quot;statistical&quot; methods are local, so they must train one different model for each time series. (ML and DL models are global, so you have &#x27;one&#x27; model for all the series.) Second, the ARIMA model usually performs poorly for a diverse set of time series, like the one considered in our experiments. The AutoARIMA is a better option, but its training time is considerably longer, given the number and length of the series. Also, AutoARIMA tends to be very slow for long series. 
In short: for the 500k series we used for benchmarcking, ARIMA would have taken literally weeks and would have been very expensive.
That is why we included many well-performing local &quot;statistical&quot; models, such as the Theta and CES. We used the implementations on our open-source ecosystem for all the baselines, including StatsForecast, MLForecast, and Neuralforecast. We will release a reproducible set of experiments on smaller subsets soon!<p>[1] <a href="https:&#x2F;&#x2F;nixtla.github.io&#x2F;statsforecast&#x2F;docs&#x2F;models&#x2F;arima.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;nixtla.github.io&#x2F;statsforecast&#x2F;docs&#x2F;models&#x2F;arima.htm...</a></div><br/></div></div><div id="37875789" class="c"><input type="checkbox" id="c-37875789" checked=""/><div class="controls bullet"><span class="by">m3at</span><span>|</span><a href="#37875565">parent</a><span>|</span><a href="#37876816">prev</a><span>|</span><a href="#37876052">next</a><span>|</span><label class="collapse" for="c-37875789">[-]</label><label class="expand" for="c-37875789">[1 more]</label></div><br/><div class="children"><div class="content">I was surprised too!<p>While I could find some excuses to exclude ARIMA, notably that in practice you need to input some important priors about your time series (periodicity, refinements for turning points, etc) for it to work decently, &quot;prohibitive compute and extensive training time&quot; are just not applicable.<p>That part is a bit wanky, but the rest of the paper, notably the zero shot capability, is very interesting if confirmed. I look forward for it to be more accessible than a &quot;contact us&quot; api to compare to ARIMA and others myself</div><br/></div></div><div id="37876052" class="c"><input type="checkbox" id="c-37876052" checked=""/><div class="controls bullet"><span class="by">alfalfasprout</span><span>|</span><a href="#37875565">parent</a><span>|</span><a href="#37875789">prev</a><span>|</span><a href="#37876951">next</a><span>|</span><label class="collapse" for="c-37876052">[-]</label><label class="expand" for="c-37876052">[2 more]</label></div><br/><div class="children"><div class="content">Excluding prophet and ARIMA makes it hard to take this seriously... those are super widely used.</div><br/><div id="37877029" class="c"><input type="checkbox" id="c-37877029" checked=""/><div class="controls bullet"><span class="by">bllguo</span><span>|</span><a href="#37875565">root</a><span>|</span><a href="#37876052">parent</a><span>|</span><a href="#37876951">next</a><span>|</span><label class="collapse" for="c-37877029">[-]</label><label class="expand" for="c-37877029">[1 more]</label></div><br/><div class="children"><div class="content">are you aware of the significant amounts of criticism for prophet?<p>arima, sure</div><br/></div></div></div></div><div id="37876951" class="c"><input type="checkbox" id="c-37876951" checked=""/><div class="controls bullet"><span class="by">loxias</span><span>|</span><a href="#37875565">parent</a><span>|</span><a href="#37876052">prev</a><span>|</span><a href="#37876473">next</a><span>|</span><label class="collapse" for="c-37876951">[-]</label><label class="expand" for="c-37876951">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  “Some popular models like Prophet [Taylor and Letham, 2018] and ARIMA were excluded from the analysis due to their prohibitive computational requirements and extensive training times.”<p>Yes, I&#x27;ve done some work in time series forecasting.  The above sentence is the one that tipped me off to this paper being BS, so I stopped reading after that. :)   I can&#x27;t take any paper about timeseries forecasting seriously by an author who isn&#x27;t familiar with the field.</div><br/></div></div><div id="37876473" class="c"><input type="checkbox" id="c-37876473" checked=""/><div class="controls bullet"><span class="by">lr1970</span><span>|</span><a href="#37875565">parent</a><span>|</span><a href="#37876951">prev</a><span>|</span><a href="#37876734">next</a><span>|</span><label class="collapse" for="c-37876473">[-]</label><label class="expand" for="c-37876473">[1 more]</label></div><br/><div class="children"><div class="content">I have need doing time series forecasting professionally. ARIMA is computationally one of the cheapest (both training and inference) forecasting models out there. It suffers from many deficiencies and shortcomings but computational efficiency is not one of them.<p>EDIT: typos</div><br/></div></div><div id="37876121" class="c"><input type="checkbox" id="c-37876121" checked=""/><div class="controls bullet"><span class="by">nojito</span><span>|</span><a href="#37875565">parent</a><span>|</span><a href="#37876734">prev</a><span>|</span><a href="#37875782">next</a><span>|</span><label class="collapse" for="c-37876121">[-]</label><label class="expand" for="c-37876121">[5 more]</label></div><br/><div class="children"><div class="content">The test data is 300k different time series. There’s no way to do an arima in a reasonable amount of time and&#x2F;or money on that volume of data</div><br/><div id="37876576" class="c"><input type="checkbox" id="c-37876576" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#37875565">root</a><span>|</span><a href="#37876121">parent</a><span>|</span><a href="#37876508">next</a><span>|</span><label class="collapse" for="c-37876576">[-]</label><label class="expand" for="c-37876576">[1 more]</label></div><br/><div class="children"><div class="content">Eh it&#x27;s not as if you could just project down the 300k time series to something lower dimensional for forecasting. The TimeGPT would have to do something similar to avoid the same problem.<p>Though I can&#x27;t quite figure out how the predicting works <i>exactly</i>, they have a lot of test series but do they input all of them simultaneously?</div><br/></div></div><div id="37876508" class="c"><input type="checkbox" id="c-37876508" checked=""/><div class="controls bullet"><span class="by">agnosticmantis</span><span>|</span><a href="#37875565">root</a><span>|</span><a href="#37876121">parent</a><span>|</span><a href="#37876576">prev</a><span>|</span><a href="#37875782">next</a><span>|</span><label class="collapse" for="c-37876508">[-]</label><label class="expand" for="c-37876508">[3 more]</label></div><br/><div class="children"><div class="content">Really? And they could do LSTMs?<p>Even if true, they could take a random subset of size 100 out of the 300k and compare on those.</div><br/><div id="37876562" class="c"><input type="checkbox" id="c-37876562" checked=""/><div class="controls bullet"><span class="by">nojito</span><span>|</span><a href="#37875565">root</a><span>|</span><a href="#37876508">parent</a><span>|</span><a href="#37875782">next</a><span>|</span><label class="collapse" for="c-37876562">[-]</label><label class="expand" for="c-37876562">[2 more]</label></div><br/><div class="children"><div class="content">ARIMA is very very slow and computational expensive.<p>&gt;Even if true, they could take a random subset of size 100 out of the 300k and compare on those.<p>Sure...but there&#x27;s a chance that ARIMA won&#x27;t even finish training on that subset either.</div><br/><div id="37876873" class="c"><input type="checkbox" id="c-37876873" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#37875565">root</a><span>|</span><a href="#37876562">parent</a><span>|</span><a href="#37875782">next</a><span>|</span><label class="collapse" for="c-37876873">[-]</label><label class="expand" for="c-37876873">[1 more]</label></div><br/><div class="children"><div class="content">It doesn’t matter.<p>If you write a paper and exclude comparisons to state of the art, this what happens.<p>They could have done <i>something</i>, and didn’t.<p>“It’s hard so we didn’t” isn’t an excuse, it’s just a lack of rigor.</div><br/></div></div></div></div></div></div></div></div><div id="37875782" class="c"><input type="checkbox" id="c-37875782" checked=""/><div class="controls bullet"><span class="by">dist-epoch</span><span>|</span><a href="#37875565">parent</a><span>|</span><a href="#37876121">prev</a><span>|</span><a href="#37876651">next</a><span>|</span><label class="collapse" for="c-37875782">[-]</label><label class="expand" for="c-37875782">[6 more]</label></div><br/><div class="children"><div class="content">ARIMA is not very good for anything but highly predictable time series (of the summer-hot winter-cold kind).</div><br/><div id="37876059" class="c"><input type="checkbox" id="c-37876059" checked=""/><div class="controls bullet"><span class="by">carbocation</span><span>|</span><a href="#37875565">root</a><span>|</span><a href="#37875782">parent</a><span>|</span><a href="#37875944">next</a><span>|</span><label class="collapse" for="c-37876059">[-]</label><label class="expand" for="c-37876059">[1 more]</label></div><br/><div class="children"><div class="content">If true, then beating it and looking good will be easy.<p>Having trained ARIMA models in my day, I will say that long training times and training cost -- compared to any deep learning model -- is not something that ever crossed my mind.</div><br/></div></div><div id="37875944" class="c"><input type="checkbox" id="c-37875944" checked=""/><div class="controls bullet"><span class="by">aerhardt</span><span>|</span><a href="#37875565">root</a><span>|</span><a href="#37875782">parent</a><span>|</span><a href="#37876059">prev</a><span>|</span><a href="#37876651">next</a><span>|</span><label class="collapse" for="c-37875944">[-]</label><label class="expand" for="c-37875944">[4 more]</label></div><br/><div class="children"><div class="content">Ok but the claim is about high training times… Wtf?</div><br/><div id="37876020" class="c"><input type="checkbox" id="c-37876020" checked=""/><div class="controls bullet"><span class="by">ugh123</span><span>|</span><a href="#37875565">root</a><span>|</span><a href="#37875944">parent</a><span>|</span><a href="#37876651">next</a><span>|</span><label class="collapse" for="c-37876020">[-]</label><label class="expand" for="c-37876020">[3 more]</label></div><br/><div class="children"><div class="content">High training times could be cost prohibitive.  Currently, its over $100mil to train GPT4 from scratch (which possibly includes other costs related to RLHF and data acquisition).  Not sure how this model compares, but its likely not cheap.</div><br/><div id="37876082" class="c"><input type="checkbox" id="c-37876082" checked=""/><div class="controls bullet"><span class="by">warkdarrior</span><span>|</span><a href="#37875565">root</a><span>|</span><a href="#37876020">parent</a><span>|</span><a href="#37876651">next</a><span>|</span><label class="collapse" for="c-37876082">[-]</label><label class="expand" for="c-37876082">[2 more]</label></div><br/><div class="children"><div class="content">The claim in the paper was the ARIMA has high training times.</div><br/><div id="37876236" class="c"><input type="checkbox" id="c-37876236" checked=""/><div class="controls bullet"><span class="by">cozzyd</span><span>|</span><a href="#37875565">root</a><span>|</span><a href="#37876082">parent</a><span>|</span><a href="#37876651">next</a><span>|</span><label class="collapse" for="c-37876236">[-]</label><label class="expand" for="c-37876236">[1 more]</label></div><br/><div class="children"><div class="content">Maybe they&#x27;re using a pure Python implementation or something...</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="37876651" class="c"><input type="checkbox" id="c-37876651" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#37875565">prev</a><span>|</span><a href="#37876795">next</a><span>|</span><label class="collapse" for="c-37876651">[-]</label><label class="expand" for="c-37876651">[4 more]</label></div><br/><div class="children"><div class="content">This is an extremely content-light paper. There&#x27;s basically zero information on anything important. Just hand-waving about the architecture and the data. Instead it spends its space on things like the equation for MAE and a diagram depicting the concept of training and inference. Red flags everywhere.</div><br/><div id="37876970" class="c"><input type="checkbox" id="c-37876970" checked=""/><div class="controls bullet"><span class="by">CGamesPlay</span><span>|</span><a href="#37876651">parent</a><span>|</span><a href="#37876795">next</a><span>|</span><label class="collapse" for="c-37876970">[-]</label><label class="expand" for="c-37876970">[3 more]</label></div><br/><div class="children"><div class="content">&gt; To request access, please visit nixtla.io.<p>Yeah, this is an ad submitted to Arxiv.</div><br/><div id="37877635" class="c"><input type="checkbox" id="c-37877635" checked=""/><div class="controls bullet"><span class="by">bfung</span><span>|</span><a href="#37876651">root</a><span>|</span><a href="#37876970">parent</a><span>|</span><a href="#37877263">next</a><span>|</span><label class="collapse" for="c-37877635">[-]</label><label class="expand" for="c-37877635">[1 more]</label></div><br/><div class="children"><div class="content">I watched the YouTube presentation afterwards (linked elsewhere in this post), and it makes more sense what they’re trying to do.<p>The guise of the academic paper def throws things off and it’s poorly communicated in the paper what the model benefits are.<p>Posting a quick write up of the talk on youtube would’ve set the right amount of rigor and expectation. (Needs marketing help, lol)</div><br/></div></div><div id="37877263" class="c"><input type="checkbox" id="c-37877263" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#37876651">root</a><span>|</span><a href="#37876970">parent</a><span>|</span><a href="#37877635">prev</a><span>|</span><a href="#37876795">next</a><span>|</span><label class="collapse" for="c-37877263">[-]</label><label class="expand" for="c-37877263">[1 more]</label></div><br/><div class="children"><div class="content">Shouldn&#x27;t there be some curation? No less, how is this on the front page of HN if not with some nefarious activity</div><br/></div></div></div></div></div></div><div id="37876795" class="c"><input type="checkbox" id="c-37876795" checked=""/><div class="controls bullet"><span class="by">stathibus</span><span>|</span><a href="#37876651">prev</a><span>|</span><a href="#37875366">next</a><span>|</span><label class="collapse" for="c-37876795">[-]</label><label class="expand" for="c-37876795">[1 more]</label></div><br/><div class="children"><div class="content">This is exactly the kind of thing the academics are warning you about when they say things like &quot;peer review is important&quot; and &quot;don&#x27;t read arxiv preprints if you&#x27;re not a subject matter expert&quot;</div><br/></div></div><div id="37875366" class="c"><input type="checkbox" id="c-37875366" checked=""/><div class="controls bullet"><span class="by">jdonaldson</span><span>|</span><a href="#37876795">prev</a><span>|</span><a href="#37875307">next</a><span>|</span><label class="collapse" for="c-37875366">[-]</label><label class="expand" for="c-37875366">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Uncertainty is an intrinsic aspect of life, a constant element that humans have tirelessly sought to navigate and comprehend.&quot;<p>I can appreciate people doing things from the heart, but in that case, it better be more poetic than a BBC documentary.</div><br/></div></div><div id="37875307" class="c"><input type="checkbox" id="c-37875307" checked=""/><div class="controls bullet"><span class="by">moralestapia</span><span>|</span><a href="#37875366">prev</a><span>|</span><a href="#37876673">next</a><span>|</span><label class="collapse" for="c-37875307">[-]</label><label class="expand" for="c-37875307">[1 more]</label></div><br/><div class="children"><div class="content">Related?<p>Inverted Transformers Are Effective for Time Series Forecasting<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37848321">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37848321</a></div><br/></div></div><div id="37876673" class="c"><input type="checkbox" id="c-37876673" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#37875307">prev</a><span>|</span><a href="#37875846">next</a><span>|</span><label class="collapse" for="c-37876673">[-]</label><label class="expand" for="c-37876673">[1 more]</label></div><br/><div class="children"><div class="content">Looks like a marketing piece for their product <a href="https:&#x2F;&#x2F;www.nixtla.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nixtla.io&#x2F;</a></div><br/></div></div><div id="37875846" class="c"><input type="checkbox" id="c-37875846" checked=""/><div class="controls bullet"><span class="by">philomath_mn</span><span>|</span><a href="#37876673">prev</a><span>|</span><a href="#37875522">next</a><span>|</span><label class="collapse" for="c-37875846">[-]</label><label class="expand" for="c-37875846">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to be corrected, but this seems to do ~20-30% better than a Seasonal Naive model -- that doesn&#x27;t sound all that useful?<p>The zero-shot nature is certainly impressive but it doesn&#x27;t look like you&#x27;d be able to _do_ much with it?</div><br/></div></div><div id="37875522" class="c"><input type="checkbox" id="c-37875522" checked=""/><div class="controls bullet"><span class="by">allanrbo</span><span>|</span><a href="#37875846">prev</a><span>|</span><a href="#37875987">next</a><span>|</span><label class="collapse" for="c-37875522">[-]</label><label class="expand" for="c-37875522">[1 more]</label></div><br/><div class="children"><div class="content">Only a hosted API for now it seems? On page 12 they mention <a href="https:&#x2F;&#x2F;www.nixtla.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nixtla.io&#x2F;</a> .</div><br/></div></div><div id="37875987" class="c"><input type="checkbox" id="c-37875987" checked=""/><div class="controls bullet"><span class="by">modo_</span><span>|</span><a href="#37875522">prev</a><span>|</span><a href="#37875739">next</a><span>|</span><label class="collapse" for="c-37875987">[-]</label><label class="expand" for="c-37875987">[1 more]</label></div><br/><div class="children"><div class="content">Their CTO did a demo of TimeGPT at their launch event last month [1]. It definitely appears to be a very easy tool to use. Love that it&#x27;s zero shot!<p>Regardless, need to see more benchmarks to better understand its true performance. If it holds up it would be a big win for time forecasting.<p>[1] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=n7luRRyxLoQ">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=n7luRRyxLoQ</a></div><br/></div></div><div id="37875739" class="c"><input type="checkbox" id="c-37875739" checked=""/><div class="controls bullet"><span class="by">fwungy</span><span>|</span><a href="#37875987">prev</a><span>|</span><a href="#37877027">next</a><span>|</span><label class="collapse" for="c-37875739">[-]</label><label class="expand" for="c-37875739">[3 more]</label></div><br/><div class="children"><div class="content">Time series works amazing until a fundamental assumption breaks or the time frame extends too far. It&#x27;s pretty much just drawing the existing pattern out further with mathematical precision. It only works till the game changes.</div><br/><div id="37875893" class="c"><input type="checkbox" id="c-37875893" checked=""/><div class="controls bullet"><span class="by">i-use-nixos-btw</span><span>|</span><a href="#37875739">parent</a><span>|</span><a href="#37877027">next</a><span>|</span><label class="collapse" for="c-37875893">[-]</label><label class="expand" for="c-37875893">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, that’s part of the game though. You can’t get perfection from modelling a complex system with the (relatively) few variables that you can actually measure. Assumptions are always evolving, and are always going to be broken at some point or another.<p>That’s the opening line, right? Uncertainty is a fact of life. With time series forecasts, the best you can ever hope to do is give probability bounds, and even then you can only really do so by either:<p>- limiting by the rules of the game (e.g. the laws of physics, or the rules of a stock exchange)<p>- using past data<p>The former is only useful if you’re the most risk averse person on the planet, and the latter is only useful if you are willing to assume the past is relevant.</div><br/><div id="37876248" class="c"><input type="checkbox" id="c-37876248" checked=""/><div class="controls bullet"><span class="by">AndrewKemendo</span><span>|</span><a href="#37875739">root</a><span>|</span><a href="#37875893">parent</a><span>|</span><a href="#37877027">next</a><span>|</span><label class="collapse" for="c-37876248">[-]</label><label class="expand" for="c-37876248">[1 more]</label></div><br/><div class="children"><div class="content">Good response. People seem to think that what I call “single pass” inference is the only thing that matters - a monolithic single process system<p>When in fact the world and intelligent agents inside it are ensembles of ensembles of systems with various and changing confidence that flow and adjust as the world does</div><br/></div></div></div></div></div></div><div id="37877027" class="c"><input type="checkbox" id="c-37877027" checked=""/><div class="controls bullet"><span class="by">maxmc</span><span>|</span><a href="#37875739">prev</a><span>|</span><a href="#37876506">next</a><span>|</span><label class="collapse" for="c-37877027">[-]</label><label class="expand" for="c-37877027">[1 more]</label></div><br/><div class="children"><div class="content">Hi,<p>Max from Nixtla here. We are surprised that this has gained so much attention and are excited about both the positive and critical responses.
Some important clarifications:<p>The primary goal of this first version of the paper is to present TimeGPT-1 and showcase our preliminary findings from a large-scale experiment, demonstrating that transfer learning at this scale is indeed possible in time series. As mentioned in the paper, we deeply believe that pre-trained models can represent a very cost-effective solution (in terms of computational resources) for many applications. Please also consider that this is a pre-print version. We are working on releasing a reproducible set of experiments on a subset of the data, so stay tuned!<p>All previous work of Nixtla has been open source and we believe TimeGPT could be a viable commercial product, offering forecasting and anomaly detection out of the box for practitioners. Some interesting details were omitted because they represent a competitive advantage that we hope to leverage in order to grow the company and keep providing better solutions and continuing to build our ecosystem.<p>As some others have mentioned in the thread, we are working to onboard as many people as possible into a free trial so that more independent practitioners can validate the accuracy for their particular use cases. You can read some initial impressions of the creators Prophet [1] and GluonTS [2] or listen to an early test by the people from H20 [3]. We hope to see some more independent benchmarcks soon.<p>[1] <a href="https:&#x2F;&#x2F;x.com&#x2F;seanjtaylor&#x2F;status&#x2F;1694745912776749296?s=20" rel="nofollow noreferrer">https:&#x2F;&#x2F;x.com&#x2F;seanjtaylor&#x2F;status&#x2F;1694745912776749296?s=20</a>
[2] <a href="https:&#x2F;&#x2F;www.linkedin.com&#x2F;posts&#x2F;tim-januschowski_foundational-timeseries-neurips-activity-7115277160948342784-rvQb?utm_source=share&amp;utm_medium=member_desktop" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.linkedin.com&#x2F;posts&#x2F;tim-januschowski_foundational...</a>
[3] <a href="https:&#x2F;&#x2F;youtu.be&#x2F;N0gyDVUFPlg?si=xH8oy5cjgLm-o_WD&amp;t=457" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;N0gyDVUFPlg?si=xH8oy5cjgLm-o_WD&amp;t=457</a></div><br/></div></div><div id="37876506" class="c"><input type="checkbox" id="c-37876506" checked=""/><div class="controls bullet"><span class="by">Difwif</span><span>|</span><a href="#37877027">prev</a><span>|</span><a href="#37874977">next</a><span>|</span><label class="collapse" for="c-37876506">[-]</label><label class="expand" for="c-37876506">[3 more]</label></div><br/><div class="children"><div class="content">Aren&#x27;t LLM&#x27;s already zero shot time series predictors? Predicting next token and forecasting seem like the exact same problem. I will admit some small tweaks in tokenization could help but it seems like we&#x27;re just pretraining on a different dataset.<p>One idea I was interested in was after reading the paper on introducing pause tokens[1] was a multimodal architecture that generalizes everything to parallel time series streams of tokens in different modalities. Pause tokens make even more sense in that setup.<p>1. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.02226" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.02226</a><p>-Arm chair ML practitioner</div><br/><div id="37876856" class="c"><input type="checkbox" id="c-37876856" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#37876506">parent</a><span>|</span><a href="#37876862">next</a><span>|</span><label class="collapse" for="c-37876856">[-]</label><label class="expand" for="c-37876856">[1 more]</label></div><br/><div class="children"><div class="content">I agree - You could frame LLMs this way. Tokens over &quot;time&quot; where time just happens to be represented by discrete, sequential memory.<p>Each token could encode a specific amplitude for the signal. You could literally just have tokens [0,1,...,MAX_AMPLITUDE] and map your input signal to this range.<p>In the most extreme case, you could have 2 tokens - zero and one. This is the scheme used in DSD audio. The only tradeoff is that you need way more samples per unit time to represent the same amount of information, but there are probably some elegant perf hacks for having only 2 states to represent per sample.<p>There are probably a lot of variations on the theme where you can &quot;resample&quot; the input sequences to different token rate vs bits per token arrangements.</div><br/></div></div><div id="37876862" class="c"><input type="checkbox" id="c-37876862" checked=""/><div class="controls bullet"><span class="by">beernet</span><span>|</span><a href="#37876506">parent</a><span>|</span><a href="#37876856">prev</a><span>|</span><a href="#37874977">next</a><span>|</span><label class="collapse" for="c-37876862">[-]</label><label class="expand" for="c-37876862">[1 more]</label></div><br/><div class="children"><div class="content">That is technically true, however, they are two very different problems from a domain&#x2F;business perspective</div><br/></div></div></div></div><div id="37874977" class="c"><input type="checkbox" id="c-37874977" checked=""/><div class="controls bullet"><span class="by">braza</span><span>|</span><a href="#37876506">prev</a><span>|</span><a href="#37875385">next</a><span>|</span><label class="collapse" for="c-37874977">[-]</label><label class="expand" for="c-37874977">[1 more]</label></div><br/><div class="children"><div class="content">Huge props for the Nixtla team for the tool.<p>A personal note here is that they could done a better job on the tokens because the announcement was so grandiose and maybe they underestimated people with legit interest.<p>I’m using all libs from Nixtla with active advocation and I did not have a token; meanwhile lots of guys posting their usages on Twitter.</div><br/></div></div><div id="37875385" class="c"><input type="checkbox" id="c-37875385" checked=""/><div class="controls bullet"><span class="by">dopidopHN</span><span>|</span><a href="#37874977">prev</a><span>|</span><a href="#37875655">next</a><span>|</span><label class="collapse" for="c-37875385">[-]</label><label class="expand" for="c-37875385">[2 more]</label></div><br/><div class="children"><div class="content">Sorry I’m not in that field. 
But I did a fair amount of statistics and probability in schools.<p>Is that not what it is at this point ? Probabilities ?</div><br/><div id="37875590" class="c"><input type="checkbox" id="c-37875590" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#37875385">parent</a><span>|</span><a href="#37875655">next</a><span>|</span><label class="collapse" for="c-37875590">[-]</label><label class="expand" for="c-37875590">[1 more]</label></div><br/><div class="children"><div class="content">The word probability can mean little or much, depending on how large the model is behind it.</div><br/></div></div></div></div><div id="37875655" class="c"><input type="checkbox" id="c-37875655" checked=""/><div class="controls bullet"><span class="by">yumraj</span><span>|</span><a href="#37875385">prev</a><span>|</span><a href="#37876147">next</a><span>|</span><label class="collapse" for="c-37875655">[-]</label><label class="expand" for="c-37875655">[4 more]</label></div><br/><div class="children"><div class="content">for stock price prediction, without taking anything else into account?<p>Purely on rational basis, this, as in predicting time series data, doesn’t seem plausible. But maybe it is.</div><br/><div id="37876391" class="c"><input type="checkbox" id="c-37876391" checked=""/><div class="controls bullet"><span class="by">emmender1</span><span>|</span><a href="#37875655">parent</a><span>|</span><a href="#37876147">next</a><span>|</span><label class="collapse" for="c-37876391">[-]</label><label class="expand" for="c-37876391">[3 more]</label></div><br/><div class="children"><div class="content">by definition, any public predictable signals (sustained edge) in stock price time series have been exploited already - thereby nullifying them.</div><br/><div id="37876920" class="c"><input type="checkbox" id="c-37876920" checked=""/><div class="controls bullet"><span class="by">chronic7490</span><span>|</span><a href="#37875655">root</a><span>|</span><a href="#37876391">parent</a><span>|</span><a href="#37876147">next</a><span>|</span><label class="collapse" for="c-37876920">[-]</label><label class="expand" for="c-37876920">[2 more]</label></div><br/><div class="children"><div class="content">&gt; by definition, any public predictable signals (sustained edge) in stock price time series have been exploited already - thereby nullifying them.<p>Completely false.<p>In reality, there are more public signals than can be exploited.<p>The market is most definitely NOT efficient.</div><br/></div></div></div></div></div></div><div id="37876147" class="c"><input type="checkbox" id="c-37876147" checked=""/><div class="controls bullet"><span class="by">iFire</span><span>|</span><a href="#37875655">prev</a><span>|</span><a href="#37876215">next</a><span>|</span><label class="collapse" for="c-37876147">[-]</label><label class="expand" for="c-37876147">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t find the TimeGPT-1 model.<p>LICENSE Apache-2<p><a href="https:&#x2F;&#x2F;github.com&#x2F;Nixtla&#x2F;statsforecast&#x2F;blob&#x2F;main&#x2F;LICENSE">https:&#x2F;&#x2F;github.com&#x2F;Nixtla&#x2F;statsforecast&#x2F;blob&#x2F;main&#x2F;LICENSE</a><p>Mentions ARIMA, ETS, CES, and Theta modeling</div><br/></div></div><div id="37876215" class="c"><input type="checkbox" id="c-37876215" checked=""/><div class="controls bullet"><span class="by">inciampati</span><span>|</span><a href="#37876147">prev</a><span>|</span><a href="#37875743">next</a><span>|</span><label class="collapse" for="c-37876215">[-]</label><label class="expand" for="c-37876215">[1 more]</label></div><br/><div class="children"><div class="content">I guess they don&#x27;t compare with S4? <a href="https:&#x2F;&#x2F;hazyresearch.stanford.edu&#x2F;blog&#x2F;2022-01-14-s4-1" rel="nofollow noreferrer">https:&#x2F;&#x2F;hazyresearch.stanford.edu&#x2F;blog&#x2F;2022-01-14-s4-1</a></div><br/></div></div><div id="37875743" class="c"><input type="checkbox" id="c-37875743" checked=""/><div class="controls bullet"><span class="by">Smith42</span><span>|</span><a href="#37876215">prev</a><span>|</span><a href="#37876202">next</a><span>|</span><label class="collapse" for="c-37875743">[-]</label><label class="expand" for="c-37875743">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t the first foundation model for time series, see EarthPT from last month: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.07207" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.07207</a></div><br/></div></div><div id="37876202" class="c"><input type="checkbox" id="c-37876202" checked=""/><div class="controls bullet"><span class="by">apienx</span><span>|</span><a href="#37875743">prev</a><span>|</span><a href="#37876916">next</a><span>|</span><label class="collapse" for="c-37876202">[-]</label><label class="expand" for="c-37876202">[1 more]</label></div><br/><div class="children"><div class="content">I’d recommend this post by Manokhin. Quite insightful and helps get some perspective. <a href="https:&#x2F;&#x2F;archive.ph&#x2F;YqPSC" rel="nofollow noreferrer">https:&#x2F;&#x2F;archive.ph&#x2F;YqPSC</a></div><br/></div></div><div id="37876916" class="c"><input type="checkbox" id="c-37876916" checked=""/><div class="controls bullet"><span class="by">streamfunk191</span><span>|</span><a href="#37876202">prev</a><span>|</span><a href="#37877709">next</a><span>|</span><label class="collapse" for="c-37876916">[-]</label><label class="expand" for="c-37876916">[3 more]</label></div><br/><div class="children"><div class="content">Training an LLM on timeseries feels limited, unless I’m missing something fundamental. If LLMs are basically prediction machines, if I have an LLM trained on cross-industry timeseries data, and I want to predict orange futures how much more effective can it be? (Genuine question). Secondly, Isn’t context hyper important? Such as weather, political climate etc.</div><br/><div id="37876956" class="c"><input type="checkbox" id="c-37876956" checked=""/><div class="controls bullet"><span class="by">not2b</span><span>|</span><a href="#37876916">parent</a><span>|</span><a href="#37877080">next</a><span>|</span><label class="collapse" for="c-37876956">[-]</label><label class="expand" for="c-37876956">[1 more]</label></div><br/><div class="children"><div class="content">A long time ago when I was a grad student, I got a consulting job with a radiologist who thought that he could use digital processing techniques to predict the options market well enough to make money. He didn&#x27;t want to shell out for a real quant; he asked my prof it he knew anyone and I decided I could use the extra money. I came up with some techniques that appeared to produce a small profit, unfortunately it was a hair less than what he&#x27;d have to pay on commissions. He wanted to keep pushing but I decided to hang it up. I&#x27;m sure that there are people here who know far more about this than my decades-old experiments taught me.<p>So in principle it could work, but the problem is that these days, the big players are all doing high frequency trading with algorithms that try to predict market swings. And the big guys have an advantage: they are closer to the stock exchanges. They trade so fast that speed-of-light limitations affect who gets the trades in first. So I think the only people who could win with an LLM technique is someone who doesn&#x27;t need to pay commissions (a market maker, Goldman Sachs or similar) with access to real time data, very close to the exchange so they get it fast.</div><br/></div></div><div id="37877080" class="c"><input type="checkbox" id="c-37877080" checked=""/><div class="controls bullet"><span class="by">ezekiel68</span><span>|</span><a href="#37876916">parent</a><span>|</span><a href="#37876956">prev</a><span>|</span><a href="#37877709">next</a><span>|</span><label class="collapse" for="c-37877080">[-]</label><label class="expand" for="c-37877080">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Isn&#x27;t context hyper important? Such as weather, political climate etc<p>(tongue firmly in cheek) Here is a bastardization of a memory of some video interview with a quantitative analyst, from over a decade ago:<p>&quot;Show us your yield data and we&#x27;ll TELL you your weather and political climate.&quot;</div><br/></div></div></div></div><div id="37877709" class="c"><input type="checkbox" id="c-37877709" checked=""/><div class="controls bullet"><span class="by">chrgy</span><span>|</span><a href="#37876916">prev</a><span>|</span><a href="#37875443">next</a><span>|</span><label class="collapse" for="c-37877709">[-]</label><label class="expand" for="c-37877709">[1 more]</label></div><br/><div class="children"><div class="content">Author could probably make more impact if you have open sourced their models, the way it is presented looks like ClosedAI sort of pathway. Meaning using papers as a way to advertise their model for developers.</div><br/></div></div><div id="37875443" class="c"><input type="checkbox" id="c-37875443" checked=""/><div class="controls bullet"><span class="by">bethekind</span><span>|</span><a href="#37877709">prev</a><span>|</span><a href="#37876545">next</a><span>|</span><label class="collapse" for="c-37875443">[-]</label><label class="expand" for="c-37875443">[1 more]</label></div><br/><div class="children"><div class="content">They include NHITS in their benchmark, but not much else.<p>I would have liked to see more detailed benchmarking, such as the electricity, exchange and weather benchmarks.</div><br/></div></div><div id="37876545" class="c"><input type="checkbox" id="c-37876545" checked=""/><div class="controls bullet"><span class="by">winddude</span><span>|</span><a href="#37875443">prev</a><span>|</span><a href="#37875708">next</a><span>|</span><label class="collapse" for="c-37876545">[-]</label><label class="expand" for="c-37876545">[1 more]</label></div><br/><div class="children"><div class="content">Where&#x27;s the dataset? Without the dataset it&#x27;s impossible to back-test, for finance for example, I&#x27;m assuming a large part of that is US stock tickers, or FRED public data, so it&#x27;s almost certain it&#x27;s seen data people would want to back-test on.<p>Also where&#x27;s the model?</div><br/></div></div><div id="37875708" class="c"><input type="checkbox" id="c-37875708" checked=""/><div class="controls bullet"><span class="by">the_optimist</span><span>|</span><a href="#37876545">prev</a><span>|</span><a href="#37876915">next</a><span>|</span><label class="collapse" for="c-37875708">[-]</label><label class="expand" for="c-37875708">[4 more]</label></div><br/><div class="children"><div class="content">This is an advertisement.</div><br/><div id="37875793" class="c"><input type="checkbox" id="c-37875793" checked=""/><div class="controls bullet"><span class="by">blorenz</span><span>|</span><a href="#37875708">parent</a><span>|</span><a href="#37875769">next</a><span>|</span><label class="collapse" for="c-37875793">[-]</label><label class="expand" for="c-37875793">[1 more]</label></div><br/><div class="children"><div class="content">Unfortunately, this is my takeaway too. It feels eerily similar to crypto where they publish a white paper then attempt to profit off their coin.</div><br/></div></div><div id="37875769" class="c"><input type="checkbox" id="c-37875769" checked=""/><div class="controls bullet"><span class="by">IOT_Apprentice</span><span>|</span><a href="#37875708">parent</a><span>|</span><a href="#37875793">prev</a><span>|</span><a href="#37876915">next</a><span>|</span><label class="collapse" for="c-37875769">[-]</label><label class="expand" for="c-37875769">[2 more]</label></div><br/><div class="children"><div class="content">How else might we know it exists?</div><br/><div id="37875994" class="c"><input type="checkbox" id="c-37875994" checked=""/><div class="controls bullet"><span class="by">the_optimist</span><span>|</span><a href="#37875708">root</a><span>|</span><a href="#37875769">parent</a><span>|</span><a href="#37876915">next</a><span>|</span><label class="collapse" for="c-37875994">[-]</label><label class="expand" for="c-37875994">[1 more]</label></div><br/><div class="children"><div class="content">Well, one way would be word of mouth from performance and experience. This is not that.  Furthermore, there’s no way to assess that from this.</div><br/></div></div></div></div></div></div><div id="37876915" class="c"><input type="checkbox" id="c-37876915" checked=""/><div class="controls bullet"><span class="by">davesque</span><span>|</span><a href="#37875708">prev</a><span>|</span><a href="#37876573">next</a><span>|</span><label class="collapse" for="c-37876915">[-]</label><label class="expand" for="c-37876915">[1 more]</label></div><br/><div class="children"><div class="content">Not sure why this is getting so many upvotes.  There is no concrete information in the paper about how the model actually works or what differentiates it from other models.</div><br/></div></div><div id="37876573" class="c"><input type="checkbox" id="c-37876573" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#37876915">prev</a><span>|</span><a href="#37875347">next</a><span>|</span><label class="collapse" for="c-37876573">[-]</label><label class="expand" for="c-37876573">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps a stupid question, but why train it only on time series data and not in conjunction with e.g. news sources like the Financial Times, etc., since LLMs are good at language so why not use it?</div><br/></div></div><div id="37875347" class="c"><input type="checkbox" id="c-37875347" checked=""/><div class="controls bullet"><span class="by">ComplexSystems</span><span>|</span><a href="#37876573">prev</a><span>|</span><a href="#37875540">next</a><span>|</span><label class="collapse" for="c-37875347">[-]</label><label class="expand" for="c-37875347">[2 more]</label></div><br/><div class="children"><div class="content">Could this be used for audio signal processing to do cool stuff?</div><br/><div id="37876523" class="c"><input type="checkbox" id="c-37876523" checked=""/><div class="controls bullet"><span class="by">LastTrain</span><span>|</span><a href="#37875347">parent</a><span>|</span><a href="#37875540">next</a><span>|</span><label class="collapse" for="c-37876523">[-]</label><label class="expand" for="c-37876523">[1 more]</label></div><br/><div class="children"><div class="content">I’m listening.. what kinds of stuff? Zero latency pitch correction?</div><br/></div></div></div></div><div id="37875540" class="c"><input type="checkbox" id="c-37875540" checked=""/><div class="controls bullet"><span class="by">henriquenunez</span><span>|</span><a href="#37875347">prev</a><span>|</span><label class="collapse" for="c-37875540">[-]</label><label class="expand" for="c-37875540">[2 more]</label></div><br/><div class="children"><div class="content">ofc we are going to connect this to the stock market</div><br/><div id="37876564" class="c"><input type="checkbox" id="c-37876564" checked=""/><div class="controls bullet"><span class="by">jldugger</span><span>|</span><a href="#37875540">parent</a><span>|</span><label class="collapse" for="c-37876564">[-]</label><label class="expand" for="c-37876564">[1 more]</label></div><br/><div class="children"><div class="content">The M7 forecasting challenge makes this goal explicit. Not the only use of forecasting, and IMO it would be good to have other timeseries data to present to models.</div><br/></div></div></div></div></div></div></div></div></div></body></html>