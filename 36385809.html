<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1687165250065" as="style"/><link rel="stylesheet" href="styles.css?v=1687165250065"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://twitter.com/id_aa_carmack/status/1670558589746905090">Neural networks in the 1990s</a> <span class="domain">(<a href="https://twitter.com">twitter.com</a>)</span></div><div class="subtext"><span>jrott</span> | <span>60 comments</span></div><br/><div><div id="36388244" class="c"><input type="checkbox" id="c-36388244" checked=""/><div class="controls bullet"><span class="by">radq</span><span>|</span><a href="#36386710">next</a><span>|</span><label class="collapse" for="c-36388244">[-]</label><label class="expand" for="c-36388244">[2 more]</label></div><br/><div class="children"><div class="content">We were missing two architecture patterns that were needed to get deeper nets to converge: residual nets [1] which solved gradient propagation, and batch normalization [2] which solved initialization.<p>[1] Residual nets (2015): <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1512.03385" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1512.03385</a><p>[2] Batch normalization (2015): <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1502.03167" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1502.03167</a></div><br/><div id="36388610" class="c"><input type="checkbox" id="c-36388610" checked=""/><div class="controls bullet"><span class="by">hzay</span><span>|</span><a href="#36388244">parent</a><span>|</span><a href="#36386710">next</a><span>|</span><label class="collapse" for="c-36388610">[-]</label><label class="expand" for="c-36388610">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but the tweet is talking about single layer networks!</div><br/></div></div></div></div><div id="36386710" class="c"><input type="checkbox" id="c-36386710" checked=""/><div class="controls bullet"><span class="by">bvan</span><span>|</span><a href="#36388244">prev</a><span>|</span><a href="#36386219">next</a><span>|</span><label class="collapse" for="c-36386710">[-]</label><label class="expand" for="c-36386710">[2 more]</label></div><br/><div class="children"><div class="content">Yikes, I’m old. There was a lot of NN work and a lot of books available on NN’s back in the mid and late 90’s. ‘Soft computing’ was the all-encompassing term for NN, genetic algorithms, AI, expert systems, fuzzy logic, ALife and all sorts of nascent computational areas back then. I still have a bunch of issues to the monthly AI Expert magazine one could buy at a decent magazine stand. Small data-sets were definitely a limiting factor as well as limited computer power. I remember certain applied fields did embrace NN’s early on, like some civil engineers and hydrologists, who were finding some use for them. At the U of Toronto, I considered doing a PhD with a biologist who was using them to investigate vision (and got help from Hinton). Physiology was one area where you could generate “long” time-series in a relatively short period of time. Those were still the days when Intel 286&#x2F;386&#x2F;486 and lowly Pentium machines were still common currency. Computer scientists at the time didn’t yet have clear break-through commercial applications which would have attracted crazy funding. A lot of theory, little real actions.</div><br/><div id="36387031" class="c"><input type="checkbox" id="c-36387031" checked=""/><div class="controls bullet"><span class="by">p_l</span><span>|</span><a href="#36386710">parent</a><span>|</span><a href="#36386219">next</a><span>|</span><label class="collapse" for="c-36387031">[-]</label><label class="expand" for="c-36387031">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s not forget that especially early 1990s are still in shock from AI Winter and there&#x27;s essentially no funding.</div><br/></div></div></div></div><div id="36386219" class="c"><input type="checkbox" id="c-36386219" checked=""/><div class="controls bullet"><span class="by">MilStdJunkie</span><span>|</span><a href="#36386710">prev</a><span>|</span><a href="#36386000">next</a><span>|</span><label class="collapse" for="c-36386219">[-]</label><label class="expand" for="c-36386219">[7 more]</label></div><br/><div class="children"><div class="content">Data, data, data, data. 1990s don&#x27;t have wikipedia, Youtube, megapixel cameras every which where, every single adult human hooked up to a sensor package 24 hours a day, and who knows what else. I know as a 1990s guy I would never have imagined the amount of data we would eventually all throw up into the ether even ten years later, to say nothing of today. Without that corpus . .</div><br/><div id="36386512" class="c"><input type="checkbox" id="c-36386512" checked=""/><div class="controls bullet"><span class="by">reverius42</span><span>|</span><a href="#36386219">parent</a><span>|</span><a href="#36387389">next</a><span>|</span><label class="collapse" for="c-36386512">[-]</label><label class="expand" for="c-36386512">[2 more]</label></div><br/><div class="children"><div class="content">And none of those examples except Wikipedia were used to train the various LLMs. I wonder how much better multi-modal models are going to get if they start incorporating the 24&#x2F;7 sensor data from billions of people.</div><br/><div id="36386749" class="c"><input type="checkbox" id="c-36386749" checked=""/><div class="controls bullet"><span class="by">ldjkfkdsjnv</span><span>|</span><a href="#36386219">root</a><span>|</span><a href="#36386512">parent</a><span>|</span><a href="#36387389">next</a><span>|</span><label class="collapse" for="c-36386749">[-]</label><label class="expand" for="c-36386749">[1 more]</label></div><br/><div class="children"><div class="content">I cant wait for the time when someone trains a multimodal LLM on all of youtube</div><br/></div></div></div></div><div id="36387389" class="c"><input type="checkbox" id="c-36387389" checked=""/><div class="controls bullet"><span class="by">signa11</span><span>|</span><a href="#36386219">parent</a><span>|</span><a href="#36386512">prev</a><span>|</span><a href="#36386665">next</a><span>|</span><label class="collapse" for="c-36387389">[-]</label><label class="expand" for="c-36387389">[1 more]</label></div><br/><div class="children"><div class="content">gpus don&#x27;t forget the gpus ! compute was too slow for the task at hand.</div><br/></div></div><div id="36386665" class="c"><input type="checkbox" id="c-36386665" checked=""/><div class="controls bullet"><span class="by">moomoo11</span><span>|</span><a href="#36386219">parent</a><span>|</span><a href="#36387389">prev</a><span>|</span><a href="#36386000">next</a><span>|</span><label class="collapse" for="c-36386665">[-]</label><label class="expand" for="c-36386665">[3 more]</label></div><br/><div class="children"><div class="content">encyclopedia Britannia existed. I came to USA in late 90s and my school had the CD set.</div><br/><div id="36386973" class="c"><input type="checkbox" id="c-36386973" checked=""/><div class="controls bullet"><span class="by">CorrectHorseBat</span><span>|</span><a href="#36386219">root</a><span>|</span><a href="#36386665">parent</a><span>|</span><a href="#36386000">next</a><span>|</span><label class="collapse" for="c-36386973">[-]</label><label class="expand" for="c-36386973">[2 more]</label></div><br/><div class="children"><div class="content">Wikipedia is ~100 bigger than the Encyclopædia Britannica<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Wikipedia:Size_of_Wikipedia" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Wikipedia:Size_of_Wikipedia</a></div><br/><div id="36388013" class="c"><input type="checkbox" id="c-36388013" checked=""/><div class="controls bullet"><span class="by">hn_thrwn</span><span>|</span><a href="#36386219">root</a><span>|</span><a href="#36386973">parent</a><span>|</span><a href="#36386000">next</a><span>|</span><label class="collapse" for="c-36388013">[-]</label><label class="expand" for="c-36388013">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s honestly much smaller than I expected.</div><br/></div></div></div></div></div></div></div></div><div id="36386000" class="c"><input type="checkbox" id="c-36386000" checked=""/><div class="controls bullet"><span class="by">robg</span><span>|</span><a href="#36386219">prev</a><span>|</span><a href="#36386955">next</a><span>|</span><label class="collapse" for="c-36386000">[-]</label><label class="expand" for="c-36386000">[3 more]</label></div><br/><div class="children"><div class="content">Highly recommend the exercises in Rumelhart and McClelland - Parallel Distributed Processing: Explorations in the Microstructure of Cognition from 1986-1987 (two volumes)<p><a href="https:&#x2F;&#x2F;direct.mit.edu&#x2F;books&#x2F;book&#x2F;4424&#x2F;Parallel-Distributed-ProcessingExplorations-in-the" rel="nofollow noreferrer">https:&#x2F;&#x2F;direct.mit.edu&#x2F;books&#x2F;book&#x2F;4424&#x2F;Parallel-Distributed-...</a></div><br/><div id="36387293" class="c"><input type="checkbox" id="c-36387293" checked=""/><div class="controls bullet"><span class="by">watersb</span><span>|</span><a href="#36386000">parent</a><span>|</span><a href="#36388352">next</a><span>|</span><label class="collapse" for="c-36387293">[-]</label><label class="expand" for="c-36387293">[1 more]</label></div><br/><div class="children"><div class="content">I was studying computer science and AI in 1987-1990; I didn&#x27;t know it was the deepest, darkest pit of AI research despair.<p>I found the two Rumelhart &amp; McClelland books, just a single copy on the shelf at Cody&#x27;s Books, soon after publication. I worked through the examples, and was immediately convinced that this low-level approach was a way forward.<p>For some reason, none of the stressed out Comp Sci professors wanted to listen to a weirdo undergraduate, a lousy student.<p>I&#x27;m glad I was there at a reboot of AI, but my timing was lousy.</div><br/></div></div><div id="36388352" class="c"><input type="checkbox" id="c-36388352" checked=""/><div class="controls bullet"><span class="by">dunefox</span><span>|</span><a href="#36386000">parent</a><span>|</span><a href="#36387293">prev</a><span>|</span><a href="#36386955">next</a><span>|</span><label class="collapse" for="c-36388352">[-]</label><label class="expand" for="c-36388352">[1 more]</label></div><br/><div class="children"><div class="content">Does it hold up for today?</div><br/></div></div></div></div><div id="36386955" class="c"><input type="checkbox" id="c-36386955" checked=""/><div class="controls bullet"><span class="by">rm999</span><span>|</span><a href="#36386000">prev</a><span>|</span><a href="#36388733">next</a><span>|</span><label class="collapse" for="c-36386955">[-]</label><label class="expand" for="c-36386955">[1 more]</label></div><br/><div class="children"><div class="content">While my experience is not from the 90s, I think I can speak to some of why this is. For some context, I first got into neural networks in the early 2000s during my undergrad research, and my first job (mid 2000s) was at an early pioneer that developed their V1 neural network models in the 90s (there is a good chance models I evolved from those V1 models influenced decisions that impacted you, however small).<p>* First off, there was no major issue with computation. Adding more units or more layers isn&#x27;t that much more expensive. Vanishing gradients and poor regulation were a challenge and meant that increasing network size rarely improved performance empirically. This was a well known challenge up until the mid&#x2F;later 2000s.<p>* There was a major &#x27;AI winter&#x27; going on in the 90s after neural networks failed to live up to their hype in the 80s. Computer vision and NLP researchers - fields that have most famously recently been benefiting from huge neural networks - largely abandoned neural networks in the 90s. My undergrad PI at a computer vision lab told me in no uncertain terms he had no interest in neural networks, but was happy to support my interest in them. My grad school advisors had similar takes.<p>* A lot of the problems that did benefit from neural networks in the 90s&#x2F;early 2000s just needed a non-linear model, but did not need huge neural networks to do well. You can very roughly consider the first layer of a 2-layer neural network to be a series of classifiers, each tackling a different aspect of the problem (e.g. the first neuron of a spam model may activate if you have never received an email from the sender, the second if the sender is tagged as spam a lot, etc). These kinds of problems didn&#x27;t need deep, large networks, and 10-50 neuron 2-layer networks were often more than enough to fully capture the complexity of the problem. Nowadays many practitioners would throw a GBM at problems like that and can get away with O(100) shallow trees, which isn&#x27;t very different from what the small neural networks were doing back then.<p>Combined, what this means from a rough perspective, is that the researchers who really could have used larger neural networks abandoned them, and almost everyone else was fine with the small networks that were readily available. The recent surge in AI is being fueled by smarter approaches and more computation, but arguably much more importantly from a ton more data that the internet made available. That last point is the real story IMO.</div><br/></div></div><div id="36388733" class="c"><input type="checkbox" id="c-36388733" checked=""/><div class="controls bullet"><span class="by">2sk21</span><span>|</span><a href="#36386955">prev</a><span>|</span><a href="#36386053">next</a><span>|</span><label class="collapse" for="c-36388733">[-]</label><label class="expand" for="c-36388733">[1 more]</label></div><br/><div class="children"><div class="content">I have one of the early PhDs in neural networks (graduated in 1992). However my work was analytical - I was able to prove a couple of theorems about the backpropagation. I just needed a simple implementation to prove that my ideas worked so I wrote my code from scratch in C.</div><br/></div></div><div id="36386053" class="c"><input type="checkbox" id="c-36386053" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#36388733">prev</a><span>|</span><a href="#36386124">next</a><span>|</span><label class="collapse" for="c-36386053">[-]</label><label class="expand" for="c-36386053">[7 more]</label></div><br/><div class="children"><div class="content">Do you think Carmack, deep down, wonders why he let himself miss the boat on the LLM revolution? He spent golden years toiling away in Facebook, only to finally announce he was quitting to focus on AGI... only for the world to be taken by storm by transformers, GPT, Midjourney, etc.<p>If anyone could have been at the forefront of this wave, it could&#x27;ve been him.<p>And now the landscape has utterly changed and no one is even convinced they need &quot;AGI&quot;. Just a continually refined LLM hooked up to tools and other endpoints.</div><br/><div id="36387867" class="c"><input type="checkbox" id="c-36387867" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#36386053">parent</a><span>|</span><a href="#36388152">next</a><span>|</span><label class="collapse" for="c-36387867">[-]</label><label class="expand" for="c-36387867">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If anyone could have been at the forefront of this wave, it could&#x27;ve been him.<p>Why does DOOM and clever programming on a NeXT imply what you assert?</div><br/></div></div><div id="36388152" class="c"><input type="checkbox" id="c-36388152" checked=""/><div class="controls bullet"><span class="by">lyu07282</span><span>|</span><a href="#36386053">parent</a><span>|</span><a href="#36387867">prev</a><span>|</span><a href="#36387377">next</a><span>|</span><label class="collapse" for="c-36388152">[-]</label><label class="expand" for="c-36388152">[2 more]</label></div><br/><div class="children"><div class="content">I sometimes wonder what could&#x27;ve happened if he stuck to the 3d graphics space. He once was a great innovator, wolfenstein, doom then quake, he did some innovation in Rage &#x2F; id Tech 5 with infinite texture streaming but it was full of technical issues. Ultimately around doom 3 &#x2F; rage, it felt like id software wasn&#x27;t anything special anymore, they were brought out and then he left Id.<p>Now the last major innovation in the space came from epic games &#x2F; unreal engine.</div><br/><div id="36388227" class="c"><input type="checkbox" id="c-36388227" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#36386053">root</a><span>|</span><a href="#36388152">parent</a><span>|</span><a href="#36387377">next</a><span>|</span><label class="collapse" for="c-36388227">[-]</label><label class="expand" for="c-36388227">[1 more]</label></div><br/><div class="children"><div class="content">He did his best work when he wrote the entire engine alone. That&#x27;s no longer possible. You can however plausibly invent AGI alone. He said that an AGI implementation is likely simple (meaning not complex), and I agree. The difficulty is in the method not lines of code, so it&#x27;s work that fits him.</div><br/></div></div></div></div><div id="36387377" class="c"><input type="checkbox" id="c-36387377" checked=""/><div class="controls bullet"><span class="by">jojobas</span><span>|</span><a href="#36386053">parent</a><span>|</span><a href="#36388152">prev</a><span>|</span><a href="#36386208">next</a><span>|</span><label class="collapse" for="c-36387377">[-]</label><label class="expand" for="c-36387377">[1 more]</label></div><br/><div class="children"><div class="content">With everything Carmack achieved two things dumbfounded me: his sycophantic relationship with Jobs (who apparently almost succeeded in getting him to postpone his wedding so that he could appear on some Apple event) and that he would go near Facebook at all.<p>Talk about having &quot;fuck you&quot; money but just not willing to say &quot;fuck you&quot;.</div><br/></div></div><div id="36386208" class="c"><input type="checkbox" id="c-36386208" checked=""/><div class="controls bullet"><span class="by">Swizec</span><span>|</span><a href="#36386053">parent</a><span>|</span><a href="#36387377">prev</a><span>|</span><a href="#36386676">next</a><span>|</span><label class="collapse" for="c-36386208">[-]</label><label class="expand" for="c-36386208">[1 more]</label></div><br/><div class="children"><div class="content">The biggest problem with AGI is definitional. How will we know when we see it?<p>Once that little detail gets solved, who’s to say that “refined LLM hooked up to tools and other specialized LLMs” won’t be it? Sure could be.<p>But it also could not be! AGI has been right around the corner my whole life and even longer. 50 years at least. Every new AI discovery is on the verge of AGI until a few years later it hits a wall. Research is hard like that.</div><br/></div></div><div id="36386676" class="c"><input type="checkbox" id="c-36386676" checked=""/><div class="controls bullet"><span class="by">moomoo11</span><span>|</span><a href="#36386053">parent</a><span>|</span><a href="#36386208">prev</a><span>|</span><a href="#36386124">next</a><span>|</span><label class="collapse" for="c-36386676">[-]</label><label class="expand" for="c-36386676">[1 more]</label></div><br/><div class="children"><div class="content">maybe he gets to go to Mars and set up a research facility there.</div><br/></div></div></div></div><div id="36386124" class="c"><input type="checkbox" id="c-36386124" checked=""/><div class="controls bullet"><span class="by">waivej</span><span>|</span><a href="#36386053">prev</a><span>|</span><a href="#36386156">next</a><span>|</span><label class="collapse" for="c-36386124">[-]</label><label class="expand" for="c-36386124">[1 more]</label></div><br/><div class="children"><div class="content">I got exposed to programming neural networks in the early 90s.  It solved certain problems incredibly fast like the traveling salesman problem.  I was tinkering with 3D graphics and fractals and map pathfinding.  Though it didn’t occur to me how much more power was there.<p>“Data” was so much smaller then.  I had a minuscule hard drive if any, no internet, 8 bit graphics but nothing photo realistic, glimpses of windows and os2, and barely a mouse.  In retrospect, it was like embedded programming.</div><br/></div></div><div id="36386156" class="c"><input type="checkbox" id="c-36386156" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#36386124">prev</a><span>|</span><a href="#36386023">next</a><span>|</span><label class="collapse" for="c-36386156">[-]</label><label class="expand" for="c-36386156">[11 more]</label></div><br/><div class="children"><div class="content">I believe the issue was not a lack of computational power, but rather that people at the time didn&#x27;t think large models with many parameters would effect meaningful change. This was even true three years ago, albeit on a different scale. As Ilya Sutskever expressed, people were not convinced there was still room to increase the scale. For the status quo to shift, two things could happen: a substantial reduction in computing costs, making large-scale experiments less a matter of conviction and more a matter of course; or the emergence of individuals with the resources and conviction to undertake larger experiments.</div><br/><div id="36386626" class="c"><input type="checkbox" id="c-36386626" checked=""/><div class="controls bullet"><span class="by">Palomides</span><span>|</span><a href="#36386156">parent</a><span>|</span><a href="#36386495">next</a><span>|</span><label class="collapse" for="c-36386626">[-]</label><label class="expand" for="c-36386626">[6 more]</label></div><br/><div class="children"><div class="content">is that really true? a modern high end GPU has more computing power than the top 20 supercomputers of the year 2000 added together</div><br/><div id="36387474" class="c"><input type="checkbox" id="c-36387474" checked=""/><div class="controls bullet"><span class="by">shagie</span><span>|</span><a href="#36386156">root</a><span>|</span><a href="#36386626">parent</a><span>|</span><a href="#36386840">next</a><span>|</span><label class="collapse" for="c-36387474">[-]</label><label class="expand" for="c-36387474">[2 more]</label></div><br/><div class="children"><div class="content">My favorite comparison for the accessibility of power is looking at a weird computer in the top 500 from a while back.<p>System X, in 2004 was the 7th most powerful computer in the world.  It was 1100 PowerPC 970 Macs with 2200 cores and claimed an Rmax of 12k GFlops. <a href="https:&#x2F;&#x2F;www.top500.org&#x2F;system&#x2F;173736&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.top500.org&#x2F;system&#x2F;173736&#x2F;</a><p>A M1 MacBook Air hits 900 Gflops ( <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26333369">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26333369</a> ).  A dozen MacBook Airs - about what you&#x27;d expect in a grade school computer lab - is more powerful than the 7th most powerful computer system in the world 2 decades ago.</div><br/><div id="36388540" class="c"><input type="checkbox" id="c-36388540" checked=""/><div class="controls bullet"><span class="by">l33tman</span><span>|</span><a href="#36386156">root</a><span>|</span><a href="#36387474">parent</a><span>|</span><a href="#36386840">next</a><span>|</span><label class="collapse" for="c-36388540">[-]</label><label class="expand" for="c-36388540">[1 more]</label></div><br/><div class="children"><div class="content">The RTX 4090 GPU (a single PCI card) hits 82k GFlops in FP32</div><br/></div></div></div></div><div id="36386840" class="c"><input type="checkbox" id="c-36386840" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#36386156">root</a><span>|</span><a href="#36386626">parent</a><span>|</span><a href="#36387474">prev</a><span>|</span><a href="#36386495">next</a><span>|</span><label class="collapse" for="c-36386840">[-]</label><label class="expand" for="c-36386840">[3 more]</label></div><br/><div class="children"><div class="content">Computers are undoubtedly more powerful now than they were in the 90s. Although computing capabilities of the 90s seem weak compared to today&#x27;s standards, they were not so inadequate that we couldn&#x27;t train and run a network comprising thousands of parameters. I vividly recall the early 2000s when I was in college. Neural networks were seen as a sort of &quot;fringe&quot; technology in a series of statistics courses. We were mostly shown examples with 6 or 12 neurons, and nobody mentioned the possibility of scaling up to hundreds of neurons. Around that time, we already had sophisticated games like The Elder Scrolls III. We could have easily scaled up the network size by at least an order of magnitude at home, not to mention the capabilities that big companies possessed at that time.</div><br/><div id="36387279" class="c"><input type="checkbox" id="c-36387279" checked=""/><div class="controls bullet"><span class="by">adrianN</span><span>|</span><a href="#36386156">root</a><span>|</span><a href="#36386840">parent</a><span>|</span><a href="#36386970">prev</a><span>|</span><a href="#36386495">next</a><span>|</span><label class="collapse" for="c-36387279">[-]</label><label class="expand" for="c-36387279">[1 more]</label></div><br/><div class="children"><div class="content">Around fifteen years ago my machine learning professor dismissed neural networks because training them is not a nice convex optimization problem.</div><br/></div></div></div></div></div></div><div id="36386495" class="c"><input type="checkbox" id="c-36386495" checked=""/><div class="controls bullet"><span class="by">a1369209993</span><span>|</span><a href="#36386156">parent</a><span>|</span><a href="#36386626">prev</a><span>|</span><a href="#36386251">next</a><span>|</span><label class="collapse" for="c-36386495">[-]</label><label class="expand" for="c-36386495">[2 more]</label></div><br/><div class="children"><div class="content">&gt; but rather that people at the time didn&#x27;t think large models with many parameters would effect meaningful change. This was even true three years ago, albeit on a different scale.<p>I&#x27;ve also noticed this, and want to ask: who <i>are</i> these people? Do they not have (~80-billion-neuron) brains? (And that&#x27;s <i>neurons</i>, with by most estimates thousands of synapses each; so you&#x27;re actually talking on the order of tens to hundreds of <i>trillions</i> of neural network parameters before you reach parity with biological examples.)</div><br/><div id="36387800" class="c"><input type="checkbox" id="c-36387800" checked=""/><div class="controls bullet"><span class="by">marmakoide</span><span>|</span><a href="#36386156">root</a><span>|</span><a href="#36386495">parent</a><span>|</span><a href="#36386251">next</a><span>|</span><label class="collapse" for="c-36387800">[-]</label><label class="expand" for="c-36387800">[1 more]</label></div><br/><div class="children"><div class="content">In the early 2000&#x27;s, it was believed that the topology of a neuron network was a major factor to get it to work well, and that throwing more neurons and computing power alone would not suffice. In a sense it was not wrong : convolutional nets were an early example of neuron network topology that enforced translation invariance while being parsimonious in tunable parameters.<p>An other factor was that SVM were all the rage back then, because they had nice math and fitted the computational resources of a contemporary workstation.</div><br/></div></div></div></div><div id="36386251" class="c"><input type="checkbox" id="c-36386251" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#36386156">parent</a><span>|</span><a href="#36386495">prev</a><span>|</span><a href="#36386023">next</a><span>|</span><label class="collapse" for="c-36386251">[-]</label><label class="expand" for="c-36386251">[2 more]</label></div><br/><div class="children"><div class="content">Did you post something nearly identical to this before? I feel like I read it before.</div><br/><div id="36386302" class="c"><input type="checkbox" id="c-36386302" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#36386156">root</a><span>|</span><a href="#36386251">parent</a><span>|</span><a href="#36386023">next</a><span>|</span><label class="collapse" for="c-36386302">[-]</label><label class="expand" for="c-36386302">[1 more]</label></div><br/><div class="children"><div class="content">Are you referring to other threads? No. However, I wouldn&#x27;t be surprised if other people developed similar beliefs following recent advances in large language models (LLMs). Of course, we wouldn&#x27;t achieve GPT-4 level results using only technology available before 2020, but with sufficient data and computational power, we could have accomplished much more than what was generally believed to be possible in the machine learning field at the time.</div><br/></div></div></div></div></div></div><div id="36386023" class="c"><input type="checkbox" id="c-36386023" checked=""/><div class="controls bullet"><span class="by">version_five</span><span>|</span><a href="#36386156">prev</a><span>|</span><a href="#36385957">next</a><span>|</span><label class="collapse" for="c-36386023">[-]</label><label class="expand" for="c-36386023">[2 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s more that modern automatic differentiation abstractions weren&#x27;t well known to researchers. From what I remember, even in the early 2000s when I went to school, backpropagation was basically hand coded.</div><br/></div></div><div id="36385957" class="c"><input type="checkbox" id="c-36385957" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36386023">prev</a><span>|</span><a href="#36387714">next</a><span>|</span><label class="collapse" for="c-36385957">[-]</label><label class="expand" for="c-36385957">[1 more]</label></div><br/><div class="children"><div class="content">1990s is beyond my time horizon.<p>The oldest nn I was exposed to was a image upscaler (mostly used for deinterlacing) called nnedi, which goes back to ~2007: <a href="http:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20130127123511&#x2F;http:&#x2F;&#x2F;forum.doom9.org&#x2F;showthread.php?t=12995" rel="nofollow noreferrer">http:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20130127123511&#x2F;http:&#x2F;&#x2F;forum.doom9...</a><p>nnedi3 is actually quite respectable today</div><br/></div></div><div id="36387714" class="c"><input type="checkbox" id="c-36387714" checked=""/><div class="controls bullet"><span class="by">rmnclmnt</span><span>|</span><a href="#36385957">prev</a><span>|</span><a href="#36388367">next</a><span>|</span><label class="collapse" for="c-36387714">[-]</label><label class="expand" for="c-36387714">[1 more]</label></div><br/><div class="children"><div class="content">Yeah good times! The other day I was browsing for the 999th time Steve Smith&#x27;s book &quot;The Scientist and Engineer&#x27;s Guide to Digital Signal Processing&quot;[1] and stumbled upon the chapter on NN[2]: I remember ready this when I was a student I could make sense of it and why it worked, but reading it 15 years later I find it is explained so clearly compared to other resources! (maybe experience is playing in my favor too)<p>You got a BASIC code snippet for training and inference and mos of all, there is an explicit use-case for digital filter approximation! At the time NN were treated as a tool among other ones, not a &quot;answer-to-everything&quot; type of thing.<p>I know Deep Learning opened new possibilities but a lot of time CNN&#x2F;RNN&#x2F;Transformers are definitely not needed: working on the data instead and using &quot;linear&quot; models can go really far (my 2 cents)<p>[1]: <a href="https:&#x2F;&#x2F;www.dspguide.com" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.dspguide.com</a>
[2]: <a href="http:&#x2F;&#x2F;www.dspguide.com&#x2F;ch26.htm" rel="nofollow noreferrer">http:&#x2F;&#x2F;www.dspguide.com&#x2F;ch26.htm</a></div><br/></div></div><div id="36388367" class="c"><input type="checkbox" id="c-36388367" checked=""/><div class="controls bullet"><span class="by">plun9</span><span>|</span><a href="#36387714">prev</a><span>|</span><a href="#36385960">next</a><span>|</span><label class="collapse" for="c-36388367">[-]</label><label class="expand" for="c-36388367">[1 more]</label></div><br/><div class="children"><div class="content">1995 car automatic transmission with neural network: <a href="https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;abs&#x2F;pii&#x2F;038943049500040E" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;abs&#x2F;pii&#x2F;038943...</a></div><br/></div></div><div id="36385960" class="c"><input type="checkbox" id="c-36385960" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#36388367">prev</a><span>|</span><a href="#36388086">next</a><span>|</span><label class="collapse" for="c-36385960">[-]</label><label class="expand" for="c-36385960">[5 more]</label></div><br/><div class="children"><div class="content">I doubt it was obvious scaling up would magically work. I suspect the experiments were limited for analytic simplicity rather than computational.</div><br/><div id="36386276" class="c"><input type="checkbox" id="c-36386276" checked=""/><div class="controls bullet"><span class="by">pavon</span><span>|</span><a href="#36385960">parent</a><span>|</span><a href="#36386014">next</a><span>|</span><label class="collapse" for="c-36386276">[-]</label><label class="expand" for="c-36386276">[1 more]</label></div><br/><div class="children"><div class="content">The only ML that I ever did was a single undergrad NN class around ~2001. That was a long time ago, but I vaguely remember being taught at that time that adding more nodes rarely helped, that you were just going to overfit to your dataset and have worse results on items outside the dataset, or worse end up with a completely degenerate NN - eg that best practice was to use the minimum number of nodes that would do the job.</div><br/></div></div><div id="36386014" class="c"><input type="checkbox" id="c-36386014" checked=""/><div class="controls bullet"><span class="by">mhh__</span><span>|</span><a href="#36385960">parent</a><span>|</span><a href="#36386276">prev</a><span>|</span><a href="#36386066">next</a><span>|</span><label class="collapse" for="c-36386014">[-]</label><label class="expand" for="c-36386014">[1 more]</label></div><br/><div class="children"><div class="content">The modern slow-but-scales way of coding them also wasn&#x27;t prevalent</div><br/></div></div><div id="36386066" class="c"><input type="checkbox" id="c-36386066" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#36385960">parent</a><span>|</span><a href="#36386014">prev</a><span>|</span><a href="#36388086">next</a><span>|</span><label class="collapse" for="c-36386066">[-]</label><label class="expand" for="c-36386066">[2 more]</label></div><br/><div class="children"><div class="content">Why couldn&#x27;t mathematical proofs&#x2F;models have predicted or revealed this to be the case back then?</div><br/><div id="36386081" class="c"><input type="checkbox" id="c-36386081" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#36385960">root</a><span>|</span><a href="#36386066">parent</a><span>|</span><a href="#36388086">next</a><span>|</span><label class="collapse" for="c-36386081">[-]</label><label class="expand" for="c-36386081">[1 more]</label></div><br/><div class="children"><div class="content">How??</div><br/></div></div></div></div></div></div><div id="36388086" class="c"><input type="checkbox" id="c-36388086" checked=""/><div class="controls bullet"><span class="by">mjan22640</span><span>|</span><a href="#36385960">prev</a><span>|</span><a href="#36387421">next</a><span>|</span><label class="collapse" for="c-36388086">[-]</label><label class="expand" for="c-36388086">[1 more]</label></div><br/><div class="children"><div class="content">In 2012 were published results of a vision processing in the brain research, that (among other things, like the retina compressing the input) figured out that visual cortex uses convolution. That got mimicked and was a breakthrough in image recognition NN, which sparked life into the whole field.</div><br/></div></div><div id="36387421" class="c"><input type="checkbox" id="c-36387421" checked=""/><div class="controls bullet"><span class="by">hax0ron3</span><span>|</span><a href="#36388086">prev</a><span>|</span><a href="#36386796">next</a><span>|</span><label class="collapse" for="c-36387421">[-]</label><label class="expand" for="c-36387421">[1 more]</label></div><br/><div class="children"><div class="content">The 1990s gamer in me gets a kick out of seeing John Carmack and Tim Sweeney talk to each other.</div><br/></div></div><div id="36386796" class="c"><input type="checkbox" id="c-36386796" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#36387421">prev</a><span>|</span><a href="#36387816">next</a><span>|</span><label class="collapse" for="c-36386796">[-]</label><label class="expand" for="c-36386796">[3 more]</label></div><br/><div class="children"><div class="content">In 1999, our “computer vision” guy - a masters student - struggled mightily to recognize very simple things in a video stream from a UAV. Today, we would take this for granted. But back then, the computation was for all intents and purposes entirely non-existent. At best he was hoping to apply an edge detection kernel maybe once every two seconds and see if he could identify some lines and arcs and then hand code some logic to recognize things.</div><br/><div id="36387250" class="c"><input type="checkbox" id="c-36387250" checked=""/><div class="controls bullet"><span class="by">Ono-Sendai</span><span>|</span><a href="#36386796">parent</a><span>|</span><a href="#36387816">next</a><span>|</span><label class="collapse" for="c-36387250">[-]</label><label class="expand" for="c-36387250">[2 more]</label></div><br/><div class="children"><div class="content">What?  There were pentium 2 and 3 machines back then that could certainly do more than a edge detection kernel every 2 seconds.
Or do you mean on an embedded CPU?</div><br/><div id="36387289" class="c"><input type="checkbox" id="c-36387289" checked=""/><div class="controls bullet"><span class="by">bandrami</span><span>|</span><a href="#36386796">root</a><span>|</span><a href="#36387250">parent</a><span>|</span><a href="#36387816">next</a><span>|</span><label class="collapse" for="c-36387289">[-]</label><label class="expand" for="c-36387289">[1 more]</label></div><br/><div class="children"><div class="content">Software is iterative. At least when I was studying in the mid 90s people had really only just gotten the idea to do a Fourier transform of an image and look for high frequencies to indicate borders. Add ~3 decades of each generation of grad student doing slightly better than the last one.</div><br/></div></div></div></div></div></div><div id="36387816" class="c"><input type="checkbox" id="c-36387816" checked=""/><div class="controls bullet"><span class="by">gattilorenz</span><span>|</span><a href="#36386796">prev</a><span>|</span><a href="#36388310">next</a><span>|</span><label class="collapse" for="c-36387816">[-]</label><label class="expand" for="c-36387816">[1 more]</label></div><br/><div class="children"><div class="content">On the topic of AI history, I would like to set up a demo of old AI and&#x2F;or general CS research on late 90s&#x2F;early 00s Sun Ultra machines.<p>Does anyone have suggestions (and links to code!) for what would be a cool demo? I’m thinking of a haar classifier to show some object recognition&#x2F;face detection, but would appreciate more options!</div><br/></div></div><div id="36388310" class="c"><input type="checkbox" id="c-36388310" checked=""/><div class="controls bullet"><span class="by">rwmj</span><span>|</span><a href="#36387816">prev</a><span>|</span><a href="#36387204">next</a><span>|</span><label class="collapse" for="c-36388310">[-]</label><label class="expand" for="c-36388310">[1 more]</label></div><br/><div class="children"><div class="content">I knew someone in the early 90s who was making a neural network on a chip for his PhD.  The chip fitted 1 neuron.  Yes he might have used float16 to cram more in but those techniques were not known at the time.<p>There really wasn&#x27;t the compute power around at the time, and as others have pointed out there wasn&#x27;t the training data, or the cameras.</div><br/></div></div><div id="36387204" class="c"><input type="checkbox" id="c-36387204" checked=""/><div class="controls bullet"><span class="by">Ono-Sendai</span><span>|</span><a href="#36388310">prev</a><span>|</span><a href="#36386592">next</a><span>|</span><label class="collapse" for="c-36387204">[-]</label><label class="expand" for="c-36387204">[1 more]</label></div><br/><div class="children"><div class="content">I personally made a quake 2 bot using neural networks in 1999, I think it had several hundred neurons and several thousand &#x27;synapses&#x27; (parameters).
At the time that felt like a lot of parameters.  Computation wasn&#x27;t much of a limit though, I could run several NNs faster than realtime.</div><br/></div></div><div id="36386592" class="c"><input type="checkbox" id="c-36386592" checked=""/><div class="controls bullet"><span class="by">bilsbie</span><span>|</span><a href="#36387204">prev</a><span>|</span><a href="#36388215">next</a><span>|</span><label class="collapse" for="c-36386592">[-]</label><label class="expand" for="c-36386592">[1 more]</label></div><br/><div class="children"><div class="content">I remember people telling me you would just get overfitting if you made the network too big.<p>I wonder how LLM’s avoid that?</div><br/></div></div><div id="36388215" class="c"><input type="checkbox" id="c-36388215" checked=""/><div class="controls bullet"><span class="by">anthk</span><span>|</span><a href="#36386592">prev</a><span>|</span><a href="#36386685">next</a><span>|</span><label class="collapse" for="c-36388215">[-]</label><label class="expand" for="c-36388215">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;tldp.org&#x2F;HOWTO&#x2F;AI-Alife-HOWTO-1.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;tldp.org&#x2F;HOWTO&#x2F;AI-Alife-HOWTO-1.html</a><p>This was a thing in early 2000s&#x2F;late 90&#x27;s.</div><br/></div></div><div id="36386685" class="c"><input type="checkbox" id="c-36386685" checked=""/><div class="controls bullet"><span class="by">29athrowaway</span><span>|</span><a href="#36388215">prev</a><span>|</span><a href="#36386206">next</a><span>|</span><label class="collapse" for="c-36386685">[-]</label><label class="expand" for="c-36386685">[1 more]</label></div><br/><div class="children"><div class="content">In the early 90s, not only there was lower computing power but there was not that much internet connectivity, low bandwidth, no digital cameras so not that many images online, and the images the images you had were low res and low color depth. Internet giants didn&#x27;t yet exist and didn&#x27;t yet collect massive amounts of data.</div><br/></div></div><div id="36386206" class="c"><input type="checkbox" id="c-36386206" checked=""/><div class="controls bullet"><span class="by">mistrial9</span><span>|</span><a href="#36386685">prev</a><span>|</span><a href="#36386720">next</a><span>|</span><label class="collapse" for="c-36386206">[-]</label><label class="expand" for="c-36386206">[2 more]</label></div><br/><div class="children"><div class="content">definitely saw NN code in the 1990s ; I recall a hardback book with mostly red cover.. not sure of the title.. Prominent and rigorous code implementations were associated with MIT at that time (the Random Forest guy was at Berkeley in the stats department)<p><i>edit</i> yes, almost certainly Neural Networks for Pattern Recognition (1995)  thx!</div><br/><div id="36386358" class="c"><input type="checkbox" id="c-36386358" checked=""/><div class="controls bullet"><span class="by">huitzitziltzin</span><span>|</span><a href="#36386206">parent</a><span>|</span><a href="#36386720">next</a><span>|</span><label class="collapse" for="c-36386358">[-]</label><label class="expand" for="c-36386358">[1 more]</label></div><br/><div class="children"><div class="content">The book “neural networks and pattern recognition” by bishop dates to 1996 and has a red cover, at least in its current softcover iteration.<p>The random forest guy you mean is&#x2F;was Leo Breiman.  His student Adele Cutler deserves some of the credit there too.</div><br/></div></div></div></div><div id="36386720" class="c"><input type="checkbox" id="c-36386720" checked=""/><div class="controls bullet"><span class="by">FrustratedMonky</span><span>|</span><a href="#36386206">prev</a><span>|</span><label class="collapse" for="c-36386720">[-]</label><label class="expand" for="c-36386720">[1 more]</label></div><br/><div class="children"><div class="content">Reading through the twitter thread, and these comments.  It reminds me of all of the back and forth when HN discusses Psychology.<p>One side, holding a pipe, &#x27;well actually, back in 1954, I put together an analog variant of a neuron perceptron built out of old speaker cables and car parts, strung it across the living room and it could say 10 words and fetch my slippers&#x27;. &#x27;Really&#x27;, &#x27;Yes, Indubitably&#x27;.<p>The other side, It&#x27;s all, &#x27;REEEEEEEEEE&#x27;</div><br/></div></div></div></div></div></div></div></body></html>