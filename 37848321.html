<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1697101265565" as="style"/><link rel="stylesheet" href="styles.css?v=1697101265565"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2310.06625">Inverted Transformers Are Effective for Time Series Forecasting</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>beefman</span> | <span>28 comments</span></div><br/><div><div id="37850823" class="c"><input type="checkbox" id="c-37850823" checked=""/><div class="controls bullet"><span class="by">sterlind</span><span>|</span><a href="#37850005">next</a><span>|</span><label class="collapse" for="c-37850823">[-]</label><label class="expand" for="c-37850823">[13 more]</label></div><br/><div class="children"><div class="content">Gah, this paper is hard to read, but here&#x27;s my understanding:<p>Let&#x27;s say you have 100 intersections, and you want to predict the traffic on each in cars&#x2F;sec. You sample every hour, and you keep 24 hours of context, and try to predict the next 4.<p>First, you&#x27;d make 100 &quot;tokens&quot; (really stretching the meaning of token here), one for each stoplight, and loading 24 samples (the history of that stoplight) into each token, and normalize.<p>Next, you run each token through a Multi-Layer Perceptron (vanilla, old-school neural network) to make a vector of dim D.<p>Next, for each layer of the transformer, you:
1. Perform &quot;cross-attention,&quot; i.e. the query&#x2F;key&#x2F;value dance. This is how the different time series (erm, tokens) get to share information.
2. Normalize across all.
3. Run another bog-standard MLP independently on each token. This is the opportunity to examine the history of each time series.
4. Normalize again across all.<p>Then, you map each &quot;token&quot; (ugh) from being D-dimensional to 4-dimensional, so for each stoplight it predicts the traffic ahead for the next 4 hours. This is also a regular MLP.<p>So specifically, if you&#x27;re only predicting a single time series (one stoplight), this method is equivalent to running a regular neural network.<p>It also, interestingly enough, skips the cool sinusoidal position embedding that transformers use to embed token position. Fair enough, since here the time dimension is fixed and the index of the feed-forward neurons in each MLP layer corresponds (roughly) to the time index of the sample.<p>The architecture looks weird to me, but apparently it works so that&#x27;s cool! But I&#x27;m not sure how well it works, and my unscientific gut feel is that there&#x27;s a better and simpler architecture crying out to be found, because this looks a bit tortured. Like, nothing in it explicitly models the time dimension - that task is left to the MLPs - and that seems weird.</div><br/><div id="37852113" class="c"><input type="checkbox" id="c-37852113" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#37850823">parent</a><span>|</span><a href="#37853105">next</a><span>|</span><label class="collapse" for="c-37852113">[-]</label><label class="expand" for="c-37852113">[7 more]</label></div><br/><div class="children"><div class="content">I had a startup a few years ago that was in the “eh we’ve got some money left from our BigTech days, let’s buy a lottery ticket that’s also a masters degree” category.<p>And in late 2018, attention&#x2F;transformers was quite the risqué idea. We were trying to forecast price action in financial markets, and while it didn’t work (I mean really Ben), it smoked all the published stuff like DeepLOB.<p>It used learned embeddings of raw order books passed through a little conv widget to smooth a bit, and then learned embeddings of order book states before passing them through big-standard positional encoding and multi-head masked self-attention.<p>This actually worked great!<p>The thing that kills you is trying to reward-shape on the policy side to avoid getting eaten by taker fees, but it’s a broken ATM with artificially lowered fees.</div><br/><div id="37853134" class="c"><input type="checkbox" id="c-37853134" checked=""/><div class="controls bullet"><span class="by">sterlind</span><span>|</span><a href="#37850823">root</a><span>|</span><a href="#37852113">parent</a><span>|</span><a href="#37854238">next</a><span>|</span><label class="collapse" for="c-37853134">[-]</label><label class="expand" for="c-37853134">[1 more]</label></div><br/><div class="children"><div class="content">Interesting, I&#x27;m trying to understand (much less knowledgeable about finance than ML, heh.) But it sounds like you fed it the raw order books (no time dimension), a sequence of order states corresponding to each (a time series), mapped them into the embedding dimension of a decoder-only transformer (the masking), and trained it to predict logits for the next order state?<p>See, that makes way more sense to me, since it sounds like you used causal self-attention , and actual position embeddings.<p>I&#x27;ve been interested in some time series stuff, like position embeddings to model actual wall-clock time offsets rather than sequence index, but for textless NLP rather than trading.</div><br/></div></div><div id="37854238" class="c"><input type="checkbox" id="c-37854238" checked=""/><div class="controls bullet"><span class="by">anonymoushn</span><span>|</span><a href="#37850823">root</a><span>|</span><a href="#37852113">parent</a><span>|</span><a href="#37853134">prev</a><span>|</span><a href="#37853016">next</a><span>|</span><label class="collapse" for="c-37854238">[-]</label><label class="expand" for="c-37854238">[1 more]</label></div><br/><div class="children"><div class="content">XTX markets seems to be doing something in the same genre as this. As I understand it, they are mostly taker.</div><br/></div></div><div id="37853016" class="c"><input type="checkbox" id="c-37853016" checked=""/><div class="controls bullet"><span class="by">anonu</span><span>|</span><a href="#37850823">root</a><span>|</span><a href="#37852113">parent</a><span>|</span><a href="#37854238">prev</a><span>|</span><a href="#37853105">next</a><span>|</span><label class="collapse" for="c-37853016">[-]</label><label class="expand" for="c-37853016">[4 more]</label></div><br/><div class="children"><div class="content">why didnt you start a hedge fund?</div><br/><div id="37854860" class="c"><input type="checkbox" id="c-37854860" checked=""/><div class="controls bullet"><span class="by">Galanwe</span><span>|</span><a href="#37850823">root</a><span>|</span><a href="#37853016">parent</a><span>|</span><a href="#37853633">next</a><span>|</span><label class="collapse" for="c-37854860">[-]</label><label class="expand" for="c-37854860">[1 more]</label></div><br/><div class="children"><div class="content">What they describe looks like a single predictor. You can&#x27;t create a strategy with a single predictor, unless it&#x27;s incredibly predictive. 99% of the time, a predictor cannot beat its transaction costs alone.<p>You need to combine hundreds of such predictors to be able to beat costs and have a net profitable strategy.<p>We have a saying in French that you need a lot of rivers to create a sea.</div><br/></div></div><div id="37853633" class="c"><input type="checkbox" id="c-37853633" checked=""/><div class="controls bullet"><span class="by">dchftcs</span><span>|</span><a href="#37850823">root</a><span>|</span><a href="#37853016">parent</a><span>|</span><a href="#37854860">prev</a><span>|</span><a href="#37853105">next</a><span>|</span><label class="collapse" for="c-37853633">[-]</label><label class="expand" for="c-37853633">[2 more]</label></div><br/><div class="children"><div class="content">They did say it didn&#x27;t work. The overwhelming majority of finance stuff in published work doesn&#x27;t work, because they&#x27;re either too simplistic, poorly backtested, or they get exploited too quickly, so beating those doesn&#x27;t imply you can run a hedge fund.<p>The main part here is that it&#x27;s one thing to predict price action, it&#x27;s another thing to trade profitably - and in particular they were not able to beat fees, which is a common hurdle if you&#x27;re new to HFT.</div><br/><div id="37854070" class="c"><input type="checkbox" id="c-37854070" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#37850823">root</a><span>|</span><a href="#37853633">parent</a><span>|</span><a href="#37853105">next</a><span>|</span><label class="collapse" for="c-37854070">[-]</label><label class="expand" for="c-37854070">[1 more]</label></div><br/><div class="children"><div class="content">Basically this. We were heavy infra pros and my cofounder was an HFT veteran so it wasn’t classic implementation shortfall so much as we didn’t solve the “do we enter” threshold on what would be a friction-free windfall.</div><br/></div></div></div></div></div></div></div></div><div id="37853105" class="c"><input type="checkbox" id="c-37853105" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#37850823">parent</a><span>|</span><a href="#37852113">prev</a><span>|</span><a href="#37851296">next</a><span>|</span><label class="collapse" for="c-37853105">[-]</label><label class="expand" for="c-37853105">[3 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t this presuppose that all the information you need to predict the future of your time series is embedded in the past of those time series?<p>Don&#x27;t <i>most</i> time series we would be interested in predicting (weather, prices, traffic volumes) tend to respond to things outside the history of the time series in question?<p>Or is the thesis here that we throw every random time series we can think of - wave height series from buoys in the San Francisco Bay, ticket sales from Taylor Swift concerts, Teslas per hour in the Holland tunnel, sales volume of MSFT... and get this thing to find the cross-correlated leading indicators needed so it can predict them all?</div><br/><div id="37853167" class="c"><input type="checkbox" id="c-37853167" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#37850823">root</a><span>|</span><a href="#37853105">parent</a><span>|</span><a href="#37851296">next</a><span>|</span><label class="collapse" for="c-37853167">[-]</label><label class="expand" for="c-37853167">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Doesn&#x27;t this presuppose that all the information you need to predict the future of your time series is embedded in the past of those time series?<p>Yes. But usually this is somewhat valid: There might not be data about the causes in your data, but the model <i>should</i> learn not be be over confident.<p>&gt; Don&#x27;t most time series we would be interested in predicting (weather, prices, traffic volumes) tend to respond to things outside the history of the time series in question?<p>Yes and no.<p>You really want the forecast to be a probability distribution: 95% of the time it will take you X minutes to get home from work if you leave at 17:30 but 5% of the time there will be disruptions.</div><br/><div id="37854290" class="c"><input type="checkbox" id="c-37854290" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#37850823">root</a><span>|</span><a href="#37853167">parent</a><span>|</span><a href="#37851296">next</a><span>|</span><label class="collapse" for="c-37854290">[-]</label><label class="expand" for="c-37854290">[1 more]</label></div><br/><div class="children"><div class="content">Big part of it is historic dice tosses that create mirage of data just waiting to be tamed.</div><br/></div></div></div></div></div></div><div id="37851296" class="c"><input type="checkbox" id="c-37851296" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#37850823">parent</a><span>|</span><a href="#37853105">prev</a><span>|</span><a href="#37851607">next</a><span>|</span><label class="collapse" for="c-37851296">[-]</label><label class="expand" for="c-37851296">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! Thank you for explaining it in simpler terms. I get about 5% out of these papers but I got a lot more out of this break down.</div><br/></div></div><div id="37851607" class="c"><input type="checkbox" id="c-37851607" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#37850823">parent</a><span>|</span><a href="#37851296">prev</a><span>|</span><a href="#37850005">next</a><span>|</span><label class="collapse" for="c-37851607">[-]</label><label class="expand" for="c-37851607">[1 more]</label></div><br/><div class="children"><div class="content">I find crossformers easier to track:<p><a href="https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=vSVLM2j9eie" rel="nofollow noreferrer">https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=vSVLM2j9eie</a></div><br/></div></div></div></div><div id="37850005" class="c"><input type="checkbox" id="c-37850005" checked=""/><div class="controls bullet"><span class="by">i-use-nixos-btw</span><span>|</span><a href="#37850823">prev</a><span>|</span><a href="#37852863">next</a><span>|</span><label class="collapse" for="c-37850005">[-]</label><label class="expand" for="c-37850005">[10 more]</label></div><br/><div class="children"><div class="content">I’m not a ML person so forgive my ignorance here.<p>It looks interesting, but I’m slightly confused about the way this is presented. It feels like it’s coming from the wrong angle.<p>Specifically, reducing a time series to a sequence of patterns and trying to predict what happens next is something that has been done for decades in some form or another. To me the unique aspect of this is that it fits the approach into a transformer.<p>So I’d expect to see comparisons against other approaches that do the same thing, not against other transformer approaches.<p>I wouldn’t be confused if the title was “Inverted Transformers are MORE EFFECTIVE THAN NORMAL TRANSFORMERS For Time Series Forecasting”.<p>However, if the target audience are transformer folk then it makes sense, it just seems that I’m looking at it from the other direction.</div><br/><div id="37850636" class="c"><input type="checkbox" id="c-37850636" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#37850005">parent</a><span>|</span><a href="#37850321">next</a><span>|</span><label class="collapse" for="c-37850636">[-]</label><label class="expand" for="c-37850636">[3 more]</label></div><br/><div class="children"><div class="content">The equivalence relationship between efficient AI and universal sequence prediction has been known for decades, so it would be surprising if AI algorithms were poor at sequence prediction. Of course, <i>optimal</i> universal sequence prediction is profoundly intractable and memory hard, which has implications for limits of AI efficiency and scalability.<p>There used to be a small hobbyist subculture on the Internet in the late 1990s that designed highly efficient approximate universal sequence predictor algorithms for the challenge of it. Now that AI is a thing, I&#x27;ve often wondered if there were some lost insights there on maximally efficient representations of learning systems on real computers. Most of those people would be deep into retirement by now.</div><br/><div id="37853798" class="c"><input type="checkbox" id="c-37853798" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#37850005">root</a><span>|</span><a href="#37850636">parent</a><span>|</span><a href="#37853801">next</a><span>|</span><label class="collapse" for="c-37853798">[-]</label><label class="expand" for="c-37853798">[1 more]</label></div><br/><div class="children"><div class="content">There’s nothing more fun than dusting off fossilized proto-AI work and running it on modern hardware.<p>Why don’t you share some citations?<p>I always enjoyed tracking down outre typewritten connectionist manuscripts from an author who had more time than compute.</div><br/></div></div><div id="37853801" class="c"><input type="checkbox" id="c-37853801" checked=""/><div class="controls bullet"><span class="by">Roark66</span><span>|</span><a href="#37850005">root</a><span>|</span><a href="#37850636">parent</a><span>|</span><a href="#37853798">prev</a><span>|</span><a href="#37850321">next</a><span>|</span><label class="collapse" for="c-37853801">[-]</label><label class="expand" for="c-37853801">[1 more]</label></div><br/><div class="children"><div class="content">Is there anything left of their output in the Internet archive? It is an interesting subject to explore.</div><br/></div></div></div></div><div id="37850321" class="c"><input type="checkbox" id="c-37850321" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#37850005">parent</a><span>|</span><a href="#37850636">prev</a><span>|</span><a href="#37852863">next</a><span>|</span><label class="collapse" for="c-37850321">[-]</label><label class="expand" for="c-37850321">[6 more]</label></div><br/><div class="children"><div class="content">I think we&#x27;re living in a world where deep learning is winning so consistently that comparison to other methods is often just a time suck. It would be nice to provide a non-DL approach as a baseline, but I would expect it to lag behind the DL methods.<p>Furthermore, often pre-DL methods can be recast as hand-tuned special cases of DL models - some sequence of linear operations with hand-picked discontinuities sprinkled around. If you can implement the pre-DL method using standard neural network components, then gradient descent training of a neural network &quot;should&quot; find an equivalent or better solution.</div><br/><div id="37851141" class="c"><input type="checkbox" id="c-37851141" checked=""/><div class="controls bullet"><span class="by">Nevermark</span><span>|</span><a href="#37850005">root</a><span>|</span><a href="#37850321">parent</a><span>|</span><a href="#37850441">next</a><span>|</span><label class="collapse" for="c-37851141">[-]</label><label class="expand" for="c-37851141">[1 more]</label></div><br/><div class="children"><div class="content"><i>Deep learning models are not better for vast problem areas which have analytical design algorithms. Deep learning&#x27;s succession of triumphs has been across areas where analytical design has proven difficult.</i><p>First, there are many optimal, or near optimal, direct design algorithms for systems that are well characterized. These solutions are more concise, easier to analyze, reveal important insights, and come with guarantees regarding reliability, accuracy, stability, resource requirements, and operating regimes. Clear advantages over inductively learned solutions.<p>Second, just <i>assuming</i> that new algorithms are better than older algorithms is completely irrational. An anathema to the purpose and benefits of science, math, and responsible research in general.<p>If you are going to propose new algorithms, you need to compare the new algorithm against the previous state of the art.<p>Otherwise practitioners and future researchers will be driven into deadends, deploy pointlessly bad designs, forget important knowledge, and worst of all, lose out on what older algorithms can suggest for improving newer algorithms. With no excuse but gross carelessness.</div><br/></div></div><div id="37850441" class="c"><input type="checkbox" id="c-37850441" checked=""/><div class="controls bullet"><span class="by">Muller20</span><span>|</span><a href="#37850005">root</a><span>|</span><a href="#37850321">parent</a><span>|</span><a href="#37851141">prev</a><span>|</span><a href="#37850875">next</a><span>|</span><label class="collapse" for="c-37850441">[-]</label><label class="expand" for="c-37850441">[1 more]</label></div><br/><div class="children"><div class="content">This something that DL researchers like to think but it is definitely not true for time series forecasting. See <a href="https:&#x2F;&#x2F;forecastingdata.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;forecastingdata.org&#x2F;</a> for some examples where simple non-DL approaches beat state-of-the-art DL systems.</div><br/></div></div><div id="37850875" class="c"><input type="checkbox" id="c-37850875" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#37850005">root</a><span>|</span><a href="#37850321">parent</a><span>|</span><a href="#37850441">prev</a><span>|</span><a href="#37850794">next</a><span>|</span><label class="collapse" for="c-37850875">[-]</label><label class="expand" for="c-37850875">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I think we&#x27;re living in a world where deep learning is winning so consistently that comparison to other methods is often just a time suck.<p>This is quite untrue. DL methods work well when there’s a lot of data in closed domains. DL works well by learning from corpuses of text and media where it can make reasonable interpolations.<p>When you don’t have enough data and you don’t have a known foundational model that you can do zero shot from, DL doesn’t work better than simpler conventional methods.</div><br/></div></div><div id="37850794" class="c"><input type="checkbox" id="c-37850794" checked=""/><div class="controls bullet"><span class="by">jldugger</span><span>|</span><a href="#37850005">root</a><span>|</span><a href="#37850321">parent</a><span>|</span><a href="#37850875">prev</a><span>|</span><a href="#37851788">next</a><span>|</span><label class="collapse" for="c-37850794">[-]</label><label class="expand" for="c-37850794">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It would be nice to provide a non-DL approach as a baseline, but I would expect it to lag behind the DL methods.<p>The M# competitions have usually shown very old forecasting algorithms work quite well, with frankly, way less training overhead and data. Ensemble models usually do best, but for a lot of use cases, DL is probably overkill versus ARIMA or triple exponential smoothing.</div><br/></div></div><div id="37851788" class="c"><input type="checkbox" id="c-37851788" checked=""/><div class="controls bullet"><span class="by">wenc</span><span>|</span><a href="#37850005">root</a><span>|</span><a href="#37850321">parent</a><span>|</span><a href="#37850794">prev</a><span>|</span><a href="#37852863">next</a><span>|</span><label class="collapse" for="c-37851788">[-]</label><label class="expand" for="c-37851788">[1 more]</label></div><br/><div class="children"><div class="content">DL also don’t win at medium scale tabular data. This paper mentions why and how DL could might better (if it indeed can, with limited sized data)<p>Why do tree-based models still outperform deep learning on tabular data?<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.08815" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.08815</a></div><br/></div></div></div></div></div></div><div id="37852863" class="c"><input type="checkbox" id="c-37852863" checked=""/><div class="controls bullet"><span class="by">davesque</span><span>|</span><a href="#37850005">prev</a><span>|</span><a href="#37849850">next</a><span>|</span><label class="collapse" for="c-37852863">[-]</label><label class="expand" for="c-37852863">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like the basic idea is as follows:<p>Typical transformers apply self-attention between tokens that vary across time.  So the dot product values for each pair of tokens in the resulting attention (correlation) matrix are basically dot products between pairs of moments in time.<p>The iTransformer authors seem to be saying that, for certain time series forecasting tasks, it&#x27;s not correct to assume that embedding channels of tokens across moments in time represent data that was collected at precisely the same moment or with similar instruments.  In reality, different varieties of data are sometimes not precisely aligned in a data set and also have very different distributions relating to how the data was collected.<p>So the iTransformer model proposes to apply self-attention across embedding channels instead of across time.  Self-attention otherwise seems to work in the same way.  Query and key matrices are calculated but they project each embedding channel separately instead of projecting a collection of channel values at a single moment.  Then the query-key calculation finds the degree to which all the entirely independent time series (embedding channels) are correlated.  Those correlations are used to weight the value vectors and obtain new embedding channels that are weighted averages.<p>Then the feed-forward layer projects each channel independently, instead of projecting across channels as it would do in a standard transformer model.<p>Also, since layer normalization acts within an embedding channel, they claim that this can reduce noise that would result from normalizing data across channels that were collected using different methods.  The distribution characteristics of each channel stay within the channel instead of bleeding across channels and potentially deleting information.<p>They lay out more of their reasoning for taking this approach in the paper and I feel like I agree with their intuitions.  But the paper needs some serious proof reading.  It&#x27;s very hard to parse the verbiage.</div><br/></div></div><div id="37849850" class="c"><input type="checkbox" id="c-37849850" checked=""/><div class="controls bullet"><span class="by">whoa_now</span><span>|</span><a href="#37852863">prev</a><span>|</span><a href="#37849939">next</a><span>|</span><label class="collapse" for="c-37849850">[-]</label><label class="expand" for="c-37849850">[2 more]</label></div><br/><div class="children"><div class="content">Impressive. What turns ratio are we talking about here?</div><br/><div id="37849955" class="c"><input type="checkbox" id="c-37849955" checked=""/><div class="controls bullet"><span class="by">smolder</span><span>|</span><a href="#37849850">parent</a><span>|</span><a href="#37849939">next</a><span>|</span><label class="collapse" for="c-37849955">[-]</label><label class="expand" for="c-37849955">[1 more]</label></div><br/><div class="children"><div class="content">The real question, I think, is autobot or decepticon?</div><br/></div></div></div></div></div></div></div></div></div></body></html>