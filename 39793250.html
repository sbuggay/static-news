<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1711184455146" as="style"/><link rel="stylesheet" href="styles.css?v=1711184455146"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2402.02622">DenseFormer: Enhancing Information Flow in Transformers</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>tipsytoad</span> | <span>26 comments</span></div><br/><div><div id="39795841" class="c"><input type="checkbox" id="c-39795841" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#39794906">next</a><span>|</span><label class="collapse" for="c-39795841">[-]</label><label class="expand" for="c-39795841">[5 more]</label></div><br/><div class="children"><div class="content">The architecture changes are very straight forward. Model merging has shown that pre-trained transformer layers are very robust. I’ll bet it’s possible to fine tune a pre-trained model like mistral to use this architecture. That would enable someone to test it with more parameters without training a whole new base model.</div><br/><div id="39796285" class="c"><input type="checkbox" id="c-39796285" checked=""/><div class="controls bullet"><span class="by">numeri</span><span>|</span><a href="#39795841">parent</a><span>|</span><a href="#39796597">next</a><span>|</span><label class="collapse" for="c-39796285">[-]</label><label class="expand" for="c-39796285">[1 more]</label></div><br/><div class="children"><div class="content">They try this in the appendix without success, unfortunately. It seems having this enabled early on in training is important.</div><br/></div></div><div id="39796597" class="c"><input type="checkbox" id="c-39796597" checked=""/><div class="controls bullet"><span class="by">bilsbie</span><span>|</span><a href="#39795841">parent</a><span>|</span><a href="#39796285">prev</a><span>|</span><a href="#39794906">next</a><span>|</span><label class="collapse" for="c-39796597">[-]</label><label class="expand" for="c-39796597">[3 more]</label></div><br/><div class="children"><div class="content">I haven’t been able to make sense of model merging. Any insights?<p>Wouldn’t weights between models be completely different? And then there are architecture differences on top of that.</div><br/><div id="39796718" class="c"><input type="checkbox" id="c-39796718" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#39795841">root</a><span>|</span><a href="#39796597">parent</a><span>|</span><a href="#39794906">next</a><span>|</span><label class="collapse" for="c-39796718">[-]</label><label class="expand" for="c-39796718">[2 more]</label></div><br/><div class="children"><div class="content">Model merging is usually done with different fine-tunes of the same model. It doesn’t work if the base models are different.<p>One of the more surprising things is that you can actually repeat layers to improve model performance, ie 1-1-2-2 instead of 1-2. That’s how you get models with higher parameter counts than the original.</div><br/><div id="39796815" class="c"><input type="checkbox" id="c-39796815" checked=""/><div class="controls bullet"><span class="by">namibj</span><span>|</span><a href="#39795841">root</a><span>|</span><a href="#39796718">parent</a><span>|</span><a href="#39794906">next</a><span>|</span><label class="collapse" for="c-39796815">[-]</label><label class="expand" for="c-39796815">[1 more]</label></div><br/><div class="children"><div class="content">C.f. also Universal Transformer: the same layer stacked a lot.
The sparse version of that is basically MoE with also a stick-breaking mechanism to prevent vanishing gradient while letting the model decide whether to terminate layer-count at a token early (ofc with training rewards to favor less layers, to represent the compute savings).</div><br/></div></div></div></div></div></div></div></div><div id="39794906" class="c"><input type="checkbox" id="c-39794906" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39795841">prev</a><span>|</span><a href="#39795683">next</a><span>|</span><label class="collapse" for="c-39794906">[-]</label><label class="expand" for="c-39794906">[8 more]</label></div><br/><div class="children"><div class="content">This method has only been tested on tiny models (&lt;1B) and tiny dataset (17B tokens). It’s not clear if it scales.</div><br/><div id="39795135" class="c"><input type="checkbox" id="c-39795135" checked=""/><div class="controls bullet"><span class="by">ml_basics</span><span>|</span><a href="#39794906">parent</a><span>|</span><a href="#39795951">next</a><span>|</span><label class="collapse" for="c-39795135">[-]</label><label class="expand" for="c-39795135">[3 more]</label></div><br/><div class="children"><div class="content">To be fair to the authors they are affiliated with a university and not a big industrial lab, so they may be working with significantly constrained resources. Not sure exactly what the best solution is for this case given that it affects most people outside of a very select few.</div><br/><div id="39795804" class="c"><input type="checkbox" id="c-39795804" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39794906">root</a><span>|</span><a href="#39795135">parent</a><span>|</span><a href="#39795951">next</a><span>|</span><label class="collapse" for="c-39795804">[-]</label><label class="expand" for="c-39795804">[2 more]</label></div><br/><div class="children"><div class="content">They could partner with big industrial labs.</div><br/><div id="39797043" class="c"><input type="checkbox" id="c-39797043" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39794906">root</a><span>|</span><a href="#39795804">parent</a><span>|</span><a href="#39795951">next</a><span>|</span><label class="collapse" for="c-39797043">[-]</label><label class="expand" for="c-39797043">[1 more]</label></div><br/><div class="children"><div class="content">Nah, nobody&#x27;s begging for people to A) come use time on their GPUs B) come watch them train their biggest models. Nor does it make sense to spend $X00M training a big model using an experimental technique before you announce it, nor does it make sense to hold back breakthroughs as an academic until someone commercializes it at scale. Category error.</div><br/></div></div></div></div></div></div><div id="39795951" class="c"><input type="checkbox" id="c-39795951" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#39794906">parent</a><span>|</span><a href="#39795135">prev</a><span>|</span><a href="#39795383">next</a><span>|</span><label class="collapse" for="c-39795951">[-]</label><label class="expand" for="c-39795951">[3 more]</label></div><br/><div class="children"><div class="content">If a genie appeared and granted one wish, I would wish that we find an extremely powerful machine learning technique that doesn&#x27;t scale. Imagine if an average desktop computer was almost as good as a billion dollar super computer.<p>In other words, I don&#x27;t really care if it scales. I almost hope it doesn&#x27;t.</div><br/><div id="39796150" class="c"><input type="checkbox" id="c-39796150" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39794906">root</a><span>|</span><a href="#39795951">parent</a><span>|</span><a href="#39796687">next</a><span>|</span><label class="collapse" for="c-39796150">[-]</label><label class="expand" for="c-39796150">[1 more]</label></div><br/><div class="children"><div class="content">Not sure I understand what you mean by “doesn’t scale”. Are you trying to say you would like to see a tiny model performing as well as a large model?</div><br/></div></div><div id="39796687" class="c"><input type="checkbox" id="c-39796687" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#39794906">root</a><span>|</span><a href="#39795951">parent</a><span>|</span><a href="#39796150">prev</a><span>|</span><a href="#39795383">next</a><span>|</span><label class="collapse" for="c-39796687">[-]</label><label class="expand" for="c-39796687">[1 more]</label></div><br/><div class="children"><div class="content">Even pocket computers (smartphones) are already better than billion dollar supercomputers from decades past.<p>What is your point?</div><br/></div></div></div></div><div id="39795383" class="c"><input type="checkbox" id="c-39795383" checked=""/><div class="controls bullet"><span class="by">jal278</span><span>|</span><a href="#39794906">parent</a><span>|</span><a href="#39795951">prev</a><span>|</span><a href="#39795683">next</a><span>|</span><label class="collapse" for="c-39795383">[-]</label><label class="expand" for="c-39795383">[1 more]</label></div><br/><div class="children"><div class="content">But it may scale -- that&#x27;s science in progress</div><br/></div></div></div></div><div id="39795683" class="c"><input type="checkbox" id="c-39795683" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#39794906">prev</a><span>|</span><a href="#39794626">next</a><span>|</span><label class="collapse" for="c-39795683">[-]</label><label class="expand" for="c-39795683">[1 more]</label></div><br/><div class="children"><div class="content">This is a very interesting idea, with DenseNets there are oftentimes some terrible memory gotchas that have gotten me over the past 7-8 years or so, so a part of me is sorta leaning back waiting for some memory usage shoe to drop not specified in the paper (even with the activation patterns!)<p>However, maybe this is not the case. I have a bit of a history of messing with residuals in neural networks, seeing more work on it is good. Fast training networks of course are a very slightly mild obsession of mine as well, and very useful to the field. Here&#x27;s hoping it pans out as a motif, curious to see where it goes.</div><br/></div></div><div id="39794626" class="c"><input type="checkbox" id="c-39794626" checked=""/><div class="controls bullet"><span class="by">sp332</span><span>|</span><a href="#39795683">prev</a><span>|</span><a href="#39795118">next</a><span>|</span><label class="collapse" for="c-39794626">[-]</label><label class="expand" for="c-39794626">[1 more]</label></div><br/><div class="children"><div class="content">Even better is the result on page 7 that perplexity drops faster by wall-clock time. Even if you&#x27;re getting fewer iterations per hour of rented GPU time, you&#x27;re still coming out ahead in model performance.</div><br/></div></div><div id="39795118" class="c"><input type="checkbox" id="c-39795118" checked=""/><div class="controls bullet"><span class="by">ml_basics</span><span>|</span><a href="#39794626">prev</a><span>|</span><a href="#39797354">next</a><span>|</span><label class="collapse" for="c-39795118">[-]</label><label class="expand" for="c-39795118">[5 more]</label></div><br/><div class="children"><div class="content">Cool paper. Really interesting to see how even quite straightforward architectural modifications haven&#x27;t yet all been exhausted yet, despite all the resources being poured into LLMs</div><br/><div id="39795576" class="c"><input type="checkbox" id="c-39795576" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39795118">parent</a><span>|</span><a href="#39797354">next</a><span>|</span><label class="collapse" for="c-39795576">[-]</label><label class="expand" for="c-39795576">[4 more]</label></div><br/><div class="children"><div class="content">The problem is that they have to be tested for 7B models at least to show promise for larger models. And that requires significant compute resources.</div><br/><div id="39795661" class="c"><input type="checkbox" id="c-39795661" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#39795118">root</a><span>|</span><a href="#39795576">parent</a><span>|</span><a href="#39796007">next</a><span>|</span><label class="collapse" for="c-39795661">[-]</label><label class="expand" for="c-39795661">[1 more]</label></div><br/><div class="children"><div class="content">Due to some of my personal experiences over the years w&#x2F; model development, I believe that this is more due to a failure of the current mainline version of Transformers (the ++ version I believe) not scaling properly, vs an indicator of scale.<p>If that is the case, then it may well be possible to fix some of the scaling issues more apparent with smaller transformer models (maybe not, though). This is at least some of the reasoning that I&#x27;ve been applying when developing hlb-gpt, for example. It&#x27;s partially also why I think changing how we use nonlinearities within the network might impact scaling, due to some of the activation spikes used in more linear regions of the network to control network behavior in a way not originally intended.<p>Agreed that it does require a ton of resources though. But I do think that the problem can be solved on a smaller scale. If we don&#x27;t have a cleanly logarithmic curve, then I think that something is dearly wrong with our base architecture. (However, of course, I may entirely be missing something here).</div><br/></div></div><div id="39796007" class="c"><input type="checkbox" id="c-39796007" checked=""/><div class="controls bullet"><span class="by">quotemstr</span><span>|</span><a href="#39795118">root</a><span>|</span><a href="#39795576">parent</a><span>|</span><a href="#39795661">prev</a><span>|</span><a href="#39797354">next</a><span>|</span><label class="collapse" for="c-39796007">[-]</label><label class="expand" for="c-39796007">[2 more]</label></div><br/><div class="children"><div class="content">I wonder whether we&#x27;re missing out on techniques that work well on large models but that don&#x27;t show promise on small ones</div><br/><div id="39796242" class="c"><input type="checkbox" id="c-39796242" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39795118">root</a><span>|</span><a href="#39796007">parent</a><span>|</span><a href="#39797354">next</a><span>|</span><label class="collapse" for="c-39796242">[-]</label><label class="expand" for="c-39796242">[1 more]</label></div><br/><div class="children"><div class="content">More like we&#x27;re missing out on techniques full stop. Proving things at scale is GPU expensive and gatekeeps publication and therefore accessibility.</div><br/></div></div></div></div></div></div></div></div><div id="39797354" class="c"><input type="checkbox" id="c-39797354" checked=""/><div class="controls bullet"><span class="by">zwaps</span><span>|</span><a href="#39795118">prev</a><span>|</span><a href="#39796345">next</a><span>|</span><label class="collapse" for="c-39797354">[-]</label><label class="expand" for="c-39797354">[1 more]</label></div><br/><div class="children"><div class="content">1. They compare with an older sort of standard implementation of a transformer Unsure whether the results would be equally significant compared to models with gated units or multiquery etc.<p>2. The difference seems to diminish with scale. Real life transformers obviously are much larger and train on many more tokens.<p>3. A very significant part of training transformer models are the throughoutput and memory optimizations. I wonder how their model would work with such fused kernels or specialized paged KV cache schemes. Or activation checkpointing, if run locally.<p>4. Indeed they claim no memory impact, but their code shows that their experiments are conducted with a special optimized version which requires all activations to reside in a single tensor at all times. Not sure this would work with 3d parallelism on multiple nodes etc.</div><br/></div></div><div id="39796345" class="c"><input type="checkbox" id="c-39796345" checked=""/><div class="controls bullet"><span class="by">efrank3</span><span>|</span><a href="#39797354">prev</a><span>|</span><a href="#39795483">next</a><span>|</span><label class="collapse" for="c-39796345">[-]</label><label class="expand" for="c-39796345">[1 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t believe nobody thought of this yet</div><br/></div></div><div id="39795483" class="c"><input type="checkbox" id="c-39795483" checked=""/><div class="controls bullet"><span class="by">aoeusnth1</span><span>|</span><a href="#39796345">prev</a><span>|</span><label class="collapse" for="c-39795483">[-]</label><label class="expand" for="c-39795483">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Impact statement:<p>&gt; This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.<p>I found this particularly charming.</div><br/><div id="39795552" class="c"><input type="checkbox" id="c-39795552" checked=""/><div class="controls bullet"><span class="by">polygamous_bat</span><span>|</span><a href="#39795483">parent</a><span>|</span><a href="#39796040">next</a><span>|</span><label class="collapse" for="c-39795552">[-]</label><label class="expand" for="c-39795552">[1 more]</label></div><br/><div class="children"><div class="content">AFAIK this was the default, copy paste impact statement by ICML template.</div><br/></div></div></div></div></div></div></div></div></div></body></html>