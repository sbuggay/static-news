<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1691226056198" as="style"/><link rel="stylesheet" href="styles.css?v=1691226056198"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://152334H.github.io/blog/non-determinism-in-gpt-4/">Non-determinism in GPT-4 is caused by Sparse MoE</a> <span class="domain">(<a href="https://152334h.github.io">152334h.github.io</a>)</span></div><div class="subtext"><span>152334H</span> | <span>130 comments</span></div><br/><div><div id="37007715" class="c"><input type="checkbox" id="c-37007715" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#37007163">next</a><span>|</span><label class="collapse" for="c-37007715">[-]</label><label class="expand" for="c-37007715">[39 more]</label></div><br/><div class="children"><div class="content">Floating point inaccuracies are generally deterministic - running the same calculations twice ought to yield the same results, down to the bit.<p>You only get divergent results if there is some other source of state or entropy: not zeroing buffers correctly, race conditions, not setting rounding mode flags consistently, etc…<p>From the quality of the code I’ve seen being cobbled together in the AI&#x2F;ML ecosystem I would assume all three of those issues going on, and maybe more.</div><br/><div id="37007811" class="c"><input type="checkbox" id="c-37007811" checked=""/><div class="controls bullet"><span class="by">n2d4</span><span>|</span><a href="#37007715">parent</a><span>|</span><a href="#37008580">next</a><span>|</span><label class="collapse" for="c-37007811">[-]</label><label class="expand" for="c-37007811">[28 more]</label></div><br/><div class="children"><div class="content">No, this is not true for GPUs.  <a href="https:&#x2F;&#x2F;www.twosigma.com&#x2F;articles&#x2F;a-workaround-for-non-determinism-in-tensorflow&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.twosigma.com&#x2F;articles&#x2F;a-workaround-for-non-deter...</a><p>(In this particular case, the order in which the numbers are summed up is non-deterministic due to GPU parallelism, which may change the result slightly.)<p>I would generally refrain from insulting other people&#x27;s code if you don&#x27;t know much about the system it&#x27;s written on.<p>.<p>Editing here since all the replies to this are mostly saying the same thing: Yes, CPUs can also be parallel and it can happen there as well, but unlike a CPU where most instructions on their own are deterministic, CUDA provides primitives that aren&#x27;t. This is very much by design (as they&#x27;re faster than their deterministic counterparts), and I mostly just take issue with how parent phrased this as a bug caused by bad code.</div><br/><div id="37007906" class="c"><input type="checkbox" id="c-37007906" checked=""/><div class="controls bullet"><span class="by">Tunabrain</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007811">parent</a><span>|</span><a href="#37007922">next</a><span>|</span><label class="collapse" for="c-37007906">[-]</label><label class="expand" for="c-37007906">[13 more]</label></div><br/><div class="children"><div class="content">GPUs are deterministic machines, even for floating point.<p>The behavior in the linked article has to do with the use of atomic adds to reduce sums in parallel. Floating point addition is not associative, so the order in which addition occurs matters. When using atomic adds this way, you get slightly different results depending on the order in which threads arrive at the atomic add call. It&#x27;s a simple race condition, although one which is usually deemed acceptable.</div><br/><div id="37007946" class="c"><input type="checkbox" id="c-37007946" checked=""/><div class="controls bullet"><span class="by">n2d4</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007906">parent</a><span>|</span><a href="#37007922">next</a><span>|</span><label class="collapse" for="c-37007946">[-]</label><label class="expand" for="c-37007946">[12 more]</label></div><br/><div class="children"><div class="content">I just edited my comment while you were writing your comment to add an explanation. The point here is that some primitives in eg. cudNN are non-deterministic. Whether you classify that as a race condition or not is a different question; but it&#x27;s intended behaviour.</div><br/><div id="37008104" class="c"><input type="checkbox" id="c-37008104" checked=""/><div class="controls bullet"><span class="by">xyzzy_plugh</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007946">parent</a><span>|</span><a href="#37009590">next</a><span>|</span><label class="collapse" for="c-37008104">[-]</label><label class="expand" for="c-37008104">[9 more]</label></div><br/><div class="children"><div class="content">Right but that&#x27;s not an inherent GPU determinism issue. It&#x27;s a software issue.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;tensorflow&#x2F;issues&#x2F;3103#issuecomment-422062420">https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;tensorflow&#x2F;issues&#x2F;3103#issueco...</a> is correct that it&#x27;s not necessary, it&#x27;s a choice.<p>Your line of reasoning appears to be &quot;GPUs are inherently non-deterministic don&#x27;t be quick to judge someone&#x27;s code&quot; which as far as I can tell is dead wrong.<p>Admittedly there are <i>some</i> cases and instructions that may result in non-determinism but they are inherently necessary. The author should thinking carefully before introducing non-determinism. There are many scenarios where it is irrelevant, but ultimately the issue we are discussing here isn&#x27;t the GPU&#x27;s fault.</div><br/><div id="37008173" class="c"><input type="checkbox" id="c-37008173" checked=""/><div class="controls bullet"><span class="by">n2d4</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37008104">parent</a><span>|</span><a href="#37009590">next</a><span>|</span><label class="collapse" for="c-37008173">[-]</label><label class="expand" for="c-37008173">[8 more]</label></div><br/><div class="children"><div class="content">What I&#x27;m saying is &quot;there are non-deterministic primitives&quot;, not &quot;there are no deterministic primitives&quot;.</div><br/><div id="37008257" class="c"><input type="checkbox" id="c-37008257" checked=""/><div class="controls bullet"><span class="by">xyzzy_plugh</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37008173">parent</a><span>|</span><a href="#37009590">next</a><span>|</span><label class="collapse" for="c-37008257">[-]</label><label class="expand" for="c-37008257">[7 more]</label></div><br/><div class="children"><div class="content">Yes, and `gettimeofday` is a non-deterministic primitive. There is nothing special about GPUs here. If you write tests that fail sometimes because you used non-deterministic primitives like gettimeofday and someone files a bug we don&#x27;t throw up our hands and say &quot;this is not a bug but due to how CPUs work.&quot; We remove the non-deterministic bit.<p>There&#x27;s no difference here. This isn&#x27;t a GPU problem.</div><br/><div id="37008441" class="c"><input type="checkbox" id="c-37008441" checked=""/><div class="controls bullet"><span class="by">cpgxiii</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37008257">parent</a><span>|</span><a href="#37009590">next</a><span>|</span><label class="collapse" for="c-37008441">[-]</label><label class="expand" for="c-37008441">[6 more]</label></div><br/><div class="children"><div class="content">Except the issue is inextricably linked to GPUs. All of the work in practical DNNs exists because of the extreme parallel performance available from GPUs, and that performance is only possible with non-deterministic threading. You can&#x27;t get reasonable training and inference time on existing hardware without it.</div><br/><div id="37008931" class="c"><input type="checkbox" id="c-37008931" checked=""/><div class="controls bullet"><span class="by">d0mine</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37008441">parent</a><span>|</span><a href="#37009438">next</a><span>|</span><label class="collapse" for="c-37008931">[-]</label><label class="expand" for="c-37008931">[4 more]</label></div><br/><div class="children"><div class="content">1000 threads can run in parallel. It doesn&#x27;t prevent us to sum their results deterministically:<p><pre><code>    results = ThreadPool(workers=1000).imap_unordered(calc, inputs)
    print(math.fsum(results))
</code></pre>
Due to the magic of the fsum alg, the result is deterministic whatever order we get results in.
<a href="https:&#x2F;&#x2F;docs.python.org&#x2F;3&#x2F;library&#x2F;math.html#math.fsum" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.python.org&#x2F;3&#x2F;library&#x2F;math.html#math.fsum</a></div><br/><div id="37009324" class="c"><input type="checkbox" id="c-37009324" checked=""/><div class="controls bullet"><span class="by">cpgxiii</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37008931">parent</a><span>|</span><a href="#37009680">next</a><span>|</span><label class="collapse" for="c-37009324">[-]</label><label class="expand" for="c-37009324">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not the operation being performed on GPUs that is the problem. The issue is that fundamentally GPUs allow for high performance operations using atomics, but this comes at the cost of nondeterministic results. You <i>can</i> get deterministic results but  doing so comes with a significant performance costs.</div><br/><div id="37010002" class="c"><input type="checkbox" id="c-37010002" checked=""/><div class="controls bullet"><span class="by">xiphias2</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37009324">parent</a><span>|</span><a href="#37009680">next</a><span>|</span><label class="collapse" for="c-37010002">[-]</label><label class="expand" for="c-37010002">[1 more]</label></div><br/><div class="children"><div class="content">Using atomics is easier than warp operations (using warp shuffle for example), but warp shuffle is quite fast.<p>I guess if determinism is so important implementations can be changed, it is just maybe not that high priority.</div><br/></div></div></div></div><div id="37009680" class="c"><input type="checkbox" id="c-37009680" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37008931">parent</a><span>|</span><a href="#37009324">prev</a><span>|</span><a href="#37009438">next</a><span>|</span><label class="collapse" for="c-37009680">[-]</label><label class="expand" for="c-37009680">[1 more]</label></div><br/><div class="children"><div class="content">That summation is slow and would not be used in practice.<p>You could use just one thread on your 10000 thread GPU too and it would be deterministic, sure. Completely beside the point.</div><br/></div></div></div></div><div id="37009438" class="c"><input type="checkbox" id="c-37009438" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37008441">parent</a><span>|</span><a href="#37008931">prev</a><span>|</span><a href="#37009590">next</a><span>|</span><label class="collapse" for="c-37009438">[-]</label><label class="expand" for="c-37009438">[1 more]</label></div><br/><div class="children"><div class="content">In my experience cuBLAS is deterministic, since matmul is the most intensive part I don‘t see other reasons for non-determinism other than sloppyness (at least when just a single GPU is involved)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37009590" class="c"><input type="checkbox" id="c-37009590" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007946">parent</a><span>|</span><a href="#37008104">prev</a><span>|</span><a href="#37007922">next</a><span>|</span><label class="collapse" for="c-37009590">[-]</label><label class="expand" for="c-37009590">[2 more]</label></div><br/><div class="children"><div class="content">If the hardware is deterministic, so are the results. You can&#x27;t generate random numbers purely in software with deterministic hardware.</div><br/><div id="37009663" class="c"><input type="checkbox" id="c-37009663" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37009590">parent</a><span>|</span><a href="#37007922">next</a><span>|</span><label class="collapse" for="c-37009663">[-]</label><label class="expand" for="c-37009663">[1 more]</label></div><br/><div class="children"><div class="content">The behaviour of atomic operations is definitely not deterministic. E.g. if you have a lot of atomic adds, every time you run the code you&#x27;ll get a different result without a random number generator.</div><br/></div></div></div></div></div></div></div></div><div id="37007922" class="c"><input type="checkbox" id="c-37007922" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007811">parent</a><span>|</span><a href="#37007906">prev</a><span>|</span><a href="#37009523">next</a><span>|</span><label class="collapse" for="c-37007922">[-]</label><label class="expand" for="c-37007922">[4 more]</label></div><br/><div class="children"><div class="content">Read the article you linked.<p>It literally says that the GPU is deterministic, the NVIDIA libraries on top are deterministic, but it is Tensorflow that introduces variability (errors!) for “performance”.<p>My argument is that it is the AI&#x2F;ML code that is introducing non-determinism, usually by sacrificing repeatability to gain performance.<p>That&#x27;s precisely what&#x27;s happening here. Tensorflow introduced a &quot;harmless&quot;[1] data race to improve performance by not having to use a deterministic but slower algorithm.<p>The individual floating point computations are deterministic, it&#x27;s the <i>multi-threaded design</i> on top that&#x27;s introducing the variability in the output.<p>[1] Used to be harmless, but cutting corners like this will make it nigh impossible to repeatably validate the safety of future models like GPT5. That seems pretty dangerous...</div><br/><div id="37007966" class="c"><input type="checkbox" id="c-37007966" checked=""/><div class="controls bullet"><span class="by">n2d4</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007922">parent</a><span>|</span><a href="#37009523">next</a><span>|</span><label class="collapse" for="c-37007966">[-]</label><label class="expand" for="c-37007966">[3 more]</label></div><br/><div class="children"><div class="content">As the article says, cuBLAS is deterministic, but other CUDA primitives (eg. some of those in cudNN) are not.<p>Yes, the non-determinism is being introduced somewhere, but that is splitting hairs. The point is that the primitives that you work with on GPUs are non-deterministic by design.<p>I mostly take issue with you phrasing it as a bug and using it to insult the authors.</div><br/><div id="37008130" class="c"><input type="checkbox" id="c-37008130" checked=""/><div class="controls bullet"><span class="by">xyzzy_plugh</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007966">parent</a><span>|</span><a href="#37009523">next</a><span>|</span><label class="collapse" for="c-37008130">[-]</label><label class="expand" for="c-37008130">[2 more]</label></div><br/><div class="children"><div class="content">How is that splitting hairs?<p>&gt; The point is that the primitives that you work with on GPUs are non-deterministic by design.<p>This is just blatantly wrong. There are _some_ operations that can be non-deterministic in some scenarios but they are not necessary.<p>GPUs are deterministic. If you ask them to add a million floats in order, you get the same result every time. If you ask them to add a million floats in some arbitrary order, then you may get different results every time. The distinction is that someone had to ask the GPU to do that. It&#x27;s a choice.<p>&gt; I mostly take issue with you phrasing it as a bug and using it to insult the authors.<p>It&#x27;s a bug, whether it insults the authors or not is irrelevant. It&#x27;s most definitely a bug.</div><br/><div id="37009167" class="c"><input type="checkbox" id="c-37009167" checked=""/><div class="controls bullet"><span class="by">spott</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37008130">parent</a><span>|</span><a href="#37009523">next</a><span>|</span><label class="collapse" for="c-37009167">[-]</label><label class="expand" for="c-37009167">[1 more]</label></div><br/><div class="children"><div class="content">Basically any parallel map-reduce operation using non-commutative reduce operators[0] is non-deterministic unless you specifically sort after&#x2F;during the gather, or block on the gather (and gather to a thread-determined memory location).  Sorting and blocking takes time.  If you remove the sort&#x2F;block, you will get a non-deterministic answer when operating on floats for a wide variety of reduce operations, but it will be faster.  This is true of any parallel map-reduce, done anywhere (MPI, cuda kernels, openMP, spark, etc.), and is not unique to gpus&#x2F;cuda.<p>&gt; If you ask them to add a million floats in order, you get the same result every time.<p>There are a bunch of ways to add a million floats in order on a gpu, but they will all get you different results.:<p>* split the million floats into ‘n’ chunks, each chunk is summed, then you sum the ‘n’ results.
* if you sum results as they are gathered (you don’t need to block) you will get a non-deterministic result, as the threads finishing (outside of a warp) is non-deterministic in order.
* if you change ‘n’, your result will change.
* if you sort after gathering , your result will change.<p>TLDR: parallel race-conditions are nondeterministic. Map-reduce has an underlying race-condition that you can prevent, but it costs time&#x2F;performance.  Sometimes you don’t care about the non-determinism enough to pay the performance penalty to fix it.<p>[0] <a href="https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;wp-content&#x2F;uploads&#x2F;2016&#x2F;02&#x2F;icsecomp14seip-seipid15-p.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;wp-content&#x2F;uploads&#x2F;...</a></div><br/></div></div></div></div></div></div></div></div><div id="37009523" class="c"><input type="checkbox" id="c-37009523" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007811">parent</a><span>|</span><a href="#37007922">prev</a><span>|</span><a href="#37008013">next</a><span>|</span><label class="collapse" for="c-37009523">[-]</label><label class="expand" for="c-37009523">[1 more]</label></div><br/><div class="children"><div class="content">The PyTorch documentation has an entire section about how to make your code deterministic. In my experience, the performance difference is negligible.<p><a href="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;notes&#x2F;randomness.html#avoiding-nondeterministic-algorithms" rel="nofollow noreferrer">https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;notes&#x2F;randomness.html#avoidi...</a><p>Unfortunately, determinism <i>across devices or even driver versions</i> is not that easy. You&#x27;d have to write your own BLAS kernels using only basic operations, which are guaranteed to follow IEEE 754 semantics.<p><a href="https:&#x2F;&#x2F;docs.nvidia.com&#x2F;cuda&#x2F;floating-point&#x2F;index.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.nvidia.com&#x2F;cuda&#x2F;floating-point&#x2F;index.html</a><p>One gotcha are fused multiply-adds, which the compiler may or may not introduce, so you have to wrap all your floating point operations with __fma* intrinsics to make sure the compiler does not interpret them differently.</div><br/></div></div><div id="37008013" class="c"><input type="checkbox" id="c-37008013" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007811">parent</a><span>|</span><a href="#37009523">prev</a><span>|</span><a href="#37007990">next</a><span>|</span><label class="collapse" for="c-37008013">[-]</label><label class="expand" for="c-37008013">[1 more]</label></div><br/><div class="children"><div class="content">As far as I can tell this article doesn&#x27;t explain why this happens on the GPU (for example, why Tensorflow&#x27;s reduce_sum is non-deterministic). My hypothesis is that this is entirely due to concurrency: if the same code can be run in two or more different interleavings, they can produce different results. This is corroborated by the first answer here [0].<p>If so, this exact same issue happens in CPU code as well: have two or more threads, run the program many times, observe different interleavings that expose race conditions which (depending on the algorithm) may or may not produce different results. This can happen even if you don&#x27;t use floating point, and has nothing to do with floating point non-determinism itself. For example, have a thread print &quot;Hello&quot; and another thread print &quot;World&quot;; even without tearing, you may see either Hello World or World Hello on the screen.<p>Now, proper floating point non-determinism happens in two cases. One is that when you run the same code in two different architectures you could have different answers (because of rounding modes, or because some architecture doesn&#x27;t support subnormal numbers or signaling nans, because transcedental functions like sine are implemented with different accuracy, etc). In this case it&#x27;s deterministic when run the same in the same machine, but may run differently in another machine with a different architecture.<p>The other case is that some &quot;optimizations&quot; actually break your code if applied carelessly (you enable those broken optimizations with -ffast-math in C for example). Among other things, this may break numerical stability of algorithms like Kahan summation. And, if you let the compiler decide which exact optimizations will be applied and in what order, you get non-determinism between different compilers. So in this case it&#x27;s deterministic when compiled with the same compiler, but may run differently with another compiler.<p>[0] <a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;50744565&#x2F;how-to-handle-non-determinism-when-training-on-a-gpu" rel="nofollow noreferrer">https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;50744565&#x2F;how-to-handle-n...</a></div><br/></div></div><div id="37007990" class="c"><input type="checkbox" id="c-37007990" checked=""/><div class="controls bullet"><span class="by">ascar</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007811">parent</a><span>|</span><a href="#37008013">prev</a><span>|</span><a href="#37009929">next</a><span>|</span><label class="collapse" for="c-37007990">[-]</label><label class="expand" for="c-37007990">[1 more]</label></div><br/><div class="children"><div class="content">To nitpick in addition to the already existing comments: this has nothing to do with GPUs per se. You would see the same issue in multithreaded code on a CPU. Even on a single core CPU this can happen with a multithreaded program depending on how the OS schedules and interrupts the threads. It just happens to be an implementation choice in a GPU library&#x2F;API.</div><br/></div></div><div id="37009929" class="c"><input type="checkbox" id="c-37009929" checked=""/><div class="controls bullet"><span class="by">zx14</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007811">parent</a><span>|</span><a href="#37007990">prev</a><span>|</span><a href="#37008495">next</a><span>|</span><label class="collapse" for="c-37009929">[-]</label><label class="expand" for="c-37009929">[2 more]</label></div><br/><div class="children"><div class="content">There isn&#x27;t much of a culture around code quality in ML &#x2F; AI &#x2F; DS.</div><br/><div id="37010076" class="c"><input type="checkbox" id="c-37010076" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37009929">parent</a><span>|</span><a href="#37008495">next</a><span>|</span><label class="collapse" for="c-37010076">[-]</label><label class="expand" for="c-37010076">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a code quality issue, there are ways to ensure determinism (sometimes you just need to set a flag), however, they are intentionally explicitly not used in order to gain performance.</div><br/></div></div></div></div><div id="37008495" class="c"><input type="checkbox" id="c-37008495" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007811">parent</a><span>|</span><a href="#37009929">prev</a><span>|</span><a href="#37008353">next</a><span>|</span><label class="collapse" for="c-37008495">[-]</label><label class="expand" for="c-37008495">[1 more]</label></div><br/><div class="children"><div class="content">I don’t know about how insulting it is, I don’t like rushing things out but we’ve all had to.<p>People are rushing like crazy to get there first with X for AI all over the place, it would be pretty shocking if there <i>weren’t</i> wires sticking out everywhere.<p>I don’t think that says anything positive or negative about the hackers involved.</div><br/></div></div><div id="37008353" class="c"><input type="checkbox" id="c-37008353" checked=""/><div class="controls bullet"><span class="by">jes5199</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007811">parent</a><span>|</span><a href="#37008495">prev</a><span>|</span><a href="#37009574">next</a><span>|</span><label class="collapse" for="c-37008353">[-]</label><label class="expand" for="c-37008353">[1 more]</label></div><br/><div class="children"><div class="content">it’s basically always reasonable to insult someone’s code because we are computer programmers and we know what we have done</div><br/></div></div><div id="37009574" class="c"><input type="checkbox" id="c-37009574" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007811">parent</a><span>|</span><a href="#37008353">prev</a><span>|</span><a href="#37008206">next</a><span>|</span><label class="collapse" for="c-37009574">[-]</label><label class="expand" for="c-37009574">[2 more]</label></div><br/><div class="children"><div class="content">So you can generate true random numbers using just the GPU parallelism? Consider me impressed!</div><br/><div id="37009860" class="c"><input type="checkbox" id="c-37009860" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37009574">parent</a><span>|</span><a href="#37008206">next</a><span>|</span><label class="collapse" for="c-37009860">[-]</label><label class="expand" for="c-37009860">[1 more]</label></div><br/><div class="children"><div class="content">Yes you can, and it&#x27;s been done:<p><a href="https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s11071-015-2287-7" rel="nofollow noreferrer">https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s11071-015-2287-7</a></div><br/></div></div></div></div><div id="37008206" class="c"><input type="checkbox" id="c-37008206" checked=""/><div class="controls bullet"><span class="by">xyzzy_plugh</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37007811">parent</a><span>|</span><a href="#37009574">prev</a><span>|</span><a href="#37008580">next</a><span>|</span><label class="collapse" for="c-37008206">[-]</label><label class="expand" for="c-37008206">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;ve moved the goal posts. You&#x27;re conflating CUDA with GPUs. From Wikipedia:<p>&gt; CUDA (or Compute Unified Device Architecture) is a proprietary and closed source parallel computing platform and application programming interface (API) that allows software to use certain types of graphics processing units (GPUs) for general purpose processing, an approach called general-purpose computing on GPUs (GPGPU). CUDA is a software layer that gives direct access to the GPU&#x27;s virtual instruction set and parallel computational elements, for the execution of compute kernels.<p>Is the issue we&#x27;re discussing because of the GPU or is it because of choices made <i>in software libraries</i>?<p>The parent is right, there is a deterministic, reproducible way to solve these problems, so if determinism is a desired or expected property, then this is a bug. It&#x27;s not an inherent problem like you make it out to be. The fact that &quot;workarounds&quot; are given in what you link prove this.</div><br/></div></div></div></div><div id="37008580" class="c"><input type="checkbox" id="c-37008580" checked=""/><div class="controls bullet"><span class="by">KolenCh</span><span>|</span><a href="#37007715">parent</a><span>|</span><a href="#37007811">prev</a><span>|</span><a href="#37008106">next</a><span>|</span><label class="collapse" for="c-37008580">[-]</label><label class="expand" for="c-37008580">[5 more]</label></div><br/><div class="children"><div class="content">What you said can be violated when parallelism is involved. One such example is that we know some floating point operations such as addition and multiplication are non-commutative, hence it depends on order of execution to complete reduction for example. And then in parallel situation, some implementation will make the order or reduction non-deterministic (for performance reason) and hence the final result also non-deterministic.</div><br/><div id="37009606" class="c"><input type="checkbox" id="c-37009606" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37008580">parent</a><span>|</span><a href="#37009635">next</a><span>|</span><label class="collapse" for="c-37009606">[-]</label><label class="expand" for="c-37009606">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s still deterministic even if the results appear not to be. If you have memory, CPU cache, CPU registers in the same state, you will get the very same results. You need a source of entropy for the results to be non deterministic.</div><br/><div id="37010222" class="c"><input type="checkbox" id="c-37010222" checked=""/><div class="controls bullet"><span class="by">namibj</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37009606">parent</a><span>|</span><a href="#37009979">next</a><span>|</span><label class="collapse" for="c-37010222">[-]</label><label class="expand" for="c-37010222">[1 more]</label></div><br/><div class="children"><div class="content">Actually, clock domain crossing for asynchronous clocks (as is AFAIK typical for granular dynamic frequency scaling, like running CPU cores at individual frequencies instead of all at the same, because it quite softly smoothes over to any new target frequency to prevent glitches) implicitly includes thermal noise in the raw transistors that determine which of the two involved clock edges happened earlier (a decision that eventually ends up truly random when they are at (almost) exactly the same time).
And this is involved in even L3 hit latency.</div><br/></div></div><div id="37009979" class="c"><input type="checkbox" id="c-37009979" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37009606">parent</a><span>|</span><a href="#37010222">prev</a><span>|</span><a href="#37009635">next</a><span>|</span><label class="collapse" for="c-37009979">[-]</label><label class="expand" for="c-37009979">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but they will never be in the same state, which can even be used as a source of entropy: <a href="https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s11071-015-2287-7" rel="nofollow noreferrer">https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s11071-015-2287-7</a></div><br/></div></div></div></div><div id="37009635" class="c"><input type="checkbox" id="c-37009635" checked=""/><div class="controls bullet"><span class="by">toxik</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37008580">parent</a><span>|</span><a href="#37009606">prev</a><span>|</span><a href="#37008106">next</a><span>|</span><label class="collapse" for="c-37009635">[-]</label><label class="expand" for="c-37009635">[1 more]</label></div><br/><div class="children"><div class="content">Minor nit but commutative is the wrong term. Floats always obey a+b == b+a, but not associativity: (a+b)+c != a+(b+c).</div><br/></div></div></div></div><div id="37008106" class="c"><input type="checkbox" id="c-37008106" checked=""/><div class="controls bullet"><span class="by">dwpdwpdwpdwpdwp</span><span>|</span><a href="#37007715">parent</a><span>|</span><a href="#37008580">prev</a><span>|</span><a href="#37009559">next</a><span>|</span><label class="collapse" for="c-37008106">[-]</label><label class="expand" for="c-37008106">[2 more]</label></div><br/><div class="children"><div class="content">Mathematically, computation is deterministic. The author dismisses or ignores the many ways that the physical apparatus driving the computation can force the result of a software application to be a function of time.<p>Calling GetTimeOfDay() could do it.<p>Clock frequency drift between multiple processors could it.</div><br/><div id="37008153" class="c"><input type="checkbox" id="c-37008153" checked=""/><div class="controls bullet"><span class="by">water9</span><span>|</span><a href="#37007715">root</a><span>|</span><a href="#37008106">parent</a><span>|</span><a href="#37009559">next</a><span>|</span><label class="collapse" for="c-37008153">[-]</label><label class="expand" for="c-37008153">[1 more]</label></div><br/><div class="children"><div class="content">When theory fails to consult reality.</div><br/></div></div></div></div><div id="37009559" class="c"><input type="checkbox" id="c-37009559" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#37007715">parent</a><span>|</span><a href="#37008106">prev</a><span>|</span><a href="#37008138">next</a><span>|</span><label class="collapse" for="c-37009559">[-]</label><label class="expand" for="c-37009559">[1 more]</label></div><br/><div class="children"><div class="content">On a large scale, not having memory with good ECC is enough to have entropy.</div><br/></div></div><div id="37008138" class="c"><input type="checkbox" id="c-37008138" checked=""/><div class="controls bullet"><span class="by">neatze</span><span>|</span><a href="#37007715">parent</a><span>|</span><a href="#37009559">prev</a><span>|</span><a href="#37008004">next</a><span>|</span><label class="collapse" for="c-37008138">[-]</label><label class="expand" for="c-37008138">[1 more]</label></div><br/><div class="children"><div class="content">hmm, how, I wonder if Alhazen’ s Circular Billiard Problem[1] results for n steps in simulation will be same for multiple runs.<p>[1] <a href="https:&#x2F;&#x2F;forumgeom.fau.edu&#x2F;FG2012volume12&#x2F;FG201216.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;forumgeom.fau.edu&#x2F;FG2012volume12&#x2F;FG201216.pdf</a></div><br/></div></div><div id="37008004" class="c"><input type="checkbox" id="c-37008004" checked=""/><div class="controls bullet"><span class="by">alexnewman</span><span>|</span><a href="#37007715">parent</a><span>|</span><a href="#37008138">prev</a><span>|</span><a href="#37007163">next</a><span>|</span><label class="collapse" for="c-37008004">[-]</label><label class="expand" for="c-37008004">[1 more]</label></div><br/><div class="children"><div class="content">Small nit. You mean errors due to floating point math</div><br/></div></div></div></div><div id="37007163" class="c"><input type="checkbox" id="c-37007163" checked=""/><div class="controls bullet"><span class="by">gojomo</span><span>|</span><a href="#37007715">prev</a><span>|</span><a href="#37006815">next</a><span>|</span><label class="collapse" for="c-37007163">[-]</label><label class="expand" for="c-37007163">[7 more]</label></div><br/><div class="children"><div class="content">Not sure I understand the excerpt from the referenced paper.<p>Is it saying that part of its more-efficient inferencing relies on mixing tokens from completely-separate inputs – eg, from other users? And then, depending on what other inputs chance into the same grouping, the relative assignment-to-&#x27;experts&#x27; varies, and thus the eventual completions?<p>If so, I&#x27;d see that as not just introducing non-determinism, but also potentially making the <i>quality</i> of your responses dependent on how-many-concurrent-requests are fighting for the same expert-allocations.<p>(For example, maybe the parts of the system best at translating&#x2F;interpreting Hindi give worse results during peak usage hours-of-the-day in India, when the most concurrent inputs are competing for that same competence.)<p>Perhaps also, this is another possible explanation for perceived quality-degradation over time. When certain tests were reliably succeeding earlier, there was less congestion for the relevant &#x27;experts&#x27;. Now, with more concurrent use, those same tests aren&#x27;t as reliably winning as much of relevant &#x27;experts&#x27; effort.<p>This may also suggest a bit of a quagmire: on whatever domains some sub-experts seem impressively good, initially, even more proportionate use will be attracted. But such new congestion means all the copycat use no longer gets the same expert allocations – and thus the initially-impressive performance degrades.<p>(And if the effect is strong, &amp; known-but-undisclosed-by-OpenAI, does it amount to a bait-and-switch? Attract users with unrepresentative excellence on an initially-uncongested Mixture-of-Experts system, but then offer them the lower-quality results from a more-congested system.)</div><br/><div id="37007683" class="c"><input type="checkbox" id="c-37007683" checked=""/><div class="controls bullet"><span class="by">spott</span><span>|</span><a href="#37007163">parent</a><span>|</span><a href="#37007305">next</a><span>|</span><label class="collapse" for="c-37007683">[-]</label><label class="expand" for="c-37007683">[1 more]</label></div><br/><div class="children"><div class="content">The results are showing essentially 12 unique responses from 30 tries… not what you would expect from mixing tokens.<p>I think it groups the batch up differently, so if I have a batch of 10, and it groups it up into 2 groups of 5, if my prompt makes it to the second group or 1st group I get a different answer.  But if I’m in the same location in the batch, then I get the same answer.<p>The whole batch is deterministic given the same batch (sequences and ordering), but if you shuffle the batch then you lose that determinism.</div><br/></div></div><div id="37007305" class="c"><input type="checkbox" id="c-37007305" checked=""/><div class="controls bullet"><span class="by">albystein</span><span>|</span><a href="#37007163">parent</a><span>|</span><a href="#37007683">prev</a><span>|</span><a href="#37006815">next</a><span>|</span><label class="collapse" for="c-37007305">[-]</label><label class="expand" for="c-37007305">[5 more]</label></div><br/><div class="children"><div class="content">this seems like a plausible outcome, and if true could spell disaster for OpenAI models relative to the competition and open source models. Currently, reliability is one of the core obstacles preventing widespread adoption of LLMs in many business critical workflows. And if these rumors, that GPT-4 is inherently un-deterministic and unreliable, are true then most enterprises are better off finetuning open source LLMs—which are just as capable—for their specific domains. they stand to gain better performance that way anyways, as domain-specific models will always outperform generalist ones</div><br/><div id="37008920" class="c"><input type="checkbox" id="c-37008920" checked=""/><div class="controls bullet"><span class="by">mrtranscendence</span><span>|</span><a href="#37007163">root</a><span>|</span><a href="#37007305">parent</a><span>|</span><a href="#37007559">next</a><span>|</span><label class="collapse" for="c-37008920">[-]</label><label class="expand" for="c-37008920">[2 more]</label></div><br/><div class="children"><div class="content">&gt; And if these rumors, that GPT-4 is inherently un-deterministic and unreliable, are true then most enterprises are better off finetuning open source LLMs—which are just as capable<p>Wait, am I misunderstanding you? I feel like I&#x27;ve had a head injury or something, because I&#x27;ve never heard of an open source LLM that&#x27;s as capable as GPT-4 (in most scenarios).</div><br/><div id="37009045" class="c"><input type="checkbox" id="c-37009045" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37007163">root</a><span>|</span><a href="#37008920">parent</a><span>|</span><a href="#37007559">next</a><span>|</span><label class="collapse" for="c-37009045">[-]</label><label class="expand" for="c-37009045">[1 more]</label></div><br/><div class="children"><div class="content">Only on specific domains, these models don&#x27;t become generalists like GPT-4, they can become task experts for a single task.</div><br/></div></div></div></div><div id="37007559" class="c"><input type="checkbox" id="c-37007559" checked=""/><div class="controls bullet"><span class="by">geysersam</span><span>|</span><a href="#37007163">root</a><span>|</span><a href="#37007305">parent</a><span>|</span><a href="#37008920">prev</a><span>|</span><a href="#37008336">next</a><span>|</span><label class="collapse" for="c-37007559">[-]</label><label class="expand" for="c-37007559">[1 more]</label></div><br/><div class="children"><div class="content">&gt; domain-specific models will always outperform generalist ones<p>That&#x27;s only true assuming you habe enough data to train a domain-specific model &#x2F; expertise to train it and test it correctly.<p>I&#x27;ve encountered cases where an image recognition task could be accomplished well with a very general model like CLIP, but people still fine-tuned another model on their own small data set because that&#x27;s considered better.<p>A domain specific model might be more likely to fail on weird outliers not present in the small domain specific training data.<p>&gt; could spell disaster for OpenAI<p>Nah I don&#x27;t think so. They are not all in on one specific model architecture.
If the current architecture is found to have serious unfixable flaws then they&#x27;ll just change architecture.</div><br/></div></div><div id="37008336" class="c"><input type="checkbox" id="c-37008336" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37007163">root</a><span>|</span><a href="#37007305">parent</a><span>|</span><a href="#37007559">prev</a><span>|</span><a href="#37006815">next</a><span>|</span><label class="collapse" for="c-37008336">[-]</label><label class="expand" for="c-37008336">[1 more]</label></div><br/><div class="children"><div class="content">&gt;as domain-specific models will always outperform generalist ones<p>This is not even close to true for Language models.</div><br/></div></div></div></div></div></div><div id="37006815" class="c"><input type="checkbox" id="c-37006815" checked=""/><div class="controls bullet"><span class="by">alpark3</span><span>|</span><a href="#37007163">prev</a><span>|</span><a href="#37006285">next</a><span>|</span><label class="collapse" for="c-37006815">[-]</label><label class="expand" for="c-37006815">[6 more]</label></div><br/><div class="children"><div class="content">_If_ 3.5 is a MoE model, doesn&#x27;t that give a lot of hope to open source movements? Once a good open source MoE model comes out, maybe even some type of variation of the decoder models available(I don&#x27;t know whether MoE models have to be trained from scratch), that implies a lot more can be done with a lot less.</div><br/><div id="37006877" class="c"><input type="checkbox" id="c-37006877" checked=""/><div class="controls bullet"><span class="by">152334H</span><span>|</span><a href="#37006815">parent</a><span>|</span><a href="#37007077">next</a><span>|</span><label class="collapse" for="c-37006877">[-]</label><label class="expand" for="c-37006877">[1 more]</label></div><br/><div class="children"><div class="content">I agree, and really hope that Meta is doing something in that vein. Reducing the FLOPs:Memory ratio (as in Soft MoE) could also open the door to CPU (or at least Apple Silicon) inference becoming more relevant.</div><br/></div></div><div id="37007077" class="c"><input type="checkbox" id="c-37007077" checked=""/><div class="controls bullet"><span class="by">osmarks</span><span>|</span><a href="#37006815">parent</a><span>|</span><a href="#37006877">prev</a><span>|</span><a href="#37006285">next</a><span>|</span><label class="collapse" for="c-37007077">[-]</label><label class="expand" for="c-37007077">[4 more]</label></div><br/><div class="children"><div class="content">It would be bad for single-consumer-GPU inference setups.</div><br/><div id="37007585" class="c"><input type="checkbox" id="c-37007585" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#37006815">root</a><span>|</span><a href="#37007077">parent</a><span>|</span><a href="#37007396">next</a><span>|</span><label class="collapse" for="c-37007585">[-]</label><label class="expand" for="c-37007585">[2 more]</label></div><br/><div class="children"><div class="content">Not an expert (no pun intended), but MoE where each expert is actually just a LoRA adaptor on top of the base model gets me pretty excited. Since LoRA adaptors can be swapped in and out at runtime, it might be possible to get decent performance without a lot of extra memory pressure.</div><br/><div id="37007795" class="c"><input type="checkbox" id="c-37007795" checked=""/><div class="controls bullet"><span class="by">arugulum</span><span>|</span><a href="#37006815">root</a><span>|</span><a href="#37007585">parent</a><span>|</span><a href="#37007396">next</a><span>|</span><label class="collapse" for="c-37007795">[-]</label><label class="expand" for="c-37007795">[1 more]</label></div><br/><div class="children"><div class="content">While MoE-LoRAs are exciting in themselves, they are a very different pitch from full on MoEs. If the idea behind MoEs is that you want completely separate layers to handle different parts of the input&#x2F;computation, then it is unlikely that you can get away with low-rank tweaks to an existing linear layer.</div><br/></div></div></div></div><div id="37007396" class="c"><input type="checkbox" id="c-37007396" checked=""/><div class="controls bullet"><span class="by">worldsayshi</span><span>|</span><a href="#37006815">root</a><span>|</span><a href="#37007077">parent</a><span>|</span><a href="#37007585">prev</a><span>|</span><a href="#37006285">next</a><span>|</span><label class="collapse" for="c-37007396">[-]</label><label class="expand" for="c-37007396">[1 more]</label></div><br/><div class="children"><div class="content">Could this work well with distributed solutions like petals?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;bigscience-workshop&#x2F;petals">https:&#x2F;&#x2F;github.com&#x2F;bigscience-workshop&#x2F;petals</a><p>I don&#x27;t understand how petals can work though. I thought LLMs were typically quite monolithic.</div><br/></div></div></div></div></div></div><div id="37006285" class="c"><input type="checkbox" id="c-37006285" checked=""/><div class="controls bullet"><span class="by">osmarks</span><span>|</span><a href="#37006815">prev</a><span>|</span><a href="#37006549">next</a><span>|</span><label class="collapse" for="c-37006285">[-]</label><label class="expand" for="c-37006285">[5 more]</label></div><br/><div class="children"><div class="content">I feel like this introduces the potential for weird and hard-to-implement side channel attacks, if the sequences in a batch can affect the routing of others.</div><br/><div id="37006434" class="c"><input type="checkbox" id="c-37006434" checked=""/><div class="controls bullet"><span class="by">tehsauce</span><span>|</span><a href="#37006285">parent</a><span>|</span><a href="#37006549">next</a><span>|</span><label class="collapse" for="c-37006434">[-]</label><label class="expand" for="c-37006434">[4 more]</label></div><br/><div class="children"><div class="content">I think you’re right. Would be very hard to exploit I imagine though.</div><br/><div id="37007187" class="c"><input type="checkbox" id="c-37007187" checked=""/><div class="controls bullet"><span class="by">catchnear4321</span><span>|</span><a href="#37006285">root</a><span>|</span><a href="#37006434">parent</a><span>|</span><a href="#37006508">next</a><span>|</span><label class="collapse" for="c-37007187">[-]</label><label class="expand" for="c-37007187">[1 more]</label></div><br/><div class="children"><div class="content">the tools available to imagine such things are limited today.<p>the language models in our heads have not caught up to the ones in our browsers.<p>as the similarities and associations crystallize a bit better, it won’t look so hard.<p>bookmark this if you think it bullshit.  eight months.</div><br/></div></div><div id="37006508" class="c"><input type="checkbox" id="c-37006508" checked=""/><div class="controls bullet"><span class="by">derwiki</span><span>|</span><a href="#37006285">root</a><span>|</span><a href="#37006434">parent</a><span>|</span><a href="#37007187">prev</a><span>|</span><a href="#37006970">next</a><span>|</span><label class="collapse" for="c-37006508">[-]</label><label class="expand" for="c-37006508">[1 more]</label></div><br/><div class="children"><div class="content">Hard like building a virtual machine in an image decoder? If there’s a way there’s a will.</div><br/></div></div><div id="37006970" class="c"><input type="checkbox" id="c-37006970" checked=""/><div class="controls bullet"><span class="by">adql</span><span>|</span><a href="#37006285">root</a><span>|</span><a href="#37006434">parent</a><span>|</span><a href="#37006508">prev</a><span>|</span><a href="#37006549">next</a><span>|</span><label class="collapse" for="c-37006970">[-]</label><label class="expand" for="c-37006970">[1 more]</label></div><br/><div class="children"><div class="content">Same thing was said about Spectre-like bugs</div><br/></div></div></div></div></div></div><div id="37006549" class="c"><input type="checkbox" id="c-37006549" checked=""/><div class="controls bullet"><span class="by">pazimzadeh</span><span>|</span><a href="#37006285">prev</a><span>|</span><a href="#37009480">next</a><span>|</span><label class="collapse" for="c-37006549">[-]</label><label class="expand" for="c-37006549">[3 more]</label></div><br/><div class="children"><div class="content">Mixture of Experts</div><br/><div id="37007694" class="c"><input type="checkbox" id="c-37007694" checked=""/><div class="controls bullet"><span class="by">TechBro8615</span><span>|</span><a href="#37006549">parent</a><span>|</span><a href="#37007614">next</a><span>|</span><label class="collapse" for="c-37007694">[-]</label><label class="expand" for="c-37007694">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. I assumed it was Margin of Error. The article doesn&#x27;t expand the acronym until midway through the post, where it appears almost accidentally. Perhaps the intended audience is a mixture of experts, of which I&#x27;m not a part.</div><br/></div></div><div id="37007614" class="c"><input type="checkbox" id="c-37007614" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#37006549">parent</a><span>|</span><a href="#37007694">prev</a><span>|</span><a href="#37009480">next</a><span>|</span><label class="collapse" for="c-37007614">[-]</label><label class="expand" for="c-37007614">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! I knew it couldn&#x27;t mean &quot;Merger of Equals&quot;... but then again, if those experts are equals, then maybe that acronym also works ;-)</div><br/></div></div></div></div><div id="37009480" class="c"><input type="checkbox" id="c-37009480" checked=""/><div class="controls bullet"><span class="by">f1shy</span><span>|</span><a href="#37006549">prev</a><span>|</span><a href="#37008226">next</a><span>|</span><label class="collapse" for="c-37009480">[-]</label><label class="expand" for="c-37009480">[2 more]</label></div><br/><div class="children"><div class="content">I see in the comments it seems to be a huge miss understanding between 2 uses of “non-deterministic”:
1) from normal English: cannot be determined beforehand (results may vary)
2) from theory of computation: loosely “parallel computation” (unknown path to the solution)</div><br/><div id="37010092" class="c"><input type="checkbox" id="c-37010092" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#37009480">parent</a><span>|</span><a href="#37008226">next</a><span>|</span><label class="collapse" for="c-37010092">[-]</label><label class="expand" for="c-37010092">[1 more]</label></div><br/><div class="children"><div class="content">For floating point math, there&#x27;s no distinction, as &quot;parralel computation with unknown path to the solution&quot; inherently implies &quot;results will vary&quot;, as (a+b)+c != a+(b+c).</div><br/></div></div></div></div><div id="37008226" class="c"><input type="checkbox" id="c-37008226" checked=""/><div class="controls bullet"><span class="by">cainxinth</span><span>|</span><a href="#37009480">prev</a><span>|</span><a href="#37009623">next</a><span>|</span><label class="collapse" for="c-37008226">[-]</label><label class="expand" for="c-37008226">[1 more]</label></div><br/><div class="children"><div class="content">I asked GPT to explain this:<p>&gt;In the MoE approach, different &quot;experts&quot; or portions of the model are selected for different parts of the input data. The selection of which experts to use can be influenced by several factors, including the specific content of the input data, the order in which data is processed in a batch, and possibly even minor variations in the internal state of the model.<p>&gt;This &quot;expert selection&quot; process introduces a level of stochasticity, or randomness, into the model&#x27;s operation. For example, if you process the same input data twice in slightly different contexts (e.g., as part of different batches), you might end up consulting slightly different sets of experts, leading to slightly different outputs.</div><br/></div></div><div id="37009623" class="c"><input type="checkbox" id="c-37009623" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#37008226">prev</a><span>|</span><a href="#37007965">next</a><span>|</span><label class="collapse" for="c-37009623">[-]</label><label class="expand" for="c-37009623">[1 more]</label></div><br/><div class="children"><div class="content">Well, a colleague of mine managed to build a non deterministic GET REST API endpoint. :D</div><br/></div></div><div id="37007965" class="c"><input type="checkbox" id="c-37007965" checked=""/><div class="controls bullet"><span class="by">hyperthesis</span><span>|</span><a href="#37009623">prev</a><span>|</span><a href="#37007071">next</a><span>|</span><label class="collapse" for="c-37007965">[-]</label><label class="expand" for="c-37007965">[3 more]</label></div><br/><div class="children"><div class="content">MoE: Mixture of Experts</div><br/><div id="37009168" class="c"><input type="checkbox" id="c-37009168" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#37007965">parent</a><span>|</span><a href="#37007071">next</a><span>|</span><label class="collapse" for="c-37009168">[-]</label><label class="expand" for="c-37009168">[2 more]</label></div><br/><div class="children"><div class="content">There’s a comment that’s 3 hours older than yours that clarifies this.</div><br/><div id="37010034" class="c"><input type="checkbox" id="c-37010034" checked=""/><div class="controls bullet"><span class="by">hyperthesis</span><span>|</span><a href="#37007965">root</a><span>|</span><a href="#37009168">parent</a><span>|</span><a href="#37007071">next</a><span>|</span><label class="collapse" for="c-37010034">[-]</label><label class="expand" for="c-37010034">[1 more]</label></div><br/><div class="children"><div class="content">I searched for MoE in the comments and didn&#x27;t see it. ah, you must mean this one <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37006549">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37006549</a>, which doesn&#x27;t include &quot;MoE&quot;, so that&#x27;s why I didn&#x27;t find it. Still, my comment&#x27;s upvotes show it was helpful to some - maybe they searched for &quot;MoE&quot; too, instead of &quot;mixture of experts&quot;.</div><br/></div></div></div></div></div></div><div id="37007071" class="c"><input type="checkbox" id="c-37007071" checked=""/><div class="controls bullet"><span class="by">throwawayadvsec</span><span>|</span><a href="#37007965">prev</a><span>|</span><a href="#37007803">next</a><span>|</span><label class="collapse" for="c-37007071">[-]</label><label class="expand" for="c-37007071">[1 more]</label></div><br/><div class="children"><div class="content">&quot;these tokens often compete against each other for available spots in expert buffers. &quot;
So is this also why ChatGPT is often just writing placeholders in place of functions when I ask him for some long code?</div><br/></div></div><div id="37007803" class="c"><input type="checkbox" id="c-37007803" checked=""/><div class="controls bullet"><span class="by">icelancer</span><span>|</span><a href="#37007071">prev</a><span>|</span><a href="#37009341">next</a><span>|</span><label class="collapse" for="c-37007803">[-]</label><label class="expand" for="c-37007803">[1 more]</label></div><br/><div class="children"><div class="content">How interesting. I was just discussing this last night with our analysts after I experimentally noticed that temp=0.0 (and all penalties&#x2F;top_p set accordingly) still showed non-determinate behavior. Wasn&#x27;t sure why this was, and now this article comes about.<p>The explanation makes quite a bit of sense.</div><br/></div></div><div id="37008078" class="c"><input type="checkbox" id="c-37008078" checked=""/><div class="controls bullet"><span class="by">afro88</span><span>|</span><a href="#37009341">prev</a><span>|</span><a href="#37006498">next</a><span>|</span><label class="collapse" for="c-37008078">[-]</label><label class="expand" for="c-37008078">[1 more]</label></div><br/><div class="children"><div class="content">&gt; these tokens often compete against each other for available spots in expert buffers.<p>Hold up, does this mean that under heavy load the results change? Does this explain why it sometimes feels like the output quality changes?</div><br/></div></div><div id="37006498" class="c"><input type="checkbox" id="c-37006498" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37008078">prev</a><span>|</span><a href="#37007226">next</a><span>|</span><label class="collapse" for="c-37006498">[-]</label><label class="expand" for="c-37006498">[16 more]</label></div><br/><div class="children"><div class="content">This is _excellent_ work, I&#x27;ve been adamantly against MoE for a set of reasons, this is the first compelling evidence I&#x27;ve seen that hasn&#x27;t been on Substack or a bare repeating of rumor.<p>I had absolutely no idea GPT4 was nondeterministic and I use it about 2 hours a day. I can see why a cursory looking wasn&#x27;t cutting it, they &quot;feel&quot; the same in your memory, a lot of similar vocab usage, but are formatted entirely differently, and have sort of a synonym-phrase thing going where some of the key words are the same.</div><br/><div id="37006632" class="c"><input type="checkbox" id="c-37006632" checked=""/><div class="controls bullet"><span class="by">152334H</span><span>|</span><a href="#37006498">parent</a><span>|</span><a href="#37006596">next</a><span>|</span><label class="collapse" for="c-37006632">[-]</label><label class="expand" for="c-37006632">[5 more]</label></div><br/><div class="children"><div class="content">Thanks. I&#x27;m really no expert (:P) on MoE research; I just noticed what was written in the Soft MoE paper and felt a need to check.<p>The non-deterministic outputs are really similar, yeah, if you check the gist examples I linked <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;152334H&#x2F;047827ad3740627f4d37826c867a196e" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;152334H&#x2F;047827ad3740627f4d37826c867a...</a>. This part is at least no surprise, since the randomness should be bounded.<p>I suspect OpenAI will figure out some way to reduce the randomness at some point, though, given their public commitment to eventually adding logprobs back to ChatCompletions.</div><br/><div id="37006816" class="c"><input type="checkbox" id="c-37006816" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#37006498">root</a><span>|</span><a href="#37006632">parent</a><span>|</span><a href="#37006596">next</a><span>|</span><label class="collapse" for="c-37006816">[-]</label><label class="expand" for="c-37006816">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think this commitment had any plausibility. Token &quot;probabilities&quot; only have a straightforward probabilistic interpretation for base models. In fine-tuned models, they do no longer represent the probability of the next token given the prompt, but rather how well the next token fulfills the ... tendencies induced by SL and RL tuning. Which is presumably pretty useless information. OpenAI has no intention to provide access to the GPT-4 base model, and they in fact removed API access to the GPT-3.5 base model.</div><br/><div id="37007056" class="c"><input type="checkbox" id="c-37007056" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37006498">root</a><span>|</span><a href="#37006816">parent</a><span>|</span><a href="#37006596">next</a><span>|</span><label class="collapse" for="c-37007056">[-]</label><label class="expand" for="c-37007056">[3 more]</label></div><br/><div class="children"><div class="content">Topic laundering, the probabilities are the probabilities, you don&#x27;t suddenly get wrong probabilities with more training on more data</div><br/><div id="37007240" class="c"><input type="checkbox" id="c-37007240" checked=""/><div class="controls bullet"><span class="by">goodside</span><span>|</span><a href="#37006498">root</a><span>|</span><a href="#37007056">parent</a><span>|</span><a href="#37006596">next</a><span>|</span><label class="collapse" for="c-37007240">[-]</label><label class="expand" for="c-37007240">[2 more]</label></div><br/><div class="children"><div class="content">You do, because it’s not just more training it’s PPO updates instead of MLE. It’s no longer trying to estimate the token distribution of the training corpus, it’s trying to shift logprobs into tokens that maximize expected reward from the RM. The GPT-4 technical report has a figure showing that logprobs become less well calibrated as confidence scores in the RLHF vs pre-train model.</div><br/><div id="37008163" class="c"><input type="checkbox" id="c-37008163" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37006498">root</a><span>|</span><a href="#37007240">parent</a><span>|</span><a href="#37006596">next</a><span>|</span><label class="collapse" for="c-37008163">[-]</label><label class="expand" for="c-37008163">[1 more]</label></div><br/><div class="children"><div class="content">Fascinating, ty</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37006596" class="c"><input type="checkbox" id="c-37006596" checked=""/><div class="controls bullet"><span class="by">derwiki</span><span>|</span><a href="#37006498">parent</a><span>|</span><a href="#37006632">prev</a><span>|</span><a href="#37008548">next</a><span>|</span><label class="collapse" for="c-37006596">[-]</label><label class="expand" for="c-37006596">[6 more]</label></div><br/><div class="children"><div class="content">GPT4 web chat for two hours a day? I buy that. Using the API repeatedly for the same inputs, eg developing a program, and the non-determinism is hard to miss.</div><br/><div id="37006744" class="c"><input type="checkbox" id="c-37006744" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#37006498">root</a><span>|</span><a href="#37006596">parent</a><span>|</span><a href="#37007292">next</a><span>|</span><label class="collapse" for="c-37006744">[-]</label><label class="expand" for="c-37006744">[3 more]</label></div><br/><div class="children"><div class="content">I would imagine that most people use nonzero temperature, so they won&#x27;t need to look for any explanation for non-determinism.</div><br/><div id="37006788" class="c"><input type="checkbox" id="c-37006788" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#37006498">root</a><span>|</span><a href="#37006744">parent</a><span>|</span><a href="#37007292">next</a><span>|</span><label class="collapse" for="c-37006788">[-]</label><label class="expand" for="c-37006788">[2 more]</label></div><br/><div class="children"><div class="content">Literally the first thing I did when I had llama.cpp working was set the temperature to 0 and repeat queries.<p>(but that&#x27;s mainly because I&#x27;m a weird old scientist with lots of experience with nondeterminism in software).</div><br/><div id="37008170" class="c"><input type="checkbox" id="c-37008170" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37006498">root</a><span>|</span><a href="#37006788">parent</a><span>|</span><a href="#37007292">next</a><span>|</span><label class="collapse" for="c-37008170">[-]</label><label class="expand" for="c-37008170">[1 more]</label></div><br/><div class="children"><div class="content">I did too, Kmeans broke me a couple years ago: but, never temperature at 0 with long length, and trusted my instinct instead of actual diffs. This is was the first time I actually diffed</div><br/></div></div></div></div></div></div><div id="37007292" class="c"><input type="checkbox" id="c-37007292" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#37006498">root</a><span>|</span><a href="#37006596">parent</a><span>|</span><a href="#37006744">prev</a><span>|</span><a href="#37008548">next</a><span>|</span><label class="collapse" for="c-37007292">[-]</label><label class="expand" for="c-37007292">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, it&#x27;s one of the first things you notice when trying to do some kind of &quot;feed GPT some data and get it to produce a novel answer to a question&quot; task with the API.</div><br/><div id="37008165" class="c"><input type="checkbox" id="c-37008165" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37006498">root</a><span>|</span><a href="#37007292">parent</a><span>|</span><a href="#37008548">next</a><span>|</span><label class="collapse" for="c-37008165">[-]</label><label class="expand" for="c-37008165">[1 more]</label></div><br/><div class="children"><div class="content">No, because if you wanted a novel answer, why would you set 0 temperature? ;)</div><br/></div></div></div></div></div></div><div id="37008548" class="c"><input type="checkbox" id="c-37008548" checked=""/><div class="controls bullet"><span class="by">bredren</span><span>|</span><a href="#37006498">parent</a><span>|</span><a href="#37006596">prev</a><span>|</span><a href="#37006770">next</a><span>|</span><label class="collapse" for="c-37008548">[-]</label><label class="expand" for="c-37008548">[1 more]</label></div><br/><div class="children"><div class="content">What do you use it for? Are you using many plugins? Curious what sort of insights someone using the tool this much might have, perhaps even through the batch of features released this week.</div><br/></div></div><div id="37006770" class="c"><input type="checkbox" id="c-37006770" checked=""/><div class="controls bullet"><span class="by">FanaHOVA</span><span>|</span><a href="#37006498">parent</a><span>|</span><a href="#37008548">prev</a><span>|</span><a href="#37007226">next</a><span>|</span><label class="collapse" for="c-37006770">[-]</label><label class="expand" for="c-37006770">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ve been adamantly against MoE for a set of reasons<p>Such as?</div><br/><div id="37007065" class="c"><input type="checkbox" id="c-37007065" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#37006498">root</a><span>|</span><a href="#37006770">parent</a><span>|</span><a href="#37007226">next</a><span>|</span><label class="collapse" for="c-37007065">[-]</label><label class="expand" for="c-37007065">[2 more]</label></div><br/><div class="children"><div class="content">It was completely unsubstantiated, based on rumours from a blog, but everyone repeated it as fact.</div><br/><div id="37007398" class="c"><input type="checkbox" id="c-37007398" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#37006498">root</a><span>|</span><a href="#37007065">parent</a><span>|</span><a href="#37007226">next</a><span>|</span><label class="collapse" for="c-37007398">[-]</label><label class="expand" for="c-37007398">[1 more]</label></div><br/><div class="children"><div class="content">I think it is pretty compelling that almost all of the people doing research into switch transformers at Google were hired into OAI. I am not sure if that is ouboicly reported but once Ghotz leaked those details about the models, I went to check where the authirs of those papers are now and.... yep</div><br/></div></div></div></div></div></div></div></div><div id="37007226" class="c"><input type="checkbox" id="c-37007226" checked=""/><div class="controls bullet"><span class="by">albystein</span><span>|</span><a href="#37006498">prev</a><span>|</span><a href="#37006908">next</a><span>|</span><label class="collapse" for="c-37007226">[-]</label><label class="expand" for="c-37007226">[1 more]</label></div><br/><div class="children"><div class="content">this hypothesis makes a lot of sense. if indeed gpt-4 is a sparse MoE—which i believe it is—then OpenAI must have tested and proved their initial idea of a large capacity MoE LLM model first training&#x2F;building a smaller one. this smaller test model might be gpt-3.5-turbo.</div><br/></div></div><div id="37007731" class="c"><input type="checkbox" id="c-37007731" checked=""/><div class="controls bullet"><span class="by">rvcdbn</span><span>|</span><a href="#37006908">prev</a><span>|</span><a href="#37007254">next</a><span>|</span><label class="collapse" for="c-37007731">[-]</label><label class="expand" for="c-37007731">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if there’s a side channel attack in there waiting to happen..</div><br/></div></div><div id="37007254" class="c"><input type="checkbox" id="c-37007254" checked=""/><div class="controls bullet"><span class="by">crazypython</span><span>|</span><a href="#37007731">prev</a><span>|</span><a href="#37009367">next</a><span>|</span><label class="collapse" for="c-37007254">[-]</label><label class="expand" for="c-37007254">[1 more]</label></div><br/><div class="children"><div class="content">The GPT-3.0 &quot;davinci-instruct-beta&quot; models have been returning non-deterministic logprobs as early as early 2021. This is speculation. CUDA itself often has nondeterminism bugs.<p>text-davinci-001 and text-davinci-002 were trained through FeedMe and SFT, while text-davinci-003 was RLHF; the models themselves have more variance at high temperature.</div><br/></div></div><div id="37009367" class="c"><input type="checkbox" id="c-37009367" checked=""/><div class="controls bullet"><span class="by">cratermoon</span><span>|</span><a href="#37007254">prev</a><span>|</span><a href="#37006874">next</a><span>|</span><label class="collapse" for="c-37009367">[-]</label><label class="expand" for="c-37009367">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It’s well-known at this point that GPT-4&#x2F;GPT-3.5-turbo is non-deterministic, even at temperature=0.0<p>Interestingly, on another discussion there was a claim that setting the temperature to 0.0 made gpt-4 deterministic: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36503146">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36503146</a></div><br/><div id="37010227" class="c"><input type="checkbox" id="c-37010227" checked=""/><div class="controls bullet"><span class="by">moonchrome</span><span>|</span><a href="#37009367">parent</a><span>|</span><a href="#37006874">next</a><span>|</span><label class="collapse" for="c-37010227">[-]</label><label class="expand" for="c-37010227">[1 more]</label></div><br/><div class="children"><div class="content">This guy probably never did anything nontrivial with the API - you notice almost instantly that the chat models (both 3.5 and 4) are nondeterministic at 0 temperature. Source - built a documentation search bot and had it crap out on me on copy pasted prompts when I was demoing it.</div><br/></div></div></div></div><div id="37006874" class="c"><input type="checkbox" id="c-37006874" checked=""/><div class="controls bullet"><span class="by">dudus</span><span>|</span><a href="#37009367">prev</a><span>|</span><label class="collapse" for="c-37006874">[-]</label><label class="expand" for="c-37006874">[36 more]</label></div><br/><div class="children"><div class="content">Off topic<p>&gt; 3 months later, reading a paper while on board a boring flight home, I have my answer.<p>I noticed people from hacker news routinely read scientific papers. This is a habit I envy but don&#x27;t share.<p>Any tips or sites for someone interested in picking up more science papers to read.</div><br/><div id="37006967" class="c"><input type="checkbox" id="c-37006967" checked=""/><div class="controls bullet"><span class="by">jldugger</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37006962">next</a><span>|</span><label class="collapse" for="c-37006967">[-]</label><label class="expand" for="c-37006967">[4 more]</label></div><br/><div class="children"><div class="content">For just getting started I recommend collections:<p>1. Ideas That Created The Future[1]. It&#x27;s a collection of fiftyish classic CS papers, with some commentary.<p>2. Wikipedia&#x27;s list[2].<p>3. Test of Time awards[3]. These are papers that have been around for a while and people still think are important.<p>4. Best paper awards[4]. Less useful than ToT as not every best paper is actually that good or important, and sometimes the award committees can&#x27;t see past names or brands for novel research.<p>5. Survey Journals[5]. Students often get their research started with a literature review and some go the extra step to collect dozens of papers into a summary paper. I subscribe to the RSS feed for that one, and usually one or two are interesting enough to read.<p>6. Citation mining -- As you read all these, consider their citation list as potential new reading material, or if an old paper leaves you wanting more, use Google Scholar to find a papers that cited what you just read.<p>[1]: <a href="https:&#x2F;&#x2F;www.amazon.com&#x2F;Ideas-That-Created-Future-Computer&#x2F;dp&#x2F;0262045303" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.amazon.com&#x2F;Ideas-That-Created-Future-Computer&#x2F;dp...</a><p>[2]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;List_of_important_publications_in_computer_science" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;List_of_important_publications...</a><p>[3]: <a href="https:&#x2F;&#x2F;www.usenix.org&#x2F;conferences&#x2F;test-of-time-awards" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.usenix.org&#x2F;conferences&#x2F;test-of-time-awards</a><p>[4]: <a href="https:&#x2F;&#x2F;jeffhuang.com&#x2F;best_paper_awards&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;jeffhuang.com&#x2F;best_paper_awards&#x2F;</a><p>[5]: <a href="https:&#x2F;&#x2F;dl.acm.org&#x2F;journal&#x2F;csur" rel="nofollow noreferrer">https:&#x2F;&#x2F;dl.acm.org&#x2F;journal&#x2F;csur</a></div><br/><div id="37007360" class="c"><input type="checkbox" id="c-37007360" checked=""/><div class="controls bullet"><span class="by">puzzledobserver</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37006967">parent</a><span>|</span><a href="#37006981">next</a><span>|</span><label class="collapse" for="c-37007360">[-]</label><label class="expand" for="c-37007360">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d like to disagree with this. In particular, about [1]: It is a collection of papers in many different topics. There is little technical overlap between Alan Turing&#x27;s Entscheidungsproblem paper, for instance, and Hoare&#x27;s paper on axiomatic semantics. Also, the papers are all from the 70s. They&#x27;re uniformly influential papers, and have shaped the field, but the fields and the vernacular used by working researchers is very different. At best, the papers approximate a four year undergrad curriculum in CS, and at worst, are a recipe to get distracted and overwhelmed. The link to Wikipedia [2] is somewhat better in that the papers appear to be more modern, but suffers even more from the problem of diversity.<p>A somewhat similar problem arises with test-of-time and best paper awards. To elaborate on my complaint, imagine the exaggerated case of someone trying to understand modern science by intensely focusing on the work of researchers who won the Nobel Prize. Clearly all very important work, but understanding the 1990 Physics Nobel Prize (on electron-proton scattering) is of no use to understanding the work for which 1991 Nobel was awarded (complex systems and polymers).<p>There are two things that (I&#x27;m assuming the OP&#x27;s field of interest is computing) a CS education provides: At the undergrad and in the early stages of grad school, breadth of topics, and their modern synthesis. You don&#x27;t spend much time reading papers (at least in an undergraduate education), but you understand the basics, and get a feel for the problems considered and the sensibilities of researchers. In an intermediate-level graduate seminar, you pick a narrow topic, and focus on papers in that topic. The first papers in the area (like Dijkstra&#x27;s papers on distributed computing), the best &#x2F; most important papers in the area, and the latest papers on topical interests (like Merkle trees and blockchains). There is thematic and technical continuity from one paper to the next, and you start to understand the the story being told. Then, late in graduate school, and in the rest of one&#x27;s professional career, one starts reviewing papers that haven&#x27;t even been published. At this point, you see the story being written: the steps and the missteps, and the memorable and not-so-memorable papers in a field. To truly understand a field, one needs to read not just the great papers, but also the middling ones.<p>And one needs to concentrate on a topic. The thing about a forum such as HackerNews is that for every topic of interest, there&#x27;s likely a person here who&#x27;s an expert in the area, but it is easy to confuse that observation with the much stronger claim that there&#x27;s a person here who&#x27;s an expert on every topic. The last of those people died in the mid-20th century, if they ever existed.</div><br/><div id="37008565" class="c"><input type="checkbox" id="c-37008565" checked=""/><div class="controls bullet"><span class="by">jldugger</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37007360">parent</a><span>|</span><a href="#37006981">next</a><span>|</span><label class="collapse" for="c-37008565">[-]</label><label class="expand" for="c-37008565">[1 more]</label></div><br/><div class="children"><div class="content">I feel like you&#x27;re giving advice on how to become a PhD student, and frankly, that&#x27;s not the point of the question, and if it is: any grad student who can&#x27;t read papers should ask their advisor for advice.<p>So I take OP&#x27;s perspective to be from a practitioner (such as myself). Apart from my colleagues in R&amp;D, we aren&#x27;t called upon to write new papers that demands expertise in ever increasing narrowness. Instead we are to solve the needs of the product, usually regardless of specific expertise. So we need to be more broadly equipped, as it&#x27;s typically better to have a screwdriver and a hammer and a screwdriver in the toolbox than a ten different screwdriver bits of varying niche application.<p>As an example, the TD-IDF paper curated in [1] has been broadly useful as a log analysis tool to surface interesting log lines and remove the mundane common &quot;error&quot; logs. There&#x27;s been many advancements since then, using bayesian techniques or deep learning, but this one is simple enough and cheap enough to deploy.</div><br/></div></div></div></div><div id="37006981" class="c"><input type="checkbox" id="c-37006981" checked=""/><div class="controls bullet"><span class="by">jldugger</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37006967">parent</a><span>|</span><a href="#37007360">prev</a><span>|</span><a href="#37006962">next</a><span>|</span><label class="collapse" for="c-37006981">[-]</label><label class="expand" for="c-37006981">[1 more]</label></div><br/><div class="children"><div class="content">From there, just keep a reading queue. If you notice a particular journal is a good source of material, consider subscribing to it.</div><br/></div></div></div></div><div id="37006962" class="c"><input type="checkbox" id="c-37006962" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37006967">prev</a><span>|</span><a href="#37009287">next</a><span>|</span><label class="collapse" for="c-37006962">[-]</label><label class="expand" for="c-37006962">[4 more]</label></div><br/><div class="children"><div class="content">&gt; I noticed people from hacker news routinely read scientific papers.<p>Do they? I suspect that most don&#x27;t, and those that do are either in specialized careers or are engaged in some kind of scientific research.<p>Some interesting research gets disseminated via Twitter and chatrooms. Or maybe you follow a podcast that mentions new research. But you might also be following new publications from a handful of reputable journals, or following an Arxiv category, or looking through new conference papers. It&#x27;s very easy to get overwhelmed with new research to read, and not knowing what&#x27;s worth your time, unless you&#x27;re already very familiar with the field and well-versed in the material.</div><br/><div id="37007808" class="c"><input type="checkbox" id="c-37007808" checked=""/><div class="controls bullet"><span class="by">i-use-nixos-btw</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37006962">parent</a><span>|</span><a href="#37008339">next</a><span>|</span><label class="collapse" for="c-37007808">[-]</label><label class="expand" for="c-37007808">[1 more]</label></div><br/><div class="children"><div class="content">I strongly agree.<p>Once upon a time, I was in condensed matter physics. I was (and remain) interested in a very specific niche within that, and I read a small handful of the papers that were published each week. I’m not actively researching or publishing anymore so I cap this to one or two per month now, and mostly scan over them to see if anything piques my interest.<p>I was still interested in condensed matter as a whole, at the time, and attended group seminars once a month to see what other people were currently excited about - there wasn’t any hope of me reading a cross section of all condensed matter papers because there is far more published per week than I’d be physically able to even glimpse at, and most of it is stuff I don’t understand or particularly care about.<p>I was likewise interested in physics as a whole, and twice a year I’d attend a departmental seminar and see what people in the entire department were interested in. Most was far over my head, but it still directed me to a small handful of papers that I’d read for the hell of it. Of course, I couldn’t do this without first hearing people review the research. There’s far more published per day in physics as a whole than I could read in a year, and most of it I’d find unrelatable and uninteresting.<p>I guess where I’m going with this is that anyone with a specific interest is already reading papers. It’s their job. Anyone with a general interest would find actively pursing paper hunting to be a waste of time with a ridiculously bad signal to noise ratio. Instead, they should use channels that align closely with their own interests, through which they can get recommendations to read papers from the aforementioned specialists who have already filtered out much of the noise themselves. At that point, they should actually read the resulting papers.<p>There is another trick, though, and that’s to find an individual who publishes two unrelated pieces of work that you find interesting, then read their work and maybe those of their coauthors. Be careful, though, because this is a slippery slope to specialising, after which you’ll find yourself back at the point where you don’t aren’t following 99.9% of the stuff you wanted to follow in the first place.</div><br/></div></div><div id="37008339" class="c"><input type="checkbox" id="c-37008339" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37006962">parent</a><span>|</span><a href="#37007808">prev</a><span>|</span><a href="#37008309">next</a><span>|</span><label class="collapse" for="c-37008339">[-]</label><label class="expand" for="c-37008339">[1 more]</label></div><br/><div class="children"><div class="content">Long time HN&#x27;er college dropout and I read a LOT of scientific papers. Probably an average of 4 a week over the past couple of decades, sometimes reading 40 in a week.<p>I probably averaged 20 a week back in March when open source AI was booming in the wake of Llama and on the heels of GPT-4.</div><br/></div></div><div id="37008309" class="c"><input type="checkbox" id="c-37008309" checked=""/><div class="controls bullet"><span class="by">CSMastermind</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37006962">parent</a><span>|</span><a href="#37008339">prev</a><span>|</span><a href="#37009287">next</a><span>|</span><label class="collapse" for="c-37008309">[-]</label><label class="expand" for="c-37008309">[1 more]</label></div><br/><div class="children"><div class="content">I typically look up and read a paper when it&#x27;s referenced in discussion or cited in something else, I&#x27;m reading&#x2F;watching, and the purported contents seem surprising to me.  This normally happens 3 or 4 times a week.<p>Honestly many papers are written in a way that&#x27;s hard to approach and difficult to understand unless you&#x27;re prepared to reread them a few times.<p>You&#x27;re better off just getting your science news from actual science communicators and not the raw source.</div><br/></div></div></div></div><div id="37009287" class="c"><input type="checkbox" id="c-37009287" checked=""/><div class="controls bullet"><span class="by">allisdust</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37006962">prev</a><span>|</span><a href="#37007289">next</a><span>|</span><label class="collapse" for="c-37009287">[-]</label><label class="expand" for="c-37009287">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t read them for the sake of reading them. Read them to solve your current problem or trying to keep up with advancements in a narrow field you love. Most papers (especially the ones in deep learning) seem to also have a mathematical fetish (to put it mildly) where needless representations are used where none are required and are self evident (for example inputs belong to Real number set). It ends up making the paper pseudo complex and unapproachable. Most papers are doing average&#x2F;summation&#x2F;series operations but instead of just saying so, use the symbols all over the place. So even if a few papers appear tough, keep reading them and digest your first paper thoroughly. You will find subsequent papers mostly are a rehash of existing work with similar fetish to make trial and error appear like mathematically sound research. Once in a while, you would find some paper which is fully theoretical and try to prove that either the inputs&#x2F;outputs&#x2F;components of models have certain well known mathematical properties and hence can be reasoned similarly. These are rare and would be difficult to parse through.<p>PS: Best papers I have seen are from deepmind where the approaches usually described are novel, varied and path breaking. Worst ones are - well no names but those that just use training and eval sets generated by GPT4 and try to prove things empirically</div><br/><div id="37009832" class="c"><input type="checkbox" id="c-37009832" checked=""/><div class="controls bullet"><span class="by">LudwigNagasena</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37009287">parent</a><span>|</span><a href="#37007289">next</a><span>|</span><label class="collapse" for="c-37009832">[-]</label><label class="expand" for="c-37009832">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Most papers (especially the ones in deep learning) seem to also have a mathematical fetish (to put it mildly) where needless representations are used where none are required and are self evident (for example inputs belong to Real number set). It ends up making the paper pseudo complex and unapproachable.<p>I completely disagree with that. Spelling out math is literally something out of 12th century. It just hinders understanding, if you have basic STEM-level math literacy, which anyone who reads an ML paper is implied to have (how could you seriously study linear algebra and calculus without it?).<p>Math may actually be the first thing you recognise in a paper, which can help you cross-reference the text to understand it.</div><br/></div></div></div></div><div id="37007289" class="c"><input type="checkbox" id="c-37007289" checked=""/><div class="controls bullet"><span class="by">brmgb</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37009287">prev</a><span>|</span><a href="#37007146">next</a><span>|</span><label class="collapse" for="c-37007289">[-]</label><label class="expand" for="c-37007289">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I noticed people from hacker news routinely read scientific papers.<p>Highly doubt that. It’s very hard to actually read scientific papers when you are not actively doing research.<p>You can’t just read a research paper in isolation. It’s next to useless. You need to understand its context, where it stands with regard to its sources and what it brings which is actually new and valuable. It’s nearly impossible to do properly if you are not fully immersed in a research subject.<p>I don’t even know how you would scheme introduction and sources to filter articles which are immediately obviously useless without being immersed in a field.<p>I guess you can obviously go though lists of papers which have be deemed worthwhile by someone else or got prices. That solves the filtering issue but then nearly every time you will be better served reading a text book presenting the ideas in said papers.<p>I fully expect the HN readership to contain a significant amount of students and actual researchers which explain why you encounter people reading papers but these people aside I would be surprised if the habit is common.</div><br/><div id="37008180" class="c"><input type="checkbox" id="c-37008180" checked=""/><div class="controls bullet"><span class="by">cypress66</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37007289">parent</a><span>|</span><a href="#37007146">next</a><span>|</span><label class="collapse" for="c-37008180">[-]</label><label class="expand" for="c-37008180">[2 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need to be doing research to read an ML paper. With some general knowledge in AI you should be able to understand most papers.<p>And even then, sometimes you don&#x27;t understand or care about their procedures, and you just want to look at the pretty results (check out this song they generated using AI!). There&#x27;s even a very popular YouTube channel that focuses on this (two minute papers).<p>Finally, you usually hear about these cool papers via Twitter &#x2F; X</div><br/><div id="37010070" class="c"><input type="checkbox" id="c-37010070" checked=""/><div class="controls bullet"><span class="by">brmgb</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37008180">parent</a><span>|</span><a href="#37007146">next</a><span>|</span><label class="collapse" for="c-37010070">[-]</label><label class="expand" for="c-37010070">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You don&#x27;t need to be doing research to read an ML paper. With some general knowledge in AI you should be able to understand most papers.<p>I have a degree which involved reading some ML papers and I seriously doubt that. The field is flooded with papers which looks good when you quickly read them but are actually worthless because they misrepresent the state of the art or intentionally don’t compare their methods with other papers they should know.<p>&gt; And even then, sometimes you don&#x27;t understand or care about their procedures, and you just want to look at the pretty results<p>That’s fair but I wouldn’t call that reading a scientific paper.</div><br/></div></div></div></div></div></div><div id="37007146" class="c"><input type="checkbox" id="c-37007146" checked=""/><div class="controls bullet"><span class="by">obblekk</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37007289">prev</a><span>|</span><a href="#37007631">next</a><span>|</span><label class="collapse" for="c-37007146">[-]</label><label class="expand" for="c-37007146">[6 more]</label></div><br/><div class="children"><div class="content">Build the habit.<p>When google doesn&#x27;t return a good result to a specific question, switch to scholar.google.com and start reading abstracts. Everything may seem like an opaque maze at first, but just keep reading and patterns start emerging quickly and become useful.</div><br/><div id="37007173" class="c"><input type="checkbox" id="c-37007173" checked=""/><div class="controls bullet"><span class="by">TechBro8615</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37007146">parent</a><span>|</span><a href="#37007631">next</a><span>|</span><label class="collapse" for="c-37007173">[-]</label><label class="expand" for="c-37007173">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t mind reading research papers, but they&#x27;re really annoying to read on a phone screen. I remember a few years ago, an HN comment shared a link to some tool that could convert a PDF to single column text and make it more readable on a phone screen, but I can&#x27;t find it. Anyone remember this or have the link?</div><br/><div id="37008014" class="c"><input type="checkbox" id="c-37008014" checked=""/><div class="controls bullet"><span class="by">jayshua</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37007173">parent</a><span>|</span><a href="#37007951">next</a><span>|</span><label class="collapse" for="c-37008014">[-]</label><label class="expand" for="c-37008014">[1 more]</label></div><br/><div class="children"><div class="content">I use an android (and iOS I think) app called Xodo. The &quot;reader mode&quot; re-flows the PDF into a screen-width single column like an e-book. The latest update really buried the option in the menus, but it&#x27;s there somewhere and works pretty well.</div><br/></div></div><div id="37007951" class="c"><input type="checkbox" id="c-37007951" checked=""/><div class="controls bullet"><span class="by">UltimateEdge</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37007173">parent</a><span>|</span><a href="#37008014">prev</a><span>|</span><a href="#37007334">next</a><span>|</span><label class="collapse" for="c-37007951">[-]</label><label class="expand" for="c-37007951">[1 more]</label></div><br/><div class="children"><div class="content">The software KOReader [1] has a PDF reflow setting which you can try.<p>[1] <a href="http:&#x2F;&#x2F;koreader.rocks&#x2F;" rel="nofollow noreferrer">http:&#x2F;&#x2F;koreader.rocks&#x2F;</a></div><br/></div></div><div id="37007334" class="c"><input type="checkbox" id="c-37007334" checked=""/><div class="controls bullet"><span class="by">segfaultbuserr</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37007173">parent</a><span>|</span><a href="#37007951">prev</a><span>|</span><a href="#37007864">next</a><span>|</span><label class="collapse" for="c-37007334">[-]</label><label class="expand" for="c-37007334">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>but they&#x27;re really annoying to read on a phone screen.</i><p>+1. I&#x27;ve already read probably 100 research papers this year in search of solutions to some technical problems, mostly while lying on bed with a tablet. I won&#x27;t read as much without it.</div><br/></div></div><div id="37007864" class="c"><input type="checkbox" id="c-37007864" checked=""/><div class="controls bullet"><span class="by">michaelmrose</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37007173">parent</a><span>|</span><a href="#37007334">prev</a><span>|</span><a href="#37007631">next</a><span>|</span><label class="collapse" for="c-37007864">[-]</label><label class="expand" for="c-37007864">[1 more]</label></div><br/><div class="children"><div class="content">How big is your phone screen and what are you using to read it? A few inches makes a lot of difference. In landscape mode my phone is 6.5&quot; wide and reading a pdf with moonreader in full screen because its wide enough to read without having to reformat anything. You can also click on figures to view only that figure.<p>If that isn&#x27;t enough you might consider a tablet or e-reader instead of trying so hard to make existing options work.<p>You CAN convert to something like epub which is trivially reflowed and this is just fine for reading fiction but just isn&#x27;t as pleasant and nicely formatted as a pdf.</div><br/></div></div></div></div></div></div><div id="37007631" class="c"><input type="checkbox" id="c-37007631" checked=""/><div class="controls bullet"><span class="by">cpeterso</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37007146">prev</a><span>|</span><a href="#37007846">next</a><span>|</span><label class="collapse" for="c-37007631">[-]</label><label class="expand" for="c-37007631">[1 more]</label></div><br/><div class="children"><div class="content">Check out the papers and talks from <i>Papers We Love</i>, a &quot;repository of academic computer science papers and a community who loves reading them&quot;:<p><a href="https:&#x2F;&#x2F;paperswelove.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;paperswelove.org&#x2F;</a></div><br/></div></div><div id="37007846" class="c"><input type="checkbox" id="c-37007846" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37007631">prev</a><span>|</span><a href="#37008984">next</a><span>|</span><label class="collapse" for="c-37007846">[-]</label><label class="expand" for="c-37007846">[1 more]</label></div><br/><div class="children"><div class="content">It depends on why you want to read papers and what you want to get out of it.<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37006967">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37006967</a> suggested some avenues for finding some classic papers.  The follow-up <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37007360">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37007360</a> pointed out some circumstances where that&#x27;s not ideal.  But in the process, implicitly assumes that you want to become familiar with current research, instead of just enjoying classic papers for some other motivation.<p>I mostly read papers in mathematics and computer science.  For other disciplines I mostly rely on pop science, like Slate Star Codex or Money Stuff and blogs.  There&#x27;s also The Monad Reader (<a href="https:&#x2F;&#x2F;wiki.haskell.org&#x2F;The_Monad.Reader" rel="nofollow noreferrer">https:&#x2F;&#x2F;wiki.haskell.org&#x2F;The_Monad.Reader</a>) if you are interested in functional programming.<p>There&#x27;s various blogs with interesting articles.  Eg Vitalik Buterin has great stuff, like <a href="https:&#x2F;&#x2F;vitalik.ca&#x2F;general&#x2F;2017&#x2F;11&#x2F;09&#x2F;starks_part_1.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;vitalik.ca&#x2F;general&#x2F;2017&#x2F;11&#x2F;09&#x2F;starks_part_1.html</a> and he links to the original papers.  (I have no conclusive opinions on whether crypto-currencies are useful or good for the real world, but I do find the math behind some of them endlessly fascinating.  Especially zero-knowledge proofs.)<p>Wikipedia is also often a good starting point.  Whenever you read about a random topic, Wikipedia usually has an article that comes with plenty of references.  Eg <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Forth_Bridge#References" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Forth_Bridge#References</a> links to <a href="http:&#x2F;&#x2F;www.bath.ac.uk&#x2F;ace&#x2F;uploads&#x2F;StudentProjects&#x2F;Bridgeconference2007&#x2F;conference&#x2F;mainpage&#x2F;Magee_Forth_Bridge.pdf" rel="nofollow noreferrer">http:&#x2F;&#x2F;www.bath.ac.uk&#x2F;ace&#x2F;uploads&#x2F;StudentProjects&#x2F;Bridgeconf...</a> and down the rabbit hole you go.<p><a href="https:&#x2F;&#x2F;gwern.net&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;gwern.net&#x2F;</a> also has great write-ups and links to original papers.</div><br/></div></div><div id="37008984" class="c"><input type="checkbox" id="c-37008984" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37007846">prev</a><span>|</span><a href="#37007310">next</a><span>|</span><label class="collapse" for="c-37008984">[-]</label><label class="expand" for="c-37008984">[1 more]</label></div><br/><div class="children"><div class="content">Pick ones that are easy to read. Some are written line a magazine article. Others are math dense, reference another paper you can’t get hold of every other sentence and are a kind of marketing material anyway.<p>Also youtube and code: Attention is all you need is not a nice paper to read for Joe programmer, but you can understand what it is doing by watching karpathy and reading his code (or someone else who has implemented it, Llama for example). But you need to do some basic torch training first (karpathy again!)</div><br/></div></div><div id="37007310" class="c"><input type="checkbox" id="c-37007310" checked=""/><div class="controls bullet"><span class="by">alecst</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37008984">prev</a><span>|</span><a href="#37009313">next</a><span>|</span><label class="collapse" for="c-37007310">[-]</label><label class="expand" for="c-37007310">[2 more]</label></div><br/><div class="children"><div class="content">Honestly a lot are really hard to read. You start with the easy ones, learn the lingo, and then just keep going. Eventually you can enjoy reading the harder ones.<p>You learn pretty quickly that if you want answers, it&#x27;s better to just go straight to the source, rather than have it filtered through someone else, where the message can (and often does) get twisted.<p>What are you interested in reading about? Maybe some people can recommend you some papers to start with.</div><br/><div id="37007859" class="c"><input type="checkbox" id="c-37007859" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#37006874">root</a><span>|</span><a href="#37007310">parent</a><span>|</span><a href="#37009313">next</a><span>|</span><label class="collapse" for="c-37007859">[-]</label><label class="expand" for="c-37007859">[1 more]</label></div><br/><div class="children"><div class="content">There are certainly easier and harder papers.  Though when you are struggling: keep in mind that there are also papers that are just badly written (and some papers that are well written).</div><br/></div></div></div></div><div id="37009313" class="c"><input type="checkbox" id="c-37009313" checked=""/><div class="controls bullet"><span class="by">rgoldste</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37007310">prev</a><span>|</span><a href="#37007836">next</a><span>|</span><label class="collapse" for="c-37009313">[-]</label><label class="expand" for="c-37009313">[1 more]</label></div><br/><div class="children"><div class="content">Don’t feel like you need to understand 100%. You can always give yourself an hour to read a paper and gloss over some notation.  If you read 5 papers over the course of a month, you can go back to your favorite and dive into the notation.</div><br/></div></div><div id="37007836" class="c"><input type="checkbox" id="c-37007836" checked=""/><div class="controls bullet"><span class="by">NalNezumi</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37009313">prev</a><span>|</span><a href="#37007244">next</a><span>|</span><label class="collapse" for="c-37007836">[-]</label><label class="expand" for="c-37007836">[1 more]</label></div><br/><div class="children"><div class="content">There are some materials about &quot;how to read scientific paper&quot;, like the pdf one from U waterloo [3] with some methodological advice. Some good advice in this old HN thread [1]<p>But I don&#x27;t see the point of reading a scientific paper <i>unless you&#x27;re actually curious about a specific topic</i>. They are often hard to read, dense, have so many field-specific jargon that if you&#x27;re new, you won&#x27;t be able to read <i>one</i> paper and grasp everything. You would have to read references, or a book&#x2F;blog that summaries core points.<p>So find a specific field you&#x27;re interested in, find a good book&#x2F;blog&#x2F;homepage&#x2F;tutorial&#x2F;video to get your basics going so that when you start reading papers you won&#x27;t be completely lost.<p>Then find a highly cited survey paper to understand what progress have been made beyond what is now basic. Then  you can follow your curiously along that survey, decide a branch of research to read upon. You&#x27;ll probably then realize that a few labs research&#x2F;publish a lot in a specific direction. Now you can follow those professors (Twitter, Google scholar email notification) to keep up to date. By reading a lot you&#x27;ll also start to notice papers that are &quot;published just to get my PhD&quot; and soon enough you can just read abstract + intro&#x2F;result to judge if it is valuable or not.<p>If ML&#x2F;LLM is your curiosity probably Lillian Wengs blog [2] is a good start for tutorials &#x2F; surveys.<p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24986727">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24986727</a><p>[2] <a href="https:&#x2F;&#x2F;lilianweng.github.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;lilianweng.github.io&#x2F;</a><p>Edit: direct link
[3]
<a href="https:&#x2F;&#x2F;web.stanford.edu&#x2F;class&#x2F;ee384m&#x2F;Handouts&#x2F;HowtoReadPaper.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;web.stanford.edu&#x2F;class&#x2F;ee384m&#x2F;Handouts&#x2F;HowtoReadPape...</a></div><br/></div></div><div id="37007244" class="c"><input type="checkbox" id="c-37007244" checked=""/><div class="controls bullet"><span class="by">necubi</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37007836">prev</a><span>|</span><a href="#37007963">next</a><span>|</span><label class="collapse" for="c-37007244">[-]</label><label class="expand" for="c-37007244">[1 more]</label></div><br/><div class="children"><div class="content">For me it&#x27;s very helpful to print out papers and read them with a pen in hand, away from my computer. Papers tend to be dense and require a level of focus that (I at least) cannot maintain when reading on a screen. It helps as well to able to easily take notes and annotate the paper.</div><br/></div></div><div id="37007963" class="c"><input type="checkbox" id="c-37007963" checked=""/><div class="controls bullet"><span class="by">AnthonBerg</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37007244">prev</a><span>|</span><a href="#37007995">next</a><span>|</span><label class="collapse" for="c-37007963">[-]</label><label class="expand" for="c-37007963">[1 more]</label></div><br/><div class="children"><div class="content">Anyone can read scientific papers. All you need to do is pierce the layer of jargon. It takes practice but you kind of just pick it up. Reading on a  computer helps because you can get words defined by clicking on them. Reading on paper is good too, it’s easier to keep at it and it sticks better.<p>Some sense of urgency helps. Most people will have a medical ailment or physiological issue of some sort. I promise you that there exist useful papers on it.</div><br/></div></div><div id="37007995" class="c"><input type="checkbox" id="c-37007995" checked=""/><div class="controls bullet"><span class="by">j7ake</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37007963">prev</a><span>|</span><a href="#37007139">next</a><span>|</span><label class="collapse" for="c-37007995">[-]</label><label class="expand" for="c-37007995">[1 more]</label></div><br/><div class="children"><div class="content">Feedly with keywords for your favorite topics or researchers works decently.<p>I imagine this routine comes from people with research backgrounds, where browsing papers is the academic way of googling around for answers.</div><br/></div></div><div id="37007139" class="c"><input type="checkbox" id="c-37007139" checked=""/><div class="controls bullet"><span class="by">obblekk</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37007995">prev</a><span>|</span><a href="#37007638">next</a><span>|</span><label class="collapse" for="c-37007139">[-]</label><label class="expand" for="c-37007139">[1 more]</label></div><br/><div class="children"><div class="content">Build the habit.<p>When google doesn&#x27;t return a good result to a specific question, switch to scholar.google.com and start reading abstracts. It&#x27;ll seem like an opaque maze at first, but just keep reading and it&#x27;ll start clearing up pretty quickly and become useful.</div><br/></div></div><div id="37007638" class="c"><input type="checkbox" id="c-37007638" checked=""/><div class="controls bullet"><span class="by">ugh123</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37007139">prev</a><span>|</span><a href="#37007543">next</a><span>|</span><label class="collapse" for="c-37007638">[-]</label><label class="expand" for="c-37007638">[1 more]</label></div><br/><div class="children"><div class="content">I usually just read the abstract and synthesize that with the comments on HN to get the gist (and legit-ness) of the research.</div><br/></div></div><div id="37007543" class="c"><input type="checkbox" id="c-37007543" checked=""/><div class="controls bullet"><span class="by">dustingetz</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37007638">prev</a><span>|</span><a href="#37007350">next</a><span>|</span><label class="collapse" for="c-37007543">[-]</label><label class="expand" for="c-37007543">[1 more]</label></div><br/><div class="children"><div class="content">read textbooks instead most papers are obtuse and poorly written even famous ones. you can find them in wikipedia footnotes</div><br/></div></div><div id="37007350" class="c"><input type="checkbox" id="c-37007350" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37007543">prev</a><span>|</span><a href="#37007796">next</a><span>|</span><label class="collapse" for="c-37007350">[-]</label><label class="expand" for="c-37007350">[1 more]</label></div><br/><div class="children"><div class="content">They read scientific papers in the same way that everyone &quot;read&quot; Capital in the 21st Century, when that was a thing.</div><br/></div></div><div id="37007796" class="c"><input type="checkbox" id="c-37007796" checked=""/><div class="controls bullet"><span class="by">jcims</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37007350">prev</a><span>|</span><a href="#37008158">next</a><span>|</span><label class="collapse" for="c-37007796">[-]</label><label class="expand" for="c-37007796">[1 more]</label></div><br/><div class="children"><div class="content">Pick something you’re interested in and have a passing knowledge of.</div><br/></div></div><div id="37008158" class="c"><input type="checkbox" id="c-37008158" checked=""/><div class="controls bullet"><span class="by">armchairhacker</span><span>|</span><a href="#37006874">parent</a><span>|</span><a href="#37007796">prev</a><span>|</span><label class="collapse" for="c-37008158">[-]</label><label class="expand" for="c-37008158">[1 more]</label></div><br/><div class="children"><div class="content">I read the abstract and look at the pretty figures :)</div><br/></div></div></div></div></div></div></div></div></div></body></html>