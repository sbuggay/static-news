<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1725008476993" as="style"/><link rel="stylesheet" href="styles.css?v=1725008476993"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.thebignewsletter.com/p/judges-rule-big-techs-free-ride-on">Judges rule Big Tech&#x27;s free ride on Section 230 is over</a> <span class="domain">(<a href="https://www.thebignewsletter.com">www.thebignewsletter.com</a>)</span></div><div class="subtext"><span>eatonphil</span> | <span>380 comments</span></div><br/><div><div id="41392867" class="c"><input type="checkbox" id="c-41392867" checked=""/><div class="controls bullet"><span class="by">nsagent</span><span>|</span><a href="#41393921">next</a><span>|</span><label class="collapse" for="c-41392867">[-]</label><label class="expand" for="c-41392867">[181 more]</label></div><br/><div class="children"><div class="content">The current comments seem to say this is rings the death knell of social media and that this just leads to government censorship. I&#x27;m not so sure.<p>I think the ultimate problem is that social media is not unbiased — it curates what people are shown. In that role they are no longer an impartial party merely hosting content. It seems this ruling is saying that the curation being algorithmic does not absolve the companies from liability.<p>In a very general sense, this ruling could be seen as a form of net neutrality. Currently social media platforms favor certain content, while down weighting others. Sure, it might be at a different level than peer agreements between ISPs and websites, but it amounts to a similar phenomenon when most people interact on social media through the feed.<p>Honestly, I think I&#x27;d love to see what changes this ruling brings about. HN is quite literally the only social media site (loosely interpreted) I even have an account on anymore, mainly because of how truly awful all the sites have become. Maybe this will make social media more palatable again? Maybe not, but I&#x27;m inclined to see what shakes out.</div><br/><div id="41393405" class="c"><input type="checkbox" id="c-41393405" checked=""/><div class="controls bullet"><span class="by">nox101</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41395202">next</a><span>|</span><label class="collapse" for="c-41393405">[-]</label><label class="expand" for="c-41393405">[105 more]</label></div><br/><div class="children"><div class="content">I&#x27;m probably mis-understanding the implications but, IIUC, as it is, HN is moderated by dang (and others?) but still falls under 230 meaning HN is not responsible for what other users post here.<p>With this ruling, HN is suddenly responsibly for all posts here specifically because of the moderation. So they have 2 options.<p>(1) Stop the moderation so they can be safe under 230. Result, HN turns to 4chan.<p>(2) enforce the moderation to a much higher degree by say, requiring non-anon accounts and TOS that make each poster responsible for their own content and&#x2F;or manually approve every comment.<p>I&#x27;m not even sure how you&#x27;d run a website with user content if you wanted to moderate that content and still avoid being liable for illegal content.</div><br/><div id="41393527" class="c"><input type="checkbox" id="c-41393527" checked=""/><div class="controls bullet"><span class="by">lcnPylGDnU4H9OF</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393405">parent</a><span>|</span><a href="#41398039">next</a><span>|</span><label class="collapse" for="c-41393527">[-]</label><label class="expand" for="c-41393527">[64 more]</label></div><br/><div class="children"><div class="content">&gt; With this ruling, HN is suddenly responsibly for all posts here specifically because of the moderation.<p>I think this is a mistaken understanding of the ruling. In this case, TikTok decided, with no other context, to make a personalized recommendation to a user who visited their recommendation page. On HN, your front page is not different from my front page. (Indeed, there is no personalized recommendation page on HN, as far as I&#x27;m aware.)</div><br/><div id="41393625" class="c"><input type="checkbox" id="c-41393625" checked=""/><div class="controls bullet"><span class="by">crummy</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393527">parent</a><span>|</span><a href="#41394181">next</a><span>|</span><label class="collapse" for="c-41393625">[-]</label><label class="expand" for="c-41393625">[36 more]</label></div><br/><div class="children"><div class="content">&gt; The Court held that a platform’s algorithm that reflects “editorial judgments” about “compiling the third-party speech it wants in the way it wants” is the platform’s own “expressive product” and is therefore protected by the First Amendment.<p>I don&#x27;t see how this is about personalization. HN has an algorithm that shows what it wants in the way it wants.</div><br/><div id="41394786" class="c"><input type="checkbox" id="c-41394786" checked=""/><div class="controls bullet"><span class="by">zerocrates</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393625">parent</a><span>|</span><a href="#41396359">next</a><span>|</span><label class="collapse" for="c-41394786">[-]</label><label class="expand" for="c-41394786">[4 more]</label></div><br/><div class="children"><div class="content">So, yes, the TikTok FYP is different from a forum with moderation.<p>But the basis of this ruling is basically &quot;well the <i>Moody</i> case says that curation&#x2F;moderation&#x2F;suggestion&#x2F;whatever is First Amendment protected speech, therefore that&#x27;s <i>your</i> speech and not somebody else&#x27;s and so 230 doesn&#x27;t apply and you can be liable for it.&quot; That rationale extends to basically any form of moderation or selection, personalized or not, and would blow a big hole in 230&#x27;s protections.<p>Given generalized anti-Big-Tech sentiment on both ends of the political spectrum, I could see something that claimed to carve out just algorithmic personalization&#x2F;suggestion from protection meeting with success, either out of the courts or Congress, but it really doesn&#x27;t match the current law.</div><br/><div id="41398310" class="c"><input type="checkbox" id="c-41398310" checked=""/><div class="controls bullet"><span class="by">lupusreal</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394786">parent</a><span>|</span><a href="#41395045">next</a><span>|</span><label class="collapse" for="c-41398310">[-]</label><label class="expand" for="c-41398310">[1 more]</label></div><br/><div class="children"><div class="content"><i>&quot;well the Moody case says that curation&#x2F;moderation&#x2F;suggestion&#x2F;whatever is First Amendment protected speech, therefore that&#x27;s your speech and not somebody else&#x27;s and so 230 doesn&#x27;t apply and you can be liable for it.&quot;</i><p>I see a lot of people saying this is a bad decision because it will have consequences they don&#x27;t like, but the logic of the decision seems pretty damn airtight as you describe it.  If the recommendation systems and moderation policies are the company&#x27;s speech, then the company can be liable when the company &quot;says&quot;, by way of their algorithmic &quot;speech&quot;, to children that they should engage in some reckless activity likely to cause their death.</div><br/></div></div></div></div><div id="41396359" class="c"><input type="checkbox" id="c-41396359" checked=""/><div class="controls bullet"><span class="by">phire</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393625">parent</a><span>|</span><a href="#41394786">prev</a><span>|</span><a href="#41393670">next</a><span>|</span><label class="collapse" for="c-41396359">[-]</label><label class="expand" for="c-41396359">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s worth noting that personalisation isn&#x27;t moderation, An app like TikTok needs both.<p>Personalisation simply matches users with the content the algorithm thinks they want to see. Moderation (which is typically also an algorithm) tries to remove harmful content from the platform altogether.<p>The ruling isn&#x27;t saying that Section 230 doesn&#x27;t apply because TikTok moderated. It&#x27;s saying Section 230 doesn&#x27;t apply because TikTok personalised, allegedly knew about the harmful content and allegedly didn&#x27;t take enough action to moderate this harmful content.</div><br/><div id="41397383" class="c"><input type="checkbox" id="c-41397383" checked=""/><div class="controls bullet"><span class="by">cdchn</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396359">parent</a><span>|</span><a href="#41393670">next</a><span>|</span><label class="collapse" for="c-41397383">[-]</label><label class="expand" for="c-41397383">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Personalisation simply matches users with the content the algorithm thinks they want to see.<p>These algorithms aren&#x27;t matching you with what you want to see, they&#x27;re trying to maximize your engagement- or, its what the operator wants you to see, so you&#x27;ll use the site more and generate more data or revenue.  Its a fine, but extremely important distinction.</div><br/><div id="41397970" class="c"><input type="checkbox" id="c-41397970" checked=""/><div class="controls bullet"><span class="by">bryanrasmussen</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41397383">parent</a><span>|</span><a href="#41393670">next</a><span>|</span><label class="collapse" for="c-41397970">[-]</label><label class="expand" for="c-41397970">[1 more]</label></div><br/><div class="children"><div class="content">What the operator wants you to see also gets into the area of manipulation, hence 230 shouldn&#x27;t apply - by making algorithms based on manipulation or paid for boosting companies move from impartial unknowing deliverers of harmful content into committed distributors of it.</div><br/></div></div></div></div></div></div><div id="41393670" class="c"><input type="checkbox" id="c-41393670" checked=""/><div class="controls bullet"><span class="by">lcnPylGDnU4H9OF</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393625">parent</a><span>|</span><a href="#41396359">prev</a><span>|</span><a href="#41394352">next</a><span>|</span><label class="collapse" for="c-41393670">[-]</label><label class="expand" for="c-41393670">[13 more]</label></div><br/><div class="children"><div class="content">From the article:<p>&gt; TikTok, Inc., via its algorithm, recommended and promoted videos posted by third parties to ten-year-old Nylah Anderson on her uniquely curated “For You Page.”</div><br/><div id="41394078" class="c"><input type="checkbox" id="c-41394078" checked=""/><div class="controls bullet"><span class="by">unyttigfjelltol</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393670">parent</a><span>|</span><a href="#41394352">next</a><span>|</span><label class="collapse" for="c-41394078">[-]</label><label class="expand" for="c-41394078">[12 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the difference between the case and a monolithic electronic bulletin board like HN. HN follows an old-school BB model very close to the models that existed when Section 230 was written.<p>Winding up in the same place as the defendant would require making a unique, dynamic, individualized BB for each user tailored to them based on pervasive online surveillance and the platform&#x27;s own editorial &quot;secret sauce.&quot;</div><br/><div id="41394906" class="c"><input type="checkbox" id="c-41394906" checked=""/><div class="controls bullet"><span class="by">tsimionescu</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394078">parent</a><span>|</span><a href="#41396055">next</a><span>|</span><label class="collapse" for="c-41394906">[-]</label><label class="expand" for="c-41394906">[3 more]</label></div><br/><div class="children"><div class="content">The HN team explicitly and manually manages the front page of HN, so I think it&#x27;s completely unarguable that they would be held liable under this ruling if at least the front page contained links to articles that caused harm. They manually promote certain posts that they find particularly good, even if they didn&#x27;t get a lot of votes, so this is even more direct than what TikTok did in this case.</div><br/><div id="41395124" class="c"><input type="checkbox" id="c-41395124" checked=""/><div class="controls bullet"><span class="by">philistine</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394906">parent</a><span>|</span><a href="#41396055">next</a><span>|</span><label class="collapse" for="c-41395124">[-]</label><label class="expand" for="c-41395124">[2 more]</label></div><br/><div class="children"><div class="content">The decision specifically mentions algorithmic recommandation as being speech, ergo the recommandation itself is the responsibility of the platform.<p>Where is the algorithmic recommandation that differs per user on HN?</div><br/><div id="41396916" class="c"><input type="checkbox" id="c-41396916" checked=""/><div class="controls bullet"><span class="by">amitport</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395124">parent</a><span>|</span><a href="#41396055">next</a><span>|</span><label class="collapse" for="c-41396916">[-]</label><label class="expand" for="c-41396916">[1 more]</label></div><br/><div class="children"><div class="content">where does it say that it matters if it differs per user?</div><br/></div></div></div></div></div></div><div id="41396055" class="c"><input type="checkbox" id="c-41396055" checked=""/><div class="controls bullet"><span class="by">skeptrune</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394078">parent</a><span>|</span><a href="#41394906">prev</a><span>|</span><a href="#41394639">next</a><span>|</span><label class="collapse" for="c-41396055">[-]</label><label class="expand" for="c-41396055">[4 more]</label></div><br/><div class="children"><div class="content">Key words are &quot;editorial&quot; and &quot;secret sauce&quot;. Platforms should not be liable for dangerous content which slips through the cracks, but certainly should be when their user-personalized algorithms mess up. Can&#x27;t have your cake and eat it to.</div><br/><div id="41396263" class="c"><input type="checkbox" id="c-41396263" checked=""/><div class="controls bullet"><span class="by">krapp</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396055">parent</a><span>|</span><a href="#41394639">next</a><span>|</span><label class="collapse" for="c-41396263">[-]</label><label class="expand" for="c-41396263">[3 more]</label></div><br/><div class="children"><div class="content">Dangerous content slipping through the cracks and the algorithms messing up is the <i>same thing</i>. There is no way for content to &quot;slip through the cracks&quot; other than via the algorithm.</div><br/><div id="41396532" class="c"><input type="checkbox" id="c-41396532" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396263">parent</a><span>|</span><a href="#41394639">next</a><span>|</span><label class="collapse" for="c-41396532">[-]</label><label class="expand" for="c-41396532">[2 more]</label></div><br/><div class="children"><div class="content">You can view the content via direct links or search, recommendation algorithms isn&#x27;t the only way to view it.<p>If you child porn that gets shared via direct links then that is bad even if nobody can see it, but it is much much worse if you start recommending that to people as well.</div><br/><div id="41396942" class="c"><input type="checkbox" id="c-41396942" checked=""/><div class="controls bullet"><span class="by">krapp</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396532">parent</a><span>|</span><a href="#41394639">next</a><span>|</span><label class="collapse" for="c-41396942">[-]</label><label class="expand" for="c-41396942">[1 more]</label></div><br/><div class="children"><div class="content">Everything is related. Search results are usually generated based on recommendations, and direct links usually influence recommendations, or include recommendations as related content.<p>It&#x27;s rarely if ever going to be the case that there is some distinct unit of code called &quot;the algorithm&quot; that can be separated and considered legally distinct from the rest of the codebase.</div><br/></div></div></div></div></div></div></div></div><div id="41394639" class="c"><input type="checkbox" id="c-41394639" checked=""/><div class="controls bullet"><span class="by">empressplay</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394078">parent</a><span>|</span><a href="#41396055">prev</a><span>|</span><a href="#41394352">next</a><span>|</span><label class="collapse" for="c-41394639">[-]</label><label class="expand" for="c-41394639">[4 more]</label></div><br/><div class="children"><div class="content">HN is _not_ a monolithic bulletin board -- the messages on a BBS were never (AFAIK) sorted by &#x27;popularity&#x27; and users didn&#x27;t generally have the power to demote or flag posts.<p>Although HN&#x27;s algorithm depends (mostly) on user input for how it presents the posts, it still favours some over others and still runs afoul here. You would need a literal &#x27;most recent&#x27; chronological view and HN doesn&#x27;t have that for comments. It probably should anyway!<p>@dang We need the option to view comments chronologically, please</div><br/><div id="41395007" class="c"><input type="checkbox" id="c-41395007" checked=""/><div class="controls bullet"><span class="by">philipkglass</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394639">parent</a><span>|</span><a href="#41396927">next</a><span>|</span><label class="collapse" for="c-41395007">[-]</label><label class="expand" for="c-41395007">[1 more]</label></div><br/><div class="children"><div class="content">Writing @dang is a no-op. He&#x27;ll respond if he sees the mention, but there&#x27;s no alert sent to him. Email hn@ycombinator.com if you want to get his attention.<p>That said, the feature you requested is <i>already</i> implemented but you have to know it is there. Dang mentioned it in a recent comment that I bookmarked: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41230703">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41230703</a><p>To see comments on this story sorted newest-first, change the link to<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;latest?id=41391868">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;latest?id=41391868</a><p>instead of<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41391868">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41391868</a></div><br/></div></div><div id="41396927" class="c"><input type="checkbox" id="c-41396927" checked=""/><div class="controls bullet"><span class="by">Majromax</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394639">parent</a><span>|</span><a href="#41395007">prev</a><span>|</span><a href="#41398083">next</a><span>|</span><label class="collapse" for="c-41396927">[-]</label><label class="expand" for="c-41396927">[1 more]</label></div><br/><div class="children"><div class="content">&gt; HN is _not_ a monolithic bulletin board -- the messages on a BBS were never (AFAIK) sorted by &#x27;popularity&#x27; and users didn&#x27;t generally have the power to demote or flag posts.<p>I don&#x27;t think the feature was that unknown.  Per Wikipedia, the CDA passed in 1996 and Slashdot was created in 1997, and I doubt the latter&#x27;s moderation&#x2F;voting system was <i>that</i> unique.</div><br/></div></div><div id="41398083" class="c"><input type="checkbox" id="c-41398083" checked=""/><div class="controls bullet"><span class="by">kardos</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394639">parent</a><span>|</span><a href="#41396927">prev</a><span>|</span><a href="#41394352">next</a><span>|</span><label class="collapse" for="c-41398083">[-]</label><label class="expand" for="c-41398083">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We need the option to view comments chronologically<p>You might like this then: <a href="https:&#x2F;&#x2F;hckrnews.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;hckrnews.com&#x2F;</a></div><br/></div></div></div></div></div></div></div></div><div id="41394352" class="c"><input type="checkbox" id="c-41394352" checked=""/><div class="controls bullet"><span class="by">klik99</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393625">parent</a><span>|</span><a href="#41393670">prev</a><span>|</span><a href="#41393822">next</a><span>|</span><label class="collapse" for="c-41394352">[-]</label><label class="expand" for="c-41394352">[5 more]</label></div><br/><div class="children"><div class="content">Specifically NetChoice argued that personalized feeds based on user data were protected due to first person speech. This went to supreme court and supreme court agreed. Now precedent is set by the highest court that those feeds are &quot;expressive product&quot;. It doesn&#x27;t make sense, but that&#x27;s how the law works - by trying to define as best as possible the things in gray areas.<p>And they probably didn&#x27;t think through how this particular argument could affect other areas of their business.</div><br/><div id="41396342" class="c"><input type="checkbox" id="c-41396342" checked=""/><div class="controls bullet"><span class="by">remich</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394352">parent</a><span>|</span><a href="#41393822">next</a><span>|</span><label class="collapse" for="c-41396342">[-]</label><label class="expand" for="c-41396342">[4 more]</label></div><br/><div class="children"><div class="content">It absolutely makes sense. What NetChoice held was that the curation aspect of algorithmic feeds makes the weighting approach equivalent to the speech of the platforms and therefore when courts evaluated challenges to <i>government</i> imposed regulation, they had to perform standard First Amendment analysis to determine if the contested regulation passed muster.<p>Importantly, this does not mean that before the Third Circuit decision platforms could just curate any which way they want and government couldn&#x27;t regulate at all -- the mandatory removal regime around CSAM content is a great example of government regulating speech and forcing platforms to comply.<p>The Third Circuit decision, in a nutshell, is telling the platforms that they can&#x27;t have their cake and eat it too. If they want to claim that their algorithmic feeds are speech that is protected from most government regulation, they can&#x27;t simultaneously claim that these same algorithmic feeds are mere passive vessels for the speech of third parties. If that were the case, then their algorithms would enjoy no 1A protection from government regulation. (The content itself would still have 1A protection based on the rights of the creators, but the curation&#x2F;ranking&#x2F;privileging aspect would not).</div><br/><div id="41396423" class="c"><input type="checkbox" id="c-41396423" checked=""/><div class="controls bullet"><span class="by">phire</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396342">parent</a><span>|</span><a href="#41393822">next</a><span>|</span><label class="collapse" for="c-41396423">[-]</label><label class="expand" for="c-41396423">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, I agree.<p>This ruling is a natural consequence of the NetChoice ruling. Social media companies can&#x27;t have it both ways.<p><i>&gt; If that were the case, then their algorithms would enjoy no 1A protection from government regulation.</i><p>Well, the companies can still probably claim some 1st Amendment protections for their recommendation algorithms (for example, a law banning algorithmic political bias would be unconstitutional). All this ruling does is strip away the safe harbour protections, which weren&#x27;t derived from the 1A in the first place.</div><br/><div id="41397372" class="c"><input type="checkbox" id="c-41397372" checked=""/><div class="controls bullet"><span class="by">codersfocus</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396423">parent</a><span>|</span><a href="#41393822">next</a><span>|</span><label class="collapse" for="c-41397372">[-]</label><label class="expand" for="c-41397372">[2 more]</label></div><br/><div class="children"><div class="content">&gt; law banning algorithmic political bias would be unconstitutional<p>Would it? The TV channels of old were heavily regulated well past 1st amendment limits.</div><br/><div id="41397486" class="c"><input type="checkbox" id="c-41397486" checked=""/><div class="controls bullet"><span class="by">kloop</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41397372">parent</a><span>|</span><a href="#41393822">next</a><span>|</span><label class="collapse" for="c-41397486">[-]</label><label class="expand" for="c-41397486">[1 more]</label></div><br/><div class="children"><div class="content">Only because they were using public airwaves.<p>Cable was never regulated like that. The medium actually mattered in this case</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41393822" class="c"><input type="checkbox" id="c-41393822" checked=""/><div class="controls bullet"><span class="by">wk_end</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393625">parent</a><span>|</span><a href="#41394352">prev</a><span>|</span><a href="#41395262">next</a><span>|</span><label class="collapse" for="c-41393822">[-]</label><label class="expand" for="c-41393822">[8 more]</label></div><br/><div class="children"><div class="content">It’d be interesting to know what constitutes an “algorithm”. Does a message board sorting by “most recent” count as one?</div><br/><div id="41394449" class="c"><input type="checkbox" id="c-41394449" checked=""/><div class="controls bullet"><span class="by">saratogacx</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393822">parent</a><span>|</span><a href="#41395262">next</a><span>|</span><label class="collapse" for="c-41394449">[-]</label><label class="expand" for="c-41394449">[7 more]</label></div><br/><div class="children"><div class="content">&gt; algorithm that reflects “editorial judgments”<p>I don&#x27;t think timestamps are, in any way, construed editorial judgement.  They are a content agnostic related attribute.</div><br/><div id="41396873" class="c"><input type="checkbox" id="c-41396873" checked=""/><div class="controls bullet"><span class="by">Izkata</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394449">parent</a><span>|</span><a href="#41394668">next</a><span>|</span><label class="collapse" for="c-41396873">[-]</label><label class="expand" for="c-41396873">[1 more]</label></div><br/><div class="children"><div class="content">On HN, timestamps are adjusted when posts are given a second-chance boost.  While the boost is done automatically, candidates are chosen manually.</div><br/></div></div><div id="41394965" class="c"><input type="checkbox" id="c-41394965" checked=""/><div class="controls bullet"><span class="by">srj</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394449">parent</a><span>|</span><a href="#41394668">prev</a><span>|</span><a href="#41395262">next</a><span>|</span><label class="collapse" for="c-41394965">[-]</label><label class="expand" for="c-41394965">[4 more]</label></div><br/><div class="children"><div class="content">What about filtering spam? Or showing the local weather &#x2F; news headlines?</div><br/><div id="41396368" class="c"><input type="checkbox" id="c-41396368" checked=""/><div class="controls bullet"><span class="by">remich</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394965">parent</a><span>|</span><a href="#41395997">next</a><span>|</span><label class="collapse" for="c-41396368">[-]</label><label class="expand" for="c-41396368">[1 more]</label></div><br/><div class="children"><div class="content">Moderating content is explicitly protected by the text of Section 230(c)(2)(a):<p>&quot;(2)Civil liability
No provider or user of an interactive computer service shall be held liable on account of—
(A)any action voluntarily taken in good faith to restrict access to or availability of material that the provider or user considers to be obscene, lewd, lascivious, filthy, excessively violent, harassing, or otherwise objectionable, whether or not such material is constitutionally protected; or&quot;<p>Algorithmic ranking, curation, and promotion are not.</div><br/></div></div><div id="41395997" class="c"><input type="checkbox" id="c-41395997" checked=""/><div class="controls bullet"><span class="by">bitshiftfaced</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394965">parent</a><span>|</span><a href="#41396368">prev</a><span>|</span><a href="#41395262">next</a><span>|</span><label class="collapse" for="c-41395997">[-]</label><label class="expand" for="c-41395997">[2 more]</label></div><br/><div class="children"><div class="content">Or ordering posts by up votes&#x2F;down votes, or some combination of that with the age of the post.</div><br/><div id="41396392" class="c"><input type="checkbox" id="c-41396392" checked=""/><div class="controls bullet"><span class="by">remich</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395997">parent</a><span>|</span><a href="#41395262">next</a><span>|</span><label class="collapse" for="c-41396392">[-]</label><label class="expand" for="c-41396392">[1 more]</label></div><br/><div class="children"><div class="content">The text of the Third Circuit decision explicitly distinguishes between algorithms that respond to user input -- such as by surfacing content that was previously searched for, or favorited, or followed. Allowing users to filter content by time, upvotes, number of replies etc would be fine.<p>The FYP algorithm that&#x27;s contested in the case surfaced the video to the minor without her searching for that topic, following any specific content creator, or positively interacting (liking&#x2F;favoriting&#x2F;upvoting) with previous instances of said content. It was fed to her based on a combination of what TikTok knew about her demographic information, what was trending on the platform, and TikTok&#x27;s editorial secret sauce. TikTok&#x27;s algorithm made an active decision to surface this content to her, despite knowing that other children had died from similar challenge videos, they promoted it and should be liable for that promotion.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41395262" class="c"><input type="checkbox" id="c-41395262" checked=""/><div class="controls bullet"><span class="by">pessimizer</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393625">parent</a><span>|</span><a href="#41393822">prev</a><span>|</span><a href="#41394181">next</a><span>|</span><label class="collapse" for="c-41395262">[-]</label><label class="expand" for="c-41395262">[2 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t seem to have anything to do with personalization to me, either. It&#x27;s about &quot;editorial judgement,&quot; and an algorithm isn&#x27;t necessarily a get out of jail free card unless the algorithm is completely transparent and user-adjustable.<p>I even think it would count if the only moderation you did on your Lionel model train site was to make sure that most of the conversation was about Lionel model trains, and that they be treated in a positive (or at least neutral) manner. That degree of moderation, for that purpose, would make you liable if you left illegal or tortious content up i.e. if you moderate, you&#x27;re a moderator, and your first duty is legal.<p>If you&#x27;re just a dumb pipe, however, you&#x27;re a dumb pipe and get section 230.<p>I wonder how this works with recommendation algorithms, though, seeing as they&#x27;re also trade secrets. Even when they&#x27;re not dark and predatory (advertising related.) If one has a recommendation algo that makes better e.g. song recommendations, you don&#x27;t want to have to share it. Would it be something you&#x27;d have to privately reveal to a government agency (like having to reveal the composition of your fracking fluid to the EPA, as an example), and they would judge whether or not it was &quot;editorial&quot; or not?<p>[edit: that being said, it would probably be very hard to break the law with a song recommendation algorithm. But I&#x27;m sure you could run afoul of some financial law still on the books about payola, etc.]</div><br/><div id="41396938" class="c"><input type="checkbox" id="c-41396938" checked=""/><div class="controls bullet"><span class="by">Majromax</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395262">parent</a><span>|</span><a href="#41394181">next</a><span>|</span><label class="collapse" for="c-41396938">[-]</label><label class="expand" for="c-41396938">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That degree of moderation, for that purpose, would make you liable if you left illegal or tortious content up i.e. if you moderate, you&#x27;re a moderator, and your first duty is legal.<p>I&#x27;m not sure that&#x27;s quite it.  As I read the article and think about its application to Tiktok, the problem was more that &quot;the algorithm&quot; was engaged in active and allegedly expressive promotion of the unsafe material.  If a site like HN just doesn&#x27;t remove bad content, then the residual promotion is not exactly Hacker News&#x27;s expression, but rather its users&#x27;.<p>The situation might change if a liability-causing article were itself given &#x27;second chance&#x27; promotion or another editorial thumb on the scale, but I certainly hope that such editorial management is done with enough care to practically avoid that case.</div><br/></div></div></div></div></div></div><div id="41394181" class="c"><input type="checkbox" id="c-41394181" checked=""/><div class="controls bullet"><span class="by">Manuel_D</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393527">parent</a><span>|</span><a href="#41393625">prev</a><span>|</span><a href="#41398096">next</a><span>|</span><label class="collapse" for="c-41394181">[-]</label><label class="expand" for="c-41394181">[9 more]</label></div><br/><div class="children"><div class="content">But something like Reddit would be held liable for showing posts, then. Because you get shown different results depending on the subreddits you subscribe to, your browsing patterns, what you&#x27;ve upvoted in the past, and more. Pretty much any recommendation engine is a no-go of this ruling becomes precedence.</div><br/><div id="41394735" class="c"><input type="checkbox" id="c-41394735" checked=""/><div class="controls bullet"><span class="by">lesuorac</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394181">parent</a><span>|</span><a href="#41394511">next</a><span>|</span><label class="collapse" for="c-41394735">[-]</label><label class="expand" for="c-41394735">[4 more]</label></div><br/><div class="children"><div class="content">TBH, Reddit really shouldn&#x27;t have 230 protection anyways.<p>You can&#x27;t be licensing user content to AI as it&#x27;s not yours. You also can&#x27;t be undeleting posts people make (otherwise it&#x27;s really reddit&#x27;s posts and not theirs).<p>When you start treating user data as your own; it should become your own and that erodes 230.</div><br/><div id="41395616" class="c"><input type="checkbox" id="c-41395616" checked=""/><div class="controls bullet"><span class="by">autoexec</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394735">parent</a><span>|</span><a href="#41395751">next</a><span>|</span><label class="collapse" for="c-41395616">[-]</label><label class="expand" for="c-41395616">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You also can&#x27;t be undeleting posts people make<p>undeleting is bad enough, but they&#x27;ve edited the content of user&#x27;s comments too.</div><br/></div></div><div id="41395751" class="c"><input type="checkbox" id="c-41395751" checked=""/><div class="controls bullet"><span class="by">Manuel_D</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394735">parent</a><span>|</span><a href="#41395616">prev</a><span>|</span><a href="#41395684">next</a><span>|</span><label class="collapse" for="c-41395751">[-]</label><label class="expand" for="c-41395751">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You can&#x27;t be licensing user content to AI as it&#x27;s not yours.<p>It is theirs. Users agreed to grant Reddit a license to use the content when they accepted the terms of service.</div><br/></div></div><div id="41395684" class="c"><input type="checkbox" id="c-41395684" checked=""/><div class="controls bullet"><span class="by">raydev</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394735">parent</a><span>|</span><a href="#41395751">prev</a><span>|</span><a href="#41394511">next</a><span>|</span><label class="collapse" for="c-41395684">[-]</label><label class="expand" for="c-41395684">[1 more]</label></div><br/><div class="children"><div class="content">It belongs to reddit, the user handed over the content willingly.</div><br/></div></div></div></div><div id="41394511" class="c"><input type="checkbox" id="c-41394511" checked=""/><div class="controls bullet"><span class="by">TheGlav</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394181">parent</a><span>|</span><a href="#41394735">prev</a><span>|</span><a href="#41395975">next</a><span>|</span><label class="collapse" for="c-41394511">[-]</label><label class="expand" for="c-41394511">[3 more]</label></div><br/><div class="children"><div class="content">From my reading, if the site only shows you based on your selections, then it wouldn&#x27;t be liable. For example, if someone else with the exact same selections gets the same results, then that&#x27;s not their platform deciding what to show.<p>If it does any customization based on what it knows about you, or what it tries to sell you because you are you, then it would be liable.<p>Yep., recommendation engines would have to be very carefully tuned, or you risk becoming liable. Recommending only curated content would be a way to protect yourself, but that costs money that companies don&#x27;t have to pay today. It would be doable.</div><br/><div id="41394890" class="c"><input type="checkbox" id="c-41394890" checked=""/><div class="controls bullet"><span class="by">Manuel_D</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394511">parent</a><span>|</span><a href="#41394831">next</a><span>|</span><label class="collapse" for="c-41394890">[-]</label><label class="expand" for="c-41394890">[1 more]</label></div><br/><div class="children"><div class="content">&gt; For example, if someone else with the exact same selections gets the same results, then that&#x27;s not their platform deciding what to show.<p>This could very well be true for TikTok. Of course &quot;selection&quot; would include liked videos, how long you spend watching each video, and how many videos you have posted<p>And on the flip side a button that brings you to a random video would supply different content to users regardless of &quot;selections&quot;.</div><br/></div></div><div id="41394831" class="c"><input type="checkbox" id="c-41394831" checked=""/><div class="controls bullet"><span class="by">djhn</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394511">parent</a><span>|</span><a href="#41394890">prev</a><span>|</span><a href="#41395975">next</a><span>|</span><label class="collapse" for="c-41394831">[-]</label><label class="expand" for="c-41394831">[1 more]</label></div><br/><div class="children"><div class="content">It could be difficult to draw the line. I assume TikTok’s suggestions are deterministic enough that an identical user would see the same things - it’s just incredibly unlikely to be identical at the level of granularity that TikTok is able to measure due to the type of content and types of interactions the platform has.</div><br/></div></div></div></div><div id="41395975" class="c"><input type="checkbox" id="c-41395975" checked=""/><div class="controls bullet"><span class="by">juliangmp</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394181">parent</a><span>|</span><a href="#41394511">prev</a><span>|</span><a href="#41398096">next</a><span>|</span><label class="collapse" for="c-41395975">[-]</label><label class="expand" for="c-41395975">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Pretty much any recommendation engine is a no-go of this ruling becomes precedence.<p>That kind of sounds... great?
The only instance where I genuinely like to have a recommendation engine around is music steaming. Like yeah sometimes it does recommend great stuff.
But anywhere else? No thank you</div><br/></div></div></div></div><div id="41398096" class="c"><input type="checkbox" id="c-41398096" checked=""/><div class="controls bullet"><span class="by">1vuio0pswjnm7</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393527">parent</a><span>|</span><a href="#41394181">prev</a><span>|</span><a href="#41398177">next</a><span>|</span><label class="collapse" for="c-41398096">[-]</label><label class="expand" for="c-41398096">[1 more]</label></div><br/><div class="children"><div class="content">&quot;I think this is a mistaken understanding of the ruling.&quot;<p>I think that is quite generous.  I think it is a deliberate reinterpretation of what the order says.  The order states that 230(c)(1) provides immunity for removing harmful content after being made aware of it, i.e., moderation.</div><br/></div></div><div id="41398177" class="c"><input type="checkbox" id="c-41398177" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393527">parent</a><span>|</span><a href="#41398096">prev</a><span>|</span><a href="#41393640">next</a><span>|</span><label class="collapse" for="c-41398177">[-]</label><label class="expand" for="c-41398177">[1 more]</label></div><br/><div class="children"><div class="content">&gt; On HN, your front page is not different from my front page.<p>It’s still curated, and not entirely automatically.  Does it make a difference whether it’s curated individually or not?</div><br/></div></div><div id="41393640" class="c"><input type="checkbox" id="c-41393640" checked=""/><div class="controls bullet"><span class="by">lesuorac</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393527">parent</a><span>|</span><a href="#41398177">prev</a><span>|</span><a href="#41396516">next</a><span>|</span><label class="collapse" for="c-41393640">[-]</label><label class="expand" for="c-41393640">[15 more]</label></div><br/><div class="children"><div class="content">Per the court of appeals, TikTok is not in trouble for showing a blackout challenge video. TikTok is in trouble for not censoring them after knowing they were causing harm.<p>&gt; &quot;What does all this mean for Anderson’s claims?
Well, § 230(c)(1)’s preemption of traditional publisher liability
precludes Anderson from holding TikTok liable for the
Blackout Challenge videos’ mere presence on TikTok’s
platform. A conclusion Anderson’s counsel all but concedes.
But § 230(c)(1) does not preempt distributor liability, so
Anderson’s claims seeking to hold TikTok liable for
continuing to host the Blackout Challenge videos knowing
they were causing the death of children can proceed.&quot;<p>As-in, Dang would be liable if say somebody started a blackout challenge post on HN and he didn&#x27;t start censoring all of them once news reports of programmers dieing broke out.<p><a href="https:&#x2F;&#x2F;fingfx.thomsonreuters.com&#x2F;gfx&#x2F;legaldocs&#x2F;mopaqabzypa&#x2F;08272024tiktok.pdf" rel="nofollow">https:&#x2F;&#x2F;fingfx.thomsonreuters.com&#x2F;gfx&#x2F;legaldocs&#x2F;mopaqabzypa&#x2F;...</a></div><br/><div id="41394264" class="c"><input type="checkbox" id="c-41394264" checked=""/><div class="controls bullet"><span class="by">whartung</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393640">parent</a><span>|</span><a href="#41394089">next</a><span>|</span><label class="collapse" for="c-41394264">[-]</label><label class="expand" for="c-41394264">[7 more]</label></div><br/><div class="children"><div class="content">Does TikTok have to know that “as a category blackout videos are bad” or that “this specific video is bad”.<p>Does TikTok have preempt this category of videos in the future or simply respond promptly when notified such a video is posted to their system?</div><br/><div id="41394904" class="c"><input type="checkbox" id="c-41394904" checked=""/><div class="controls bullet"><span class="by">jay_kyburz</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394264">parent</a><span>|</span><a href="#41394089">next</a><span>|</span><label class="collapse" for="c-41394904">[-]</label><label class="expand" for="c-41394904">[6 more]</label></div><br/><div class="children"><div class="content">Are you asking about the law, or are you asking our opinion?<p>Do you think its reasonable for social media to send videos to people without considering how harmful they are?<p>Do you even think its reasonable for search engine to respond to a specific request for this information?</div><br/><div id="41395816" class="c"><input type="checkbox" id="c-41395816" checked=""/><div class="controls bullet"><span class="by">autoexec</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394904">parent</a><span>|</span><a href="#41395364">next</a><span>|</span><label class="collapse" for="c-41395816">[-]</label><label class="expand" for="c-41395816">[2 more]</label></div><br/><div class="children"><div class="content">Personally, I wouldn&#x27;t want search engines censoring results for things explicitly searched for, but I&#x27;d still expect that social media should be responsible for harmful content they push onto users who never asked for it in the first place.  Push vs Pull is an important distinction that should be considered.</div><br/><div id="41396682" class="c"><input type="checkbox" id="c-41396682" checked=""/><div class="controls bullet"><span class="by">MadnessASAP</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395816">parent</a><span>|</span><a href="#41395364">next</a><span>|</span><label class="collapse" for="c-41396682">[-]</label><label class="expand" for="c-41396682">[1 more]</label></div><br/><div class="children"><div class="content">That IS the distinction at play here.</div><br/></div></div></div></div><div id="41395364" class="c"><input type="checkbox" id="c-41395364" checked=""/><div class="controls bullet"><span class="by">oceanplexian</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394904">parent</a><span>|</span><a href="#41395816">prev</a><span>|</span><a href="#41394089">next</a><span>|</span><label class="collapse" for="c-41395364">[-]</label><label class="expand" for="c-41395364">[3 more]</label></div><br/><div class="children"><div class="content">Did some hands come out of the screen, pull a rope out then choke someone? Platforms shouldn’t be held responsible when 1 out of a million users wins a Darwin award.</div><br/><div id="41395842" class="c"><input type="checkbox" id="c-41395842" checked=""/><div class="controls bullet"><span class="by">autoexec</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395364">parent</a><span>|</span><a href="#41395452">next</a><span>|</span><label class="collapse" for="c-41395842">[-]</label><label class="expand" for="c-41395842">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s a very different conversation when you&#x27;re talking about social media sites pushing content they know is harmful onto people who they know are literal children.</div><br/></div></div></div></div></div></div></div></div><div id="41394089" class="c"><input type="checkbox" id="c-41394089" checked=""/><div class="controls bullet"><span class="by">mattigames</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393640">parent</a><span>|</span><a href="#41394264">prev</a><span>|</span><a href="#41393790">next</a><span>|</span><label class="collapse" for="c-41394089">[-]</label><label class="expand" for="c-41394089">[1 more]</label></div><br/><div class="children"><div class="content">The ingenuity of kids to believe and be easily influenced by what they see online had a big role in this ruling, disregarding that is a huge disservice to a productive discussion.</div><br/></div></div><div id="41393790" class="c"><input type="checkbox" id="c-41393790" checked=""/><div class="controls bullet"><span class="by">wahnfrieden</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393640">parent</a><span>|</span><a href="#41394089">prev</a><span>|</span><a href="#41394003">next</a><span>|</span><label class="collapse" for="c-41393790">[-]</label><label class="expand" for="c-41393790">[3 more]</label></div><br/><div class="children"><div class="content">What constitutes &quot;censoring all of them&quot;</div><br/><div id="41393964" class="c"><input type="checkbox" id="c-41393964" checked=""/><div class="controls bullet"><span class="by">altairprime</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393790">parent</a><span>|</span><a href="#41394019">next</a><span>|</span><label class="collapse" for="c-41393964">[-]</label><label class="expand" for="c-41393964">[1 more]</label></div><br/><div class="children"><div class="content">Trying to define &quot;all&quot; is an impossibility; but, by virtue of having taken no action whatsoever, answering that question is irrelevant in the context of this particular judgment: Tiktok took no action, so the definition of &quot;all&quot; is irrelevant. See also for example: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41393921">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41393921</a><p>In general, judges will be ultimately responsible for evaluating whether &quot;any&quot;, &quot;sufficient&quot;, &quot;appropriate&quot;, etc. actions were taken in each future case judgement they make. As with all things legalese, it&#x27;s impossible to define with certainty a specific <i>degree</i> of action that is the uniform boundary of acceptable; but, as evident here, &quot;none&quot; is no longer permissible in that set.<p>(I am not your lawyer, this is not legal advice.)</div><br/></div></div><div id="41394019" class="c"><input type="checkbox" id="c-41394019" checked=""/><div class="controls bullet"><span class="by">mattigames</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393790">parent</a><span>|</span><a href="#41393964">prev</a><span>|</span><a href="#41394003">next</a><span>|</span><label class="collapse" for="c-41394019">[-]</label><label class="expand" for="c-41394019">[1 more]</label></div><br/><div class="children"><div class="content">Any good will attempt at censoring would have been as a reasonable defense even if technically they don&#x27;t censor 100% of them, such as blocking videos with the word &quot;blackout&quot; on their title or manually approving videos with such thing, but they did nothing instead.</div><br/></div></div></div></div><div id="41394003" class="c"><input type="checkbox" id="c-41394003" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393640">parent</a><span>|</span><a href="#41393790">prev</a><span>|</span><a href="#41396516">next</a><span>|</span><label class="collapse" for="c-41394003">[-]</label><label class="expand" for="c-41394003">[3 more]</label></div><br/><div class="children"><div class="content">&gt; TikTok is in trouble for not censoring them after knowing they were causing harm.<p>This has interesting higher-order effects on free speech. Let&#x27;s apply the same ruling to vaccine misinformation, or the ability to organize protests on social media (which opponents will probably call riots if there are any injuries)</div><br/><div id="41394646" class="c"><input type="checkbox" id="c-41394646" checked=""/><div class="controls bullet"><span class="by">lesuorac</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394003">parent</a><span>|</span><a href="#41396516">next</a><span>|</span><label class="collapse" for="c-41394646">[-]</label><label class="expand" for="c-41394646">[2 more]</label></div><br/><div class="children"><div class="content">Uh yeah, the court of appeals has reached an interesting decision.<p>But I mean what do you expect from a group of judges that themselves have written they&#x27;re moving away from precedent?</div><br/><div id="41394823" class="c"><input type="checkbox" id="c-41394823" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394646">parent</a><span>|</span><a href="#41396516">next</a><span>|</span><label class="collapse" for="c-41394823">[-]</label><label class="expand" for="c-41394823">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t doubt the same court relishes the thought of deciding what &quot;harm&quot; is on a case-by-case basis. The continued politicization of the courts will not end well for a society that nominally believes in the rule of law. Some quarters have been agitating for removing §230 safe harbor protections (or repealing it entirely), and the courts have delivered.</div><br/></div></div></div></div></div></div></div></div><div id="41396516" class="c"><input type="checkbox" id="c-41396516" checked=""/><div class="controls bullet"><span class="by">cbsmith</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393527">parent</a><span>|</span><a href="#41393640">prev</a><span>|</span><a href="#41398039">next</a><span>|</span><label class="collapse" for="c-41396516">[-]</label><label class="expand" for="c-41396516">[1 more]</label></div><br/><div class="children"><div class="content">The personalized aspect wasn&#x27;t emphasized at all in the ruling. It was the curation. I don&#x27;t think TikTok would have avoided liability by simply sharing the video with everyone.</div><br/></div></div></div></div><div id="41398039" class="c"><input type="checkbox" id="c-41398039" checked=""/><div class="controls bullet"><span class="by">pdpi</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393405">parent</a><span>|</span><a href="#41393527">prev</a><span>|</span><a href="#41394001">next</a><span>|</span><label class="collapse" for="c-41398039">[-]</label><label class="expand" for="c-41398039">[1 more]</label></div><br/><div class="children"><div class="content">Section 230 hasn&#x27;t changed or been revoked or anything, so, from what I understand, manual moderation is perfectly fine, as long as that is what it is: moderation. What the ruling says is that &quot;recommended&quot; content and personalised &quot;for you&quot; pages are themselves speech by the platform, rather than moderation, and are therefore not under the purview of Section 230.<p>For HN, Dang&#x27;s efforts at keeping civility don&#x27;t interfere with Section 230. The part relevant to this ruling is whatever system takes recency and upvotes, and ranks the front page posts and comments within each post.</div><br/></div></div><div id="41394001" class="c"><input type="checkbox" id="c-41394001" checked=""/><div class="controls bullet"><span class="by">tboyd47</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393405">parent</a><span>|</span><a href="#41398039">prev</a><span>|</span><a href="#41393615">next</a><span>|</span><label class="collapse" for="c-41394001">[-]</label><label class="expand" for="c-41394001">[1 more]</label></div><br/><div class="children"><div class="content">Under Judge Matey&#x27;s interpretation of Section 230, I don&#x27;t even think option 1 would remain on the table. He includes every act except mere &quot;hosting&quot; as part of publisher liability.</div><br/></div></div><div id="41393615" class="c"><input type="checkbox" id="c-41393615" checked=""/><div class="controls bullet"><span class="by">spamizbad</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393405">parent</a><span>|</span><a href="#41394001">prev</a><span>|</span><a href="#41393506">next</a><span>|</span><label class="collapse" for="c-41393615">[-]</label><label class="expand" for="c-41393615">[9 more]</label></div><br/><div class="children"><div class="content">I feel like the end result of path #1 is that your site just becomes overrun with spams and scams. See also: mail, telephones.</div><br/><div id="41396812" class="c"><input type="checkbox" id="c-41396812" checked=""/><div class="controls bullet"><span class="by">stale2002</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393615">parent</a><span>|</span><a href="#41393798">next</a><span>|</span><label class="collapse" for="c-41396812">[-]</label><label class="expand" for="c-41396812">[3 more]</label></div><br/><div class="children"><div class="content">No, that&#x27;s not the end the result.<p>It would be perfectly legal for a platform to choose to allow a user to decide on their own to filter out spam.<p>Maybe a user could sign up for such an algorithm, but if they choose to whitelist certain accounts, that would also be allowed.<p>Problem solved.</div><br/><div id="41396888" class="c"><input type="checkbox" id="c-41396888" checked=""/><div class="controls bullet"><span class="by">jojobas</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396812">parent</a><span>|</span><a href="#41393798">next</a><span>|</span><label class="collapse" for="c-41396888">[-]</label><label class="expand" for="c-41396888">[2 more]</label></div><br/><div class="children"><div class="content">Exactly. Moderation is not a problem as long as you can opt out of it, for both reading and writing.</div><br/><div id="41398213" class="c"><input type="checkbox" id="c-41398213" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396888">parent</a><span>|</span><a href="#41393798">next</a><span>|</span><label class="collapse" for="c-41398213">[-]</label><label class="expand" for="c-41398213">[1 more]</label></div><br/><div class="children"><div class="content">If I were to start posting defamatory material about you on various internet forums, how would you opt out of that?</div><br/></div></div></div></div></div></div><div id="41393798" class="c"><input type="checkbox" id="c-41393798" checked=""/><div class="controls bullet"><span class="by">aftbit</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393615">parent</a><span>|</span><a href="#41396812">prev</a><span>|</span><a href="#41393506">next</a><span>|</span><label class="collapse" for="c-41393798">[-]</label><label class="expand" for="c-41393798">[5 more]</label></div><br/><div class="children"><div class="content">Yeah, no moderation leads to spams, scams, rampant hate, and CSAM. I spent all of an hour on Voat when it was in its heyday and it mostly literal Nazis calling for the extermination of undesirables. The normies just stayed on moderated Reddit.</div><br/><div id="41394729" class="c"><input type="checkbox" id="c-41394729" checked=""/><div class="controls bullet"><span class="by">redeeman</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393798">parent</a><span>|</span><a href="#41393506">next</a><span>|</span><label class="collapse" for="c-41394729">[-]</label><label class="expand" for="c-41394729">[4 more]</label></div><br/><div class="children"><div class="content">voat wasnt exactly a single place, any more than reddit is</div><br/><div id="41394810" class="c"><input type="checkbox" id="c-41394810" checked=""/><div class="controls bullet"><span class="by">snapcaster</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394729">parent</a><span>|</span><a href="#41393506">next</a><span>|</span><label class="collapse" for="c-41394810">[-]</label><label class="expand" for="c-41394810">[3 more]</label></div><br/><div class="children"><div class="content">Were there non KKK&#x2F;nazi&#x2F;qanon whatever subvoats (or whatever they call them?) the one time i visited the site every single post on the frontpage was alt right nonsense</div><br/><div id="41395298" class="c"><input type="checkbox" id="c-41395298" checked=""/><div class="controls bullet"><span class="by">tzs</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394810">parent</a><span>|</span><a href="#41395745">next</a><span>|</span><label class="collapse" for="c-41395298">[-]</label><label class="expand" for="c-41395298">[1 more]</label></div><br/><div class="children"><div class="content">Yes. There were a ton of them for various categories of sex drawings, mostly in the style common in Japanese comics and cartoons.</div><br/></div></div><div id="41395745" class="c"><input type="checkbox" id="c-41395745" checked=""/><div class="controls bullet"><span class="by">autoexec</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394810">parent</a><span>|</span><a href="#41395298">prev</a><span>|</span><a href="#41393506">next</a><span>|</span><label class="collapse" for="c-41395745">[-]</label><label class="expand" for="c-41395745">[1 more]</label></div><br/><div class="children"><div class="content">It was the people who were chased out of other websites that drove much of their traffic so it&#x27;s no surprise that their content got the front page. It&#x27;s a shame that they scared so many other people away and downvoted other perspectives because it made diversity difficult.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41393506" class="c"><input type="checkbox" id="c-41393506" checked=""/><div class="controls bullet"><span class="by">supriyo-biswas</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393405">parent</a><span>|</span><a href="#41393615">prev</a><span>|</span><a href="#41394551">next</a><span>|</span><label class="collapse" for="c-41393506">[-]</label><label class="expand" for="c-41393506">[4 more]</label></div><br/><div class="children"><div class="content">Not sure about the downvotes on this comment; but what parent says has precedent in Cubby Inc. vs Compuserve Inc.[1] and this is one of the reasons Section 230 came about to be in the first place.<p>HN is also heavily moderated with moderators actively trying to promote thoughtful comments over other, less thoughtful or incendiary contributions by downranking them (which is entirely separate from flagging or voting; and unlike what people like to believe, this place relies more on moderator actions as opposed to voting patterns to maintain its vibe.) I couldn&#x27;t possibly see this working with the removal of Section 230.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cubby,_Inc._v._CompuServe_Inc" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cubby,_Inc._v._CompuServe_Inc</a>.</div><br/><div id="41393566" class="c"><input type="checkbox" id="c-41393566" checked=""/><div class="controls bullet"><span class="by">singleshot_</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393506">parent</a><span>|</span><a href="#41394551">next</a><span>|</span><label class="collapse" for="c-41393566">[-]</label><label class="expand" for="c-41393566">[3 more]</label></div><br/><div class="children"><div class="content">If I upvote something illegal, my liability was the same before, during, and after 230 exists, right?</div><br/><div id="41395951" class="c"><input type="checkbox" id="c-41395951" checked=""/><div class="controls bullet"><span class="by">hn_acker</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393566">parent</a><span>|</span><a href="#41397207">next</a><span>|</span><label class="collapse" for="c-41395951">[-]</label><label class="expand" for="c-41395951">[1 more]</label></div><br/><div class="children"><div class="content">Theoretically, your liability is the same because the First Amendment is what absolves you of liability for someone else&#x27;s speech. Section 230 provides an avenue for early dismissal in such a case if you get sued; without Section 230, you&#x27;ll risk having to fight the lawsuit on the merits, which will require spending more time (more fees).</div><br/></div></div><div id="41397207" class="c"><input type="checkbox" id="c-41397207" checked=""/><div class="controls bullet"><span class="by">robbiewxyz</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393566">parent</a><span>|</span><a href="#41395951">prev</a><span>|</span><a href="#41394551">next</a><span>|</span><label class="collapse" for="c-41397207">[-]</label><label class="expand" for="c-41397207">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d probably like the upvote itself to be considered &quot;speech&quot;. The practical effect of upvoting is to endorse, together with the site&#x27;s moderators and algorithm-curators, the comment to be shown to a wider audience.<p>Along those lines then then an upvote i.e. endorsement would be protected, up to any point where it violated one of the free speech exceptions, e.g. incitement.</div><br/></div></div></div></div></div></div><div id="41394551" class="c"><input type="checkbox" id="c-41394551" checked=""/><div class="controls bullet"><span class="by">itishappy</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393405">parent</a><span>|</span><a href="#41393506">prev</a><span>|</span><a href="#41395406">next</a><span>|</span><label class="collapse" for="c-41394551">[-]</label><label class="expand" for="c-41394551">[1 more]</label></div><br/><div class="children"><div class="content">4chan is actually moderated too.</div><br/></div></div><div id="41395406" class="c"><input type="checkbox" id="c-41395406" checked=""/><div class="controls bullet"><span class="by">coryrc</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393405">parent</a><span>|</span><a href="#41394551">prev</a><span>|</span><a href="#41398145">next</a><span>|</span><label class="collapse" for="c-41395406">[-]</label><label class="expand" for="c-41395406">[1 more]</label></div><br/><div class="children"><div class="content">2) Require confirmation you are a real person (check ID) and attach accounts per person. The commercial Internet has to follow the laws they&#x27;re currently ignoring and the non-commercial Internet can do what they choose (because of being untraceable).</div><br/></div></div><div id="41398145" class="c"><input type="checkbox" id="c-41398145" checked=""/><div class="controls bullet"><span class="by">doikor</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393405">parent</a><span>|</span><a href="#41395406">prev</a><span>|</span><a href="#41394249">next</a><span>|</span><label class="collapse" for="c-41398145">[-]</label><label class="expand" for="c-41398145">[1 more]</label></div><br/><div class="children"><div class="content">4chan is moderated and the moderation is different on each board with the only real global moderation rule being &quot;no illegal stuff&quot;. In addition to that the site does curate the content it shows you using an algorithm even though it is a very basic one (the thread with last reply goes to the top of the page and threads older then X are removed automatically.)<p>For example the qanon conspiracy nuts got moderated out of &#x2F;pol&#x2F; for arguing in bad faith&#x2F;just being too crazy to actually have any kind of conversation with and they fled to another board (8chan and later 8kun) that has even less moderation.</div><br/></div></div><div id="41393709" class="c"><input type="checkbox" id="c-41393709" checked=""/><div class="controls bullet"><span class="by">wredue</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393405">parent</a><span>|</span><a href="#41394249">prev</a><span>|</span><a href="#41396918">next</a><span>|</span><label class="collapse" for="c-41393709">[-]</label><label class="expand" for="c-41393709">[1 more]</label></div><br/><div class="children"><div class="content">Nah. HN is not the same as these others.<p>TikTok. Facebook. Twitter. YouTube.<p>All of these have their algorithms specifically curated to try to keep you angry. YouTube outright ignores your blocks every couple months, and no matter how many people dropping n-bombs you report and block, it never endingly pushes more and more.<p>These company know that their algorithms are harmful and they push them anyway. They absolutely should have liability for what their algorithm pushes.</div><br/></div></div><div id="41396918" class="c"><input type="checkbox" id="c-41396918" checked=""/><div class="controls bullet"><span class="by">jojobas</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393405">parent</a><span>|</span><a href="#41393709">prev</a><span>|</span><a href="#41393970">next</a><span>|</span><label class="collapse" for="c-41396918">[-]</label><label class="expand" for="c-41396918">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Result, HN turns to 4chan.<p>As if it was something bad. 4chan has &#x2F;g and it&#x27;s absolutely awesome.</div><br/></div></div><div id="41393970" class="c"><input type="checkbox" id="c-41393970" checked=""/><div class="controls bullet"><span class="by">akira2501</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393405">parent</a><span>|</span><a href="#41396918">prev</a><span>|</span><a href="#41394700">next</a><span>|</span><label class="collapse" for="c-41393970">[-]</label><label class="expand" for="c-41393970">[10 more]</label></div><br/><div class="children"><div class="content">There&#x27;s moderation to manage disruption to a service.  There&#x27;s editorial control to manage the actual content on a service.<p>HN engages in the former but not the latter.  The big three engage in the latter.</div><br/><div id="41394063" class="c"><input type="checkbox" id="c-41394063" checked=""/><div class="controls bullet"><span class="by">closeparen</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393970">parent</a><span>|</span><a href="#41396597">next</a><span>|</span><label class="collapse" for="c-41394063">[-]</label><label class="expand" for="c-41394063">[7 more]</label></div><br/><div class="children"><div class="content">HN engages in the latter. For example, user votes are weighted based on their alignment with the moderation team&#x27;s view of good content.</div><br/><div id="41394226" class="c"><input type="checkbox" id="c-41394226" checked=""/><div class="controls bullet"><span class="by">akira2501</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394063">parent</a><span>|</span><a href="#41396597">next</a><span>|</span><label class="collapse" for="c-41394226">[-]</label><label class="expand" for="c-41394226">[6 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand your explanation.  Do you mean just voting itself?  That&#x27;s not controlled or managed by HN.  That&#x27;s just more &quot;user generated content.&quot;  That posts get hidden or flagged due to thresholding is non-discriminatory and not _individually_ controlled by the staff here.<p>Or..  are you suggesting there&#x27;s more to how this works?  Is dang watching votes and then making decisions based on those votes?<p>&quot;Editorial control&quot; is more of a term of art and has a narrower definition then you&#x27;re allowing for.</div><br/><div id="41394880" class="c"><input type="checkbox" id="c-41394880" checked=""/><div class="controls bullet"><span class="by">tsimionescu</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394226">parent</a><span>|</span><a href="#41394676">next</a><span>|</span><label class="collapse" for="c-41394880">[-]</label><label class="expand" for="c-41394880">[3 more]</label></div><br/><div class="children"><div class="content">The HN moderation team makes a lot of editorial choices, which is what gives HN its specific character. For example, highly politically charged posts are manually moderated and kept off the main page regardless of votes, with limited exceptions entirely up to the judgement of the editors. For example, content about the wars in Ukraine and Israel is not allowed on the mainpage except on rare occasions. dang has talked a lot about the reasoning behind this.<p>The same applies to comments on HN. Comments are not moderated based purely on legal or certain general &quot;good manners&quot; grounds, they are moderated to keep a certain kind of discourse level. For example, shallow jokes or meme comments are not generally allowed on HN. Comments that start discussing controversial topics, even if civil, are also discouraged when they are not on-topic.<p>Overall, HN is very much curated in the direction of a newspaper &quot;letter to the editor&quot; section, then more algorithmic and hands-off like the Facebook wall or TikTok feed. So there is no doubt whatsoever, I believe, that HN would be considered responsible for user content (and is, in fact, already pretty good at policing that in my experience, at least on the front page).</div><br/><div id="41395474" class="c"><input type="checkbox" id="c-41395474" checked=""/><div class="controls bullet"><span class="by">zahlman</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394880">parent</a><span>|</span><a href="#41394676">next</a><span>|</span><label class="collapse" for="c-41395474">[-]</label><label class="expand" for="c-41395474">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The HN moderation team makes a lot of editorial choices, which is what gives HN its specific character. For example, highly politically charged posts are manually moderated and kept off the main page regardless of votes, with limited exceptions entirely up to the judgement of the editors. For example, content about the wars in Ukraine and Israel is not allowed on the mainpage except on rare occasions. dang has talked a lot about the reasoning behind this.<p>This is meaningfully different in kind from only excluding posts that reflect <i>certain perspectives</i> on such a conflict. Maintaining topicality is not imposing a bias.</div><br/><div id="41398238" class="c"><input type="checkbox" id="c-41398238" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395474">parent</a><span>|</span><a href="#41394676">next</a><span>|</span><label class="collapse" for="c-41398238">[-]</label><label class="expand" for="c-41398238">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This is meaningfully different in kind from only excluding posts that reflect certain perspectives on such a conflict. Maintaining topicality is not imposing a bias.<p>Maintaining topicality is literally a bias.  Excluding posts that reflect certain perspectives is censorship.</div><br/></div></div></div></div></div></div><div id="41394676" class="c"><input type="checkbox" id="c-41394676" checked=""/><div class="controls bullet"><span class="by">empressplay</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394226">parent</a><span>|</span><a href="#41394880">prev</a><span>|</span><a href="#41394711">next</a><span>|</span><label class="collapse" for="c-41394676">[-]</label><label class="expand" for="c-41394676">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s things like &#x27;second chance&#x27; where the editorial team can re-up posts they feel didn&#x27;t get a fair shake the first time around, sometimes if a post gets too &#x27;hot&#x27; they will cool it down -- all of this is understandable but unfortunately does mean they are actively moderating content and thus are responsible for all of it.</div><br/></div></div><div id="41394711" class="c"><input type="checkbox" id="c-41394711" checked=""/><div class="controls bullet"><span class="by">krapp</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394226">parent</a><span>|</span><a href="#41394676">prev</a><span>|</span><a href="#41396597">next</a><span>|</span><label class="collapse" for="c-41394711">[-]</label><label class="expand" for="c-41394711">[1 more]</label></div><br/><div class="children"><div class="content">Dang has been open about voting being only one part of the way HN works, and that manual moderator intervention does occur. They will downweigh the votes of &quot;problem&quot; accounts, manually adjust the order of the frontpage, and do whatever they feel necessary to maintain a high signal to noise ratio.</div><br/></div></div></div></div></div></div><div id="41396597" class="c"><input type="checkbox" id="c-41396597" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393970">parent</a><span>|</span><a href="#41394063">prev</a><span>|</span><a href="#41394700">next</a><span>|</span><label class="collapse" for="c-41396597">[-]</label><label class="expand" for="c-41396597">[2 more]</label></div><br/><div class="children"><div class="content">Every time you see a comment marked as [dead] that means a moderator deleted it. There is no auto-deletion resulting from downvotes.<p>Even mentioning certain topics, such as Israel&#x27;s invasion of Palestine, even when the mention is on-topic and not disruptive, as in this comment you are reading, is practically a death sentence for a comment. Not because of votes, but because of the moderators. Downvotes may prioritize which comments go in front of moderators (we don&#x27;t know) but moderators make the final decision; comments that are downvoted but not removed merely stick around in a light grey colour.<p>By enabling showdead in your user preferences and using the site for a while, especially reading controversial threads, you can get a feel for what kinds of comments are deleted by moderators exercising. It is clear that most moderation is about editorial control and not simply the removal of disruption.<p>This comment may be dead by the time you read it, due to the 
previous mention of Palestine - hi to users with showdead enabled. Its parent will probably merely be down voted because it&#x27;s wrong but doesn&#x27;t contain anything that would irk the mods.</div><br/><div id="41396702" class="c"><input type="checkbox" id="c-41396702" checked=""/><div class="controls bullet"><span class="by">philipkglass</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396597">parent</a><span>|</span><a href="#41394700">next</a><span>|</span><label class="collapse" for="c-41396702">[-]</label><label class="expand" for="c-41396702">[1 more]</label></div><br/><div class="children"><div class="content">Comments that are marked [dead] without the [flagged] indicator are like that because the user that posted the comment has been banned. For green (new) accounts this can be due to automatic filters that threw up false positives for new accounts. For old accounts this shows that the account (not the individual comment) has been banned by moderators. Users who have been banned can email hn@ycombinator.com pledging to follow the rules in the future and they&#x27;ll be granted another chance. Even if a user remains banned, you can unhide a good [dead] comment by clicking on its timestamp and clicking &quot;vouch.&quot;<p>Comments are marked [flagged] [dead] when ordinary users have clicked on the timestamp and selected &quot;flag.&quot; So user downvotes cannot kill a comment, but flagging by ordinary non-moderator users can kill it.</div><br/></div></div></div></div></div></div><div id="41394700" class="c"><input type="checkbox" id="c-41394700" checked=""/><div class="controls bullet"><span class="by">pointnatu</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393405">parent</a><span>|</span><a href="#41393970">prev</a><span>|</span><a href="#41393482">next</a><span>|</span><label class="collapse" for="c-41394700">[-]</label><label class="expand" for="c-41394700">[1 more]</label></div><br/><div class="children"><div class="content">Freedom of speech, not reach of their personal curation preferences, narrative shaping due to confirmation bias and survivorship bias. Tech is in the put them on scales to increase their signal, decrease others based upon some hokey story of academic and free market genius.<p>The pro-science crowd (which includes me fwiw) seems incapable of providing a proof any given scientist is <i>that</i> important. Same old social politics norms inflate some deflate others and we confirm our survival means we special. Ones education is vacuous prestige given physics applies equally; oh you did the math! Yeah I just tell the computer to do it. Oh you memorized the circumlocutions and dialectic of some long dead physicist. Outstanding.<p>There’s a lot of ego driven banal classist nonsense in tech and science. At the end of the day just meat suits with the same general human condition.</div><br/></div></div><div id="41393482" class="c"><input type="checkbox" id="c-41393482" checked=""/><div class="controls bullet"><span class="by">jtriangle</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393405">parent</a><span>|</span><a href="#41394700">prev</a><span>|</span><a href="#41395202">next</a><span>|</span><label class="collapse" for="c-41393482">[-]</label><label class="expand" for="c-41393482">[8 more]</label></div><br/><div class="children"><div class="content">(1) 4chin is too dumb to use HN, and there&#x27;s no image posting so, I doubt they&#x27;d even be interested in raiding us
(2) I&#x27;ve never seen anything illegal here, I&#x27;m sure it happens, and it gets dealt with quickly enough that it&#x27;s not really ever going to be a problem if things continue as they have been.<p>They may lose 230 protection, sure, but probably not really a problem here. For Facebook et al, it&#x27;s going to be an issue, no doubt. I suppose they could drop their algos and bring back the chronological feeds, but, my guess is that wouldn&#x27;t be profitable given that ad-tech and content feeds are one in the same at this point.<p>I&#x27;d also assume that &quot;curation&quot; is the sticking point here, if a platform can claim that they do not curate content, they probably keep 230 protection.</div><br/><div id="41393746" class="c"><input type="checkbox" id="c-41393746" checked=""/><div class="controls bullet"><span class="by">wredue</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393482">parent</a><span>|</span><a href="#41394960">next</a><span>|</span><label class="collapse" for="c-41393746">[-]</label><label class="expand" for="c-41393746">[6 more]</label></div><br/><div class="children"><div class="content">Certain boards most definitely raid various HN threads.<p>Specifically, every political or science thread that makes it, is raided by 4chan. 4chan also regularly pushes anti&#x2F;science and anti-education agenda threads to the top here, along with posts from various alt-right figures on occasion.</div><br/><div id="41394213" class="c"><input type="checkbox" id="c-41394213" checked=""/><div class="controls bullet"><span class="by">jtriangle</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393746">parent</a><span>|</span><a href="#41394960">next</a><span>|</span><label class="collapse" for="c-41394213">[-]</label><label class="expand" for="c-41394213">[5 more]</label></div><br/><div class="children"><div class="content">search: site:4chan.org news.ycombinator.com<p>Seems pretty sparse to me, and from a casual perusal, I haven&#x27;t seen any actual calls to raiding anything here, it&#x27;s more of a reference where articles&#x2F;posts have happened, and people talking about them.<p>Remember, not everyone who you disagree with comes from 4chan, some of them probably work with you, you might even be friends with them, and they&#x27;re perfectly serviceable people with lives, hopes, dreams, same as yours, they simply think differently than you.</div><br/><div id="41394636" class="c"><input type="checkbox" id="c-41394636" checked=""/><div class="controls bullet"><span class="by">wredue</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394213">parent</a><span>|</span><a href="#41394960">next</a><span>|</span><label class="collapse" for="c-41394636">[-]</label><label class="expand" for="c-41394636">[4 more]</label></div><br/><div class="children"><div class="content">lol dude. Nobody said that 4chan links are posted to HN, just that 4chan definitely raids HN.<p>4chan is very well known for brigading. It is also well known that using 4chan as well as a number of other locations, such as discord, to post links for brigades are an extremely common thing that the alt-right does to try to raise the “validity” of their statements.<p>I also did not claim that only these opinions come from 4chan. Nice strawman bro.<p>Also, my friends do not believe these things. I do not make a habit of being friends with people that believe in genociding others purely because of sexual orientation or identity.</div><br/><div id="41396124" class="c"><input type="checkbox" id="c-41396124" checked=""/><div class="controls bullet"><span class="by">jtriangle</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394636">parent</a><span>|</span><a href="#41394960">next</a><span>|</span><label class="collapse" for="c-41396124">[-]</label><label class="expand" for="c-41396124">[3 more]</label></div><br/><div class="children"><div class="content">Go ahead and type that search query into google and see what happens.<p>Also the alt-right is a giant threat, if you categorize everyone right of you as alt-right, which seems to be the standard definition.<p>That&#x27;s not how I&#x27;ve chosen to live, and I find that it&#x27;s peaceful to choose something more reasonable. The body politic is cancer on the individual, and on the list of things that are important in life, it&#x27;s not truly important. With enough introspection you&#x27;ll find that the tendency to latch onto politics, or anything politics-adjacent, comes from an overall lack of agency over the other aspects of life you truly care about. It&#x27;s a vicious cycle. You have a finite amount of mental energy, and the more you spend on worthless things, the less you have to spend on things that matter, which leads to you latching further on to the worthless things, and having even less to spend on things that matter.<p>It&#x27;s a race to the bottom that has only losers. If you&#x27;re looking for genocide, that&#x27;s the genocide of the modern mind, and you&#x27;re one foot in the grave already. You can choose to step out now and probably be ok, but it&#x27;s going to be uncomfortable to do so.<p>That&#x27;s all not to say there aren&#x27;t horrid, problem-causing individuals out in the world, there certainly are, it&#x27;s just that the less you fixate on them, the more you realize that they&#x27;re such an extreme minority that you feel silly fixating on them in the first place. That goes for anyone that anyone deems &#x27;horrid and problem-causing&#x27; mind you, not just whatever idea you have of that class of person.</div><br/><div id="41397118" class="c"><input type="checkbox" id="c-41397118" checked=""/><div class="controls bullet"><span class="by">halfcat</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396124">parent</a><span>|</span><a href="#41397866">next</a><span>|</span><label class="collapse" for="c-41397118">[-]</label><label class="expand" for="c-41397118">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Go ahead and type that search query into google and see what happens.</i><p>What are you expecting it to show? That site removes all content after a matter of days.</div><br/></div></div><div id="41397866" class="c"><input type="checkbox" id="c-41397866" checked=""/><div class="controls bullet"><span class="by">wredue</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396124">parent</a><span>|</span><a href="#41397118">prev</a><span>|</span><a href="#41394960">next</a><span>|</span><label class="collapse" for="c-41397866">[-]</label><label class="expand" for="c-41397866">[1 more]</label></div><br/><div class="children"><div class="content">These people win elections and make news cycles. They are not an “ignorable, small minority”.<p>For the record, ensuring that those who wish to genocide LGBT+ people are not the majority voice on the internet is absolutely not “a worthless matter”, not by any stretch. I would definitely rather not have to do this, but then, the people who dedicate their lives to trolling and hate are extremely active.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41394960" class="c"><input type="checkbox" id="c-41394960" checked=""/><div class="controls bullet"><span class="by">Dr_Incelheimer</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393482">parent</a><span>|</span><a href="#41393746">prev</a><span>|</span><a href="#41395202">next</a><span>|</span><label class="collapse" for="c-41394960">[-]</label><label class="expand" for="c-41394960">[1 more]</label></div><br/><div class="children"><div class="content">&gt;4chin is too dumb to use HN<p>I don&#x27;t frequent 4cuck, I use soyjak.party which I guess from your perspective is even worse, but there are of plenty of smart people on the &#x27;cuck thoughbeit, like the gemmy &#x2F;lit&#x2F; schizo. I think you would feel right at home in &#x2F;sci&#x2F;.</div><br/></div></div></div></div></div></div><div id="41395202" class="c"><input type="checkbox" id="c-41395202" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41393405">prev</a><span>|</span><a href="#41396513">next</a><span>|</span><label class="collapse" for="c-41395202">[-]</label><label class="expand" for="c-41395202">[11 more]</label></div><br/><div class="children"><div class="content">&gt; I think the ultimate problem is that social media is not unbiased — it curates what people are shown.<p>This is literally the purpose of Section 230. It&#x27;s Section 230 of the <i>Communications Decency Act</i>. The purpose was to change the law so platforms could moderate content without incurring liability, because the law was previously that doing any moderation made you liable for whatever users posted, and you don&#x27;t want a world where removing&#x2F;downranking spam or pornography or trolling causes you to get sued for unrelated things you didn&#x27;t remove.</div><br/><div id="41397673" class="c"><input type="checkbox" id="c-41397673" checked=""/><div class="controls bullet"><span class="by">itsdrewmiller</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395202">parent</a><span>|</span><a href="#41395306">next</a><span>|</span><label class="collapse" for="c-41397673">[-]</label><label class="expand" for="c-41397673">[3 more]</label></div><br/><div class="children"><div class="content">The CDA was about making it clearly criminal to send obscene content to minors via the internet.  Section 230 was intended to clarify the common carrier role of ISPs and similar providers of third party content.  It does have a subsection to clarify that attempting to remove objectionable content doesn&#x27;t remove your common carrier protections, but I don&#x27;t believe that was a response to pre-CDA status quo.</div><br/><div id="41398028" class="c"><input type="checkbox" id="c-41398028" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41397673">parent</a><span>|</span><a href="#41398004">next</a><span>|</span><label class="collapse" for="c-41398028">[-]</label><label class="expand" for="c-41398028">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The CDA was about making it clearly criminal to send obscene content to minors via the internet.<p>Basically true.<p>&gt;  Section 230 was intended to clarify the common carrier role of ISPs and similar providers of third party content.<p>No, it wasn&#x27;t, and you can tell that because there is literally not a single word to that effect in Section 230. It was to <i>enable</i> information service providers to exercise editorial control over user-submitted content without acquiring publisher-style liability, because the alternative, giving liability decisions occurring at the time and the way providers were reacting to them, was that any site using user-sourced content at scale would, to mitigate legal risk, be completely <i>un</i>moderated, which was the opposite of the vision the authors of Section 230 and the broader CDA had for the internet. There are no &quot;common carrier&quot; obligations or protections in Section 230. The terms of the protection are the <i>opposite</i> of common carrier, and while there are limitations on the protections, there are no common carrier like obligations attached to them.<p>&gt;</div><br/></div></div><div id="41398004" class="c"><input type="checkbox" id="c-41398004" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41397673">parent</a><span>|</span><a href="#41398028">prev</a><span>|</span><a href="#41395306">next</a><span>|</span><label class="collapse" for="c-41398004">[-]</label><label class="expand" for="c-41398004">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The CDA was about making it clearly criminal to send obscene content to minors via the internet.<p>That part of the law was unconstitutional and pretty quickly got struck down, but it still goes to the same point that the intent of Congress was for sites to remove stuff and <i>not</i> be &quot;common carriers&quot; that leave everything up.<p>&gt; Section 230 was intended to clarify the common carrier role of ISPs and similar providers of third party content. It does have a subsection to clarify that attempting to remove objectionable content doesn&#x27;t remove your common carrier protections, but I don&#x27;t believe that was a response to pre-CDA status quo.<p>If you can forgive Masnick&#x27;s chronic irateness he does a decent job of explaining the situation:<p><a href="https:&#x2F;&#x2F;www.techdirt.com&#x2F;2024&#x2F;08&#x2F;29&#x2F;third-circuits-section-230-tiktok-ruling-deliberately-ignores-precedent-defies-logic&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.techdirt.com&#x2F;2024&#x2F;08&#x2F;29&#x2F;third-circuits-section-2...</a></div><br/></div></div></div></div><div id="41395306" class="c"><input type="checkbox" id="c-41395306" checked=""/><div class="controls bullet"><span class="by">samrus</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395202">parent</a><span>|</span><a href="#41397673">prev</a><span>|</span><a href="#41395410">next</a><span>|</span><label class="collapse" for="c-41395306">[-]</label><label class="expand" for="c-41395306">[2 more]</label></div><br/><div class="children"><div class="content">Yeah but they&#x27;re not just removing spam and porn. They&#x27;re picking out things that makes them money even if it harms people. That was never in the spirit of the law</div><br/><div id="41397371" class="c"><input type="checkbox" id="c-41397371" checked=""/><div class="controls bullet"><span class="by">habinero</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395306">parent</a><span>|</span><a href="#41395410">next</a><span>|</span><label class="collapse" for="c-41397371">[-]</label><label class="expand" for="c-41397371">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it is. Section 230 doesn&#x27;t replace the 1A, and deciding what you want to show or not show is classic 1A activity.</div><br/></div></div></div></div><div id="41395410" class="c"><input type="checkbox" id="c-41395410" checked=""/><div class="controls bullet"><span class="by">zahlman</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395202">parent</a><span>|</span><a href="#41395306">prev</a><span>|</span><a href="#41396513">next</a><span>|</span><label class="collapse" for="c-41395410">[-]</label><label class="expand" for="c-41395410">[5 more]</label></div><br/><div class="children"><div class="content">&gt; The purpose was to change the law so platforms could moderate content<p>What part of deliberately showing political content to people algorithmically expected to agree with it, constitutes &quot;moderation&quot;?<p>What part of deliberately showing political content to people algorithmically expected to <i>disagree</i> with it, constitutes &quot;moderation&quot;?<p>What part of deliberately suppressing or promoting political content based on the opinions of those in charge of the platform, constitutes &quot;moderation&quot;?<p>What part of suppressing &quot;misinformation&quot; on the basis of what&#x27;s said in &quot;reliable sources&quot; (rather than any independent investigation - but really the point would still stand), constitutes &quot;moderation&quot;?<p>What part of favouring content from already popular content creators because it brings in more ad revenue, constitutes &quot;moderation&quot;?<p>What part of algorithmically associating content with ads for specific products or services, constitutes &quot;moderation&quot;?</div><br/><div id="41395427" class="c"><input type="checkbox" id="c-41395427" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395410">parent</a><span>|</span><a href="#41395478">next</a><span>|</span><label class="collapse" for="c-41395427">[-]</label><label class="expand" for="c-41395427">[3 more]</label></div><br/><div class="children"><div class="content">Prosaically, all of your examples are moderation. And as a private space that a user must choose to access, I&#x27;d argue that&#x27;s great.</div><br/><div id="41396696" class="c"><input type="checkbox" id="c-41396696" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395427">parent</a><span>|</span><a href="#41395478">next</a><span>|</span><label class="collapse" for="c-41396696">[-]</label><label class="expand" for="c-41396696">[2 more]</label></div><br/><div class="children"><div class="content">There is (or should be, in any case) a difference between moderation and recommendation.</div><br/><div id="41397375" class="c"><input type="checkbox" id="c-41397375" checked=""/><div class="controls bullet"><span class="by">habinero</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396696">parent</a><span>|</span><a href="#41395478">next</a><span>|</span><label class="collapse" for="c-41397375">[-]</label><label class="expand" for="c-41397375">[1 more]</label></div><br/><div class="children"><div class="content">There is no difference. Both are editorial choices and protected 1A activity.</div><br/></div></div></div></div></div></div><div id="41395478" class="c"><input type="checkbox" id="c-41395478" checked=""/><div class="controls bullet"><span class="by">crooked-v</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395410">parent</a><span>|</span><a href="#41395427">prev</a><span>|</span><a href="#41396513">next</a><span>|</span><label class="collapse" for="c-41395478">[-]</label><label class="expand" for="c-41395478">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What part of deliberately showing political content to people algorithmically expected to agree with it, constitutes &quot;moderation&quot;?<p>Well, maybe it&#x27;s just me, but only showing political content that doesn&#x27;t include &quot;kill all the (insert minority here)&quot;, and expecting users to not object to that standard, is a pretty typical aspect of moderation for discussion sites.<p>&gt; What part of deliberately suppressing or promoting political content based on the opinions of those in charge of the platform, constitutes &quot;moderation&quot;?<p>Again, deliberately suppressing support for literal and obvious facism, based on the opinions of those in charge of the platform, is a kind of moderation so typical that it&#x27;s noteworthy when it doesn&#x27;t happen (e.g. Stormfront).<p>&gt; What part of suppressing &quot;misinformation&quot; on the basis of what&#x27;s said in &quot;reliable sources&quot; (rather than any independent investigation - but really the point would still stand), constitutes &quot;moderation&quot;?<p>Literally all of Wikipedia, where the whole point of the reliable sources policy is that the people running it don&#x27;t have to be experts to have a decently objective standard for what can be published.</div><br/></div></div></div></div></div></div><div id="41396513" class="c"><input type="checkbox" id="c-41396513" checked=""/><div class="controls bullet"><span class="by">cbsmith</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41395202">prev</a><span>|</span><a href="#41397839">next</a><span>|</span><label class="collapse" for="c-41396513">[-]</label><label class="expand" for="c-41396513">[10 more]</label></div><br/><div class="children"><div class="content">The rise of social media was largely predicated on the curation it provided. People, and particularly advertisers, <i>wanted</i> a curated environment. That was the key differentiator to the wild west of the world wide web.<p>The idea that curation is a problem with social media is always a head scratcher for me. The option to just directly publish to the world wide web without social media is always available, but time and again, that option is largely not chosen... this ruling could well narrow it down that being the only option.<p>Now, in practice, I don&#x27;t think that will happen. This will raise the costs of operating social media, and those costs will be reflected in prices advertisers pay to advertise on social media. That may shrink the social media ecosystem, but what it will definitely do is raise the draw bridge over the moat around the major social media players. You&#x27;re going to see less competition.</div><br/><div id="41396831" class="c"><input type="checkbox" id="c-41396831" checked=""/><div class="controls bullet"><span class="by">stale2002</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396513">parent</a><span>|</span><a href="#41396721">next</a><span>|</span><label class="collapse" for="c-41396831">[-]</label><label class="expand" for="c-41396831">[5 more]</label></div><br/><div class="children"><div class="content">&gt; People, and particularly advertisers, wanted a curated environment<p>Then give the choice to the user.<p>If a user wants to opt in, or change their moderation preferences then they should be allowed.<p>By all means offer a choice of moderation decisions. And let the user change them, opt out conditionally and ignore them if they so choose.</div><br/><div id="41397385" class="c"><input type="checkbox" id="c-41397385" checked=""/><div class="controls bullet"><span class="by">habinero</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396831">parent</a><span>|</span><a href="#41396976">next</a><span>|</span><label class="collapse" for="c-41397385">[-]</label><label class="expand" for="c-41397385">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re free to to make your own site with your own moderation controls. And nobody will use it, because it&#x27;ll rapidly become 99.999% spam, CSAM and porn.</div><br/></div></div><div id="41396976" class="c"><input type="checkbox" id="c-41396976" checked=""/><div class="controls bullet"><span class="by">cbsmith</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396831">parent</a><span>|</span><a href="#41397385">prev</a><span>|</span><a href="#41396721">next</a><span>|</span><label class="collapse" for="c-41396976">[-]</label><label class="expand" for="c-41396976">[3 more]</label></div><br/><div class="children"><div class="content">You say that like that choice doesn&#x27;t exist.</div><br/><div id="41397376" class="c"><input type="checkbox" id="c-41397376" checked=""/><div class="controls bullet"><span class="by">stale2002</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396976">parent</a><span>|</span><a href="#41396721">next</a><span>|</span><label class="collapse" for="c-41397376">[-]</label><label class="expand" for="c-41397376">[2 more]</label></div><br/><div class="children"><div class="content">&gt; You say that like that choice doesn&#x27;t exist.<p>You said this: &quot;People, and particularly advertisers, wanted a curated environment.&quot;<p>If moderation choices are put in the hands of the user, then what you are describing is not a problem, as the user can have that.<p>Therefore, you saying that this choice exists, means that there isn&#x27;t a problem for anyone who chooses to not have the spam, and your original complaint is refuted.</div><br/><div id="41397582" class="c"><input type="checkbox" id="c-41397582" checked=""/><div class="controls bullet"><span class="by">cbsmith</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41397376">parent</a><span>|</span><a href="#41396721">next</a><span>|</span><label class="collapse" for="c-41397582">[-]</label><label class="expand" for="c-41397582">[1 more]</label></div><br/><div class="children"><div class="content">There absolutely can be a problem despite choice existing. I&#x27;m not saying otherwise.<p>I&#x27;m saying the choice exists. The choices we make <i>are</i> the problem.</div><br/></div></div></div></div></div></div></div></div><div id="41396721" class="c"><input type="checkbox" id="c-41396721" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396513">parent</a><span>|</span><a href="#41396831">prev</a><span>|</span><a href="#41397839">next</a><span>|</span><label class="collapse" for="c-41396721">[-]</label><label class="expand" for="c-41396721">[4 more]</label></div><br/><div class="children"><div class="content">&gt;The option to just directly publish to the world wide web without social media is always available,<p>Not exactly. You still have to procure web hosting somewhere, and that hosting provider might choose to refuse your money and kick you off.<p>You might also have to procure the services of Cloudflare if you face significant traffic, and Cloudflare might choose to refuse your money and kick you off.<p>&gt;that option is largely not chosen...<p>That&#x27;s because most people do not have neither the time nor the will to learn and speak computer.<p>Social media and immediate predecessors like Wordpress were and are successful because they brought down the lowest common denominator to <i>&quot;Smack keys and tap Submit&quot;</i>. HTML? CSS? Nobody has time for our pig latin.</div><br/><div id="41397017" class="c"><input type="checkbox" id="c-41397017" checked=""/><div class="controls bullet"><span class="by">cbsmith</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396721">parent</a><span>|</span><a href="#41397839">next</a><span>|</span><label class="collapse" for="c-41397017">[-]</label><label class="expand" for="c-41397017">[3 more]</label></div><br/><div class="children"><div class="content">&gt; You still have to procure web hosting somewhere, and that hosting provider might choose to refuse your money and kick you off.<p>Who says you need to procure a web hosting provider?<p>But yes, if you connect your computer up to other computers, the other computers may decide they don&#x27;t want any part of what you have to offer.<p>Without that, I wouldn&#x27;t want to be in the Internet. I don&#x27;t want to be forced to ingest bytes from anyone who would send them my way. That&#x27;s just not a good value proposition for me.<p>&gt; That&#x27;s because most people do not have neither the time nor the will to learn and speak computer.<p>I&#x27;m sorry, but no. You can literally type in to a word processor or any number of other tools and select &quot;save as web content&quot;, and then use any number of products to take a web page and serve it up to the world wide web. It&#x27;s been that way for the better part of 25 years. No HTML or CSS knowledge needed. If you can&#x27;t handle that you can just record a video, save it to a file, and serve it up over a web server. Yes, you need to be able to use a computer to participate on the world wide web, but no more than you do to use social media.<p>Now, what you <i>won&#x27;t</i> get is a distribution platform that gets your content up in front of people who never asked for it. That is what social media provides. It lowers the effort for the people <i>receiving</i> the content, as in exactly the curation process that the judge was ruling about.</div><br/><div id="41398234" class="c"><input type="checkbox" id="c-41398234" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41397017">parent</a><span>|</span><a href="#41397839">next</a><span>|</span><label class="collapse" for="c-41398234">[-]</label><label class="expand" for="c-41398234">[2 more]</label></div><br/><div class="children"><div class="content">&gt;You can literally type in to a word processor or any number of other tools<p>Most people these days don&#x27;t have a word processor or, indeed, &quot;any number of other tools&quot;. It&#x27;s all &quot;in the cloud&quot;, usually Google Docs or Office 365 Browser Edition(tm).<p>&gt;select &quot;save as web content&quot;<p>Most people these days don&#x27;t (arguably never) understand files and folders.<p>&gt;and then use any number of products to take a web page and serve it up to the world wide web.<p>Most people these days cannot be bothered. Especially when the counter proposal is &quot;Make an X account, smash some keys, and press Submit to get internet points&quot;.<p>&gt;If you can&#x27;t handle that you can just record a video, save it to a file, and serve it up over a web server.<p>I&#x27;m going to stop you right here: You are <i>vastly</i> overestimating both the will and the computer-aptitude of most people. There is a reason Youtube and Twitch have killed off literally every other video sharing service; there is a reason smartphones killed off personal computers (desktops and to a lesser degree laptops).<p>Social media became the juggernaut it is today because businesses figured out how to capitalize on the latent demand for <i>easy</i> sharing of information: Literal One Click Solutions(tm) that anyone can understand.<p>&gt;what you won&#x27;t get is a distribution platform that gets your content up in front of people who never asked for it.<p>The internet and more specifically search engines in general have always been that distribution platform. The only thing that changed in the last 30 years is how easy it is to get your stuff on that platform.</div><br/><div id="41398419" class="c"><input type="checkbox" id="c-41398419" checked=""/><div class="controls bullet"><span class="by">cbsmith</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41398234">parent</a><span>|</span><a href="#41397839">next</a><span>|</span><label class="collapse" for="c-41398419">[-]</label><label class="expand" for="c-41398419">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Most people these days don&#x27;t have a word processor or, indeed, &quot;any number of other tools&quot;. It&#x27;s all &quot;in the cloud&quot;, usually Google Docs or Office 365 Browser Edition(tm).<p>Read that again. ;-)<p>&gt; Most people these days don&#x27;t (arguably never) understand files and folders.<p>We can debate on the skills of &quot;most people&quot; back and forth, but I think it&#x27;s fair to say that &quot;save as web content&quot; is easier to figure out than figuring out how to navigate a social media site (and that doesn&#x27;t necessarily require files or folders). If that really is too hard for someone, there are products out there designed to make it even easier. Way back before social media took over, everyone and their dog managed to figure out how to put stuff on the web. People who couldn&#x27;t make it through high school were successfully producing web pages, blogs, podcasts, video content, you name it.<p>&gt; I&#x27;m going to stop you right here: You are vastly overestimating both the will and the computer-aptitude of most people.<p>I disagree. I think they don&#x27;t have the will to do it, because they&#x27;d rather use social media. I do believe if they had the will to do it, they would. I agree there are some people who lack the computer-aptitude to get content on the web. Where I struggle is believing those same people manage to put content on social media... which I&#x27;ll point out is on the web.<p>&gt; There is a reason Youtube and Twitch have killed off literally every other video sharing service<p>Yes, because video sharing at scale is fairly difficult and requires real skill. If you don&#x27;t have that skill, you&#x27;re going to have to pay someone to do it, or find someone who has their own agenda that makes them want to do it without charging you... like Youtube or Twitch.<p>On the other hand, putting a video up on the web that no one knows about, no one looks for, and no one consumes unless you personally convince them to do so is comparatively simple.<p>&gt; there is a reason smartphones killed off personal computers (desktops and to a lesser degree laptops)<p>Yes, that reason is that smartphones were subsidies by carriers. ;-)<p>But it&#x27;s good that you mentioned smartphones, because smart phones will let you send content to anyone in your contacts without you having anything that most would describe as &quot;computer-aptitude&quot;. No social media needed... and yet the prevailing preference is for people to go through a process of logging in, shaping content to suit the demands of social media services, attempting to tune the content to get &quot;the algorithm&quot; to show it to as many people as possible, and put their content there. That takes more will&#x2F;aptitude&#x2F;whatever, but they do it for the distribution&#x2F;audience.<p>&gt; Social media became the juggernaut it is today because businesses figured out how to capitalize on the latent demand for easy sharing of information: Literal One Click Solutions(tm) that anyone can understand.<p>I&#x27;d agree with you if you said &quot;distribute&quot; instead of &quot;sharing&quot;. It&#x27;s really hard to get millions of people to consume your content. That is, until social media came along and basically eliminated the cost of distribution. So any idiot can push their content out to millions and fill the world with whatever they want.... and now there&#x27;s a sense of entitlement about it, where if a platform doesn&#x27;t push that content on other people, at no cost to them, that they&#x27;re being censored.<p>Yup, that does really require social media.<p>&gt; The internet and more specifically search engines in general have always been that distribution platform. The only thing that changed in the last 30 years is how easy it is to get your stuff on that platform.<p>No, the Internet &amp; the web required you to go looking for the content you wanted. Search engines (at least at one time) were designed to accelerate that proces of find exactly the content you were looking for faster, and get you off their platform ASAP. Social media is kind of the opposite of search engines. They want you to stay on their platform; they want you to keep scrolling at whatever &quot;engaging&quot; content they can find, regardless of what you&#x27;re looking for; if you forget about whatever you were originally looking for, that&#x27;s a bonus. It&#x27;s that ability to have your content show up when no one is looking for it where social media provides an advantage over the web for content makers.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41397839" class="c"><input type="checkbox" id="c-41397839" checked=""/><div class="controls bullet"><span class="by">hungie</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41396513">prev</a><span>|</span><a href="#41398119">next</a><span>|</span><label class="collapse" for="c-41397839">[-]</label><label class="expand" for="c-41397839">[1 more]</label></div><br/><div class="children"><div class="content">&gt; social media is not unbiased ...<p>Media, generally, social or otherwise, is not unbiased. All media has bias. The human act of editing, selecting stories, framing those stories, authoring or retelling them... it&#x27;s all biased.<p>I wish we would stop seeking unbiased media as some sort of ideal, and instead seek open biases -- tell me enough about yourself and where your biases lie, so I can make informed decisions.<p>This reasoning is not far off from the court&#x27;s thinking: editing is speech. A for you page is edited, and is TikTok&#x27;s own speech.<p>That said, I do agree with your meta point. Social media (hn not excluded) is a generally unpleasant place to be.</div><br/></div></div><div id="41398119" class="c"><input type="checkbox" id="c-41398119" checked=""/><div class="controls bullet"><span class="by">gigatexal</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41397839">prev</a><span>|</span><a href="#41393322">next</a><span>|</span><label class="collapse" for="c-41398119">[-]</label><label class="expand" for="c-41398119">[1 more]</label></div><br/><div class="children"><div class="content">If it is a reckoning for social media then so be it. Social media net-net was probably a mistake.<p>But I doubt this gets held on appeal. Given how fickle this Supreme Court is they’ll probably overrule themselves to fit their agenda since they don’t seem to think precedent is worth a damn.</div><br/></div></div><div id="41393322" class="c"><input type="checkbox" id="c-41393322" checked=""/><div class="controls bullet"><span class="by">tboyd47</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41398119">prev</a><span>|</span><a href="#41398305">next</a><span>|</span><label class="collapse" for="c-41393322">[-]</label><label class="expand" for="c-41393322">[9 more]</label></div><br/><div class="children"><div class="content">It all comes down to the assertion made by the author:<p>&gt; There is no way to run a targeted ad social media company with 40% margins if you have to make sure children aren’t harmed by your product.</div><br/><div id="41393521" class="c"><input type="checkbox" id="c-41393521" checked=""/><div class="controls bullet"><span class="by">philippejara</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393322">parent</a><span>|</span><a href="#41393991">next</a><span>|</span><label class="collapse" for="c-41393521">[-]</label><label class="expand" for="c-41393521">[3 more]</label></div><br/><div class="children"><div class="content">I find it hard to see a way to run a targeted ad social media company at all if you have to make sure children aren&#x27;t harmed by your product.</div><br/><div id="41394199" class="c"><input type="checkbox" id="c-41394199" checked=""/><div class="controls bullet"><span class="by">stevenicr</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393521">parent</a><span>|</span><a href="#41393991">next</a><span>|</span><label class="collapse" for="c-41394199">[-]</label><label class="expand" for="c-41394199">[2 more]</label></div><br/><div class="children"><div class="content">don&#x27;t let children use?
In TN it that will be illegal Jan 1 - unless social media creates a method for parents to provide ID and opt out of them being blocked I think?<p>Wouldn&#x27;t that put the responsibility back on the parents?<p>The state told you XYZ was bad for your kids and it&#x27;s illegal for them to use, but then you bypassed that restriction and put the sugar back into their hands with an access-blocker-blocker..<p>Random wondering</div><br/><div id="41394278" class="c"><input type="checkbox" id="c-41394278" checked=""/><div class="controls bullet"><span class="by">ghaff</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394199">parent</a><span>|</span><a href="#41393991">next</a><span>|</span><label class="collapse" for="c-41394278">[-]</label><label class="expand" for="c-41394278">[1 more]</label></div><br/><div class="children"><div class="content">Age limitations for things are pretty widespread. Of course, they can be bypassed to various degrees but, depending upon how draconian you want to be, you can presumably be seen as doing the best you reasonably can in a virtual world.</div><br/></div></div></div></div></div></div><div id="41393991" class="c"><input type="checkbox" id="c-41393991" checked=""/><div class="controls bullet"><span class="by">hyeonwho4</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393322">parent</a><span>|</span><a href="#41393521">prev</a><span>|</span><a href="#41393809">next</a><span>|</span><label class="collapse" for="c-41393991">[-]</label><label class="expand" for="c-41393991">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure about video, but we are no longer in an era when manual moderation is necessary. Certainly for text, moderation for child safety could be as easy as taking the written instructions currently given to human moderators and having an LLM interpreter (only needs to output a few bits of information) do the same job.</div><br/><div id="41394064" class="c"><input type="checkbox" id="c-41394064" checked=""/><div class="controls bullet"><span class="by">tboyd47</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393991">parent</a><span>|</span><a href="#41393809">next</a><span>|</span><label class="collapse" for="c-41394064">[-]</label><label class="expand" for="c-41394064">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s great, but can your LLM remove everything harmful? If not, you&#x27;re still liable for that one piece of content that it missed under this interpretation.</div><br/><div id="41397717" class="c"><input type="checkbox" id="c-41397717" checked=""/><div class="controls bullet"><span class="by">itsdrewmiller</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394064">parent</a><span>|</span><a href="#41393809">next</a><span>|</span><label class="collapse" for="c-41397717">[-]</label><label class="expand" for="c-41397717">[1 more]</label></div><br/><div class="children"><div class="content">There are two questions - one is &quot;should social media companies be globally immune from liability for any algorithmic decisions&quot; which this case says &quot;no&quot;.  Then there is &quot;in any given case, is the social media company guilty of the harm of which it is accused&quot;.  Outcomes for that would evolve over time (and I would hope for clarifying legislation as well).</div><br/></div></div></div></div></div></div><div id="41393809" class="c"><input type="checkbox" id="c-41393809" checked=""/><div class="controls bullet"><span class="by">aftbit</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393322">parent</a><span>|</span><a href="#41393991">prev</a><span>|</span><a href="#41398305">next</a><span>|</span><label class="collapse" for="c-41393809">[-]</label><label class="expand" for="c-41393809">[2 more]</label></div><br/><div class="children"><div class="content">What about 0% margins? Is there actually enough money in social media to pay for moderation even with no profit?</div><br/><div id="41395241" class="c"><input type="checkbox" id="c-41395241" checked=""/><div class="controls bullet"><span class="by">Ajedi32</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393809">parent</a><span>|</span><a href="#41398305">next</a><span>|</span><label class="collapse" for="c-41395241">[-]</label><label class="expand" for="c-41395241">[1 more]</label></div><br/><div class="children"><div class="content">At the scale social media companies operate at, absolutely perfect moderation with zero false negatives is unavailable at any price. Even if they had a highly trained human expert manually review every single post (which is obviously way too expensive to be viable) some bad stuff would still get through due to mistakes or laziness. Without at least some form of Section 230, the internet as we know it cannot exist.</div><br/></div></div></div></div></div></div><div id="41398305" class="c"><input type="checkbox" id="c-41398305" checked=""/><div class="controls bullet"><span class="by">raxxorraxor</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41393322">prev</a><span>|</span><a href="#41398673">next</a><span>|</span><label class="collapse" for="c-41398305">[-]</label><label class="expand" for="c-41398305">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In a very general sense, this ruling could be seen as a form of net neutrality<p>In reality this will not be the case and instead it will introduce the bias of regulators to replace the bias companies want there to be. And even with their motivation to sell users attention, I cannot see this as an improvement. No, the result will probably be worse.</div><br/></div></div><div id="41398673" class="c"><input type="checkbox" id="c-41398673" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41398305">prev</a><span>|</span><a href="#41395722">next</a><span>|</span><label class="collapse" for="c-41398673">[-]</label><label class="expand" for="c-41398673">[1 more]</label></div><br/><div class="children"><div class="content">We should just ditch advertisements as a monetization model, and see what happens.</div><br/></div></div><div id="41395722" class="c"><input type="checkbox" id="c-41395722" checked=""/><div class="controls bullet"><span class="by">EasyMark</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41398673">prev</a><span>|</span><a href="#41393315">next</a><span>|</span><label class="collapse" for="c-41395722">[-]</label><label class="expand" for="c-41395722">[1 more]</label></div><br/><div class="children"><div class="content">I think HN sees this as just more activist judges trying to overrule the will of the people (via Congress). This judge is attempting to interject his opinion on the way things should be vs what a law passed by the highest legislative body in the nation as if that doesn’t count. He is also doing it on very shaky ground, but I wouldn’t expect anything less of the 3rd circuit (much like the 5th)</div><br/></div></div><div id="41393315" class="c"><input type="checkbox" id="c-41393315" checked=""/><div class="controls bullet"><span class="by">kstrauser</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41395722">prev</a><span>|</span><a href="#41397752">next</a><span>|</span><label class="collapse" for="c-41393315">[-]</label><label class="expand" for="c-41393315">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Social media&quot; is a broad brush though. I operate a Mastodon instance with a few thousand users. Our content timeline algorithm is &quot;newest on top&quot;. Our moderation is heavily tailored to the users on my instance, and if a user says something grossly out of line with our general vibe, we&#x27;ll remove them. That user is free to create an account on any other server who&#x27;ll have them. We&#x27;re not limiting their access to Mastodon. We&#x27;re saying that we don&#x27;t want their stuff on our own server.<p>What are the legal ramifications for the many thousands of similar operators which are much closer in feel to a message board than to Facebook or Twitter? Does a server run by Republicans have to accept Communist Party USA members and their posts? Does a vegan instance have to allow beef farmers? A PlayStation fan server host pro-PC content?</div><br/><div id="41393986" class="c"><input type="checkbox" id="c-41393986" checked=""/><div class="controls bullet"><span class="by">dudus</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393315">parent</a><span>|</span><a href="#41397752">next</a><span>|</span><label class="collapse" for="c-41393986">[-]</label><label class="expand" for="c-41393986">[1 more]</label></div><br/><div class="children"><div class="content">You are directly responsible for everything they say and legally liable for any damages it may cause. Or not IANAL</div><br/></div></div></div></div><div id="41397752" class="c"><input type="checkbox" id="c-41397752" checked=""/><div class="controls bullet"><span class="by">gorgoiler</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41393315">prev</a><span>|</span><a href="#41397851">next</a><span>|</span><label class="collapse" for="c-41397752">[-]</label><label class="expand" for="c-41397752">[1 more]</label></div><br/><div class="children"><div class="content">For the case in question the major problem seems to be, specifically, what content do we allow children to access.<p>There’s an enormous difference in the debate between what should be prohibited and what should be prohibited <i>for children</i>.</div><br/></div></div><div id="41397851" class="c"><input type="checkbox" id="c-41397851" checked=""/><div class="controls bullet"><span class="by">amarant</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41397752">prev</a><span>|</span><a href="#41397572">next</a><span>|</span><label class="collapse" for="c-41397851">[-]</label><label class="expand" for="c-41397851">[2 more]</label></div><br/><div class="children"><div class="content">But what are the implications?<p>No more moderation? This seems bad.<p>No more recommendation&#x2F;personalization? This could go either way, I&#x27;m also willing to see where this one goes.<p>No more public comment sections? Arstechnica claimed back in the day when section 230 was under fire last time that this would be the result if it was ever taken away. This seems bad.<p>I&#x27;m not sure what will happen, I see 2 possible outcomes that are bad and one that is maybe good. At first glance this seems like bad odds.<p>Actually there&#x27;s a fourth possibility, and that&#x27;s holding Google responsible for whatever links they find for you. This is the nuclear option. If this happens, the internet will have to shut all of its American offices to get around this law.</div><br/><div id="41397881" class="c"><input type="checkbox" id="c-41397881" checked=""/><div class="controls bullet"><span class="by">jacoblambda</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41397851">parent</a><span>|</span><a href="#41397572">next</a><span>|</span><label class="collapse" for="c-41397881">[-]</label><label class="expand" for="c-41397881">[1 more]</label></div><br/><div class="children"><div class="content">Would bluesky not solve this issue?<p>The underlying hosted service is nearly completely unmoderated and unpersonalised. It&#x27;s just streams of bits and data routing. You can scan for&#x2F;limit the propagation of CSAM or DMCA content to some degree as an infrastructure provider but that&#x27;s really about it and even then you can only really do so to fairly limited degrees and that doesn&#x27;t stop other providers (or self hosted participants) from propagating that anyways.<p>Then you provide custom feed algorithms, labelling services, moderation services, etc on top of that but none of them change or control the underlying data streams. They just annotate on top or provide options to the client.<p>Then the user&#x27;s client is the one that directly consumes all these different services on top of the base service to produce the end result.<p>It&#x27;s a true, unbiased section 230 compatible protocol (under even the strictest interpretation) that the user then can optionally combine with any number of secondary services and addons that they use to craft their personalised social media experience.</div><br/></div></div></div></div><div id="41397572" class="c"><input type="checkbox" id="c-41397572" checked=""/><div class="controls bullet"><span class="by">IG_Semmelweiss</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41397851">prev</a><span>|</span><a href="#41393645">next</a><span>|</span><label class="collapse" for="c-41397572">[-]</label><label class="expand" for="c-41397572">[2 more]</label></div><br/><div class="children"><div class="content">I always wondered why Section 230 does not have a carve-out exemption to deal with the censorship issue.<p>I think we&#x27;d all agree that most websites are better off with curation and moderation of some kind. If you don&#x27;t like it, you are free to leave the forum, website, etc. The problem is that Big Tech fails to work in the same way, because those properties are becoming effectively the &quot;public highways&quot; where everyone must pass by.<p>This is not dissimilar from say, public utilities.<p>So, why not define how a tech company becomes a Big Tech &quot;utility&quot;, and therefore, cannot hide behind 230 exception for things that it willingly does, like censorship ?</div><br/><div id="41397693" class="c"><input type="checkbox" id="c-41397693" checked=""/><div class="controls bullet"><span class="by">itsdrewmiller</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41397572">parent</a><span>|</span><a href="#41393645">next</a><span>|</span><label class="collapse" for="c-41397693">[-]</label><label class="expand" for="c-41397693">[1 more]</label></div><br/><div class="children"><div class="content">Wonder no longer!  It&#x27;s Section 230 of the communications &quot;decency&quot; act, not the communication freedoms and regulations act.  It doesn&#x27;t talk about censorship because that wasn&#x27;t in the scope of the bill.  (And actually it does talk about censorship of obscene material in order to explicitly encourage it.)</div><br/></div></div></div></div><div id="41393645" class="c"><input type="checkbox" id="c-41393645" checked=""/><div class="controls bullet"><span class="by">ryandrake</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41397572">prev</a><span>|</span><a href="#41392941">next</a><span>|</span><label class="collapse" for="c-41393645">[-]</label><label class="expand" for="c-41393645">[2 more]</label></div><br/><div class="children"><div class="content">I look at forums and social media as analogous to writing a &quot;Letter to the Editor&quot; to a newspaper:<p>In the newspaper case, you write your post, send it to the newspaper, and some editor at the newspaper decides whether or not to publish it.<p>In Social Media, the same thing happens, but it&#x27;s just super fast and algorithmic: You write your post, send it to the Social Media site (or forum), an algorithm (or moderator) at the Social Media site decides whether or not to publish it.<p>I feel like it&#x27;s reasonable to interpret this kind of editorial selection as &quot;promotion&quot; and &quot;recommendation&quot; of that comment, particularly if the social media company&#x27;s algorithm deliberately places that content into someone&#x27;s feed.</div><br/><div id="41395026" class="c"><input type="checkbox" id="c-41395026" checked=""/><div class="controls bullet"><span class="by">jay_kyburz</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393645">parent</a><span>|</span><a href="#41392941">next</a><span>|</span><label class="collapse" for="c-41395026">[-]</label><label class="expand" for="c-41395026">[1 more]</label></div><br/><div class="children"><div class="content">I agree.<p>I think if social media companies relayed communication between it&#x27;s users with no moderation at all, then they should be entitled to carrier protections.<p>As soon as they start making any moderation decisions, they are implicitly endorsing all other content, and should therefore be held responsible for it.<p>There are two things social media can do. Firstly, they should accurately identify its users before allowing them to post, so they can counter sue that person if post harms them, and secondly, they can moderate every post.<p>Everybody says this will kill social media as we know it, but I say the world will be a better place as a result.</div><br/></div></div></div></div><div id="41392941" class="c"><input type="checkbox" id="c-41392941" checked=""/><div class="controls bullet"><span class="by">whatshisface</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41393645">prev</a><span>|</span><a href="#41392930">next</a><span>|</span><label class="collapse" for="c-41392941">[-]</label><label class="expand" for="c-41392941">[26 more]</label></div><br/><div class="children"><div class="content">The diverse biases of newspapers or social media sites are preferable to the monolithic bias a legal solution will impress.</div><br/><div id="41392970" class="c"><input type="checkbox" id="c-41392970" checked=""/><div class="controls bullet"><span class="by">nick238</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41392941">parent</a><span>|</span><a href="#41397729">next</a><span>|</span><label class="collapse" for="c-41392970">[-]</label><label class="expand" for="c-41392970">[22 more]</label></div><br/><div class="children"><div class="content">So the solution is &quot;more speech?&quot; I don&#x27;t know how that will unhook minors from the feedback loop of recommendation algorithms and their plastic brains. It&#x27;s like saying &#x27;we don&#x27;t need to put laws in place to combat heroin use, those people could go enjoy a good book instead!&#x27;.</div><br/><div id="41393197" class="c"><input type="checkbox" id="c-41393197" checked=""/><div class="controls bullet"><span class="by">nostrademons</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41392970">parent</a><span>|</span><a href="#41393231">next</a><span>|</span><label class="collapse" for="c-41393197">[-]</label><label class="expand" for="c-41393197">[16 more]</label></div><br/><div class="children"><div class="content">Yes, the solution is more speech.  Teach your kids critical thinking or they will be fodder for somebody else who has it.  That happens regardless of who&#x27;s in charge, government or private companies.  If you can&#x27;t think for yourself and synthesize lots of disparate information, somebody else will do the thinking for you.</div><br/><div id="41394115" class="c"><input type="checkbox" id="c-41394115" checked=""/><div class="controls bullet"><span class="by">chongli</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393197">parent</a><span>|</span><a href="#41393343">next</a><span>|</span><label class="collapse" for="c-41394115">[-]</label><label class="expand" for="c-41394115">[8 more]</label></div><br/><div class="children"><div class="content">You&#x27;re mistaken as to what this ruling is about. Ultimately, when it comes right down to it, the Third Circuit is saying this (directed at social media companies):<p>&quot;The speech is either wholly your speech or wholly someone else&#x27;s. You can&#x27;t have it both ways.&quot;<p>Either they get to act as a common carrier (telephone companies are not liable for what you say on a phone call because it is wholly your own speech and they are merely carrying it) or they act as a publisher (liable for everything said on their platforms because they are exercising editorial control via algorithm). If this ruling is upheld by the Supreme Court, then they will have to choose:<p>* Either claim the safe harbour protections afforded to common carriers and lose the ability to curate algorithmically<p>or<p>* Claim the free speech protections of the First Amendment but be liable for all content as it is their own speech.</div><br/><div id="41394727" class="c"><input type="checkbox" id="c-41394727" checked=""/><div class="controls bullet"><span class="by">whatshisface</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394115">parent</a><span>|</span><a href="#41393343">next</a><span>|</span><label class="collapse" for="c-41394727">[-]</label><label class="expand" for="c-41394727">[7 more]</label></div><br/><div class="children"><div class="content">Algorithmic libel detectors don&#x27;t exist. The second option isn&#x27;t possible. The result will be the separation of search and recommendation engines from social media platforms. Since there&#x27;s effectively one search company in each national protectionist bloc, the result will be the creation of several new monopolies that hold the power to decide what news is front-page, and what is buried or practically unavailable. In the English-speaking world that right would go to Alphabet.</div><br/><div id="41396084" class="c"><input type="checkbox" id="c-41396084" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394727">parent</a><span>|</span><a href="#41395049">next</a><span>|</span><label class="collapse" for="c-41396084">[-]</label><label class="expand" for="c-41396084">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Algorithmic libel detectors don&#x27;t exist<p>Automatic libel generators, on the other hand, are mych closer at hand. :p<p><a href="https:&#x2F;&#x2F;papers.ssrn.com&#x2F;sol3&#x2F;papers.cfm?abstract_id=4546063" rel="nofollow">https:&#x2F;&#x2F;papers.ssrn.com&#x2F;sol3&#x2F;papers.cfm?abstract_id=4546063</a></div><br/></div></div><div id="41395049" class="c"><input type="checkbox" id="c-41395049" checked=""/><div class="controls bullet"><span class="by">chongli</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394727">parent</a><span>|</span><a href="#41396084">prev</a><span>|</span><a href="#41393343">next</a><span>|</span><label class="collapse" for="c-41395049">[-]</label><label class="expand" for="c-41395049">[5 more]</label></div><br/><div class="children"><div class="content">The second option isn’t really meant for social media anyway. It’s meant for traditional publishers such as newspapers.<p>If this goes through I don’t think it will be such a big boost for Google search as you suggest. For one thing, it has no effect on OpenAI and other LLM providers. That’s a real problem for Google, as I see a long term trend away from traditional search and towards LLMs for getting questions answered, especially among young people. Also note that YouTube is social media and features a curation algorithm to deliver personalized content feeds.<p>As for social media, I think we’re better off without it! There’s countless stories in the news about all the damage it’s causing to society. I don’t think we’ll be able to roll all that back but I hope we’ll be able to make things better.</div><br/><div id="41395119" class="c"><input type="checkbox" id="c-41395119" checked=""/><div class="controls bullet"><span class="by">whatshisface</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395049">parent</a><span>|</span><a href="#41393343">next</a><span>|</span><label class="collapse" for="c-41395119">[-]</label><label class="expand" for="c-41395119">[4 more]</label></div><br/><div class="children"><div class="content">If the ruling was upheld, Google wouldn&#x27;t gain any new liability for putting a TikTok-like frontend on video search results; the only reason they&#x27;re not doing it now is that all existing platforms (including YouTube) funnel all the recommendation clicks back into themselves. If YouTube had to stop offering recommendations, Google could take over their user experience and spin them off into a hosting company that derived its revenue from AdSense and its traffic from &quot;Google Shorts.&quot;<p>This ruling is not a ban on algorithms, it&#x27;s a ban on the vertical integration between search or recommendation and hosting that today makes it possible for search engines other than Google to see traffic.</div><br/><div id="41396778" class="c"><input type="checkbox" id="c-41396778" checked=""/><div class="controls bullet"><span class="by">chongli</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41395119">parent</a><span>|</span><a href="#41393343">next</a><span>|</span><label class="collapse" for="c-41396778">[-]</label><label class="expand" for="c-41396778">[3 more]</label></div><br/><div class="children"><div class="content">I actually don&#x27;t think Google search will be protected in its current form. Google doesn&#x27;t show you unadulterated search results anymore, they personalize (read: editorialize) the results based on the data they&#x27;ve collected on you, the user. This is why two different people entering the same query can see dramatically different results.<p>If Google wants to preserve their safe harbour protections they&#x27;ll need to roll back to a neutral algorithm that delivers the same results to everyone given an identical query. This won&#x27;t be the end of the world for Google but it will produce lower quality results (at least in the eyes of normal users who aren&#x27;t annoyed by the personalization). Lower quality results will further open the doors to LLMs as a competitor to search.</div><br/><div id="41396884" class="c"><input type="checkbox" id="c-41396884" checked=""/><div class="controls bullet"><span class="by">whatshisface</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396778">parent</a><span>|</span><a href="#41393343">next</a><span>|</span><label class="collapse" for="c-41396884">[-]</label><label class="expand" for="c-41396884">[2 more]</label></div><br/><div class="children"><div class="content">Newspapers editorialize and also give the same results to everybody.</div><br/><div id="41397500" class="c"><input type="checkbox" id="c-41397500" checked=""/><div class="controls bullet"><span class="by">chongli</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41396884">parent</a><span>|</span><a href="#41393343">next</a><span>|</span><label class="collapse" for="c-41397500">[-]</label><label class="expand" for="c-41397500">[1 more]</label></div><br/><div class="children"><div class="content">And newspapers decide every single word they publish, because they’re liable for it. If a newspaper defames someone they can be sued.<p>This whole case comes down to having your cake and eating it too. Newspapers don’t have that. They have free speech protections but they aren’t absolved of liability for what they publish. They aren’t protected under section 230.<p>If the ruling is upheld by SCOTUS, Google will have to choose: section 230 (and no editorial control) or first amendment plus liability for everything they publish on SERPs.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="41393343" class="c"><input type="checkbox" id="c-41393343" checked=""/><div class="controls bullet"><span class="by">kelnos</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393197">parent</a><span>|</span><a href="#41394115">prev</a><span>|</span><a href="#41393447">next</a><span>|</span><label class="collapse" for="c-41393343">[-]</label><label class="expand" for="c-41393343">[2 more]</label></div><br/><div class="children"><div class="content">Solution that require everyone to do a thing, and do it well, are doomed to fail.<p>Yes, it would be great if parents would, universally, parent better, but getting all of them (or a large enough portion of them for it to make a difference) to do so is essentially impossible.</div><br/><div id="41393401" class="c"><input type="checkbox" id="c-41393401" checked=""/><div class="controls bullet"><span class="by">nostrademons</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393343">parent</a><span>|</span><a href="#41393447">next</a><span>|</span><label class="collapse" for="c-41393401">[-]</label><label class="expand" for="c-41393401">[1 more]</label></div><br/><div class="children"><div class="content">Government controls aren&#x27;t a solution either though.  The people with critical thinking skills, who can effectively tell others what to think, simply capture the government.  Meet the new boss, same as the old boss.</div><br/></div></div></div></div><div id="41393447" class="c"><input type="checkbox" id="c-41393447" checked=""/><div class="controls bullet"><span class="by">jrockway</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393197">parent</a><span>|</span><a href="#41393343">prev</a><span>|</span><a href="#41393522">next</a><span>|</span><label class="collapse" for="c-41393447">[-]</label><label class="expand" for="c-41393447">[2 more]</label></div><br/><div class="children"><div class="content">I agree with this.  Kids are already subject to an agenda; for example, never once in my K-12 education did I learn anything about sex.  This was because it was politically controversial at the time (and maybe it still is now), so my school district just avoided the issue entirely.<p>I remember my mom being so mad about the curriculum in general that she ran for the school board and won.  (I believe it was more of a math and science type thing.  She was upset with how many coloring assignments I had.  Frankly, I completely agreed with her then and I do now.)</div><br/><div id="41393605" class="c"><input type="checkbox" id="c-41393605" checked=""/><div class="controls bullet"><span class="by">nostrademons</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393447">parent</a><span>|</span><a href="#41393522">next</a><span>|</span><label class="collapse" for="c-41393605">[-]</label><label class="expand" for="c-41393605">[1 more]</label></div><br/><div class="children"><div class="content">I was lucky enough to go to a charter school where my teachers encouraged me to read books like &quot;People&#x27;s History of the U.S&quot; and &quot;Lies My Teacher Told Me&quot;.  They have an agenda too, but understanding that there&#x27;s a whole world of disagreement out there and that I should seek out multiple information sources and triangulate between them has been a huge superpower since.  It&#x27;s pretty shocking to understand the history of public education and realize that it wasn&#x27;t created to benefit the student, but to benefit the future employers of those students.</div><br/></div></div></div></div><div id="41393522" class="c"><input type="checkbox" id="c-41393522" checked=""/><div class="controls bullet"><span class="by">wvenable</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393197">parent</a><span>|</span><a href="#41393447">prev</a><span>|</span><a href="#41393533">next</a><span>|</span><label class="collapse" for="c-41393522">[-]</label><label class="expand" for="c-41393522">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Yes, the solution is more speech.<p>I think we&#x27;ve reached the point now that there is more speech than any person can consume by a factor of a million.  It now comes to down to picking what speech you want to hear.  This is exactly what content algorithms are doing -&gt; out of the millions of hours of speech produced in a day, it&#x27;s giving you your 24 hours of it.<p>Saying &quot;teach your kids critical thinking&quot; is <i>a</i> solution but it&#x27;s not <i>the</i> solution.  At some point, you have to <i>discover</i> content out of those millions or hours a day.  It&#x27;s impossible to do yourself -- it&#x27;s always going to be curated.<p>EDIT: To whomever downvoted this comment, you made my point.  You should have replied instead.</div><br/></div></div><div id="41393533" class="c"><input type="checkbox" id="c-41393533" checked=""/><div class="controls bullet"><span class="by">forgetfreeman</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393197">parent</a><span>|</span><a href="#41393522">prev</a><span>|</span><a href="#41395472">next</a><span>|</span><label class="collapse" for="c-41393533">[-]</label><label class="expand" for="c-41393533">[1 more]</label></div><br/><div class="children"><div class="content">K so several of the most well-funded tech companies on the planet sink literally billions of dollars into psyops research to reinforce addictive behavior and average parents are expected to successfully compete against it with...a lecture.</div><br/></div></div><div id="41395472" class="c"><input type="checkbox" id="c-41395472" checked=""/><div class="controls bullet"><span class="by">bsder</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393197">parent</a><span>|</span><a href="#41393533">prev</a><span>|</span><a href="#41393231">next</a><span>|</span><label class="collapse" for="c-41395472">[-]</label><label class="expand" for="c-41395472">[1 more]</label></div><br/><div class="children"><div class="content">We have seen that adults can&#x27;t seem to unhook from these dopamine delivery systems and you&#x27;re expecting that children can do so?<p>Sorry.  That&#x27;s simply disingenuous.<p>Yes, children and especially teenagers do lots of things even though their parents try to prevent them from doing so.  Even if children and teenagers still get them, we don&#x27;t throw up our hands and sell them tobacco and alcohol anyway.</div><br/></div></div></div></div><div id="41393231" class="c"><input type="checkbox" id="c-41393231" checked=""/><div class="controls bullet"><span class="by">aeternum</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41392970">parent</a><span>|</span><a href="#41393197">prev</a><span>|</span><a href="#41397729">next</a><span>|</span><label class="collapse" for="c-41393231">[-]</label><label class="expand" for="c-41393231">[5 more]</label></div><br/><div class="children"><div class="content">Open-source the algorithm and have users choose.  A marketplace is the best solution to most problems.<p>It is pretty clear that china already forces a very different tiktok ranking algo for kids within the country vs outside the country.  Forcing a single algo is pretty unamerican though and can easily be abused, let&#x27;s instead open it up.</div><br/><div id="41393354" class="c"><input type="checkbox" id="c-41393354" checked=""/><div class="controls bullet"><span class="by">kelnos</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393231">parent</a><span>|</span><a href="#41395505">next</a><span>|</span><label class="collapse" for="c-41393354">[-]</label><label class="expand" for="c-41393354">[3 more]</label></div><br/><div class="children"><div class="content">80% of users will leave things at the default setting, or &quot;choose&quot; whatever the first thing in the list is.  They won&#x27;t understand the options; they&#x27;ll just want to see their news feed.</div><br/><div id="41394598" class="c"><input type="checkbox" id="c-41394598" checked=""/><div class="controls bullet"><span class="by">aeternum</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393354">parent</a><span>|</span><a href="#41395505">next</a><span>|</span><label class="collapse" for="c-41394598">[-]</label><label class="expand" for="c-41394598">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not so sure, the feed is quite important and users understand that.  Look at how many people switched between X and Threads given their political view.  People switched off Reddit or cancelled their FB account at times in the past also.</div><br/><div id="41394778" class="c"><input type="checkbox" id="c-41394778" checked=""/><div class="controls bullet"><span class="by">kfajdsl</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41394598">parent</a><span>|</span><a href="#41395505">next</a><span>|</span><label class="collapse" for="c-41394778">[-]</label><label class="expand" for="c-41394778">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty sure going from X to Threads had very little to do with the feed algorithm for most people. It had everything to do with one platform being run by Musk and the other one not.</div><br/></div></div></div></div></div></div><div id="41395505" class="c"><input type="checkbox" id="c-41395505" checked=""/><div class="controls bullet"><span class="by">mindslight</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41393231">parent</a><span>|</span><a href="#41393354">prev</a><span>|</span><a href="#41397729">next</a><span>|</span><label class="collapse" for="c-41395505">[-]</label><label class="expand" for="c-41395505">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Open-source the algorithm&quot; would be at best openwashing. The way to create the type of choice you&#x27;re thinking is to force the unbundling of client software from hosting services.</div><br/></div></div></div></div></div></div><div id="41397729" class="c"><input type="checkbox" id="c-41397729" checked=""/><div class="controls bullet"><span class="by">itsdrewmiller</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41392941">parent</a><span>|</span><a href="#41392970">prev</a><span>|</span><a href="#41393121">next</a><span>|</span><label class="collapse" for="c-41397729">[-]</label><label class="expand" for="c-41397729">[1 more]</label></div><br/><div class="children"><div class="content">Newspaper biases are more diverse despite being subject to the liability social media companies are trying to escape.</div><br/></div></div><div id="41393121" class="c"><input type="checkbox" id="c-41393121" checked=""/><div class="controls bullet"><span class="by">mathgradthrow</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41392941">parent</a><span>|</span><a href="#41397729">prev</a><span>|</span><a href="#41393185">next</a><span>|</span><label class="collapse" for="c-41393121">[-]</label><label class="expand" for="c-41393121">[1 more]</label></div><br/><div class="children"><div class="content">Seems like the bias will be against manipulative algorithms. How does tiktok escape liability here? They give control of what is promoted to users to users.</div><br/></div></div><div id="41393185" class="c"><input type="checkbox" id="c-41393185" checked=""/><div class="controls bullet"><span class="by">danaris</span><span>|</span><a href="#41392867">root</a><span>|</span><a href="#41392941">parent</a><span>|</span><a href="#41393121">prev</a><span>|</span><a href="#41392930">next</a><span>|</span><label class="collapse" for="c-41393185">[-]</label><label class="expand" for="c-41393185">[1 more]</label></div><br/><div class="children"><div class="content">Unfortunately, the biases of newspapers and social media sites are only diverse if they are not all under the strong influence of the wealthy.<p>Even if they may have different skews on some issues, under a system where <i>all</i> such entities are operated entirely for-profit, they will tend to converge on other issues, largely related to maintaining the rights of capital over labor and over government.</div><br/></div></div></div></div><div id="41392930" class="c"><input type="checkbox" id="c-41392930" checked=""/><div class="controls bullet"><span class="by">WCSTombs</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41392941">prev</a><span>|</span><a href="#41395561">next</a><span>|</span><label class="collapse" for="c-41392930">[-]</label><label class="expand" for="c-41392930">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, pretty much. What&#x27;s not clear to me though is how <i>non-targeted</i> content curation, like simply &quot;trending videos&quot; or &quot;related videos&quot; on YouTube, is impacted. IMO that&#x27;s not nearly as problematic and can be useful.</div><br/></div></div><div id="41395561" class="c"><input type="checkbox" id="c-41395561" checked=""/><div class="controls bullet"><span class="by">shadowgovt</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41392930">prev</a><span>|</span><a href="#41396838">next</a><span>|</span><label class="collapse" for="c-41395561">[-]</label><label class="expand" for="c-41395561">[1 more]</label></div><br/><div class="children"><div class="content">HN also has an algorithm.<p>I&#x27;ll have to read the third circuit&#x27;s ruling in detail to figure out whether they are trying to draw a line in the Sand on whether an algorithm satisfies the requirements for section 230 protection or falls outside of it. If that&#x27;s what they&#x27;re doing, I wouldn&#x27;t assume a priori that a site like Hacker News won&#x27;t also fall afoul of the law.</div><br/></div></div><div id="41396838" class="c"><input type="checkbox" id="c-41396838" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41395561">prev</a><span>|</span><a href="#41395555">next</a><span>|</span><label class="collapse" for="c-41396838">[-]</label><label class="expand" for="c-41396838">[1 more]</label></div><br/><div class="children"><div class="content">Threads is actually pretty good if you ruthlessly block people that you dislike.</div><br/></div></div><div id="41395555" class="c"><input type="checkbox" id="c-41395555" checked=""/><div class="controls bullet"><span class="by">bsder</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41396838">prev</a><span>|</span><a href="#41396184">next</a><span>|</span><label class="collapse" for="c-41395555">[-]</label><label class="expand" for="c-41395555">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I think the ultimate problem is that social media is not unbiased — it curates what people are shown.<p>It is not only <i>biased</i> but also <i>biased for maximum engagement</i>.<p>People come to these services for various reasons but then have this <i>specifically biased</i> stuff jammed down their throats in a way to induce <i>specific behavior</i>.<p>I personally don&#x27;t understand why we don&#x27;t hammer these social media sites for conducting psychological experiments without consent.</div><br/></div></div><div id="41396184" class="c"><input type="checkbox" id="c-41396184" checked=""/><div class="controls bullet"><span class="by">smrtinsert</span><span>|</span><a href="#41392867">parent</a><span>|</span><a href="#41395555">prev</a><span>|</span><a href="#41393921">next</a><span>|</span><label class="collapse" for="c-41396184">[-]</label><label class="expand" for="c-41396184">[1 more]</label></div><br/><div class="children"><div class="content">This is a much needed regulation.  If anything it will probably spur innovation to solve safety in algorithms.<p>I think of this more along the lines of preventing a factoring from polluting a water supply or requiring a bank to have minimum reserves.</div><br/></div></div></div></div><div id="41393921" class="c"><input type="checkbox" id="c-41393921" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#41392867">prev</a><span>|</span><a href="#41393713">next</a><span>|</span><label class="collapse" for="c-41393921">[-]</label><label class="expand" for="c-41393921">[6 more]</label></div><br/><div class="children"><div class="content">This turns on what TikTok &quot;knew&quot;:<p><i>&quot;But by the time Nylah viewed these
videos, TikTok knew that: 1) “the deadly Blackout Challenge
was spreading through its app,” 2) “its algorithm was
specifically feeding the Blackout Challenge to children,” and
3) several children had died while attempting the Blackout
Challenge after viewing videos of the Challenge on their For
You Pages. App. 31–32. Yet TikTok “took no and&#x2F;or
completely inadequate action to extinguish and prevent the
spread of the Blackout Challenge and specifically to prevent
the Blackout Challenge from being shown to children on their
[For You Pages].” App. 32–33. Instead, TikTok continued to
recommend these videos to children like Nylah.</i>&quot;<p>We need to see another document, &quot;App 31-32&quot;, to see what TikTok &quot;knew&quot;. Could someone find that, please? A Pacer account may be required. Did they ignore an abuse report?<p>See also Gonzales vs. Google (2023), where a similar issue reached the U.S. Supreme Court.[1] That was
about whether recommending videos which encouraged the viewer to support the Islamic State&#x27;s jihad led someone to go fight in it, where they were killed. The Court rejected the terrorism claim and declined to address the Section 230 claim.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Gonzalez_v._Google_LLC" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Gonzalez_v._Google_LLC</a></div><br/><div id="41394409" class="c"><input type="checkbox" id="c-41394409" checked=""/><div class="controls bullet"><span class="by">Scaevolus</span><span>|</span><a href="#41393921">parent</a><span>|</span><a href="#41397690">next</a><span>|</span><label class="collapse" for="c-41394409">[-]</label><label class="expand" for="c-41394409">[3 more]</label></div><br/><div class="children"><div class="content">IIRC, TikTok has (had?) a relatively high-touch content moderation pipeline, where any video receiving more than a few thousand views is checked by a human reviewer.<p>Their review process was developed to hit the much more stringent speech standards of the Chinese market, but it opens them up to even more liability here.<p>I unfortunately can&#x27;t find the source articles for this any more, they&#x27;re buried under &quot;how to make your video go viral&quot; flowcharts that elide the &quot;when things get banned&quot; decisions.</div><br/><div id="41396907" class="c"><input type="checkbox" id="c-41396907" checked=""/><div class="controls bullet"><span class="by">Izkata</span><span>|</span><a href="#41393921">root</a><span>|</span><a href="#41394409">parent</a><span>|</span><a href="#41397690">next</a><span>|</span><label class="collapse" for="c-41396907">[-]</label><label class="expand" for="c-41396907">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Their review process was developed to hit the much more stringent speech standards of the Chinese market<p>TikTok isn&#x27;t available in China.  They have a separate app called Douyin.</div><br/><div id="41398453" class="c"><input type="checkbox" id="c-41398453" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#41393921">root</a><span>|</span><a href="#41396907">parent</a><span>|</span><a href="#41397690">next</a><span>|</span><label class="collapse" for="c-41398453">[-]</label><label class="expand" for="c-41398453">[1 more]</label></div><br/><div class="children"><div class="content">They are saying that the reason TikTok also has high-touch moderation is that it grew out of Douyin.</div><br/></div></div></div></div></div></div><div id="41397690" class="c"><input type="checkbox" id="c-41397690" checked=""/><div class="controls bullet"><span class="by">qingcharles</span><span>|</span><a href="#41393921">parent</a><span>|</span><a href="#41394409">prev</a><span>|</span><a href="#41394643">next</a><span>|</span><label class="collapse" for="c-41397690">[-]</label><label class="expand" for="c-41397690">[1 more]</label></div><br/><div class="children"><div class="content">Should be pages A31-32 in this appendix for the appellant&#x27;s brief I think:<p>&quot;The TikTok Defendants Knew the Deadly Blackout Challenge Had Killed 
Multiple Children&quot;<p><a href="https:&#x2F;&#x2F;storage.courtlistener.com&#x2F;recap&#x2F;gov.uscourts.ca3.118730&#x2F;gov.uscourts.ca3.118730.20.0.pdf" rel="nofollow">https:&#x2F;&#x2F;storage.courtlistener.com&#x2F;recap&#x2F;gov.uscourts.ca3.118...</a><p>Rest of the filings here:<p><a href="https:&#x2F;&#x2F;www.courtlistener.com&#x2F;docket&#x2F;67500541&#x2F;tawainna-anderson-v-tiktok-inc&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.courtlistener.com&#x2F;docket&#x2F;67500541&#x2F;tawainna-ander...</a></div><br/></div></div><div id="41394643" class="c"><input type="checkbox" id="c-41394643" checked=""/><div class="controls bullet"><span class="by">itsdrewmiller</span><span>|</span><a href="#41393921">parent</a><span>|</span><a href="#41397690">prev</a><span>|</span><a href="#41393713">next</a><span>|</span><label class="collapse" for="c-41394643">[-]</label><label class="expand" for="c-41394643">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think any of that actually matters for the CDA liability question, but it is definitely material in whether they are found guilty assuming they can be held liable at all.</div><br/></div></div></div></div><div id="41393713" class="c"><input type="checkbox" id="c-41393713" checked=""/><div class="controls bullet"><span class="by">delichon</span><span>|</span><a href="#41393921">prev</a><span>|</span><a href="#41396307">next</a><span>|</span><label class="collapse" for="c-41393713">[-]</label><label class="expand" for="c-41393713">[34 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  TikTok, Inc., via its algorithm, recommended and promoted videos posted by third parties to ten-year-old Nylah Anderson on her uniquely curated “For You Page.” One video depicted the “Blackout Challenge,” which encourages viewers to record themselves engaging in acts of self-asphyxiation. After watching the video, Nylah attempted the conduct depicted in the challenge and unintentionally hanged herself. -- https:&#x2F;&#x2F;cases.justia.com&#x2F;federal&#x2F;appellate-courts&#x2F;ca3&#x2F;22-3061&#x2F;22-3061-2024-08-27.pdf?ts=1724792413
</code></pre>
An algorithm accidentally enticed a child to hang herself. I&#x27;ve got code running on dozens of websites that recommends articles to read based on user demographics. There&#x27;s nothing in that code that would or could prevent an article about self-asphyxiation being recommended to a child. It just depends on the clients that use the software not posting that kind of content, people with similar demographics to the child not reading it, and a child who gets the recommendation not reading it and acting it out. If those assumptions fail should I or my employer be liable?</div><br/><div id="41394620" class="c"><input type="checkbox" id="c-41394620" checked=""/><div class="controls bullet"><span class="by">mihaaly</span><span>|</span><a href="#41393713">parent</a><span>|</span><a href="#41394317">next</a><span>|</span><label class="collapse" for="c-41394620">[-]</label><label class="expand" for="c-41394620">[14 more]</label></div><br/><div class="children"><div class="content">Yes.<p>Or you do things that gives you rewards - and do not care what it will result otherwise - but you want to be saved from any responsibility (automatically!) for what it causes just because it is an algorithm?<p>The enjoying the benefits but running away from responsibility is a cowardly and childish act. Childish acts need supervision from adults.</div><br/><div id="41396297" class="c"><input type="checkbox" id="c-41396297" checked=""/><div class="controls bullet"><span class="by">anigbrowl</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41394620">parent</a><span>|</span><a href="#41396928">next</a><span>|</span><label class="collapse" for="c-41396297">[-]</label><label class="expand" for="c-41396297">[1 more]</label></div><br/><div class="children"><div class="content">You seem to be overlooking the fact of the late plaintiff being 10 years old. The case turns on whether it&#x27;s reasonable to expect that Tiktok would knowingly share content encouraging users to attempt life-threatening activities to children.</div><br/></div></div><div id="41396928" class="c"><input type="checkbox" id="c-41396928" checked=""/><div class="controls bullet"><span class="by">Nasrudith</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41394620">parent</a><span>|</span><a href="#41396297">prev</a><span>|</span><a href="#41395772">next</a><span>|</span><label class="collapse" for="c-41396928">[-]</label><label class="expand" for="c-41396928">[2 more]</label></div><br/><div class="children"><div class="content">You want to bake cookies yet refuse to take responsibility for the possibility of somebody choking on them, or sell cars without maling crashes impossible!<p>Impossible goals are an asinine standard and &quot;responsibility&quot; and &quot;accountability&quot; are the favorite weasel words of those who want absolute discretion to abuse power.</div><br/><div id="41398229" class="c"><input type="checkbox" id="c-41398229" checked=""/><div class="controls bullet"><span class="by">BriggyDwiggs42</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41396928">parent</a><span>|</span><a href="#41395772">next</a><span>|</span><label class="collapse" for="c-41398229">[-]</label><label class="expand" for="c-41398229">[1 more]</label></div><br/><div class="children"><div class="content">Aren’t there lots of regulations on safety standards for food and for cars? I think you might have chosen the wrong examples.</div><br/></div></div></div></div><div id="41395772" class="c"><input type="checkbox" id="c-41395772" checked=""/><div class="controls bullet"><span class="by">EasyMark</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41394620">parent</a><span>|</span><a href="#41396928">prev</a><span>|</span><a href="#41394317">next</a><span>|</span><label class="collapse" for="c-41395772">[-]</label><label class="expand" for="c-41395772">[10 more]</label></div><br/><div class="children"><div class="content">What happened to that child is on the parents not some programmer who coded an optimization algorithm. It’s really as simple as that. No 10 year old should be on TikTok, I’m not sure anyone under 18 should be given the garbage, dangerous misinformation, intentional disinformation, and lack of any ability to control what your child sees.</div><br/><div id="41395848" class="c"><input type="checkbox" id="c-41395848" checked=""/><div class="controls bullet"><span class="by">itishappy</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41395772">parent</a><span>|</span><a href="#41394317">next</a><span>|</span><label class="collapse" for="c-41395848">[-]</label><label class="expand" for="c-41395848">[9 more]</label></div><br/><div class="children"><div class="content">Do you feel the same way about the sale of alcohol? I do see the argument for parental responsibility, but I&#x27;m not sure how parents will enforce that if the law allows people to sell kids alcohol free from liability.</div><br/><div id="41395928" class="c"><input type="checkbox" id="c-41395928" checked=""/><div class="controls bullet"><span class="by">tines</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41395848">parent</a><span>|</span><a href="#41395943">next</a><span>|</span><label class="collapse" for="c-41395928">[-]</label><label class="expand" for="c-41395928">[2 more]</label></div><br/><div class="children"><div class="content">This is a good argument I didn&#x27;t think of before. What&#x27;s the response to it?</div><br/><div id="41396153" class="c"><input type="checkbox" id="c-41396153" checked=""/><div class="controls bullet"><span class="by">Flozzin</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41395928">parent</a><span>|</span><a href="#41395943">next</a><span>|</span><label class="collapse" for="c-41396153">[-]</label><label class="expand" for="c-41396153">[1 more]</label></div><br/><div class="children"><div class="content">We regulate the sale of all sorts of things that can do damage but also have other uses. You can&#x27;t buy large amounts of certain cold medicines, and you need to be an adult to do so. You can&#x27;t buy fireworks if you are a minor in most places. In some countries they won&#x27;t even sell you a set of steak knives if you are underage.<p>Someone else&#x27;s response was that a 10 year old should not be on ticktoc. Well then how did they get past the age restrictions?(I&#x27;m guessing its a check box at best). So its inadequately gated. But really, I don&#x27;t think its the sort of thing that needs an age gate.<p>They are responsible for a product that is actively targeting harmful behavior at children and adults. It&#x27;s not ok in either situation. You cannot allow your platform to be hijacked for content like this. Full stop.<p>These &#x27;services&#x27; need better ways to moderate content. If that is more controls that allow them to delete certain posts and videos or some other method to contain videos like this. You cannot just allow users to upload and share whatever they want. And further, have your own systems promote these videos.<p>Everyone who makes a product(especially for mass consumption), has a responsibility to make sure their product is safe. If your product is so complicated that you can&#x27;t control it, then you need to step back and re-evaluate how it&#x27;s functioning. Not just plow ahead, making money, letting it harm people.</div><br/></div></div></div></div><div id="41395943" class="c"><input type="checkbox" id="c-41395943" checked=""/><div class="controls bullet"><span class="by">EasyMark</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41395848">parent</a><span>|</span><a href="#41395928">prev</a><span>|</span><a href="#41394317">next</a><span>|</span><label class="collapse" for="c-41395943">[-]</label><label class="expand" for="c-41395943">[6 more]</label></div><br/><div class="children"><div class="content">Alcohol (the consumption form)  serves only one purpose to get you buzzed. Unlike algorithms and hammers which are generic and serve many purposes, some of which are positive, especially when used correctly.  You can’t sue the people who make hammers if someone kills another person with one.</div><br/><div id="41396514" class="c"><input type="checkbox" id="c-41396514" checked=""/><div class="controls bullet"><span class="by">nmeagent</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41395943">parent</a><span>|</span><a href="#41396993">next</a><span>|</span><label class="collapse" for="c-41396514">[-]</label><label class="expand" for="c-41396514">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Alcohol (the consumption form) serves only one purpose to get you buzzed.<p>Since consumable alcohol has other legitimate uses besides getting a buzz on, I don&#x27;t think this point stands.  For example, it&#x27;s used quite often in cooking and (most of the time?) no intoxicating effects remain in the final product.</div><br/></div></div><div id="41396993" class="c"><input type="checkbox" id="c-41396993" checked=""/><div class="controls bullet"><span class="by">rfrey</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41395943">parent</a><span>|</span><a href="#41396514">prev</a><span>|</span><a href="#41396258">next</a><span>|</span><label class="collapse" for="c-41396993">[-]</label><label class="expand" for="c-41396993">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re not talking about &quot;all algorithms&quot; any more than the alcohol example is talking about &quot;all liquids&quot;. Social media algorithms have one purpose: to manipulate people into more engagement, to manoeuvre them into forgoing other activities in favour of more screen time, in the service of showing them more ads.</div><br/></div></div><div id="41396258" class="c"><input type="checkbox" id="c-41396258" checked=""/><div class="controls bullet"><span class="by">rbetts</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41395943">parent</a><span>|</span><a href="#41396993">prev</a><span>|</span><a href="#41398568">next</a><span>|</span><label class="collapse" for="c-41396258">[-]</label><label class="expand" for="c-41396258">[2 more]</label></div><br/><div class="children"><div class="content">You could sue a hammer manufacturer if they regularly advertised hammers as weapons to children and children started killing each other with them, though.</div><br/><div id="41396363" class="c"><input type="checkbox" id="c-41396363" checked=""/><div class="controls bullet"><span class="by">tines</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41396258">parent</a><span>|</span><a href="#41398568">next</a><span>|</span><label class="collapse" for="c-41396363">[-]</label><label class="expand" for="c-41396363">[1 more]</label></div><br/><div class="children"><div class="content">You said sue the hammer manufacturer. Why didn’t you say to sue the newspaper that ran the ads? The fact that you couldn’t keep that straight in your analogy undermines your argument significantly imo.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41394317" class="c"><input type="checkbox" id="c-41394317" checked=""/><div class="controls bullet"><span class="by">depingus</span><span>|</span><a href="#41393713">parent</a><span>|</span><a href="#41394620">prev</a><span>|</span><a href="#41394817">next</a><span>|</span><label class="collapse" for="c-41394317">[-]</label><label class="expand" for="c-41394317">[10 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t it usually the case that when someone builds a shitty thing and people get hurt, the builder is liable?</div><br/><div id="41395343" class="c"><input type="checkbox" id="c-41395343" checked=""/><div class="controls bullet"><span class="by">x0x0</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41394317">parent</a><span>|</span><a href="#41395862">prev</a><span>|</span><a href="#41394521">next</a><span>|</span><label class="collapse" for="c-41395343">[-]</label><label class="expand" for="c-41395343">[3 more]</label></div><br/><div class="children"><div class="content">You would think so, wouldn&#x27;t you?<p>Except right now youtube have a self advertisement in the middle of the page warning people <i>not to trust the content on youtube</i>.  A company warning people not to trust the product they built and the videos they choose to show you... we need to rethink 230.  We&#x27;ve gone seriously awry.</div><br/><div id="41395911" class="c"><input type="checkbox" id="c-41395911" checked=""/><div class="controls bullet"><span class="by">tines</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41395343">parent</a><span>|</span><a href="#41394521">next</a><span>|</span><label class="collapse" for="c-41395911">[-]</label><label class="expand" for="c-41395911">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more nuanced than that. If I sent a hateful letter through the mail and someone gets hurt by it (even physically), who is responsible, me or the post office?<p>I know youtube is different in important ways than the post, but it&#x27;s also different in important ways from e.g. somebody who builds a building that falls down.</div><br/><div id="41397784" class="c"><input type="checkbox" id="c-41397784" checked=""/><div class="controls bullet"><span class="by">cmrdporcupine</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41395911">parent</a><span>|</span><a href="#41394521">next</a><span>|</span><label class="collapse" for="c-41397784">[-]</label><label class="expand" for="c-41397784">[1 more]</label></div><br/><div class="children"><div class="content">The Post Office just delivers your mail, it doesn&#x27;t do any curation.<p>YouTube, TikTok, etc. differ by applying an algorithm to &quot;decide&quot; what to show you. Those algorithms have all sorts of weights and measures, but they&#x27;re ultimately <i>personalized</i> to you. And if they&#x27;re making <i>personalized</i> recommendations that include &quot;how to kill yourself&quot;... I think we have a problem?<p>It&#x27;s simply not just a FIFO of content in content out, and in many cases (Facebook &amp; Instagram especially) the user barely gets to a choice in what is shown in the feed...<p>Contrast with e.g. Mastodon where there is no algorithm and it only shows you what you explicitly followed, and in the exact linear order it was posted.<p>(Which is actually how Facebook <i>used to be</i>)</div><br/></div></div></div></div></div></div><div id="41394521" class="c"><input type="checkbox" id="c-41394521" checked=""/><div class="controls bullet"><span class="by">ineedaj0b</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41394317">parent</a><span>|</span><a href="#41395343">prev</a><span>|</span><a href="#41394817">next</a><span>|</span><label class="collapse" for="c-41394521">[-]</label><label class="expand" for="c-41394521">[5 more]</label></div><br/><div class="children"><div class="content">Yeah, but buying a hammer and hitting yourself with it is different.<p>The dangers of social media are unknown to most still.</div><br/><div id="41394641" class="c"><input type="checkbox" id="c-41394641" checked=""/><div class="controls bullet"><span class="by">depingus</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41394521">parent</a><span>|</span><a href="#41395518">next</a><span>|</span><label class="collapse" for="c-41394641">[-]</label><label class="expand" for="c-41394641">[1 more]</label></div><br/><div class="children"><div class="content">Yes. Buying a hammer and hitting yourself with it IS different.</div><br/></div></div><div id="41395518" class="c"><input type="checkbox" id="c-41395518" checked=""/><div class="controls bullet"><span class="by">ThunderSizzle</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41394521">parent</a><span>|</span><a href="#41394641">prev</a><span>|</span><a href="#41395374">next</a><span>|</span><label class="collapse" for="c-41395518">[-]</label><label class="expand" for="c-41395518">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s be more akin to buying a hammer and then the hammer starts morphing into a screw driver without you noticing.<p>Then when you accidentally hit your hand with the hammer, you actually stabbed yourself. And that&#x27;s when you realized your hammer is now a screwdriver.</div><br/><div id="41396261" class="c"><input type="checkbox" id="c-41396261" checked=""/><div class="controls bullet"><span class="by">ineedaj0b</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41395518">parent</a><span>|</span><a href="#41395374">next</a><span>|</span><label class="collapse" for="c-41396261">[-]</label><label class="expand" for="c-41396261">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I thought that’s what I said - no one knows the shape of danger social media is currently.<p>It’s like trying to draw tiger but you’ve never seen an animal. We only have the faintest clue what social media is right now. It will change in the next 25+ years as well.<p>Sure we know some dangers but… I think we need more time to know them all.</div><br/></div></div></div></div><div id="41395374" class="c"><input type="checkbox" id="c-41395374" checked=""/><div class="controls bullet"><span class="by">spacemadness</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41394521">parent</a><span>|</span><a href="#41395518">prev</a><span>|</span><a href="#41394817">next</a><span>|</span><label class="collapse" for="c-41395374">[-]</label><label class="expand" for="c-41395374">[1 more]</label></div><br/><div class="children"><div class="content">Yes, because a mechanical tool made of solid metal is the same thing as software that can change its behavior at any time and is controlled live by some company with its own motives.</div><br/></div></div></div></div></div></div><div id="41394817" class="c"><input type="checkbox" id="c-41394817" checked=""/><div class="controls bullet"><span class="by">itishappy</span><span>|</span><a href="#41393713">parent</a><span>|</span><a href="#41394317">prev</a><span>|</span><a href="#41396436">next</a><span>|</span><label class="collapse" for="c-41394817">[-]</label><label class="expand" for="c-41394817">[1 more]</label></div><br/><div class="children"><div class="content">It sounds like your algorithm targets children with unmoderated content. That feels like a dangerous position with potential for strong arguments in either direction. I think the only reasonable advice here is to keep close tabs on this case.</div><br/></div></div><div id="41396436" class="c"><input type="checkbox" id="c-41396436" checked=""/><div class="controls bullet"><span class="by">plandis</span><span>|</span><a href="#41393713">parent</a><span>|</span><a href="#41394817">prev</a><span>|</span><a href="#41395723">next</a><span>|</span><label class="collapse" for="c-41396436">[-]</label><label class="expand" for="c-41396436">[1 more]</label></div><br/><div class="children"><div class="content">Part of the claim is that TikTok knew about this content being promoted and other cases where children had died as a result.<p>&gt; But by the time Nylah viewed these videos, TikTok knew that: 1) &quot;the deadly Blackout Challenge was spreading through its app,&quot; 2) &quot;its algorithm was specifically feeding the Blackout Challenge to children,&quot; and
3) several children had died while attempting the Blackout Challenge after viewing videos of the Challenge on their For You Pages. App. 31-32. Yet TikTok &quot;took no and&#x2F;or completely inadequate action to extinguish and prevent the spread of the Blackout Challenge and specifically to prevent the Blackout Challenge from being shown to children on their [For You Pages].&quot; App. 32-33. Instead, TikTok continued to recommend these videos to children like Nylah.<p>Do you think this should be legal? Would you do nothing if you knew children were dying directly because of the content you were feeding them?</div><br/></div></div><div id="41395723" class="c"><input type="checkbox" id="c-41395723" checked=""/><div class="controls bullet"><span class="by">thih9</span><span>|</span><a href="#41393713">parent</a><span>|</span><a href="#41396436">prev</a><span>|</span><a href="#41395477">next</a><span>|</span><label class="collapse" for="c-41395723">[-]</label><label class="expand" for="c-41395723">[1 more]</label></div><br/><div class="children"><div class="content">Yes, if a product actively contributes to child fatalities then the manufacturer should be liable.<p>Then again, I guess your platform is about article recommendation and not about recording yourself doing popular trends. And perhaps children are not your target audience, or an audience at all. In many ways the situation was different for TikTok.</div><br/></div></div><div id="41395477" class="c"><input type="checkbox" id="c-41395477" checked=""/><div class="controls bullet"><span class="by">awongh</span><span>|</span><a href="#41393713">parent</a><span>|</span><a href="#41395723">prev</a><span>|</span><a href="#41395190">next</a><span>|</span><label class="collapse" for="c-41395477">[-]</label><label class="expand" for="c-41395477">[1 more]</label></div><br/><div class="children"><div class="content">I think it depends on some technical specifics, like which meta data was associated with that content, and the degree to which that content was surfaced to users that fit the demographic profile of a ten year old child.<p>If your algorithm decides that things in the 90th percentile of shock value will boost engagement to a user profile that can also include users who are ten years old then you maybe have built a negligent algorithm. Maybe that’s not the case in this particular instance but it could be possible.</div><br/></div></div><div id="41395190" class="c"><input type="checkbox" id="c-41395190" checked=""/><div class="controls bullet"><span class="by">troyvit</span><span>|</span><a href="#41393713">parent</a><span>|</span><a href="#41395477">prev</a><span>|</span><a href="#41394830">next</a><span>|</span><label class="collapse" for="c-41395190">[-]</label><label class="expand" for="c-41395190">[3 more]</label></div><br/><div class="children"><div class="content">Right?<p>Like if I&#x27;m a cement company, and I build a sidewalk that&#x27;s really good and stable, stable enough for a person to plant a milk crate on it, and stand on that milk crate, and hold up a big sign that gives clear instructions on self-asphyxiation, and a child reads that sign, tries it out and dies, am I going to get sued? All I did was build the foundation for a platform.</div><br/><div id="41395310" class="c"><input type="checkbox" id="c-41395310" checked=""/><div class="controls bullet"><span class="by">averageRoyalty</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41395190">parent</a><span>|</span><a href="#41394830">next</a><span>|</span><label class="collapse" for="c-41395310">[-]</label><label class="expand" for="c-41395310">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not a fair analogy though. To be fairer, you&#x27;d have to monitor said footpath 24&#x2F;7 and have a robot and&#x2F;or a number of people removing milk crate signs that you deemed inappropriate for your foothpath. They&#x27;d also move various milk crate signs in front of people as they walked and hide others.<p>If you were indeed monitoring the footpath for milk crate signs and moving them, yes you may be liable for showing or not removing one to someone it wouldn&#x27;t be appropriate for.</div><br/><div id="41395554" class="c"><input type="checkbox" id="c-41395554" checked=""/><div class="controls bullet"><span class="by">troyvit</span><span>|</span><a href="#41393713">root</a><span>|</span><a href="#41395310">parent</a><span>|</span><a href="#41394830">next</a><span>|</span><label class="collapse" for="c-41395554">[-]</label><label class="expand" for="c-41395554">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a good point, and actually the heart of the issue, and what I missed.<p>In my analogy the stable sidewalk that can hold the milk crate is both the platform and the optimization algorithm. But to your point there&#x27;s actually a lot more going on with the optimization than just building a place where any rando can market self-asphyxiation. It&#x27;s about how they willfully targeted people with that content.</div><br/></div></div></div></div></div></div><div id="41394830" class="c"><input type="checkbox" id="c-41394830" checked=""/><div class="controls bullet"><span class="by">drpossum</span><span>|</span><a href="#41393713">parent</a><span>|</span><a href="#41395190">prev</a><span>|</span><a href="#41395696">next</a><span>|</span><label class="collapse" for="c-41394830">[-]</label><label class="expand" for="c-41394830">[1 more]</label></div><br/><div class="children"><div class="content">You sure are if you knew about it (like tiktok was)</div><br/></div></div><div id="41395696" class="c"><input type="checkbox" id="c-41395696" checked=""/><div class="controls bullet"><span class="by">thinkingtoilet</span><span>|</span><a href="#41393713">parent</a><span>|</span><a href="#41394830">prev</a><span>|</span><a href="#41396307">next</a><span>|</span><label class="collapse" for="c-41395696">[-]</label><label class="expand" for="c-41395696">[1 more]</label></div><br/><div class="children"><div class="content">Of course you should be. Just because an algorithm gave you an output doesn&#x27;t absolve you from using it. It&#x27;s some magical mystical thing. It&#x27;s something you created and you are 100% responsible for what you do with the output of it.</div><br/></div></div></div></div><div id="41396307" class="c"><input type="checkbox" id="c-41396307" checked=""/><div class="controls bullet"><span class="by">hn_acker</span><span>|</span><a href="#41393713">prev</a><span>|</span><a href="#41392756">next</a><span>|</span><label class="collapse" for="c-41396307">[-]</label><label class="expand" for="c-41396307">[4 more]</label></div><br/><div class="children"><div class="content">For anyone making claims about what the authors of Section 230 intended or the extent to which Section 230 applies to targeted recommendations by algorithms, the authors of Section 230 (Ron Wyden and Chris Cox) wrote an amicus brief [1] for Google v. Gonzalez (2023). Here is an excerpt from the corresponding press release [2] by Wyden:<p>&gt; “Section 230 protects targeted recommendations to the same extent that it protects other forms of content presentation,” the members wrote. “That interpretation enables Section 230 to fulfill Congress’s purpose of encouraging innovation in content presentation and moderation. The real-time transmission of user-generated content that Section 230 fosters has become a backbone of online activity, relied upon by innumerable Internet users and platforms alike. Section 230’s protection remains as essential today as it was when the provision was enacted.”<p>[1][PDF] <a href="https:&#x2F;&#x2F;www.wyden.senate.gov&#x2F;download&#x2F;wyden-cox-amicus-brief-section-230" rel="nofollow">https:&#x2F;&#x2F;www.wyden.senate.gov&#x2F;download&#x2F;wyden-cox-amicus-brief...</a><p>[2] <a href="https:&#x2F;&#x2F;www.wyden.senate.gov&#x2F;news&#x2F;press-releases&#x2F;sen-wyden-and-former-rep-cox-urge-supreme-court-to-uphold-precedent-on-section-230" rel="nofollow">https:&#x2F;&#x2F;www.wyden.senate.gov&#x2F;news&#x2F;press-releases&#x2F;sen-wyden-a...</a></div><br/><div id="41396728" class="c"><input type="checkbox" id="c-41396728" checked=""/><div class="controls bullet"><span class="by">nsagent</span><span>|</span><a href="#41396307">parent</a><span>|</span><a href="#41396417">next</a><span>|</span><label class="collapse" for="c-41396728">[-]</label><label class="expand" for="c-41396728">[2 more]</label></div><br/><div class="children"><div class="content">This statement from Wyden&#x27;s press release seems to be in contrast to Chris Cox&#x27;s reasoning in his journal article [1] (linked in the amicus).<p><pre><code>  It is now firmly established in the case law that Section 230 cannot act as a shield whenever a website is in any way complicit in the creation or development of illegal content.

  ...

  In FTC v. Accusearch,[69] the Tenth Circuit Court of Appeals held that a website’s mere posting of content that it had no role whatsoever in creating — telephone records of private individuals — constituted “development” of that information, and so deprived it of Section 230 immunity. Even though the content was wholly created by others, the website knowingly transformed what had previously been private information into a publicly available commodity. Such complicity in illegality is what defines “development” of content, as distinguished from its creation.
</code></pre>
He goes on to list multiple similar cases and how they fit the original intent of the law. Then further clarifies that it&#x27;s not just about illegal content, but all legal obligations:<p><pre><code>  In writing Section 230, Rep. Wyden and I, and ultimately the entire Congress, decided that these legal rules should continue to apply on the internet just as in the offline world. Every business, whether operating through its online facility or through a brick-and-mortar facility, would continue to be responsible for all of its own legal obligations.
</code></pre>
Though, ultimately the original reasoning matters little in this case, as the courts are the ones to interpret the law. In fact Section 230 is one part of the larger Communications Decency Act that was mostly struck down by the Supreme Court.<p>EDIT: Added quote about additional legal obligations.<p>[1]: <a href="https:&#x2F;&#x2F;jolt.richmond.edu&#x2F;2020&#x2F;08&#x2F;27&#x2F;the-origins-and-original-intent-of-section-230-of-the-communications-decency-act&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jolt.richmond.edu&#x2F;2020&#x2F;08&#x2F;27&#x2F;the-origins-and-origina...</a></div><br/><div id="41397268" class="c"><input type="checkbox" id="c-41397268" checked=""/><div class="controls bullet"><span class="by">hn_acker</span><span>|</span><a href="#41396307">root</a><span>|</span><a href="#41396728">parent</a><span>|</span><a href="#41396417">next</a><span>|</span><label class="collapse" for="c-41397268">[-]</label><label class="expand" for="c-41397268">[1 more]</label></div><br/><div class="children"><div class="content">The Accusearch case was a situation in which the very act of reselling a specific kind of private information would&#x27;ve been illegal under the FTC Act if you temporarily ignore Section 230. If you add Section 230 into consideration, then you have to consider knowledge, but the knowledge analysis is trivial. Accusearch should&#x27;ve known that reselling any 1 phone number was illegal, so it doesn&#x27;t matter whether Accusearch knew the actual phone numbers it sold. Similarly, a social media site that only allows blackout challenge posts would be illegal regardless of whether the site employees know whether post #123 is actually a blackout challenge post. In contrast, most of the posts on TikTok are legal, and TikTok is designed for an indeterminate range of legal posts. Knowledge of specific posts matters.<p>Whether an intermediary has knowledge of specific content that is illegal to redistribute is very different from whether the intermediary has &quot;knowledge&quot; that the algorithm it designed to rank legally distributable content can &quot;sometimes&quot; produce a high ranking to &quot;some&quot; content that&#x27;s illegal to distribute. The latter case can be split further into specific illegal content that the intermediary has knowledge of and illegal content that the intermediary lacks knowledge of. Unless a law such as KOSA passes (which it shouldn&#x27;t [1]), the intermediary has no legal obligation to search for the illegal content that it isn&#x27;t yet aware of. The intermediary need only respond to reports, and depending on the volume of reports the intermediary isn&#x27;t obligated to respond within a &quot;short&quot; time period (except in &quot;intellectual property cases&quot;, which are explicitly exempt from Section 230). &quot;TikTok knows that TikTok has blackout challenge posts&quot; is not knowledge of post PQR. &quot;TikTok knows that post PQR on TikTok is a blackout challenge post&quot; is knowledge of post PQR.<p>Was TikTok aware that specific users were being recommended specific &quot;blackout challenge&quot; posts? If so, then TikTok should&#x27;ve deleted those posts. Afterward, TikTok employees should&#x27;ve known that its algorithm was recommending some blackout challenge posts to some users. Suppose that TikTok employees are already aware of post PQR. Then TikTok has an obligation to delete PQR. If in a week blackout challenge post HIJ shows up in the recommendations for user @abc and @xyz, then TikTok shouldn&#x27;t be liable for recommendations of HIJ until TikTok employees read a report about it and then confirm that HIJ is a blackout challenge post. Outwardly, @abc and @xyz will think that TikTok has done nothing or &quot;not enough&quot; even though TikTok removed PQR and isn&#x27;t yet aware of HIJ until a second week passes. The algorithm doesn&#x27;t create knowledge of HIJ no matter how high the algorithm ranks HIJ for user @abc. The algorithm may be TikTok&#x27;s first-party speech, but the content that is being recommended is still third-party speech. Suppose that @abc sues TikTok for failing to prevent HIJ from being recommended to @abc during the first elapsed week. The First Amendment would prevent TikTok from being held liable for HIJ (third party speech that TikTok lacked knowledge of during the first week). As a statute that provides an immunity (as opposed to a defense) in situations involving redistribution of third-party speech, Section 230 would allow TikTok to dismiss the case early; early dismissals save time and court fees. Does the featured ruling by the Third Circuit mean that Section 230 wouldn&#x27;t apply to TikTok&#x27;s recommendation of HIJ to @abc in the first elapsed week? Because if so, then I really don&#x27;t think that the Third Circuit is reading Section 230 correctly. At the very least, the Third Circuit&#x27;s ruling will create a chilling effect on complex algorithms in violation of social media websites&#x27; First Amendment freedom of expression. And I don&#x27;t believe that Ron Wyden and Chris Cox intended for websites to only sort user posts by chronological order (like multiple commenters on this post are hoping will happen as a result of the ruling) when they wrote Section 230.<p>[1] <a href="https:&#x2F;&#x2F;reason.com&#x2F;2024&#x2F;08&#x2F;20&#x2F;censoring-the-internet-wont-protect-kids&#x2F;" rel="nofollow">https:&#x2F;&#x2F;reason.com&#x2F;2024&#x2F;08&#x2F;20&#x2F;censoring-the-internet-wont-pr...</a></div><br/></div></div></div></div><div id="41396417" class="c"><input type="checkbox" id="c-41396417" checked=""/><div class="controls bullet"><span class="by">remich</span><span>|</span><a href="#41396307">parent</a><span>|</span><a href="#41396728">prev</a><span>|</span><a href="#41392756">next</a><span>|</span><label class="collapse" for="c-41396417">[-]</label><label class="expand" for="c-41396417">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m skeptical that Ron Wyden anticipated algorithmic social media feeds in 1996. But I&#x27;m pretty sure he gets a decent amount of lobbying cash from interested parties.</div><br/></div></div></div></div><div id="41392756" class="c"><input type="checkbox" id="c-41392756" checked=""/><div class="controls bullet"><span class="by">mjevans</span><span>|</span><a href="#41396307">prev</a><span>|</span><a href="#41393189">next</a><span>|</span><label class="collapse" for="c-41392756">[-]</label><label class="expand" for="c-41392756">[18 more]</label></div><br/><div class="children"><div class="content">&quot;&quot;&quot;The Court held that a platform&#x27;s algorithm that reflects &quot;editorial judgments&quot; about &quot;compiling the third-party speech it wants in the way it wants&quot; is the platform&#x27;s own &quot;expressive product&quot; and is therefore protected by the First Amendment.<p>Given the Supreme Court&#x27;s observations that platforms engage in protected first-party speech under the First Amendment when they curate compilations of others&#x27; content via their expressive algorithms, it follows that doing so amounts to first-party speech under Section 230, too.&quot;&quot;&quot;<p>I&#x27;ve agreed for years.  It&#x27;s a choice in selection rather than a &#x27;natural consequence&#x27; such as a chronological, threaded, or even &#x27;__end user__ upvoted &#x2F;moderated&#x27; (outside the site&#x27;s control) weighted sort.</div><br/><div id="41392811" class="c"><input type="checkbox" id="c-41392811" checked=""/><div class="controls bullet"><span class="by">bentley</span><span>|</span><a href="#41392756">parent</a><span>|</span><a href="#41393189">next</a><span>|</span><label class="collapse" for="c-41392811">[-]</label><label class="expand" for="c-41392811">[17 more]</label></div><br/><div class="children"><div class="content">If I as a forum administrator delete posts by obvious spambots, am I making an editorial judgment that makes me legally liable for every single post I don’t delete?<p>If my forum has a narrow scope (say, 4×4 offroading), and I delete a post that’s obviously by a human but is seriously off‐topic (say, U.S. politics), does <i>that</i> make me legally liable for every single post I don’t delete?<p>What are the limits here, for those of us who unlike silicon valley corporations, don’t have massive legal teams?</div><br/><div id="41393518" class="c"><input type="checkbox" id="c-41393518" checked=""/><div class="controls bullet"><span class="by">lesuorac</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41392811">parent</a><span>|</span><a href="#41392907">next</a><span>|</span><label class="collapse" for="c-41393518">[-]</label><label class="expand" for="c-41393518">[3 more]</label></div><br/><div class="children"><div class="content">&gt; If my forum has a narrow scope (say, 4×4 offroading), and I delete a post that’s obviously by a human but is seriously off‐topic (say, U.S. politics), does that make me legally liable for every single post I don’t delete?<p>No.<p>From the court of appeals [1], &quot;We reach this conclusion specifically because
TikTok’s promotion of a Blackout Challenge video on Nylah’s
FYP was not contingent upon any specific user input. Had
Nylah viewed a Blackout Challenge video through TikTok’s
search function, rather than through her FYP, then TikTok may
be viewed more like a repository of third-party content than an
affirmative promoter of such content.&quot;<p>So, given (an assumption) that users on your forum choose some kind of &quot;4x4 Topic&quot; they&#x27;re intending to navigate a repository of third-party content. If you curate that repository it&#x27;s still a collection of third-party content and not your own speech.<p>Now, if you were to have a landing page that showed &quot;featured content&quot; then that seems like you could get into trouble. Although one wonders what the difference is between navigating to a &quot;4x4 Topic&quot; or &quot;Featured Content&quot; since it&#x27;s both a user-action.<p>[1]: <a href="https:&#x2F;&#x2F;fingfx.thomsonreuters.com&#x2F;gfx&#x2F;legaldocs&#x2F;mopaqabzypa&#x2F;08272024tiktok.pdf" rel="nofollow">https:&#x2F;&#x2F;fingfx.thomsonreuters.com&#x2F;gfx&#x2F;legaldocs&#x2F;mopaqabzypa&#x2F;...</a></div><br/><div id="41394069" class="c"><input type="checkbox" id="c-41394069" checked=""/><div class="controls bullet"><span class="by">ApolloFortyNine</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41393518">parent</a><span>|</span><a href="#41394015">next</a><span>|</span><label class="collapse" for="c-41394069">[-]</label><label class="expand" for="c-41394069">[1 more]</label></div><br/><div class="children"><div class="content">&gt;then TikTok may be viewed more like a repository of third-party content than an affirmative promoter of such content.&quot;<p>&quot;may&quot;<p>Basically until the next court case when someone learns that search is an algorithm too, and asks why the first result wasn&#x27;t a warning.<p>The real truth is, if this is allowed to stand, it will be selectively enforced at best. If it&#x27;s low enough volume it&#x27;ll just become a price of doing business, sometimes a judge has it out for you and you have to pay a fine, you just have to work it into the budget. Fine for big companies, game ender for small ones.</div><br/></div></div><div id="41394015" class="c"><input type="checkbox" id="c-41394015" checked=""/><div class="controls bullet"><span class="by">shagie</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41393518">parent</a><span>|</span><a href="#41394069">prev</a><span>|</span><a href="#41392907">next</a><span>|</span><label class="collapse" for="c-41394015">[-]</label><label class="expand" for="c-41394015">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Now, if you were to have a landing page that showed &quot;featured content&quot; then that seems like you could get into trouble. Although one wonders what the difference is between navigating to a &quot;4x4 Topic&quot; or &quot;Featured Content&quot; since it&#x27;s both a user-action.<p>Consider HackerNews&#x27;s functionality of flamewar suppression.  <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39231821">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39231821</a><p>And this is the difference between <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;news">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;news</a> and <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newest">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newest</a> (with showdead enabled).</div><br/></div></div></div></div><div id="41392907" class="c"><input type="checkbox" id="c-41392907" checked=""/><div class="controls bullet"><span class="by">WCSTombs</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41392811">parent</a><span>|</span><a href="#41393518">prev</a><span>|</span><a href="#41393327">next</a><span>|</span><label class="collapse" for="c-41392907">[-]</label><label class="expand" for="c-41392907">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If my forum has a narrow scope (say, 4×4 offroading), and I delete a post that’s obviously by a human but is seriously off‐topic (say, U.S. politics), does that make me legally liable for every single post I don’t delete?<p>According to the article, probably not:<p>&gt; A platform is not liable for “any action voluntarily taken in good faith to restrict access to or availability of material that the provider or user considers to be obscene, lewd, lascivious, filthy, excessively violent, harassing, or otherwise objectionable.”<p>&quot;Otherwise objectionable&quot; looks like a catch-all phrase to allow content moderation generally, but I could be misreading it here.</div><br/></div></div><div id="41393327" class="c"><input type="checkbox" id="c-41393327" checked=""/><div class="controls bullet"><span class="by">Phrodo_00</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41392811">parent</a><span>|</span><a href="#41392907">prev</a><span>|</span><a href="#41393676">next</a><span>|</span><label class="collapse" for="c-41393327">[-]</label><label class="expand" for="c-41393327">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m guessing you&#x27;re not a lawyer, and I&#x27;m not either, so there might be some details that are not obvious about it, but the regulation draws the line at allowing you to do[1]:<p>&gt; any action voluntarily taken in good faith to restrict access to or availability of material that the provider or user considers to be obscene, lewd, lascivious, filthy, excessively violent, harassing, or otherwise objectionable, whether or not such material is constitutionally protected<p>I think that allows your use case without liability.<p>[1] <a href="https:&#x2F;&#x2F;www.law.cornell.edu&#x2F;uscode&#x2F;text&#x2F;47&#x2F;230" rel="nofollow">https:&#x2F;&#x2F;www.law.cornell.edu&#x2F;uscode&#x2F;text&#x2F;47&#x2F;230</a></div><br/><div id="41394540" class="c"><input type="checkbox" id="c-41394540" checked=""/><div class="controls bullet"><span class="by">zerocrates</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41393327">parent</a><span>|</span><a href="#41393368">next</a><span>|</span><label class="collapse" for="c-41394540">[-]</label><label class="expand" for="c-41394540">[1 more]</label></div><br/><div class="children"><div class="content">That subsection of 230 is about protecting you from being sued <i>for</i> moderating, like being sued by the people who posted the content you took down.<p>The &quot;my moderation makes me liable for everything I don&#x27;t moderate&quot; problem, that&#x27;s what&#x27;s addressed by the preceding section, the core of the law and the part that&#x27;s most often at issue, which says that you can&#x27;t be treated as publisher&#x2F;speaker of anyone else&#x27;s content.</div><br/></div></div><div id="41393368" class="c"><input type="checkbox" id="c-41393368" checked=""/><div class="controls bullet"><span class="by">kelnos</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41393327">parent</a><span>|</span><a href="#41394540">prev</a><span>|</span><a href="#41393676">next</a><span>|</span><label class="collapse" for="c-41393368">[-]</label><label class="expand" for="c-41393368">[3 more]</label></div><br/><div class="children"><div class="content">Wow, &quot;or otherwise objectionable&quot; would seemingly give providers a loophole wide enough to drive a truck through.</div><br/><div id="41393537" class="c"><input type="checkbox" id="c-41393537" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41393368">parent</a><span>|</span><a href="#41393676">next</a><span>|</span><label class="collapse" for="c-41393537">[-]</label><label class="expand" for="c-41393537">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a loophole. That&#x27;s the intended meaning, otherwise it would be a violation of freedom of association.<p>That doesn&#x27;t mean anyone is free to <i>promote</i> content without liability, just that moderating by deleting content doesn&#x27;t make it an &quot;expressive product.&quot;</div><br/><div id="41397400" class="c"><input type="checkbox" id="c-41397400" checked=""/><div class="controls bullet"><span class="by">habinero</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41393537">parent</a><span>|</span><a href="#41393676">next</a><span>|</span><label class="collapse" for="c-41397400">[-]</label><label class="expand" for="c-41397400">[1 more]</label></div><br/><div class="children"><div class="content">Both are protected, because both are 1A activity.</div><br/></div></div></div></div></div></div></div></div><div id="41393676" class="c"><input type="checkbox" id="c-41393676" checked=""/><div class="controls bullet"><span class="by">supriyo-biswas</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41392811">parent</a><span>|</span><a href="#41393327">prev</a><span>|</span><a href="#41393108">next</a><span>|</span><label class="collapse" for="c-41393676">[-]</label><label class="expand" for="c-41393676">[1 more]</label></div><br/><div class="children"><div class="content">What the other replies are not quite getting is that there can be other kinds of moderator actions that aren&#x27;t acting on posts that are offtopic or offensive, but that do not meet the bar for the forum in question — are they considered out of scope with this ruling?<p>As an example, suppose on a HN thread about the Coq theorem prover, someone starts a discussion about the name, and it&#x27;s highly upvoted but the moderators downrank that post manually to stimulate more productive discussions. Is this considered curation, and can this be no longer done given this ruling?<p>It seems to me that this is indeed the case, but in case I&#x27;m mistaken I&#x27;d love to know.</div><br/></div></div><div id="41393108" class="c"><input type="checkbox" id="c-41393108" checked=""/><div class="controls bullet"><span class="by">_DeadFred_</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41392811">parent</a><span>|</span><a href="#41393676">prev</a><span>|</span><a href="#41393190">next</a><span>|</span><label class="collapse" for="c-41393108">[-]</label><label class="expand" for="c-41393108">[1 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t it more be you are responsible for pinned posts at the top of thread lists? If you pin a thread promoting an unsafe onroad product, say telling people they should be replacing their steering with heim joints that aren&#x27;t street legal, you could be liable. Whereas if you just left the thread among all the others you aren&#x27;t. (Especially if the heim joints are sold by a forum sponsor or the forum has a special &#x27;discount&#x27; code for the vendor).</div><br/></div></div><div id="41393190" class="c"><input type="checkbox" id="c-41393190" checked=""/><div class="controls bullet"><span class="by">mathgradthrow</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41392811">parent</a><span>|</span><a href="#41393108">prev</a><span>|</span><a href="#41393012">next</a><span>|</span><label class="collapse" for="c-41393190">[-]</label><label class="expand" for="c-41393190">[1 more]</label></div><br/><div class="children"><div class="content">You are simply not shielded from liability, I cannot imagine a scenario in which this moderation policy would result in significant liability. I&#x27;m sure someobe would be willing to sell you some insurance to that effect. I certainly would.</div><br/></div></div><div id="41393012" class="c"><input type="checkbox" id="c-41393012" checked=""/><div class="controls bullet"><span class="by">doe_eyes</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41392811">parent</a><span>|</span><a href="#41393190">prev</a><span>|</span><a href="#41395120">next</a><span>|</span><label class="collapse" for="c-41393012">[-]</label><label class="expand" for="c-41393012">[2 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re looking for the kind of precision that just doesn&#x27;t exist in the legal system. It will almost certainly hinge on intent and the extent to which your actions actually stifle legitimate speech.<p>I imagine that getting rid of spam wouldn&#x27;t meet the bar, and neither would enforcing that conversations are on-topic. But if you&#x27;re removing and demoting posts because they express views you disagree with, you&#x27;re implicitly endorsing the opinions expressed in the posts you allow to stay up, and therefore are exercising editorial control.<p>I think the lesson here is: either keep your communities small so that you can comfortably reason about the content that&#x27;s up there, or don&#x27;t play the thought police. The only weird aspect of this is that you have courts saying one thing, but then the government breathing down your neck and demanding that you go after misinformation.</div><br/><div id="41393559" class="c"><input type="checkbox" id="c-41393559" checked=""/><div class="controls bullet"><span class="by">Sakos</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41393012">parent</a><span>|</span><a href="#41395120">next</a><span>|</span><label class="collapse" for="c-41393559">[-]</label><label class="expand" for="c-41393559">[1 more]</label></div><br/><div class="children"><div class="content">A lot of people seem to missing the part where if it ends up in court, you have to argue that what you removed was objectionable on the same level as the other named types of content and there will be a judge you&#x27;ll need to convince that you didn&#x27;t re-interpret the law to your benefit. This isn&#x27;t like arguing on HN or social media, you being &quot;clever&quot; doesn&#x27;t necessarily protect you from liability or consequences.</div><br/></div></div></div></div><div id="41395120" class="c"><input type="checkbox" id="c-41395120" checked=""/><div class="controls bullet"><span class="by">jay_kyburz</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41392811">parent</a><span>|</span><a href="#41393012">prev</a><span>|</span><a href="#41393189">next</a><span>|</span><label class="collapse" for="c-41395120">[-]</label><label class="expand" for="c-41395120">[2 more]</label></div><br/><div class="children"><div class="content">Let me ask you a question in return.<p>If you discovered a thread on the forum where a bunch of users were excitedly talking about doing something incredibly dangerous in their 4x4s, like getting high and trying some dangerous maneuver, would you let sit on your forum?<p>How would you feel if somebody read about it on your forum and died trying to do it?<p>Update: The point I&#x27;m trying to make is that _I_ wouldn&#x27;t let this sit on my forum, so I don&#x27;t think its unethical to ask others to remove it from their forums as well.</div><br/><div id="41397043" class="c"><input type="checkbox" id="c-41397043" checked=""/><div class="controls bullet"><span class="by">romanows</span><span>|</span><a href="#41392756">root</a><span>|</span><a href="#41395120">parent</a><span>|</span><a href="#41393189">next</a><span>|</span><label class="collapse" for="c-41397043">[-]</label><label class="expand" for="c-41397043">[1 more]</label></div><br/><div class="children"><div class="content">Not the OP, but if I thought we were all joking around, and it was the type of forum that allowed people to be a bit silly, I would let it stand.  Or if I thought people on the forum would point out the danger and hopefully dissuade the poster and&#x2F;or others from engaging in that behavior, I would let it stand.<p>However, if my hypothetical forum received a persistent flood of posts designed to soften people up to dangerous behaviors, I&#x27;d be pretty liberal removing posts that smelled funny until the responsible clique moved elsewhere.</div><br/></div></div></div></div></div></div></div></div><div id="41393189" class="c"><input type="checkbox" id="c-41393189" checked=""/><div class="controls bullet"><span class="by">Xcelerate</span><span>|</span><a href="#41392756">prev</a><span>|</span><a href="#41392613">next</a><span>|</span><label class="collapse" for="c-41393189">[-]</label><label class="expand" for="c-41393189">[9 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not at all opposed to implementing <i>new</i> laws that society believes will reduce harm to online users (particularly children).<p>However, if Section 230 is on its way out, won&#x27;t this just benefit the largest tech companies that already have massive legal resources and the ability to afford ML-based or manual content moderation? The barriers to entry into the market for startups will become insurmountable. Perhaps I&#x27;m missing something here, but it sounds like the existing companies essentially got a free pass with regard to liability of user-provided content and had plenty of time to grow, and now the government is pulling the ladder up after them.</div><br/><div id="41393347" class="c"><input type="checkbox" id="c-41393347" checked=""/><div class="controls bullet"><span class="by">tboyd47</span><span>|</span><a href="#41393189">parent</a><span>|</span><a href="#41393451">next</a><span>|</span><label class="collapse" for="c-41393347">[-]</label><label class="expand" for="c-41393347">[3 more]</label></div><br/><div class="children"><div class="content">The assertion made by the author is that the way these companies grew is only sustainable in the current legal environment. So the advantage they have right now by being bigger is nullified.</div><br/><div id="41394313" class="c"><input type="checkbox" id="c-41394313" checked=""/><div class="controls bullet"><span class="by">xboxnolifes</span><span>|</span><a href="#41393189">root</a><span>|</span><a href="#41393347">parent</a><span>|</span><a href="#41393451">next</a><span>|</span><label class="collapse" for="c-41394313">[-]</label><label class="expand" for="c-41394313">[2 more]</label></div><br/><div class="children"><div class="content">Yes, the way they <i>grew</i> is only sustainable from the current. What about not growing, but maintaining?</div><br/><div id="41395455" class="c"><input type="checkbox" id="c-41395455" checked=""/><div class="controls bullet"><span class="by">lelandbatey</span><span>|</span><a href="#41393189">root</a><span>|</span><a href="#41394313">parent</a><span>|</span><a href="#41393451">next</a><span>|</span><label class="collapse" for="c-41395455">[-]</label><label class="expand" for="c-41395455">[1 more]</label></div><br/><div class="children"><div class="content">The parent said &quot;grew&quot;, but I think a closer reading of the article indicates a more robust idea that tboyd47 merely misrepresented. A better sentence is potentially:<p><i>are able to profit to the tune of a 40% margin on advertising revenue</i><p>With that, they&#x27;re saying that they&#x27;re only going to be able to profit this much in this current regulatory environment. If that goes away, so too does much of their margin, potentially all of it. That&#x27;s a big blow no matter the size, though Facebook may weather it better than smaller competitors.</div><br/></div></div></div></div></div></div><div id="41397721" class="c"><input type="checkbox" id="c-41397721" checked=""/><div class="controls bullet"><span class="by">root_axis</span><span>|</span><a href="#41393189">parent</a><span>|</span><a href="#41393451">prev</a><span>|</span><a href="#41398520">next</a><span>|</span><label class="collapse" for="c-41397721">[-]</label><label class="expand" for="c-41397721">[1 more]</label></div><br/><div class="children"><div class="content">Section 230 isn&#x27;t on it&#x27;s way out, this happened because the court found that TikTok knowingly headlined dangerous content that lead to someone&#x27;s death.</div><br/></div></div><div id="41398520" class="c"><input type="checkbox" id="c-41398520" checked=""/><div class="controls bullet"><span class="by">fedeb95</span><span>|</span><a href="#41393189">parent</a><span>|</span><a href="#41397721">prev</a><span>|</span><a href="#41394302">next</a><span>|</span><label class="collapse" for="c-41398520">[-]</label><label class="expand" for="c-41398520">[1 more]</label></div><br/><div class="children"><div class="content">not necessarily. It can also open a new market for startups. Content moderation.</div><br/></div></div><div id="41394302" class="c"><input type="checkbox" id="c-41394302" checked=""/><div class="controls bullet"><span class="by">2OEH8eoCRo0</span><span>|</span><a href="#41393189">parent</a><span>|</span><a href="#41398520">prev</a><span>|</span><a href="#41392613">next</a><span>|</span><label class="collapse" for="c-41394302">[-]</label><label class="expand" for="c-41394302">[2 more]</label></div><br/><div class="children"><div class="content">&gt; won&#x27;t this just benefit the largest tech companies<p>I&#x27;d wager the bigger you are the harder it gets. How would they fend off tens of thousands of simultaneous lawsuits?</div><br/><div id="41396980" class="c"><input type="checkbox" id="c-41396980" checked=""/><div class="controls bullet"><span class="by">Nasrudith</span><span>|</span><a href="#41393189">root</a><span>|</span><a href="#41394302">parent</a><span>|</span><a href="#41392613">next</a><span>|</span><label class="collapse" for="c-41396980">[-]</label><label class="expand" for="c-41396980">[1 more]</label></div><br/><div class="children"><div class="content">By turning them all into expensive tarpits of time and money - through the power of strategic spite. Making it so expensive that all plantiffs cannot win even if they prevail in a lawsuit. It is a far harder standard to get legal costs covered and if it costs tens of millions to possibly get a few million in a decade interest dries up fast.</div><br/></div></div></div></div></div></div><div id="41392613" class="c"><input type="checkbox" id="c-41392613" checked=""/><div class="controls bullet"><span class="by">octopoc</span><span>|</span><a href="#41393189">prev</a><span>|</span><a href="#41397804">next</a><span>|</span><label class="collapse" for="c-41392613">[-]</label><label class="expand" for="c-41392613">[2 more]</label></div><br/><div class="children"><div class="content">&gt; In other words, the fundamental issue here is not really whether big tech platforms should be regulated as speakers, as that’s a misconception of what they do. They don’t speak, they are middlemen. And hopefully, we will follow the logic of Matey’s opinion, and start to see the policy problem as what to do about that.<p>This is a pretty good take, and it relies on pre-Internet legal concepts like distributor and producer. There&#x27;s this idea that our legal &#x2F; governmental structures are not designed to handle the Internet age and therefore need to be revamped, but this is a counterexample that is both relevant and significant.</div><br/><div id="41396020" class="c"><input type="checkbox" id="c-41396020" checked=""/><div class="controls bullet"><span class="by">postalrat</span><span>|</span><a href="#41392613">parent</a><span>|</span><a href="#41397804">next</a><span>|</span><label class="collapse" for="c-41396020">[-]</label><label class="expand" for="c-41396020">[1 more]</label></div><br/><div class="children"><div class="content">They are more than middlemen when they are very carefully choosing what content each person sees or doesn&#x27;t see.</div><br/></div></div></div></div><div id="41397804" class="c"><input type="checkbox" id="c-41397804" checked=""/><div class="controls bullet"><span class="by">skeltoac</span><span>|</span><a href="#41392613">prev</a><span>|</span><a href="#41392421">next</a><span>|</span><label class="collapse" for="c-41397804">[-]</label><label class="expand" for="c-41397804">[1 more]</label></div><br/><div class="children"><div class="content">Disclosures: I read the ruling before reading Matt Stoller’s article. I am a subscriber of his. I have written content recommendation algorithms for large audiences. I recommend doing one of these three things.<p>Section 230 is not canceled. This is a significant but fairly narrow refinement of what constitutes original content and Stoller’s take (“The business model of big tech is over”) is vastly overstating it.<p>Some kinds of recommendation algorithms produce original content (speech) by selecting and arranging feeds of other user generated content and the creators of the algorithms can be sued for harms caused by those recommendations. This correctly attaches liability to risky business.<p>The businesses using this model need to exercise a duty of care toward the public. It’s about time they start.</div><br/></div></div><div id="41392421" class="c"><input type="checkbox" id="c-41392421" checked=""/><div class="controls bullet"><span class="by">tboyd47</span><span>|</span><a href="#41397804">prev</a><span>|</span><a href="#41392982">next</a><span>|</span><label class="collapse" for="c-41392421">[-]</label><label class="expand" for="c-41392421">[9 more]</label></div><br/><div class="children"><div class="content">Fantastic write-up. The author appears to be making more than a few assumptions about how this will play out, but I share his enthusiasm for the end of the &quot;lawless no-man’s-land&quot; (as he put it) era of the internet. It comes at a great time too, as we&#x27;re all eagerly awaiting the AI-generated content apocalypse. Just switch one apocalypse for a kinder, more human-friendly one.<p>&gt; So what happens going forward? Well we’re going to have to start thinking about what a world without this expansive reading of Section 230 looks like.<p>There was an internet before the CDA. From what I remember, it was actually pretty rad. There can be an internet after, too. Who knows what it would look like. Maybe it will be a lot less crowded, less toxic, less triggering, and less addictive without these gigantic megacorps spending buku dollars to light up our amygdalas with nonsense all day.</div><br/><div id="41393854" class="c"><input type="checkbox" id="c-41393854" checked=""/><div class="controls bullet"><span class="by">tboyd47</span><span>|</span><a href="#41392421">parent</a><span>|</span><a href="#41397395">next</a><span>|</span><label class="collapse" for="c-41393854">[-]</label><label class="expand" for="c-41393854">[7 more]</label></div><br/><div class="children"><div class="content">I read the decision. -&gt; <a href="https:&#x2F;&#x2F;cases.justia.com&#x2F;federal&#x2F;appellate-courts&#x2F;ca3&#x2F;22-3061&#x2F;22-3061-2024-08-27.pdf?ts=1724792413" rel="nofollow">https:&#x2F;&#x2F;cases.justia.com&#x2F;federal&#x2F;appellate-courts&#x2F;ca3&#x2F;22-306...</a><p>Judge Matey&#x27;s basic point of contention is that Section 230 does not provide immunity for any of TikTok&#x27;s actions except &quot;hosting&quot; the blackout challenge video on its server.<p>Defining it in this way may lead to a tricky technical problem for the courts to solve... While working in web, I understand &quot;hosting&quot; to mean the act of storing files on a computer somewhere. That&#x27;s it. Is that how the courts will understand it? Or does their definition of hosting include acts that I would call serving, caching, indexing, linking, formatting, and rendering? If publishers are liable for even some of those acts, then this takes us to a very different place from where we were in 1995. Interesting times ahead for the industry.</div><br/><div id="41394679" class="c"><input type="checkbox" id="c-41394679" checked=""/><div class="controls bullet"><span class="by">itsdrewmiller</span><span>|</span><a href="#41392421">root</a><span>|</span><a href="#41393854">parent</a><span>|</span><a href="#41397395">next</a><span>|</span><label class="collapse" for="c-41394679">[-]</label><label class="expand" for="c-41394679">[6 more]</label></div><br/><div class="children"><div class="content">You&#x27;re reading it too literally here - the CDA applies to:<p>&gt;(2) Interactive computer service The term “interactive computer service” means any information service, system, or access software provider that provides or enables computer access by multiple users to a computer server, including specifically a service or system that provides access to the Internet and such systems operated or services offered by libraries or educational institutions.</div><br/><div id="41394853" class="c"><input type="checkbox" id="c-41394853" checked=""/><div class="controls bullet"><span class="by">tboyd47</span><span>|</span><a href="#41392421">root</a><span>|</span><a href="#41394679">parent</a><span>|</span><a href="#41397395">next</a><span>|</span><label class="collapse" for="c-41394853">[-]</label><label class="expand" for="c-41394853">[5 more]</label></div><br/><div class="children"><div class="content">What definition of &quot;hosting&quot; do you think the courts would apply instead of the technical one?</div><br/><div id="41394977" class="c"><input type="checkbox" id="c-41394977" checked=""/><div class="controls bullet"><span class="by">itsdrewmiller</span><span>|</span><a href="#41392421">root</a><span>|</span><a href="#41394853">parent</a><span>|</span><a href="#41394922">next</a><span>|</span><label class="collapse" for="c-41394977">[-]</label><label class="expand" for="c-41394977">[2 more]</label></div><br/><div class="children"><div class="content">&quot;hosting&quot; isn&#x27;t actually used in the text of the relevant law - it&#x27;s only shorthand in the decision.  If they want to know what the CDA exempts they would read the CDA along with caselaw specifically interpreting it.</div><br/><div id="41395236" class="c"><input type="checkbox" id="c-41395236" checked=""/><div class="controls bullet"><span class="by">tboyd47</span><span>|</span><a href="#41392421">root</a><span>|</span><a href="#41394977">parent</a><span>|</span><a href="#41394922">next</a><span>|</span><label class="collapse" for="c-41395236">[-]</label><label class="expand" for="c-41395236">[1 more]</label></div><br/><div class="children"><div class="content">True</div><br/></div></div></div></div><div id="41394922" class="c"><input type="checkbox" id="c-41394922" checked=""/><div class="controls bullet"><span class="by">jen20</span><span>|</span><a href="#41392421">root</a><span>|</span><a href="#41394853">parent</a><span>|</span><a href="#41394977">prev</a><span>|</span><a href="#41397395">next</a><span>|</span><label class="collapse" for="c-41394922">[-]</label><label class="expand" for="c-41394922">[2 more]</label></div><br/><div class="children"><div class="content">I’d imagine one that reasonable people would understand to be the meaning. If a “web hosting” company told me they only stored things on a server with no way to serve it to users, I’d laugh them out the room.</div><br/><div id="41395235" class="c"><input type="checkbox" id="c-41395235" checked=""/><div class="controls bullet"><span class="by">tboyd47</span><span>|</span><a href="#41392421">root</a><span>|</span><a href="#41394922">parent</a><span>|</span><a href="#41397395">next</a><span>|</span><label class="collapse" for="c-41395235">[-]</label><label class="expand" for="c-41395235">[1 more]</label></div><br/><div class="children"><div class="content">Good point</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41397395" class="c"><input type="checkbox" id="c-41397395" checked=""/><div class="controls bullet"><span class="by">HDThoreaun</span><span>|</span><a href="#41392421">parent</a><span>|</span><a href="#41393854">prev</a><span>|</span><a href="#41392982">next</a><span>|</span><label class="collapse" for="c-41397395">[-]</label><label class="expand" for="c-41397395">[1 more]</label></div><br/><div class="children"><div class="content">&gt; but I share his enthusiasm for the end of the &quot;lawless no-man’s-land&quot;<p>That&#x27;s crazy, I feel like being a lawless no-man&#x27;s land is the best part of the internet.</div><br/></div></div></div></div><div id="41392982" class="c"><input type="checkbox" id="c-41392982" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#41392421">prev</a><span>|</span><a href="#41398527">next</a><span>|</span><label class="collapse" for="c-41392982">[-]</label><label class="expand" for="c-41392982">[18 more]</label></div><br/><div class="children"><div class="content">The ruling itself says that this is not about 230, it&#x27;s about TikTok&#x27;s curation and collation of the specific videos. TikTok is not held liable for the user content but for the part that they do their &#x27;for you&#x27; section. I guess it makes sense, manipulating people is not OK whether it&#x27;s for political purposes as facebook and twitter do, or whatever. So 230 is not over<p>It would be nice to see those &#x27;For you&#x27; and youtube&#x27;s recomendations gone. Chronological timelines are the best , and will bring back some sanity. Don&#x27;t like it? don&#x27;t follow it<p>&gt; Accordingly, TikTok’s algorithm, which recommended
the Blackout Challenge to Nylah on her FYP, was TikTok’s
own “expressive activity,” id., and thus its first-party speech.<p>&gt;<p>&gt; Section 230 immunizes only information
“provided by another[,]” 47 U.S.C. § 230(c)(1), and here,
because the information that forms the basis of Anderson’s
lawsuit—i.e., TikTok’s recommendations via its FYP  algorithm—is TikTok’s own expressive activity, § 230 does
not bar Anderson’s claims.</div><br/><div id="41393047" class="c"><input type="checkbox" id="c-41393047" checked=""/><div class="controls bullet"><span class="by">falcolas</span><span>|</span><a href="#41392982">parent</a><span>|</span><a href="#41394988">next</a><span>|</span><label class="collapse" for="c-41393047">[-]</label><label class="expand" for="c-41393047">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Don&#x27;t like it? don&#x27;t follow it<p>How did you find <i>it</i> in the first place? A search? Without any kind of filtering (that&#x27;s an algorithm that could be used to manipulate people), all you&#x27;ll see is pages and pages of SEO.<p>Opening up liability like this is a quagmire that&#x27;s not going to do good things for the internet.</div><br/><div id="41393254" class="c"><input type="checkbox" id="c-41393254" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#41392982">root</a><span>|</span><a href="#41393047">parent</a><span>|</span><a href="#41393196">next</a><span>|</span><label class="collapse" for="c-41393254">[-]</label><label class="expand" for="c-41393254">[3 more]</label></div><br/><div class="children"><div class="content">retweets&#x2F;sharing. that&#x27;s how it used to be</div><br/><div id="41393393" class="c"><input type="checkbox" id="c-41393393" checked=""/><div class="controls bullet"><span class="by">falcolas</span><span>|</span><a href="#41392982">root</a><span>|</span><a href="#41393254">parent</a><span>|</span><a href="#41393196">next</a><span>|</span><label class="collapse" for="c-41393393">[-]</label><label class="expand" for="c-41393393">[2 more]</label></div><br/><div class="children"><div class="content">How did they find it? How did you see the tweet? How did the shared link show up in any of your pages?<p>Also, lists of content (or blind links to random pages - web rings) have been a thing since well before Twitter or Dig.</div><br/><div id="41394669" class="c"><input type="checkbox" id="c-41394669" checked=""/><div class="controls bullet"><span class="by">skydhash</span><span>|</span><a href="#41392982">root</a><span>|</span><a href="#41393393">parent</a><span>|</span><a href="#41393196">next</a><span>|</span><label class="collapse" for="c-41394669">[-]</label><label class="expand" for="c-41394669">[1 more]</label></div><br/><div class="children"><div class="content">How do rumors and news propagates? And we still consider that the person sharing it with us is partially responsible (especially if it&#x27;s fake).</div><br/></div></div></div></div></div></div><div id="41393196" class="c"><input type="checkbox" id="c-41393196" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#41392982">root</a><span>|</span><a href="#41393047">parent</a><span>|</span><a href="#41393254">prev</a><span>|</span><a href="#41393596">next</a><span>|</span><label class="collapse" for="c-41393196">[-]</label><label class="expand" for="c-41393196">[3 more]</label></div><br/><div class="children"><div class="content">&gt;not going to do good things for the internet.<p>Not sure if you&#x27;ve noticed, but the internet seemingly ran out of good things quite some time back.</div><br/><div id="41393866" class="c"><input type="checkbox" id="c-41393866" checked=""/><div class="controls bullet"><span class="by">rtkwe</span><span>|</span><a href="#41392982">root</a><span>|</span><a href="#41393196">parent</a><span>|</span><a href="#41393269">next</a><span>|</span><label class="collapse" for="c-41393866">[-]</label><label class="expand" for="c-41393866">[1 more]</label></div><br/><div class="children"><div class="content">The question though is how do you do a useful search without having some kind of algorithmic answer to what you think the user will like. Explicit user or exact match strings are simple but if I search &quot;cats&quot; looking for cat videos how does that list get presented without being a curated list made by the company?</div><br/></div></div><div id="41393269" class="c"><input type="checkbox" id="c-41393269" checked=""/><div class="controls bullet"><span class="by">falcolas</span><span>|</span><a href="#41392982">root</a><span>|</span><a href="#41393196">parent</a><span>|</span><a href="#41393866">prev</a><span>|</span><a href="#41393596">next</a><span>|</span><label class="collapse" for="c-41393269">[-]</label><label class="expand" for="c-41393269">[1 more]</label></div><br/><div class="children"><div class="content">Irrelevant and untrue.<p>For example, just today there was a highly entertaining and interesting article about how to replace a tablet-based thermostat. And it was posted on the internet, and surfaced via an algorithm on Hacker News.</div><br/></div></div></div></div><div id="41393596" class="c"><input type="checkbox" id="c-41393596" checked=""/><div class="controls bullet"><span class="by">jimbob45</span><span>|</span><a href="#41392982">root</a><span>|</span><a href="#41393047">parent</a><span>|</span><a href="#41393196">prev</a><span>|</span><a href="#41394988">next</a><span>|</span><label class="collapse" for="c-41393596">[-]</label><label class="expand" for="c-41393596">[1 more]</label></div><br/><div class="children"><div class="content"><i>Without any kind of filtering (that&#x27;s an algorithm that could be used to manipulate people)</i><p>Do you genuinely believe a judge is going to rule that a Boyer-Moore implementation is fundamentally biased? It seems likely that sticking with standard string matching will remain safe.</div><br/></div></div></div></div><div id="41394988" class="c"><input type="checkbox" id="c-41394988" checked=""/><div class="controls bullet"><span class="by">dvngnt_</span><span>|</span><a href="#41392982">parent</a><span>|</span><a href="#41393047">prev</a><span>|</span><a href="#41393059">next</a><span>|</span><label class="collapse" for="c-41394988">[-]</label><label class="expand" for="c-41394988">[1 more]</label></div><br/><div class="children"><div class="content">how does that work for something like tiktok. Chronological doesn&#x27;t have much value if you&#x27;re trying to discover interesting content relevant to your interest.</div><br/></div></div><div id="41393059" class="c"><input type="checkbox" id="c-41393059" checked=""/><div class="controls bullet"><span class="by">WalterBright</span><span>|</span><a href="#41392982">parent</a><span>|</span><a href="#41394988">prev</a><span>|</span><a href="#41393053">next</a><span>|</span><label class="collapse" for="c-41393059">[-]</label><label class="expand" for="c-41393059">[3 more]</label></div><br/><div class="children"><div class="content">&gt; manipulating people is not OK whether it&#x27;s for political purposes as facebook and twitter do<p>Not to mention CNN, MSNBC, the New York Times, NPR, etc.</div><br/><div id="41393260" class="c"><input type="checkbox" id="c-41393260" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#41392982">root</a><span>|</span><a href="#41393059">parent</a><span>|</span><a href="#41393053">next</a><span>|</span><label class="collapse" for="c-41393260">[-]</label><label class="expand" for="c-41393260">[2 more]</label></div><br/><div class="children"><div class="content">Those are subject to legal liability for the content they produce.</div><br/><div id="41396985" class="c"><input type="checkbox" id="c-41396985" checked=""/><div class="controls bullet"><span class="by">Nasrudith</span><span>|</span><a href="#41392982">root</a><span>|</span><a href="#41393260">parent</a><span>|</span><a href="#41393053">next</a><span>|</span><label class="collapse" for="c-41396985">[-]</label><label class="expand" for="c-41396985">[1 more]</label></div><br/><div class="children"><div class="content">But not for manipulation. That isn’t a crime.</div><br/></div></div></div></div></div></div><div id="41393053" class="c"><input type="checkbox" id="c-41393053" checked=""/><div class="controls bullet"><span class="by">aiauthoritydev</span><span>|</span><a href="#41392982">parent</a><span>|</span><a href="#41393059">prev</a><span>|</span><a href="#41398527">next</a><span>|</span><label class="collapse" for="c-41393053">[-]</label><label class="expand" for="c-41393053">[5 more]</label></div><br/><div class="children"><div class="content">&gt;  Chronological timelines are the best , and will bring back some sanity. Don&#x27;t like it? don&#x27;t follow it<p>You realize that there is immense arrogance in this statement where you have decided that something is good for me ? I am totally fine with youtube&#x27;s recommendations or even Tiktok&#x27;s algorithms that according to you &quot;manipulate&quot; me.</div><br/><div id="41393274" class="c"><input type="checkbox" id="c-41393274" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#41392982">root</a><span>|</span><a href="#41393053">parent</a><span>|</span><a href="#41398592">next</a><span>|</span><label class="collapse" for="c-41393274">[-]</label><label class="expand" for="c-41393274">[3 more]</label></div><br/><div class="children"><div class="content">You can have them, but they have legal consequences for the owner.</div><br/><div id="41394383" class="c"><input type="checkbox" id="c-41394383" checked=""/><div class="controls bullet"><span class="by">cvalka</span><span>|</span><a href="#41392982">root</a><span>|</span><a href="#41393274">parent</a><span>|</span><a href="#41398592">next</a><span>|</span><label class="collapse" for="c-41394383">[-]</label><label class="expand" for="c-41394383">[2 more]</label></div><br/><div class="children"><div class="content">How can they have them if they are prohibited?</div><br/><div id="41394688" class="c"><input type="checkbox" id="c-41394688" checked=""/><div class="controls bullet"><span class="by">skydhash</span><span>|</span><a href="#41392982">root</a><span>|</span><a href="#41394383">parent</a><span>|</span><a href="#41398592">next</a><span>|</span><label class="collapse" for="c-41394688">[-]</label><label class="expand" for="c-41394688">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re not prohibited. They&#x27;re just liable for it, just like manufacturers are liable for defective products that endangers people.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41398527" class="c"><input type="checkbox" id="c-41398527" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#41392982">prev</a><span>|</span><a href="#41392458">next</a><span>|</span><label class="collapse" for="c-41398527">[-]</label><label class="expand" for="c-41398527">[1 more]</label></div><br/><div class="children"><div class="content">Part of the reason social media has grown so big and been so profitable is that these platforms have scaled past their own abilities to do what normal companies are required to do.<p>Facebook has a “marketplace” but no customer support line. Google is serving people scam ads for months, leading to millions in losses. (Imagine if a newspaper did that.) And feeds are allowed to recommend content that would be beyond the pale if a human were curating it. But because “it’s just an algorithm bro” we give them a pass because they can claim plausible deniability.<p>If fixing this means certain companies can’t scale to a trillion dollars with no customer support, too bad. Google can’t vet every ad? They could, but choose not to. Figure it out.<p>And content for children should have an even higher bar than that. Kids should not be dying from watching videos.</div><br/></div></div><div id="41392458" class="c"><input type="checkbox" id="c-41392458" checked=""/><div class="controls bullet"><span class="by">chucke1992</span><span>|</span><a href="#41398527">prev</a><span>|</span><a href="#41394738">next</a><span>|</span><label class="collapse" for="c-41392458">[-]</label><label class="expand" for="c-41392458">[50 more]</label></div><br/><div class="children"><div class="content">So basically closer and closer to governmental control over social networks. Seems like a global trend everywhere. Governments will define the rules by which communication services (and social networks) should operate.</div><br/><div id="41393370" class="c"><input type="checkbox" id="c-41393370" checked=""/><div class="controls bullet"><span class="by">aidenn0</span><span>|</span><a href="#41392458">parent</a><span>|</span><a href="#41392983">next</a><span>|</span><label class="collapse" for="c-41393370">[-]</label><label class="expand" for="c-41393370">[1 more]</label></div><br/><div class="children"><div class="content">IANAL, but it seems to me that Facebook from 20ish years ago would likely be fine under this ruling; it just showed you stuff that people you have marked as friends post.  However, if Facebook wants to specifically pick things to surface, that&#x27;s where potential liability is involved.<p>The alleged activity in this lawsuit was TikTok either knew or should have known that it was targeting content to minors that contained challenges that was likely to result in harm if repeated.  That goes well beyond simple moderation, and is even something that various social media companies have argued in court is speech made by the companies.</div><br/></div></div><div id="41392983" class="c"><input type="checkbox" id="c-41392983" checked=""/><div class="controls bullet"><span class="by">amscanne</span><span>|</span><a href="#41392458">parent</a><span>|</span><a href="#41393370">prev</a><span>|</span><a href="#41394820">next</a><span>|</span><label class="collapse" for="c-41392983">[-]</label><label class="expand" for="c-41392983">[4 more]</label></div><br/><div class="children"><div class="content">Not at all. It’s merely a question of whether social networks are shielded from liability for their recommendations, recognizing that what they choose to show you is a form of free expression that may have consequences — not an attempt to control that expression.</div><br/><div id="41395622" class="c"><input type="checkbox" id="c-41395622" checked=""/><div class="controls bullet"><span class="by">srackey</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392983">parent</a><span>|</span><a href="#41394820">next</a><span>|</span><label class="collapse" for="c-41395622">[-]</label><label class="expand" for="c-41395622">[3 more]</label></div><br/><div class="children"><div class="content">Of course Comrade, there must be consequences for these firms pushing Counter-Revolutionary content. They can have free expression, but they must realize these algorithms are causing great harm to the Proletariat by platforming such content.</div><br/><div id="41398255" class="c"><input type="checkbox" id="c-41398255" checked=""/><div class="controls bullet"><span class="by">BriggyDwiggs42</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41395622">parent</a><span>|</span><a href="#41396915">next</a><span>|</span><label class="collapse" for="c-41398255">[-]</label><label class="expand" for="c-41398255">[1 more]</label></div><br/><div class="children"><div class="content">Brother, the child asphyxiation challenge isn’t political content getting unfairly banned. They would only be liable for harm that can be proven, far as I’m aware, so political speech wouldn’t be affected unless it was defamatory or something like a direct threat.</div><br/></div></div><div id="41396915" class="c"><input type="checkbox" id="c-41396915" checked=""/><div class="controls bullet"><span class="by">stale2002</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41395622">parent</a><span>|</span><a href="#41398255">prev</a><span>|</span><a href="#41394820">next</a><span>|</span><label class="collapse" for="c-41396915">[-]</label><label class="expand" for="c-41396915">[1 more]</label></div><br/><div class="children"><div class="content">Well it would be the same exact protections that are provided to everyone else&#x27;s free speech.<p>Yes, if you as a person start making death threats or direct calls to violence, then you could be held liable for that.<p>Were you not aware of that?<p>An algorithm isn&#x27;t any different from any other sort of speech.</div><br/></div></div></div></div></div></div><div id="41394820" class="c"><input type="checkbox" id="c-41394820" checked=""/><div class="controls bullet"><span class="by">pelorat</span><span>|</span><a href="#41392458">parent</a><span>|</span><a href="#41392983">prev</a><span>|</span><a href="#41392690">next</a><span>|</span><label class="collapse" for="c-41394820">[-]</label><label class="expand" for="c-41394820">[1 more]</label></div><br/><div class="children"><div class="content">All large platforms already enact EU law over US law. Moderation is required of all online services which actively target EU users in order to shield themselves from liability for user generated content. The directive in question is 2000&#x2F;31&#x2F;EC and is 24 years old already. It&#x27;s the precursor of the EU DSA and just like it, 2000&#x2F;31&#x2F;EC has extraterritorial reach.</div><br/></div></div><div id="41392690" class="c"><input type="checkbox" id="c-41392690" checked=""/><div class="controls bullet"><span class="by">passwordoops</span><span>|</span><a href="#41392458">parent</a><span>|</span><a href="#41394820">prev</a><span>|</span><a href="#41397538">next</a><span>|</span><label class="collapse" for="c-41392690">[-]</label><label class="expand" for="c-41392690">[34 more]</label></div><br/><div class="children"><div class="content">How is an elected government with checks and balances worse than a faceless corporation?</div><br/><div id="41392996" class="c"><input type="checkbox" id="c-41392996" checked=""/><div class="controls bullet"><span class="by">dehrmann</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392690">parent</a><span>|</span><a href="#41392747">next</a><span>|</span><label class="collapse" for="c-41392996">[-]</label><label class="expand" for="c-41392996">[7 more]</label></div><br/><div class="children"><div class="content">I can sue the corporation. I can start a competing corporation.<p>Elected governments also aren&#x27;t as free as you&#x27;d think. Two parties control 99% of US politics. Suppose I&#x27;m not a fan of trade wars; both parties are in favor of them right now.</div><br/><div id="41398279" class="c"><input type="checkbox" id="c-41398279" checked=""/><div class="controls bullet"><span class="by">BriggyDwiggs42</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392996">parent</a><span>|</span><a href="#41393764">next</a><span>|</span><label class="collapse" for="c-41398279">[-]</label><label class="expand" for="c-41398279">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I can sue the corporation. I can start a competing corporation.<p>Yeah good luck with that buddy. I’m sorry, but you can’t do a thing to these behemoths. At least when a government bends you over it loses your vote, which sorta kinda matters to them. A corporation is incentivized to disregard your interests unless you are profitable to them, in which case they treat you like glorified livestock.</div><br/></div></div><div id="41393764" class="c"><input type="checkbox" id="c-41393764" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392996">parent</a><span>|</span><a href="#41398279">prev</a><span>|</span><a href="#41393164">next</a><span>|</span><label class="collapse" for="c-41393764">[-]</label><label class="expand" for="c-41393764">[4 more]</label></div><br/><div class="children"><div class="content">Big Tech <i>is</i> a government, we just call it a corporation.</div><br/><div id="41394188" class="c"><input type="checkbox" id="c-41394188" checked=""/><div class="controls bullet"><span class="by">matwood</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41393764">parent</a><span>|</span><a href="#41394255">prev</a><span>|</span><a href="#41395641">next</a><span>|</span><label class="collapse" for="c-41394188">[-]</label><label class="expand" for="c-41394188">[1 more]</label></div><br/><div class="children"><div class="content">And it&#x27;s unelected.</div><br/></div></div><div id="41395641" class="c"><input type="checkbox" id="c-41395641" checked=""/><div class="controls bullet"><span class="by">passwordoops</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41393764">parent</a><span>|</span><a href="#41394188">prev</a><span>|</span><a href="#41393164">next</a><span>|</span><label class="collapse" for="c-41395641">[-]</label><label class="expand" for="c-41395641">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what most people miss</div><br/></div></div></div></div><div id="41393164" class="c"><input type="checkbox" id="c-41393164" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392996">parent</a><span>|</span><a href="#41393764">prev</a><span>|</span><a href="#41392747">next</a><span>|</span><label class="collapse" for="c-41393164">[-]</label><label class="expand" for="c-41393164">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I can sue the corporation. I can start a competing corporation.<p>Ah, the libertarian way.<p>I, earning $40,000 a year will take on the corporate giant that has a multimillion dollar legal budget and 30 full time lawyers and win... I know, I saw it in a movie once.<p>The law books are filled with story after story of corporations doing fully illegal shit, then using money to delay it in court for decades... then laughably getting a tiny fine that represents less than 1% of the profits.<p>TANSTAFL.</div><br/></div></div></div></div><div id="41392747" class="c"><input type="checkbox" id="c-41392747" checked=""/><div class="controls bullet"><span class="by">bentley</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392690">parent</a><span>|</span><a href="#41392996">prev</a><span>|</span><a href="#41392718">next</a><span>|</span><label class="collapse" for="c-41392747">[-]</label><label class="expand" for="c-41392747">[13 more]</label></div><br/><div class="children"><div class="content">A faceless corporation can&#x27;t throw me in jail for hosting an indie web forum.</div><br/><div id="41392761" class="c"><input type="checkbox" id="c-41392761" checked=""/><div class="controls bullet"><span class="by">mikewarot</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392747">parent</a><span>|</span><a href="#41392899">next</a><span>|</span><label class="collapse" for="c-41392761">[-]</label><label class="expand" for="c-41392761">[5 more]</label></div><br/><div class="children"><div class="content">A faceless corporation could be encouraged to use it&#x27;s algorithm for profit in a way that gets you killed.... as was the main point of the article.</div><br/><div id="41392858" class="c"><input type="checkbox" id="c-41392858" checked=""/><div class="controls bullet"><span class="by">falcolas</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392761">parent</a><span>|</span><a href="#41394952">next</a><span>|</span><label class="collapse" for="c-41392858">[-]</label><label class="expand" for="c-41392858">[1 more]</label></div><br/><div class="children"><div class="content">That can also be done today by way of the government (at least in the US): swatting.<p>To be a bit cliched, there&#x27;s a rather a lot of inattention and time that lets a child kill themselves after watching a video.</div><br/></div></div><div id="41394952" class="c"><input type="checkbox" id="c-41394952" checked=""/><div class="controls bullet"><span class="by">tedunangst</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392761">parent</a><span>|</span><a href="#41392858">prev</a><span>|</span><a href="#41392836">next</a><span>|</span><label class="collapse" for="c-41394952">[-]</label><label class="expand" for="c-41394952">[1 more]</label></div><br/><div class="children"><div class="content">So the theory is the girl in question was going to start competing with TikTok, so they showed a suicide inducing video to silence her?</div><br/></div></div><div id="41392836" class="c"><input type="checkbox" id="c-41392836" checked=""/><div class="controls bullet"><span class="by">krapp</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392761">parent</a><span>|</span><a href="#41394952">prev</a><span>|</span><a href="#41392842">next</a><span>|</span><label class="collapse" for="c-41392836">[-]</label><label class="expand" for="c-41392836">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s far more abstract than sending men with guns to your house.</div><br/></div></div></div></div><div id="41392899" class="c"><input type="checkbox" id="c-41392899" checked=""/><div class="controls bullet"><span class="by">nradov</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392747">parent</a><span>|</span><a href="#41392761">prev</a><span>|</span><a href="#41392718">next</a><span>|</span><label class="collapse" for="c-41392899">[-]</label><label class="expand" for="c-41392899">[7 more]</label></div><br/><div class="children"><div class="content">True, but this particular case and Section 230 are only about <i>civil</i> liability. Regardless of the final outcome after the inevitable appeals, no one will go to jail. At most they&#x27;ll have to pay damages.</div><br/><div id="41393068" class="c"><input type="checkbox" id="c-41393068" checked=""/><div class="controls bullet"><span class="by">falcolas</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392899">parent</a><span>|</span><a href="#41392718">next</a><span>|</span><label class="collapse" for="c-41393068">[-]</label><label class="expand" for="c-41393068">[6 more]</label></div><br/><div class="children"><div class="content">&gt; no one will go to jail<p>Did you know that there&#x27;s has been a homeowner jailed for breaking his HOA&#x27;s rules about lawn maintenance?<p>The chances are good that someone will go to jail.</div><br/><div id="41393263" class="c"><input type="checkbox" id="c-41393263" checked=""/><div class="controls bullet"><span class="by">nradov</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41393068">parent</a><span>|</span><a href="#41392718">next</a><span>|</span><label class="collapse" for="c-41393263">[-]</label><label class="expand" for="c-41393263">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know that because it&#x27;s obviously false. If someone was jailed in relation to such a case then it was because they did something way beyond violating the HOA CC&amp;Rs, such as assaulting an HOA employee or refusing to comply with a court order. HOAs have no police powers and private criminal prosecutions haven&#x27;t been allowed in any US state for many years.<p>Citation needed.</div><br/><div id="41393360" class="c"><input type="checkbox" id="c-41393360" checked=""/><div class="controls bullet"><span class="by">falcolas</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41393263">parent</a><span>|</span><a href="#41392718">next</a><span>|</span><label class="collapse" for="c-41393360">[-]</label><label class="expand" for="c-41393360">[4 more]</label></div><br/><div class="children"><div class="content">Google is your friend. Sorry to be so trite, but there are literally dozens upon dozens of sources.<p>One such example happened in 2008. The man&#x27;s name is &quot;Joseph Prudente&quot;, and he was jailed because he could not pay the HOA fine for a brown lawn. Yes, there was a judge hitting Joseph Prudente with a &quot;contempt of court&quot; to land him in jail (with an end date of &quot;the lawn is fixed or the fine is paid&quot;), but his only &quot;crime&quot; was ever being too poor to maintain his lawn to the HOA&#x27;s standards.<p>&gt; “It’s a sad situation,” says [HOA] board president Bob Ryan. “But in the end, I have to say he brought it upon himself.”</div><br/><div id="41394638" class="c"><input type="checkbox" id="c-41394638" checked=""/><div class="controls bullet"><span class="by">nradov</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41393360">parent</a><span>|</span><a href="#41392718">next</a><span>|</span><label class="collapse" for="c-41394638">[-]</label><label class="expand" for="c-41394638">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not my job to do your legal research for you and you&#x27;re misrepresenting the facts of the case.<p>As I expected, Mr. Prudente wasn&#x27;t jailed for violating a HOA rule but rather for refusing to comply with a regular court order. It&#x27;s a tragic situation and I sympathize with the defendant but when someone buys property in an HOA they agree to comply with the CC&amp;R. If they subsequently lack the financial means to comply then they have the option of selling the property, or of filing bankruptcy which would at least delay most collections activities. HOAs are not charities, and poverty is not a legally valid reason for failing to meet contractual obligations.</div><br/><div id="41396204" class="c"><input type="checkbox" id="c-41396204" checked=""/><div class="controls bullet"><span class="by">falcolas</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41394638">parent</a><span>|</span><a href="#41392718">next</a><span>|</span><label class="collapse" for="c-41396204">[-]</label><label class="expand" for="c-41396204">[2 more]</label></div><br/><div class="children"><div class="content">So, having a bad lawn is ultimately worse than being convicted of a crime, maybe even of killing someone, since there&#x27;s no sentence. There&#x27;s no appeal. There&#x27;s no concept of &quot;doing your time&quot;. Your lawn goes brown, and you can be put in jail forever because they got a court order which makes it all perfectly legal.<p>&gt; It&#x27;s not my job to do your legal research for you and you&#x27;re misrepresenting the facts of the case.<p>So, since it&#x27;s not your job, you&#x27;re happy to be ignorant of what can be found with a simple Google search? It&#x27;s not looking up legal precedent or finding a section in the reams of law - it&#x27;s a well reported and repeated story.<p>And let&#x27;s be honest with each other - while by the letter of the law he was put into jail for failing to fulfill a court order, in practice he was put into jail for having a bad lawn. I&#x27;ll go so far to assert that the bits in between don&#x27;t really matter, since the failure to maintain the lawn lead directly to being in jail until the lawn was fixed.<p>So no, we don&#x27;t have a de jure debtor&#x27;s prison. But we do have a de facto debtor&#x27;s prison.</div><br/><div id="41396432" class="c"><input type="checkbox" id="c-41396432" checked=""/><div class="controls bullet"><span class="by">nradov</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41396204">parent</a><span>|</span><a href="#41392718">next</a><span>|</span><label class="collapse" for="c-41396432">[-]</label><label class="expand" for="c-41396432">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s be honest with each other: you&#x27;re attempting to distort and misrepresent what happened in one Florida case to try and support your narrative about what happened in a different and entirely unrelated federal case. The case of Anderson v. TikTok under discussion here doesn&#x27;t involve a contempt of court order, no one has gone to jail, nor has the trial court even reached a decision on damages.<p>The reality is that this case is going to spend years working through the normal appeals process. Before anyone panics or celebrates let&#x27;s be patient and wait for that to run its course. Until that happens it&#x27;s all speculation. Calm down.<p>The US legal system gives authority to judges to use contempt orders to jail people when necessary as a last resort. This is essential to make the system work because otherwise some people would just ignore orders with no consequence. Whether the underlying case is about a debt owed to an HOA or any other issue is irrelevant. And the party subject to a contempt order can always take that up with a higher court.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="41392718" class="c"><input type="checkbox" id="c-41392718" checked=""/><div class="controls bullet"><span class="by">genocidicbunny</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392690">parent</a><span>|</span><a href="#41392747">prev</a><span>|</span><a href="#41393589">next</a><span>|</span><label class="collapse" for="c-41392718">[-]</label><label class="expand" for="c-41392718">[4 more]</label></div><br/><div class="children"><div class="content">The government tends to have a monopoly on violence, which is quite the difference. A faceless corporation will have a harder time fining you, garnishing your wages, charging you with criminal acts. (For now at least...)</div><br/><div id="41393228" class="c"><input type="checkbox" id="c-41393228" checked=""/><div class="controls bullet"><span class="by">lcnPylGDnU4H9OF</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392718">parent</a><span>|</span><a href="#41393313">next</a><span>|</span><label class="collapse" for="c-41393228">[-]</label><label class="expand" for="c-41393228">[1 more]</label></div><br/><div class="children"><div class="content">Conversely, the US government in particular will have a harder time with bans (first amendment), shadow bans (sixth amendment), hiding details about their recommendation algorithms (FOIA). The &quot;checks and balances&quot; part is important.</div><br/></div></div><div id="41393313" class="c"><input type="checkbox" id="c-41393313" checked=""/><div class="controls bullet"><span class="by">mrguyorama</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392718">parent</a><span>|</span><a href="#41393228">prev</a><span>|</span><a href="#41393589">next</a><span>|</span><label class="collapse" for="c-41393313">[-]</label><label class="expand" for="c-41393313">[2 more]</label></div><br/><div class="children"><div class="content">&gt;The government tends to have a monopoly on violence<p>They don&#x27;t literally, as can be seen by that guy who got roughed up by the Pinkertons for the horror of accidentally being sent a Magic card he shouldn&#x27;t have been.<p>Nobody went to jail for that. So corporations have at least as much power over your life as the government, and you don&#x27;t get to vote out corporations.<p>Tell me, how do I &quot;choose a different company&quot; with, for example, Experian, who keeps losing my private info, refuses to assign me a valid credit score despite having a robust financial history, and can legally ruin my life?</div><br/><div id="41393490" class="c"><input type="checkbox" id="c-41393490" checked=""/><div class="controls bullet"><span class="by">aidenn0</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41393313">parent</a><span>|</span><a href="#41393589">next</a><span>|</span><label class="collapse" for="c-41393490">[-]</label><label class="expand" for="c-41393490">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They don&#x27;t literally, as can be seen by that guy who got roughed up by the Pinkertons for the horror of accidentally being sent a Magic card he shouldn&#x27;t have been.<p>Source for that?<p>I found [1] which sounds like intimidation; maybe a case for assault depending on how they &quot;frightened his wife&quot; but nothing about potentiall battery, which &quot;roughed up&quot; would seem to imply.  The Pinkertons do enough shady stuff that there&#x27;s not a need to exaggerate what they do.<p>1: <a href="https:&#x2F;&#x2F;www.polygon.com&#x2F;23695923&#x2F;mtg-aftermath-pinkerton-raid-leaked-cards" rel="nofollow">https:&#x2F;&#x2F;www.polygon.com&#x2F;23695923&#x2F;mtg-aftermath-pinkerton-rai...</a></div><br/></div></div></div></div></div></div><div id="41393589" class="c"><input type="checkbox" id="c-41393589" checked=""/><div class="controls bullet"><span class="by">lostmsu</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392690">parent</a><span>|</span><a href="#41392718">prev</a><span>|</span><a href="#41392755">next</a><span>|</span><label class="collapse" for="c-41393589">[-]</label><label class="expand" for="c-41393589">[3 more]</label></div><br/><div class="children"><div class="content">You can trivially choose not to associate with a corporation. You can&#x27;t really do so with your government.</div><br/><div id="41394059" class="c"><input type="checkbox" id="c-41394059" checked=""/><div class="controls bullet"><span class="by">vkou</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41393589">parent</a><span>|</span><a href="#41392755">next</a><span>|</span><label class="collapse" for="c-41394059">[-]</label><label class="expand" for="c-41394059">[2 more]</label></div><br/><div class="children"><div class="content">Trivially is doing a lot of lifting in that.<p>By that same logic, you can &#x27;trivially&#x27; influence a democratic government, you have no such control over a corporation.</div><br/><div id="41394456" class="c"><input type="checkbox" id="c-41394456" checked=""/><div class="controls bullet"><span class="by">opo</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41394059">parent</a><span>|</span><a href="#41392755">next</a><span>|</span><label class="collapse" for="c-41394456">[-]</label><label class="expand" for="c-41394456">[1 more]</label></div><br/><div class="children"><div class="content">&gt;...By that same logic, you can &#x27;trivially&#x27; influence a democratic government, you have no such control over a corporation.<p>That is a misrepresentation of the message you are replying too:<p>&gt;&gt;You can trivially choose not to associate with a corporation. You can&#x27;t really do so with your government.<p>You won&#x27;t get into legal trouble if you don&#x27;t have a Facebook account, or a Twitter account, or use a search engine than Google, etc.  Try to ignore the rules setup by your government and you will very quickly learn what having a monopoly of physical force within a given territory means.  This is a huge difference between the two.<p>As far as influencing a government or a corporation, I suspect (for example) that a letter to the CEO of even a large corporation will generally have more impact than a letter to the POTUS.  (For example, customer emails forwarded from Bezos:  <a href="https:&#x2F;&#x2F;www.quora.com&#x2F;Whats-it-like-to-receive-a-question-mark-e-mail-from-Jeff-Bezos-at-Amazon-Inc" rel="nofollow">https:&#x2F;&#x2F;www.quora.com&#x2F;Whats-it-like-to-receive-a-question-ma...</a>). This obviously will vary from company to company and maybe the President does something similar but my guess is maybe not.</div><br/></div></div></div></div></div></div><div id="41393541" class="c"><input type="checkbox" id="c-41393541" checked=""/><div class="controls bullet"><span class="by">gspencley</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392690">parent</a><span>|</span><a href="#41392755">prev</a><span>|</span><a href="#41397538">next</a><span>|</span><label class="collapse" for="c-41393541">[-]</label><label class="expand" for="c-41393541">[5 more]</label></div><br/><div class="children"><div class="content">Government is force. It is laws, police, courts and the ability to seriously screw up your life if it chooses.<p>A corporation might have &quot;power&quot; in an economic sense. It might have market significant presence in the marketplace. That presence might pressure or influence you in certain ways that you would prefer it not, such as the fact that all of your friends and family are customers&#x2F;users of that faceless corporation.<p>But what the corporation cannot do is put you in jail, seize your assets, prevent you from starting a business, dictate what you can or can&#x27;t do with your home etc.<p>Government is a necessary good. I&#x27;m no anarchist. But government is far more of a potential threat to liberty than the most &quot;powerful&quot; corporation could ever be.</div><br/><div id="41394057" class="c"><input type="checkbox" id="c-41394057" checked=""/><div class="controls bullet"><span class="by">ohashi</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41393541">parent</a><span>|</span><a href="#41394928">next</a><span>|</span><label class="collapse" for="c-41394057">[-]</label><label class="expand" for="c-41394057">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Banana_Wars#American_fruit_companies" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Banana_Wars#American_fruit_com...</a><p>You sure?</div><br/></div></div><div id="41394928" class="c"><input type="checkbox" id="c-41394928" checked=""/><div class="controls bullet"><span class="by">em-bee</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41393541">parent</a><span>|</span><a href="#41394057">prev</a><span>|</span><a href="#41397538">next</a><span>|</span><label class="collapse" for="c-41394928">[-]</label><label class="expand" for="c-41394928">[3 more]</label></div><br/><div class="children"><div class="content"><i>But what the corporation cannot do is put you in jail, seize your assets, prevent you from starting a business, dictate what you can or can&#x27;t do with your home etc.</i><p>a corporation can &quot;put me in jail&quot; for copyright violations, accuse me of criminal conduct (happened in the UK, took them years to fix), seize my money (paypal, etc),
destroy my business (amazon, google)...<p><i>But government is far more of a potential threat to liberty than the most &quot;powerful&quot; corporation could ever be.</i><p>you (in the US) should vote for a better government. i&#x27;ll trust my government to protect my liberty over most corporations any day.</div><br/><div id="41395673" class="c"><input type="checkbox" id="c-41395673" checked=""/><div class="controls bullet"><span class="by">srackey</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41394928">parent</a><span>|</span><a href="#41397538">next</a><span>|</span><label class="collapse" for="c-41395673">[-]</label><label class="expand" for="c-41395673">[2 more]</label></div><br/><div class="children"><div class="content">No, they can appeal to the state to get <i>them</i> to do it.<p>But you still think parliament actually controls the government as opposed to Whitehall, so I understand why this may be a little intellectually challenging for you.</div><br/><div id="41396511" class="c"><input type="checkbox" id="c-41396511" checked=""/><div class="controls bullet"><span class="by">em-bee</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41395673">parent</a><span>|</span><a href="#41397538">next</a><span>|</span><label class="collapse" for="c-41396511">[-]</label><label class="expand" for="c-41396511">[1 more]</label></div><br/><div class="children"><div class="content"><i>they can appeal to the state to get them to do it</i><p>the end result is the same.<p><i>Whitehall</i><p>1: i was talking about the US government and my own.<p>2: i am not from the UK<p>therefore your comment is entirely inappropriate.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41397538" class="c"><input type="checkbox" id="c-41397538" checked=""/><div class="controls bullet"><span class="by">dartharva</span><span>|</span><a href="#41392458">parent</a><span>|</span><a href="#41392690">prev</a><span>|</span><a href="#41392769">next</a><span>|</span><label class="collapse" for="c-41397538">[-]</label><label class="expand" for="c-41397538">[1 more]</label></div><br/><div class="children"><div class="content">Well as these social networks are increasingly dominating internet use to the level that they end up being the only thing used in the internet by constituent plebeians, it makes sense that they receive as much regulatory oversight as telecom providers do.</div><br/></div></div><div id="41392769" class="c"><input type="checkbox" id="c-41392769" checked=""/><div class="controls bullet"><span class="by">JumpCrisscross</span><span>|</span><a href="#41392458">parent</a><span>|</span><a href="#41397538">prev</a><span>|</span><a href="#41393325">next</a><span>|</span><label class="collapse" for="c-41392769">[-]</label><label class="expand" for="c-41392769">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Governments will define the rules by which communication services (and social networks) should operate</i><p>As opposed to when they didn’t?</div><br/></div></div><div id="41393325" class="c"><input type="checkbox" id="c-41393325" checked=""/><div class="controls bullet"><span class="by">jlarocco</span><span>|</span><a href="#41392458">parent</a><span>|</span><a href="#41392769">prev</a><span>|</span><a href="#41392610">next</a><span>|</span><label class="collapse" for="c-41393325">[-]</label><label class="expand" for="c-41393325">[1 more]</label></div><br/><div class="children"><div class="content">I feel like that&#x27;s a poor interpretation of what happened.  Corporations and businesses don&#x27;t inherently have rights - they only have them because we&#x27;ve granted them certain rights, and we <i>already</i> put limits on them.  We don&#x27;t allow cigarette, alcohol, and marijuana advertising to children, for example.  And now they&#x27;ll have to face the consequences of sending stupid stuff like the &quot;black out challenge&quot; to children.<p>It&#x27;s one thing to say, &quot;Some idiot posted this on our platform.&quot;  It&#x27;s another thing altogether to promote and endorse the post and send it out to everybody.<p>Businesses should be held responsible for their actions.</div><br/></div></div><div id="41392610" class="c"><input type="checkbox" id="c-41392610" checked=""/><div class="controls bullet"><span class="by">whatshisface</span><span>|</span><a href="#41392458">parent</a><span>|</span><a href="#41393325">prev</a><span>|</span><a href="#41393898">next</a><span>|</span><label class="collapse" for="c-41392610">[-]</label><label class="expand" for="c-41392610">[3 more]</label></div><br/><div class="children"><div class="content">Given that the alternative was public control over governments, I guess it&#x27;s inevitable that this would become a worldwide civil rights battle.</div><br/><div id="41394357" class="c"><input type="checkbox" id="c-41394357" checked=""/><div class="controls bullet"><span class="by">zerodensity</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41392610">parent</a><span>|</span><a href="#41393898">next</a><span>|</span><label class="collapse" for="c-41394357">[-]</label><label class="expand" for="c-41394357">[2 more]</label></div><br/><div class="children"><div class="content">What does public control over governments mean?</div><br/><div id="41394637" class="c"><input type="checkbox" id="c-41394637" checked=""/><div class="controls bullet"><span class="by">whatshisface</span><span>|</span><a href="#41392458">root</a><span>|</span><a href="#41394357">parent</a><span>|</span><a href="#41393898">next</a><span>|</span><label class="collapse" for="c-41394637">[-]</label><label class="expand" for="c-41394637">[1 more]</label></div><br/><div class="children"><div class="content">It means that the process of assimilating new information, coming to conclusions, and deciding what a nation should do is carried out in the minds of the public, not in the offices of relatively small groups who decide what they want the government to do, figure out what conclusions would support it, and then make sure the public only assimilates information that would lead them to such conclusions.</div><br/></div></div></div></div></div></div><div id="41393898" class="c"><input type="checkbox" id="c-41393898" checked=""/><div class="controls bullet"><span class="by">blackeyeblitzar</span><span>|</span><a href="#41392458">parent</a><span>|</span><a href="#41392610">prev</a><span>|</span><a href="#41392673">next</a><span>|</span><label class="collapse" for="c-41393898">[-]</label><label class="expand" for="c-41393898">[1 more]</label></div><br/><div class="children"><div class="content">I think it is broader than that. It’s government control over the Internet. Sure we’re talking about forced moderation (that is, censorship) and liability issues <i>right now</i>. But it ultimately normalizes a type of intervention and method of control that can extend much further. Just like we’ve seen the Patriot Act normalize many violations of civil liberties, this will go much further. I hope not, but I can’t help but be cynical when I see the degree to which censorship by tech oligarchs has been accepted by society over the last 8 years.</div><br/></div></div><div id="41392673" class="c"><input type="checkbox" id="c-41392673" checked=""/><div class="controls bullet"><span class="by">titusjohnson</span><span>|</span><a href="#41392458">parent</a><span>|</span><a href="#41393898">prev</a><span>|</span><a href="#41392923">next</a><span>|</span><label class="collapse" for="c-41392673">[-]</label><label class="expand" for="c-41392673">[1 more]</label></div><br/><div class="children"><div class="content">Is it really adding governmental control, or is it removing a governmental control? From my perspective Section 280 was controlling me, a private citizen, by saying &quot;you cannot touch these entities&quot;</div><br/></div></div><div id="41392923" class="c"><input type="checkbox" id="c-41392923" checked=""/><div class="controls bullet"><span class="by">krapp</span><span>|</span><a href="#41392458">parent</a><span>|</span><a href="#41392673">prev</a><span>|</span><a href="#41394738">next</a><span>|</span><label class="collapse" for="c-41392923">[-]</label><label class="expand" for="c-41392923">[1 more]</label></div><br/><div class="children"><div class="content">The fix was in as soon as both parties came up with a rationale to support it and people openly started speaking about &quot;algorithms&quot; in the same spooky scary tones usually reserved for implied communist threats.</div><br/></div></div></div></div><div id="41394738" class="c"><input type="checkbox" id="c-41394738" checked=""/><div class="controls bullet"><span class="by">hnburnsy</span><span>|</span><a href="#41392458">prev</a><span>|</span><a href="#41396099">next</a><span>|</span><label class="collapse" for="c-41394738">[-]</label><label class="expand" for="c-41394738">[1 more]</label></div><br/><div class="children"><div class="content">To me this decision doesn&#x27;t feel it is demolishing 230, but reducing its scope, a scope that was exanded by other court decisions. Per the article 230 said not liable for user content and not liable for restricting content. This case is about liability for reinforcing content.<p>Would love to have a timeline only, non reinforcing content feed.</div><br/></div></div><div id="41396099" class="c"><input type="checkbox" id="c-41396099" checked=""/><div class="controls bullet"><span class="by">Smithalicious</span><span>|</span><a href="#41394738">prev</a><span>|</span><a href="#41396335">next</a><span>|</span><label class="collapse" for="c-41396099">[-]</label><label class="expand" for="c-41396099">[3 more]</label></div><br/><div class="children"><div class="content">Hurting kids, hurting kids, hurting kids -- but, of course, there is zero chance any of this makes it to the top 30 causes of child mortality. Much to complain about with big tech, but children hanging themselves is just an outlier.</div><br/><div id="41397764" class="c"><input type="checkbox" id="c-41397764" checked=""/><div class="controls bullet"><span class="by">itsdrewmiller</span><span>|</span><a href="#41396099">parent</a><span>|</span><a href="#41396127">next</a><span>|</span><label class="collapse" for="c-41397764">[-]</label><label class="expand" for="c-41397764">[1 more]</label></div><br/><div class="children"><div class="content">This would be considered &quot;accidental injury&quot; which is the #1 cause of teenager mortality.  The #3 cause is suicide which is influenced by social media as well - <a href="https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC6278213&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC6278213&#x2F;</a></div><br/></div></div></div></div><div id="41396335" class="c"><input type="checkbox" id="c-41396335" checked=""/><div class="controls bullet"><span class="by">ssalka</span><span>|</span><a href="#41396099">prev</a><span>|</span><a href="#41394037">next</a><span>|</span><label class="collapse" for="c-41396335">[-]</label><label class="expand" for="c-41396335">[2 more]</label></div><br/><div class="children"><div class="content">&gt; There is no way to run a targeted ad social media company with 40% margins if you have to make sure children aren’t harmed by your product.<p>More specific than being harmed by your product, Section 230 cares about <i>content you publish</i> and whether you are acting as a publisher (liable for content) or a platform (not liable for content). This quote is supposing what would happen if Section 230 were overturned. But in fact, there is a way that companies would protect themselves: simply don&#x27;t moderate content at all. Then you act purely as a platform, and don&#x27;t have to ever worry about being treated as a publisher. Of course, this would turn the whole internet into 4chan, which nobody wants. IMO, this is one of the main reasons Section 230 continues to be used in this way.</div><br/><div id="41396537" class="c"><input type="checkbox" id="c-41396537" checked=""/><div class="controls bullet"><span class="by">ssalka</span><span>|</span><a href="#41396335">parent</a><span>|</span><a href="#41394037">next</a><span>|</span><label class="collapse" for="c-41396537">[-]</label><label class="expand" for="c-41396537">[1 more]</label></div><br/><div class="children"><div class="content">Also want to note that the inverse solution that companies could take is to be overly Draconian in moderating content, so as to take down <i>anything</i> that could come back negatively on them (in this case, the role of publisher is assumed and thus content moderation needs to be sufficiently robust so as to cover the company&#x27;s ass).</div><br/></div></div></div></div><div id="41394037" class="c"><input type="checkbox" id="c-41394037" checked=""/><div class="controls bullet"><span class="by">blueflow</span><span>|</span><a href="#41396335">prev</a><span>|</span><a href="#41398431">next</a><span>|</span><label class="collapse" for="c-41394037">[-]</label><label class="expand" for="c-41394037">[5 more]</label></div><br/><div class="children"><div class="content">Might be a cultural difference (im not from the US), but leaving a 10 year unsupervised with content from (potentially malicious) strangers really throws me off.<p>Wouldn&#x27;t this be the perfect precedence case on why minors should not be allowed on social media?</div><br/><div id="41395363" class="c"><input type="checkbox" id="c-41395363" checked=""/><div class="controls bullet"><span class="by">Yeul</span><span>|</span><a href="#41394037">parent</a><span>|</span><a href="#41395847">next</a><span>|</span><label class="collapse" for="c-41395363">[-]</label><label class="expand" for="c-41395363">[1 more]</label></div><br/><div class="children"><div class="content">Look your kids are going to discover all kinds of nasty things online or offline so either you prepare them for it or it&#x27;s going to be like that scene in Stephen King&#x27;s Carrie.</div><br/></div></div><div id="41395847" class="c"><input type="checkbox" id="c-41395847" checked=""/><div class="controls bullet"><span class="by">EasyMark</span><span>|</span><a href="#41394037">parent</a><span>|</span><a href="#41395363">prev</a><span>|</span><a href="#41395234">next</a><span>|</span><label class="collapse" for="c-41395847">[-]</label><label class="expand" for="c-41395847">[1 more]</label></div><br/><div class="children"><div class="content">You are correct. US parents often use social media as a baby sitter and don’t pay attention to what they are watching. No 10 year old should be on social media or even the internet in an unsupervised manner; they are simply too impressionable and trusting. It’s just negligence, my kids never got SM accounts before 15, after I’d had time to introduce them to some common sense and much needed skepticism of people and information on the internet.</div><br/></div></div><div id="41395234" class="c"><input type="checkbox" id="c-41395234" checked=""/><div class="controls bullet"><span class="by">hyeonwho4</span><span>|</span><a href="#41394037">parent</a><span>|</span><a href="#41395847">prev</a><span>|</span><a href="#41398431">next</a><span>|</span><label class="collapse" for="c-41395234">[-]</label><label class="expand" for="c-41395234">[2 more]</label></div><br/><div class="children"><div class="content">I am also a little confused by this. I thought websites were not allowed to collect data from minors under 13 years of age, and that TikTok doesn&#x27;t allow minors under 13 to create accounts. Why is TikTok not liable for personalizing content to minors? Apparently (from the court filings) TikTok even knew these videos were going viral among children... which should increase their liability under the Children&#x27;s Online Privacy Protection Act.</div><br/><div id="41396135" class="c"><input type="checkbox" id="c-41396135" checked=""/><div class="controls bullet"><span class="by">ratorx</span><span>|</span><a href="#41394037">root</a><span>|</span><a href="#41395234">parent</a><span>|</span><a href="#41398431">next</a><span>|</span><label class="collapse" for="c-41396135">[-]</label><label class="expand" for="c-41396135">[1 more]</label></div><br/><div class="children"><div class="content">Assuming TikTok collect age, and the minimum possible age is 13 (ToS) and a parent lets their child access the app despite that, I don’t see how TikTok is liable.<p>Also, I’m not sure how TikTok would know that the videos are viral among the protected demographic if the protected demographic cannot even put in the information to classify them as such?<p>I don’t think requiring moderation is the answer in all cases. As an adult, I should be allowed to consume unmoderated content. Should people younger than 18 be allowed to? Maybe.<p>I agree that below age X, all content should be moderated. If you choose not to do this for your platform, then age-restrict the content. However, historically age-restriction on the internet is an unsolved problem. I think what would be useful is tighter legislation on how this is enforced etc.<p>This case is not a moderation question. It is a liability question, because a minor has been granted access to age-restricted content. I think the key question is whether TikTok should be liable for the child&#x2F;their parents having bypassed the age restriction (too easily)? Maybe. I’m leaning towards the opinion that a large amount of this responsibility is on the parents. If this is onerous, then the law should legislate stricter guidelines on content targeting the protected demographic as well as the gates blocking them.</div><br/></div></div></div></div></div></div><div id="41398431" class="c"><input type="checkbox" id="c-41398431" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#41394037">prev</a><span>|</span><a href="#41398464">next</a><span>|</span><label class="collapse" for="c-41398431">[-]</label><label class="expand" for="c-41398431">[1 more]</label></div><br/><div class="children"><div class="content">Hoo boy.<p>So- platforms aren’t publishers, they are distributors (like news stands or pharmacies).<p>So they are responsible for the goods they sell.<p>They aren’t responsible for user content - but they are responsible for what they choose to show.<p>This is going to be dramatic.</div><br/></div></div><div id="41398464" class="c"><input type="checkbox" id="c-41398464" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#41398431">prev</a><span>|</span><a href="#41395886">next</a><span>|</span><label class="collapse" for="c-41398464">[-]</label><label class="expand" for="c-41398464">[1 more]</label></div><br/><div class="children"><div class="content">This seems like it contradicts the case where YouTube wasn’t liable for recommending terrorist videos to someone.</div><br/></div></div><div id="41392710" class="c"><input type="checkbox" id="c-41392710" checked=""/><div class="controls bullet"><span class="by">WCSTombs</span><span>|</span><a href="#41395886">prev</a><span>|</span><a href="#41398156">next</a><span>|</span><label class="collapse" for="c-41392710">[-]</label><label class="expand" for="c-41392710">[14 more]</label></div><br/><div class="children"><div class="content">From the article:<p>&gt; Because TikTok’s “algorithm curates and recommends a tailored compilation of videos for a user’s FYP based on a variety of factors, including the user’s age and other demographics, online interactions, and other metadata,” it becomes TikTok’s own speech. And now TikTok has to answer for it in court. Basically, the court ruled that when a company is choosing what to show kids and elderly parents, and seeks to keep them addicted to sell more ads, they can’t pretend it’s everyone else’s fault when the inevitable horrible thing happens.<p>If that reading is correct, then Section 230 isn&#x27;t nullified, but there&#x27;s something that isn&#x27;t shielded from liability any more, which IIUC is basically the &quot;Recommended For You&quot;-type content feed curation algorithms. But I haven&#x27;t read the ruling itself, so it could potentially be more expansive than that.<p>But assuming Matt Stoller&#x27;s analysis there is accurate: frankly, I avoid those recommendation systems like the plague anyway, so if the platforms have to roll them back or at least be a little more thoughtful about how they&#x27;re implemented, it&#x27;s not necessarily a bad thing. There&#x27;s no new liability for what users post (which is good overall IMO), but there can be liability <i>for the platform implementation itself</i> in some cases. But I think we&#x27;ll have to see how this plays out.</div><br/><div id="41392882" class="c"><input type="checkbox" id="c-41392882" checked=""/><div class="controls bullet"><span class="by">falcolas</span><span>|</span><a href="#41392710">parent</a><span>|</span><a href="#41394569">next</a><span>|</span><label class="collapse" for="c-41392882">[-]</label><label class="expand" for="c-41392882">[12 more]</label></div><br/><div class="children"><div class="content">What is &quot;recommended for you&quot; if not a search result with no terms? From a practical point of view, unless you go the route of OnlyFans and disallow discovery on your own website, how do you allow any discovery if any form of algorithmic recommendation is outlawed?</div><br/><div id="41392926" class="c"><input type="checkbox" id="c-41392926" checked=""/><div class="controls bullet"><span class="by">lcnPylGDnU4H9OF</span><span>|</span><a href="#41392710">root</a><span>|</span><a href="#41392882">parent</a><span>|</span><a href="#41394569">next</a><span>|</span><label class="collapse" for="c-41392926">[-]</label><label class="expand" for="c-41392926">[11 more]</label></div><br/><div class="children"><div class="content">If it were the results of a search with no terms then it wouldn&#x27;t be &quot;for&quot; a given subject. The &quot;you&quot; in &quot;recommended for you&quot; is the search term.</div><br/><div id="41392968" class="c"><input type="checkbox" id="c-41392968" checked=""/><div class="controls bullet"><span class="by">falcolas</span><span>|</span><a href="#41392710">root</a><span>|</span><a href="#41392926">parent</a><span>|</span><a href="#41394569">next</a><span>|</span><label class="collapse" for="c-41392968">[-]</label><label class="expand" for="c-41392968">[10 more]</label></div><br/><div class="children"><div class="content">That&#x27;s just branding. It&#x27;s called Home in Facebook and Instagram, and it&#x27;s the exact same thing. It&#x27;s a form of discovery that&#x27;s tailored to the user, just like normal searches are (even on Google and Bing etc).</div><br/><div id="41393055" class="c"><input type="checkbox" id="c-41393055" checked=""/><div class="controls bullet"><span class="by">lcnPylGDnU4H9OF</span><span>|</span><a href="#41392710">root</a><span>|</span><a href="#41392968">parent</a><span>|</span><a href="#41394569">next</a><span>|</span><label class="collapse" for="c-41393055">[-]</label><label class="expand" for="c-41393055">[9 more]</label></div><br/><div class="children"><div class="content">Indeed, regardless of the branding for the feature, the service is making a decision about what to show a given user based on what the service knows about them. That is not a search result with no terms; the user is the term.</div><br/><div id="41393149" class="c"><input type="checkbox" id="c-41393149" checked=""/><div class="controls bullet"><span class="by">falcolas</span><span>|</span><a href="#41392710">root</a><span>|</span><a href="#41393055">parent</a><span>|</span><a href="#41394569">next</a><span>|</span><label class="collapse" for="c-41393149">[-]</label><label class="expand" for="c-41393149">[8 more]</label></div><br/><div class="children"><div class="content">Now for a followup question: How does <i>any</i> website surface <i>any</i> content when they&#x27;re liable for the content?<p>When you can be held liable for surfacing the wrong (for unclear definitions of wrong) content to the wrong person, even Google could be held liable. Imagine if this child found a blackout video on the fifth page of their search results on &quot;blackout&quot;. After all, YouTube hosted such videos as well.</div><br/><div id="41393334" class="c"><input type="checkbox" id="c-41393334" checked=""/><div class="controls bullet"><span class="by">lcnPylGDnU4H9OF</span><span>|</span><a href="#41392710">root</a><span>|</span><a href="#41393149">parent</a><span>|</span><a href="#41393843">next</a><span>|</span><label class="collapse" for="c-41393334">[-]</label><label class="expand" for="c-41393334">[3 more]</label></div><br/><div class="children"><div class="content">TikTok is not being held liable for hosting and serving the content. They&#x27;re being held liable for recommending the content to a user with no other search context provided by said user. In this case, it is <i>because the visitor of the site was a young girl</i> that they chose to surface this video and there was no other context. The girl did not search &quot;blackout&quot;.</div><br/><div id="41393561" class="c"><input type="checkbox" id="c-41393561" checked=""/><div class="controls bullet"><span class="by">falcolas</span><span>|</span><a href="#41392710">root</a><span>|</span><a href="#41393334">parent</a><span>|</span><a href="#41393843">next</a><span>|</span><label class="collapse" for="c-41393561">[-]</label><label class="expand" for="c-41393561">[2 more]</label></div><br/><div class="children"><div class="content">&gt; because the visitor of the site was a young girl that they chose to surface this video<p>That&#x27;s one hell of a specific accusation - that they looked at her age alone and determined solely based on that to show her that specific video?<p>First off, at 10, she should have had an age-gated account that shows curated content specifically for children. There&#x27;s nothing to indicate that her parents set up such an account for her.<p>Also, it&#x27;s well understood that Tiktok takes a user&#x27;s previously watched videos into account when recommending videos. It can identify traits about the people based off that (and by personal experience, I can assert that it will lock down your account if it thinks you&#x27;re a child), but they have no hard data on someone&#x27;s age. Something about her video history triggered displaying this video (alongside thousands of other videos).<p>Finally, no, the girl did not do a search (that we&#x27;re aware of). But would the judge&#x27;s opinion have changed? I don&#x27;t believe so, based off of their logic. TikTok used an algorithm to recommend a video. TikTok uses that same algorithm with a filter to show search results.<p>In any case, a tragedy happened. But putting the blame on TikTok seems more like an attack on TikTok and not an attempt to reign in the industry at large.<p>Plus, at some point, we have to ask the question: where were the parents in all of this?<p>Anyways.</div><br/><div id="41393688" class="c"><input type="checkbox" id="c-41393688" checked=""/><div class="controls bullet"><span class="by">lcnPylGDnU4H9OF</span><span>|</span><a href="#41392710">root</a><span>|</span><a href="#41393561">parent</a><span>|</span><a href="#41393843">next</a><span>|</span><label class="collapse" for="c-41393688">[-]</label><label class="expand" for="c-41393688">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That&#x27;s one hell of a specific accusation - that they looked at her age alone and determined solely based on that to show her that specific video?<p>I suppose I did not phrase that very carefully. What I meant is that they chose to surface the video because a specific young girl visited the site -- one who had a specific history of watched videos.<p>&gt; In any case, a tragedy happened. But putting the blame on TikTok seems more like an attack on TikTok and not an attempt to reign in the industry at large.<p>It&#x27;s always going to start with one case. This could be protectionism but it very well could instead be the start of reining in the industry.</div><br/></div></div></div></div></div></div><div id="41393843" class="c"><input type="checkbox" id="c-41393843" checked=""/><div class="controls bullet"><span class="by">kaibee</span><span>|</span><a href="#41392710">root</a><span>|</span><a href="#41393149">parent</a><span>|</span><a href="#41393334">prev</a><span>|</span><a href="#41394569">next</a><span>|</span><label class="collapse" for="c-41393843">[-]</label><label class="expand" for="c-41393843">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Now for a followup question: How does any website surface any content when they&#x27;re liable for the content?<p>Chronological order, location based, posts-by-followed-accounts, etc.  &quot;Most liked&quot;, etc.<p>Essentially by only using &#x27;simple&#x27; algorithms.</div><br/><div id="41394134" class="c"><input type="checkbox" id="c-41394134" checked=""/><div class="controls bullet"><span class="by">TylerE</span><span>|</span><a href="#41392710">root</a><span>|</span><a href="#41393843">parent</a><span>|</span><a href="#41394569">next</a><span>|</span><label class="collapse" for="c-41394134">[-]</label><label class="expand" for="c-41394134">[3 more]</label></div><br/><div class="children"><div class="content">Is not the set of such things offered still editorial judgement?<p>(And as an addendum, even if you think the answer to that is no, do you trust a judge who can probably barely work an iphone to come to the same conclusion, with your company in the crosshairs?)</div><br/><div id="41394308" class="c"><input type="checkbox" id="c-41394308" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#41392710">root</a><span>|</span><a href="#41394134">parent</a><span>|</span><a href="#41394557">next</a><span>|</span><label class="collapse" for="c-41394308">[-]</label><label class="expand" for="c-41394308">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say no, because they averages over the entire group. If you ranked based on say, most liked in your friends circle, or most liked by people with a high cosine similarity to your profile, then it starts to slide back into editorial judgment.</div><br/></div></div><div id="41394557" class="c"><input type="checkbox" id="c-41394557" checked=""/><div class="controls bullet"><span class="by">skydhash</span><span>|</span><a href="#41392710">root</a><span>|</span><a href="#41394134">parent</a><span>|</span><a href="#41394308">prev</a><span>|</span><a href="#41394569">next</a><span>|</span><label class="collapse" for="c-41394557">[-]</label><label class="expand" for="c-41394557">[1 more]</label></div><br/><div class="children"><div class="content">Not really, as the variables comes from the content itself, not from the company intention.<p>And for the addendum, that&#x27;s why we have hearings and experts. No judge can be expected to be knowledgable about everything in life.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="41394569" class="c"><input type="checkbox" id="c-41394569" checked=""/><div class="controls bullet"><span class="by">itsdrewmiller</span><span>|</span><a href="#41392710">parent</a><span>|</span><a href="#41392882">prev</a><span>|</span><a href="#41398156">next</a><span>|</span><label class="collapse" for="c-41394569">[-]</label><label class="expand" for="c-41394569">[1 more]</label></div><br/><div class="children"><div class="content">This is only a circuit court ruling - there is a good chance it will be overturned by the supreme court.  The cited supreme court case (Moody v. NetChoice) does not require personalization:<p>&gt; presenting a curated and “edited compilation of [third party] speech” is itself protected speech.<p>This circuit court case mentions the personalization but doesn&#x27;t limit its judgment based on its presence - almost any type of curation other than the kind of moderation explicitly exempted by the CDA could create liability, though in practice I don&#x27;t think &quot;sorting by upvotes with some decay&quot; would end up qualifying.</div><br/></div></div></div></div><div id="41398156" class="c"><input type="checkbox" id="c-41398156" checked=""/><div class="controls bullet"><span class="by">tempeler</span><span>|</span><a href="#41392710">prev</a><span>|</span><a href="#41395210">next</a><span>|</span><label class="collapse" for="c-41398156">[-]</label><label class="expand" for="c-41398156">[1 more]</label></div><br/><div class="children"><div class="content">Finally, it goes to end of global social media. jurisdiction cannot be use as a weapon. if you use it as a weapon. they don&#x27;t hesitate use that to you as a weapon.</div><br/></div></div><div id="41395210" class="c"><input type="checkbox" id="c-41395210" checked=""/><div class="controls bullet"><span class="by">kevwil</span><span>|</span><a href="#41398156">prev</a><span>|</span><a href="#41398544">next</a><span>|</span><label class="collapse" for="c-41395210">[-]</label><label class="expand" for="c-41395210">[2 more]</label></div><br/><div class="children"><div class="content">Whatever this means, I hope it means less censorship. That&#x27;s all my feeble brain can focus on here: free speech good, censorship bad. :)</div><br/><div id="41395810" class="c"><input type="checkbox" id="c-41395810" checked=""/><div class="controls bullet"><span class="by">EasyMark</span><span>|</span><a href="#41395210">parent</a><span>|</span><a href="#41398544">next</a><span>|</span><label class="collapse" for="c-41395810">[-]</label><label class="expand" for="c-41395810">[1 more]</label></div><br/><div class="children"><div class="content">This judge supports censorship and not free speech, it a tendency of the current generation of judges populating the court. They prefer government control over personal responsibility in most cases, especially the more conservative they get.</div><br/></div></div></div></div><div id="41398544" class="c"><input type="checkbox" id="c-41398544" checked=""/><div class="controls bullet"><span class="by">rsingel</span><span>|</span><a href="#41395210">prev</a><span>|</span><a href="#41393853">next</a><span>|</span><label class="collapse" for="c-41398544">[-]</label><label class="expand" for="c-41398544">[1 more]</label></div><br/><div class="children"><div class="content">With no sense of irony, this blog is written on a platform that allows some Nazis, algorithmically promotes publishers, allows comments, and is thus only financially viable because of Section 230.<p>If you actually want to understand something about the decision, I highly recommend Eric Goldman&#x27;s blog post:<p><a href="https:&#x2F;&#x2F;blog.ericgoldman.org&#x2F;archives&#x2F;2024&#x2F;08&#x2F;bonkers-opinion-repeals-section-230-in-the-third-circuit-anderson-v-tiktok.htm" rel="nofollow">https:&#x2F;&#x2F;blog.ericgoldman.org&#x2F;archives&#x2F;2024&#x2F;08&#x2F;bonkers-opinio...</a></div><br/></div></div><div id="41393853" class="c"><input type="checkbox" id="c-41393853" checked=""/><div class="controls bullet"><span class="by">dwallin</span><span>|</span><a href="#41398544">prev</a><span>|</span><a href="#41397905">next</a><span>|</span><label class="collapse" for="c-41393853">[-]</label><label class="expand" for="c-41393853">[1 more]</label></div><br/><div class="children"><div class="content">The link to the actual decision:
<a href="https:&#x2F;&#x2F;cases.justia.com&#x2F;federal&#x2F;appellate-courts&#x2F;ca3&#x2F;22-3061&#x2F;22-3061-2024-08-27.pdf?ts=1724792413" rel="nofollow">https:&#x2F;&#x2F;cases.justia.com&#x2F;federal&#x2F;appellate-courts&#x2F;ca3&#x2F;22-306...</a></div><br/></div></div><div id="41397905" class="c"><input type="checkbox" id="c-41397905" checked=""/><div class="controls bullet"><span class="by">DidYaWipe</span><span>|</span><a href="#41393853">prev</a><span>|</span><a href="#41395872">next</a><span>|</span><label class="collapse" for="c-41397905">[-]</label><label class="expand" for="c-41397905">[1 more]</label></div><br/><div class="children"><div class="content">While this guy&#x27;s missives are not always on target (his one supporting the DOJ&#x27;s laughable and absurd case against Apple being an example of failure), some are on target... and indeed this ruling correctly calls out sites for exerting editorial control.<p>If you&#x27;re going to throw up your hands and say, &quot;Well, users posted this, not us!&quot; then you&#x27;d better not promote or bury any content with any algorithm, period. These assholes (TikTok et al) are now getting what they asked for with their abusive behavior.</div><br/></div></div><div id="41395872" class="c"><input type="checkbox" id="c-41395872" checked=""/><div class="controls bullet"><span class="by">game_the0ry</span><span>|</span><a href="#41397905">prev</a><span>|</span><a href="#41395973">next</a><span>|</span><label class="collapse" for="c-41395872">[-]</label><label class="expand" for="c-41395872">[2 more]</label></div><br/><div class="children"><div class="content">Pavel gets arrested, Brazil threatens Elon, now this.<p>I am not happy with how governments think they can dictate what internet users can and cannot see.<p>With respect to TikTok, parents need have some discipline and not give smart phones to their ten-year-olds. You might as well give them a crack pipe.</div><br/><div id="41396144" class="c"><input type="checkbox" id="c-41396144" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#41395872">parent</a><span>|</span><a href="#41395973">next</a><span>|</span><label class="collapse" for="c-41396144">[-]</label><label class="expand" for="c-41396144">[1 more]</label></div><br/><div class="children"><div class="content"><i>shrug</i> maybe our communication protocols should be distributed and not owned by billionaires.  That would solve this problem neatly.</div><br/></div></div></div></div><div id="41395973" class="c"><input type="checkbox" id="c-41395973" checked=""/><div class="controls bullet"><span class="by">ratorx</span><span>|</span><a href="#41395872">prev</a><span>|</span><a href="#41393412">next</a><span>|</span><label class="collapse" for="c-41395973">[-]</label><label class="expand" for="c-41395973">[3 more]</label></div><br/><div class="children"><div class="content">I think a bigger issue in this case is the age. A 10-year old should not have access to TikTok unsupervised, especially when the ToS states the 13-year age threshold, regardless of the law’s opinion on moderation.<p>I think especially content for children should be much more severely restricted, as it is with other media.<p>It’s pretty well-known that age is easy to fake on the internet. I think that’s something that needs tightening as well. I’m not sure what the best way to approach it is though. There’s a parental education aspect, but I don’t see how general content on the internet can be restricted without putting everything behind an ID-verified login screen or mandating parental filters, which seems quite unrealistic.</div><br/><div id="41396218" class="c"><input type="checkbox" id="c-41396218" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#41395973">parent</a><span>|</span><a href="#41393412">next</a><span>|</span><label class="collapse" for="c-41396218">[-]</label><label class="expand" for="c-41396218">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  I’m not sure what the best way to approach it is though.<p>Pretty much every option is full of pain, but I think the <i>least-terrible</i> approach would be for for sites to <i>describe</i> content with metadata (e.g. HTTP headers) and push all responsibility for blocking&#x2F;filtering onto the client device.<p>This has several benefits:<p>1. Cost. The people paying the most expense for the development and maintenance of blocking infrastructure will be the same parents who want to actually <i>use</i> it, instead of creating an enormous implicit tax on the entire digital world.<p>2. Privacy. The websites of the world don&#x27;t need to know anything at all about the user. No birthdays, no geographical information to figure out what legal jurisdiction they live in, and no giant national lookup database that can track every website any resident registers to. Just isolated local devices that could be as simple as a Boolean for whether the child lock is currently unabled. (In practice I&#x27;m sure there will be local user accounts.)<p>3. Leveraging physical security. Parents do not need to be programmers to understand and enforce &quot;little Timmy shouldn&#x27;t be using anything except the tablet we specially set up for him that&#x27;s covered with stickers of his favorite cartoon.&quot; Sure, Timmy might gain access to an unlocked device, but that&#x27;s a challenge parents and communities are equipped to understand and handle.<p>4. Rule complexity. The individual devices can be programmed with whatever the local legal rules are for ages of majority, or it can simply be parents&#x27; responsibility to change things on a notable birthday. Parents who think ankles on women should never be shown at <i>any</i> age would be responsible for putting on plugins that add extra restrictions, instead of forcing that logic on the rest of the world.</div><br/><div id="41396488" class="c"><input type="checkbox" id="c-41396488" checked=""/><div class="controls bullet"><span class="by">ratorx</span><span>|</span><a href="#41395973">root</a><span>|</span><a href="#41396218">parent</a><span>|</span><a href="#41393412">next</a><span>|</span><label class="collapse" for="c-41396488">[-]</label><label class="expand" for="c-41396488">[1 more]</label></div><br/><div class="children"><div class="content">I think this is the most privacy-friendly and reasonable approach. However, as a devil’s advocate, this is still pretty fingerprintable.<p>“Most users load n pages with ankles, the likelihood of a user only loading a single page with ankles is someone under the age of X from country Y with Z% likelihood”</div><br/></div></div></div></div></div></div><div id="41393412" class="c"><input type="checkbox" id="c-41393412" checked=""/><div class="controls bullet"><span class="by">jrockway</span><span>|</span><a href="#41395973">prev</a><span>|</span><a href="#41392686">next</a><span>|</span><label class="collapse" for="c-41393412">[-]</label><label class="expand" for="c-41393412">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure that Big Tech is over.  Media companies have had a viable business forever.  What happens here is that instead of going to social media and hearing about how to fight insurance companies, you&#x27;ll just get NFL Wednesday Night Football Presented By TikTok.</div><br/></div></div><div id="41392686" class="c"><input type="checkbox" id="c-41392686" checked=""/><div class="controls bullet"><span class="by">falcolas</span><span>|</span><a href="#41393412">prev</a><span>|</span><a href="#41394716">next</a><span>|</span><label class="collapse" for="c-41392686">[-]</label><label class="expand" for="c-41392686">[5 more]</label></div><br/><div class="children"><div class="content">&gt; the internet grew tremendously, encompassing the kinds of activities that did not exist in 1996<p>I guess that&#x27;s one way to say that you never experienced the early internet. In three words: rotten dot com. Makes all the N-chans look like teenagers smoking on the corner, and Facebook et.al. look like toddlers in paddded cribs.<p>This will frankly hurt any and all attempts to host any content online, and if anyone can survive it, it will be the biggest corporations alone. Section 230 also protected ISPs and hosting companies (linode, Hetzer, etc) after all.<p>Their targeting may not be intentional, but will that matter? Are they willing to be jailed in a foreign country because of their perceived inaction?</div><br/><div id="41394244" class="c"><input type="checkbox" id="c-41394244" checked=""/><div class="controls bullet"><span class="by">stackskipton</span><span>|</span><a href="#41392686">parent</a><span>|</span><a href="#41393096">next</a><span>|</span><label class="collapse" for="c-41394244">[-]</label><label class="expand" for="c-41394244">[1 more]</label></div><br/><div class="children"><div class="content">This was purely about was &quot;Is using algorithms made you a publisher?&quot;, this judge ruled yes and therefore, no Section 230.<p>The Judge made no ruling on Section 230 protection for anyone who truly just hosts the content so ISPs&#x2F;Hosting Companies should be fine.</div><br/></div></div><div id="41393096" class="c"><input type="checkbox" id="c-41393096" checked=""/><div class="controls bullet"><span class="by">amanaplanacanal</span><span>|</span><a href="#41392686">parent</a><span>|</span><a href="#41394244">prev</a><span>|</span><a href="#41394716">next</a><span>|</span><label class="collapse" for="c-41393096">[-]</label><label class="expand" for="c-41393096">[3 more]</label></div><br/><div class="children"><div class="content">Jail? This was a civil suit, no criminal penalties apply, just monetary.</div><br/><div id="41393212" class="c"><input type="checkbox" id="c-41393212" checked=""/><div class="controls bullet"><span class="by">falcolas</span><span>|</span><a href="#41392686">root</a><span>|</span><a href="#41393096">parent</a><span>|</span><a href="#41394716">next</a><span>|</span><label class="collapse" for="c-41393212">[-]</label><label class="expand" for="c-41393212">[2 more]</label></div><br/><div class="children"><div class="content">Thanks to &quot;Contempt of Court&quot; anybody can go to jail, even if they&#x27;re not found liable for the presented case.<p>But more on point, we&#x27;re discussing modification of how laws are interpreted. If someone can be held civilly liable, why <i>can&#x27;t</i> they be held criminally liable if the &quot;recommended&quot; content breaks criminal laws (CSAM, for example)? There&#x27;s nothing that prevents this interpretation from being considered in a criminal case.</div><br/><div id="41396213" class="c"><input type="checkbox" id="c-41396213" checked=""/><div class="controls bullet"><span class="by">hn_acker</span><span>|</span><a href="#41392686">root</a><span>|</span><a href="#41393212">parent</a><span>|</span><a href="#41394716">next</a><span>|</span><label class="collapse" for="c-41396213">[-]</label><label class="expand" for="c-41396213">[1 more]</label></div><br/><div class="children"><div class="content">Section 230 already doesn&#x27;t apply to content that breaks federal criminal liability, so CSAM is already exempted. Certain third-party liability cases will still be protected by the First Amendment (no third-party liability without knowledge of CSAM, for example) but won&#x27;t be dismissed early by Section 230.</div><br/></div></div></div></div></div></div></div></div><div id="41394716" class="c"><input type="checkbox" id="c-41394716" checked=""/><div class="controls bullet"><span class="by">6gvONxR4sf7o</span><span>|</span><a href="#41392686">prev</a><span>|</span><label class="collapse" for="c-41394716">[-]</label><label class="expand" for="c-41394716">[1 more]</label></div><br/><div class="children"><div class="content">So under this new reading of the law, is it saying that AWS is still not liable for what someone says on reddit, but now reddit might be responsible for it?</div><br/></div></div></div></div></div></div></div></body></html>