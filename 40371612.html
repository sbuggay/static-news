<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1715850055887" as="style"/><link rel="stylesheet" href="styles.css?v=1715850055887"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/ggerganov/llama.cpp/pull/7154">New exponent functions that make SiLU and SoftMax 2x faster, at full accuracy</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>weinzierl</span> | <span>62 comments</span></div><br/><div><div id="40373241" class="c"><input type="checkbox" id="c-40373241" checked=""/><div class="controls bullet"><span class="by">mjcohen</span><span>|</span><a href="#40372955">next</a><span>|</span><label class="collapse" for="c-40373241">[-]</label><label class="expand" for="c-40373241">[5 more]</label></div><br/><div class="children"><div class="content">About 20 years ago, I was programming for the Hughes radar signal processor, a highly parallel pipelined machine which accounted for much of Hughes success in radar processing. Anyway, I needed to compute e^x for 0 &lt; x &lt; 1. The processor had a multiply, so I used four 256 long tables of e^x for each possible 8-bit values in the 4 blocks in the 32-bit word, multiplied them to get the final value. It was about 5 times as fast as the previous best e^x routine.
That machine was fun! It is obsolete now, but for many years is could process radar signals faster that processors that were nominally many times faster.</div><br/><div id="40373932" class="c"><input type="checkbox" id="c-40373932" checked=""/><div class="controls bullet"><span class="by">evmar</span><span>|</span><a href="#40373241">parent</a><span>|</span><a href="#40372955">next</a><span>|</span><label class="collapse" for="c-40373932">[-]</label><label class="expand" for="c-40373932">[4 more]</label></div><br/><div class="children"><div class="content">In case anyone else wasn&#x27;t quite following this, I think the idea is<p>e^x<p>= e^(a+b+c+d)  (where abcd are the bytes of x)<p>= e^a * e^b * e^d * e*d<p>and then you make a lookup table for each of those e^a, e^b values.<p>(I fudged &quot;a&quot; here, where it&#x27;s really more like &quot;high byte &lt;&lt; 24&quot;, but I think you just make your lookup table for e^a be a map from a =&gt; e^(a&lt;&lt;24), and similar for the other bytes.)</div><br/><div id="40374399" class="c"><input type="checkbox" id="c-40374399" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40373241">root</a><span>|</span><a href="#40373932">parent</a><span>|</span><a href="#40372955">next</a><span>|</span><label class="collapse" for="c-40374399">[-]</label><label class="expand" for="c-40374399">[3 more]</label></div><br/><div class="children"><div class="content">When I took Huffman&#x27;s Cybernetics course as an undergrad, his tests were full of multiplication and you couldn&#x27;t use a calculator, so he provided a table of exponents and logarithms, and you&#x27;d look up in one table, sum the values, then look up the result in another table.  I was not a fan.</div><br/><div id="40375113" class="c"><input type="checkbox" id="c-40375113" checked=""/><div class="controls bullet"><span class="by">gdevenyi</span><span>|</span><a href="#40373241">root</a><span>|</span><a href="#40374399">parent</a><span>|</span><a href="#40372955">next</a><span>|</span><label class="collapse" for="c-40375113">[-]</label><label class="expand" for="c-40375113">[2 more]</label></div><br/><div class="children"><div class="content">This is literally what a sliderule is.</div><br/></div></div></div></div></div></div></div></div><div id="40372955" class="c"><input type="checkbox" id="c-40372955" checked=""/><div class="controls bullet"><span class="by">mysteria</span><span>|</span><a href="#40373241">prev</a><span>|</span><a href="#40372830">next</a><span>|</span><label class="collapse" for="c-40372955">[-]</label><label class="expand" for="c-40372955">[8 more]</label></div><br/><div class="children"><div class="content">How much do these silu and softmax improvements affect the LLM inference speed as a whole? Correct me if I&#x27;m wrong but I feel that this change will only have a small effect as the majority of the time is spent doing matrix multiplications.</div><br/><div id="40373403" class="c"><input type="checkbox" id="c-40373403" checked=""/><div class="controls bullet"><span class="by">terafo</span><span>|</span><a href="#40372955">parent</a><span>|</span><a href="#40372830">next</a><span>|</span><label class="collapse" for="c-40373403">[-]</label><label class="expand" for="c-40373403">[7 more]</label></div><br/><div class="children"><div class="content">Overwhelming majority of flops is indeed spent on matmuls, but softmax disproportionately uses memory bandwidth, so it generally takes much longer than you&#x27;d expect from just looking at flops.</div><br/><div id="40373743" class="c"><input type="checkbox" id="c-40373743" checked=""/><div class="controls bullet"><span class="by">tehsauce</span><span>|</span><a href="#40372955">root</a><span>|</span><a href="#40373403">parent</a><span>|</span><a href="#40373504">next</a><span>|</span><label class="collapse" for="c-40373743">[-]</label><label class="expand" for="c-40373743">[1 more]</label></div><br/><div class="children"><div class="content">If cpu softmax were limited by memory bandwidth, then these vectorization optimizations wouldn&#x27;t improve performance.</div><br/></div></div><div id="40373504" class="c"><input type="checkbox" id="c-40373504" checked=""/><div class="controls bullet"><span class="by">cgearhart</span><span>|</span><a href="#40372955">root</a><span>|</span><a href="#40373403">parent</a><span>|</span><a href="#40373743">prev</a><span>|</span><a href="#40372830">next</a><span>|</span><label class="collapse" for="c-40373504">[-]</label><label class="expand" for="c-40373504">[5 more]</label></div><br/><div class="children"><div class="content">Why does it disproportionately use bandwidth?</div><br/><div id="40373861" class="c"><input type="checkbox" id="c-40373861" checked=""/><div class="controls bullet"><span class="by">jacobn</span><span>|</span><a href="#40372955">root</a><span>|</span><a href="#40373504">parent</a><span>|</span><a href="#40372830">next</a><span>|</span><label class="collapse" for="c-40373861">[-]</label><label class="expand" for="c-40373861">[4 more]</label></div><br/><div class="children"><div class="content">In transformers the attention matrix is N*N, so there are a lot of values to go over. Typically makes it memory bandwidth bound, not compute bound.</div><br/><div id="40373986" class="c"><input type="checkbox" id="c-40373986" checked=""/><div class="controls bullet"><span class="by">cgearhart</span><span>|</span><a href="#40372955">root</a><span>|</span><a href="#40373861">parent</a><span>|</span><a href="#40374992">next</a><span>|</span><label class="collapse" for="c-40373986">[-]</label><label class="expand" for="c-40373986">[1 more]</label></div><br/><div class="children"><div class="content">Oooooh, I forgot that the self attention layer has a softmax. I thought this was referring to a softmax on the dense forward layer. Thanks!<p>Next question: does the softmax in the SA block cause it to be bandwidth bound—won’t it have to materialize all the parameters of the N^2 matrix either way? Does SM cause redundant data reads?</div><br/></div></div><div id="40374992" class="c"><input type="checkbox" id="c-40374992" checked=""/><div class="controls bullet"><span class="by">bjornsing</span><span>|</span><a href="#40372955">root</a><span>|</span><a href="#40373861">parent</a><span>|</span><a href="#40373986">prev</a><span>|</span><a href="#40372830">next</a><span>|</span><label class="collapse" for="c-40374992">[-]</label><label class="expand" for="c-40374992">[2 more]</label></div><br/><div class="children"><div class="content">Wouldn’t the softmax typically be “fused” with the matmul though?</div><br/><div id="40375434" class="c"><input type="checkbox" id="c-40375434" checked=""/><div class="controls bullet"><span class="by">anewhnaccount2</span><span>|</span><a href="#40372955">root</a><span>|</span><a href="#40374992">parent</a><span>|</span><a href="#40372830">next</a><span>|</span><label class="collapse" for="c-40375434">[-]</label><label class="expand" for="c-40375434">[1 more]</label></div><br/><div class="children"><div class="content">Yes but as far as I understand this is only really usefully possible with FlashAttention. (The main idea is that you have to use the log-sum-exp trick when computing the softmax, but can&#x27;t compute the max activation incrementally so have to rescale everything.)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40372830" class="c"><input type="checkbox" id="c-40372830" checked=""/><div class="controls bullet"><span class="by">SeanAnderson</span><span>|</span><a href="#40372955">prev</a><span>|</span><a href="#40372944">next</a><span>|</span><label class="collapse" for="c-40372830">[-]</label><label class="expand" for="c-40372830">[6 more]</label></div><br/><div class="children"><div class="content">off topic, but sheesh. I was skimming this and thought, &quot;This seems like a crazy optimization. It&#x27;s complex and the code in question has had a ton of eyes on it already.&quot; and then I saw the contributor and was like &quot;Of course it&#x27;s jart. It&#x27;s <i>always</i> jart with the crazy good solutions.&quot;<p>well done :)</div><br/><div id="40372922" class="c"><input type="checkbox" id="c-40372922" checked=""/><div class="controls bullet"><span class="by">mhh__</span><span>|</span><a href="#40372830">parent</a><span>|</span><a href="#40374153">next</a><span>|</span><label class="collapse" for="c-40372922">[-]</label><label class="expand" for="c-40372922">[1 more]</label></div><br/><div class="children"><div class="content">&gt; adapted from arm limited optimized routine<p>Shoulders of giants and all that</div><br/></div></div><div id="40374153" class="c"><input type="checkbox" id="c-40374153" checked=""/><div class="controls bullet"><span class="by">larodi</span><span>|</span><a href="#40372830">parent</a><span>|</span><a href="#40372922">prev</a><span>|</span><a href="#40373054">next</a><span>|</span><label class="collapse" for="c-40374153">[-]</label><label class="expand" for="c-40374153">[3 more]</label></div><br/><div class="children"><div class="content">Its not something that they teach at asymptotic analysis lectures, right? which reminds me of professor who famously said - that constant that everyone&#x27;s just overlooking can in engineering terms very much eat your head.</div><br/><div id="40374319" class="c"><input type="checkbox" id="c-40374319" checked=""/><div class="controls bullet"><span class="by">boshalfoshal</span><span>|</span><a href="#40372830">root</a><span>|</span><a href="#40374153">parent</a><span>|</span><a href="#40375122">next</a><span>|</span><label class="collapse" for="c-40374319">[-]</label><label class="expand" for="c-40374319">[1 more]</label></div><br/><div class="children"><div class="content">Unrelated but I see this sentiment a lot. Algorithms (and related courses) are supposed to be largely mathematical&#x2F;theory based so it sort of has to be hardware agnostic.<p>Imo an algorithms course doesn&#x27;t really need to teach you about lowering the constant factor (unless the optimizations for lowering said factor are reasonably abstracted away from physical HW), thats the purpose of Computer Architecture or Operating Systems course.<p>And realistically in the real world you&#x27;d be benchmarking these things anyway if a supposedly &quot;optimal&quot; algorithm is really the bottleneck, and you&#x27;d apply context specific optimizations to speed it up.</div><br/></div></div><div id="40375122" class="c"><input type="checkbox" id="c-40375122" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#40372830">root</a><span>|</span><a href="#40374153">parent</a><span>|</span><a href="#40374319">prev</a><span>|</span><a href="#40373054">next</a><span>|</span><label class="collapse" for="c-40375122">[-]</label><label class="expand" for="c-40375122">[1 more]</label></div><br/><div class="children"><div class="content">A big C could be $3M in losses per year vs $2M in profits.<p>Or always coming in second on contracts or sales and getting $0.</div><br/></div></div></div></div><div id="40373054" class="c"><input type="checkbox" id="c-40373054" checked=""/><div class="controls bullet"><span class="by">neonsunset</span><span>|</span><a href="#40372830">parent</a><span>|</span><a href="#40374153">prev</a><span>|</span><a href="#40372944">next</a><span>|</span><label class="collapse" for="c-40373054">[-]</label><label class="expand" for="c-40373054">[1 more]</label></div><br/><div class="children"><div class="content">Mostly looks scary* because that&#x27;s just how it is with intrinsics syntax in C and C++. As with many things there, this pain is mostly self-inflicted.<p>There are C++ libraries that allow for C#-style SIMD and hardware instrinsics syntax as far as I&#x27;m aware. It comes at a disadvantage as you can&#x27;t directly lookup mnemonics in ISAs documentation though.<p>*not to seem as if dismissing the importance of the work done there, just highlighting that it could have been much more accessible to wider audience, but I&#x27;m not gonna suggest something that everyone here would consider preposterous such as rewriting inference back-end in C# just yet</div><br/></div></div></div></div><div id="40372944" class="c"><input type="checkbox" id="c-40372944" checked=""/><div class="controls bullet"><span class="by">mhh__</span><span>|</span><a href="#40372830">prev</a><span>|</span><a href="#40372320">next</a><span>|</span><label class="collapse" for="c-40372944">[-]</label><label class="expand" for="c-40372944">[7 more]</label></div><br/><div class="children"><div class="content">&gt; replaces short[65536] look up table<p>Is that not quite dim to begin with (having a LUT the size of the whole L1 cache?) or does it work surprisingly well because of some probabilistic fudging?</div><br/><div id="40373247" class="c"><input type="checkbox" id="c-40373247" checked=""/><div class="controls bullet"><span class="by">Tuna-Fish</span><span>|</span><a href="#40372944">parent</a><span>|</span><a href="#40373056">next</a><span>|</span><label class="collapse" for="c-40373247">[-]</label><label class="expand" for="c-40373247">[2 more]</label></div><br/><div class="children"><div class="content">The lookup table does surprisingly well because the workload is otherwise extremely cache-hostile, and it doesn&#x27;t really matter if you blow up your L1 cache, none of the data you evicted because you needed to fit the LUT was ever going to be reused anyway.<p>ML loads in general are streaming loads that linearly load the entire dataset for every iteration.</div><br/><div id="40373613" class="c"><input type="checkbox" id="c-40373613" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#40372944">root</a><span>|</span><a href="#40373247">parent</a><span>|</span><a href="#40373056">next</a><span>|</span><label class="collapse" for="c-40373613">[-]</label><label class="expand" for="c-40373613">[1 more]</label></div><br/><div class="children"><div class="content">One interesting option for these big memory scans on x86 and ARM CPUs is using the non-temporal load&#x2F;store instructions.  Those actually bypass caching* and may help with the cache pressure of LLM workloads that just do scans.  The lookup table is still probably the wrong solution even with this sort of thing.<p>* Not quite all of it - There are still buffers to do write combining and some read caching on scans.</div><br/></div></div></div></div><div id="40373056" class="c"><input type="checkbox" id="c-40373056" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#40372944">parent</a><span>|</span><a href="#40373247">prev</a><span>|</span><a href="#40372320">next</a><span>|</span><label class="collapse" for="c-40373056">[-]</label><label class="expand" for="c-40373056">[4 more]</label></div><br/><div class="children"><div class="content">Why you (probably) shouldn&#x27;t use a lookup table <a href="https:&#x2F;&#x2F;specbranch.com&#x2F;posts&#x2F;lookup-tables&#x2F;" rel="nofollow">https:&#x2F;&#x2F;specbranch.com&#x2F;posts&#x2F;lookup-tables&#x2F;</a> gives some discussion about when it&#x27;s appropriate generally. My narrow experience is that you can do a lot of real time calculation before it&#x27;s faster to do a lookup.</div><br/><div id="40373417" class="c"><input type="checkbox" id="c-40373417" checked=""/><div class="controls bullet"><span class="by">o11c</span><span>|</span><a href="#40372944">root</a><span>|</span><a href="#40373056">parent</a><span>|</span><a href="#40375024">next</a><span>|</span><label class="collapse" for="c-40373417">[-]</label><label class="expand" for="c-40373417">[2 more]</label></div><br/><div class="children"><div class="content">One case that doesn&#x27;t mention explicitly: sometimes your problem is lucky enough that you have the choice between:<p>1. for each function, for each datum, call function with datum<p>2. for each datum, for each function, call function with datum<p>For 1, if your main data is fetched predictably but does not saturate bandwidth, it can be beneficial to use even a fairly large LUT. Note also that while actual L1i is semi-ignorable, e.g. branch prediction benefits much more from remaining &quot;in cache&quot;.<p>For 2, assuming other functions also have their own miscellaneous data, the non-LUT approach is likely to win. But you should probably benchmark against #1.<p>But sometimes you can get the best of both worlds:<p>3. for each arbitrary chunk of data (smaller than L3, or maybe even L2 depending on when your throughput saturates), for each function, for each datum in the chunk, call function with datum<p>Yes, you should of course do whole-system benchmarks, but being able to guess helps you write a useful implementation to benchmark in the first place.</div><br/><div id="40373698" class="c"><input type="checkbox" id="c-40373698" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#40372944">root</a><span>|</span><a href="#40373417">parent</a><span>|</span><a href="#40375024">next</a><span>|</span><label class="collapse" for="c-40373698">[-]</label><label class="expand" for="c-40373698">[1 more]</label></div><br/><div class="children"><div class="content">In the two cases you have specified here, #2 is almost always the winner on performance.  So much so that in performance-sensitive code, many people will (justifiably) default to it without a benchmark.  Computers almost always operate in a memory bandwidth bound state, and have comparatively idle cores, and #1 is likely to just be wasteful of the resource that will almost always be the binding constraint.<p>Examples are ECS systems in games, and async run-to-completion runtimes on servers.  HPC systems also tend to operate this way.<p>Also, in the interest of disclosure, I wrote the blog post you are responding to.</div><br/></div></div></div></div><div id="40375024" class="c"><input type="checkbox" id="c-40375024" checked=""/><div class="controls bullet"><span class="by">anonymoushn</span><span>|</span><a href="#40372944">root</a><span>|</span><a href="#40373056">parent</a><span>|</span><a href="#40373417">prev</a><span>|</span><a href="#40372320">next</a><span>|</span><label class="collapse" for="c-40375024">[-]</label><label class="expand" for="c-40375024">[1 more]</label></div><br/><div class="children"><div class="content">For some parsing, serialization, or filtering tasks, you end up with quite large lookup tables of shuffles that would otherwise be pretty costly to compute on the fly, but real workloads hit only a tiny part of the lookup table at a time. (This comment fits entirely within his point about benchmarks, I guess)</div><br/></div></div></div></div></div></div><div id="40372320" class="c"><input type="checkbox" id="c-40372320" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#40372944">prev</a><span>|</span><a href="#40372203">next</a><span>|</span><label class="collapse" for="c-40372320">[-]</label><label class="expand" for="c-40372320">[5 more]</label></div><br/><div class="children"><div class="content">(in llama.cpp, for CPU)</div><br/><div id="40372603" class="c"><input type="checkbox" id="c-40372603" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#40372320">parent</a><span>|</span><a href="#40372203">next</a><span>|</span><label class="collapse" for="c-40372603">[-]</label><label class="expand" for="c-40372603">[4 more]</label></div><br/><div class="children"><div class="content">I developed this originally for llamafile, which was included in the last two releases: <a href="https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile&#x2F;releases&#x2F;tag&#x2F;0.8.2">https:&#x2F;&#x2F;github.com&#x2F;Mozilla-Ocho&#x2F;llamafile&#x2F;releases&#x2F;tag&#x2F;0.8.2</a> Now we&#x27;re upstreaming it to the llama.cpp project. There are other performance enhancements you can currently only get from llamafile, such as Kawrakow&#x27;s work making K quants go much faster.</div><br/><div id="40372788" class="c"><input type="checkbox" id="c-40372788" checked=""/><div class="controls bullet"><span class="by">breakingcups</span><span>|</span><a href="#40372320">root</a><span>|</span><a href="#40372603">parent</a><span>|</span><a href="#40373853">next</a><span>|</span><label class="collapse" for="c-40372788">[-]</label><label class="expand" for="c-40372788">[1 more]</label></div><br/><div class="children"><div class="content">Is that just because nobody has made an effort yet to port them upstream, or is there something inherently difficult about making those changes work in llama.cpp?</div><br/></div></div><div id="40373853" class="c"><input type="checkbox" id="c-40373853" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#40372320">root</a><span>|</span><a href="#40372603">parent</a><span>|</span><a href="#40372788">prev</a><span>|</span><a href="#40372203">next</a><span>|</span><label class="collapse" for="c-40373853">[-]</label><label class="expand" for="c-40373853">[2 more]</label></div><br/><div class="children"><div class="content">Wait, computing SiLU directly using some numerical analysis is probably a lot faster than doing an exp each time.  Is there a significant perf impact to doing this?</div><br/><div id="40373874" class="c"><input type="checkbox" id="c-40373874" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#40372320">root</a><span>|</span><a href="#40373853">parent</a><span>|</span><a href="#40372203">next</a><span>|</span><label class="collapse" for="c-40373874">[-]</label><label class="expand" for="c-40373874">[1 more]</label></div><br/><div class="children"><div class="content">With expf() most of the work had already been done and I could kill two birds with one stone. If you want to do the math for doing SiLU directly, that&#x27;d be an awesome change I&#x27;d happily merge into llamafile. You might even be able to get that into PyTorch and even bigger name projects.</div><br/></div></div></div></div></div></div></div></div><div id="40372203" class="c"><input type="checkbox" id="c-40372203" checked=""/><div class="controls bullet"><span class="by">koe123</span><span>|</span><a href="#40372320">prev</a><span>|</span><a href="#40374748">next</a><span>|</span><label class="collapse" for="c-40372203">[-]</label><label class="expand" for="c-40372203">[16 more]</label></div><br/><div class="children"><div class="content">Perhaps somewhat off topic, does anyone know how stuff like ggml compares to runtimes (tensorflow lite, onnxruntime, etc.)?</div><br/><div id="40372306" class="c"><input type="checkbox" id="c-40372306" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#40372203">parent</a><span>|</span><a href="#40372389">next</a><span>|</span><label class="collapse" for="c-40372306">[-]</label><label class="expand" for="c-40372306">[4 more]</label></div><br/><div class="children"><div class="content">On what hardware exactly?</div><br/><div id="40373020" class="c"><input type="checkbox" id="c-40373020" checked=""/><div class="controls bullet"><span class="by">koe123</span><span>|</span><a href="#40372203">root</a><span>|</span><a href="#40372306">parent</a><span>|</span><a href="#40372389">next</a><span>|</span><label class="collapse" for="c-40373020">[-]</label><label class="expand" for="c-40373020">[3 more]</label></div><br/><div class="children"><div class="content">Probably should have specified that! I’m referring to cpu inference.</div><br/><div id="40373720" class="c"><input type="checkbox" id="c-40373720" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#40372203">root</a><span>|</span><a href="#40373020">parent</a><span>|</span><a href="#40372389">next</a><span>|</span><label class="collapse" for="c-40373720">[-]</label><label class="expand" for="c-40373720">[2 more]</label></div><br/><div class="children"><div class="content">On what CPU?</div><br/><div id="40376068" class="c"><input type="checkbox" id="c-40376068" checked=""/><div class="controls bullet"><span class="by">koe123</span><span>|</span><a href="#40372203">root</a><span>|</span><a href="#40373720">parent</a><span>|</span><a href="#40372389">next</a><span>|</span><label class="collapse" for="c-40376068">[-]</label><label class="expand" for="c-40376068">[1 more]</label></div><br/><div class="children"><div class="content">Typically I&#x27;m considering embedded applications (armv8, armv7)</div><br/></div></div></div></div></div></div></div></div><div id="40372389" class="c"><input type="checkbox" id="c-40372389" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40372203">parent</a><span>|</span><a href="#40372306">prev</a><span>|</span><a href="#40374748">next</a><span>|</span><label class="collapse" for="c-40372389">[-]</label><label class="expand" for="c-40372389">[11 more]</label></div><br/><div class="children"><div class="content">I&#x27;m intimately familiar, maintain ONNX and llama.cpp Flutter libraries across all 6 True Platforms.<p>Quick opinionated TL;DR:<p>- llama.cpp for LLMs and can do whisper with it&#x27;s core dependency, GGML.<p>- ONNX for everything else.<p>- TF is the Apple of ML, it&#x27;s great if you&#x27;re completely wedded to Google ML ecosystem. Virtually dead outside that. (Something absurd ,like, 94%, of HF models are Pytorch)<p>- only chance I&#x27;d have to do a direct comparison in inference performance is Whisper in ONNX vs. GGML, someone got my llama .cpp lib running with Whisper and didn&#x27;t report significant perf difference</div><br/><div id="40373208" class="c"><input type="checkbox" id="c-40373208" checked=""/><div class="controls bullet"><span class="by">catgary</span><span>|</span><a href="#40372203">root</a><span>|</span><a href="#40372389">parent</a><span>|</span><a href="#40373128">next</a><span>|</span><label class="collapse" for="c-40373208">[-]</label><label class="expand" for="c-40373208">[5 more]</label></div><br/><div class="children"><div class="content">I don’t think that’s a terribly fair description of Google - AWS’s chips (inferon and trainium) both have robust XLA&#x2F;JAX support. Plus JAX now exports MLIR, so there is a really compelling JAX -&gt; IREE pipeline so JAX models can more or less be deployed anywhere, even on bare metal embedded devices.</div><br/><div id="40373291" class="c"><input type="checkbox" id="c-40373291" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40372203">root</a><span>|</span><a href="#40373208">parent</a><span>|</span><a href="#40373128">next</a><span>|</span><label class="collapse" for="c-40373291">[-]</label><label class="expand" for="c-40373291">[4 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right, if you need to go from data =&gt; model running in web app, I&#x27;d do TF - the inartful Apple analogy is meant to indicate that: great vertical integration.<p>For local inference of an existing model, TF Lite pales in comparison to ONNX. ONNX goes out of its way for you get ~anything running ~anywhere on the best accelerator available on the platform.* AFAIK TF Lite only helps if your model was in TF.<p>And there simply isn&#x27;t an LLM scene for TensorFlow, so it &quot;loses&quot; to llama.cpp for that. There isn&#x27;t an ONNX LLM scene either, though. (see below)<p>* There&#x27;s one key exception...until recently...LLMs! ONNX&#x27;s model format was limited due to protobuf, IIRC it was 2-4 GB. Part of the Phi-3 announcement was this library they&#x27;ve been stubbing out that&#x27;s on top of ONNX, but more specialized for LLMs. That being said, haven&#x27;t seen any LLMs in it except Phi-3, and it&#x27;s an absolute mess, the library was announced weeks ahead of when it was planned to be released, and then throw in the standard 6-week slippage, I&#x27;m probably not trying it again until June.</div><br/><div id="40373452" class="c"><input type="checkbox" id="c-40373452" checked=""/><div class="controls bullet"><span class="by">catgary</span><span>|</span><a href="#40372203">root</a><span>|</span><a href="#40373291">parent</a><span>|</span><a href="#40373128">next</a><span>|</span><label class="collapse" for="c-40373452">[-]</label><label class="expand" for="c-40373452">[3 more]</label></div><br/><div class="children"><div class="content">I didn’t mention tensorflow or TF Lite? IREE is a different technology, it is essentially a compiler&#x2F;VM for MLIR. Since JAX exports MLIR, you can run JAX on any platform supported by IREE (which is basically any platform from embedded hardware to datacenter GPUs). It is less mature than ONNX at this point,  
but much more promising when it comes to deploying on edge devices.</div><br/><div id="40373523" class="c"><input type="checkbox" id="c-40373523" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40372203">root</a><span>|</span><a href="#40373452">parent</a><span>|</span><a href="#40373128">next</a><span>|</span><label class="collapse" for="c-40373523">[-]</label><label class="expand" for="c-40373523">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I didn’t mention tensorflow or TF Lite<p>I know.<p>OP didn&#x27;t mention JAX, or IREE.<p>I avoided having a knee-jerk &quot;Incorrect, the thing you are talking about is off-topic&quot; reaction because it&#x27;s inimical to friendly conversation.<p>Mentioning ONNX and llama.cpp made it clear they were talking about inference. In that context, TF Lite isn&#x27;t helpful unless you have TF. JAX is irrelevant. IREE is expressly not there yet[1] and has nothing to do with Google.<p>[1] c.f. the GitHub &quot;IREE is still in its early phase. We have settled down on the overarching infrastructure&quot;. From a llama.cpp&#x2F;ONNX&#x2F;local inference perspective, there&#x27;s little more than &quot;well, we&#x27;ll get MLIR, then we can make N binaries for N model x arch x platform options.&quot; Doesn&#x27;t sound so helpful to me, but I got it easy right now, models are treated as data instead of code in both ONNX and llama.cpp. I&#x27;m not certain that&#x27;s the right thing long-term, ex. it incentivizes &quot;kitchen sink&quot; ML frameworks, people always want me to add a model to the library, rather than use my library as a dependency.</div><br/><div id="40373967" class="c"><input type="checkbox" id="c-40373967" checked=""/><div class="controls bullet"><span class="by">catgary</span><span>|</span><a href="#40372203">root</a><span>|</span><a href="#40373523">parent</a><span>|</span><a href="#40373128">next</a><span>|</span><label class="collapse" for="c-40373967">[-]</label><label class="expand" for="c-40373967">[1 more]</label></div><br/><div class="children"><div class="content">Speaking as someone who deploys ML models on edge devices - having the MLIR and being able to make binaries for different platforms is terrifically useful! You’re often supporting a range of devices (e.g. video game consoles, phones, embedded, etc) and want to tune the compilation to maximize the performance for your deployment target. And there are a lot of algorithms&#x2F;models in robotics and animation that simply won’t work with ONNX as they involve computing gradients (which torch cannot export, and the gradients ONNXRuntime give you only work in training sessions, which aren’t as widely). supported.<p>Also, if you have JAX you can get TF and therefore TFLite. And IREE is part of the OpenXLA project, which Google started.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40373128" class="c"><input type="checkbox" id="c-40373128" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#40372203">root</a><span>|</span><a href="#40372389">parent</a><span>|</span><a href="#40373208">prev</a><span>|</span><a href="#40372677">next</a><span>|</span><label class="collapse" for="c-40373128">[-]</label><label class="expand" for="c-40373128">[2 more]</label></div><br/><div class="children"><div class="content">What’re your thoughts on MLX? It’s been phenomenal on my MBP.</div><br/><div id="40373339" class="c"><input type="checkbox" id="c-40373339" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40372203">root</a><span>|</span><a href="#40373128">parent</a><span>|</span><a href="#40372677">next</a><span>|</span><label class="collapse" for="c-40373339">[-]</label><label class="expand" for="c-40373339">[1 more]</label></div><br/><div class="children"><div class="content">No time* to try it unfortunately :( Sounds great, though, and Mac just kicks the pants of every other platform on local inference thanks to Metal, I imagine MLX must extend that lead to the point Qualcomm&#x2F;Google has to a serious investment in open source acceleration. Cheapest iPhone from 2022 kicks the most expensive Android from 2023 (Pixel Fold) around the block, 2x on inference. 12 tkns&#x2F;s vs. 6&#x2F;s.<p>* it sounded like a <i>great</i> idea to do an OpenAI LLM x search app. Then it sounded like a great idea to add embeddings locally for privacy (thus, ONNX). Then it sounded like a great idea to do a local LLM (thus, llama.cpp). Then it sounded like a great idea to differentiate by being on all platforms, supported equally. Really taxing. Think I went too far this time. It works but, jeez, the workload...hopefully, after release, it turns out maintenance load is relatively low</div><br/></div></div></div></div><div id="40372677" class="c"><input type="checkbox" id="c-40372677" checked=""/><div class="controls bullet"><span class="by">svnt</span><span>|</span><a href="#40372203">root</a><span>|</span><a href="#40372389">parent</a><span>|</span><a href="#40373128">prev</a><span>|</span><a href="#40374748">next</a><span>|</span><label class="collapse" for="c-40372677">[-]</label><label class="expand" for="c-40372677">[3 more]</label></div><br/><div class="children"><div class="content">What are the True Platforms, in this case?</div><br/><div id="40372734" class="c"><input type="checkbox" id="c-40372734" checked=""/><div class="controls bullet"><span class="by">crthpl</span><span>|</span><a href="#40372203">root</a><span>|</span><a href="#40372677">parent</a><span>|</span><a href="#40374748">next</a><span>|</span><label class="collapse" for="c-40372734">[-]</label><label class="expand" for="c-40372734">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m guessing MacOS, Linux, Android, Windows, iOS, and Web?</div><br/><div id="40373220" class="c"><input type="checkbox" id="c-40373220" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40372203">root</a><span>|</span><a href="#40372734">parent</a><span>|</span><a href="#40374748">next</a><span>|</span><label class="collapse" for="c-40373220">[-]</label><label class="expand" for="c-40373220">[1 more]</label></div><br/><div class="children"><div class="content">Correct</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40374748" class="c"><input type="checkbox" id="c-40374748" checked=""/><div class="controls bullet"><span class="by">petermcneeley</span><span>|</span><a href="#40372203">prev</a><span>|</span><a href="#40375311">next</a><span>|</span><label class="collapse" for="c-40374748">[-]</label><label class="expand" for="c-40374748">[2 more]</label></div><br/><div class="children"><div class="content">One can vectorize LUTs as well
<a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;docs&#x2F;intrinsics-guide&#x2F;index.html#avxnewtechs=AVX2&amp;text=gather" rel="nofollow">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;docs&#x2F;intrinsics-guid...</a><p>I wrote about the kinds of things that are possible with LUTs awhile back <a href="https:&#x2F;&#x2F;darkcephas.blogspot.com&#x2F;2018&#x2F;10&#x2F;validating-utf8-strings-with-lookup.html" rel="nofollow">https:&#x2F;&#x2F;darkcephas.blogspot.com&#x2F;2018&#x2F;10&#x2F;validating-utf8-stri...</a></div><br/><div id="40374922" class="c"><input type="checkbox" id="c-40374922" checked=""/><div class="controls bullet"><span class="by">boulos</span><span>|</span><a href="#40374748">parent</a><span>|</span><a href="#40375311">next</a><span>|</span><label class="collapse" for="c-40374922">[-]</label><label class="expand" for="c-40374922">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but a direct `exp` implementation is only like 10-20 FMAs depending on how much accuracy you want. No gathering or permuting will really compete with straight math.</div><br/></div></div></div></div><div id="40375311" class="c"><input type="checkbox" id="c-40375311" checked=""/><div class="controls bullet"><span class="by">rdevulap</span><span>|</span><a href="#40374748">prev</a><span>|</span><a href="#40375650">next</a><span>|</span><label class="collapse" for="c-40375311">[-]</label><label class="expand" for="c-40375311">[2 more]</label></div><br/><div class="children"><div class="content">On similar lines, faster tanh <a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;onnxruntime&#x2F;pull&#x2F;20612">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;onnxruntime&#x2F;pull&#x2F;20612</a></div><br/><div id="40375616" class="c"><input type="checkbox" id="c-40375616" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#40375311">parent</a><span>|</span><a href="#40375650">next</a><span>|</span><label class="collapse" for="c-40375616">[-]</label><label class="expand" for="c-40375616">[1 more]</label></div><br/><div class="children"><div class="content">Great work. But what&#x27;s their goal? Are they trying to make that GeLU approximation go faster? Things would probably go a lot faster going back to the erff().</div><br/></div></div></div></div><div id="40375650" class="c"><input type="checkbox" id="c-40375650" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#40375311">prev</a><span>|</span><a href="#40373222">next</a><span>|</span><label class="collapse" for="c-40375650">[-]</label><label class="expand" for="c-40375650">[4 more]</label></div><br/><div class="children"><div class="content">With all such optimizations, isn&#x27;t llama too slow to be run on a CPU with a reasonable amount of parameters?</div><br/><div id="40375773" class="c"><input type="checkbox" id="c-40375773" checked=""/><div class="controls bullet"><span class="by">jart</span><span>|</span><a href="#40375650">parent</a><span>|</span><a href="#40375683">next</a><span>|</span><label class="collapse" for="c-40375773">[-]</label><label class="expand" for="c-40375773">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m gpu poor so I use cpu to run large models. For example, llamafile running mixtral 8x22b on my threadripper can chew through long legal documents and give me advice in a few minutes, using f16 weights and kahan summation. Show me someone who has that many graphics cards.</div><br/></div></div><div id="40375683" class="c"><input type="checkbox" id="c-40375683" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#40375650">parent</a><span>|</span><a href="#40375773">prev</a><span>|</span><a href="#40375674">next</a><span>|</span><label class="collapse" for="c-40375683">[-]</label><label class="expand" for="c-40375683">[1 more]</label></div><br/><div class="children"><div class="content">Models get better for equal parameters, CPUs get faster and good folks at intel, amd and arm are probably working very hard to catch up with apple silicon in terms of memory architecture. I can see this be very relevant in a couple of years.</div><br/></div></div><div id="40375674" class="c"><input type="checkbox" id="c-40375674" checked=""/><div class="controls bullet"><span class="by">nxpnsv</span><span>|</span><a href="#40375650">parent</a><span>|</span><a href="#40375683">prev</a><span>|</span><a href="#40373222">next</a><span>|</span><label class="collapse" for="c-40375674">[-]</label><label class="expand" for="c-40375674">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see how that conclusion follows from this optimization.</div><br/></div></div></div></div><div id="40373222" class="c"><input type="checkbox" id="c-40373222" checked=""/><div class="controls bullet"><span class="by">lxe</span><span>|</span><a href="#40375650">prev</a><span>|</span><a href="#40373312">next</a><span>|</span><label class="collapse" for="c-40373222">[-]</label><label class="expand" for="c-40373222">[2 more]</label></div><br/><div class="children"><div class="content">At this point, is gguf&#x2F;llama.cpp a more performant solution for unbatched inference on CUDA devices, or is exllamav2+flashattention still reigning supreme?</div><br/><div id="40373389" class="c"><input type="checkbox" id="c-40373389" checked=""/><div class="controls bullet"><span class="by">GuuD</span><span>|</span><a href="#40373222">parent</a><span>|</span><a href="#40373312">next</a><span>|</span><label class="collapse" for="c-40373389">[-]</label><label class="expand" for="c-40373389">[1 more]</label></div><br/><div class="children"><div class="content">The difference is negligible on 2x 4090. There are more important differences like 4 bit KV cache.</div><br/></div></div></div></div><div id="40373312" class="c"><input type="checkbox" id="c-40373312" checked=""/><div class="controls bullet"><span class="by">KaoruAoiShiho</span><span>|</span><a href="#40373222">prev</a><span>|</span><a href="#40372335">next</a><span>|</span><label class="collapse" for="c-40373312">[-]</label><label class="expand" for="c-40373312">[1 more]</label></div><br/><div class="children"><div class="content">This does help the gguf usecase of partial offloading to GPU? It&#x27;ll help the CPU part be faster too?</div><br/></div></div></div></div></div></div></div></body></html>