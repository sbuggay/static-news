<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="apple-mobile-web-app-capable" content="yes"/><link rel="preload" href="styles.css?v=1682522832960" as="style"/><link rel="stylesheet" href="styles.css?v=1682522832960"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://chris-said.io/2023/04/21/double-descent-in-human-learning/">Double Descent in Human Learning</a> <span class="domain">(<a href="https://chris-said.io">chris-said.io</a>)</span></div><div class="subtext"><span>tim_sw</span> | <span>30 comments</span></div><br/><div><div id="35711693" class="c"><input type="checkbox" id="c-35711693" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#35712486">next</a><span>|</span><label class="collapse" for="c-35711693">[-]</label><label class="expand" for="c-35711693">[1 more]</label></div><br/><div class="children"><div class="content">The linear regression is somewhat interesting, but also points out that the double descent might have a somewhat strange cause. What technically happens is that as you have more parameters you not only optimize for the model fit, but also for the size of the parameters.<p>If you <i>force</i> the model to fit your training data perfectly then it is no wonder that you can only start to optimize the size of your parameters <i>after</i> you have enough leeway to easily fit the entire training set.<p>However another way to achieve the same effect is to make the assumption that the parameters should be small(ish) explicit. Fitting the whole training set is nice but ultimately pointless if you&#x27;re just fitting noise. If the <i>actual</i> data is a simple polynomial + random noise then fitting all the noise will give a <i>worse</i> estimate.</div><br/></div></div><div id="35712486" class="c"><input type="checkbox" id="c-35712486" checked=""/><div class="controls bullet"><span class="by">topaz0</span><span>|</span><a href="#35711693">prev</a><span>|</span><a href="#35712991">next</a><span>|</span><label class="collapse" for="c-35712486">[-]</label><label class="expand" for="c-35712486">[1 more]</label></div><br/><div class="children"><div class="content">I would have thought the &quot;few parameter&quot; high-error region corresponds more naturally to the part of language learning where the learner overgeneralizes and thinks everything is regular, and the &quot;many parameter&quot; high-error region corresponds to knowing the irregular forms for each word that you&#x27;ve encountered. But this blog seems to think of it in the other way around. Maybe I&#x27;m missing something.</div><br/></div></div><div id="35712991" class="c"><input type="checkbox" id="c-35712991" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#35712486">prev</a><span>|</span><a href="#35710032">next</a><span>|</span><label class="collapse" for="c-35712991">[-]</label><label class="expand" for="c-35712991">[2 more]</label></div><br/><div class="children"><div class="content">Has anyone been able to reproduce double descent without trying to invert a seriously ill-posed linear system? Showing that a solver has numerical issues which go away when enough parameters are introduced does not really prove anything. This looks much more like regularization though floating point rounding.<p>By the way, the linked Colab notebook is missing a &quot;self.&quot; in front of the &quot;lamb&quot; in the unused regularization branch of the fit function.</div><br/><div id="35713361" class="c"><input type="checkbox" id="c-35713361" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35712991">parent</a><span>|</span><a href="#35710032">next</a><span>|</span><label class="collapse" for="c-35713361">[-]</label><label class="expand" for="c-35713361">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI paper on double descent: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1912.02292.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1912.02292.pdf</a><p>They show it happens across a variety of architectures and real world datasets</div><br/></div></div></div></div><div id="35710032" class="c"><input type="checkbox" id="c-35710032" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#35712991">prev</a><span>|</span><a href="#35712588">next</a><span>|</span><label class="collapse" for="c-35710032">[-]</label><label class="expand" for="c-35710032">[7 more]</label></div><br/><div class="children"><div class="content">Gradient descent finds the minimum L2 norm solution of the least squares? So adding a L2 regularization term should do nothing if the objective is convex, right? Is this common knowledge? I must have missed the memo, or be getting rusty.</div><br/><div id="35710883" class="c"><input type="checkbox" id="c-35710883" checked=""/><div class="controls bullet"><span class="by">macleginn</span><span>|</span><a href="#35710032">parent</a><span>|</span><a href="#35710107">next</a><span>|</span><label class="collapse" for="c-35710883">[-]</label><label class="expand" for="c-35710883">[1 more]</label></div><br/><div class="children"><div class="content">It depends on the inputs. E.g., if two elements of the input are highly correlated, the network can assign an arbitrarily high positive weight to one element and then correct this by assigning an arbitrarily high negative weight to the other, with the total contribution of the two elements being reasonable. However, if these input elements are not as tightly correlated in the test data, the results may be bad. L2 regularisation ensures that this does not happen. (In practice, the effect will be mitigated by the activation function, but weights do blow up in training, leading to &quot;saturated&quot; models, which exhibit weird behaviours.)</div><br/></div></div><div id="35710107" class="c"><input type="checkbox" id="c-35710107" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#35710032">parent</a><span>|</span><a href="#35710883">prev</a><span>|</span><a href="#35710256">next</a><span>|</span><label class="collapse" for="c-35710107">[-]</label><label class="expand" for="c-35710107">[1 more]</label></div><br/><div class="children"><div class="content">The formal claim is in Appendix A here: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2303.14151.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2303.14151.pdf</a></div><br/></div></div><div id="35710256" class="c"><input type="checkbox" id="c-35710256" checked=""/><div class="controls bullet"><span class="by">svantana</span><span>|</span><a href="#35710032">parent</a><span>|</span><a href="#35710107">prev</a><span>|</span><a href="#35710423">next</a><span>|</span><label class="collapse" for="c-35710256">[-]</label><label class="expand" for="c-35710256">[1 more]</label></div><br/><div class="children"><div class="content">It finds the minimum with regards to the training data. The double descent phenomenon is about error on unseen data.</div><br/></div></div><div id="35710423" class="c"><input type="checkbox" id="c-35710423" checked=""/><div class="controls bullet"><span class="by">jwilber</span><span>|</span><a href="#35710032">parent</a><span>|</span><a href="#35710256">prev</a><span>|</span><a href="#35711168">next</a><span>|</span><label class="collapse" for="c-35710423">[-]</label><label class="expand" for="c-35710423">[2 more]</label></div><br/><div class="children"><div class="content">There’s definitely more to the picture than that - it’s really about how models in the interpolation region behave (see <a href="https:&#x2F;&#x2F;mlu-explain.github.io&#x2F;double-descent&#x2F;" rel="nofollow">https:&#x2F;&#x2F;mlu-explain.github.io&#x2F;double-descent&#x2F;</a> or <a href="https:&#x2F;&#x2F;mlu-explain.github.io&#x2F;double-descent2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;mlu-explain.github.io&#x2F;double-descent2&#x2F;</a>)</div><br/></div></div><div id="35711168" class="c"><input type="checkbox" id="c-35711168" checked=""/><div class="controls bullet"><span class="by">elcomet</span><span>|</span><a href="#35710032">parent</a><span>|</span><a href="#35710423">prev</a><span>|</span><a href="#35712588">next</a><span>|</span><label class="collapse" for="c-35711168">[-]</label><label class="expand" for="c-35711168">[1 more]</label></div><br/><div class="children"><div class="content">The objective is usually not convex in deep learning.</div><br/></div></div></div></div><div id="35712588" class="c"><input type="checkbox" id="c-35712588" checked=""/><div class="controls bullet"><span class="by">clbrmbr</span><span>|</span><a href="#35710032">prev</a><span>|</span><a href="#35711210">next</a><span>|</span><label class="collapse" for="c-35712588">[-]</label><label class="expand" for="c-35712588">[1 more]</label></div><br/><div class="children"><div class="content">In the polynomial fit case: it’s pretty clear to me that if the number of parameters is higher than the degrees of freedom in the data, then the model can simply memorize the input.<p>That doesn’t explain why it generalizes well, though.</div><br/></div></div><div id="35711210" class="c"><input type="checkbox" id="c-35711210" checked=""/><div class="controls bullet"><span class="by">version_five</span><span>|</span><a href="#35712588">prev</a><span>|</span><a href="#35710396">next</a><span>|</span><label class="collapse" for="c-35711210">[-]</label><label class="expand" for="c-35711210">[1 more]</label></div><br/><div class="children"><div class="content">Intuition of double descent: when you have the about the same number of parameters as data points, a fit curve looks like one of those shitty &quot;polynomial fits&quot; in excel, where away from the data points it over and under shoots the curve wildly. Lots more parameters and this calms down to give smooth interpolation between the curves.<p>I can imagine some tasks where people have the same problem. You&#x27;re overfit to a very specific task, but outside it &quot;when you have a hammer everything looks like a nail&quot; and you end up doing something dumb.</div><br/></div></div><div id="35710396" class="c"><input type="checkbox" id="c-35710396" checked=""/><div class="controls bullet"><span class="by">jwilber</span><span>|</span><a href="#35711210">prev</a><span>|</span><a href="#35710035">next</a><span>|</span><label class="collapse" for="c-35710396">[-]</label><label class="expand" for="c-35710396">[2 more]</label></div><br/><div class="children"><div class="content">In case you need a refresher, I made a very visual introduction to Double Descent here: <a href="https:&#x2F;&#x2F;mlu-explain.github.io&#x2F;double-descent&#x2F;" rel="nofollow">https:&#x2F;&#x2F;mlu-explain.github.io&#x2F;double-descent&#x2F;</a><p>(There’s a math-ier follow up linked as wel).</div><br/><div id="35710996" class="c"><input type="checkbox" id="c-35710996" checked=""/><div class="controls bullet"><span class="by">dir_balak</span><span>|</span><a href="#35710396">parent</a><span>|</span><a href="#35710035">next</a><span>|</span><label class="collapse" for="c-35710996">[-]</label><label class="expand" for="c-35710996">[1 more]</label></div><br/><div class="children"><div class="content">Fantastic material. Big thanks!</div><br/></div></div></div></div><div id="35710035" class="c"><input type="checkbox" id="c-35710035" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#35710396">prev</a><span>|</span><label class="collapse" for="c-35710035">[-]</label><label class="expand" for="c-35710035">[14 more]</label></div><br/><div class="children"><div class="content">I’ve heard the argument made that LLMs are simply models overfit on the entirety of the internet. Double decent is a good counter point to that claim, it suggests something more interesting than simple overfitting is happening with large parameter counts.</div><br/><div id="35711303" class="c"><input type="checkbox" id="c-35711303" checked=""/><div class="controls bullet"><span class="by">bheadmaster</span><span>|</span><a href="#35710035">parent</a><span>|</span><a href="#35711144">next</a><span>|</span><label class="collapse" for="c-35711303">[-]</label><label class="expand" for="c-35711303">[9 more]</label></div><br/><div class="children"><div class="content">I find it hard to believe that every single problem has a solution on the internet, which is what &quot;overfitting&quot; would imply.<p>I was playing Fallout: New Vegas on Wine, and for some reason, the music on my pipboy wasn&#x27;t playing. I searched the internet for the Wine errors from the terminal to no avail, and as a last resort asked ChatGPT. It gave me step-by-step instructions on how to fix it, and it worked.<p>If that doesn&#x27;t demonstrate that LLMs have <i>some</i> kind of internal model of the world and understanding of it, then I don&#x27;t know what will.</div><br/><div id="35711397" class="c"><input type="checkbox" id="c-35711397" checked=""/><div class="controls bullet"><span class="by">alphydan</span><span>|</span><a href="#35710035">root</a><span>|</span><a href="#35711303">parent</a><span>|</span><a href="#35713390">next</a><span>|</span><label class="collapse" for="c-35711397">[-]</label><label class="expand" for="c-35711397">[2 more]</label></div><br/><div class="children"><div class="content">An alternative explanation is that your google-fu is not as good as openai&#x27;s crawlers or the WebText corpus (which can go a lot deeper than any of your searches).</div><br/><div id="35713494" class="c"><input type="checkbox" id="c-35713494" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#35710035">root</a><span>|</span><a href="#35711397">parent</a><span>|</span><a href="#35713390">next</a><span>|</span><label class="collapse" for="c-35713494">[-]</label><label class="expand" for="c-35713494">[1 more]</label></div><br/><div class="children"><div class="content">Makes me wonder how Google would compare if a good portion of the entire world hadn&#x27;t spent decades trying to game their system. OpenAI created a completely new system and reaps the benefits of training on data that hasn&#x27;t been twisted-half-way-to-hell to exploit it.</div><br/></div></div></div></div><div id="35713390" class="c"><input type="checkbox" id="c-35713390" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#35710035">root</a><span>|</span><a href="#35711303">parent</a><span>|</span><a href="#35711397">prev</a><span>|</span><a href="#35712261">next</a><span>|</span><label class="collapse" for="c-35713390">[-]</label><label class="expand" for="c-35713390">[2 more]</label></div><br/><div class="children"><div class="content">ML is fundamentally pattern finding and matching.<p>The fact that it is useful for finding patterns that may be different than humans tend to find is not an indication of understanding of the underlying data.<p>It is no different than clustering in traditional stats.  While those found patterns are sometimes incredibly useful, clustering knows nothing outside of the provided dataset.<p>As other have mentioned, Google&#x27;s search results are actually really bad at finding novel results these days due to many factors like battling SEO tricks etc...<p>But while the results of LLMs is impressive, there is no mechanisms for it to have an &#x27;internal model of the world&#x27; in their current form.<p>It may help to remember that current LLMs would require an infinity of RAM to be even computationally complete right now.</div><br/><div id="35713853" class="c"><input type="checkbox" id="c-35713853" checked=""/><div class="controls bullet"><span class="by">bheadmaster</span><span>|</span><a href="#35710035">root</a><span>|</span><a href="#35713390">parent</a><span>|</span><a href="#35712261">next</a><span>|</span><label class="collapse" for="c-35713853">[-]</label><label class="expand" for="c-35713853">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The fact that it is useful for finding patterns that may be different than humans tend to find is not an indication of understanding of the underlying data.<p>Without invoking your own self-awareness as an argument, how do you know that other people &quot;understand&quot; stuff, and aren&#x27;t merely &quot;finding patterns&quot;? In other words, in what way do you define &quot;understanding&quot;, such that you can be sure that LLMs have no such thing?<p>&gt; there is no mechanisms for it to have an &#x27;internal model of the world&#x27; in their current form.<p>How do you know that? We don&#x27;t even know why humans have an internal model of the world. What if internal modelling of the world is just sufficiently-complex pattern-matching?</div><br/></div></div></div></div><div id="35712261" class="c"><input type="checkbox" id="c-35712261" checked=""/><div class="controls bullet"><span class="by">fauxpause_</span><span>|</span><a href="#35710035">root</a><span>|</span><a href="#35711303">parent</a><span>|</span><a href="#35713390">prev</a><span>|</span><a href="#35711452">next</a><span>|</span><label class="collapse" for="c-35712261">[-]</label><label class="expand" for="c-35712261">[3 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.gog.com&#x2F;forum&#x2F;fallout_series&#x2F;new_vegas_music_is_not_playing_on_the_radio" rel="nofollow">https:&#x2F;&#x2F;www.gog.com&#x2F;forum&#x2F;fallout_series&#x2F;new_vegas_music_is_...</a><p>I haven’t played this game so I don’t know what I’m searching for but this was my first result. Seems on the money, no?</div><br/><div id="35713955" class="c"><input type="checkbox" id="c-35713955" checked=""/><div class="controls bullet"><span class="by">bheadmaster</span><span>|</span><a href="#35710035">root</a><span>|</span><a href="#35712261">parent</a><span>|</span><a href="#35711452">next</a><span>|</span><label class="collapse" for="c-35713955">[-]</label><label class="expand" for="c-35713955">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s the thing - it isn&#x27;t.<p>I&#x27;ve been through this forum, many reddit posts and other sites - none of the solutions worked. What worked was that ChatGPT figured out that I need to add the following line to ~&#x2F;.wine&#x2F;system.reg:<p><pre><code>    [Software\\Wine\\GStreamer]
    &quot;DllOverrides&quot;=&quot;mscoree,mshtml=&quot;
</code></pre>
And install 32-bit version of gstreamer good plugins:<p><pre><code>    sudo apt-get install gstreamer1.0-plugins-good:i386
</code></pre>
If you happen to find these exact instructions anywhere on the internet, please share, as that will be enough to convince me that LLMs aren&#x27;t anything more than glorified search engines.<p>Otherwise, I can&#x27;t help but be skeptical. If nothing else, it&#x27;s <i>plausible</i> that LLMs have <i>some</i> kind of internal representation of the world.</div><br/><div id="35714226" class="c"><input type="checkbox" id="c-35714226" checked=""/><div class="controls bullet"><span class="by">fauxpause_</span><span>|</span><a href="#35710035">root</a><span>|</span><a href="#35713955">parent</a><span>|</span><a href="#35711452">next</a><span>|</span><label class="collapse" for="c-35714226">[-]</label><label class="expand" for="c-35714226">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;baronhk.wordpress.com&#x2F;2021&#x2F;10&#x2F;05&#x2F;wine-still-needs-32-bit-mp3-plug-in-from-gstreamer-that-the-package-doesnt-depend-on-in-order-to-play-mp3-in-certain-bethesda-video-games-gaming-on-debian-11-gnu-linux&#x2F;" rel="nofollow">https:&#x2F;&#x2F;baronhk.wordpress.com&#x2F;2021&#x2F;10&#x2F;05&#x2F;wine-still-needs-32...</a><p>How about this guy? You sure the first instruction is necessary?<p>I do think there is some decent ability to piece things together. But this example seems too niche.</div><br/></div></div></div></div></div></div><div id="35711452" class="c"><input type="checkbox" id="c-35711452" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#35710035">root</a><span>|</span><a href="#35711303">parent</a><span>|</span><a href="#35712261">prev</a><span>|</span><a href="#35711144">next</a><span>|</span><label class="collapse" for="c-35711452">[-]</label><label class="expand" for="c-35711452">[1 more]</label></div><br/><div class="children"><div class="content">I can’t find anything on Google anymore so I’m not sure this proves much except open ai has better search.</div><br/></div></div></div></div><div id="35711144" class="c"><input type="checkbox" id="c-35711144" checked=""/><div class="controls bullet"><span class="by">version_five</span><span>|</span><a href="#35710035">parent</a><span>|</span><a href="#35711303">prev</a><span>|</span><a href="#35710538">next</a><span>|</span><label class="collapse" for="c-35711144">[-]</label><label class="expand" for="c-35711144">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be interested to know if anyone has studied how overfitting translates into the domain of llm output: it&#x27;s easy to understand when you&#x27;re fitting a line though data, or building a classifier, you overfit and your test set loss is higher than your training set loss, and this directly relates to worse performance of the model. For an llm picking probably next words, what&#x27;s the analogy, and does overfitting make it &quot;worse&quot; even if a test set loss is higher?</div><br/></div></div><div id="35710538" class="c"><input type="checkbox" id="c-35710538" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#35710035">parent</a><span>|</span><a href="#35711144">prev</a><span>|</span><a href="#35710368">next</a><span>|</span><label class="collapse" for="c-35710538">[-]</label><label class="expand" for="c-35710538">[2 more]</label></div><br/><div class="children"><div class="content">How is double descent a good counterpoint to the claim?</div><br/><div id="35710720" class="c"><input type="checkbox" id="c-35710720" checked=""/><div class="controls bullet"><span class="by">qumpis</span><span>|</span><a href="#35710035">root</a><span>|</span><a href="#35710538">parent</a><span>|</span><a href="#35710368">next</a><span>|</span><label class="collapse" for="c-35710720">[-]</label><label class="expand" for="c-35710720">[1 more]</label></div><br/><div class="children"><div class="content">That novel reasoning is due to the model transitioning to a generalization regime, not merely parroting training data?</div><br/></div></div></div></div><div id="35710368" class="c"><input type="checkbox" id="c-35710368" checked=""/><div class="controls bullet"><span class="by">malux85</span><span>|</span><a href="#35710035">parent</a><span>|</span><a href="#35710538">prev</a><span>|</span><label class="collapse" for="c-35710368">[-]</label><label class="expand" for="c-35710368">[1 more]</label></div><br/><div class="children"><div class="content">Overfitting is lack of genralisation, LLMs can produce novel output by using abstraction composition, which you cannot do without generalisation.<p>Other signs of non-overfitting: Abstraction laddering, task decomposition, novel (i.e. unseen) joke explanation</div><br/></div></div></div></div></div></div></div></div></div></body></html>