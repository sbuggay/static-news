<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1691226056198" as="style"/><link rel="stylesheet" href="styles.css?v=1691226056198"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.techradar.com/computing/gpu/watch-out-amd-intel-arc-a580-could-be-the-next-great-affordable-gpu">Intel Arc A580 could be the next great affordable GPU</a> <span class="domain">(<a href="https://www.techradar.com">www.techradar.com</a>)</span></div><div class="subtext"><span>mikece</span> | <span>87 comments</span></div><br/><div><div id="37010207" class="c"><input type="checkbox" id="c-37010207" checked=""/><div class="controls bullet"><span class="by">BaculumMeumEst</span><span>|</span><a href="#37010235">next</a><span>|</span><label class="collapse" for="c-37010207">[-]</label><label class="expand" for="c-37010207">[3 more]</label></div><br/><div class="children"><div class="content">An 8GB card is not &quot;coming out swinging&quot;. My 1070 I got on the cheap in 2016 has 8GB. Give me some fucking VRAM already.</div><br/><div id="37010243" class="c"><input type="checkbox" id="c-37010243" checked=""/><div class="controls bullet"><span class="by">ceeam</span><span>|</span><a href="#37010207">parent</a><span>|</span><a href="#37010235">next</a><span>|</span><label class="collapse" for="c-37010243">[-]</label><label class="expand" for="c-37010243">[2 more]</label></div><br/><div class="children"><div class="content">Why can&#x27;t we have DIMM slots on video cards?</div><br/><div id="37010263" class="c"><input type="checkbox" id="c-37010263" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#37010207">root</a><span>|</span><a href="#37010243">parent</a><span>|</span><a href="#37010235">next</a><span>|</span><label class="collapse" for="c-37010263">[-]</label><label class="expand" for="c-37010263">[1 more]</label></div><br/><div class="children"><div class="content">Because that reduces bandwidth and occupies more space</div><br/></div></div></div></div></div></div><div id="37010235" class="c"><input type="checkbox" id="c-37010235" checked=""/><div class="controls bullet"><span class="by">vimarsh6739</span><span>|</span><a href="#37010207">prev</a><span>|</span><a href="#37009548">next</a><span>|</span><label class="collapse" for="c-37010235">[-]</label><label class="expand" for="c-37010235">[1 more]</label></div><br/><div class="children"><div class="content">How does oneAPI&#x2F;SYCL compare to CUDA? We certainly need an alternative to OpenCL, but every day, I can&#x27;t help but notice the widening gulf between CUDA and any other GPGPU API out there.</div><br/></div></div><div id="37009548" class="c"><input type="checkbox" id="c-37009548" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#37010235">prev</a><span>|</span><a href="#37009225">next</a><span>|</span><label class="collapse" for="c-37009548">[-]</label><label class="expand" for="c-37009548">[16 more]</label></div><br/><div class="children"><div class="content">The revolution here is not that it is as good as the competition, but that it might have a much better price, leading to a much better price&#x2F;performance ratio.<p>If Intel is patient to compete for lower grade GPUs for a few years without killing its GPU line, then we might see them competing for the top in the future. And the top is where the big money is. They&#x27;ll also need a CUDA compatible API to stand a chance, but they have time.<p>The only thing that saved Intel until now is that Nvidia can&#x27;t legally produce x86 CPUs, although they tried.<p>The more competition, the better.</div><br/><div id="37010209" class="c"><input type="checkbox" id="c-37010209" checked=""/><div class="controls bullet"><span class="by">sameerds</span><span>|</span><a href="#37009548">parent</a><span>|</span><a href="#37009615">next</a><span>|</span><label class="collapse" for="c-37010209">[-]</label><label class="expand" for="c-37010209">[1 more]</label></div><br/><div class="children"><div class="content">Did you forget a &quot;&#x2F;s&quot; tag? Because that pretty much sounds like ten years of AMD history!</div><br/></div></div><div id="37009615" class="c"><input type="checkbox" id="c-37009615" checked=""/><div class="controls bullet"><span class="by">0xakhil</span><span>|</span><a href="#37009548">parent</a><span>|</span><a href="#37010209">prev</a><span>|</span><a href="#37009985">next</a><span>|</span><label class="collapse" for="c-37009615">[-]</label><label class="expand" for="c-37009615">[1 more]</label></div><br/><div class="children"><div class="content">Apart from the software stack, Nvidia has invested a lot in the networking infrastructure for high bandwidth between gpu in a cluster. I think their acquisition of Mellanox might have helped here.<p>So competing on server grade gpu might be more difficult for intel.</div><br/></div></div><div id="37009985" class="c"><input type="checkbox" id="c-37009985" checked=""/><div class="controls bullet"><span class="by">irdc</span><span>|</span><a href="#37009548">parent</a><span>|</span><a href="#37009615">prev</a><span>|</span><a href="#37009600">next</a><span>|</span><label class="collapse" for="c-37009985">[-]</label><label class="expand" for="c-37009985">[4 more]</label></div><br/><div class="children"><div class="content">&gt; And the top is where the big money is.<p>But the bottom is where volume and profits are. If the PC market has shown us anything, it’s that only producing for the top of the market gets you outcompeted.</div><br/><div id="37010081" class="c"><input type="checkbox" id="c-37010081" checked=""/><div class="controls bullet"><span class="by">ffhhttt</span><span>|</span><a href="#37009548">root</a><span>|</span><a href="#37009985">parent</a><span>|</span><a href="#37009600">next</a><span>|</span><label class="collapse" for="c-37010081">[-]</label><label class="expand" for="c-37010081">[3 more]</label></div><br/><div class="children"><div class="content">I guess the top in the case might be the datacenter and stuff like A&#x2F;H100 which have totally obscene margins compared to any consumer grade GPUs</div><br/><div id="37010119" class="c"><input type="checkbox" id="c-37010119" checked=""/><div class="controls bullet"><span class="by">irdc</span><span>|</span><a href="#37009548">root</a><span>|</span><a href="#37010081">parent</a><span>|</span><a href="#37010120">next</a><span>|</span><label class="collapse" for="c-37010119">[-]</label><label class="expand" for="c-37010119">[1 more]</label></div><br/><div class="children"><div class="content">Sure. But it’s a lot easier to pay back your investment in product development if you can amortise it over 50.000.000 GPUs instead of 50.000 GPUs. Same for ironing out the bugs in your software.<p>Getting paid a small yearly salary for every one of your products may look really nice but the size of the market makes it a precarious business.</div><br/></div></div><div id="37010120" class="c"><input type="checkbox" id="c-37010120" checked=""/><div class="controls bullet"><span class="by">ZiiS</span><span>|</span><a href="#37009548">root</a><span>|</span><a href="#37010081">parent</a><span>|</span><a href="#37010119">prev</a><span>|</span><a href="#37009600">next</a><span>|</span><label class="collapse" for="c-37010120">[-]</label><label class="expand" for="c-37010120">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia never needs to make a GPU again.</div><br/></div></div></div></div></div></div><div id="37009600" class="c"><input type="checkbox" id="c-37009600" checked=""/><div class="controls bullet"><span class="by">debaserab2</span><span>|</span><a href="#37009548">parent</a><span>|</span><a href="#37009985">prev</a><span>|</span><a href="#37009225">next</a><span>|</span><label class="collapse" for="c-37009600">[-]</label><label class="expand" for="c-37009600">[9 more]</label></div><br/><div class="children"><div class="content">Why can&#x27;t nvidia legally produce x86 chips?</div><br/><div id="37010134" class="c"><input type="checkbox" id="c-37010134" checked=""/><div class="controls bullet"><span class="by">ZiiS</span><span>|</span><a href="#37009548">root</a><span>|</span><a href="#37009600">parent</a><span>|</span><a href="#37009624">next</a><span>|</span><label class="collapse" for="c-37010134">[-]</label><label class="expand" for="c-37010134">[1 more]</label></div><br/><div class="children"><div class="content">This is a redherring; Amazon and Apple are happier on ARM. Nvidia may have been blocked from buying them outright but are free to still make them.</div><br/></div></div><div id="37009624" class="c"><input type="checkbox" id="c-37009624" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#37009548">root</a><span>|</span><a href="#37009600">parent</a><span>|</span><a href="#37010134">prev</a><span>|</span><a href="#37009951">next</a><span>|</span><label class="collapse" for="c-37009624">[-]</label><label class="expand" for="c-37009624">[4 more]</label></div><br/><div class="children"><div class="content">Because you need a license and very few companies have one.</div><br/><div id="37009685" class="c"><input type="checkbox" id="c-37009685" checked=""/><div class="controls bullet"><span class="by">lordnacho</span><span>|</span><a href="#37009548">root</a><span>|</span><a href="#37009624">parent</a><span>|</span><a href="#37009976">next</a><span>|</span><label class="collapse" for="c-37009685">[-]</label><label class="expand" for="c-37009685">[1 more]</label></div><br/><div class="children"><div class="content">IP laws strike again. This is like saying someone can&#x27;t make Lego compatible plastic bricks, or a Torx wrench.<p>Seem unreasonable to me. Even without a legal protection there is some degree of protection for the original creator something as complicated as a CPU.</div><br/></div></div><div id="37009976" class="c"><input type="checkbox" id="c-37009976" checked=""/><div class="controls bullet"><span class="by">magnat</span><span>|</span><a href="#37009548">root</a><span>|</span><a href="#37009624">parent</a><span>|</span><a href="#37009685">prev</a><span>|</span><a href="#37009951">next</a><span>|</span><label class="collapse" for="c-37009976">[-]</label><label class="expand" for="c-37009976">[2 more]</label></div><br/><div class="children"><div class="content">Can Intel refuse to sell such license to nVidia or ask for much higher fees than other companies? If nVidia bought one of those companies, would that allow them to produce x86 chips at will?</div><br/></div></div></div></div><div id="37009619" class="c"><input type="checkbox" id="c-37009619" checked=""/><div class="controls bullet"><span class="by">Teever</span><span>|</span><a href="#37009548">root</a><span>|</span><a href="#37009600">parent</a><span>|</span><a href="#37009951">prev</a><span>|</span><a href="#37009225">next</a><span>|</span><label class="collapse" for="c-37009619">[-]</label><label class="expand" for="c-37009619">[2 more]</label></div><br/><div class="children"><div class="content">Because you need a license to do so and Intel and AMD are loathe to give a license to would-be competitors.<p>I believe that the person you&#x27;re replying to is referring to an attempt to produce x86 compatible processors that Nvidia undertook when they bought the license that Via owned.<p>IIRC that didn&#x27;t go so well for Nvidia because the license didn&#x27;t include new parts of the instruction set like the 64 bit instructions that AMD made in the mid 2000s and AMD wouldn&#x27;t license them.</div><br/><div id="37009809" class="c"><input type="checkbox" id="c-37009809" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#37009548">root</a><span>|</span><a href="#37009619">parent</a><span>|</span><a href="#37009225">next</a><span>|</span><label class="collapse" for="c-37009809">[-]</label><label class="expand" for="c-37009809">[1 more]</label></div><br/><div class="children"><div class="content">I do believe they&#x27;ve tried to circumvent the IP by implementing a translation layer like in Transmeta Crusoe, but Intel still threatened them with lawsuits.</div><br/></div></div></div></div></div></div></div></div><div id="37009225" class="c"><input type="checkbox" id="c-37009225" checked=""/><div class="controls bullet"><span class="by">halotrope</span><span>|</span><a href="#37009548">prev</a><span>|</span><a href="#37009096">next</a><span>|</span><label class="collapse" for="c-37009225">[-]</label><label class="expand" for="c-37009225">[24 more]</label></div><br/><div class="children"><div class="content">I have an a770 as my daily driver and it’s a great card. Especially at the prices. 16gb of vram, av1 encoder, can play most modern games with decent settings, good blender integration and the oneAPI libraries for compute are slowly maturing. If you don’t need CUDA or bleeding edge performance, it’s a great mid-level card at a price thats hard to beat.</div><br/><div id="37009812" class="c"><input type="checkbox" id="c-37009812" checked=""/><div class="controls bullet"><span class="by">valianteffort</span><span>|</span><a href="#37009225">parent</a><span>|</span><a href="#37010035">next</a><span>|</span><label class="collapse" for="c-37009812">[-]</label><label class="expand" for="c-37009812">[1 more]</label></div><br/><div class="children"><div class="content">&gt;and the oneAPI libraries for compute are slowly maturing<p>I&#x27;ll say I had a miserable time trying so setup oneAPI for VS Code on Windows. Their website made it seem like it was possible, there are even plugins, but I never got it working. Eventually I just gave up and downloaded Visual Studio.<p>Getting oneAPI to work on Arch Linux, where it&#x27;s not even officially supported, was also tricky but considerably easier.<p>Haven&#x27;t been able to successfully get PyTorch or TensorFlow code to utilize the GPU but did get some DPC++ working with oneAPI. All-in-all it&#x27;s not a terrible experience but could be a lot better.</div><br/></div></div><div id="37010035" class="c"><input type="checkbox" id="c-37010035" checked=""/><div class="controls bullet"><span class="by">22c</span><span>|</span><a href="#37009225">parent</a><span>|</span><a href="#37009812">prev</a><span>|</span><a href="#37009246">next</a><span>|</span><label class="collapse" for="c-37010035">[-]</label><label class="expand" for="c-37010035">[1 more]</label></div><br/><div class="children"><div class="content">&gt; 16gb of vram<p>Having only 8GB of VRAM seems to be the biggest crippling factor for this A580. I was looking at A770s on eBay and on good day you can get them for quite a good price.</div><br/></div></div><div id="37009246" class="c"><input type="checkbox" id="c-37009246" checked=""/><div class="controls bullet"><span class="by">Mistletoe</span><span>|</span><a href="#37009225">parent</a><span>|</span><a href="#37010035">prev</a><span>|</span><a href="#37009917">next</a><span>|</span><label class="collapse" for="c-37009246">[-]</label><label class="expand" for="c-37009246">[19 more]</label></div><br/><div class="children"><div class="content">I had never priced it and expected something reasonable and it is $426 on Amazon…<p>I don’t mean to start a holy war but this is why people buy consoles.  I feel like the entire PC video card universe is so disconnected from reality.  A PS5 or Xbox Series X is $499 and is a fully functioning device.</div><br/><div id="37009351" class="c"><input type="checkbox" id="c-37009351" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009246">parent</a><span>|</span><a href="#37009385">next</a><span>|</span><label class="collapse" for="c-37009351">[-]</label><label class="expand" for="c-37009351">[4 more]</label></div><br/><div class="children"><div class="content">In the console ecosystem, you pay more for games, subscriptions, fees and such, depending on your habits.<p>And you don&#x27;t get a fully functioning work PC out of it... Which is stupid, as both consoles would be great PCs, especially the XSX with bigger RAM ICs. They are kind of like high power Mac M1s.</div><br/><div id="37009870" class="c"><input type="checkbox" id="c-37009870" checked=""/><div class="controls bullet"><span class="by">erinnh</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009351">parent</a><span>|</span><a href="#37009385">next</a><span>|</span><label class="collapse" for="c-37009870">[-]</label><label class="expand" for="c-37009870">[3 more]</label></div><br/><div class="children"><div class="content">I don’t know how it is in the US, but where I live no employer allows you to use your private pc for work.<p>I agree with your first point. However a plus on the console side is that I can actually still own my games instead of licensing them (if you buy physical).</div><br/><div id="37010175" class="c"><input type="checkbox" id="c-37010175" checked=""/><div class="controls bullet"><span class="by">NekkoDroid</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009870">parent</a><span>|</span><a href="#37009385">next</a><span>|</span><label class="collapse" for="c-37010175">[-]</label><label class="expand" for="c-37010175">[2 more]</label></div><br/><div class="children"><div class="content">&gt; However a plus on the console side is that I can actually still own my games instead of licensing them (if you buy physical).<p>LOL, LMAO even.<p>Sweet summer child, wait until you actually find out.</div><br/><div id="37010267" class="c"><input type="checkbox" id="c-37010267" checked=""/><div class="controls bullet"><span class="by">erinnh</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37010175">parent</a><span>|</span><a href="#37009385">next</a><span>|</span><label class="collapse" for="c-37010267">[-]</label><label class="expand" for="c-37010267">[1 more]</label></div><br/><div class="children"><div class="content">Really helpful comment.<p>Mind explaining what you mean (preferably without being insufferable)?</div><br/></div></div></div></div></div></div></div></div><div id="37009385" class="c"><input type="checkbox" id="c-37009385" checked=""/><div class="controls bullet"><span class="by">nazgulsenpai</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009246">parent</a><span>|</span><a href="#37009351">prev</a><span>|</span><a href="#37009587">next</a><span>|</span><label class="collapse" for="c-37009385">[-]</label><label class="expand" for="c-37009385">[1 more]</label></div><br/><div class="children"><div class="content">A770 is $289 at NewEgg
<a href="https:&#x2F;&#x2F;www.newegg.com&#x2F;asrock-arc-a770-a770-pgd-8go&#x2F;p&#x2F;N82E16814930077" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.newegg.com&#x2F;asrock-arc-a770-a770-pgd-8go&#x2F;p&#x2F;N82E16...</a></div><br/></div></div><div id="37009587" class="c"><input type="checkbox" id="c-37009587" checked=""/><div class="controls bullet"><span class="by">neilv</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009246">parent</a><span>|</span><a href="#37009385">prev</a><span>|</span><a href="#37010215">next</a><span>|</span><label class="collapse" for="c-37009587">[-]</label><label class="expand" for="c-37009587">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>this is why people buy consoles.</i><p>Gaming consoles also seem to have fewer cheaters.<p>A console also compartmentalizes video gaming away from your compute&#x2F;Internet stuff.  (Both technically&#x2F;security&#x2F;privacy-wise, and in terms of distraction.)<p>(Source: Uses only a PS4 Pro for gaming, even though I have a PC with an RTX 3090 and gobs of RAM, sitting usually idle.)</div><br/><div id="37010130" class="c"><input type="checkbox" id="c-37010130" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009587">parent</a><span>|</span><a href="#37010215">next</a><span>|</span><label class="collapse" for="c-37010130">[-]</label><label class="expand" for="c-37010130">[2 more]</label></div><br/><div class="children"><div class="content">If it&#x27;s just sitting there, why not have it earn money for you? (vast.ai&#x2F;etc)</div><br/><div id="37010153" class="c"><input type="checkbox" id="c-37010153" checked=""/><div class="controls bullet"><span class="by">Hamuko</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37010130">parent</a><span>|</span><a href="#37010215">next</a><span>|</span><label class="collapse" for="c-37010153">[-]</label><label class="expand" for="c-37010153">[1 more]</label></div><br/><div class="children"><div class="content">Where are they paying to rent out my GPU so that it covers the electricity and the wear?</div><br/></div></div></div></div></div></div><div id="37010215" class="c"><input type="checkbox" id="c-37010215" checked=""/><div class="controls bullet"><span class="by">globular-toast</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009246">parent</a><span>|</span><a href="#37009587">prev</a><span>|</span><a href="#37009573">next</a><span>|</span><label class="collapse" for="c-37010215">[-]</label><label class="expand" for="c-37010215">[1 more]</label></div><br/><div class="children"><div class="content">They are too expensive but it&#x27;s slightly better if you consider it as a marginal cost on top of a PC you already have. Upgradability is the whole point of PCs really. You can keep a system going for many years by taking advantage of this. And a console isn&#x27;t really a &quot;fully functional system&quot; when compared to a PC.</div><br/></div></div><div id="37009573" class="c"><input type="checkbox" id="c-37009573" checked=""/><div class="controls bullet"><span class="by">nixass</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009246">parent</a><span>|</span><a href="#37010215">prev</a><span>|</span><a href="#37009638">next</a><span>|</span><label class="collapse" for="c-37009573">[-]</label><label class="expand" for="c-37009573">[2 more]</label></div><br/><div class="children"><div class="content">Sole reason of not being able to use keyboard and mouse (and actually run software on it) is worth my PC&#x27;a money, and it&#x27;s not much more expensive than console.</div><br/><div id="37009811" class="c"><input type="checkbox" id="c-37009811" checked=""/><div class="controls bullet"><span class="by">beAbU</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009573">parent</a><span>|</span><a href="#37009638">next</a><span>|</span><label class="collapse" for="c-37009811">[-]</label><label class="expand" for="c-37009811">[1 more]</label></div><br/><div class="children"><div class="content">Many xbox games these days support KBM.<p>I play Modern Warfare II almost exclusively with a mouse and keeb.</div><br/></div></div></div></div><div id="37009638" class="c"><input type="checkbox" id="c-37009638" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009246">parent</a><span>|</span><a href="#37009573">prev</a><span>|</span><a href="#37009654">next</a><span>|</span><label class="collapse" for="c-37009638">[-]</label><label class="expand" for="c-37009638">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  A PS5 or Xbox Series X is $499 and is a fully functioning device.<p>They can play games and nothing else. So completely worthless comparison.</div><br/></div></div><div id="37009654" class="c"><input type="checkbox" id="c-37009654" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009246">parent</a><span>|</span><a href="#37009638">prev</a><span>|</span><a href="#37009284">next</a><span>|</span><label class="collapse" for="c-37009654">[-]</label><label class="expand" for="c-37009654">[3 more]</label></div><br/><div class="children"><div class="content">Console and PC are very different gaming experiences though. One is no substitute for the other.</div><br/><div id="37010245" class="c"><input type="checkbox" id="c-37010245" checked=""/><div class="controls bullet"><span class="by">nottorp</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009654">parent</a><span>|</span><a href="#37009764">next</a><span>|</span><label class="collapse" for="c-37010245">[-]</label><label class="expand" for="c-37010245">[1 more]</label></div><br/><div class="children"><div class="content">&gt; They can play games and nothing else. So completely worthless comparison.<p>Except they cost less than a high end video card (not to mention the rest of the PC) and your games are guaranteed to work.<p>&gt; Console and PC are very different gaming experiences though. One is no substitute for the other.<p>That&#x27;s absolutely correct, but innovation in PC games is on the indie side and those aren&#x27;t so GPU hungry. Most AAA titles are the same on consoles and PC these days so might as well get them for the console.</div><br/></div></div></div></div><div id="37009284" class="c"><input type="checkbox" id="c-37009284" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009246">parent</a><span>|</span><a href="#37009654">prev</a><span>|</span><a href="#37009650">next</a><span>|</span><label class="collapse" for="c-37009284">[-]</label><label class="expand" for="c-37009284">[2 more]</label></div><br/><div class="children"><div class="content">An A770 should be ~$370, and you can buy A750s for ~$250, new of course. $426 sounds like someone&#x27;s trying to fleece you <i>or</i> there just isn&#x27;t enough floating around to sell anymore as Intel gears up for the next lineup.</div><br/><div id="37009433" class="c"><input type="checkbox" id="c-37009433" checked=""/><div class="controls bullet"><span class="by">Mistletoe</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009284">parent</a><span>|</span><a href="#37009650">next</a><span>|</span><label class="collapse" for="c-37009433">[-]</label><label class="expand" for="c-37009433">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.newegg.com&#x2F;amp&#x2F;intel-21p01j00ba&#x2F;p&#x2F;N82E16814883001" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.newegg.com&#x2F;amp&#x2F;intel-21p01j00ba&#x2F;p&#x2F;N82E1681488300...</a><p>The next google result was that one on Newegg.  I saw it was out of stock and said I’ve seen all this before and gave up.</div><br/></div></div></div></div><div id="37009650" class="c"><input type="checkbox" id="c-37009650" checked=""/><div class="controls bullet"><span class="by">Teever</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009246">parent</a><span>|</span><a href="#37009284">prev</a><span>|</span><a href="#37009917">next</a><span>|</span><label class="collapse" for="c-37009650">[-]</label><label class="expand" for="c-37009650">[1 more]</label></div><br/><div class="children"><div class="content">You know that most consoles are sold below cost, with the expectation that the difference will be made up in the premiums on games, right?</div><br/></div></div></div></div><div id="37009917" class="c"><input type="checkbox" id="c-37009917" checked=""/><div class="controls bullet"><span class="by">christkv</span><span>|</span><a href="#37009225">parent</a><span>|</span><a href="#37009246">prev</a><span>|</span><a href="#37009096">next</a><span>|</span><label class="collapse" for="c-37009917">[-]</label><label class="expand" for="c-37009917">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the state of Linux drivers for it?</div><br/><div id="37010248" class="c"><input type="checkbox" id="c-37010248" checked=""/><div class="controls bullet"><span class="by">onli</span><span>|</span><a href="#37009225">root</a><span>|</span><a href="#37009917">parent</a><span>|</span><a href="#37009096">next</a><span>|</span><label class="collapse" for="c-37010248">[-]</label><label class="expand" for="c-37010248">[1 more]</label></div><br/><div class="children"><div class="content">Last I looked not great. They work, but with worse performance than under Windows. <a href="https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=nkSkt7JqR9U">https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=nkSkt7JqR9U</a> was my source. They did get some performance improvements afterwards, but not enough to make up the difference, estimated.</div><br/></div></div></div></div></div></div><div id="37009096" class="c"><input type="checkbox" id="c-37009096" checked=""/><div class="controls bullet"><span class="by">ksec</span><span>|</span><a href="#37009225">prev</a><span>|</span><a href="#37009476">next</a><span>|</span><label class="collapse" for="c-37009096">[-]</label><label class="expand" for="c-37009096">[15 more]</label></div><br/><div class="children"><div class="content">I dont want to be pessimistic but that is still a lot of &quot;if&quot; and &quot;could&quot;. I think they are still <i>at least</i> 2 years away from seriously competing against Nvidia.<p>And this is <i>6 years</i> after Raja has taken their GPU team lead ( and now left ).</div><br/><div id="37009477" class="c"><input type="checkbox" id="c-37009477" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#37009096">parent</a><span>|</span><a href="#37009148">next</a><span>|</span><label class="collapse" for="c-37009477">[-]</label><label class="expand" for="c-37009477">[10 more]</label></div><br/><div class="children"><div class="content">Has Nvidia assembled one of the greatest engineering teams in human history, or is everyone else just not interested in competing?<p>I don&#x27;t get it. Intel have decades of relevant expertise, and have been printing money for years due to AMD&#x27;s effective absence from the top-class CPU market between 2005 and 2015.<p>And yet years and years and years go by, and Nvidia is now the world&#x27;s fifth most valuable company, far surpassing all other chip manufacturers (except Apple), and <i>nothing is happening on the market.</i><p>How the fuck is this even possible?</div><br/><div id="37009710" class="c"><input type="checkbox" id="c-37009710" checked=""/><div class="controls bullet"><span class="by">thanatos519</span><span>|</span><a href="#37009096">root</a><span>|</span><a href="#37009477">parent</a><span>|</span><a href="#37009702">next</a><span>|</span><label class="collapse" for="c-37009710">[-]</label><label class="expand" for="c-37009710">[2 more]</label></div><br/><div class="children"><div class="content">CUDA. Jensen Huang saw the high performance computing market so many years ahead that it is built on nvidia&#x27;s stuff. It is a software victory.</div><br/><div id="37009848" class="c"><input type="checkbox" id="c-37009848" checked=""/><div class="controls bullet"><span class="by">kmac_</span><span>|</span><a href="#37009096">root</a><span>|</span><a href="#37009710">parent</a><span>|</span><a href="#37009702">next</a><span>|</span><label class="collapse" for="c-37009848">[-]</label><label class="expand" for="c-37009848">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia managed to hit the right spot to win the market - developers. I wrote shaders for depth detection before CUDA, and it was already great back then, but was very graphics centric. Then CUDA came and pushed everything even further forward. I completely agree - it was a software victory.</div><br/></div></div></div></div><div id="37009702" class="c"><input type="checkbox" id="c-37009702" checked=""/><div class="controls bullet"><span class="by">inglor</span><span>|</span><a href="#37009096">root</a><span>|</span><a href="#37009477">parent</a><span>|</span><a href="#37009710">prev</a><span>|</span><a href="#37009910">next</a><span>|</span><label class="collapse" for="c-37009702">[-]</label><label class="expand" for="c-37009702">[1 more]</label></div><br/><div class="children"><div class="content">Intel has a lot of hidden unemployment, internal politics and people who honestly don’t really like working.<p>Intel hires about half of the graduates from my uni, everyone who likes coding fled to startups or super specific teams.<p>There are strong engineers in Intel for sure but it has a pretty awful culture.<p>Nvidia’a isn’t great but it’s heaps better in terms of actually taking results into account.<p>My personal experience is only with the local (Israel) branches of both.</div><br/></div></div><div id="37009910" class="c"><input type="checkbox" id="c-37009910" checked=""/><div class="controls bullet"><span class="by">bmacho</span><span>|</span><a href="#37009096">root</a><span>|</span><a href="#37009477">parent</a><span>|</span><a href="#37009702">prev</a><span>|</span><a href="#37009597">next</a><span>|</span><label class="collapse" for="c-37009910">[-]</label><label class="expand" for="c-37009910">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Has Nvidia assembled one of the greatest engineering teams in human history, or is everyone else just not interested in competing?<p>&gt; I don&#x27;t get it. Intel have decades of relevant expertise, and have been printing money for years due to AMD&#x27;s effective absence from the top-class CPU market between 2005 and 2015.<p>Noone gets it. From an armchair-expert point of view, anything nvidia can do, anyone else can do, they only need money, and they (Intel, Qualcomm, FAANG) have it.<p>Probably indeed noone was interested in competing, it didn&#x27;t seemed to be a good deal, until the past few years.</div><br/></div></div><div id="37009597" class="c"><input type="checkbox" id="c-37009597" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#37009096">root</a><span>|</span><a href="#37009477">parent</a><span>|</span><a href="#37009910">prev</a><span>|</span><a href="#37009148">next</a><span>|</span><label class="collapse" for="c-37009597">[-]</label><label class="expand" for="c-37009597">[5 more]</label></div><br/><div class="children"><div class="content">Consumer GPUs are not as large a market as you give credit for. Most GPUs are going to datacenters and consoles and devices like tablets and phones. The fact that Intel is willing to spend trillions to get a foot in the door is astonishing, and AMD is willing to let nVidia focus on ML and shoot themselves in the foot with the gamer market to hold on consoles and maybe get a leg up on desktops.</div><br/><div id="37009689" class="c"><input type="checkbox" id="c-37009689" checked=""/><div class="controls bullet"><span class="by">pbalcer</span><span>|</span><a href="#37009096">root</a><span>|</span><a href="#37009597">parent</a><span>|</span><a href="#37009641">next</a><span>|</span><label class="collapse" for="c-37009689">[-]</label><label class="expand" for="c-37009689">[2 more]</label></div><br/><div class="children"><div class="content">Even in 2022, gaming was still the largest revenue generating sector for Nvidia...</div><br/><div id="37010121" class="c"><input type="checkbox" id="c-37010121" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#37009096">root</a><span>|</span><a href="#37009689">parent</a><span>|</span><a href="#37009641">next</a><span>|</span><label class="collapse" for="c-37010121">[-]</label><label class="expand" for="c-37010121">[1 more]</label></div><br/><div class="children"><div class="content">Pretty close in 2022, but third quarter 2023 gaming revenue decreased 51% from previous year and data center revenue increased 31%, so it may be helpful to use up-to-date figures. I couldn&#x27;t find profit breadowns by sector but I would be surprised if gaming even came close to datacenter profit-wise.<p>* <a href="https:&#x2F;&#x2F;nvidianews.nvidia.com&#x2F;news&#x2F;nvidia-announces-financial-results-for-third-quarter-fiscal-2023" rel="nofollow noreferrer">https:&#x2F;&#x2F;nvidianews.nvidia.com&#x2F;news&#x2F;nvidia-announces-financia...</a></div><br/></div></div></div></div><div id="37009641" class="c"><input type="checkbox" id="c-37009641" checked=""/><div class="controls bullet"><span class="by">Teever</span><span>|</span><a href="#37009096">root</a><span>|</span><a href="#37009597">parent</a><span>|</span><a href="#37009689">prev</a><span>|</span><a href="#37009148">next</a><span>|</span><label class="collapse" for="c-37009641">[-]</label><label class="expand" for="c-37009641">[2 more]</label></div><br/><div class="children"><div class="content">Trillions?</div><br/><div id="37010122" class="c"><input type="checkbox" id="c-37010122" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#37009096">root</a><span>|</span><a href="#37009641">parent</a><span>|</span><a href="#37009148">next</a><span>|</span><label class="collapse" for="c-37010122">[-]</label><label class="expand" for="c-37010122">[1 more]</label></div><br/><div class="children"><div class="content">Excuse a bit of hyperbole.</div><br/></div></div></div></div></div></div></div></div><div id="37009148" class="c"><input type="checkbox" id="c-37009148" checked=""/><div class="controls bullet"><span class="by">ororroro</span><span>|</span><a href="#37009096">parent</a><span>|</span><a href="#37009477">prev</a><span>|</span><a href="#37009112">next</a><span>|</span><label class="collapse" for="c-37009148">[-]</label><label class="expand" for="c-37009148">[1 more]</label></div><br/><div class="children"><div class="content">Intel has a habit of claiming victory before releasing and then furiously fixing bugs post release to achieve near parity in general and small victories in limited scenarios. Their bug fixing velocity is impressive though. There is such a vibrant community of independent reviewers now that they don&#x27;t get away with it like they used to.</div><br/></div></div><div id="37009112" class="c"><input type="checkbox" id="c-37009112" checked=""/><div class="controls bullet"><span class="by">fbdab103</span><span>|</span><a href="#37009096">parent</a><span>|</span><a href="#37009148">prev</a><span>|</span><a href="#37009476">next</a><span>|</span><label class="collapse" for="c-37009112">[-]</label><label class="expand" for="c-37009112">[3 more]</label></div><br/><div class="children"><div class="content">I do not think it needs to trade blows on performance, just something that can be in the market nipping at the low-mid tier market is an improvement from the current conditions. We are currently in a market with 1.5 suppliers.</div><br/><div id="37009878" class="c"><input type="checkbox" id="c-37009878" checked=""/><div class="controls bullet"><span class="by">qwytw</span><span>|</span><a href="#37009096">root</a><span>|</span><a href="#37009112">parent</a><span>|</span><a href="#37009179">next</a><span>|</span><label class="collapse" for="c-37009878">[-]</label><label class="expand" for="c-37009878">[1 more]</label></div><br/><div class="children"><div class="content">After all the driver updates a770&#x2F;750 already seem to be quite competitive in that segment already even if the launch was quite rough.</div><br/></div></div><div id="37009179" class="c"><input type="checkbox" id="c-37009179" checked=""/><div class="controls bullet"><span class="by">snnn</span><span>|</span><a href="#37009096">root</a><span>|</span><a href="#37009112">parent</a><span>|</span><a href="#37009878">prev</a><span>|</span><a href="#37009476">next</a><span>|</span><label class="collapse" for="c-37009179">[-]</label><label class="expand" for="c-37009179">[1 more]</label></div><br/><div class="children"><div class="content">I think Intel has occupied the low-end laptop market. Nowadays nobody wants to build a laptop on ARM(except Apple). Chromebooks still have a large market, and they do not need any fancy GPU.</div><br/></div></div></div></div></div></div><div id="37009476" class="c"><input type="checkbox" id="c-37009476" checked=""/><div class="controls bullet"><span class="by">snvzz</span><span>|</span><a href="#37009096">prev</a><span>|</span><a href="#37010178">next</a><span>|</span><label class="collapse" for="c-37009476">[-]</label><label class="expand" for="c-37009476">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d still just buy the AMD gpu.<p>Mature drivers, and better tested alongside AMD cpus.<p>Also, I never liked Intel[0].<p>0. <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=osSMJRyxG0k">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=osSMJRyxG0k</a></div><br/></div></div><div id="37010178" class="c"><input type="checkbox" id="c-37010178" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#37009476">prev</a><span>|</span><a href="#37009316">next</a><span>|</span><label class="collapse" for="c-37010178">[-]</label><label class="expand" for="c-37010178">[1 more]</label></div><br/><div class="children"><div class="content">There’s no facts here just rumors.</div><br/></div></div><div id="37009316" class="c"><input type="checkbox" id="c-37009316" checked=""/><div class="controls bullet"><span class="by">xbmcuser</span><span>|</span><a href="#37010178">prev</a><span>|</span><a href="#37009207">next</a><span>|</span><label class="collapse" for="c-37009316">[-]</label><label class="expand" for="c-37009316">[1 more]</label></div><br/><div class="children"><div class="content">For a plex or jellyfin server intel graphic cards are supposedly great as you get multiple encoding  decoding 4k streams and better compatibility with linux for cheap</div><br/></div></div><div id="37009207" class="c"><input type="checkbox" id="c-37009207" checked=""/><div class="controls bullet"><span class="by">WhereIsTheTruth</span><span>|</span><a href="#37009316">prev</a><span>|</span><a href="#37008995">next</a><span>|</span><label class="collapse" for="c-37009207">[-]</label><label class="expand" for="c-37009207">[4 more]</label></div><br/><div class="children"><div class="content">&gt; AMD have been slacking on low-end graphics card releases this gen<p>That&#x27;s blatant misinformation, this A580 seems on par with RX 7600, same price, AMD has lower TDP, it was released May 2023, and the journalist dare to say AMD has been slacking off?! What Intel doing for releasing same GPU as competition 1 year later?!</div><br/><div id="37009498" class="c"><input type="checkbox" id="c-37009498" checked=""/><div class="controls bullet"><span class="by">AlotOfReading</span><span>|</span><a href="#37009207">parent</a><span>|</span><a href="#37009366">next</a><span>|</span><label class="collapse" for="c-37009498">[-]</label><label class="expand" for="c-37009498">[1 more]</label></div><br/><div class="children"><div class="content">Releasing a competitive GPU as a first-gen product and only being a year late is impressive as hell.<p>Let me go through why it&#x27;s hard to release GPUs at the right time as someone who&#x27;s done this at a different company. Development probably started 3-5 years ago, before COVID. They would have estimated what AMD&#x2F;Nvidia&#x27;s performance would be that far out and what TSMC&#x27;s process would look like, then sketched out something that might meet a reasonable performance&#x2F;cost target, with random guesses at a bunch of important and completely unknown factors like driver quality. Then COVID hit and the entire manufacturing world descended into utter chaos for the better part of 2 years. This is probably a large factor behind why Intel didn&#x27;t manage their initial 2020&#x2F;2021 estimates, with another part being that management at any company has no idea how to estimate silicon timelines. On top of that, the 10, 20, and 30 series all had completely different performance increases, probably generating a few revisions of that performance target on their own. Every time they revise the design to keep it competitive though, firmware is screaming at them about stability, and software is screaming at them about timelines, and leadership is screaming about costs.<p>But somehow despite all of that, they manage a relatively timely release and it&#x27;s competitive in the low end after a few months of driver improvements. Unfortunately there&#x27;s another problem. These things are fabbed at TSMC, not Intel. Intel doesn&#x27;t have that many wafers at TSMC compared to Nvidia&#x2F;AMD and meaningfully increasing that is expensive. So instead they make the rational decision to release the higher-margin stuff first and let the drivers bake some more. Everything else can get price-adjusted (and minimally design-adjusted) before it releases to make it competitive, which is why the article doesn&#x27;t have actual pricing.</div><br/></div></div><div id="37009366" class="c"><input type="checkbox" id="c-37009366" checked=""/><div class="controls bullet"><span class="by">samspenc</span><span>|</span><a href="#37009207">parent</a><span>|</span><a href="#37009498">prev</a><span>|</span><a href="#37008995">next</a><span>|</span><label class="collapse" for="c-37009366">[-]</label><label class="expand" for="c-37009366">[2 more]</label></div><br/><div class="children"><div class="content">Yeah if I look up Intel&#x27;s GPUs in the open-source Blender benchmark for 3D performance [1], the leading Intel Arc A770 GPU is slower than ... the NVidia GeForce RTX 2070?! Which came out in 2019 [2], so Intel&#x27;s latest GPUs are slower than NVidia&#x27;s chips that are 2 generations and 4 years old.<p><pre><code>  - [1] https:&#x2F;&#x2F;opendata.blender.org&#x2F;benchmarks&#x2F;query&#x2F;?compute_type=OPTIX&amp;compute_type=CUDA&amp;compute_type=HIP&amp;compute_type=METAL&amp;compute_type=ONEAPI&amp;group_by=device_name&amp;blender_version=3.6.0
  - [2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GeForce_20_series</code></pre></div><br/><div id="37009969" class="c"><input type="checkbox" id="c-37009969" checked=""/><div class="controls bullet"><span class="by">jdboyd</span><span>|</span><a href="#37009207">root</a><span>|</span><a href="#37009366">parent</a><span>|</span><a href="#37008995">next</a><span>|</span><label class="collapse" for="c-37009969">[-]</label><label class="expand" for="c-37009969">[1 more]</label></div><br/><div class="children"><div class="content">The Arc A770 came out in the time of the 30x0 series, so it would be more fair to say that it is slower than a higher end card one generation older than it. That isn&#x27;t an uncommon occurrence.  For instance, the 2070s are often faster than the 3060s, and the A770 was supposed to be roughly aiming for 3060 performance.<p>Overall, I&#x27;d call it close enough in performance that I&#x27;d be interested in giving it a try. On the work side, I worry more about being bitten by lack of CUDA (although I like the look of SYCL that Intel&#x2F;Kronos push as an alternative), and on the gaming side, I worry more about how if DX10&#x2F;DX11 titles will ever really get sorted out. Still, when I get a desktop that supports Resizable BAR, I will likely put an Intel GPU in that machine to give it a try.</div><br/></div></div></div></div></div></div><div id="37008995" class="c"><input type="checkbox" id="c-37008995" checked=""/><div class="controls bullet"><span class="by">fbdab103</span><span>|</span><a href="#37009207">prev</a><span>|</span><a href="#37009436">next</a><span>|</span><label class="collapse" for="c-37008995">[-]</label><label class="expand" for="c-37008995">[12 more]</label></div><br/><div class="children"><div class="content">Is there any potential on the horizon for an Intel card to work with CUDA?</div><br/><div id="37009082" class="c"><input type="checkbox" id="c-37009082" checked=""/><div class="controls bullet"><span class="by">snnn</span><span>|</span><a href="#37008995">parent</a><span>|</span><a href="#37009358">next</a><span>|</span><label class="collapse" for="c-37009082">[-]</label><label class="expand" for="c-37009082">[4 more]</label></div><br/><div class="children"><div class="content">Bet on WebGPU. Intel and Google are the two main drafters of WebGPU spec.  I can tell you that the card definitely will support WebGPU well, and WebGPU is not just for web. It will run on any device with a very small footprint runtime library and does not need a browser.</div><br/><div id="37009260" class="c"><input type="checkbox" id="c-37009260" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#37008995">root</a><span>|</span><a href="#37009082">parent</a><span>|</span><a href="#37009358">next</a><span>|</span><label class="collapse" for="c-37009260">[-]</label><label class="expand" for="c-37009260">[3 more]</label></div><br/><div class="children"><div class="content">WebGPU isn&#x27;t polyglot as CUDA, so already out of question, let alone non existing tooling like NInsights.</div><br/><div id="37009467" class="c"><input type="checkbox" id="c-37009467" checked=""/><div class="controls bullet"><span class="by">WanderPanda</span><span>|</span><a href="#37008995">root</a><span>|</span><a href="#37009260">parent</a><span>|</span><a href="#37009358">next</a><span>|</span><label class="collapse" for="c-37009467">[-]</label><label class="expand" for="c-37009467">[2 more]</label></div><br/><div class="children"><div class="content">What do you mean by polyglot? Things like CUDA.jl?</div><br/><div id="37009882" class="c"><input type="checkbox" id="c-37009882" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#37008995">root</a><span>|</span><a href="#37009467">parent</a><span>|</span><a href="#37009358">next</a><span>|</span><label class="collapse" for="c-37009882">[-]</label><label class="expand" for="c-37009882">[1 more]</label></div><br/><div class="children"><div class="content">Being able to write C, C++, Fortran, Haskell, Java, C#, F#, Futhark, Julia code to run on the GPU, or any other language with a compiler toolchain able to target PTX bytecode.<p>And the related graphical tooling for debugging such workflows when stuff goes wrong on the GPU.</div><br/></div></div></div></div></div></div></div></div><div id="37009358" class="c"><input type="checkbox" id="c-37009358" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37008995">parent</a><span>|</span><a href="#37009082">prev</a><span>|</span><a href="#37009269">next</a><span>|</span><label class="collapse" for="c-37009358">[-]</label><label class="expand" for="c-37009358">[1 more]</label></div><br/><div class="children"><div class="content">OpenVINO is super easy in PyTorch and a few other places.<p>And there are other specific efforts like Embree (which now supports Intel GPU rendering), and Vukan ML efforts like Apache TVM and MLIR-based projects.</div><br/></div></div><div id="37009269" class="c"><input type="checkbox" id="c-37009269" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#37008995">parent</a><span>|</span><a href="#37009358">prev</a><span>|</span><a href="#37009064">next</a><span>|</span><label class="collapse" for="c-37009269">[-]</label><label class="expand" for="c-37009269">[1 more]</label></div><br/><div class="children"><div class="content">Any CUDA alternative has to be on the same level supporting polyglot programming, IDE and graphical debugging, libraries.<p>Alternatives based on C or shading languages, have already lost before the game has even started.</div><br/></div></div><div id="37009064" class="c"><input type="checkbox" id="c-37009064" checked=""/><div class="controls bullet"><span class="by">monocasa</span><span>|</span><a href="#37008995">parent</a><span>|</span><a href="#37009269">prev</a><span>|</span><a href="#37009436">next</a><span>|</span><label class="collapse" for="c-37009064">[-]</label><label class="expand" for="c-37009064">[5 more]</label></div><br/><div class="children"><div class="content">Not any time soon.  CUDA is very Nvidia specific.</div><br/><div id="37009141" class="c"><input type="checkbox" id="c-37009141" checked=""/><div class="controls bullet"><span class="by">jms55</span><span>|</span><a href="#37008995">root</a><span>|</span><a href="#37009064">parent</a><span>|</span><a href="#37009073">next</a><span>|</span><label class="collapse" for="c-37009141">[-]</label><label class="expand" for="c-37009141">[2 more]</label></div><br/><div class="children"><div class="content">When people say CUDA is Nvidia specific, I don&#x27;t think they realize _how_ specific. It&#x27;s not just a matter of having the API implementation.<p>CUDA does things like depend on specific scheduler behavior for GPU threads in order to guarantee forward progress, allowing more efficient single-pass computation routines. Or allowing CUDA kernels to launch additional kernels or allocate GPU memory from within the kernel, without host communication. Or checking the GPU architecture and performing specific micro-optimizations designed around that hardware&#x27;s internal design.<p>GPU&#x27;s are nothing like CPU&#x27;s, in that the x86 architecture is fairly stable. There&#x27;s not a _ton_ of difference between different CPUs. Sure, performance characteristics and cache sizes might be different, but generally they have the same instruction set. Each GPU generation is basically a completely different architecture, much less between GPU companies. That&#x27;s (partly) why Vulkan is so complicated - it tries to support the lowest spec 2013 mobile GPU, and the highest spec 2023 desktop GPU in one single API.</div><br/><div id="37009220" class="c"><input type="checkbox" id="c-37009220" checked=""/><div class="controls bullet"><span class="by">nologic01</span><span>|</span><a href="#37008995">root</a><span>|</span><a href="#37009141">parent</a><span>|</span><a href="#37009073">next</a><span>|</span><label class="collapse" for="c-37009220">[-]</label><label class="expand" for="c-37009220">[1 more]</label></div><br/><div class="children"><div class="content">This is the result of GPU computationally intensive tasks not being as simply defined as the CPU workloads. It was always thus for the various accelerators and HPC designs. They must make design choices that favor a relatively narrow range of computational patterns whereas the CPU will happily chew through a wider range of instruction sequences with the same performance.<p>The corresponding GPU software complexity is something that feels not quite appreciated. Yet if we are going to see the massive adoption of GPU&#x27;s that the market projects, with multiple providers of hardware, we&#x27;ll need some serious advances of the software side.</div><br/></div></div></div></div><div id="37009861" class="c"><input type="checkbox" id="c-37009861" checked=""/><div class="controls bullet"><span class="by">Grum9</span><span>|</span><a href="#37008995">root</a><span>|</span><a href="#37009064">parent</a><span>|</span><a href="#37009073">prev</a><span>|</span><a href="#37009436">next</a><span>|</span><label class="collapse" for="c-37009861">[-]</label><label class="expand" for="c-37009861">[1 more]</label></div><br/><div class="children"><div class="content">CUDA on AMD attempt<p><a href="https:&#x2F;&#x2F;community.amd.com&#x2F;t5&#x2F;rocm&#x2F;available-now-new-hip-sdk-helps-democratize-gpu-computing&#x2F;ba-p&#x2F;621029&#x2F;jump-to&#x2F;first-unread-message" rel="nofollow noreferrer">https:&#x2F;&#x2F;community.amd.com&#x2F;t5&#x2F;rocm&#x2F;available-now-new-hip-sdk-...</a></div><br/></div></div></div></div></div></div><div id="37009436" class="c"><input type="checkbox" id="c-37009436" checked=""/><div class="controls bullet"><span class="by">georgeburdell</span><span>|</span><a href="#37008995">prev</a><span>|</span><a href="#37009371">next</a><span>|</span><label class="collapse" for="c-37009436">[-]</label><label class="expand" for="c-37009436">[4 more]</label></div><br/><div class="children"><div class="content">I don’t understand how Intel can afford to sell these so cheaply given they are fabbed at TSMC.</div><br/><div id="37009449" class="c"><input type="checkbox" id="c-37009449" checked=""/><div class="controls bullet"><span class="by">adam_arthur</span><span>|</span><a href="#37009436">parent</a><span>|</span><a href="#37009445">next</a><span>|</span><label class="collapse" for="c-37009449">[-]</label><label class="expand" for="c-37009449">[2 more]</label></div><br/><div class="children"><div class="content">NVDA also fabs at TSMC and has 60% gross margins, so there&#x27;s a lot of room to undercut on price while still being profitable.<p>The real question is why doesn&#x27;t TSMC charge more? How can a customer using the most cutting edge fab get 60% margins? (Similar for other customers).<p>Clearly there&#x27;s a lot of pricing power on the fab side that&#x27;s not being asserted.</div><br/><div id="37010044" class="c"><input type="checkbox" id="c-37010044" checked=""/><div class="controls bullet"><span class="by">Panzer04</span><span>|</span><a href="#37009436">root</a><span>|</span><a href="#37009449">parent</a><span>|</span><a href="#37009445">next</a><span>|</span><label class="collapse" for="c-37010044">[-]</label><label class="expand" for="c-37010044">[1 more]</label></div><br/><div class="children"><div class="content">Samsung Foundry exists, and Nvidia used them for the 30 series iirc. TSMC just doesn&#x27;t have that much pricing power as a result. Their best processes are obviously good, but if they try to gouge too much customers probably just take the hit for a year. If they have to they can always sacrifice size&#x2F;power for performance on an older, cheaper process.</div><br/></div></div></div></div><div id="37009445" class="c"><input type="checkbox" id="c-37009445" checked=""/><div class="controls bullet"><span class="by">mahkeiro</span><span>|</span><a href="#37009436">parent</a><span>|</span><a href="#37009449">prev</a><span>|</span><a href="#37009371">next</a><span>|</span><label class="collapse" for="c-37009445">[-]</label><label class="expand" for="c-37009445">[1 more]</label></div><br/><div class="children"><div class="content">Have a look at Nvidia financial statement to get an idea why ;)</div><br/></div></div></div></div><div id="37009371" class="c"><input type="checkbox" id="c-37009371" checked=""/><div class="controls bullet"><span class="by">chx</span><span>|</span><a href="#37009436">prev</a><span>|</span><a href="#37009846">next</a><span>|</span><label class="collapse" for="c-37009371">[-]</label><label class="expand" for="c-37009371">[1 more]</label></div><br/><div class="children"><div class="content">So<p>&gt; a TBP of 175W.<p>The RTX 3060 was 170 Watt, the RX 7600 is 165 W. So it&#x27;s on par efficiency wise as well, it would seem.</div><br/></div></div><div id="37009846" class="c"><input type="checkbox" id="c-37009846" checked=""/><div class="controls bullet"><span class="by">ShadowBanThis01</span><span>|</span><a href="#37009371">prev</a><span>|</span><a href="#37009610">next</a><span>|</span><label class="collapse" for="c-37009846">[-]</label><label class="expand" for="c-37009846">[1 more]</label></div><br/><div class="children"><div class="content">I thought Intel was giving up on GPUs already...</div><br/></div></div><div id="37009610" class="c"><input type="checkbox" id="c-37009610" checked=""/><div class="controls bullet"><span class="by">MrBuddyCasino</span><span>|</span><a href="#37009846">prev</a><span>|</span><label class="collapse" for="c-37009610">[-]</label><label class="expand" for="c-37009610">[2 more]</label></div><br/><div class="children"><div class="content">In 2023, 8GB of RAM is simply insufficient for anything above entry-level.<p>Already Ratchet &amp; Clank in FHD has a &gt;50% performance advantage when you compare the 16GiB 4060TI version against the 8GB version, when comparing avg. fps P1 it rises to +70%.<p>This is the biggest outlier, but many other games suffer badly with just 8GB [0]. It will only get worse going forward, so don’t buy a 8GB card if don’t have to, it will age badly.<p>[0] <a href="https:&#x2F;&#x2F;www.pcgameshardware.de&#x2F;Geforce-RTX-4060-Ti-16GB-Grafikkarte-279647&#x2F;Specials&#x2F;Test-Review-Preis-Release-8GB-vs-16GB-Specs-Benchmark-1425140&#x2F;galerie&#x2F;3770818&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.pcgameshardware.de&#x2F;Geforce-RTX-4060-Ti-16GB-Graf...</a></div><br/><div id="37010266" class="c"><input type="checkbox" id="c-37010266" checked=""/><div class="controls bullet"><span class="by">nottorp</span><span>|</span><a href="#37009610">parent</a><span>|</span><label class="collapse" for="c-37010266">[-]</label><label class="expand" for="c-37010266">[1 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t read that because I don&#x27;t understand the popup that covers the article and I&#x27;m not clicking on something I can&#x27;t read :)<p>However, isn&#x27;t this specific game you&#x27;re mentioned just a case of a crap port from consoles?<p>Personally I wouldn&#x27;t try any traditional console franchise on a PC. They&#x27;re just not designed for it, even if you plug in a controller.</div><br/></div></div></div></div></div></div></div></div></div></body></html>