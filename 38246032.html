<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1699866063396" as="style"/><link rel="stylesheet" href="styles.css?v=1699866063396"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://spectrum.ieee.org/generative-ai-training">MLPerf training tests put Nvidia ahead, Intel close, and Google well behind</a> <span class="domain">(<a href="https://spectrum.ieee.org">spectrum.ieee.org</a>)</span></div><div class="subtext"><span>vissidarte_choi</span> | <span>30 comments</span></div><br/><div><div id="38246465" class="c"><input type="checkbox" id="c-38246465" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#38246569">next</a><span>|</span><label class="collapse" for="c-38246465">[-]</label><label class="expand" for="c-38246465">[4 more]</label></div><br/><div class="children"><div class="content">One bad sign is that Google recently put out a press release that they’ve achieved the largest training job ever run, but they didn’t actually train any models. It was a bandwidth test: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38227836">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38227836</a><p>(Shoutout to DavidSJ who sleuthed this out based on the careful wording of the post.)<p>This was a big shock to me since MLPerf is the gold standard for performance comparisons. The reason you want to train an actual model on real data is because it gives a precise idea of what you can expect in the field. Google’s 2019 MLPerf results are why I dove in to TPU land and fell in love with TPUs. Nowadays it’s all about language models and giant training runs and there’s not a lot of compute left over for average joes (which I very much was), but rants aren’t productive.<p>What would be productive is for Google to focus on the basics: focus on MLPerf. It’s the standard benchmark for a reason. I can’t fault them for going where the money is, but it’s still sad to see the sorry state of Google being dead last. I’m still rooting for you, Jax team. I believe in you.<p>TRC is looking better these days. If you need spare compute or just want to try out TPUs, give them a spin for a month. When I was complaining on Twitter in 2023 about TPUs being deleted after 30 minutes circa 2022, two people stepped in and said that they were happy with their quotas and haven’t observed similar pain points. So maybe they’ve solved the provisioning issues, and they’re starting to treat TRC members as more than third-class citizens again. But, especially in modern times, it’s hard to complain when compute is so scarce; you can do some pretty amazing things with TPUs if you focus on them for a few months. Thank you to the organizers of TRC (Jonathan in particular, their support lead, who is single handedly the most impressive support person I’ve ever seen by a huge margin; and Zak for keeping so many balls in the air while leading the TPU infra).</div><br/><div id="38246768" class="c"><input type="checkbox" id="c-38246768" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#38246465">parent</a><span>|</span><a href="#38247253">next</a><span>|</span><label class="collapse" for="c-38246768">[-]</label><label class="expand" for="c-38246768">[2 more]</label></div><br/><div class="children"><div class="content">&gt; What would be productive is for Google to focus on the basics: focus on MLPerf. It’s the standard benchmark for a reason. I can’t fault them for going where the money is, but it’s still sad to see the sorry state of Google being dead last.<p>You seem to be suggesting that Google isn&#x27;t focusing on performance on MLPerf but on something else that&#x27;s more lucrative. Do you want to expound on what that is?</div><br/><div id="38246838" class="c"><input type="checkbox" id="c-38246838" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#38246465">root</a><span>|</span><a href="#38246768">parent</a><span>|</span><a href="#38247253">next</a><span>|</span><label class="collapse" for="c-38246838">[-]</label><label class="expand" for="c-38246838">[1 more]</label></div><br/><div class="children"><div class="content">Large LLM training workloads. It’s where the money’s at, and it’s why they went for &quot;world’s biggest training run&quot; headline rather than leading the pack on MLPerf standards. Not the least of which is because they can build their own LLM framework for customers to use, rather than spending time getting an MLPerf result that customers can’t possibly use. (I know because I’ve tried; it was quite a lot of archaeology to figure out how to dig out their 2019 MLPerf code for my own needs.)<p>I don’t blame them. It’s the right move. I just miss the old days. But those aren’t coming back, and for better or worse the entire world is GPT and Diffusion focused for at least the next N years.<p>Hopefully nerds can figure out how to scrape enough compute together to do interesting things during this gold rush period. It’s why TRC is that much more important, since it’s an opportunity to mint the next generation of engineers-turned-researchers. It would be smart for nvidia to offer something similar using their aging A100s. But I think only Google understands how powerful it is to capture the college students and interns.</div><br/></div></div></div></div><div id="38247253" class="c"><input type="checkbox" id="c-38247253" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#38246465">parent</a><span>|</span><a href="#38246768">prev</a><span>|</span><a href="#38246569">next</a><span>|</span><label class="collapse" for="c-38247253">[-]</label><label class="expand" for="c-38247253">[1 more]</label></div><br/><div class="children"><div class="content">Lot of MLPerf tasks takes less than a minute. Meaning it likely takes more time starting the training and warming up than the actual training itself. MLPerf is widely outdated.</div><br/></div></div></div></div><div id="38246569" class="c"><input type="checkbox" id="c-38246569" checked=""/><div class="controls bullet"><span class="by">joshvm</span><span>|</span><a href="#38246465">prev</a><span>|</span><a href="#38247770">next</a><span>|</span><label class="collapse" for="c-38246569">[-]</label><label class="expand" for="c-38246569">[6 more]</label></div><br/><div class="children"><div class="content">The headline feels like clickbait. Does MLPerf also benchmark power consumption&#x2F;cost per submission? It&#x27;s not particularly illuminating that if you use a 10k GPU cluster, you can train things quickly and a &quot;per chip&quot; comparison is vague.<p>For most people the bottom line is cost and availability anyway - does it matter if a TPU is twice as slow if it costs half as much? I think I read somewhere that Apple Silicon is actually one of the most efficient platforms to use for development (e.g. a tricked out Mini makes quite a good inference server); I&#x27;ve been able to train recent object detection models on my M2 without much trouble (and it&#x27;s much cheaper than paying for cloud time).</div><br/><div id="38246612" class="c"><input type="checkbox" id="c-38246612" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#38246569">parent</a><span>|</span><a href="#38247589">next</a><span>|</span><label class="collapse" for="c-38246612">[-]</label><label class="expand" for="c-38246612">[3 more]</label></div><br/><div class="children"><div class="content">Nah, MLPerf is legit. I was skeptical of it too, but wanted to leave a quick note (I have to run) that they’re solid. I’d be the first to call it out if it was pointless or misleading, but it turns out to be the only way to get a true idea of comparisons across different hardware. It forces everyone to achieve <i>the same goal</i>, which is key; otherwise you’re left with a bunch of slight (and large) variations that tell you nothing about achieving the actual goal, which is all you care about.<p>It’s a bit like a shared space race. Getting that top-N accuracy to 74 point yada yada percent in 37 seconds on a certain resnet architecture tells you that you can do the same thing on that hardware, which means you can do your own things just as quickly. So it’s a worthwhile time investment. If you force everyone to get that same accuracy with the same model arch, you can make informed decisions, which is especially important when throwing millions of VC dollars around.<p>EDIT: Sorry, I completely misread you.<p>It’s difficult to quantify what you’d like to measure: the bottom line price of who is cheaper for your expected workload. There are immense trade offs, not least of which is time spent learning a particular (esoteric) stack. CUDA knowledge doesn’t port to JAX and vice-versa.<p>Prices are always changing, and you can usually work out a deal with the sales team to get lower than advertised. Especially if they want you to choose them instead of some competitor. So in general it’s hard to figure out what you can expect for production workloads in terms of total dev cost vs price vs speed.<p>I will say that as a researcher, there’s no substitute for fast iteration cycles. I’m one of the few who believe in scaling down your models as much as possible when testing experimental ideas, precisely because you can try 30 runs instead of 3. So all else being equal, I’d take speed.<p>But all else isn’t equal. The only thing I want nowadays is free plus stable. It’s looking like a 4090 might be the way to get that, which is enough to try out some interesting ideas.</div><br/><div id="38246669" class="c"><input type="checkbox" id="c-38246669" checked=""/><div class="controls bullet"><span class="by">Permit</span><span>|</span><a href="#38246569">root</a><span>|</span><a href="#38246612">parent</a><span>|</span><a href="#38247589">next</a><span>|</span><label class="collapse" for="c-38246669">[-]</label><label class="expand" for="c-38246669">[2 more]</label></div><br/><div class="children"><div class="content">The person you’re replying to is not questioning MLPerf, but rather this article’s interpretation of the results.</div><br/><div id="38246696" class="c"><input type="checkbox" id="c-38246696" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#38246569">root</a><span>|</span><a href="#38246669">parent</a><span>|</span><a href="#38247589">next</a><span>|</span><label class="collapse" for="c-38246696">[-]</label><label class="expand" for="c-38246696">[1 more]</label></div><br/><div class="children"><div class="content">Oops. Thank you.</div><br/></div></div></div></div></div></div><div id="38247589" class="c"><input type="checkbox" id="c-38247589" checked=""/><div class="controls bullet"><span class="by">latency-guy2</span><span>|</span><a href="#38246569">parent</a><span>|</span><a href="#38246612">prev</a><span>|</span><a href="#38246631">next</a><span>|</span><label class="collapse" for="c-38247589">[-]</label><label class="expand" for="c-38247589">[1 more]</label></div><br/><div class="children"><div class="content">Cost, availability, and speed.<p>Salaries more often than not, costs a hell of a lot more than compute and energy bills cost. If you have a 10k cluster, you&#x27;re probably a mega-corp running a few teams sharing the resources running experiments in parallel to each other and themselves.<p>Now this is an very much so an overestimate on the actual cost of compute, the $10&#x2F;hr&#x2F;GPU cost is in all likelihood still significantly cheaper than the cash you pay to your researchers, and all the things that are needed to support them. Unless your team of researchers is about 10 people running very efficiently and making use of ALL the GPUs each hour of every day.</div><br/></div></div><div id="38246631" class="c"><input type="checkbox" id="c-38246631" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38246569">parent</a><span>|</span><a href="#38247589">prev</a><span>|</span><a href="#38247770">next</a><span>|</span><label class="collapse" for="c-38246631">[-]</label><label class="expand" for="c-38246631">[1 more]</label></div><br/><div class="children"><div class="content">The H100 costs 10x on AWS compared to the TPUv5e on GCP. And it is apparently 5x faster in GPT training. Which makes the headline sort of backwards.</div><br/></div></div></div></div><div id="38247770" class="c"><input type="checkbox" id="c-38247770" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#38246569">prev</a><span>|</span><a href="#38246487">next</a><span>|</span><label class="collapse" for="c-38247770">[-]</label><label class="expand" for="c-38247770">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  &quot;We delivered more than what was promised-a 103 percent reduction in time-to- train<p>Can someone explain this math to me?</div><br/></div></div><div id="38246487" class="c"><input type="checkbox" id="c-38246487" checked=""/><div class="controls bullet"><span class="by">jfim</span><span>|</span><a href="#38247770">prev</a><span>|</span><a href="#38246556">next</a><span>|</span><label class="collapse" for="c-38246487">[-]</label><label class="expand" for="c-38246487">[1 more]</label></div><br/><div class="children"><div class="content">Oddly the article talks about training performance, but the link to the tests links to the inference benchmark. The correct link to the training benchmark results is at <a href="https:&#x2F;&#x2F;mlcommons.org&#x2F;benchmarks&#x2F;training&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;mlcommons.org&#x2F;benchmarks&#x2F;training&#x2F;</a></div><br/></div></div><div id="38246556" class="c"><input type="checkbox" id="c-38246556" checked=""/><div class="controls bullet"><span class="by">frays</span><span>|</span><a href="#38246487">prev</a><span>|</span><a href="#38246741">next</a><span>|</span><label class="collapse" for="c-38246556">[-]</label><label class="expand" for="c-38246556">[8 more]</label></div><br/><div class="children"><div class="content">This needs to be called out anytime an AI benchmarking article with Google as one of its key comparisons is published: many of the outstanding AI researchers and innovators who gave Google the reputation it has today are no longer at the company.<p>Startups such as Essential AI, Ideogram, Character.ai were all founded by ex-Googlers. Plenty of engineers and researchers from Google Brain&#x2F;Research have pivoted into these types of start-ups, not necessarily as founders but as individual contributors continuing their engineering and research work at these companies instead.<p>Heck, Google is even considering investing into Character.ai (ex-Google founder) based on the news released a few days ago: <a href="https:&#x2F;&#x2F;www.reuters.com&#x2F;technology&#x2F;google-talks-invest-ai-startup-characterai-sources-2023-11-10&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reuters.com&#x2F;technology&#x2F;google-talks-invest-ai-st...</a><p>It&#x27;s not surprising that Google isn&#x27;t the leader of AI anymore. The people who put Google in that lead position are gone.</div><br/><div id="38246736" class="c"><input type="checkbox" id="c-38246736" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#38246556">parent</a><span>|</span><a href="#38247413">prev</a><span>|</span><a href="#38246741">next</a><span>|</span><label class="collapse" for="c-38246736">[-]</label><label class="expand" for="c-38246736">[6 more]</label></div><br/><div class="children"><div class="content">Google doesn&#x27;t build, it buys.</div><br/><div id="38247042" class="c"><input type="checkbox" id="c-38247042" checked=""/><div class="controls bullet"><span class="by">VirusNewbie</span><span>|</span><a href="#38246556">root</a><span>|</span><a href="#38246736">parent</a><span>|</span><a href="#38246741">next</a><span>|</span><label class="collapse" for="c-38247042">[-]</label><label class="expand" for="c-38247042">[5 more]</label></div><br/><div class="children"><div class="content">Did they buy K8s, TPU, Jax or TF?</div><br/><div id="38247060" class="c"><input type="checkbox" id="c-38247060" checked=""/><div class="controls bullet"><span class="by">deaddodo</span><span>|</span><a href="#38246556">root</a><span>|</span><a href="#38247042">parent</a><span>|</span><a href="#38246741">next</a><span>|</span><label class="collapse" for="c-38247060">[-]</label><label class="expand" for="c-38247060">[4 more]</label></div><br/><div class="children"><div class="content">Or even, you know, the entire Google suite?<p>There&#x27;s plenty to criticize Alphabet&#x2F;GOOG on. But pretending they don&#x27;t develop things in house is just naive.</div><br/><div id="38247321" class="c"><input type="checkbox" id="c-38247321" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#38246556">root</a><span>|</span><a href="#38247060">parent</a><span>|</span><a href="#38247316">next</a><span>|</span><label class="collapse" for="c-38247321">[-]</label><label class="expand" for="c-38247321">[1 more]</label></div><br/><div class="children"><div class="content">While I agree with your larger point - I think GSuite was built around an acquisition (Writely) - or that may have been just Google docs.</div><br/></div></div><div id="38247316" class="c"><input type="checkbox" id="c-38247316" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#38246556">root</a><span>|</span><a href="#38247060">parent</a><span>|</span><a href="#38247321">prev</a><span>|</span><a href="#38246741">next</a><span>|</span><label class="collapse" for="c-38247316">[-]</label><label class="expand" for="c-38247316">[2 more]</label></div><br/><div class="children"><div class="content">Maybe not entire.<p>Google Ads: built from great many pieces, one of the largest was DoubleClick, acquired in 2008.
Google Docs: acquired in 2006 from Writely.
Google Sheets: acquired in 2006 from XL2Web. 
Google Maps: acquired in 2004.<p>I&#x27;m not trying to say that Google acquired everything and hasn&#x27;t built anything; not at all. But a few crown jewels were indeed acquisitions.</div><br/><div id="38248337" class="c"><input type="checkbox" id="c-38248337" checked=""/><div class="controls bullet"><span class="by">Arainach</span><span>|</span><a href="#38246556">root</a><span>|</span><a href="#38247316">parent</a><span>|</span><a href="#38246741">next</a><span>|</span><label class="collapse" for="c-38248337">[-]</label><label class="expand" for="c-38248337">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s true of most large companies.<p>Microsoft acquired Powerpoint.<p>Meta acquired Instagram.<p>Apple acquired NeXT and Siri.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38246741" class="c"><input type="checkbox" id="c-38246741" checked=""/><div class="controls bullet"><span class="by">huac</span><span>|</span><a href="#38246556">prev</a><span>|</span><a href="#38246390">next</a><span>|</span><label class="collapse" for="c-38246741">[-]</label><label class="expand" for="c-38246741">[3 more]</label></div><br/><div class="children"><div class="content">10k H100&#x27;s cost around $400M at sticker price. Sometimes I wonder if a massive setup like this is less expensive (volume discount) or more.</div><br/><div id="38247054" class="c"><input type="checkbox" id="c-38247054" checked=""/><div class="controls bullet"><span class="by">fnbr</span><span>|</span><a href="#38246741">parent</a><span>|</span><a href="#38247273">next</a><span>|</span><label class="collapse" for="c-38247054">[-]</label><label class="expand" for="c-38247054">[1 more]</label></div><br/><div class="children"><div class="content">Less expensive. Big discounts exist.</div><br/></div></div><div id="38247273" class="c"><input type="checkbox" id="c-38247273" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#38246741">parent</a><span>|</span><a href="#38247054">prev</a><span>|</span><a href="#38246390">next</a><span>|</span><label class="collapse" for="c-38247273">[-]</label><label class="expand" for="c-38247273">[1 more]</label></div><br/><div class="children"><div class="content">It takes $3k for Nvidia to produce H100[1], so the total for Nvidia will be $30m, not small but definitely not the most expensive supercomputer.<p>[1]: <a href="https:&#x2F;&#x2F;the-decoder.com&#x2F;nvidias-h100-gpu-sells-like-hot-cakes-with-high-profit-margins" rel="nofollow noreferrer">https:&#x2F;&#x2F;the-decoder.com&#x2F;nvidias-h100-gpu-sells-like-hot-cake...</a></div><br/></div></div></div></div><div id="38246390" class="c"><input type="checkbox" id="c-38246390" checked=""/><div class="controls bullet"><span class="by">shmerl</span><span>|</span><a href="#38246741">prev</a><span>|</span><a href="#38247052">next</a><span>|</span><label class="collapse" for="c-38246390">[-]</label><label class="expand" for="c-38246390">[4 more]</label></div><br/><div class="children"><div class="content">Why is AMD not compared?</div><br/><div id="38246428" class="c"><input type="checkbox" id="c-38246428" checked=""/><div class="controls bullet"><span class="by">ioedward</span><span>|</span><a href="#38246390">parent</a><span>|</span><a href="#38248024">next</a><span>|</span><label class="collapse" for="c-38246428">[-]</label><label class="expand" for="c-38246428">[2 more]</label></div><br/><div class="children"><div class="content">AMD didn&#x27;t submit results, despite geohot&#x2F;Lisa Su&#x27;s plan to get AMD on MLPerf: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;realGeorgeHotz&#x2F;status&#x2F;1669803464082489347" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;realGeorgeHotz&#x2F;status&#x2F;166980346408248934...</a></div><br/><div id="38247170" class="c"><input type="checkbox" id="c-38247170" checked=""/><div class="controls bullet"><span class="by">dhruvdh</span><span>|</span><a href="#38246390">root</a><span>|</span><a href="#38246428">parent</a><span>|</span><a href="#38248024">next</a><span>|</span><label class="collapse" for="c-38247170">[-]</label><label class="expand" for="c-38247170">[1 more]</label></div><br/><div class="children"><div class="content">If Dr. Lisa Su, the CEO, wants AMD to submit to MLPerf they would have already submitted to MLPref. This tweet is about a discussion George had with her on AMD&#x27;s open-source practices and code quality for the contributions they make.</div><br/></div></div></div></div><div id="38248024" class="c"><input type="checkbox" id="c-38248024" checked=""/><div class="controls bullet"><span class="by">zwaps</span><span>|</span><a href="#38246390">parent</a><span>|</span><a href="#38246428">prev</a><span>|</span><a href="#38247052">next</a><span>|</span><label class="collapse" for="c-38248024">[-]</label><label class="expand" for="c-38248024">[1 more]</label></div><br/><div class="children"><div class="content">AMD does not do ML. They don&#x27;t have the capability and engineers, nor desire in their leadership, to develop the software stack.</div><br/></div></div></div></div><div id="38247052" class="c"><input type="checkbox" id="c-38247052" checked=""/><div class="controls bullet"><span class="by">fnbr</span><span>|</span><a href="#38246390">prev</a><span>|</span><label class="collapse" for="c-38247052">[-]</label><label class="expand" for="c-38247052">[2 more]</label></div><br/><div class="children"><div class="content">This is dumb- they’re testing against google’s v5e chip, which isn’t the H100 equivalent (the V5P is). Of course Google came out worse here.</div><br/><div id="38247112" class="c"><input type="checkbox" id="c-38247112" checked=""/><div class="controls bullet"><span class="by">kaelinl</span><span>|</span><a href="#38247052">parent</a><span>|</span><label class="collapse" for="c-38247112">[-]</label><label class="expand" for="c-38247112">[1 more]</label></div><br/><div class="children"><div class="content">MLPerf does not test hardware. Companies submit results from their own hardware. This is what each company chose to submit.</div><br/></div></div></div></div></div></div></div></div></div></body></html>