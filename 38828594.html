<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1704099666227" as="style"/><link rel="stylesheet" href="styles.css?v=1704099666227"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://simonwillison.net/2023/Dec/31/ai-in-2023/">Stuff we figured out about AI in 2023</a> <span class="domain">(<a href="https://simonwillison.net">simonwillison.net</a>)</span></div><div class="subtext"><span>simonw</span> | <span>12 comments</span></div><br/><div><div id="38830373" class="c"><input type="checkbox" id="c-38830373" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#38830499">next</a><span>|</span><label class="collapse" for="c-38830373">[-]</label><label class="expand" for="c-38830373">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version!<p>actually its not just a basic version. Llama 1&#x2F;2&#x27;s model.py is 500 lines: <a href="https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;llama&#x2F;blob&#x2F;main&#x2F;llama&#x2F;model.py">https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;llama&#x2F;blob&#x2F;main&#x2F;llama&#x2F;mo...</a><p>Mistral (is rumored to have) forked llama and is 369 lines: <a href="https:&#x2F;&#x2F;github.com&#x2F;mistralai&#x2F;mistral-src&#x2F;blob&#x2F;main&#x2F;mistral&#x2F;model.py">https:&#x2F;&#x2F;github.com&#x2F;mistralai&#x2F;mistral-src&#x2F;blob&#x2F;main&#x2F;mistral&#x2F;m...</a><p>and both of these are SOTA open source models.</div><br/><div id="38830473" class="c"><input type="checkbox" id="c-38830473" checked=""/><div class="controls bullet"><span class="by">ganzuul</span><span>|</span><a href="#38830373">parent</a><span>|</span><a href="#38830499">next</a><span>|</span><label class="collapse" for="c-38830473">[-]</label><label class="expand" for="c-38830473">[1 more]</label></div><br/><div class="children"><div class="content">This is an interesting point of fact about what Boltzmann Brains could be.</div><br/></div></div></div></div><div id="38830499" class="c"><input type="checkbox" id="c-38830499" checked=""/><div class="controls bullet"><span class="by">justinl33</span><span>|</span><a href="#38830373">prev</a><span>|</span><a href="#38830365">next</a><span>|</span><label class="collapse" for="c-38830499">[-]</label><label class="expand" for="c-38830499">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them.</i><p>LLM&#x27;s aren&#x27;t black boxes, intelligence is. Not understanding anything about things which display emergent intelligence is not a new trend: first cells, then the human brain, and now LLM&#x27;s. To explicate the magic of how an LLM is able to maintain knowledge when would be analogous to understanding how the human brain synthesizes output. Yes - it is still something we should strive to understand in an explicit, programmatic way. But to de-black box the LLM would be to crack intelligence itself.</div><br/></div></div><div id="38830365" class="c"><input type="checkbox" id="c-38830365" checked=""/><div class="controls bullet"><span class="by">XenophileJKO</span><span>|</span><a href="#38830499">prev</a><span>|</span><a href="#38830198">next</a><span>|</span><label class="collapse" for="c-38830365">[-]</label><label class="expand" for="c-38830365">[3 more]</label></div><br/><div class="children"><div class="content">I think &quot;Gullability&quot; might not be as hard a problem as we think it is.<p>I suspecte the primary reason it is a problem now is likely because we haven&#x27;t really created a training method to tune the desired level of compliance. What I mean is we probably don&#x27;t have any counter-factual examples in the training sets. Where in an interaction we want the model to prefer it&#x27;s own knowledge.<p>Given the use with RAG (retrieval augmented generation) and many of the other current use-cases, the preference is for the model to &quot;trust&quot; the system message content more than the training data.<p>Training with some additional context is likely all that is needed.</div><br/><div id="38830388" class="c"><input type="checkbox" id="c-38830388" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38830365">parent</a><span>|</span><a href="#38830198">next</a><span>|</span><label class="collapse" for="c-38830388">[-]</label><label class="expand" for="c-38830388">[2 more]</label></div><br/><div class="children"><div class="content">If you can train a model that isn&#x27;t gullible - even when faced with multiple paragraphs of devious text trying to trick it - I&#x27;ll be very impressed!<p>And even then, there are gullibility problems that seem unsolvable to me.<p>If the Supreme Court make a decision that changes how copyright law is applied, and you then tell a model &quot;the Supreme Court just announced that...&quot; - should the model believe you?<p>If not, how should it &quot;fact check&quot; what you are telling it, especially if you maintain full control over its access to the outside world?</div><br/><div id="38830444" class="c"><input type="checkbox" id="c-38830444" checked=""/><div class="controls bullet"><span class="by">XenophileJKO</span><span>|</span><a href="#38830365">root</a><span>|</span><a href="#38830388">parent</a><span>|</span><a href="#38830198">next</a><span>|</span><label class="collapse" for="c-38830444">[-]</label><label class="expand" for="c-38830444">[1 more]</label></div><br/><div class="children"><div class="content">Funny you should say that. When web access was initally setup on the GPT-4 ChatGPT Pro accounts, I actually had the model fact-check things that were &quot;improbable&quot; based on what it knew from the training data.<p>It stopped doing that when they started fine-tuning how&#x2F;when the model should use the internet in the later model revisions.<p>(Edit to add detail..)<p>I found the interaction facinating. Basically what happened was I made a statement, but didn&#x27;t ask about it. I said something like this. &quot;Did you know {something improbable to the model} is true?&quot;<p>It immediately searched the internet to validate that is was true before it responded. When I asked it why it searched, it said that what I said wasn&#x27;t likely to be true, so it wanted to make sure it was true.</div><br/></div></div></div></div></div></div><div id="38830198" class="c"><input type="checkbox" id="c-38830198" checked=""/><div class="controls bullet"><span class="by">jbandela1</span><span>|</span><a href="#38830365">prev</a><span>|</span><a href="#38830332">next</a><span>|</span><label class="collapse" for="c-38830198">[-]</label><label class="expand" for="c-38830198">[4 more]</label></div><br/><div class="children"><div class="content">I think what is also interesting is what LLMs help us learn about ourselves.<p>I think we understand human intelligence a bit better because of LLMs, and I think people have been surprised how far you could get just by processing text.<p>I think (just a guess relying on intuition and my very shallow survey) that human intelligence is multilayered where we have something at least vaguely analogous to a LLM combined with something else that knows the physics of the world as well as something else that is able to do logic and symbolic processing.<p>But this is exciting to see the power of relatively simple neural networks when they are trained with a giant corpus.</div><br/><div id="38830528" class="c"><input type="checkbox" id="c-38830528" checked=""/><div class="controls bullet"><span class="by">growingkittens</span><span>|</span><a href="#38830198">parent</a><span>|</span><a href="#38830413">next</a><span>|</span><label class="collapse" for="c-38830528">[-]</label><label class="expand" for="c-38830528">[1 more]</label></div><br/><div class="children"><div class="content">I find something lacking from many AI discussions because my experience with having a brain is radically different from average. Mine is damaged by a childhood brain injury, which wasn&#x27;t diagnosed until I was 30.<p>Another thing about the injury is that my lower order thinking skills seem to be damaged, such as sequencing or anything based on working memory. However, my higher order thinking skills are intact - I think they&#x27;ve been compensating for the broken skills this whole time, albeit with much difficulty.<p>My thoughts on LLMs from a previous comment:<p>&gt;  I think in pictures; I remember information with emotions, music, movement, and basically anywhere my brain could stuff information. Words are ephemeral, I often forget the beginning of a sentence by the time I reach the end.<p>&gt; The human brain is <i>so much more</i> than language.<p>Also:<p>&gt; There are many specialists and not many generalists. Generalists are needed to tie together the multiple domains required to emulate the human brain.</div><br/></div></div><div id="38830413" class="c"><input type="checkbox" id="c-38830413" checked=""/><div class="controls bullet"><span class="by">MeImCounting</span><span>|</span><a href="#38830198">parent</a><span>|</span><a href="#38830528">prev</a><span>|</span><a href="#38830414">next</a><span>|</span><label class="collapse" for="c-38830413">[-]</label><label class="expand" for="c-38830413">[1 more]</label></div><br/><div class="children"><div class="content">I really believe this to be the case. Its remarkable what language alone can do but its not the be all end all of intelligence.</div><br/></div></div><div id="38830414" class="c"><input type="checkbox" id="c-38830414" checked=""/><div class="controls bullet"><span class="by">bormaj</span><span>|</span><a href="#38830198">parent</a><span>|</span><a href="#38830413">prev</a><span>|</span><a href="#38830332">next</a><span>|</span><label class="collapse" for="c-38830414">[-]</label><label class="expand" for="c-38830414">[1 more]</label></div><br/><div class="children"><div class="content">Did the prevalence of LLMs shed any light on what intelligence <i>is</i> vs what it <i>is not</i>?</div><br/></div></div></div></div><div id="38830332" class="c"><input type="checkbox" id="c-38830332" checked=""/><div class="controls bullet"><span class="by">globalnode</span><span>|</span><a href="#38830198">prev</a><span>|</span><label class="collapse" for="c-38830332">[-]</label><label class="expand" for="c-38830332">[1 more]</label></div><br/><div class="children"><div class="content">one thing a friend was suggesting to me was that he thinks the difference is that human intelligence is analogue while computing intelligence is digital. there are nuances in our thinking and language that current digital systems cannot compete with until the become fine grained enough to appear analogue.</div><br/></div></div></div></div></div></div></div></body></html>