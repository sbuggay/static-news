<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1719651672257" as="style"/><link rel="stylesheet" href="styles.css?v=1719651672257"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://twitter.com/AIatMeta/status/1806361623831171318/photo/1">Meta LLM Compiler: neural optimizer and disassembler</a>Â <span class="domain">(<a href="https://twitter.com">twitter.com</a>)</span></div><div class="subtext"><span>foobazgt</span> | <span>74 comments</span></div><br/><div><div id="40813452" class="c"><input type="checkbox" id="c-40813452" checked=""/><div class="controls bullet"><span class="by">HanClinto</span><span>|</span><a href="#40828640">next</a><span>|</span><label class="collapse" for="c-40813452">[-]</label><label class="expand" for="c-40813452">[30 more]</label></div><br/><div class="children"><div class="content">Huh. This is a very... &quot;interesting&quot; application for an LLM. I&#x27;m not the brightest crayon in the box, but if anyone else would like to follow along with my non-expert opinion as I read through the paper, here&#x27;s my take on it.<p>It&#x27;s pretty important for compilers &#x2F; decompilers to be reliable and accurate -- compilers behaving in a deterministic and predictable way is an important fundamental of pipelines.<p>LLMs are inherently unpredictable, and so using an LLM for compilation &#x2F; decompilation -- even an LLM that has 99.99% accuracy -- feels a bit odd to include as a piece in my build pipeline.<p>That said, let&#x27;s look at the paper and see what they did.<p>They essentially started with CodeLlama, and then went further to train the model on three tasks -- one primary, and two downstream.<p>The first task is compilation: given input code and a set of compiler flags, can we predict the output assembly? Given the inability to verify correctness without using a traditional compiler, this feels like it&#x27;s of limited use on its own. However, training a model on this as a primary task enables a couple of downstream tasks. Namely:<p>The second task (and first downstream task) is compiler flag prediction &#x2F; optimization to predict &#x2F; optimize for smaller assembly sizes. It&#x27;s a bit disappointing that they only seem to be able to optimize for assembly size (and not execution speed), but it&#x27;s not without its uses. Because the output of this task (compiler flags) are then passed to a deterministic function (a traditional compiler), then the instability of the LLM is mitigated.<p>The third task (second downstream task) is decompilation. This is not the first time that LLMs have been trained to do better decompilation -- however, because of the pretraining that they did on the primary task, they feel that this provides some advantages over previous approaches. Sadly, they only compare LLM Compiler to Code Llama and GPT-4 Turbo, and not against any other LLMs fine-tuned for the decompilation task, so it&#x27;s difficult to see in context how much better their approach is.<p>Regarding the verifiability of the disassembly approach, the authors note that there are issues regarding correctness. So the authors employ round-tripping -- recompiling the decompiled code (using the same compiler flags) to verify correctness &#x2F; exact-match. This still puts accuracy in the 45% or so (if I understand their output numbers), so it&#x27;s not entirely trustworthy yet, but it might be able to still be useful (especially if used alongside a traditional decompiler, and this model&#x27;s outputs only used when they are verifiably correct).<p>Overall I&#x27;m happy to see this model be released as it seems like an interesting use-case. I may need to read more, but at first blush I&#x27;m not immediately excited by the possibilities that this unlocks. Most of all, I would like to see it explored if these methods could be extended to optimize for performance -- not just size of assembly.</div><br/><div id="40828762" class="c"><input type="checkbox" id="c-40828762" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#40813452">parent</a><span>|</span><a href="#40822771">next</a><span>|</span><label class="collapse" for="c-40828762">[-]</label><label class="expand" for="c-40828762">[1 more]</label></div><br/><div class="children"><div class="content">&gt;compilers behaving in a deterministic and predictable way is an important fundamental of pipelines.
LLMs are inherently unpredictable, and so using an LLM for compilation &#x2F; decompilation -- even an LLM that has 99.99% accuracy<p>You&#x27;re confusing different concepts here. An llm is technically not unpredictable by itself (at least the ones we are talking about here, there are different problems with beasts like GPT4 [1]). The &quot;randomness&quot; of llms you are probably experiencing stems from the autoregressive completion, which samples from probabilities for a temperature T&gt;0 (which is very common because it makes sense in chat applications). But there is nothing that prevents you from simply choosing greedy sampling, which would make your output 100% deterministic and reproducible. That is particularly useful for disassembling&#x2F;decompiling and has the chance to vastly improve over existing tools, because it is common knowledge that they are often not the sharpest tools and humans are much better at piecing together working code.<p>The other question here is accuracy for compiling. For that it is important whether the llm can follow a specification correctly. Because once you write unspecified behaviour, your code is fair game for other compilers as well. So the real question is how well does it follow the spec how good is it at dealing with situations where normal compilers will flounder.<p>[1] <a href="https:&#x2F;&#x2F;152334h.github.io&#x2F;blog&#x2F;non-determinism-in-gpt-4&#x2F;" rel="nofollow">https:&#x2F;&#x2F;152334h.github.io&#x2F;blog&#x2F;non-determinism-in-gpt-4&#x2F;</a></div><br/></div></div><div id="40822771" class="c"><input type="checkbox" id="c-40822771" checked=""/><div class="controls bullet"><span class="by">riedel</span><span>|</span><a href="#40813452">parent</a><span>|</span><a href="#40828762">prev</a><span>|</span><a href="#40825074">next</a><span>|</span><label class="collapse" for="c-40822771">[-]</label><label class="expand" for="c-40822771">[24 more]</label></div><br/><div class="children"><div class="content">It is normally not a necessary feature of a compiler to be determistic. A compiler should be correct against a specification. If the specification allows indeterminism a compiler should be able to exploit them. I remember the story of the sather-k compiler that did things differently based on the phase of the moon.</div><br/><div id="40823479" class="c"><input type="checkbox" id="c-40823479" checked=""/><div class="controls bullet"><span class="by">munificent</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40822771">parent</a><span>|</span><a href="#40825090">next</a><span>|</span><label class="collapse" for="c-40823479">[-]</label><label class="expand" for="c-40823479">[15 more]</label></div><br/><div class="children"><div class="content">It&#x27;s technically correct that a language specification is rarely precise enough to require compiler output to be deterministic.<p>But it&#x27;s pragmatically true that engineers will want to murder you if your compiler is non-deterministic. All sorts of build systems, benchmark harnesses, supply chain validation tools, and other bits of surrounding ecosystem will shit the bed if the compiler doesn&#x27;t produce bitwise identical output on the same input and compiler flags.</div><br/><div id="40824059" class="c"><input type="checkbox" id="c-40824059" checked=""/><div class="controls bullet"><span class="by">foobazgt</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40823479">parent</a><span>|</span><a href="#40826377">next</a><span>|</span><label class="collapse" for="c-40824059">[-]</label><label class="expand" for="c-40824059">[9 more]</label></div><br/><div class="children"><div class="content">Can vouch for this having fixed non-determinism bugs in a compiler. Nobody is happy if your builds aren&#x27;t reproducible. You&#x27;ll also suffer crazy performance problems as everything downstream rebuilds randomly and all your build caches randomly miss.</div><br/><div id="40824580" class="c"><input type="checkbox" id="c-40824580" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40824059">parent</a><span>|</span><a href="#40826545">next</a><span>|</span><label class="collapse" for="c-40824580">[-]</label><label class="expand" for="c-40824580">[7 more]</label></div><br/><div class="children"><div class="content">NixOS with its nixpkgs [0] and cache [1] would also not work if compilers weren&#x27;t reproducible. Though they won&#x27;t use something like PGO or some specific optimization flags as these would very likely lead to unreproducible builds. For example most distros ship a PGO optimized build of Python while nixos does not.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;nixos&#x2F;nixpkgs">https:&#x2F;&#x2F;github.com&#x2F;nixos&#x2F;nixpkgs</a><p>[1] <a href="https:&#x2F;&#x2F;cache.nixos.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cache.nixos.org&#x2F;</a></div><br/><div id="40825015" class="c"><input type="checkbox" id="c-40825015" checked=""/><div class="controls bullet"><span class="by">boomanaiden154</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40824580">parent</a><span>|</span><a href="#40827017">next</a><span>|</span><label class="collapse" for="c-40825015">[-]</label><label class="expand" for="c-40825015">[5 more]</label></div><br/><div class="children"><div class="content">PGO can be used in such situations, but the profile needs to be checked in. Same code + same profile -&gt; same binary (assuming the compiler is deterministic, which is tested quite extensively).<p>There are several big projects that use PGO (like Chrome), and you can get a deterministic build at whatever revision using PGO as the profiles are checked in to the repository.</div><br/><div id="40826267" class="c"><input type="checkbox" id="c-40826267" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40825015">parent</a><span>|</span><a href="#40827017">next</a><span>|</span><label class="collapse" for="c-40826267">[-]</label><label class="expand" for="c-40826267">[4 more]</label></div><br/><div class="children"><div class="content">Itâs called autofdo although Iâve struggled to get it working well in Rust.</div><br/><div id="40826417" class="c"><input type="checkbox" id="c-40826417" checked=""/><div class="controls bullet"><span class="by">boomanaiden154</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40826267">parent</a><span>|</span><a href="#40827017">next</a><span>|</span><label class="collapse" for="c-40826417">[-]</label><label class="expand" for="c-40826417">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not called AutoFDO. AutoFDO refers to a specific sampling-based profile technique out of Google (<a href="https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;abs&#x2F;10.1145&#x2F;2854038.2854044" rel="nofollow">https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;abs&#x2F;10.1145&#x2F;2854038.2854044</a>). Sometimes people will refer to that as PGO though (with PGO and FDO being somewhat synonymous, but PGO seeming to be the preferred term in the open source LLVM world). Chrome specifically uses instrumented PGO which is very much not AutoFDO.<p>PGO works just fine in Rust and has support built into the compiler (<a href="https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;rustc&#x2F;profile-guided-optimization.html" rel="nofollow">https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;rustc&#x2F;profile-guided-optimization....</a>).</div><br/><div id="40826942" class="c"><input type="checkbox" id="c-40826942" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40826417">parent</a><span>|</span><a href="#40827017">next</a><span>|</span><label class="collapse" for="c-40826942">[-]</label><label class="expand" for="c-40826942">[2 more]</label></div><br/><div class="children"><div class="content">I wasnât trying to conflate the two. PGO traditionally meant a trace build but as a term itâs pretty generic, at least to me to the general concept of âyou have profile information that replaces generically tuned heuristics that the compiler uses). AutoFDO Iâd classify as an extension to that concept to a more general PGO technique; kind of ThinLTO vs LTO. Specifically, it generates the âsameâ information to supplant compiler heuristics, but is more flexible in that the sample can be fed back into âarbitraryâ versions of the code using normal sampling techniques instead of an instrumented trace. The reason sampling is better is that it more easily fits into capturing data from production which is much harder to accomplish for the tracing variant (due to perf overheads). Additionally, because it works across versions the amortized compile cost drops from 2x to 1x because you only need to reseed your profile data periodically.<p>I was under the impression they had switched to AutoFDO across the board but maybe thatâs just for their cloud stuff and Chrome continues to run a representative workload since that path is more mature. I would guess that if itâs not being used already, theyâre exploring how to make Chrome run AutoFDO for the same reason everyone started using ThinLTO - it brought most of the advantages while fixing the disadvantages that hampered adoption.<p>And yes, while PGO is available natively, AutoFDO isnât quite as smooth.</div><br/><div id="40827257" class="c"><input type="checkbox" id="c-40827257" checked=""/><div class="controls bullet"><span class="by">boomanaiden154</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40826942">parent</a><span>|</span><a href="#40827017">next</a><span>|</span><label class="collapse" for="c-40827257">[-]</label><label class="expand" for="c-40827257">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure where you&#x27;re getting your information from.<p>Chrome (and many other performance-critical workloads) is using instrumented PGO because it gives better performance gains, not because it&#x27;s a more mature path. AutoFDO is only used in situations where collecting data with an instrumented build is difficult.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40827017" class="c"><input type="checkbox" id="c-40827017" checked=""/><div class="controls bullet"><span class="by">c0balt</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40824580">parent</a><span>|</span><a href="#40825015">prev</a><span>|</span><a href="#40826545">next</a><span>|</span><label class="collapse" for="c-40827017">[-]</label><label class="expand" for="c-40827017">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, and nixpkgs also, last time I checked, does patch GCC&#x2F; clang to ensure determinism. Many compilers and toolchain by default want to, e.g., embed build information that may leak from the build env in a non-deterministic&#x2F; non-reprodicible manner.</div><br/></div></div></div></div><div id="40826545" class="c"><input type="checkbox" id="c-40826545" checked=""/><div class="controls bullet"><span class="by">munificent</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40824059">parent</a><span>|</span><a href="#40824580">prev</a><span>|</span><a href="#40826377">next</a><span>|</span><label class="collapse" for="c-40826545">[-]</label><label class="expand" for="c-40826545">[1 more]</label></div><br/><div class="children"><div class="content">Yup. Even so much as inserting the build timestamp into the generated executable  (which is strangely common) causes havoc with build caching.</div><br/></div></div></div></div><div id="40826377" class="c"><input type="checkbox" id="c-40826377" checked=""/><div class="controls bullet"><span class="by">Sophira</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40823479">parent</a><span>|</span><a href="#40824059">prev</a><span>|</span><a href="#40826379">next</a><span>|</span><label class="collapse" for="c-40826377">[-]</label><label class="expand" for="c-40826377">[1 more]</label></div><br/><div class="children"><div class="content">Plus, these models are entirely black boxes. Even given weights, we don&#x27;t know how to look at them and meaningfully tell what&#x27;s happening - and not only that, but training these models is likely not cheap at all.<p>Stable output is how we can verify that attacks like the one described in Reflections on Trusting Trust[0] don&#x27;t happen.<p>[0] <a href="https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~rdriley&#x2F;487&#x2F;papers&#x2F;Thompson_1984_ReflectionsonTrustingTrust.pdf" rel="nofollow">https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~rdriley&#x2F;487&#x2F;papers&#x2F;Thompson_1984_Ref...</a></div><br/></div></div><div id="40826379" class="c"><input type="checkbox" id="c-40826379" checked=""/><div class="controls bullet"><span class="by">a_t48</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40823479">parent</a><span>|</span><a href="#40826377">prev</a><span>|</span><a href="#40827904">next</a><span>|</span><label class="collapse" for="c-40826379">[-]</label><label class="expand" for="c-40826379">[2 more]</label></div><br/><div class="children"><div class="content">NVCC CUDA builds were nondeterministic last time I checked, it made certain things (trying to get very clever with generating patches) difficult. This was also hampered by certain libraries (maybe GTSAM?) wanting to write __DATE__ somewhere in the build output, creating endlessly changing builds.</div><br/><div id="40828712" class="c"><input type="checkbox" id="c-40828712" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40826379">parent</a><span>|</span><a href="#40827904">next</a><span>|</span><label class="collapse" for="c-40828712">[-]</label><label class="expand" for="c-40828712">[1 more]</label></div><br/><div class="children"><div class="content">In parallel computing you run into nondeterminism pretty quickly anyways - especially with CUDA because of undetermined execution order and floating point accuracy.</div><br/></div></div></div></div><div id="40827904" class="c"><input type="checkbox" id="c-40827904" checked=""/><div class="controls bullet"><span class="by">azinman2</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40823479">parent</a><span>|</span><a href="#40826379">prev</a><span>|</span><a href="#40827595">next</a><span>|</span><label class="collapse" for="c-40827904">[-]</label><label class="expand" for="c-40827904">[1 more]</label></div><br/><div class="children"><div class="content">Just fix the random seed :)</div><br/></div></div><div id="40827595" class="c"><input type="checkbox" id="c-40827595" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40823479">parent</a><span>|</span><a href="#40827904">prev</a><span>|</span><a href="#40825090">next</a><span>|</span><label class="collapse" for="c-40827595">[-]</label><label class="expand" for="c-40827595">[1 more]</label></div><br/><div class="children"><div class="content">LLMs can be deterministic if you set the random seed and pin it to a certain version of the weights.<p>My bigger concern would be bugs in the machine code would be very, very difficult to track down.</div><br/></div></div></div></div><div id="40825090" class="c"><input type="checkbox" id="c-40825090" checked=""/><div class="controls bullet"><span class="by">pton_xd</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40822771">parent</a><span>|</span><a href="#40823479">prev</a><span>|</span><a href="#40826154">next</a><span>|</span><label class="collapse" for="c-40825090">[-]</label><label class="expand" for="c-40825090">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It is normally not a necessary feature of a compiler to be determistic. A compiler should be correct against a specification.<p>That sounds like a nightmare. Optimizing code to play nice with black-box heuristic compilers like V8&#x27;s TurboFan is, already in fact, a continual maintenance nightmare.<p>If you don&#x27;t care about performance, non-deterministic compilation is probably &quot;good enough.&quot; See TurboFan.</div><br/></div></div><div id="40826154" class="c"><input type="checkbox" id="c-40826154" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40822771">parent</a><span>|</span><a href="#40825090">prev</a><span>|</span><a href="#40826985">next</a><span>|</span><label class="collapse" for="c-40826154">[-]</label><label class="expand" for="c-40826154">[6 more]</label></div><br/><div class="children"><div class="content">LLMs are deterministic. We inject randomness after the fact, just because we don&#x27;t like our text being deterministic. Turn temperature to 0 and you&#x27;re good.</div><br/><div id="40826234" class="c"><input type="checkbox" id="c-40826234" checked=""/><div class="controls bullet"><span class="by">waldrews</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40826154">parent</a><span>|</span><a href="#40826400">next</a><span>|</span><label class="collapse" for="c-40826234">[-]</label><label class="expand" for="c-40826234">[4 more]</label></div><br/><div class="children"><div class="content">But temperature 0 LLM&#x27;s don&#x27;t exhibit the emergent phenomena we like, even in apparently non-creative tasks.  The randomness is, in some sense, a cheap proxy for an infeasible search over all completion sequences, much like simulated annealing with zero temperature is a search for a local optimum but adding randomness makes it explore globally and find more interesting possibilities.</div><br/><div id="40826361" class="c"><input type="checkbox" id="c-40826361" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40826234">parent</a><span>|</span><a href="#40826400">next</a><span>|</span><label class="collapse" for="c-40826361">[-]</label><label class="expand" for="c-40826361">[3 more]</label></div><br/><div class="children"><div class="content">Sure but you could add pseudo random noise instead and get the same behavior while retaining determinism.</div><br/><div id="40826695" class="c"><input type="checkbox" id="c-40826695" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40826361">parent</a><span>|</span><a href="#40826400">next</a><span>|</span><label class="collapse" for="c-40826695">[-]</label><label class="expand" for="c-40826695">[2 more]</label></div><br/><div class="children"><div class="content">Temperature is at ~1.2 in this thread, here&#x27;s some 0.0:<p>- Yes, temperature 0.0 is less creative.<p>- Injecting pseudo-random noise to get deterministic creative outputs is &quot;not even wrong&quot;, in the Wolfgang Pauli sense. It&#x27;s fixing something that isn&#x27;t broken, with something that can&#x27;t fix it, that if it could, would be replicating the original behavior - more simply, it&#x27;s proposing non-deterministic determinism.<p>- Temperature 0.0, in practice, is an LLM. There aren&#x27;t emergent phenomena, in the sense &quot;emergent phenomena&quot; is used with LLMs, missing. Many, many, many, applications use this.<p>- In simplistic scenarios, on very small models, 0.0 could get stuck literally repeating the same token.<p>- There&#x27;s a whole other layer of ex. repeat penalties&#x2F;frequency penalties and such that are used during inference to limit this. Only OpenAI and llama.cpp expose repeat&#x2F;frequency.<p>- Temperature 0.0 is still non-deterministic on ex. OpenAI, though <i>substantially</i> the same, and even <i>the</i> same most of the time. It&#x27;s hard to notice differences. (Reproducible builds require extra engineering effort, the same way ensuring temperature = 0.0 is <i>truly</i> deterministic requires engineering effort.)<p>- Pedantically, only temperature 0.0 <i>at the same seed</i> (initial state) is deterministic.</div><br/></div></div></div></div></div></div><div id="40826400" class="c"><input type="checkbox" id="c-40826400" checked=""/><div class="controls bullet"><span class="by">Sophira</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40826154">parent</a><span>|</span><a href="#40826234">prev</a><span>|</span><a href="#40826985">next</a><span>|</span><label class="collapse" for="c-40826400">[-]</label><label class="expand" for="c-40826400">[1 more]</label></div><br/><div class="children"><div class="content">Even then, though, the output could change drastically based on a single change to the input (such as a comment).<p>That&#x27;s not something you want in a compiler.</div><br/></div></div></div></div><div id="40826985" class="c"><input type="checkbox" id="c-40826985" checked=""/><div class="controls bullet"><span class="by">wakawaka28</span><span>|</span><a href="#40813452">root</a><span>|</span><a href="#40822771">parent</a><span>|</span><a href="#40826154">prev</a><span>|</span><a href="#40825074">next</a><span>|</span><label class="collapse" for="c-40826985">[-]</label><label class="expand" for="c-40826985">[1 more]</label></div><br/><div class="children"><div class="content">It is very important for a compiler to be deterministic. Otherwise you can&#x27;t validate the integrity of binaries! We already have issues with reproducibility without adding this shit in the mix.</div><br/></div></div></div></div><div id="40825074" class="c"><input type="checkbox" id="c-40825074" checked=""/><div class="controls bullet"><span class="by">boomanaiden154</span><span>|</span><a href="#40813452">parent</a><span>|</span><a href="#40822771">prev</a><span>|</span><a href="#40824236">next</a><span>|</span><label class="collapse" for="c-40825074">[-]</label><label class="expand" for="c-40825074">[1 more]</label></div><br/><div class="children"><div class="content">Sure, performance is more interesting, but it&#x27;s significantly harder.<p>With code size, you just need to run the code through the compiler and you have a deterministic measurement for evaluation.<p>Performance has no such metric. Benchmarks are expensive and noisy. Cost models seem like a promising direction, but they aren&#x27;t really there yet.</div><br/></div></div><div id="40824236" class="c"><input type="checkbox" id="c-40824236" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#40813452">parent</a><span>|</span><a href="#40825074">prev</a><span>|</span><a href="#40826680">next</a><span>|</span><label class="collapse" for="c-40824236">[-]</label><label class="expand" for="c-40824236">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for the summary. My memory of SOTA on disassembly about a year ago was subâ30% accuracy, so this is a significant step forward.<p>I do think the idea of a 90%+-ish forward and backward assembler LLM is pretty intriguing. Thereâs bound to be a lot of uses for it; especially if youâre of the mind that to get there it would have to have learned a lot about computers in the foundation model training phase.<p>Like, youâd definitely want to have those weights somehow baked into a typical coding assistant LLM, and of course youâd be able to automate round one of a lot of historical archiving projects that would like to get compilable modern code but only have a binary, youâd be able to turn some PDP-1 code into something that would compile on a modern machine, â¦ youâd probably be able to leverage it into building chip simulations &#x2F; code easily, it would be really useful for writing Verilog, (maybe), anyway, the use cases seem pretty broad to me.</div><br/></div></div><div id="40826680" class="c"><input type="checkbox" id="c-40826680" checked=""/><div class="controls bullet"><span class="by">joshuanapoli</span><span>|</span><a href="#40813452">parent</a><span>|</span><a href="#40824236">prev</a><span>|</span><a href="#40826682">next</a><span>|</span><label class="collapse" for="c-40826680">[-]</label><label class="expand" for="c-40826680">[1 more]</label></div><br/><div class="children"><div class="content">Maybe they are thinking about embedding a program generator and execution environment into their LLM inferencing loop in a tighter way. The model invents a program that guides the output in a specific&#x2F;algorithmic way, tailored to the prompt.</div><br/></div></div><div id="40826682" class="c"><input type="checkbox" id="c-40826682" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#40813452">parent</a><span>|</span><a href="#40826680">prev</a><span>|</span><a href="#40828640">next</a><span>|</span><label class="collapse" for="c-40826682">[-]</label><label class="expand" for="c-40826682">[1 more]</label></div><br/><div class="children"><div class="content">some comments like this make me waant to subscribe to you for all your future comments. thanks for doing the hard work of summarizing and taking the bold step of sharing your thoughts in public. i wish more HNers were like you.</div><br/></div></div></div></div><div id="40828640" class="c"><input type="checkbox" id="c-40828640" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#40813452">prev</a><span>|</span><a href="#40819775">next</a><span>|</span><label class="collapse" for="c-40828640">[-]</label><label class="expand" for="c-40828640">[2 more]</label></div><br/><div class="children"><div class="content">I donât understand the purpose of this. Feels like a task for function calling and sending it to an actual compiler.<p>Is there an obvious use case Iâm missing?</div><br/><div id="40828736" class="c"><input type="checkbox" id="c-40828736" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#40828640">parent</a><span>|</span><a href="#40819775">next</a><span>|</span><label class="collapse" for="c-40828736">[-]</label><label class="expand" for="c-40828736">[1 more]</label></div><br/><div class="children"><div class="content">GPT 6 can write software directly (as assembly) instead of writing c first.<p>Lots of training data for binary, and it can train itself by seeing if the program does what it expects it to do.</div><br/></div></div></div></div><div id="40819775" class="c"><input type="checkbox" id="c-40819775" checked=""/><div class="controls bullet"><span class="by">mmphosis</span><span>|</span><a href="#40828640">prev</a><span>|</span><a href="#40827607">next</a><span>|</span><label class="collapse" for="c-40819775">[-]</label><label class="expand" for="c-40819775">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;xcancel.com&#x2F;AIatMeta&#x2F;status&#x2F;1806361623831171318" rel="nofollow">https:&#x2F;&#x2F;xcancel.com&#x2F;AIatMeta&#x2F;status&#x2F;1806361623831171318</a></div><br/></div></div><div id="40827607" class="c"><input type="checkbox" id="c-40827607" checked=""/><div class="controls bullet"><span class="by">nothrowaways</span><span>|</span><a href="#40819775">prev</a><span>|</span><a href="#40823658">next</a><span>|</span><label class="collapse" for="c-40827607">[-]</label><label class="expand" for="c-40827607">[2 more]</label></div><br/><div class="children"><div class="content">It is so funny that meta has to post it on X.</div><br/><div id="40827726" class="c"><input type="checkbox" id="c-40827726" checked=""/><div class="controls bullet"><span class="by">rising-sky</span><span>|</span><a href="#40827607">parent</a><span>|</span><a href="#40823658">next</a><span>|</span><label class="collapse" for="c-40827726">[-]</label><label class="expand" for="c-40827726">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.threads.net&#x2F;@aiatmeta&#x2F;post&#x2F;C8ubaKupPwC" rel="nofollow">https:&#x2F;&#x2F;www.threads.net&#x2F;@aiatmeta&#x2F;post&#x2F;C8ubaKupPwC</a></div><br/></div></div></div></div><div id="40823658" class="c"><input type="checkbox" id="c-40823658" checked=""/><div class="controls bullet"><span class="by">zitterbewegung</span><span>|</span><a href="#40827607">prev</a><span>|</span><a href="#40823382">next</a><span>|</span><label class="collapse" for="c-40823658">[-]</label><label class="expand" for="c-40823658">[1 more]</label></div><br/><div class="children"><div class="content">Some previous work in the space is at <a href="https:&#x2F;&#x2F;github.com&#x2F;albertan017&#x2F;LLM4Decompile">https:&#x2F;&#x2F;github.com&#x2F;albertan017&#x2F;LLM4Decompile</a></div><br/></div></div><div id="40823382" class="c"><input type="checkbox" id="c-40823382" checked=""/><div class="controls bullet"><span class="by">soist</span><span>|</span><a href="#40823658">prev</a><span>|</span><a href="#40822851">next</a><span>|</span><label class="collapse" for="c-40823382">[-]</label><label class="expand" for="c-40823382">[1 more]</label></div><br/><div class="children"><div class="content">How do they verify the output preserves semantics of the input?</div><br/></div></div><div id="40822851" class="c"><input type="checkbox" id="c-40822851" checked=""/><div class="controls bullet"><span class="by">chad1n</span><span>|</span><a href="#40823382">prev</a><span>|</span><a href="#40827893">next</a><span>|</span><label class="collapse" for="c-40822851">[-]</label><label class="expand" for="c-40822851">[27 more]</label></div><br/><div class="children"><div class="content">As usual, Twitter is impressed by this, but I&#x27;m very skeptical, the chance of it breaking your program is pretty high. The thing that makes optimizations so hard to make is that they have to match the behavior without optimizations (unless you have UBs), which is something that LLMs probably will struggle with since they can&#x27;t exactly understand the code and execution tree.</div><br/><div id="40824813" class="c"><input type="checkbox" id="c-40824813" checked=""/><div class="controls bullet"><span class="by">cec</span><span>|</span><a href="#40822851">parent</a><span>|</span><a href="#40824705">next</a><span>|</span><label class="collapse" for="c-40824813">[-]</label><label class="expand" for="c-40824813">[7 more]</label></div><br/><div class="children"><div class="content">Hey! The idea isn&#x27;t to replace the compiler with an LLM, the tech is not there yet. Where we see value is in using these models to guide an existing compiler. E.g. orchestrating optimization passes. That way the LLM won&#x27;t break your code, nor will the compiler (to the extent that your compiler is free from bugs, which can tricky to detect - cf Sec 3.1 of our paper).</div><br/><div id="40825317" class="c"><input type="checkbox" id="c-40825317" checked=""/><div class="controls bullet"><span class="by">verditelabs</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40824813">parent</a><span>|</span><a href="#40826694">next</a><span>|</span><label class="collapse" for="c-40825317">[-]</label><label class="expand" for="c-40825317">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve done some similar LLM compiler work, obviously not on Meta&#x27;s scale, teaching an LLM to do optimization by feeding an encoder&#x2F;decoder pairs of -O0 and -O3 code and even on my small scale I managed to get the LLM to spit out the correct optimization every once and a while.<p>I think there&#x27;s a lot of value in LLM compilers to specifically be used for superoptimization where you can generate many possible optimizations, verify the correctness, and pick the most optimal one. I&#x27;m excited to see where y&#x27;all go with this.</div><br/><div id="40825442" class="c"><input type="checkbox" id="c-40825442" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40825317">parent</a><span>|</span><a href="#40826694">next</a><span>|</span><label class="collapse" for="c-40825442">[-]</label><label class="expand" for="c-40825442">[2 more]</label></div><br/><div class="children"><div class="content">Thank you for freeing me from one of my to-do projects. I wanted to do a similar autoencoder with optimisations. Did you write about it anywhere? I&#x27;d love to read the details.</div><br/><div id="40825516" class="c"><input type="checkbox" id="c-40825516" checked=""/><div class="controls bullet"><span class="by">verditelabs</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40825442">parent</a><span>|</span><a href="#40826694">next</a><span>|</span><label class="collapse" for="c-40825516">[-]</label><label class="expand" for="c-40825516">[1 more]</label></div><br/><div class="children"><div class="content">No writeup, but the code is here:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;SuperOptimizer&#x2F;supercompiler">https:&#x2F;&#x2F;github.com&#x2F;SuperOptimizer&#x2F;supercompiler</a><p>There&#x27;s code there to generate unoptimized &#x2F; optimized pairs via C generators like yarpgen and csmith, then compile, train, inference, and disassemble the results</div><br/></div></div></div></div></div></div><div id="40826694" class="c"><input type="checkbox" id="c-40826694" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40824813">parent</a><span>|</span><a href="#40825317">prev</a><span>|</span><a href="#40825744">next</a><span>|</span><label class="collapse" for="c-40826694">[-]</label><label class="expand" for="c-40826694">[2 more]</label></div><br/><div class="children"><div class="content">then maybe dont name it &quot;LLM Compiler&quot;, just &quot;Compiler Guidance with LLMs&quot; or &quot;LLM-aided Compiler optimization&quot; or something - will get much more to the point without overpromising</div><br/><div id="40827714" class="c"><input type="checkbox" id="c-40827714" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40826694">parent</a><span>|</span><a href="#40825744">next</a><span>|</span><label class="collapse" for="c-40827714">[-]</label><label class="expand" for="c-40827714">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, the name was misleading. I thought it was going to be source to object translation maybe with techniques like how they translate foreign languages.</div><br/></div></div></div></div><div id="40825744" class="c"><input type="checkbox" id="c-40825744" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40824813">parent</a><span>|</span><a href="#40826694">prev</a><span>|</span><a href="#40824705">next</a><span>|</span><label class="collapse" for="c-40825744">[-]</label><label class="expand" for="c-40825744">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The idea isn&#x27;t to replace the compiler with an LLM, the tech is not there yet<p>What do you mean the tech isn&#x27;t there <i>yet</i>, why would it ever even go into that direction? I mean we do those kinds of things for shits and giggles but for any practical use? I mean come on. From fast and reliable to glacial and not even working a quarter of the time.<p>I guess maybe if all compiler designers die in a freak accident and there&#x27;s literally nobody to replace them, then we&#x27;ll have to resort to that after the existing versions break.</div><br/></div></div></div></div><div id="40824705" class="c"><input type="checkbox" id="c-40824705" checked=""/><div class="controls bullet"><span class="by">Lockal</span><span>|</span><a href="#40822851">parent</a><span>|</span><a href="#40824813">prev</a><span>|</span><a href="#40828648">next</a><span>|</span><label class="collapse" for="c-40824705">[-]</label><label class="expand" for="c-40824705">[9 more]</label></div><br/><div class="children"><div class="content">As this LLM operates on LLVM intermediate representation language, the result can be fed into <a href="https:&#x2F;&#x2F;alive2.llvm.org&#x2F;ce&#x2F;" rel="nofollow">https:&#x2F;&#x2F;alive2.llvm.org&#x2F;ce&#x2F;</a> and formally verified. For those who don&#x27;t know what to print there: here is an example of C++ spaceship operator: <a href="https:&#x2F;&#x2F;alive2.llvm.org&#x2F;ce&#x2F;z&#x2F;YJPr84" rel="nofollow">https:&#x2F;&#x2F;alive2.llvm.org&#x2F;ce&#x2F;z&#x2F;YJPr84</a> (try to replace -1 with -2 there to break). This is kind of a Swiss knife for LLVM developers, they often start optimizations with this tool.<p>What they missed is to mention verification (they probably don&#x27;t know about alive2) and comparison with other compilers. It is very likely that LLM Compiler &quot;learned&quot; from GCC and with huge computational effort simply generates what GCC can do out of the box.</div><br/><div id="40825046" class="c"><input type="checkbox" id="c-40825046" checked=""/><div class="controls bullet"><span class="by">boomanaiden154</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40824705">parent</a><span>|</span><a href="#40825182">next</a><span>|</span><label class="collapse" for="c-40825046">[-]</label><label class="expand" for="c-40825046">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m reasonably certain the authors are aware of alive2.<p>The problem with using alive2 to verify LLM based compilation is that alive2 isn&#x27;t really designed for that. It&#x27;s an amazing tool for catching correctness issues in LLVM, but it&#x27;s expensive to run and will time out reasonably often, especially on cases involving floating point. It&#x27;s explicitly designed to minimize the rate of false-positive correctness issues to serve the primary purpose of alerting compiler developers to correctness issues that need to be fixed.</div><br/><div id="40825719" class="c"><input type="checkbox" id="c-40825719" checked=""/><div class="controls bullet"><span class="by">hughleat</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40825046">parent</a><span>|</span><a href="#40825182">next</a><span>|</span><label class="collapse" for="c-40825719">[-]</label><label class="expand" for="c-40825719">[1 more]</label></div><br/><div class="children"><div class="content">Yep, we tried it :-) These were exactly the problems we had with it.</div><br/></div></div></div></div><div id="40825182" class="c"><input type="checkbox" id="c-40825182" checked=""/><div class="controls bullet"><span class="by">boomanaiden154</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40824705">parent</a><span>|</span><a href="#40825046">prev</a><span>|</span><a href="#40825887">next</a><span>|</span><label class="collapse" for="c-40825182">[-]</label><label class="expand" for="c-40825182">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure it&#x27;s likely that the LLM here learned from gcc. The size optimization work here is focused on learning phase orderings for LLVM passes&#x2F;the LLVM pipeline, which wouldn&#x27;t be at all applicable to gcc.<p>Additionally, they train approximately half on assembly and half on LLVM-IR. They don&#x27;t talk much about how they generate the dataset other than that they generated it from the CodeLlama dataset, but I would guess they compile as much code as they can into LLVM-IR and then just lower that into assembly, leaving gcc out of the loop completely for the vast majority of the compiler specific training.</div><br/><div id="40825731" class="c"><input type="checkbox" id="c-40825731" checked=""/><div class="controls bullet"><span class="by">hughleat</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40825182">parent</a><span>|</span><a href="#40825887">next</a><span>|</span><label class="collapse" for="c-40825731">[-]</label><label class="expand" for="c-40825731">[2 more]</label></div><br/><div class="children"><div class="content">Yep! No GCC on this one.
And yep, that&#x27;s not far off how the pretraining data was gathered - but with random optimisations to give it a bit of variety.</div><br/><div id="40826071" class="c"><input type="checkbox" id="c-40826071" checked=""/><div class="controls bullet"><span class="by">boomanaiden154</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40825731">parent</a><span>|</span><a href="#40825887">next</a><span>|</span><label class="collapse" for="c-40826071">[-]</label><label class="expand" for="c-40826071">[1 more]</label></div><br/><div class="children"><div class="content">Do you have more information on how the dataset was constructed?<p>It seems like somehow build systems were invoked given the different targets present in the final version?<p>Was it mostly C&#x2F;C++ (if so, how did you resolve missing includes&#x2F;build flags), or something else?</div><br/></div></div></div></div></div></div><div id="40825887" class="c"><input type="checkbox" id="c-40825887" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40824705">parent</a><span>|</span><a href="#40825182">prev</a><span>|</span><a href="#40828648">next</a><span>|</span><label class="collapse" for="c-40825887">[-]</label><label class="expand" for="c-40825887">[3 more]</label></div><br/><div class="children"><div class="content">&gt; C++ spaceship operator<p>&gt; (A &lt;=&gt; B) &lt; 0 is true if A &lt; B<p>&gt; (A &lt;=&gt; B) &gt; 0 is true if A &gt; B<p>&gt; (A &lt;=&gt; B) == 0 is true if A and B are equal&#x2F;equivalent.<p>TIL of the spaceship operator. Was this added as an april fools?</div><br/><div id="40826170" class="c"><input type="checkbox" id="c-40826170" checked=""/><div class="controls bullet"><span class="by">samatman</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40825887">parent</a><span>|</span><a href="#40828648">next</a><span>|</span><label class="collapse" for="c-40826170">[-]</label><label class="expand" for="c-40826170">[2 more]</label></div><br/><div class="children"><div class="content">This is one of the oldest computer operators in the game: the arithmetic IF statement from FORTRAN.<p>It&#x27;s useful for stable-sorting collections with a single test. Also, overloading &lt;=&gt; for a type, gives all comparison operators &quot;for free&quot;: ==, !=, &lt;, &lt;=, &gt;=, &gt;</div><br/><div id="40827289" class="c"><input type="checkbox" id="c-40827289" checked=""/><div class="controls bullet"><span class="by">pklausler</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40826170">parent</a><span>|</span><a href="#40828648">next</a><span>|</span><label class="collapse" for="c-40827289">[-]</label><label class="expand" for="c-40827289">[1 more]</label></div><br/><div class="children"><div class="content">It also has no good definition for its semantics when presented with a NaN.</div><br/></div></div></div></div></div></div></div></div><div id="40828648" class="c"><input type="checkbox" id="c-40828648" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#40822851">parent</a><span>|</span><a href="#40824705">prev</a><span>|</span><a href="#40824641">next</a><span>|</span><label class="collapse" for="c-40828648">[-]</label><label class="expand" for="c-40828648">[1 more]</label></div><br/><div class="children"><div class="content">This feels like going insane honestly. It&#x27;s like reading that people are super excited about using bouncing castles to mix concrete.</div><br/></div></div><div id="40824641" class="c"><input type="checkbox" id="c-40824641" checked=""/><div class="controls bullet"><span class="by">solarexplorer</span><span>|</span><a href="#40822851">parent</a><span>|</span><a href="#40828648">prev</a><span>|</span><a href="#40823033">next</a><span>|</span><label class="collapse" for="c-40824641">[-]</label><label class="expand" for="c-40824641">[3 more]</label></div><br/><div class="children"><div class="content">If I understand correctly, the AI is only choosing the optimization passes and their relative order. Each individual optimization step would still be designed and verified manually, and maybe even proven to be correct mathematically.</div><br/><div id="40824971" class="c"><input type="checkbox" id="c-40824971" checked=""/><div class="controls bullet"><span class="by">boomanaiden154</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40824641">parent</a><span>|</span><a href="#40823033">next</a><span>|</span><label class="collapse" for="c-40824971">[-]</label><label class="expand" for="c-40824971">[2 more]</label></div><br/><div class="children"><div class="content">Right, it&#x27;s only solving phase ordering.<p>In practice though, correctness even over ordering of hand-written passes is difficult. Within the paper they describe a methodology to evaluate phase orderings against a small test set as a smoke test for correctness (PassListEval) and observe that ~10% of the phase orderings result in assertion failures&#x2F;compiler crashes&#x2F;correctness issues.<p>You will end up with a lot more correctness issues adjusting phase orderings like this than you would using one of the more battle-tested default optimization pipelines.<p>Correctness in a production compiler is a pretty hard problem.</div><br/><div id="40825786" class="c"><input type="checkbox" id="c-40825786" checked=""/><div class="controls bullet"><span class="by">hughleat</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40824971">parent</a><span>|</span><a href="#40823033">next</a><span>|</span><label class="collapse" for="c-40825786">[-]</label><label class="expand" for="c-40825786">[1 more]</label></div><br/><div class="children"><div class="content">There are two models.<p>- foundation model is pretrained on asm and ir. Then it is trained to emulate the compiler (ir + passes -&gt; ir or asm)<p>- ftd model is fine tuned for solving phase ordering and disassembling<p>FTD is there to demo capabilities. We hope people will fine tune for other optimisations. It will be much, much cheaper than starting from scratch.<p>Yep, correctness in compilers is a pain. Auto-tuning is a very easy way to break a compiler.</div><br/></div></div></div></div></div></div><div id="40823033" class="c"><input type="checkbox" id="c-40823033" checked=""/><div class="controls bullet"><span class="by">ramon156</span><span>|</span><a href="#40822851">parent</a><span>|</span><a href="#40824641">prev</a><span>|</span><a href="#40824738">next</a><span>|</span><label class="collapse" for="c-40823033">[-]</label><label class="expand" for="c-40823033">[4 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s be real, at least 40% of those comments are bots</div><br/><div id="40823783" class="c"><input type="checkbox" id="c-40823783" checked=""/><div class="controls bullet"><span class="by">extheat</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40823033">parent</a><span>|</span><a href="#40824738">next</a><span>|</span><label class="collapse" for="c-40823783">[-]</label><label class="expand" for="c-40823783">[3 more]</label></div><br/><div class="children"><div class="content">People simply have no idea what they&#x27;re talking about. It&#x27;s just jumping on to the latest hype train. My first impression here was per the name that it was actually some sort of compiler in it of itself--ie programming language in and pure machine code or some other IR out. It&#x27;s got bits and pieces of that here and there but that&#x27;s not what it really is at all. It&#x27;s more of a predictive engine for an optimizer and not a very generalized one for that.<p>What would be more interesting is training a large model on pure (code, assembly) pairs like a normal translation task. Presumably a very generalized model would be good at even doing the inverse: given some assembly, write code that will produce the given assembly. Unlike human language there is a finite set of possible correct answers here and you have the convenience of being able to generate synthetic data for cheap. I think optimizations would arise as a natural side effect this way: if there&#x27;s multiple trees of possible generations (like choosing between logits in an LLM) you could try different branches to see what&#x27;s smaller in terms of byte code or faster in terms of execution.</div><br/><div id="40825828" class="c"><input type="checkbox" id="c-40825828" checked=""/><div class="controls bullet"><span class="by">hughleat</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40823783">parent</a><span>|</span><a href="#40824599">next</a><span>|</span><label class="collapse" for="c-40825828">[-]</label><label class="expand" for="c-40825828">[1 more]</label></div><br/><div class="children"><div class="content">It can emulate the compiler (IR + passes -&gt; IR or ASM).<p>&gt; What would be more interesting is training a large model on pure (code, assembly) pairs like a normal translation task.<p>It is that.<p>&gt; Presumably a very generalized model would be good at even doing the inverse: given some assembly, write code that will produce the given assembly.<p>Is has been trained to disassemble. It is much, much better than other models at that.</div><br/></div></div><div id="40824599" class="c"><input type="checkbox" id="c-40824599" checked=""/><div class="controls bullet"><span class="by">quonn</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40823783">parent</a><span>|</span><a href="#40825828">prev</a><span>|</span><a href="#40824738">next</a><span>|</span><label class="collapse" for="c-40824599">[-]</label><label class="expand" for="c-40824599">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Presumably a very generalized model would be good at even doing the inverse: given some assembly, write code that will produce the given assembly.<p>ChatGPT does this, unreliably.</div><br/></div></div></div></div></div></div><div id="40824738" class="c"><input type="checkbox" id="c-40824738" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#40822851">parent</a><span>|</span><a href="#40823033">prev</a><span>|</span><a href="#40827893">next</a><span>|</span><label class="collapse" for="c-40824738">[-]</label><label class="expand" for="c-40824738">[2 more]</label></div><br/><div class="children"><div class="content">AFAIK this is a heuristic, not a category. The underlying grammar would be preserved.<p>Personally I thought we were way too close to perfect to make meaningful progress on compilation, but thatâs probably just naÃ¯vetÃ©</div><br/><div id="40824997" class="c"><input type="checkbox" id="c-40824997" checked=""/><div class="controls bullet"><span class="by">boomanaiden154</span><span>|</span><a href="#40822851">root</a><span>|</span><a href="#40824738">parent</a><span>|</span><a href="#40827893">next</a><span>|</span><label class="collapse" for="c-40824997">[-]</label><label class="expand" for="c-40824997">[1 more]</label></div><br/><div class="children"><div class="content">I would not say we are anywhere close to perfect in compilation.<p>Even just looking at inlining for size, there are multiple recent studies showing ~10+% improvement (<a href="https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;abs&#x2F;10.1145&#x2F;3503222.3507744" rel="nofollow">https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;abs&#x2F;10.1145&#x2F;3503222.3507744</a>, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2101.04808" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2101.04808</a>).<p>There is a massive amount of headroom, and even tiny bits still matter as ~0.5% gains on code size, or especially performance, can be huge.</div><br/></div></div></div></div></div></div><div id="40827893" class="c"><input type="checkbox" id="c-40827893" checked=""/><div class="controls bullet"><span class="by">0x1ceb00da</span><span>|</span><a href="#40822851">prev</a><span>|</span><a href="#40823116">next</a><span>|</span><label class="collapse" for="c-40827893">[-]</label><label class="expand" for="c-40827893">[1 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t &quot;Compiler LLM&quot; be a more accurate name than &quot;LLM Compiler&quot;?</div><br/></div></div><div id="40823116" class="c"><input type="checkbox" id="c-40823116" checked=""/><div class="controls bullet"><span class="by">muglug</span><span>|</span><a href="#40827893">prev</a><span>|</span><a href="#40825379">next</a><span>|</span><label class="collapse" for="c-40823116">[-]</label><label class="expand" for="c-40823116">[2 more]</label></div><br/><div class="children"><div class="content">Unlike many other AI-themed papers at Meta this one omits any mention of the model output getting used at Instagram, Facebook or Meta. Research is great! But doesn&#x27;t seem all that actionable today.</div><br/><div id="40825118" class="c"><input type="checkbox" id="c-40825118" checked=""/><div class="controls bullet"><span class="by">boomanaiden154</span><span>|</span><a href="#40823116">parent</a><span>|</span><a href="#40825379">next</a><span>|</span><label class="collapse" for="c-40825118">[-]</label><label class="expand" for="c-40825118">[1 more]</label></div><br/><div class="children"><div class="content">This would be difficult to deploy as-is in production.<p>There are correctness issues mentioned in the paper regarding adjusting phase orderings away from the well-trodden O0&#x2F;O1&#x2F;O2&#x2F;O3&#x2F;Os&#x2F;Oz path. Their methodology works for a research project quite well, but I personally wouldn&#x27;t trust it in production. While some obvious issues can be caught by a small test suite and unit tests, there are others that won&#x27;t be, and that&#x27;s really risky in production scenarios.<p>There are also some practical software engineering things like deployment in the compiler. There is actually tooling in upstream LLVM to do this (<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=mQu1CLZ3uWs" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=mQu1CLZ3uWs</a>), but running models on a GPU would  be difficult and I would expect CPU inference to massively blow up compile times.</div><br/></div></div></div></div><div id="40825379" class="c"><input type="checkbox" id="c-40825379" checked=""/><div class="controls bullet"><span class="by">jameshart</span><span>|</span><a href="#40823116">prev</a><span>|</span><a href="#40823963">next</a><span>|</span><label class="collapse" for="c-40825379">[-]</label><label class="expand" for="c-40825379">[1 more]</label></div><br/><div class="children"><div class="content">Pretty sure I remember trading 300 creds for a Meta Technologies Neural Optimizer and Disassembler in one of the early <i>Deus Ex</i> games.</div><br/></div></div><div id="40823963" class="c"><input type="checkbox" id="c-40823963" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#40825379">prev</a><span>|</span><a href="#40822890">next</a><span>|</span><label class="collapse" for="c-40823963">[-]</label><label class="expand" for="c-40823963">[1 more]</label></div><br/><div class="children"><div class="content">Reading the title, I thought this was a tool for optimizing and disassembling LLMs, not an LLM designed to optimize and disassemble. Seeing it&#x27;s just a model is a little disappointing in comparison.</div><br/></div></div><div id="40822890" class="c"><input type="checkbox" id="c-40822890" checked=""/><div class="controls bullet"><span class="by">ldjkfkdsjnv</span><span>|</span><a href="#40823963">prev</a><span>|</span><label class="collapse" for="c-40822890">[-]</label><label class="expand" for="c-40822890">[4 more]</label></div><br/><div class="children"><div class="content">I love this company. Advancing ai and keeping the rest of us in the loop.</div><br/><div id="40828286" class="c"><input type="checkbox" id="c-40828286" checked=""/><div class="controls bullet"><span class="by">Slyfox33</span><span>|</span><a href="#40822890">parent</a><span>|</span><a href="#40823943">next</a><span>|</span><label class="collapse" for="c-40828286">[-]</label><label class="expand" for="c-40828286">[1 more]</label></div><br/><div class="children"><div class="content">Is this a bot comment?</div><br/></div></div><div id="40823943" class="c"><input type="checkbox" id="c-40823943" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#40822890">parent</a><span>|</span><a href="#40828286">prev</a><span>|</span><a href="#40823308">next</a><span>|</span><label class="collapse" for="c-40823943">[-]</label><label class="expand" for="c-40823943">[1 more]</label></div><br/><div class="children"><div class="content">My love for Meta is strictly confined to FAIR and the PyTorch team. The rest of the company is basically cancer.</div><br/></div></div><div id="40823308" class="c"><input type="checkbox" id="c-40823308" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#40822890">parent</a><span>|</span><a href="#40823943">prev</a><span>|</span><label class="collapse" for="c-40823308">[-]</label><label class="expand" for="c-40823308">[1 more]</label></div><br/><div class="children"><div class="content">I hate the company (Facebook), but I still think them having been publicly releasing a bunch of the research they&#x27;ve been doing (and models they&#x27;ve been making) has been a net good for almost everybody, at least in terms of exploring the field of LLMs.</div><br/></div></div></div></div></div></div></div></div></div></body></html>