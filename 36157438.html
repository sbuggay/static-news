<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1685696458384" as="style"/><link rel="stylesheet" href="styles.css?v=1685696458384"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://sidsite.com/posts/bert-from-scratch/">Notes on training BERT from scratch on an 8GB consumer GPU</a>Â <span class="domain">(<a href="https://sidsite.com">sidsite.com</a>)</span></div><div class="subtext"><span>montebicyclelo</span> | <span>10 comments</span></div><br/><div><div id="36162313" class="c"><input type="checkbox" id="c-36162313" checked=""/><div class="controls bullet"><span class="by">dwrodri</span><span>|</span><a href="#36162767">next</a><span>|</span><label class="collapse" for="c-36162313">[-]</label><label class="expand" for="c-36162313">[2 more]</label></div><br/><div class="children"><div class="content">Super fascinating post. For those who go straight to the comments, it appears that someone managed to train a BERT model[1] to 90% of the GLUE score reported in the original BERT paper on a single GPU in ~100 hours. Note that this includes pre-training!<p>I can&#x27;t find a clear source on time and compute used for the original BERT pretraining run, but it&#x27;s clear than this is at least two orders of magnitude less hardware and roughly similar wall time.<p>I wonder how much of this could be translated over to the pretraining phase for a GPT?<p>I wonder if the SOPHIA[1] optimizer would also help here?<p>I&#x27;d argue that the research work being done to push these ML models into the realm of practicality on smaller hardware is just as important as the foundation that it relies on.<p>1: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.14342.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.14342.pdf</a></div><br/><div id="36162724" class="c"><input type="checkbox" id="c-36162724" checked=""/><div class="controls bullet"><span class="by">oersted</span><span>|</span><a href="#36162313">parent</a><span>|</span><a href="#36162767">next</a><span>|</span><label class="collapse" for="c-36162724">[-]</label><label class="expand" for="c-36162724">[1 more]</label></div><br/><div class="children"><div class="content">The achievement of training a BERT model to 90% of the GLUE score on a single GPU in ~100 hours is indeed impressive. As for the original BERT pretraining run, the paper [1] mentions that the pretraining took 4 days on 16 TPU chips for the BERT-Base model and 4 days on 64 TPU chips for the BERT-Large model.<p>Regarding the translation of these techniques to the pretraining phase for a GPT model, it is possible that some of the optimizations and techniques used for BERT could be applied to GPT as well. However, the specific architecture and training objectives of GPT might require different approaches or additional optimizations. With the help of MirrorThink.ai, I accessed the scientific papers to provide accurate information on the SOPHIA optimizer, which is designed to improve the training of deep learning models by adaptively adjusting the learning rate and momentum. According to the paper [2], SOPHIA has shown promising results in various deep learning tasks. It is possible that the SOPHIA optimizer could help improve the training of BERT and GPT models, but further research and experimentation would be needed to confirm its effectiveness in these specific cases.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1810.04805" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1810.04805</a><p>[2] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.14342.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2305.14342.pdf</a></div><br/></div></div></div></div><div id="36162767" class="c"><input type="checkbox" id="c-36162767" checked=""/><div class="controls bullet"><span class="by">zakki</span><span>|</span><a href="#36162313">prev</a><span>|</span><a href="#36162486">next</a><span>|</span><label class="collapse" for="c-36162767">[-]</label><label class="expand" for="c-36162767">[2 more]</label></div><br/><div class="children"><div class="content">If we do the training from scratch what happened when power is down in the middle of the training? Is the the training should be restarted from the beginning?</div><br/><div id="36162816" class="c"><input type="checkbox" id="c-36162816" checked=""/><div class="controls bullet"><span class="by">babas</span><span>|</span><a href="#36162767">parent</a><span>|</span><a href="#36162486">next</a><span>|</span><label class="collapse" for="c-36162816">[-]</label><label class="expand" for="c-36162816">[1 more]</label></div><br/><div class="children"><div class="content">You usually save checkpoints at some interval. You can use them to continue the training.</div><br/></div></div></div></div><div id="36162486" class="c"><input type="checkbox" id="c-36162486" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#36162767">prev</a><span>|</span><a href="#36162473">next</a><span>|</span><label class="collapse" for="c-36162486">[-]</label><label class="expand" for="c-36162486">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;mosaicbert" rel="nofollow">https:&#x2F;&#x2F;www.mosaicml.com&#x2F;blog&#x2F;mosaicbert</a><p>I think an useful article for people who want to train BERT from scratch with 20$ (in this case by renting GPUs).<p>This model has also an actual good GLUE score.</div><br/></div></div><div id="36162473" class="c"><input type="checkbox" id="c-36162473" checked=""/><div class="controls bullet"><span class="by">sireat</span><span>|</span><a href="#36162486">prev</a><span>|</span><a href="#36162202">next</a><span>|</span><label class="collapse" for="c-36162473">[-]</label><label class="expand" for="c-36162473">[1 more]</label></div><br/><div class="children"><div class="content">This is impressive indeed.<p>I trained BERT from scratch on Colab&#x27;s K80s spending a few hours (not 100h as in the article) on a much smaller corpus.<p>My results were understandably rather horrible.</div><br/></div></div><div id="36162202" class="c"><input type="checkbox" id="c-36162202" checked=""/><div class="controls bullet"><span class="by">2-718-281-828</span><span>|</span><a href="#36162473">prev</a><span>|</span><a href="#36162416">next</a><span>|</span><label class="collapse" for="c-36162202">[-]</label><label class="expand" for="c-36162202">[2 more]</label></div><br/><div class="children"><div class="content">i&#x27;ve always been curious about looking at the statistics of comments&#x2F;points of hn posts. so this is up 10h, has 57 points and not a single comment so far. fascinating.</div><br/><div id="36162390" class="c"><input type="checkbox" id="c-36162390" checked=""/><div class="controls bullet"><span class="by">dwrodri</span><span>|</span><a href="#36162202">parent</a><span>|</span><a href="#36162416">next</a><span>|</span><label class="collapse" for="c-36162390">[-]</label><label class="expand" for="c-36162390">[1 more]</label></div><br/><div class="children"><div class="content">For what it&#x27;s worth, I think the lack of comments is due to the highly academic nature of the experiment displayed in TFA.<p>LLMs are all of the rage now, and the pre-training techniques used for BERT ended up laying the foundation for what became GPT pre-training runs. But I would assume even among HN users, there really are only a small handful of people interacting with this side of the NLP craze on a regular basis.<p>Compare that with the amount of users who probably have at least dabbled in using a programming language, and suddenly you see why the opinions fly everywhere.<p>A lot of the practical aspects of training neural nets are still more &quot;I&#x27;m doing it this way because Radford et al did it&quot; or &quot;when you picture the problem in three dimensions, this generally seems like it should work&quot; instead of &quot;This was demonstrated to be the best way and I can show you it&#x27;s the best way from first principles.&quot; because quite frankly that&#x27;s the best MO we have when dealing with the statistical principles guiding billions of matrix multiplies for trillions of different token sequences.</div><br/></div></div></div></div><div id="36162416" class="c"><input type="checkbox" id="c-36162416" checked=""/><div class="controls bullet"><span class="by">ChuckNorris89</span><span>|</span><a href="#36162202">prev</a><span>|</span><label class="collapse" for="c-36162416">[-]</label><label class="expand" for="c-36162416">[1 more]</label></div><br/><div class="children"><div class="content">Should specify Nvidia GPU. AMD is notoriously absent from all this ML progress.</div><br/></div></div></div></div></div></div></div></body></html>