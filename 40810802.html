<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1719565266066" as="style"/><link rel="stylesheet" href="styles.css?v=1719565266066"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf">Gemma 2: Improving Open Language Models at a Practical Size [pdf]</a> <span class="domain">(<a href="https://storage.googleapis.com">storage.googleapis.com</a>)</span></div><div class="subtext"><span>tosh</span> | <span>159 comments</span></div><br/><div><div id="40812501" class="c"><input type="checkbox" id="c-40812501" checked=""/><div class="controls bullet"><span class="by">aubanel</span><span>|</span><a href="#40811067">next</a><span>|</span><label class="collapse" for="c-40812501">[-]</label><label class="expand" for="c-40812501">[21 more]</label></div><br/><div class="children"><div class="content">It&#x27;s exceptionally strong.
In LMSys Chatbot Arena, the 27B version scores above LLama-3-70B, at the level of OpenAI GPT-4 and Claude-3 Sonnet!</div><br/><div id="40813648" class="c"><input type="checkbox" id="c-40813648" checked=""/><div class="controls bullet"><span class="by">typpo</span><span>|</span><a href="#40812501">parent</a><span>|</span><a href="#40816428">next</a><span>|</span><label class="collapse" for="c-40813648">[-]</label><label class="expand" for="c-40813648">[9 more]</label></div><br/><div class="children"><div class="content">If anyone is interested in evaling Gemma locally, this can be done pretty easily using ollama[0] and promptfoo[1] with the following config:<p><pre><code>  prompts:
    - &#x27;Answer this coding problem in Python: {{ask}}&#x27;

  providers:
    - ollama:chat:gemma2:9b
    - ollama:chat:llama3:8b

  tests:
    - vars:
        ask: function to find the nth fibonacci number
    - vars:
        ask: calculate pi to the nth digit
    - # ...
</code></pre>
One small thing I&#x27;ve always appreciated about Gemma is that it doesn&#x27;t include a &quot;Sure, I can help you&quot; preamble.  It just gets right into the code, and follows it with an explanation.  The training seems to emphasize response structure and ease of comprehension.<p>Also, best to run evals that don&#x27;t rely on rote memorization of public code... so please substitute with your personal tests :)<p>[0] <a href="https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;gemma2">https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;gemma2</a><p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;promptfoo&#x2F;promptfoo">https:&#x2F;&#x2F;github.com&#x2F;promptfoo&#x2F;promptfoo</a></div><br/><div id="40815940" class="c"><input type="checkbox" id="c-40815940" checked=""/><div class="controls bullet"><span class="by">roywiggins</span><span>|</span><a href="#40812501">root</a><span>|</span><a href="#40813648">parent</a><span>|</span><a href="#40815850">next</a><span>|</span><label class="collapse" for="c-40815940">[-]</label><label class="expand" for="c-40815940">[7 more]</label></div><br/><div class="children"><div class="content">In Ollama, Gemma:9b works fine, but 27b seems to be producing a lot of nonsense for me. Asking for a bit of python or JavaScript code rapidly devolves into producing code-like gobbledegook, extending for hundreds of lines.</div><br/><div id="40817338" class="c"><input type="checkbox" id="c-40817338" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#40812501">root</a><span>|</span><a href="#40815940">parent</a><span>|</span><a href="#40816456">next</a><span>|</span><label class="collapse" for="c-40817338">[-]</label><label class="expand" for="c-40817338">[4 more]</label></div><br/><div class="children"><div class="content">Had a chance to do some testing and it seems quite good on oneshot tasks with a small context window but as you approach context saturation it starts to go way off the rails. Maybe this is an implementation issue? I&#x27;m using Q6_K quants of both sizes in ollama. I&#x27;ll report back if I figure it out.<p>A larger context window really helps on RAG tasks, it&#x27;s frustrating that a lot of the foundational models have such small windows.</div><br/><div id="40817753" class="c"><input type="checkbox" id="c-40817753" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#40812501">root</a><span>|</span><a href="#40817338">parent</a><span>|</span><a href="#40816456">next</a><span>|</span><label class="collapse" for="c-40817753">[-]</label><label class="expand" for="c-40817753">[3 more]</label></div><br/><div class="children"><div class="content">Sorry about this – working on fixing the issue with hitting the context limit. Gemma 2 supports a 8192 context limit – which can be selected if you provide the `num_ctx` parameter in the API or via `ollama run` with `&#x2F;set parameter num_ctx 8192`</div><br/><div id="40817790" class="c"><input type="checkbox" id="c-40817790" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#40812501">root</a><span>|</span><a href="#40817753">parent</a><span>|</span><a href="#40816456">next</a><span>|</span><label class="collapse" for="c-40817790">[-]</label><label class="expand" for="c-40817790">[2 more]</label></div><br/><div class="children"><div class="content">Thanks! If you have a moment can you give me a quick explainer on what happens when you hit the context limit in ollama? I had assumed that ollama would just trunc the context to whatever is set in the model, but I guess this isn&#x27;t the case?</div><br/><div id="40818556" class="c"><input type="checkbox" id="c-40818556" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#40812501">root</a><span>|</span><a href="#40817790">parent</a><span>|</span><a href="#40816456">next</a><span>|</span><label class="collapse" for="c-40818556">[-]</label><label class="expand" for="c-40818556">[1 more]</label></div><br/><div class="children"><div class="content">Currently when the context limit is hit, there&#x27;s a halving of the context window (or a &quot;context shift&quot;) to allow inference to continue – this is helpful for smaller (e.g. 1-2k) context windows.<p>However, not all models (especially newer ones) respond well to this, which makes sense. We&#x27;re working on changing the behavior in Ollama&#x27;s API to be more similar to OpenAI, Anthropic and similar APIs so that when the context limit is hit, the API returns a &quot;limit&quot; finish&#x2F;done reason. Hope this is helpful!</div><br/></div></div></div></div></div></div></div></div><div id="40816456" class="c"><input type="checkbox" id="c-40816456" checked=""/><div class="controls bullet"><span class="by">brandall10</span><span>|</span><a href="#40812501">root</a><span>|</span><a href="#40815940">parent</a><span>|</span><a href="#40817338">prev</a><span>|</span><a href="#40816324">next</a><span>|</span><label class="collapse" for="c-40816456">[-]</label><label class="expand" for="c-40816456">[1 more]</label></div><br/><div class="children"><div class="content">27b is working fine for me, hosted on ollama w&#x2F; continue.dev in VSCode.</div><br/></div></div><div id="40816324" class="c"><input type="checkbox" id="c-40816324" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#40812501">root</a><span>|</span><a href="#40815940">parent</a><span>|</span><a href="#40816456">prev</a><span>|</span><a href="#40815850">next</a><span>|</span><label class="collapse" for="c-40816324">[-]</label><label class="expand" for="c-40816324">[1 more]</label></div><br/><div class="children"><div class="content">The tokenizer in llama.cpp probably needs fixing then or it has some other bug.</div><br/></div></div></div></div></div></div><div id="40816428" class="c"><input type="checkbox" id="c-40816428" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#40812501">parent</a><span>|</span><a href="#40813648">prev</a><span>|</span><a href="#40816229">next</a><span>|</span><label class="collapse" for="c-40816428">[-]</label><label class="expand" for="c-40816428">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;d encourage people to test for themselves (and to let the Chatbot Arena scores to settle) before getting caught up in too much hype. I just did a personal eval and I found gemma-2-27b-it (tested on AI Studio) performed far worse in my testing than Llama 3 70B, especially for reasoning and basic world understanding queries.</div><br/><div id="40816632" class="c"><input type="checkbox" id="c-40816632" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#40812501">root</a><span>|</span><a href="#40816428">parent</a><span>|</span><a href="#40816553">next</a><span>|</span><label class="collapse" for="c-40816632">[-]</label><label class="expand" for="c-40816632">[1 more]</label></div><br/><div class="children"><div class="content">I also prefer to use &quot;Coding&quot; or &quot;Hard Prompts (Overall)&quot; instead of default &quot;Overall&quot; in Chatbot Arena scores to determine the actual performance level of LLMs. Seems much more align to my vibe test in terms reasoning. I guess the &quot;Overall&quot; contains a lot of creative tasks, which is not what I use the most in the daily tasks.</div><br/></div></div><div id="40816553" class="c"><input type="checkbox" id="c-40816553" checked=""/><div class="controls bullet"><span class="by">nacs</span><span>|</span><a href="#40812501">root</a><span>|</span><a href="#40816428">parent</a><span>|</span><a href="#40816632">prev</a><span>|</span><a href="#40816229">next</a><span>|</span><label class="collapse" for="c-40816553">[-]</label><label class="expand" for="c-40816553">[1 more]</label></div><br/><div class="children"><div class="content">Same. I tried 27B and found it to be not even close to llama3-70b.<p>Even llama-8b did better in some of my tests than Gemma 27b.</div><br/></div></div></div></div><div id="40816229" class="c"><input type="checkbox" id="c-40816229" checked=""/><div class="controls bullet"><span class="by">usaar333</span><span>|</span><a href="#40812501">parent</a><span>|</span><a href="#40816428">prev</a><span>|</span><a href="#40813330">next</a><span>|</span><label class="collapse" for="c-40816229">[-]</label><label class="expand" for="c-40816229">[1 more]</label></div><br/><div class="children"><div class="content">I think this is just due to better non-English training data.<p>It&#x27;s 15 ELO under Llama-3-70B on english hard prompts and 41 ELO under Llama-3-70B (the latter is actually stat sig) for general English.</div><br/></div></div><div id="40813330" class="c"><input type="checkbox" id="c-40813330" checked=""/><div class="controls bullet"><span class="by">screye</span><span>|</span><a href="#40812501">parent</a><span>|</span><a href="#40816229">prev</a><span>|</span><a href="#40813869">next</a><span>|</span><label class="collapse" for="c-40813330">[-]</label><label class="expand" for="c-40813330">[4 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the most obvious standouts?<p>In my experience, smaller models tend to do well on benchmarks and fail at generalization. Phi-2 comes to mind.</div><br/><div id="40813501" class="c"><input type="checkbox" id="c-40813501" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#40812501">root</a><span>|</span><a href="#40813330">parent</a><span>|</span><a href="#40813869">next</a><span>|</span><label class="collapse" for="c-40813501">[-]</label><label class="expand" for="c-40813501">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s multilingual. Genuinely. Compared my results with some people on reddit and the consensus is that the 27B is near perfect in a few obscure languages and likely perfect in most common ones. The 9B is not as good but it&#x27;s still coherent enough to use in a pinch.<p>It&#x27;s literally the first omni-translation tool that actually works that you can run offline at home. I&#x27;m amazed that Google mentioned absolutely nothing about this in their paper.</div><br/><div id="40814327" class="c"><input type="checkbox" id="c-40814327" checked=""/><div class="controls bullet"><span class="by">jug</span><span>|</span><a href="#40812501">root</a><span>|</span><a href="#40813501">parent</a><span>|</span><a href="#40813659">next</a><span>|</span><label class="collapse" for="c-40814327">[-]</label><label class="expand" for="c-40814327">[1 more]</label></div><br/><div class="children"><div class="content">Wow, that&#x27;s very impressive and indeed a game changer. I&#x27;ve previously had trouble with various Scandinavian languages, but last I checked with was Llama 2 and I kind of gave up on it. I had expected we were going to need special purpose small models for these uses as a crutch, like SW-GPT3.<p>So I guess Gemma 2 is going to become Gemini 2.0 in their truly large and closed variants then? Or is it the open version of Gemini 1.5?</div><br/></div></div></div></div></div></div><div id="40815098" class="c"><input type="checkbox" id="c-40815098" checked=""/><div class="controls bullet"><span class="by">resource_waste</span><span>|</span><a href="#40812501">parent</a><span>|</span><a href="#40813869">prev</a><span>|</span><a href="#40811067">next</a><span>|</span><label class="collapse" for="c-40815098">[-]</label><label class="expand" for="c-40815098">[2 more]</label></div><br/><div class="children"><div class="content">Do we believe that? I&#x27;ve been told Google&#x27;s AI was going to be great 4 times now, and its consistently #4 behind OpenAI, Facebook, and Claude.</div><br/><div id="40816041" class="c"><input type="checkbox" id="c-40816041" checked=""/><div class="controls bullet"><span class="by">aubanel</span><span>|</span><a href="#40812501">root</a><span>|</span><a href="#40815098">parent</a><span>|</span><a href="#40811067">next</a><span>|</span><label class="collapse" for="c-40816041">[-]</label><label class="expand" for="c-40816041">[1 more]</label></div><br/><div class="children"><div class="content">LMSys Chatbot Arena is a crowd-sourced ranking with an ELO system: basically users a presented with 2 hidden models, they get the answers of the 2 models when presenting their request, and they vote which one performed bests, which realized one marche and updates the ELO scores. This is the closest thing that we have to a gold truth for LLM evaluation: and Gemma2-27B performs extremely well in Chatbot Arena ELO.</div><br/></div></div></div></div></div></div><div id="40811067" class="c"><input type="checkbox" id="c-40811067" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#40812501">prev</a><span>|</span><a href="#40818097">next</a><span>|</span><label class="collapse" for="c-40811067">[-]</label><label class="expand" for="c-40811067">[49 more]</label></div><br/><div class="children"><div class="content">Hello (again) from the Gemma team! We are quite excited to push this release out and happy to answer any questions!<p>Opinions are our own and not of Google DeepMind.</div><br/><div id="40812141" class="c"><input type="checkbox" id="c-40812141" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#40811067">parent</a><span>|</span><a href="#40811825">next</a><span>|</span><label class="collapse" for="c-40812141">[-]</label><label class="expand" for="c-40812141">[15 more]</label></div><br/><div class="children"><div class="content">It&#x27;s fairly easy to pay OpenAI or Mistral money to use their API&#x27;s.
Figuring out how Google Cloud Vertex works and how it&#x27;s billed is more complicated. Azure and AWS are similar in how complex they are to use for this.
Could Google Cloud please provide an OpenAI compatible API and service?
I know it&#x27;s a different department. But it&#x27;d make using your models way easier.
It often feels like Google Cloud has no UX or end-user testing done on it at all (not true for aistudio.google.com - that is better than before, for sure!).</div><br/><div id="40812996" class="c"><input type="checkbox" id="c-40812996" checked=""/><div class="controls bullet"><span class="by">Deathmax</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40812141">parent</a><span>|</span><a href="#40812969">next</a><span>|</span><label class="collapse" for="c-40812996">[-]</label><label class="expand" for="c-40812996">[5 more]</label></div><br/><div class="children"><div class="content">Gemini models on Vertex AI can be called via a preview OpenAI-compatible endpoint [1], but shoving it into existing tooling where you don&#x27;t have programmatic control over the API key and is long lived is non-trivial because GCP uses short lived access tokens (and long-lived ones are not great security-wise).<p>Billing for the Gemini models (on Vertex AI, the Generative Language AI variant still charges by tokens) I would argue is simpler than every other provider, simply because you&#x27;re charged by characters&#x2F;image&#x2F;video-second&#x2F;audio-second and don&#x27;t need to run a tokenizer (if it&#x27;s even available <i>cough</i> Claude 3 and Gemini) and having to figure out what the chat template is to calculate the token cost per message [2] or figure out how to calculate tokens for an image [3] to get cost estimates before actually submitting the request and getting usage info back.<p>[1]: <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;vertex-ai&#x2F;generative-ai&#x2F;docs&#x2F;multimodal&#x2F;call-gemini-using-openai-library" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;vertex-ai&#x2F;generative-ai&#x2F;docs&#x2F;multim...</a><p>[2]: <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;text-generation&#x2F;managing-tokens" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;text-generation&#x2F;mana...</a><p>[3]: <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;vision&#x2F;calculating-costs" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;vision&#x2F;calculating-c...</a></div><br/><div id="40814443" class="c"><input type="checkbox" id="c-40814443" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40812996">parent</a><span>|</span><a href="#40812969">next</a><span>|</span><label class="collapse" for="c-40814443">[-]</label><label class="expand" for="c-40814443">[4 more]</label></div><br/><div class="children"><div class="content">Good to know about this API preview. Hopefully the billing problem and UI maze of Vertex AI can be sorted too?</div><br/><div id="40815209" class="c"><input type="checkbox" id="c-40815209" checked=""/><div class="controls bullet"><span class="by">Flumio</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40814443">parent</a><span>|</span><a href="#40812969">next</a><span>|</span><label class="collapse" for="c-40815209">[-]</label><label class="expand" for="c-40815209">[3 more]</label></div><br/><div class="children"><div class="content">Google does plenty of ux studies on gcp. I took part in at least 3 of them.<p>I&#x27;m also not sure if I understand your problem with pricing? Depending on what you do with it, it&#x27;s not just an LLM. It actually started before llms.<p>Pricing for image classification and other features are completely different products like an LLM.</div><br/><div id="40815641" class="c"><input type="checkbox" id="c-40815641" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40815209">parent</a><span>|</span><a href="#40812969">next</a><span>|</span><label class="collapse" for="c-40815641">[-]</label><label class="expand" for="c-40815641">[2 more]</label></div><br/><div class="children"><div class="content">They should do a whole lot more then!  Ideally they&#x27;d have effective impact.
It&#x27;s a busy mess on GCP. If they wanted to compete well, they should do much better with UX design, especially for onboarding. Compare how easy setting up a Mistral account is with GCP to do some generative LLM in a Python script. GCP is a maze. Did you make an account to reply to this? I&#x27;m curious what you do with GCP? Are you a heavy user?</div><br/><div id="40818354" class="c"><input type="checkbox" id="c-40818354" checked=""/><div class="controls bullet"><span class="by">Flumio</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40815641">parent</a><span>|</span><a href="#40812969">next</a><span>|</span><label class="collapse" for="c-40818354">[-]</label><label class="expand" for="c-40818354">[1 more]</label></div><br/><div class="children"><div class="content">I create new accounts because I use hn too much.<p>I use gcp professional every day and always found it quite intuitive.<p>Did plenty of image classification with vertex ai too</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40812969" class="c"><input type="checkbox" id="c-40812969" checked=""/><div class="controls bullet"><span class="by">ankeshanand</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40812141">parent</a><span>|</span><a href="#40812996">prev</a><span>|</span><a href="#40812350">next</a><span>|</span><label class="collapse" for="c-40812969">[-]</label><label class="expand" for="c-40812969">[3 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re an individual developer and not an enterprise, just go straight to Google AIStudio or GeminiAPI instead: <a href="https:&#x2F;&#x2F;aistudio.google.com&#x2F;app&#x2F;apikey" rel="nofollow">https:&#x2F;&#x2F;aistudio.google.com&#x2F;app&#x2F;apikey</a>. It&#x27;s dead simple getting an API key and calling with a rest client.</div><br/><div id="40814393" class="c"><input type="checkbox" id="c-40814393" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40812969">parent</a><span>|</span><a href="#40815351">next</a><span>|</span><label class="collapse" for="c-40814393">[-]</label><label class="expand" for="c-40814393">[1 more]</label></div><br/><div class="children"><div class="content">Interesting but when I tried it, I couldn&#x27;t figure out the billing model because it&#x27;s all connected to Google projects, and there can be different billing things for each of them.<p>Each thing seems to have a bunch of clicks to setup that startup LLM providers don&#x27;t hassle people with. They&#x27;re more likely to just let you sign in with some generic third party oAuth, slap on Stripe billing, let you generate keys, show you some usage stats, getting started docs, with example queries and a prompt playground etc.<p>What about the Vertex models though? Are they all actually available via Google AI Studio?</div><br/></div></div><div id="40815351" class="c"><input type="checkbox" id="c-40815351" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40812969">parent</a><span>|</span><a href="#40814393">prev</a><span>|</span><a href="#40812350">next</a><span>|</span><label class="collapse" for="c-40815351">[-]</label><label class="expand" for="c-40815351">[1 more]</label></div><br/><div class="children"><div class="content">Sadly, while gemma-2-27b-it is available (as a Preview model) on the AI Studio playground, it didn&#x27;t show up via API on list_models() for me.</div><br/></div></div></div></div><div id="40812350" class="c"><input type="checkbox" id="c-40812350" checked=""/><div class="controls bullet"><span class="by">bapcon</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40812141">parent</a><span>|</span><a href="#40812969">prev</a><span>|</span><a href="#40812336">next</a><span>|</span><label class="collapse" for="c-40812350">[-]</label><label class="expand" for="c-40812350">[2 more]</label></div><br/><div class="children"><div class="content">I have to agree with all of this. I tried switching to Gemini, but the lack of clear billing&#x2F;quotas, horrible documentation, and even poor implementation of status codes on failed requests have led me to stick with OpenAI.<p>I don&#x27;t know who writes Google&#x27;s documentation or does the copyediting for their console, but it is hard to adapt. I have spent hours troubleshooting, only to find out it&#x27;s because the documentation is referring to the same thing by two different names. It&#x27;s 2024 also, I shouldn&#x27;t be seeing print statements without parentheses.</div><br/></div></div><div id="40812336" class="c"><input type="checkbox" id="c-40812336" checked=""/><div class="controls bullet"><span class="by">hnuser123456</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40812141">parent</a><span>|</span><a href="#40812350">prev</a><span>|</span><a href="#40812188">next</a><span>|</span><label class="collapse" for="c-40812336">[-]</label><label class="expand" for="c-40812336">[1 more]</label></div><br/><div class="children"><div class="content">I plan on downloading a Q5 or Q6 version of the 27b for my 3090 once someone puts quants on HF, loading it in LM studio and starting the API server to call it from my scripts based on openai api. Hopefully it&#x27;s better at code gen than llama 3 8b.</div><br/></div></div><div id="40812188" class="c"><input type="checkbox" id="c-40812188" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40812141">parent</a><span>|</span><a href="#40812336">prev</a><span>|</span><a href="#40811825">next</a><span>|</span><label class="collapse" for="c-40812188">[-]</label><label class="expand" for="c-40812188">[3 more]</label></div><br/><div class="children"><div class="content">Happy to pass on any feedback to our Google Cloud friends. :)</div><br/><div id="40815445" class="c"><input type="checkbox" id="c-40815445" checked=""/><div class="controls bullet"><span class="by">anxman</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40812188">parent</a><span>|</span><a href="#40812464">next</a><span>|</span><label class="collapse" for="c-40815445">[-]</label><label class="expand" for="c-40815445">[1 more]</label></div><br/><div class="children"><div class="content">I also hate the billing. It feels like configuring AWS more than calling APIs.</div><br/></div></div><div id="40812464" class="c"><input type="checkbox" id="c-40812464" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40812188">parent</a><span>|</span><a href="#40815445">prev</a><span>|</span><a href="#40811825">next</a><span>|</span><label class="collapse" for="c-40812464">[-]</label><label class="expand" for="c-40812464">[1 more]</label></div><br/><div class="children"><div class="content">Thank you!</div><br/></div></div></div></div></div></div><div id="40811825" class="c"><input type="checkbox" id="c-40811825" checked=""/><div class="controls bullet"><span class="by">canyon289</span><span>|</span><a href="#40811067">parent</a><span>|</span><a href="#40812141">prev</a><span>|</span><a href="#40813908">next</a><span>|</span><label class="collapse" for="c-40811825">[-]</label><label class="expand" for="c-40811825">[2 more]</label></div><br/><div class="children"><div class="content">I also work at Google and on Gemma (so same disclaimers)<p>You can try 27b at www.aistudio,google.com. Send in your favorite prompts, and we hope you like the responses.</div><br/><div id="40818224" class="c"><input type="checkbox" id="c-40818224" checked=""/><div class="controls bullet"><span class="by">dandanua</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40811825">parent</a><span>|</span><a href="#40813908">next</a><span>|</span><label class="collapse" for="c-40818224">[-]</label><label class="expand" for="c-40818224">[1 more]</label></div><br/><div class="children"><div class="content">Why is AIStudio not available in Ukraine? I have no problem with using Gemini web UI or other LLM providers from Ukraine, but this Google API constrain is strange.</div><br/></div></div></div></div><div id="40813908" class="c"><input type="checkbox" id="c-40813908" checked=""/><div class="controls bullet"><span class="by">causal</span><span>|</span><a href="#40811067">parent</a><span>|</span><a href="#40811825">prev</a><span>|</span><a href="#40811679">next</a><span>|</span><label class="collapse" for="c-40813908">[-]</label><label class="expand" for="c-40813908">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for your work on this; excited to try it out!<p>The Google API models support 1M+ tokens, but these are just 8K. Is there a fundamental architecture difference, training set, something else?</div><br/></div></div><div id="40811679" class="c"><input type="checkbox" id="c-40811679" checked=""/><div class="controls bullet"><span class="by">jpcapdevila</span><span>|</span><a href="#40811067">parent</a><span>|</span><a href="#40813908">prev</a><span>|</span><a href="#40811747">next</a><span>|</span><label class="collapse" for="c-40811679">[-]</label><label class="expand" for="c-40811679">[4 more]</label></div><br/><div class="children"><div class="content">Will gemma2 be available through gemma.cpp? <a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;gemma.cpp">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;gemma.cpp</a></div><br/><div id="40811963" class="c"><input type="checkbox" id="c-40811963" checked=""/><div class="controls bullet"><span class="by">austinvhuang</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40811679">parent</a><span>|</span><a href="#40811747">next</a><span>|</span><label class="collapse" for="c-40811963">[-]</label><label class="expand" for="c-40811963">[3 more]</label></div><br/><div class="children"><div class="content">This is in the works in the dev branch (thanks pchx :)<p><a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;gemma.cpp&#x2F;pull&#x2F;274">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;gemma.cpp&#x2F;pull&#x2F;274</a></div><br/><div id="40813265" class="c"><input type="checkbox" id="c-40813265" checked=""/><div class="controls bullet"><span class="by">janwas</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40811963">parent</a><span>|</span><a href="#40811747">next</a><span>|</span><label class="collapse" for="c-40813265">[-]</label><label class="expand" for="c-40813265">[2 more]</label></div><br/><div class="children"><div class="content">:) Confirmed working. We&#x27;ve just pushed the dev branch to main.</div><br/><div id="40814093" class="c"><input type="checkbox" id="c-40814093" checked=""/><div class="controls bullet"><span class="by">jpcapdevila</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40813265">parent</a><span>|</span><a href="#40811747">next</a><span>|</span><label class="collapse" for="c-40814093">[-]</label><label class="expand" for="c-40814093">[1 more]</label></div><br/><div class="children"><div class="content">Awesome, I love this .cpp trend! Thanks for your work!!</div><br/></div></div></div></div></div></div></div></div><div id="40811747" class="c"><input type="checkbox" id="c-40811747" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#40811067">parent</a><span>|</span><a href="#40811679">prev</a><span>|</span><a href="#40811140">next</a><span>|</span><label class="collapse" for="c-40811747">[-]</label><label class="expand" for="c-40811747">[7 more]</label></div><br/><div class="children"><div class="content">Given the goal of mitigating self-proliferation risks, have you observed a decrease in the model&#x27;s ability to do things like help a user setup a local LLM with local or cloud software?<p>How much is pre-training dataset changes, how much is tuning?<p>How do you think about this problem, how do you solve it?<p>Seems tricky to me.</div><br/><div id="40813091" class="c"><input type="checkbox" id="c-40813091" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40811747">parent</a><span>|</span><a href="#40812171">next</a><span>|</span><label class="collapse" for="c-40813091">[-]</label><label class="expand" for="c-40813091">[5 more]</label></div><br/><div class="children"><div class="content">To quote Ludovic Peran, our amazing safety lead:<p>Literature has identified self-proliferation as dangerous capability of models, and details about how to define it and example of form it can take have been openly discussed by GDM (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2403.13793" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2403.13793</a>).<p>Current Gemma 2 models&#x27; success rate to end-to-end challenges is null (0 out 10), so the capabilities to perform such tasks are currently limited.</div><br/><div id="40814974" class="c"><input type="checkbox" id="c-40814974" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40813091">parent</a><span>|</span><a href="#40813727">next</a><span>|</span><label class="collapse" for="c-40814974">[-]</label><label class="expand" for="c-40814974">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s an interesting paper. 
`Install Mistral 7B on a GCP instance and use it to answer a simple question`.
Some hosting providers and inference software might be easier to setup, for now. ;)
But do you have to make it less capable, by being careful on what it&#x27;s trained on? E.g: banning certain topics (like how to use Lamafile&#x2F;llama.cpp, knowing what hosting providers have free trials, learning about ways to jailbreak web apps, free inference providers etc)?<p>Or does the model have to later be finetuned, to not be good at certain tasks?<p>Or are we not at that stage yet?<p>Is something like tree-of-thought used, to get the best of the models for these tasks?</div><br/></div></div><div id="40813727" class="c"><input type="checkbox" id="c-40813727" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40813091">parent</a><span>|</span><a href="#40814974">prev</a><span>|</span><a href="#40812171">next</a><span>|</span><label class="collapse" for="c-40813727">[-]</label><label class="expand" for="c-40813727">[3 more]</label></div><br/><div class="children"><div class="content">Turns out LLM alignment is super easy, barely an inconvenience.</div><br/><div id="40814518" class="c"><input type="checkbox" id="c-40814518" checked=""/><div class="controls bullet"><span class="by">josh-sematic</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40813727">parent</a><span>|</span><a href="#40814493">next</a><span>|</span><label class="collapse" for="c-40814518">[-]</label><label class="expand" for="c-40814518">[1 more]</label></div><br/><div class="children"><div class="content">Alignment is tight!</div><br/></div></div><div id="40814493" class="c"><input type="checkbox" id="c-40814493" checked=""/><div class="controls bullet"><span class="by">dinosaurdynasty</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40813727">parent</a><span>|</span><a href="#40814518">prev</a><span>|</span><a href="#40812171">next</a><span>|</span><label class="collapse" for="c-40814493">[-]</label><label class="expand" for="c-40814493">[1 more]</label></div><br/><div class="children"><div class="content">One should not confuse alignment and current incapability.</div><br/></div></div></div></div></div></div><div id="40812171" class="c"><input type="checkbox" id="c-40812171" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40811747">parent</a><span>|</span><a href="#40813091">prev</a><span>|</span><a href="#40811140">next</a><span>|</span><label class="collapse" for="c-40812171">[-]</label><label class="expand" for="c-40812171">[1 more]</label></div><br/><div class="children"><div class="content">Wow I&#x27;m kinda shocked this was downvoted. That&#x27;s not cool, it&#x27;s a reasonable question directly about the research - the main article link!</div><br/></div></div></div></div><div id="40811140" class="c"><input type="checkbox" id="c-40811140" checked=""/><div class="controls bullet"><span class="by">coreypreston</span><span>|</span><a href="#40811067">parent</a><span>|</span><a href="#40811747">prev</a><span>|</span><a href="#40813130">next</a><span>|</span><label class="collapse" for="c-40811140">[-]</label><label class="expand" for="c-40811140">[1 more]</label></div><br/><div class="children"><div class="content">No question. Thanks for thinking of 27B.</div><br/></div></div><div id="40813130" class="c"><input type="checkbox" id="c-40813130" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#40811067">parent</a><span>|</span><a href="#40811140">prev</a><span>|</span><a href="#40811485">next</a><span>|</span><label class="collapse" for="c-40813130">[-]</label><label class="expand" for="c-40813130">[2 more]</label></div><br/><div class="children"><div class="content">The paper suggests on one hand Gemma is on the same Pareto curve as Llama3, while on the other hand seems to suggest it’s exceeded its efficiency.<p>Is this a contradiction or am I misunderstanding something?<p>Btw overall very impressive work great job.</div><br/><div id="40813599" class="c"><input type="checkbox" id="c-40813599" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40813130">parent</a><span>|</span><a href="#40811485">next</a><span>|</span><label class="collapse" for="c-40813599">[-]</label><label class="expand" for="c-40813599">[1 more]</label></div><br/><div class="children"><div class="content">I think it makes sense to compare models trained with the same recipe on token count - usually more tokens will give you a better model.<p>However, I wouldn&#x27;t draw conclusions about different model families, like Llama and Gemma, based on their token count alone. There are many other variables at play - the quality of those tokens, number of epochs, model architecture, hyperparameters, distillation, etc. that will have an influence on training efficiency.</div><br/></div></div></div></div><div id="40811485" class="c"><input type="checkbox" id="c-40811485" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#40811067">parent</a><span>|</span><a href="#40813130">prev</a><span>|</span><a href="#40817055">next</a><span>|</span><label class="collapse" for="c-40811485">[-]</label><label class="expand" for="c-40811485">[9 more]</label></div><br/><div class="children"><div class="content">Any gemma-2-9b or 27b 4 bit GGUF&#x27;s on HuggingFace yet? Thanks!</div><br/><div id="40812431" class="c"><input type="checkbox" id="c-40812431" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40811485">parent</a><span>|</span><a href="#40811566">next</a><span>|</span><label class="collapse" for="c-40812431">[-]</label><label class="expand" for="c-40812431">[4 more]</label></div><br/><div class="children"><div class="content">Actually for the 9B model, this has 4-bit quantised weights (and others):
<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;bartowski&#x2F;gemma-2-9b-it-GGUF" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;bartowski&#x2F;gemma-2-9b-it-GGUF</a><p>Still no 27B 4-bit GGUF quants on HF yet!<p>I&#x27;m monitoring this search: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;models?library=gguf&amp;sort=trending&amp;search=gemma-2+27b" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;models?library=gguf&amp;sort=trending&amp;sea...</a></div><br/><div id="40816832" class="c"><input type="checkbox" id="c-40816832" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40812431">parent</a><span>|</span><a href="#40816543">next</a><span>|</span><label class="collapse" for="c-40816832">[-]</label><label class="expand" for="c-40816832">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious about the quantization quality claims in the table there. Is this a Gemma 2 specific thing (more subtlety in the weights somehow?). In my testing and testing I&#x27;ve seen elsewhere at least for llama3 8B (and some less rigorous testing with other models) q_8 -&gt; q4_K_M are basically indistinguishable from one another?</div><br/><div id="40817984" class="c"><input type="checkbox" id="c-40817984" checked=""/><div class="controls bullet"><span class="by">janwas</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40816832">parent</a><span>|</span><a href="#40816543">next</a><span>|</span><label class="collapse" for="c-40817984">[-]</label><label class="expand" for="c-40817984">[1 more]</label></div><br/><div class="children"><div class="content">Yes, PPL and certain benchmarks do not detect differences from quantization. But recent work gives cause for concern, e.g., <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2310.01382" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2310.01382</a>, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2405.18137" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2405.18137</a>.</div><br/></div></div></div></div><div id="40816543" class="c"><input type="checkbox" id="c-40816543" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40812431">parent</a><span>|</span><a href="#40816832">prev</a><span>|</span><a href="#40811566">next</a><span>|</span><label class="collapse" for="c-40816543">[-]</label><label class="expand" for="c-40816543">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;bartowski&#x2F;gemma-2-27b-it-GGUF" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;bartowski&#x2F;gemma-2-27b-it-GGUF</a></div><br/></div></div></div></div><div id="40811566" class="c"><input type="checkbox" id="c-40811566" checked=""/><div class="controls bullet"><span class="by">XzAeRosho</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40811485">parent</a><span>|</span><a href="#40812431">prev</a><span>|</span><a href="#40811614">next</a><span>|</span><label class="collapse" for="c-40811566">[-]</label><label class="expand" for="c-40811566">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s on HuggingFace already: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;google&#x2F;gemma-2-9b" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;google&#x2F;gemma-2-9b</a></div><br/><div id="40811787" class="c"><input type="checkbox" id="c-40811787" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40811566">parent</a><span>|</span><a href="#40811614">next</a><span>|</span><label class="collapse" for="c-40811787">[-]</label><label class="expand" for="c-40811787">[1 more]</label></div><br/><div class="children"><div class="content">I know the safe tensors are there, but I said GGUF 4-bit quantised, which is kinda the standard for useful local applications, a typical balanced sweet spot of performance and quality. It&#x27;s makes it much easier to use, works in more places, be it personal devices or a server etc.</div><br/></div></div></div></div><div id="40811614" class="c"><input type="checkbox" id="c-40811614" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40811485">parent</a><span>|</span><a href="#40811566">prev</a><span>|</span><a href="#40817055">next</a><span>|</span><label class="collapse" for="c-40811614">[-]</label><label class="expand" for="c-40811614">[2 more]</label></div><br/><div class="children"><div class="content">If you are still looking for it, I just made it available on an app[1] that I am working on with Gemma2 support.<p><a href="https:&#x2F;&#x2F;msty.app" rel="nofollow">https:&#x2F;&#x2F;msty.app</a></div><br/><div id="40812153" class="c"><input type="checkbox" id="c-40812153" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40811614">parent</a><span>|</span><a href="#40817055">next</a><span>|</span><label class="collapse" for="c-40812153">[-]</label><label class="expand" for="c-40812153">[1 more]</label></div><br/><div class="children"><div class="content">Are you saying you put a 4-bit GGUF on HuggingFace?</div><br/></div></div></div></div></div></div><div id="40817055" class="c"><input type="checkbox" id="c-40817055" checked=""/><div class="controls bullet"><span class="by">kristianpaul</span><span>|</span><a href="#40811067">parent</a><span>|</span><a href="#40811485">prev</a><span>|</span><a href="#40811205">next</a><span>|</span><label class="collapse" for="c-40817055">[-]</label><label class="expand" for="c-40817055">[1 more]</label></div><br/><div class="children"><div class="content">Do run gemma2 on your Google phone?</div><br/></div></div><div id="40811205" class="c"><input type="checkbox" id="c-40811205" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#40811067">parent</a><span>|</span><a href="#40817055">prev</a><span>|</span><a href="#40811113">next</a><span>|</span><label class="collapse" for="c-40811205">[-]</label><label class="expand" for="c-40811205">[2 more]</label></div><br/><div class="children"><div class="content">The 4k sliding window context seems like a controversial choice after Mistral 7B mostly failed at showing any benefits from it. What was the rationale behind that instead of just going for full 8k or 16k?</div><br/><div id="40812177" class="c"><input type="checkbox" id="c-40812177" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40811205">parent</a><span>|</span><a href="#40811113">next</a><span>|</span><label class="collapse" for="c-40812177">[-]</label><label class="expand" for="c-40812177">[1 more]</label></div><br/><div class="children"><div class="content">This is mostly about inference speed, while maintaining long context performance.</div><br/></div></div></div></div><div id="40811113" class="c"><input type="checkbox" id="c-40811113" checked=""/><div class="controls bullet"><span class="by">zerojames</span><span>|</span><a href="#40811067">parent</a><span>|</span><a href="#40811205">prev</a><span>|</span><a href="#40814127">next</a><span>|</span><label class="collapse" for="c-40811113">[-]</label><label class="expand" for="c-40811113">[2 more]</label></div><br/><div class="children"><div class="content">How is Gemma-2 licensed?</div><br/><div id="40811332" class="c"><input type="checkbox" id="c-40811332" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40811113">parent</a><span>|</span><a href="#40814127">next</a><span>|</span><label class="collapse" for="c-40811332">[-]</label><label class="expand" for="c-40811332">[1 more]</label></div><br/><div class="children"><div class="content">The terms of use remain the same as Gemma 1 - <a href="https:&#x2F;&#x2F;ai.google.dev&#x2F;gemma&#x2F;terms" rel="nofollow">https:&#x2F;&#x2F;ai.google.dev&#x2F;gemma&#x2F;terms</a>.</div><br/></div></div></div></div><div id="40814127" class="c"><input type="checkbox" id="c-40814127" checked=""/><div class="controls bullet"><span class="by">np_space</span><span>|</span><a href="#40811067">parent</a><span>|</span><a href="#40811113">prev</a><span>|</span><a href="#40818097">next</a><span>|</span><label class="collapse" for="c-40814127">[-]</label><label class="expand" for="c-40814127">[2 more]</label></div><br/><div class="children"><div class="content">Are Gemma-2 models available via API yet? Looks to me like it&#x27;s not yet on vertexai</div><br/><div id="40815377" class="c"><input type="checkbox" id="c-40815377" checked=""/><div class="controls bullet"><span class="by">zone411</span><span>|</span><a href="#40811067">root</a><span>|</span><a href="#40814127">parent</a><span>|</span><a href="#40818097">next</a><span>|</span><label class="collapse" for="c-40815377">[-]</label><label class="expand" for="c-40815377">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Soon&quot; 
<a href="https:&#x2F;&#x2F;x.com&#x2F;LechMazur&#x2F;status&#x2F;1806366744706998732" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;LechMazur&#x2F;status&#x2F;1806366744706998732</a></div><br/></div></div></div></div></div></div><div id="40818097" class="c"><input type="checkbox" id="c-40818097" checked=""/><div class="controls bullet"><span class="by">raffraffraff</span><span>|</span><a href="#40811067">prev</a><span>|</span><a href="#40811052">next</a><span>|</span><label class="collapse" for="c-40818097">[-]</label><label class="expand" for="c-40818097">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We use the same data filtering techniques as Gemma 1. Specifically, we filter the pre-
training dataset to reduce the risk of unwanted or unsafe utterances.<p>Hmmm. I&#x27;d love to know what qualifies as &quot;unsafe&quot;.</div><br/><div id="40818213" class="c"><input type="checkbox" id="c-40818213" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#40818097">parent</a><span>|</span><a href="#40811052">next</a><span>|</span><label class="collapse" for="c-40818213">[-]</label><label class="expand" for="c-40818213">[1 more]</label></div><br/><div class="children"><div class="content">It will refuse to describe the process of making napalm using only double entendres.</div><br/></div></div></div></div><div id="40811052" class="c"><input type="checkbox" id="c-40811052" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#40818097">prev</a><span>|</span><a href="#40810945">next</a><span>|</span><label class="collapse" for="c-40811052">[-]</label><label class="expand" for="c-40811052">[6 more]</label></div><br/><div class="children"><div class="content">Shouldn&#x27;t this (2.6B&#x2F;9B) be compared with Microsoft&#x27;s Phi-3 mini (3.8B) instead of Mistral and Llama-3?<p>(table 13 on page 7) vs <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2404.14219" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2404.14219</a> (page 6, quite better in general)<p>The report on knowledge distillation training is interesting, though.</div><br/><div id="40811292" class="c"><input type="checkbox" id="c-40811292" checked=""/><div class="controls bullet"><span class="by">philipkglass</span><span>|</span><a href="#40811052">parent</a><span>|</span><a href="#40812021">next</a><span>|</span><label class="collapse" for="c-40811292">[-]</label><label class="expand" for="c-40811292">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s such a wide range of model sizes that I could see why they compare with Llama 3 70b as well as Llama 3 8b (tables 12, 13). I agree that the Phi-3 series is a stronger competitor for knowledge extraction&#x2F;summarizing and would make a good comparison. My current favorite for such tasks, on a VRAM-limited workstation, is Phi-3 medium (phi3:14b-instruct).</div><br/></div></div><div id="40812021" class="c"><input type="checkbox" id="c-40812021" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40811052">parent</a><span>|</span><a href="#40811292">prev</a><span>|</span><a href="#40810945">next</a><span>|</span><label class="collapse" for="c-40812021">[-]</label><label class="expand" for="c-40812021">[3 more]</label></div><br/><div class="children"><div class="content">Picking up from there: The games in this paper and model are annoying.<p>The 2.6B would get stomped by Phi-3, so there&#x27;s no comparison.<p>Fair enough. 2.6B vs. 3.8B is a fairly substantial size difference thats hard to intuit when its 2.6 vs 3.8 versus 2,600,000,000 and 3,800,000,000.<p>But then we get what I&#x27;m going to &quot;parameter creep&quot;: Mistral 7B vs. Llama 8B vs. Gemma 9B. I worried after Llama 3 went 8B that we&#x27;d start seeing games with parameters, but, thought I was being silly.</div><br/><div id="40815441" class="c"><input type="checkbox" id="c-40815441" checked=""/><div class="controls bullet"><span class="by">kouteiheika</span><span>|</span><a href="#40811052">root</a><span>|</span><a href="#40812021">parent</a><span>|</span><a href="#40813323">next</a><span>|</span><label class="collapse" for="c-40815441">[-]</label><label class="expand" for="c-40815441">[1 more]</label></div><br/><div class="children"><div class="content">There was no parameter creep with Llama. Llama 8B is actually a ~7B model comparable to Mistral 7B if you strip away multilingual embeddings and match what Mistral 7B supports.</div><br/></div></div><div id="40813323" class="c"><input type="checkbox" id="c-40813323" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#40811052">root</a><span>|</span><a href="#40812021">parent</a><span>|</span><a href="#40815441">prev</a><span>|</span><a href="#40810945">next</a><span>|</span><label class="collapse" for="c-40813323">[-]</label><label class="expand" for="c-40813323">[1 more]</label></div><br/><div class="children"><div class="content">In the Llama 3 case I think the increase in parameters is mostly due to the input embeddings and output logits layers, reflecting the context size increase.</div><br/></div></div></div></div></div></div><div id="40810945" class="c"><input type="checkbox" id="c-40810945" checked=""/><div class="controls bullet"><span class="by">rsolva</span><span>|</span><a href="#40811052">prev</a><span>|</span><a href="#40813385">next</a><span>|</span><label class="collapse" for="c-40810945">[-]</label><label class="expand" for="c-40810945">[2 more]</label></div><br/><div class="children"><div class="content">The 9B and 27B versions are available for Ollama: <a href="https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;gemma2">https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;gemma2</a></div><br/><div id="40811855" class="c"><input type="checkbox" id="c-40811855" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#40810945">parent</a><span>|</span><a href="#40813385">next</a><span>|</span><label class="collapse" for="c-40811855">[-]</label><label class="expand" for="c-40811855">[1 more]</label></div><br/><div class="children"><div class="content">The 27B model is also available in AI studio<p><a href="https:&#x2F;&#x2F;aistudio.google.com&#x2F;app&#x2F;prompts&#x2F;new_chat?model=gemma-2-27b-it" rel="nofollow">https:&#x2F;&#x2F;aistudio.google.com&#x2F;app&#x2F;prompts&#x2F;new_chat?model=gemma...</a><p>So far it seems pretty strong for its size.</div><br/></div></div></div></div><div id="40813385" class="c"><input type="checkbox" id="c-40813385" checked=""/><div class="controls bullet"><span class="by">dongobread</span><span>|</span><a href="#40810945">prev</a><span>|</span><a href="#40811596">next</a><span>|</span><label class="collapse" for="c-40813385">[-]</label><label class="expand" for="c-40813385">[8 more]</label></div><br/><div class="children"><div class="content">The knowledge distillation is very interesting but generating trillions of outputs from a large teacher model seems insanely expensive. Is this really more cost efficient than just using that compute instead for training your model with more data&#x2F;more epochs?</div><br/><div id="40813492" class="c"><input type="checkbox" id="c-40813492" checked=""/><div class="controls bullet"><span class="by">DebtDeflation</span><span>|</span><a href="#40813385">parent</a><span>|</span><a href="#40813498">next</a><span>|</span><label class="collapse" for="c-40813492">[-]</label><label class="expand" for="c-40813492">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m also curious.  It seems like 6 months ago everyone was afraid of &quot;model collapse&quot; but now synthetic training generation and teacher models are all the rage.  Have we solved the problem of model collapse?</div><br/><div id="40814576" class="c"><input type="checkbox" id="c-40814576" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#40813385">root</a><span>|</span><a href="#40813492">parent</a><span>|</span><a href="#40815050">next</a><span>|</span><label class="collapse" for="c-40814576">[-]</label><label class="expand" for="c-40814576">[3 more]</label></div><br/><div class="children"><div class="content">Model collapse was basically a coping idea made up by artists who were hoping AI image generators would all magically destroy themselves at some point; I don&#x27;t think it was ever considered likely to happen.<p>It does seem to be true that clean data works better than low quality data.</div><br/><div id="40815285" class="c"><input type="checkbox" id="c-40815285" checked=""/><div class="controls bullet"><span class="by">groby_b</span><span>|</span><a href="#40813385">root</a><span>|</span><a href="#40814576">parent</a><span>|</span><a href="#40815050">next</a><span>|</span><label class="collapse" for="c-40815285">[-]</label><label class="expand" for="c-40815285">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re confusing it with data poisoning.<p>Model collapse itself is(was?) a fairly serious research topic: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.17493" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.17493</a><p>We&#x27;ve by now reached a &quot;probably not inevitable&quot; - <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.01413" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.01413</a> argues there&#x27;s a finite upper bound to error - but I&#x27;d also point out that that paper assumes training data cardinality increases with the number of training generations and is strictly accumulative.<p>To a first order, that means you better have a pre-2022 dataset to get started, and have archived it well.<p>but it&#x27;s probably fair to say current SOTA is still more or less &quot;it&#x27;s neither impossible nor inevitable&quot;.</div><br/><div id="40817972" class="c"><input type="checkbox" id="c-40817972" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#40813385">root</a><span>|</span><a href="#40815285">parent</a><span>|</span><a href="#40815050">next</a><span>|</span><label class="collapse" for="c-40817972">[-]</label><label class="expand" for="c-40817972">[1 more]</label></div><br/><div class="children"><div class="content">Oh, no, they definitely believe both are going to happen and ChatGPT is just going to stop working because it&#x27;ll see itself on the internet. It goes with the common belief that LLMs learn from what you type into them.<p>&gt; To a first order, that means you better have a pre-2022 dataset to get started, and have archived it well.<p>I think that will always be available, or at least, a dataset with the distribution you want will be available.</div><br/></div></div></div></div></div></div><div id="40815050" class="c"><input type="checkbox" id="c-40815050" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#40813385">root</a><span>|</span><a href="#40813492">parent</a><span>|</span><a href="#40814576">prev</a><span>|</span><a href="#40813498">next</a><span>|</span><label class="collapse" for="c-40815050">[-]</label><label class="expand" for="c-40815050">[2 more]</label></div><br/><div class="children"><div class="content">Pay attention because it&#x27;s only once you will get to watch humans learn they are nothing special in real time.</div><br/><div id="40817242" class="c"><input type="checkbox" id="c-40817242" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#40813385">root</a><span>|</span><a href="#40815050">parent</a><span>|</span><a href="#40813498">next</a><span>|</span><label class="collapse" for="c-40817242">[-]</label><label class="expand" for="c-40817242">[1 more]</label></div><br/><div class="children"><div class="content">Historically, similar things happened with heliocentrism and evolution, but I guess we weren&#x27;t there to see it.</div><br/></div></div></div></div></div></div><div id="40813498" class="c"><input type="checkbox" id="c-40813498" checked=""/><div class="controls bullet"><span class="by">agi_is_coming</span><span>|</span><a href="#40813385">parent</a><span>|</span><a href="#40813492">prev</a><span>|</span><a href="#40811596">next</a><span>|</span><label class="collapse" for="c-40813498">[-]</label><label class="expand" for="c-40813498">[1 more]</label></div><br/><div class="children"><div class="content">The distillation is done on-policy like RLHF -- the student model is generating the sequences and teacher is providing feedback in terms of logits.</div><br/></div></div></div></div><div id="40811596" class="c"><input type="checkbox" id="c-40811596" checked=""/><div class="controls bullet"><span class="by">chown</span><span>|</span><a href="#40813385">prev</a><span>|</span><a href="#40812524">next</a><span>|</span><label class="collapse" for="c-40811596">[-]</label><label class="expand" for="c-40811596">[7 more]</label></div><br/><div class="children"><div class="content">This is a great release! If you are looking to try it locally with a great interface, I am working on an app [1] and I just pushed an update to support Gemma2.<p>1: <a href="https:&#x2F;&#x2F;msty.app" rel="nofollow">https:&#x2F;&#x2F;msty.app</a></div><br/><div id="40816838" class="c"><input type="checkbox" id="c-40816838" checked=""/><div class="controls bullet"><span class="by">bayesianbot</span><span>|</span><a href="#40811596">parent</a><span>|</span><a href="#40812656">next</a><span>|</span><label class="collapse" for="c-40816838">[-]</label><label class="expand" for="c-40816838">[3 more]</label></div><br/><div class="children"><div class="content">Looks cool even though closed source makes me wary.<p>Trying to save Anthropic API key on Arch Linux doesn&#x27;t do anything and there&#x27;s a message &quot;If you&#x27;re experiencing problems saving API keys especially on Linux, contact Discord&quot;, if it&#x27;s so common problem maybe you should have a link with possible fixes? Adding another Discord server and searching for answers for a question that clearly has been asked often enough feels like quite a hurdle for testing it out.</div><br/><div id="40817932" class="c"><input type="checkbox" id="c-40817932" checked=""/><div class="controls bullet"><span class="by">aashu_dwivedi</span><span>|</span><a href="#40811596">root</a><span>|</span><a href="#40816838">parent</a><span>|</span><a href="#40812656">next</a><span>|</span><label class="collapse" for="c-40817932">[-]</label><label class="expand" for="c-40817932">[2 more]</label></div><br/><div class="children"><div class="content">What does closed source mean in this context? The weights are open and the model architecture has to be open for people to use it for inference.</div><br/><div id="40818071" class="c"><input type="checkbox" id="c-40818071" checked=""/><div class="controls bullet"><span class="by">onel</span><span>|</span><a href="#40811596">root</a><span>|</span><a href="#40817932">parent</a><span>|</span><a href="#40812656">next</a><span>|</span><label class="collapse" for="c-40818071">[-]</label><label class="expand" for="c-40818071">[1 more]</label></div><br/><div class="children"><div class="content">I think he was referring to Msty which is closed-source</div><br/></div></div></div></div></div></div><div id="40812656" class="c"><input type="checkbox" id="c-40812656" checked=""/><div class="controls bullet"><span class="by">tr3ntg</span><span>|</span><a href="#40811596">parent</a><span>|</span><a href="#40816838">prev</a><span>|</span><a href="#40816131">next</a><span>|</span><label class="collapse" for="c-40812656">[-]</label><label class="expand" for="c-40812656">[1 more]</label></div><br/><div class="children"><div class="content">Wow, msty looks really cool. I&#x27;ve bookmarked it to look into more later as a replacement for how I use a locally-hosted instance of LibreChat. It&#x27;d be a huge improvement to use local models rather than remote ones, for much of my queries.<p>That said, do you have a reason for keeping msty closed source rather than open? I read your FAQ for &quot;why should I trust msty&quot; and it feels lacking.<p>&gt; We are a small team of developers who are passionate about AI and privacy. We have worked on projects before that have been used by thousands of people such as this (I&#x27;ve never heard of Cleavr). There are real faces (real faces = Twitter account link?) behind the product. And come chat with us on our Discord server to know us better.<p>This is much, much better than having no attribution, but it&#x27;s miles away from being able to verify trust by reading the code. Would love to hear what your reasons against this are.<p>Still thinking about trying it out, anyway...</div><br/></div></div><div id="40816131" class="c"><input type="checkbox" id="c-40816131" checked=""/><div class="controls bullet"><span class="by">Alifatisk</span><span>|</span><a href="#40811596">parent</a><span>|</span><a href="#40812656">prev</a><span>|</span><a href="#40815497">next</a><span>|</span><label class="collapse" for="c-40816131">[-]</label><label class="expand" for="c-40816131">[1 more]</label></div><br/><div class="children"><div class="content">Any plans on adding this to Chocolatey for Windows download?</div><br/></div></div><div id="40815497" class="c"><input type="checkbox" id="c-40815497" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#40811596">parent</a><span>|</span><a href="#40816131">prev</a><span>|</span><a href="#40812524">next</a><span>|</span><label class="collapse" for="c-40815497">[-]</label><label class="expand" for="c-40815497">[1 more]</label></div><br/><div class="children"><div class="content">What the heck, this looks cool! How have I missed it. Gonna give it a whirl.</div><br/></div></div></div></div><div id="40812524" class="c"><input type="checkbox" id="c-40812524" checked=""/><div class="controls bullet"><span class="by">jakobov</span><span>|</span><a href="#40811596">prev</a><span>|</span><a href="#40811377">next</a><span>|</span><label class="collapse" for="c-40812524">[-]</label><label class="expand" for="c-40812524">[1 more]</label></div><br/><div class="children"><div class="content">How much faster (in terms of the number of iterations to a given performance) is training from distillation?</div><br/></div></div><div id="40811377" class="c"><input type="checkbox" id="c-40811377" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#40812524">prev</a><span>|</span><a href="#40811115">next</a><span>|</span><label class="collapse" for="c-40811377">[-]</label><label class="expand" for="c-40811377">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious about the use of explicit tokens like
 &lt;start_of_turn&gt;, 
 &lt;end_of_turn&gt;, 
 &lt;bos&gt;, and
 &lt;eos&gt;.
What happens if the user insert those in their message? Does that provide an easy way to &quot;ignore previous instructions&quot;?<p>Do I have to manually sanitize the input before I give it to the model?</div><br/><div id="40817821" class="c"><input type="checkbox" id="c-40817821" checked=""/><div class="controls bullet"><span class="by">richdougherty</span><span>|</span><a href="#40811377">parent</a><span>|</span><a href="#40811115">next</a><span>|</span><label class="collapse" for="c-40817821">[-]</label><label class="expand" for="c-40817821">[1 more]</label></div><br/><div class="children"><div class="content">If you have control of the tokenizer you could make sure it doesn&#x27;t produce these tokens on user input. I.e. instead of the special &quot;&lt;eos&gt;&quot; token, produce something like &quot;&lt;&quot;, &quot;eos&quot;, &quot;&gt;&quot; - whatever the &#x27;natural&#x27; encoding of that string is.<p>See for example, the llama3 tokenizer has options to control special token tokenization:<p>Tokenization method with args to control special token handling: <a href="https:&#x2F;&#x2F;github.com&#x2F;meta-llama&#x2F;llama3&#x2F;blob&#x2F;bf8d18cd087a4a0b3f61075b7de0b86cf6c70697&#x2F;llama&#x2F;tokenizer.py#L99-L128">https:&#x2F;&#x2F;github.com&#x2F;meta-llama&#x2F;llama3&#x2F;blob&#x2F;bf8d18cd087a4a0b3f...</a><p>And you can see how it is used combined with special tokens and user input here: <a href="https:&#x2F;&#x2F;github.com&#x2F;meta-llama&#x2F;llama3&#x2F;blob&#x2F;bf8d18cd087a4a0b3f61075b7de0b86cf6c70697&#x2F;llama&#x2F;tokenizer.py#L202-L229">https:&#x2F;&#x2F;github.com&#x2F;meta-llama&#x2F;llama3&#x2F;blob&#x2F;bf8d18cd087a4a0b3f...</a><p>If you don&#x27;t have control of the tokenizer, I guess it needs to be sanitized in the input like you say.</div><br/></div></div></div></div><div id="40811115" class="c"><input type="checkbox" id="c-40811115" checked=""/><div class="controls bullet"><span class="by">iamronaldo</span><span>|</span><a href="#40811377">prev</a><span>|</span><a href="#40811311">next</a><span>|</span><label class="collapse" for="c-40811115">[-]</label><label class="expand" for="c-40811115">[14 more]</label></div><br/><div class="children"><div class="content">So it&#x27;s twice the size of phi 3 and considerably worse? What am I missing</div><br/><div id="40811248" class="c"><input type="checkbox" id="c-40811248" checked=""/><div class="controls bullet"><span class="by">ertgbnm</span><span>|</span><a href="#40811115">parent</a><span>|</span><a href="#40813352">next</a><span>|</span><label class="collapse" for="c-40811248">[-]</label><label class="expand" for="c-40811248">[1 more]</label></div><br/><div class="children"><div class="content">They used two non-mutually exclusive techniques. Phi-3 is mostly a curriculum training breakthrough. By filtering training set for high quality tokens and training on synthetic data, they were able to achieve great results. Gemma-2 is a distillation breakthrough. By training LLMs with guidance from larger teacher LLMs, they were able to achieve great results too.<p>Porque no los dos?</div><br/></div></div><div id="40813352" class="c"><input type="checkbox" id="c-40813352" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#40811115">parent</a><span>|</span><a href="#40811248">prev</a><span>|</span><a href="#40811591">next</a><span>|</span><label class="collapse" for="c-40813352">[-]</label><label class="expand" for="c-40813352">[3 more]</label></div><br/><div class="children"><div class="content">Phi-3 does well in benchmarks but underperforms IRL; for example, Phi-3-Medium gets beaten badly by Llama-3-8b on the LMSYS Chatbot Arena despite doing better on benchmarks.<p>Gemma&#x27;s performance if anything seems understated on benchmarks: the 27b is currently ahead of Llama3-70b on the Chatbot Arena leaderboard.</div><br/><div id="40815252" class="c"><input type="checkbox" id="c-40815252" checked=""/><div class="controls bullet"><span class="by">ertgbnm</span><span>|</span><a href="#40811115">root</a><span>|</span><a href="#40813352">parent</a><span>|</span><a href="#40811591">next</a><span>|</span><label class="collapse" for="c-40815252">[-]</label><label class="expand" for="c-40815252">[2 more]</label></div><br/><div class="children"><div class="content">I suspect Phi-3 is not robust to normal human input like typos and strange grammar since it&#x27;s only trained on filtered &quot;high quality&quot; tokens and synthetic data. Since it doesn&#x27;t need to waste a ton of parameters learning how to error correct input, it&#x27;s much smarter on well curated benchmarks compared to its weight class. However, it can&#x27;t operate out of distribution at all.</div><br/><div id="40816137" class="c"><input type="checkbox" id="c-40816137" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#40811115">root</a><span>|</span><a href="#40815252">parent</a><span>|</span><a href="#40811591">next</a><span>|</span><label class="collapse" for="c-40816137">[-]</label><label class="expand" for="c-40816137">[1 more]</label></div><br/><div class="children"><div class="content">Personally vibe checking Phi-3-Medium is worse in my experience, no matter how well you spell — it just isn&#x27;t good at all compared to Llama3-8b, despite being significantly larger in param count. I suspect the &quot;high quality tokens&quot; were &quot;high quality&quot; in the sense that they resembled tokens one might encounter in benchmarks, and not &quot;high quality&quot; in the sense of representing human-like input&#x2F;output.</div><br/></div></div></div></div></div></div><div id="40811591" class="c"><input type="checkbox" id="c-40811591" checked=""/><div class="controls bullet"><span class="by">ferretj</span><span>|</span><a href="#40811115">parent</a><span>|</span><a href="#40813352">prev</a><span>|</span><a href="#40811570">next</a><span>|</span><label class="collapse" for="c-40811591">[-]</label><label class="expand" for="c-40811591">[1 more]</label></div><br/><div class="children"><div class="content">Another take on this: phi-3 small has 1100 ELO on LMSYS (ranked #52) while the confidence interval for Gemma 2 9B is [1170, 1200] ELO (ranked btw #15 and #25).</div><br/></div></div><div id="40811570" class="c"><input type="checkbox" id="c-40811570" checked=""/><div class="controls bullet"><span class="by">floridianfisher</span><span>|</span><a href="#40811115">parent</a><span>|</span><a href="#40811591">prev</a><span>|</span><a href="#40811447">next</a><span>|</span><label class="collapse" for="c-40811570">[-]</label><label class="expand" for="c-40811570">[2 more]</label></div><br/><div class="children"><div class="content">Why not try it here and make your comparisons that way? <a href="https:&#x2F;&#x2F;aistudio.google.com&#x2F;app&#x2F;prompts&#x2F;new_chat?model=gemma-2-27b-it" rel="nofollow">https:&#x2F;&#x2F;aistudio.google.com&#x2F;app&#x2F;prompts&#x2F;new_chat?model=gemma...</a></div><br/><div id="40813778" class="c"><input type="checkbox" id="c-40813778" checked=""/><div class="controls bullet"><span class="by">pona-a</span><span>|</span><a href="#40811115">root</a><span>|</span><a href="#40811570">parent</a><span>|</span><a href="#40811447">next</a><span>|</span><label class="collapse" for="c-40813778">[-]</label><label class="expand" for="c-40813778">[1 more]</label></div><br/><div class="children"><div class="content">One compelling reason not to would be a region block... [0]<p><a href="https:&#x2F;&#x2F;ai.google.dev&#x2F;gemini-api&#x2F;docs&#x2F;available-regions" rel="nofollow">https:&#x2F;&#x2F;ai.google.dev&#x2F;gemini-api&#x2F;docs&#x2F;available-regions</a></div><br/></div></div></div></div><div id="40811447" class="c"><input type="checkbox" id="c-40811447" checked=""/><div class="controls bullet"><span class="by">azeirah</span><span>|</span><a href="#40811115">parent</a><span>|</span><a href="#40811570">prev</a><span>|</span><a href="#40811373">next</a><span>|</span><label class="collapse" for="c-40811447">[-]</label><label class="expand" for="c-40811447">[1 more]</label></div><br/><div class="children"><div class="content">Have you tried Phi 3? It&#x27;s smart which makes it perform well on benchmarks, but it&#x27;s not great at conversation or as a chatbot.<p>I imagine Gemma 2 is a better general-purpose assistant for most people, whereas Phi 3 is a solid small LLM (SLM?) for more specific use-cases like summarization, RAG, learning about math and stuff.</div><br/></div></div><div id="40811373" class="c"><input type="checkbox" id="c-40811373" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#40811115">parent</a><span>|</span><a href="#40811447">prev</a><span>|</span><a href="#40811311">next</a><span>|</span><label class="collapse" for="c-40811373">[-]</label><label class="expand" for="c-40811373">[5 more]</label></div><br/><div class="children"><div class="content">Worse in some aspects, better in other.<p>Small models are never going to be generalists, so having several small models allows you to pick the one that best fits your needs.</div><br/><div id="40812279" class="c"><input type="checkbox" id="c-40812279" checked=""/><div class="controls bullet"><span class="by">k__</span><span>|</span><a href="#40811115">root</a><span>|</span><a href="#40811373">parent</a><span>|</span><a href="#40811311">next</a><span>|</span><label class="collapse" for="c-40812279">[-]</label><label class="expand" for="c-40812279">[4 more]</label></div><br/><div class="children"><div class="content">When would you use which?</div><br/><div id="40815110" class="c"><input type="checkbox" id="c-40815110" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#40811115">root</a><span>|</span><a href="#40812279">parent</a><span>|</span><a href="#40813353">next</a><span>|</span><label class="collapse" for="c-40815110">[-]</label><label class="expand" for="c-40815110">[1 more]</label></div><br/><div class="children"><div class="content">Whichever model works better for your use. It&#x27;s hard to know without testing it at the moment.<p>I&#x27;ve found Gemini to be better at some use-cases, and GPT-4 better at others for my specific taste and use-case. You can kind of go by the benchmark scores to have an idea if it&#x27;s good at logic, creativity, etc.</div><br/></div></div><div id="40813353" class="c"><input type="checkbox" id="c-40813353" checked=""/><div class="controls bullet"><span class="by">Aerbil313</span><span>|</span><a href="#40811115">root</a><span>|</span><a href="#40812279">parent</a><span>|</span><a href="#40815110">prev</a><span>|</span><a href="#40811311">next</a><span>|</span><label class="collapse" for="c-40813353">[-]</label><label class="expand" for="c-40813353">[2 more]</label></div><br/><div class="children"><div class="content">Obviously another small model would be specialized in determining that.</div><br/><div id="40813616" class="c"><input type="checkbox" id="c-40813616" checked=""/><div class="controls bullet"><span class="by">k__</span><span>|</span><a href="#40811115">root</a><span>|</span><a href="#40813353">parent</a><span>|</span><a href="#40811311">next</a><span>|</span><label class="collapse" for="c-40813616">[-]</label><label class="expand" for="c-40813616">[1 more]</label></div><br/><div class="children"><div class="content">Is it models all the way down?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40811311" class="c"><input type="checkbox" id="c-40811311" checked=""/><div class="controls bullet"><span class="by">mixtureoftakes</span><span>|</span><a href="#40811115">prev</a><span>|</span><a href="#40811637">next</a><span>|</span><label class="collapse" for="c-40811311">[-]</label><label class="expand" for="c-40811311">[1 more]</label></div><br/><div class="children"><div class="content">Good realease but the annoying part is they&#x27;re very unclear about which types of models they are comparing. 
They provide benchmark comparisons for the base models only and arena comparisons for instruct only? 
Was that intentional? 
Why would you ever do that? 
This makes things unnecessary complicated imo and the only payoff is a short term win for google on paper.<p>Guess I&#x27;ll just fully test it for my own tasks to know for sure</div><br/></div></div><div id="40811637" class="c"><input type="checkbox" id="c-40811637" checked=""/><div class="controls bullet"><span class="by">jakobov</span><span>|</span><a href="#40811311">prev</a><span>|</span><a href="#40816151">next</a><span>|</span><label class="collapse" for="c-40811637">[-]</label><label class="expand" for="c-40811637">[4 more]</label></div><br/><div class="children"><div class="content">Nice! Can you explain what you mean by &quot;simulate training beyond the number of available tokens&quot;?<p>Why does using distillation from a larger model simulate training with more tokens?</div><br/><div id="40812031" class="c"><input type="checkbox" id="c-40812031" checked=""/><div class="controls bullet"><span class="by">suryabhupa</span><span>|</span><a href="#40811637">parent</a><span>|</span><a href="#40811684">next</a><span>|</span><label class="collapse" for="c-40812031">[-]</label><label class="expand" for="c-40812031">[2 more]</label></div><br/><div class="children"><div class="content">Surya here from the core Gemma team -- we can think of a distillation loss as learning to model the entire distribution of tokens that are likely to follow the prefix thus far, instead of only the token in the training example. If you do some back of the envelope calculations, we can see that learning to model a larger distribution yields many more bits of information to learn from.</div><br/><div id="40812470" class="c"><input type="checkbox" id="c-40812470" checked=""/><div class="controls bullet"><span class="by">jakobov</span><span>|</span><a href="#40811637">root</a><span>|</span><a href="#40812031">parent</a><span>|</span><a href="#40811684">next</a><span>|</span><label class="collapse" for="c-40812470">[-]</label><label class="expand" for="c-40812470">[1 more]</label></div><br/><div class="children"><div class="content">Gotcha. That makes sense. Thanks!<p>What are the theories as to why this works better than training on a larger quantity of non-simulated tokens?<p>Is it because the gradient from the non-simulated tokens is too noisy for a small model to model correctly?</div><br/></div></div></div></div><div id="40811684" class="c"><input type="checkbox" id="c-40811684" checked=""/><div class="controls bullet"><span class="by">canyon289</span><span>|</span><a href="#40811637">parent</a><span>|</span><a href="#40812031">prev</a><span>|</span><a href="#40816151">next</a><span>|</span><label class="collapse" for="c-40811684">[-]</label><label class="expand" for="c-40811684">[1 more]</label></div><br/><div class="children"><div class="content">Hi, I work on the Gemma team (same as Alek opinions are my own).<p>Essentially instead of tokens that are &quot;already there&quot; in text, the distillation allows us to simulate training data from a larger model</div><br/></div></div></div></div><div id="40816151" class="c"><input type="checkbox" id="c-40816151" checked=""/><div class="controls bullet"><span class="by">xkgt</span><span>|</span><a href="#40811637">prev</a><span>|</span><a href="#40811359">next</a><span>|</span><label class="collapse" for="c-40816151">[-]</label><label class="expand" for="c-40816151">[1 more]</label></div><br/><div class="children"><div class="content">Do we know if Gemma models are fundamentally different from the ones hosted as Gemini? Gemini 1.5 flash seems to produce good results for the price and performance.</div><br/></div></div><div id="40811359" class="c"><input type="checkbox" id="c-40811359" checked=""/><div class="controls bullet"><span class="by">Flumio</span><span>|</span><a href="#40816151">prev</a><span>|</span><a href="#40811673">next</a><span>|</span><label class="collapse" for="c-40811359">[-]</label><label class="expand" for="c-40811359">[1 more]</label></div><br/><div class="children"><div class="content">This is great:)<p>And when we continue fine-tune.how much and what type of data we learn it on, I&#x27;m pretty sure for a smart agent who is not a knowledgeable expert but primarily a agent (understand what and how) this will get smaller and easier to run everywhere.</div><br/></div></div><div id="40811500" class="c"><input type="checkbox" id="c-40811500" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#40811673">prev</a><span>|</span><a href="#40814838">next</a><span>|</span><label class="collapse" for="c-40811500">[-]</label><label class="expand" for="c-40811500">[5 more]</label></div><br/><div class="children"><div class="content">Are these small Gemma 2 distilled models available anywhere? I&#x27;m not finding them on huggingface.co, etc. but maybe I don&#x27;t know the exact model names they are published.<p>Are the weights released yet?</div><br/><div id="40811528" class="c"><input type="checkbox" id="c-40811528" checked=""/><div class="controls bullet"><span class="by">floridianfisher</span><span>|</span><a href="#40811500">parent</a><span>|</span><a href="#40811554">next</a><span>|</span><label class="collapse" for="c-40811528">[-]</label><label class="expand" for="c-40811528">[1 more]</label></div><br/><div class="children"><div class="content">The huggingface weights are here: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;collections&#x2F;google&#x2F;gemma-2-release-667d6600fd5220e7b967f315" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;collections&#x2F;google&#x2F;gemma-2-release-66...</a></div><br/></div></div><div id="40811554" class="c"><input type="checkbox" id="c-40811554" checked=""/><div class="controls bullet"><span class="by">mchiang</span><span>|</span><a href="#40811500">parent</a><span>|</span><a href="#40811528">prev</a><span>|</span><a href="#40812158">next</a><span>|</span><label class="collapse" for="c-40811554">[-]</label><label class="expand" for="c-40811554">[1 more]</label></div><br/><div class="children"><div class="content">They are available on Hugging Face: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;collections&#x2F;google&#x2F;gemma-2-release-667d6600fd5220e7b967f315" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;collections&#x2F;google&#x2F;gemma-2-release-66...</a><p>Ollama:
<a href="https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;gemma2">https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;gemma2</a></div><br/></div></div><div id="40812158" class="c"><input type="checkbox" id="c-40812158" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#40811500">parent</a><span>|</span><a href="#40811554">prev</a><span>|</span><a href="#40814838">next</a><span>|</span><label class="collapse" for="c-40812158">[-]</label><label class="expand" for="c-40812158">[2 more]</label></div><br/><div class="children"><div class="content">In addition to the HF links shared by sibling comments, the 2B will be released soon.</div><br/><div id="40814456" class="c"><input type="checkbox" id="c-40814456" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#40811500">root</a><span>|</span><a href="#40812158">parent</a><span>|</span><a href="#40814838">next</a><span>|</span><label class="collapse" for="c-40814456">[-]</label><label class="expand" for="c-40814456">[1 more]</label></div><br/><div class="children"><div class="content">that&#x27;s actually the particular one I was looking for and couldn&#x27;t find. Also had googled for the other ones but maybe it was so recent that it hadn&#x27;t been indexed. Thanks!</div><br/></div></div></div></div></div></div><div id="40814838" class="c"><input type="checkbox" id="c-40814838" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#40811500">prev</a><span>|</span><a href="#40811819">next</a><span>|</span><label class="collapse" for="c-40814838">[-]</label><label class="expand" for="c-40814838">[1 more]</label></div><br/><div class="children"><div class="content">It has a tiny context window of 8k, that thing will have the memory of a goldfish.</div><br/></div></div><div id="40811819" class="c"><input type="checkbox" id="c-40811819" checked=""/><div class="controls bullet"><span class="by">rosslazer</span><span>|</span><a href="#40814838">prev</a><span>|</span><a href="#40811520">next</a><span>|</span><label class="collapse" for="c-40811819">[-]</label><label class="expand" for="c-40811819">[1 more]</label></div><br/><div class="children"><div class="content">Are there examples of the prompt or transcripts for the human testing?</div><br/></div></div><div id="40811520" class="c"><input type="checkbox" id="c-40811520" checked=""/><div class="controls bullet"><span class="by">QuesnayJr</span><span>|</span><a href="#40811819">prev</a><span>|</span><a href="#40814780">next</a><span>|</span><label class="collapse" for="c-40811520">[-]</label><label class="expand" for="c-40811520">[2 more]</label></div><br/><div class="children"><div class="content">There are two new chatbots on Chatbot Arena, called &quot;late-june-chatbot&quot; and &quot;im-just-another-late-june-chatbot&quot;.  Both of them report that they are Gemma if you ask.  I&#x27;m assuming it&#x27;s these two models, but AFAIK there has been no official announcement.</div><br/><div id="40812044" class="c"><input type="checkbox" id="c-40812044" checked=""/><div class="controls bullet"><span class="by">suryabhupa</span><span>|</span><a href="#40811520">parent</a><span>|</span><a href="#40814780">next</a><span>|</span><label class="collapse" for="c-40812044">[-]</label><label class="expand" for="c-40812044">[1 more]</label></div><br/><div class="children"><div class="content">The announcements are live on Twitter! See this for example: <a href="https:&#x2F;&#x2F;x.com&#x2F;suryabhupa&#x2F;status&#x2F;1806342617191379167" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;suryabhupa&#x2F;status&#x2F;1806342617191379167</a></div><br/></div></div></div></div><div id="40814780" class="c"><input type="checkbox" id="c-40814780" checked=""/><div class="controls bullet"><span class="by">mistercheph</span><span>|</span><a href="#40811520">prev</a><span>|</span><a href="#40810959">next</a><span>|</span><label class="collapse" for="c-40814780">[-]</label><label class="expand" for="c-40814780">[1 more]</label></div><br/><div class="children"><div class="content">Playing with it, and I like how much I can influence it with a system prompt, llama3 reacts pretty mildly to any system prompts I&#x27;ve tried.</div><br/></div></div><div id="40810959" class="c"><input type="checkbox" id="c-40810959" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#40814780">prev</a><span>|</span><a href="#40811238">next</a><span>|</span><label class="collapse" for="c-40810959">[-]</label><label class="expand" for="c-40810959">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Table 4 | Relevant formatting control tokens used for Gemma models<p>&gt; User turn: user<p>&gt; Model turn: model<p>&gt; Start of conversation turn: &lt;start_of_turn&gt;<p>&gt; End of conversation turn: &lt;end_of_turn&gt;<p>&gt; Beginning of sequence: &lt;bos&gt;<p>&gt; End of sequence: &lt;eos&gt;<p>You know I keep wondering why &lt;bos&gt; and &lt;eos&gt; tokens are even a thing in general. No model is tuned to keep generating multiple turns after its &lt;end_of_turn&gt; equivalent is sent, and what&#x27;s the point of &lt;bos&gt; when you&#x27;re parsing the entire context anyway. If it&#x27;s an attempt to ignore text before it... then why is that text there? Just remove it from context, you&#x27;re throwing away compute.</div><br/><div id="40811310" class="c"><input type="checkbox" id="c-40811310" checked=""/><div class="controls bullet"><span class="by">alekandreev</span><span>|</span><a href="#40810959">parent</a><span>|</span><a href="#40811050">next</a><span>|</span><label class="collapse" for="c-40811310">[-]</label><label class="expand" for="c-40811310">[4 more]</label></div><br/><div class="children"><div class="content">Your training input has the shape of (sequence length x batch size). If a lot of your samples are shorter than sequence length, as is usually the case, you will have a lot of padding tokens in the input, which is wasted compute.<p>To compensate for that, you can pack multiple examples in the same sequence. This is there EOS and BOS come in, as they indicate to the model that the two parts of the sequence are not related.</div><br/><div id="40811559" class="c"><input type="checkbox" id="c-40811559" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#40810959">root</a><span>|</span><a href="#40811310">parent</a><span>|</span><a href="#40811050">next</a><span>|</span><label class="collapse" for="c-40811559">[-]</label><label class="expand" for="c-40811559">[3 more]</label></div><br/><div class="children"><div class="content">You can just do that my shaping the attention mask, no?
That also gives you an actual guarantee that no information is leaked between conversations.</div><br/><div id="40812058" class="c"><input type="checkbox" id="c-40812058" checked=""/><div class="controls bullet"><span class="by">suryabhupa</span><span>|</span><a href="#40810959">root</a><span>|</span><a href="#40811559">parent</a><span>|</span><a href="#40814963">next</a><span>|</span><label class="collapse" for="c-40812058">[-]</label><label class="expand" for="c-40812058">[1 more]</label></div><br/><div class="children"><div class="content">In practice, and at scale, that&#x27;s exactly what having &lt;bos&gt; and &lt;eos&gt; tokens allow you to easily and programmatically do.</div><br/></div></div><div id="40814963" class="c"><input type="checkbox" id="c-40814963" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#40810959">root</a><span>|</span><a href="#40811559">parent</a><span>|</span><a href="#40812058">prev</a><span>|</span><a href="#40811050">next</a><span>|</span><label class="collapse" for="c-40814963">[-]</label><label class="expand" for="c-40814963">[1 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t pack multiple examples into a single row of a matrix without knowing where one begins and one ends.</div><br/></div></div></div></div></div></div><div id="40811050" class="c"><input type="checkbox" id="c-40811050" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#40810959">parent</a><span>|</span><a href="#40811310">prev</a><span>|</span><a href="#40811238">next</a><span>|</span><label class="collapse" for="c-40811050">[-]</label><label class="expand" for="c-40811050">[3 more]</label></div><br/><div class="children"><div class="content">think about training.</div><br/><div id="40811269" class="c"><input type="checkbox" id="c-40811269" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#40810959">root</a><span>|</span><a href="#40811050">parent</a><span>|</span><a href="#40811238">next</a><span>|</span><label class="collapse" for="c-40811269">[-]</label><label class="expand" for="c-40811269">[2 more]</label></div><br/><div class="children"><div class="content">I suppose it would act as a concrete separator when instruct tuning, but lots of prompt templates don&#x27;t use it, especially older ones like Alpaca. Maybe it leads to more overall coherence?</div><br/><div id="40811360" class="c"><input type="checkbox" id="c-40811360" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#40810959">root</a><span>|</span><a href="#40811269">parent</a><span>|</span><a href="#40811238">next</a><span>|</span><label class="collapse" for="c-40811360">[-]</label><label class="expand" for="c-40811360">[1 more]</label></div><br/><div class="children"><div class="content">Not instruct tuning, you use it in general training.<p>If you have a bunch of small prompts&#x2F;answers, you can fit them into bigger batches if you use start&#x2F;stop tokens.</div><br/></div></div></div></div></div></div></div></div><div id="40811238" class="c"><input type="checkbox" id="c-40811238" checked=""/><div class="controls bullet"><span class="by">msoad</span><span>|</span><a href="#40810959">prev</a><span>|</span><a href="#40811082">next</a><span>|</span><label class="collapse" for="c-40811238">[-]</label><label class="expand" for="c-40811238">[5 more]</label></div><br/><div class="children"><div class="content">Phi-3 blow this out of the water.<p><pre><code>                      Benchmark  |  Gemma 2 (9B)  |  Phi-3 Small (7B)
    -----------------------------|----------------|-------------------
                  MMLU (5-Shot)  |       63.6     |       75.7
             HellaSwag (5-Shot)  |       49.8     |       77.0
                  ANLI (7-Shot)  |       48.7     |       58.1
           GSM-8K (8-Shot; CoT)  |       59.8     |       89.6
                 MedQA (2-Shot)  |       49.6     |       65.4
               AGIEval (0-Shot)  |       42.1     |       45.1
              TriviaQA (5-Shot)  |       72.3     |       58.1
                Arc-C (10-Shot)  |       78.3     |       90.7
                Arc-E (10-Shot)  |       91.4     |       97.0
                  PIQA (5-Shot)  |       78.1     |       86.9
                SociQA (5-Shot)  |       65.5     |       79.2
    BigBench-Hard (3-Shot; CoT)  |       59.6     |       79.1
            WinoGrande (5-Shot)  |       55.6     |       81.5
           OpenBookQA (10-Shot)  |       78.6     |       88.0
                 BoolQ (2-Shot)  |       66.0     |       84.8
        CommonSenseQA (10-Shot)  |       76.2     |       80.0
      TruthfulQA (10-Shot; MC2)  |       52.1     |       70.2
             HumanEval (0-Shot)  |       34.1     |       61.0
                  MBPP (3-Shot)  |       51.5     |       71.7</code></pre></div><br/><div id="40811441" class="c"><input type="checkbox" id="c-40811441" checked=""/><div class="controls bullet"><span class="by">ferretj</span><span>|</span><a href="#40811238">parent</a><span>|</span><a href="#40811801">next</a><span>|</span><label class="collapse" for="c-40811441">[-]</label><label class="expand" for="c-40811441">[1 more]</label></div><br/><div class="children"><div class="content">Another take on this: phi-3 small has 1100 ELO on LMSYS (ranked #52) while the confidence interval for Gemma 2 9B is [1170, 1200] ELO (ranked btw #15 and #25).</div><br/></div></div><div id="40811801" class="c"><input type="checkbox" id="c-40811801" checked=""/><div class="controls bullet"><span class="by">Garcia98</span><span>|</span><a href="#40811238">parent</a><span>|</span><a href="#40811441">prev</a><span>|</span><a href="#40811425">next</a><span>|</span><label class="collapse" for="c-40811801">[-]</label><label class="expand" for="c-40811801">[1 more]</label></div><br/><div class="children"><div class="content">Pretraining on the Test Set Is All You Need<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.08632" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2309.08632</a></div><br/></div></div><div id="40811425" class="c"><input type="checkbox" id="c-40811425" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#40811238">parent</a><span>|</span><a href="#40811801">prev</a><span>|</span><a href="#40811473">next</a><span>|</span><label class="collapse" for="c-40811425">[-]</label><label class="expand" for="c-40811425">[1 more]</label></div><br/><div class="children"><div class="content">Phi is notorious for benchmark overfitting. It&#x27;s good, but not as good as it looks on the charts. On the Lmsys leaderboard it places a whole 23 spots behind Llama-3-8B which it also claims to soundly beat on the above. So YMMV.</div><br/></div></div></div></div><div id="40811082" class="c"><input type="checkbox" id="c-40811082" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#40811238">prev</a><span>|</span><label class="collapse" for="c-40811082">[-]</label><label class="expand" for="c-40811082">[15 more]</label></div><br/><div class="children"><div class="content">I gave up hope on r&quot;Gem[ma|ini]&quot; long time ago. I don&#x27;t believe that Google can&#x27;t produce good LLMs because of its massive company size; Microsoft is also a giant company (more market cap than Google) but it keeps surprising us with the ϕ models.<p>I think Google just lacks the vision to understand what makes a good LLM. Theoretical contributions by research teams are valuable, but the real-world is built around engineering ideas that may lack the &quot;purity&quot; and elegance of theory but damn it they work.</div><br/><div id="40812705" class="c"><input type="checkbox" id="c-40812705" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#40811082">parent</a><span>|</span><a href="#40811165">next</a><span>|</span><label class="collapse" for="c-40812705">[-]</label><label class="expand" for="c-40812705">[3 more]</label></div><br/><div class="children"><div class="content">&gt;long time ago<p>This is an incredible statement to make about a field that no one was talking about 24 months ago, a family of SOTA models that didn&#x27;t exist until 8 months ago, and a family of small local models that didn&#x27;t exist 6 months ago. But sure, give up hope after the first generation of a model family doesn&#x27;t impress you.<p>People seem to forget how incredibly early we are in this whole thing. The fact that so much progress has been made in such a short amount of time should make everyone super excited!</div><br/><div id="40812809" class="c"><input type="checkbox" id="c-40812809" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#40811082">root</a><span>|</span><a href="#40812705">parent</a><span>|</span><a href="#40811165">next</a><span>|</span><label class="collapse" for="c-40812809">[-]</label><label class="expand" for="c-40812809">[2 more]</label></div><br/><div class="children"><div class="content">To be fair, LLMs (especially <i>Google</i> LLMs) aren&#x27;t merely 24 months old. This is part of a long line of models that draw their heritage from BERT and t5-flan. Google has been at this longer than most, <i>particularly</i> in the field of edge-compute models. This isn&#x27;t even close to a first-generation model family.<p>That&#x27;s not to say this is an insignificant contribution. New models are great, especially when released for free, and it&#x27;s important for big firms to keep the ball rolling for tech to progress. Though there is also legitimate concern that <i>all</i> LLMs aren&#x27;t improving as fast as they used to improve, and we may have hit the proverbial bathtub curve of AI progress.</div><br/><div id="40813023" class="c"><input type="checkbox" id="c-40813023" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#40811082">root</a><span>|</span><a href="#40812809">parent</a><span>|</span><a href="#40811165">next</a><span>|</span><label class="collapse" for="c-40813023">[-]</label><label class="expand" for="c-40813023">[1 more]</label></div><br/><div class="children"><div class="content">I think there is valid criticism of google for inventing a cool technology only to have the rest of the industry discover its usefulness before them. But to say Gemini 1.0 or OG Gemma aren&#x27;t first generation models because BERT and flan existed before is like saying the iPad wasn&#x27;t a first generation device because Apple made the Newton. Like sure, they&#x27;re the same in that they&#x27;re transformers trained on language and text, but these are new families of models. The training mechanisms are different, their architectures are different, the data sets are different, the intended purpose of the models are completely different, etc. At some point I guess it&#x27;s a semantic difference, maybe.</div><br/></div></div></div></div></div></div><div id="40811165" class="c"><input type="checkbox" id="c-40811165" checked=""/><div class="controls bullet"><span class="by">johnfn</span><span>|</span><a href="#40811082">parent</a><span>|</span><a href="#40812705">prev</a><span>|</span><a href="#40811362">next</a><span>|</span><label class="collapse" for="c-40811165">[-]</label><label class="expand" for="c-40811165">[2 more]</label></div><br/><div class="children"><div class="content">Maybe you gave up before Google released Gemini Advanced? This viewpoint seemed more accurate before it was related, but Gemini Advanced is the third best LLM as rated here [1]. In fact, had second place until a few days ago when Claude 3.5 came out.<p>[1]: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;lmsys&#x2F;chatbot-arena-leaderboard" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;lmsys&#x2F;chatbot-arena-leaderboar...</a></div><br/><div id="40814291" class="c"><input type="checkbox" id="c-40814291" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#40811082">root</a><span>|</span><a href="#40811165">parent</a><span>|</span><a href="#40811362">next</a><span>|</span><label class="collapse" for="c-40814291">[-]</label><label class="expand" for="c-40814291">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t Gemini Advanced Gemini Pro attached to some sort of an internet search program?   If it has that advantage over other models it isn&#x27;t a sign of AI chops.</div><br/></div></div></div></div><div id="40811362" class="c"><input type="checkbox" id="c-40811362" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#40811082">parent</a><span>|</span><a href="#40811165">prev</a><span>|</span><a href="#40811191">next</a><span>|</span><label class="collapse" for="c-40811362">[-]</label><label class="expand" for="c-40811362">[1 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t speak to Gemma, but I found 1.5 superior to Claude and ChatGPT 4 when it came out. The trend seems to be each taking the lead when it comes out, being king of the hill for a couple weeks, and then being surpassed by the next.<p>Claude&#x27;s reign has begun, and I&#x27;d say it has a solid enough lead for at least another two weeks of dominance before it&#x27;s dethroned.</div><br/></div></div><div id="40811191" class="c"><input type="checkbox" id="c-40811191" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#40811082">parent</a><span>|</span><a href="#40811362">prev</a><span>|</span><a href="#40811189">next</a><span>|</span><label class="collapse" for="c-40811191">[-]</label><label class="expand" for="c-40811191">[6 more]</label></div><br/><div class="children"><div class="content">I wonder if Google is making Deepmind people switch from their cool original research to doing LLMs like everybody else. Having their scale in money and data, I would hire new teams of <i>engineers</i> who want to do LLMs and leave the Deepmind <i>researchers</i> do their thing. Not killing the goose that lays golden eggs.</div><br/><div id="40811341" class="c"><input type="checkbox" id="c-40811341" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#40811082">root</a><span>|</span><a href="#40811191">parent</a><span>|</span><a href="#40811189">next</a><span>|</span><label class="collapse" for="c-40811341">[-]</label><label class="expand" for="c-40811341">[5 more]</label></div><br/><div class="children"><div class="content">Google is in a fight for their lives, I&#x27;ve fully moved over to paid services and haven&#x27;t used google in about a month now.</div><br/><div id="40811766" class="c"><input type="checkbox" id="c-40811766" checked=""/><div class="controls bullet"><span class="by">kkkkkkk</span><span>|</span><a href="#40811082">root</a><span>|</span><a href="#40811341">parent</a><span>|</span><a href="#40811189">next</a><span>|</span><label class="collapse" for="c-40811766">[-]</label><label class="expand" for="c-40811766">[4 more]</label></div><br/><div class="children"><div class="content">If this were a common sentiment or rooted in reality I would imagine their stock would not be at an all time high...</div><br/><div id="40816808" class="c"><input type="checkbox" id="c-40816808" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#40811082">root</a><span>|</span><a href="#40811766">parent</a><span>|</span><a href="#40812211">next</a><span>|</span><label class="collapse" for="c-40816808">[-]</label><label class="expand" for="c-40816808">[1 more]</label></div><br/><div class="children"><div class="content">Ironically I was just thinking earlier today how the most valuable Google products to me are YouTube and Android... and that&#x27;s it.<p>I gave up on Chrome a decade ago, going back to Firefox. I don&#x27;t use Google for search anymore, I do use Gmail but I also got Protonmail so could easily migrate the Gmail traffic there.<p>A lot of non-techies I know have complained for some time how Google search sucks, and while a lot use Chrome it seems to be mainly inertia.<p>Not saying Google is dying, but it seems vulnerable for disruption.</div><br/></div></div><div id="40812211" class="c"><input type="checkbox" id="c-40812211" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#40811082">root</a><span>|</span><a href="#40811766">parent</a><span>|</span><a href="#40816808">prev</a><span>|</span><a href="#40811189">next</a><span>|</span><label class="collapse" for="c-40812211">[-]</label><label class="expand" for="c-40812211">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m an early adopter. The rest of you will catch up in the next five years.</div><br/><div id="40813288" class="c"><input type="checkbox" id="c-40813288" checked=""/><div class="controls bullet"><span class="by">popalchemist</span><span>|</span><a href="#40811082">root</a><span>|</span><a href="#40812211">parent</a><span>|</span><a href="#40811189">next</a><span>|</span><label class="collapse" for="c-40813288">[-]</label><label class="expand" for="c-40813288">[1 more]</label></div><br/><div class="children"><div class="content"><i>Here&#x27;s a napkin for when you&#x27;re finished.</i></div><br/></div></div></div></div></div></div></div></div></div></div><div id="40811416" class="c"><input type="checkbox" id="c-40811416" checked=""/><div class="controls bullet"><span class="by">anxman</span><span>|</span><a href="#40811082">parent</a><span>|</span><a href="#40811189">prev</a><span>|</span><label class="collapse" for="c-40811416">[-]</label><label class="expand" for="c-40811416">[1 more]</label></div><br/><div class="children"><div class="content">And the training samples are overly tied to Vertex</div><br/></div></div></div></div></div></div></div></div></div></body></html>