<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1707555659427" as="style"/><link rel="stylesheet" href="styles.css?v=1707555659427"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.symas.com/post/are-you-sure-you-want-to-use-mmap-in-your-dbms">Are you sure you want to use MMAP in your DBMS?</a> <span class="domain">(<a href="https://www.symas.com">www.symas.com</a>)</span></div><div class="subtext"><span>mfiguiere</span> | <span>45 comments</span></div><br/><div><div id="39323613" class="c"><input type="checkbox" id="c-39323613" checked=""/><div class="controls bullet"><span class="by">cperciva</span><span>|</span><a href="#39322110">next</a><span>|</span><label class="collapse" for="c-39323613">[-]</label><label class="expand" for="c-39323613">[4 more]</label></div><br/><div class="children"><div class="content">Yes and no.<p>The authors are very correct in drawing a distinction between read-only mmaps and writable mmaps.  Read-only is pretty safe -- the biggest concern is probably sanity-checking data being read from disk, but for some applications on-disk corruption isn&#x27;t a big concern anyway -- while writable mmaps need very careful use of msync to ensure crash safety and it&#x27;s <i>very</i> easy to have subtle bugs in that sort of code.<p>On the other hand, they hand-wave away I&#x2F;O stalls saying that the application is going to stall regardless of whether disk reads are implicit or explicit; this fails to account for the possible advantages of narrowing down <i>when</i> a stall might occur.  A DBMS might be servicing many requests at once; using explicit I&#x2F;O, blocking reads could be shunted off to a different context (dedicated I&#x2F;O threads, I&#x2F;O request rings, etc) allowing unblocked operations to complete without waiting.<p>The authors also make the odd claim that &quot;[LMDB&#x27;s] B+tree design is naturally optimal with an LRU cache&quot;, which is highly suspect in principle (LRU is a standard not-horribly-bad cache eviction scheme, but it&#x27;s hard to imagine it being <i>optimal</i> for anything but a trivial workload) but also ignores the fact that a database inherently knows which pages it doesn&#x27;t need any more.  This <i>can</i> be communicated to the kernel with madvise, but that adds more syscalls with inherent overhead.<p>Finally, there&#x27;s an issue of paging granularity: If you have a database with a large amount of data in core, you really want to be using superpages in order to reduce your TLB miss costs -- but you can&#x27;t mmap data with superpages if you want the kernel to be able to evict unused pages.<p>In short: Using mmap in a database absolutely <i>can</i> work, especially if (like the authors) you&#x27;re only doing it read-only; but the case for using mmap is nowhere near as strong as the authors make it out to be.</div><br/><div id="39323753" class="c"><input type="checkbox" id="c-39323753" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#39323613">parent</a><span>|</span><a href="#39322110">next</a><span>|</span><label class="collapse" for="c-39323753">[-]</label><label class="expand" for="c-39323753">[3 more]</label></div><br/><div class="children"><div class="content">&gt; hand-wave away I&#x2F;O stalls
&gt; A DBMS might be servicing many requests at once;<p>Yes, a DBMS may be servicing many requests at once, which is why individual stalls don&#x27;t matter. Other threads will be making progress.<p>&gt; a database inherently knows which pages it doesn&#x27;t need any more.<p>DBMS engineers always make this BS claim and that&#x27;s what it is, BS.
You may know exactly what information you need for a single given query. But you don&#x27;t know what the next query will be. You don&#x27;t know if the next query will need the same indices or something else entirely. If you throw something away because you&#x27;re done with it on one query, and it&#x27;s referenced again on the next, you&#x27;ll have pessimized your performance for no good reason.<p>TLB miss costs - this is totally irrelevant. Go check out any benchmark you can find of LMDB against any other DBMS. Whether conducted by us or any 3rd party. Find <i>any</i> system that gets anywhere close to LMDB&#x27;s read performance and scalability under heavy concurrent load, and prove that TLB misses are costing LMDB efficiency.</div><br/><div id="39324269" class="c"><input type="checkbox" id="c-39324269" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#39323613">root</a><span>|</span><a href="#39323753">parent</a><span>|</span><a href="#39324201">next</a><span>|</span><label class="collapse" for="c-39324269">[-]</label><label class="expand" for="c-39324269">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You may know exactly what information you need for a single given query. But you don&#x27;t know what the next query will be. You don&#x27;t know if the next query will need the same indices or something else entirely.<p>I think this misrepresents how many modern database engines work. Queries are generatively decomposed into many thousands (or millions) of independent page operations, depending on the amount of data they necessarily touch, before they get to the execution queue. Ops from concurrent queries are all blended together. The I&#x2F;O and execution scheduler can, in fact, see upwards of a million page operations into the future across all concurrent operations in some systems and adaptively, dynamically reorder the entire schedule to optimize for locality across concurrent operations while preserving invariants. This enables powerful optimizations that circumvent theoretical limits on cache effectiveness and improves locality under high concurrency in ways simple caching cannot. This is not a new idea, it is how silicon is designed to deal with data access patterns that intrinsically have poor cache locality. A modern server can retire tens of millions of these ops per second, so you can get incredible throughput with tight tail latencies because, again, you control what happens and when. This works beautifully even when the working storage is orders of magnitude larger than available memory. It does require exquisitely fine control of I&#x2F;O and execution scheduling as a matter of architecture.<p>It is worth acknowledging that the throughput limits are not an mmap() problem. Most non-mmap() database kernels struggle to saturate modern storage hardware too. Classic database architectures are not effective at consistently and inexpensively generating enough concurrency to use the available hardware.<p>I used to use mmap for database kernel-y stuff. The recurring pain point was that it is impossible to saturate fast storage arrays using it for many real-world workloads when the data won’t mostly fit in memory, and the computational cost of trying to do so was high. And then I did a stint in supercomputing where it turned out everyone knew you could circumvent these limits in highly parallel systems with precise I&#x2F;O and execution scheduling, and relying much less on caching, but those people don’t work on databases.</div><br/></div></div><div id="39324201" class="c"><input type="checkbox" id="c-39324201" checked=""/><div class="controls bullet"><span class="by">cperciva</span><span>|</span><a href="#39323613">root</a><span>|</span><a href="#39323753">parent</a><span>|</span><a href="#39324269">prev</a><span>|</span><a href="#39322110">next</a><span>|</span><label class="collapse" for="c-39324201">[-]</label><label class="expand" for="c-39324201">[1 more]</label></div><br/><div class="children"><div class="content"><i>Yes, a DBMS may be servicing many requests at once, which is why individual stalls don&#x27;t matter. Other threads will be making progress.</i><p>That works <i>if</i> you have a thread per request.  Not all servers are structured this way; thread-per-CPU with non-blocking I&#x2F;O is increasingly popular.<p><i>DBMS engineers always make this BS claim and that&#x27;s what it is, BS. You may know exactly what information you need for a single given query. But you don&#x27;t know what the next query will be.</i><p>Let me rephrase that: A database doesn&#x27;t know which data is <i>most likely</i> to be used again, but if does know which pages <i>are guaranteed to not be used again</i> because they&#x27;re not reachable in the b+tree.<p><i>Find any system that gets anywhere close to LMDB&#x27;s read performance and scalability under heavy concurrent load [...]</i><p>I would be interested to see how LMDB compares to Kivaloo.  In my benchmarking (40 byte keys, 40 byte values, single core on my laptop, highly parallel workload, in core) I get 10^6 reads per second over the network; if I understand correctly LMDB doesn&#x27;t serve a network protocol but is at a similar level of performance?</div><br/></div></div></div></div></div></div><div id="39322110" class="c"><input type="checkbox" id="c-39322110" checked=""/><div class="controls bullet"><span class="by">jitl</span><span>|</span><a href="#39323613">prev</a><span>|</span><a href="#39322020">next</a><span>|</span><label class="collapse" for="c-39322110">[-]</label><label class="expand" for="c-39322110">[3 more]</label></div><br/><div class="children"><div class="content">It’s funny that the article holds up RavenDB’s rebuttal about MMAP and then discusses correctness, because Aphyr&#x2F;Jepsen’s latest analysis on RavenDB showed huge correctness issues there: <a href="https:&#x2F;&#x2F;jepsen.io&#x2F;analyses&#x2F;ravendb-6.0.2" rel="nofollow">https:&#x2F;&#x2F;jepsen.io&#x2F;analyses&#x2F;ravendb-6.0.2</a><p>Absolutely brutal read on RavenDB.</div><br/><div id="39323361" class="c"><input type="checkbox" id="c-39323361" checked=""/><div class="controls bullet"><span class="by">password4321</span><span>|</span><a href="#39322110">parent</a><span>|</span><a href="#39323972">next</a><span>|</span><label class="collapse" for="c-39323361">[-]</label><label class="expand" for="c-39323361">[1 more]</label></div><br/><div class="children"><div class="content"><i>RavenDB 6.0.2 (A Jepsen Report)</i><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39204622">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39204622</a><p>10 days ago</div><br/></div></div></div></div><div id="39322020" class="c"><input type="checkbox" id="c-39322020" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#39322110">prev</a><span>|</span><a href="#39323895">next</a><span>|</span><label class="collapse" for="c-39322020">[-]</label><label class="expand" for="c-39322020">[5 more]</label></div><br/><div class="children"><div class="content">Author is right that a read only mmap with writing occurring normally is a better design. It’s a weakness in the paper but in fairness few databases did what LMDB did sanely. Surprising that that’s the case but that is what people chose to do.<p>Where they’re wrong is about the performance analysis:<p>&gt; In section 3.2 &quot;I&#x2F;O stalls&quot; is again a non-issue; no matter how your DBMS handles I&#x2F;O internally, synchronously or asynchronously, the calling application can&#x27;t make any progress until the I&#x2F;O completes, and if the data isn&#x27;t already in memory then the application must wait.<p>A TLB shoot down request to pull in a page or evict it can cause pauses not just in your own application but also many others because it’s basically a system wide mutex. There is a trade off obviously where the kernel can evict I&#x2F;O buffers more efficiently. But to me all that says is that the interface for managing I&#x2F;O buffers in user space is really immature and needs some better performing API so that we have something in between mmap and buffer management. Also an asynchronous application can switch to doing other work. If your synchronous memory access is blocking your thread, other unrelated work is being blocked.<p>As for workload optimization, I think OP is overstating how much sharing resources mitigates this - when I’m doing a blind search through all data for some reason (maybe some kind of background GC) or a range lookup I can choose to not cache any of that data. Similarly, if I’m doing a point lookup I can do a pre read. However, mmap has no higher level concept about the workload that’s accessing the memory. This a very real problem the kernel tried to tackle back in the 90s &#x2F; early 00s when disk indexing was starting out - a disk index would cause the cache for all other apps to be evicted resulting in waking up to a sluggish machine paging everything back in.<p>The more high level context you can retain, the better your decision making about resource allocation is. When that high level context is lost&#x2F;discarded you lose efficiency.</div><br/><div id="39322592" class="c"><input type="checkbox" id="c-39322592" checked=""/><div class="controls bullet"><span class="by">Negitivefrags</span><span>|</span><a href="#39322020">parent</a><span>|</span><a href="#39323762">next</a><span>|</span><label class="collapse" for="c-39322592">[-]</label><label class="expand" for="c-39322592">[1 more]</label></div><br/><div class="children"><div class="content">When I was writing a game engine and doing a multithreaded resource loading system using job queues, I was concerned about TLB shootdowns, but was unable to really experience the issues with them.<p>I tried both regular IO and mmap and was unable to make regular IO as fast as mmap.<p>This is on a machine with 32 real cores running 64 job threads, saturating all of them with jobs. The jobs are a mixture of small (metadata) and big (textures and models) files. In theory, this should be a pretty bad situation for the TLB shootdown, and I was concerned about them.<p>But in reality it seemed to be fine, and a fair amount faster than with regular IO due to being able to have one less extra copy of the data.</div><br/></div></div><div id="39323762" class="c"><input type="checkbox" id="c-39323762" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#39322020">parent</a><span>|</span><a href="#39322592">prev</a><span>|</span><a href="#39322404">next</a><span>|</span><label class="collapse" for="c-39323762">[-]</label><label class="expand" for="c-39323762">[1 more]</label></div><br/><div class="children"><div class="content">You can say the performance analysis is wrong, when you find any benchmarks showing that LMDB performance suffers because of it.<p>Just like those paper&#x27;s authors can claim using mmap increases complexity when they can find any DBMS with LMDB&#x27;s features, in fewer lines of code.</div><br/></div></div><div id="39322404" class="c"><input type="checkbox" id="c-39322404" checked=""/><div class="controls bullet"><span class="by">inkyoto</span><span>|</span><a href="#39322020">parent</a><span>|</span><a href="#39323762">prev</a><span>|</span><a href="#39323522">next</a><span>|</span><label class="collapse" for="c-39322404">[-]</label><label class="expand" for="c-39322404">[1 more]</label></div><br/><div class="children"><div class="content">&gt; There is a trade off obviously where the kernel can evict I&#x2F;O buffers more efficiently. But to me all that says is that the interface for managing I&#x2F;O buffers in user space is really immature and needs some better performing API so that we have something in between mmap and buffer management.<p>This was essentially the argument against adding the raw device support to Linux, which delayed the Oracle database Linux port for years, with Linus himself being adamant that operating system&#x27;s own VMM and page cache management was superior to that of the database (except it was not), which was proven to be a fallacy as the transaction and error handling database requires completely different guarantees, and the VMM pursuing different design objectives that most of the time are incompatible with the database expectations.<p>The raw devices have been used precisely for that reason: «let&#x27;s bypass the VMM and the page cache for this device, and we will take it from there». That is what the Oracle database did on Solaris – it wanted an entire raw device that it would claim for the table space. Granted, Oracle has had its own VMM and page cache (so to speak) implementation within the DB, an OS mini-kernel if you like.<p>The argument was eventually settled with adding the O_DIRECT flag to open(2) accomplishing the same outcome, and the raw devices have never made into Linux.</div><br/></div></div><div id="39323522" class="c"><input type="checkbox" id="c-39323522" checked=""/><div class="controls bullet"><span class="by">cmrdporcupine</span><span>|</span><a href="#39322020">parent</a><span>|</span><a href="#39322404">prev</a><span>|</span><a href="#39323895">next</a><span>|</span><label class="collapse" for="c-39323522">[-]</label><label class="expand" for="c-39323522">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>buffers in user space is really immature and needs some better performing API so that we have something in between mmap and buffer management.</i><p>If you don&#x27;t know about it already, you should look at Viktor Leis&#x27; et al&#x27;s work on this -- kernel module + supporting buffer mgmt code to do the kind of thing you&#x27;re talking about. The code is not production quality, but the idea is nifty:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;viktorleis&#x2F;vmcache">https:&#x2F;&#x2F;github.com&#x2F;viktorleis&#x2F;vmcache</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;tuhhosg&#x2F;exmap">https:&#x2F;&#x2F;github.com&#x2F;tuhhosg&#x2F;exmap</a><p><a href="https:&#x2F;&#x2F;www.cs.cit.tum.de&#x2F;fileadmin&#x2F;w00cfj&#x2F;dis&#x2F;_my_direct_uploads&#x2F;vmcache.pdf" rel="nofollow">https:&#x2F;&#x2F;www.cs.cit.tum.de&#x2F;fileadmin&#x2F;w00cfj&#x2F;dis&#x2F;_my_direct_up...</a><p>I&#x27;ve personally experimented a bit with userfaultfd and this kind of thing, but it&#x27;s got issues.</div><br/></div></div></div></div><div id="39323895" class="c"><input type="checkbox" id="c-39323895" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#39322020">prev</a><span>|</span><a href="#39321416">next</a><span>|</span><label class="collapse" for="c-39323895">[-]</label><label class="expand" for="c-39323895">[1 more]</label></div><br/><div class="children"><div class="content">Before we go any further here, you should all read the original LMDB design paper, to get a grasp of what we traded away first.  <a href="https:&#x2F;&#x2F;openldap.org&#x2F;pub&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openldap.org&#x2F;pub&#x2F;</a><p>You can also watch the presentation at CMU <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=tEa5sAh-kVk" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=tEa5sAh-kVk</a>  (slides are here <a href="http:&#x2F;&#x2F;www.lmdb.tech&#x2F;SymasDocs&#x2F;20151008-CMU-LMDB.pdf" rel="nofollow">http:&#x2F;&#x2F;www.lmdb.tech&#x2F;SymasDocs&#x2F;20151008-CMU-LMDB.pdf</a> )</div><br/></div></div><div id="39321416" class="c"><input type="checkbox" id="c-39321416" checked=""/><div class="controls bullet"><span class="by">beoberha</span><span>|</span><a href="#39323895">prev</a><span>|</span><a href="#39322248">next</a><span>|</span><label class="collapse" for="c-39321416">[-]</label><label class="expand" for="c-39321416">[4 more]</label></div><br/><div class="children"><div class="content">I really wish HYC was more articulate and less egomaniacal. He makes a lot of claims and doesn’t really flesh out great arguments.<p>Would be really interesting to have him discuss with Andy Pavlo :)</div><br/><div id="39323788" class="c"><input type="checkbox" id="c-39323788" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#39321416">parent</a><span>|</span><a href="#39323573">next</a><span>|</span><label class="collapse" for="c-39323788">[-]</label><label class="expand" for="c-39323788">[1 more]</label></div><br/><div class="children"><div class="content">Andy and I have had this conversation many times over.<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=tEa5sAh-kVk" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=tEa5sAh-kVk</a><p>Feel free to point out specific arguments that you think need further fleshing out. Every aspect of LMDB&#x27;s design and performance characteristics have been documented already, happy to link you to answers to any of your questions.</div><br/></div></div><div id="39323573" class="c"><input type="checkbox" id="c-39323573" checked=""/><div class="controls bullet"><span class="by">cmrdporcupine</span><span>|</span><a href="#39321416">parent</a><span>|</span><a href="#39323788">prev</a><span>|</span><a href="#39322248">next</a><span>|</span><label class="collapse" for="c-39323573">[-]</label><label class="expand" for="c-39323573">[2 more]</label></div><br/><div class="children"><div class="content">Yeah the strident and somewhat impolite tone is really hard to tolerate. Esp when directed against respected researchers like Pavlo and Leis who, well, they know what they&#x27;re talking about.<p>Everything has compromises and trade-offs. LMDB looks great. But it&#x27;s also, like a lot of similar systems, built to perform best at single-writer &#x2F; multiple-reader workloads. That&#x27;s not the only workload out there.</div><br/><div id="39323808" class="c"><input type="checkbox" id="c-39323808" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#39321416">root</a><span>|</span><a href="#39323573">parent</a><span>|</span><a href="#39322248">next</a><span>|</span><label class="collapse" for="c-39323808">[-]</label><label class="expand" for="c-39323808">[1 more]</label></div><br/><div class="children"><div class="content">If they knew what they were talking about they wouldn&#x27;t have used a raw filesystem benchmarking tool to prove their arguments. They would have used a pair of actual, comparable DBMS implementations.<p>Single-writer is all you need if each write is fast enough.<p>BerkeleyDB, which we used before writing LMDB, is multi-writer. It&#x27;s still slower than LMDB in every workload, because of all of the lock management that comes with being multi-writer.</div><br/></div></div></div></div></div></div><div id="39322248" class="c"><input type="checkbox" id="c-39322248" checked=""/><div class="controls bullet"><span class="by">gorset</span><span>|</span><a href="#39321416">prev</a><span>|</span><a href="#39323183">next</a><span>|</span><label class="collapse" for="c-39322248">[-]</label><label class="expand" for="c-39322248">[2 more]</label></div><br/><div class="children"><div class="content">I struggle to use lmdb when using resource isolation (like docker, nomad exec driver). The hidden complexities really become apparent when you build largish systems using shared infrastructure. Anything is easy if you have a dedicated machine or have dataset smaller than your memory allocation.<p>Mmap is so simple for the application&#x2F;developer, but the price you pay is giving up clear ownership of memory and clean separation between kernel page cache and application space. The interaction between the mmu, mmap, tlb and the linux kernel IO layer is complicated. You don&#x27;t want to touch that complexity at all if you care about ultra low latency.<p>The article rephrases the question into<p><pre><code>    &quot;do you really want to reimplement everything the OS already does for you?&quot;
</code></pre>
but I would rephrase it again into questioning why fairly straight forward concepts like a buffer cache can&#x27;t be implemented directly in the application?</div><br/><div id="39323481" class="c"><input type="checkbox" id="c-39323481" checked=""/><div class="controls bullet"><span class="by">natmaka</span><span>|</span><a href="#39322248">parent</a><span>|</span><a href="#39323183">next</a><span>|</span><label class="collapse" for="c-39323481">[-]</label><label class="expand" for="c-39323481">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You don&#x27;t want to touch that complexity at all if you care about ultra low latency.<p>This is less about a given criterion (such as ultra low latency) than about an adequate global performance (multiple requirements: max latency, average latency, perceived interactive latency, throughput...) on a high-yield machine (more often than not the implication being that it simultaneously has many different and changing missions).</div><br/></div></div></div></div><div id="39323183" class="c"><input type="checkbox" id="c-39323183" checked=""/><div class="controls bullet"><span class="by">raccoonone</span><span>|</span><a href="#39322248">prev</a><span>|</span><a href="#39323170">next</a><span>|</span><label class="collapse" for="c-39323183">[-]</label><label class="expand" for="c-39323183">[2 more]</label></div><br/><div class="children"><div class="content">I wrote redb (<a href="https:&#x2F;&#x2F;github.com&#x2F;cberner&#x2F;redb">https:&#x2F;&#x2F;github.com&#x2F;cberner&#x2F;redb</a>) using mmap, initially. However, I later removed it and switched to read()&#x2F;write() with my own user space cache. I&#x27;m sure it&#x27;s not as good as the OS page cache, but the difference was only 1.2-1.5x performance on the benchmarks I cared about, and the cache is less than 500 lines of code.
Also, by removing mmap() I was able to remove all the unsafe code associated with it, so now redb is memory-safe.</div><br/><div id="39323785" class="c"><input type="checkbox" id="c-39323785" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#39323183">parent</a><span>|</span><a href="#39323170">next</a><span>|</span><label class="collapse" for="c-39323785">[-]</label><label class="expand" for="c-39323785">[1 more]</label></div><br/><div class="children"><div class="content">500 lines of code is still 500 lines of added complexity. For LMDB that&#x27;d be a 7% increase in LOCs, which would also introduce the need for manual cache configuration and tuning (further increasing complexity for the end user) and a 50% performance loss? Doesn&#x27;t seem like a good tradeoff to me.<p>But at least you thought about the decision. Nice benchmark, by the way. <a href="https:&#x2F;&#x2F;mastodon.social&#x2F;@hyc&#x2F;111887577620902329" rel="nofollow">https:&#x2F;&#x2F;mastodon.social&#x2F;@hyc&#x2F;111887577620902329</a></div><br/></div></div></div></div><div id="39323170" class="c"><input type="checkbox" id="c-39323170" checked=""/><div class="controls bullet"><span class="by">dboreham</span><span>|</span><a href="#39323183">prev</a><span>|</span><a href="#39322418">next</a><span>|</span><label class="collapse" for="c-39323170">[-]</label><label class="expand" for="c-39323170">[1 more]</label></div><br/><div class="children"><div class="content">You need to understand (well) what the kernel is going to do. That&#x27;s more difficult than if you call read&#x2F;write yourself (because their semantics are well defined). Otoh the problem is significantly less than it was back in the day because there&#x27;s really only one kernel now.</div><br/></div></div><div id="39322418" class="c"><input type="checkbox" id="c-39322418" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#39323170">prev</a><span>|</span><a href="#39322759">next</a><span>|</span><label class="collapse" for="c-39322418">[-]</label><label class="expand" for="c-39322418">[8 more]</label></div><br/><div class="children"><div class="content">Something I ~never see mentioned in mmap discussions is that it completely bypasses the expensive syscall transition for read calls, which has only grown rapidly in cost thanks to all the speculative execution mitigations.<p>In benchmarking I found an mmap approach to be integral to achieving maximum performance in IO-heavy workloads on modern OSes [0]. (Note that it still is a win even with mitigations disabled.)<p>[0]: <a href="https:&#x2F;&#x2F;neosmart.net&#x2F;blog&#x2F;using-simd-acceleration-in-rust-to-create-the-worlds-fastest-tac&#x2F;" rel="nofollow">https:&#x2F;&#x2F;neosmart.net&#x2F;blog&#x2F;using-simd-acceleration-in-rust-to...</a></div><br/><div id="39324333" class="c"><input type="checkbox" id="c-39324333" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#39322418">parent</a><span>|</span><a href="#39323261">next</a><span>|</span><label class="collapse" for="c-39324333">[-]</label><label class="expand" for="c-39324333">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, we knew that LMDB would be unaffected. Other folks demonstrated that too. <a href="https:&#x2F;&#x2F;www.pugetsystems.com&#x2F;labs&#x2F;hpc&#x2F;Intel-CPU-flaw-kernel-patch-effects---GPU-compute-Tensorflow-Caffe-and-LMDB-database-creation-1093&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.pugetsystems.com&#x2F;labs&#x2F;hpc&#x2F;Intel-CPU-flaw-kernel-...</a></div><br/></div></div><div id="39323261" class="c"><input type="checkbox" id="c-39323261" checked=""/><div class="controls bullet"><span class="by">lathiat</span><span>|</span><a href="#39322418">parent</a><span>|</span><a href="#39324333">prev</a><span>|</span><a href="#39322460">next</a><span>|</span><label class="collapse" for="c-39323261">[-]</label><label class="expand" for="c-39323261">[2 more]</label></div><br/><div class="children"><div class="content">You can likely be solve that with io_uring</div><br/><div id="39323566" class="c"><input type="checkbox" id="c-39323566" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#39322418">root</a><span>|</span><a href="#39323261">parent</a><span>|</span><a href="#39322460">next</a><span>|</span><label class="collapse" for="c-39323566">[-]</label><label class="expand" for="c-39323566">[1 more]</label></div><br/><div class="children"><div class="content">If it&#x27;s not solved by io_uring, then what is io_uring <i>for</i>?</div><br/></div></div></div></div><div id="39322460" class="c"><input type="checkbox" id="c-39322460" checked=""/><div class="controls bullet"><span class="by">lvlabguy</span><span>|</span><a href="#39322418">parent</a><span>|</span><a href="#39323261">prev</a><span>|</span><a href="#39322462">next</a><span>|</span><label class="collapse" for="c-39322460">[-]</label><label class="expand" for="c-39322460">[2 more]</label></div><br/><div class="children"><div class="content">Then you have page faults to deal with.</div><br/></div></div><div id="39322462" class="c"><input type="checkbox" id="c-39322462" checked=""/><div class="controls bullet"><span class="by">hnlmorg</span><span>|</span><a href="#39322418">parent</a><span>|</span><a href="#39322460">prev</a><span>|</span><a href="#39322759">next</a><span>|</span><label class="collapse" for="c-39322462">[-]</label><label class="expand" for="c-39322462">[2 more]</label></div><br/><div class="children"><div class="content">Unless I’m completely missing your point, and to be fair I might be because it’s late in Europe and I should have gone to bed hours ago, read speeds due to bypassing expensive syscall overhead is an often cited part of the mmap discussion.</div><br/><div id="39322519" class="c"><input type="checkbox" id="c-39322519" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#39322418">root</a><span>|</span><a href="#39322462">parent</a><span>|</span><a href="#39322759">next</a><span>|</span><label class="collapse" for="c-39322519">[-]</label><label class="expand" for="c-39322519">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, I meant specifically the post-mitigations aspect. It really does change the calculus.</div><br/></div></div></div></div></div></div><div id="39322759" class="c"><input type="checkbox" id="c-39322759" checked=""/><div class="controls bullet"><span class="by">dwattttt</span><span>|</span><a href="#39322418">prev</a><span>|</span><a href="#39324158">next</a><span>|</span><label class="collapse" for="c-39322759">[-]</label><label class="expand" for="c-39322759">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The ones who got it wrong are irrelevant, because correct solutions clearly exist for all the potential problems.<p>This is a pretty weak argument. Correct solutions exist for doing extremely unsafe things, but that doesn&#x27;t mean using safer approaches is bad. If a bunch of different projects get something wrong, yes that _is_ a signal that you should think twice before using it yourself.</div><br/></div></div><div id="39324158" class="c"><input type="checkbox" id="c-39324158" checked=""/><div class="controls bullet"><span class="by">leonardb2</span><span>|</span><a href="#39322759">prev</a><span>|</span><a href="#39323357">next</a><span>|</span><label class="collapse" for="c-39324158">[-]</label><label class="expand" for="c-39324158">[2 more]</label></div><br/><div class="children"><div class="content">I love lmdb and mmap, but I wish there was prefix compression for keys. I&#x27;m on the fence between rocksdb and lmdb, both using mmap and will likely support both. My database file was so much smaller with rocksdb because it compresses segments, but lmdb is so much more concise and will never do background jobs like rocksdb. One can use different subtables in lmdb, but the amount must be set up front with mdb_env_set_maxdbs and it says that there is a linear search involved.</div><br/><div id="39324372" class="c"><input type="checkbox" id="c-39324372" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#39324158">parent</a><span>|</span><a href="#39323357">next</a><span>|</span><label class="collapse" for="c-39324372">[-]</label><label class="expand" for="c-39324372">[1 more]</label></div><br/><div class="children"><div class="content">Prefix compression adds a lot of complexity. If you insert a new record that makes an existing prefix non-unique, you then have to expand and rewrite every key in the page, and that will ripple up to every parent page as well. And the expansion can then cause the records to no longer fit in their current page, so then you need to do a bunch of page splits. It&#x27;s gross and ugly and easy to get wrong. (The btree.c that LMDB was based on has prefix compression. We ripped it out after finding bugs in it.) If your data values are small enough, you can use DUPSORT records instead, and manage your prefixes manually.<p>There is a linear search involved to open a DBI handle, yes. But you only do that once, and keep it open for the life of the program. Not a big deal.</div><br/></div></div></div></div><div id="39323357" class="c"><input type="checkbox" id="c-39323357" checked=""/><div class="controls bullet"><span class="by">xyzzy_plugh</span><span>|</span><a href="#39324158">prev</a><span>|</span><a href="#39323876">next</a><span>|</span><label class="collapse" for="c-39323357">[-]</label><label class="expand" for="c-39323357">[1 more]</label></div><br/><div class="children"><div class="content">In the days of yore, everyone eschewed mmap because it required platform specific tuning. Then everyone switched to one platform and continued to eschew mmap because they didn&#x27;t want to read the docs.<p>Sure, you can manifest destiny and write it all yourself, and many do, but if you learn how to make mmap really fly, then you can do that again and again without having to even so much as smell a DBMS.<p>The Linux kernel defaults are pretty horrendous for anything resembling high performance hardware, let alone disks that act like memory, but it can most comfortably accommodate all of those things handedly if you just configure the damn thing properly.</div><br/></div></div><div id="39323876" class="c"><input type="checkbox" id="c-39323876" checked=""/><div class="controls bullet"><span class="by">IamLoading</span><span>|</span><a href="#39323357">prev</a><span>|</span><a href="#39322122">next</a><span>|</span><label class="collapse" for="c-39323876">[-]</label><label class="expand" for="c-39323876">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=1BRGU_AS25c" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=1BRGU_AS25c</a> - video regarding the paper findings by the authors.</div><br/></div></div><div id="39324374" class="c"><input type="checkbox" id="c-39324374" checked=""/><div class="controls bullet"><span class="by">pengaru</span><span>|</span><a href="#39322122">prev</a><span>|</span><a href="#39324205">next</a><span>|</span><label class="collapse" for="c-39324374">[-]</label><label class="expand" for="c-39324374">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; memcpy() (ie &quot;read()&quot; in this case) is _always_ going to be faster in many
  &gt; cases, just because it avoids all the extra complexity.  While mmap() is
  &gt; going to be faster in other cases.
  &gt; - Linus Torvalds [0]
</code></pre>
At least a long-running db process can amortize the page table costs across many operations, unlike something ephemeral like journalctl&#x2F;systemctl where it really hurts.<p>[0] <a href="https:&#x2F;&#x2F;yarchive.net&#x2F;comp&#x2F;mmap.html" rel="nofollow">https:&#x2F;&#x2F;yarchive.net&#x2F;comp&#x2F;mmap.html</a></div><br/></div></div><div id="39324205" class="c"><input type="checkbox" id="c-39324205" checked=""/><div class="controls bullet"><span class="by">BohuTANG</span><span>|</span><a href="#39324374">prev</a><span>|</span><a href="#39322473">next</a><span>|</span><label class="collapse" for="c-39324205">[-]</label><label class="expand" for="c-39324205">[1 more]</label></div><br/><div class="children"><div class="content">These articles all miss the point.<p>If you are a DBMS kernel developer, the most urgent need is how to control memory more precisely to prevent out-of-memory (OOM) situations. Clearly, MMAP cannot achieve this, which is also why most databases still use their own memory pool + direct-IO.</div><br/></div></div><div id="39322473" class="c"><input type="checkbox" id="c-39322473" checked=""/><div class="controls bullet"><span class="by">pengaru</span><span>|</span><a href="#39324205">prev</a><span>|</span><a href="#39322229">next</a><span>|</span><label class="collapse" for="c-39322473">[-]</label><label class="expand" for="c-39322473">[5 more]</label></div><br/><div class="children"><div class="content">The problem with using mmap-based IO is you generally depend on the kernel&#x27;s async readahead&#x2F;readaround to keep things fast while writing blocking-style, obvious procedural style code, but it&#x27;s totally naive, and sequential relative to the faulted addresses, much like a spinning disk prefers to be read.<p>That worked well when the storage system was made of spinning disks, and you were incentivized to use data structures laying data out appropriate for that access pattern.  It was all complementary and cooperated pretty well.<p>Then SSDs became commonplace.<p>Today you want to exploit their fast random access, utilizing whatever data structures make the most sense in a world without seek times.  Introducing mmap() into that stack kind of reintroduces spinning disk style performance characteristics, if you&#x27;re relying on mmap()&#x27;s async readahead&#x2F;readaround magic to keep your accesses from blocking on IO.<p>I&#x27;m not saying mmap() was ever a great choice for a database.  But it&#x27;s <i>really</i> awful for random access patterns until everything&#x27;s hot in the page cache.<p>This happens to be a large part of why systemd-journald&#x27;s journals can be slow to process w&#x2F;`journalctl -u` or `systemctl status $service` type queries when cold.  The access patterns are very random combined with tiny objects being read, while the kernel goes off prefetching heaps of ~unrelated nearby pages in the hopes of preventing blocking subsequent accesses.</div><br/><div id="39323869" class="c"><input type="checkbox" id="c-39323869" checked=""/><div class="controls bullet"><span class="by">hyc_symas</span><span>|</span><a href="#39322473">parent</a><span>|</span><a href="#39323837">next</a><span>|</span><label class="collapse" for="c-39323869">[-]</label><label class="expand" for="c-39323869">[2 more]</label></div><br/><div class="children"><div class="content">&gt; That worked well when the storage system was made of spinning disks, and you were incentivized to use data structures laying data out appropriate for that access pattern. It was all complementary and cooperated pretty well.<p>&gt; Then SSDs became commonplace.<p>You are laughably wrong. Random I&#x2F;O is random I&#x2F;O whether you do it using lseek&#x2F;read or whether you access a page offset in a mmap. And we&#x27;ve got hundreds of hours of benchmarks that show that LMDB outperforms everything else, whether on spinning rust or solid state. <a href="http:&#x2F;&#x2F;www.lmdb.tech&#x2F;bench&#x2F;" rel="nofollow">http:&#x2F;&#x2F;www.lmdb.tech&#x2F;bench&#x2F;</a></div><br/><div id="39324109" class="c"><input type="checkbox" id="c-39324109" checked=""/><div class="controls bullet"><span class="by">pengaru</span><span>|</span><a href="#39322473">root</a><span>|</span><a href="#39323869">parent</a><span>|</span><a href="#39323837">next</a><span>|</span><label class="collapse" for="c-39324109">[-]</label><label class="expand" for="c-39324109">[1 more]</label></div><br/><div class="children"><div class="content">My comment isn&#x27;t really targeted at LMDB specifically, it&#x27;s more generally regarding mmap().<p>If you design the data format to work well on a spinning disk, it should work well with an mmap based access.  That&#x27;s what my comment implies, and it&#x27;s completely consistent with what you&#x27;re saying regarding the benchmarks.</div><br/></div></div></div></div><div id="39323837" class="c"><input type="checkbox" id="c-39323837" checked=""/><div class="controls bullet"><span class="by">jnwatson</span><span>|</span><a href="#39322473">parent</a><span>|</span><a href="#39323869">prev</a><span>|</span><a href="#39322229">next</a><span>|</span><label class="collapse" for="c-39323837">[-]</label><label class="expand" for="c-39323837">[2 more]</label></div><br/><div class="children"><div class="content">Readahead is configurable via madvise, and it is configurable in LMDB as well.</div><br/><div id="39324364" class="c"><input type="checkbox" id="c-39324364" checked=""/><div class="controls bullet"><span class="by">jandrewrogers</span><span>|</span><a href="#39322473">root</a><span>|</span><a href="#39323837">parent</a><span>|</span><a href="#39322229">next</a><span>|</span><label class="collapse" for="c-39324364">[-]</label><label class="expand" for="c-39324364">[1 more]</label></div><br/><div class="children"><div class="content">With the caveat that I have not looked at this in over a decade, readahead via madvise() is imprecise to the point of near uselessness for high-throughput database workloads. The biggest issue is that, as the name implies, Linux is free to ignore madvise() or do something that is not quite what you requested, and often does. Many types of finely choreographed locality optimizations require more determinism than it offers.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>