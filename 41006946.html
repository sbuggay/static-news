<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1721466050736" as="style"/><link rel="stylesheet" href="styles.css?v=1721466050736"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://coroot.com/blog/instrumenting-python-gil-with-ebpf">Instrumenting Python GIL with eBPF</a> <span class="domain">(<a href="https://coroot.com">coroot.com</a>)</span></div><div class="subtext"><span>lukastyrychtr</span> | <span>27 comments</span></div><br/><div><div id="41008345" class="c"><input type="checkbox" id="c-41008345" checked=""/><div class="controls bullet"><span class="by">polotics</span><span>|</span><a href="#41009390">next</a><span>|</span><label class="collapse" for="c-41008345">[-]</label><label class="expand" for="c-41008345">[16 more]</label></div><br/><div class="children"><div class="content">Wow, 36ms per second is only 3.6% of the time. Python waiting on the GIL is then a pretty overblown problem, as this is not very significant.   
I wonder if this measure could be run on apps built with various frameworks. I expect that with uvloop and all, the percentage would be even less.</div><br/><div id="41013147" class="c"><input type="checkbox" id="c-41013147" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41008345">parent</a><span>|</span><a href="#41008403">next</a><span>|</span><label class="collapse" for="c-41013147">[-]</label><label class="expand" for="c-41013147">[2 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; only 3.6%
  &gt; GIL is then a pretty overblown problem, as this is not very significant
</code></pre>
How do you conclude this? 3.6% of the time spent being locked seems pretty significant. As another user notes, this is 3-4 fewer machines you need to pay for out of every 100 you currently use.<p>But the problem is worse than that. That&#x27;s 3.6% of the time you&#x27;re locked, not 3.6% of the time your program takes vs the time it would take without GIL. During that same time other processes could be run. I&#x27;ve seen plenty of mundane and ordinary tasks be 10x to 1000x faster when comparing serial to even unoptimized parallel. I&#x27;d be really careful to take this conclusion unless you actually have experience in writing parallel and&#x2F;or optimized code.</div><br/><div id="41013539" class="c"><input type="checkbox" id="c-41013539" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#41008345">root</a><span>|</span><a href="#41013147">parent</a><span>|</span><a href="#41008403">next</a><span>|</span><label class="collapse" for="c-41013539">[-]</label><label class="expand" for="c-41013539">[1 more]</label></div><br/><div class="children"><div class="content">people who think 3.6% doesn&#x27;t matter don&#x27;t have any imagination. ballpark whatever you imagine is FB&#x27;s compute cost vis-a-vis pytorch and take 3.6% of that and ask yourself whether you&#x27;d be happy passing up that kind of bonus&#x2F;pay-bump&#x2F;....lottery winnings. and of course for many pytorch workloads (smallish kernels in a highly multithreaded environment) i&#x27;m sure that number can be higher than 3.6%.</div><br/></div></div></div></div><div id="41008403" class="c"><input type="checkbox" id="c-41008403" checked=""/><div class="controls bullet"><span class="by">black_puppydog</span><span>|</span><a href="#41008345">parent</a><span>|</span><a href="#41013147">prev</a><span>|</span><a href="#41014864">next</a><span>|</span><label class="collapse" for="c-41008403">[-]</label><label class="expand" for="c-41008403">[8 more]</label></div><br/><div class="children"><div class="content">It <i>may</i> not be very significant <i>in this context</i>.<p>But the author literally starts by explaining that that&#x27;s a typical argument <i>for webservers</i> because they&#x27;re mostly I&#x2F;O bound. Anyone working with code that&#x27;s more CPU bound will have very different numbers, and interpret them differently.</div><br/><div id="41013219" class="c"><input type="checkbox" id="c-41013219" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41008345">root</a><span>|</span><a href="#41008403">parent</a><span>|</span><a href="#41009119">next</a><span>|</span><label class="collapse" for="c-41013219">[-]</label><label class="expand" for="c-41013219">[2 more]</label></div><br/><div class="children"><div class="content">&gt; for webservers because they&#x27;re mostly I&#x2F;O bound<p>I&#x27;m not a web person, but can&#x27;t you also gain I&#x2F;O from parallelization? The I&#x2F;O bound is waiting on responses right? So parallelization should increase I&#x2F;O because you can make multiple asynchronous requests and even if there is dependence you can often stage or do partial computation in the mean time (at least this is common in scientific computing). (And if disk, well reading&#x2F;writing to disk in parallel is far faster than serial but idk why you&#x27;d read&#x2F;write to disk with pure python. Though it seems people do). So wouldn&#x27;t this have significant effects that are more than the 36ms that we see in TFA? Or am I missing something and can someone explain why my guess is wrong?</div><br/><div id="41014362" class="c"><input type="checkbox" id="c-41014362" checked=""/><div class="controls bullet"><span class="by">Flimm</span><span>|</span><a href="#41008345">root</a><span>|</span><a href="#41013219">parent</a><span>|</span><a href="#41009119">next</a><span>|</span><label class="collapse" for="c-41014362">[-]</label><label class="expand" for="c-41014362">[1 more]</label></div><br/><div class="children"><div class="content">The GIL doesn&#x27;t prevent all types of parallelization. You have always been able to use threads or to use async code to make I&#x2F;O happen in parallel.</div><br/></div></div></div></div><div id="41009119" class="c"><input type="checkbox" id="c-41009119" checked=""/><div class="controls bullet"><span class="by">polotics</span><span>|</span><a href="#41008345">root</a><span>|</span><a href="#41008403">parent</a><span>|</span><a href="#41013219">prev</a><span>|</span><a href="#41014864">next</a><span>|</span><label class="collapse" for="c-41009119">[-]</label><label class="expand" for="c-41009119">[5 more]</label></div><br/><div class="children"><div class="content">are you sure though that being more CPU-bound will imply more waiting on the GIL? CPU-bound python in my experience means libraries, like eg. numpy, that are well-designed and release the GIL.</div><br/><div id="41009190" class="c"><input type="checkbox" id="c-41009190" checked=""/><div class="controls bullet"><span class="by">fleetfox</span><span>|</span><a href="#41008345">root</a><span>|</span><a href="#41009119">parent</a><span>|</span><a href="#41014864">next</a><span>|</span><label class="collapse" for="c-41009190">[-]</label><label class="expand" for="c-41009190">[4 more]</label></div><br/><div class="children"><div class="content">If you are interested PEP703 describes the scenarios pretty well:
<a href="https:&#x2F;&#x2F;peps.python.org&#x2F;pep-0703&#x2F;#motivation" rel="nofollow">https:&#x2F;&#x2F;peps.python.org&#x2F;pep-0703&#x2F;#motivation</a></div><br/><div id="41009413" class="c"><input type="checkbox" id="c-41009413" checked=""/><div class="controls bullet"><span class="by">johnjr</span><span>|</span><a href="#41008345">root</a><span>|</span><a href="#41009190">parent</a><span>|</span><a href="#41014864">next</a><span>|</span><label class="collapse" for="c-41009413">[-]</label><label class="expand" for="c-41009413">[3 more]</label></div><br/><div class="children"><div class="content">I just wrote a post about how the Cpython is much faster without GIL:<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40988244">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40988244</a></div><br/><div id="41011838" class="c"><input type="checkbox" id="c-41011838" checked=""/><div class="controls bullet"><span class="by">arp242</span><span>|</span><a href="#41008345">root</a><span>|</span><a href="#41009413">parent</a><span>|</span><a href="#41014864">next</a><span>|</span><label class="collapse" for="c-41011838">[-]</label><label class="expand" for="c-41011838">[2 more]</label></div><br/><div class="children"><div class="content">I mean, only the threaded version, which is expected. For tons of cases Python without the GIL is not just slower, but <i>significantly</i> slower; &quot;somewhere from 30-50%&quot; according to one of the people working on this: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40949628">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40949628</a><p>All of this is why the GIL wasn&#x27;t removed 20 years ago. There are real trade-offs here.</div><br/><div id="41014695" class="c"><input type="checkbox" id="c-41014695" checked=""/><div class="controls bullet"><span class="by">BossingAround</span><span>|</span><a href="#41008345">root</a><span>|</span><a href="#41011838">parent</a><span>|</span><a href="#41014864">next</a><span>|</span><label class="collapse" for="c-41014695">[-]</label><label class="expand" for="c-41014695">[1 more]</label></div><br/><div class="children"><div class="content">How is single-threaded code slower without GIL?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41014864" class="c"><input type="checkbox" id="c-41014864" checked=""/><div class="controls bullet"><span class="by">klysm</span><span>|</span><a href="#41008345">parent</a><span>|</span><a href="#41008403">prev</a><span>|</span><a href="#41010146">next</a><span>|</span><label class="collapse" for="c-41014864">[-]</label><label class="expand" for="c-41014864">[1 more]</label></div><br/><div class="children"><div class="content">I don’t really agree with the characterization. That can matter at scale and the other thing to consider is latency</div><br/></div></div><div id="41010146" class="c"><input type="checkbox" id="c-41010146" checked=""/><div class="controls bullet"><span class="by">memco</span><span>|</span><a href="#41008345">parent</a><span>|</span><a href="#41014864">prev</a><span>|</span><a href="#41010523">next</a><span>|</span><label class="collapse" for="c-41010146">[-]</label><label class="expand" for="c-41010146">[1 more]</label></div><br/><div class="children"><div class="content">That’s over 40 minutes of a full day or 5 hours a week spent locking! As the author states it’s highly context dependent but in some cases this is a lot of time to do other work if it can be reduced.</div><br/></div></div><div id="41010523" class="c"><input type="checkbox" id="c-41010523" checked=""/><div class="controls bullet"><span class="by">in_a_society</span><span>|</span><a href="#41008345">parent</a><span>|</span><a href="#41010146">prev</a><span>|</span><a href="#41010267">next</a><span>|</span><label class="collapse" for="c-41010523">[-]</label><label class="expand" for="c-41010523">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure your conclusion is a fair take. In the app I work on, GIL acquisition would easily take 2-3x longer than the postgres queries which would subsequently be issued.</div><br/></div></div><div id="41010267" class="c"><input type="checkbox" id="c-41010267" checked=""/><div class="controls bullet"><span class="by">kevingadd</span><span>|</span><a href="#41008345">parent</a><span>|</span><a href="#41010523">prev</a><span>|</span><a href="#41013436">next</a><span>|</span><label class="collapse" for="c-41010267">[-]</label><label class="expand" for="c-41010267">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re running your application on 100 servers, that&#x27;s potentially 3-4 entire machines you no longer have to pay EC2&#x2F;S3&#x2F;etc fees for, or it&#x27;s more scaling headroom because you&#x27;ve decreased load across your whole fleet by 3.6%.<p>It&#x27;s &quot;small&quot;, sure, but in production performance issues are often &quot;death by a thousand cuts&quot; situations, so a 3.6% reduction is a big win compared to optimizations that are often in the 1% range.</div><br/></div></div><div id="41013436" class="c"><input type="checkbox" id="c-41013436" checked=""/><div class="controls bullet"><span class="by">galdosdi</span><span>|</span><a href="#41008345">parent</a><span>|</span><a href="#41010267">prev</a><span>|</span><a href="#41009390">next</a><span>|</span><label class="collapse" for="c-41013436">[-]</label><label class="expand" for="c-41013436">[1 more]</label></div><br/><div class="children"><div class="content">Thoroughput vs Latency</div><br/></div></div></div></div><div id="41009390" class="c"><input type="checkbox" id="c-41009390" checked=""/><div class="controls bullet"><span class="by">noident</span><span>|</span><a href="#41008345">prev</a><span>|</span><a href="#41008974">next</a><span>|</span><label class="collapse" for="c-41009390">[-]</label><label class="expand" for="c-41009390">[5 more]</label></div><br/><div class="children"><div class="content">&gt;Every Python developer has heard about the GIL<p>Sadly, that is not the world we live in.<p>I&#x27;ve cleaned up dozens of applications written by people with flawed understandings of threads, multiprocessing, and asyncio. I don&#x27;t even blame the developers for this; it&#x27;s a glaring language design problem.<p>If you need parallelism, Python is not the language you should reach for. Nobody ever takes my word for it until it&#x27;s release day and the product is a broken pile of spaghetti code.</div><br/><div id="41011613" class="c"><input type="checkbox" id="c-41011613" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41009390">parent</a><span>|</span><a href="#41012194">next</a><span>|</span><label class="collapse" for="c-41011613">[-]</label><label class="expand" for="c-41011613">[2 more]</label></div><br/><div class="children"><div class="content">How do you feel about the GIL free option coming in Python 3.13?</div><br/><div id="41012242" class="c"><input type="checkbox" id="c-41012242" checked=""/><div class="controls bullet"><span class="by">noident</span><span>|</span><a href="#41009390">root</a><span>|</span><a href="#41011613">parent</a><span>|</span><a href="#41012194">next</a><span>|</span><label class="collapse" for="c-41012242">[-]</label><label class="expand" for="c-41012242">[1 more]</label></div><br/><div class="children"><div class="content">It looks like it will be some time before free-threading can be safely used in production. I don&#x27;t want to have to worry about whether the underlying C code supports no-GIL mode, or trade off single-threaded performance. [0]<p>Maybe someday this will make parallelism in Python at least as sane as other languages, but in the meantime, I still want to use a compiled language any time performance matters and wait for the kinks in no-GIL to be ironed out.<p>[0] <a href="https:&#x2F;&#x2F;docs.python.org&#x2F;3.13&#x2F;whatsnew&#x2F;3.13.html#free-threaded-cpython" rel="nofollow">https:&#x2F;&#x2F;docs.python.org&#x2F;3.13&#x2F;whatsnew&#x2F;3.13.html#free-threade...</a></div><br/></div></div></div></div><div id="41012194" class="c"><input type="checkbox" id="c-41012194" checked=""/><div class="controls bullet"><span class="by">icedchai</span><span>|</span><a href="#41009390">parent</a><span>|</span><a href="#41011613">prev</a><span>|</span><a href="#41009557">next</a><span>|</span><label class="collapse" for="c-41012194">[-]</label><label class="expand" for="c-41012194">[1 more]</label></div><br/><div class="children"><div class="content">Same. I &quot;inherited&quot; an app doing crazy slow sync IO from their &quot;async&quot; functions... Sure, there was often some async IO mixed in, too, but what&#x27;s the point?</div><br/></div></div></div></div></div></div></div></div></div></body></html>