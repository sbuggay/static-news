<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1724835687388" as="style"/><link rel="stylesheet" href="styles.css?v=1724835687388"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://chipsandcheese.com/2024/08/27/teslas-ttpoe-at-hot-chips-2024-replacing-tcp-for-low-latency-applications/">Tesla&#x27;s TTPoE at Hot Chips 2024: Replacing TCP for Low Latency Applications</a>Â <span class="domain">(<a href="https://chipsandcheese.com">chipsandcheese.com</a>)</span></div><div class="subtext"><span>ksec</span> | <span>46 comments</span></div><br/><div><div id="41375584" class="c"><input type="checkbox" id="c-41375584" checked=""/><div class="controls bullet"><span class="by">cout</span><span>|</span><a href="#41375772">next</a><span>|</span><label class="collapse" for="c-41375584">[-]</label><label class="expand" for="c-41375584">[22 more]</label></div><br/><div class="children"><div class="content">This kind of computing must be a different kind of world than the one I work in.  80 microseconds of latency seems high to me when infiniband can do single digit latency with unreliable datagrams, which turn out to be mostly reliable due to the credit system.</div><br/><div id="41377074" class="c"><input type="checkbox" id="c-41377074" checked=""/><div class="controls bullet"><span class="by">starspangled</span><span>|</span><a href="#41375584">parent</a><span>|</span><a href="#41377195">next</a><span>|</span><label class="collapse" for="c-41377074">[-]</label><label class="expand" for="c-41377074">[1 more]</label></div><br/><div class="children"><div class="content">AFAIKS the protocol can tolerate <i>up to</i> about 80 microseconds of latency.<p>The graph at the end shows they measured (one way) latency at 1.3 microseconds (compared with 2.0 for IB).</div><br/></div></div><div id="41377195" class="c"><input type="checkbox" id="c-41377195" checked=""/><div class="controls bullet"><span class="by">eecc</span><span>|</span><a href="#41375584">parent</a><span>|</span><a href="#41377074">prev</a><span>|</span><a href="#41375823">next</a><span>|</span><label class="collapse" for="c-41377195">[-]</label><label class="expand" for="c-41377195">[1 more]</label></div><br/><div class="children"><div class="content">Well, whether it matters depends on the workload: IB is basically remote DMA so if you need to pick and poke remote data I guess it&#x27;ll work as another NUMA tier.<p>But for AI training, where you&#x27;re simply shuffling around large stacks of matrices, my guess is latency constraints weaken.</div><br/></div></div><div id="41375823" class="c"><input type="checkbox" id="c-41375823" checked=""/><div class="controls bullet"><span class="by">namibj</span><span>|</span><a href="#41375584">parent</a><span>|</span><a href="#41377195">prev</a><span>|</span><a href="#41375905">next</a><span>|</span><label class="collapse" for="c-41375823">[-]</label><label class="expand" for="c-41375823">[1 more]</label></div><br/><div class="children"><div class="content">Also PCIe is worth mentioning for it&#x27;s credit-based full reliability (in the absence of hardware failures, which are still signaled).</div><br/></div></div><div id="41375905" class="c"><input type="checkbox" id="c-41375905" checked=""/><div class="controls bullet"><span class="by">iforgotpassword</span><span>|</span><a href="#41375584">parent</a><span>|</span><a href="#41375823">prev</a><span>|</span><a href="#41375635">next</a><span>|</span><label class="collapse" for="c-41375905">[-]</label><label class="expand" for="c-41375905">[8 more]</label></div><br/><div class="children"><div class="content">I assume infiniband is much more expensive, but then again you have to offset all the development cost first.</div><br/><div id="41376030" class="c"><input type="checkbox" id="c-41376030" checked=""/><div class="controls bullet"><span class="by">jabl</span><span>|</span><a href="#41375584">root</a><span>|</span><a href="#41375905">parent</a><span>|</span><a href="#41375635">next</a><span>|</span><label class="collapse" for="c-41376030">[-]</label><label class="expand" for="c-41376030">[7 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not, really. It&#x27;s been a while since I&#x27;ve checked pricing so my data might be old, but an IB switch is in the same ballpark as an ethernet switch with the same port speed. Same for HCA&#x27;s.<p>There&#x27;s no analogue in the Infiniband world to dirt cheap 1GbE RJ-45 switches though.</div><br/><div id="41376445" class="c"><input type="checkbox" id="c-41376445" checked=""/><div class="controls bullet"><span class="by">creshal</span><span>|</span><a href="#41375584">root</a><span>|</span><a href="#41376030">parent</a><span>|</span><a href="#41376547">next</a><span>|</span><label class="collapse" for="c-41376445">[-]</label><label class="expand" for="c-41376445">[4 more]</label></div><br/><div class="children"><div class="content">&gt; an IB switch is in the same ballpark as an ethernet switch with the same port speed<p>And both price tags will make Elon&#x27;s &quot;someone&#x27;s scamming me with a &#x27;you&#x27;re an enterprise customer&#x27; surcharge&quot; sense tingle. The price tags for anything enterprise networking related are seriously inflated, and I would not be surprised if just making your own NICs and switches is cheaper once you hit a certain deployment size.</div><br/><div id="41376656" class="c"><input type="checkbox" id="c-41376656" checked=""/><div class="controls bullet"><span class="by">_zoltan_</span><span>|</span><a href="#41375584">root</a><span>|</span><a href="#41376445">parent</a><span>|</span><a href="#41376547">next</a><span>|</span><label class="collapse" for="c-41376656">[-]</label><label class="expand" for="c-41376656">[3 more]</label></div><br/><div class="children"><div class="content">Networking has never been so cheap at the highest end. Look at the road we&#x27;ve been through in the last 8 in years, rapidly going from 40 to 100 to 400 (200 was somewhat of a dud, 400 came too early) to 800 to 1600Gbps. It&#x27;s amazing.<p>I&#x27;m having trouble feeding things at 400GB&#x2F;s (not a typo, it&#x27;s gigabyte&#x2F;s) per H100 box.<p>For 10 boxes ideally you want 4TB&#x2F;s...</div><br/><div id="41377095" class="c"><input type="checkbox" id="c-41377095" checked=""/><div class="controls bullet"><span class="by">starspangled</span><span>|</span><a href="#41375584">root</a><span>|</span><a href="#41376656">parent</a><span>|</span><a href="#41376547">next</a><span>|</span><label class="collapse" for="c-41377095">[-]</label><label class="expand" for="c-41377095">[2 more]</label></div><br/><div class="children"><div class="content">No, but this isn&#x27;t high end (in 2024), or &quot;enterprise&quot;. It&#x27;s their own designed 100Gb dumb NIC.</div><br/><div id="41377283" class="c"><input type="checkbox" id="c-41377283" checked=""/><div class="controls bullet"><span class="by">_zoltan_</span><span>|</span><a href="#41375584">root</a><span>|</span><a href="#41377095">parent</a><span>|</span><a href="#41376547">next</a><span>|</span><label class="collapse" for="c-41377283">[-]</label><label class="expand" for="c-41377283">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve replied in a thread, and what I&#x27;ve replied to above was about the enterprise tax and that it&#x27;s surely cheaper to do your own, not if the original article is about enterprise or not.</div><br/></div></div></div></div></div></div></div></div><div id="41376547" class="c"><input type="checkbox" id="c-41376547" checked=""/><div class="controls bullet"><span class="by">charleshn</span><span>|</span><a href="#41375584">root</a><span>|</span><a href="#41376030">parent</a><span>|</span><a href="#41376445">prev</a><span>|</span><a href="#41375635">next</a><span>|</span><label class="collapse" for="c-41376547">[-]</label><label class="expand" for="c-41376547">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t believe the pricing is in the same ballpark.<p>There&#x27;s also other differences, such as port counts. 
AFAICT Spectrum switches at 400Gbps have up to 128 ports whereas equivalent Infiniband NDR Quantum only have 64 [0].<p>When building clusters of 32K+ GPUs the network cost, power, transceivers etc start to add up.<p>[0] <a href="https:&#x2F;&#x2F;www.semianalysis.com&#x2F;p&#x2F;100000-h100-clusters-power-network" rel="nofollow">https:&#x2F;&#x2F;www.semianalysis.com&#x2F;p&#x2F;100000-h100-clusters-power-ne...</a></div><br/><div id="41376637" class="c"><input type="checkbox" id="c-41376637" checked=""/><div class="controls bullet"><span class="by">_zoltan_</span><span>|</span><a href="#41375584">root</a><span>|</span><a href="#41376547">parent</a><span>|</span><a href="#41375635">next</a><span>|</span><label class="collapse" for="c-41376637">[-]</label><label class="expand" for="c-41376637">[1 more]</label></div><br/><div class="children"><div class="content">With IB (Quantum X800) you have 72x physical OSFP ports that can do 144 ports. Each electrical lane runs at 200G-PAM4, so it&#x27;s 144x(4x200G-PAM4).<p>With the SN5600 for Ethernet (Spectrum-X), which is 64 physical ports, you&#x27;re running each port at 8x100G-PAM4).</div><br/></div></div></div></div></div></div></div></div><div id="41375635" class="c"><input type="checkbox" id="c-41375635" checked=""/><div class="controls bullet"><span class="by">exabrial</span><span>|</span><a href="#41375584">parent</a><span>|</span><a href="#41375905">prev</a><span>|</span><a href="#41375647">next</a><span>|</span><label class="collapse" for="c-41375635">[-]</label><label class="expand" for="c-41375635">[4 more]</label></div><br/><div class="children"><div class="content">whats the credit system?</div><br/><div id="41375758" class="c"><input type="checkbox" id="c-41375758" checked=""/><div class="controls bullet"><span class="by">xtacy</span><span>|</span><a href="#41375584">root</a><span>|</span><a href="#41375635">parent</a><span>|</span><a href="#41375647">next</a><span>|</span><label class="collapse" for="c-41375758">[-]</label><label class="expand" for="c-41375758">[3 more]</label></div><br/><div class="children"><div class="content">OP is referring to &quot;Credit based flow control&quot;, which is a way to ensure a sender does not overwhelm a receiver with more data than it can handle.<p>Usually, this is line-rate, but if the other side is slow for whatever reason (say the consumer is not draining data), you wouldn&#x27;t want the sender to continue sending data.<p>If you also have N hosts sending data to 1 host, you would need some way of distributing the bandwidth among the N hosts.  That&#x27;s another scenario where the credit system comes.  Think of it as an admission control for packets so as to guarantee that no packets are lost.  Congestion control is a looser form of admission control that tolerates lossy networks, by retransmitting packets should they be lost.</div><br/><div id="41375970" class="c"><input type="checkbox" id="c-41375970" checked=""/><div class="controls bullet"><span class="by">jcims</span><span>|</span><a href="#41375584">root</a><span>|</span><a href="#41375758">parent</a><span>|</span><a href="#41375647">next</a><span>|</span><label class="collapse" for="c-41375970">[-]</label><label class="expand" for="c-41375970">[2 more]</label></div><br/><div class="children"><div class="content">Those token ring folks were on to something.</div><br/><div id="41376933" class="c"><input type="checkbox" id="c-41376933" checked=""/><div class="controls bullet"><span class="by">philjohn</span><span>|</span><a href="#41375584">root</a><span>|</span><a href="#41375970">parent</a><span>|</span><a href="#41375647">next</a><span>|</span><label class="collapse" for="c-41376933">[-]</label><label class="expand" for="c-41376933">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;d respond to your kind words, but there are two faulty cables in their token ring network, and as such, no redundant paths for the beacon frame to get through.</div><br/></div></div></div></div></div></div></div></div><div id="41375647" class="c"><input type="checkbox" id="c-41375647" checked=""/><div class="controls bullet"><span class="by">yumraj</span><span>|</span><a href="#41375584">parent</a><span>|</span><a href="#41375635">prev</a><span>|</span><a href="#41375663">next</a><span>|</span><label class="collapse" for="c-41375647">[-]</label><label class="expand" for="c-41375647">[1 more]</label></div><br/><div class="children"><div class="content">&gt; infiniband<p>Or I guess even RoCE</div><br/></div></div><div id="41375656" class="c"><input type="checkbox" id="c-41375656" checked=""/><div class="controls bullet"><span class="by">kamikaz1k</span><span>|</span><a href="#41375584">parent</a><span>|</span><a href="#41375663">prev</a><span>|</span><a href="#41375772">next</a><span>|</span><label class="collapse" for="c-41375656">[-]</label><label class="expand" for="c-41375656">[4 more]</label></div><br/><div class="children"><div class="content">Not really familiar with this space but I think the entire Dojo&#x2F;DIY strategy was kicked off because Elon wanted to not get cornered on supply or cost by nvidia. And infiniband is an nvidia technology, so they wouldnât use that simply from strategic POV.<p>Are there other technologies they could have used?<p>Also, the 80us is supposed to be the worst case, where typical is supposed to be &lt;10us. Again not knowing anything about infiniband, whatâs the typical perf? I tried to google but the people who are talking about it are in the know in ways Iâm not.<p>Thanks!</div><br/><div id="41376483" class="c"><input type="checkbox" id="c-41376483" checked=""/><div class="controls bullet"><span class="by">charleshn</span><span>|</span><a href="#41375584">root</a><span>|</span><a href="#41375656">parent</a><span>|</span><a href="#41375856">next</a><span>|</span><label class="collapse" for="c-41376483">[-]</label><label class="expand" for="c-41376483">[2 more]</label></div><br/><div class="children"><div class="content">Indeed, it seems that 80usec is just given as an upper bound based on the 1MB buffer at 100Gbps.<p>It is definitely possible to go much lower than 80usec on Ethernet. But obviously it depends on the scale, utilisation etc.<p>At the sizes of GPU clusters we&#x27;re talking about these days - 32K and up - things get tricky.<p>The main alternative to Infiniband used in the industry is RoCE - Meta has written a lot about it [0].<p>There&#x27;s several reasons to avoid Infiniband, such as cost, availability, vendor lock in, lack of experience etc.<p>Those are some of the reasons why many players are trying hard to make Ethernet work, see Ultra Ethernet [1].<p>[0] <a href="https:&#x2F;&#x2F;engineering.fb.com&#x2F;2024&#x2F;08&#x2F;05&#x2F;data-center-engineering&#x2F;roce-network-distributed-ai-training-at-scale&#x2F;" rel="nofollow">https:&#x2F;&#x2F;engineering.fb.com&#x2F;2024&#x2F;08&#x2F;05&#x2F;data-center-engineerin...</a><p>[1] <a href="https:&#x2F;&#x2F;ultraethernet.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ultraethernet.org&#x2F;</a></div><br/><div id="41376596" class="c"><input type="checkbox" id="c-41376596" checked=""/><div class="controls bullet"><span class="by">foobiekr</span><span>|</span><a href="#41375584">root</a><span>|</span><a href="#41376483">parent</a><span>|</span><a href="#41375856">next</a><span>|</span><label class="collapse" for="c-41376596">[-]</label><label class="expand" for="c-41376596">[1 more]</label></div><br/><div class="children"><div class="content">Itâs not even rare for Ethernet to be 1.5usec or less latency per switch. 80usec would be impossible to sell in any compute cluster.</div><br/></div></div></div></div><div id="41375856" class="c"><input type="checkbox" id="c-41375856" checked=""/><div class="controls bullet"><span class="by">justahuman74</span><span>|</span><a href="#41375584">root</a><span>|</span><a href="#41375656">parent</a><span>|</span><a href="#41376483">prev</a><span>|</span><a href="#41375772">next</a><span>|</span><label class="collapse" for="c-41375856">[-]</label><label class="expand" for="c-41375856">[1 more]</label></div><br/><div class="children"><div class="content">Is RoCE no good?</div><br/></div></div></div></div></div></div><div id="41375772" class="c"><input type="checkbox" id="c-41375772" checked=""/><div class="controls bullet"><span class="by">choilive</span><span>|</span><a href="#41375584">prev</a><span>|</span><a href="#41375706">next</a><span>|</span><label class="collapse" for="c-41375772">[-]</label><label class="expand" for="c-41375772">[3 more]</label></div><br/><div class="children"><div class="content">This is all technically impressive but was it all technically necessary? Was infiniband really just not good enough? All this R&amp;D for a custom protocol and custom NICs seems to just be a massive flex of Tesla&#x27;s engineering muscle.</div><br/><div id="41376878" class="c"><input type="checkbox" id="c-41376878" checked=""/><div class="controls bullet"><span class="by">karlgkk</span><span>|</span><a href="#41375772">parent</a><span>|</span><a href="#41376759">next</a><span>|</span><label class="collapse" for="c-41376878">[-]</label><label class="expand" for="c-41376878">[1 more]</label></div><br/><div class="children"><div class="content">It wasn&#x27;t technically necessary, but neither was RISC-V. It&#x27;s a matter of licensing independence.</div><br/></div></div><div id="41376759" class="c"><input type="checkbox" id="c-41376759" checked=""/><div class="controls bullet"><span class="by">2rsf</span><span>|</span><a href="#41375772">parent</a><span>|</span><a href="#41376878">prev</a><span>|</span><a href="#41375706">next</a><span>|</span><label class="collapse" for="c-41376759">[-]</label><label class="expand" for="c-41376759">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not only about being &quot;good enough&quot; but also about reliability and maintenance, new protocols and hardware may take time to mature while other solutions are already there. Ah, wait, Tesla doesn&#x27;t care about those kind of things too much...</div><br/></div></div></div></div><div id="41375706" class="c"><input type="checkbox" id="c-41375706" checked=""/><div class="controls bullet"><span class="by">xtacy</span><span>|</span><a href="#41375772">prev</a><span>|</span><a href="#41376847">next</a><span>|</span><label class="collapse" for="c-41375706">[-]</label><label class="expand" for="c-41375706">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s also a bit odd that they do not implement congestion control.  Congestion control is fundamental unless you only have point-to-point data transfers, which is rarely the case.  All-reduce operation during training requires N to 1 data transfer.  In these scenarios the sender needs to control its data transfer rates so as to not overwhelm not just the receiver, but also the network... if this is not done, it will cause congestion collapse (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Network_congestion#:~:text=service%20attack.-,Congestive%20collapse,-%5Bedit%5D" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Network_congestion#:~:text=ser...</a>).</div><br/><div id="41376073" class="c"><input type="checkbox" id="c-41376073" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#41375706">parent</a><span>|</span><a href="#41375962">next</a><span>|</span><label class="collapse" for="c-41376073">[-]</label><label class="expand" for="c-41376073">[2 more]</label></div><br/><div class="children"><div class="content">Current public SOTA seems to be âno congestion controlâ<p>&gt; We proceeded without DCQCN for our 400G deployments. At this time, we have had over a year of experience with just PFC for flow control, without any other transport-level congestion control. We have observed stable performance and lack of persistent congestion for training collectives.<p><a href="https:&#x2F;&#x2F;engineering.fb.com&#x2F;2024&#x2F;08&#x2F;05&#x2F;data-center-engineering&#x2F;roce-network-distributed-ai-training-at-scale&#x2F;" rel="nofollow">https:&#x2F;&#x2F;engineering.fb.com&#x2F;2024&#x2F;08&#x2F;05&#x2F;data-center-engineerin...</a></div><br/><div id="41376783" class="c"><input type="checkbox" id="c-41376783" checked=""/><div class="controls bullet"><span class="by">_zoltan_</span><span>|</span><a href="#41375706">root</a><span>|</span><a href="#41376073">parent</a><span>|</span><a href="#41375962">next</a><span>|</span><label class="collapse" for="c-41376783">[-]</label><label class="expand" for="c-41376783">[1 more]</label></div><br/><div class="children"><div class="content">PFC <i>is</i> congestion control.</div><br/></div></div></div></div><div id="41375962" class="c"><input type="checkbox" id="c-41375962" checked=""/><div class="controls bullet"><span class="by">jcims</span><span>|</span><a href="#41375706">parent</a><span>|</span><a href="#41376073">prev</a><span>|</span><a href="#41376847">next</a><span>|</span><label class="collapse" for="c-41375962">[-]</label><label class="expand" for="c-41375962">[2 more]</label></div><br/><div class="children"><div class="content">I probably shouldn&#x27;t be commenting because I don&#x27;t have any experience at this level, but given it&#x27;s a closed system where they control supply and demand it seems they could manage away most congestion issues with scheduling&#x2F;orchestration.  They still have a primitive flow control in the protocol and it seems like you could create something akin to a virtual sliding window just by instrumenting the retransmits.<p>But now I am curious with the distribution of observed window sizes is in the wild.<p>Edit: I&#x27;d bet the simpler protocol is more vulnerable to various spoofing attacks though.<p>Edit2: Lol I hope the frame IDs are for illustrative purposes only - <a href="https:&#x2F;&#x2F;chipsandcheese.com&#x2F;2024&#x2F;08&#x2F;27&#x2F;teslas-ttpoe-at-hot-chips-2024-replacing-tcp-for-low-latency-applications&#x2F;hc2024_tesla_ttp_handshakes&#x2F;" rel="nofollow">https:&#x2F;&#x2F;chipsandcheese.com&#x2F;2024&#x2F;08&#x2F;27&#x2F;teslas-ttpoe-at-hot-ch...</a></div><br/><div id="41376032" class="c"><input type="checkbox" id="c-41376032" checked=""/><div class="controls bullet"><span class="by">xtacy</span><span>|</span><a href="#41375706">root</a><span>|</span><a href="#41375962">parent</a><span>|</span><a href="#41376847">next</a><span>|</span><label class="collapse" for="c-41376032">[-]</label><label class="expand" for="c-41376032">[1 more]</label></div><br/><div class="children"><div class="content">In principle, with perfect knowledge of flows at any given instant, you can assign credits&#x2F;rate-of-transmission for each flow to prevent congestion.  But, in practice this is somewhat nuanced to build, and there are various tradeoffs to consider: what happens if the flows are so short that coordinating with a centralised scheduler incurs a latency overhead that is comparable to the flow duration?  There&#x27;s been research to demonstrate that one can strike a sweet spot, but I don&#x27;t think it&#x27;s practical nor has it been really deployed in the wild.  And of course, this scheduler has to be made reliable as it&#x27;s a single point of failure.<p>Such ideas are, however, worth revisiting when the workload is unique enough (in this case, it is), and the performance gains are so big enough...</div><br/></div></div></div></div></div></div><div id="41376847" class="c"><input type="checkbox" id="c-41376847" checked=""/><div class="controls bullet"><span class="by">sgt</span><span>|</span><a href="#41375706">prev</a><span>|</span><a href="#41376196">next</a><span>|</span><label class="collapse" for="c-41376847">[-]</label><label class="expand" for="c-41376847">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this what Dolphin Interconnect have been doing for a couple of decades? <a href="https:&#x2F;&#x2F;www.dolphinics.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.dolphinics.com&#x2F;</a></div><br/></div></div><div id="41376196" class="c"><input type="checkbox" id="c-41376196" checked=""/><div class="controls bullet"><span class="by">jeroenvlek</span><span>|</span><a href="#41376847">prev</a><span>|</span><a href="#41375803">next</a><span>|</span><label class="collapse" for="c-41376196">[-]</label><label class="expand" for="c-41376196">[1 more]</label></div><br/><div class="children"><div class="content">Seems like Tesla could really benefit from this about to be released optimizer that reduces intra-GPU communication [0].<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;NousResearch&#x2F;DisTrO&#x2F;blob&#x2F;main&#x2F;A_Preliminary_Report_on_DisTrO.pdf">https:&#x2F;&#x2F;github.com&#x2F;NousResearch&#x2F;DisTrO&#x2F;blob&#x2F;main&#x2F;A_Prelimina...</a></div><br/></div></div><div id="41375803" class="c"><input type="checkbox" id="c-41375803" checked=""/><div class="controls bullet"><span class="by">speransky</span><span>|</span><a href="#41376196">prev</a><span>|</span><a href="#41375575">next</a><span>|</span><label class="collapse" for="c-41375803">[-]</label><label class="expand" for="c-41375803">[1 more]</label></div><br/><div class="children"><div class="content">Tuned RoCE with udp is really low latency and no need to implement extra layer of silicon. May be there are more motivation then described in article</div><br/></div></div><div id="41375575" class="c"><input type="checkbox" id="c-41375575" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#41375803">prev</a><span>|</span><a href="#41376060">next</a><span>|</span><label class="collapse" for="c-41375575">[-]</label><label class="expand" for="c-41375575">[1 more]</label></div><br/><div class="children"><div class="content">Interesting! No mention of UDP, or the application being run, or the GPU&#x2F;TPUs on the nodes, so it&#x27;ll have to be a mystery as to how much bang for their buck they&#x27;re getting with this particular bit of work.<p>What&#x27;s disappointing is that it&#x27;s impossible to do a new protocol on the Internet because of all the middleware boxes that drop packets that aren&#x27;t IMCP or TCP or UDP.</div><br/></div></div><div id="41376060" class="c"><input type="checkbox" id="c-41376060" checked=""/><div class="controls bullet"><span class="by">efitz</span><span>|</span><a href="#41375575">prev</a><span>|</span><a href="#41375582">next</a><span>|</span><label class="collapse" for="c-41376060">[-]</label><label class="expand" for="c-41376060">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I never FIN my connections eithRST</div><br/></div></div><div id="41375582" class="c"><input type="checkbox" id="c-41375582" checked=""/><div class="controls bullet"><span class="by">7e</span><span>|</span><a href="#41376060">prev</a><span>|</span><label class="collapse" for="c-41375582">[-]</label><label class="expand" for="c-41375582">[10 more]</label></div><br/><div class="children"><div class="content">100Gbps Ethernet cards? The world has moved way past that for training. Their accelerator stack must be really slow if this is good enough for them.</div><br/><div id="41375674" class="c"><input type="checkbox" id="c-41375674" checked=""/><div class="controls bullet"><span class="by">teruakohatu</span><span>|</span><a href="#41375582">parent</a><span>|</span><label class="collapse" for="c-41375674">[-]</label><label class="expand" for="c-41375674">[9 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t say they can&#x27;t aggregate links. I wouldn&#x27;t say the world has moved way past that yet, but Tesla probably doesn&#x27;t want to be dependant, like everyone else but Apple, on Nvidia (InfiniBand).</div><br/><div id="41375728" class="c"><input type="checkbox" id="c-41375728" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#41375582">root</a><span>|</span><a href="#41375674">parent</a><span>|</span><a href="#41376788">next</a><span>|</span><label class="collapse" for="c-41375728">[-]</label><label class="expand" for="c-41375728">[7 more]</label></div><br/><div class="children"><div class="content">&gt; like everyone else but Apple, on Nvidia (InfiniBand)<p>Google also does not depend on NVIDIA, thx to TPUs. Rents NVIDIA GPUs to external customers - sure, it&#x27;s a nice side business, but internally TPUs are king and there&#x27;s no dependency on NVIDIA for that.</div><br/><div id="41375813" class="c"><input type="checkbox" id="c-41375813" checked=""/><div class="controls bullet"><span class="by">morepork</span><span>|</span><a href="#41375582">root</a><span>|</span><a href="#41375728">parent</a><span>|</span><a href="#41376602">next</a><span>|</span><label class="collapse" for="c-41375813">[-]</label><label class="expand" for="c-41375813">[3 more]</label></div><br/><div class="children"><div class="content">Likewise Microsoft and Meta have developed in house AI chips, but I don&#x27;t know what fraction of their AI workloads run on them.</div><br/><div id="41376458" class="c"><input type="checkbox" id="c-41376458" checked=""/><div class="controls bullet"><span class="by">theincredulousk</span><span>|</span><a href="#41375582">root</a><span>|</span><a href="#41375813">parent</a><span>|</span><a href="#41376602">next</a><span>|</span><label class="collapse" for="c-41376458">[-]</label><label class="expand" for="c-41376458">[2 more]</label></div><br/><div class="children"><div class="content">Not a meaningful amount :). Their âAI chipsâ are, for now, marketing.</div><br/><div id="41377022" class="c"><input type="checkbox" id="c-41377022" checked=""/><div class="controls bullet"><span class="by">SuchAnonMuchWow</span><span>|</span><a href="#41375582">root</a><span>|</span><a href="#41376458">parent</a><span>|</span><a href="#41376602">next</a><span>|</span><label class="collapse" for="c-41377022">[-]</label><label class="expand" for="c-41377022">[1 more]</label></div><br/><div class="children"><div class="content">From what I know, Meta AI chips are used in production today, but are made for their recommendations tasks which is a very different IA than GPTs and LLMs for which they still rely on GPUs.</div><br/></div></div></div></div></div></div><div id="41376602" class="c"><input type="checkbox" id="c-41376602" checked=""/><div class="controls bullet"><span class="by">foobiekr</span><span>|</span><a href="#41375582">root</a><span>|</span><a href="#41375728">parent</a><span>|</span><a href="#41375813">prev</a><span>|</span><a href="#41376471">next</a><span>|</span><label class="collapse" for="c-41376602">[-]</label><label class="expand" for="c-41376602">[2 more]</label></div><br/><div class="children"><div class="content">This is not true at all.</div><br/><div id="41376696" class="c"><input type="checkbox" id="c-41376696" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#41375582">root</a><span>|</span><a href="#41376602">parent</a><span>|</span><a href="#41376471">next</a><span>|</span><label class="collapse" for="c-41376696">[-]</label><label class="expand" for="c-41376696">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s very hard to respond to a comment like that, since there&#x27;s no specifics, just a plain disagreement.<p>On my side, I would like to point that the today HN thread ([1]) that discusses a paper GameNGen ([2]) that runs Doom with diffusion models was trained on TPUs.<p>I don&#x27;t see a dependency on NVIDIA there.<p>If there&#x27;s a more specific rebuttal to my original statement, please, don&#x27;t hesitate to state it.<p>1. <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41375548">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41375548</a><p>2. <a href="https:&#x2F;&#x2F;gamengen.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;gamengen.github.io&#x2F;</a><p>3. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.14837" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.14837</a></div><br/></div></div></div></div><div id="41376471" class="c"><input type="checkbox" id="c-41376471" checked=""/><div class="controls bullet"><span class="by">theincredulousk</span><span>|</span><a href="#41375582">root</a><span>|</span><a href="#41375728">parent</a><span>|</span><a href="#41376602">prev</a><span>|</span><a href="#41376788">next</a><span>|</span><label class="collapse" for="c-41376471">[-]</label><label class="expand" for="c-41376471">[1 more]</label></div><br/><div class="children"><div class="content">TPUs are essentially garbage compared to NVIDIA hardware.  TPUs are king of nothing, but a primary ingredient in Kool-Aid</div><br/></div></div></div></div><div id="41376788" class="c"><input type="checkbox" id="c-41376788" checked=""/><div class="controls bullet"><span class="by">stonogo</span><span>|</span><a href="#41375582">root</a><span>|</span><a href="#41375674">parent</a><span>|</span><a href="#41375728">prev</a><span>|</span><label class="collapse" for="c-41376788">[-]</label><label class="expand" for="c-41376788">[1 more]</label></div><br/><div class="children"><div class="content">You can get 400gbit ethernet from half a dozen vendors.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>