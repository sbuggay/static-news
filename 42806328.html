<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1737709274090" as="style"/><link rel="stylesheet" href="styles.css?v=1737709274090"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/ggml-org/llama.vim">Llama.vim – Local LLM-assisted text completion</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>kgwgk</span> | <span>92 comments</span></div><br/><div><div id="42806546" class="c"><input type="checkbox" id="c-42806546" checked=""/><div class="controls bullet"><span class="by">ggerganov</span><span>|</span><a href="#42811378">next</a><span>|</span><label class="collapse" for="c-42806546">[-]</label><label class="expand" for="c-42806546">[23 more]</label></div><br/><div class="children"><div class="content">Hi HN, happy to see this here!<p>I highly recommend to take a look at the technical details of the server implementation that enables large context usage with this plugin - I think it is interesting and has some cool ideas [0].<p>Also, the same plugin is available for VS Code [1].<p>Let me know if you have any questions about the plugin - happy to explain. Btw, the performance has improved compared to what is seen in the README videos thanks to client-side caching.<p>[0] - <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;9787">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;9787</a><p>[1] - <a href="https:&#x2F;&#x2F;github.com&#x2F;ggml-org&#x2F;llama.vscode">https:&#x2F;&#x2F;github.com&#x2F;ggml-org&#x2F;llama.vscode</a></div><br/><div id="42806612" class="c"><input type="checkbox" id="c-42806612" checked=""/><div class="controls bullet"><span class="by">amrrs</span><span>|</span><a href="#42806546">parent</a><span>|</span><a href="#42807599">next</a><span>|</span><label class="collapse" for="c-42806612">[-]</label><label class="expand" for="c-42806612">[6 more]</label></div><br/><div class="children"><div class="content">For those who don&#x27;t know, He is the gg of `gguf`. Thank you for all your contributions! Literally the core of Ollama, LMStudio, Jan and multiple other apps!</div><br/><div id="42810055" class="c"><input type="checkbox" id="c-42810055" checked=""/><div class="controls bullet"><span class="by">kennethologist</span><span>|</span><a href="#42806546">root</a><span>|</span><a href="#42806612">parent</a><span>|</span><a href="#42807064">next</a><span>|</span><label class="collapse" for="c-42810055">[-]</label><label class="expand" for="c-42810055">[1 more]</label></div><br/><div class="children"><div class="content">A. Legend. Thanks for having DeepSeek available so quickly in LM Studio.</div><br/></div></div><div id="42807064" class="c"><input type="checkbox" id="c-42807064" checked=""/><div class="controls bullet"><span class="by">sergiotapia</span><span>|</span><a href="#42806546">root</a><span>|</span><a href="#42806612">parent</a><span>|</span><a href="#42810055">prev</a><span>|</span><a href="#42807366">next</a><span>|</span><label class="collapse" for="c-42807064">[-]</label><label class="expand" for="c-42807064">[1 more]</label></div><br/><div class="children"><div class="content">well hot damn! killing it!</div><br/></div></div></div></div><div id="42807599" class="c"><input type="checkbox" id="c-42807599" checked=""/><div class="controls bullet"><span class="by">bangaladore</span><span>|</span><a href="#42806546">parent</a><span>|</span><a href="#42806612">prev</a><span>|</span><a href="#42806574">next</a><span>|</span><label class="collapse" for="c-42807599">[-]</label><label class="expand" for="c-42807599">[3 more]</label></div><br/><div class="children"><div class="content">Quick testing on vscode to see if I&#x27;d consider replacing Copilot with this.
Biggest showstopper right now for me is the output length is substantially small. The default length is set to 256, but even if I up it to 4096, I&#x27;m not getting any larger chunks of code.<p>Is this because of a max latency setting, or the internal prompt, or am I doing something wrong? Or is it only really make to try to autocomplete lines and not blocks like Copilot will.<p>Thanks :)</div><br/><div id="42807696" class="c"><input type="checkbox" id="c-42807696" checked=""/><div class="controls bullet"><span class="by">ggerganov</span><span>|</span><a href="#42806546">root</a><span>|</span><a href="#42807599">parent</a><span>|</span><a href="#42806574">next</a><span>|</span><label class="collapse" for="c-42807696">[-]</label><label class="expand" for="c-42807696">[2 more]</label></div><br/><div class="children"><div class="content">There are 4 stopping criteria atm:<p>- Generation time exceeded (configurable in the plugin config)<p>- Number of tokens exceeded (not the case since you increased it)<p>- Indentation - stops generating if the next line has shorter indent than the first line<p>- Small probability of the sampled token<p>Most likely you are hitting the last criteria. It&#x27;s something that should be improved in some way, but I am not very sure how. Currently, it is using a very basic token sampling strategy with a custom threshold logic to stop generating when the token probability is too low. Likely this logic is too conservative.</div><br/><div id="42807809" class="c"><input type="checkbox" id="c-42807809" checked=""/><div class="controls bullet"><span class="by">bangaladore</span><span>|</span><a href="#42806546">root</a><span>|</span><a href="#42807696">parent</a><span>|</span><a href="#42806574">next</a><span>|</span><label class="collapse" for="c-42807809">[-]</label><label class="expand" for="c-42807809">[1 more]</label></div><br/><div class="children"><div class="content">Hmm, interesting.<p>I didn&#x27;t catch T_max_predict_ms and upped that to 5000ms for fun. Doesn&#x27;t seem to make a difference, so I&#x27;m guessing you are right.</div><br/></div></div></div></div></div></div><div id="42806574" class="c"><input type="checkbox" id="c-42806574" checked=""/><div class="controls bullet"><span class="by">jerpint</span><span>|</span><a href="#42806546">parent</a><span>|</span><a href="#42807599">prev</a><span>|</span><a href="#42806791">next</a><span>|</span><label class="collapse" for="c-42806574">[-]</label><label class="expand" for="c-42806574">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for all of your incredible contributions!</div><br/></div></div><div id="42806791" class="c"><input type="checkbox" id="c-42806791" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#42806546">parent</a><span>|</span><a href="#42806574">prev</a><span>|</span><a href="#42808830">next</a><span>|</span><label class="collapse" for="c-42806791">[-]</label><label class="expand" for="c-42806791">[6 more]</label></div><br/><div class="children"><div class="content">KV cache shifting is interesting!<p>Just curious: how much of your code nowadays completed by LLM?</div><br/><div id="42806843" class="c"><input type="checkbox" id="c-42806843" checked=""/><div class="controls bullet"><span class="by">ggerganov</span><span>|</span><a href="#42806546">root</a><span>|</span><a href="#42806791">parent</a><span>|</span><a href="#42808830">next</a><span>|</span><label class="collapse" for="c-42806843">[-]</label><label class="expand" for="c-42806843">[5 more]</label></div><br/><div class="children"><div class="content">Yes, I think it is surprising that it works.<p>I think a fairly large amount, though can&#x27;t give a good number. I have been using Github Copilot from the very early days and with the release of Qwen Coder last year have fully switched to using local completions. I don&#x27;t use the chat workflow to code though, only FIM.</div><br/><div id="42811505" class="c"><input type="checkbox" id="c-42811505" checked=""/><div class="controls bullet"><span class="by">menaerus</span><span>|</span><a href="#42806546">root</a><span>|</span><a href="#42806843">parent</a><span>|</span><a href="#42808432">next</a><span>|</span><label class="collapse" for="c-42811505">[-]</label><label class="expand" for="c-42811505">[1 more]</label></div><br/><div class="children"><div class="content">Interesting approach.<p>Am I correct to understand that you&#x27;re basically minimizing the latencies and required compute&#x2F;mem-bw by avoiding the KV cache? And encoding the (local) context in the input tokens instead?<p>I ask this because you set the prompt&#x2F;context size to 0 (--ctx-size 0) and the batch size to 1024 (-b 1024). Former would mean that llama.cpp will only be using the context that is already encoded in the model itself but no local (code) context besides the one provided in the input tokens but perhaps I misunderstood something.<p>Thanks for your contributions and obviously the large amount of time you take to document your work!</div><br/></div></div><div id="42808432" class="c"><input type="checkbox" id="c-42808432" checked=""/><div class="controls bullet"><span class="by">gloflo</span><span>|</span><a href="#42806546">root</a><span>|</span><a href="#42806843">parent</a><span>|</span><a href="#42811505">prev</a><span>|</span><a href="#42808830">next</a><span>|</span><label class="collapse" for="c-42808432">[-]</label><label class="expand" for="c-42808432">[3 more]</label></div><br/><div class="children"><div class="content">What is FIM?</div><br/><div id="42808477" class="c"><input type="checkbox" id="c-42808477" checked=""/><div class="controls bullet"><span class="by">jjnoakes</span><span>|</span><a href="#42806546">root</a><span>|</span><a href="#42808432">parent</a><span>|</span><a href="#42808479">next</a><span>|</span><label class="collapse" for="c-42808477">[-]</label><label class="expand" for="c-42808477">[1 more]</label></div><br/><div class="children"><div class="content">Fill-in-the-middle. If your cursor is in the middle of a file instead of at the end, then the LLM will consider text after the cursor in addition to the text before the cursor. Some LLMs can only look before the cursor; for coding,.ones that can FIM work better (for me at least).</div><br/></div></div><div id="42808479" class="c"><input type="checkbox" id="c-42808479" checked=""/><div class="controls bullet"><span class="by">rav</span><span>|</span><a href="#42806546">root</a><span>|</span><a href="#42808432">parent</a><span>|</span><a href="#42808477">prev</a><span>|</span><a href="#42808830">next</a><span>|</span><label class="collapse" for="c-42808479">[-]</label><label class="expand" for="c-42808479">[1 more]</label></div><br/><div class="children"><div class="content">FIM is &quot;fill in middle&quot;, i.e. completion in a text editor using context on both sides of the cursor.</div><br/></div></div></div></div></div></div></div></div><div id="42808830" class="c"><input type="checkbox" id="c-42808830" checked=""/><div class="controls bullet"><span class="by">attentive</span><span>|</span><a href="#42806546">parent</a><span>|</span><a href="#42806791">prev</a><span>|</span><a href="#42806714">next</a><span>|</span><label class="collapse" for="c-42808830">[-]</label><label class="expand" for="c-42808830">[2 more]</label></div><br/><div class="children"><div class="content">Is it correct to assume this plugin won&#x27;t work with ollama?<p>If so, what&#x27;s ollama missing?</div><br/><div id="42809447" class="c"><input type="checkbox" id="c-42809447" checked=""/><div class="controls bullet"><span class="by">mistercheph</span><span>|</span><a href="#42806546">root</a><span>|</span><a href="#42808830">parent</a><span>|</span><a href="#42806714">next</a><span>|</span><label class="collapse" for="c-42809447">[-]</label><label class="expand" for="c-42809447">[1 more]</label></div><br/><div class="children"><div class="content">this plugin is designed specifically for the llama.cpp server api, if you want copilot like features with ollama, you can use an ollama instance as a drop-in replacement for github copilot with this plugin: <a href="https:&#x2F;&#x2F;github.com&#x2F;bernardo-bruning&#x2F;ollama-copilot">https:&#x2F;&#x2F;github.com&#x2F;bernardo-bruning&#x2F;ollama-copilot</a><p>There is also <a href="https:&#x2F;&#x2F;github.com&#x2F;olimorris&#x2F;codecompanion.nvim">https:&#x2F;&#x2F;github.com&#x2F;olimorris&#x2F;codecompanion.nvim</a> which doesn&#x27;t have text completion, but supports a lot of other AI editor workflows that I believe are inspired by Zed and supports ollama out of the box</div><br/></div></div></div></div><div id="42806714" class="c"><input type="checkbox" id="c-42806714" checked=""/><div class="controls bullet"><span class="by">nancyp</span><span>|</span><a href="#42806546">parent</a><span>|</span><a href="#42808830">prev</a><span>|</span><a href="#42807359">next</a><span>|</span><label class="collapse" for="c-42806714">[-]</label><label class="expand" for="c-42806714">[2 more]</label></div><br/><div class="children"><div class="content">TIL: VIM has it&#x27;s own language. Thanks Georgi for LLAMA.cpp!</div><br/><div id="42807152" class="c"><input type="checkbox" id="c-42807152" checked=""/><div class="controls bullet"><span class="by">nacs</span><span>|</span><a href="#42806546">root</a><span>|</span><a href="#42806714">parent</a><span>|</span><a href="#42807359">next</a><span>|</span><label class="collapse" for="c-42807152">[-]</label><label class="expand" for="c-42807152">[1 more]</label></div><br/><div class="children"><div class="content">Vim is incredibly extensible.<p>You can use C or VIMscript but programs like Neovim support Lua as well which makes it really easy to make plugins.</div><br/></div></div></div></div><div id="42807359" class="c"><input type="checkbox" id="c-42807359" checked=""/><div class="controls bullet"><span class="by">halyconWays</span><span>|</span><a href="#42806546">parent</a><span>|</span><a href="#42806714">prev</a><span>|</span><a href="#42807193">next</a><span>|</span><label class="collapse" for="c-42807359">[-]</label><label class="expand" for="c-42807359">[1 more]</label></div><br/><div class="children"><div class="content">Please make one for Jetbrains&#x27; IDEs!</div><br/></div></div></div></div><div id="42811378" class="c"><input type="checkbox" id="c-42811378" checked=""/><div class="controls bullet"><span class="by">mijoharas</span><span>|</span><a href="#42806546">prev</a><span>|</span><a href="#42810842">next</a><span>|</span><label class="collapse" for="c-42811378">[-]</label><label class="expand" for="c-42811378">[2 more]</label></div><br/><div class="children"><div class="content">Can anyone compare this to Tabbyml?[0] I just set that up yesterday for emacs to check it out.<p>The context gathering seems very interesting[1], and very vim-integrated, so I&#x27;m guessing there isn&#x27;t anything very similar for Tabby. I skimmed the docs and saw some stuff about context for the Tabby chat feature[2] which I&#x27;m not super interested in using even if the docs adding sounds nice, but nothing obvious for the auto completion[3].<p>Does anyone have more insight or info to compare the two?<p>As a note, I quite like that the LLM context here &quot;follows&quot; what you&#x27;re doing. It seems like a nice idea. Does anyone know if anyone else does something similar?<p>[0] <a href="https:&#x2F;&#x2F;www.tabbyml.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.tabbyml.com&#x2F;</a><p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;9787#issue-2572915687">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;9787#issue-25729...</a> &quot;global context onwards&quot;<p>[2] <a href="https:&#x2F;&#x2F;tabby.tabbyml.com&#x2F;docs&#x2F;administration&#x2F;context&#x2F;" rel="nofollow">https:&#x2F;&#x2F;tabby.tabbyml.com&#x2F;docs&#x2F;administration&#x2F;context&#x2F;</a><p>[3] <a href="https:&#x2F;&#x2F;tabby.tabbyml.com&#x2F;docs&#x2F;administration&#x2F;code-completion&#x2F;" rel="nofollow">https:&#x2F;&#x2F;tabby.tabbyml.com&#x2F;docs&#x2F;administration&#x2F;code-completio...</a></div><br/><div id="42811513" class="c"><input type="checkbox" id="c-42811513" checked=""/><div class="controls bullet"><span class="by">ghthor</span><span>|</span><a href="#42811378">parent</a><span>|</span><a href="#42810842">next</a><span>|</span><label class="collapse" for="c-42811513">[-]</label><label class="expand" for="c-42811513">[1 more]</label></div><br/><div class="children"><div class="content">I’ve been using Tabby happily since May 2023</div><br/></div></div></div></div><div id="42810842" class="c"><input type="checkbox" id="c-42810842" checked=""/><div class="controls bullet"><span class="by">estreeper</span><span>|</span><a href="#42811378">prev</a><span>|</span><a href="#42807189">next</a><span>|</span><label class="collapse" for="c-42810842">[-]</label><label class="expand" for="c-42810842">[1 more]</label></div><br/><div class="children"><div class="content">Very exciting - I&#x27;m a long-time vim user but most of my coworkers use VSCode, and I&#x27;ve been wanting to try out in-editor completion tools like this.<p>After using it for a couple hours (on Elixir code) with Qwen2.5-Coder-3B and no attempts to customize it, this checks a lot of boxes for me:<p><pre><code>  - I pretty much want fancy autocomplete: filling in obvious things and saving my fingers the work, and these suggestions are pretty good
  - the default keybindings work for me, I like that I can keep current line or multi-line suggestions
  - no concerns around sending code off to a third-party
  - works offline when I&#x27;m traveling
  - it&#x27;s fast!
</code></pre>
So I don&#x27;t need to remember how to run the server, I&#x27;ll probably set up a script to check if it&#x27;s running and if not start it up in the background and run vim, and alias vim to use that. I looked in the help documents but didn&#x27;t see a way to disable the &quot;stats&quot; text after the suggestions, though I&#x27;m not sure it will bother me that much.</div><br/></div></div><div id="42807189" class="c"><input type="checkbox" id="c-42807189" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#42810842">prev</a><span>|</span><a href="#42806594">next</a><span>|</span><label class="collapse" for="c-42807189">[-]</label><label class="expand" for="c-42807189">[5 more]</label></div><br/><div class="children"><div class="content">This guy is a national treasure and has contributed so much value to the open source AI ecosystem. I hope he’s able to attract enough funding to continue making software like this and releasing it as true “no strings attached” open source.</div><br/><div id="42808118" class="c"><input type="checkbox" id="c-42808118" checked=""/><div class="controls bullet"><span class="by">nacs</span><span>|</span><a href="#42807189">parent</a><span>|</span><a href="#42808542">next</a><span>|</span><label class="collapse" for="c-42808118">[-]</label><label class="expand" for="c-42808118">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This guy is a national treasure<p>Agreed but he&#x27;s an <i>international</i> treasure (his Github profile states Bulgaria).</div><br/></div></div><div id="42808542" class="c"><input type="checkbox" id="c-42808542" checked=""/><div class="controls bullet"><span class="by">feznyng</span><span>|</span><a href="#42807189">parent</a><span>|</span><a href="#42808118">prev</a><span>|</span><a href="#42807974">next</a><span>|</span><label class="collapse" for="c-42808542">[-]</label><label class="expand" for="c-42808542">[2 more]</label></div><br/><div class="children"><div class="content">They have: <a href="https:&#x2F;&#x2F;ggml.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ggml.ai&#x2F;</a> under the Company heading.</div><br/><div id="42810988" class="c"><input type="checkbox" id="c-42810988" checked=""/><div class="controls bullet"><span class="by">cosmojg</span><span>|</span><a href="#42807189">root</a><span>|</span><a href="#42808542">parent</a><span>|</span><a href="#42807974">next</a><span>|</span><label class="collapse" for="c-42810988">[-]</label><label class="expand" for="c-42810988">[1 more]</label></div><br/><div class="children"><div class="content">Georgi Gerganov is the &quot;gg&quot; in &quot;ggml&quot;</div><br/></div></div></div></div><div id="42807974" class="c"><input type="checkbox" id="c-42807974" checked=""/><div class="controls bullet"><span class="by">frankfrank13</span><span>|</span><a href="#42807189">parent</a><span>|</span><a href="#42808542">prev</a><span>|</span><a href="#42806594">next</a><span>|</span><label class="collapse" for="c-42807974">[-]</label><label class="expand" for="c-42807974">[1 more]</label></div><br/><div class="children"><div class="content">Hard agree. This alone replaces GH Copilot&#x2F;Cursor ($10+ a month)</div><br/></div></div></div></div><div id="42806594" class="c"><input type="checkbox" id="c-42806594" checked=""/><div class="controls bullet"><span class="by">msoloviev</span><span>|</span><a href="#42807189">prev</a><span>|</span><a href="#42808640">next</a><span>|</span><label class="collapse" for="c-42806594">[-]</label><label class="expand" for="c-42806594">[1 more]</label></div><br/><div class="children"><div class="content">I wonder how the &quot;ring context&quot; works under the hood. I have previously had (and recently messed around with again) a somewhat similar project designed for a more toy&#x2F;exploratory setting (<a href="https:&#x2F;&#x2F;github.com&#x2F;blackhole89&#x2F;autopen">https:&#x2F;&#x2F;github.com&#x2F;blackhole89&#x2F;autopen</a> - demo video at <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=1O1T2q2t7i4" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=1O1T2q2t7i4</a>), and one of the main problems to address definitively is the question of how to manage your KV cache cleverly so you don&#x27;t have to constantly perform too much expensive recomputation whenever the buffer undergoes local changes.<p>The solution I came up with involved maintaining a tree of tokens branching whenever an alternative next token was explored, with full LLM state snapshots at fixed depth intervals so that the buffer would only have to be &quot;replayed&quot; for a few tokens when something changed. I wonder if there are some mathematical properties of how the important parts of the state (really, the KV cache, which can be thought of as a partial precomputation of the operation that one LLM iteration performs on the context) work that could have made this more efficient, like to avoid saving full snapshots or perhaps to be able to prune the &quot;oldest&quot; tokens out of a state efficiently.<p>(edit: Georgi&#x27;s comment that beat me by 3 minutes appears to be pointing at information that would go some way to answer my questions!)</div><br/></div></div><div id="42808640" class="c"><input type="checkbox" id="c-42808640" checked=""/><div class="controls bullet"><span class="by">h14h</span><span>|</span><a href="#42806594">prev</a><span>|</span><a href="#42811289">next</a><span>|</span><label class="collapse" for="c-42808640">[-]</label><label class="expand" for="c-42808640">[3 more]</label></div><br/><div class="children"><div class="content">A little bit of a tangent, but I&#x27;m really curious what benefits could come from integrating these LLM tools more closely with data from LSPs, compilers, and other static analysis tools.<p>Intuitively, it seems like you could provide much more context and better output as a result. Even better would be if you could fine-tune LLMs on a per-language basis and ship them alongside typical editor tooling.<p>A problem I see w&#x2F; these AI tools is that they work much better with old, popular languages, and I worry that this will grow as a significant factor when choosing a language. Anecdotally, I see far better results when using TypeScript than Gleam, for example.<p>It would be very cool to be able to install a Gleam-specific model that could be fed data from the LSP and compiler, and wouldn&#x27;t constantly hallucinate invalid syntax. I also wonder if, with additional context &amp; fine-tuning, you could make these models smaller and more feasible to run locally on modest hardware.</div><br/><div id="42809149" class="c"><input type="checkbox" id="c-42809149" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42808640">parent</a><span>|</span><a href="#42809689">next</a><span>|</span><label class="collapse" for="c-42809149">[-]</label><label class="expand" for="c-42809149">[1 more]</label></div><br/><div class="children"><div class="content">&gt; work much better with old, popular languages<p>I think this will improve significantly over time when when hardware becomes cheaper. As long as newer languages can map to older languages (syntax&#x2F;function wise), we should be able generate enough synthetic data to make working with less known languages easier.</div><br/></div></div><div id="42809689" class="c"><input type="checkbox" id="c-42809689" checked=""/><div class="controls bullet"><span class="by">rabiescow</span><span>|</span><a href="#42808640">parent</a><span>|</span><a href="#42809149">prev</a><span>|</span><a href="#42811289">next</a><span>|</span><label class="collapse" for="c-42809689">[-]</label><label class="expand" for="c-42809689">[1 more]</label></div><br/><div class="children"><div class="content">you are free to contribute your own gleam llms. They&#x27;re only as good as their inputs, thus if there&#x27;s very few publicly available packages for a certain language that&#x27;s all the input they got...</div><br/></div></div></div></div><div id="42811289" class="c"><input type="checkbox" id="c-42811289" checked=""/><div class="controls bullet"><span class="by">mrinterweb</span><span>|</span><a href="#42808640">prev</a><span>|</span><a href="#42807230">next</a><span>|</span><label class="collapse" for="c-42811289">[-]</label><label class="expand" for="c-42811289">[1 more]</label></div><br/><div class="children"><div class="content">Been using this for a couple hours, and this is really nice. It is a great alternative to something like Github Copilot. Appreciate how simple and fast this is.</div><br/></div></div><div id="42807230" class="c"><input type="checkbox" id="c-42807230" checked=""/><div class="controls bullet"><span class="by">frankfrank13</span><span>|</span><a href="#42811289">prev</a><span>|</span><a href="#42807538">next</a><span>|</span><label class="collapse" for="c-42807230">[-]</label><label class="expand" for="c-42807230">[1 more]</label></div><br/><div class="children"><div class="content">Is this more or less the same as your VSCode version? (<a href="https:&#x2F;&#x2F;github.com&#x2F;ggml-org&#x2F;llama.vscode">https:&#x2F;&#x2F;github.com&#x2F;ggml-org&#x2F;llama.vscode</a>)</div><br/></div></div><div id="42807538" class="c"><input type="checkbox" id="c-42807538" checked=""/><div class="controls bullet"><span class="by">mohsen1</span><span>|</span><a href="#42807230">prev</a><span>|</span><a href="#42807877">next</a><span>|</span><label class="collapse" for="c-42807538">[-]</label><label class="expand" for="c-42807538">[1 more]</label></div><br/><div class="children"><div class="content">Terminal coding FTW!<p>And when you&#x27;re really stuck you can use DeepSeek R1 for a deeper analysis in your terminal using `askds`<p><a href="https:&#x2F;&#x2F;github.com&#x2F;bodo-run&#x2F;askds">https:&#x2F;&#x2F;github.com&#x2F;bodo-run&#x2F;askds</a></div><br/></div></div><div id="42807877" class="c"><input type="checkbox" id="c-42807877" checked=""/><div class="controls bullet"><span class="by">opk</span><span>|</span><a href="#42807538">prev</a><span>|</span><a href="#42808901">next</a><span>|</span><label class="collapse" for="c-42807877">[-]</label><label class="expand" for="c-42807877">[7 more]</label></div><br/><div class="children"><div class="content">Has anyone actually got this llama stuff to be usable on even moderate hardware? I find it just crashes because it doesn&#x27;t find enough RAM. I&#x27;ve got 2G of VRAM on an AMD graphics card and 16G of system RAM and that doesn&#x27;t seem to be enough. The impression I got from reading up was that it worked for most Apple stuff because the memory is unified and other than that, you need very expensive Nvidia GPUs with lots of VRAM. Are there any affordable options?</div><br/><div id="42808539" class="c"><input type="checkbox" id="c-42808539" checked=""/><div class="controls bullet"><span class="by">horsawlarway</span><span>|</span><a href="#42807877">parent</a><span>|</span><a href="#42808041">next</a><span>|</span><label class="collapse" for="c-42808539">[-]</label><label class="expand" for="c-42808539">[1 more]</label></div><br/><div class="children"><div class="content">Yes.  Although I suspect my definition of &quot;moderate hardware&quot; doesn&#x27;t really match yours.<p>I can run 2b-14b models just fine on the CPU on my laptop (framework 13 with 32gb ram).  They aren&#x27;t super fast, and the 14b models have limited context length unless I run a quantized version, but they run.<p>If you just want generation and it doesn&#x27;t need to be fast... drop the $200 for 128gb of system ram, and you can run the vast majority of the available models (up to ~70b quantized).  Note - it won&#x27;t be quick (expect 1-2 tokens&#x2F;second, sometimes less).<p>If you want something faster in the &quot;low end&quot; range still - look at picking up a pair of Nvidia p40s (~$400) which will give you 16gb of ram and be faster for 2b to 7b models.<p>If you want to hit my level for &quot;moderate&quot;, I use 2x3090 (I bought refurbed for ~$1600 a couple years ago) and they do quite a bit of work.  Ex - I get ~15t&#x2F;s generation for 70b 4 quant models, and 50-100t&#x2F;s for 7b models.  That&#x27;s plenty usable for basically everything I want to run at home.  They&#x27;re faster than the m2 pro I was issued for work, and a good chunk cheaper (the m2 was in the 3k range).<p>That said - the m1&#x2F;m2 macs are generally pretty zippy here, I was quite surprised at how well they perform.<p>Some folks claim to have success with the k80s, but I haven&#x27;t tried and while 24g vram for under $100 seems nice (even if it&#x27;s slow), the linux compatibility issues make me inclined to just go for the p40s right now.<p>I run some tasks on much older hardware (ex - willow inference runs on an old 4gb gtx 970 just fine)<p>So again - I&#x27;m not really sure we&#x27;d agree on moderate (I generally spend ~$1000 every 4-6 years to build a machine to play games, and the machine you&#x27;re describing would match the specs for a machine I would have built 12+ years ago)<p>But you just need literal memory.  bumping to 32gb of system ram would unlock a lot of stuff for you (at low speeds) and costs $50.  Bumping to 124gb only costs a couple hundred, and lets you run basically all of them (again - slowly).</div><br/></div></div><div id="42808041" class="c"><input type="checkbox" id="c-42808041" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#42807877">parent</a><span>|</span><a href="#42808539">prev</a><span>|</span><a href="#42808325">next</a><span>|</span><label class="collapse" for="c-42808041">[-]</label><label class="expand" for="c-42808041">[2 more]</label></div><br/><div class="children"><div class="content">2G is pretty low and the sizes things you can get to run fast on that set up probably aren&#x27;t particularly attractive. &quot;moderate hardware&quot; varies but you can grab a 12 GB RTX 3060 on ebay for ~$200. You can get a lot more RAM for $200 but it&#x27;ll be so slow compare the the GPU I&#x27;m not sure I&#x27;d recommend it if you actually want to use things like this interactively.<p>If &quot;moderate hardware&quot; is your average office PC then it&#x27;s unlikely to be very usable. Anyone with a gaming GPU from the last several years should be workable though.</div><br/><div id="42808693" class="c"><input type="checkbox" id="c-42808693" checked=""/><div class="controls bullet"><span class="by">horsawlarway</span><span>|</span><a href="#42807877">root</a><span>|</span><a href="#42808041">parent</a><span>|</span><a href="#42808325">next</a><span>|</span><label class="collapse" for="c-42808693">[-]</label><label class="expand" for="c-42808693">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll second this, actually - $250 for a 12gb rtx 3060 is probably a better buy than $400 for 2xp40s for 16gb.<p>It&#x27;d been a minute since I checked refurb prices and $250 for the rtx 3060 12gb is a good price.<p>Easier on the rest of the system than a 2x card setup, and is probably a drop in replacement.</div><br/></div></div></div></div><div id="42808325" class="c"><input type="checkbox" id="c-42808325" checked=""/><div class="controls bullet"><span class="by">bhelkey</span><span>|</span><a href="#42807877">parent</a><span>|</span><a href="#42808041">prev</a><span>|</span><a href="#42808017">next</a><span>|</span><label class="collapse" for="c-42808325">[-]</label><label class="expand" for="c-42808325">[1 more]</label></div><br/><div class="children"><div class="content">Have you tried Ollama [1]? You should be able to run a 8b model in RAM and a 1b model in VRAM.<p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42069453">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42069453</a></div><br/></div></div><div id="42808017" class="c"><input type="checkbox" id="c-42808017" checked=""/><div class="controls bullet"><span class="by">basilgohar</span><span>|</span><a href="#42807877">parent</a><span>|</span><a href="#42808325">prev</a><span>|</span><a href="#42809612">next</a><span>|</span><label class="collapse" for="c-42808017">[-]</label><label class="expand" for="c-42808017">[1 more]</label></div><br/><div class="children"><div class="content">I can run 7B models with Q4 quantization on a 7000 series AMD APU without GPU acceleration quite acceptably fast. This is with DDR5600 RAM which is the current roadblock for performance.<p>Larger models work but slow down. I do have 64GB of RAM but I think 32 could work. 16GB is pushing is, but should be possible if you don&#x27;t have anything else open.<p>Memory requirements depend on numerous factors. 2GB VRAM is not enough for most GenAI stuff today.</div><br/></div></div><div id="42809612" class="c"><input type="checkbox" id="c-42809612" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#42807877">parent</a><span>|</span><a href="#42808017">prev</a><span>|</span><a href="#42808901">next</a><span>|</span><label class="collapse" for="c-42809612">[-]</label><label class="expand" for="c-42809612">[1 more]</label></div><br/><div class="children"><div class="content">2g of vram is pretty bad...</div><br/></div></div></div></div><div id="42808901" class="c"><input type="checkbox" id="c-42808901" checked=""/><div class="controls bullet"><span class="by">binary132</span><span>|</span><a href="#42807877">prev</a><span>|</span><a href="#42810023">next</a><span>|</span><label class="collapse" for="c-42808901">[-]</label><label class="expand" for="c-42808901">[1 more]</label></div><br/><div class="children"><div class="content">I am curious to see what will be possible with consumer grade hardware and more improvements to quantization over the next decade.  Right now, even a 24GB gpu with the best models isn’t able to match the barely acceptable performance of hosted services I’m not willing to even pay $20 a month for.</div><br/></div></div><div id="42810023" class="c"><input type="checkbox" id="c-42810023" checked=""/><div class="controls bullet"><span class="by">cfiggers</span><span>|</span><a href="#42808901">prev</a><span>|</span><a href="#42808918">next</a><span>|</span><label class="collapse" for="c-42810023">[-]</label><label class="expand" for="c-42810023">[1 more]</label></div><br/><div class="children"><div class="content">Do people with &quot;Copilot+ PCs&quot; get benefits running stuff like this from the much-vaunted AI coprocessors in for e.g. Snapdragon X Elite chips?</div><br/></div></div><div id="42808918" class="c"><input type="checkbox" id="c-42808918" checked=""/><div class="controls bullet"><span class="by">morcus</span><span>|</span><a href="#42810023">prev</a><span>|</span><a href="#42809800">next</a><span>|</span><label class="collapse" for="c-42808918">[-]</label><label class="expand" for="c-42808918">[2 more]</label></div><br/><div class="children"><div class="content">Looking for advice from someone who knows about the space - Suppose I&#x27;m willing to go out and buy a card for this purpose, what&#x27;s a modestly priced graphics card with which I can get somewhat usable results running local LLM?</div><br/><div id="42810753" class="c"><input type="checkbox" id="c-42810753" checked=""/><div class="controls bullet"><span class="by">estreeper</span><span>|</span><a href="#42808918">parent</a><span>|</span><a href="#42809800">next</a><span>|</span><label class="collapse" for="c-42810753">[-]</label><label class="expand" for="c-42810753">[1 more]</label></div><br/><div class="children"><div class="content">This is a tough question to answer, because it depends a lot on what you want to do! One way to approach it may be to look at what models you want to run and check the amount of VRAM they need. A back-of-the-napkin method taken from here[0] is:<p><pre><code>    VRAM (GB) = 1.2 * number of parameters (in billions) * bits per parameter &#x2F; 8
</code></pre>
The 1.2 is just an estimation factor to account for the VRAM needed for things that <i>aren&#x27;t</i> model parameters.<p>Because quantization is often nearly free in terms of output quality, you should usually look for quantized versions. For example, Llama 3.2 uses 16-bit parameters but has a 4-bit quantized version, and looking at the formula above you can see that will allow you to run a 4x larger model.<p>Having enough VRAM will allow you to <i>run</i> a model, but performance is dependent on a lot of other factors. For a much deeper dive into how all of this works along with price&#x2F;dollar recommendations (though from last year!), Tim Dettmers wrote this excellent article: <a href="https:&#x2F;&#x2F;timdettmers.com&#x2F;2023&#x2F;01&#x2F;30&#x2F;which-gpu-for-deep-learning&#x2F;" rel="nofollow">https:&#x2F;&#x2F;timdettmers.com&#x2F;2023&#x2F;01&#x2F;30&#x2F;which-gpu-for-deep-learni...</a><p>Worth mentioning for the benefit of those who don&#x27;t want to buy a GPU: there are also models which have been converted to run on CPU.<p>[0] <a href="https:&#x2F;&#x2F;blog.runpod.io&#x2F;understanding-vram-and-how-much-your-llm-needs&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.runpod.io&#x2F;understanding-vram-and-how-much-your-...</a></div><br/></div></div></div></div><div id="42809800" class="c"><input type="checkbox" id="c-42809800" checked=""/><div class="controls bullet"><span class="by">awwaiid</span><span>|</span><a href="#42808918">prev</a><span>|</span><a href="#42806527">next</a><span>|</span><label class="collapse" for="c-42809800">[-]</label><label class="expand" for="c-42809800">[1 more]</label></div><br/><div class="children"><div class="content">The blinking cursor in demo videos is giving me heart palpitations! But this is super cool. It makes me wonder how Linux is doing on M* hardware.</div><br/></div></div><div id="42806527" class="c"><input type="checkbox" id="c-42806527" checked=""/><div class="controls bullet"><span class="by">jerpint</span><span>|</span><a href="#42809800">prev</a><span>|</span><a href="#42808657">next</a><span>|</span><label class="collapse" for="c-42806527">[-]</label><label class="expand" for="c-42806527">[7 more]</label></div><br/><div class="children"><div class="content">It’s funny because I actually use vim mostly when I don’t want LLM assisted code. Sometimes it just gets in the way.<p>If I do, I load up cursor with vim bindings.</div><br/><div id="42809505" class="c"><input type="checkbox" id="c-42809505" checked=""/><div class="controls bullet"><span class="by">rapind</span><span>|</span><a href="#42806527">parent</a><span>|</span><a href="#42807035">next</a><span>|</span><label class="collapse" for="c-42809505">[-]</label><label class="expand" for="c-42809505">[2 more]</label></div><br/><div class="children"><div class="content">What I&#x27;d really like is a non-intrusive LLM running that I can put on a third monitor which sees what I&#x27;m working on, while I work, and tries to mirror and make suggestions WITHOUT interjecting itself into my editor. Then I just want a mappable prompt command (like &lt;leader&gt;P) where I can provide feedback &#x2F; suggestions (or maybe I just do that via mic).<p>This way I can choose to just ignore it for the most part, but if I see something interesting, I can refine it, prompt, cherry pick, etc. I really don&#x27;t want autocomplete for anything.</div><br/><div id="42811381" class="c"><input type="checkbox" id="c-42811381" checked=""/><div class="controls bullet"><span class="by">survirtual</span><span>|</span><a href="#42806527">root</a><span>|</span><a href="#42809505">parent</a><span>|</span><a href="#42807035">next</a><span>|</span><label class="collapse" for="c-42811381">[-]</label><label class="expand" for="c-42811381">[1 more]</label></div><br/><div class="children"><div class="content">Hey I was working on something like this, because I wanted something similar.  I had it confined to just the terminal though, because I don&#x27;t want just a vim helper, I want an everything helper.<p>I was going to have the helper generate suggestions in a separate window, which could be moved to any monitor or screen.  It would make suggestions and autocompletes, and you can chat with it to generate commands etc on the fly.<p>Maybe I will pick it up again soon.</div><br/></div></div></div></div><div id="42807035" class="c"><input type="checkbox" id="c-42807035" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#42806527">parent</a><span>|</span><a href="#42809505">prev</a><span>|</span><a href="#42808657">next</a><span>|</span><label class="collapse" for="c-42807035">[-]</label><label class="expand" for="c-42807035">[4 more]</label></div><br/><div class="children"><div class="content">Funny. I think the most common usage I have is using it at the command line to write commands with vim set as my EDITOR so the AI completion really helps.<p>This will help for offline support (on planes and such).</div><br/><div id="42807374" class="c"><input type="checkbox" id="c-42807374" checked=""/><div class="controls bullet"><span class="by">qup</span><span>|</span><a href="#42806527">root</a><span>|</span><a href="#42807035">parent</a><span>|</span><a href="#42808657">next</a><span>|</span><label class="collapse" for="c-42807374">[-]</label><label class="expand" for="c-42807374">[3 more]</label></div><br/><div class="children"><div class="content">Can you more specifically talk about how you use this, like with a small example?</div><br/><div id="42807662" class="c"><input type="checkbox" id="c-42807662" checked=""/><div class="controls bullet"><span class="by">VMG</span><span>|</span><a href="#42806527">root</a><span>|</span><a href="#42807374">parent</a><span>|</span><a href="#42807419">next</a><span>|</span><label class="collapse" for="c-42807662">[-]</label><label class="expand" for="c-42807662">[1 more]</label></div><br/><div class="children"><div class="content">- composing a commit message<p>- anything bash script related</div><br/></div></div><div id="42807419" class="c"><input type="checkbox" id="c-42807419" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#42806527">root</a><span>|</span><a href="#42807374">parent</a><span>|</span><a href="#42807662">prev</a><span>|</span><a href="#42808657">next</a><span>|</span><label class="collapse" for="c-42807419">[-]</label><label class="expand" for="c-42807419">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I mentioned it here first and haven&#x27;t changed it since (Twitter link and links include video of use)<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34769611">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34769611</a><p>Which leads (used to lead?) here <a href="https:&#x2F;&#x2F;wiki.roshangeorge.dev&#x2F;index.php&#x2F;AI_Completion_In_The_Shell" rel="nofollow">https:&#x2F;&#x2F;wiki.roshangeorge.dev&#x2F;index.php&#x2F;AI_Completion_In_The...</a></div><br/></div></div></div></div></div></div></div></div><div id="42808657" class="c"><input type="checkbox" id="c-42808657" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#42806527">prev</a><span>|</span><a href="#42807114">next</a><span>|</span><label class="collapse" for="c-42808657">[-]</label><label class="expand" for="c-42808657">[1 more]</label></div><br/><div class="children"><div class="content">This looks very interesting. Can this be trained on the user&#x27;s codebase, or is the idea that everything must fit inside the context buffer?</div><br/></div></div><div id="42806507" class="c"><input type="checkbox" id="c-42806507" checked=""/><div class="controls bullet"><span class="by">dingnuts</span><span>|</span><a href="#42807114">prev</a><span>|</span><a href="#42807946">next</a><span>|</span><label class="collapse" for="c-42806507">[-]</label><label class="expand" for="c-42806507">[22 more]</label></div><br/><div class="children"><div class="content">Is anyone actually getting value out of these models? I wired one up to Emacs and the local models all produce a huge volume of garbage output.<p>Occasionally I find a hosted LLM useful but I haven&#x27;t found any output from the models I can run in Ollama on my gaming PC to be useful.<p>It&#x27;s all plausible-looking but incorrect. I feel like I&#x27;m taking crazy pills when I read about others&#x27; experiences. Surely I am not alone?</div><br/><div id="42806716" class="c"><input type="checkbox" id="c-42806716" checked=""/><div class="controls bullet"><span class="by">remexre</span><span>|</span><a href="#42806507">parent</a><span>|</span><a href="#42807515">next</a><span>|</span><label class="collapse" for="c-42806716">[-]</label><label class="expand" for="c-42806716">[6 more]</label></div><br/><div class="children"><div class="content">I work on compilers. A friend of mine works on webapps. I&#x27;ve seen Cursor give him lots of useful code, but it&#x27;s never been particularly useful on any of the code of mine that I&#x27;ve tried it on.<p>It seems very logical to me that there&#x27;d be orders of magnitude more training data for some domains than others, and that existing models&#x27; skill is not evenly distributed cross-domain.</div><br/><div id="42807006" class="c"><input type="checkbox" id="c-42807006" checked=""/><div class="controls bullet"><span class="by">dkga</span><span>|</span><a href="#42806507">root</a><span>|</span><a href="#42806716">parent</a><span>|</span><a href="#42807107">next</a><span>|</span><label class="collapse" for="c-42807006">[-]</label><label class="expand" for="c-42807006">[1 more]</label></div><br/><div class="children"><div class="content">This. Also across languages. For example, I suppose there is a lot more content in python and javascript than Apple script, for example. (And to be fair not a lot of the python suggestions I receive are actually mindblowing good)</div><br/></div></div><div id="42807107" class="c"><input type="checkbox" id="c-42807107" checked=""/><div class="controls bullet"><span class="by">q0uaur</span><span>|</span><a href="#42806507">root</a><span>|</span><a href="#42806716">parent</a><span>|</span><a href="#42807006">prev</a><span>|</span><a href="#42807515">next</a><span>|</span><label class="collapse" for="c-42807107">[-]</label><label class="expand" for="c-42807107">[4 more]</label></div><br/><div class="children"><div class="content">i&#x27;m still patiently waiting for an easy way to point a model at some documentation, and make it actually use that.<p>My usecase is gdscript for godot games, and all the models i&#x27;ve tried so far use godot 2 stuff that&#x27;s just not around anymore, even if you tell it to use godot 4 it gives way too much wrong output to be useful.<p>I wish i could just point it at the latest godot docs and have it give up to date answers. but seeing as that&#x27;s still not a thing, i guess it&#x27;s more complicated than i expect.</div><br/><div id="42808169" class="c"><input type="checkbox" id="c-42808169" checked=""/><div class="controls bullet"><span class="by">psytrx</span><span>|</span><a href="#42806507">root</a><span>|</span><a href="#42807107">parent</a><span>|</span><a href="#42807344">next</a><span>|</span><label class="collapse" for="c-42808169">[-]</label><label class="expand" for="c-42808169">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s llms.txt [0], but it&#x27;s not gaining much popularity.<p>My web framework of choice provides these [1], but they&#x27;re not easily injected into the LLM context without much fuss. It would be a game changer if more LLM tools implemented them.<p>[0] <a href="https:&#x2F;&#x2F;llmstxt.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;llmstxt.org&#x2F;</a>
[1] <a href="https:&#x2F;&#x2F;svelte.dev&#x2F;docs&#x2F;llms" rel="nofollow">https:&#x2F;&#x2F;svelte.dev&#x2F;docs&#x2F;llms</a></div><br/></div></div><div id="42807344" class="c"><input type="checkbox" id="c-42807344" checked=""/><div class="controls bullet"><span class="by">doctoboggan</span><span>|</span><a href="#42806507">root</a><span>|</span><a href="#42807107">parent</a><span>|</span><a href="#42808169">prev</a><span>|</span><a href="#42807576">next</a><span>|</span><label class="collapse" for="c-42807344">[-]</label><label class="expand" for="c-42807344">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s definitely a thing already. Look up &quot;RAG&quot; (Retrieval Augmented Generation). Most of the popular closed source companies offer RAG services via their APIs, and you can also do it with local llms using open-webui and probably many other local UIs.</div><br/></div></div><div id="42807576" class="c"><input type="checkbox" id="c-42807576" checked=""/><div class="controls bullet"><span class="by">mohsen1</span><span>|</span><a href="#42806507">root</a><span>|</span><a href="#42807107">parent</a><span>|</span><a href="#42807344">prev</a><span>|</span><a href="#42807515">next</a><span>|</span><label class="collapse" for="c-42807576">[-]</label><label class="expand" for="c-42807576">[1 more]</label></div><br/><div class="children"><div class="content">Cursor can follow links</div><br/></div></div></div></div></div></div><div id="42807515" class="c"><input type="checkbox" id="c-42807515" checked=""/><div class="controls bullet"><span class="by">fovc</span><span>|</span><a href="#42806507">parent</a><span>|</span><a href="#42806716">prev</a><span>|</span><a href="#42808196">next</a><span>|</span><label class="collapse" for="c-42807515">[-]</label><label class="expand" for="c-42807515">[5 more]</label></div><br/><div class="children"><div class="content">&gt; I feel like I&#x27;m taking crazy pills when I read about others&#x27; experiences. Surely I am not alone?<p>You&#x27;re not alone :-) I asked a very similar question about a month ago: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42552653">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42552653</a> and have continued researching since.<p>My takeaway was that autocomplete, boiler plate, and one-off scripts are the main use cases. To use an analogy, I think the code assistants are more like an upgrade from handsaw to power tools and less like hiring a carpenter. (Which is not what the hype engine will claim).<p>For me, only the one-off script (write-only code) use-case is useful. I&#x27;ve had the best results on this with Claude.<p>Emacs abbrevs&#x2F;snippets (+ choice of language) virtually eliminate the boiler plate problem, so I don&#x27;t have a use for assistants there.<p>For autocomplete, I find that LSP completion engines provide 95% of the value for 1% of the latency. Physically typing the code is a small % of my time&#x2F;energy, so the value is more about getting the right names, argument order, and other fiddly details I may not remember exactly. But I find, that LSP-powered autocomplete and tooltips largely solve those challenges.</div><br/><div id="42808933" class="c"><input type="checkbox" id="c-42808933" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42806507">root</a><span>|</span><a href="#42807515">parent</a><span>|</span><a href="#42807673">next</a><span>|</span><label class="collapse" for="c-42808933">[-]</label><label class="expand" for="c-42808933">[3 more]</label></div><br/><div class="children"><div class="content">&gt; like an upgrade from handsaw to power tools and less like hiring a carpenter. (Which is not what the hype engine will claim).<p>I 100% agree with the not hiring a carpenter part but we need a better way to describe the improvement over just a handsaw. If you have domain knowledge, it can become an incredible design aid&#x2F;partner.  Here is a real world example as to how it is changing things for me.<p>I have a TreeTable component which I built 100% with LLM and when I need to update it, I just follow the instructions in this chat:<p><a href="http:&#x2F;&#x2F;beta.gitsense.com&#x2F;?chat=dd997ccd-5b37-4591-9200-b975f274450c" rel="nofollow">http:&#x2F;&#x2F;beta.gitsense.com&#x2F;?chat=dd997ccd-5b37-4591-9200-b975f...</a><p>Right now, I am thinking about adding folders to organize chats, and here is the chat with DeepSeek for that feature:<p><a href="http:&#x2F;&#x2F;beta.gitsense.com&#x2F;?chat=3a94ce40-86f2-4e68-b5d7-88d337bd11ba" rel="nofollow">http:&#x2F;&#x2F;beta.gitsense.com&#x2F;?chat=3a94ce40-86f2-4e68-b5d7-88d33...</a><p>I&#x27;m thoroughly impressed as it suggested data structures and more for me to think about. And here I am asking it to review what was discussed to make the information easier to understand.<p><a href="http:&#x2F;&#x2F;beta.gitsense.com&#x2F;?chat=8c6bf5db-49a7-4511-990c-5e6ad3a41955" rel="nofollow">http:&#x2F;&#x2F;beta.gitsense.com&#x2F;?chat=8c6bf5db-49a7-4511-990c-5e6ad...</a><p>All of this cost me less than a penny.  I&#x27;m still waiting for my Anthropic API limit to reset and I&#x27;m going to ask Sonnet for feedback as well, and I figure that will cost me 5 cents.<p>I fully understand the not hiring a carpenter part, but I think what LLMs bring to the table is SO MUCH more than an upgrade to a power tool. If you know what you need and can clearly articulate it well enough, there really is no limit to what you can build with proper instructions, provided the solution is in its training data and you have a good enough BS detector.</div><br/><div id="42809958" class="c"><input type="checkbox" id="c-42809958" checked=""/><div class="controls bullet"><span class="by">strogonoff</span><span>|</span><a href="#42806507">root</a><span>|</span><a href="#42808933">parent</a><span>|</span><a href="#42807673">next</a><span>|</span><label class="collapse" for="c-42809958">[-]</label><label class="expand" for="c-42809958">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If you know what you need and can clearly articulate it well enough, there really is no limit to what you can build with proper instructions, provided the solution is in its training data and you have a good enough BS detector.<p>In other words: you must already know how to do what you are asking the LLM to do.<p>In other words: it may make sense if typing speed is your bottleneck and you are dealing with repetitive tasks that have well been solved many times (i.e., you want an advanced autocomplete).<p>This basically makes it useless for me. Typing speed is not a bottleneck, I automate or abstract away repetition, and I seek novel tasks that have not yet been well solved—or I just reuse those existing solutions (maybe even contributing to respective OSS projects).<p>The cases where something new is needed in areas that I don’t know well it completely failed me. NB: I never actually used it myself, I only gave into a suggestion by a friend (whom LLM reportedly helps) to use his LLM wrangling skills in a thorny case.</div><br/><div id="42810072" class="c"><input type="checkbox" id="c-42810072" checked=""/><div class="controls bullet"><span class="by">sdesol</span><span>|</span><a href="#42806507">root</a><span>|</span><a href="#42809958">parent</a><span>|</span><a href="#42807673">next</a><span>|</span><label class="collapse" for="c-42810072">[-]</label><label class="expand" for="c-42810072">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In other words: you must already know how to do what you are asking the LLM to do.<p>Those that will benefit the most will be senior developers. They might not know the exact problem or language, but they should know enough to guide the LLM.<p>&gt; In other words: it may make sense if typing speed is your bottleneck and you are dealing with repetitive tasks that have well been solved many times (i.e., you want an advanced autocomplete).<p>I definitely use a LLM as a typist and I love it. I&#x27;ve come to a point now where I mentally ask myself, &quot;Will it take more time to do it myself or to explain it?&quot;  Another factor is cost, as you can rack up a bill pretty quickly with Claude Sonnet if you ask it to generate a lot of code.<p>But honestly, what I love about integrating LLM into my workflow is, I&#x27;m better able to capture and summarize my thought process. I&#x27;ve also found LLMs can better articulate my thoughts most of the time. If you know how to prompt a LLM, it almost feels like you are working with a knowledgeable colleague.<p>&gt; I never actually used it myself, I only gave into a suggestion by a friend (whom LLM reportedly helps) to use his LLM wrangling skills in a thorny case.<p>LLMs are definitely not for everyone, but I personally cannot see myself coding without LLMs now.  Just asking for variable name suggestions is pretty useful.  Or describing something vague and having it properly articulate my thoughts is amazing. I think we like to believe what we do is rather unique, but I think a lot of things that we need to do have already been done.  Whether it is in the training data is another thing, though.</div><br/></div></div></div></div></div></div><div id="42807673" class="c"><input type="checkbox" id="c-42807673" checked=""/><div class="controls bullet"><span class="by">barrell</span><span>|</span><a href="#42806507">root</a><span>|</span><a href="#42807515">parent</a><span>|</span><a href="#42808933">prev</a><span>|</span><a href="#42808196">next</a><span>|</span><label class="collapse" for="c-42807673">[-]</label><label class="expand" for="c-42807673">[1 more]</label></div><br/><div class="children"><div class="content">I think you make a very good point about your existing devenv. I recently turned off GitHub copilot after maybe 2 years of use — I didn’t realize how often I was using its completions over LSPs.<p>Quality of Life went up massively. LSPs and nvim-cmp have come a long way (although one of these days I’ll try blink.cmp)</div><br/></div></div></div></div><div id="42808196" class="c"><input type="checkbox" id="c-42808196" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#42806507">parent</a><span>|</span><a href="#42807515">prev</a><span>|</span><a href="#42807347">next</a><span>|</span><label class="collapse" for="c-42808196">[-]</label><label class="expand" for="c-42808196">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Is anyone actually getting value out of these models?<p>I&#x27;ve found incredible value in having LLMs help me write unit tests. The quality of the test code is far from perfect, but AI tooling - Claude Sonnet specifically - is good at coming up with reasonable unit test cases after I&#x27;ve written the code under test (sue me, TDD zealots). I probably have to fix 30% of the tests and expand the test cases, but I&#x27;d say it cuts the number if test code lines I author by more than 80%. This has decreased the friction so much, I&#x27;ve added Continuous Integration to small, years-old personal projects that had no tests before.<p>I&#x27;ve found lesser value with refactoring and adding code docs, but that&#x27;s more of autocomplete++ using natural language rather than AST-derived code.</div><br/></div></div><div id="42807347" class="c"><input type="checkbox" id="c-42807347" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#42806507">parent</a><span>|</span><a href="#42808196">prev</a><span>|</span><a href="#42807234">next</a><span>|</span><label class="collapse" for="c-42807347">[-]</label><label class="expand" for="c-42807347">[5 more]</label></div><br/><div class="children"><div class="content">&gt; I wired one up<p>“One”? Wired up <i>how</i>? There is a huge difference between the best and worst. They aren’t fungible. <i>Which</i> one? How long ago? Did it even support FIM (fill in middle), or was it blindly guessing from the left side? Did the plugin even gather appropriate context from related files, or was it only looking at the current file?<p>If you try Copilot or Cursor today, you can experience what “the best” looks like, which gives you a benchmark to measure smaller, dumber models and plugins against. No, Copilot and Cursor are not available for emacs, as far as I know… but if you want to understand if a technology is useful, you don’t start with the worst version and judge from that. (Not saying emacs itself is the worst… just that <i>without more context</i>, my assumption is that whatever plugin you probably encountered was probably using a bottom tier model, and I doubt the plugin itself was helping that model do its best.)<p>There are some local code completion models that I think are perfectly fine, but I don’t know where you will draw the line on how good is good enough. If you can prove to yourself that the best models are good enough, then you can try out different local models and see if one of those works for you.</div><br/><div id="42807513" class="c"><input type="checkbox" id="c-42807513" checked=""/><div class="controls bullet"><span class="by">Lanedo</span><span>|</span><a href="#42806507">root</a><span>|</span><a href="#42807347">parent</a><span>|</span><a href="#42807760">next</a><span>|</span><label class="collapse" for="c-42807513">[-]</label><label class="expand" for="c-42807513">[1 more]</label></div><br/><div class="children"><div class="content">There is <a href="https:&#x2F;&#x2F;github.com&#x2F;copilot-emacs&#x2F;copilot.el">https:&#x2F;&#x2F;github.com&#x2F;copilot-emacs&#x2F;copilot.el</a> that gets copilot to work in emacs via JS glue code and binaries provided by copilot.vim.<p>I hacked up a slim alternative localpilot.js layer that uses llama-server instead of the copilot API, so copilot.el can be used with local LLMs, but I find the copilot.el overlays kinda buggy... 
It&#x27;d probably be better to instead write a llamapilot.el for local LLMs from scratch for emacs.</div><br/></div></div><div id="42807760" class="c"><input type="checkbox" id="c-42807760" checked=""/><div class="controls bullet"><span class="by">b5n</span><span>|</span><a href="#42806507">root</a><span>|</span><a href="#42807347">parent</a><span>|</span><a href="#42807513">prev</a><span>|</span><a href="#42807482">next</a><span>|</span><label class="collapse" for="c-42807760">[-]</label><label class="expand" for="c-42807760">[1 more]</label></div><br/><div class="children"><div class="content">Emacs has had multiple llm integration packages available for quite awhile (relative to the rise of llms). `gptel` supports multiple providers including anthropic, openai, ollama, etc.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;karthink&#x2F;gptel">https:&#x2F;&#x2F;github.com&#x2F;karthink&#x2F;gptel</a></div><br/></div></div><div id="42807482" class="c"><input type="checkbox" id="c-42807482" checked=""/><div class="controls bullet"><span class="by">yoyohello13</span><span>|</span><a href="#42806507">root</a><span>|</span><a href="#42807347">parent</a><span>|</span><a href="#42807760">prev</a><span>|</span><a href="#42809623">next</a><span>|</span><label class="collapse" for="c-42807482">[-]</label><label class="expand" for="c-42807482">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;copilot-emacs&#x2F;copilot.el">https:&#x2F;&#x2F;github.com&#x2F;copilot-emacs&#x2F;copilot.el</a></div><br/></div></div><div id="42809623" class="c"><input type="checkbox" id="c-42809623" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#42806507">root</a><span>|</span><a href="#42807347">parent</a><span>|</span><a href="#42807482">prev</a><span>|</span><a href="#42807234">next</a><span>|</span><label class="collapse" for="c-42809623">[-]</label><label class="expand" for="c-42809623">[1 more]</label></div><br/><div class="children"><div class="content">there&#x27;s avante.nvim</div><br/></div></div></div></div><div id="42807234" class="c"><input type="checkbox" id="c-42807234" checked=""/><div class="controls bullet"><span class="by">codingdave</span><span>|</span><a href="#42806507">parent</a><span>|</span><a href="#42807347">prev</a><span>|</span><a href="#42809621">next</a><span>|</span><label class="collapse" for="c-42807234">[-]</label><label class="expand" for="c-42807234">[1 more]</label></div><br/><div class="children"><div class="content">Yep - I don&#x27;t get a ton of value out of autocompletions, but I get decent value from asking an LLM how they would approach more complex functions or features. I rarely get code back that I can copy&#x2F;paste, but reading their output is something I can react to - whether it is good or bad, just having a starting point speeds up the design of new features vs. me burning time creating my first&#x2F;worst draft. And that is the goal here, isn&#x27;t it? To get some productivity gains?<p>So maybe it is just a difference in perspective? Even incorrect code and bad ideas can still be helpful. It is only useless if you expect them to hand you working code.</div><br/></div></div><div id="42809621" class="c"><input type="checkbox" id="c-42809621" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#42806507">parent</a><span>|</span><a href="#42807234">prev</a><span>|</span><a href="#42808359">next</a><span>|</span><label class="collapse" for="c-42809621">[-]</label><label class="expand" for="c-42809621">[1 more]</label></div><br/><div class="children"><div class="content">i don&#x27;t find value from the models that it makes economical sense to self-host. i do get value out of a llama 70b, for instance, though</div><br/></div></div><div id="42808359" class="c"><input type="checkbox" id="c-42808359" checked=""/><div class="controls bullet"><span class="by">righthand</span><span>|</span><a href="#42806507">parent</a><span>|</span><a href="#42809621">prev</a><span>|</span><a href="#42808028">next</a><span>|</span><label class="collapse" for="c-42808359">[-]</label><label class="expand" for="c-42808359">[1 more]</label></div><br/><div class="children"><div class="content">Honestly just disabled my TabNine plugin and have found that LSP server is good enough for 99% of what I do. I really don’t need hypothetical output suggested to me. I’m comfortable reading docs though so others may feel different.</div><br/></div></div><div id="42808028" class="c"><input type="checkbox" id="c-42808028" checked=""/><div class="controls bullet"><span class="by">tomr75</span><span>|</span><a href="#42806507">parent</a><span>|</span><a href="#42808359">prev</a><span>|</span><a href="#42807946">next</a><span>|</span><label class="collapse" for="c-42808028">[-]</label><label class="expand" for="c-42808028">[1 more]</label></div><br/><div class="children"><div class="content">try cursor</div><br/></div></div></div></div><div id="42807946" class="c"><input type="checkbox" id="c-42807946" checked=""/><div class="controls bullet"><span class="by">colordrops</span><span>|</span><a href="#42806507">prev</a><span>|</span><a href="#42807156">next</a><span>|</span><label class="collapse" for="c-42807946">[-]</label><label class="expand" for="c-42807946">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen several posts and projects like this. Is there a summary&#x2F;comparison somewhere of the various ways of running local completion&#x2F;copilot?</div><br/></div></div><div id="42807156" class="c"><input type="checkbox" id="c-42807156" checked=""/><div class="controls bullet"><span class="by">entelechy0</span><span>|</span><a href="#42807946">prev</a><span>|</span><a href="#42806483">next</a><span>|</span><label class="collapse" for="c-42807156">[-]</label><label class="expand" for="c-42807156">[1 more]</label></div><br/><div class="children"><div class="content">I use this on-and-off again. It is nice that I can flip between this and Copilot by commenting out one line in my init.lua</div><br/></div></div></div></div></div></div></div></body></html>