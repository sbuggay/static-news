<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708765281071" as="style"/><link rel="stylesheet" href="styles.css?v=1708765281071"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://read.engineerscodex.com/p/metas-new-llm-based-test-generator">Meta&#x27;s new LLM-based test generator is a sneak peek to the future of development</a> <span class="domain">(<a href="https://read.engineerscodex.com">read.engineerscodex.com</a>)</span></div><div class="subtext"><span>ben_s</span> | <span>141 comments</span></div><br/><div><div id="39488146" class="c"><input type="checkbox" id="c-39488146" checked=""/><div class="controls bullet"><span class="by">ajmurmann</span><span>|</span><a href="#39487422">next</a><span>|</span><label class="collapse" for="c-39488146">[-]</label><label class="expand" for="c-39488146">[27 more]</label></div><br/><div class="children"><div class="content">I find it interesting that generally the first instinct seems to be to use LLMs for writing test code rather than the implementation. Maybe I&#x27;ve done too much TDD, but to me the tests describe how the system is supposed to behave. This is very much what I want the human to define and the code should fit within the guardrails set by the tests.<p>I could see it as very helpful though for an LLM to point out underspecified areas. Maybe having it propose unit tests for underspecified areas is a way to do look at that and what&#x27;s happening here?<p>Edit: Even before LLMs were a thing, I sometimes wondered if monkeys on type writers could write my application once I&#x27;ve written all the tests.</div><br/><div id="39488254" class="c"><input type="checkbox" id="c-39488254" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#39488146">parent</a><span>|</span><a href="#39490209">next</a><span>|</span><label class="collapse" for="c-39488254">[-]</label><label class="expand" for="c-39488254">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Maybe I&#x27;ve done too much TDD, but to me the tests describe how the system is supposed to behave. This is very much what I want the human to define and the code should fit within the guardrails set by the tests.<p>People who work on legacy code bases often build what are called “characterisation tests” - tests which define how the current code base actually behaves, as opposed to how some human believes it ought to behave. They enable you to rewrite&#x2F;refactor&#x2F;rearchitect code while minimising the risk of introducing regressions. The problem with many legacy code bases is nobody understands how they are <i>supposed</i> to work, sometimes even the users believe it is supposed to work a certain way which is different from how it actually does - but the most important thing is to avoid changing behaviour except when changes are explicitly desired.</div><br/><div id="39488530" class="c"><input type="checkbox" id="c-39488530" checked=""/><div class="controls bullet"><span class="by">ajmurmann</span><span>|</span><a href="#39488146">root</a><span>|</span><a href="#39488254">parent</a><span>|</span><a href="#39488272">next</a><span>|</span><label class="collapse" for="c-39488530">[-]</label><label class="expand" for="c-39488530">[1 more]</label></div><br/><div class="children"><div class="content">Agreed, that&#x27;s a great use case for autogenerated tests.</div><br/></div></div><div id="39488272" class="c"><input type="checkbox" id="c-39488272" checked=""/><div class="controls bullet"><span class="by">totetsu</span><span>|</span><a href="#39488146">root</a><span>|</span><a href="#39488254">parent</a><span>|</span><a href="#39488530">prev</a><span>|</span><a href="#39490209">next</a><span>|</span><label class="collapse" for="c-39488272">[-]</label><label class="expand" for="c-39488272">[5 more]</label></div><br/><div class="children"><div class="content">Couldn’t an llm provided with the right level of logs write really good characterization tests?</div><br/><div id="39488475" class="c"><input type="checkbox" id="c-39488475" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#39488146">root</a><span>|</span><a href="#39488272">parent</a><span>|</span><a href="#39488386">next</a><span>|</span><label class="collapse" for="c-39488475">[-]</label><label class="expand" for="c-39488475">[1 more]</label></div><br/><div class="children"><div class="content">A significant part of writing characterisation tests can be simply staring at a code coverage report and asking “can I write a test (possibly by modifying an existing one) which hits this line&#x2F;branch”. Sometimes that’s easy, sometimes that’s hard, sometimes that’s impossible (code bases, especially crapulent legacy ones, sometimes contain large sections of dead code which are impossible to reach given any input).<p>An LLM doesn’t have to always get it right to be useful-have it generate a whole bunch of tests, run them all, keep the ones which hit new lines&#x2F;conditions, maybe even feed those results back in to see if it can iteratively improve, stop when it is no longer generating useful tests. Hopefully, that addresses most of the low-hanging fruit, and leaves the harder cases to a human.<p>There already exist automated test generation systems which can do some of this–for example, concolic testing-but an LLM can be viewed as just another tool in the toolbox, which may sometimes be able to generate tests which concolic testing can’t, or possibly produce the same tests quicker than concolic testing would. There is also the potential for them to interact synergistically - the LLM might produce a test which concolic testing couldn’t, but then concolic testing might then use that to discover further tests which the LLM couldn’t.</div><br/></div></div><div id="39488386" class="c"><input type="checkbox" id="c-39488386" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#39488146">root</a><span>|</span><a href="#39488272">parent</a><span>|</span><a href="#39488475">prev</a><span>|</span><a href="#39490209">next</a><span>|</span><label class="collapse" for="c-39488386">[-]</label><label class="expand" for="c-39488386">[3 more]</label></div><br/><div class="children"><div class="content">The seems like a perfect use case. Quickly find all the foot guns you didn’t know to look for.</div><br/><div id="39488660" class="c"><input type="checkbox" id="c-39488660" checked=""/><div class="controls bullet"><span class="by">wahnfrieden</span><span>|</span><a href="#39488146">root</a><span>|</span><a href="#39488386">parent</a><span>|</span><a href="#39490209">next</a><span>|</span><label class="collapse" for="c-39488660">[-]</label><label class="expand" for="c-39488660">[2 more]</label></div><br/><div class="children"><div class="content">as long as you have process to dismantle the tests and move fully over to a new system, if you are indeed migrating&#x2F;upgrading. leaving a legacy thing dangling and tightly coupled tests lingering for years happens easily when going from 95% to 100% can cost too much for management and stakeholders in various ways relative to other pressing needs</div><br/><div id="39488927" class="c"><input type="checkbox" id="c-39488927" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#39488146">root</a><span>|</span><a href="#39488660">parent</a><span>|</span><a href="#39490209">next</a><span>|</span><label class="collapse" for="c-39488927">[-]</label><label class="expand" for="c-39488927">[1 more]</label></div><br/><div class="children"><div class="content">&gt; as long as you have process to dismantle the tests and move fully over to a new system, if you are indeed migrating&#x2F;upgrading. leaving a legacy thing dangling and tightly coupled tests lingering for years happens easily when going from 95% to 100% can cost too much for management and stakeholders in various ways relative to other pressing needs<p>Characterisation tests are not supposed to be tightly coupled – they are supposed to be integration&#x2F;end-to-end tests not unit tests – the point is to ensure that some business process continues to produce the same outputs given the same inputs, not that the internals of <i>how</i> it produces that output are unchanged. Code coverage is used as an (imperfect) measure of how complete your set of test inputs is, and as a tool to help discover new test inputs, and minimise test inputs (if two test inputs all hit the same lines&#x2F;branches, maybe it is wasteful to keep both of them–although it isn&#x27;t just about code coverage, e.g. extreme values such as maximums and minimums can be valuable in the test suite even if they don&#x27;t actually increase coverage.)<p>They can take the form of unit tests if you are focusing on refactoring a specific component, and want to ensure its interactions with the rest of the application are not changed. But at some point, a larger redesign may get rid of that component entirely, at which point you can throw those unit tests away, but you&#x27;ll likely keep the system-level tests</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39490209" class="c"><input type="checkbox" id="c-39490209" checked=""/><div class="controls bullet"><span class="by">janosdebugs</span><span>|</span><a href="#39488146">parent</a><span>|</span><a href="#39488254">prev</a><span>|</span><a href="#39490198">next</a><span>|</span><label class="collapse" for="c-39490209">[-]</label><label class="expand" for="c-39490209">[1 more]</label></div><br/><div class="children"><div class="content">This kind of thinking is sadly lost on many. I have seen copious amounts of nonsensical tests slapped full of hard-wired mocks and any change in the functionality would break hundreds of tests. In such a scenario an LLM might be the bandaid many are looking for. Then again, the value of such tests is questionable.</div><br/></div></div><div id="39490198" class="c"><input type="checkbox" id="c-39490198" checked=""/><div class="controls bullet"><span class="by">MASNeo</span><span>|</span><a href="#39488146">parent</a><span>|</span><a href="#39490209">prev</a><span>|</span><a href="#39489248">next</a><span>|</span><label class="collapse" for="c-39490198">[-]</label><label class="expand" for="c-39490198">[1 more]</label></div><br/><div class="children"><div class="content">If you had as many monkeys as parameters in LLM they might run your business ;-)<p>I dread the morning after a night of getting something to work…somehow.</div><br/></div></div><div id="39489248" class="c"><input type="checkbox" id="c-39489248" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#39488146">parent</a><span>|</span><a href="#39490198">prev</a><span>|</span><a href="#39489922">next</a><span>|</span><label class="collapse" for="c-39489248">[-]</label><label class="expand" for="c-39489248">[1 more]</label></div><br/><div class="children"><div class="content">At the risk of telling you something you already know, I’d bring to your attention for example property-based testing, probably most popularized by Hypothesis, which is great wud I recommend, but by no means the only approach or high-quality implementation. I think QuickCheck for Haskell was around when it got big enough to show up on HN.<p>Just in case any reader hasn’t tried this, the basic idea is to make statements about code’s behavior that are weaker than a totally closed-form proof system (which also have their place) stated as “properties” than are checked up to some inherently probabilistic bound, which can be quite useful statements.<p>The “canonical” example is reversing a string: two applications of string reverse is generally intended to produce the input. But with 1 line of code, you can check as many weird Unicode edge cases or whatever as you have time and electricity.<p>I know this example seems trite, but I met this because some hard CUDA hackers doing the autodiff and kernels and shit that became PyTorch used it to tremendous effect and probably got 5x the confidence in the code for half the effort&#x2F;price.<p>It doesn’t always work out, but when it does it’s great, and LLMs seems to be able to get a Hypothesis case sort of, closer than starting from scratch.</div><br/></div></div><div id="39489922" class="c"><input type="checkbox" id="c-39489922" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#39488146">parent</a><span>|</span><a href="#39489248">prev</a><span>|</span><a href="#39489017">next</a><span>|</span><label class="collapse" for="c-39489922">[-]</label><label class="expand" for="c-39489922">[1 more]</label></div><br/><div class="children"><div class="content">FWIW, writing implementation is much more pleasant&#x2F;interesting experience, because you&#x27;re writing the actual thing the application is supposed to do. In contrast, when writing tests, you&#x27;re <i>describing</i> what the application is supposed to do, using an extremely bloated, constrained language, requiring you to write dozens or hundreds of lines of setup code, just to be able to then add few glorified if&#x2F;else statements.<p>In my experience, at least in languages like C++ or Java, unit tests <i>are made of tedium</i>, so I&#x27;m absolutely not surprised that the first instinct is to use LLMs to write that for you.</div><br/></div></div><div id="39489017" class="c"><input type="checkbox" id="c-39489017" checked=""/><div class="controls bullet"><span class="by">grogenaut</span><span>|</span><a href="#39488146">parent</a><span>|</span><a href="#39489922">prev</a><span>|</span><a href="#39488236">next</a><span>|</span><label class="collapse" for="c-39489017">[-]</label><label class="expand" for="c-39489017">[2 more]</label></div><br/><div class="children"><div class="content">One reason I can think of is that many engineers really don&#x27;t do testing. They write tests after the fact because they have to. I&#x27;ve worked with a bunch of engineers who will code for days then write a few tests &quot;proving&quot; the system works. They have low covergage and are usually brittle.<p>This system would be a godsend in the minds of engineers who think &#x2F; operate that way.<p>I&#x27;ve also had managers who told me I wasn&#x27;t allowed to write tests firsts as it was slower. Luckily I was able to override &#x2F; ignore them as I was on loan &quot;take it up with my boss&quot;. They&#x27;re probbably thinking the same as the above engineers.<p>Another way to think of this is most devs hate documentation... if they had an AI that would write great docs from the code they&#x27;d love it. And these to these devs docs they don&#x27;t have to write are great docs :)</div><br/><div id="39489274" class="c"><input type="checkbox" id="c-39489274" checked=""/><div class="controls bullet"><span class="by">tjpnz</span><span>|</span><a href="#39488146">root</a><span>|</span><a href="#39489017">parent</a><span>|</span><a href="#39488236">next</a><span>|</span><label class="collapse" for="c-39489274">[-]</label><label class="expand" for="c-39489274">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I&#x27;ve also had managers who told me I wasn&#x27;t allowed to write tests firsts as it was slower.<p>Sounds like a great place to work.</div><br/></div></div></div></div><div id="39488236" class="c"><input type="checkbox" id="c-39488236" checked=""/><div class="controls bullet"><span class="by">xboxnolifes</span><span>|</span><a href="#39488146">parent</a><span>|</span><a href="#39489017">prev</a><span>|</span><a href="#39488493">next</a><span>|</span><label class="collapse" for="c-39488236">[-]</label><label class="expand" for="c-39488236">[4 more]</label></div><br/><div class="children"><div class="content">&gt; I find it interesting that generally the first instinct seems to be to use LLMs for writing test code rather than the implementation. Maybe I&#x27;ve done too much TDD, but to me the tests describe how the system is supposed to behave. This is very much what I want the human to define and the code should fit within the guardrails set by the tests.<p>I feel the same way about how test code is viewed even outside of AI. A lot of the time the test code is treated as a lower priority code given to more junior engineers, which seems like the opposite of what you would want.</div><br/><div id="39490193" class="c"><input type="checkbox" id="c-39490193" checked=""/><div class="controls bullet"><span class="by">pydry</span><span>|</span><a href="#39488146">root</a><span>|</span><a href="#39488236">parent</a><span>|</span><a href="#39488306">next</a><span>|</span><label class="collapse" for="c-39490193">[-]</label><label class="expand" for="c-39490193">[1 more]</label></div><br/><div class="children"><div class="content">This is how I feel too. For me, tests usually end up being a concrete specification which I can execute.<p>Getting LLMs to write tests is like getting LLMs to write my spec.</div><br/></div></div><div id="39488306" class="c"><input type="checkbox" id="c-39488306" checked=""/><div class="controls bullet"><span class="by">zeroonetwothree</span><span>|</span><a href="#39488146">root</a><span>|</span><a href="#39488236">parent</a><span>|</span><a href="#39490193">prev</a><span>|</span><a href="#39488493">next</a><span>|</span><label class="collapse" for="c-39488306">[-]</label><label class="expand" for="c-39488306">[2 more]</label></div><br/><div class="children"><div class="content">When I do code review I always review the tests first. If they look through and reasonable then I can be a lot less careful reviewing the rest.</div><br/><div id="39489348" class="c"><input type="checkbox" id="c-39489348" checked=""/><div class="controls bullet"><span class="by">postalrat</span><span>|</span><a href="#39488146">root</a><span>|</span><a href="#39488306">parent</a><span>|</span><a href="#39488493">next</a><span>|</span><label class="collapse" for="c-39489348">[-]</label><label class="expand" for="c-39489348">[1 more]</label></div><br/><div class="children"><div class="content">Tests never cover everything so exactly what are you looking for?</div><br/></div></div></div></div></div></div><div id="39488493" class="c"><input type="checkbox" id="c-39488493" checked=""/><div class="controls bullet"><span class="by">pokstad</span><span>|</span><a href="#39488146">parent</a><span>|</span><a href="#39488236">prev</a><span>|</span><a href="#39489020">next</a><span>|</span><label class="collapse" for="c-39488493">[-]</label><label class="expand" for="c-39488493">[1 more]</label></div><br/><div class="children"><div class="content">I agree, humans should write tests. Humans are the oracles of the program output who know whether the code did the right or wrong thing.<p>I’m guessing they want to automate tests because most engineers skimp on them. Compensating for lack of discipline.</div><br/></div></div><div id="39489020" class="c"><input type="checkbox" id="c-39489020" checked=""/><div class="controls bullet"><span class="by">anu7df</span><span>|</span><a href="#39488146">parent</a><span>|</span><a href="#39488493">prev</a><span>|</span><a href="#39489246">next</a><span>|</span><label class="collapse" for="c-39489020">[-]</label><label class="expand" for="c-39489020">[1 more]</label></div><br/><div class="children"><div class="content">I really believe this &quot;application&quot; is the result of thinking about tests as a chore and requirement without great benefits. Your thought of LLM writing application give the tests is interesting also from test pass&#x2F;fail as optimization that ca be run online by the LLM to improve the result without human feedback.</div><br/></div></div><div id="39489246" class="c"><input type="checkbox" id="c-39489246" checked=""/><div class="controls bullet"><span class="by">makk</span><span>|</span><a href="#39488146">parent</a><span>|</span><a href="#39489020">prev</a><span>|</span><a href="#39488398">next</a><span>|</span><label class="collapse" for="c-39489246">[-]</label><label class="expand" for="c-39489246">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I find it interesting that generally the first instinct seems to be to use LLMs for writing test code rather than the implementation.<p>When you try to get the LLM to write the code, you find that it’s easier to get it to write the tests. So you do that and publish about that first.</div><br/></div></div><div id="39488398" class="c"><input type="checkbox" id="c-39488398" checked=""/><div class="controls bullet"><span class="by">mrbonner</span><span>|</span><a href="#39488146">parent</a><span>|</span><a href="#39489246">prev</a><span>|</span><a href="#39489047">next</a><span>|</span><label class="collapse" for="c-39488398">[-]</label><label class="expand" for="c-39488398">[1 more]</label></div><br/><div class="children"><div class="content">I wrote a simple LLM backed chat application. My primary usage right now is to copy paste the code I have written (Java and Python) into the chat and ask it to generate unit test cases. I think it has reduced my development time a huge amount. It also generates tests for edge cases. The generated code usually are usable 90% of the time. It also is very good at making mocks for service calls. I&#x27;m using Claude 2.1 model with Bedrock.<p>It&#x27;s nowhere as fancy as FB tool but I know it is blessed by company.</div><br/></div></div><div id="39489047" class="c"><input type="checkbox" id="c-39489047" checked=""/><div class="controls bullet"><span class="by">closeparen</span><span>|</span><a href="#39488146">parent</a><span>|</span><a href="#39488398">prev</a><span>|</span><a href="#39488394">next</a><span>|</span><label class="collapse" for="c-39489047">[-]</label><label class="expand" for="c-39489047">[1 more]</label></div><br/><div class="children"><div class="content">Covering all the &quot;if err != nil { return err }&quot; branches in Go is pretty mindless work.</div><br/></div></div><div id="39488394" class="c"><input type="checkbox" id="c-39488394" checked=""/><div class="controls bullet"><span class="by">madeofpalk</span><span>|</span><a href="#39488146">parent</a><span>|</span><a href="#39489047">prev</a><span>|</span><a href="#39488213">next</a><span>|</span><label class="collapse" for="c-39488394">[-]</label><label class="expand" for="c-39488394">[2 more]</label></div><br/><div class="children"><div class="content">&gt; This is very much what I want the human to define and the code should fit within the guardrails set by the tests.<p>Most systems are pretty predictable. it(&quot;displays the user&#x27;s name&quot;) isn&#x27;t very novel, and is probably pretty easy for a LLM to generate.</div><br/><div id="39489282" class="c"><input type="checkbox" id="c-39489282" checked=""/><div class="controls bullet"><span class="by">ajmurmann</span><span>|</span><a href="#39488146">root</a><span>|</span><a href="#39488394">parent</a><span>|</span><a href="#39488213">next</a><span>|</span><label class="collapse" for="c-39489282">[-]</label><label class="expand" for="c-39489282">[1 more]</label></div><br/><div class="children"><div class="content">Well, someone needs to define that the username should be shown in the first place</div><br/></div></div></div></div><div id="39488213" class="c"><input type="checkbox" id="c-39488213" checked=""/><div class="controls bullet"><span class="by">ralusek</span><span>|</span><a href="#39488146">parent</a><span>|</span><a href="#39488394">prev</a><span>|</span><a href="#39487422">next</a><span>|</span><label class="collapse" for="c-39488213">[-]</label><label class="expand" for="c-39488213">[2 more]</label></div><br/><div class="children"><div class="content">I basically agree with this but some caveats. I often find there are maybe 5% of the tests I <i>should</i> write that only I <i>could</i> write, because they deal with the specifics of the application that actually give it its primary purpose&#x2F;defining features. As in, it&#x27;s not that there is any test I believe AI eventually wouldn&#x27;t be able to write, it&#x27;s more that there are certain tests that define the &quot;keyframes&quot; of the application, that without defining explicitly, you&#x27;d be failing to describe your application properly.<p>For the remaining 95% of uninteresting surfaces I&#x27;d be perfectly happy to let an AI interpolate between my key cases and write the tests that I was mostly not going to bother writing anyway.</div><br/><div id="39488240" class="c"><input type="checkbox" id="c-39488240" checked=""/><div class="controls bullet"><span class="by">ajmurmann</span><span>|</span><a href="#39488146">root</a><span>|</span><a href="#39488213">parent</a><span>|</span><a href="#39487422">next</a><span>|</span><label class="collapse" for="c-39488240">[-]</label><label class="expand" for="c-39488240">[1 more]</label></div><br/><div class="children"><div class="content">You are probably right and the percentages change with the language and framework being used. When I write Ruby I write enormous amounts of tests and many of these could probably be derived from the higher-level integration tests I stared with. In Rust on the other hand, I write very few tests. I wonder if this also shows which code could be entirely generated based on the high-level tests.</div><br/></div></div></div></div></div></div><div id="39487422" class="c"><input type="checkbox" id="c-39487422" checked=""/><div class="controls bullet"><span class="by">nicklecompte</span><span>|</span><a href="#39488146">prev</a><span>|</span><a href="#39487872">next</a><span>|</span><label class="collapse" for="c-39487422">[-]</label><label class="expand" for="c-39487422">[7 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t want to review this whole thing but one part in particular seems <i>way</i> off. [Caveat: I sorta-read the original paper shortly after it was posted, my memory is fuzzy and I am only skimming it now.]<p>From the blog:<p>&gt; Most of the test cases created by Meta’s TestGen-LLM only covered an extra 2.5 lines. However, one test case covered 1326 lines! The value of that one test case is exponentially more valuable than most of the previous test cases and exponentially improves the value of TestGen-LLM. LLMs can vigorously “think outside the box” and the value of catching unexpected edge cases is very high here.<p>Of course &quot;exponentially more valuable&quot; should set off your BS detector. But to verify, from the paper:<p>&gt; However, this result arose due to a single test case, which achieved 1,326 lines covered. This test case managed to ‘hit the jackpot’ in terms of unit test coverage for a single test case. Because TestGen-LLM is typically adding to the existing coverage, and seeking to cover corner cases, the typical expected number of lines of code covered per test case is much lower....The median number of lines of code added by a TestGen-LLM test in the test-a-thon was 2.5. This is a more realistic assessment of the expected additional line coverage from a single test generated by TestGen-LLM.<p>Nowhere do the authors mention &quot;unexpected edge cases&quot; or &quot;thinking outside the box.&quot; They clearly present this 1,326 lines of coverage test as a fluke, e.g. maybe the test case checked one branch of a horrible switch statement, or perhaps it was even a fluke in how code coverage is counted. It is noteworthy that the authors do not seem to have looked into it any further, even in the &quot;qualitative results&quot; section.<p>Inaccurate editorializing really doesn&#x27;t help anyone. The internet is too damn full of people pretending to understand things they pretended to read.</div><br/><div id="39487683" class="c"><input type="checkbox" id="c-39487683" checked=""/><div class="controls bullet"><span class="by">engineercodex</span><span>|</span><a href="#39487422">parent</a><span>|</span><a href="#39487881">next</a><span>|</span><label class="collapse" for="c-39487683">[-]</label><label class="expand" for="c-39487683">[5 more]</label></div><br/><div class="children"><div class="content">Hey! Thanks for your comment - I&#x27;m the one who wrote this article. I wasn&#x27;t trying to say that the paper authors talked about &quot;unexpected edge cases&quot; or &quot;thinking outside the box.&quot; I edited the post to be more clear that some of these takeaways are my own opinions.<p>This article is less of a summary of a paper and rather commentary on what the results of the paper entails. After all, Hacker News is meant for discussion :)<p>I will say though that I do believe that I still stand by the &quot;exponentially more valuable&quot; portion. I think the fact that LLMs can fluke their way into &quot;hitting a jackpot&quot; in terms of test coverage is exactly why they&#x27;re so valuable. When you have something constantly trying out different combinations, if it hits even one jackpot, like in the paper, it&#x27;s extremely valuable to the team. It&#x27;s a case that could have been either non-obvious or simply too tedious to write a test for manually. I think there&#x27;s tremendous value in that, especially speaking as someone who has spend way too much time simply figuring out <i>how</i> to test something within a Big Tech codebase (F&#x2F;G) when I already knew <i>what</i> to test.</div><br/><div id="39488231" class="c"><input type="checkbox" id="c-39488231" checked=""/><div class="controls bullet"><span class="by">camkego</span><span>|</span><a href="#39487422">root</a><span>|</span><a href="#39487683">parent</a><span>|</span><a href="#39488277">next</a><span>|</span><label class="collapse" for="c-39488231">[-]</label><label class="expand" for="c-39488231">[1 more]</label></div><br/><div class="children"><div class="content">Pedantic warning here. In fast and loose day-to-day common English language &quot;exponentially more&quot; means &quot;fast growth&quot; or &quot;a whole lot&quot;. But that usage is meaningless! Why?, technically, you can&#x27;t have exponential growth without a dependent variable. You can have exponential growth as a function of time, height, spend, distance, any freaking metric or variable. But it has to be as a function of a value.<p>You CAN&#x27;T have exponential growth that is not a function of some value or variable or input.<p>I suppose in this case you could argue you have exponential growth as a function of the discrete using-an-LLM or not-using-an-LLM, but I&#x27;ve never heard of exponential growth as a function of a discrete.<p>Often people using the term &quot;exponential growth&quot; in common English don&#x27;t understand what it means. Sorry.</div><br/></div></div><div id="39488277" class="c"><input type="checkbox" id="c-39488277" checked=""/><div class="controls bullet"><span class="by">nicklecompte</span><span>|</span><a href="#39487422">root</a><span>|</span><a href="#39487683">parent</a><span>|</span><a href="#39488231">prev</a><span>|</span><a href="#39488078">next</a><span>|</span><label class="collapse" for="c-39488277">[-]</label><label class="expand" for="c-39488277">[1 more]</label></div><br/><div class="children"><div class="content">Seconding digdugdirk&#x27;s comment :) Thanks for the thoughtful response and I apologize if I came across as mean.<p>My problem is we have no clue what those lines actually were. If it was effectively dead code, then it&#x27;s not surprising that it was untested, and the LLM-generated test wouldn&#x27;t be valuable to the team. We have no clue what the value of the test actually was, and using a single stat like &quot;lines of code covered&quot; doesn&#x27;t actually tell us anything. Saying the test was &quot;exponentially more valuable&quot; is pure speculation, and IMO not an especially well-founded one. (Sort of like saying people who write more lines of code are more productive.)<p>This speculation seems downright irresponsible when the paper specifically emphasizes that this result was a fluke. When the authors said &quot;hit the jackpot&quot; they did not mean &quot;hit the jackpot with a valuable test&quot;, they meant &quot;hit the jackpot with an outlier that somewhat artificially juked the stats.&quot; I truly believe if the LLM managed to write a unusually valuable test with such broad coverage they would have mentioned it in the qualitative discussion. Instead they went out of their way to dismiss the importance of the 1,326 figure.</div><br/></div></div><div id="39488078" class="c"><input type="checkbox" id="c-39488078" checked=""/><div class="controls bullet"><span class="by">digdugdirk</span><span>|</span><a href="#39487422">root</a><span>|</span><a href="#39487683">parent</a><span>|</span><a href="#39488277">prev</a><span>|</span><a href="#39487881">next</a><span>|</span><label class="collapse" for="c-39488078">[-]</label><label class="expand" for="c-39488078">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for engaging with the above constructive criticism, it&#x27;s a refreshing change from what is sadly the norm.<p>One additional question - do you forsee any issues with this application where LLMs enter a non-value add &quot;doom loop&quot;? I can imagine a scenario where a test generation LLM gets hooked on the lower value simplistic tests, and yet management sees such a huge increase on the test metric (&quot;100x increase in unit tests in an afternoon? Let&#x27;s do it again!&quot;) that they continue to bloat the test suite to near-infinity. Now we&#x27;re in a situation where all future training data is now training on complete cesspool of meaningless tests that technically add coverage, but mostly just to cover an edge case that only an LLM would create.<p>Not sure if that makes sense, but tl;dr - having LLMs in the loop for both code creation and code testing seems like it&#x27;s a feedback loop waiting to happen, with what seems like solely negative repercussions for future LLM training data.</div><br/><div id="39488395" class="c"><input type="checkbox" id="c-39488395" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#39487422">root</a><span>|</span><a href="#39488078">parent</a><span>|</span><a href="#39487881">next</a><span>|</span><label class="collapse" for="c-39488395">[-]</label><label class="expand" for="c-39488395">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps there should be domains of focus for the test LLMs - even if they are clones, but assigned to only a particular domain, then their results have to be PR&#x27;s etc...<p>Why not treat every LLM as a dev contributing to git such that Humans, or other LLms need to gatekeep in case something like that happens? (start by treating them as Interns, rather than Professors with office hours)</div><br/></div></div></div></div></div></div><div id="39487881" class="c"><input type="checkbox" id="c-39487881" checked=""/><div class="controls bullet"><span class="by">fermentation</span><span>|</span><a href="#39487422">parent</a><span>|</span><a href="#39487683">prev</a><span>|</span><a href="#39487872">next</a><span>|</span><label class="collapse" for="c-39487881">[-]</label><label class="expand" for="c-39487881">[1 more]</label></div><br/><div class="children"><div class="content">The incentives at meta around code production are all wrong. The team behind this is absolutely gearing this around lines of code and number of diffs produced. This&#x27;ll just be another codegen tool creating another mountain of code that is difficult to debug.</div><br/></div></div></div></div><div id="39487872" class="c"><input type="checkbox" id="c-39487872" checked=""/><div class="controls bullet"><span class="by">siliconc0w</span><span>|</span><a href="#39487422">prev</a><span>|</span><a href="#39490192">next</a><span>|</span><label class="collapse" for="c-39487872">[-]</label><label class="expand" for="c-39487872">[14 more]</label></div><br/><div class="children"><div class="content">Good testing is hard to do - coverage is not a categorical good.  You can easily write <i>too many</i> tests that calcify programs and basically just creates a change-detector program.  Oh it looks like you changed something, oh no - all the tests are broken, but it&#x27;s okay we can now ask the LLM to regenerate them!  100% Coverage! Amazing! What progress!</div><br/><div id="39488045" class="c"><input type="checkbox" id="c-39488045" checked=""/><div class="controls bullet"><span class="by">suzzer99</span><span>|</span><a href="#39487872">parent</a><span>|</span><a href="#39489397">next</a><span>|</span><label class="collapse" for="c-39488045">[-]</label><label class="expand" for="c-39488045">[3 more]</label></div><br/><div class="children"><div class="content">Agreed. Good tests are an order of magnitude harder than good code.</div><br/><div id="39490112" class="c"><input type="checkbox" id="c-39490112" checked=""/><div class="controls bullet"><span class="by">brabel</span><span>|</span><a href="#39487872">root</a><span>|</span><a href="#39488045">parent</a><span>|</span><a href="#39489364">next</a><span>|</span><label class="collapse" for="c-39490112">[-]</label><label class="expand" for="c-39490112">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know where you&#x27;re getting that from but it&#x27;s simply wrong.
Testing is quite easy IMO. I&#x27;ve been working at the same place for almost 10 years and I introduced our testing framework. We have hundreds of thousands of tests. New guys have some trouble to get started, but after a couple of months they&#x27;re writing tests for our systems like a pro.<p>Most tests are use-case based or written for checking error-handling.<p>Use-case tests are easy to write: you don&#x27;t even need to be a programmer (in fact, it&#x27;s good if use-cases are defined by a Product Owner or Tester), though of course some cases are only known to the programmer as they&#x27;re the only ones who dive into the details. The programmer should come up with all use-cases the PO missed, of course, and judge whether or not they need to test those too... sometimes it&#x27;s ok to not test as the cost-benefit is low. Anyway, once you have this use-case based test mentality, it&#x27;s very easy to write the tests (using a proper language to do it is important! Don&#x27;t use just JUnit if you&#x27;re doing Java as it will be really tedious to write and you will stop midway - I know, I&#x27;ve been there... I highly recommend using Spock, though other frameworks to make writing test pleasurable exist).<p>This applies mostly for &quot;integration tests&quot;. For unit tests, hopefully you don&#x27;t find them difficult to write?! I find them quite easy to write since I know how to write testable code, which takes a while to learn but once you do, it&#x27;s really easy.<p>If you have examples of difficult to write tests, I would be curious to see it! Perhaps we can discuss how to make them easy.</div><br/></div></div><div id="39489364" class="c"><input type="checkbox" id="c-39489364" checked=""/><div class="controls bullet"><span class="by">postalrat</span><span>|</span><a href="#39487872">root</a><span>|</span><a href="#39488045">parent</a><span>|</span><a href="#39490112">prev</a><span>|</span><a href="#39489397">next</a><span>|</span><label class="collapse" for="c-39489364">[-]</label><label class="expand" for="c-39489364">[1 more]</label></div><br/><div class="children"><div class="content">Which is why they should be treated as a waste of time unless specific tests can be justified.</div><br/></div></div></div></div><div id="39489397" class="c"><input type="checkbox" id="c-39489397" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#39487872">parent</a><span>|</span><a href="#39488045">prev</a><span>|</span><a href="#39488591">next</a><span>|</span><label class="collapse" for="c-39489397">[-]</label><label class="expand" for="c-39489397">[1 more]</label></div><br/><div class="children"><div class="content">I know for sure that code with no coverage has terrible tests. For everything else I have to read through five other people’s idea of good test.<p>We are all terrible at writing tests. We just find our own ways to do it.</div><br/></div></div><div id="39488591" class="c"><input type="checkbox" id="c-39488591" checked=""/><div class="controls bullet"><span class="by">pshc</span><span>|</span><a href="#39487872">parent</a><span>|</span><a href="#39489397">prev</a><span>|</span><a href="#39489451">next</a><span>|</span><label class="collapse" for="c-39488591">[-]</label><label class="expand" for="c-39488591">[1 more]</label></div><br/><div class="children"><div class="content">One gig I worked had web component tests where they committed a snapshot of the expected DOM and asserted that the component spat it out... so for every subsequent change the dev would naturally hit the re-generate button and commit it all. Plentiful deltas, questionable signal.</div><br/></div></div><div id="39489451" class="c"><input type="checkbox" id="c-39489451" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#39487872">parent</a><span>|</span><a href="#39488591">prev</a><span>|</span><a href="#39487939">next</a><span>|</span><label class="collapse" for="c-39489451">[-]</label><label class="expand" for="c-39489451">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s all about the long tail cases.</div><br/></div></div><div id="39487939" class="c"><input type="checkbox" id="c-39487939" checked=""/><div class="controls bullet"><span class="by">webdood90</span><span>|</span><a href="#39487872">parent</a><span>|</span><a href="#39489451">prev</a><span>|</span><a href="#39490192">next</a><span>|</span><label class="collapse" for="c-39487939">[-]</label><label class="expand" for="c-39487939">[7 more]</label></div><br/><div class="children"><div class="content">&gt; ... basically just creates a change-detector program<p>interesting perspective - why do you think this is a bad thing?<p>to me, it&#x27;s an opportunity to verify that the change is intended. without it, how do you know that the program does what it is supposed to do?</div><br/><div id="39488013" class="c"><input type="checkbox" id="c-39488013" checked=""/><div class="controls bullet"><span class="by">siliconc0w</span><span>|</span><a href="#39487872">root</a><span>|</span><a href="#39487939">parent</a><span>|</span><a href="#39488161">next</a><span>|</span><label class="collapse" for="c-39488013">[-]</label><label class="expand" for="c-39488013">[1 more]</label></div><br/><div class="children"><div class="content">Without deliberate tests it can be very difficult and time consuming to parse out intended change from unwanted or incidental change.</div><br/></div></div><div id="39488161" class="c"><input type="checkbox" id="c-39488161" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#39487872">root</a><span>|</span><a href="#39487939">parent</a><span>|</span><a href="#39488013">prev</a><span>|</span><a href="#39488001">next</a><span>|</span><label class="collapse" for="c-39488161">[-]</label><label class="expand" for="c-39488161">[1 more]</label></div><br/><div class="children"><div class="content">It tightly couples domain needs with implementation details.<p>Thinking of it as a leaky abstraction helps me.<p>I try hard to separate domain logic tests from implementation specific tests.<p>Your code could be loosely coupled with high cohesion, but with lots of random tests like you get when code coverage is a performance metric, you have to add a lot of complexity that only relates to an implementation.</div><br/></div></div><div id="39488001" class="c"><input type="checkbox" id="c-39488001" checked=""/><div class="controls bullet"><span class="by">whoisjuan</span><span>|</span><a href="#39487872">root</a><span>|</span><a href="#39487939">parent</a><span>|</span><a href="#39488161">prev</a><span>|</span><a href="#39488288">next</a><span>|</span><label class="collapse" for="c-39488001">[-]</label><label class="expand" for="c-39488001">[2 more]</label></div><br/><div class="children"><div class="content">No op, but I don’t think test-driven development resounds with everyone who writes code.<p>I don’t want to write tests for everything. I just want to write the ones that matter.</div><br/><div id="39488123" class="c"><input type="checkbox" id="c-39488123" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#39487872">root</a><span>|</span><a href="#39488001">parent</a><span>|</span><a href="#39488288">next</a><span>|</span><label class="collapse" for="c-39488123">[-]</label><label class="expand" for="c-39488123">[1 more]</label></div><br/><div class="children"><div class="content">That is a common misconception about TDD.<p>TDD is _about_ writing tests that matter, but most people think it is about writing all unit tests first.<p>If you are following TDD anywhere close to the way it is described, you will only be writing tests that relate to domain functionality first.<p>Note how it is described here, although it is turse.<p><a href="https:&#x2F;&#x2F;martinfowler.com&#x2F;bliki&#x2F;TestDrivenDevelopment.html" rel="nofollow">https:&#x2F;&#x2F;martinfowler.com&#x2F;bliki&#x2F;TestDrivenDevelopment.html</a><p>The coverage metric as a goal writing style doesn&#x27;t work for TDD, sorry you were exposed to that.<p>You are correct that model doesn&#x27;t work.</div><br/></div></div></div></div><div id="39488288" class="c"><input type="checkbox" id="c-39488288" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#39487872">root</a><span>|</span><a href="#39487939">parent</a><span>|</span><a href="#39488001">prev</a><span>|</span><a href="#39490192">next</a><span>|</span><label class="collapse" for="c-39488288">[-]</label><label class="expand" for="c-39488288">[2 more]</label></div><br/><div class="children"><div class="content">How do you know that the tests accurately define what the code is supposed to do?<p>Another way, if you know what the code is supposed to do, why write it down in two places?</div><br/><div id="39489914" class="c"><input type="checkbox" id="c-39489914" checked=""/><div class="controls bullet"><span class="by">bbojan</span><span>|</span><a href="#39487872">root</a><span>|</span><a href="#39488288">parent</a><span>|</span><a href="#39490192">next</a><span>|</span><label class="collapse" for="c-39489914">[-]</label><label class="expand" for="c-39489914">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Another way, if you know what the code is supposed to do, why write it down in two places?<p>This would be like criticizing double-entry accounting by asking &quot;if you know what the amount is, why write it down in two places?&quot;<p>We write the code down in two places because that gives us advantages that far outweigh the added effort:<p>- Once written, your test will catch regressions forever<p>- A test is often excellent documentation on what the code does<p>- It&#x27;s now much easier to refactor the code, making it more likely that it will be refactored when needed.</div><br/></div></div></div></div></div></div></div></div><div id="39490192" class="c"><input type="checkbox" id="c-39490192" checked=""/><div class="controls bullet"><span class="by">sandGorgon</span><span>|</span><a href="#39487872">prev</a><span>|</span><a href="#39490190">next</a><span>|</span><label class="collapse" for="c-39490192">[-]</label><label class="expand" for="c-39490192">[1 more]</label></div><br/><div class="children"><div class="content">&gt;<i>using private, internal LLMs that are probably fine-tuned with Meta’s codebase.</i><p>what does this mean ? i would have thought they would simply use codellama. is there any research around privately finetuned code llms ? why would they be better ?</div><br/></div></div><div id="39490190" class="c"><input type="checkbox" id="c-39490190" checked=""/><div class="controls bullet"><span class="by">MASNeo</span><span>|</span><a href="#39490192">prev</a><span>|</span><a href="#39487848">next</a><span>|</span><label class="collapse" for="c-39490190">[-]</label><label class="expand" for="c-39490190">[1 more]</label></div><br/><div class="children"><div class="content">Ok, so test case generation has been around a while and now that it is working, where is the GitHub Action?</div><br/></div></div><div id="39487848" class="c"><input type="checkbox" id="c-39487848" checked=""/><div class="controls bullet"><span class="by">elzbardico</span><span>|</span><a href="#39490190">prev</a><span>|</span><a href="#39488480">next</a><span>|</span><label class="collapse" for="c-39487848">[-]</label><label class="expand" for="c-39487848">[13 more]</label></div><br/><div class="children"><div class="content">I feel for the future maintainers of all this crappy LLM legacy code in the future. 
It’s gonna be ugly.</div><br/><div id="39487866" class="c"><input type="checkbox" id="c-39487866" checked=""/><div class="controls bullet"><span class="by">idle_zealot</span><span>|</span><a href="#39487848">parent</a><span>|</span><a href="#39487919">next</a><span>|</span><label class="collapse" for="c-39487866">[-]</label><label class="expand" for="c-39487866">[1 more]</label></div><br/><div class="children"><div class="content">Surely we will get LLMs to maintain it.</div><br/></div></div><div id="39487919" class="c"><input type="checkbox" id="c-39487919" checked=""/><div class="controls bullet"><span class="by">duderific</span><span>|</span><a href="#39487848">parent</a><span>|</span><a href="#39487866">prev</a><span>|</span><a href="#39487970">next</a><span>|</span><label class="collapse" for="c-39487919">[-]</label><label class="expand" for="c-39487919">[2 more]</label></div><br/><div class="children"><div class="content">So, I guess LLMs are actually creating jobs rather than destroying them. Not exactly fun jobs though.</div><br/><div id="39489352" class="c"><input type="checkbox" id="c-39489352" checked=""/><div class="controls bullet"><span class="by">steve_adams_86</span><span>|</span><a href="#39487848">root</a><span>|</span><a href="#39487919">parent</a><span>|</span><a href="#39487970">next</a><span>|</span><label class="collapse" for="c-39489352">[-]</label><label class="expand" for="c-39489352">[1 more]</label></div><br/><div class="children"><div class="content">Not exactly well paid either, I suspect.</div><br/></div></div></div></div><div id="39487970" class="c"><input type="checkbox" id="c-39487970" checked=""/><div class="controls bullet"><span class="by">bigfudge</span><span>|</span><a href="#39487848">parent</a><span>|</span><a href="#39487919">prev</a><span>|</span><a href="#39488651">next</a><span>|</span><label class="collapse" for="c-39487970">[-]</label><label class="expand" for="c-39487970">[3 more]</label></div><br/><div class="children"><div class="content">I suspect it will be no worse than  enterprisey code. It might even look quite similar, although the comments and docs will be more thorough and less likely to be actively wrong.</div><br/><div id="39487991" class="c"><input type="checkbox" id="c-39487991" checked=""/><div class="controls bullet"><span class="by">jachee</span><span>|</span><a href="#39487848">root</a><span>|</span><a href="#39487970">parent</a><span>|</span><a href="#39488147">next</a><span>|</span><label class="collapse" for="c-39487991">[-]</label><label class="expand" for="c-39487991">[1 more]</label></div><br/><div class="children"><div class="content">…unless the LLM hallucinates the comments and docs.</div><br/></div></div><div id="39488147" class="c"><input type="checkbox" id="c-39488147" checked=""/><div class="controls bullet"><span class="by">Nathanba</span><span>|</span><a href="#39487848">root</a><span>|</span><a href="#39487970">parent</a><span>|</span><a href="#39487991">prev</a><span>|</span><a href="#39488651">next</a><span>|</span><label class="collapse" for="c-39488147">[-]</label><label class="expand" for="c-39488147">[1 more]</label></div><br/><div class="children"><div class="content">It will be worse because there will be a lot more of it</div><br/></div></div></div></div><div id="39488651" class="c"><input type="checkbox" id="c-39488651" checked=""/><div class="controls bullet"><span class="by">armchairhacker</span><span>|</span><a href="#39487848">parent</a><span>|</span><a href="#39487970">prev</a><span>|</span><a href="#39488801">next</a><span>|</span><label class="collapse" for="c-39488651">[-]</label><label class="expand" for="c-39488651">[2 more]</label></div><br/><div class="children"><div class="content">Just delete the tests, problem solved. Your CI dashboard even gives you the green checkmark.</div><br/><div id="39489373" class="c"><input type="checkbox" id="c-39489373" checked=""/><div class="controls bullet"><span class="by">steve_adams_86</span><span>|</span><a href="#39487848">root</a><span>|</span><a href="#39488651">parent</a><span>|</span><a href="#39488801">next</a><span>|</span><label class="collapse" for="c-39489373">[-]</label><label class="expand" for="c-39489373">[1 more]</label></div><br/><div class="children"><div class="content">This made me think of that midwit meme with “delete the tests, green check mark in CI” on either side of the graph and “100% coverage” in the centre. Not totally valid, but… A bit of truth, haha.<p>Maybe the right side should be something about fuzzing and using static types. Use systemic and automated checks. I’m a little ashamed that I thought in memes so readily.</div><br/></div></div></div></div><div id="39488801" class="c"><input type="checkbox" id="c-39488801" checked=""/><div class="controls bullet"><span class="by">block_dagger</span><span>|</span><a href="#39487848">parent</a><span>|</span><a href="#39488651">prev</a><span>|</span><a href="#39488114">next</a><span>|</span><label class="collapse" for="c-39488801">[-]</label><label class="expand" for="c-39488801">[2 more]</label></div><br/><div class="children"><div class="content">I too feel compassion for the AI agents that will be dealing with this code. 99% of human developers will be out of the loop by then.</div><br/><div id="39488891" class="c"><input type="checkbox" id="c-39488891" checked=""/><div class="controls bullet"><span class="by">travoc</span><span>|</span><a href="#39487848">root</a><span>|</span><a href="#39488801">parent</a><span>|</span><a href="#39488114">next</a><span>|</span><label class="collapse" for="c-39488891">[-]</label><label class="expand" for="c-39488891">[1 more]</label></div><br/><div class="children"><div class="content">I’m old enough to remember the first time they said this about offshoring.</div><br/></div></div></div></div><div id="39488114" class="c"><input type="checkbox" id="c-39488114" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#39487848">parent</a><span>|</span><a href="#39488801">prev</a><span>|</span><a href="#39488480">next</a><span>|</span><label class="collapse" for="c-39488114">[-]</label><label class="expand" for="c-39488114">[2 more]</label></div><br/><div class="children"><div class="content">Agreed.<p>LLMs will never get any better than they are right now and haven&#x27;t improved at all in 2 years. Just fancy Markov chains.<p>The only way they can be used to write code is by people who don&#x27;t know how to code blindly commiting code to prod without any review whatsoever.<p>People who do know how to code couldn&#x27;t possibly have a use case and it won&#x27;t make them any more productive.<p>I&#x27;m just going to ignore all this LLM nonsense that isn&#x27;t changing the world at all and you definitely should too.</div><br/><div id="39489207" class="c"><input type="checkbox" id="c-39489207" checked=""/><div class="controls bullet"><span class="by">SnowTile</span><span>|</span><a href="#39487848">root</a><span>|</span><a href="#39488114">parent</a><span>|</span><a href="#39488480">next</a><span>|</span><label class="collapse" for="c-39489207">[-]</label><label class="expand" for="c-39489207">[1 more]</label></div><br/><div class="children"><div class="content">Disagree, I find them very useful to quickly explain new libraries or do tedious things like regex</div><br/></div></div></div></div></div></div><div id="39488480" class="c"><input type="checkbox" id="c-39488480" checked=""/><div class="controls bullet"><span class="by">Jtsummers</span><span>|</span><a href="#39487848">prev</a><span>|</span><a href="#39488440">next</a><span>|</span><label class="collapse" for="c-39488480">[-]</label><label class="expand" for="c-39488480">[1 more]</label></div><br/><div class="children"><div class="content">Quoting myself  (lightly edited) from when the paper itself came up. They misrepresent the stats in their writeup.<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39406726">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39406726</a><p>Their abstract doesn&#x27;t match their actual paper contents. That&#x27;s unfortunate. Their summary indicates rates in terms of test cases:<p>&gt; 75% of test cases built correctly, 57% passed reliably [implying test cases by context], and 25% increased coverage [same implication]
The actual report talks about test classes, where each class has one or more test cases.<p>&gt; (1) 75% of test classes had at least one new test case that builds correctly.<p>&gt; (2) 57% of test classes had at least one test case that builds cor- rectly and passes reliably.<p>&gt; (3) 25% of test classes had at least one test case that builds cor- rectly, passes and increases line coverage compared to all other test classes that share the same build target.<p>Those are two very different statements. They even have a footnote acknowledging this:<p>&gt; For a given attempt to extend a test class, there can be many attempts to generate a test case, so the success rate per test case is typically considerably lower than that per test class.<p>But then in their conclusion they misrepresent their findings again, like the abstract:<p>&gt; When we use TestGen-LLM in its experimental mode (free from the confounding factors inherent in deployment), we found that the success rate per test case was 25% (See Section 3.3). However, line coverage is a stringent requirement for success. Were we to relax the requirement to require only that test cases build and pass, then the success rate rises to 57%.</div><br/></div></div><div id="39488440" class="c"><input type="checkbox" id="c-39488440" checked=""/><div class="controls bullet"><span class="by">acituan</span><span>|</span><a href="#39488480">prev</a><span>|</span><a href="#39489710">next</a><span>|</span><label class="collapse" for="c-39488440">[-]</label><label class="expand" for="c-39488440">[1 more]</label></div><br/><div class="children"><div class="content">Unless well separated, this will easily turn developer-hostile by some clueless management demanding high coverage and enthusiastic juniors smuggling in massive amounts of AI tests so that at the end of the day you will need get a rubberstamp from an hard-to-maintain llm-gen test code each time you want to submit your work.<p>Yes authoring <i>some</i> tests might be sped up but not necessarily maintaining them - or maintaining the code under test because you are not necessarily generating <i>good</i> ones. Not to mention sweating over tests usually help developers with checking the design of the code early on too; if not very testable, usually not a good design either, e.g not sufficiently abstracted component contracts which suck in a context where you need to coauthor code with others.<p>What some people miss is that tests are <i>supposed to be</i> sacrifical code, that most of which will not catch anything during their lifetime - and that is OK because it gives an automated peace of mind and saves from potential false clues when things fail. <i>But</i> that also means max investment into a probabilistic safeguard is not gonna pan out at all times; you will always have diminishing marginal utility as the coverage tops. Unless you&#x27;re writing some high traffic part of the execution path - e.g. a standard library - touting high coverage is not gonna pay off.<p>Not to mention almost always an ecology of tests need be there - not just unittests but integration, system etc - to make the thing keep chugging at the end of the day. Will llm&#x27;s sit at the design meetings and understand the architecture to write tests for them too? Or what they <i>can</i> do will be oversold at the expense of what <i>should</i> be done. A sense of &quot;what is <i>relevant</i>&quot; is needed while investing effort in tests - not just at write-time but also at design-time and maintain-time - which is what humans are pretty OK at, and AI tools are not.<p>What llms can save time with is keystrokes of an experienced developer who already has a sense of what is a good thing to test and what is not. It can also be - and has been - a hinderance with making the developers smuggle not-so-relevant things into the code.<p>I don&#x27;t want an economy of producing keystrokes, I want an appropriately thought set of highly relevant out keystrokes, and I want the latter well separated from the former so that their objective utility - or lack thereof - can be demonstrated in time.</div><br/></div></div><div id="39489710" class="c"><input type="checkbox" id="c-39489710" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#39488440">prev</a><span>|</span><a href="#39489820">next</a><span>|</span><label class="collapse" for="c-39489710">[-]</label><label class="expand" for="c-39489710">[1 more]</label></div><br/><div class="children"><div class="content">How does the AI know what tests it should write?<p>I think this is an interesting experiment but somewhat dubious. The way I see AI would best help software development is that I the programmer  have a question about my or somebody else&#x27;s code, which the AI then answers, sometimes with a code-proposal but not always. It should be able to answer questions like &quot;Is there a way to simplify this code? What are some inputs that would cause an error?&quot; etc.<p>AI should help us understand the code, and understand how to improve it. Not write all of it on its own because if we don&#x27;t tell it what to do, it cannot know what we want it to do. Tests is a good example. What do we want it to test?</div><br/></div></div><div id="39489820" class="c"><input type="checkbox" id="c-39489820" checked=""/><div class="controls bullet"><span class="by">TeeWEE</span><span>|</span><a href="#39489710">prev</a><span>|</span><a href="#39488071">next</a><span>|</span><label class="collapse" for="c-39489820">[-]</label><label class="expand" for="c-39489820">[1 more]</label></div><br/><div class="children"><div class="content">The proof is in the pudding, show me the code!<p>In my experience LLM are smart but sometimes inconsistent and over a long chat it might say things that are logically self contradictions… when you tell it that it confirms it.<p>It just seems like it lacks a consistent world view.<p>I don’t trust them yet. Maybe with even more scale they become better.<p>They act a little bit like young children, with a lot of domain knowledge.</div><br/></div></div><div id="39488071" class="c"><input type="checkbox" id="c-39488071" checked=""/><div class="controls bullet"><span class="by">aussieguy1234</span><span>|</span><a href="#39489820">prev</a><span>|</span><a href="#39489964">next</a><span>|</span><label class="collapse" for="c-39488071">[-]</label><label class="expand" for="c-39488071">[2 more]</label></div><br/><div class="children"><div class="content">Already done it with GPT-4.<p>I showed it a TypeScript module, asked it to generate a unit test and it made a working test not only covering the happy paths but a few edge cases as well.</div><br/><div id="39488241" class="c"><input type="checkbox" id="c-39488241" checked=""/><div class="controls bullet"><span class="by">ramoz</span><span>|</span><a href="#39488071">parent</a><span>|</span><a href="#39489964">next</a><span>|</span><label class="collapse" for="c-39488241">[-]</label><label class="expand" for="c-39488241">[1 more]</label></div><br/><div class="children"><div class="content">Yea… agree.<p>I’m not resonating with the downvotes here on similar comments.<p>ChatGPT goes above and beyond for me in many ways.<p>Tests seem… easy in terms of gpt capabilities.<p>Last week I had it write python that traversed an AST and construct a react flow graph as well as the component. I made no edits, went through a few iterations of prompt feedback, and it worked great. Many similar interesting abilities I’ve observed from gpt.</div><br/></div></div></div></div><div id="39489964" class="c"><input type="checkbox" id="c-39489964" checked=""/><div class="controls bullet"><span class="by">Fricken</span><span>|</span><a href="#39488071">prev</a><span>|</span><a href="#39489749">next</a><span>|</span><label class="collapse" for="c-39489964">[-]</label><label class="expand" for="c-39489964">[1 more]</label></div><br/><div class="children"><div class="content">Meta likes to release positive news about itself in the wake of it&#x27;s competitors misfortunes.</div><br/></div></div><div id="39489749" class="c"><input type="checkbox" id="c-39489749" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#39489964">prev</a><span>|</span><a href="#39487122">next</a><span>|</span><label class="collapse" for="c-39489749">[-]</label><label class="expand" for="c-39489749">[1 more]</label></div><br/><div class="children"><div class="content">Developers will do anything not to write tests</div><br/></div></div><div id="39487122" class="c"><input type="checkbox" id="c-39487122" checked=""/><div class="controls bullet"><span class="by">samsk</span><span>|</span><a href="#39489749">prev</a><span>|</span><a href="#39488841">next</a><span>|</span><label class="collapse" for="c-39487122">[-]</label><label class="expand" for="c-39487122">[6 more]</label></div><br/><div class="children"><div class="content">Finally some AI Codegen, that makes sense to me.</div><br/><div id="39487158" class="c"><input type="checkbox" id="c-39487158" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#39487122">parent</a><span>|</span><a href="#39488841">next</a><span>|</span><label class="collapse" for="c-39487158">[-]</label><label class="expand" for="c-39487158">[5 more]</label></div><br/><div class="children"><div class="content">Finally?</div><br/><div id="39487339" class="c"><input type="checkbox" id="c-39487339" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39487122">root</a><span>|</span><a href="#39487158">parent</a><span>|</span><a href="#39487345">next</a><span>|</span><label class="collapse" for="c-39487339">[-]</label><label class="expand" for="c-39487339">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a persistent rather large minority that has a nuanced take: it can&#x27;t write code they like (don&#x27;t want to edit), but it&#x27;s great for weekend projects (where they&#x27;re trying new things without established personal preferences).<p>Forest for the trees if you ask me, but, to each their own.</div><br/><div id="39487445" class="c"><input type="checkbox" id="c-39487445" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#39487122">root</a><span>|</span><a href="#39487339">parent</a><span>|</span><a href="#39487345">next</a><span>|</span><label class="collapse" for="c-39487445">[-]</label><label class="expand" for="c-39487445">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes, I find writing pseudocode easier than code. And then I ask an AI to turn it into code. Sometimes the results aren’t too bad, and just need a few tweaks for me to use it-overall I’ve saved mental effort compared to translating the pseudocode into code by hand. And if the results aren’t useful, I’ve only wasted a few seconds, and then I just have to do it manually.</div><br/></div></div></div></div></div></div></div></div><div id="39488841" class="c"><input type="checkbox" id="c-39488841" checked=""/><div class="controls bullet"><span class="by">anoopelias</span><span>|</span><a href="#39487122">prev</a><span>|</span><a href="#39487468">next</a><span>|</span><label class="collapse" for="c-39488841">[-]</label><label class="expand" for="c-39488841">[1 more]</label></div><br/><div class="children"><div class="content">I thought that unit tests are a balance. A balance of not too much, not too little. &quot;Too little&quot; means you are not covered on the edges. &quot;Too much&quot; means the tests are too rigid its scary to change the code.<p>Ideally, &quot;one change&quot; (Whatever that might be) in production code should cause exactly 1 test to fail.<p>How does TestGen-LLM address this problem?</div><br/></div></div><div id="39487468" class="c"><input type="checkbox" id="c-39487468" checked=""/><div class="controls bullet"><span class="by">gxt</span><span>|</span><a href="#39488841">prev</a><span>|</span><a href="#39488144">next</a><span>|</span><label class="collapse" for="c-39487468">[-]</label><label class="expand" for="c-39487468">[7 more]</label></div><br/><div class="children"><div class="content">Elementary tests, like unit tests, should be mecanically generated by walking the AST, differences ack`d and snapshoted when commiting. Every language should come with this built-in.</div><br/><div id="39487633" class="c"><input type="checkbox" id="c-39487633" checked=""/><div class="controls bullet"><span class="by">bluefishinit</span><span>|</span><a href="#39487468">parent</a><span>|</span><a href="#39487593">next</a><span>|</span><label class="collapse" for="c-39487633">[-]</label><label class="expand" for="c-39487633">[1 more]</label></div><br/><div class="children"><div class="content">This is called compiling with a type system.</div><br/></div></div><div id="39487593" class="c"><input type="checkbox" id="c-39487593" checked=""/><div class="controls bullet"><span class="by">superb_dev</span><span>|</span><a href="#39487468">parent</a><span>|</span><a href="#39487633">prev</a><span>|</span><a href="#39488144">next</a><span>|</span><label class="collapse" for="c-39487593">[-]</label><label class="expand" for="c-39487593">[5 more]</label></div><br/><div class="children"><div class="content">What exactly are we testing at that point?</div><br/><div id="39487764" class="c"><input type="checkbox" id="c-39487764" checked=""/><div class="controls bullet"><span class="by">Groxx</span><span>|</span><a href="#39487468">root</a><span>|</span><a href="#39487593">parent</a><span>|</span><a href="#39488144">next</a><span>|</span><label class="collapse" for="c-39487764">[-]</label><label class="expand" for="c-39487764">[4 more]</label></div><br/><div class="children"><div class="content">Ensuring that `if x == 1` works when x == 1.<p>Very important. Very valuable.<p>Imagine if `if err != nil { return err }` just <i>stopped working tomorrow</i>.  Your tests would detect it!  Outage prevented!</div><br/><div id="39487818" class="c"><input type="checkbox" id="c-39487818" checked=""/><div class="controls bullet"><span class="by">Cthulhu_</span><span>|</span><a href="#39487468">root</a><span>|</span><a href="#39487764">parent</a><span>|</span><a href="#39487879">next</a><span>|</span><label class="collapse" for="c-39487818">[-]</label><label class="expand" for="c-39487818">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re being sarcastic but honestly, I&#x27;ve never found a regression because of a unit test.<p>Only past few days though, I did find two bugs that would&#x27;ve been prevented if the original code was covered by a decent unit test.</div><br/><div id="39488854" class="c"><input type="checkbox" id="c-39488854" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#39487468">root</a><span>|</span><a href="#39487818">parent</a><span>|</span><a href="#39487879">next</a><span>|</span><label class="collapse" for="c-39488854">[-]</label><label class="expand" for="c-39488854">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You&#x27;re being sarcastic but honestly, I&#x27;ve never found a regression because of a unit test<p>So you&#x27;ve never made a change caused a unit test to fail? If not, how large is your codebase, and is ownership shared across multiple teams?<p>I caught dozens of latent or unreported bugs by writing unit tests for a 6kloc JS app which had 0% coverage before.</div><br/></div></div></div></div><div id="39487879" class="c"><input type="checkbox" id="c-39487879" checked=""/><div class="controls bullet"><span class="by">cgdub</span><span>|</span><a href="#39487468">root</a><span>|</span><a href="#39487764">parent</a><span>|</span><a href="#39487818">prev</a><span>|</span><a href="#39488144">next</a><span>|</span><label class="collapse" for="c-39487879">[-]</label><label class="expand" for="c-39487879">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t write Perl or Ruby anymore, but this would have been immensely helpful back then.</div><br/></div></div></div></div></div></div></div></div><div id="39488144" class="c"><input type="checkbox" id="c-39488144" checked=""/><div class="controls bullet"><span class="by">yes_man</span><span>|</span><a href="#39487468">prev</a><span>|</span><a href="#39487459">next</a><span>|</span><label class="collapse" for="c-39488144">[-]</label><label class="expand" for="c-39488144">[1 more]</label></div><br/><div class="children"><div class="content">I think the future of development is the other way around. Devs and PMs define the goalposts with tests, AI will handle the implementation</div><br/></div></div><div id="39487459" class="c"><input type="checkbox" id="c-39487459" checked=""/><div class="controls bullet"><span class="by">romwell</span><span>|</span><a href="#39488144">prev</a><span>|</span><a href="#39490167">next</a><span>|</span><label class="collapse" for="c-39487459">[-]</label><label class="expand" for="c-39487459">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, after working in semiconductor industry (computational lithography) where test-driven design is the norm... I&#x27;m not convinced.<p>I&#x27;m not saying that writing tests <i>before</i> the production code is something that should always be done.<p>But tests are just as much a part of the codebase as anything else, and <i>absolutely</i> must be written alongside the code being tested.<p>The most important part of the test is that it showcases <i>intent</i> of the developer. A test suite demonstrates the following:<p>* <i>How</i> the code <i>should</i> be used<p>* What the code <i>does</i><p>* What the code <i>doesn&#x27;t do</i><p>* What it was written <i>for</i><p>Then when that code is used or modified by another developer, they don&#x27;t have to hunt for clues in the codebase like they&#x27;re Sherlock Holmes.<p>If the tests aren&#x27;t telling a <i>story</i>, you&#x27;re writing tests wrong.<p>And until the computers gain the ability to <i>read your mind</i> and do a better job at understanding <i>what you want to do</i>, AI&#x2F;LLM-based generators can&#x27;t do this job for you.<p>Of course, if the only goal of your test suite is getting a green checkmark on a pre-commit check (and being able to show great coverage numbers), then yeah, you can double your productivity with AI.<p>Automatic code generators will surely help you write more bad code  at lightning speed.<p>And if others complain that tons of boilerplate make the code bloated and hard to understand — just tell them to use AI to deal with it. Worked for you!<p>That really <i>does</i> seem to be the future of development. But not the future I&#x27;m looking forward to.</div><br/><div id="39487626" class="c"><input type="checkbox" id="c-39487626" checked=""/><div class="controls bullet"><span class="by">azeirah</span><span>|</span><a href="#39487459">parent</a><span>|</span><a href="#39490167">next</a><span>|</span><label class="collapse" for="c-39487626">[-]</label><label class="expand" for="c-39487626">[2 more]</label></div><br/><div class="children"><div class="content">I agree with almost everything you said, although I do think this type of testing has a place.<p>There are different types of testing, what you&#x27;re describing sounds to me like testing the &quot;core&quot; of your code, part documentation, part validation, part stability, etc.<p>Other types of testing like fuzzing provide an entirely kind of value. I believe this AI- driven testing can inherit a space to target tests at the tail end of the distribution, many tests with little value. Providing extra coverage where human energy and time is lacking.<p>That is how I see the current state of AI tooling regardless, as a cognitive assistant.<p>I&#x27;d be surprised if this line of research doesn&#x27;t end up being very fruitful in the coming years.</div><br/><div id="39487854" class="c"><input type="checkbox" id="c-39487854" checked=""/><div class="controls bullet"><span class="by">romwell</span><span>|</span><a href="#39487459">root</a><span>|</span><a href="#39487626">parent</a><span>|</span><a href="#39490167">next</a><span>|</span><label class="collapse" for="c-39487854">[-]</label><label class="expand" for="c-39487854">[1 more]</label></div><br/><div class="children"><div class="content">That I can fully agree with (particularly, comparison with fuzzing).<p>Your comment presents a way more grounded perspective on the future of LLMs in programming than the article does.</div><br/></div></div></div></div></div></div><div id="39490167" class="c"><input type="checkbox" id="c-39490167" checked=""/><div class="controls bullet"><span class="by">Temporary_31337</span><span>|</span><a href="#39487459">prev</a><span>|</span><a href="#39488143">next</a><span>|</span><label class="collapse" for="c-39490167">[-]</label><label class="expand" for="c-39490167">[1 more]</label></div><br/><div class="children"><div class="content">All this to write another CRUD app ;)</div><br/></div></div><div id="39488143" class="c"><input type="checkbox" id="c-39488143" checked=""/><div class="controls bullet"><span class="by">yes_man</span><span>|</span><a href="#39490167">prev</a><span>|</span><a href="#39488582">next</a><span>|</span><label class="collapse" for="c-39488143">[-]</label><label class="expand" for="c-39488143">[1 more]</label></div><br/><div class="children"><div class="content">I think the future of development is the other way around. Devs and PMs define the goalposts with tests, AI will do the implementation</div><br/></div></div><div id="39488582" class="c"><input type="checkbox" id="c-39488582" checked=""/><div class="controls bullet"><span class="by">cavisne</span><span>|</span><a href="#39488143">prev</a><span>|</span><a href="#39488510">next</a><span>|</span><label class="collapse" for="c-39488582">[-]</label><label class="expand" for="c-39488582">[1 more]</label></div><br/><div class="children"><div class="content">Doesn’t meta famously not do much testing at all? Ie they use experiments to “test in prod”.</div><br/></div></div><div id="39488510" class="c"><input type="checkbox" id="c-39488510" checked=""/><div class="controls bullet"><span class="by">jimbob45</span><span>|</span><a href="#39488582">prev</a><span>|</span><a href="#39487650">next</a><span>|</span><label class="collapse" for="c-39488510">[-]</label><label class="expand" for="c-39488510">[1 more]</label></div><br/><div class="children"><div class="content">For greenfield projects, these LLM coders would be invaluable. For my old codebase with observed requirements and magic numbers? Lol it’s going to be just as confused as I am.</div><br/></div></div><div id="39487086" class="c"><input type="checkbox" id="c-39487086" checked=""/><div class="controls bullet"><span class="by">holoduke</span><span>|</span><a href="#39487097">prev</a><span>|</span><a href="#39487590">next</a><span>|</span><label class="collapse" for="c-39487086">[-]</label><label class="expand" for="c-39487086">[29 more]</label></div><br/><div class="children"><div class="content">I am using copilots now since a few months and it really makes me a 2x more productive developer. Its like you become an orchestrator of a dev team. You still need to look into details, but things just flow much much faster. I can only imagine how it will be like if I also have access to a AI debugger &#x2F; end to end tester. Completing the loop and making it super efficient. 
Also think that this is not only the case for developers. Even for lawyer it could be the same thing. Expected production output of a worker is going to rise. The ones who do not embrace AI assistants in the near future will have a hard time in the future.</div><br/><div id="39487104" class="c"><input type="checkbox" id="c-39487104" checked=""/><div class="controls bullet"><span class="by">zdragnar</span><span>|</span><a href="#39487086">parent</a><span>|</span><a href="#39487202">next</a><span>|</span><label class="collapse" for="c-39487104">[-]</label><label class="expand" for="c-39487104">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Even for lawyer it could be the same thing<p>Funny you mention that, since lawyers have already gotten in trouble for citing fictional cases when submitting work performed by chatgpt.<p>It&#x27;s useful for rote things, but for anything that you depend on, you still need to give it just as much attention as if you&#x27;d done it yourself.</div><br/><div id="39487627" class="c"><input type="checkbox" id="c-39487627" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487104">parent</a><span>|</span><a href="#39487512">next</a><span>|</span><label class="collapse" for="c-39487627">[-]</label><label class="expand" for="c-39487627">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Funny you mention that, since lawyers have already gotten in trouble for citing fictional cases when submitting work performed by chatgpt.<p>For something like legal work, you don’t want a raw LLM. You want an agent integrated with a legal database, in such a way that it <i>can’t</i> cite cases which don’t exist in the database. And it can’t generate a direct quote from a case unless that text actually occurs in the case.<p>You still need a human lawyer familiar with the law to pick up on subtler errors, but better technology can prevent the grosser ones. And the subtler errors (e.g. misrepresenting what a case says through partial or out of context quoting) is the kind of error human lawyers sometimes make too - like every other profession, lawyers vary greatly in their competence, and often only the grosser cases of incompetence incur sanctions</div><br/><div id="39487836" class="c"><input type="checkbox" id="c-39487836" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487627">parent</a><span>|</span><a href="#39487512">next</a><span>|</span><label class="collapse" for="c-39487836">[-]</label><label class="expand" for="c-39487836">[2 more]</label></div><br/><div class="children"><div class="content">Tried Lexis+ AI, and… it’s just not very good yet.<p>Much like ChatGPT, it can only handle recall and summation — and even then, I don’t fully trust it because it often misses key ideas.<p>And much like ChatGPT, it can’t do anything coherent at length without a lot of help and working around its faults.<p>And seems entirely unaware of similar sounding words having distinct legal meanings. Which is not good.</div><br/><div id="39488159" class="c"><input type="checkbox" id="c-39488159" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487836">parent</a><span>|</span><a href="#39487512">next</a><span>|</span><label class="collapse" for="c-39488159">[-]</label><label class="expand" for="c-39488159">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Tried Lexis+ AI, and… it’s just not very good yet.<p>I wonder to what extent that’s due to current inherent limitations of the technology, and to what extent it is due to quality of implementation issues.<p>It is hard to say because (I presume) there is only limited public information on how it is actually implemented.<p>e.g. which LLM is it using? How much fine-tuning has been done? Are they using other potentially helpful techniques such as guided sampling? Or breaking down the task into parts and having multiple agents each specialised to handle one particular part?<p>&gt; Much like ChatGPT<p>When you compare it to ChatGPT, do you mean GPT3.5 or GPT4?<p>I also would guess that certain areas of law (especially criminal law) may be prone to triggering “safeguards” which result in poorer performance than if those safeguards were absent. Arguing that what your client did was legal (even be it ethically unsavoury) is an essential part of a lawyer’s job</div><br/></div></div></div></div></div></div><div id="39487512" class="c"><input type="checkbox" id="c-39487512" checked=""/><div class="controls bullet"><span class="by">cloverich</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487104">parent</a><span>|</span><a href="#39487627">prev</a><span>|</span><a href="#39487142">next</a><span>|</span><label class="collapse" for="c-39487512">[-]</label><label class="expand" for="c-39487512">[1 more]</label></div><br/><div class="children"><div class="content">&gt; you still need to give it just as much attention as if you&#x27;d done it yourself.<p>It&#x27;s different than a Lawyers case, where the facts require manually cross referencing. In our case, that verification can come via directly and immediately running the code. How good the actual code is varies, but the fundamental difference remains.</div><br/></div></div><div id="39487142" class="c"><input type="checkbox" id="c-39487142" checked=""/><div class="controls bullet"><span class="by">qup</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487104">parent</a><span>|</span><a href="#39487512">prev</a><span>|</span><a href="#39487202">next</a><span>|</span><label class="collapse" for="c-39487142">[-]</label><label class="expand" for="c-39487142">[2 more]</label></div><br/><div class="children"><div class="content">I agree, but it does change the shape of the work you would be doing. Validating work isn&#x27;t the same as generating new work.<p>Whether that&#x27;s better, or useful, probably differs by situation.</div><br/><div id="39487428" class="c"><input type="checkbox" id="c-39487428" checked=""/><div class="controls bullet"><span class="by">blibble</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487142">parent</a><span>|</span><a href="#39487202">next</a><span>|</span><label class="collapse" for="c-39487428">[-]</label><label class="expand" for="c-39487428">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Validating work isn&#x27;t the same as generating new work.<p>in many cases it&#x27;s way harder</div><br/></div></div></div></div></div></div><div id="39487202" class="c"><input type="checkbox" id="c-39487202" checked=""/><div class="controls bullet"><span class="by">unshavedyak</span><span>|</span><a href="#39487086">parent</a><span>|</span><a href="#39487104">prev</a><span>|</span><a href="#39487358">next</a><span>|</span><label class="collapse" for="c-39487202">[-]</label><label class="expand" for="c-39487202">[12 more]</label></div><br/><div class="children"><div class="content">I still haven&#x27;t figured out how people use this that much. I use LLMs almost daily via ChatGPT, Phind, Kagi Ultimate (my current one), etc. However i spend too much time pushing the LLM towards my goal that i can&#x27;t imagine it speeding my coding up.<p>I clearly find value in LLMs to some degree, but speeding up my coding is not yet one.. i&#x27;d love to, but i just don&#x27;t understand. I can only imagine typing more in explanation for the LLM than it would take me to write it to begin with.</div><br/><div id="39487672" class="c"><input type="checkbox" id="c-39487672" checked=""/><div class="controls bullet"><span class="by">azeirah</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487202">parent</a><span>|</span><a href="#39487401">next</a><span>|</span><label class="collapse" for="c-39487672">[-]</label><label class="expand" for="c-39487672">[1 more]</label></div><br/><div class="children"><div class="content">For myself I have several distinct cognitive weaknesses that I can often just unload onto an AI in a conversational style.<p>For instance I know I&#x27;m good at breaking up large tasks into smaller ones, but because planning and executive functioning is by far my worst energy consuming skill (adhd) I can save a lot of energy (but not time) by approaching it conversationally. I&#x27;d often use colleagues for the same thing, but then my productivity costs 2 salaries<p>Similarly, I have some trouble with memory and cognitive speed when I&#x27;m tired which is unfortunately often the case due to my health, I know well enough what I kinda &quot;want&quot; to do and I let the AI generate something that comes near what I need and I can work from there once I have the right starting point.<p>Just my personal experience I&#x27;d wanted to share.</div><br/></div></div><div id="39487401" class="c"><input type="checkbox" id="c-39487401" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487202">parent</a><span>|</span><a href="#39487672">prev</a><span>|</span><a href="#39488834">next</a><span>|</span><label class="collapse" for="c-39487401">[-]</label><label class="expand" for="c-39487401">[2 more]</label></div><br/><div class="children"><div class="content">It all seems to depend on what you are doing. The more niche and technical the task, the less the AI can help. If you are just generating crud and points for a common web language and framework, it can be an 80% boost.<p>I think the real problem with this is that people aren&#x27;t differentiating these different types of work when they give these numbers.</div><br/><div id="39487802" class="c"><input type="checkbox" id="c-39487802" checked=""/><div class="controls bullet"><span class="by">ickyforce</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487401">parent</a><span>|</span><a href="#39488834">next</a><span>|</span><label class="collapse" for="c-39487802">[-]</label><label class="expand" for="c-39487802">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not just the type of work but also experience. Here are two cases:<p>1. After working for many years in Java I needed to build a service. I spent few days designing it and then a month on implementation. I used DBs and libraries I knew very well. I didn&#x27;t need to access google&#x2F;stackoverflow, I didn&#x27;t need to look up names of (std)lib methods and their parameters and if something wasn&#x27;t working it was fairly obvious what I needed to change.<p>2. Recently I wanted to create a simple page which fetched a bunch of stuff from some URLs and showed the results, simple stuff. But with React since that was what frontend team was using. I never used React and rarely touched web in the recent years. Most of my time was spent googling about React and how exactly CORS&#x2F;SOP work in the browsers, and with polishing it took a couple of days.<p>I&#x27;m pretty sure that in case 1) AI wouldn&#x27;t help me much. Maybe just as a more fancy code completion.<p>In case 2) AI would probably be a significant time save. I could just ask it to write some draft for me and then I could make few tweaks, without having to figure out React.<p>But somehow nobody quantifies their experience with the languages&#x2F;tools when they are using AI - I&#x27;m sure there&#x27;s a staggering difference between 1 month and 10 yoe.</div><br/></div></div></div></div><div id="39488834" class="c"><input type="checkbox" id="c-39488834" checked=""/><div class="controls bullet"><span class="by">samatman</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487202">parent</a><span>|</span><a href="#39487401">prev</a><span>|</span><a href="#39487333">next</a><span>|</span><label class="collapse" for="c-39488834">[-]</label><label class="expand" for="c-39488834">[1 more]</label></div><br/><div class="children"><div class="content">So I wanted to take a heatmap and bin it into fifteen values on a logarithmic basis. So I asked ChatGPT to do it for me, and it just worked.<p>Would this have been difficult for me to just write? No. I wouldn&#x27;t call it difficult. But it would have involved <i>effort</i>, which is a resource I&#x27;m happy to conserve. It&#x27;s like the difference between a sandwich you make and a sandwich you ask someone to make.<p>Then I asked it to generate the integer ranges which correspond to each bucket. It screwed that one up, which I found out by copying the function to a scratch file and trying some representative values in the REPL. So I told it to iterate all the values between in and max and generate the ranges that way. That one worked. Net of less effort, plus I had a test for it which I could copy-paste from the REPL to the test suite.<p>Faster? Maybe, maybe not. But I&#x27;m rate-limited by gumption, not minutes. When it&#x27;s easier to describe the function than write it, and it&#x27;s simple enough that the robot won&#x27;t screw the pooch, I hand it off to the LLM. It&#x27;s a great addition to the toolkit.</div><br/></div></div><div id="39487333" class="c"><input type="checkbox" id="c-39487333" checked=""/><div class="controls bullet"><span class="by">swatcoder</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487202">parent</a><span>|</span><a href="#39488834">prev</a><span>|</span><a href="#39488814">next</a><span>|</span><label class="collapse" for="c-39487333">[-]</label><label class="expand" for="c-39487333">[2 more]</label></div><br/><div class="children"><div class="content">Some revered novelists spend all day to write a single page and others are able to get in a flow and produce a chapter in that same time.<p>Without getting into touchy questions of quality and talent and the whole 10x trope, there are a lot of working engineers out there that produce code a lot more slowly than others and that work on more common problems than others. My sense is copilot-like products provide the biggest boon to those people and are a lot harder for people who are more naturally prolific or who work on more esoteric things.</div><br/><div id="39488517" class="c"><input type="checkbox" id="c-39488517" checked=""/><div class="controls bullet"><span class="by">holoduke</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487333">parent</a><span>|</span><a href="#39488814">next</a><span>|</span><label class="collapse" for="c-39488517">[-]</label><label class="expand" for="c-39488517">[1 more]</label></div><br/><div class="children"><div class="content">Have you ever tried it?. It sounds like you are a bit against the idea of an AI supporting your work.</div><br/></div></div></div></div><div id="39488814" class="c"><input type="checkbox" id="c-39488814" checked=""/><div class="controls bullet"><span class="by">block_dagger</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487202">parent</a><span>|</span><a href="#39487333">prev</a><span>|</span><a href="#39487259">next</a><span>|</span><label class="collapse" for="c-39488814">[-]</label><label class="expand" for="c-39488814">[1 more]</label></div><br/><div class="children"><div class="content">Try this: in a situation where you need a small change to an existing class that you haven&#x27;t looked at in a long time, dump the code and spec to ChatGPT with a request to add the feature or make the change along with a supporting spec. This can really speed up getting to the final result.</div><br/></div></div><div id="39487259" class="c"><input type="checkbox" id="c-39487259" checked=""/><div class="controls bullet"><span class="by">oblio</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487202">parent</a><span>|</span><a href="#39488814">prev</a><span>|</span><a href="#39487268">next</a><span>|</span><label class="collapse" for="c-39487259">[-]</label><label class="expand" for="c-39487259">[1 more]</label></div><br/><div class="children"><div class="content">IMO a chunk of it is like Peel developers: people that prioritize their own speed above all else: readability, team reviews, etc. I&#x27;d love to be proven wrong.</div><br/></div></div><div id="39487268" class="c"><input type="checkbox" id="c-39487268" checked=""/><div class="controls bullet"><span class="by">010101010101</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487202">parent</a><span>|</span><a href="#39487259">prev</a><span>|</span><a href="#39489177">next</a><span>|</span><label class="collapse" for="c-39487268">[-]</label><label class="expand" for="c-39487268">[1 more]</label></div><br/><div class="children"><div class="content">I rarely find interfacing with an external chat interface useful, but integration with the coding environment (e.g. Copilot) is an immediate productivity boost.</div><br/></div></div><div id="39489177" class="c"><input type="checkbox" id="c-39489177" checked=""/><div class="controls bullet"><span class="by">okdood64</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487202">parent</a><span>|</span><a href="#39487268">prev</a><span>|</span><a href="#39488329">next</a><span>|</span><label class="collapse" for="c-39489177">[-]</label><label class="expand" for="c-39489177">[1 more]</label></div><br/><div class="children"><div class="content">The co-pilot equivalent tool I use makes me about 10-15% more productive. Noticeable but not life changing.</div><br/></div></div><div id="39488329" class="c"><input type="checkbox" id="c-39488329" checked=""/><div class="controls bullet"><span class="by">holoduke</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487202">parent</a><span>|</span><a href="#39489177">prev</a><span>|</span><a href="#39487358">next</a><span>|</span><label class="collapse" for="c-39488329">[-]</label><label class="expand" for="c-39488329">[1 more]</label></div><br/><div class="children"><div class="content">Its usually with simple things like code completion on logging, boiler plate, repetitive tasks etc. I am surprised sometimes how copilot knows what my next step is. I am typing in the start of an algorithm and copilot gives me the rest. And it predicts a lot of things correctly. It saves me tons of time. Another thing what i like is that I no longer need to no all the programming language related syntax. In the past I looked up stuff on stackoverflow. Now i simply type a short comment like &quot;reverse the array...&quot;. Copilot automatically suggests the right syntaxes. Sometimes need some adjustments, but thats fine.</div><br/></div></div></div></div><div id="39487358" class="c"><input type="checkbox" id="c-39487358" checked=""/><div class="controls bullet"><span class="by">rglover</span><span>|</span><a href="#39487086">parent</a><span>|</span><a href="#39487202">prev</a><span>|</span><a href="#39488278">next</a><span>|</span><label class="collapse" for="c-39487358">[-]</label><label class="expand" for="c-39487358">[7 more]</label></div><br/><div class="children"><div class="content">&gt; The ones who do not embrace AI assistants in the near future will have a hard time in the future.<p>The exact opposite will be true and the funny (sad?) part is that they will lack the skills necessary (because they got lazy and over-trusted the AI) to fix mistakes&#x2F;incompatible solutions.</div><br/><div id="39487666" class="c"><input type="checkbox" id="c-39487666" checked=""/><div class="controls bullet"><span class="by">fhd2</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487358">parent</a><span>|</span><a href="#39488051">next</a><span>|</span><label class="collapse" for="c-39487666">[-]</label><label class="expand" for="c-39487666">[3 more]</label></div><br/><div class="children"><div class="content">My thinking as well. If coding assistants become so good that I&#x27;m at a competitive disadvantage, I&#x27;ll just start using them. It&#x27;s not rocket science. So far, everything I&#x27;ve tried largely slowed me down. As a Google&#x2F;SO replacement for some types of questions, they sure save me maybe an hour per week, but that&#x27;s really all I could extract so far.<p>Maybe my work is not too typical though, I spend only a fraction of my time actually typing in code. And I do eliminate the need for boilerplate through other means (picking frameworks&#x2F;libraries that are a good fit for the problem, refactoring, meta programming, scripts, suitable tool chain etc).</div><br/><div id="39487877" class="c"><input type="checkbox" id="c-39487877" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487666">parent</a><span>|</span><a href="#39488051">next</a><span>|</span><label class="collapse" for="c-39487877">[-]</label><label class="expand" for="c-39487877">[2 more]</label></div><br/><div class="children"><div class="content">1 hour per week of increased coding is a 5-7% boost in productivity, using the Amazon guidelines for how SDEs use their time — 50%&#x2F;20hrs for SDE1 and 33%&#x2F;13hrs for SDE3.<p>Is that enough to be competitive?<p>I’m not sure — but at scale that would be a 5% reduction in headcount for the same work, or ~$12M&#x2F;yr for every 1,000 engineers.<p>If you can figure out how to get 2-3 hours more coding done a week, we’re talking real gains.</div><br/><div id="39488237" class="c"><input type="checkbox" id="c-39488237" checked=""/><div class="controls bullet"><span class="by">fhd2</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487877">parent</a><span>|</span><a href="#39488051">next</a><span>|</span><label class="collapse" for="c-39488237">[-]</label><label class="expand" for="c-39488237">[1 more]</label></div><br/><div class="children"><div class="content">Depends on where the time is saved. If it&#x27;s in figuring out how to do something in Django where StackOverflow is flooded with outdated answers, sure. I see those kinds of savings. But the tragic beauty of programming is that a little time saved today can very well mean lots of time lost later. The former you can measure, the latter is a tougher nut.</div><br/></div></div></div></div></div></div><div id="39488051" class="c"><input type="checkbox" id="c-39488051" checked=""/><div class="controls bullet"><span class="by">skwirl</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39487358">parent</a><span>|</span><a href="#39487666">prev</a><span>|</span><a href="#39488278">next</a><span>|</span><label class="collapse" for="c-39488051">[-]</label><label class="expand" for="c-39488051">[3 more]</label></div><br/><div class="children"><div class="content">People said the same thing about garbage collectors.</div><br/><div id="39488165" class="c"><input type="checkbox" id="c-39488165" checked=""/><div class="controls bullet"><span class="by">swatcoder</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39488051">parent</a><span>|</span><a href="#39488278">next</a><span>|</span><label class="collapse" for="c-39488165">[-]</label><label class="expand" for="c-39488165">[2 more]</label></div><br/><div class="children"><div class="content">And sure enough, people practiced only in garbage collected environments are the ones who struggle most to work with Rust&#x27;s borrow checker or write sound embedded&#x2F;IOT code, or to attend to refence leaks in things like event listeners.<p>Did they help people write lots of effective code faster? Yes.<p>Did they breed a generation of people with little intuition around how memory works? Also yes.</div><br/><div id="39488844" class="c"><input type="checkbox" id="c-39488844" checked=""/><div class="controls bullet"><span class="by">samatman</span><span>|</span><a href="#39487086">root</a><span>|</span><a href="#39488165">parent</a><span>|</span><a href="#39488278">next</a><span>|</span><label class="collapse" for="c-39488844">[-]</label><label class="expand" for="c-39488844">[1 more]</label></div><br/><div class="children"><div class="content">This statement boils down to &quot;people who know C or C++ have an easier time learning Rust&quot; which is not especially informative, or even interesting.</div><br/></div></div></div></div></div></div></div></div><div id="39488278" class="c"><input type="checkbox" id="c-39488278" checked=""/><div class="controls bullet"><span class="by">rco8786</span><span>|</span><a href="#39487086">parent</a><span>|</span><a href="#39487358">prev</a><span>|</span><a href="#39487635">next</a><span>|</span><label class="collapse" for="c-39488278">[-]</label><label class="expand" for="c-39488278">[1 more]</label></div><br/><div class="children"><div class="content">I’ve had it enabled for months across both Javascript and Kotlin codebases and it’s…fine? Good enough that I leave it enabled. But only barely. I’m certainly not orchestrating a dev team.<p>It has probably the same productivity boost that intellisense gave back when it came out. Which is good, but still marginal. Certainly not replacing anyone’s job.</div><br/></div></div><div id="39487635" class="c"><input type="checkbox" id="c-39487635" checked=""/><div class="controls bullet"><span class="by">taude</span><span>|</span><a href="#39487086">parent</a><span>|</span><a href="#39488278">prev</a><span>|</span><a href="#39487590">next</a><span>|</span><label class="collapse" for="c-39487635">[-]</label><label class="expand" for="c-39487635">[1 more]</label></div><br/><div class="children"><div class="content">in the future, it seems like we might just become PR reviewers</div><br/></div></div></div></div><div id="39487590" class="c"><input type="checkbox" id="c-39487590" checked=""/><div class="controls bullet"><span class="by">kissgyorgy</span><span>|</span><a href="#39487086">prev</a><span>|</span><a href="#39487179">next</a><span>|</span><label class="collapse" for="c-39487590">[-]</label><label class="expand" for="c-39487590">[4 more]</label></div><br/><div class="children"><div class="content">What future? LLMs got into our tech stack faster than a JavaScript framework was created!
If you are not using some kind  of Copilot TODAY, you are missing out a lot.</div><br/><div id="39487661" class="c"><input type="checkbox" id="c-39487661" checked=""/><div class="controls bullet"><span class="by">josefresco</span><span>|</span><a href="#39487590">parent</a><span>|</span><a href="#39487652">next</a><span>|</span><label class="collapse" for="c-39487661">[-]</label><label class="expand" for="c-39487661">[1 more]</label></div><br/><div class="children"><div class="content">My success rate for writing code with ChatGTP or Copilot is about 5%. Started much better but now I can’t get either to fix any mistakes or generate useful code. Anecdotal but it hasn’t changed my life as a coder.</div><br/></div></div><div id="39487652" class="c"><input type="checkbox" id="c-39487652" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#39487590">parent</a><span>|</span><a href="#39487661">prev</a><span>|</span><a href="#39487973">next</a><span>|</span><label class="collapse" for="c-39487652">[-]</label><label class="expand" for="c-39487652">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but for me ChatGPT today was more useless than a rubber duck. Only when I said &quot;thanks for nothing&quot; it tried to turn all the blabla into code, which was unusable.<p>GitHub copilot instead, as an intelligent Intellisense, is absolutely great; a real blessing and gift to coders.</div><br/></div></div><div id="39487973" class="c"><input type="checkbox" id="c-39487973" checked=""/><div class="controls bullet"><span class="by">nozzlegear</span><span>|</span><a href="#39487590">parent</a><span>|</span><a href="#39487652">prev</a><span>|</span><a href="#39487179">next</a><span>|</span><label class="collapse" for="c-39487973">[-]</label><label class="expand" for="c-39487973">[1 more]</label></div><br/><div class="children"><div class="content">YMMV but trying to use any code that ChatGPT or Copilot generates for F# (my main language) just leads to a lot of compilation errors or worse, subtly incorrect code.</div><br/></div></div></div></div><div id="39487179" class="c"><input type="checkbox" id="c-39487179" checked=""/><div class="controls bullet"><span class="by">LASR</span><span>|</span><a href="#39487590">prev</a><span>|</span><label class="collapse" for="c-39487179">[-]</label><label class="expand" for="c-39487179">[8 more]</label></div><br/><div class="children"><div class="content">Everyone by now should be writing unit tests using ChatGPT4.<p>I paste in functions &#x2F; classes I want to write unit tests for. Paste in a sample unit test, and it does a solid job of writing tests for it in the same manner as in the sample.<p>For unit tests, you don&#x27;t even need the multi-step coverage optimization in this article. You just manually inspect, adjust it etc.</div><br/><div id="39487229" class="c"><input type="checkbox" id="c-39487229" checked=""/><div class="controls bullet"><span class="by">swatcoder</span><span>|</span><a href="#39487179">parent</a><span>|</span><a href="#39487251">next</a><span>|</span><label class="collapse" for="c-39487229">[-]</label><label class="expand" for="c-39487229">[1 more]</label></div><br/><div class="children"><div class="content">If I have a choice between a clever, practiced antagonist to plan and write my tests and an automated system that can fit common testing patterns to my code... I&#x27;m always going to get more robust results from the former.<p>But yeah, if you&#x27;re just working solo on basic stuff and need to protect against off-by-one errors and accidental mutations in later refactoring, it&#x27;s a great tool. You&#x27;d never write duly antagonistic tests for your own code anyway.</div><br/></div></div><div id="39487251" class="c"><input type="checkbox" id="c-39487251" checked=""/><div class="controls bullet"><span class="by">interroboink</span><span>|</span><a href="#39487179">parent</a><span>|</span><a href="#39487229">prev</a><span>|</span><a href="#39487449">next</a><span>|</span><label class="collapse" for="c-39487251">[-]</label><label class="expand" for="c-39487251">[3 more]</label></div><br/><div class="children"><div class="content">What about people who don&#x27;t trust sending their code to a 3rd party for processing?<p>(edit: I didn&#x27;t downvote you, but I do think your claim is over-broad)</div><br/><div id="39488157" class="c"><input type="checkbox" id="c-39488157" checked=""/><div class="controls bullet"><span class="by">biot</span><span>|</span><a href="#39487179">root</a><span>|</span><a href="#39487251">parent</a><span>|</span><a href="#39487501">next</a><span>|</span><label class="collapse" for="c-39488157">[-]</label><label class="expand" for="c-39488157">[1 more]</label></div><br/><div class="children"><div class="content">I suspect a lot of people overestimate how special their code is. Also, if your code exists in a private repo on GitHub, then you&#x27;re already trusting the same third party when using GitHub Copilot.</div><br/></div></div></div></div><div id="39487449" class="c"><input type="checkbox" id="c-39487449" checked=""/><div class="controls bullet"><span class="by">petesergeant</span><span>|</span><a href="#39487179">parent</a><span>|</span><a href="#39487251">prev</a><span>|</span><label class="collapse" for="c-39487449">[-]</label><label class="expand" for="c-39487449">[3 more]</label></div><br/><div class="children"><div class="content">Doesn’t that just bake in any bugs in the original implementation?</div><br/><div id="39488861" class="c"><input type="checkbox" id="c-39488861" checked=""/><div class="controls bullet"><span class="by">samatman</span><span>|</span><a href="#39487179">root</a><span>|</span><a href="#39487449">parent</a><span>|</span><a href="#39487796">next</a><span>|</span><label class="collapse" for="c-39488861">[-]</label><label class="expand" for="c-39488861">[1 more]</label></div><br/><div class="children"><div class="content">Oddly enough it doesn&#x27;t.<p>The secret sauce is: the robot doesn&#x27;t run the tests. It digests the code and writes tests for it.<p>So it doesn&#x27;t know if they&#x27;ll pass or not. And sure enough, some of them don&#x27;t, because the code was buggy.<p>I&#x27;ve seen some awful code come out of ChatGPT, but never a bad test. Good tests are short, which makes them hard to screw up. It has a reasonable grasp on what an edge case is as well.</div><br/></div></div><div id="39487796" class="c"><input type="checkbox" id="c-39487796" checked=""/><div class="controls bullet"><span class="by">romwell</span><span>|</span><a href="#39487179">root</a><span>|</span><a href="#39487449">parent</a><span>|</span><a href="#39488861">prev</a><span>|</span><label class="collapse" for="c-39487796">[-]</label><label class="expand" for="c-39487796">[1 more]</label></div><br/><div class="children"><div class="content">Of course not!<p>The <i>Power of AI™</i> can figure out the true <i>intent</i> of the code by looking at the initial (and, potentially, buggy) implementation, and help the programmer by generating edge test cases where the code doesn&#x27;t produce correct results.<p>The programmer will easily tell those test cases from the ones where the AI did a mistake and generated a flawed test case because the AI doesn&#x27;t make such silly mistakes; <i>clearly</i>, it&#x27;s the programmer&#x27;s code that needs to be corrected.<p>In fact, the AI would do a better job at that, too, which clearly speeds up the development cycle.<p>The correct way to use the tool is to let the AI both generate the test cases and modify the code so that it would pass the tests it generates.<p>After all, if the AI can&#x27;t figure out what you wanted to do in the first place — how can you?<p>Of course, there&#x27;s more to it.<p>Whichever problems you run into can be surely attributed to writing the code in an AI-unfriendly way.<p>In the past, we had the adage that the code is <i>read</i> more times than it&#x27;s <i>written</i>. This is still true, but we need to abandon the old habit habit of having a <i>human</i> reader in mind.<p>Just like you rearrange your furniture to make your house more accessible to the robot vacuum cleaner, you need to write the code with the AI in mind.<p>When you write a Google query or a prompt for ChatGPT (effectively the same thing anyway), you don&#x27;t write it like you&#x27;d talk to a person.<p>You&#x27;re going to have to write code the same way to be truly effective,  and think a little bit like AI to get the most use out of it.<p>That <i>might</i> sound like a lot of work to get code that does what you want.<p>But of course, that&#x27;s not the case.<p>Just use the AI for this.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>