<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1695373261159" as="style"/><link rel="stylesheet" href="styles.css?v=1695373261159"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html">Outperforming larger language models with less training data and smaller models</a> <span class="domain">(<a href="https://blog.research.google">blog.research.google</a>)</span></div><div class="subtext"><span>atg_abhishek</span> | <span>72 comments</span></div><br/><div><div id="37607237" class="c"><input type="checkbox" id="c-37607237" checked=""/><div class="controls bullet"><span class="by">greatpostman</span><span>|</span><a href="#37608611">next</a><span>|</span><label class="collapse" for="c-37607237">[-]</label><label class="expand" for="c-37607237">[24 more]</label></div><br/><div class="children"><div class="content">Still waiting for google to release a model that matches gpt4. Until then, I’m assuming their presumed ai supremacy is marketing</div><br/><div id="37607366" class="c"><input type="checkbox" id="c-37607366" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#37607237">parent</a><span>|</span><a href="#37609258">next</a><span>|</span><label class="collapse" for="c-37607366">[-]</label><label class="expand" for="c-37607366">[19 more]</label></div><br/><div class="children"><div class="content">There&#x27;s AI-research supremacy and there&#x27;s AI-product supremacy. Google has many brains (pun intended) on the former, but lacks incentive&#x2F;structure for the latter.</div><br/><div id="37607589" class="c"><input type="checkbox" id="c-37607589" checked=""/><div class="controls bullet"><span class="by">6D794163636F756</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37607366">parent</a><span>|</span><a href="#37609258">next</a><span>|</span><label class="collapse" for="c-37607589">[-]</label><label class="expand" for="c-37607589">[18 more]</label></div><br/><div class="children"><div class="content">They&#x27;re often also slow to market. They weren&#x27;t the first search engine, they weren&#x27;t the first email platform, they weren&#x27;t the first video site, but now they&#x27;re the biggest in those categories. I think Bard&#x27;s growth will be very interesting to watch given that track record.<p>Or it&#x27;ll get dropped within a year.</div><br/><div id="37607742" class="c"><input type="checkbox" id="c-37607742" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37607589">parent</a><span>|</span><a href="#37607751">next</a><span>|</span><label class="collapse" for="c-37607742">[-]</label><label class="expand" for="c-37607742">[6 more]</label></div><br/><div class="children"><div class="content">&gt; They&#x27;re...<p>It&#x27;s worth mentioning that the Google you&#x27;re talking about was way way different than it is today. Google Search was a startup. Google Search + Email was a small company. Google Search + Email + YouTube was a midsize firm. Now they&#x27;re a humungous megacorp that&#x27;s slow to make necessary changes when there&#x27;s a paradigm shift like LLMs.</div><br/><div id="37608703" class="c"><input type="checkbox" id="c-37608703" checked=""/><div class="controls bullet"><span class="by">d3vmax</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37607742">parent</a><span>|</span><a href="#37608880">next</a><span>|</span><label class="collapse" for="c-37608703">[-]</label><label class="expand" for="c-37608703">[1 more]</label></div><br/><div class="children"><div class="content">I agree with the point you are trying to make, but Google Search + Email was not a small company. I remember Gmail Beta, Google was already known world over and definitely not a &#x27;small&#x27; company.</div><br/></div></div><div id="37608880" class="c"><input type="checkbox" id="c-37608880" checked=""/><div class="controls bullet"><span class="by">piyushpr134</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37607742">parent</a><span>|</span><a href="#37608703">prev</a><span>|</span><a href="#37608151">next</a><span>|</span><label class="collapse" for="c-37608880">[-]</label><label class="expand" for="c-37608880">[2 more]</label></div><br/><div class="children"><div class="content">android and chrome both are also late but they won. In Cloud also they are late  but they look like to be making big progress.</div><br/><div id="37609332" class="c"><input type="checkbox" id="c-37609332" checked=""/><div class="controls bullet"><span class="by">oezi</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37608880">parent</a><span>|</span><a href="#37608151">next</a><span>|</span><label class="collapse" for="c-37609332">[-]</label><label class="expand" for="c-37609332">[1 more]</label></div><br/><div class="children"><div class="content">Android didn&#x27;t win, they just prevented Apple to have a 100% monopoly on smartphones.</div><br/></div></div></div></div><div id="37608151" class="c"><input type="checkbox" id="c-37608151" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37607742">parent</a><span>|</span><a href="#37608880">prev</a><span>|</span><a href="#37607769">next</a><span>|</span><label class="collapse" for="c-37608151">[-]</label><label class="expand" for="c-37608151">[1 more]</label></div><br/><div class="children"><div class="content">Also keep in mind that YouTube was an acquisition.</div><br/></div></div><div id="37607769" class="c"><input type="checkbox" id="c-37607769" checked=""/><div class="controls bullet"><span class="by">peddling-brink</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37607742">parent</a><span>|</span><a href="#37608151">prev</a><span>|</span><a href="#37607751">next</a><span>|</span><label class="collapse" for="c-37607769">[-]</label><label class="expand" for="c-37607769">[1 more]</label></div><br/><div class="children"><div class="content">Doesn’t seem that slow. They fired a bunch of people and are integrating LLMs all over the place.<p>OAI had a big head start for sure.</div><br/></div></div></div></div><div id="37607751" class="c"><input type="checkbox" id="c-37607751" checked=""/><div class="controls bullet"><span class="by">mellosouls</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37607589">parent</a><span>|</span><a href="#37607742">prev</a><span>|</span><a href="#37607629">next</a><span>|</span><label class="collapse" for="c-37607751">[-]</label><label class="expand" for="c-37607751">[1 more]</label></div><br/><div class="children"><div class="content">They were the best search engine from the get-go and had a massive impact. Gmail&#x2F;maps also to a lesser extent.<p>In AI they were giants in what looks post-ChatGPT like a mediocre field. Their search now is looking very jaded.<p>The trajectory here isn&#x27;t remotely like their past performances; it&#x27;s not a safe bet to assume they&#x27;ll win through with Bard or anything else.<p>The agility of OpenAI and the revolutionary impact of gpt3+ has made the former incumbents like Google look like posturing, self-satisfied, giant lumbering has-beens. They aren&#x27;t getting back on top without massive internal changes.</div><br/></div></div><div id="37607629" class="c"><input type="checkbox" id="c-37607629" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37607589">parent</a><span>|</span><a href="#37607751">prev</a><span>|</span><a href="#37608243">next</a><span>|</span><label class="collapse" for="c-37607629">[-]</label><label class="expand" for="c-37607629">[6 more]</label></div><br/><div class="children"><div class="content">It’s not getting dropped. If OpenAI’s models get good enough, search traffic will crater and Google will fall on extremely hard times for lack of ad revenue. At the same time, if they fail to integrate state of the art generative features into Workspace, people will go to Microsoft, where GPT-4 is presently handing Google their ass. Yes, Google Duet for Workspace totally sucks; I suspect their trial conversion rate approaches 0%.<p>This is a make it or break it problem for Google and they will get it right or they won’t get it at all.</div><br/><div id="37608956" class="c"><input type="checkbox" id="c-37608956" checked=""/><div class="controls bullet"><span class="by">piyushpr134</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37607629">parent</a><span>|</span><a href="#37608261">next</a><span>|</span><label class="collapse" for="c-37608956">[-]</label><label class="expand" for="c-37608956">[1 more]</label></div><br/><div class="children"><div class="content">not at all. I feel (this data is from running SEM for large website(s)) that Google earns most of their revenue from tactical searches. For instance, most people search for website names instead of typing the url. This creates a massive tactical search traffic base for google and brings largest revenue for them. There is a huge competition in this category as most competitors bid on others.<p>Similarly, product searches are second largest category in which they make tonnes of money. This is also done by people as they don&#x27;t really like to search on amazon or other ecommerce sites. This is also a huge money spinner for them.<p>Both of these are not going anywhere as both of these are tactical spends.<p>Now let us come to long tail. These are again big money and are at risk for Google. However, you have to understand that Goog ads are clicked by most tier 2 users. We, techies, do not really click at ads. We go for organic ranking (mostly). We are the base of chatGPT right now. Tier 2 and lower users don&#x27;t really use chatgpt.<p>Even if they do, they would not do it for product discovery or site discovery as it has too much friction: go to chat.openai.com, type in your question, it responds in slow, jerky manner vs just type in browser bar what you are thinking.<p>To top it, Chatgpt also has stale data. Moreover, it is heavily lobotomized to not give any controversial or edgy answers. This curtails usefulness of chatgpt.</div><br/></div></div><div id="37608261" class="c"><input type="checkbox" id="c-37608261" checked=""/><div class="controls bullet"><span class="by">azinman2</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37607629">parent</a><span>|</span><a href="#37608956">prev</a><span>|</span><a href="#37607721">next</a><span>|</span><label class="collapse" for="c-37608261">[-]</label><label class="expand" for="c-37608261">[3 more]</label></div><br/><div class="children"><div class="content">Have you switched to chatgpt for everything you googled? I haven’t. And most of the time I’m wanting destinations versus a specific answer.</div><br/><div id="37608972" class="c"><input type="checkbox" id="c-37608972" checked=""/><div class="controls bullet"><span class="by">carom</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37608261">parent</a><span>|</span><a href="#37609115">next</a><span>|</span><label class="collapse" for="c-37608972">[-]</label><label class="expand" for="c-37608972">[1 more]</label></div><br/><div class="children"><div class="content">I have for programming searches. ChatGPT can give me things that would take 5+ minutes of searching to hone in on. Then again, I run an ad blocker so no loss for them.</div><br/></div></div><div id="37609115" class="c"><input type="checkbox" id="c-37609115" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37608261">parent</a><span>|</span><a href="#37608972">prev</a><span>|</span><a href="#37607721">next</a><span>|</span><label class="collapse" for="c-37609115">[-]</label><label class="expand" for="c-37609115">[1 more]</label></div><br/><div class="children"><div class="content">For any domain specific questions or discussions I use ChatGPT because it’s so much better than Google. People underestimate how many fields it’s already valuable in beyond programming.<p>I use Google for basic bitch things like finding a company website or what time is it in Tokyo.</div><br/></div></div></div></div><div id="37607721" class="c"><input type="checkbox" id="c-37607721" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37607629">parent</a><span>|</span><a href="#37608261">prev</a><span>|</span><a href="#37608243">next</a><span>|</span><label class="collapse" for="c-37607721">[-]</label><label class="expand" for="c-37607721">[1 more]</label></div><br/><div class="children"><div class="content">There was a graph that showed Google&#x27;s search didn&#x27;t get affected much after ChatGPT was announced.</div><br/></div></div></div></div><div id="37608243" class="c"><input type="checkbox" id="c-37608243" checked=""/><div class="controls bullet"><span class="by">antupis</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37607589">parent</a><span>|</span><a href="#37607629">prev</a><span>|</span><a href="#37608655">next</a><span>|</span><label class="collapse" for="c-37608243">[-]</label><label class="expand" for="c-37608243">[1 more]</label></div><br/><div class="children"><div class="content">Google 2023 is completely different company than it was in 2005. It is like comparing HP in 1999 and 1981.</div><br/></div></div><div id="37608655" class="c"><input type="checkbox" id="c-37608655" checked=""/><div class="controls bullet"><span class="by">passion__desire</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37607589">parent</a><span>|</span><a href="#37608243">prev</a><span>|</span><a href="#37608657">next</a><span>|</span><label class="collapse" for="c-37608655">[-]</label><label class="expand" for="c-37608655">[1 more]</label></div><br/><div class="children"><div class="content">You forgot about Android. It was an acquisition.</div><br/></div></div><div id="37608657" class="c"><input type="checkbox" id="c-37608657" checked=""/><div class="controls bullet"><span class="by">FooBarWidget</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37607589">parent</a><span>|</span><a href="#37608655">prev</a><span>|</span><a href="#37609258">next</a><span>|</span><label class="collapse" for="c-37608657">[-]</label><label class="expand" for="c-37608657">[2 more]</label></div><br/><div class="children"><div class="content">&gt; they weren&#x27;t the first video site, but now they&#x27;re the biggest in those categories<p>Acquisition doesn&#x27;t count</div><br/><div id="37608781" class="c"><input type="checkbox" id="c-37608781" checked=""/><div class="controls bullet"><span class="by">two_in_one</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37608657">parent</a><span>|</span><a href="#37609258">next</a><span>|</span><label class="collapse" for="c-37608781">[-]</label><label class="expand" for="c-37608781">[1 more]</label></div><br/><div class="children"><div class="content">They still can grab OpenAI, MS Edge history will repeat.</div><br/></div></div></div></div></div></div></div></div><div id="37609258" class="c"><input type="checkbox" id="c-37609258" checked=""/><div class="controls bullet"><span class="by">druskacik</span><span>|</span><a href="#37607237">parent</a><span>|</span><a href="#37607366">prev</a><span>|</span><a href="#37608175">next</a><span>|</span><label class="collapse" for="c-37609258">[-]</label><label class="expand" for="c-37609258">[1 more]</label></div><br/><div class="children"><div class="content">The article is just a fine-tuning concept that could have been published by anyone, using any large LLM and small LLM in combination. This has nothing to do with marketing.</div><br/></div></div><div id="37608175" class="c"><input type="checkbox" id="c-37608175" checked=""/><div class="controls bullet"><span class="by">bobmaxup</span><span>|</span><a href="#37607237">parent</a><span>|</span><a href="#37609258">prev</a><span>|</span><a href="#37608304">next</a><span>|</span><label class="collapse" for="c-37608175">[-]</label><label class="expand" for="c-37608175">[2 more]</label></div><br/><div class="children"><div class="content">Bard is getting decent in search results. They adopted some patterns Phind took in early interfaces with citations and made it much better in terms of interaction and response time compared to what Phind is doing today.<p>Also, with more recent changes I can get decent summaries much easier (and at no cost to me) for every document in a google drive.<p>In my day to day, those features can be pretty powerful tool, even if its not at GPT4 level 6 months after GPT4 and Bard were released in March.<p>To my knowledge, no one is really integrating search at the level that Google is with generative language models.</div><br/><div id="37608246" class="c"><input type="checkbox" id="c-37608246" checked=""/><div class="controls bullet"><span class="by">Palmik</span><span>|</span><a href="#37607237">root</a><span>|</span><a href="#37608175">parent</a><span>|</span><a href="#37608304">next</a><span>|</span><label class="collapse" for="c-37608246">[-]</label><label class="expand" for="c-37608246">[1 more]</label></div><br/><div class="children"><div class="content">Bing has citations, Perplexity has citations, etc.<p>I would not be surprised if ChatGPT moves some of the plugins to the free tier as well, to stay competitive over the longer term.</div><br/></div></div></div></div></div></div><div id="37608611" class="c"><input type="checkbox" id="c-37608611" checked=""/><div class="controls bullet"><span class="by">fbnbr</span><span>|</span><a href="#37607237">prev</a><span>|</span><a href="#37607290">next</a><span>|</span><label class="collapse" for="c-37608611">[-]</label><label class="expand" for="c-37608611">[1 more]</label></div><br/><div class="children"><div class="content">I think smaller expert models will dominate the majority of applications. there is an optimum and fine balance to strike when it comes to size and usability. There will be many mechanisms like demonstrated in the post to find that optimum and realize it.</div><br/></div></div><div id="37607290" class="c"><input type="checkbox" id="c-37607290" checked=""/><div class="controls bullet"><span class="by">pedrovhb</span><span>|</span><a href="#37608611">prev</a><span>|</span><a href="#37606785">next</a><span>|</span><label class="collapse" for="c-37607290">[-]</label><label class="expand" for="c-37607290">[4 more]</label></div><br/><div class="children"><div class="content">Interesting that they use T5 for the distilled model. I was under the impression that encoder-decoder architectures were on the way of the Dodo, but it seems they may still be relevant after all.<p>Also interesting is that this isn&#x27;t an inconceivably clever and out of the box idea. It shows there&#x27;s still a lot of low hanging fruit to explore, and the future of LLMs isn&#x27;t set in stone yet. Could be that the real deal is a mixture of experts trained in this style. It&#x27;s exciting that it feels the holy grail is close to being achievable if only the right combination of ideas is tried.</div><br/><div id="37608116" class="c"><input type="checkbox" id="c-37608116" checked=""/><div class="controls bullet"><span class="by">rolisz</span><span>|</span><a href="#37607290">parent</a><span>|</span><a href="#37607360">next</a><span>|</span><label class="collapse" for="c-37608116">[-]</label><label class="expand" for="c-37608116">[1 more]</label></div><br/><div class="children"><div class="content">T5 family is awesome. FastChat-T5 has amazing text generation quality (eg for RAG chatbots) and it can easily run on CPU fast enough to do a live conversation.</div><br/></div></div><div id="37607360" class="c"><input type="checkbox" id="c-37607360" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#37607290">parent</a><span>|</span><a href="#37608116">prev</a><span>|</span><a href="#37606785">next</a><span>|</span><label class="collapse" for="c-37607360">[-]</label><label class="expand" for="c-37607360">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I was under the impression that encoder-decoder architectures were on the way of the Dodo,...<p>Why is that?</div><br/><div id="37609665" class="c"><input type="checkbox" id="c-37609665" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#37607290">root</a><span>|</span><a href="#37607360">parent</a><span>|</span><a href="#37606785">next</a><span>|</span><label class="collapse" for="c-37609665">[-]</label><label class="expand" for="c-37609665">[1 more]</label></div><br/><div class="children"><div class="content">Decoder-only is pretty good on some tasks. Translation for example was a classic encoded-decoder task but it seems decoders can handle it.</div><br/></div></div></div></div></div></div><div id="37606785" class="c"><input type="checkbox" id="c-37606785" checked=""/><div class="controls bullet"><span class="by">xnx</span><span>|</span><a href="#37607290">prev</a><span>|</span><a href="#37608216">next</a><span>|</span><label class="collapse" for="c-37606785">[-]</label><label class="expand" for="c-37606785">[1 more]</label></div><br/><div class="children"><div class="content">The amount of activity and progress in the LLM&#x2F;ML&#x2F;AI spaces is truly fantastic. Optimizations like this are particularly valuable when hardware (e.g. Nvidia) is so expensive.</div><br/></div></div><div id="37608216" class="c"><input type="checkbox" id="c-37608216" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#37606785">prev</a><span>|</span><a href="#37608088">next</a><span>|</span><label class="collapse" for="c-37608216">[-]</label><label class="expand" for="c-37608216">[1 more]</label></div><br/><div class="children"><div class="content">So this <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.08410" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.08410</a> but one year later</div><br/></div></div><div id="37608088" class="c"><input type="checkbox" id="c-37608088" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37608216">prev</a><span>|</span><a href="#37607622">next</a><span>|</span><label class="collapse" for="c-37608088">[-]</label><label class="expand" for="c-37608088">[4 more]</label></div><br/><div class="children"><div class="content">I am not a researcher, but it always seemed intuitive to me that the most effective models would be multimodal and trained with a core carefully tailored curriculum.<p>I would want to ensure that the system gains and retains the fundamental structures and skills that you know it needs to effectively and accurately generalize. While maintaining those things you then feed it lots of diverse data to learn the exceptions and ways the skills can be combined. But somehow you need to ensure those core skills and knowledge throughout. Maybe you could do that just by including outputting those understandings or manipulations in addition to the final answer. Similar to what the paper does.<p>For example, a code generation model might be required to output a  state machine simulation of the requested program.</div><br/><div id="37608145" class="c"><input type="checkbox" id="c-37608145" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#37608088">parent</a><span>|</span><a href="#37607622">next</a><span>|</span><label class="collapse" for="c-37608145">[-]</label><label class="expand" for="c-37608145">[3 more]</label></div><br/><div class="children"><div class="content">I agree that multimodal seems like the way to go, but it&#x27;s not at all intuitive why we should expect the curriculum needing to be carefully tailored.  Compare <a href="https:&#x2F;&#x2F;gwern.net&#x2F;scaling-hypothesis" rel="nofollow noreferrer">https:&#x2F;&#x2F;gwern.net&#x2F;scaling-hypothesis</a></div><br/><div id="37608501" class="c"><input type="checkbox" id="c-37608501" checked=""/><div class="controls bullet"><span class="by">mcmoor</span><span>|</span><a href="#37608088">root</a><span>|</span><a href="#37608145">parent</a><span>|</span><a href="#37607622">next</a><span>|</span><label class="collapse" for="c-37608501">[-]</label><label class="expand" for="c-37608501">[2 more]</label></div><br/><div class="children"><div class="content">Yeah so far looking at the progression since 70s, domain-specific attempts stagnant while just-increase-the-input-and-core attempts get us where we are now</div><br/><div id="37608849" class="c"><input type="checkbox" id="c-37608849" checked=""/><div class="controls bullet"><span class="by">two_in_one</span><span>|</span><a href="#37608088">root</a><span>|</span><a href="#37608501">parent</a><span>|</span><a href="#37607622">next</a><span>|</span><label class="collapse" for="c-37608849">[-]</label><label class="expand" for="c-37608849">[1 more]</label></div><br/><div class="children"><div class="content">My impression is that while domain specific models do better in their domain. They do not generalize well and cannot be transferred to different domains without retraining. Hence comes mixture of experts (domain specifics). The trick is to juggle them all. Which needs another quite capable &#x27;manager&#x27; model. The good thing is that they can be trained separately. Also at inference there in no need to call them all. Existing models, like GPT-4, can be used to create datasets for experts, or enhance&#x2F;balance existing datasets.</div><br/></div></div></div></div></div></div></div></div><div id="37607622" class="c"><input type="checkbox" id="c-37607622" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#37608088">prev</a><span>|</span><a href="#37607000">next</a><span>|</span><label class="collapse" for="c-37607622">[-]</label><label class="expand" for="c-37607622">[9 more]</label></div><br/><div class="children"><div class="content"><i>&gt; given the input question “Sammy wanted to go to where the people are. Where might he go? Answer Choices: (a) populated areas, (b) race track, (c) desert, (d) apartment, (e) roadblock”, distilling step-by-step provides the correct answer to the question, “(a) populated areas”</i><p>Huh? My answer as a human would have been &quot;race track&quot;, as that is probably &quot;where the people are&quot; (during a race).<p>Did I fail? Am I a poor language model? Or is the whole thing just tea leaf reading to begin with?</div><br/><div id="37607988" class="c"><input type="checkbox" id="c-37607988" checked=""/><div class="controls bullet"><span class="by">Reubend</span><span>|</span><a href="#37607622">parent</a><span>|</span><a href="#37609482">next</a><span>|</span><label class="collapse" for="c-37607988">[-]</label><label class="expand" for="c-37607988">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Did I fail? Am I a poor language model? Or is the whole thing just tea leaf reading to begin with?<p>No offense to you, but I never would have picked &quot;race track&quot;. That answer doesn&#x27;t make much sense since the original prompt doesn&#x27;t mention anything about racing, and the definition of a populated area fits well with the question being asked.</div><br/></div></div><div id="37609482" class="c"><input type="checkbox" id="c-37609482" checked=""/><div class="controls bullet"><span class="by">diziet</span><span>|</span><a href="#37607622">parent</a><span>|</span><a href="#37607988">prev</a><span>|</span><a href="#37607687">next</a><span>|</span><label class="collapse" for="c-37609482">[-]</label><label class="expand" for="c-37609482">[1 more]</label></div><br/><div class="children"><div class="content">A google image search for &quot;race track&quot; has 0 people visible in the 17 images above the fold.</div><br/></div></div><div id="37607687" class="c"><input type="checkbox" id="c-37607687" checked=""/><div class="controls bullet"><span class="by">smeagull</span><span>|</span><a href="#37607622">parent</a><span>|</span><a href="#37609482">prev</a><span>|</span><a href="#37607000">next</a><span>|</span><label class="collapse" for="c-37607687">[-]</label><label class="expand" for="c-37607687">[6 more]</label></div><br/><div class="children"><div class="content">A race track or an apartment are likely to be populated areas as well. &quot;A&quot; is a broader answer that encompasses more possibilities.</div><br/><div id="37607756" class="c"><input type="checkbox" id="c-37607756" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#37607622">root</a><span>|</span><a href="#37607687">parent</a><span>|</span><a href="#37607740">next</a><span>|</span><label class="collapse" for="c-37607756">[-]</label><label class="expand" for="c-37607756">[3 more]</label></div><br/><div class="children"><div class="content">&quot;Where the people are&quot; doesn&#x27;t simply mean &quot;where a large number of people live&quot;. It also has connotations of &quot;where it&#x27;s happening&quot;, as in &quot;where people are coming together (right now)&quot;.<p>But my point is really that speaking of the <i>correct</i> answer with a question as vague and open to interpretation as this one is absurd.</div><br/><div id="37608013" class="c"><input type="checkbox" id="c-37608013" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37607622">root</a><span>|</span><a href="#37607756">parent</a><span>|</span><a href="#37607740">next</a><span>|</span><label class="collapse" for="c-37608013">[-]</label><label class="expand" for="c-37608013">[2 more]</label></div><br/><div class="children"><div class="content">If you really want to go full logician on this, even the format in which you&#x27;re asked the question should make (a) being the correct answer clear.<p>You&#x27;re being asked a multiple choice question:<p>- There&#x27;s an implication of intended vagueness&#x2F;indirection and multiple possibly suitable answers.<p>- Textual entailment is a very common type of multiple choice format, which would lean towards a more plain semantic correlation</div><br/><div id="37609547" class="c"><input type="checkbox" id="c-37609547" checked=""/><div class="controls bullet"><span class="by">kybernetikos</span><span>|</span><a href="#37607622">root</a><span>|</span><a href="#37608013">parent</a><span>|</span><a href="#37607740">next</a><span>|</span><label class="collapse" for="c-37609547">[-]</label><label class="expand" for="c-37609547">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, I pattern matched (a) as the correct answer too, but on reflection on the content of the words and what it would all actually mean, I think that (a) is a bad answer. If we aren&#x27;t careful we&#x27;ll train our AIs to be good at giving incorrect, pat answers to inadequately thought out questions.</div><br/></div></div></div></div></div></div><div id="37607740" class="c"><input type="checkbox" id="c-37607740" checked=""/><div class="controls bullet"><span class="by">Zuiii</span><span>|</span><a href="#37607622">root</a><span>|</span><a href="#37607687">parent</a><span>|</span><a href="#37607756">prev</a><span>|</span><a href="#37607000">next</a><span>|</span><label class="collapse" for="c-37607740">[-]</label><label class="expand" for="c-37607740">[2 more]</label></div><br/><div class="children"><div class="content">A depending on the size of a &quot;populated area&quot;, the density can be far lower than a crowded race track.</div><br/><div id="37608790" class="c"><input type="checkbox" id="c-37608790" checked=""/><div class="controls bullet"><span class="by">kybernetikos</span><span>|</span><a href="#37607622">root</a><span>|</span><a href="#37607740">parent</a><span>|</span><a href="#37607000">next</a><span>|</span><label class="collapse" for="c-37608790">[-]</label><label class="expand" for="c-37608790">[1 more]</label></div><br/><div class="children"><div class="content">Yeah &quot;I felt lonely and wanted to be where the people are, so I wandered through the suburbs&quot; <i>really</i> doesn&#x27;t make sense. I think this is just a bad question.</div><br/></div></div></div></div></div></div></div></div><div id="37607000" class="c"><input type="checkbox" id="c-37607000" checked=""/><div class="controls bullet"><span class="by">sinuhe69</span><span>|</span><a href="#37607622">prev</a><span>|</span><a href="#37607114">next</a><span>|</span><label class="collapse" for="c-37607000">[-]</label><label class="expand" for="c-37607000">[2 more]</label></div><br/><div class="children"><div class="content">Why the amount of the training data for LLM is less than for the distilled and task-specific models (in the first figure)?<p>Or did the authors count the amount of training data for the LLMs to the required training data for the destined&#x2F;task-specific models?<p><a href="https:&#x2F;&#x2F;blogger.googleusercontent.com&#x2F;img&#x2F;b&#x2F;R29vZ2xl&#x2F;AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQKqwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2&#x2F;s1570&#x2F;image3.png" rel="nofollow noreferrer">https:&#x2F;&#x2F;blogger.googleusercontent.com&#x2F;img&#x2F;b&#x2F;R29vZ2xl&#x2F;AVvXsEj...</a></div><br/><div id="37607056" class="c"><input type="checkbox" id="c-37607056" checked=""/><div class="controls bullet"><span class="by">swsieber</span><span>|</span><a href="#37607000">parent</a><span>|</span><a href="#37607114">next</a><span>|</span><label class="collapse" for="c-37607056">[-]</label><label class="expand" for="c-37607056">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Or did the authors count the amount of training data for the LLMs to the required training data for the destined&#x2F;task-specific models?<p>Yes.<p>They are counting the amount of data you need to collect to solve your problem. I can grab a pretrained LLM, and the data I have to collect in that instance is what I need to fine tune it.</div><br/></div></div></div></div><div id="37607114" class="c"><input type="checkbox" id="c-37607114" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#37607000">prev</a><span>|</span><a href="#37606983">next</a><span>|</span><label class="collapse" for="c-37607114">[-]</label><label class="expand" for="c-37607114">[14 more]</label></div><br/><div class="children"><div class="content">&gt; For instance, serving a single 175 billion LLM requires at least 350GB of GPU memory using specialized infrastructure<p>Apple ships the Mac Studio which support up to 144GB of usable GPU memory.<p>Would be amusing if they were to release a Mac Pro with 300+ GB and dominate the LLM serving space.</div><br/><div id="37609412" class="c"><input type="checkbox" id="c-37609412" checked=""/><div class="controls bullet"><span class="by">oezi</span><span>|</span><a href="#37607114">parent</a><span>|</span><a href="#37607535">next</a><span>|</span><label class="collapse" for="c-37609412">[-]</label><label class="expand" for="c-37609412">[1 more]</label></div><br/><div class="children"><div class="content">I am wondering who will be the first to raise RAM capacity of their hardware offerings dramatically to win the LLM crowd. Seems like the way to win market share.</div><br/></div></div><div id="37607535" class="c"><input type="checkbox" id="c-37607535" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37607114">parent</a><span>|</span><a href="#37609412">prev</a><span>|</span><a href="#37607344">next</a><span>|</span><label class="collapse" for="c-37607535">[-]</label><label class="expand" for="c-37607535">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Would be amusing if they were to release a Mac Pro with 300+ GB and dominate the LLM serving space<p>Is there any framework that can batch LLMs on Metal? I don&#x27;t think GGML or MLC have it yet.<p>Otherwise that is just another reason they wouldn&#x27;t be good for LLM hosting at this moment.<p>Anyway, the real disruptor is Intel. They could theoretically barge in with a 2x48GB Arc card and undercut the market that AMD&#x2F;Nvidia refuse to dive into because of their Pro card clients.</div><br/><div id="37608071" class="c"><input type="checkbox" id="c-37608071" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37607114">root</a><span>|</span><a href="#37607535">parent</a><span>|</span><a href="#37607344">next</a><span>|</span><label class="collapse" for="c-37608071">[-]</label><label class="expand" for="c-37608071">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s coming to llama.cpp soon.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;3228">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;3228</a></div><br/><div id="37608355" class="c"><input type="checkbox" id="c-37608355" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37607114">root</a><span>|</span><a href="#37608071">parent</a><span>|</span><a href="#37608358">next</a><span>|</span><label class="collapse" for="c-37608355">[-]</label><label class="expand" for="c-37608355">[1 more]</label></div><br/><div class="children"><div class="content">Amazing!<p>Its really a shame that Apple (and AMD&#x2F;Intel or pretty much <i>any</i> other infrence vendor) are not directly contributing to llama.cpp. The feature set is amazing and growing at a stunning pace.</div><br/></div></div></div></div></div></div><div id="37607393" class="c"><input type="checkbox" id="c-37607393" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#37607114">parent</a><span>|</span><a href="#37607344">prev</a><span>|</span><a href="#37607281">next</a><span>|</span><label class="collapse" for="c-37607393">[-]</label><label class="expand" for="c-37607393">[1 more]</label></div><br/><div class="children"><div class="content">That number is not quantized either. Quantize 175B param at 4 bits then it&#x27;d fit in ~120GB VRAM (one can fit 34B models at 4 bit quantization in a single RTX3090 24GB VRAM)</div><br/></div></div><div id="37607281" class="c"><input type="checkbox" id="c-37607281" checked=""/><div class="controls bullet"><span class="by">rubyn00bie</span><span>|</span><a href="#37607114">parent</a><span>|</span><a href="#37607393">prev</a><span>|</span><a href="#37606983">next</a><span>|</span><label class="collapse" for="c-37607281">[-]</label><label class="expand" for="c-37607281">[6 more]</label></div><br/><div class="children"><div class="content">Apple&#x27;s hardware advantage is something I&#x27;m hoping to see really unleashed with the M3 generation. The fact the A17 Pro has ray tracing support is giving me hope they can catch up with the incumbents quickly. It&#x27;s honestly the one thing that has kept me away from their newer hardware-- I primarily use my computer at a desk and PC hardware (GPU primarily) is leagues ahead of what their very best can do. I honestly can&#x27;t justify spending nearly 4k when Linux works great for everything I do professionally AND when I&#x27;m done with work I can still play a game.</div><br/><div id="37607377" class="c"><input type="checkbox" id="c-37607377" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#37607114">root</a><span>|</span><a href="#37607281">parent</a><span>|</span><a href="#37606983">next</a><span>|</span><label class="collapse" for="c-37607377">[-]</label><label class="expand" for="c-37607377">[5 more]</label></div><br/><div class="children"><div class="content">3 most important things for Apple are:<p>- HW design<p>- UI<p>- UX<p>That Macs come with awesome VRAM is only because #3. Apple has no explicit incentive to make LLM-ready machines.</div><br/><div id="37607576" class="c"><input type="checkbox" id="c-37607576" checked=""/><div class="controls bullet"><span class="by">leetharris</span><span>|</span><a href="#37607114">root</a><span>|</span><a href="#37607377">parent</a><span>|</span><a href="#37607441">next</a><span>|</span><label class="collapse" for="c-37607576">[-]</label><label class="expand" for="c-37607576">[2 more]</label></div><br/><div class="children"><div class="content">You can find many leaked email threads from Apple where they talk about the value of high powered local compute. It is a fundamental part of their strategy. They do not want the cloud approach for high end compute like Google, they want you to have high end and powerful devices in your hands.</div><br/><div id="37607701" class="c"><input type="checkbox" id="c-37607701" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#37607114">root</a><span>|</span><a href="#37607576">parent</a><span>|</span><a href="#37607441">next</a><span>|</span><label class="collapse" for="c-37607701">[-]</label><label class="expand" for="c-37607701">[1 more]</label></div><br/><div class="children"><div class="content">Got a link to those emails?</div><br/></div></div></div></div><div id="37607441" class="c"><input type="checkbox" id="c-37607441" checked=""/><div class="controls bullet"><span class="by">rubyn00bie</span><span>|</span><a href="#37607114">root</a><span>|</span><a href="#37607377">parent</a><span>|</span><a href="#37607576">prev</a><span>|</span><a href="#37606983">next</a><span>|</span><label class="collapse" for="c-37607441">[-]</label><label class="expand" for="c-37607441">[2 more]</label></div><br/><div class="children"><div class="content">I disagree. ML is eating the world by storm and one thing Apple loves is high margins. Folks like myself would gladly pay that margin to someone besides Nvidia. I paid $2k for my last GPU and was happy to do it. That doesn’t even factor the fact that businesses would pay even more when the alternative are machines that start in the tens of thousands for just the GPU ability. GPUs right now are carrying fat margins and that makes more than enough incentive for Apple IMHO.</div><br/><div id="37607532" class="c"><input type="checkbox" id="c-37607532" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#37607114">root</a><span>|</span><a href="#37607441">parent</a><span>|</span><a href="#37606983">next</a><span>|</span><label class="collapse" for="c-37607532">[-]</label><label class="expand" for="c-37607532">[1 more]</label></div><br/><div class="children"><div class="content">But you wouldn&#x27;t be able to upgrade an Apple Mac when the next generation of LLMs is out.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37606983" class="c"><input type="checkbox" id="c-37606983" checked=""/><div class="controls bullet"><span class="by">ziofill</span><span>|</span><a href="#37607114">prev</a><span>|</span><label class="collapse" for="c-37606983">[-]</label><label class="expand" for="c-37606983">[11 more]</label></div><br/><div class="children"><div class="content">Is it that a lot of capacity is unused in those behemoth LLMs, or that the smaller language model just mimics the reasoning task? (Mimics the mimicking?)</div><br/><div id="37607055" class="c"><input type="checkbox" id="c-37607055" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37606983">parent</a><span>|</span><a href="#37609001">next</a><span>|</span><label class="collapse" for="c-37607055">[-]</label><label class="expand" for="c-37607055">[9 more]</label></div><br/><div class="children"><div class="content">There is no actual distinction between the &quot;real&quot; thing and &quot;mimicking&quot;.<p>The datasets behemoth LLMs are trained on include a lot of noise that derail progress. They also just contain a lot of irrelevant knowledge that the LLM has to learn or memorize so an obscene amount of parameters is required.<p>When you&#x27;re not trying to teach a language model the sum total of human knowledge and you provide a high quality curated dataset, the scale barrier is much lower.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.07759" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.07759</a></div><br/><div id="37607369" class="c"><input type="checkbox" id="c-37607369" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#37606983">root</a><span>|</span><a href="#37607055">parent</a><span>|</span><a href="#37609001">next</a><span>|</span><label class="collapse" for="c-37607369">[-]</label><label class="expand" for="c-37607369">[8 more]</label></div><br/><div class="children"><div class="content">Why don&#x27;t people just train LLMs on pure Wikipedia, arXiv, and other scientific websites? That would reduce the noise and improve hallucinations, no?</div><br/><div id="37607562" class="c"><input type="checkbox" id="c-37607562" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37606983">root</a><span>|</span><a href="#37607369">parent</a><span>|</span><a href="#37608908">next</a><span>|</span><label class="collapse" for="c-37607562">[-]</label><label class="expand" for="c-37607562">[1 more]</label></div><br/><div class="children"><div class="content">That is essentially Phi. The results are promising.<p>But LLMs do gain useful &quot;emergent&quot; properties from training on massive lower quality datasets.</div><br/></div></div><div id="37608908" class="c"><input type="checkbox" id="c-37608908" checked=""/><div class="controls bullet"><span class="by">msp26</span><span>|</span><a href="#37606983">root</a><span>|</span><a href="#37607369">parent</a><span>|</span><a href="#37607562">prev</a><span>|</span><a href="#37608078">next</a><span>|</span><label class="collapse" for="c-37608908">[-]</label><label class="expand" for="c-37608908">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been done before. &quot;Textbooks are all you need&quot;.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.11644" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.11644</a></div><br/></div></div><div id="37608078" class="c"><input type="checkbox" id="c-37608078" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37606983">root</a><span>|</span><a href="#37607369">parent</a><span>|</span><a href="#37608908">prev</a><span>|</span><a href="#37607415">next</a><span>|</span><label class="collapse" for="c-37608078">[-]</label><label class="expand" for="c-37608078">[2 more]</label></div><br/><div class="children"><div class="content">It has to learn the meaning of words, including implicit associations, and to do that it needs to see approximately all the English text ever. We don&#x27;t know how to balance this with only feeding it useful knowledge.</div><br/><div id="37608170" class="c"><input type="checkbox" id="c-37608170" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#37606983">root</a><span>|</span><a href="#37608078">parent</a><span>|</span><a href="#37607415">next</a><span>|</span><label class="collapse" for="c-37608170">[-]</label><label class="expand" for="c-37608170">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t necessarily have to see approximately all the English text ever.  Real people don&#x27;t learn English like that, for example.<p>It&#x27;s just that given what we know about neural networks, it&#x27;s often easier and simpler and more effective to increase the amount of training data than to change anything else.</div><br/></div></div></div></div><div id="37607415" class="c"><input type="checkbox" id="c-37607415" checked=""/><div class="controls bullet"><span class="by">vczf</span><span>|</span><a href="#37606983">root</a><span>|</span><a href="#37607369">parent</a><span>|</span><a href="#37608078">prev</a><span>|</span><a href="#37607568">next</a><span>|</span><label class="collapse" for="c-37607415">[-]</label><label class="expand" for="c-37607415">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it would know how to have a conversation if it&#x27;s never been exposed to one before.</div><br/><div id="37608135" class="c"><input type="checkbox" id="c-37608135" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#37606983">root</a><span>|</span><a href="#37607415">parent</a><span>|</span><a href="#37607568">next</a><span>|</span><label class="collapse" for="c-37608135">[-]</label><label class="expand" for="c-37608135">[1 more]</label></div><br/><div class="children"><div class="content">That’s what the fine tuning is about. It learns the language, concepts etc. from the main dataset and is then tweaked by continuing to train on a smaller, high quality, hand curated dataset. That’s how it learns to generate conversational responses by default instead of needing a complicated prompt.</div><br/></div></div></div></div></div></div></div></div><div id="37609001" class="c"><input type="checkbox" id="c-37609001" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#37606983">parent</a><span>|</span><a href="#37607055">prev</a><span>|</span><label class="collapse" for="c-37609001">[-]</label><label class="expand" for="c-37609001">[1 more]</label></div><br/><div class="children"><div class="content">That question seems synonymous with &quot;are those [current] behemoth LLMs close to optimal?&quot; and it seems obvious that they are not.<p>I wonder what ideas there are about how one could estimate the optimal size.</div><br/></div></div></div></div></div></div></div></div></div></body></html>