<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1731834066719" as="style"/><link rel="stylesheet" href="styles.css?v=1731834066719"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2310.03684">SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>amai</span> | <span>12 comments</span></div><br/><div><div id="42161812" class="c"><input type="checkbox" id="c-42161812" checked=""/><div class="controls bullet"><span class="by">freeone3000</span><span>|</span><a href="#42161322">next</a><span>|</span><label class="collapse" for="c-42161812">[-]</label><label class="expand" for="c-42161812">[6 more]</label></div><br/><div class="children"><div class="content">I find it very interesting that “aligning with human desires” somehow includes prevention of a human trying to bypass the safeguards to generate “objectionable” content (whatever that is). I think the “safeguards” are a bigger problem with aligning with my desires.</div><br/><div id="42162181" class="c"><input type="checkbox" id="c-42162181" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#42161812">parent</a><span>|</span><a href="#42162295">next</a><span>|</span><label class="collapse" for="c-42162181">[-]</label><label class="expand" for="c-42162181">[1 more]</label></div><br/><div class="children"><div class="content">Another question is whether that initial unalignment comes from poor filtering of datasets, or is it emergent from regular, pre-filtered cultured texts.<p>In other words, was an “unaligned” LLM taught bad things from bad people, or does it simply <i>see it naturally</i> and point it out with the purity of a child? The latter would mean something about ourselves. Personally I think that people tend to selectively ignore things too much.</div><br/></div></div><div id="42162295" class="c"><input type="checkbox" id="c-42162295" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#42161812">parent</a><span>|</span><a href="#42162181">prev</a><span>|</span><a href="#42162664">next</a><span>|</span><label class="collapse" for="c-42162295">[-]</label><label class="expand" for="c-42162295">[2 more]</label></div><br/><div class="children"><div class="content">The safeguards stems from a desire to make tools like Claude accessible to a very wide audience as use cases such as education are very important.<p>And so it seems like people such as yourself who do have an issue with safeguards should seek out LLMs that are catered to adult audiences rather than trying to remove safeguards entirely.</div><br/><div id="42162675" class="c"><input type="checkbox" id="c-42162675" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#42161812">root</a><span>|</span><a href="#42162295">parent</a><span>|</span><a href="#42162664">next</a><span>|</span><label class="collapse" for="c-42162675">[-]</label><label class="expand" for="c-42162675">[1 more]</label></div><br/><div class="children"><div class="content">How does making it harder for the user to extract information they are trying to extract make it safer for a wider audience?</div><br/></div></div></div></div><div id="42162664" class="c"><input type="checkbox" id="c-42162664" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#42161812">parent</a><span>|</span><a href="#42162295">prev</a><span>|</span><a href="#42162124">next</a><span>|</span><label class="collapse" for="c-42162664">[-]</label><label class="expand" for="c-42162664">[1 more]</label></div><br/><div class="children"><div class="content">What tools do we have to defend against LLM lockdown attacks?</div><br/></div></div><div id="42162124" class="c"><input type="checkbox" id="c-42162124" checked=""/><div class="controls bullet"><span class="by">ipython</span><span>|</span><a href="#42161812">parent</a><span>|</span><a href="#42162664">prev</a><span>|</span><a href="#42161322">next</a><span>|</span><label class="collapse" for="c-42162124">[-]</label><label class="expand" for="c-42162124">[1 more]</label></div><br/><div class="children"><div class="content">We’ve seen where that ends up. <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Tay_(chatbot)" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Tay_(chatbot)</a></div><br/></div></div></div></div><div id="42161322" class="c"><input type="checkbox" id="c-42161322" checked=""/><div class="controls bullet"><span class="by">ipython</span><span>|</span><a href="#42161812">prev</a><span>|</span><a href="#42161544">next</a><span>|</span><label class="collapse" for="c-42161322">[-]</label><label class="expand" for="c-42161322">[1 more]</label></div><br/><div class="children"><div class="content">It concerns me that these defensive techniques themselves often require even more llm inference calls.<p>Just skimmed the GitHub repo for this one and the read me mentions four additional llm inferences for each incoming request - so now we’ve 5x’ed the (already expensive) compute required to answer a query?</div><br/></div></div><div id="42161544" class="c"><input type="checkbox" id="c-42161544" checked=""/><div class="controls bullet"><span class="by">padolsey</span><span>|</span><a href="#42161322">prev</a><span>|</span><a href="#42161547">next</a><span>|</span><label class="collapse" for="c-42161544">[-]</label><label class="expand" for="c-42161544">[2 more]</label></div><br/><div class="children"><div class="content">So basically this just adds random characters to input prompts to break jailbreaking attempts? IMHO If you can&#x27;t make a single-inference solution, you may as well just run a couple of output filters, no? That appeared to have reasonable results, and if you make such filtering more domain-specific, you&#x27;ll probably make it even better. Intuition says there&#x27;s no &quot;general solution&quot; to jailbreaking, so maybe it&#x27;s a lost cause and we need to build up layers of obscurity, of which smooth-llm is just one part.</div><br/><div id="42161677" class="c"><input type="checkbox" id="c-42161677" checked=""/><div class="controls bullet"><span class="by">ipython</span><span>|</span><a href="#42161544">parent</a><span>|</span><a href="#42161547">next</a><span>|</span><label class="collapse" for="c-42161677">[-]</label><label class="expand" for="c-42161677">[1 more]</label></div><br/><div class="children"><div class="content">Right. This seems to be the latest in the “throw random stuff at the wall and see what sticks” series of generative ai papers.<p>I don’t know if I’m too stupid to understand or if truly this is just “add random stuff to prompt” dressed up in flowery academic language.</div><br/></div></div></div></div><div id="42161547" class="c"><input type="checkbox" id="c-42161547" checked=""/><div class="controls bullet"><span class="by">mapmeld</span><span>|</span><a href="#42161544">prev</a><span>|</span><a href="#42160989">next</a><span>|</span><label class="collapse" for="c-42161547">[-]</label><label class="expand" for="c-42161547">[1 more]</label></div><br/><div class="children"><div class="content">There are some authors in common with a more recent paper &quot;Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing&quot; <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.16192" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.16192</a></div><br/></div></div><div id="42160989" class="c"><input type="checkbox" id="c-42160989" checked=""/><div class="controls bullet"><span class="by">handfuloflight</span><span>|</span><a href="#42161547">prev</a><span>|</span><label class="collapse" for="c-42160989">[-]</label><label class="expand" for="c-42160989">[1 more]</label></div><br/><div class="children"><div class="content">Github: <a href="https:&#x2F;&#x2F;github.com&#x2F;arobey1&#x2F;smooth-llm">https:&#x2F;&#x2F;github.com&#x2F;arobey1&#x2F;smooth-llm</a></div><br/></div></div></div></div></div></div></div></body></html>