<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1710925267191" as="style"/><link rel="stylesheet" href="styles.css?v=1710925267191"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://news.mit.edu/2024/featup-algorithm-unlocks-high-resolution-insights-computer-vision-0318">New algorithm unlocks high-resolution insights for computer vision</a>Â <span class="domain">(<a href="https://news.mit.edu">news.mit.edu</a>)</span></div><div class="subtext"><span>zerojames</span> | <span>18 comments</span></div><br/><div><div id="39760466" class="c"><input type="checkbox" id="c-39760466" checked=""/><div class="controls bullet"><span class="by">TOMDM</span><span>|</span><a href="#39760917">next</a><span>|</span><label class="collapse" for="c-39760466">[-]</label><label class="expand" for="c-39760466">[1 more]</label></div><br/><div class="children"><div class="content">The papers actual page feels like a clearer explanation to me.<p><a href="https:&#x2F;&#x2F;mhamilton.net&#x2F;featup.html" rel="nofollow">https:&#x2F;&#x2F;mhamilton.net&#x2F;featup.html</a></div><br/></div></div><div id="39760917" class="c"><input type="checkbox" id="c-39760917" checked=""/><div class="controls bullet"><span class="by">fxtentacle</span><span>|</span><a href="#39760466">prev</a><span>|</span><a href="#39761525">next</a><span>|</span><label class="collapse" for="c-39760917">[-]</label><label class="expand" for="c-39760917">[7 more]</label></div><br/><div class="children"><div class="content">What an amazing idea :)<p>They reproject the input images and run the low-res network multiple times. Then they use an approach similar to NeRF to merge the knowledge from those reprojected images into a super-resolution result.<p>So in a way, this is quite similar to how modern Pixel phones can take a burst of frames and merge them into a final image that has a higher resolution than the sensor. Except that they run useful AI processing in between and then do the super-resolution merge on the results.</div><br/><div id="39764043" class="c"><input type="checkbox" id="c-39764043" checked=""/><div class="controls bullet"><span class="by">Grieverheart</span><span>|</span><a href="#39760917">parent</a><span>|</span><a href="#39761350">next</a><span>|</span><label class="collapse" for="c-39764043">[-]</label><label class="expand" for="c-39764043">[1 more]</label></div><br/><div class="children"><div class="content">Also similar to temporal antialiasing <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Temporal_anti-aliasing" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Temporal_anti-aliasing</a> .</div><br/></div></div><div id="39761350" class="c"><input type="checkbox" id="c-39761350" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#39760917">parent</a><span>|</span><a href="#39764043">prev</a><span>|</span><a href="#39763624">next</a><span>|</span><label class="collapse" for="c-39761350">[-]</label><label class="expand" for="c-39761350">[4 more]</label></div><br/><div class="children"><div class="content">Very interesting, I am curious how do people reach that train of thought to a successful idea. So many great algorithms based on small twists.</div><br/><div id="39761518" class="c"><input type="checkbox" id="c-39761518" checked=""/><div class="controls bullet"><span class="by">kreelman</span><span>|</span><a href="#39760917">root</a><span>|</span><a href="#39761350">parent</a><span>|</span><a href="#39762504">next</a><span>|</span><label class="collapse" for="c-39761518">[-]</label><label class="expand" for="c-39761518">[2 more]</label></div><br/><div class="children"><div class="content">It is interesting indeed.
One wonders if the researchers of this particular bit of work made it mandatory to go for walks at lunch and think about how their vision chunked&#x2F;filtered the information it was receiving.
Interesting that they &quot;perturb&quot; the image to get some noise involved. I&#x27;ll need to read it over again.</div><br/><div id="39762358" class="c"><input type="checkbox" id="c-39762358" checked=""/><div class="controls bullet"><span class="by">davecanderson</span><span>|</span><a href="#39760917">root</a><span>|</span><a href="#39761518">parent</a><span>|</span><a href="#39762504">next</a><span>|</span><label class="collapse" for="c-39762358">[-]</label><label class="expand" for="c-39762358">[1 more]</label></div><br/><div class="children"><div class="content">Nature is such a good source of inspiration, the &quot;perturb&quot; approach reminded me of [fixational eye movement][1] but maybe that&#x27;s only a clear link in retrospect.<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fixation_(visual)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fixation_(visual)</a></div><br/></div></div></div></div><div id="39762504" class="c"><input type="checkbox" id="c-39762504" checked=""/><div class="controls bullet"><span class="by">cryptonector</span><span>|</span><a href="#39760917">root</a><span>|</span><a href="#39761350">parent</a><span>|</span><a href="#39761518">prev</a><span>|</span><a href="#39763624">next</a><span>|</span><label class="collapse" for="c-39762504">[-]</label><label class="expand" for="c-39762504">[1 more]</label></div><br/><div class="children"><div class="content">This seems like it could have been inspired by how human vision works.</div><br/></div></div></div></div></div></div><div id="39761525" class="c"><input type="checkbox" id="c-39761525" checked=""/><div class="controls bullet"><span class="by">radq</span><span>|</span><a href="#39760917">prev</a><span>|</span><a href="#39761018">next</a><span>|</span><label class="collapse" for="c-39761525">[-]</label><label class="expand" for="c-39761525">[1 more]</label></div><br/><div class="children"><div class="content">The training technique used here (fitting something similar to a NeRF to different views of the same image) is pretty similar to this paper which uses a similar technique to denoise (instead of upscale) output features: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.02957" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.02957</a></div><br/></div></div><div id="39761018" class="c"><input type="checkbox" id="c-39761018" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#39761525">prev</a><span>|</span><a href="#39761473">next</a><span>|</span><label class="collapse" for="c-39761018">[-]</label><label class="expand" for="c-39761018">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not that clear why they are downsampling and then upsampling again. Why not do all the work at the original resolution?<p>Apparently, the issue is that some vision algorithms only output a low-res representation and <i>that</i> needs to be upsampled to match the original?</div><br/><div id="39761085" class="c"><input type="checkbox" id="c-39761085" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#39761018">parent</a><span>|</span><a href="#39761353">next</a><span>|</span><label class="collapse" for="c-39761085">[-]</label><label class="expand" for="c-39761085">[1 more]</label></div><br/><div class="children"><div class="content">&gt;It&#x27;s not that clear why they are downsampling and then upsampling again. Why not do all the work at the original resolution?<p>For NNs, This is pretty much a compute efficiency thing. Working on the original resolution directly is more compute intensive.</div><br/></div></div><div id="39761353" class="c"><input type="checkbox" id="c-39761353" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39761018">parent</a><span>|</span><a href="#39761085">prev</a><span>|</span><a href="#39761473">next</a><span>|</span><label class="collapse" for="c-39761353">[-]</label><label class="expand" for="c-39761353">[1 more]</label></div><br/><div class="children"><div class="content">Correct, s&#x2F;some&#x2F;vast majority of. Ex. major video conference software ML blur algos run at like 100x100 - the weird edge is much more about resolution of input&#x2F;output than ML.</div><br/></div></div></div></div><div id="39761473" class="c"><input type="checkbox" id="c-39761473" checked=""/><div class="controls bullet"><span class="by">kreelman</span><span>|</span><a href="#39761018">prev</a><span>|</span><a href="#39762617">next</a><span>|</span><label class="collapse" for="c-39761473">[-]</label><label class="expand" for="c-39761473">[1 more]</label></div><br/><div class="children"><div class="content">This looks like it could be useful.
Remote sensing uses feature extraction tools. Being able to upsample again would make the data a lot easier to view and interpret.
Nice work.</div><br/></div></div><div id="39762617" class="c"><input type="checkbox" id="c-39762617" checked=""/><div class="controls bullet"><span class="by">albert_e</span><span>|</span><a href="#39761473">prev</a><span>|</span><a href="#39760882">next</a><span>|</span><label class="collapse" for="c-39762617">[-]</label><label class="expand" for="c-39762617">[1 more]</label></div><br/><div class="children"><div class="content">What can this do for Satellite imagery?</div><br/></div></div><div id="39760882" class="c"><input type="checkbox" id="c-39760882" checked=""/><div class="controls bullet"><span class="by">frozenport</span><span>|</span><a href="#39762617">prev</a><span>|</span><label class="collapse" for="c-39760882">[-]</label><label class="expand" for="c-39760882">[3 more]</label></div><br/><div class="children"><div class="content">Is a learned downsampler a form of inverse crime? <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;math-ph&#x2F;0401050" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;math-ph&#x2F;0401050</a></div><br/><div id="39761587" class="c"><input type="checkbox" id="c-39761587" checked=""/><div class="controls bullet"><span class="by">ta8645</span><span>|</span><a href="#39760882">parent</a><span>|</span><label class="collapse" for="c-39761587">[-]</label><label class="expand" for="c-39761587">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t think that&#x27;s applicable in this case.  This &quot;FeatUp&quot; technique does not feed its output back into the model in any way.<p>Rather, it&#x27;s just producing a higher resolution output by taking multiple passes of the input image (subtly shifting the input image before each pass) producing a slightly different low-resolution feature map.<p>Each of these low-resolution feature maps represent contributions from differing areas of the input image.  &quot;FeatUp&quot; can then create a higher-resolution feature map, &quot;simply&quot; by taking the color from the pass with the most appropriate input shift.<p>A very rough sketch:<p><pre><code>     Input Image:  abcdefgh
</code></pre>
Create multiple low resolution feature maps using your model,
shifting the input image, a few pixels each pass:<p><pre><code>     Pass 1:  abcdefgh   --&gt; ACEG    
     Pass 2:  bcdefgh    --&gt; BDFH
</code></pre>
Now take all the low resolution feature passes
and combine into a single higher resolution version:<p><pre><code>     FeatUp:  ACEG,BDFH --&gt;  ABCDEFGH</code></pre></div><br/><div id="39762029" class="c"><input type="checkbox" id="c-39762029" checked=""/><div class="controls bullet"><span class="by">pksebben</span><span>|</span><a href="#39760882">root</a><span>|</span><a href="#39761587">parent</a><span>|</span><label class="collapse" for="c-39762029">[-]</label><label class="expand" for="c-39762029">[1 more]</label></div><br/><div class="children"><div class="content">I wonder what you&#x27;d get if you did something similar on the latent space in a diffusion model, before decoding to an image.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>