<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1692954061573" as="style"/><link rel="stylesheet" href="styles.css?v=1692954061573"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/">Code Llama, a state-of-the-art large language model for coding</a> <span class="domain">(<a href="https://ai.meta.com">ai.meta.com</a>)</span></div><div class="subtext"><span>marcopicentini</span> | <span>269 comments</span></div><br/><div><div id="37251995" class="c"><input type="checkbox" id="c-37251995" checked=""/><div class="controls bullet"><span class="by">daemonologist</span><span>|</span><a href="#37249294">next</a><span>|</span><label class="collapse" for="c-37251995">[-]</label><label class="expand" for="c-37251995">[57 more]</label></div><br/><div class="children"><div class="content">Works nearly out of the box with llama.cpp, which makes it easy to try locally: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;2766">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;2766</a><p>Here&#x27;s some output from q4_0 quantization of CodeLlama-7b-Python (first four lines are the prompt):<p><pre><code>    # prints the first ten prime numbers 
    def print_primes(): 
        i = 2 
        num_printed = 0 # end of prompt
        while num_printed &lt; 10:
            if is_prime(i):
                print(i)
                num_printed += 1
            i += 1

    def is_prime(n):
        i = 2
        while i * i &lt;= n:
            if n % i == 0:
                return False
            i += 1
        return True

    def main():
        print_primes()

    if __name__ == &#x27;__main__&#x27;:
        main()

</code></pre>
It will be interesting to see how the larger models perform, especially after community tuning and with better context&#x2F;prompting.</div><br/><div id="37256822" class="c"><input type="checkbox" id="c-37256822" checked=""/><div class="controls bullet"><span class="by">kilolima</span><span>|</span><a href="#37251995">parent</a><span>|</span><a href="#37254548">next</a><span>|</span><label class="collapse" for="c-37256822">[-]</label><label class="expand" for="c-37256822">[5 more]</label></div><br/><div class="children"><div class="content">For printing the first 10 prime numbers, there&#x27;s a one line solution to this problem:<p>print(&quot;1, 2, 3, 5, 7, 11... and so on!</div><br/><div id="37257316" class="c"><input type="checkbox" id="c-37257316" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37256822">parent</a><span>|</span><a href="#37258686">next</a><span>|</span><label class="collapse" for="c-37257316">[-]</label><label class="expand" for="c-37257316">[1 more]</label></div><br/><div class="children"><div class="content">That code shows me a parse error.</div><br/></div></div><div id="37258686" class="c"><input type="checkbox" id="c-37258686" checked=""/><div class="controls bullet"><span class="by">tuukkah</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37256822">parent</a><span>|</span><a href="#37257316">prev</a><span>|</span><a href="#37254548">next</a><span>|</span><label class="collapse" for="c-37258686">[-]</label><label class="expand" for="c-37258686">[3 more]</label></div><br/><div class="children"><div class="content">That can&#x27;t be, because primes are numbers greater than 1.</div><br/><div id="37259370" class="c"><input type="checkbox" id="c-37259370" checked=""/><div class="controls bullet"><span class="by">timjver</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37258686">parent</a><span>|</span><a href="#37254548">next</a><span>|</span><label class="collapse" for="c-37259370">[-]</label><label class="expand" for="c-37259370">[2 more]</label></div><br/><div class="children"><div class="content">They said nothing about not printing any non-primes.</div><br/><div id="37259491" class="c"><input type="checkbox" id="c-37259491" checked=""/><div class="controls bullet"><span class="by">tuukkah</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37259370">parent</a><span>|</span><a href="#37254548">next</a><span>|</span><label class="collapse" for="c-37259491">[-]</label><label class="expand" for="c-37259491">[1 more]</label></div><br/><div class="children"><div class="content">I suppose it&#x27;s indeed possible to read the assignment as &quot;print whatever you like as long as it includes 10 or more primes&quot; but I&#x27;d fail you for that based on not being able to communicate with humans.</div><br/></div></div></div></div></div></div></div></div><div id="37254548" class="c"><input type="checkbox" id="c-37254548" checked=""/><div class="controls bullet"><span class="by">d0mine</span><span>|</span><a href="#37251995">parent</a><span>|</span><a href="#37256822">prev</a><span>|</span><a href="#37259445">next</a><span>|</span><label class="collapse" for="c-37254548">[-]</label><label class="expand" for="c-37254548">[7 more]</label></div><br/><div class="children"><div class="content">Simple, concise, more efficient:<p><pre><code>  def primes_upto(limit: int):
    &quot;&quot;&quot;Generate prime numbers &lt; *limit*.&quot;&quot;&quot; 
    # Sieve of Eratosthene
    is_prime = [True] * limit 
    for n in range(2, limit):
        if is_prime[n]:
            yield n  # found prime number
            for c in range(n*n, limit, n):  # start with square, less values are marked already
                is_prime[c] = False # mark composites

  if __name__ == &quot;__main__&quot;:                
    from itertools import islice
    
    print(*islice(primes_upto(100), 10)) # -&gt; 2 3 5 7 11 13 17 19 23 29</code></pre></div><br/><div id="37254888" class="c"><input type="checkbox" id="c-37254888" checked=""/><div class="controls bullet"><span class="by">someplaceguy</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254548">parent</a><span>|</span><a href="#37259445">next</a><span>|</span><label class="collapse" for="c-37254888">[-]</label><label class="expand" for="c-37254888">[6 more]</label></div><br/><div class="children"><div class="content">Yeah, but yours was generated by the &quot;post unoptimized code to HN and wait for someone to optimize it&quot; model, which, although free and doesn&#x27;t require a GPU, is a much slower model.</div><br/><div id="37254988" class="c"><input type="checkbox" id="c-37254988" checked=""/><div class="controls bullet"><span class="by">saurik</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254888">parent</a><span>|</span><a href="#37255497">next</a><span>|</span><label class="collapse" for="c-37254988">[-]</label><label class="expand" for="c-37254988">[2 more]</label></div><br/><div class="children"><div class="content">But, unless you are trying to find a prime number low enough that you might as well look it up in a pre-generated table, it might still be end-to-end more efficient?</div><br/><div id="37255019" class="c"><input type="checkbox" id="c-37255019" checked=""/><div class="controls bullet"><span class="by">someplaceguy</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254988">parent</a><span>|</span><a href="#37255497">next</a><span>|</span><label class="collapse" for="c-37255019">[-]</label><label class="expand" for="c-37255019">[1 more]</label></div><br/><div class="children"><div class="content">Ah, good point :) Touché!</div><br/></div></div></div></div><div id="37255497" class="c"><input type="checkbox" id="c-37255497" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254888">parent</a><span>|</span><a href="#37254988">prev</a><span>|</span><a href="#37259445">next</a><span>|</span><label class="collapse" for="c-37255497">[-]</label><label class="expand" for="c-37255497">[3 more]</label></div><br/><div class="children"><div class="content">Someone should turn this into a product! You highlight the code you want to optimize, and it posts it to hn as a semi-contextually-appropriate comment to invite code golfing, and the highest rated reply gets posted back to your repo as  a PR.</div><br/><div id="37259221" class="c"><input type="checkbox" id="c-37259221" checked=""/><div class="controls bullet"><span class="by">easygenes</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37255497">parent</a><span>|</span><a href="#37256891">next</a><span>|</span><label class="collapse" for="c-37259221">[-]</label><label class="expand" for="c-37259221">[1 more]</label></div><br/><div class="children"><div class="content">What are some existing data source that are somewhat analogous to this? e.g., Project Euler.</div><br/></div></div><div id="37256891" class="c"><input type="checkbox" id="c-37256891" checked=""/><div class="controls bullet"><span class="by">badloginagain</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37255497">parent</a><span>|</span><a href="#37259221">prev</a><span>|</span><a href="#37259445">next</a><span>|</span><label class="collapse" for="c-37256891">[-]</label><label class="expand" for="c-37256891">[1 more]</label></div><br/><div class="children"><div class="content">We then train the LLM on all code optimized like this.</div><br/></div></div></div></div></div></div></div></div><div id="37259445" class="c"><input type="checkbox" id="c-37259445" checked=""/><div class="controls bullet"><span class="by">FrozenSynapse</span><span>|</span><a href="#37251995">parent</a><span>|</span><a href="#37254548">prev</a><span>|</span><a href="#37255208">next</a><span>|</span><label class="collapse" for="c-37259445">[-]</label><label class="expand" for="c-37259445">[1 more]</label></div><br/><div class="children"><div class="content">how much VRAM do you need to run quantised 7b model?</div><br/></div></div><div id="37255208" class="c"><input type="checkbox" id="c-37255208" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37251995">parent</a><span>|</span><a href="#37259445">prev</a><span>|</span><a href="#37252749">next</a><span>|</span><label class="collapse" for="c-37255208">[-]</label><label class="expand" for="c-37255208">[1 more]</label></div><br/><div class="children"><div class="content">Funny watching HN be nerd sniped by a machine :-)</div><br/></div></div><div id="37252749" class="c"><input type="checkbox" id="c-37252749" checked=""/><div class="controls bullet"><span class="by">blibble</span><span>|</span><a href="#37251995">parent</a><span>|</span><a href="#37255208">prev</a><span>|</span><a href="#37249294">next</a><span>|</span><label class="collapse" for="c-37252749">[-]</label><label class="expand" for="c-37252749">[42 more]</label></div><br/><div class="children"><div class="content">I&#x27;d fail an interview candidate that suggested adding 1 each time for subsequent prime testing</div><br/><div id="37253997" class="c"><input type="checkbox" id="c-37253997" checked=""/><div class="controls bullet"><span class="by">csmpltn</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37252749">parent</a><span>|</span><a href="#37254138">next</a><span>|</span><label class="collapse" for="c-37253997">[-]</label><label class="expand" for="c-37253997">[7 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;I&#x27;d fail an interview candidate that suggested adding 1 each time for subsequent prime testing&quot;<p>Congratulations! You must be that arrogant guy everybody hates interviewing with, the one with the superiority complex.<p>How about instead of just failing people over literally nothing (wasting everybody&#x27;s time and money) - just ask the candidate whether they could somehow reduce the search space by utilizing the properties of a prime number?</div><br/><div id="37256337" class="c"><input type="checkbox" id="c-37256337" checked=""/><div class="controls bullet"><span class="by">hyperbovine</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37253997">parent</a><span>|</span><a href="#37258583">next</a><span>|</span><label class="collapse" for="c-37256337">[-]</label><label class="expand" for="c-37256337">[5 more]</label></div><br/><div class="children"><div class="content">How many even numbers are prime?</div><br/><div id="37256650" class="c"><input type="checkbox" id="c-37256650" checked=""/><div class="controls bullet"><span class="by">wavemode</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37256337">parent</a><span>|</span><a href="#37258583">next</a><span>|</span><label class="collapse" for="c-37256650">[-]</label><label class="expand" for="c-37256650">[4 more]</label></div><br/><div class="children"><div class="content">One.</div><br/><div id="37257958" class="c"><input type="checkbox" id="c-37257958" checked=""/><div class="controls bullet"><span class="by">awwaiid</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37256650">parent</a><span>|</span><a href="#37258583">next</a><span>|</span><label class="collapse" for="c-37257958">[-]</label><label class="expand" for="c-37257958">[3 more]</label></div><br/><div class="children"><div class="content">Two when you include -2, which I certainly think we should in this circumstance.</div><br/><div id="37258243" class="c"><input type="checkbox" id="c-37258243" checked=""/><div class="controls bullet"><span class="by">chaboud</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37257958">parent</a><span>|</span><a href="#37258519">next</a><span>|</span><label class="collapse" for="c-37258243">[-]</label><label class="expand" for="c-37258243">[1 more]</label></div><br/><div class="children"><div class="content">Are we including it just to poke the bear?  Prime numbers are typically defined as numbers with no positive divisors other than one and the number in question.</div><br/></div></div><div id="37258519" class="c"><input type="checkbox" id="c-37258519" checked=""/><div class="controls bullet"><span class="by">squeaky-clean</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37257958">parent</a><span>|</span><a href="#37258243">prev</a><span>|</span><a href="#37258583">next</a><span>|</span><label class="collapse" for="c-37258519">[-]</label><label class="expand" for="c-37258519">[1 more]</label></div><br/><div class="children"><div class="content">Negatives are not prime.</div><br/></div></div></div></div></div></div></div></div><div id="37258583" class="c"><input type="checkbox" id="c-37258583" checked=""/><div class="controls bullet"><span class="by">lynx23</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37253997">parent</a><span>|</span><a href="#37256337">prev</a><span>|</span><a href="#37254138">next</a><span>|</span><label class="collapse" for="c-37258583">[-]</label><label class="expand" for="c-37258583">[1 more]</label></div><br/><div class="children"><div class="content">Downvoting this is not enough.  I&#x27;d pay $5 (reddit style) to have this properly killfiled if HN allowed for that.  Besides, not &quot;everybody hates them, only those inmature enough to still need intellectual babysitting.</div><br/></div></div></div></div><div id="37254138" class="c"><input type="checkbox" id="c-37254138" checked=""/><div class="controls bullet"><span class="by">tasubotadas</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37252749">parent</a><span>|</span><a href="#37253997">prev</a><span>|</span><a href="#37256615">next</a><span>|</span><label class="collapse" for="c-37254138">[-]</label><label class="expand" for="c-37254138">[2 more]</label></div><br/><div class="children"><div class="content">Finally we meet the lifeless drone that everybody complains about in the interviews.<p>My suggestion for your next interview: decide to hire them just based on their leetcode score, but invite to the interview just to flex that you&#x27;re still better at puzzle solving :-D<p>Perfect</div><br/><div id="37258886" class="c"><input type="checkbox" id="c-37258886" checked=""/><div class="controls bullet"><span class="by">internet101010</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254138">parent</a><span>|</span><a href="#37256615">next</a><span>|</span><label class="collapse" for="c-37258886">[-]</label><label class="expand" for="c-37258886">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Use &lt;insert language&gt; and optimize for runtime.&quot;<p>&lt;pastes question and constraints from leetcode&gt;<p>&quot;solution begins with &lt;insert default solution template from leetcode&gt;&quot;.<p>Copy solution from gpt, paste in leetcode, run, submit.<p>&quot;faster&quot;<p>Repeat.<p>Repeat.<p>Next question.</div><br/></div></div></div></div><div id="37256615" class="c"><input type="checkbox" id="c-37256615" checked=""/><div class="controls bullet"><span class="by">jckahn</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37252749">parent</a><span>|</span><a href="#37254138">prev</a><span>|</span><a href="#37254159">next</a><span>|</span><label class="collapse" for="c-37256615">[-]</label><label class="expand" for="c-37256615">[1 more]</label></div><br/><div class="children"><div class="content">So I take it you typically produce fully optimized, thoughtful, and correct code on the first iteration while being actively judged by a stranger, yes?</div><br/></div></div><div id="37254159" class="c"><input type="checkbox" id="c-37254159" checked=""/><div class="controls bullet"><span class="by">maleldil</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37252749">parent</a><span>|</span><a href="#37256615">prev</a><span>|</span><a href="#37253953">next</a><span>|</span><label class="collapse" for="c-37254159">[-]</label><label class="expand" for="c-37254159">[4 more]</label></div><br/><div class="children"><div class="content">I assume you meant that you should add 2? If yes, that&#x27;s such a mind boggling basic thing to do that I agree with you, and it makes no sense that you&#x27;re being crucified.</div><br/><div id="37256318" class="c"><input type="checkbox" id="c-37256318" checked=""/><div class="controls bullet"><span class="by">warent</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254159">parent</a><span>|</span><a href="#37254388">next</a><span>|</span><label class="collapse" for="c-37256318">[-]</label><label class="expand" for="c-37256318">[2 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  it makes no sense that you&#x27;re being crucified.
</code></pre>
Probably because there&#x27;s significant overlap in the Venn diagram of people with years experience who professionally develop products that generate $millions in wealth&#x2F;value, and people who would fail that interview.<p>Or we have worked with junior developers who have really grown and flourished under our care, who would never have gotten that chance with such insane Draconian judgements.<p>It&#x27;s such an obvious &quot;GOTCHA!!&quot; setting someone up for failure.<p>The way it&#x27;s framed is very cringy because it signals that they don&#x27;t care in their interviews about determining how objectively effective a software developer is.</div><br/><div id="37259524" class="c"><input type="checkbox" id="c-37259524" checked=""/><div class="controls bullet"><span class="by">maleldil</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37256318">parent</a><span>|</span><a href="#37254388">next</a><span>|</span><label class="collapse" for="c-37259524">[-]</label><label class="expand" for="c-37259524">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t get it. This is an extremely basic fact that most people can figure out after thinking about primes for a minute. Maybe if you ask for &quot;what&#x27;s an easy optimisation here?&quot; This would make the candidate think more closely about invariants that their code should hold, which in itself is a very valuable skill.</div><br/></div></div></div></div><div id="37254388" class="c"><input type="checkbox" id="c-37254388" checked=""/><div class="controls bullet"><span class="by">blibble</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254159">parent</a><span>|</span><a href="#37256318">prev</a><span>|</span><a href="#37253953">next</a><span>|</span><label class="collapse" for="c-37254388">[-]</label><label class="expand" for="c-37254388">[1 more]</label></div><br/><div class="children"><div class="content">yes</div><br/></div></div></div></div><div id="37253953" class="c"><input type="checkbox" id="c-37253953" checked=""/><div class="controls bullet"><span class="by">throwuxiytayq</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37252749">parent</a><span>|</span><a href="#37254159">prev</a><span>|</span><a href="#37258537">next</a><span>|</span><label class="collapse" for="c-37253953">[-]</label><label class="expand" for="c-37253953">[3 more]</label></div><br/><div class="children"><div class="content">i’d walk out from an interview that asked me to write a prime number generator</div><br/><div id="37254706" class="c"><input type="checkbox" id="c-37254706" checked=""/><div class="controls bullet"><span class="by">belenos46</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37253953">parent</a><span>|</span><a href="#37258537">next</a><span>|</span><label class="collapse" for="c-37254706">[-]</label><label class="expand" for="c-37254706">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve done that (maybe it was fizzbuzz, now that I&#x27;m thinking about it) and boy howdy does that get the people you&#x27;re interviewing with agitated. Saying &quot;I&#x27;m interviewing for a architect level container orchestration position. If I&#x27;m reinventing the wheel writing algorithms, something is <i>terribly</i> wrong&quot; shuts them up, but doesn&#x27;t make them any happier.</div><br/><div id="37256425" class="c"><input type="checkbox" id="c-37256425" checked=""/><div class="controls bullet"><span class="by">zaphirplane</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254706">parent</a><span>|</span><a href="#37258537">next</a><span>|</span><label class="collapse" for="c-37256425">[-]</label><label class="expand" for="c-37256425">[1 more]</label></div><br/><div class="children"><div class="content">Is the job role just a for example ?<p>what does and container orchestration architect do ? Something like this cluster should use envoy and Prometheus. The new clusters rate isn’t usually high enough for the stack to change.<p>Real question I love these non conventional (swe, sre, pm, manager ) roles in tech</div><br/></div></div></div></div></div></div><div id="37258537" class="c"><input type="checkbox" id="c-37258537" checked=""/><div class="controls bullet"><span class="by">squeaky-clean</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37252749">parent</a><span>|</span><a href="#37253953">prev</a><span>|</span><a href="#37258744">next</a><span>|</span><label class="collapse" for="c-37258537">[-]</label><label class="expand" for="c-37258537">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;d reject a candidate that is willing and legally able to work for free while also cloning themselves so they can pair program with every one of your employees at once?</div><br/></div></div><div id="37254760" class="c"><input type="checkbox" id="c-37254760" checked=""/><div class="controls bullet"><span class="by">dontupvoteme</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37252749">parent</a><span>|</span><a href="#37254339">prev</a><span>|</span><a href="#37257715">next</a><span>|</span><label class="collapse" for="c-37254760">[-]</label><label class="expand" for="c-37254760">[1 more]</label></div><br/><div class="children"><div class="content">Simply prompting the output with &quot;Optimize &quot; prepended adds your suggestion, and some others.</div><br/></div></div><div id="37257715" class="c"><input type="checkbox" id="c-37257715" checked=""/><div class="controls bullet"><span class="by">beanjuiceII</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37252749">parent</a><span>|</span><a href="#37254760">prev</a><span>|</span><a href="#37253013">next</a><span>|</span><label class="collapse" for="c-37257715">[-]</label><label class="expand" for="c-37257715">[1 more]</label></div><br/><div class="children"><div class="content">Also don&#x27;t these only ever need to be computed once</div><br/></div></div><div id="37253013" class="c"><input type="checkbox" id="c-37253013" checked=""/><div class="controls bullet"><span class="by">droopyEyelids</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37252749">parent</a><span>|</span><a href="#37257715">prev</a><span>|</span><a href="#37254729">next</a><span>|</span><label class="collapse" for="c-37253013">[-]</label><label class="expand" for="c-37253013">[18 more]</label></div><br/><div class="children"><div class="content">The simple-to-understand, greedy algorithm is always the correct first choice till you have to deal with a constraint.</div><br/><div id="37253038" class="c"><input type="checkbox" id="c-37253038" checked=""/><div class="controls bullet"><span class="by">blibble</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37253013">parent</a><span>|</span><a href="#37254729">next</a><span>|</span><label class="collapse" for="c-37253038">[-]</label><label class="expand" for="c-37253038">[17 more]</label></div><br/><div class="children"><div class="content">it&#x27;s not that though, there&#x27;s several other typical optimisations in there<p>just not the super obvious one that demonstrates extremely basic understanding of what a prime number is</div><br/><div id="37253991" class="c"><input type="checkbox" id="c-37253991" checked=""/><div class="controls bullet"><span class="by">jpeterson</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37253038">parent</a><span>|</span><a href="#37254096">next</a><span>|</span><label class="collapse" for="c-37253991">[-]</label><label class="expand" for="c-37253991">[10 more]</label></div><br/><div class="children"><div class="content">Having &quot;extremely basic understanding&quot; of prime numbers immediately at one&#x27;s command is important for approximately 0% of software engineering jobs. If you instant-fail a candidate for this, it says a lot more about you and your organization than the candidate.</div><br/><div id="37255881" class="c"><input type="checkbox" id="c-37255881" checked=""/><div class="controls bullet"><span class="by">SideQuark</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37253991">parent</a><span>|</span><a href="#37254316">next</a><span>|</span><label class="collapse" for="c-37255881">[-]</label><label class="expand" for="c-37255881">[2 more]</label></div><br/><div class="children"><div class="content">Approx 0% of devs need to know what the earth is, but from lots of interviews I&#x27;ve given I&#x27;ve found consistent correlation between lack of basic knowledge and lack of ability to solve many things. It was so strong we found it much more cost effective to cut people early that didn&#x27;t know at least a few of some standard knowledge items.</div><br/><div id="37256906" class="c"><input type="checkbox" id="c-37256906" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37255881">parent</a><span>|</span><a href="#37254316">next</a><span>|</span><label class="collapse" for="c-37256906">[-]</label><label class="expand" for="c-37256906">[1 more]</label></div><br/><div class="children"><div class="content">This is some really good advice here.
It&#x27;s always a good idea to throw out all candidates that can&#x27;t immediately recall what the first theoretical result of the rest mass of a Higgs boson was in the first paper describing was.  Basic knowledge like this just correlates so well with ability to make proper decisions in API architecture.</div><br/></div></div></div></div><div id="37254316" class="c"><input type="checkbox" id="c-37254316" checked=""/><div class="controls bullet"><span class="by">blibble</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37253991">parent</a><span>|</span><a href="#37255881">prev</a><span>|</span><a href="#37254096">next</a><span>|</span><label class="collapse" for="c-37254316">[-]</label><label class="expand" for="c-37254316">[7 more]</label></div><br/><div class="children"><div class="content">&gt; If you instant-fail a candidate for this, it says a lot more about you and your organization than the candidate.<p>yes, we expect professional software developers to have basic maths skills<p>&quot;what is a prime number&quot; is taught to 7 year olds, it&#x27;s not vector calculus<p>what else would you consider to be an unreasonable thing for an employer to require?<p>reading and writing skills of a typical 7 year old?</div><br/><div id="37255028" class="c"><input type="checkbox" id="c-37255028" checked=""/><div class="controls bullet"><span class="by">daok</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254316">parent</a><span>|</span><a href="#37256599">next</a><span>|</span><label class="collapse" for="c-37255028">[-]</label><label class="expand" for="c-37255028">[3 more]</label></div><br/><div class="children"><div class="content">You probably do not have a child of 7 years old because they do not know at that age what is a prime number.<p>Second, basic math still that you never or rarely use or with very large time between usage might get rusty. You may understand the concept but not find the optimal solution. The way you are responding here shows quite a lot about how you are short sighted by instant-failing someone with a single question instead of trying to asses the whole person as much as you can. On you side, you are wasting opportunity to have a great person that could be a key player in your team by bringing other set of skill on the table.</div><br/><div id="37255391" class="c"><input type="checkbox" id="c-37255391" checked=""/><div class="controls bullet"><span class="by">blibble</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37255028">parent</a><span>|</span><a href="#37256376">next</a><span>|</span><label class="collapse" for="c-37255391">[-]</label><label class="expand" for="c-37255391">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  You probably do not have a child of 7 years old because they do not know at that age what is a prime number.<p>it&#x27;s part of the curriculum for children of this age where I grew up (I did check)<p>&gt; The way you are responding here shows quite a lot about how you are short sighted by instant-failing someone with a single question instead of trying to asses the whole person as much as you can. On you side, you are wasting opportunity to have a great person that could be a key player in your team by bringing other set of skill on the table.<p>it may also be the case that I have more in depth knowledge about the roles that I&#x27;ve interviewed candidates for<p>most recently: hiring people to work for quants<p>not instantly knowing that even numbers (other than 2) are not prime is a very strong signal</div><br/></div></div><div id="37256376" class="c"><input type="checkbox" id="c-37256376" checked=""/><div class="controls bullet"><span class="by">hyperbovine</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37255028">parent</a><span>|</span><a href="#37255391">prev</a><span>|</span><a href="#37256599">next</a><span>|</span><label class="collapse" for="c-37256376">[-]</label><label class="expand" for="c-37256376">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You probably do not have a child of 7 years old because they do not know at that age what is a prime number.<p>A few do. And in 20 years you&#x27;re reallyreally going to want to hire them.</div><br/></div></div></div></div><div id="37256599" class="c"><input type="checkbox" id="c-37256599" checked=""/><div class="controls bullet"><span class="by">jpeterson</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254316">parent</a><span>|</span><a href="#37255028">prev</a><span>|</span><a href="#37254722">next</a><span>|</span><label class="collapse" for="c-37256599">[-]</label><label class="expand" for="c-37256599">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re not testing for &quot;basic math skills&quot; here. What you&#x27;re testing for is more like &quot;immediately retrieves an irrelevant math fact after many years of having no need to think about it.&quot;<p>Look, if you think this sort of thing allows you to identify great candidates, good for you. But in my experience, not only is this kind of practice stupid on its face, but it leads to engineering orgs packed with people who are good at memorizing trivia but terrible at solving real problems.</div><br/></div></div><div id="37254722" class="c"><input type="checkbox" id="c-37254722" checked=""/><div class="controls bullet"><span class="by">ungruntled</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254316">parent</a><span>|</span><a href="#37256599">prev</a><span>|</span><a href="#37254096">next</a><span>|</span><label class="collapse" for="c-37254722">[-]</label><label class="expand" for="c-37254722">[2 more]</label></div><br/><div class="children"><div class="content">I think the key problem here is that is is a bad programming question. If you know anything about prime numbers then coming up with an answer is trivial. If you expect a more optimized solution, then you are really only gauging the interviewee’s understanding of prime numbers. So effectively the interview is more about mathematics than it is about programming or problem solving.</div><br/></div></div></div></div></div></div><div id="37254096" class="c"><input type="checkbox" id="c-37254096" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37253038">parent</a><span>|</span><a href="#37253991">prev</a><span>|</span><a href="#37254729">next</a><span>|</span><label class="collapse" for="c-37254096">[-]</label><label class="expand" for="c-37254096">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m mad at myself now that it has eaten 15 minutes of my time trying to come up with the right optimization. What&#x27;s the trick? 2, +1, and then +2 from there on seems obvious but once you get to 9 is it worth building a list of nonprimes to skip?</div><br/><div id="37254355" class="c"><input type="checkbox" id="c-37254355" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254096">parent</a><span>|</span><a href="#37255979">next</a><span>|</span><label class="collapse" for="c-37254355">[-]</label><label class="expand" for="c-37254355">[2 more]</label></div><br/><div class="children"><div class="content">I think they&#x27;re suggesting simply doing +2<p>+1 is not a good idea since ~half of all numbers are effectively non-prime simply by being even numbers.<p>You can double the speed by using +2 without using any fancy tricks, just changing a single character.</div><br/><div id="37256442" class="c"><input type="checkbox" id="c-37256442" checked=""/><div class="controls bullet"><span class="by">nojs</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254355">parent</a><span>|</span><a href="#37255979">next</a><span>|</span><label class="collapse" for="c-37256442">[-]</label><label class="expand" for="c-37256442">[1 more]</label></div><br/><div class="children"><div class="content">Well it doesn’t double the speed, since anything with a factor of 2 undergoes only one loop iteration inside is_prime. It basically just saves a function call</div><br/></div></div></div></div><div id="37255979" class="c"><input type="checkbox" id="c-37255979" checked=""/><div class="controls bullet"><span class="by">dahfizz</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254096">parent</a><span>|</span><a href="#37254355">prev</a><span>|</span><a href="#37254324">next</a><span>|</span><label class="collapse" for="c-37255979">[-]</label><label class="expand" for="c-37255979">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s lots of more advanced optimizations, which would be an interesting avenue for discussion in an interview, but the drain dead algorithm would just use +2 instead of +1</div><br/></div></div><div id="37254324" class="c"><input type="checkbox" id="c-37254324" checked=""/><div class="controls bullet"><span class="by">Our_Benefactors</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254096">parent</a><span>|</span><a href="#37255979">prev</a><span>|</span><a href="#37254729">next</a><span>|</span><label class="collapse" for="c-37254324">[-]</label><label class="expand" for="c-37254324">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;54544012&#x2F;1336678" rel="nofollow noreferrer">https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;54544012&#x2F;1336678</a><p>Common approach is to use square roots, this reduces the runtime. Recommend checking out project euler if you like solving hard math-code-o(n)-puzzles.</div><br/><div id="37254762" class="c"><input type="checkbox" id="c-37254762" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#37251995">root</a><span>|</span><a href="#37254324">parent</a><span>|</span><a href="#37254729">next</a><span>|</span><label class="collapse" for="c-37254762">[-]</label><label class="expand" for="c-37254762">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t want to cheat by looking on S.O. but thanks ;)<p>Yes it makes sense (in the GPT code) that you&#x27;d only go up to i * i ... although looking at pythonic while: statements is just gross to me in this context, it would feel a lot more readable to say, e.g. in PHP:<p>for ($i=2;$i&lt;sqrt($n);) {
  $i+=($i==2 ? 1 : 2); &#x2F;&#x2F;although the first one should just be outside the loop
}</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="37249294" class="c"><input type="checkbox" id="c-37249294" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37251995">prev</a><span>|</span><a href="#37249141">next</a><span>|</span><label class="collapse" for="c-37249294">[-]</label><label class="expand" for="c-37249294">[22 more]</label></div><br/><div class="children"><div class="content">The highlight IMO<p>&gt; The Code Llama models provide stable generations with up to 100,000 tokens of context. All models are trained on sequences of 16,000 tokens and show improvements on inputs with up to 100,000 tokens.<p>Edit: Reading the paper, key retrieval accuracy really deteriorates after 16k tokens, so it remains to be seen how useful the 100k context is.</div><br/><div id="37250440" class="c"><input type="checkbox" id="c-37250440" checked=""/><div class="controls bullet"><span class="by">nabakin</span><span>|</span><a href="#37249294">parent</a><span>|</span><a href="#37249339">next</a><span>|</span><label class="collapse" for="c-37250440">[-]</label><label class="expand" for="c-37250440">[14 more]</label></div><br/><div class="children"><div class="content">Looks like they aren&#x27;t releasing a pretty interesting model too. In the paper they mention a &quot;Unnatural Code Llama&quot; which wipes the floor with every other model&#x2F;finetune on every benchmark except for slightly losing to Code Llama Python on MBPP pass@100 and slightly losing to GPT-4 on HumanEval pass@1 which is insane.<p>Meta says later on that they aren&#x27;t releasing it and give no explanation. I wonder why given how incredible it seems to be.</div><br/><div id="37250872" class="c"><input type="checkbox" id="c-37250872" checked=""/><div class="controls bullet"><span class="by">EvgeniyZh</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37250440">parent</a><span>|</span><a href="#37250716">next</a><span>|</span><label class="collapse" for="c-37250872">[-]</label><label class="expand" for="c-37250872">[2 more]</label></div><br/><div class="children"><div class="content">Note that current GPT-4 pass@1 for HumanEval is closer to 90% than to 67% reported in GPT-4 technical report, as reported, e.g., in [1]<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.01210" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.01210</a></div><br/><div id="37251197" class="c"><input type="checkbox" id="c-37251197" checked=""/><div class="controls bullet"><span class="by">nabakin</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37250872">parent</a><span>|</span><a href="#37250716">next</a><span>|</span><label class="collapse" for="c-37251197">[-]</label><label class="expand" for="c-37251197">[1 more]</label></div><br/><div class="children"><div class="content">Good point, I guess Meta should be using that number in their chart</div><br/></div></div></div></div><div id="37250716" class="c"><input type="checkbox" id="c-37250716" checked=""/><div class="controls bullet"><span class="by">jonchurch_</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37250440">parent</a><span>|</span><a href="#37250872">prev</a><span>|</span><a href="#37250516">next</a><span>|</span><label class="collapse" for="c-37250716">[-]</label><label class="expand" for="c-37250716">[6 more]</label></div><br/><div class="children"><div class="content">The paper states it was instruction fine tuned with synthetic data (LLM generated instructions) ala another paper (“Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor”).<p>The github repo associated with that paper is linked below. It links to the paper on arxiv, but also has some data in the repo.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;orhonovich&#x2F;unnatural-instructions">https:&#x2F;&#x2F;github.com&#x2F;orhonovich&#x2F;unnatural-instructions</a></div><br/><div id="37250850" class="c"><input type="checkbox" id="c-37250850" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37250716">parent</a><span>|</span><a href="#37250516">next</a><span>|</span><label class="collapse" for="c-37250850">[-]</label><label class="expand" for="c-37250850">[5 more]</label></div><br/><div class="children"><div class="content">Maybe they used GPT-4 to train it. OpenAI terms of use don&#x27;t allow that to be released commercially.</div><br/><div id="37251154" class="c"><input type="checkbox" id="c-37251154" checked=""/><div class="controls bullet"><span class="by">nabakin</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37250850">parent</a><span>|</span><a href="#37251002">next</a><span>|</span><label class="collapse" for="c-37251154">[-]</label><label class="expand" for="c-37251154">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen this argued a lot but is it fact? OpenAI was able to train on data from other platforms and surely, those platforms weren&#x27;t letting their data go if they could help it. Unless some new laws have been passed, I don&#x27;t think OpenAI can legally prevent others from using their data to train models. OpenAI can&#x27;t have their cake and eat it too. After all, any content generated by AI can&#x27;t be copyrighted.</div><br/><div id="37252119" class="c"><input type="checkbox" id="c-37252119" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37251154">parent</a><span>|</span><a href="#37251002">next</a><span>|</span><label class="collapse" for="c-37252119">[-]</label><label class="expand" for="c-37252119">[2 more]</label></div><br/><div class="children"><div class="content">It is indeed a fact that OpenAI&#x27;s Terms of Use do state that you can&#x27;t use their service to develop competing models: Section 2.c.iii - 
<a href="https:&#x2F;&#x2F;openai.com&#x2F;policies&#x2F;terms-of-use" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;policies&#x2F;terms-of-use</a><p>Now of course, the terms are not the law (so don&#x27;t govern the use of the generated data by any third party), they are an agreement between two parties. If you did click &quot;agree&quot; then that&#x27;s a binding agreement and there could be legal&#x2F;contractual repercussions (some of which are outlined in the terms).</div><br/><div id="37253342" class="c"><input type="checkbox" id="c-37253342" checked=""/><div class="controls bullet"><span class="by">haldujai</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37252119">parent</a><span>|</span><a href="#37251002">next</a><span>|</span><label class="collapse" for="c-37253342">[-]</label><label class="expand" for="c-37253342">[1 more]</label></div><br/><div class="children"><div class="content">That seems like a likely explanation, probably won&#x27;t get into legal trouble for using an OpenAI model for a research paper but redistributing said model may be upsetting enough for OpenAI trigger a legal challenge.<p>Unnatural language used davinci-002 although that was a while ago, they only say &quot;similarly&quot; in this paper and don&#x27;t specify what they used. I can&#x27;t see a reason why they wouldn&#x27;t be releasing it if the unnatural prompts were generated by LLaMA2-family.<p>In any case, replicating this training seems trivial and very cheap compute-wise for anyone who wanted to do it.</div><br/></div></div></div></div></div></div><div id="37251002" class="c"><input type="checkbox" id="c-37251002" checked=""/><div class="controls bullet"><span class="by">nkohari</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37250850">parent</a><span>|</span><a href="#37251154">prev</a><span>|</span><a href="#37250516">next</a><span>|</span><label class="collapse" for="c-37251002">[-]</label><label class="expand" for="c-37251002">[1 more]</label></div><br/><div class="children"><div class="content">This is the most likely explanation for both why they wouldn&#x27;t release it and wouldn&#x27;t explain why.</div><br/></div></div></div></div></div></div><div id="37250516" class="c"><input type="checkbox" id="c-37250516" checked=""/><div class="controls bullet"><span class="by">kapp_in_life</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37250440">parent</a><span>|</span><a href="#37250716">prev</a><span>|</span><a href="#37249339">next</a><span>|</span><label class="collapse" for="c-37250516">[-]</label><label class="expand" for="c-37250516">[5 more]</label></div><br/><div class="children"><div class="content">Likely trained on internal code.</div><br/><div id="37252321" class="c"><input type="checkbox" id="c-37252321" checked=""/><div class="controls bullet"><span class="by">mediaman</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37250516">parent</a><span>|</span><a href="#37249339">next</a><span>|</span><label class="collapse" for="c-37252321">[-]</label><label class="expand" for="c-37252321">[4 more]</label></div><br/><div class="children"><div class="content">That model is trained on synthetically AI-generated code, not internal code.<p>It suggests that synthetic training could be the future in increasing capability of smaller models (and perhaps bigger ones too). AI will train AI.</div><br/><div id="37253252" class="c"><input type="checkbox" id="c-37253252" checked=""/><div class="controls bullet"><span class="by">haldujai</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37252321">parent</a><span>|</span><a href="#37252449">next</a><span>|</span><label class="collapse" for="c-37253252">[-]</label><label class="expand" for="c-37253252">[1 more]</label></div><br/><div class="children"><div class="content">I thought this specific model was referring to self-instruction using both synthetic prompts (generated from few-shot in-context prompting of presumably some OpenAI model, the original paper used text-davinci-002) as well as synthetic code (presumably Code Llama 7 like for self-instruct) subsequently validated with execution?<p>The differences being it&#x27;s not just training on unvalidated synthetic data and this specific method (per the unnatural questions paper) results in increased instruction diversity which confers some added advantage and I&#x27;m assuming explains the performance gain over the also synthetic self-instruct code?<p>I may be misunderstanding but this seems more nuanced than just training on synthetically AI-generated code and is more validating of synthetic instructions (i.e. low resource setting) rather than synthetic code (i.e. high resource setting).</div><br/></div></div><div id="37252449" class="c"><input type="checkbox" id="c-37252449" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37252321">parent</a><span>|</span><a href="#37253252">prev</a><span>|</span><a href="#37249339">next</a><span>|</span><label class="collapse" for="c-37252449">[-]</label><label class="expand" for="c-37252449">[2 more]</label></div><br/><div class="children"><div class="content">That is the basis for <a href="https:&#x2F;&#x2F;synthesis.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;synthesis.ai&#x2F;</a></div><br/><div id="37252831" class="c"><input type="checkbox" id="c-37252831" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37252449">parent</a><span>|</span><a href="#37249339">next</a><span>|</span><label class="collapse" for="c-37252831">[-]</label><label class="expand" for="c-37252831">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m an amateur, but it seems to me that methods to synthesize will have to be distinct from methods of the generative model.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37249339" class="c"><input type="checkbox" id="c-37249339" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37249294">parent</a><span>|</span><a href="#37250440">prev</a><span>|</span><a href="#37251502">next</a><span>|</span><label class="collapse" for="c-37249339">[-]</label><label class="expand" for="c-37249339">[5 more]</label></div><br/><div class="children"><div class="content">Did Meta add scalable rope to the official implementation?</div><br/><div id="37249934" class="c"><input type="checkbox" id="c-37249934" checked=""/><div class="controls bullet"><span class="by">snippyhollow</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37249339">parent</a><span>|</span><a href="#37251502">next</a><span>|</span><label class="collapse" for="c-37249934">[-]</label><label class="expand" for="c-37249934">[4 more]</label></div><br/><div class="children"><div class="content">We changed RoPE&#x27;s theta from 10k to 1m and fine-tuned with 16k tokens long sequences.</div><br/><div id="37250240" class="c"><input type="checkbox" id="c-37250240" checked=""/><div class="controls bullet"><span class="by">malwrar</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37249934">parent</a><span>|</span><a href="#37250736">next</a><span>|</span><label class="collapse" for="c-37250240">[-]</label><label class="expand" for="c-37250240">[2 more]</label></div><br/><div class="children"><div class="content">Curious, what led you to adjusting the parameters this way? Also, have you guys experimented with ALiBi[1] which claims better extrapolative results than rotary positional encoding?<p>[1]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2108.12409" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2108.12409</a> (charts on page two if you’re skimming)</div><br/><div id="37250293" class="c"><input type="checkbox" id="c-37250293" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37250240">parent</a><span>|</span><a href="#37250736">next</a><span>|</span><label class="collapse" for="c-37250293">[-]</label><label class="expand" for="c-37250293">[1 more]</label></div><br/><div class="children"><div class="content">Undoubtedly, they have tried ALiBi…</div><br/></div></div></div></div></div></div></div></div><div id="37251502" class="c"><input type="checkbox" id="c-37251502" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#37249294">parent</a><span>|</span><a href="#37249339">prev</a><span>|</span><a href="#37249141">next</a><span>|</span><label class="collapse" for="c-37251502">[-]</label><label class="expand" for="c-37251502">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The Code Llama models provide stable generations with up to 100,000 tokens of context.<p>what is the trick to achieve 100k context? They can&#x27;t just use 100k wide transformer layer, it is cost prohibitive, right?..</div><br/><div id="37251683" class="c"><input type="checkbox" id="c-37251683" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#37249294">root</a><span>|</span><a href="#37251502">parent</a><span>|</span><a href="#37249141">next</a><span>|</span><label class="collapse" for="c-37251683">[-]</label><label class="expand" for="c-37251683">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty sure they don&#x27;t do that, but for code the relevant relationship between two tokens is easy to determine with the semantics of the language alone (for instance you can say that tokens related to a local variable have no relationship with tokens outside), so it would lead to a sparse matrix in the transformer, reducing the cost of big contexts by a lot. But it would require language specific preprocessing, and whether you can make it fast is also dubious. I don&#x27;t think it&#x27;s been tried so far.</div><br/></div></div></div></div></div></div><div id="37249141" class="c"><input type="checkbox" id="c-37249141" checked=""/><div class="controls bullet"><span class="by">up6w6</span><span>|</span><a href="#37249294">prev</a><span>|</span><a href="#37250926">next</a><span>|</span><label class="collapse" for="c-37249141">[-]</label><label class="expand" for="c-37249141">[27 more]</label></div><br/><div class="children"><div class="content">Even the 7B model of code llama seems to be competitive with Codex, the model behind copilot<p><a href="https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;code-llama-large-language-model-coding&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;code-llama-large-language-model-cod...</a></div><br/><div id="37251416" class="c"><input type="checkbox" id="c-37251416" checked=""/><div class="controls bullet"><span class="by">SparkyMcUnicorn</span><span>|</span><a href="#37249141">parent</a><span>|</span><a href="#37249199">next</a><span>|</span><label class="collapse" for="c-37251416">[-]</label><label class="expand" for="c-37251416">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure copilot is using codex anymore[0]. They&#x27;ve also been talking about a shift towards GPT-4 with &quot;Copilot X&quot; a few times now[1][2].<p>[0] <a href="https:&#x2F;&#x2F;github.blog&#x2F;2023-07-28-smarter-more-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-model&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;github.blog&#x2F;2023-07-28-smarter-more-efficient-coding...</a><p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;features&#x2F;preview&#x2F;copilot-x">https:&#x2F;&#x2F;github.com&#x2F;features&#x2F;preview&#x2F;copilot-x</a><p>[2] <a href="https:&#x2F;&#x2F;github.blog&#x2F;2023-07-20-github-copilot-chat-beta-now-available-for-every-organization&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;github.blog&#x2F;2023-07-20-github-copilot-chat-beta-now-...</a></div><br/><div id="37256777" class="c"><input type="checkbox" id="c-37256777" checked=""/><div class="controls bullet"><span class="by">zarzavat</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37251416">parent</a><span>|</span><a href="#37256163">next</a><span>|</span><label class="collapse" for="c-37256777">[-]</label><label class="expand" for="c-37256777">[1 more]</label></div><br/><div class="children"><div class="content">Copilot X is just their name for their project to bring AI to more areas of VSCode. I don’t believe they can use GPT-4 for completions because it’s a chat-optimized model. It seems that they are using something else, that blog post seems to imply it’s a custom-trained model.</div><br/></div></div><div id="37256163" class="c"><input type="checkbox" id="c-37256163" checked=""/><div class="controls bullet"><span class="by">up6w6</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37251416">parent</a><span>|</span><a href="#37256777">prev</a><span>|</span><a href="#37249199">next</a><span>|</span><label class="collapse" for="c-37256163">[-]</label><label class="expand" for="c-37256163">[1 more]</label></div><br/><div class="children"><div class="content">True. The results from codex are actually from code-cushman-001 (Chen et al. 2021), which is an older model that Copilot was based on.</div><br/></div></div></div></div><div id="37249199" class="c"><input type="checkbox" id="c-37249199" checked=""/><div class="controls bullet"><span class="by">ramesh31</span><span>|</span><a href="#37249141">parent</a><span>|</span><a href="#37251416">prev</a><span>|</span><a href="#37250926">next</a><span>|</span><label class="collapse" for="c-37249199">[-]</label><label class="expand" for="c-37249199">[23 more]</label></div><br/><div class="children"><div class="content">&gt;Even the 7B model of code llama seems to be competitive with Codex, the model behind copilot<p>It&#x27;s extremely good. I keep a terminal tab open with 7b running for all of my &quot;how do I do this random thing&quot; questions while coding. It&#x27;s pretty much replaced Google&#x2F;SO for me.</div><br/><div id="37249212" class="c"><input type="checkbox" id="c-37249212" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37249199">parent</a><span>|</span><a href="#37250260">next</a><span>|</span><label class="collapse" for="c-37249212">[-]</label><label class="expand" for="c-37249212">[14 more]</label></div><br/><div class="children"><div class="content">You&#x27;ve already downloaded and thoroughly tested the 7B parameter model of &quot;code llama&quot;? I&#x27;m skeptical.</div><br/><div id="37249447" class="c"><input type="checkbox" id="c-37249447" checked=""/><div class="controls bullet"><span class="by">Eddygandr</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37249212">parent</a><span>|</span><a href="#37249335">next</a><span>|</span><label class="collapse" for="c-37249447">[-]</label><label class="expand" for="c-37249447">[1 more]</label></div><br/><div class="children"><div class="content">Maybe confused Code Llama with Llama 2?</div><br/></div></div><div id="37249335" class="c"><input type="checkbox" id="c-37249335" checked=""/><div class="controls bullet"><span class="by">realce</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37249212">parent</a><span>|</span><a href="#37249447">prev</a><span>|</span><a href="#37250135">next</a><span>|</span><label class="collapse" for="c-37249335">[-]</label><label class="expand" for="c-37249335">[6 more]</label></div><br/><div class="children"><div class="content">Just sign up at meta and you&#x27;ll get an email link in like 5 minutes</div><br/><div id="37249346" class="c"><input type="checkbox" id="c-37249346" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37249335">parent</a><span>|</span><a href="#37250135">next</a><span>|</span><label class="collapse" for="c-37249346">[-]</label><label class="expand" for="c-37249346">[5 more]</label></div><br/><div class="children"><div class="content">Yes, that&#x27;s not a response to my comment.<p>No one who has been using any model for just the past 30 minutes would say that it has &quot;pretty much replaced Google&#x2F;SO&quot; for them, unless they were being facetious.</div><br/><div id="37249458" class="c"><input type="checkbox" id="c-37249458" checked=""/><div class="controls bullet"><span class="by">tyre</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37249346">parent</a><span>|</span><a href="#37249559">next</a><span>|</span><label class="collapse" for="c-37249458">[-]</label><label class="expand" for="c-37249458">[2 more]</label></div><br/><div class="children"><div class="content">They said 7b llama which I read as the base LLaMa model, not this one specifically. All of these LLMs are trained on Stack Overflow so it makes sense that they’d be good out of the box.</div><br/><div id="37249762" class="c"><input type="checkbox" id="c-37249762" checked=""/><div class="controls bullet"><span class="by">brandall10</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37249458">parent</a><span>|</span><a href="#37249559">next</a><span>|</span><label class="collapse" for="c-37249762">[-]</label><label class="expand" for="c-37249762">[1 more]</label></div><br/><div class="children"><div class="content">The top level comment is specifically citing performance of code llama against codex.</div><br/></div></div></div></div><div id="37249559" class="c"><input type="checkbox" id="c-37249559" checked=""/><div class="controls bullet"><span class="by">dataangel</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37249346">parent</a><span>|</span><a href="#37249458">prev</a><span>|</span><a href="#37249788">next</a><span>|</span><label class="collapse" for="c-37249559">[-]</label><label class="expand" for="c-37249559">[1 more]</label></div><br/><div class="children"><div class="content">GPT4 has replaced SO for me and I&#x27;ve been using it for months.</div><br/></div></div></div></div></div></div><div id="37250135" class="c"><input type="checkbox" id="c-37250135" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37249212">parent</a><span>|</span><a href="#37249335">prev</a><span>|</span><a href="#37249271">next</a><span>|</span><label class="collapse" for="c-37250135">[-]</label><label class="expand" for="c-37250135">[2 more]</label></div><br/><div class="children"><div class="content">It was made available internally, I believe. So this is one of the many Meta engineers on this site —- after all, Facebook is now less hated than Google here ;)</div><br/></div></div><div id="37249271" class="c"><input type="checkbox" id="c-37249271" checked=""/><div class="controls bullet"><span class="by">lddemi</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37249212">parent</a><span>|</span><a href="#37250135">prev</a><span>|</span><a href="#37250260">next</a><span>|</span><label class="collapse" for="c-37249271">[-]</label><label class="expand" for="c-37249271">[4 more]</label></div><br/><div class="children"><div class="content">Likely meta employee?</div><br/><div id="37249878" class="c"><input type="checkbox" id="c-37249878" checked=""/><div class="controls bullet"><span class="by">MertsA</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37249271">parent</a><span>|</span><a href="#37250260">next</a><span>|</span><label class="collapse" for="c-37249878">[-]</label><label class="expand" for="c-37249878">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using this or something similar internally  for months and love it. The thing that gets downright spooky is the comments believe it or not. I&#x27;ll have some method with a short variable name in a larger program and not only does it often suggest a pretty good snippet of code the comments will be correct and explain what the intent behind the code is. It&#x27;s just a LLM but you really start to get the feeling the whole is greater than the sum of the parts.</div><br/><div id="37250514" class="c"><input type="checkbox" id="c-37250514" checked=""/><div class="controls bullet"><span class="by">coder543</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37249878">parent</a><span>|</span><a href="#37250260">next</a><span>|</span><label class="collapse" for="c-37250514">[-]</label><label class="expand" for="c-37250514">[2 more]</label></div><br/><div class="children"><div class="content">I just don’t understand how anyone is making practical use of local code completion models. Is there a VS Code extension that I’ve been unable to find? HuggingFace released one that is meant to use their service for inference, not your local GPU.<p>The instruct version of code llama could certainly be run locally without trouble, and that’s interesting too, but I keep wanting to test out a local CoPilot alternative that uses these nice, new completion models.</div><br/><div id="37253429" class="c"><input type="checkbox" id="c-37253429" checked=""/><div class="controls bullet"><span class="by">fredoliveira</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37250514">parent</a><span>|</span><a href="#37250260">next</a><span>|</span><label class="collapse" for="c-37253429">[-]</label><label class="expand" for="c-37253429">[1 more]</label></div><br/><div class="children"><div class="content">There are a bunch of VSCode extensions that make use of local models. Tabby seems to be the most friendly right now, but I admittedly haven&#x27;t tried it myself: <a href="https:&#x2F;&#x2F;tabbyml.github.io&#x2F;tabby&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;tabbyml.github.io&#x2F;tabby&#x2F;</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="37250260" class="c"><input type="checkbox" id="c-37250260" checked=""/><div class="controls bullet"><span class="by">ohyes</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37249199">parent</a><span>|</span><a href="#37249212">prev</a><span>|</span><a href="#37249282">next</a><span>|</span><label class="collapse" for="c-37250260">[-]</label><label class="expand" for="c-37250260">[7 more]</label></div><br/><div class="children"><div class="content">What hardware do you have that lets you run 7b and do other stuff at the same time?</div><br/><div id="37258940" class="c"><input type="checkbox" id="c-37258940" checked=""/><div class="controls bullet"><span class="by">selfhoster11</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37250260">parent</a><span>|</span><a href="#37250509">next</a><span>|</span><label class="collapse" for="c-37258940">[-]</label><label class="expand" for="c-37258940">[2 more]</label></div><br/><div class="children"><div class="content">A 7B model at 8-bit quantization takes up 7 GB of RAM. Less if you use a 6-bit quantization, which is nearly as good. Otherwise it&#x27;s just a question of having enough system RAM and CPU cores, plus maybe a small discrete GPU.</div><br/><div id="37259476" class="c"><input type="checkbox" id="c-37259476" checked=""/><div class="controls bullet"><span class="by">FrozenSynapse</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37258940">parent</a><span>|</span><a href="#37250509">next</a><span>|</span><label class="collapse" for="c-37259476">[-]</label><label class="expand" for="c-37259476">[1 more]</label></div><br/><div class="children"><div class="content">how&#x27;s the generation speed on CPU?</div><br/></div></div></div></div><div id="37250509" class="c"><input type="checkbox" id="c-37250509" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37250260">parent</a><span>|</span><a href="#37258940">prev</a><span>|</span><a href="#37250353">next</a><span>|</span><label class="collapse" for="c-37250509">[-]</label><label class="expand" for="c-37250509">[1 more]</label></div><br/><div class="children"><div class="content">Pretty much any PC with 16GB+ of fast RAM can do this, any PC with a dGPU can do it well.</div><br/></div></div><div id="37250353" class="c"><input type="checkbox" id="c-37250353" checked=""/><div class="controls bullet"><span class="by">hmottestad</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37250260">parent</a><span>|</span><a href="#37250509">prev</a><span>|</span><a href="#37253981">next</a><span>|</span><label class="collapse" for="c-37250353">[-]</label><label class="expand" for="c-37250353">[1 more]</label></div><br/><div class="children"><div class="content">Maybe a MacBook Pro. The Apple silicon chops can offload a special AI inference engine, and all ram is accessible by all parts of the chip.</div><br/></div></div><div id="37253981" class="c"><input type="checkbox" id="c-37253981" checked=""/><div class="controls bullet"><span class="by">gzer0</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37250260">parent</a><span>|</span><a href="#37250353">prev</a><span>|</span><a href="#37250816">next</a><span>|</span><label class="collapse" for="c-37253981">[-]</label><label class="expand" for="c-37253981">[1 more]</label></div><br/><div class="children"><div class="content">An M1 Max with 64GB of RAM allows me to run multiple models simultaneously, on top of stable diffusion generating images non-stop + normal chrome, vscode, etc. Definitely feeling the heat, but it&#x27;s working. Well worth the investment.</div><br/></div></div><div id="37250816" class="c"><input type="checkbox" id="c-37250816" checked=""/><div class="controls bullet"><span class="by">_joel</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37250260">parent</a><span>|</span><a href="#37253981">prev</a><span>|</span><a href="#37249282">next</a><span>|</span><label class="collapse" for="c-37250816">[-]</label><label class="expand" for="c-37250816">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re willing to sacrifice token&#x2F;s you can even run these on your phone.</div><br/></div></div></div></div><div id="37249282" class="c"><input type="checkbox" id="c-37249282" checked=""/><div class="controls bullet"><span class="by">solarkraft</span><span>|</span><a href="#37249141">root</a><span>|</span><a href="#37249199">parent</a><span>|</span><a href="#37250260">prev</a><span>|</span><a href="#37250926">next</a><span>|</span><label class="collapse" for="c-37249282">[-]</label><label class="expand" for="c-37249282">[1 more]</label></div><br/><div class="children"><div class="content">Huh? Do you perhaps mean standard Llama?</div><br/></div></div></div></div></div></div><div id="37250926" class="c"><input type="checkbox" id="c-37250926" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#37249141">prev</a><span>|</span><a href="#37250704">next</a><span>|</span><label class="collapse" for="c-37250926">[-]</label><label class="expand" for="c-37250926">[28 more]</label></div><br/><div class="children"><div class="content">TheBloke doesn’t joke around [1]. I’m guessing we’ll have the quantized ones by the end of the day. I’m super excited to use the 34B Python 4 bit quantized one that should just fit on a 3090.<p>[1] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;CodeLlama-13B-Python-fp16" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;CodeLlama-13B-Python-fp16</a></div><br/><div id="37251578" class="c"><input type="checkbox" id="c-37251578" checked=""/><div class="controls bullet"><span class="by">mchiang</span><span>|</span><a href="#37250926">parent</a><span>|</span><a href="#37251410">next</a><span>|</span><label class="collapse" for="c-37251578">[-]</label><label class="expand" for="c-37251578">[17 more]</label></div><br/><div class="children"><div class="content">Ollama supports it already:<p>`ollama run codellama:7b-instruct`<p><a href="https:&#x2F;&#x2F;ollama.ai&#x2F;blog&#x2F;run-code-llama-locally">https:&#x2F;&#x2F;ollama.ai&#x2F;blog&#x2F;run-code-llama-locally</a><p>More models uploaded as we speak:<p><a href="https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;codellama">https:&#x2F;&#x2F;ollama.ai&#x2F;library&#x2F;codellama</a></div><br/><div id="37251873" class="c"><input type="checkbox" id="c-37251873" checked=""/><div class="controls bullet"><span class="by">jerrysievert</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251578">parent</a><span>|</span><a href="#37251704">next</a><span>|</span><label class="collapse" for="c-37251873">[-]</label><label class="expand" for="c-37251873">[10 more]</label></div><br/><div class="children"><div class="content">while it supports it, so far I&#x27;ve only managed to get infinite streams of near nonsense from the ollama models (codellama:7b-q4_0 and codellama:latest)<p>my questions were asking how to construct an indexam for postgres in c, how to write an r-tree in javascript, and how to write a binary tree in javascript.</div><br/><div id="37252677" class="c"><input type="checkbox" id="c-37252677" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251873">parent</a><span>|</span><a href="#37252089">next</a><span>|</span><label class="collapse" for="c-37252677">[-]</label><label class="expand" for="c-37252677">[1 more]</label></div><br/><div class="children"><div class="content">&gt; managed to get infinite streams of near nonsense<p>This should be fixed now! To update you&#x27;ll have to run:<p><pre><code>  ollama pull codellama:7b-instruct</code></pre></div><br/></div></div><div id="37252089" class="c"><input type="checkbox" id="c-37252089" checked=""/><div class="controls bullet"><span class="by">mchiang</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251873">parent</a><span>|</span><a href="#37252677">prev</a><span>|</span><a href="#37252060">next</a><span>|</span><label class="collapse" for="c-37252089">[-]</label><label class="expand" for="c-37252089">[2 more]</label></div><br/><div class="children"><div class="content">still modifying the code completion (foundation &#x2F; python models) to see what&#x27;s causing the behavior.<p>Have had some good success with the instruct model:<p>codellama:7b-instruct</div><br/><div id="37252418" class="c"><input type="checkbox" id="c-37252418" checked=""/><div class="controls bullet"><span class="by">jerrysievert</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37252089">parent</a><span>|</span><a href="#37252060">next</a><span>|</span><label class="collapse" for="c-37252418">[-]</label><label class="expand" for="c-37252418">[1 more]</label></div><br/><div class="children"><div class="content">thanks! this give me some results, but I&#x27;ve had to use a specific construct to get anything meaningful:<p>using &lt;language&gt; write me a &lt;thing&gt;<p>it&#x27;s managed to spit out code, rather than &quot;write a traversal function&quot;.</div><br/></div></div></div></div><div id="37252060" class="c"><input type="checkbox" id="c-37252060" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251873">parent</a><span>|</span><a href="#37252089">prev</a><span>|</span><a href="#37251922">next</a><span>|</span><label class="collapse" for="c-37252060">[-]</label><label class="expand" for="c-37252060">[3 more]</label></div><br/><div class="children"><div class="content">Same, just tried it and it would give me infinite amount of blank lines</div><br/><div id="37252690" class="c"><input type="checkbox" id="c-37252690" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37252060">parent</a><span>|</span><a href="#37251922">next</a><span>|</span><label class="collapse" for="c-37252690">[-]</label><label class="expand" for="c-37252690">[2 more]</label></div><br/><div class="children"><div class="content">Sorry, this should be fixed now! To update you&#x27;ll have to run:<p><pre><code>  ollama pull codellama:7b-instruct</code></pre></div><br/><div id="37257585" class="c"><input type="checkbox" id="c-37257585" checked=""/><div class="controls bullet"><span class="by">pmayrgundter</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37252690">parent</a><span>|</span><a href="#37251922">next</a><span>|</span><label class="collapse" for="c-37257585">[-]</label><label class="expand" for="c-37257585">[1 more]</label></div><br/><div class="children"><div class="content">This is amazing!<p>I was up and running from clone&#x2F;build-from-scratch&#x2F;download in ~5m.<p>It&#x27;s running on my M1.. it knows WebGL JS APIs better than I do, makes a passable attempt at VT100 ascii art, and well, should read more about Wolfram Automata, but does seem to know Game of Life!<p>Thank you so much!</div><br/></div></div></div></div></div></div><div id="37251922" class="c"><input type="checkbox" id="c-37251922" checked=""/><div class="controls bullet"><span class="by">carbocation</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251873">parent</a><span>|</span><a href="#37252060">prev</a><span>|</span><a href="#37251704">next</a><span>|</span><label class="collapse" for="c-37251922">[-]</label><label class="expand" for="c-37251922">[3 more]</label></div><br/><div class="children"><div class="content">Similarly, I had it emit hundreds of blank lines before cancelling it.</div><br/><div id="37252436" class="c"><input type="checkbox" id="c-37252436" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251922">parent</a><span>|</span><a href="#37252932">next</a><span>|</span><label class="collapse" for="c-37252436">[-]</label><label class="expand" for="c-37252436">[1 more]</label></div><br/><div class="children"><div class="content">What fortune, I so happen to need hundreds of blank lines.</div><br/></div></div><div id="37252932" class="c"><input type="checkbox" id="c-37252932" checked=""/><div class="controls bullet"><span class="by">justinsaccount</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251922">parent</a><span>|</span><a href="#37252436">prev</a><span>|</span><a href="#37251704">next</a><span>|</span><label class="collapse" for="c-37252932">[-]</label><label class="expand" for="c-37252932">[1 more]</label></div><br/><div class="children"><div class="content">Maybe it&#x27;s outputting <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Whitespace_(programming_language)" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Whitespace_(programming_langua...</a> :-)</div><br/></div></div></div></div></div></div><div id="37251704" class="c"><input type="checkbox" id="c-37251704" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251578">parent</a><span>|</span><a href="#37251873">prev</a><span>|</span><a href="#37251668">next</a><span>|</span><label class="collapse" for="c-37251704">[-]</label><label class="expand" for="c-37251704">[1 more]</label></div><br/><div class="children"><div class="content">Whoa, it’s absolutely astounding how fast the community is reacting to these model release!</div><br/></div></div><div id="37252066" class="c"><input type="checkbox" id="c-37252066" checked=""/><div class="controls bullet"><span class="by">Pesthuf</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251578">parent</a><span>|</span><a href="#37251668">prev</a><span>|</span><a href="#37253487">next</a><span>|</span><label class="collapse" for="c-37252066">[-]</label><label class="expand" for="c-37252066">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t ollama terminal only?  For code, that wouldn&#x27;t be good.</div><br/><div id="37252337" class="c"><input type="checkbox" id="c-37252337" checked=""/><div class="controls bullet"><span class="by">natrys</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37252066">parent</a><span>|</span><a href="#37253487">next</a><span>|</span><label class="collapse" for="c-37252337">[-]</label><label class="expand" for="c-37252337">[2 more]</label></div><br/><div class="children"><div class="content">They have a server&#x2F;client model. The binary comes with a basic terminal front-end but you can just create your own self-hosted GUI or editor integration against the API[1]:<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;docs&#x2F;api.md">https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama&#x2F;blob&#x2F;main&#x2F;docs&#x2F;api.md</a></div><br/><div id="37252819" class="c"><input type="checkbox" id="c-37252819" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37252337">parent</a><span>|</span><a href="#37253487">next</a><span>|</span><label class="collapse" for="c-37252819">[-]</label><label class="expand" for="c-37252819">[1 more]</label></div><br/><div class="children"><div class="content">Indeed! After pulling a model with &quot;ollama pull codellama&quot; you can access it via the REST API:<p><pre><code>  curl -X POST http:&#x2F;&#x2F;localhost:11434&#x2F;api&#x2F;generate -d &#x27;{                        
    &quot;model&quot;: &quot;codellama&quot;,
    &quot;prompt&quot;:&quot;write a python script to add two numbers&quot;
  }&#x27;</code></pre></div><br/></div></div></div></div></div></div><div id="37253487" class="c"><input type="checkbox" id="c-37253487" checked=""/><div class="controls bullet"><span class="by">comechao</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251578">parent</a><span>|</span><a href="#37252066">prev</a><span>|</span><a href="#37251410">next</a><span>|</span><label class="collapse" for="c-37253487">[-]</label><label class="expand" for="c-37253487">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m testing it on my M2 Air (16GB). Quite fast!</div><br/></div></div></div></div><div id="37251410" class="c"><input type="checkbox" id="c-37251410" checked=""/><div class="controls bullet"><span class="by">stuckinhell</span><span>|</span><a href="#37250926">parent</a><span>|</span><a href="#37251578">prev</a><span>|</span><a href="#37252332">next</a><span>|</span><label class="collapse" for="c-37251410">[-]</label><label class="expand" for="c-37251410">[4 more]</label></div><br/><div class="children"><div class="content">What kind of cpu&#x2F;gpu power do you need for quantization or these new gguf formats ?</div><br/><div id="37251512" class="c"><input type="checkbox" id="c-37251512" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251410">parent</a><span>|</span><a href="#37258954">next</a><span>|</span><label class="collapse" for="c-37251512">[-]</label><label class="expand" for="c-37251512">[1 more]</label></div><br/><div class="children"><div class="content">I haven’t quantized these myself since TheBloke has been the main provider for all the quantized models. But when I did a 8 bit quantization to see how it compares to the transformers library load_in_8bit 4 months ago(?), it didn’t use my GPU but loaded each shard into the RAM during the conversion. I had an old 4C&#x2F;8T CPU and the conversion took like 30 mins for a 13B.</div><br/></div></div><div id="37258954" class="c"><input type="checkbox" id="c-37258954" checked=""/><div class="controls bullet"><span class="by">selfhoster11</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251410">parent</a><span>|</span><a href="#37251512">prev</a><span>|</span><a href="#37252545">next</a><span>|</span><label class="collapse" for="c-37258954">[-]</label><label class="expand" for="c-37258954">[1 more]</label></div><br/><div class="children"><div class="content">I can quantize models up to 70B just fine with around 40-50 GB of system RAM, using the GGMLv3 format.<p>GGUF seems not optimised yet, since quantizing with a newer version of llama.cpp supporting the format fails on the same hardware. I expect that to be fixed shortly.<p>For inference, I understand that the hardware requirements will be identical as before.</div><br/></div></div><div id="37252545" class="c"><input type="checkbox" id="c-37252545" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251410">parent</a><span>|</span><a href="#37258954">prev</a><span>|</span><a href="#37252332">next</a><span>|</span><label class="collapse" for="c-37252545">[-]</label><label class="expand" for="c-37252545">[1 more]</label></div><br/><div class="children"><div class="content">i run llama2 13B models with 4-6 k-quantized oin a 3060 with 12Gb VRam</div><br/></div></div></div></div><div id="37252332" class="c"><input type="checkbox" id="c-37252332" checked=""/><div class="controls bullet"><span class="by">UncleOxidant</span><span>|</span><a href="#37250926">parent</a><span>|</span><a href="#37251410">prev</a><span>|</span><a href="#37251256">next</a><span>|</span><label class="collapse" for="c-37252332">[-]</label><label class="expand" for="c-37252332">[2 more]</label></div><br/><div class="children"><div class="content">If I don&#x27;t want to run this locally is it runnable somewhere on huggingface?</div><br/><div id="37253328" class="c"><input type="checkbox" id="c-37253328" checked=""/><div class="controls bullet"><span class="by">emporas</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37252332">parent</a><span>|</span><a href="#37251256">next</a><span>|</span><label class="collapse" for="c-37253328">[-]</label><label class="expand" for="c-37253328">[1 more]</label></div><br/><div class="children"><div class="content">Replicate has already hosted Llama2 13B, the chat version. My guess is, in a short span of days or weeks they will host the code version too. They charge a dollar for 2000 generations if i am not mistaken.<p><a href="https:&#x2F;&#x2F;replicate.com&#x2F;a16z-infra&#x2F;llama-2-13b-chat">https:&#x2F;&#x2F;replicate.com&#x2F;a16z-infra&#x2F;llama-2-13b-chat</a></div><br/></div></div></div></div><div id="37251256" class="c"><input type="checkbox" id="c-37251256" checked=""/><div class="controls bullet"><span class="by">suyash</span><span>|</span><a href="#37250926">parent</a><span>|</span><a href="#37252332">prev</a><span>|</span><a href="#37253428">next</a><span>|</span><label class="collapse" for="c-37251256">[-]</label><label class="expand" for="c-37251256">[3 more]</label></div><br/><div class="children"><div class="content">can it be quantised further so it can run locally on a normal laptop of a developer?</div><br/><div id="37251403" class="c"><input type="checkbox" id="c-37251403" checked=""/><div class="controls bullet"><span class="by">syntaxing</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251256">parent</a><span>|</span><a href="#37258028">next</a><span>|</span><label class="collapse" for="c-37251403">[-]</label><label class="expand" for="c-37251403">[1 more]</label></div><br/><div class="children"><div class="content">“Normal laptop” is kind of hard to gauge but if you have a M series MacBook with 16GB+ RAM, you will be able to run 7B comfortably and 13B but stretching your RAM (cause of the unified RAM) at 4 bit quantization. These go all the way down to 2 bit but I personally I find the model noticeably deteriorate anything below 4 bit. You can see how much (V)RAM you need here [1].<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp#quantization">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp#quantization</a></div><br/></div></div><div id="37258028" class="c"><input type="checkbox" id="c-37258028" checked=""/><div class="controls bullet"><span class="by">totallywrong</span><span>|</span><a href="#37250926">root</a><span>|</span><a href="#37251256">parent</a><span>|</span><a href="#37251403">prev</a><span>|</span><a href="#37253428">next</a><span>|</span><label class="collapse" for="c-37258028">[-]</label><label class="expand" for="c-37258028">[1 more]</label></div><br/><div class="children"><div class="content">Just started playing with this, there&#x27;s a tool called ollama that runs Llama2 13B on my 16GB M1 Pro really smoothly with zero config.</div><br/></div></div></div></div></div></div><div id="37250704" class="c"><input type="checkbox" id="c-37250704" checked=""/><div class="controls bullet"><span class="by">Palmik</span><span>|</span><a href="#37250926">prev</a><span>|</span><a href="#37248924">next</a><span>|</span><label class="collapse" for="c-37250704">[-]</label><label class="expand" for="c-37250704">[2 more]</label></div><br/><div class="children"><div class="content">The best model, Unnatural Code Llama, is not released. Likely because it&#x27;s trained on GPT4 based data, and might violate OpenAI TOS, because as per the &quot;Unnatural&quot; paper [1], the &quot;unnatural&quot; data is generated with the help of some LLM -- and you would want to use as good of an LLM as possible.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2212.09689.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2212.09689.pdf</a></div><br/><div id="37250808" class="c"><input type="checkbox" id="c-37250808" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37250704">parent</a><span>|</span><a href="#37248924">next</a><span>|</span><label class="collapse" for="c-37250808">[-]</label><label class="expand" for="c-37250808">[1 more]</label></div><br/><div class="children"><div class="content">The good thing is that if it&#x27;s only finetuned on 15k instructions, we should see a community made model like that very soon.</div><br/></div></div></div></div><div id="37248924" class="c"><input type="checkbox" id="c-37248924" checked=""/><div class="controls bullet"><span class="by">reacharavindh</span><span>|</span><a href="#37250704">prev</a><span>|</span><a href="#37249394">next</a><span>|</span><label class="collapse" for="c-37248924">[-]</label><label class="expand" for="c-37248924">[7 more]</label></div><br/><div class="children"><div class="content">Code llama Python is very interesting. Specifically tuned for Python.<p>I wonder if we could make such specific LLMs (one that is proficient in all things Rust, another- all things Linux, all things genomics, all things physics modeling etc) and have them talk to each other to collaboratively solve problems.<p>That would be a crazy future thing! Putting machines truly to work..</div><br/><div id="37249213" class="c"><input type="checkbox" id="c-37249213" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#37248924">parent</a><span>|</span><a href="#37249120">next</a><span>|</span><label class="collapse" for="c-37249213">[-]</label><label class="expand" for="c-37249213">[2 more]</label></div><br/><div class="children"><div class="content">I think this is called &quot;mixture of experts&quot; and also there&#x27;s a lot of speculation that it&#x27;s how GPT-4 works, although probably with just a few large models rather than many small ones.</div><br/><div id="37250664" class="c"><input type="checkbox" id="c-37250664" checked=""/><div class="controls bullet"><span class="by">jmiskovic</span><span>|</span><a href="#37248924">root</a><span>|</span><a href="#37249213">parent</a><span>|</span><a href="#37249120">next</a><span>|</span><label class="collapse" for="c-37250664">[-]</label><label class="expand" for="c-37250664">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been confirmed by multiple (unofficial) sources that GPT-4 is 8 models, each 220B parameters. Another rumor is GPT-4 being 16x111B models.<p>There&#x27;s a quite fresh and active project replicating something similar with herd of llamas: <a href="https:&#x2F;&#x2F;github.com&#x2F;jondurbin&#x2F;airoboros">https:&#x2F;&#x2F;github.com&#x2F;jondurbin&#x2F;airoboros</a></div><br/></div></div></div></div><div id="37249120" class="c"><input type="checkbox" id="c-37249120" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37248924">parent</a><span>|</span><a href="#37249213">prev</a><span>|</span><a href="#37253918">next</a><span>|</span><label class="collapse" for="c-37249120">[-]</label><label class="expand" for="c-37249120">[1 more]</label></div><br/><div class="children"><div class="content">If you can find a large body of good, permissively licensed example code, you can finetune an LLM on it!<p>There was a similar attempt for Godot script trained a few months ago, and its reportedly pretty good:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;minosvasilias&#x2F;godot-dodo">https:&#x2F;&#x2F;github.com&#x2F;minosvasilias&#x2F;godot-dodo</a><p>I think more attempts havent been made because base llama is not that great at coding in general, relative to its other strengths, and stuff like Starcoder has flown under the radar.</div><br/></div></div><div id="37253918" class="c"><input type="checkbox" id="c-37253918" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#37248924">parent</a><span>|</span><a href="#37249120">prev</a><span>|</span><a href="#37249143">next</a><span>|</span><label class="collapse" for="c-37253918">[-]</label><label class="expand" for="c-37253918">[1 more]</label></div><br/><div class="children"><div class="content">Start with a CodeLlama for C, and start treating these systems as natural language compilers. C is low level enough and still readable for those rare moments</div><br/></div></div><div id="37250246" class="c"><input type="checkbox" id="c-37250246" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#37248924">parent</a><span>|</span><a href="#37249143">prev</a><span>|</span><a href="#37249394">next</a><span>|</span><label class="collapse" for="c-37250246">[-]</label><label class="expand" for="c-37250246">[1 more]</label></div><br/><div class="children"><div class="content">Mark my words: you’ve caught a glimpse of the near future :). Google “Society of Mind” if you’re not yet familiar</div><br/></div></div></div></div><div id="37249394" class="c"><input type="checkbox" id="c-37249394" checked=""/><div class="controls bullet"><span class="by">benvolio</span><span>|</span><a href="#37248924">prev</a><span>|</span><a href="#37251468">next</a><span>|</span><label class="collapse" for="c-37249394">[-]</label><label class="expand" for="c-37249394">[18 more]</label></div><br/><div class="children"><div class="content">&gt;The Code Llama models provide stable generations with up to 100,000 tokens of context.<p>Not a bad context window, but makes me wonder how embedded code models would pick that context when dealing with a codebase larger than 100K tokens.<p>And this makes me further wonder if, when coding with such a tool (or at least a knowledge that they’re becoming more widely used and leaned on), are there some new considerations that we should be applying (or at least starting to think about) when programming? Perhaps having more or fewer comments, perhaps more terse and less readable code that would consume fewer tokens, perhaps different file structures, or even more deliberate naming conventions (like Hungarian notation but for code models) to facilitate searching or token pattern matching of some kind. Ultimately, in what ways could (or should) we adapt to make the most of these tools?</div><br/><div id="37249694" class="c"><input type="checkbox" id="c-37249694" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#37249394">parent</a><span>|</span><a href="#37249730">next</a><span>|</span><label class="collapse" for="c-37249694">[-]</label><label class="expand" for="c-37249694">[3 more]</label></div><br/><div class="children"><div class="content">That seems daft.<p>You can, I suppose, contract your code so that it’s context free and uses less tokens, but that makes it more confusing for humans <i>and language models</i>.<p>Taken to the extreme, you can see obviously with one letter functions and variables like i, j, k the model will be able to infer literally nothing and, thus, produce arbitrary nonsense.<p>Clearly the solution is to do what we <i>already</i> do to manage complexity which is to decompose large tasks into smaller black box modules with an api where the (large number of tokens) implementation is hidden and not known or relevant to using it.<p>If you give an LLM a function signature and good description, maybe some usage examples, it doesn’t need the implementation to use it.<p>Terseness <i>decreases</i> the ability of LLMs to process code; it doesn’t solve context length, and even at best it doesn’t scale.<p>100k tokens is plenty.<p>You don’t need to do anything like that.</div><br/><div id="37257805" class="c"><input type="checkbox" id="c-37257805" checked=""/><div class="controls bullet"><span class="by">sicariusnoctis</span><span>|</span><a href="#37249394">root</a><span>|</span><a href="#37249694">parent</a><span>|</span><a href="#37253858">next</a><span>|</span><label class="collapse" for="c-37257805">[-]</label><label class="expand" for="c-37257805">[1 more]</label></div><br/><div class="children"><div class="content">64k tokens ought to be enough for anybody.</div><br/></div></div><div id="37253858" class="c"><input type="checkbox" id="c-37253858" checked=""/><div class="controls bullet"><span class="by">emporas</span><span>|</span><a href="#37249394">root</a><span>|</span><a href="#37249694">parent</a><span>|</span><a href="#37257805">prev</a><span>|</span><a href="#37249730">next</a><span>|</span><label class="collapse" for="c-37253858">[-]</label><label class="expand" for="c-37253858">[1 more]</label></div><br/><div class="children"><div class="content">The process of decomposing the task into smaller steps and generate each step independently seems to be the correct way in my experience too. It works very well with GPT (chatGPT or GPT4).<p>&gt;100k tokens is plenty.<p>The context window can be really helpful, in case there is a release of a new library and the user wants to generate code targeting the API of the library. When the training date stops at August 2023, any library released after that date is not known to the engine.<p>My general opinion in regards to context window, is that 1 trillion tokens context window still may not be enough for all use cases.</div><br/></div></div></div></div><div id="37249730" class="c"><input type="checkbox" id="c-37249730" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#37249394">parent</a><span>|</span><a href="#37249694">prev</a><span>|</span><a href="#37249979">next</a><span>|</span><label class="collapse" for="c-37249730">[-]</label><label class="expand" for="c-37249730">[1 more]</label></div><br/><div class="children"><div class="content">Your developer tool already maps out the entire code base in useful ways, such as knowing all the symbols available in the current context and the structure of classes. This information can be distilled for presentation to the LLM. For instance, if you’re wanting to generate a method implementation inside a C++ class, the LLM can be given a condensed version of the header files that the compiler would have access to on compiling that specific class. Removing white space and comments and boiling macros down saves a lot of tokens.<p>You can also probably skip including standard library headers since those will be well known to the LLM through its fine tuning.<p>Either way, consider that a typical preprocessed C++ file would push against the 100K limit even with some optimizations. You will definitely want to have some middleware doing additional refinement before presenting that file to the LLM.</div><br/></div></div><div id="37249979" class="c"><input type="checkbox" id="c-37249979" checked=""/><div class="controls bullet"><span class="by">roughly</span><span>|</span><a href="#37249394">parent</a><span>|</span><a href="#37249730">prev</a><span>|</span><a href="#37252895">next</a><span>|</span><label class="collapse" for="c-37249979">[-]</label><label class="expand" for="c-37249979">[3 more]</label></div><br/><div class="children"><div class="content">I’ve found the utility of the coding LLMs gets a lot higher when you’ve got code comments and descriptive variable and function names - the LLM makes better inferences and suggestions. We’ve seen similar on data - properly tagged data and descriptive field names helps the LLM to produce much more useful responses. I’m secretly hoping the spread of these tools will finally lead my fellow developers to comment their code and stop using three character variable names.</div><br/><div id="37250070" class="c"><input type="checkbox" id="c-37250070" checked=""/><div class="controls bullet"><span class="by">GreedClarifies</span><span>|</span><a href="#37249394">root</a><span>|</span><a href="#37249979">parent</a><span>|</span><a href="#37252895">next</a><span>|</span><label class="collapse" for="c-37250070">[-]</label><label class="expand" for="c-37250070">[2 more]</label></div><br/><div class="children"><div class="content">Commenting the code in this manner sounds like a job for an LLM, maybe with human assistance in the short run.</div><br/><div id="37250211" class="c"><input type="checkbox" id="c-37250211" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#37249394">root</a><span>|</span><a href="#37250070">parent</a><span>|</span><a href="#37252895">next</a><span>|</span><label class="collapse" for="c-37250211">[-]</label><label class="expand" for="c-37250211">[1 more]</label></div><br/><div class="children"><div class="content">This is my ultimate (short term) AI fear - letting it get into a feedback loop with itself, leading to perverse and incorrect results.<p>To state my position more clearly: I don’t think an AI could comment code from scratch very well - how would it know all the decisions made, business logic considerations, historical conventions, micro-industry standards, etc?<p>A good benchmark I was told once was “if a human expert couldn’t do it, an AI probably can’t either”. And commenting code I didn’t write would certainly test the bounds of my abilities</div><br/></div></div></div></div></div></div><div id="37252895" class="c"><input type="checkbox" id="c-37252895" checked=""/><div class="controls bullet"><span class="by">gonzan</span><span>|</span><a href="#37249394">parent</a><span>|</span><a href="#37249979">prev</a><span>|</span><a href="#37249482">next</a><span>|</span><label class="collapse" for="c-37252895">[-]</label><label class="expand" for="c-37252895">[1 more]</label></div><br/><div class="children"><div class="content">I built a VS code extension a while back that I still use that wraps GPT-4 and writes code directly in my editor.<p>The method I used to choose which files to feed GPT-4 was embeddings-based. I got an embedding for each file and then an embedding from the instruction + some simple processing to pick the files more likely to be relevant. It isn&#x27;t perfect but good enough most of the time in medium-sized codebases (not very large ones).<p>The one thing I started doing because of how I implemented this is make files shorter and move stuff into different files. Having a 1k+ LOC file is prohibitive because it eats up all the context window (although with 100k context window maybe less so). I think it&#x27;s a good idea to keep files short anyways.<p>There&#x27;s other smarter things that can be done (like embed and pass individual functions&#x2F;classes instead of entire files) so I have no doubt someone will build something smarter soon. You&#x27;ll likely not have to change your coding patterns at all to make use of AI.</div><br/></div></div><div id="37249482" class="c"><input type="checkbox" id="c-37249482" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37249394">parent</a><span>|</span><a href="#37252895">prev</a><span>|</span><a href="#37258691">next</a><span>|</span><label class="collapse" for="c-37249482">[-]</label><label class="expand" for="c-37249482">[4 more]</label></div><br/><div class="children"><div class="content">This sounds like a job for middleware. Condensing split code into a single huge file, shortening comments, removing whitespace and such can be done by a preprocessor for the llm.</div><br/><div id="37249680" class="c"><input type="checkbox" id="c-37249680" checked=""/><div class="controls bullet"><span class="by">gabereiser</span><span>|</span><a href="#37249394">root</a><span>|</span><a href="#37249482">parent</a><span>|</span><a href="#37258691">next</a><span>|</span><label class="collapse" for="c-37249680">[-]</label><label class="expand" for="c-37249680">[3 more]</label></div><br/><div class="children"><div class="content">So now we need an llmpack like we did webpack? Could it be smart enough to truncate comments, white space, etc?</div><br/><div id="37249794" class="c"><input type="checkbox" id="c-37249794" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37249394">root</a><span>|</span><a href="#37249680">parent</a><span>|</span><a href="#37258691">next</a><span>|</span><label class="collapse" for="c-37249794">[-]</label><label class="expand" for="c-37249794">[2 more]</label></div><br/><div class="children"><div class="content">You dont even need an llm for trimming whitespace, just a smart parser with language rules like ide code checkers already use. Existing llms are fine at summarizing comments, especially with language specific grammar constraints.</div><br/><div id="37252381" class="c"><input type="checkbox" id="c-37252381" checked=""/><div class="controls bullet"><span class="by">gabereiser</span><span>|</span><a href="#37249394">root</a><span>|</span><a href="#37249794">parent</a><span>|</span><a href="#37258691">next</a><span>|</span><label class="collapse" for="c-37252381">[-]</label><label class="expand" for="c-37252381">[1 more]</label></div><br/><div class="children"><div class="content">My point. We don’t need the middleware.</div><br/></div></div></div></div></div></div></div></div><div id="37258691" class="c"><input type="checkbox" id="c-37258691" checked=""/><div class="controls bullet"><span class="by">rawrawrawrr</span><span>|</span><a href="#37249394">parent</a><span>|</span><a href="#37249482">prev</a><span>|</span><a href="#37250966">next</a><span>|</span><label class="collapse" for="c-37258691">[-]</label><label class="expand" for="c-37258691">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Not a bad context<p>A little understated, this is state of the art. GPT-4 only offers 32k.</div><br/></div></div><div id="37250966" class="c"><input type="checkbox" id="c-37250966" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#37249394">parent</a><span>|</span><a href="#37258691">prev</a><span>|</span><a href="#37249781">next</a><span>|</span><label class="collapse" for="c-37250966">[-]</label><label class="expand" for="c-37250966">[1 more]</label></div><br/><div class="children"><div class="content">A good practice is to have a prompt file where you keep the information you want the model to have at its disposal.  Then you put it in the start of your conversations with GPT-4. It&#x27;s also good documentation for people.<p>You start a project by defining the task. Then as you  iterate, you can add new information to the  prompt. But it can be also partially automated - the model can have a view of the file structure, classes, routes, assets and latest errors.<p>I was really hoping that the one year update of Codex would be that - a LLM that can see deep into the project, not just code, but runtime execution, debugging, inspecting and monitoring. Something that can iterate like autoGPT. Unfortunately it didn&#x27;t improve much and has weird conflicts with the native code completion in VSCode, you get freezes or  doubled brackets.</div><br/></div></div><div id="37249781" class="c"><input type="checkbox" id="c-37249781" checked=""/><div class="controls bullet"><span class="by">adamgordonbell</span><span>|</span><a href="#37249394">parent</a><span>|</span><a href="#37250966">prev</a><span>|</span><a href="#37251468">next</a><span>|</span><label class="collapse" for="c-37249781">[-]</label><label class="expand" for="c-37249781">[3 more]</label></div><br/><div class="children"><div class="content">Solutions exist that feed LLMS ctags, and seem to work well. The function signatures and symbols names for a code base are much smaller than the actual code.</div><br/><div id="37257352" class="c"><input type="checkbox" id="c-37257352" checked=""/><div class="controls bullet"><span class="by">sean_flanigan</span><span>|</span><a href="#37249394">root</a><span>|</span><a href="#37249781">parent</a><span>|</span><a href="#37251468">next</a><span>|</span><label class="collapse" for="c-37257352">[-]</label><label class="expand" for="c-37257352">[2 more]</label></div><br/><div class="children"><div class="content">I know about <a href="https:&#x2F;&#x2F;github.com&#x2F;paul-gauthier&#x2F;aider">https:&#x2F;&#x2F;github.com&#x2F;paul-gauthier&#x2F;aider</a>. Have you got a link to any others?</div><br/><div id="37259079" class="c"><input type="checkbox" id="c-37259079" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#37249394">root</a><span>|</span><a href="#37257352">parent</a><span>|</span><a href="#37251468">next</a><span>|</span><label class="collapse" for="c-37259079">[-]</label><label class="expand" for="c-37259079">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using this right now, but it&#x27;s noted that &quot;ctags only work with GPT4&quot; so I&#x27;m yet to get them working with llama locally.</div><br/></div></div></div></div></div></div></div></div><div id="37251468" class="c"><input type="checkbox" id="c-37251468" checked=""/><div class="controls bullet"><span class="by">jmorgan</span><span>|</span><a href="#37249394">prev</a><span>|</span><a href="#37250370">next</a><span>|</span><label class="collapse" for="c-37251468">[-]</label><label class="expand" for="c-37251468">[1 more]</label></div><br/><div class="children"><div class="content">To run Code Llama locally, the 7B parameter quantized version can be downloaded and run with the open-source tool Ollama: <a href="https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama">https:&#x2F;&#x2F;github.com&#x2F;jmorganca&#x2F;ollama</a><p><pre><code>   ollama run codellama &quot;write a python function to add two numbers&quot;
</code></pre>
More models coming soon (completion, python and more parameter counts)</div><br/></div></div><div id="37250370" class="c"><input type="checkbox" id="c-37250370" checked=""/><div class="controls bullet"><span class="by">lordnacho</span><span>|</span><a href="#37251468">prev</a><span>|</span><a href="#37250772">next</a><span>|</span><label class="collapse" for="c-37250370">[-]</label><label class="expand" for="c-37250370">[17 more]</label></div><br/><div class="children"><div class="content">Copilot has been working great for me thus far, but it&#x27;s limited by its interface. It seems like it only knows how to make predictions for the next bit of text.<p>Is anyone working on a code AI that can suggest refactorings?<p>&quot;You should pull these lines into a function, it&#x27;s repetitive&quot;<p>&quot;You should change this structure so it is easier to use&quot;<p>Etc</div><br/><div id="37250594" class="c"><input type="checkbox" id="c-37250594" checked=""/><div class="controls bullet"><span class="by">adocomplete</span><span>|</span><a href="#37250370">parent</a><span>|</span><a href="#37250429">next</a><span>|</span><label class="collapse" for="c-37250594">[-]</label><label class="expand" for="c-37250594">[2 more]</label></div><br/><div class="children"><div class="content">Give Cody a try! (Cody.dev)<p>With Cody you can create embeddings for your entire repo, so Cody will have much greater context about your code base and the problems you&#x27;re trying to solve.<p>Disclaimer: I just joined Sourcegraph a few weeks ago.</div><br/><div id="37254508" class="c"><input type="checkbox" id="c-37254508" checked=""/><div class="controls bullet"><span class="by">stuzenz</span><span>|</span><a href="#37250370">root</a><span>|</span><a href="#37250594">parent</a><span>|</span><a href="#37250429">next</a><span>|</span><label class="collapse" for="c-37254508">[-]</label><label class="expand" for="c-37254508">[1 more]</label></div><br/><div class="children"><div class="content">Cody is great, it had become my go-to (and I pay for Github Co-pilot).<p>With that said, they have recently changed the architecture, with the local install required, and I have not managed (yet) to get it working with NixOS. Once I have some more time, I will try again - it looks like there will be some hoops to go through. <a href="https:&#x2F;&#x2F;nixos.org&#x2F;manual&#x2F;nixpkgs&#x2F;stable&#x2F;#ssec-pkgs-appimageTools-wrapping" rel="nofollow noreferrer">https:&#x2F;&#x2F;nixos.org&#x2F;manual&#x2F;nixpkgs&#x2F;stable&#x2F;#ssec-pkgs-appimageT...</a><p>Kudos to the Source Graph team, Source Graph&#x27;s original product was nicely thought out and ahead of it&#x27;s time. Nice to see how the original product gave a nice basis for building out Cody.</div><br/></div></div></div></div><div id="37250429" class="c"><input type="checkbox" id="c-37250429" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#37250370">parent</a><span>|</span><a href="#37250594">prev</a><span>|</span><a href="#37253541">next</a><span>|</span><label class="collapse" for="c-37250429">[-]</label><label class="expand" for="c-37250429">[1 more]</label></div><br/><div class="children"><div class="content">SourceGraph Cody is going in that direction, as is Copilot Chat. But it&#x27;s still early days. I don&#x27;t think there&#x27;s anything robust here yet.</div><br/></div></div><div id="37253541" class="c"><input type="checkbox" id="c-37253541" checked=""/><div class="controls bullet"><span class="by">nvm0n2</span><span>|</span><a href="#37250370">parent</a><span>|</span><a href="#37250429">prev</a><span>|</span><a href="#37252107">next</a><span>|</span><label class="collapse" for="c-37253541">[-]</label><label class="expand" for="c-37253541">[1 more]</label></div><br/><div class="children"><div class="content">Neither of those tasks require AI. IntelliJ IDEA will happily suggest both for you today, locally. It can find large chunks of duplicated code and automatically refactor them out to functions for you. And it has many inspections that suggest refactorings to make code clearer.</div><br/></div></div><div id="37252107" class="c"><input type="checkbox" id="c-37252107" checked=""/><div class="controls bullet"><span class="by">sestinj</span><span>|</span><a href="#37250370">parent</a><span>|</span><a href="#37253541">prev</a><span>|</span><a href="#37250632">next</a><span>|</span><label class="collapse" for="c-37252107">[-]</label><label class="expand" for="c-37252107">[4 more]</label></div><br/><div class="children"><div class="content">You can use Continue for all of this, as easy as highlighting code and making the request. We also support using Code Llama: <a href="https:&#x2F;&#x2F;continue.dev&#x2F;docs&#x2F;walkthroughs&#x2F;codellama">https:&#x2F;&#x2F;continue.dev&#x2F;docs&#x2F;walkthroughs&#x2F;codellama</a></div><br/><div id="37252308" class="c"><input type="checkbox" id="c-37252308" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#37250370">root</a><span>|</span><a href="#37252107">parent</a><span>|</span><a href="#37250632">next</a><span>|</span><label class="collapse" for="c-37252308">[-]</label><label class="expand" for="c-37252308">[3 more]</label></div><br/><div class="children"><div class="content">Any plans to support IntelliJ?</div><br/><div id="37255564" class="c"><input type="checkbox" id="c-37255564" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#37250370">root</a><span>|</span><a href="#37252308">parent</a><span>|</span><a href="#37253334">next</a><span>|</span><label class="collapse" for="c-37255564">[-]</label><label class="expand" for="c-37255564">[1 more]</label></div><br/><div class="children"><div class="content">Yeah this would be a crucial feature - interoperability with Jetbrains IDEs.</div><br/></div></div><div id="37253334" class="c"><input type="checkbox" id="c-37253334" checked=""/><div class="controls bullet"><span class="by">GordonS</span><span>|</span><a href="#37250370">root</a><span>|</span><a href="#37252308">parent</a><span>|</span><a href="#37255564">prev</a><span>|</span><a href="#37250632">next</a><span>|</span><label class="collapse" for="c-37253334">[-]</label><label class="expand" for="c-37253334">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d also be really keen on this.</div><br/></div></div></div></div></div></div><div id="37250632" class="c"><input type="checkbox" id="c-37250632" checked=""/><div class="controls bullet"><span class="by">armchairhacker</span><span>|</span><a href="#37250370">parent</a><span>|</span><a href="#37252107">prev</a><span>|</span><a href="#37251540">next</a><span>|</span><label class="collapse" for="c-37250632">[-]</label><label class="expand" for="c-37250632">[1 more]</label></div><br/><div class="children"><div class="content">Copilot calls these &quot;Code Brushes&quot; <a href="https:&#x2F;&#x2F;githubnext.com&#x2F;projects&#x2F;code-brushes&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;githubnext.com&#x2F;projects&#x2F;code-brushes&#x2F;</a><p>Last I heard they are in beta and don&#x27;t work very well (even on the examples page: the &quot;add types&quot; brush is too strict, since `a` and `b` are checked for `null`, and the &quot;fix simple bug&quot; is a typo)</div><br/></div></div><div id="37251540" class="c"><input type="checkbox" id="c-37251540" checked=""/><div class="controls bullet"><span class="by">claytongrassick</span><span>|</span><a href="#37250370">parent</a><span>|</span><a href="#37250632">prev</a><span>|</span><a href="#37253329">next</a><span>|</span><label class="collapse" for="c-37251540">[-]</label><label class="expand" for="c-37251540">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using Cursor (<a href="https:&#x2F;&#x2F;www.cursor.so&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.cursor.so&#x2F;</a>) and it can do embeddings of the entire codebase, refactoring entire classes, etc. I had it rewrite a UI to add state to show one item at a time and have a selection list to the left and it executed it perfectly in MUI controls, first try.</div><br/></div></div><div id="37253329" class="c"><input type="checkbox" id="c-37253329" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#37250370">parent</a><span>|</span><a href="#37251540">prev</a><span>|</span><a href="#37250428">next</a><span>|</span><label class="collapse" for="c-37253329">[-]</label><label class="expand" for="c-37253329">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;docs.github.com&#x2F;en&#x2F;copilot&#x2F;github-copilot-chat&#x2F;using-github-copilot-chat" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.github.com&#x2F;en&#x2F;copilot&#x2F;github-copilot-chat&#x2F;using...</a> can basically do that if you&#x27;re in the beta.</div><br/></div></div><div id="37250428" class="c"><input type="checkbox" id="c-37250428" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#37250370">parent</a><span>|</span><a href="#37253329">prev</a><span>|</span><a href="#37250426">next</a><span>|</span><label class="collapse" for="c-37250428">[-]</label><label class="expand" for="c-37250428">[3 more]</label></div><br/><div class="children"><div class="content">There&#x27;s an instruct model in there, you can definitely use it for this, that&#x27;s one of the objectives.<p>An instruct model means that you can ask it to do what you want, including asking it to give you refactoring ideas from the code you will give it.</div><br/><div id="37250626" class="c"><input type="checkbox" id="c-37250626" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#37250370">root</a><span>|</span><a href="#37250428">parent</a><span>|</span><a href="#37251020">next</a><span>|</span><label class="collapse" for="c-37250626">[-]</label><label class="expand" for="c-37250626">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like what&#x27;s needed is a bit of tooling in the background consistently asking the LLM &quot;How would you improve this code?&quot; so you don&#x27;t need to actually ask it.</div><br/></div></div><div id="37251020" class="c"><input type="checkbox" id="c-37251020" checked=""/><div class="controls bullet"><span class="by">lordnacho</span><span>|</span><a href="#37250370">root</a><span>|</span><a href="#37250428">parent</a><span>|</span><a href="#37250626">prev</a><span>|</span><a href="#37250426">next</a><span>|</span><label class="collapse" for="c-37251020">[-]</label><label class="expand" for="c-37251020">[1 more]</label></div><br/><div class="children"><div class="content">How do I access it from my IDE? Jetbrains&#x2F;VSCode?</div><br/></div></div></div></div><div id="37250426" class="c"><input type="checkbox" id="c-37250426" checked=""/><div class="controls bullet"><span class="by">artificialLimbs</span><span>|</span><a href="#37250370">parent</a><span>|</span><a href="#37250428">prev</a><span>|</span><a href="#37250772">next</a><span>|</span><label class="collapse" for="c-37250426">[-]</label><label class="expand" for="c-37250426">[2 more]</label></div><br/><div class="children"><div class="content">I let mine generate whatever it likes, then add a comment below such as &quot;# Refactor the above to foo..&quot;
Works fairly well at times.</div><br/><div id="37251030" class="c"><input type="checkbox" id="c-37251030" checked=""/><div class="controls bullet"><span class="by">lordnacho</span><span>|</span><a href="#37250370">root</a><span>|</span><a href="#37250426">parent</a><span>|</span><a href="#37250772">next</a><span>|</span><label class="collapse" for="c-37251030">[-]</label><label class="expand" for="c-37251030">[1 more]</label></div><br/><div class="children"><div class="content">Can it suggest deletions? Just seems like I don&#x27;t know how to use it.</div><br/></div></div></div></div></div></div><div id="37250772" class="c"><input type="checkbox" id="c-37250772" checked=""/><div class="controls bullet"><span class="by">Draiken</span><span>|</span><a href="#37250370">prev</a><span>|</span><a href="#37249094">next</a><span>|</span><label class="collapse" for="c-37250772">[-]</label><label class="expand" for="c-37250772">[13 more]</label></div><br/><div class="children"><div class="content">As a complete noob at actually running these models, what kind of hardware are we talking here? Couldn&#x27;t pick that up from the README.<p>I absolutely love the idea of using one of these models without having to upload my source code to a tech giant.</div><br/><div id="37250840" class="c"><input type="checkbox" id="c-37250840" checked=""/><div class="controls bullet"><span class="by">dangelov</span><span>|</span><a href="#37250772">parent</a><span>|</span><a href="#37250839">next</a><span>|</span><label class="collapse" for="c-37250840">[-]</label><label class="expand" for="c-37250840">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve used Ollama to run Llama 2 (all variants) on my 2020 Intel MacBook Pro - it&#x27;s incredibly easy. You just install the app and run a couple of shell commands. I&#x27;m guessing soon-ish this model will be available too and then you&#x27;d be able to use it with the Continue VS Code extension.<p>Edited to add: Though somewhat slow, swap seems to have been a good enough replacement for not having the loads of RAM required. Ollama says &quot;32 GB to run the 13B models&quot;, but I&#x27;m running the llama2:13b model on a 16 GB MBP.</div><br/><div id="37251498" class="c"><input type="checkbox" id="c-37251498" checked=""/><div class="controls bullet"><span class="by">j45</span><span>|</span><a href="#37250772">root</a><span>|</span><a href="#37250840">parent</a><span>|</span><a href="#37250839">next</a><span>|</span><label class="collapse" for="c-37251498">[-]</label><label class="expand" for="c-37251498">[6 more]</label></div><br/><div class="children"><div class="content">Apple Silicon, especially an M1 Max Studio seems to be an interesting machine to hang on to as the models become more and more efficient with using less and less.<p>If there&#x27;s nay other opinions or thoughts on this, I&#x27;d be very happy to learn as well. I have considered the eGPU route connected to a 1L PC such as a thinkcentre m80&#x2F;90.</div><br/><div id="37254037" class="c"><input type="checkbox" id="c-37254037" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37250772">root</a><span>|</span><a href="#37251498">parent</a><span>|</span><a href="#37250839">next</a><span>|</span><label class="collapse" for="c-37254037">[-]</label><label class="expand" for="c-37254037">[5 more]</label></div><br/><div class="children"><div class="content">I have a 64 GB M1 Max MBP, and I&#x27;d say unless you really have some academic interest towards messing with open models, for now accessing SOTA models via a REST API has better latency for a given quality.<p>Claude 1.2 instant is as fast as 3.5, follows instructions at a quality closer to 4, and has a 100k context window. Hard to compete with that with an open source model right now.</div><br/><div id="37255151" class="c"><input type="checkbox" id="c-37255151" checked=""/><div class="controls bullet"><span class="by">jkeisling</span><span>|</span><a href="#37250772">root</a><span>|</span><a href="#37254037">parent</a><span>|</span><a href="#37250839">next</a><span>|</span><label class="collapse" for="c-37255151">[-]</label><label class="expand" for="c-37255151">[4 more]</label></div><br/><div class="children"><div class="content">How does open source compete with the Claude API? Easy: actually let you use the model. From the signup page:<p>&gt; Anthropic is rolling out Claude slowly and incrementally, as we work to ensure the safety and scalability of it, in alignment with our company values.<p>&gt; We&#x27;re working with select partners to roll out Claude in their products. If you&#x27;re interested in becoming one of those partners, we are accepting applications. Keep in mind that, due to the overwhelming interest we&#x27;ve received so far, we may take a while to reply.<p>No thanks, I&#x27;d much rather not wait months to see if my app deserves their oh-so-limited attention, or &quot;aligns with the values&quot; of a company taking $400m from Sam Bankman-Fried.<p>To be more charitable to your underlying point, Claude 2 is free to chat with via Anthropic&#x27;s website, Poe, or Slack, and the GPT-4 API is open to use. If you&#x27;re building a prototype or just need a chatbot, these <i>do</i> have better results and dev experience, at least for now. But I don&#x27;t think picking on your Claude API example is unfair. These companies could randomly refuse your prompts via some opaque &quot;moderation API&quot; (that all GPT fine-tuning data goes through!), train on your company&#x27;s proprietary data, spy on your most intimate questions, or just not find you worth the trouble and cut you off, at any time. THAT is why open source beats proprietary hands down: My device, my data, my weights, my own business.</div><br/><div id="37257168" class="c"><input type="checkbox" id="c-37257168" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37250772">root</a><span>|</span><a href="#37255151">parent</a><span>|</span><a href="#37250839">next</a><span>|</span><label class="collapse" for="c-37257168">[-]</label><label class="expand" for="c-37257168">[3 more]</label></div><br/><div class="children"><div class="content">Perfect example of why I said academic interest.<p>Awkward tie-ins between SBF and value systems (?) have no effect on practical usage.<p>A theoretical concern they might train on my API data after saying they won&#x27;t doesn&#x27;t either. Amazon might be training on everything not bolted down in S3, not worth wasting brain power on that.<p>The moderation API isn&#x27;t some magic gotcha, it&#x27;s documented.  They don&#x27;t want to deal with people fine tuning for porn. Maybe you have some ideological disagreement on that but it&#x27;s not of practical relevance when trying to write code.<p>At the end of the day you&#x27;re not alone in these opinions. But some of us prefer pragmatism over hype. Until someone catches OpenAI or Anthropic trying to kill their golden goose by breaking their GDPR,  HIPPA, and SOC2 certifications, I&#x27;m going to take delivered value over theoretical harm.</div><br/><div id="37257417" class="c"><input type="checkbox" id="c-37257417" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#37250772">root</a><span>|</span><a href="#37257168">parent</a><span>|</span><a href="#37250839">next</a><span>|</span><label class="collapse" for="c-37257417">[-]</label><label class="expand" for="c-37257417">[2 more]</label></div><br/><div class="children"><div class="content">In my opinion the risk is coupling accelerated intelligence to competitive business models.</div><br/><div id="37258073" class="c"><input type="checkbox" id="c-37258073" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37250772">root</a><span>|</span><a href="#37257417">parent</a><span>|</span><a href="#37250839">next</a><span>|</span><label class="collapse" for="c-37258073">[-]</label><label class="expand" for="c-37258073">[1 more]</label></div><br/><div class="children"><div class="content">The accelerated intelligence wouldn&#x27;t exist without competitive business models.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="37250839" class="c"><input type="checkbox" id="c-37250839" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#37250772">parent</a><span>|</span><a href="#37250840">prev</a><span>|</span><a href="#37250906">next</a><span>|</span><label class="collapse" for="c-37250839">[-]</label><label class="expand" for="c-37250839">[4 more]</label></div><br/><div class="children"><div class="content">34B <i>should</i> be able to run on 24GiB consumer graphics card, or 32GiB Mac (M1 &#x2F; M2 chips) with quantization (5~6bit) (and 7B <i>should</i> be able to run on your smart toaster).</div><br/><div id="37251394" class="c"><input type="checkbox" id="c-37251394" checked=""/><div class="controls bullet"><span class="by">epolanski</span><span>|</span><a href="#37250772">root</a><span>|</span><a href="#37250839">parent</a><span>|</span><a href="#37250906">next</a><span>|</span><label class="collapse" for="c-37251394">[-]</label><label class="expand" for="c-37251394">[3 more]</label></div><br/><div class="children"><div class="content">Are there cloud offerings to run those models on somebody&#x27;s else computer?<p>Any &quot;eli5&quot; tutorial on how to do so, if so?<p>I want to give these models a run but I have no powerful GPU to run them on so don&#x27;t know where to start.</div><br/><div id="37257466" class="c"><input type="checkbox" id="c-37257466" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#37250772">root</a><span>|</span><a href="#37251394">parent</a><span>|</span><a href="#37251559">next</a><span>|</span><label class="collapse" for="c-37257466">[-]</label><label class="expand" for="c-37257466">[1 more]</label></div><br/><div class="children"><div class="content">I started something here about this: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37121384">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37121384</a></div><br/></div></div><div id="37251559" class="c"><input type="checkbox" id="c-37251559" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37250772">root</a><span>|</span><a href="#37251394">parent</a><span>|</span><a href="#37257466">prev</a><span>|</span><a href="#37250906">next</a><span>|</span><label class="collapse" for="c-37251559">[-]</label><label class="expand" for="c-37251559">[1 more]</label></div><br/><div class="children"><div class="content">On runpod there is a TheBloke template with everything set up for you. An A6000 is good enough to run 70b 4bit.</div><br/></div></div></div></div></div></div><div id="37250906" class="c"><input type="checkbox" id="c-37250906" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37250772">parent</a><span>|</span><a href="#37250839">prev</a><span>|</span><a href="#37249094">next</a><span>|</span><label class="collapse" for="c-37250906">[-]</label><label class="expand" for="c-37250906">[1 more]</label></div><br/><div class="children"><div class="content">If you want to run them fast, a 12GB GPU (e.g 3060) for the 13B and a 24GB GPU for the 34B (e.g 3090). Otherwise llama.cpp CPU inference would work on most machines.</div><br/></div></div></div></div><div id="37249094" class="c"><input type="checkbox" id="c-37249094" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#37250772">prev</a><span>|</span><a href="#37249662">next</a><span>|</span><label class="collapse" for="c-37249094">[-]</label><label class="expand" for="c-37249094">[7 more]</label></div><br/><div class="children"><div class="content">Interesting that there&#x27;s a 34B model. That was missing from the original Llama 2 release. I wonder if it&#x27;s still usable for general non-code chat tasks or if the code fine tuning destroyed that. It should be the best model that would still fit on 24GB gaming GPUs with quantization, because 70B doesn&#x27;t fit.</div><br/><div id="37249293" class="c"><input type="checkbox" id="c-37249293" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37249094">parent</a><span>|</span><a href="#37250379">next</a><span>|</span><label class="collapse" for="c-37249293">[-]</label><label class="expand" for="c-37249293">[1 more]</label></div><br/><div class="children"><div class="content">Someone &quot;grafted&quot; llama 33B onto llama v2 13B to make &quot;llama 22B&quot;<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;chargoddard&#x2F;llama2-22b" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;chargoddard&#x2F;llama2-22b</a><p>Theoretically this is an even better size, as it would fit on a 20GB-24GB GPU with more relaxed quantization and much longer context.<p>Metrics are slightly below 13B, but the theory is that the higher parameter count is more amenable to finetuning. If you search for 22B on huggingface, you can see that frankenllama experiments are ongoing:<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;models?sort=modified&amp;search=22b" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;models?sort=modified&amp;search=22b</a></div><br/></div></div><div id="37250379" class="c"><input type="checkbox" id="c-37250379" checked=""/><div class="controls bullet"><span class="by">nabakin</span><span>|</span><a href="#37249094">parent</a><span>|</span><a href="#37249293">prev</a><span>|</span><a href="#37250113">next</a><span>|</span><label class="collapse" for="c-37250379">[-]</label><label class="expand" for="c-37250379">[2 more]</label></div><br/><div class="children"><div class="content">Looks like they left out another model though. In the paper they mention a &quot;Unnatural Code Llama&quot; which wipes the floor with every other model&#x2F;finetune on every benchmark except for slightly losing to Code Llama Python on MBPP pass@100 and slightly losing to GPT-4 on HumanEval pass@1 which is insane.<p>Meta says later on that they aren&#x27;t releasing it and give no explanation. I wonder why given how incredible it seems to be.</div><br/><div id="37251144" class="c"><input type="checkbox" id="c-37251144" checked=""/><div class="controls bullet"><span class="by">ImprobableTruth</span><span>|</span><a href="#37249094">root</a><span>|</span><a href="#37250379">parent</a><span>|</span><a href="#37250113">next</a><span>|</span><label class="collapse" for="c-37251144">[-]</label><label class="expand" for="c-37251144">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s &quot;unnatural&quot; because it was finetuned on generated data using another model, almost certainly gpt-4 (whose TOS forbid this).</div><br/></div></div></div></div><div id="37250113" class="c"><input type="checkbox" id="c-37250113" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37249094">parent</a><span>|</span><a href="#37250379">prev</a><span>|</span><a href="#37249662">next</a><span>|</span><label class="collapse" for="c-37250113">[-]</label><label class="expand" for="c-37250113">[3 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t imagine it being better than Llama1 33B, after all this code finetuning.</div><br/><div id="37250228" class="c"><input type="checkbox" id="c-37250228" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#37249094">root</a><span>|</span><a href="#37250113">parent</a><span>|</span><a href="#37249662">next</a><span>|</span><label class="collapse" for="c-37250228">[-]</label><label class="expand" for="c-37250228">[2 more]</label></div><br/><div class="children"><div class="content">But the license for llama 2 is a whole lot better.</div><br/><div id="37250431" class="c"><input type="checkbox" id="c-37250431" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37249094">root</a><span>|</span><a href="#37250228">parent</a><span>|</span><a href="#37249662">next</a><span>|</span><label class="collapse" for="c-37250431">[-]</label><label class="expand" for="c-37250431">[1 more]</label></div><br/><div class="children"><div class="content">Meh.<p>If you&#x27;re using it commercially you&#x27;re probably deploying it on a server where you&#x27;re not limited by the 24GB and you can just run llama 2 70b.<p>The majority of people who want to run it locally on 24GB either want roleplay (so non commercial) or code (you have codellama)</div><br/></div></div></div></div></div></div></div></div><div id="37249662" class="c"><input type="checkbox" id="c-37249662" checked=""/><div class="controls bullet"><span class="by">scriptsmith</span><span>|</span><a href="#37249094">prev</a><span>|</span><a href="#37250277">next</a><span>|</span><label class="collapse" for="c-37249662">[-]</label><label class="expand" for="c-37249662">[8 more]</label></div><br/><div class="children"><div class="content">How are people using these local code models? I would much prefer using these in-context in an editor, but most of them seem to be deployed just in an instruction context. There&#x27;s a lot of value to not having to context switch, or have a conversation.<p>I see the GitHub copilot extensions gets a new release one every few days, so is it just that the way they&#x27;re integrated is more complicated so not worth the effort?</div><br/><div id="37252065" class="c"><input type="checkbox" id="c-37252065" checked=""/><div class="controls bullet"><span class="by">sestinj</span><span>|</span><a href="#37249662">parent</a><span>|</span><a href="#37250191">next</a><span>|</span><label class="collapse" for="c-37252065">[-]</label><label class="expand" for="c-37252065">[1 more]</label></div><br/><div class="children"><div class="content">You can use Continue as a drop-in replacement for Copilot Chat with Code Llama. We&#x27;ve released a short tutorial here: <a href="https:&#x2F;&#x2F;continue.dev&#x2F;docs&#x2F;walkthroughs&#x2F;codellama">https:&#x2F;&#x2F;continue.dev&#x2F;docs&#x2F;walkthroughs&#x2F;codellama</a>. It should save you a lot of time context-switching; you can just highlight code and ask questions or make edits, all with keyboard shortcuts</div><br/></div></div><div id="37250191" class="c"><input type="checkbox" id="c-37250191" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#37249662">parent</a><span>|</span><a href="#37252065">prev</a><span>|</span><a href="#37250313">next</a><span>|</span><label class="collapse" for="c-37250191">[-]</label><label class="expand" for="c-37250191">[3 more]</label></div><br/><div class="children"><div class="content"><a href="http:&#x2F;&#x2F;cursor.sh" rel="nofollow noreferrer">http:&#x2F;&#x2F;cursor.sh</a> integrates GPT-4 into vscode in a sensible way. Just swapping this in place of GPT-4 would likely work perfectly. Has anyone cloned the OpenAI HTTP API yet?</div><br/><div id="37252566" class="c"><input type="checkbox" id="c-37252566" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#37249662">root</a><span>|</span><a href="#37250191">parent</a><span>|</span><a href="#37250436">next</a><span>|</span><label class="collapse" for="c-37252566">[-]</label><label class="expand" for="c-37252566">[1 more]</label></div><br/><div class="children"><div class="content">LocalAI <a href="https:&#x2F;&#x2F;localai.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;localai.io&#x2F;</a> and LMStudio <a href="https:&#x2F;&#x2F;lmstudio.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;lmstudio.ai&#x2F;</a> both have fairly complete OpenAI compatibility layers. llama-cpp-python has a FastAPI server as well: <a href="https:&#x2F;&#x2F;github.com&#x2F;abetlen&#x2F;llama-cpp-python&#x2F;blob&#x2F;main&#x2F;llama_cpp&#x2F;server&#x2F;app.py">https:&#x2F;&#x2F;github.com&#x2F;abetlen&#x2F;llama-cpp-python&#x2F;blob&#x2F;main&#x2F;llama_...</a> (as of this moment it hasn&#x27;t merged GGUF update yet though)</div><br/></div></div><div id="37250436" class="c"><input type="checkbox" id="c-37250436" checked=""/><div class="controls bullet"><span class="by">fudged71</span><span>|</span><a href="#37249662">root</a><span>|</span><a href="#37250191">parent</a><span>|</span><a href="#37252566">prev</a><span>|</span><a href="#37250313">next</a><span>|</span><label class="collapse" for="c-37250436">[-]</label><label class="expand" for="c-37250436">[1 more]</label></div><br/><div class="children"><div class="content">I was tasked with a massive project over the last month and I&#x27;m not sure I could have done it as fast as I have without Cursor. Also check out the Warp terminal replacement. Together it&#x27;s a winning combo!</div><br/></div></div></div></div><div id="37250313" class="c"><input type="checkbox" id="c-37250313" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#37249662">parent</a><span>|</span><a href="#37250191">prev</a><span>|</span><a href="#37250277">next</a><span>|</span><label class="collapse" for="c-37250313">[-]</label><label class="expand" for="c-37250313">[3 more]</label></div><br/><div class="children"><div class="content">For in-editor like copilot you can try this locally - <a href="https:&#x2F;&#x2F;github.com&#x2F;smallcloudai&#x2F;refact">https:&#x2F;&#x2F;github.com&#x2F;smallcloudai&#x2F;refact</a><p>This works well for me except the 15B+ don&#x27;t run fast enough on a 4090 - hopefully exllama supports non-llama models, or maybe it&#x27;ll support CodeLLaMa already I&#x27;m not sure.<p>For general chat testing&#x2F;usage this works pretty well with lots of options - 
 <a href="https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui&#x2F;</a></div><br/><div id="37250644" class="c"><input type="checkbox" id="c-37250644" checked=""/><div class="controls bullet"><span class="by">msp26</span><span>|</span><a href="#37249662">root</a><span>|</span><a href="#37250313">parent</a><span>|</span><a href="#37250277">next</a><span>|</span><label class="collapse" for="c-37250644">[-]</label><label class="expand" for="c-37250644">[2 more]</label></div><br/><div class="children"><div class="content">&gt;This works well for me except the 15B+ don&#x27;t run fast enough on a 4090<p>I assume quantized models will run a lot better. TheBloke already seems like he&#x27;s on it.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;CodeLlama-13B-fp16" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke&#x2F;CodeLlama-13B-fp16</a></div><br/><div id="37251635" class="c"><input type="checkbox" id="c-37251635" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#37249662">root</a><span>|</span><a href="#37250644">parent</a><span>|</span><a href="#37250277">next</a><span>|</span><label class="collapse" for="c-37251635">[-]</label><label class="expand" for="c-37251635">[1 more]</label></div><br/><div class="children"><div class="content">Unfortunately what I tested was StarCoder 4bit.
We really need exllama which should make even 30b viable from what I can tell.<p>Because codellama is llama based it may just work possibly?</div><br/></div></div></div></div></div></div></div></div><div id="37250277" class="c"><input type="checkbox" id="c-37250277" checked=""/><div class="controls bullet"><span class="by">mymac</span><span>|</span><a href="#37249662">prev</a><span>|</span><a href="#37249185">next</a><span>|</span><label class="collapse" for="c-37250277">[-]</label><label class="expand" for="c-37250277">[21 more]</label></div><br/><div class="children"><div class="content">Never before in the history of mankind was a group so absolutely besotted with the idea of putting themselves out of a job.</div><br/><div id="37250383" class="c"><input type="checkbox" id="c-37250383" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#37250277">parent</a><span>|</span><a href="#37255325">next</a><span>|</span><label class="collapse" for="c-37250383">[-]</label><label class="expand" for="c-37250383">[11 more]</label></div><br/><div class="children"><div class="content">That’s just one perspective… Another perspective is that LLMs enable programmers to skip a lot of the routine and boring aspects of coding - looking up stuff, essentially - so they can focus on the fun parts that engage creativity.</div><br/><div id="37259328" class="c"><input type="checkbox" id="c-37259328" checked=""/><div class="controls bullet"><span class="by">KingOfCoders</span><span>|</span><a href="#37250277">root</a><span>|</span><a href="#37250383">parent</a><span>|</span><a href="#37250412">next</a><span>|</span><label class="collapse" for="c-37259328">[-]</label><label class="expand" for="c-37259328">[1 more]</label></div><br/><div class="children"><div class="content">One coachman to the other: &quot;Another perspective about this car thing, you can skip all the routine and boring trips - they are done with cars. You can focus on the nice trips that make you feel good&quot;.</div><br/></div></div><div id="37250412" class="c"><input type="checkbox" id="c-37250412" checked=""/><div class="controls bullet"><span class="by">mymac</span><span>|</span><a href="#37250277">root</a><span>|</span><a href="#37250383">parent</a><span>|</span><a href="#37259328">prev</a><span>|</span><a href="#37255325">next</a><span>|</span><label class="collapse" for="c-37250412">[-]</label><label class="expand" for="c-37250412">[9 more]</label></div><br/><div class="children"><div class="content">But it won&#x27;t stop there. Why would it stop at some arbitrarily defined boundary? The savings associated with no longer having to pay programmers the amounts of money that they believe they are worth (high enough to result in collusion between employers) are just too tempting.</div><br/><div id="37250799" class="c"><input type="checkbox" id="c-37250799" checked=""/><div class="controls bullet"><span class="by">lsmeducation</span><span>|</span><a href="#37250277">root</a><span>|</span><a href="#37250412">parent</a><span>|</span><a href="#37250996">next</a><span>|</span><label class="collapse" for="c-37250799">[-]</label><label class="expand" for="c-37250799">[2 more]</label></div><br/><div class="children"><div class="content">Okay, think about it this way. This thing helps generate tons and tons of  code. The more code people (or this thing) writes, the more shit there is to debug. More and more code, each calling each other means more and more insane bugs.<p>We’re going to move from debugging some crap the last developer wrote to debugging an order of magnitude more code the last developer generated.<p>It’s going to be wonderful for job prospects really.</div><br/><div id="37258936" class="c"><input type="checkbox" id="c-37258936" checked=""/><div class="controls bullet"><span class="by">Sakthimm</span><span>|</span><a href="#37250277">root</a><span>|</span><a href="#37250799">parent</a><span>|</span><a href="#37250996">next</a><span>|</span><label class="collapse" for="c-37258936">[-]</label><label class="expand" for="c-37258936">[1 more]</label></div><br/><div class="children"><div class="content">Until AI figures out debugging</div><br/></div></div></div></div><div id="37250996" class="c"><input type="checkbox" id="c-37250996" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37250277">root</a><span>|</span><a href="#37250412">parent</a><span>|</span><a href="#37250799">prev</a><span>|</span><a href="#37253510">next</a><span>|</span><label class="collapse" for="c-37250996">[-]</label><label class="expand" for="c-37250996">[4 more]</label></div><br/><div class="children"><div class="content">Some form of AI will eventually take over almost all existing jobs. Whether those jobs evolve or not somehow and new jobs replace them, we will see.<p>But it&#x27;s definitely not just programmers. And it will take time.<p>Society needs to adjust. Stopping progress would not be a solution and is not possible.<p>However, hopefully we can pause before we create digital animals with hyperspeed reasoning and typical animal instincts like self-preservation. Researchers like LeCun are already moving on from things like LLMs and working on approaches that really imitate animal cognition (like humans) and will eventually blow all existing techniques out of the water.<p>The path that we are on seems to make humans obsolete within three generations or so.<p>So the long term concern is not jobs, but for humans to lose control of the planet in less than a century.<p>On the way there we might be able to manage a new golden age -- a crescendo for human civilization.</div><br/><div id="37251267" class="c"><input type="checkbox" id="c-37251267" checked=""/><div class="controls bullet"><span class="by">lsmeducation</span><span>|</span><a href="#37250277">root</a><span>|</span><a href="#37250996">parent</a><span>|</span><a href="#37253510">next</a><span>|</span><label class="collapse" for="c-37251267">[-]</label><label class="expand" for="c-37251267">[3 more]</label></div><br/><div class="children"><div class="content">Continuing your aside…<p>Humans don’t become obsolete, we become bored. This tech will make us bored. When humans get too bored and need shit to stir up, we’ll start a war. Take US and China, global prosperity is not enough right? We need to stoke the flames of war over Taiwan.<p>In the next 300 years we’ll wipe out most of each other in some ridiculous war, and then rebuild.</div><br/><div id="37251449" class="c"><input type="checkbox" id="c-37251449" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37250277">root</a><span>|</span><a href="#37251267">parent</a><span>|</span><a href="#37253510">next</a><span>|</span><label class="collapse" for="c-37251449">[-]</label><label class="expand" for="c-37251449">[2 more]</label></div><br/><div class="children"><div class="content">I agree that WWIII is a concern but I don&#x27;t think it will be brought about by boredom.<p>&quot;Global prosperity&quot; might be true in a very long-term historical sense, but it&#x27;s misleading to apply it to the immediate situation.<p>Taiwan is not just a talking point. Control over Taiwan is critical for maintaining hegemony. When that is no longer assured, there will likely be a bloody battle before China is given the free reign that it desires.<p>WWIII is likely to fully break out within the next 3-30 years. We don&#x27;t really have the facilities to imagine what 300 years from now will look like, but it will likely be posthuman.</div><br/><div id="37251653" class="c"><input type="checkbox" id="c-37251653" checked=""/><div class="controls bullet"><span class="by">lsmeducation</span><span>|</span><a href="#37250277">root</a><span>|</span><a href="#37251449">parent</a><span>|</span><a href="#37253510">next</a><span>|</span><label class="collapse" for="c-37251653">[-]</label><label class="expand" for="c-37251653">[1 more]</label></div><br/><div class="children"><div class="content">I’ll go with the 30 year mark. Countries like Russia or China don’t get humbled in a loss (like Germany didn’t in WW1). Russia will negotiate some terms for Ukraine (or maintain perpetual war), but I believe it will become a military state that will funnel all money into the defense sector. The same with Iran, and the same with China.<p>Iran supplies Russia with drones. I can promise you Russia will help Iran enrich their uranium. They are both pariah states, what do they have to lose? Nuclear Iran, here enters Israel.<p>Everyone’s arming up, there’s a gun fight coming.</div><br/></div></div></div></div></div></div></div></div><div id="37253510" class="c"><input type="checkbox" id="c-37253510" checked=""/><div class="controls bullet"><span class="by">swader999</span><span>|</span><a href="#37250277">root</a><span>|</span><a href="#37250412">parent</a><span>|</span><a href="#37250996">prev</a><span>|</span><a href="#37255325">next</a><span>|</span><label class="collapse" for="c-37253510">[-]</label><label class="expand" for="c-37253510">[2 more]</label></div><br/><div class="children"><div class="content">The answer to AI stealing your job is to go ahead and start a company, solve a hard problem, sell the solution and leverage AI to do this.</div><br/><div id="37258090" class="c"><input type="checkbox" id="c-37258090" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37250277">root</a><span>|</span><a href="#37253510">parent</a><span>|</span><a href="#37255325">next</a><span>|</span><label class="collapse" for="c-37258090">[-]</label><label class="expand" for="c-37258090">[1 more]</label></div><br/><div class="children"><div class="content">The only thing that takes anyone&#x27;s job is demand shortfalls. Productivity increases certainly don&#x27;t do it. It&#x27;s like saying getting a raise makes you poorer.</div><br/></div></div></div></div></div></div></div></div><div id="37255325" class="c"><input type="checkbox" id="c-37255325" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37250277">parent</a><span>|</span><a href="#37250383">prev</a><span>|</span><a href="#37251790">next</a><span>|</span><label class="collapse" for="c-37255325">[-]</label><label class="expand" for="c-37255325">[1 more]</label></div><br/><div class="children"><div class="content">The best interpretation of this is you mean eventually ML&#x2F;AI will put programmers out of a job, and not Code LLama specifically.<p>However it is hard to tell how that might pan out. Can such an ML&#x2F;AI do all the parts of the job effectively? A lot of non-coding skill bleed into the coder&#x27;s job. For example talking to people who need an input to the task and finding out what they are really asking for, and beyond that, what the best solution is that solves the underlying problem of what they ask for, while meeting nonfunctional  requirements such as performance, reliability, code complexity, and is a good fit for the business.<p>On the other hand eventually the end users of a lot of services might be bots. You are more likely to have a pricing.json than a pricing.html page, and bots discover the services they need from searches, negotiate deals, read contracts and sue each other etc.<p>Once the programming job (which is really a &quot;technical problem solver&quot; job) is replaced either it will just be same-but-different (like how most programmers use high level languages not C) or we have invented AGI that will take many other jobs.<p>In which case the &quot;job&quot; aspect of it is almost moot. Since we will be living in post-scarcity and you would need to figure out the &quot;power&quot; aspect and what it means to even be sentient&#x2F;human.</div><br/></div></div><div id="37251790" class="c"><input type="checkbox" id="c-37251790" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#37250277">parent</a><span>|</span><a href="#37255325">prev</a><span>|</span><a href="#37252661">next</a><span>|</span><label class="collapse" for="c-37251790">[-]</label><label class="expand" for="c-37251790">[1 more]</label></div><br/><div class="children"><div class="content">Is automation not what every engineer strives for when possible? Especially software developers.<p>From my experience with github copilot and GPT4 - developers are NOT going anywhere anytime soon. You&#x27;ll certainly be faster though.</div><br/></div></div><div id="37252661" class="c"><input type="checkbox" id="c-37252661" checked=""/><div class="controls bullet"><span class="by">037</span><span>|</span><a href="#37250277">parent</a><span>|</span><a href="#37251790">prev</a><span>|</span><a href="#37255636">next</a><span>|</span><label class="collapse" for="c-37252661">[-]</label><label class="expand" for="c-37252661">[1 more]</label></div><br/><div class="children"><div class="content">I understand the fear of losing your job or becoming less relevant, but many of us love this work because we&#x27;re passionate about technology, programming, science, and the whole world of possibilities that this makes... possible.<p>That&#x27;s why we&#x27;re so excited to see these extraordinary advances that I personally didn&#x27;t think I&#x27;d see in my lifetime.<p>The fear is legitimate and I respect the opinions of those who oppose these advances because they have children to provide for and have worked a lifetime to get where they are. But at least in my case, the curiosity and excitement to see what will happen is far greater than my little personal garden.
Damn, we are living what we used to read in the most entertaining sci-fi literature!<p>(And that&#x27;s not to say that I don&#x27;t see the risks in all of this... in fact, I think there will be consequences far more serious than just &quot;losing a job,&quot; but I could be wrong)</div><br/></div></div><div id="37255636" class="c"><input type="checkbox" id="c-37255636" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#37250277">parent</a><span>|</span><a href="#37252661">prev</a><span>|</span><a href="#37258084">next</a><span>|</span><label class="collapse" for="c-37255636">[-]</label><label class="expand" for="c-37255636">[1 more]</label></div><br/><div class="children"><div class="content">If we get to the point where these large language models can create complete applications and software solutions from design specs alone, then there&#x27;s no reason to believe that this would be limited to merely replacing software devs.<p>It would likely impact a <i>far</i> larger swath of the engineering &#x2F; design industry.</div><br/></div></div><div id="37258084" class="c"><input type="checkbox" id="c-37258084" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37250277">parent</a><span>|</span><a href="#37255636">prev</a><span>|</span><a href="#37252999">next</a><span>|</span><label class="collapse" for="c-37258084">[-]</label><label class="expand" for="c-37258084">[1 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t get promoted unless you put yourself out of a job.</div><br/></div></div><div id="37252999" class="c"><input type="checkbox" id="c-37252999" checked=""/><div class="controls bullet"><span class="by">PUSH_AX</span><span>|</span><a href="#37250277">parent</a><span>|</span><a href="#37258084">prev</a><span>|</span><a href="#37252523">next</a><span>|</span><label class="collapse" for="c-37252999">[-]</label><label class="expand" for="c-37252999">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re not looking at a product that&#x27;s putting anyone out of a job though, we&#x27;re looking at a product that frees up a lot of time, and time is great.</div><br/></div></div><div id="37252523" class="c"><input type="checkbox" id="c-37252523" checked=""/><div class="controls bullet"><span class="by">kbrannigan</span><span>|</span><a href="#37250277">parent</a><span>|</span><a href="#37252999">prev</a><span>|</span><a href="#37251095">next</a><span>|</span><label class="collapse" for="c-37252523">[-]</label><label class="expand" for="c-37252523">[1 more]</label></div><br/><div class="children"><div class="content">Do you really want to spend you days writing REDUX accumulators?</div><br/></div></div><div id="37251095" class="c"><input type="checkbox" id="c-37251095" checked=""/><div class="controls bullet"><span class="by">yborg</span><span>|</span><a href="#37250277">parent</a><span>|</span><a href="#37252523">prev</a><span>|</span><a href="#37253671">next</a><span>|</span><label class="collapse" for="c-37251095">[-]</label><label class="expand" for="c-37251095">[1 more]</label></div><br/><div class="children"><div class="content">When mechanized textile machinery was invented, the weavers that had jobs after their introduction were those that learned how to use them.</div><br/></div></div><div id="37253671" class="c"><input type="checkbox" id="c-37253671" checked=""/><div class="controls bullet"><span class="by">worksonmine</span><span>|</span><a href="#37250277">parent</a><span>|</span><a href="#37251095">prev</a><span>|</span><a href="#37249185">next</a><span>|</span><label class="collapse" for="c-37253671">[-]</label><label class="expand" for="c-37253671">[1 more]</label></div><br/><div class="children"><div class="content">This should be the only goal of mankind so we can smell the flowers instead of wasting our years in some cubicle. Some people will always want to work, but it shouldn&#x27;t be the norm. What&#x27;s the point really unless we&#x27;re doing something we&#x27;re passionate about? The economy?</div><br/></div></div></div></div><div id="37249185" class="c"><input type="checkbox" id="c-37249185" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37250277">prev</a><span>|</span><a href="#37254502">next</a><span>|</span><label class="collapse" for="c-37249185">[-]</label><label class="expand" for="c-37249185">[4 more]</label></div><br/><div class="children"><div class="content">Between this, ideogram.ai (image generator which can spell, from former Google Imagen team member and others), and ChatGPT fine-tuning, this has been a truly epic week.<p>I would argue that many teams will have to reevaluate their LLM strategy _again_ for the second time in a week.</div><br/><div id="37258106" class="c"><input type="checkbox" id="c-37258106" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37249185">parent</a><span>|</span><a href="#37253642">next</a><span>|</span><label class="collapse" for="c-37258106">[-]</label><label class="expand" for="c-37258106">[1 more]</label></div><br/><div class="children"><div class="content">SDXL and DeepFloyd can spell. It&#x27;s more or less just a matter of having a good enough text encoder.<p>I tried Ideogram yesterday and it felt too much like existing generators (base SD and Midjourney). DALLE2 actually has some interestingly different outputs, the problem is they never update it or fix the bad image quality.</div><br/></div></div><div id="37253642" class="c"><input type="checkbox" id="c-37253642" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#37249185">parent</a><span>|</span><a href="#37258106">prev</a><span>|</span><a href="#37254502">next</a><span>|</span><label class="collapse" for="c-37253642">[-]</label><label class="expand" for="c-37253642">[2 more]</label></div><br/><div class="children"><div class="content">Did ideogram release a checkpoint?</div><br/><div id="37254209" class="c"><input type="checkbox" id="c-37254209" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37249185">root</a><span>|</span><a href="#37253642">parent</a><span>|</span><a href="#37254502">next</a><span>|</span><label class="collapse" for="c-37254209">[-]</label><label class="expand" for="c-37254209">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t find any info or Discord or forum or anything. I think it&#x27;s a closed service that they plan to sell to make money.</div><br/></div></div></div></div></div></div><div id="37254502" class="c"><input type="checkbox" id="c-37254502" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#37249185">prev</a><span>|</span><a href="#37250446">next</a><span>|</span><label class="collapse" for="c-37254502">[-]</label><label class="expand" for="c-37254502">[2 more]</label></div><br/><div class="children"><div class="content">How much am I’m missing out on with tools like this or code pilot, compared to using GPT-4?<p>I guess since Xcode doesn’t have a good plug-in architecture for this I began experimenting more with a chat interface.<p>So far gpt-4 has seemed quite useful for generating code, reviewing
code for certain problems, etc.</div><br/><div id="37256076" class="c"><input type="checkbox" id="c-37256076" checked=""/><div class="controls bullet"><span class="by">citruscomputing</span><span>|</span><a href="#37254502">parent</a><span>|</span><a href="#37250446">next</a><span>|</span><label class="collapse" for="c-37256076">[-]</label><label class="expand" for="c-37256076">[1 more]</label></div><br/><div class="children"><div class="content">Editor plugins are fantastic about completing based on a pattern. That&#x27;s the main thing you&#x27;re missing out on imo - it&#x27;s worth it to hit tab, but not to copy&#x2F;paste and say &quot;finish this line for me, it looks almost like the one above.&quot;<p>There&#x27;s also the real-time aspect where you can see that it&#x27;s wrong via the virtual text, type a few characters, then it gets what you&#x27;re doing and you can tab complete the rest of the line.<p>It&#x27;s faster to converse with when you don&#x27;t have to actually have a conversation, if that makes sense? The feedback loop is much shorter and doesn&#x27;t require natural language, or nearly as much context switching.</div><br/></div></div></div></div><div id="37250446" class="c"><input type="checkbox" id="c-37250446" checked=""/><div class="controls bullet"><span class="by">natch</span><span>|</span><a href="#37254502">prev</a><span>|</span><a href="#37253821">next</a><span>|</span><label class="collapse" for="c-37250446">[-]</label><label class="expand" for="c-37250446">[4 more]</label></div><br/><div class="children"><div class="content">Why wouldn’t they provide a hosted version? Seems like a no brainer… they have the money, the hardware, the bandwidth, the people to build support for it, and they could design the experience and gather more learning data about usage in the initial stages, while putting a dent in ChatGPT commercial prospects, and all while still letting others host and use it elsewhere. I don’t get it. Maybe it was just the fastest option?</div><br/><div id="37250583" class="c"><input type="checkbox" id="c-37250583" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37250446">parent</a><span>|</span><a href="#37253821">next</a><span>|</span><label class="collapse" for="c-37250583">[-]</label><label class="expand" for="c-37250583">[3 more]</label></div><br/><div class="children"><div class="content">Probably the researchers at meta are only interested in research, and productionizing this would be up to other teams.</div><br/><div id="37253440" class="c"><input type="checkbox" id="c-37253440" checked=""/><div class="controls bullet"><span class="by">natch</span><span>|</span><a href="#37250446">root</a><span>|</span><a href="#37250583">parent</a><span>|</span><a href="#37253821">next</a><span>|</span><label class="collapse" for="c-37253440">[-]</label><label class="expand" for="c-37253440">[2 more]</label></div><br/><div class="children"><div class="content">But Yann LeCun seems to think the safety problems of eventual AGI will be solved somehow.<p>Nobody is saying this model is AGI obviously.<p>But this would be an entry point into researching one small sliver of the alignment problem. If you follow my thinking, it’s odd that he professes confidence that AI safety is a non issue, yet from this he seems to want no part in understanding it.<p>I realize their research interest may just be the optimization &#x2F; mathy research… that’s their prerogative but it’s odd imho.</div><br/><div id="37253707" class="c"><input type="checkbox" id="c-37253707" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#37250446">root</a><span>|</span><a href="#37253440">parent</a><span>|</span><a href="#37253821">next</a><span>|</span><label class="collapse" for="c-37253707">[-]</label><label class="expand" for="c-37253707">[1 more]</label></div><br/><div class="children"><div class="content">It’s not that odd and I think you’re overestimating the importance of user submitted data for the purposes of alignment research. In particular because it’s more liability for them to try to be responsible for outputs. Really though, this way they get a bunch of free work from volunteers in open source&#x2F;ML communities.</div><br/></div></div></div></div></div></div></div></div><div id="37253821" class="c"><input type="checkbox" id="c-37253821" checked=""/><div class="controls bullet"><span class="by">1024core</span><span>|</span><a href="#37250446">prev</a><span>|</span><a href="#37259040">next</a><span>|</span><label class="collapse" for="c-37253821">[-]</label><label class="expand" for="c-37253821">[3 more]</label></div><br/><div class="children"><div class="content">If GPT-4&#x27;s accuracy is 67% and this is 54%, how can these guys claim to be SOTA?</div><br/><div id="37255896" class="c"><input type="checkbox" id="c-37255896" checked=""/><div class="controls bullet"><span class="by">rgbrgb</span><span>|</span><a href="#37253821">parent</a><span>|</span><a href="#37255327">next</a><span>|</span><label class="collapse" for="c-37255896">[-]</label><label class="expand" for="c-37255896">[1 more]</label></div><br/><div class="children"><div class="content">This runs locally on a MacBook.</div><br/></div></div><div id="37255327" class="c"><input type="checkbox" id="c-37255327" checked=""/><div class="controls bullet"><span class="by">binreaper</span><span>|</span><a href="#37253821">parent</a><span>|</span><a href="#37255896">prev</a><span>|</span><a href="#37259040">next</a><span>|</span><label class="collapse" for="c-37255327">[-]</label><label class="expand" for="c-37255327">[1 more]</label></div><br/><div class="children"><div class="content">Seriously, I was expecting to read the article and them be on a level on-par with GPT-4 or higher. For all this chat of how long Google&#x2F;Facebook have been in the AI space longer than OpenAI, their products don&#x27;t speak to that..</div><br/></div></div></div></div><div id="37259040" class="c"><input type="checkbox" id="c-37259040" checked=""/><div class="controls bullet"><span class="by">m00nsome</span><span>|</span><a href="#37253821">prev</a><span>|</span><a href="#37259305">next</a><span>|</span><label class="collapse" for="c-37259040">[-]</label><label class="expand" for="c-37259040">[1 more]</label></div><br/><div class="children"><div class="content">Why do they not release the unnatural Variant of the model? According to the paper it beats all of the other variants and seems to be close to GPT-4.</div><br/></div></div><div id="37259305" class="c"><input type="checkbox" id="c-37259305" checked=""/><div class="controls bullet"><span class="by">KingOfCoders</span><span>|</span><a href="#37259040">prev</a><span>|</span><a href="#37258120">next</a><span>|</span><label class="collapse" for="c-37259305">[-]</label><label class="expand" for="c-37259305">[1 more]</label></div><br/><div class="children"><div class="content">Any performance tests? (e.G. tokens&#x2F;s on a 4090?)</div><br/></div></div><div id="37258120" class="c"><input type="checkbox" id="c-37258120" checked=""/><div class="controls bullet"><span class="by">awwaiid</span><span>|</span><a href="#37259305">prev</a><span>|</span><a href="#37256743">next</a><span>|</span><label class="collapse" for="c-37258120">[-]</label><label class="expand" for="c-37258120">[1 more]</label></div><br/><div class="children"><div class="content">I want to see (more) code models trained on git diffs</div><br/></div></div><div id="37256743" class="c"><input type="checkbox" id="c-37256743" checked=""/><div class="controls bullet"><span class="by">TheRealClay</span><span>|</span><a href="#37258120">prev</a><span>|</span><a href="#37249518">next</a><span>|</span><label class="collapse" for="c-37256743">[-]</label><label class="expand" for="c-37256743">[3 more]</label></div><br/><div class="children"><div class="content">Anyone know of a docker image that provides an HTTP API interface to Llama? I&#x27;m looking for a super simple sort of &#x27;drop-in&#x27; solution like that which I can add to my web stack, to enable LLM in my web app.</div><br/><div id="37257048" class="c"><input type="checkbox" id="c-37257048" checked=""/><div class="controls bullet"><span class="by">nodja</span><span>|</span><a href="#37256743">parent</a><span>|</span><a href="#37249518">next</a><span>|</span><label class="collapse" for="c-37257048">[-]</label><label class="expand" for="c-37257048">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;abetlen&#x2F;llama-cpp-python">https:&#x2F;&#x2F;github.com&#x2F;abetlen&#x2F;llama-cpp-python</a> has a web server mode that replicates openai&#x27;s API iirc and the readme shows it has docker builds already.</div><br/><div id="37257179" class="c"><input type="checkbox" id="c-37257179" checked=""/><div class="controls bullet"><span class="by">TheRealClay</span><span>|</span><a href="#37256743">root</a><span>|</span><a href="#37257048">parent</a><span>|</span><a href="#37249518">next</a><span>|</span><label class="collapse" for="c-37257179">[-]</label><label class="expand" for="c-37257179">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! As someone just getting started, I really appreciate the tip!</div><br/></div></div></div></div></div></div><div id="37249518" class="c"><input type="checkbox" id="c-37249518" checked=""/><div class="controls bullet"><span class="by">jasfi</span><span>|</span><a href="#37256743">prev</a><span>|</span><a href="#37250361">next</a><span>|</span><label class="collapse" for="c-37249518">[-]</label><label class="expand" for="c-37249518">[3 more]</label></div><br/><div class="children"><div class="content">Now we need code quality benchmarks comparing this against GPT-4 and other contenders.</div><br/><div id="37251361" class="c"><input type="checkbox" id="c-37251361" checked=""/><div class="controls bullet"><span class="by">nick0garvey</span><span>|</span><a href="#37249518">parent</a><span>|</span><a href="#37250361">next</a><span>|</span><label class="collapse" for="c-37251361">[-]</label><label class="expand" for="c-37251361">[2 more]</label></div><br/><div class="children"><div class="content">They show the benchmarks in the original post, a few pages down</div><br/><div id="37251505" class="c"><input type="checkbox" id="c-37251505" checked=""/><div class="controls bullet"><span class="by">jasfi</span><span>|</span><a href="#37249518">root</a><span>|</span><a href="#37251361">parent</a><span>|</span><a href="#37250361">next</a><span>|</span><label class="collapse" for="c-37251505">[-]</label><label class="expand" for="c-37251505">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, I missed that somehow.</div><br/></div></div></div></div></div></div><div id="37250361" class="c"><input type="checkbox" id="c-37250361" checked=""/><div class="controls bullet"><span class="by">andrewjl</span><span>|</span><a href="#37249518">prev</a><span>|</span><a href="#37251444">next</a><span>|</span><label class="collapse" for="c-37250361">[-]</label><label class="expand" for="c-37250361">[1 more]</label></div><br/><div class="children"><div class="content">What I found interesting in Meta&#x27;s paper is the mention of HumanEval[1] and MBPP[2] as benchmarks for code quality. (Admittedly maybe they&#x27;re well-known to those working in the field.)<p>I haven&#x27;t yet read the whole paper (nor have I looked at the benchmark docs which might very well cover this) but curious how these are designed to avoid issues with overfitting. My thinking here is that canned algorithm type problems common in software engineering interviews are probably over represented in the training data used for these models. Which might point to artificially better performance  by LLMs versus their performance on more domain-specific type tasks they might be used for in day-to-day work.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;human-eval">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;human-eval</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;google-research&#x2F;google-research&#x2F;tree&#x2F;master&#x2F;mbpp">https:&#x2F;&#x2F;github.com&#x2F;google-research&#x2F;google-research&#x2F;tree&#x2F;mast...</a></div><br/></div></div><div id="37251444" class="c"><input type="checkbox" id="c-37251444" checked=""/><div class="controls bullet"><span class="by">e12e</span><span>|</span><a href="#37250361">prev</a><span>|</span><a href="#37257298">next</a><span>|</span><label class="collapse" for="c-37251444">[-]</label><label class="expand" for="c-37251444">[3 more]</label></div><br/><div class="children"><div class="content">Curious if there are projects to enable working with these things self-hosted, tuned to a git repo as context on the cli, like a Unix filter - or with editors like vim? (I&#x27;d love to use this with Helix)<p>I see both vscode and netbeans have a concept of &quot;inference URL&quot; - are there any efforts like language server (lsp) - but for inference?</div><br/><div id="37254273" class="c"><input type="checkbox" id="c-37254273" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37251444">parent</a><span>|</span><a href="#37254018">next</a><span>|</span><label class="collapse" for="c-37254273">[-]</label><label class="expand" for="c-37254273">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;runvnc&#x2F;smartcat">https:&#x2F;&#x2F;github.com&#x2F;runvnc&#x2F;smartcat</a></div><br/></div></div><div id="37254018" class="c"><input type="checkbox" id="c-37254018" checked=""/><div class="controls bullet"><span class="by">ingridpan</span><span>|</span><a href="#37251444">parent</a><span>|</span><a href="#37254273">prev</a><span>|</span><a href="#37257298">next</a><span>|</span><label class="collapse" for="c-37254018">[-]</label><label class="expand" for="c-37254018">[1 more]</label></div><br/><div class="children"><div class="content">not quite self-hosted but gradient.ai gives you access to llama2 via CLI</div><br/></div></div></div></div><div id="37257298" class="c"><input type="checkbox" id="c-37257298" checked=""/><div class="controls bullet"><span class="by">dchuk</span><span>|</span><a href="#37251444">prev</a><span>|</span><a href="#37249323">next</a><span>|</span><label class="collapse" for="c-37257298">[-]</label><label class="expand" for="c-37257298">[2 more]</label></div><br/><div class="children"><div class="content">Given this can produce code when prompted, could it also be used to interpret html from a crawler and then be used to scrape arbitrary URLs and extract structured attributes? Basically like MarkupLM but with massively more token context?</div><br/><div id="37257387" class="c"><input type="checkbox" id="c-37257387" checked=""/><div class="controls bullet"><span class="by">stevofolife</span><span>|</span><a href="#37257298">parent</a><span>|</span><a href="#37249323">next</a><span>|</span><label class="collapse" for="c-37257387">[-]</label><label class="expand" for="c-37257387">[1 more]</label></div><br/><div class="children"><div class="content">Also curious about this. There must be a better way to scrape using LLM.</div><br/></div></div></div></div><div id="37249323" class="c"><input type="checkbox" id="c-37249323" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37257298">prev</a><span>|</span><a href="#37253467">next</a><span>|</span><label class="collapse" for="c-37249323">[-]</label><label class="expand" for="c-37249323">[5 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;codellama">https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;codellama</a></div><br/><div id="37249439" class="c"><input type="checkbox" id="c-37249439" checked=""/><div class="controls bullet"><span class="by">ceejayoz</span><span>|</span><a href="#37249323">parent</a><span>|</span><a href="#37249437">next</a><span>|</span><label class="collapse" for="c-37249439">[-]</label><label class="expand" for="c-37249439">[3 more]</label></div><br/><div class="children"><div class="content">This is 404ing now. (Not your fault, the email&#x27;s link is similarly broken.)</div><br/><div id="37249554" class="c"><input type="checkbox" id="c-37249554" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37249323">root</a><span>|</span><a href="#37249439">parent</a><span>|</span><a href="#37249437">next</a><span>|</span><label class="collapse" for="c-37249554">[-]</label><label class="expand" for="c-37249554">[2 more]</label></div><br/><div class="children"><div class="content">Really? Works for me.</div><br/><div id="37249647" class="c"><input type="checkbox" id="c-37249647" checked=""/><div class="controls bullet"><span class="by">ceejayoz</span><span>|</span><a href="#37249323">root</a><span>|</span><a href="#37249554">parent</a><span>|</span><a href="#37249437">next</a><span>|</span><label class="collapse" for="c-37249647">[-]</label><label class="expand" for="c-37249647">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s back now.</div><br/></div></div></div></div></div></div></div></div><div id="37253467" class="c"><input type="checkbox" id="c-37253467" checked=""/><div class="controls bullet"><span class="by">gorbypark</span><span>|</span><a href="#37249323">prev</a><span>|</span><a href="#37248961">next</a><span>|</span><label class="collapse" for="c-37253467">[-]</label><label class="expand" for="c-37253467">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t wait for some models fine tuned on other languages.  I&#x27;m not a Python developer, so I downloaded the 13B-instruct variant (4 bit quantized Q4_K_M) and it&#x27;s pretty bad at doing javascript.  I asked it to write me a basic React Native component that has a name prop and displays that name.  Once it returned a regular React component, and when I asked it to make sure it uses React Native components, it said sure and outputted a bunch of random CSS and an HTML file that was initializing a React project.<p>It might be the quantization or my lacklustre prompting skills affecting it, though.  To be fair I did get it to output a little bit of useful code after trying a few times.</div><br/></div></div><div id="37248961" class="c"><input type="checkbox" id="c-37248961" checked=""/><div class="controls bullet"><span class="by">msoad</span><span>|</span><a href="#37253467">prev</a><span>|</span><a href="#37254596">next</a><span>|</span><label class="collapse" for="c-37248961">[-]</label><label class="expand" for="c-37248961">[3 more]</label></div><br/><div class="children"><div class="content">Is there any place we can try those models? Are they available on HuggingFace?</div><br/><div id="37249155" class="c"><input type="checkbox" id="c-37249155" checked=""/><div class="controls bullet"><span class="by">jspisak</span><span>|</span><a href="#37248961">parent</a><span>|</span><a href="#37254596">next</a><span>|</span><label class="collapse" for="c-37249155">[-]</label><label class="expand" for="c-37249155">[2 more]</label></div><br/><div class="children"><div class="content">Partner integrations will follow. For now we just have the weights available.<p>But don&#x27;t worry, this community moves fast!</div><br/><div id="37249486" class="c"><input type="checkbox" id="c-37249486" checked=""/><div class="controls bullet"><span class="by">Eddygandr</span><span>|</span><a href="#37248961">root</a><span>|</span><a href="#37249155">parent</a><span>|</span><a href="#37254596">next</a><span>|</span><label class="collapse" for="c-37249486">[-]</label><label class="expand" for="c-37249486">[1 more]</label></div><br/><div class="children"><div class="content">Probably superseded (by y’all) within a week!</div><br/></div></div></div></div></div></div><div id="37254596" class="c"><input type="checkbox" id="c-37254596" checked=""/><div class="controls bullet"><span class="by">KaiserPro</span><span>|</span><a href="#37248961">prev</a><span>|</span><a href="#37256311">next</a><span>|</span><label class="collapse" for="c-37254596">[-]</label><label class="expand" for="c-37254596">[2 more]</label></div><br/><div class="children"><div class="content">This is great for asking questions like &quot;how do I do x with y&quot; and this code &lt;&lt;some code&gt;&gt; isn&#x27;t working, whats wrong? Much faster that googling, or a great basis for forming a more accurate google search.<p>Where its a bit shit is when its used to provide auto suggest. It hallucinates plausible sounding functions&#x2F;names, which for me personally are hard to stop if they are wrong (I suspect that&#x27;s a function of the plugin)</div><br/><div id="37258213" class="c"><input type="checkbox" id="c-37258213" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#37254596">parent</a><span>|</span><a href="#37256311">next</a><span>|</span><label class="collapse" for="c-37258213">[-]</label><label class="expand" for="c-37258213">[1 more]</label></div><br/><div class="children"><div class="content">hallucinations can be resuces by incorporating &#x27;retrieval automated generation&#x27; , RAG, on the front end. likely function library defs could be automagically entered as prompt&#x2F;memory inputs.</div><br/></div></div></div></div><div id="37256311" class="c"><input type="checkbox" id="c-37256311" checked=""/><div class="controls bullet"><span class="by">akulbe</span><span>|</span><a href="#37254596">prev</a><span>|</span><label class="collapse" for="c-37256311">[-]</label><label class="expand" for="c-37256311">[1 more]</label></div><br/><div class="children"><div class="content">Random tangential question given this is about llama, but how do you get llama.cpp or kobold (or whatever tool you use) to make use of multiple GPUs if you don&#x27;t have NVlink in place?<p>I got a bridge, but it was the wrong size.<p>Thanks, in advance.</div><br/></div></div></div></div></div></div></div></body></html>