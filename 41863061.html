<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1729155666290" as="style"/><link rel="stylesheet" href="styles.css?v=1729155666290"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/usefulsensors/qc_npu_benchmark">AI PCs Aren&#x27;t Good at AI: The CPU Beats the NPU</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>dbreunig</span> | <span>223 comments</span></div><br/><div><div id="41867674" class="c"><input type="checkbox" id="c-41867674" checked=""/><div class="controls bullet"><span class="by">pzo</span><span>|</span><a href="#41863460">next</a><span>|</span><label class="collapse" for="c-41867674">[-]</label><label class="expand" for="c-41867674">[1 more]</label></div><br/><div class="children"><div class="content">Haven&#x27;t played much with Qualcomm NPU but Apple Neural Engine available in iOS and MacOS for many Computer Vision models was significantly faster than when running on  CPU or GPU (e.g. mediapipe models, yolo, depth-anything) - to the point that inference was much faster on Macbook M2 Max using its NPU that is the same as on older iPhones rather than executing on all 38 GPU cores.<p>This all depends on model architecture, conversions and tuning. Apple provides good tooling in XCode for benchmarking models up to execution time of single operators and where such operator got executed (CPU, GPU, NPU) in case couldn&#x27;t been executed on NPU and have to fallback to CPU&#x2F;GPU. Sometimes model have be tweaked to slightly different operator if it&#x27;s not available in NPU.</div><br/></div></div><div id="41863460" class="c"><input type="checkbox" id="c-41863460" checked=""/><div class="controls bullet"><span class="by">isusmelj</span><span>|</span><a href="#41867674">prev</a><span>|</span><a href="#41863390">next</a><span>|</span><label class="collapse" for="c-41863460">[-]</label><label class="expand" for="c-41863460">[70 more]</label></div><br/><div class="children"><div class="content">I think the results show that just in general the compute is not used well. That the CPU took 8.4ms and GPU took 3.2ms shows a very small gap. I&#x27;d expect more like 10x - 20x difference here.
I&#x27;d assume that the onnxruntime might be the issue. I think some hardware vendors just release the compute units without shipping proper support yet. Let&#x27;s see how fast that will change.<p>Also, people often mistake the reason for an NPU is &quot;speed&quot;. That&#x27;s not correct. The whole point of the NPU is rather to focus on low power consumption. To focus on speed you&#x27;d need to get rid of the memory bottleneck. Then you end up designing your own ASIC with it&#x27;s own memory. The NPUs we see in most devices are part of the SoC around the CPU to offload AI computations.
It would be interesting to run this benchmark in a infinite loop for the three devices (CPU, NPU, GPU) and measure power consumption.
I&#x27;d expect the NPU to be lowest and also best in terms of &quot;ops&#x2F;watt&quot;</div><br/><div id="41863552" class="c"><input type="checkbox" id="c-41863552" checked=""/><div class="controls bullet"><span class="by">AlexandrB</span><span>|</span><a href="#41863460">parent</a><span>|</span><a href="#41864928">next</a><span>|</span><label class="collapse" for="c-41863552">[-]</label><label class="expand" for="c-41863552">[56 more]</label></div><br/><div class="children"><div class="content">&gt; Also, people often mistake the reason for an NPU is &quot;speed&quot;. That&#x27;s not correct. The whole point of the NPU is rather to focus on low power consumption.<p>I have a sneaking suspicion that the real real reason for an NPU is marketing. &quot;Oh look, NVDA is worth $3.3T - let&#x27;s make sure we stick some AI stuff in our products too.&quot;</div><br/><div id="41865968" class="c"><input type="checkbox" id="c-41865968" checked=""/><div class="controls bullet"><span class="by">Spooky23</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41863552">parent</a><span>|</span><a href="#41866423">next</a><span>|</span><label class="collapse" for="c-41865968">[-]</label><label class="expand" for="c-41865968">[18 more]</label></div><br/><div class="children"><div class="content">Microsoft needs to throw something in the gap to slow down MacBook attrition.<p>The M processors changed the game. My teams support 250k users. I went from 50 MacBooks in 2020 to over 10,000 today. I added zero staff - we manage them like iPhones.</div><br/><div id="41866126" class="c"><input type="checkbox" id="c-41866126" checked=""/><div class="controls bullet"><span class="by">cj</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865968">parent</a><span>|</span><a href="#41866658">next</a><span>|</span><label class="collapse" for="c-41866126">[-]</label><label class="expand" for="c-41866126">[14 more]</label></div><br/><div class="children"><div class="content">Rightly so.<p>The M processor really did completely eliminate all sense of “lag” for basic computing (web browsing, restarting your computer, etc). Everything happens nearly instantly, even on the first generation M1 processor. The experience of “waiting for something to load” went away.<p>Not to mention these machines easily last 5-10 years.</div><br/><div id="41866918" class="c"><input type="checkbox" id="c-41866918" checked=""/><div class="controls bullet"><span class="by">morsch</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866126">parent</a><span>|</span><a href="#41866165">next</a><span>|</span><label class="collapse" for="c-41866918">[-]</label><label class="expand" for="c-41866918">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s fine. For basic computing, my M3 doesn&#x27;t feel much faster than my Linux desktop that&#x27;s like 8 years old. I think the standard for laptops was just really, really low.</div><br/></div></div><div id="41866165" class="c"><input type="checkbox" id="c-41866165" checked=""/><div class="controls bullet"><span class="by">nxobject</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866126">parent</a><span>|</span><a href="#41866918">prev</a><span>|</span><a href="#41866450">next</a><span>|</span><label class="collapse" for="c-41866165">[-]</label><label class="expand" for="c-41866165">[9 more]</label></div><br/><div class="children"><div class="content">As a very happy M1 Max user (should&#x27;ve shelled out for 64GB of RAM, though, for local LLMs!), I don&#x27;t look forward to seeing how the Google Workspace&#x2F;Notions&#x2F;etc. of the world somehow reintroduce lag back in.</div><br/><div id="41866309" class="c"><input type="checkbox" id="c-41866309" checked=""/><div class="controls bullet"><span class="by">bugbuddy</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866165">parent</a><span>|</span><a href="#41866791">next</a><span>|</span><label class="collapse" for="c-41866309">[-]</label><label class="expand" for="c-41866309">[6 more]</label></div><br/><div class="children"><div class="content">The problem for Intel and AMD is they are stuck with an OS that ships with a lag-inducing Anti-malware suite. I just did a simple git log and it took 2000% longer than usual because the Antivirus was triggered to scan and run a simulation on each machine instruction and byte of data accessed. The commit log window stayed blank waiting to load long enough for me to complete another tiny project. It always ruin my day.</div><br/><div id="41866574" class="c"><input type="checkbox" id="c-41866574" checked=""/><div class="controls bullet"><span class="by">zdw</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866309">parent</a><span>|</span><a href="#41866628">next</a><span>|</span><label class="collapse" for="c-41866574">[-]</label><label class="expand" for="c-41866574">[1 more]</label></div><br/><div class="children"><div class="content">This is most likely due to corporate malware.<p>Even modern macs can be brought to their knees by something that rhymes with FrowdStrike Calcon and interrupts all IO.</div><br/></div></div><div id="41866628" class="c"><input type="checkbox" id="c-41866628" checked=""/><div class="controls bullet"><span class="by">alisonatwork</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866309">parent</a><span>|</span><a href="#41866574">prev</a><span>|</span><a href="#41866791">next</a><span>|</span><label class="collapse" for="c-41866628">[-]</label><label class="expand" for="c-41866628">[4 more]</label></div><br/><div class="children"><div class="content">Pro tip: turn off malware scanning in your git repos[0]. There is also the new Dev Drive feature in Windows 11 that makes it even easier for developers (and IT admins) to set this kind of thing up via policies[1].<p>In companies where I worked where the IT team rolled out &quot;security&quot; software to the Mac-based developers, their computers were not noticeably faster than Windows PCs at all, especially given the majority of containers are still linux&#x2F;amd64, reflecting the actual deployment environment. Meanwhile Windows also runs on ARM anyway, so it&#x27;s not really something useful to generalize about.<p>[0] <a href="https:&#x2F;&#x2F;support.microsoft.com&#x2F;en-us&#x2F;topic&#x2F;how-to-add-a-file-type-or-process-exclusion-to-windows-security-e524cbc2-3975-63c2-f9d1-7c2eb5331e53" rel="nofollow">https:&#x2F;&#x2F;support.microsoft.com&#x2F;en-us&#x2F;topic&#x2F;how-to-add-a-file-...</a><p>[1] <a href="https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;windows&#x2F;dev-drive&#x2F;" rel="nofollow">https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;windows&#x2F;dev-drive&#x2F;</a></div><br/><div id="41866770" class="c"><input type="checkbox" id="c-41866770" checked=""/><div class="controls bullet"><span class="by">bugbuddy</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866628">parent</a><span>|</span><a href="#41866861">next</a><span>|</span><label class="collapse" for="c-41866770">[-]</label><label class="expand" for="c-41866770">[2 more]</label></div><br/><div class="children"><div class="content">Unfortunately, the IT department people think they are literal GODs for knowing how to configure Domain Policies and lock down everything. They even refuse to help or even answer requests for help when there are false positives on our own software builds that we cannot unmark as false positives. These people are proactively antagonistic to productivity. Management could not careless…</div><br/><div id="41867349" class="c"><input type="checkbox" id="c-41867349" checked=""/><div class="controls bullet"><span class="by">lynx23</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866770">parent</a><span>|</span><a href="#41866861">next</a><span>|</span><label class="collapse" for="c-41867349">[-]</label><label class="expand" for="c-41867349">[1 more]</label></div><br/><div class="children"><div class="content">Nobody wants to be resonsible for giving allowing exceptions in security-matters.  Its far easier to ignore the problems at hand, then to risk being wrong just once.</div><br/></div></div></div></div><div id="41866861" class="c"><input type="checkbox" id="c-41866861" checked=""/><div class="controls bullet"><span class="by">xxs</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866628">parent</a><span>|</span><a href="#41866770">prev</a><span>|</span><a href="#41866791">next</a><span>|</span><label class="collapse" for="c-41866861">[-]</label><label class="expand" for="c-41866861">[1 more]</label></div><br/><div class="children"><div class="content">the short answer is that you can&#x27;t without the necessary permissions, and even if you do - the next roll out will wipe out your changes.<p>So the pro-part of the tip does not apply.<p>On my own machines anti-virus is one the very first things to be removed. Most of the time I&#x27;d turn off all the swap file, yet Windows doesn&#x27;t overcommit and certain applications are notorious for allocating memory w&#x2F;o even using it.</div><br/></div></div></div></div></div></div><div id="41866791" class="c"><input type="checkbox" id="c-41866791" checked=""/><div class="controls bullet"><span class="by">n8cpdx</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866165">parent</a><span>|</span><a href="#41866309">prev</a><span>|</span><a href="#41866670">next</a><span>|</span><label class="collapse" for="c-41866791">[-]</label><label class="expand" for="c-41866791">[1 more]</label></div><br/><div class="children"><div class="content">Chrome managed it. Not sure how since Edge still works reasonably well and Safari is instant to start (even faster than system settings, which is really an indictment of SwiftUI).</div><br/></div></div><div id="41866670" class="c"><input type="checkbox" id="c-41866670" checked=""/><div class="controls bullet"><span class="by">djur</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866165">parent</a><span>|</span><a href="#41866791">prev</a><span>|</span><a href="#41866450">next</a><span>|</span><label class="collapse" for="c-41866670">[-]</label><label class="expand" for="c-41866670">[1 more]</label></div><br/><div class="children"><div class="content">Oh, just work for a company that uses Crowdstrike or similar. You&#x27;ll get back all the lag you want.</div><br/></div></div></div></div><div id="41866450" class="c"><input type="checkbox" id="c-41866450" checked=""/><div class="controls bullet"><span class="by">ddingus</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866126">parent</a><span>|</span><a href="#41866165">prev</a><span>|</span><a href="#41866737">next</a><span>|</span><label class="collapse" for="c-41866450">[-]</label><label class="expand" for="c-41866450">[2 more]</label></div><br/><div class="children"><div class="content">I have a first gen M1 and it holds up very nicely even today.  I&#x2F;O is crazy fast and high compute loads get done efficiently.<p>One can bury the machine and lose very little basic interactivity.  That part users really like.<p>Frankly the only downside of the MacBook Air is the tiny storage.  The 8GB RAM is actually enough most of the time.  But general system storage with only 1&#x2F;4 TB is cramped consistently.<p>Been thinking about sending the machine out to one of those upgrade shops...</div><br/><div id="41866621" class="c"><input type="checkbox" id="c-41866621" checked=""/><div class="controls bullet"><span class="by">lynguist</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866450">parent</a><span>|</span><a href="#41866737">next</a><span>|</span><label class="collapse" for="c-41866621">[-]</label><label class="expand" for="c-41866621">[1 more]</label></div><br/><div class="children"><div class="content">Why did you buy a 256GB device for personal use in the first place? Too good of a deal? Or saving these $400 for upgrades for something else?</div><br/></div></div></div></div><div id="41866737" class="c"><input type="checkbox" id="c-41866737" checked=""/><div class="controls bullet"><span class="by">bzzzt</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866126">parent</a><span>|</span><a href="#41866450">prev</a><span>|</span><a href="#41866658">next</a><span>|</span><label class="collapse" for="c-41866737">[-]</label><label class="expand" for="c-41866737">[1 more]</label></div><br/><div class="children"><div class="content">Depends on the application as well. Just try to start up Microsoft Teams.</div><br/></div></div></div></div><div id="41866658" class="c"><input type="checkbox" id="c-41866658" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865968">parent</a><span>|</span><a href="#41866126">prev</a><span>|</span><a href="#41866423">next</a><span>|</span><label class="collapse" for="c-41866658">[-]</label><label class="expand" for="c-41866658">[3 more]</label></div><br/><div class="children"><div class="content">Microsoft has indeed a problem, however only in countries whose people can afford Apple level prices, and not everyone is a G7 citizen.</div><br/><div id="41866904" class="c"><input type="checkbox" id="c-41866904" checked=""/><div class="controls bullet"><span class="by">jocaal</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866658">parent</a><span>|</span><a href="#41866423">next</a><span>|</span><label class="collapse" for="c-41866904">[-]</label><label class="expand" for="c-41866904">[2 more]</label></div><br/><div class="children"><div class="content">Microsoft is slowly being squeezed from both sides of the market. Chromebooks have silently become wildly popular on the low end. The only advantage I see windows have is corporate and gaming. But valve is slowly chopping away at the gaming advantage as well.</div><br/><div id="41866915" class="c"><input type="checkbox" id="c-41866915" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866904">parent</a><span>|</span><a href="#41866423">next</a><span>|</span><label class="collapse" for="c-41866915">[-]</label><label class="expand" for="c-41866915">[1 more]</label></div><br/><div class="children"><div class="content">Chromebooks are no where to be seen outside US school market.<p>Coffe shops, trains and airports in Europe? Nope, rare animal on tables.<p>European schools? Most countries parents buy their kids a computer, and most often it is a desktop used by the whole family, or a laptop of some kind running Windows, unless we are talking about the countries where buying Apple isn&#x27;t an issue on the monthly expenses.<p>Popular? In Germany, the few times they get displayed on shopping mall stores, they get rountinely discounted, or bundled with something else, until finally they get rid of them.<p>Valve is heavily dependent on game studios producing Windows games.</div><br/></div></div></div></div></div></div></div></div><div id="41866423" class="c"><input type="checkbox" id="c-41866423" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41863552">parent</a><span>|</span><a href="#41865968">prev</a><span>|</span><a href="#41863654">next</a><span>|</span><label class="collapse" for="c-41866423">[-]</label><label class="expand" for="c-41866423">[2 more]</label></div><br/><div class="children"><div class="content">The correct way to make a true &quot;NPU&quot; is to 10x your memory bandwidth and feed a regular old multicore CPU with SIMD&#x2F;vector instructions (and maybe a matrix multiply unit).<p>Most of these small NPUs are actually made for CNNs and other models where &quot;stream data through weights&quot; applies. They have a huge speedup there. When you stream weights across data (any LLM or other large model), you are almost certain to be bound by memory bandwidth.</div><br/><div id="41866896" class="c"><input type="checkbox" id="c-41866896" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866423">parent</a><span>|</span><a href="#41863654">next</a><span>|</span><label class="collapse" for="c-41866896">[-]</label><label class="expand" for="c-41866896">[1 more]</label></div><br/><div class="children"><div class="content">I’m sure we’ll get GPNPU. Low precision matvecs could be fun to play with.</div><br/></div></div></div></div><div id="41863654" class="c"><input type="checkbox" id="c-41863654" checked=""/><div class="controls bullet"><span class="by">itishappy</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41863552">parent</a><span>|</span><a href="#41866423">prev</a><span>|</span><a href="#41866150">next</a><span>|</span><label class="collapse" for="c-41863654">[-]</label><label class="expand" for="c-41863654">[21 more]</label></div><br/><div class="children"><div class="content">I assume you&#x27;re both right. I&#x27;m sure NPUs exist to fill a very real niche, but I&#x27;m also sure they&#x27;re being shoehorned in everywhere regardless of product fit because &quot;AI big right now.&quot;</div><br/><div id="41864463" class="c"><input type="checkbox" id="c-41864463" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41863654">parent</a><span>|</span><a href="#41865770">next</a><span>|</span><label class="collapse" for="c-41864463">[-]</label><label class="expand" for="c-41864463">[1 more]</label></div><br/><div class="children"><div class="content">Looking at it slightly differently: putting low-power NPUs into laptop and phone SoCs is how to get on the AI bandwagon in a way that NVIDIA cannot easily disrupt. There are plenty of systems where a NVIDIA discrete GPU cannot fit into the budget (of $ or Watts). So even if NPUs are still somewhat of a solution in search of a problem (aka a killer app or two), they&#x27;re not necessarily a sign that these manufacturers are acting entirely without strategy.</div><br/></div></div><div id="41865770" class="c"><input type="checkbox" id="c-41865770" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41863654">parent</a><span>|</span><a href="#41864463">prev</a><span>|</span><a href="#41866150">next</a><span>|</span><label class="collapse" for="c-41865770">[-]</label><label class="expand" for="c-41865770">[19 more]</label></div><br/><div class="children"><div class="content">The shoehorning only works if there is buyer demand.<p>As a company, if customers are willing to pay a premium for a NPU, or if they are unwilling to buy a product without one, it is not your place to say “hey we don’t really believe in the AI hype so we’re going to sell products people don’t want to prove a point”</div><br/><div id="41866019" class="c"><input type="checkbox" id="c-41866019" checked=""/><div class="controls bullet"><span class="by">Spooky23</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865770">parent</a><span>|</span><a href="#41865911">next</a><span>|</span><label class="collapse" for="c-41866019">[-]</label><label class="expand" for="c-41866019">[13 more]</label></div><br/><div class="children"><div class="content">Apple will have a completely AI capable product line in 18 months, with the major platforms basically done.<p>Microsoft is built around the broken Intel tick&#x2F;tick model of incremental improvement — they are stuck with OEM shitware that will take years to flush out of the channel. That means for AI, they are stuck with cloud based OpenAI, where NVIDIA has them by the balls and the hyperscalers are all fighting for GPU.<p>Apple will deliver local AI features as software (the hardware is “free”) at a much higher margin - while Office 365 AI is like $400+ a year per user.<p>You’ll have people getting iPhones to get AI assisted emails or whatever Apple does that is useful.</div><br/><div id="41866461" class="c"><input type="checkbox" id="c-41866461" checked=""/><div class="controls bullet"><span class="by">hakfoo</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866019">parent</a><span>|</span><a href="#41866405">next</a><span>|</span><label class="collapse" for="c-41866461">[-]</label><label class="expand" for="c-41866461">[9 more]</label></div><br/><div class="children"><div class="content">We&#x27;re still looking for &quot;that is useful&quot;.<p>The stuff they&#x27;ve been trying to sell AI to the public with is increasingly looking as absurd as every 1978 &quot;you&#x27;ll store your recipes on the home computer&quot; argument.<p>AI text became a Human Centipede story:  Start with a coherent 10-word sentence, let AI balloon it into five pages of flowery nonsense, send it to someone else, who has their AI smash it back down to 10 meaningful words.<p>Coding assistance, even as spicy autocorrect, is often a net negative as you have to plow through hallucinations and weird guesses as to what you want but lack the tools to explain to it.<p>Image generation is already heading rapidly into cringe territory, in part due to some very public social media operations.  I can imagine your kids&#x27; kids in 2040 finding out they generated AI images in the 2020s and looking at them with the same embarrassment you&#x27;d see if they dug out your high-school emo fursona.<p>There might well be some more &quot;closed-loop&quot; AI applications that make sense.  But are they going to be running on every desktop in the world?  Or are they going to be mostly used in datacentres and purpose-built embedded devices?<p>I also wonder how well some of the models and techniques scale down.  I know Microsoft pushed a minimum spec to promote a machine as Copilot-ready, but that seems like it&#x27;s going to be &quot;Vista Basic Ready&quot; redux as people try to run tools designed for datacentres full of Quadro cards, or at least high-end GPUs, on their $299 HP laptop.</div><br/><div id="41866517" class="c"><input type="checkbox" id="c-41866517" checked=""/><div class="controls bullet"><span class="by">jjmarr</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866461">parent</a><span>|</span><a href="#41866405">next</a><span>|</span><label class="collapse" for="c-41866517">[-]</label><label class="expand" for="c-41866517">[8 more]</label></div><br/><div class="children"><div class="content">Cringe emo girls are trendy now because the nostalgia cycle is hitting the early 2000s. Your kid would be impressed if you told them you were a goth gf. It&#x27;s not hard to imagine the same will happen with primitive AIs in the 40s.</div><br/><div id="41866595" class="c"><input type="checkbox" id="c-41866595" checked=""/><div class="controls bullet"><span class="by">defrost</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866517">parent</a><span>|</span><a href="#41866405">next</a><span>|</span><label class="collapse" for="c-41866595">[-]</label><label class="expand" for="c-41866595">[7 more]</label></div><br/><div class="children"><div class="content">Early 2000&#x27;s ??<p>Bela Lugosi Died in 1979, and Peter Murphy was onto his next band by 1984.<p>By 2000 Goth was fully a distant dot in the rear view mirror for the OG&#x27;s<p><pre><code>    In 2002, Murphy released *Dust* with Turkish-Canadian composer and producer Mercan Dede, which utilizes traditional Turkish instrumentation and songwriting, abandoning Murphy&#x27;s previous pop and rock incarnations, and juxtaposing elements from progressive rock, trance, classical music, and Middle Eastern music, coupled with Dede&#x27;s trademark atmospheric electronics.
</code></pre>
<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Yy9h2q_dr9k" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Yy9h2q_dr9k</a><p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bauhaus_(band)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bauhaus_(band)</a></div><br/><div id="41866683" class="c"><input type="checkbox" id="c-41866683" checked=""/><div class="controls bullet"><span class="by">djur</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866595">parent</a><span>|</span><a href="#41866897">next</a><span>|</span><label class="collapse" for="c-41866683">[-]</label><label class="expand" for="c-41866683">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure what &quot;gothic music existed in the 1980s&quot; is meant to indicate as a response to &quot;goths existed in the early 2000s as a cultural archetype&quot;.</div><br/><div id="41866722" class="c"><input type="checkbox" id="c-41866722" checked=""/><div class="controls bullet"><span class="by">defrost</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866683">parent</a><span>|</span><a href="#41866897">next</a><span>|</span><label class="collapse" for="c-41866722">[-]</label><label class="expand" for="c-41866722">[3 more]</label></div><br/><div class="children"><div class="content">That Goths in 2000&#x27;s were at best second wave nostalgia cycle of Goths from the 1980s.<p>That people recalling Goths in that period should beware of thinking that was a source and not an echo.<p>In 2006 Noel Fielding&#x27;s Richmond Felicity Avenal was a basement dwelling leftover from many years past.</div><br/><div id="41866888" class="c"><input type="checkbox" id="c-41866888" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866722">parent</a><span>|</span><a href="#41866897">next</a><span>|</span><label class="collapse" for="c-41866888">[-]</label><label class="expand" for="c-41866888">[2 more]</label></div><br/><div class="children"><div class="content">True Goth died our way before any of that. They totally sold out when the sacked Rome, the gold went to their heads and everything since then has been nostalgia.</div><br/><div id="41866910" class="c"><input type="checkbox" id="c-41866910" checked=""/><div class="controls bullet"><span class="by">defrost</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866888">parent</a><span>|</span><a href="#41866897">next</a><span>|</span><label class="collapse" for="c-41866910">[-]</label><label class="expand" for="c-41866910">[1 more]</label></div><br/><div class="children"><div class="content">That was just the faux life Westside Visigoths .. what&#x27;d you expect?<p>#Ostrogoth #TwueGoth</div><br/></div></div></div></div></div></div></div></div><div id="41866897" class="c"><input type="checkbox" id="c-41866897" checked=""/><div class="controls bullet"><span class="by">carlob</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866595">parent</a><span>|</span><a href="#41866683">prev</a><span>|</span><a href="#41866405">next</a><span>|</span><label class="collapse" for="c-41866897">[-]</label><label class="expand" for="c-41866897">[2 more]</label></div><br/><div class="children"><div class="content">There was a submission here a few months ago about the various incarnations of goth starting from the late Roman empire.<p><a href="https:&#x2F;&#x2F;www.the-hinternet.com&#x2F;p&#x2F;the-goths" rel="nofollow">https:&#x2F;&#x2F;www.the-hinternet.com&#x2F;p&#x2F;the-goths</a></div><br/><div id="41866940" class="c"><input type="checkbox" id="c-41866940" checked=""/><div class="controls bullet"><span class="by">defrost</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866897">parent</a><span>|</span><a href="#41866405">next</a><span>|</span><label class="collapse" for="c-41866940">[-]</label><label class="expand" for="c-41866940">[1 more]</label></div><br/><div class="children"><div class="content">Was there? This one: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41232761">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41232761</a> ?<p>Nice: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=VZvSqgn_Zf4" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=VZvSqgn_Zf4</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="41866405" class="c"><input type="checkbox" id="c-41866405" checked=""/><div class="controls bullet"><span class="by">nxobject</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866019">parent</a><span>|</span><a href="#41866461">prev</a><span>|</span><a href="#41866402">next</a><span>|</span><label class="collapse" for="c-41866405">[-]</label><label class="expand" for="c-41866405">[1 more]</label></div><br/><div class="children"><div class="content">I hope that once they get a baseline level of AI functionality in, they start working with larger LLMs to enable some form of RAG... that might be their next generational shift.</div><br/></div></div><div id="41866402" class="c"><input type="checkbox" id="c-41866402" checked=""/><div class="controls bullet"><span class="by">justahuman74</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866019">parent</a><span>|</span><a href="#41866405">prev</a><span>|</span><a href="#41866768">next</a><span>|</span><label class="collapse" for="c-41866402">[-]</label><label class="expand" for="c-41866402">[1 more]</label></div><br/><div class="children"><div class="content">Who is getting $400&#x2F;y of value from that?</div><br/></div></div><div id="41866768" class="c"><input type="checkbox" id="c-41866768" checked=""/><div class="controls bullet"><span class="by">im3w1l</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866019">parent</a><span>|</span><a href="#41866402">prev</a><span>|</span><a href="#41865911">next</a><span>|</span><label class="collapse" for="c-41866768">[-]</label><label class="expand" for="c-41866768">[1 more]</label></div><br/><div class="children"><div class="content">Until AI chips become abundant, and we are not there yet, cloud AI just makes too much sense. Using a chip constantly vs using it 0.1% of the time is just so many orders of magnitude better.<p>Local inference does have privacy benefits. I think at the moment it might make sense to send most of queries to a beefy cloud model, and send sensitive queries to a smaller local one.</div><br/></div></div></div></div><div id="41865911" class="c"><input type="checkbox" id="c-41865911" checked=""/><div class="controls bullet"><span class="by">MBCook</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865770">parent</a><span>|</span><a href="#41866019">prev</a><span>|</span><a href="#41865951">next</a><span>|</span><label class="collapse" for="c-41865911">[-]</label><label class="expand" for="c-41865911">[4 more]</label></div><br/><div class="children"><div class="content">Is there demand? Or do they just assume there is?<p>If they shove it in every single product and that’s all anyone advertises, whether consumers know it will help them or not, you don’t get a lot of choice.<p>If you want the latest chip, you’re getting AI stuff. That’s all there is to it.</div><br/><div id="41866176" class="c"><input type="checkbox" id="c-41866176" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865911">parent</a><span>|</span><a href="#41865951">next</a><span>|</span><label class="collapse" for="c-41866176">[-]</label><label class="expand" for="c-41866176">[3 more]</label></div><br/><div class="children"><div class="content">&quot;The math is clear: 100% of our our car sales come from models with our company logo somewhere on the front, which shows incredible customer desire for logos. We should consider offering a new luxury trim level with more of them.&quot;<p>&quot;How many models to we have without logos?&quot;<p>&quot;Huh? Why would we do that?&quot;</div><br/><div id="41866278" class="c"><input type="checkbox" id="c-41866278" checked=""/><div class="controls bullet"><span class="by">MBCook</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866176">parent</a><span>|</span><a href="#41865951">next</a><span>|</span><label class="collapse" for="c-41866278">[-]</label><label class="expand" for="c-41866278">[2 more]</label></div><br/><div class="children"><div class="content">Heh. Yeah more or less.<p>To some degree I understand it, because as we’ve all noticed computers have pretty much plateaued for the average person. They last much longer. You don’t need to replace them every two years anymore because the software isn’t out stripping them so fast.<p>AI is the first thing to come along in quite a while that not only needs significant power but it’s just something different. It’s something they can say your old computer doesn’t have that the new one does. Other than being 5% faster or whatever.<p>So even if people don’t need it, and even if they notice they don’t need it, it’s something to market on.<p>The stuff up thread about it being the hotness that Wall Street loves is absolutely a thing too.</div><br/><div id="41866459" class="c"><input type="checkbox" id="c-41866459" checked=""/><div class="controls bullet"><span class="by">ddingus</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41866278">parent</a><span>|</span><a href="#41865951">next</a><span>|</span><label class="collapse" for="c-41866459">[-]</label><label class="expand" for="c-41866459">[1 more]</label></div><br/><div class="children"><div class="content">That was all true nearly 10 years ago.  And it has only improved.  Almost any computer one finds these days is capable of the basics.</div><br/></div></div></div></div></div></div></div></div><div id="41865951" class="c"><input type="checkbox" id="c-41865951" checked=""/><div class="controls bullet"><span class="by">bdd8f1df777b</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865770">parent</a><span>|</span><a href="#41865911">prev</a><span>|</span><a href="#41866150">next</a><span>|</span><label class="collapse" for="c-41865951">[-]</label><label class="expand" for="c-41865951">[1 more]</label></div><br/><div class="children"><div class="content">There are two kinds of buyer demands: product, buyers, and the stock buyers. The AI hype can certainly convince some of the stock buyers.</div><br/></div></div></div></div></div></div><div id="41866150" class="c"><input type="checkbox" id="c-41866150" checked=""/><div class="controls bullet"><span class="by">conradev</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41863552">parent</a><span>|</span><a href="#41863654">prev</a><span>|</span><a href="#41863644">next</a><span>|</span><label class="collapse" for="c-41866150">[-]</label><label class="expand" for="c-41866150">[1 more]</label></div><br/><div class="children"><div class="content">The real consumers of the NPUs are the operating systems themselves. Google’s TPU and Apple’s ANE are used to power OS features like Apple’s Face ID and Google’s image enhancements.<p>We’re seeing these things in traditional PCs now because Microsoft has demanded it so that Microsoft can use it in Windows 11.<p>Any use by third party software is a lower priority</div><br/></div></div><div id="41863644" class="c"><input type="checkbox" id="c-41863644" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41863552">parent</a><span>|</span><a href="#41866150">prev</a><span>|</span><a href="#41867045">next</a><span>|</span><label class="collapse" for="c-41863644">[-]</label><label class="expand" for="c-41863644">[7 more]</label></div><br/><div class="children"><div class="content">You forget &quot;Because Apple is doing it&quot;, too.</div><br/><div id="41864659" class="c"><input type="checkbox" id="c-41864659" checked=""/><div class="controls bullet"><span class="by">rjsw</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41863644">parent</a><span>|</span><a href="#41867045">next</a><span>|</span><label class="collapse" for="c-41864659">[-]</label><label class="expand" for="c-41864659">[6 more]</label></div><br/><div class="children"><div class="content">I think other ARM SoC vendors like Rockchip added NPUs before Apple, or at least around the same time.</div><br/><div id="41864882" class="c"><input type="checkbox" id="c-41864882" checked=""/><div class="controls bullet"><span class="by">acchow</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41864659">parent</a><span>|</span><a href="#41865956">next</a><span>|</span><label class="collapse" for="c-41864882">[-]</label><label class="expand" for="c-41864882">[4 more]</label></div><br/><div class="children"><div class="content">I was curious so looked it up. Apple&#x27;s first chip with an NPU was the A11 bionic in Sept 2017. Rockchip&#x27;s was the RK1808 in Sept 2019.</div><br/><div id="41865486" class="c"><input type="checkbox" id="c-41865486" checked=""/><div class="controls bullet"><span class="by">j16sdiz</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41864882">parent</a><span>|</span><a href="#41865370">next</a><span>|</span><label class="collapse" for="c-41865486">[-]</label><label class="expand" for="c-41865486">[2 more]</label></div><br/><div class="children"><div class="content">Google TPU was introduced around same time as apple. Basically everybody knew it can be something around that time, just don&#x27;t know exactly how</div><br/><div id="41866672" class="c"><input type="checkbox" id="c-41866672" checked=""/><div class="controls bullet"><span class="by">Someone</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865486">parent</a><span>|</span><a href="#41865370">next</a><span>|</span><label class="collapse" for="c-41866672">[-]</label><label class="expand" for="c-41866672">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Tensor_Processing_Unit#Products" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Tensor_Processing_Unit#Product...</a> shows the first one is from 2015 (publicly announced in 2016). It also shows they have a TDP of 75+W.<p>I can’t find TDP for Apple’s Neural Engine (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Neural_Engine" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Neural_Engine</a>), but the first version shipped in the iPhone 8, which has a 7 Wh battery, so these are targeting different markets.</div><br/></div></div></div></div><div id="41865370" class="c"><input type="checkbox" id="c-41865370" checked=""/><div class="controls bullet"><span class="by">GeekyBear</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41864882">parent</a><span>|</span><a href="#41865486">prev</a><span>|</span><a href="#41865956">next</a><span>|</span><label class="collapse" for="c-41865370">[-]</label><label class="expand" for="c-41865370">[1 more]</label></div><br/><div class="children"><div class="content">Face ID was the first tent pole feature that ran on the NPU.</div><br/></div></div></div></div><div id="41865956" class="c"><input type="checkbox" id="c-41865956" checked=""/><div class="controls bullet"><span class="by">bdd8f1df777b</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41864659">parent</a><span>|</span><a href="#41864882">prev</a><span>|</span><a href="#41867045">next</a><span>|</span><label class="collapse" for="c-41865956">[-]</label><label class="expand" for="c-41865956">[1 more]</label></div><br/><div class="children"><div class="content">Even if it were true, they wouldn’t have the same influence as Apple has.</div><br/></div></div></div></div></div></div><div id="41867045" class="c"><input type="checkbox" id="c-41867045" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41863552">parent</a><span>|</span><a href="#41863644">prev</a><span>|</span><a href="#41865529">next</a><span>|</span><label class="collapse" for="c-41867045">[-]</label><label class="expand" for="c-41867045">[1 more]</label></div><br/><div class="children"><div class="content">yeah I&#x27;m not sure being 1% utilised helps power consumption</div><br/></div></div><div id="41865529" class="c"><input type="checkbox" id="c-41865529" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41863552">parent</a><span>|</span><a href="#41867045">prev</a><span>|</span><a href="#41864928">next</a><span>|</span><label class="collapse" for="c-41865529">[-]</label><label class="expand" for="c-41865529">[5 more]</label></div><br/><div class="children"><div class="content">There are no <i>nerves</i> in a <i>neural</i> processing unit, so yes: It&#x27;s 300% bullshit marketing.</div><br/><div id="41865788" class="c"><input type="checkbox" id="c-41865788" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865529">parent</a><span>|</span><a href="#41865574">next</a><span>|</span><label class="collapse" for="c-41865788">[-]</label><label class="expand" for="c-41865788">[3 more]</label></div><br/><div class="children"><div class="content"><i>Neural</i> is an adjective. Adjectives do not require their associated nouns to be present. See also: digital computers have mo fingers at all.</div><br/><div id="41865928" class="c"><input type="checkbox" id="c-41865928" checked=""/><div class="controls bullet"><span class="by">-mlv</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865788">parent</a><span>|</span><a href="#41865574">next</a><span>|</span><label class="collapse" for="c-41865928">[-]</label><label class="expand" for="c-41865928">[2 more]</label></div><br/><div class="children"><div class="content">I always thought &#x27;digital&#x27; referred to numbers, not fingers.</div><br/><div id="41865964" class="c"><input type="checkbox" id="c-41865964" checked=""/><div class="controls bullet"><span class="by">bdd8f1df777b</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865928">parent</a><span>|</span><a href="#41865574">next</a><span>|</span><label class="collapse" for="c-41865964">[-]</label><label class="expand" for="c-41865964">[1 more]</label></div><br/><div class="children"><div class="content">The derivative meaning has been use so widely that it has surpassed its original one in usage. But it doesn’t change the fact that it originally refers to the fingers.</div><br/></div></div></div></div></div></div><div id="41865574" class="c"><input type="checkbox" id="c-41865574" checked=""/><div class="controls bullet"><span class="by">jcgrillo</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865529">parent</a><span>|</span><a href="#41865788">prev</a><span>|</span><a href="#41864928">next</a><span>|</span><label class="collapse" for="c-41865574">[-]</label><label class="expand" for="c-41865574">[1 more]</label></div><br/><div class="children"><div class="content">Maybe the N secretly stands for NFT.. Like the tesla self driving hardware only smaller and made of silicon.</div><br/></div></div></div></div></div></div><div id="41864928" class="c"><input type="checkbox" id="c-41864928" checked=""/><div class="controls bullet"><span class="by">theresistor</span><span>|</span><a href="#41863460">parent</a><span>|</span><a href="#41863552">prev</a><span>|</span><a href="#41863639">next</a><span>|</span><label class="collapse" for="c-41864928">[-]</label><label class="expand" for="c-41864928">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Also, people often mistake the reason for an NPU is &quot;speed&quot;. That&#x27;s not correct. The whole point of the NPU is rather to focus on low power consumption.<p>It&#x27;s also often about offload. Depending on the use case, the CPU and GPU may be busy with other tasks, so the NPU is free bandwidth that can be used without stealing from the others. Consider AI-powered photo filters: the GPU is probably busy rendering the preview, and the CPU is busy drawing UI and handling user inputs.</div><br/><div id="41865137" class="c"><input type="checkbox" id="c-41865137" checked=""/><div class="controls bullet"><span class="by">cakoose</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41864928">parent</a><span>|</span><a href="#41863639">next</a><span>|</span><label class="collapse" for="c-41865137">[-]</label><label class="expand" for="c-41865137">[4 more]</label></div><br/><div class="children"><div class="content">Offload only makes sense if there are other advantages, e.g. speed, power.<p>Without those, wouldn&#x27;t it be better to use the NPUs silicon budget on more CPU?</div><br/><div id="41865703" class="c"><input type="checkbox" id="c-41865703" checked=""/><div class="controls bullet"><span class="by">theresistor</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865137">parent</a><span>|</span><a href="#41865175">next</a><span>|</span><label class="collapse" for="c-41865703">[-]</label><label class="expand" for="c-41865703">[1 more]</label></div><br/><div class="children"><div class="content">If you know that you need to offload matmuls, then building matmul hardware is more area efficient than adding an entire extra CPU. Various intermediate points exist along that spectrum, e.g. Cell&#x27;s SPUs.</div><br/></div></div><div id="41865175" class="c"><input type="checkbox" id="c-41865175" checked=""/><div class="controls bullet"><span class="by">heavyset_go</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865137">parent</a><span>|</span><a href="#41865703">prev</a><span>|</span><a href="#41865735">next</a><span>|</span><label class="collapse" for="c-41865175">[-]</label><label class="expand" for="c-41865175">[1 more]</label></div><br/><div class="children"><div class="content">More CPU means siphoning off more of the power budget on mobile devices. The theoretical value of NPUs is power efficiency on a limited budget.</div><br/></div></div><div id="41865735" class="c"><input type="checkbox" id="c-41865735" checked=""/><div class="controls bullet"><span class="by">avianlyric</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865137">parent</a><span>|</span><a href="#41865175">prev</a><span>|</span><a href="#41863639">next</a><span>|</span><label class="collapse" for="c-41865735">[-]</label><label class="expand" for="c-41865735">[1 more]</label></div><br/><div class="children"><div class="content">Not really. To get extra CPU performance that likely means more cores, or some other general compute silicon. That stuff tends to be quite big, simply because it’s so flexible.<p>NPUs focus on one specific type of computation, matrix multiplication, and usually with low precision integers, because that’s all a neural net needs. That vast reduction in flexibility means you can take lots of shortcuts in your design, allowing you cram more compute into a smaller footprint.<p>If you look at the M1 chip[1], you can see the entire 16-Neural engine has a foot print about the size of 4 performance cores (excluding their caches). It’s not perfect comparison, without numbers on what the performance core can achieve in terms of ops&#x2F;second vs the Neural Engine. But it seems reasonable to be that the Neural Engine and handily outperform the performance core complex when doing matmul operations.<p>[1] <a href="https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;16226&#x2F;apple-silicon-m1-a14-deep-dive" rel="nofollow">https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;16226&#x2F;apple-silicon-m1-a14-de...</a></div><br/></div></div></div></div></div></div><div id="41863639" class="c"><input type="checkbox" id="c-41863639" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#41863460">parent</a><span>|</span><a href="#41864928">prev</a><span>|</span><a href="#41864933">next</a><span>|</span><label class="collapse" for="c-41863639">[-]</label><label class="expand" for="c-41863639">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I think some hardware vendors just release the compute units without shipping proper support yet<p>This is Nvidia&#x27;s moat. Everything has optimized kernels for CUDA, and <i>maybe</i> Apple Accelerate (which is the only way to touch the CPU matrix unit before M4, and the NPU at all). If you want to use anything else, either prepare to upstream patches in your ML framework of choice or prepare to write your own training and inference code.</div><br/></div></div><div id="41864933" class="c"><input type="checkbox" id="c-41864933" checked=""/><div class="controls bullet"><span class="by">spookie</span><span>|</span><a href="#41863460">parent</a><span>|</span><a href="#41863639">prev</a><span>|</span><a href="#41864898">next</a><span>|</span><label class="collapse" for="c-41864933">[-]</label><label class="expand" for="c-41864933">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been building an app in pure C using onnxruntime, and it outperforms a comparable one done with python by a substancial amount. There are many other gains to be made.<p>(In the end python just calls C, but it&#x27;s pretty interesting how much performance is lost)</div><br/><div id="41867439" class="c"><input type="checkbox" id="c-41867439" checked=""/><div class="controls bullet"><span class="by">dacryn</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41864933">parent</a><span>|</span><a href="#41864898">next</a><span>|</span><label class="collapse" for="c-41867439">[-]</label><label class="expand" for="c-41867439">[2 more]</label></div><br/><div class="children"><div class="content">agree there, but then again using ort in Rust is faster again.<p>You cannot compare python with a onxx executor.<p>I don&#x27;t know what you used in Python, but if it&#x27;s pytorch or similar, those are built with flexibility in mind, for optimal performance you want to export those to onxx and use whatever executor that is optimized for your env. onxxruntime is one of them, but definitely not the only one, and given it&#x27;s from Microsoft, some prefer to avoid it and choose among the many free alternatives.</div><br/><div id="41867667" class="c"><input type="checkbox" id="c-41867667" checked=""/><div class="controls bullet"><span class="by">rerdavies</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41867439">parent</a><span>|</span><a href="#41864898">next</a><span>|</span><label class="collapse" for="c-41867667">[-]</label><label class="expand" for="c-41867667">[1 more]</label></div><br/><div class="children"><div class="content">Why would the two not be entirely comparable? PyTorch may be slower at building the models; but once the model is compiled and loaded on the NPU, there&#x27;s just not a whole lot of Python involved anymore. A few hundred CPU cycles to push the input data using python; a few hundred CPU cycles to receive the results using python. And everything in-between gets executed on the NPU.</div><br/></div></div></div></div></div></div><div id="41864898" class="c"><input type="checkbox" id="c-41864898" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41863460">parent</a><span>|</span><a href="#41864933">prev</a><span>|</span><a href="#41866594">next</a><span>|</span><label class="collapse" for="c-41864898">[-]</label><label class="expand" for="c-41864898">[3 more]</label></div><br/><div class="children"><div class="content">They definitely aren&#x27;t doing the timing properly, but also what you might think is timing is not what is generally marketed. But I will say, those marketed versions are often easier to compare. One such example is that if you&#x27;re using GPU then have you actually considered that there&#x27;s an asynchronous operation as part of your timing?<p>If you&#x27;re naively doing `time.time()` then what happens is this<p><pre><code>  start = time.time() # cpu records time
  pred = model(input.cuda()).cuda() # push data and model (if not already there) to GPU memory and start computation. This is asynchronous
  end = time.time() # cpu records time, regardless of if pred stores data
</code></pre>
You probably aren&#x27;t expecting that if you don&#x27;t know systems and hardware. But python (and really any language) is designed to be smart and compile into more optimized things than what you actually wrote. There&#x27;s no lock, and so we&#x27;re not going to block operations for cpu tasks. You might ask why do this? Well no one knows what you actually want to do. And do you want the timer library now checking for accelerators (i.e. GPU) every time it records a time? That&#x27;s going to mess up your timer! (at best you&#x27;d have to do a constructor to say &quot;enable locking for this accelerator&quot;) So you gotta do something a bit more nuanced.<p>If you want to actually time GPU tasks, you should look at cuda event timers (in pytorch this is `torch.cuda.Event(enable_timing=True)`. I have another comment with boilerplate)<p>Edit:<p>There&#x27;s also complicated issues like memory size and shape. They definitely are not being nice to the NPU here on either of those. They (and GPUs!!!) want channels last. They did [1,6,1500,1500] but you&#x27;d want [1,1500,1500,6]. There&#x27;s also the issue of how memory is allocated (and they noted IO being an issue). 1500 is a weird number (as is 6) so they aren&#x27;t doing any favors to the NPU, and I wouldn&#x27;t be surprised that this is a surprisingly big hit considering how new these things are<p>And here&#x27;s my longer comment with more details: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41864828">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41864828</a></div><br/><div id="41865375" class="c"><input type="checkbox" id="c-41865375" checked=""/><div class="controls bullet"><span class="by">artemisart</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41864898">parent</a><span>|</span><a href="#41866594">next</a><span>|</span><label class="collapse" for="c-41865375">[-]</label><label class="expand" for="c-41865375">[2 more]</label></div><br/><div class="children"><div class="content">Important precision: the async part is absolutely not python specific, but comes from CUDA, indeed for performance, and you will have to use cuda events too in C++ to properly time it.<p>For ONNX the runtimes I know of are synchronous as we don&#x27;t do each operation individually but whole models at once, there is no need for async, the timings should be correct.</div><br/><div id="41865495" class="c"><input type="checkbox" id="c-41865495" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41863460">root</a><span>|</span><a href="#41865375">parent</a><span>|</span><a href="#41866594">next</a><span>|</span><label class="collapse" for="c-41865495">[-]</label><label class="expand" for="c-41865495">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it isn&#x27;t python, it is... hardware. Not even CUDA specific. It is about memory moving around and optimization (remember, even the CPUs do speculative execution). I say a little more in the larger comment.<p>I&#x27;m less concerned about the CPU baseline and more concerned about the NPU timing. Especially given the other issues</div><br/></div></div></div></div></div></div></div></div><div id="41863390" class="c"><input type="checkbox" id="c-41863390" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#41863460">prev</a><span>|</span><a href="#41863546">next</a><span>|</span><label class="collapse" for="c-41863390">[-]</label><label class="expand" for="c-41863390">[52 more]</label></div><br/><div class="children"><div class="content">These NPUs are tying up a substantial amount of silicon area so it would be a real shame if they end up not being used for much. I can&#x27;t find a die analysis of the Snapdragon X which isolates the NPU specifically but AMDs equivalent with the same ~50 TOPS performance target can be seen here, and takes up about as much area as three high performance CPU cores:<p><a href="https:&#x2F;&#x2F;www.techpowerup.com&#x2F;325035&#x2F;amd-strix-point-silicon-pictured-and-annotated#g325035-2" rel="nofollow">https:&#x2F;&#x2F;www.techpowerup.com&#x2F;325035&#x2F;amd-strix-point-silicon-p...</a></div><br/><div id="41863905" class="c"><input type="checkbox" id="c-41863905" checked=""/><div class="controls bullet"><span class="by">ezst</span><span>|</span><a href="#41863390">parent</a><span>|</span><a href="#41863880">next</a><span>|</span><label class="collapse" for="c-41863905">[-]</label><label class="expand" for="c-41863905">[23 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t wait for the LLM fad to be over so we get some sanity (and efficiency) back. I personally have no use for this extra hardware (&quot;GenAI&quot; doesn&#x27;t help me in any way nor supports any work-related tasks). Worse, most people have no use for that (and recent surveys even show predominant hostility towards AI creep). We shouldn&#x27;t be paying extra for that, it should be opt-in, and then it would become clear (by looking at the sales and how few are willing to pay a premium for &quot;AI&quot;) how overblown and unnecessary this is.</div><br/><div id="41865589" class="c"><input type="checkbox" id="c-41865589" checked=""/><div class="controls bullet"><span class="by">kalleboo</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41863905">parent</a><span>|</span><a href="#41865168">next</a><span>|</span><label class="collapse" for="c-41865589">[-]</label><label class="expand" for="c-41865589">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>most people have no use for that</i><p>Apple originally added their NPUs before the current LLM wave to support things like indexing your photo library so that objects and people are searchable. These features are still very popular. I don&#x27;t think these NPUs are fast enough for GenAI anyway.</div><br/><div id="41865887" class="c"><input type="checkbox" id="c-41865887" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41865589">parent</a><span>|</span><a href="#41865902">next</a><span>|</span><label class="collapse" for="c-41865887">[-]</label><label class="expand" for="c-41865887">[2 more]</label></div><br/><div class="children"><div class="content">MS Copilot and &quot;Apple Intelligence&quot; are running a small language model and image generation on the NPU so that should count as &quot;GenAI&quot;.</div><br/><div id="41866463" class="c"><input type="checkbox" id="c-41866463" checked=""/><div class="controls bullet"><span class="by">kalleboo</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41865887">parent</a><span>|</span><a href="#41865902">next</a><span>|</span><label class="collapse" for="c-41866463">[-]</label><label class="expand" for="c-41866463">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s still in beta so we&#x27;ll see how things go but I saw someone testing what Apple Intelligence ran on-device vs sent off to the &quot;private secure cloud&quot; and even stuff like text summaries were being sent to the cloud.</div><br/></div></div></div></div><div id="41865902" class="c"><input type="checkbox" id="c-41865902" checked=""/><div class="controls bullet"><span class="by">grugagag</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41865589">parent</a><span>|</span><a href="#41865887">prev</a><span>|</span><a href="#41865168">next</a><span>|</span><label class="collapse" for="c-41865902">[-]</label><label class="expand" for="c-41865902">[1 more]</label></div><br/><div class="children"><div class="content">I wish I could turn that off on my phone.</div><br/></div></div></div></div><div id="41865168" class="c"><input type="checkbox" id="c-41865168" checked=""/><div class="controls bullet"><span class="by">mardifoufs</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41863905">parent</a><span>|</span><a href="#41865589">prev</a><span>|</span><a href="#41865651">next</a><span>|</span><label class="collapse" for="c-41865168">[-]</label><label class="expand" for="c-41865168">[1 more]</label></div><br/><div class="children"><div class="content">NPUs were a thing (and a very common one in mobile CPUs too) way before the LLM craze.</div><br/></div></div><div id="41865651" class="c"><input type="checkbox" id="c-41865651" checked=""/><div class="controls bullet"><span class="by">jcgrillo</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41863905">parent</a><span>|</span><a href="#41865168">prev</a><span>|</span><a href="#41864134">next</a><span>|</span><label class="collapse" for="c-41865651">[-]</label><label class="expand" for="c-41865651">[2 more]</label></div><br/><div class="children"><div class="content">I just got an iphone and the whole photos thing is absolutely garbage. All I wanted to do was look through my damn photos and find one I took recently but it started playing some random music and organized them in no discernible order.. like it wasn&#x27;t the reverse time sorted.. Idk what kind of fucked up &quot;creative process&quot; came up with that bullshit but I sure wish they&#x27;d unfuck it stat.<p>The camera is real good though.</div><br/><div id="41866225" class="c"><input type="checkbox" id="c-41866225" checked=""/><div class="controls bullet"><span class="by">james_marks</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41865651">parent</a><span>|</span><a href="#41864134">next</a><span>|</span><label class="collapse" for="c-41866225">[-]</label><label class="expand" for="c-41866225">[1 more]</label></div><br/><div class="children"><div class="content">There’s an album called “Recents” that’s chronological and scrolled to the end.<p>“Recent” seems to mean everything; I’ve got 6k+ photos, I think since the last fresh install, which is many devices ago.<p>Sounds like the view you’re looking for and will stick as the default once you find it, but you do have to bat away some BS at first.</div><br/></div></div></div></div><div id="41864134" class="c"><input type="checkbox" id="c-41864134" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41863905">parent</a><span>|</span><a href="#41865651">prev</a><span>|</span><a href="#41863966">next</a><span>|</span><label class="collapse" for="c-41864134">[-]</label><label class="expand" for="c-41864134">[14 more]</label></div><br/><div class="children"><div class="content">I was telling someone this and they gave me link to a laptop with higher battery life and better performance than my own, but I kept explaining to them that the feature I cared most about was die size. They couldn&#x27;t understand it so I just had to leave them alone. Non-technical people don&#x27;t get it. Die size is what I care about. It&#x27;s a critical feature and so many mainstream companies are missing out on <i>my money</i> because they won&#x27;t optimize die size. Disgusting.</div><br/><div id="41864691" class="c"><input type="checkbox" id="c-41864691" checked=""/><div class="controls bullet"><span class="by">_zoltan_</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864134">parent</a><span>|</span><a href="#41864304">next</a><span>|</span><label class="collapse" for="c-41864691">[-]</label><label class="expand" for="c-41864691">[2 more]</label></div><br/><div class="children"><div class="content">News flash: you&#x27;re in the niche of the niche. People don&#x27;t care about die size.<p>I&#x27;d be willing to bet that the amount of money they are missing out on is miniscule and is by far offset by people&#x27;s money who care about other stuff. Like you know, performance and battery life, just to stick to your examples.</div><br/><div id="41865605" class="c"><input type="checkbox" id="c-41865605" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864691">parent</a><span>|</span><a href="#41864304">next</a><span>|</span><label class="collapse" for="c-41865605">[-]</label><label class="expand" for="c-41865605">[1 more]</label></div><br/><div class="children"><div class="content">That’s exactly what the poster is arguing- they are being sarcastic.</div><br/></div></div></div></div><div id="41864304" class="c"><input type="checkbox" id="c-41864304" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864134">parent</a><span>|</span><a href="#41864691">prev</a><span>|</span><a href="#41866907">next</a><span>|</span><label class="collapse" for="c-41864304">[-]</label><label class="expand" for="c-41864304">[6 more]</label></div><br/><div class="children"><div class="content">Is this a parody?<p>Why would anyone care about die size? And if you do why not get one of the many low power laptops with Atoms etc that do have small die size?</div><br/><div id="41864462" class="c"><input type="checkbox" id="c-41864462" checked=""/><div class="controls bullet"><span class="by">thfuran</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864304">parent</a><span>|</span><a href="#41864496">next</a><span>|</span><label class="collapse" for="c-41864462">[-]</label><label class="expand" for="c-41864462">[3 more]</label></div><br/><div class="children"><div class="content">Yes, they&#x27;re making fun of the comment they replied to.</div><br/><div id="41866239" class="c"><input type="checkbox" id="c-41866239" checked=""/><div class="controls bullet"><span class="by">singlepaynews</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864462">parent</a><span>|</span><a href="#41864496">next</a><span>|</span><label class="collapse" for="c-41866239">[-]</label><label class="expand" for="c-41866239">[2 more]</label></div><br/><div class="children"><div class="content">Would you do me the favor of explaining the joke?  I get the premise—nobody cares about die size, but the comment being mocked seems perfectly innocuous to me?  They want a laptop without an NPU b&#x2F;c according to link we get more out of CPU anyways?  What am I missing here?</div><br/><div id="41867686" class="c"><input type="checkbox" id="c-41867686" checked=""/><div class="controls bullet"><span class="by">michaelt</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41866239">parent</a><span>|</span><a href="#41864496">next</a><span>|</span><label class="collapse" for="c-41867686">[-]</label><label class="expand" for="c-41867686">[1 more]</label></div><br/><div class="children"><div class="content">It has been the norm for several decades to have hardware features that go unused.<p>The realities of mass manufacturing and supply chains and whatnot mean it&#x27;s cheaper to get a laptop with a webcam I don&#x27;t use, a fingerprint reader I don&#x27;t use, and an SD card reader I don&#x27;t use. It&#x27;s cheaper to get a CPU with integrated graphics I don&#x27;t use, a trusted execution environment I don&#x27;t use, remote management features I don&#x27;t use. It&#x27;s cheaper to get a discrete GPU with RGB LEDs I don&#x27;t use, directx support I don&#x27;t use, four outputs when I only need one. It&#x27;s cheaper to get a motherboard with integrated wifi than one without.</div><br/></div></div></div></div></div></div><div id="41864496" class="c"><input type="checkbox" id="c-41864496" checked=""/><div class="controls bullet"><span class="by">tedunangst</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864304">parent</a><span>|</span><a href="#41864462">prev</a><span>|</span><a href="#41864414">next</a><span>|</span><label class="collapse" for="c-41864496">[-]</label><label class="expand" for="c-41864496">[1 more]</label></div><br/><div class="children"><div class="content">No, no, no, you just don&#x27;t get it. The only thing Dell will sell me is a laptop 324mm wide, which is totally appalling, but if they offered me a laptop that&#x27;s 320mm wide, I&#x27;d immediately buy it. In my line of work, which is totally serious business, every millimeter counts.</div><br/></div></div><div id="41864414" class="c"><input type="checkbox" id="c-41864414" checked=""/><div class="controls bullet"><span class="by">throwaway48476</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864304">parent</a><span>|</span><a href="#41864496">prev</a><span>|</span><a href="#41866907">next</a><span>|</span><label class="collapse" for="c-41864414">[-]</label><label class="expand" for="c-41864414">[1 more]</label></div><br/><div class="children"><div class="content">Maybe through a game of telephone they confused die size and node size?</div><br/></div></div></div></div><div id="41866907" class="c"><input type="checkbox" id="c-41866907" checked=""/><div class="controls bullet"><span class="by">ezst</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864134">parent</a><span>|</span><a href="#41864304">prev</a><span>|</span><a href="#41864921">next</a><span>|</span><label class="collapse" for="c-41866907">[-]</label><label class="expand" for="c-41866907">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m fine with the mockery, I genuinely hadn&#x27;t realized that &quot;wanting to pay for what one needs&quot; was such a hot and controversial take.</div><br/></div></div><div id="41864921" class="c"><input type="checkbox" id="c-41864921" checked=""/><div class="controls bullet"><span class="by">waveBidder</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864134">parent</a><span>|</span><a href="#41866907">prev</a><span>|</span><a href="#41866254">next</a><span>|</span><label class="collapse" for="c-41864921">[-]</label><label class="expand" for="c-41864921">[3 more]</label></div><br/><div class="children"><div class="content">your satire is off base enough that people don&#x27;t understand it&#x27;s satire.</div><br/><div id="41866598" class="c"><input type="checkbox" id="c-41866598" checked=""/><div class="controls bullet"><span class="by">0xDEAFBEAD</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864921">parent</a><span>|</span><a href="#41865190">next</a><span>|</span><label class="collapse" for="c-41866598">[-]</label><label class="expand" for="c-41866598">[1 more]</label></div><br/><div class="children"><div class="content">Says a lot about HN that so many believed he was genuine.</div><br/></div></div><div id="41865190" class="c"><input type="checkbox" id="c-41865190" checked=""/><div class="controls bullet"><span class="by">heavyset_go</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864921">parent</a><span>|</span><a href="#41866598">prev</a><span>|</span><a href="#41866254">next</a><span>|</span><label class="collapse" for="c-41865190">[-]</label><label class="expand" for="c-41865190">[1 more]</label></div><br/><div class="children"><div class="content">The Poe&#x27;s Law means it&#x27;s working.</div><br/></div></div></div></div><div id="41866254" class="c"><input type="checkbox" id="c-41866254" checked=""/><div class="controls bullet"><span class="by">fijiaarone</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864134">parent</a><span>|</span><a href="#41864921">prev</a><span>|</span><a href="#41863966">next</a><span>|</span><label class="collapse" for="c-41866254">[-]</label><label class="expand" for="c-41866254">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I know what you mean.  I hate lugging around a big CPU core.</div><br/></div></div></div></div><div id="41863966" class="c"><input type="checkbox" id="c-41863966" checked=""/><div class="controls bullet"><span class="by">DrillShopper</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41863905">parent</a><span>|</span><a href="#41864134">prev</a><span>|</span><a href="#41863880">next</a><span>|</span><label class="collapse" for="c-41863966">[-]</label><label class="expand" for="c-41863966">[1 more]</label></div><br/><div class="children"><div class="content">Corporatized gains in the market from hype
Socialized losses in increased carbon emissions, upheaval from job loss, and higher prices on hardware.<p>The more they say the future will be better the more that it looks like the status quo.</div><br/></div></div></div></div><div id="41863880" class="c"><input type="checkbox" id="c-41863880" checked=""/><div class="controls bullet"><span class="by">Kon-Peki</span><span>|</span><a href="#41863390">parent</a><span>|</span><a href="#41863905">prev</a><span>|</span><a href="#41864412">next</a><span>|</span><label class="collapse" for="c-41863880">[-]</label><label class="expand" for="c-41863880">[12 more]</label></div><br/><div class="children"><div class="content">Modern chips have to dedicate a certain percentage of the die to dark silicon [1] (or else they melt&#x2F;throttle to uselessness), and these kinds of components count towards that amount.  So the point of these components is to be used, but not to be used too much.<p>Instead of an NPU, they could have used those transistors and die space for any number of things.  But they wouldn&#x27;t have put additional high performance CPU cores there - that would increase the power density too much and cause thermal issues that can only be solved with permanent throttling.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dark_silicon" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dark_silicon</a></div><br/><div id="41865813" class="c"><input type="checkbox" id="c-41865813" checked=""/><div class="controls bullet"><span class="by">jcgrillo</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41863880">parent</a><span>|</span><a href="#41864171">next</a><span>|</span><label class="collapse" for="c-41865813">[-]</label><label class="expand" for="c-41865813">[5 more]</label></div><br/><div class="children"><div class="content">Question--what&#x27;s to be lost by making your features sufficiently not dense to allow them to cool at full tilt?</div><br/><div id="41865917" class="c"><input type="checkbox" id="c-41865917" checked=""/><div class="controls bullet"><span class="by">AlotOfReading</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41865813">parent</a><span>|</span><a href="#41866644">next</a><span>|</span><label class="collapse" for="c-41865917">[-]</label><label class="expand" for="c-41865917">[3 more]</label></div><br/><div class="children"><div class="content">Messes with timing, among other things. A lot of those structures are relatively fixed blocks that are designed for specific sizes. Signals take more time to propagate longer distances, and longer conductors have worse properties. Dense and hot is faster and more broadly useful.</div><br/><div id="41866001" class="c"><input type="checkbox" id="c-41866001" checked=""/><div class="controls bullet"><span class="by">jcgrillo</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41865917">parent</a><span>|</span><a href="#41866644">next</a><span>|</span><label class="collapse" for="c-41866001">[-]</label><label class="expand" for="c-41866001">[2 more]</label></div><br/><div class="children"><div class="content">Interesting, so does that mean we&#x27;re basically out of runway without aggressive cooling?</div><br/><div id="41867074" class="c"><input type="checkbox" id="c-41867074" checked=""/><div class="controls bullet"><span class="by">joha4270</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41866001">parent</a><span>|</span><a href="#41866644">next</a><span>|</span><label class="collapse" for="c-41867074">[-]</label><label class="expand" for="c-41867074">[1 more]</label></div><br/><div class="children"><div class="content">No.<p>Every successive semiconductor node uses less power than the previous <i>per transistor</i> at the same clock speed. 
Its just that we then immediately use this headroom to pack more transistors closer and run them faster, so every chip keeps running into power limits, even if they continually do more with said power.</div><br/></div></div></div></div></div></div><div id="41866644" class="c"><input type="checkbox" id="c-41866644" checked=""/><div class="controls bullet"><span class="by">positr0n</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41865813">parent</a><span>|</span><a href="#41865917">prev</a><span>|</span><a href="#41864171">next</a><span>|</span><label class="collapse" for="c-41866644">[-]</label><label class="expand" for="c-41866644">[1 more]</label></div><br/><div class="children"><div class="content">Good discussion on how at multi GHz clock speeds, the speed of light is actually limiting on some circuit design choices: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=12384596">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=12384596</a></div><br/></div></div></div></div><div id="41864171" class="c"><input type="checkbox" id="c-41864171" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41863880">parent</a><span>|</span><a href="#41865813">prev</a><span>|</span><a href="#41864412">next</a><span>|</span><label class="collapse" for="c-41864171">[-]</label><label class="expand" for="c-41864171">[6 more]</label></div><br/><div class="children"><div class="content">If they aren&#x27;t being used it would be better to dedicate the space to more SRAM.</div><br/><div id="41864316" class="c"><input type="checkbox" id="c-41864316" checked=""/><div class="controls bullet"><span class="by">a2l3aQ</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864171">parent</a><span>|</span><a href="#41864412">next</a><span>|</span><label class="collapse" for="c-41864316">[-]</label><label class="expand" for="c-41864316">[5 more]</label></div><br/><div class="children"><div class="content">The point is parts of the CPU have to be off or throttled down when other components are under load to maintain TDP, adding cache that would almost certainly be being used defeats the point of that.</div><br/><div id="41864415" class="c"><input type="checkbox" id="c-41864415" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864316">parent</a><span>|</span><a href="#41866947">next</a><span>|</span><label class="collapse" for="c-41864415">[-]</label><label class="expand" for="c-41864415">[3 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t SRAM have much lower power density than logic with the same area though? Hence why AMD can get away with physically stacking cache on top of more cache in their X3D parts, without the bottom layer melting.</div><br/><div id="41864778" class="c"><input type="checkbox" id="c-41864778" checked=""/><div class="controls bullet"><span class="by">Kon-Peki</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864415">parent</a><span>|</span><a href="#41864937">next</a><span>|</span><label class="collapse" for="c-41864778">[-]</label><label class="expand" for="c-41864778">[1 more]</label></div><br/><div class="children"><div class="content">Yes, cache has a much lower power density and could have been a candidate for that space.<p>But I wasn’t on the design team and have no basis for second-guessing them.  I’m just saying that cramming more performance CPU cores onto this die isn’t a realistic option.</div><br/></div></div><div id="41864937" class="c"><input type="checkbox" id="c-41864937" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864415">parent</a><span>|</span><a href="#41864778">prev</a><span>|</span><a href="#41866947">next</a><span>|</span><label class="collapse" for="c-41864937">[-]</label><label class="expand" for="c-41864937">[1 more]</label></div><br/><div class="children"><div class="content">The SRAM that AMD is stacking also has the benefit of being last-level cache, so it doesn&#x27;t need to run at anywhere near the frequency and voltage that eg. L1 cache operates at.</div><br/></div></div></div></div><div id="41866947" class="c"><input type="checkbox" id="c-41866947" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864316">parent</a><span>|</span><a href="#41864415">prev</a><span>|</span><a href="#41864412">next</a><span>|</span><label class="collapse" for="c-41866947">[-]</label><label class="expand" for="c-41866947">[1 more]</label></div><br/><div class="children"><div class="content">Cache doesn&#x27;t use nearly as much power as active computation; that was my point.</div><br/></div></div></div></div></div></div></div></div><div id="41864412" class="c"><input type="checkbox" id="c-41864412" checked=""/><div class="controls bullet"><span class="by">JohnFen</span><span>|</span><a href="#41863390">parent</a><span>|</span><a href="#41863880">prev</a><span>|</span><a href="#41865466">next</a><span>|</span><label class="collapse" for="c-41864412">[-]</label><label class="expand" for="c-41864412">[14 more]</label></div><br/><div class="children"><div class="content">&gt; These NPUs are tying up a substantial amount of silicon area so it would be a real shame if they end up not being used for much.<p>This has been my thinking. Today you have to go out of your way to buy a system with an NPU, so I don&#x27;t have any. But tomorrow, will they just be included by default? That seems like a waste for those of us who aren&#x27;t going to be running models. I wonder what other uses they could be put to?</div><br/><div id="41864488" class="c"><input type="checkbox" id="c-41864488" checked=""/><div class="controls bullet"><span class="by">jonas21</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864412">parent</a><span>|</span><a href="#41864879">next</a><span>|</span><label class="collapse" for="c-41864488">[-]</label><label class="expand" for="c-41864488">[4 more]</label></div><br/><div class="children"><div class="content">NPUs are already included by default in the Apple ecosystem. Nobody seems to mind.</div><br/><div id="41864903" class="c"><input type="checkbox" id="c-41864903" checked=""/><div class="controls bullet"><span class="by">acchow</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864488">parent</a><span>|</span><a href="#41864549">next</a><span>|</span><label class="collapse" for="c-41864903">[-]</label><label class="expand" for="c-41864903">[1 more]</label></div><br/><div class="children"><div class="content">It enables many features on the phone that people like, all without sending your personal data to the cloud. Like searching your photos for &quot;dog&quot; or &quot;receipt&quot;.</div><br/></div></div><div id="41864549" class="c"><input type="checkbox" id="c-41864549" checked=""/><div class="controls bullet"><span class="by">JohnFen</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864488">parent</a><span>|</span><a href="#41864903">prev</a><span>|</span><a href="#41865200">next</a><span>|</span><label class="collapse" for="c-41864549">[-]</label><label class="expand" for="c-41864549">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not really a question of minding if it&#x27;s there, unless its presence increases cost, anyway. It just seems a waste to let it go idle, so my mind wanders to what other use I could put that circuitry to.</div><br/></div></div><div id="41865200" class="c"><input type="checkbox" id="c-41865200" checked=""/><div class="controls bullet"><span class="by">shepherdjerred</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864488">parent</a><span>|</span><a href="#41864549">prev</a><span>|</span><a href="#41864879">next</a><span>|</span><label class="collapse" for="c-41865200">[-]</label><label class="expand" for="c-41865200">[1 more]</label></div><br/><div class="children"><div class="content">I actually love that Apple includes this — especially now that they’re actually doing something with it via Apple Intelligence</div><br/></div></div></div></div><div id="41864879" class="c"><input type="checkbox" id="c-41864879" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864412">parent</a><span>|</span><a href="#41864488">prev</a><span>|</span><a href="#41865208">next</a><span>|</span><label class="collapse" for="c-41864879">[-]</label><label class="expand" for="c-41864879">[2 more]</label></div><br/><div class="children"><div class="content">Aren&#x27;t they used for speech recognition -- for dictation? Also for FaceID.<p>They&#x27;re useful for more things than just LLM&#x27;s.</div><br/><div id="41866451" class="c"><input type="checkbox" id="c-41866451" checked=""/><div class="controls bullet"><span class="by">JohnFen</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864879">parent</a><span>|</span><a href="#41865208">next</a><span>|</span><label class="collapse" for="c-41866451">[-]</label><label class="expand" for="c-41866451">[1 more]</label></div><br/><div class="children"><div class="content">Yes, but I&#x27;m not interested in those sorts of uses. I&#x27;m wondering what else an NPU could be used for. I don&#x27;t know what an NPU actually is at a technical level, so I&#x27;m ignorant of the possibilities.</div><br/></div></div></div></div><div id="41865208" class="c"><input type="checkbox" id="c-41865208" checked=""/><div class="controls bullet"><span class="by">heavyset_go</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864412">parent</a><span>|</span><a href="#41864879">prev</a><span>|</span><a href="#41864427">next</a><span>|</span><label class="collapse" for="c-41865208">[-]</label><label class="expand" for="c-41865208">[3 more]</label></div><br/><div class="children"><div class="content">The idea is that your OS and apps will integrate ML models, so you will be running models whether you know it or not.</div><br/><div id="41866421" class="c"><input type="checkbox" id="c-41866421" checked=""/><div class="controls bullet"><span class="by">JohnFen</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41865208">parent</a><span>|</span><a href="#41864427">next</a><span>|</span><label class="collapse" for="c-41866421">[-]</label><label class="expand" for="c-41866421">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m confident that I&#x27;ll be able to know and control whether or not my Linux and BSD machines will be using ML models.</div><br/><div id="41866482" class="c"><input type="checkbox" id="c-41866482" checked=""/><div class="controls bullet"><span class="by">hollerith</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41866421">parent</a><span>|</span><a href="#41864427">next</a><span>|</span><label class="collapse" for="c-41866482">[-]</label><label class="expand" for="c-41866482">[1 more]</label></div><br/><div class="children"><div class="content">--and whether anyone is using your interactions with your computer to train a model.</div><br/></div></div></div></div></div></div><div id="41864427" class="c"><input type="checkbox" id="c-41864427" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864412">parent</a><span>|</span><a href="#41865208">prev</a><span>|</span><a href="#41865384">next</a><span>|</span><label class="collapse" for="c-41864427">[-]</label><label class="expand" for="c-41864427">[3 more]</label></div><br/><div class="children"><div class="content">&gt; But tomorrow, will they just be included by default?<p>That&#x27;s already the way things are going due to Microsoft decreeing that Copilot+ is the future of Windows, so AMD and Intel are both putting NPUs which meet the Copilot+ performance standard into every consumer part they make going forwards to secure OEM sales.</div><br/><div id="41864643" class="c"><input type="checkbox" id="c-41864643" checked=""/><div class="controls bullet"><span class="by">AlexAndScripts</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864427">parent</a><span>|</span><a href="#41865384">next</a><span>|</span><label class="collapse" for="c-41864643">[-]</label><label class="expand" for="c-41864643">[2 more]</label></div><br/><div class="children"><div class="content">It almost makes me want to find some use for them on my Linux box (not that is has an NPU), but I truly can&#x27;t think of anything. Too small to run a meaningful LLM, and I&#x27;d want that in bursts anyway, I hate voice controls (at least with the current tech), and Recall sounds thoroughly useless. Could you do mediocre machine translation on it, perhaps? Local github copilot? An LLM that is purely used to build an abstract index of my notes in the background?<p>Actually, could they be used to make better AI in games? That&#x27;d be neat. A shooter character with some kind of organic tactics, or a Civilisation&#x2F;Stellaris AI that doesn&#x27;t suck.</div><br/><div id="41867610" class="c"><input type="checkbox" id="c-41867610" checked=""/><div class="controls bullet"><span class="by">ywvcbk</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864643">parent</a><span>|</span><a href="#41865384">next</a><span>|</span><label class="collapse" for="c-41867610">[-]</label><label class="expand" for="c-41867610">[1 more]</label></div><br/><div class="children"><div class="content">&gt; box<p>Presumably you have a GPU? If so there is nothing an NPU can do that a discrete GPU can’t (and it would be much slower than a recent GPU).<p>The real benefits are power efficiency and cost since they are built into the SoC which are not necessarily that useful on a desktop PC.</div><br/></div></div></div></div></div></div><div id="41865384" class="c"><input type="checkbox" id="c-41865384" checked=""/><div class="controls bullet"><span class="by">idunnoman1222</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41864412">parent</a><span>|</span><a href="#41864427">prev</a><span>|</span><a href="#41865466">next</a><span>|</span><label class="collapse" for="c-41865384">[-]</label><label class="expand" for="c-41865384">[1 more]</label></div><br/><div class="children"><div class="content">Voice to text</div><br/></div></div></div></div><div id="41865466" class="c"><input type="checkbox" id="c-41865466" checked=""/><div class="controls bullet"><span class="by">kllrnohj</span><span>|</span><a href="#41863390">parent</a><span>|</span><a href="#41864412">prev</a><span>|</span><a href="#41863546">next</a><span>|</span><label class="collapse" for="c-41865466">[-]</label><label class="expand" for="c-41865466">[2 more]</label></div><br/><div class="children"><div class="content">Snapdragon X still has a full 12 cores (all same cores, it&#x27;s homogeneous) and the Strix Point is also 12 cores but in a 4+8 configuration but with the &quot;little&quot; cores not sacrificing that much (nothing like the little cores in ARM&#x27;s designs which might as well not even exist, they are a complete waste of silicon). Consumer software doesn&#x27;t scale to that, so what are you going to do with more transistors allocated to the CPU?<p>It&#x27;s not unlike why Apple puts so many video engines in their SoCs - they don&#x27;t actually have much else to do with the transistor budget they can afford. Making single thread performance better isn&#x27;t limited by transistor count anymore and software is bad at multithreading.</div><br/><div id="41865909" class="c"><input type="checkbox" id="c-41865909" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#41863390">root</a><span>|</span><a href="#41865466">parent</a><span>|</span><a href="#41863546">next</a><span>|</span><label class="collapse" for="c-41865909">[-]</label><label class="expand" for="c-41865909">[1 more]</label></div><br/><div class="children"><div class="content">GPU &quot;infinity&quot; cache would increase 3D performance and there&#x27;s a rumor that AMD removed it to make room for the NPU. They&#x27;re not out of ideas for features to put on the chip.</div><br/></div></div></div></div></div></div><div id="41863546" class="c"><input type="checkbox" id="c-41863546" checked=""/><div class="controls bullet"><span class="by">eightysixfour</span><span>|</span><a href="#41863390">prev</a><span>|</span><a href="#41863883">next</a><span>|</span><label class="collapse" for="c-41863546">[-]</label><label class="expand" for="c-41863546">[40 more]</label></div><br/><div class="children"><div class="content">I thought the purpose of these things was not to be fast, but to be able to run small models with very little power usage? I have a newer AMD laptop with an NPU, and my power usage doesn&#x27;t change using the video effects that supposedly run on it, but goes up when using the nvidia studio effects.<p>It seems like the NPUs are for very optimized models that do small tasks, like eye contact, background blur, autocorrect models, transcription, and OCR. In particular, on Windows, I assumed they were running the full screen OCR (and maybe embeddings for search) for the rewind feature.</div><br/><div id="41863779" class="c"><input type="checkbox" id="c-41863779" checked=""/><div class="controls bullet"><span class="by">boomskats</span><span>|</span><a href="#41863546">parent</a><span>|</span><a href="#41863632">next</a><span>|</span><label class="collapse" for="c-41863779">[-]</label><label class="expand" for="c-41863779">[23 more]</label></div><br/><div class="children"><div class="content">That&#x27;s especially true because yours is a Xilinx FPGA. The one that they just attached to the latest gen mobile ryzens is 5x more capable too.<p>AMD are doing some fantastic work at the moment, they just don&#x27;t seem to be shouting about it. This one is particularly interesting <a href="https:&#x2F;&#x2F;lore.kernel.org&#x2F;lkml&#x2F;DM6PR12MB3993D5ECA50B27682AEBE19FCD67A@DM6PR12MB3993.namprd12.prod.outlook.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lore.kernel.org&#x2F;lkml&#x2F;DM6PR12MB3993D5ECA50B27682AEBE1...</a><p>edit: not an FPGA. TIL. :&#x27;(</div><br/><div id="41863852" class="c"><input type="checkbox" id="c-41863852" checked=""/><div class="controls bullet"><span class="by">errantspark</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863779">parent</a><span>|</span><a href="#41864048">next</a><span>|</span><label class="collapse" for="c-41863852">[-]</label><label class="expand" for="c-41863852">[16 more]</label></div><br/><div class="children"><div class="content">Wait sorry back up a bit here. I can buy a laptop that has a daughter FPGA in it? Does it have GPIO??? Are we seriously building hardware worth buying again in 2024? Do you have a link?</div><br/><div id="41863959" class="c"><input type="checkbox" id="c-41863959" checked=""/><div class="controls bullet"><span class="by">eightysixfour</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863852">parent</a><span>|</span><a href="#41864293">next</a><span>|</span><label class="collapse" for="c-41863959">[-]</label><label class="expand" for="c-41863959">[12 more]</label></div><br/><div class="children"><div class="content">It isn&#x27;t as fun as you think - they are setup for specific use cases and quite small. Here&#x27;s a link to the software page: <a href="https:&#x2F;&#x2F;ryzenai.docs.amd.com&#x2F;en&#x2F;latest&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;ryzenai.docs.amd.com&#x2F;en&#x2F;latest&#x2F;index.html</a><p>The teeny-tiny &quot;NPU,&quot; which is actually an FPGA, is 10 TOPS.<p>Edit: I&#x27;ve been corrected, not an FPGA, just an IP block from Xilinx.</div><br/><div id="41864036" class="c"><input type="checkbox" id="c-41864036" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863959">parent</a><span>|</span><a href="#41864062">next</a><span>|</span><label class="collapse" for="c-41864036">[-]</label><label class="expand" for="c-41864036">[7 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a FPGA. It&#x27;s an NPU IP block from the Xilinx side of the company. It was presumably originally developed to be run on a Xilinx FPGA, but that doesn&#x27;t mean AMD did the stupid thing and actually fabbed a FPGA fabric instead of properly synthesizing the design for their laptop ASIC. Xilinx involvement does not automatically mean it&#x27;s an FPGA.</div><br/><div id="41864111" class="c"><input type="checkbox" id="c-41864111" checked=""/><div class="controls bullet"><span class="by">boomskats</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41864036">parent</a><span>|</span><a href="#41864064">next</a><span>|</span><label class="collapse" for="c-41864111">[-]</label><label class="expand" for="c-41864111">[5 more]</label></div><br/><div class="children"><div class="content">Do you have any more reading on this? How come the XDNA drivers depend on Xilinx&#x27; XRT runtime?</div><br/><div id="41864296" class="c"><input type="checkbox" id="c-41864296" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41864111">parent</a><span>|</span><a href="#41864232">next</a><span>|</span><label class="collapse" for="c-41864296">[-]</label><label class="expand" for="c-41864296">[2 more]</label></div><br/><div class="children"><div class="content">It would be surprising and strange if AMD <i>didn&#x27;t</i> reuse the software framework they&#x27;ve already built for doing AI when that IP block is instantiated on an FPGA fabric rather than hardened in an ASIC.</div><br/><div id="41864630" class="c"><input type="checkbox" id="c-41864630" checked=""/><div class="controls bullet"><span class="by">boomskats</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41864296">parent</a><span>|</span><a href="#41864232">next</a><span>|</span><label class="collapse" for="c-41864630">[-]</label><label class="expand" for="c-41864630">[1 more]</label></div><br/><div class="children"><div class="content">Well, I&#x27;m irrationally disappointed, but thanks. Appreciate the correction.</div><br/></div></div></div></div><div id="41864232" class="c"><input type="checkbox" id="c-41864232" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41864111">parent</a><span>|</span><a href="#41864296">prev</a><span>|</span><a href="#41864064">next</a><span>|</span><label class="collapse" for="c-41864232">[-]</label><label class="expand" for="c-41864232">[2 more]</label></div><br/><div class="children"><div class="content">because XRT has a plugin architecture: XRT&lt;-shim plugin&lt;-kernel driver. The shims register themselves with XRT. The XDNA driver repo houses both the shim and the kernel driver.</div><br/><div id="41864611" class="c"><input type="checkbox" id="c-41864611" checked=""/><div class="controls bullet"><span class="by">boomskats</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41864232">parent</a><span>|</span><a href="#41864064">next</a><span>|</span><label class="collapse" for="c-41864611">[-]</label><label class="expand" for="c-41864611">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, that makes sense.</div><br/></div></div></div></div></div></div><div id="41864064" class="c"><input type="checkbox" id="c-41864064" checked=""/><div class="controls bullet"><span class="by">eightysixfour</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41864036">parent</a><span>|</span><a href="#41864111">prev</a><span>|</span><a href="#41864062">next</a><span>|</span><label class="collapse" for="c-41864064">[-]</label><label class="expand" for="c-41864064">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the correction, edited.</div><br/></div></div></div></div><div id="41864062" class="c"><input type="checkbox" id="c-41864062" checked=""/><div class="controls bullet"><span class="by">boomskats</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863959">parent</a><span>|</span><a href="#41864036">prev</a><span>|</span><a href="#41864293">next</a><span>|</span><label class="collapse" for="c-41864062">[-]</label><label class="expand" for="c-41864062">[4 more]</label></div><br/><div class="children"><div class="content">Yes, the one on the ryzen 7000 chips like the 7840u isn&#x27;t massive, but that&#x27;s the last gen model. The one they&#x27;ve just released with the HX370 chip is estimated at 50 TOPS, which is better than Qualcomm&#x27;s ARM flagship that this post is about. It&#x27;s a fivefold improvement in a single generation, it&#x27;s pretty exciting.<p>A̵n̵d̵ ̵i̵t̵&#x27;̵s̵ ̵a̵n̵ ̵F̵P̵G̵A̵ It&#x27;s not an FPGA</div><br/><div id="41864248" class="c"><input type="checkbox" id="c-41864248" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41864062">parent</a><span>|</span><a href="#41864293">next</a><span>|</span><label class="collapse" for="c-41864248">[-]</label><label class="expand" for="c-41864248">[3 more]</label></div><br/><div class="children"><div class="content">&gt; And it&#x27;s an FPGA.<p>nope it&#x27;s not.</div><br/><div id="41864925" class="c"><input type="checkbox" id="c-41864925" checked=""/><div class="controls bullet"><span class="by">boomskats</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41864248">parent</a><span>|</span><a href="#41864293">next</a><span>|</span><label class="collapse" for="c-41864925">[-]</label><label class="expand" for="c-41864925">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve just ordered myself a jump to conclusions mat.</div><br/><div id="41865072" class="c"><input type="checkbox" id="c-41865072" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41864925">parent</a><span>|</span><a href="#41864293">next</a><span>|</span><label class="collapse" for="c-41865072">[-]</label><label class="expand" for="c-41865072">[1 more]</label></div><br/><div class="children"><div class="content">Lol during grad school my advisor would frequently cut me off and try to jump to a conclusion, while I was explaining something technical often enough he was wrong. So I did really buy him one (off eBay or something). He wasn&#x27;t pleased.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41864293" class="c"><input type="checkbox" id="c-41864293" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863852">parent</a><span>|</span><a href="#41863959">prev</a><span>|</span><a href="#41864048">next</a><span>|</span><label class="collapse" for="c-41864293">[-]</label><label class="expand" for="c-41864293">[3 more]</label></div><br/><div class="children"><div class="content">If you want GPIOs, you don&#x27;t need (or want) an FPGA.<p>I don&#x27;t know the details of your use case, but I work with low level hardware driven by GPIOs and after a bit of investigation, concluded that having direect GPIO access in a modern PC was not necessary or desirable compared to the alternatives.</div><br/><div id="41866390" class="c"><input type="checkbox" id="c-41866390" checked=""/><div class="controls bullet"><span class="by">errantspark</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41864293">parent</a><span>|</span><a href="#41864048">next</a><span>|</span><label class="collapse" for="c-41866390">[-]</label><label class="expand" for="c-41866390">[2 more]</label></div><br/><div class="children"><div class="content">I get a lot of use out of the PRUs on the BeagleboneBlack, I would absolutely get use out of an FPGA in a laptop.</div><br/><div id="41866503" class="c"><input type="checkbox" id="c-41866503" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41866390">parent</a><span>|</span><a href="#41864048">next</a><span>|</span><label class="collapse" for="c-41866503">[-]</label><label class="expand" for="c-41866503">[1 more]</label></div><br/><div class="children"><div class="content">It makes more sense to me to just use the BeagleboneBlack in concert with the FPGA.  Unless you have highly specific compute or data movement needs that can&#x27;t be satisfied over a USB serial link.  If you have those needs, and you need a laptop, I guess an FPGA makes sense but that&#x27;s a teeny market.</div><br/></div></div></div></div></div></div></div></div><div id="41864048" class="c"><input type="checkbox" id="c-41864048" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863779">parent</a><span>|</span><a href="#41863852">prev</a><span>|</span><a href="#41863876">next</a><span>|</span><label class="collapse" for="c-41864048">[-]</label><label class="expand" for="c-41864048">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not an FPGA. It&#x27;s a VLIW DSP that Xilinx built to go into an FPGA-SoC to help run ML models.</div><br/><div id="41864242" class="c"><input type="checkbox" id="c-41864242" checked=""/><div class="controls bullet"><span class="by">almostgotcaught</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41864048">parent</a><span>|</span><a href="#41863876">next</a><span>|</span><label class="collapse" for="c-41864242">[-]</label><label class="expand" for="c-41864242">[1 more]</label></div><br/><div class="children"><div class="content">this is the correct answer. one of the compilers for this DSP is <a href="https:&#x2F;&#x2F;github.com&#x2F;Xilinx&#x2F;llvm-aie">https:&#x2F;&#x2F;github.com&#x2F;Xilinx&#x2F;llvm-aie</a>.</div><br/></div></div></div></div><div id="41863876" class="c"><input type="checkbox" id="c-41863876" checked=""/><div class="controls bullet"><span class="by">beeflet</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863779">parent</a><span>|</span><a href="#41864048">prev</a><span>|</span><a href="#41865733">next</a><span>|</span><label class="collapse" for="c-41863876">[-]</label><label class="expand" for="c-41863876">[2 more]</label></div><br/><div class="children"><div class="content">It would be cool if most PCs had a general purpose FPGA that could be repurposed by the operating system. For example you could use it as a security processor like a TPM or as a bootrom, or you could repurpose it for DSP or something.<p>It just seems like this would be better in terms of firmware&#x2F;security&#x2F;bootloading because you would be more able to fix it if an exploit gets discovered, and it would be leaner because different operating systems can implement their own stuff (for example linux might not want pluton in-chip security, windows might not want coreboot or linux-based boot, bare metal applications can have much simpler boot).</div><br/><div id="41864617" class="c"><input type="checkbox" id="c-41864617" checked=""/><div class="controls bullet"><span class="by">walterbell</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863876">parent</a><span>|</span><a href="#41865733">next</a><span>|</span><label class="collapse" for="c-41864617">[-]</label><label class="expand" for="c-41864617">[1 more]</label></div><br/><div class="children"><div class="content">Xilinx Artix 7-series PicoEVB fits in M.2 wifi slot and has an OSS toolchain, <a href="http:&#x2F;&#x2F;www.enjoy-digital.fr&#x2F;" rel="nofollow">http:&#x2F;&#x2F;www.enjoy-digital.fr&#x2F;</a></div><br/></div></div></div></div><div id="41865733" class="c"><input type="checkbox" id="c-41865733" checked=""/><div class="controls bullet"><span class="by">davemp</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863779">parent</a><span>|</span><a href="#41863876">prev</a><span>|</span><a href="#41864435">next</a><span>|</span><label class="collapse" for="c-41865733">[-]</label><label class="expand" for="c-41865733">[1 more]</label></div><br/><div class="children"><div class="content">Unfortunately FPGA fabric is ~2x less power efficient than equivalent ASIC logic at the same clock speeds last time I checked. So implementing general purpose logic on an FPGA is not usually the right option even if you don’t care about FMAX or transistor counts.</div><br/></div></div><div id="41864435" class="c"><input type="checkbox" id="c-41864435" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863779">parent</a><span>|</span><a href="#41865733">prev</a><span>|</span><a href="#41863632">next</a><span>|</span><label class="collapse" for="c-41864435">[-]</label><label class="expand" for="c-41864435">[1 more]</label></div><br/><div class="children"><div class="content">Sorry for an OT comment but what is going on with that ascii art!? The content fits within 80 columns just fine[1], is it GPT generated?<p>1: <a href="https:&#x2F;&#x2F;pastebin.com&#x2F;raw&#x2F;R9BrqETR" rel="nofollow">https:&#x2F;&#x2F;pastebin.com&#x2F;raw&#x2F;R9BrqETR</a></div><br/></div></div></div></div><div id="41863632" class="c"><input type="checkbox" id="c-41863632" checked=""/><div class="controls bullet"><span class="by">conradev</span><span>|</span><a href="#41863546">parent</a><span>|</span><a href="#41863779">prev</a><span>|</span><a href="#41864828">next</a><span>|</span><label class="collapse" for="c-41863632">[-]</label><label class="expand" for="c-41863632">[5 more]</label></div><br/><div class="children"><div class="content">That is my understanding as well: low power and low latency.<p>You can see this in action when evaluating a CoreML model on a macOS machine. The ANE takes half as long as the GPU which takes half as long as the CPU (actual factors being model dependent)</div><br/><div id="41863665" class="c"><input type="checkbox" id="c-41863665" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863632">parent</a><span>|</span><a href="#41864828">next</a><span>|</span><label class="collapse" for="c-41863665">[-]</label><label class="expand" for="c-41863665">[4 more]</label></div><br/><div class="children"><div class="content">To take half as long, doesn’t it have to perform twice as fast? Or am I misreading your comment?</div><br/><div id="41863726" class="c"><input type="checkbox" id="c-41863726" checked=""/><div class="controls bullet"><span class="by">eightysixfour</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863665">parent</a><span>|</span><a href="#41865127">next</a><span>|</span><label class="collapse" for="c-41863726">[-]</label><label class="expand" for="c-41863726">[2 more]</label></div><br/><div class="children"><div class="content">No, you can have latency that is independent of compute performance. The CPU&#x2F;GPU may have other tasks and the work has to wait for the existing threads to finish, or for them to clock up, or have slower memory paths, etc.<p>If you and I have the same calculator but I&#x27;m working on a set of problems and you&#x27;re not, and we&#x27;re both asked to do some math, it may take me longer to return it, even though the instantaneous performance of the math is the same.</div><br/><div id="41863792" class="c"><input type="checkbox" id="c-41863792" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863726">parent</a><span>|</span><a href="#41865127">next</a><span>|</span><label class="collapse" for="c-41863792">[-]</label><label class="expand" for="c-41863792">[1 more]</label></div><br/><div class="children"><div class="content">In isolation, makes sense.<p>Wouldn&#x27;t it be odd for OP to present examples that are the <i>opposite</i> of their claim, just to get us thinking about &quot;well the CPU is busy?&quot;<p>Curious for their input.</div><br/></div></div></div></div><div id="41865127" class="c"><input type="checkbox" id="c-41865127" checked=""/><div class="controls bullet"><span class="by">conradev</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863665">parent</a><span>|</span><a href="#41863726">prev</a><span>|</span><a href="#41864828">next</a><span>|</span><label class="collapse" for="c-41865127">[-]</label><label class="expand" for="c-41865127">[1 more]</label></div><br/><div class="children"><div class="content">The GPU is stateful and requires loading shaders and initializing pipelines before doing any work. That is where its latency comes from. It is also extremely power hungry.<p>The CPU is zero latency to get started, but takes longer because it isn&#x27;t specialized at any one task and isn&#x27;t massively parallel, so that is why the CPU takes even longer.<p>The NPU often has a simpler bytecode to do more complex things like matrix multiplication implemented in hardware, rather than having to instantiate a generic compute kernel on the GPU.</div><br/></div></div></div></div></div></div><div id="41864828" class="c"><input type="checkbox" id="c-41864828" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41863546">parent</a><span>|</span><a href="#41863632">prev</a><span>|</span><a href="#41863821">next</a><span>|</span><label class="collapse" for="c-41864828">[-]</label><label class="expand" for="c-41864828">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; but to be able to run small models with very little power usage
</code></pre>
yes<p>But first, I should also say you probably don&#x27;t want to be programming these things with python. I doubt you&#x27;ll get good performance there, especially as the newness means optimizations haven&#x27;t been ported well (even using things like TensorRT is not going to be as fast as writing it from scratch, and Nvidia is throwing a lot of man power at that -- for good reason! But it sure as hell will get close and save you a lot of time writing).<p>They are, like you say, generally optimized for doing repeated similar tasks. That&#x27;s also where I suspect some of the info gathered here is inaccurate.<p><pre><code>  (I have not used these NPU chips so what follows is more educated guesses, but I&#x27;ll explain. Please correct me if I&#x27;ve made an error)
</code></pre>
Second, I don&#x27;t trust the timing here. I&#x27;m certain the CUDA timing (at the end) is incorrect, as the code written wouldn&#x27;t properly time. Timing is surprisingly not easy. I suspect the advertised operations are only counting operations directly on the NPU while OP would have included CPU operations in their NPU and GPU timings[0]. But the docs have benchmarking tools, so I suspect they&#x27;re doing something similar. I&#x27;d be interested to know the variance and how this holds after doing warmups.  <i>They do identify the IO as an issue, and so I think this is evidence of this being an issue.</i><p>Third, their data is improperly formatted.<p><pre><code>  MATRIX_COUNT, MATRIX_A, MATRIX_B, MATRIX_K = (6, 1500, 1500, 256)
  INPUT0_SHAPE = [1, MATRIX_COUNT, MATRIX_A, MATRIX_K]
  INPUT1_SHAPE = [1, MATRIX_COUNT, MATRIX_K, MATRIX_B]
  OUTPUT_SHAPE = [1, MATRIX_COUNT, MATRIX_A, MATRIX_B]
</code></pre>
You want &quot;channels last&quot; here. I suspected this (do this in pytorch too!) and the docs they link confirm.<p>1500 is also an odd choice and this could be cause for extra misses. I wonder how things would change with 1536, 2048, or even 256. Might (probably) even want to look smaller, since this might be a common preprocessing step. Your models are not processing full res images and if you&#x27;re going to optimize architecture for models, you&#x27;re going to use that shape information. Shape optimization is actually pretty important in ML[1]. I suspect this will be quite a large miss.<p>Fourth, a quick look at the docs and I think the setup is improper. Under &quot;Model Workflow&quot; they mention that they want data in 8 or 16 bit *<i>float*</i>. I&#x27;m not going to look too deep, but note that there are different types of floats (e.g. pytorch&#x27;s bfloat is not the same as torch.half or torch.float16). Mixed precision is still a confusing subject and if you&#x27;re hitting issues like these it is worth looking at. I very much suggest not just running a standard quantization procedure and calling it a day (start there! But don&#x27;t end there unless it&#x27;s &quot;good enough&quot;, which doesn&#x27;t seem too meaningful here.)<p>FWIW, I still do think these results are useful, but I think they need to be improved upon. This type of stuff is surprisingly complex, but a large amount of that is due to things being new and much of the details still being worked out. Remember that when you&#x27;re comparing to things like CPU or GPU (especially CUDA) that these have had hundreds of thousands of man hours put into then and at least tens of thousands into high level language libraries (i.e. python) to handle these. I don&#x27;t think these devices are ready for the average user where you can just work with them from your favorite language&#x27;s abstraction level, but they&#x27;re pretty useful if you&#x27;re willing to work close to the metal.<p>[0] I don&#x27;t know what the timing is for this, but I do this in pytorch a lot so here&#x27;s the boilerplate<p><pre><code>    times = torch.empty(rounds)
    # Don&#x27;t need use dummy data, but here
    input_data = torch.randn((batch_size, *data_shape), device=&quot;cuda&quot;)
    # Do some warmups first. There&#x27;s background actions dealing with IO we don&#x27;t want to measure
    #    You can remove that line and do a dist of times if you want to see this
    # Make sure you generate data and save to a variable (write) or else this won&#x27;t do anything
    for _ in range(warmup):
        data = model(input_data)
    for i in range(rounds):
        starter = torch.cuda.Event(enable_timing=True)
        ender = torch.cuda.Event(enable_timing=True)
        starter.record()
        data = model(input_data)
        ender.record()
        torch.cuda.synchronize()
        times[i] = starter.elapsed_time(ender)&#x2F;1000
    total_time = times.sum()
</code></pre>
The reason we do it this way is if we just wrap the model output with a timer then we&#x27;re looking at CPU time but the GPU operations are asynchronous so you could get deceptively fast (or slow) times<p>[1] <a href="https:&#x2F;&#x2F;www.thonking.ai&#x2F;p&#x2F;what-shapes-do-matrix-multiplications" rel="nofollow">https:&#x2F;&#x2F;www.thonking.ai&#x2F;p&#x2F;what-shapes-do-matrix-multiplicati...</a></div><br/></div></div><div id="41863821" class="c"><input type="checkbox" id="c-41863821" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#41863546">parent</a><span>|</span><a href="#41864828">prev</a><span>|</span><a href="#41864628">next</a><span>|</span><label class="collapse" for="c-41863821">[-]</label><label class="expand" for="c-41863821">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re absolutely right IMO, given what I heard when launching on-device speech recognition on Pixel, and after leaving Google, what I see from ex. Apple Neural Engine vs. CPU when running ONNX stuff.<p>I&#x27;m a bit suspicious of the article&#x27;s specific conclusion, because it is Qualcomm&#x27;s ONNX, and it be out of date. Also, Android loved talking shit about Qualcomm software engineering.<p>That being said, its directionally correct, insomuch as consumer hardware AI acceleration claims are near-universally BS unless you&#x27;re A) writing 1P software B) someone in the 1P really wants you to take advantage.</div><br/><div id="41864564" class="c"><input type="checkbox" id="c-41864564" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41863821">parent</a><span>|</span><a href="#41864628">next</a><span>|</span><label class="collapse" for="c-41864564">[-]</label><label class="expand" for="c-41864564">[2 more]</label></div><br/><div class="children"><div class="content">1P?</div><br/><div id="41864574" class="c"><input type="checkbox" id="c-41864574" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#41863546">root</a><span>|</span><a href="#41864564">parent</a><span>|</span><a href="#41864628">next</a><span>|</span><label class="collapse" for="c-41864574">[-]</label><label class="expand" for="c-41864574">[1 more]</label></div><br/><div class="children"><div class="content">First party, i.e. Google&#x2F;Apple&#x2F;Microsoft</div><br/></div></div></div></div></div></div></div></div><div id="41863883" class="c"><input type="checkbox" id="c-41863883" checked=""/><div class="controls bullet"><span class="by">protastus</span><span>|</span><a href="#41863546">prev</a><span>|</span><a href="#41863329">next</a><span>|</span><label class="collapse" for="c-41863883">[-]</label><label class="expand" for="c-41863883">[3 more]</label></div><br/><div class="children"><div class="content">Deploying a model on an NPU requires significant profile based optimization. Picking up a model that works fine on the CPU but hasn&#x27;t been optimized for an NPU usually leads to disappointing results.</div><br/><div id="41864649" class="c"><input type="checkbox" id="c-41864649" checked=""/><div class="controls bullet"><span class="by">CAP_NET_ADMIN</span><span>|</span><a href="#41863883">parent</a><span>|</span><a href="#41864613">next</a><span>|</span><label class="collapse" for="c-41864649">[-]</label><label class="expand" for="c-41864649">[1 more]</label></div><br/><div class="children"><div class="content">Beauty of CPUs - they&#x27;ll chew through whatever bs code you throw at them at a reasonable speed.</div><br/></div></div><div id="41864613" class="c"><input type="checkbox" id="c-41864613" checked=""/><div class="controls bullet"><span class="by">catgary</span><span>|</span><a href="#41863883">parent</a><span>|</span><a href="#41864649">prev</a><span>|</span><a href="#41863329">next</a><span>|</span><label class="collapse" for="c-41864613">[-]</label><label class="expand" for="c-41864613">[1 more]</label></div><br/><div class="children"><div class="content">Yeah whenever I’ve spoken to people who work on stuff like IREE or OpenXLA they gave me the impression that understanding how to use those compilers&#x2F;runtimes is an entire job.</div><br/></div></div></div></div><div id="41863329" class="c"><input type="checkbox" id="c-41863329" checked=""/><div class="controls bullet"><span class="by">fancyfredbot</span><span>|</span><a href="#41863883">prev</a><span>|</span><a href="#41865626">next</a><span>|</span><label class="collapse" for="c-41863329">[-]</label><label class="expand" for="c-41863329">[3 more]</label></div><br/><div class="children"><div class="content">The write up on the GitHub repo is much more informative than the blog.<p>When running int8 matmul using onnx performance is ~0.6TF.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;usefulsensors&#x2F;qc_npu_benchmark">https:&#x2F;&#x2F;github.com&#x2F;usefulsensors&#x2F;qc_npu_benchmark</a></div><br/><div id="41863591" class="c"><input type="checkbox" id="c-41863591" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#41863329">parent</a><span>|</span><a href="#41865626">next</a><span>|</span><label class="collapse" for="c-41863591">[-]</label><label class="expand" for="c-41863591">[2 more]</label></div><br/><div class="children"><div class="content">Thanks—we changed the URL to that from <a href="https:&#x2F;&#x2F;petewarden.com&#x2F;2024&#x2F;10&#x2F;16&#x2F;ai-pcs-arent-very-good-at-ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;petewarden.com&#x2F;2024&#x2F;10&#x2F;16&#x2F;ai-pcs-arent-very-good-at-...</a>. Readers may way want to look at both, of course!</div><br/><div id="41865253" class="c"><input type="checkbox" id="c-41865253" checked=""/><div class="controls bullet"><span class="by">dhruvdh</span><span>|</span><a href="#41863329">root</a><span>|</span><a href="#41863591">parent</a><span>|</span><a href="#41865626">next</a><span>|</span><label class="collapse" for="c-41865253">[-]</label><label class="expand" for="c-41865253">[1 more]</label></div><br/><div class="children"><div class="content">Oh, maybe also change the title? I flagged it because of the title&#x2F;url not matching.</div><br/></div></div></div></div></div></div><div id="41865626" class="c"><input type="checkbox" id="c-41865626" checked=""/><div class="controls bullet"><span class="by">cjbgkagh</span><span>|</span><a href="#41863329">prev</a><span>|</span><a href="#41867363">next</a><span>|</span><label class="collapse" for="c-41865626">[-]</label><label class="expand" for="c-41865626">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We&#x27;ve tried to avoid that by making both the input matrices more square, so that tiling and reuse should be possible.<p>While it might be possible it would not surprise me if a number of possible optimizations had not made it into Onnx. It appears that Qualcomm does not give direct access to the NPU and users are expected to use frameworks to convert models over to it, and in my experience conversion tools generally suck and leave a lot of optimizations on the table. It could be less of NPUs suck and more of the conversions tools suck. I&#x27;ll wait until I get direct access - I don&#x27;t trust conversion tools.<p>My view of NPUs is that they&#x27;re great for tiny ML models and very fast function approximations which is my intended use case. While LLMs are the new hotness there are huge number of specialized tasks that small models are really useful for.</div><br/><div id="41865847" class="c"><input type="checkbox" id="c-41865847" checked=""/><div class="controls bullet"><span class="by">jaygreco</span><span>|</span><a href="#41865626">parent</a><span>|</span><a href="#41867363">next</a><span>|</span><label class="collapse" for="c-41865847">[-]</label><label class="expand" for="c-41865847">[1 more]</label></div><br/><div class="children"><div class="content">I came here to say this. I haven’t worked with the Elite X but the past gen stuff I’ve used (865 mostly) the accelerators - compute DSP and much smaller NPU - required _very_ specific setup, compilation with a bespoke toolchain, and communication via RPC to name a few.<p>I would hope the NPU on Elite X is easier to get to considering the whole copilot+ thing, but I bring this up mainly to make the point that I doubt it’s just as easy as “run general purpose model, expect it to magically teleport onto the NPU”.</div><br/></div></div></div></div><div id="41867363" class="c"><input type="checkbox" id="c-41867363" checked=""/><div class="controls bullet"><span class="by">fschutze</span><span>|</span><a href="#41865626">prev</a><span>|</span><a href="#41867284">next</a><span>|</span><label class="collapse" for="c-41867363">[-]</label><label class="expand" for="c-41867363">[1 more]</label></div><br/><div class="children"><div class="content">Is there a possibility to use the Qualcomm SNPE SDK? I thought this SDK isn&#x27;t bad. Also, for those who have access to the Qualcomm NPU: Is the Hexagon SDK working properly? Do apps still need to be signed (which i never got to work) when using Hexagon?</div><br/></div></div><div id="41867284" class="c"><input type="checkbox" id="c-41867284" checked=""/><div class="controls bullet"><span class="by">woadwarrior01</span><span>|</span><a href="#41867363">prev</a><span>|</span><a href="#41866184">next</a><span>|</span><label class="collapse" for="c-41867284">[-]</label><label class="expand" for="c-41867284">[1 more]</label></div><br/><div class="children"><div class="content">IMO, benchmarking accelerator hardware with onnxruntime is like benchmarking a CPU with a Python script.<p>&gt; We&#x27;ve seen similar performance results to those shown here using the Qualcomm QNN SDK directly.<p>Why not include those results?</div><br/></div></div><div id="41866184" class="c"><input type="checkbox" id="c-41866184" checked=""/><div class="controls bullet"><span class="by">_davide_</span><span>|</span><a href="#41867284">prev</a><span>|</span><a href="#41865681">next</a><span>|</span><label class="collapse" for="c-41866184">[-]</label><label class="expand" for="c-41866184">[2 more]</label></div><br/><div class="children"><div class="content">The RTX 4080 should be capable of ~40 TFLOPS, yet they only report 2,160 billion operations per second. Shouldn&#x27;t this be enough to reconsider the benchmark?
They probably made some serious error in measuring FLOPS.
Regarding the fact that CPU beats NPU is possible but they should benchmark many matrix multiplications without any application synchronization in order to have a decent comparison.</div><br/><div id="41866435" class="c"><input type="checkbox" id="c-41866435" checked=""/><div class="controls bullet"><span class="by">Grimblewald</span><span>|</span><a href="#41866184">parent</a><span>|</span><a href="#41865681">next</a><span>|</span><label class="collapse" for="c-41866435">[-]</label><label class="expand" for="c-41866435">[1 more]</label></div><br/><div class="children"><div class="content">That isnt the half of it. A quick skim of the documentation shows that the cpu inference wasnt done in a comparable way either.</div><br/></div></div></div></div><div id="41865681" class="c"><input type="checkbox" id="c-41865681" checked=""/><div class="controls bullet"><span class="by">freehorse</span><span>|</span><a href="#41866184">prev</a><span>|</span><a href="#41865893">next</a><span>|</span><label class="collapse" for="c-41865681">[-]</label><label class="expand" for="c-41865681">[1 more]</label></div><br/><div class="children"><div class="content">I always thought that the main point of NPUs is energy efficiency (and being able to run ML models without taking over all computer resources, making it practical to integrate ML applications in the OS itself in ways that it does not disturb the user or the workflow) rather than being exceptionally faster. At least this has been my experience with running stable diffusion on macs. Similar to using other specialised hardware like media encoders; they are not necessarily faster than a CPU if you throw a dozen+ cpu cores on the task, but it will draw a minuscule part of the power.</div><br/></div></div><div id="41865893" class="c"><input type="checkbox" id="c-41865893" checked=""/><div class="controls bullet"><span class="by">guelermus</span><span>|</span><a href="#41865681">prev</a><span>|</span><a href="#41865083">next</a><span>|</span><label class="collapse" for="c-41865893">[-]</label><label class="expand" for="c-41865893">[1 more]</label></div><br/><div class="children"><div class="content">One should pay attention also to power efficiency, a direct comparison could be misleading here.</div><br/></div></div><div id="41865083" class="c"><input type="checkbox" id="c-41865083" checked=""/><div class="controls bullet"><span class="by">teilo</span><span>|</span><a href="#41865893">prev</a><span>|</span><a href="#41863839">next</a><span>|</span><label class="collapse" for="c-41865083">[-]</label><label class="expand" for="c-41865083">[4 more]</label></div><br/><div class="children"><div class="content">Actual article title: Benchmarking Qualcomm&#x27;s NPU on the Microsoft Surface Tablet<p>Because this isn&#x27;t about NPUs. It&#x27;s about a specific NPU, on a specific benchmark, with a specific set of libraries and frameworks. So basically, this proves nothing.</div><br/><div id="41865650" class="c"><input type="checkbox" id="c-41865650" checked=""/><div class="controls bullet"><span class="by">gnabgib</span><span>|</span><a href="#41865083">parent</a><span>|</span><a href="#41865206">next</a><span>|</span><label class="collapse" for="c-41865650">[-]</label><label class="expand" for="c-41865650">[1 more]</label></div><br/><div class="children"><div class="content">The title is from the original article (<a href="https:&#x2F;&#x2F;petewarden.com&#x2F;2024&#x2F;10&#x2F;16&#x2F;ai-pcs-arent-very-good-at-ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;petewarden.com&#x2F;2024&#x2F;10&#x2F;16&#x2F;ai-pcs-arent-very-good-at-...</a>), the URL was changed by dang: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41863591">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41863591</a></div><br/></div></div><div id="41865206" class="c"><input type="checkbox" id="c-41865206" checked=""/><div class="controls bullet"><span class="by">iml7</span><span>|</span><a href="#41865083">parent</a><span>|</span><a href="#41865650">prev</a><span>|</span><a href="#41863839">next</a><span>|</span><label class="collapse" for="c-41865206">[-]</label><label class="expand" for="c-41865206">[2 more]</label></div><br/><div class="children"><div class="content">But you can’t get more clicks. You have to attack enough people to get clicks.I feel like this place is becoming more and more filled with posts and titles like this.</div><br/><div id="41865622" class="c"><input type="checkbox" id="c-41865622" checked=""/><div class="controls bullet"><span class="by">gerdesj</span><span>|</span><a href="#41865083">root</a><span>|</span><a href="#41865206">parent</a><span>|</span><a href="#41863839">next</a><span>|</span><label class="collapse" for="c-41865622">[-]</label><label class="expand" for="c-41865622">[1 more]</label></div><br/><div class="children"><div class="content">Internet points are a bit crap but HN generally discusses things properly and off topic and downright weird stuff generally gets downvoted to doom.</div><br/></div></div></div></div></div></div><div id="41863839" class="c"><input type="checkbox" id="c-41863839" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#41865083">prev</a><span>|</span><a href="#41864797">next</a><span>|</span><label class="collapse" for="c-41863839">[-]</label><label class="expand" for="c-41863839">[1 more]</label></div><br/><div class="children"><div class="content">This headline is seriously misleading because the author did not test AMD or Intel NPUs. If Qualcomm is slow don&#x27;t say all AI PCs are not good.</div><br/></div></div><div id="41864797" class="c"><input type="checkbox" id="c-41864797" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#41863839">prev</a><span>|</span><a href="#41865019">next</a><span>|</span><label class="collapse" for="c-41864797">[-]</label><label class="expand" for="c-41864797">[2 more]</label></div><br/><div class="children"><div class="content">NPUs are efficient, not especially fast. The CPU is much bigger than the NPU and has better cache access. Of course it&#x27;ll perform better.</div><br/><div id="41865195" class="c"><input type="checkbox" id="c-41865195" checked=""/><div class="controls bullet"><span class="by">acdha</span><span>|</span><a href="#41864797">parent</a><span>|</span><a href="#41865019">next</a><span>|</span><label class="collapse" for="c-41865195">[-]</label><label class="expand" for="c-41865195">[1 more]</label></div><br/><div class="children"><div class="content">It’s more complicated than that (you’re assuming that the bigger CPU is optimized for the same workload) but it’s also irrelevant to the topic at hand: they’re seeing this NPU within a factor of 2-4 of the CPU, but if it performed half as well as Qualcomm claims it would be an order of magnitude faster. The story here isn’t another round of the specialized versus general debate but that they fell so far short of their marketing claims.</div><br/></div></div></div></div><div id="41865019" class="c"><input type="checkbox" id="c-41865019" checked=""/><div class="controls bullet"><span class="by">p1necone</span><span>|</span><a href="#41864797">prev</a><span>|</span><a href="#41863532">next</a><span>|</span><label class="collapse" for="c-41865019">[-]</label><label class="expand" for="c-41865019">[1 more]</label></div><br/><div class="children"><div class="content">I might be overly cynical but I just assumed that the entire purpose of &quot;AI PCs&quot; was marketing - of course they don&#x27;t actually achieve much. Any real hardware that&#x27;s supposedly for the &quot;AI&quot; features will actually be just special purpose hardware for literally anything the sales department can lump under that category.</div><br/></div></div><div id="41863532" class="c"><input type="checkbox" id="c-41863532" checked=""/><div class="controls bullet"><span class="by">jamesy0ung</span><span>|</span><a href="#41865019">prev</a><span>|</span><a href="#41864862">next</a><span>|</span><label class="collapse" for="c-41863532">[-]</label><label class="expand" for="c-41863532">[10 more]</label></div><br/><div class="children"><div class="content">What exactly does Windows do with a NPU? I don&#x27;t own an &#x27;AI PC&#x27; but it seems like the NPUs are slow and can&#x27;t run much.<p>I know Apple&#x27;s Neural Engine is used to power Face ID and the facial recognition stuff in Photos, among other things.</div><br/><div id="41863977" class="c"><input type="checkbox" id="c-41863977" checked=""/><div class="controls bullet"><span class="by">DrillShopper</span><span>|</span><a href="#41863532">parent</a><span>|</span><a href="#41864404">next</a><span>|</span><label class="collapse" for="c-41863977">[-]</label><label class="expand" for="c-41863977">[6 more]</label></div><br/><div class="children"><div class="content">It supports Microsoft&#x27;s Recall (now required) spyware</div><br/><div id="41864269" class="c"><input type="checkbox" id="c-41864269" checked=""/><div class="controls bullet"><span class="by">Janicc</span><span>|</span><a href="#41863532">root</a><span>|</span><a href="#41863977">parent</a><span>|</span><a href="#41864404">next</a><span>|</span><label class="collapse" for="c-41864269">[-]</label><label class="expand" for="c-41864269">[5 more]</label></div><br/><div class="children"><div class="content">Please remind me again how Recall sends data to Microsoft. I must&#x27;ve missed that part. Or are you against the print screen button too? I heard that takes images too. Very scary.</div><br/><div id="41864416" class="c"><input type="checkbox" id="c-41864416" checked=""/><div class="controls bullet"><span class="by">cmeacham98</span><span>|</span><a href="#41863532">root</a><span>|</span><a href="#41864269">parent</a><span>|</span><a href="#41864559">next</a><span>|</span><label class="collapse" for="c-41864416">[-]</label><label class="expand" for="c-41864416">[1 more]</label></div><br/><div class="children"><div class="content">While calling it spyware like GP is over-exaggeration to a ridiculous level, comparing Recall to Print Screen is also inaccurate:<p>Print Screen takes images on demand, Recall does so effectively at random. This means Recall could inadvertently screenshot and store information you didn&#x27;t intend to keep a record of (To give an extreme example: Imagine an abuser uses Recall to discover their spouse browsing online domestic violence resources).</div><br/></div></div><div id="41864559" class="c"><input type="checkbox" id="c-41864559" checked=""/><div class="controls bullet"><span class="by">bloated5048</span><span>|</span><a href="#41863532">root</a><span>|</span><a href="#41864269">parent</a><span>|</span><a href="#41864416">prev</a><span>|</span><a href="#41864507">next</a><span>|</span><label class="collapse" for="c-41864559">[-]</label><label class="expand" for="c-41864559">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s always safe to assume it does if it&#x27;s closed source. I rather be suspicious of big corporations seeking to profit at every step than naive.<p>Also, it&#x27;s security risk which already been exploited. Sure, MS fixed it, but can you be certain it won&#x27;t be exploited some time in the future again?</div><br/></div></div><div id="41864507" class="c"><input type="checkbox" id="c-41864507" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#41863532">root</a><span>|</span><a href="#41864269">parent</a><span>|</span><a href="#41864559">prev</a><span>|</span><a href="#41864321">next</a><span>|</span><label class="collapse" for="c-41864507">[-]</label><label class="expand" for="c-41864507">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Please remind me again how Recall sends data to Microsoft. I must&#x27;ve missed that part.<p>Sure, just post the source code and I&#x27;ll point out where it does so, I somehow misplaced my copy. &#x2F;s<p>The core problem here is trust, and over the last several years Microsoft has burned a hell of a lot of theirs with power-users of Windows. Even their most strident public promises of Recall being &quot;opt-in&quot; and &quot;on-device only&quot; will--paradoxically--only be <i>kept</i> as long as enough people remain suspicious.<p>Glance away and MS go back to their old games, pushing a mandatory &quot;security update&quot; which reset or entirely-removes your privacy settings and adding new &quot;telemetry&quot; streams which you cannot inspect.</div><br/></div></div></div></div></div></div><div id="41864404" class="c"><input type="checkbox" id="c-41864404" checked=""/><div class="controls bullet"><span class="by">dagaci</span><span>|</span><a href="#41863532">parent</a><span>|</span><a href="#41863977">prev</a><span>|</span><a href="#41864731">next</a><span>|</span><label class="collapse" for="c-41864404">[-]</label><label class="expand" for="c-41864404">[1 more]</label></div><br/><div class="children"><div class="content">Its used for improving video calls, special effects, image editing&#x2F; effects and noise cancelling, teams stuff</div><br/></div></div><div id="41864731" class="c"><input type="checkbox" id="c-41864731" checked=""/><div class="controls bullet"><span class="by">downrightmike</span><span>|</span><a href="#41863532">parent</a><span>|</span><a href="#41864404">prev</a><span>|</span><a href="#41864862">next</a><span>|</span><label class="collapse" for="c-41864731">[-]</label><label class="expand" for="c-41864731">[2 more]</label></div><br/><div class="children"><div class="content">AI PC is just a marketing term, doesn&#x27;t have any real substance</div><br/><div id="41865216" class="c"><input type="checkbox" id="c-41865216" checked=""/><div class="controls bullet"><span class="by">acdha</span><span>|</span><a href="#41863532">root</a><span>|</span><a href="#41864731">parent</a><span>|</span><a href="#41864862">next</a><span>|</span><label class="collapse" for="c-41865216">[-]</label><label class="expand" for="c-41865216">[1 more]</label></div><br/><div class="children"><div class="content">Yea, we know that. I believe that’s why the person you’re replying too was asking for examples of real usage.</div><br/></div></div></div></div></div></div><div id="41864862" class="c"><input type="checkbox" id="c-41864862" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#41863532">prev</a><span>|</span><a href="#41865671">next</a><span>|</span><label class="collapse" for="c-41864862">[-]</label><label class="expand" for="c-41864862">[1 more]</label></div><br/><div class="children"><div class="content">&gt;We see 1.3% of Qualcomm&#x27;s NPU 45 Teraops&#x2F;s claim<p>To me that suggests that the test is wrong.<p>I could see intel massaging results, but that far off seems incredibly improbable</div><br/></div></div><div id="41865671" class="c"><input type="checkbox" id="c-41865671" checked=""/><div class="controls bullet"><span class="by">piskov</span><span>|</span><a href="#41864862">prev</a><span>|</span><a href="#41864231">next</a><span>|</span><label class="collapse" for="c-41865671">[-]</label><label class="expand" for="c-41865671">[1 more]</label></div><br/><div class="children"><div class="content">Snapdragon touts 45 TOPS but it’s only int8.<p>For example Apple&#x27;s m3 neural engine is mere 18 TOPS but it’s FP16.<p>So windows has bigger number but it’s not apple to apple comparison.<p>Did author test int8 performance?</div><br/></div></div><div id="41864231" class="c"><input type="checkbox" id="c-41864231" checked=""/><div class="controls bullet"><span class="by">lostmsu</span><span>|</span><a href="#41865671">prev</a><span>|</span><a href="#41865213">next</a><span>|</span><label class="collapse" for="c-41864231">[-]</label><label class="expand" for="c-41864231">[1 more]</label></div><br/><div class="children"><div class="content">The author&#x27;s benchmark sucks if he could only get 2 tops from a laptop 4080. The thing should be doing somewhere around 80 tops.<p>Given that you should take his NPU results with a truckload of salt.</div><br/></div></div><div id="41865213" class="c"><input type="checkbox" id="c-41865213" checked=""/><div class="controls bullet"><span class="by">NoPicklez</span><span>|</span><a href="#41864231">prev</a><span>|</span><a href="#41865653">next</a><span>|</span><label class="collapse" for="c-41865213">[-]</label><label class="expand" for="c-41865213">[1 more]</label></div><br/><div class="children"><div class="content">Fairly misleading title, boiling down AI PCs to just the Microsoft Surface running Qualcomm</div><br/></div></div><div id="41865653" class="c"><input type="checkbox" id="c-41865653" checked=""/><div class="controls bullet"><span class="by">stanleykm</span><span>|</span><a href="#41865213">prev</a><span>|</span><a href="#41866247">next</a><span>|</span><label class="collapse" for="c-41865653">[-]</label><label class="expand" for="c-41865653">[1 more]</label></div><br/><div class="children"><div class="content">the ARM SME could be an interesting alternative to NPUs in the future. Unlike the NPUs which have at best some fixed function API it will be possible to program the SMEs more directly</div><br/></div></div><div id="41866247" class="c"><input type="checkbox" id="c-41866247" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#41865653">prev</a><span>|</span><a href="#41863335">next</a><span>|</span><label class="collapse" for="c-41866247">[-]</label><label class="expand" for="c-41866247">[1 more]</label></div><br/><div class="children"><div class="content">Memory bound workload is memory bound. Doesn’t matter how many TOPS you have if you’re sitting idle waiting on DRAM during generation. You will, however notice a difference in prefill for long prompts.</div><br/></div></div><div id="41863335" class="c"><input type="checkbox" id="c-41863335" checked=""/><div class="controls bullet"><span class="by">dmitrygr</span><span>|</span><a href="#41866247">prev</a><span>|</span><a href="#41864673">next</a><span>|</span><label class="collapse" for="c-41863335">[-]</label><label class="expand" for="c-41863335">[8 more]</label></div><br/><div class="children"><div class="content">In general MAC unit utilization tends to be low for transformers, but 1.3% seems pretty bad. I wonder if they fucked up the memory interface for the NPU. All the MACs in the world are useless if you cannot feed them.</div><br/><div id="41863595" class="c"><input type="checkbox" id="c-41863595" checked=""/><div class="controls bullet"><span class="by">Hizonner</span><span>|</span><a href="#41863335">parent</a><span>|</span><a href="#41863438">next</a><span>|</span><label class="collapse" for="c-41863595">[-]</label><label class="expand" for="c-41863595">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a tablet. It probably has like one DDR channel. It&#x27;s not so much that they &quot;fucked it up&quot; as that they knowingly built a grossly unbalanced system so they could report a pointless number.</div><br/><div id="41863634" class="c"><input type="checkbox" id="c-41863634" checked=""/><div class="controls bullet"><span class="by">dmitrygr</span><span>|</span><a href="#41863335">root</a><span>|</span><a href="#41863595">parent</a><span>|</span><a href="#41863438">next</a><span>|</span><label class="collapse" for="c-41863634">[-]</label><label class="expand" for="c-41863634">[3 more]</label></div><br/><div class="children"><div class="content">Well, no. If the CPU can hit better numbers on the same model then the bandwidth from the DDR <i>IS</i> there. Probably the NPU does not attach to the proper cache level, or just has a very thin pipe to it</div><br/><div id="41863711" class="c"><input type="checkbox" id="c-41863711" checked=""/><div class="controls bullet"><span class="by">Hizonner</span><span>|</span><a href="#41863335">root</a><span>|</span><a href="#41863634">parent</a><span>|</span><a href="#41863438">next</a><span>|</span><label class="collapse" for="c-41863711">[-]</label><label class="expand" for="c-41863711">[2 more]</label></div><br/><div class="children"><div class="content">The CPU is only about twice as good as the NPU, though (four times as good on one test). The NPU is being advertised as capable of 45 trillion operations per second, and he&#x27;s getting 1.3 percent of that.<p>So, OK, yeah, I concede that the NPU may have even worse access to memory than the CPU, but the bottom line is that neither one of them has anything close to what it needs to to actually delivering anything like the marketing headline performance number on any realistic workload.<p>I bet a lot of people have bought those things after seeing &quot;45 TOPS&quot;, thinking that they&#x27;d be able to usefully run transformers the size of main memory, and that&#x27;s not happening on CPU <i>or</i> NPU.</div><br/><div id="41863731" class="c"><input type="checkbox" id="c-41863731" checked=""/><div class="controls bullet"><span class="by">dmitrygr</span><span>|</span><a href="#41863335">root</a><span>|</span><a href="#41863711">parent</a><span>|</span><a href="#41863438">next</a><span>|</span><label class="collapse" for="c-41863731">[-]</label><label class="expand" for="c-41863731">[1 more]</label></div><br/><div class="children"><div class="content">Yup, sad all round. We are in agreement.</div><br/></div></div></div></div></div></div></div></div><div id="41863438" class="c"><input type="checkbox" id="c-41863438" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#41863335">parent</a><span>|</span><a href="#41863595">prev</a><span>|</span><a href="#41864673">next</a><span>|</span><label class="collapse" for="c-41863438">[-]</label><label class="expand" for="c-41863438">[3 more]</label></div><br/><div class="children"><div class="content">I recall looking over the Ryzen AI architecture and the NPU is just plugged into PCIe and thus gets completely crap memory bandwidth. I would expect it might be similar here.</div><br/><div id="41863770" class="c"><input type="checkbox" id="c-41863770" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#41863335">root</a><span>|</span><a href="#41863438">parent</a><span>|</span><a href="#41864166">next</a><span>|</span><label class="collapse" for="c-41863770">[-]</label><label class="expand" for="c-41863770">[1 more]</label></div><br/><div class="children"><div class="content">I spent a lot of time with a business partner and an expert looking at the design space for accelerators and it was made very clear to me that the memory interface puts a hard limit on what you can do and that it is difficult to make the most of.  Particularly if a half-baked product is being rushed out because of FOMO you’d practically expect them to ship something that gives a few percent of the performance because the memory interface doesn’t really work, it happens to the best of them:<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cell_(processor)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cell_(processor)</a></div><br/></div></div><div id="41864166" class="c"><input type="checkbox" id="c-41864166" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#41863335">root</a><span>|</span><a href="#41863438">parent</a><span>|</span><a href="#41863770">prev</a><span>|</span><a href="#41864673">next</a><span>|</span><label class="collapse" for="c-41864166">[-]</label><label class="expand" for="c-41864166">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s unlikely to be literally connected over PCIe when it&#x27;s on the same chip. It just <i>looks</i> like it&#x27;s connected over PCIe because that&#x27;s how you make peripherals discoverable to the OS. The integrated GPU also appears to be connected over PCIe, but obviously has access to far more memory bandwidth.</div><br/></div></div></div></div></div></div><div id="41864673" class="c"><input type="checkbox" id="c-41864673" checked=""/><div class="controls bullet"><span class="by">Mistletoe</span><span>|</span><a href="#41863335">prev</a><span>|</span><a href="#41864642">next</a><span>|</span><label class="collapse" for="c-41864673">[-]</label><label class="expand" for="c-41864673">[1 more]</label></div><br/><div class="children"><div class="content">&gt;The second conclusion is that the measured performance of 573 billion operations per second is only 1.3% of the 45 trillion ops&#x2F;s that the marketing material promises.<p>It just gets so hard to take this industry seriously.</div><br/></div></div><div id="41864642" class="c"><input type="checkbox" id="c-41864642" checked=""/><div class="controls bullet"><span class="by">downrightmike</span><span>|</span><a href="#41864673">prev</a><span>|</span><a href="#41863365">next</a><span>|</span><label class="collapse" for="c-41864642">[-]</label><label class="expand" for="c-41864642">[1 more]</label></div><br/><div class="children"><div class="content">They should have just made a pci card and not tried to push whole new machines on us. We are all good with the machines we already have. If you want to sell a new feature, then it needs to be an add-on</div><br/></div></div><div id="41863365" class="c"><input type="checkbox" id="c-41863365" checked=""/><div class="controls bullet"><span class="by">pram</span><span>|</span><a href="#41864642">prev</a><span>|</span><a href="#41863436">next</a><span>|</span><label class="collapse" for="c-41863365">[-]</label><label class="expand" for="c-41863365">[2 more]</label></div><br/><div class="children"><div class="content">I laughed when I saw that the Qualcomm “AI PC” is described as this in the ComfyUI docs:<p>&quot;Avoid&quot;, &quot;Nothing works&quot;, &quot;Worthless for any AI use&quot;</div><br/></div></div><div id="41863436" class="c"><input type="checkbox" id="c-41863436" checked=""/><div class="controls bullet"><span class="by">tromp</span><span>|</span><a href="#41863365">prev</a><span>|</span><a href="#41864384">next</a><span>|</span><label class="collapse" for="c-41863436">[-]</label><label class="expand" for="c-41863436">[7 more]</label></div><br/><div class="children"><div class="content">&gt;  the 45 trillion operations per second that’s listed in the specs<p>Such a spec should be ideally be accompanied by code demonstrating or approximating the claimed performance. I can&#x27;t imagine a sports car advertising a 0-100km&#x2F;h spec of 2.0 seconds where a user is unable to get below 5 seconds.</div><br/><div id="41864522" class="c"><input type="checkbox" id="c-41864522" checked=""/><div class="controls bullet"><span class="by">tedunangst</span><span>|</span><a href="#41863436">parent</a><span>|</span><a href="#41863452">next</a><span>|</span><label class="collapse" for="c-41864522">[-]</label><label class="expand" for="c-41864522">[3 more]</label></div><br/><div class="children"><div class="content">I have some bad news for you regarding how car acceleration is measured.</div><br/><div id="41865349" class="c"><input type="checkbox" id="c-41865349" checked=""/><div class="controls bullet"><span class="by">otterley</span><span>|</span><a href="#41863436">root</a><span>|</span><a href="#41864522">parent</a><span>|</span><a href="#41863452">next</a><span>|</span><label class="collapse" for="c-41865349">[-]</label><label class="expand" for="c-41865349">[2 more]</label></div><br/><div class="children"><div class="content">Well, what is it?</div><br/><div id="41865985" class="c"><input type="checkbox" id="c-41865985" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#41863436">root</a><span>|</span><a href="#41865349">parent</a><span>|</span><a href="#41863452">next</a><span>|</span><label class="collapse" for="c-41865985">[-]</label><label class="expand" for="c-41865985">[1 more]</label></div><br/><div class="children"><div class="content">Everything from rolling starts to perfect road conditions and specific tires, I suppose.</div><br/></div></div></div></div></div></div><div id="41863452" class="c"><input type="checkbox" id="c-41863452" checked=""/><div class="controls bullet"><span class="by">dmitrygr</span><span>|</span><a href="#41863436">parent</a><span>|</span><a href="#41864522">prev</a><span>|</span><a href="#41863444">next</a><span>|</span><label class="collapse" for="c-41863452">[-]</label><label class="expand" for="c-41863452">[2 more]</label></div><br/><div class="children"><div class="content">most likely multiplying the same 128x128 matrix from cache to cache. That gets you perfect MAC utilization with no need to hit memory. Gets you a big number that is not directly a lie - that perf <i>IS</i> attainable, on a useless synthetic benchmark</div><br/><div id="41863655" class="c"><input type="checkbox" id="c-41863655" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#41863436">root</a><span>|</span><a href="#41863452">parent</a><span>|</span><a href="#41863444">next</a><span>|</span><label class="collapse" for="c-41863655">[-]</label><label class="expand" for="c-41863655">[1 more]</label></div><br/><div class="children"><div class="content">Sounds great for RNNs! &#x2F;s</div><br/></div></div></div></div></div></div><div id="41864384" class="c"><input type="checkbox" id="c-41864384" checked=""/><div class="controls bullet"><span class="by">hkgjjgjfjfjfjf</span><span>|</span><a href="#41863436">prev</a><span>|</span><a href="#41863088">next</a><span>|</span><label class="collapse" for="c-41864384">[-]</label><label class="expand" for="c-41864384">[1 more]</label></div><br/><div class="children"><div class="content">Sutherland&#x27;s wheel of reincarnation turns.</div><br/></div></div></div></div></div></div></div></body></html>