<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1702112456277" as="style"/><link rel="stylesheet" href="styles.css?v=1702112456277"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.together.ai/blog/stripedhyena-7b">Paving the way to efficient architectures: StripedHyena-7B</a> <span class="domain">(<a href="https://www.together.ai">www.together.ai</a>)</span></div><div class="subtext"><span>minimaxir</span> | <span>52 comments</span></div><br/><div><div id="38575282" class="c"><input type="checkbox" id="c-38575282" checked=""/><div class="controls bullet"><span class="by">kcorbitt</span><span>|</span><a href="#38576119">next</a><span>|</span><label class="collapse" for="c-38575282">[-]</label><label class="expand" for="c-38575282">[3 more]</label></div><br/><div class="children"><div class="content">For short context tasks looks like it&#x27;s slightly stronger than Llama 7B and slightly weaker than Mistral 7B. Really impressive showing for a completely new architecture. I&#x27;ve also heard that it was trained on far fewer tokens than Mistral, so likely still room to grow.<p>Overall incredibly impressive work from the team at Together!</div><br/><div id="38575960" class="c"><input type="checkbox" id="c-38575960" checked=""/><div class="controls bullet"><span class="by">tempusalaria</span><span>|</span><a href="#38575282">parent</a><span>|</span><a href="#38576119">next</a><span>|</span><label class="collapse" for="c-38575960">[-]</label><label class="expand" for="c-38575960">[2 more]</label></div><br/><div class="children"><div class="content">Did they disclose the training compute&#x2F;token count?</div><br/><div id="38576929" class="c"><input type="checkbox" id="c-38576929" checked=""/><div class="controls bullet"><span class="by">stellaathena</span><span>|</span><a href="#38575282">root</a><span>|</span><a href="#38575960">parent</a><span>|</span><a href="#38576119">next</a><span>|</span><label class="collapse" for="c-38576929">[-]</label><label class="expand" for="c-38576929">[1 more]</label></div><br/><div class="children"><div class="content">Nope :(</div><br/></div></div></div></div></div></div><div id="38576119" class="c"><input type="checkbox" id="c-38576119" checked=""/><div class="controls bullet"><span class="by">icyfox</span><span>|</span><a href="#38575282">prev</a><span>|</span><a href="#38575405">next</a><span>|</span><label class="collapse" for="c-38576119">[-]</label><label class="expand" for="c-38576119">[3 more]</label></div><br/><div class="children"><div class="content">Common wisdom in most industries is to release bad PR announcements on a Friday and good ones towards the start of the week. It&#x27;s interesting how the advent of twitter communication has shifted the ML ecosystem to publishing work whenever it&#x27;s ready versus trying to find an optimal weekday. Or maybe they&#x27;re optimizing for the weekend hackers that will take improvements released late in the week and put them into practice on Saturdays and Sundays.</div><br/><div id="38576262" class="c"><input type="checkbox" id="c-38576262" checked=""/><div class="controls bullet"><span class="by">alsodumb</span><span>|</span><a href="#38576119">parent</a><span>|</span><a href="#38576173">next</a><span>|</span><label class="collapse" for="c-38576262">[-]</label><label class="expand" for="c-38576262">[1 more]</label></div><br/><div class="children"><div class="content">NeurIPS is also starting this Sunday, I&#x27;m sure some of these releases are meant to get the word going during the conference.</div><br/></div></div><div id="38576173" class="c"><input type="checkbox" id="c-38576173" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38576119">parent</a><span>|</span><a href="#38576262">prev</a><span>|</span><a href="#38575405">next</a><span>|</span><label class="collapse" for="c-38576173">[-]</label><label class="expand" for="c-38576173">[1 more]</label></div><br/><div class="children"><div class="content">It is interesting, been thinking about that myself a bit: some random associated thoughts: NeurIPS is next week, &amp; there was OpenAI drama news that went under the radar due to Friday rule.</div><br/></div></div></div></div><div id="38575405" class="c"><input type="checkbox" id="c-38575405" checked=""/><div class="controls bullet"><span class="by">bratao</span><span>|</span><a href="#38576119">prev</a><span>|</span><a href="#38575526">next</a><span>|</span><label class="collapse" for="c-38575405">[-]</label><label class="expand" for="c-38575405">[4 more]</label></div><br/><div class="children"><div class="content">And this uses Hyena, that can be considered a &quot;previous generation&quot; of Mamba. I think that this anwsers the question about the scalability of SSM and the transformer finally found an opponent.</div><br/><div id="38576403" class="c"><input type="checkbox" id="c-38576403" checked=""/><div class="controls bullet"><span class="by">algo_trader</span><span>|</span><a href="#38575405">parent</a><span>|</span><a href="#38578119">next</a><span>|</span><label class="collapse" for="c-38576403">[-]</label><label class="expand" for="c-38576403">[1 more]</label></div><br/><div class="children"><div class="content">&gt; H 7B is always faster than optimized Transformers
&gt; (&gt;10%, &gt;20% and &gt;50% end-to-end faster than FlashAttention<p>50% improvement on very large sequences is great. But it is not yet transformative (pun intended..)<p>Presumably FlashAttention already does lots of the less-than-quadratic improvement</div><br/></div></div><div id="38578119" class="c"><input type="checkbox" id="c-38578119" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#38575405">parent</a><span>|</span><a href="#38576403">prev</a><span>|</span><a href="#38575600">next</a><span>|</span><label class="collapse" for="c-38578119">[-]</label><label class="expand" for="c-38578119">[1 more]</label></div><br/><div class="children"><div class="content">Well, since Together did hire the author of Mamba…</div><br/></div></div></div></div><div id="38575526" class="c"><input type="checkbox" id="c-38575526" checked=""/><div class="controls bullet"><span class="by">goalonetwo</span><span>|</span><a href="#38575405">prev</a><span>|</span><a href="#38575480">next</a><span>|</span><label class="collapse" for="c-38575526">[-]</label><label class="expand" for="c-38575526">[16 more]</label></div><br/><div class="children"><div class="content">There seems to be a new model every single day. How do people have time to keep track with everything going on in AI?</div><br/><div id="38575635" class="c"><input type="checkbox" id="c-38575635" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#38575526">parent</a><span>|</span><a href="#38575902">next</a><span>|</span><label class="collapse" for="c-38575635">[-]</label><label class="expand" for="c-38575635">[9 more]</label></div><br/><div class="children"><div class="content">From decades of observing at a distance and observing observers at a distance, I think it&#x27;s safe to say that, like fusion, there are walls that AI run into, not unlike the risers on a staircase, and when we collectively hit one, there&#x27;s a lot of scuttling back forth. A lot of movement, but no real progress. If that plateau goes on too long, excitement (and funding) dry up and things die down.<p>Then someone figures out how to get past the current plateau, and the whole process repeats. That could be new tech, a new architecture, or it could be old tech that was infeasible and had to wait for Moore&#x27;s Law.<p>Right now we are on the vertical part of the sawtooth pattern. Everyone hopes this will be the time that takes us to infinity, but the old people are just waiting for people to crash into the new wall.</div><br/><div id="38575866" class="c"><input type="checkbox" id="c-38575866" checked=""/><div class="controls bullet"><span class="by">fvv</span><span>|</span><a href="#38575526">root</a><span>|</span><a href="#38575635">parent</a><span>|</span><a href="#38575697">next</a><span>|</span><label class="collapse" for="c-38575866">[-]</label><label class="expand" for="c-38575866">[6 more]</label></div><br/><div class="children"><div class="content">Why things should dry up when contrary to fusion ai is already usable by millions daily ? Even if prpgress should stall a bit the product or fine-tunes or normal progress will still be super supeful , the &quot;too soon&quot; point has been surpassed</div><br/><div id="38577752" class="c"><input type="checkbox" id="c-38577752" checked=""/><div class="controls bullet"><span class="by">blagie</span><span>|</span><a href="#38575526">root</a><span>|</span><a href="#38575866">parent</a><span>|</span><a href="#38576422">next</a><span>|</span><label class="collapse" for="c-38577752">[-]</label><label class="expand" for="c-38577752">[3 more]</label></div><br/><div class="children"><div class="content">A lot of previous plateaus in AI are usable and used by billions daily, for example, giving good navigation routes on your phone, managing NPCs in a video game, showing ads, or recommending movies.<p>It&#x27;s not that they don&#x27;t have value -- they do, and in the trillions of dollars -- but once understood, they move from &quot;AI&quot; to &quot;algorithms&quot; and stop being exciting.<p>The current progress feels different to me, though. The current step in capability is much higher than previous ones, as is the potential disruption.</div><br/><div id="38578278" class="c"><input type="checkbox" id="c-38578278" checked=""/><div class="controls bullet"><span class="by">Al-Khwarizmi</span><span>|</span><a href="#38575526">root</a><span>|</span><a href="#38577752">parent</a><span>|</span><a href="#38577891">next</a><span>|</span><label class="collapse" for="c-38578278">[-]</label><label class="expand" for="c-38578278">[1 more]</label></div><br/><div class="children"><div class="content">I think what makes the current iteration of AI different is that we don&#x27;t understand how the emerging abilities work.<p>A map navigation algorithm: we understand it, we know where the limit is (basically it cannot do anything that isn&#x27;t map navigation), so it stops being exciting.<p>GPT: we don&#x27;t understand it, we don&#x27;t know where the limit is... And it doesn&#x27;t seem it will stop being exciting until we do.</div><br/></div></div><div id="38577891" class="c"><input type="checkbox" id="c-38577891" checked=""/><div class="controls bullet"><span class="by">TheDudeMan</span><span>|</span><a href="#38575526">root</a><span>|</span><a href="#38577752">parent</a><span>|</span><a href="#38578278">prev</a><span>|</span><a href="#38576422">next</a><span>|</span><label class="collapse" for="c-38577891">[-]</label><label class="expand" for="c-38577891">[1 more]</label></div><br/><div class="children"><div class="content">Yes. The thing that makes the current generation of AI different is that the architectures scale. Another $10 million in training effort WILL yield improvement. And Moore’s law pairs nicely with scaling behavior. In other words, there is currently no end in sight. Plus, algo advancements like this make things happen ever faster. Plus, increased VC money means more money to throw at hardware and more folks trying new things in software. Soon we’ll be replaced :(</div><br/></div></div></div></div><div id="38576422" class="c"><input type="checkbox" id="c-38576422" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#38575526">root</a><span>|</span><a href="#38575866">parent</a><span>|</span><a href="#38577752">prev</a><span>|</span><a href="#38579739">next</a><span>|</span><label class="collapse" for="c-38576422">[-]</label><label class="expand" for="c-38576422">[1 more]</label></div><br/><div class="children"><div class="content">Depends on what you are looking for. I have this hesitation too. What we have and are on track for is useful and cool, but how far will we come in this spurt until we are back at slight incremental gains?<p><i>Implementation</i> wise in business, we are very early though. It feels like email in 1995, we have barely scratched the surface of what LLMs can mean for business and everyday life.</div><br/></div></div><div id="38579739" class="c"><input type="checkbox" id="c-38579739" checked=""/><div class="controls bullet"><span class="by">ReptileMan</span><span>|</span><a href="#38575526">root</a><span>|</span><a href="#38575866">parent</a><span>|</span><a href="#38576422">prev</a><span>|</span><a href="#38575697">next</a><span>|</span><label class="collapse" for="c-38579739">[-]</label><label class="expand" for="c-38579739">[1 more]</label></div><br/><div class="children"><div class="content">Because suddenly the tech moves from world transformative to world enhancing. The potential profits from trillions to mere billions. From immortality to slightly longer lifespan.</div><br/></div></div></div></div><div id="38575697" class="c"><input type="checkbox" id="c-38575697" checked=""/><div class="controls bullet"><span class="by">goalonetwo</span><span>|</span><a href="#38575526">root</a><span>|</span><a href="#38575635">parent</a><span>|</span><a href="#38575866">prev</a><span>|</span><a href="#38579172">next</a><span>|</span><label class="collapse" for="c-38575697">[-]</label><label class="expand" for="c-38575697">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for putting this so eloquently. That&#x27;s exactly how I feel as well.</div><br/></div></div><div id="38579172" class="c"><input type="checkbox" id="c-38579172" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#38575526">root</a><span>|</span><a href="#38575635">parent</a><span>|</span><a href="#38575697">prev</a><span>|</span><a href="#38575902">next</a><span>|</span><label class="collapse" for="c-38579172">[-]</label><label class="expand" for="c-38579172">[1 more]</label></div><br/><div class="children"><div class="content">We know for a fact that there is no wall till GPT 4 and open source still has long way to reach there.</div><br/></div></div></div></div><div id="38575902" class="c"><input type="checkbox" id="c-38575902" checked=""/><div class="controls bullet"><span class="by">holoduke</span><span>|</span><a href="#38575526">parent</a><span>|</span><a href="#38575635">prev</a><span>|</span><a href="#38576578">next</a><span>|</span><label class="collapse" for="c-38575902">[-]</label><label class="expand" for="c-38575902">[2 more]</label></div><br/><div class="children"><div class="content">I know. The new reddit look sucks big. But the sub reddits still give you good insights in the latest developments. I am playing arround a lot with image related stuff arround stable diffusion. Comfyui subreddit gives me daily. Now after a few weeks i think i have a fair good understanding in what is hot. Checkpoints, ipadapters, facemodels etc.
Just playing arround and you will get a grasp of it.
I guess its similar with text generation.</div><br/><div id="38578358" class="c"><input type="checkbox" id="c-38578358" checked=""/><div class="controls bullet"><span class="by">tavavex</span><span>|</span><a href="#38575526">root</a><span>|</span><a href="#38575902">parent</a><span>|</span><a href="#38576578">next</a><span>|</span><label class="collapse" for="c-38578358">[-]</label><label class="expand" for="c-38578358">[1 more]</label></div><br/><div class="children"><div class="content">FYI: if you want a faster experience, <a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;old.reddit.com&#x2F;</a> still works perfectly</div><br/></div></div></div></div><div id="38576578" class="c"><input type="checkbox" id="c-38576578" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#38575526">parent</a><span>|</span><a href="#38575902">prev</a><span>|</span><a href="#38577452">next</a><span>|</span><label class="collapse" for="c-38576578">[-]</label><label class="expand" for="c-38576578">[1 more]</label></div><br/><div class="children"><div class="content">I sorta keep huggingface open next to HN now. Follow like “TheBloke” and a few others and you’ll know what’s up.</div><br/></div></div><div id="38577452" class="c"><input type="checkbox" id="c-38577452" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#38575526">parent</a><span>|</span><a href="#38576578">prev</a><span>|</span><a href="#38577486">next</a><span>|</span><label class="collapse" for="c-38577452">[-]</label><label class="expand" for="c-38577452">[1 more]</label></div><br/><div class="children"><div class="content">You look at the comments here to see if it’s any different or getting rave reviews.  Most model are crap, the new mistral one from yesterday seem to be pretty good.  But most models are not very practical or useful for anything other than amusement right now.  I imagine in a year we’d get close to gpt4 models locally and with low spec requirements where a 2 series NVidia card can run.</div><br/></div></div><div id="38577486" class="c"><input type="checkbox" id="c-38577486" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#38575526">parent</a><span>|</span><a href="#38577452">prev</a><span>|</span><a href="#38576177">next</a><span>|</span><label class="collapse" for="c-38577486">[-]</label><label class="expand" for="c-38577486">[1 more]</label></div><br/><div class="children"><div class="content">Is almost like everything’s accelerating exponentially.</div><br/></div></div><div id="38576177" class="c"><input type="checkbox" id="c-38576177" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38575526">parent</a><span>|</span><a href="#38577486">prev</a><span>|</span><a href="#38575480">next</a><span>|</span><label class="collapse" for="c-38576177">[-]</label><label class="expand" for="c-38576177">[1 more]</label></div><br/><div class="children"><div class="content">I honestly don&#x27;t think I could if I still had a job not in it</div><br/></div></div></div></div><div id="38575480" class="c"><input type="checkbox" id="c-38575480" checked=""/><div class="controls bullet"><span class="by">firejake308</span><span>|</span><a href="#38575526">prev</a><span>|</span><a href="#38576722">next</a><span>|</span><label class="collapse" for="c-38575480">[-]</label><label class="expand" for="c-38575480">[6 more]</label></div><br/><div class="children"><div class="content">Darn, I was hoping the RWKV people had finally obtained reportable results. This is still interesting, though. Maybe we will see more alternatives to transformers soon</div><br/><div id="38576890" class="c"><input type="checkbox" id="c-38576890" checked=""/><div class="controls bullet"><span class="by">stellaathena</span><span>|</span><a href="#38575480">parent</a><span>|</span><a href="#38577269">next</a><span>|</span><label class="collapse" for="c-38576890">[-]</label><label class="expand" for="c-38576890">[1 more]</label></div><br/><div class="children"><div class="content">RWKV had a paper accepted at EMNLP and released models which match the performance of equivalent transformers.<p>What else are you looking for?</div><br/></div></div><div id="38577269" class="c"><input type="checkbox" id="c-38577269" checked=""/><div class="controls bullet"><span class="by">senseiV</span><span>|</span><a href="#38575480">parent</a><span>|</span><a href="#38576890">prev</a><span>|</span><a href="#38576393">next</a><span>|</span><label class="collapse" for="c-38577269">[-]</label><label class="expand" for="c-38577269">[1 more]</label></div><br/><div class="children"><div class="content">They Do, the latest rwkv v5, matches mamba at 3b scale, and from the benchmarks I see, its similar to hyena</div><br/></div></div><div id="38576393" class="c"><input type="checkbox" id="c-38576393" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38575480">parent</a><span>|</span><a href="#38577269">prev</a><span>|</span><a href="#38576722">next</a><span>|</span><label class="collapse" for="c-38576393">[-]</label><label class="expand" for="c-38576393">[3 more]</label></div><br/><div class="children"><div class="content">There are RWKV numbers in there, so you sort of got your wish sideways. :)<p>Has RWKV not released anything until now? I thought it was an open project that was in use, if sort of by a hipster 1%</div><br/><div id="38576910" class="c"><input type="checkbox" id="c-38576910" checked=""/><div class="controls bullet"><span class="by">stellaathena</span><span>|</span><a href="#38575480">root</a><span>|</span><a href="#38576393">parent</a><span>|</span><a href="#38577287">next</a><span>|</span><label class="collapse" for="c-38576910">[-]</label><label class="expand" for="c-38576910">[1 more]</label></div><br/><div class="children"><div class="content">Paper: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.13048" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.13048</a><p>Models (v4 are the ones from the paper): <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;RWKV" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;RWKV</a><p>GitHub: <a href="https:&#x2F;&#x2F;github.com&#x2F;BlinkDL&#x2F;RWKV-LM">https:&#x2F;&#x2F;github.com&#x2F;BlinkDL&#x2F;RWKV-LM</a></div><br/></div></div><div id="38577287" class="c"><input type="checkbox" id="c-38577287" checked=""/><div class="controls bullet"><span class="by">senseiV</span><span>|</span><a href="#38575480">root</a><span>|</span><a href="#38576393">parent</a><span>|</span><a href="#38576910">prev</a><span>|</span><a href="#38576722">next</a><span>|</span><label class="collapse" for="c-38577287">[-]</label><label class="expand" for="c-38577287">[1 more]</label></div><br/><div class="children"><div class="content">V5 7b is out, close to hyena, gets 1400 t&#x2F;s on a 3090, while an h100 llama 7b 8bit is 1200 t&#x2F;s</div><br/></div></div></div></div></div></div><div id="38575469" class="c"><input type="checkbox" id="c-38575469" checked=""/><div class="controls bullet"><span class="by">skerit</span><span>|</span><a href="#38576722">prev</a><span>|</span><a href="#38574976">next</a><span>|</span><label class="collapse" for="c-38575469">[-]</label><label class="expand" for="c-38575469">[14 more]</label></div><br/><div class="children"><div class="content">7B models are so exciting. So much is happening with those smaller models.</div><br/><div id="38578823" class="c"><input type="checkbox" id="c-38578823" checked=""/><div class="controls bullet"><span class="by">Metricon</span><span>|</span><a href="#38575469">parent</a><span>|</span><a href="#38575927">next</a><span>|</span><label class="collapse" for="c-38578823">[-]</label><label class="expand" for="c-38578823">[3 more]</label></div><br/><div class="children"><div class="content">BTW, for anyone who might not be aware of it, this model trained by Intel based on the Mistral architecture is probably the single best general 7B model available currently:<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Intel&#x2F;neural-chat-7b-v3-2" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Intel&#x2F;neural-chat-7b-v3-2</a> (also see <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Intel&#x2F;neural-chat-7b-v3-1" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Intel&#x2F;neural-chat-7b-v3-1</a> from the previous version for more details)<p>It&#x27;s licensed Apache 2.0 and unaligned (uncensored).</div><br/><div id="38579361" class="c"><input type="checkbox" id="c-38579361" checked=""/><div class="controls bullet"><span class="by">gardnr</span><span>|</span><a href="#38575469">root</a><span>|</span><a href="#38578823">parent</a><span>|</span><a href="#38575927">next</a><span>|</span><label class="collapse" for="c-38579361">[-]</label><label class="expand" for="c-38579361">[2 more]</label></div><br/><div class="children"><div class="content">How is it better than the model from the team that made the dataset? <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Open-Orca&#x2F;Mistral-7B-SlimOrca" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Open-Orca&#x2F;Mistral-7B-SlimOrca</a></div><br/><div id="38579430" class="c"><input type="checkbox" id="c-38579430" checked=""/><div class="controls bullet"><span class="by">Metricon</span><span>|</span><a href="#38575469">root</a><span>|</span><a href="#38579361">parent</a><span>|</span><a href="#38575927">next</a><span>|</span><label class="collapse" for="c-38579430">[-]</label><label class="expand" for="c-38579430">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t personally tried that one, but on the HuggingFace LLM Leaderboard:<p>Open-Orca&#x2F;Mistral-7B-SlimOrca - AVG: 60.37, ARC: 62.54, HellaSwag: 83.86, MMLU: 62.77, TruthfulQA: 54.23, Winogrande: 77.43, GSM8k: 21.38<p>Intel&#x2F;neural-chat-7b-v3-2 - AVG: 68.29, ARC: 67.49, HellaSwag: 83.92, MMLU: 63.55, TruthfulQA: 59.68, Winogrande: 79.95, GSM8k: 55.12</div><br/></div></div></div></div></div></div><div id="38575927" class="c"><input type="checkbox" id="c-38575927" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38575469">parent</a><span>|</span><a href="#38578823">prev</a><span>|</span><a href="#38577505">next</a><span>|</span><label class="collapse" for="c-38575927">[-]</label><label class="expand" for="c-38575927">[4 more]</label></div><br/><div class="children"><div class="content">I wish they were a smidge smaller since 7B LLMs just barely run on a 16GB VRAM GPU (like a T4 server GPU) without quantization shenanigans.<p>Fortunately the learnings from finding better 7B models will trickle down, or more will be done with distillation (e.g. Gemini Nano)</div><br/><div id="38576039" class="c"><input type="checkbox" id="c-38576039" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#38575469">root</a><span>|</span><a href="#38575927">parent</a><span>|</span><a href="#38577505">next</a><span>|</span><label class="collapse" for="c-38576039">[-]</label><label class="expand" for="c-38576039">[3 more]</label></div><br/><div class="children"><div class="content">What&#x27;s wrong with quantization?</div><br/><div id="38576218" class="c"><input type="checkbox" id="c-38576218" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38575469">root</a><span>|</span><a href="#38576039">parent</a><span>|</span><a href="#38577505">next</a><span>|</span><label class="collapse" for="c-38576218">[-]</label><label class="expand" for="c-38576218">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s still a (subjective) generative quality loss, even with recent tricks to minimize it.</div><br/><div id="38576483" class="c"><input type="checkbox" id="c-38576483" checked=""/><div class="controls bullet"><span class="by">leetharris</span><span>|</span><a href="#38575469">root</a><span>|</span><a href="#38576218">parent</a><span>|</span><a href="#38577505">next</a><span>|</span><label class="collapse" for="c-38576483">[-]</label><label class="expand" for="c-38576483">[1 more]</label></div><br/><div class="children"><div class="content">Q8 is generally less than 1% degradation, Q5KM is around 3%. After that is when it starts to really degrade.</div><br/></div></div></div></div></div></div></div></div><div id="38577505" class="c"><input type="checkbox" id="c-38577505" checked=""/><div class="controls bullet"><span class="by">triyambakam</span><span>|</span><a href="#38575469">parent</a><span>|</span><a href="#38575927">prev</a><span>|</span><a href="#38574976">next</a><span>|</span><label class="collapse" for="c-38577505">[-]</label><label class="expand" for="c-38577505">[6 more]</label></div><br/><div class="children"><div class="content">Can you explain for a noob why?</div><br/><div id="38578760" class="c"><input type="checkbox" id="c-38578760" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#38575469">root</a><span>|</span><a href="#38577505">parent</a><span>|</span><a href="#38578134">next</a><span>|</span><label class="collapse" for="c-38578760">[-]</label><label class="expand" for="c-38578760">[1 more]</label></div><br/><div class="children"><div class="content">Easier to train, easier to experiment with. Most research and prototyping happens on the scale that is just barely out of the &quot;toy&quot; category.</div><br/></div></div><div id="38578134" class="c"><input type="checkbox" id="c-38578134" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#38575469">root</a><span>|</span><a href="#38577505">parent</a><span>|</span><a href="#38578760">prev</a><span>|</span><a href="#38574976">next</a><span>|</span><label class="collapse" for="c-38578134">[-]</label><label class="expand" for="c-38578134">[4 more]</label></div><br/><div class="children"><div class="content">Quantization means reducing the number of bits used to encode each floating point number constituting a parameter in the model So instead of having billions of possible values per weight, you might have just 255. The model has to have its weights crammed into a much smaller number of possible values, which reduces its ability to produce good outputs.</div><br/><div id="38578517" class="c"><input type="checkbox" id="c-38578517" checked=""/><div class="controls bullet"><span class="by">triyambakam</span><span>|</span><a href="#38575469">root</a><span>|</span><a href="#38578134">parent</a><span>|</span><a href="#38574976">next</a><span>|</span><label class="collapse" for="c-38578517">[-]</label><label class="expand" for="c-38578517">[3 more]</label></div><br/><div class="children"><div class="content">Sorry, my question is, why are the 7B models so exciting?</div><br/><div id="38579373" class="c"><input type="checkbox" id="c-38579373" checked=""/><div class="controls bullet"><span class="by">aseipp</span><span>|</span><a href="#38575469">root</a><span>|</span><a href="#38578517">parent</a><span>|</span><a href="#38579597">next</a><span>|</span><label class="collapse" for="c-38579373">[-]</label><label class="expand" for="c-38579373">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t require really expensive and power-hungry components to run, i.e. a mid-range GPU can run a (4-or-5-bit quantized) 7B model at +50 tokens&#x2F;second, so it&#x27;s completely feasible to run on a small budget. They are easier to fine-tune, because they are smaller, and you can even just do CPU inference if you really want. There are good OSS implementations like llama.cpp and exllama. And there is a lot of belief that 7B models are not yet tapped out in terms of efficacy, so they will keep improving.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38574976" class="c"><input type="checkbox" id="c-38574976" checked=""/><div class="controls bullet"><span class="by">mmaunder</span><span>|</span><a href="#38575469">prev</a><span>|</span><a href="#38575215">next</a><span>|</span><label class="collapse" for="c-38574976">[-]</label><label class="expand" for="c-38574976">[3 more]</label></div><br/><div class="children"><div class="content">Is the model available or is this just an API&#x2F;app?</div><br/><div id="38575032" class="c"><input type="checkbox" id="c-38575032" checked=""/><div class="controls bullet"><span class="by">lelag</span><span>|</span><a href="#38574976">parent</a><span>|</span><a href="#38575215">next</a><span>|</span><label class="collapse" for="c-38575032">[-]</label><label class="expand" for="c-38575032">[2 more]</label></div><br/><div class="children"><div class="content">Weights seem available at <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;togethercomputer&#x2F;StripedHyena-Nous-7B&#x2F;tree&#x2F;main" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;togethercomputer&#x2F;StripedHyena-Nous-7B...</a>.</div><br/><div id="38575113" class="c"><input type="checkbox" id="c-38575113" checked=""/><div class="controls bullet"><span class="by">SparkyMcUnicorn</span><span>|</span><a href="#38574976">root</a><span>|</span><a href="#38575032">parent</a><span>|</span><a href="#38575215">next</a><span>|</span><label class="collapse" for="c-38575113">[-]</label><label class="expand" for="c-38575113">[1 more]</label></div><br/><div class="children"><div class="content">And here&#x27;s the base model: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;togethercomputer&#x2F;StripedHyena-Hessian-7B" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;togethercomputer&#x2F;StripedHyena-Hessian...</a><p>And GH repo: <a href="https:&#x2F;&#x2F;github.com&#x2F;togethercomputer&#x2F;stripedhyena">https:&#x2F;&#x2F;github.com&#x2F;togethercomputer&#x2F;stripedhyena</a></div><br/></div></div></div></div></div></div><div id="38575215" class="c"><input type="checkbox" id="c-38575215" checked=""/><div class="controls bullet"><span class="by">anon373839</span><span>|</span><a href="#38574976">prev</a><span>|</span><label class="collapse" for="c-38575215">[-]</label><label class="expand" for="c-38575215">[1 more]</label></div><br/><div class="children"><div class="content">This is a seriously impressive model.</div><br/></div></div></div></div></div></div></div></body></html>