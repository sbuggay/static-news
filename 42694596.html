<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1736845251939" as="style"/><link rel="stylesheet" href="styles.css?v=1736845251939"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/openzfs/zfs/releases/tag/zfs-2.3.0">ZFS 2.3.0 released with ZFS raidz expansion</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>scrp</span> | <span>28 comments</span></div><br/><div><div id="42694713" class="c"><input type="checkbox" id="c-42694713" checked=""/><div class="controls bullet"><span class="by">uniqueuid</span><span>|</span><a href="#42694597">next</a><span>|</span><label class="collapse" for="c-42694713">[-]</label><label class="expand" for="c-42694713">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s good to see that they were pretty conservative about the expansion.<p>Not only is expansion completely transparent and resumable, it also maintains redundancy throughout the process.<p>That said, there is one tiny caveat people should be aware of:<p>&gt; After the expansion completes, old blocks remain with their old data-to-parity ratio (e.g. 5-wide RAIDZ2, has 3 data to 2 parity), but distributed among the larger set of disks. New blocks will be written with the new data-to-parity
ratio (e.g. a 5-wide RAIDZ2 which has been expanded once to 6-wide, has 4 data to 2 parity).</div><br/></div></div><div id="42694597" class="c"><input type="checkbox" id="c-42694597" checked=""/><div class="controls bullet"><span class="by">scrp</span><span>|</span><a href="#42694713">prev</a><span>|</span><a href="#42695153">next</a><span>|</span><label class="collapse" for="c-42694597">[-]</label><label class="expand" for="c-42694597">[14 more]</label></div><br/><div class="children"><div class="content">After years in the making ZFS raidz expansaion is finally here.<p>Major features added in release:<p><pre><code>  - RAIDZ Expansion: Add new devices to an existing RAIDZ pool, increasing storage capacity without downtime.

  - Fast Dedup: A major performance upgrade to the original OpenZFS deduplication functionality.

  - Direct IO: Allows bypassing the ARC for reads&#x2F;writes, improving performance in scenarios like NVMe devices where caching may hinder efficiency.

  - JSON: Optional JSON output for the most used commands.

  - Long names: Support for file and directory names up to 1023 characters.</code></pre></div><br/><div id="42694731" class="c"><input type="checkbox" id="c-42694731" checked=""/><div class="controls bullet"><span class="by">jdboyd</span><span>|</span><a href="#42694597">parent</a><span>|</span><a href="#42694702">next</a><span>|</span><label class="collapse" for="c-42694731">[-]</label><label class="expand" for="c-42694731">[2 more]</label></div><br/><div class="children"><div class="content">The first 4 seem like really big deals.</div><br/><div id="42695081" class="c"><input type="checkbox" id="c-42695081" checked=""/><div class="controls bullet"><span class="by">snvzz</span><span>|</span><a href="#42694597">root</a><span>|</span><a href="#42694731">parent</a><span>|</span><a href="#42694702">next</a><span>|</span><label class="collapse" for="c-42695081">[-]</label><label class="expand" for="c-42695081">[1 more]</label></div><br/><div class="children"><div class="content">The fifth is also, once you consider non-ascii names.</div><br/></div></div></div></div><div id="42694702" class="c"><input type="checkbox" id="c-42694702" checked=""/><div class="controls bullet"><span class="by">cm2187</span><span>|</span><a href="#42694597">parent</a><span>|</span><a href="#42694731">prev</a><span>|</span><a href="#42695153">next</a><span>|</span><label class="collapse" for="c-42694702">[-]</label><label class="expand" for="c-42694702">[11 more]</label></div><br/><div class="children"><div class="content">But I presume it is still not possible to remove a vdev.</div><br/><div id="42695168" class="c"><input type="checkbox" id="c-42695168" checked=""/><div class="controls bullet"><span class="by">ryao</span><span>|</span><a href="#42694597">root</a><span>|</span><a href="#42694702">parent</a><span>|</span><a href="#42694725">next</a><span>|</span><label class="collapse" for="c-42695168">[-]</label><label class="expand" for="c-42695168">[1 more]</label></div><br/><div class="children"><div class="content">That was added a while ago:<p><a href="https:&#x2F;&#x2F;openzfs.github.io&#x2F;openzfs-docs&#x2F;man&#x2F;master&#x2F;8&#x2F;zpool-remove.8.html" rel="nofollow">https:&#x2F;&#x2F;openzfs.github.io&#x2F;openzfs-docs&#x2F;man&#x2F;master&#x2F;8&#x2F;zpool-re...</a><p>It works by making a readonly copy of the vdev being removed inside the remaining space. The existing vdev is then removed. Data can still be accessed from the copy, but new writes will go to an actual vdev while data no longer needed on the copy is gradually reclaimed as free space as the old data is no longer needed.</div><br/></div></div><div id="42694725" class="c"><input type="checkbox" id="c-42694725" checked=""/><div class="controls bullet"><span class="by">mustache_kimono</span><span>|</span><a href="#42694597">root</a><span>|</span><a href="#42694702">parent</a><span>|</span><a href="#42695168">prev</a><span>|</span><a href="#42695153">next</a><span>|</span><label class="collapse" for="c-42694725">[-]</label><label class="expand" for="c-42694725">[9 more]</label></div><br/><div class="children"><div class="content">Is this possible elsewhere (re: other filesystems)?</div><br/><div id="42694753" class="c"><input type="checkbox" id="c-42694753" checked=""/><div class="controls bullet"><span class="by">cm2187</span><span>|</span><a href="#42694597">root</a><span>|</span><a href="#42694725">parent</a><span>|</span><a href="#42694784">next</a><span>|</span><label class="collapse" for="c-42694753">[-]</label><label class="expand" for="c-42694753">[6 more]</label></div><br/><div class="children"><div class="content">It is possible with windows storage space (remove drive from a pool) and mdadm&#x2F;lvm (remove disk from a RAID array, remove volume from lvm), which to me are the two major alternatives. Don&#x27;t know about unraid.</div><br/><div id="42694777" class="c"><input type="checkbox" id="c-42694777" checked=""/><div class="controls bullet"><span class="by">mustache_kimono</span><span>|</span><a href="#42694597">root</a><span>|</span><a href="#42694753">parent</a><span>|</span><a href="#42694857">next</a><span>|</span><label class="collapse" for="c-42694777">[-]</label><label class="expand" for="c-42694777">[4 more]</label></div><br/><div class="children"><div class="content">&gt; It is possible with windows storage space (remove drive from a pool) and mdadm&#x2F;lvm (remove disk from a RAID array, remove volume from lvm), which to me are the two major alternatives. Don&#x27;t know about unraid.<p>Perhaps I am misunderstanding you, but you can offline and remove drives from a ZFS pool.<p>Do you mean WSS and mdadm&#x2F;lvm will allow an automatic live rebalance and then reconfigure of the drive topo?</div><br/><div id="42694859" class="c"><input type="checkbox" id="c-42694859" checked=""/><div class="controls bullet"><span class="by">cm2187</span><span>|</span><a href="#42694597">root</a><span>|</span><a href="#42694777">parent</a><span>|</span><a href="#42694955">next</a><span>|</span><label class="collapse" for="c-42694859">[-]</label><label class="expand" for="c-42694859">[1 more]</label></div><br/><div class="children"><div class="content">So for instance I have a ZFS pool with 3 HDD data vdevs, and 2 SSD special vdevs. I want to convert the two SSD vdevs into a single one (or possibly remove one of them). From what I read the only way to do that is to destroy the entire pool and recreate it (it&#x27;s in a server in a datacentre, don&#x27;t want to reupload that much data).<p>In windows, you can set a disk for removal, and as long as the other disks have enough space and are compatible with the virtual disks (eg you need at least 5 disks if you have parity with number of columns=5), it will rebalance the blocks onto the other disks until you can safely remove the disk. If you use thin provisioning, you can also change your mind about the settings of a virtual disk, create a new one on the same pool, and move the data from one to the other.<p>Mdadm&#x2F;lvm will do the same albeit with more of a pain in the arse as RAID requires to resilver not just the occupied space but also the free space so takes a lot more time and IO than it should.<p>It&#x27;s one of my beef with ZFS, there are lots of no return decisions. That and I ran into some race conditions with loading a ZFS array on boot with nvme drives on ubuntu. They seem to not be ready, resulting in randomly degraded arrays. Fixed by loading the pool with a delay.</div><br/></div></div><div id="42694955" class="c"><input type="checkbox" id="c-42694955" checked=""/><div class="controls bullet"><span class="by">Sesse__</span><span>|</span><a href="#42694597">root</a><span>|</span><a href="#42694777">parent</a><span>|</span><a href="#42694859">prev</a><span>|</span><a href="#42695089">next</a><span>|</span><label class="collapse" for="c-42694955">[-]</label><label class="expand" for="c-42694955">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Do you mean WSS and mdadm&#x2F;lvm will allow an automatic live rebalance and then reconfigure of the drive topo?<p>mdadm can convert RAID-5 to a larger or smaller RAID-5, RAID-6 to a larger or smaller RAID-6, RAID-5 to RAID-6 or the other way around, RAID-0 to a degraded RAID-5, and many other fairly reasonable operations, while the array is online, resistant to power loss and the likes.<p>I wrote the first version of this md code in 2005 (against kernel 2.6.13), and Neil Brown rewrote and mainlined it at some point in 2006. ZFS is… a bit late to the party.</div><br/></div></div><div id="42695089" class="c"><input type="checkbox" id="c-42695089" checked=""/><div class="controls bullet"><span class="by">TiredOfLife</span><span>|</span><a href="#42694597">root</a><span>|</span><a href="#42694777">parent</a><span>|</span><a href="#42694955">prev</a><span>|</span><a href="#42694857">next</a><span>|</span><label class="collapse" for="c-42695089">[-]</label><label class="expand" for="c-42695089">[1 more]</label></div><br/><div class="children"><div class="content">Storage Spaces doesn&#x27;t dedicate drive to single purpose. It operates in chunks (256MB i think). So one drive can, at the same time, be part of a mirror and raid-5 and raid-0. This allows fully using drives with various sizes. And choosing to remove drive will cause it to redistribute the chunks to other available drives, without going offline.</div><br/></div></div></div></div><div id="42694857" class="c"><input type="checkbox" id="c-42694857" checked=""/><div class="controls bullet"><span class="by">lloeki</span><span>|</span><a href="#42694597">root</a><span>|</span><a href="#42694753">parent</a><span>|</span><a href="#42694777">prev</a><span>|</span><a href="#42694784">next</a><span>|</span><label class="collapse" for="c-42694857">[-]</label><label class="expand" for="c-42694857">[1 more]</label></div><br/><div class="children"><div class="content">IIUC the ask (I have a hard time wrapping my head around zfs vernacular), btrfs allows this at least in some cases.<p>If you can convince <i>btrfs balance</i> to not use the dev to remove it will simply rebalance data to the other devs and then you can <i>btrfs device remove</i>.</div><br/></div></div></div></div><div id="42694784" class="c"><input type="checkbox" id="c-42694784" checked=""/><div class="controls bullet"><span class="by">c45y</span><span>|</span><a href="#42694597">root</a><span>|</span><a href="#42694725">parent</a><span>|</span><a href="#42694753">prev</a><span>|</span><a href="#42695153">next</a><span>|</span><label class="collapse" for="c-42694784">[-]</label><label class="expand" for="c-42694784">[2 more]</label></div><br/><div class="children"><div class="content">Bcachefs allows it</div><br/><div id="42694851" class="c"><input type="checkbox" id="c-42694851" checked=""/><div class="controls bullet"><span class="by">eptcyka</span><span>|</span><a href="#42694597">root</a><span>|</span><a href="#42694784">parent</a><span>|</span><a href="#42695153">next</a><span>|</span><label class="collapse" for="c-42694851">[-]</label><label class="expand" for="c-42694851">[1 more]</label></div><br/><div class="children"><div class="content">Cool, just have to wait before it is stable enough for daily use of mission critical data. I am personally optimistic about bcachefs, but incredibly pessimistic about changing filesystems.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42695153" class="c"><input type="checkbox" id="c-42695153" checked=""/><div class="controls bullet"><span class="by">endorphine</span><span>|</span><a href="#42694597">prev</a><span>|</span><a href="#42694735">next</a><span>|</span><label class="collapse" for="c-42695153">[-]</label><label class="expand" for="c-42695153">[2 more]</label></div><br/><div class="children"><div class="content">Can someone describe why they would use ZFS (or similar) for home usage?</div><br/><div id="42695177" class="c"><input type="checkbox" id="c-42695177" checked=""/><div class="controls bullet"><span class="by">lutorm</span><span>|</span><a href="#42695153">parent</a><span>|</span><a href="#42694735">next</a><span>|</span><label class="collapse" for="c-42695177">[-]</label><label class="expand" for="c-42695177">[1 more]</label></div><br/><div class="children"><div class="content">Apart from just peace of mind from bitrot, I use it for the snapshotting capability which makes it super easy to do backups. You can snapshot and send the snapshots to other storage with e.g zfs-autobackup and it&#x27;s trivial and you can&#x27;t screw it up. If the snapshots exist on the other drive, you know you have a backup.</div><br/></div></div></div></div><div id="42694735" class="c"><input type="checkbox" id="c-42694735" checked=""/><div class="controls bullet"><span class="by">cgeier</span><span>|</span><a href="#42695153">prev</a><span>|</span><a href="#42695035">next</a><span>|</span><label class="collapse" for="c-42694735">[-]</label><label class="expand" for="c-42694735">[8 more]</label></div><br/><div class="children"><div class="content">This is huge news for ZFS users (probably mostly those in the hobbyist&#x2F;home use space, but still). raidz expansion has been one of the most requested features for years.</div><br/><div id="42694876" class="c"><input type="checkbox" id="c-42694876" checked=""/><div class="controls bullet"><span class="by">jfreax</span><span>|</span><a href="#42694735">parent</a><span>|</span><a href="#42695035">next</a><span>|</span><label class="collapse" for="c-42694876">[-]</label><label class="expand" for="c-42694876">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not yet familiar with zfs and couldn&#x27;t find it in the release note: Does expansion only works with disk of the same size? Or is adding are bigger&#x2F;smaller disks possible or do all disk need to have the same size?</div><br/><div id="42695024" class="c"><input type="checkbox" id="c-42695024" checked=""/><div class="controls bullet"><span class="by">zelcon</span><span>|</span><a href="#42694735">root</a><span>|</span><a href="#42694876">parent</a><span>|</span><a href="#42694974">next</a><span>|</span><label class="collapse" for="c-42695024">[-]</label><label class="expand" for="c-42695024">[3 more]</label></div><br/><div class="children"><div class="content">You need to buy the same exact drive with the same capacity and speed. Your raidz vdev be as small and as slow as your smallest and slowest drive.<p>btrfs and the new bcachefs can do RAID with mixed drives, but I can’t trust either of them with my data yet.</div><br/><div id="42695085" class="c"><input type="checkbox" id="c-42695085" checked=""/><div class="controls bullet"><span class="by">Mashimo</span><span>|</span><a href="#42694735">root</a><span>|</span><a href="#42695024">parent</a><span>|</span><a href="#42694974">next</a><span>|</span><label class="collapse" for="c-42695085">[-]</label><label class="expand" for="c-42695085">[2 more]</label></div><br/><div class="children"><div class="content">&gt; You need to buy the same exact drive<p>AFAIK you can add larger and faster drives, you will just not get any benefits from it.</div><br/><div id="42695104" class="c"><input type="checkbox" id="c-42695104" checked=""/><div class="controls bullet"><span class="by">bpye</span><span>|</span><a href="#42694735">root</a><span>|</span><a href="#42695085">parent</a><span>|</span><a href="#42694974">next</a><span>|</span><label class="collapse" for="c-42695104">[-]</label><label class="expand" for="c-42695104">[1 more]</label></div><br/><div class="children"><div class="content">You can get read speed benefits with faster drives, but your writes will be limited by your slowest.</div><br/></div></div></div></div></div></div><div id="42694974" class="c"><input type="checkbox" id="c-42694974" checked=""/><div class="controls bullet"><span class="by">chasil</span><span>|</span><a href="#42694735">root</a><span>|</span><a href="#42694876">parent</a><span>|</span><a href="#42695024">prev</a><span>|</span><a href="#42694947">next</a><span>|</span><label class="collapse" for="c-42694974">[-]</label><label class="expand" for="c-42694974">[1 more]</label></div><br/><div class="children"><div class="content">IIRC, you could always replace drives in a raidset with larger devices. When the last drive is replaced, then the new space is recognized.<p>This new operation seems somewhat more sophisticated.</div><br/></div></div><div id="42694947" class="c"><input type="checkbox" id="c-42694947" checked=""/><div class="controls bullet"><span class="by">shiroiushi</span><span>|</span><a href="#42694735">root</a><span>|</span><a href="#42694876">parent</a><span>|</span><a href="#42694974">prev</a><span>|</span><a href="#42695035">next</a><span>|</span><label class="collapse" for="c-42694947">[-]</label><label class="expand" for="c-42694947">[2 more]</label></div><br/><div class="children"><div class="content">As far as I understand, ZFS doesn&#x27;t work at all with disks of differing sizes (in the same array).  So if you try it, it just finds the size of the smallest disk, and uses that for all disks.  So if you put an 8TB drive in an array with a bunch of 10TB drives, they&#x27;ll all be treated as 8TB drives, and the extra 2TB will be ignored on those disks.<p>However, if you replace the smallest disk with a new, larger drive, and resilver, then it&#x27;ll now use the new smallest disk as the baseline, and use that extra space on the other drives.<p>(Someone please correct me if I&#x27;m wrong.)</div><br/><div id="42695183" class="c"><input type="checkbox" id="c-42695183" checked=""/><div class="controls bullet"><span class="by">mustache_kimono</span><span>|</span><a href="#42694735">root</a><span>|</span><a href="#42694947">parent</a><span>|</span><a href="#42695035">next</a><span>|</span><label class="collapse" for="c-42695183">[-]</label><label class="expand" for="c-42695183">[1 more]</label></div><br/><div class="children"><div class="content">&gt; As far as I understand, ZFS doesn&#x27;t work at all with disks of differing sizes (in the same array).<p>This isn&#x27;t exactly correct.<p>You can use 2x10TB mirrors as vdev0, and 6x12TB in RAIDZ2 as vdev1 in the same array.</div><br/></div></div></div></div></div></div></div></div><div id="42695035" class="c"><input type="checkbox" id="c-42695035" checked=""/><div class="controls bullet"><span class="by">zelcon</span><span>|</span><a href="#42694735">prev</a><span>|</span><a href="#42695124">next</a><span>|</span><label class="collapse" for="c-42695035">[-]</label><label class="expand" for="c-42695035">[1 more]</label></div><br/><div class="children"><div class="content">Been running it since rc2. It’s insane how long this took to finally ship.</div><br/></div></div><div id="42695124" class="c"><input type="checkbox" id="c-42695124" checked=""/><div class="controls bullet"><span class="by">senectus1</span><span>|</span><a href="#42695035">prev</a><span>|</span><label class="collapse" for="c-42695124">[-]</label><label class="expand" for="c-42695124">[1 more]</label></div><br/><div class="children"><div class="content">Would love to use ZFS, but unfortunately Fedora just cant keep up with it...</div><br/></div></div></div></div></div></div></div></body></html>