<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1724835687388" as="style"/><link rel="stylesheet" href="styles.css?v=1724835687388"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/NousResearch/DisTrO">DisTrO – a family of low latency distributed optimizers</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>SchwKatze</span> | <span>26 comments</span></div><br/><div><div id="41372670" class="c"><input type="checkbox" id="c-41372670" checked=""/><div class="controls bullet"><span class="by">arjvik</span><span>|</span><a href="#41372305">next</a><span>|</span><label class="collapse" for="c-41372670">[-]</label><label class="expand" for="c-41372670">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no information about what this is, beyond a teaser of a loss graph. Really hoping this is something that gets released to the world, not hidden behind closed doors.</div><br/><div id="41373176" class="c"><input type="checkbox" id="c-41373176" checked=""/><div class="controls bullet"><span class="by">arilotter</span><span>|</span><a href="#41372670">parent</a><span>|</span><a href="#41372305">next</a><span>|</span><label class="collapse" for="c-41373176">[-]</label><label class="expand" for="c-41373176">[1 more]</label></div><br/><div class="children"><div class="content">Paper &amp; code in the next couple months. We&#x27;re workin on em :)</div><br/></div></div></div></div><div id="41372305" class="c"><input type="checkbox" id="c-41372305" checked=""/><div class="controls bullet"><span class="by">iamronaldo</span><span>|</span><a href="#41372670">prev</a><span>|</span><a href="#41371891">next</a><span>|</span><label class="collapse" for="c-41372305">[-]</label><label class="expand" for="c-41372305">[2 more]</label></div><br/><div class="children"><div class="content">This seems huge no? Couldn&#x27;t this enable &quot;community based&quot; ai training at home?</div><br/><div id="41376441" class="c"><input type="checkbox" id="c-41376441" checked=""/><div class="controls bullet"><span class="by">FuckButtons</span><span>|</span><a href="#41372305">parent</a><span>|</span><a href="#41371891">next</a><span>|</span><label class="collapse" for="c-41376441">[-]</label><label class="expand" for="c-41376441">[1 more]</label></div><br/><div class="children"><div class="content">AFAIK, the main bottleneck on training is memory bandwidth. Distributed gpu compute has multiple orders of magnitude less than an equivalent number of GPUs colocated, because they don’t share a physical bus, but have a network connection instead. This work improves on that, but the fundamental limitations remain.</div><br/></div></div></div></div><div id="41371891" class="c"><input type="checkbox" id="c-41371891" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41372305">prev</a><span>|</span><a href="#41371992">next</a><span>|</span><label class="collapse" for="c-41371891">[-]</label><label class="expand" for="c-41371891">[8 more]</label></div><br/><div class="children"><div class="content">Most of the information about this is in this PDF (I hate when people publish interesting information exclusively in PDFs): <a href="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;NousResearch&#x2F;DisTrO&#x2F;main&#x2F;A_Preliminary_Report_on_DisTrO.pdf" rel="nofollow">https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;NousResearch&#x2F;DisTrO&#x2F;main&#x2F;A...</a><p>I converted it to Markdown (using Gemini 1.5 Pro) and pasted it into a Gist here: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;46a33d66e069efe5c10b63625fdabb4e#a-preliminary-report-on-distro" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;46a33d66e069efe5c10b63625fdab...</a><p>From the abstract:<p>&gt; Training large scale neural networks typically involves sharing gradients between all accelerators, which necessitates specialized, high-speed interconnects. To address this, we introduce DisTrO, a family of architecture-agnostic and network-agnostic distributed optimizers that reduces the inter-GPU communication requirements by four to five orders of magnitude without relying on amortized analysis, enabling low-latency training of large neural networks on slow internet bandwidths with heterogeneous networking hardware.<p>This could be a HUGE deal.<p>Currently if you want to train giant LLMs you need a big pile of GPUs in the same location as each other due to the amount of information that needs to shuffle between them during training.<p>If DisTrO works as intended, it will be possible to train models using GPUs in different places - potentially enabling SETI@home style training where thousands of people with gaming PCs at home could donate their GPU time to a large training effort.<p>Their tweet about this has more: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;NousResearch&#x2F;status&#x2F;1828121648383566270" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;NousResearch&#x2F;status&#x2F;1828121648383566270</a><p>&gt; Nous Research is proud to release a preliminary report on DisTrO (Distributed Training Over-the-Internet) a family of architecture-agnostic and network-agnostic distributed optimizers that reduces the inter-GPU communication requirements by 1000x to 10,000x without relying on amortized analysis, and matches AdamW+All-Reduce in convergence rates. This enables low-latency training of large neural networks on slow internet bandwidths with heterogeneous networking hardware.<p>&gt; DisTrO can increase the resilience and robustness of training LLMs by minimizing dependency on a single entity for computation. DisTrO is one step towards a more secure and equitable environment for all participants involved in building LLMs.<p>&gt; Without relying on a single company to manage and control the training process, researchers and institutions can have more freedom to collaborate and experiment with new techniques, algorithms, and models. This increased competition fosters innovation, drives progress, and ultimately benefits society as a whole.</div><br/><div id="41372013" class="c"><input type="checkbox" id="c-41372013" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#41371891">parent</a><span>|</span><a href="#41374942">next</a><span>|</span><label class="collapse" for="c-41372013">[-]</label><label class="expand" for="c-41372013">[6 more]</label></div><br/><div class="children"><div class="content">As much as I liked the team, there is really no information other than the loss graph :(</div><br/><div id="41373188" class="c"><input type="checkbox" id="c-41373188" checked=""/><div class="controls bullet"><span class="by">az226</span><span>|</span><a href="#41371891">root</a><span>|</span><a href="#41372013">parent</a><span>|</span><a href="#41374627">next</a><span>|</span><label class="collapse" for="c-41373188">[-]</label><label class="expand" for="c-41373188">[3 more]</label></div><br/><div class="children"><div class="content">That’s not quite true. They also tested benchmarks and compared with an AdamW trained model.</div><br/><div id="41374581" class="c"><input type="checkbox" id="c-41374581" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#41371891">root</a><span>|</span><a href="#41373188">parent</a><span>|</span><a href="#41374627">next</a><span>|</span><label class="collapse" for="c-41374581">[-]</label><label class="expand" for="c-41374581">[2 more]</label></div><br/><div class="children"><div class="content">Not reproducible, so this doesn’t exist.</div><br/><div id="41375197" class="c"><input type="checkbox" id="c-41375197" checked=""/><div class="controls bullet"><span class="by">arilotter</span><span>|</span><a href="#41371891">root</a><span>|</span><a href="#41374581">parent</a><span>|</span><a href="#41374627">next</a><span>|</span><label class="collapse" for="c-41375197">[-]</label><label class="expand" for="c-41375197">[1 more]</label></div><br/><div class="children"><div class="content">we&#x27;re excited to drop the paper &amp; code and let you run the code for yourself to see otherwise !! coming in the next couple months :)</div><br/></div></div></div></div></div></div><div id="41374627" class="c"><input type="checkbox" id="c-41374627" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#41371891">root</a><span>|</span><a href="#41372013">parent</a><span>|</span><a href="#41373188">prev</a><span>|</span><a href="#41374942">next</a><span>|</span><label class="collapse" for="c-41374627">[-]</label><label class="expand" for="c-41374627">[2 more]</label></div><br/><div class="children"><div class="content">Look at the PDF in simonw’s first link. There’s plenty of information. One part looked like the communication requirements were reduced down to under 100MB. That suggests a communication rate that could be handled by dirt-cheap instances spread across the globe. Like on vast.ai or something.</div><br/><div id="41375464" class="c"><input type="checkbox" id="c-41375464" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#41371891">root</a><span>|</span><a href="#41374627">parent</a><span>|</span><a href="#41374942">next</a><span>|</span><label class="collapse" for="c-41375464">[-]</label><label class="expand" for="c-41375464">[1 more]</label></div><br/><div class="children"><div class="content">GaLore can do that too (if you transplant it). There are similar methods prior LLM era doing the same. They are not quite there on the loss graph side though (I am actually unsure about GaLore, but both FLoRA and ReLoRA were not quite there on loss graph side).</div><br/></div></div></div></div></div></div><div id="41374942" class="c"><input type="checkbox" id="c-41374942" checked=""/><div class="controls bullet"><span class="by">atlas_hugged</span><span>|</span><a href="#41371891">parent</a><span>|</span><a href="#41372013">prev</a><span>|</span><a href="#41371992">next</a><span>|</span><label class="collapse" for="c-41374942">[-]</label><label class="expand" for="c-41374942">[1 more]</label></div><br/><div class="children"><div class="content">Awwww snaaaaap, decentralized skyneeet here we goooooo! Kidding of course, but very exciting breakthrough if this pans out.</div><br/></div></div></div></div><div id="41371992" class="c"><input type="checkbox" id="c-41371992" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#41371891">prev</a><span>|</span><a href="#41371084">next</a><span>|</span><label class="collapse" for="c-41371992">[-]</label><label class="expand" for="c-41371992">[12 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to believe it&#x27;s true but I suspect they&#x27;re overstating the result, or it&#x27;s a fluke. Presumably teams at large firms like Meta would have put a lot of effort into checking whether not-synchronise-every-step training could match synchronise-every-step training before investing hundreds of millions of dollars into the low-latency, high-throughput network hardware necessary for the latter.</div><br/><div id="41372718" class="c"><input type="checkbox" id="c-41372718" checked=""/><div class="controls bullet"><span class="by">arilotter</span><span>|</span><a href="#41371992">parent</a><span>|</span><a href="#41372876">next</a><span>|</span><label class="collapse" for="c-41372718">[-]</label><label class="expand" for="c-41372718">[5 more]</label></div><br/><div class="children"><div class="content">We&#x27;re pretty confident it&#x27;s not a fluke, and paper + code are the next step, within a couple months.
It&#x27;s not &quot;synchronize every step&quot;, but it&#x27;s &quot;do something every step&quot;.<p>We double and triple and quadruple checked our results, to make sure that we are in fact getting results like this while only doing our thing every step, and it really keeps holding up.<p>Don&#x27;t trust our word for it, though, you&#x27;ll see when the paper comes out :)</div><br/><div id="41373068" class="c"><input type="checkbox" id="c-41373068" checked=""/><div class="controls bullet"><span class="by">RicoElectrico</span><span>|</span><a href="#41371992">root</a><span>|</span><a href="#41372718">parent</a><span>|</span><a href="#41372876">next</a><span>|</span><label class="collapse" for="c-41373068">[-]</label><label class="expand" for="c-41373068">[4 more]</label></div><br/><div class="children"><div class="content">Um, so why announce something before even a paper with replicable details is available? To put it bluntly, what are we supposed to do with the information?<p>I could be less harsh if this was some grant requirement to release a report before a certain date, but I don&#x27;t see any grant funding declaration.</div><br/><div id="41373183" class="c"><input type="checkbox" id="c-41373183" checked=""/><div class="controls bullet"><span class="by">arilotter</span><span>|</span><a href="#41371992">root</a><span>|</span><a href="#41373068">parent</a><span>|</span><a href="#41373127">next</a><span>|</span><label class="collapse" for="c-41373183">[-]</label><label class="expand" for="c-41373183">[2 more]</label></div><br/><div class="children"><div class="content">We&#x27;re excited about the potential and want to find other folks also excited about it that are interested in working for&#x2F;with us to build things on the foundations of DisTrO!
Plus also it&#x27;s so cool and mind boggling to us that we wanted to share the hype a little bit, it was hard not being able to tell anyone we were working on it</div><br/><div id="41374110" class="c"><input type="checkbox" id="c-41374110" checked=""/><div class="controls bullet"><span class="by">SchwKatze</span><span>|</span><a href="#41371992">root</a><span>|</span><a href="#41373183">parent</a><span>|</span><a href="#41373127">next</a><span>|</span><label class="collapse" for="c-41374110">[-]</label><label class="expand" for="c-41374110">[1 more]</label></div><br/><div class="children"><div class="content">I sent a email yesterday to you guys to find a way I can help to build this pretty pretty cool idea.</div><br/></div></div></div></div><div id="41373127" class="c"><input type="checkbox" id="c-41373127" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#41371992">root</a><span>|</span><a href="#41373068">parent</a><span>|</span><a href="#41373183">prev</a><span>|</span><a href="#41372876">next</a><span>|</span><label class="collapse" for="c-41373127">[-]</label><label class="expand" for="c-41373127">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m happy to have the project on my radar, and though they could be a bit clearer about the provisional nature of the research I don&#x27;t think it&#x27;s wrong to want to hype the potential of it a bit.</div><br/></div></div></div></div></div></div><div id="41372876" class="c"><input type="checkbox" id="c-41372876" checked=""/><div class="controls bullet"><span class="by">hobofan</span><span>|</span><a href="#41371992">parent</a><span>|</span><a href="#41372718">prev</a><span>|</span><a href="#41374559">next</a><span>|</span><label class="collapse" for="c-41372876">[-]</label><label class="expand" for="c-41372876">[4 more]</label></div><br/><div class="children"><div class="content">Is synchronize-every-step training the status quo for training LLMs?<p>I&#x27;ve not kept up-to-date with training&#x2F;optimizer research for quite some time but during the deep learning craze there were papers like the ones about DistBelief&#x2F;Downpour SDG[0] that showed how to scale up training by only doing occasional synchronization. Did that not transfer to transformer&#x2F;LLM training?<p>[0]: <a href="https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper_files&#x2F;paper&#x2F;2012&#x2F;hash&#x2F;6aca97005c68f1206823815f66102863-Abstract.html" rel="nofollow">https:&#x2F;&#x2F;proceedings.neurips.cc&#x2F;paper_files&#x2F;paper&#x2F;2012&#x2F;hash&#x2F;6...</a></div><br/><div id="41373274" class="c"><input type="checkbox" id="c-41373274" checked=""/><div class="controls bullet"><span class="by">adw</span><span>|</span><a href="#41371992">root</a><span>|</span><a href="#41372876">parent</a><span>|</span><a href="#41374559">next</a><span>|</span><label class="collapse" for="c-41373274">[-]</label><label class="expand" for="c-41373274">[3 more]</label></div><br/><div class="children"><div class="content">Yes, ultimately everyone is currently doing something which looks like synchronous data parallel training on the outside.<p>The linked PDF is very light on detail, but what results they do claim are about a 1.2bn parameter model. This is tiny; you don&#x27;t need network-bound distributed training (ie, anything beyond a single datacenter class machine, or less if you&#x27;re patient) to train a model that size. The comms requirements also scale with the model size, so I strongly suspect people hoping for embarrassingly-parallel-style scaling properties are going to be disappointed.<p>(They also appear to have, in part, reinvented parameter servers.)</div><br/><div id="41373464" class="c"><input type="checkbox" id="c-41373464" checked=""/><div class="controls bullet"><span class="by">huac</span><span>|</span><a href="#41371992">root</a><span>|</span><a href="#41373274">parent</a><span>|</span><a href="#41374260">next</a><span>|</span><label class="collapse" for="c-41373464">[-]</label><label class="expand" for="c-41373464">[1 more]</label></div><br/><div class="children"><div class="content">in particular it appears that they only implement data parallel DP - at 1.2B you can fit full copy of model into memory, but larger models require splitting the weights across multiple machines (different techniques eg distributed data parallel DDP, tensor parallel TP, pipeline parallel TP, ...)<p>without more details it&#x27;s unclear if the proposed technique keeps its speedups in that case</div><br/></div></div><div id="41374260" class="c"><input type="checkbox" id="c-41374260" checked=""/><div class="controls bullet"><span class="by">itkovian_</span><span>|</span><a href="#41371992">root</a><span>|</span><a href="#41373274">parent</a><span>|</span><a href="#41373464">prev</a><span>|</span><a href="#41374559">next</a><span>|</span><label class="collapse" for="c-41374260">[-]</label><label class="expand" for="c-41374260">[1 more]</label></div><br/><div class="children"><div class="content">This is not true</div><br/></div></div></div></div></div></div><div id="41374559" class="c"><input type="checkbox" id="c-41374559" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#41371992">parent</a><span>|</span><a href="#41372876">prev</a><span>|</span><a href="#41372653">next</a><span>|</span><label class="collapse" for="c-41374559">[-]</label><label class="expand" for="c-41374559">[1 more]</label></div><br/><div class="children"><div class="content">I was into Nous at first, but it seems they mostly just do graphic design and vibes stuff so a16z gives them money. Which, whatever, nice work if your can get it, but don’t use the same tactics for research projects.</div><br/></div></div><div id="41372653" class="c"><input type="checkbox" id="c-41372653" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#41371992">parent</a><span>|</span><a href="#41374559">prev</a><span>|</span><a href="#41371084">next</a><span>|</span><label class="collapse" for="c-41372653">[-]</label><label class="expand" for="c-41372653">[1 more]</label></div><br/><div class="children"><div class="content">Not if it cost them a month to do so.</div><br/></div></div></div></div></div></div></div></div></div></body></html>