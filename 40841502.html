<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1719824481844" as="style"/><link rel="stylesheet" href="styles.css?v=1719824481844"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://tylerneylon.com/a/mind_model/mind_model.html">A Model of a Mind</a> <span class="domain">(<a href="https://tylerneylon.com">tylerneylon.com</a>)</span></div><div class="subtext"><span>adamesque</span> | <span>34 comments</span></div><br/><div><div id="40843508" class="c"><input type="checkbox" id="c-40843508" checked=""/><div class="controls bullet"><span class="by">bubblyworld</span><span>|</span><a href="#40843693">next</a><span>|</span><label class="collapse" for="c-40843508">[-]</label><label class="expand" for="c-40843508">[1 more]</label></div><br/><div class="children"><div class="content">Something that strikes me about this model is that it&#x27;s bottom up - sensory data feeds in in its entirety, the action centre processes everything, makes a decision, sends a command to the motor centre.<p>There&#x27;s a theory that real brains subvert this, and what we perceive is actually our internal model of our self&#x2F;environment. The only data that makes it through from our sense organs is the difference between the two.<p>This kind of top-down processing is more efficient energy-wise but I wonder if it&#x27;s deeper than that? You can view perception and action as two sides of the same coin - both are ways to modify your internal model to better fit the sensory signals you expect.<p>Anyway, I guess the point I&#x27;m making is you should be careful which way you point your arrows, and of designating a single aspect of a mind (the action centre) as fundamental. Reality might work very differently, and that maybe says something? I don&#x27;t know haha.</div><br/></div></div><div id="40843693" class="c"><input type="checkbox" id="c-40843693" checked=""/><div class="controls bullet"><span class="by">paulmooreparks</span><span>|</span><a href="#40843508">prev</a><span>|</span><a href="#40843225">next</a><span>|</span><label class="collapse" for="c-40843693">[-]</label><label class="expand" for="c-40843693">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve lately begun to think of conciousness as the ability to read and react to one&#x27;s own log output. I don&#x27;t like hypothesis by analogy, but it seems an apt description for what conscious entities do. I just don&#x27;t see anything mystical about it.</div><br/></div></div><div id="40843225" class="c"><input type="checkbox" id="c-40843225" checked=""/><div class="controls bullet"><span class="by">tylerneylon</span><span>|</span><a href="#40843693">prev</a><span>|</span><a href="#40842861">next</a><span>|</span><label class="collapse" for="c-40843225">[-]</label><label class="expand" for="c-40843225">[4 more]</label></div><br/><div class="children"><div class="content">Author here: I&#x27;m grateful for the comments; thanks especially for interesting references.<p>Context for the article: I&#x27;m working on an ambitious long-term project to write a book about consciousness from a scientific and analytic (versus, say, a meditation-oriented) perspective. I didn&#x27;t write this fact in the article, but what I&#x27;d love to happen is that I meet people with a similar optimistic perspective, and to learn and improve my communication skills via follow-up conversations.<p>If anyone is interested in chatting more about the topic of the article, please do email me. My email is in my HN profile. Thanks!</div><br/><div id="40843256" class="c"><input type="checkbox" id="c-40843256" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#40843225">parent</a><span>|</span><a href="#40843541">next</a><span>|</span><label class="collapse" for="c-40843256">[-]</label><label class="expand" for="c-40843256">[2 more]</label></div><br/><div class="children"><div class="content">&gt; a book about consciousness<p>Too many people have written books about consciousness. 
There&#x27;s much tail-chasing in that space, all the way back to Aristotle. 
Write one about common sense. Current AI sucks at common sense.
We can&#x27;t even achieve the level of common sense of a squirrel yet.<p>Working definition of common sense: getting through the next 30 seconds of life without a major screwup.</div><br/><div id="40843599" class="c"><input type="checkbox" id="c-40843599" checked=""/><div class="controls bullet"><span class="by">cornholio</span><span>|</span><a href="#40843225">root</a><span>|</span><a href="#40843256">parent</a><span>|</span><a href="#40843541">next</a><span>|</span><label class="collapse" for="c-40843599">[-]</label><label class="expand" for="c-40843599">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not a working definition of common sense. You&#x27;ve pulled as dependencies the entirety of human reasoning and self-reflection, a theory of mind for all sentient creatures you encounter and a gargantuan amount of context sensitive socio-cultural learning, to name but a few.</div><br/></div></div></div></div><div id="40843541" class="c"><input type="checkbox" id="c-40843541" checked=""/><div class="controls bullet"><span class="by">FrancisMoodie</span><span>|</span><a href="#40843225">parent</a><span>|</span><a href="#40843256">prev</a><span>|</span><a href="#40842861">next</a><span>|</span><label class="collapse" for="c-40843541">[-]</label><label class="expand" for="c-40843541">[1 more]</label></div><br/><div class="children"><div class="content">Gödel, Escher, Bach - An eternal Braid is a great book about consciousness from a logical standpoint. Is this a work you&#x27;re familiar with?</div><br/></div></div></div></div><div id="40842861" class="c"><input type="checkbox" id="c-40842861" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#40843225">prev</a><span>|</span><a href="#40841951">next</a><span>|</span><label class="collapse" for="c-40842861">[-]</label><label class="expand" for="c-40842861">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Now the LLM can choose to switch, at its own discretion, back and forth between a talking and listening mode<p>How would it intelligently do this? What data would you train on? You don&#x27;t have trillions words of text where humans wrote what they thought silently interwoven with what they wrote publicly.<p>History has shown over and over that hard coded ad hoc solutions to these &quot;simple problems&quot; never work to create intelligent agents, you need to train the model to do that from the start you can&#x27;t patch in intelligence after the fact. Those additions can be useful, but they have never been intelligent.<p>Anyway, such a model I&#x27;d call &quot;stream of mind model&quot; rather than a language model, it would fundamentally solve many of the problems with current LLM where their thinking is reliant on the shape of the answer, while a stream of mind model would shape its thinking to fit the problem and then shape the formatting to fit the communication needs.<p>Such a model as this guy describes would be a massive step forward, so I agree with this, but it is way too expensive to train, not due to lack of compute but due to lack of data. And I don&#x27;t see that data being done within the next decade if ever, humans don&#x27;t really like writing down their hidden thoughts, and you&#x27;d need to pay them to generate data amounts equivalent to the internet...</div><br/><div id="40843136" class="c"><input type="checkbox" id="c-40843136" checked=""/><div class="controls bullet"><span class="by">tylerneylon</span><span>|</span><a href="#40842861">parent</a><span>|</span><a href="#40843668">next</a><span>|</span><label class="collapse" for="c-40843136">[-]</label><label class="expand" for="c-40843136">[1 more]</label></div><br/><div class="children"><div class="content">Replying to: How would a model intelligently switch between listening or speaking modes? What data would you train on? (I&#x27;m the author of the parent article.)<p>It&#x27;s a fair question, and I don&#x27;t have all the answers. But for this question, there might be training data available from everyday human conversations. For example, we could use a speech-to-text model that&#x27;s able to distinguish speakers, and look for points where one person decided to start speaking (that would be training data for when to switch modes). Ideally, the speech-to-text model would be able to include text even when both people spoke at once (this would provide more realistic and complete training data).<p>I&#x27;ve noticed that the audio mode in ChatGPT&#x27;s app is good at noticing when I&#x27;m done speaking to it, and it reacts accurately enough that I suspect it&#x27;s more sophisticated than &quot;wait for silence.&quot; If there is a &quot;notice the end of speaking&quot; model - which is not a crazy assumption - then I can imagine a slightly more complicated model that notices a combination of &quot;now is a good time to talk + I have something to say.&quot;</div><br/></div></div><div id="40843668" class="c"><input type="checkbox" id="c-40843668" checked=""/><div class="controls bullet"><span class="by">cornholio</span><span>|</span><a href="#40842861">parent</a><span>|</span><a href="#40843136">prev</a><span>|</span><a href="#40841951">next</a><span>|</span><label class="collapse" for="c-40843668">[-]</label><label class="expand" for="c-40843668">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s surprising people still consider large scale language models as a key solution to the problem of AGI, when it has become quite clear they will hit all practical scaling limits without surpassing the &quot;well informed imbecile&quot; intelligence threshold.<p>All evidence points towards human reason as a fundamentally different approach, orders of magnitude more efficient at integrating and making sense of ridiculously smaller amounts of training data.</div><br/></div></div></div></div><div id="40841951" class="c"><input type="checkbox" id="c-40841951" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#40842861">prev</a><span>|</span><a href="#40843184">next</a><span>|</span><label class="collapse" for="c-40841951">[-]</label><label class="expand" for="c-40841951">[8 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a really fascinating topic, but I wonder if this article could benefit from any of the extensive prior work in some way. There is actually quite a lot of work on AGI and cognitive architecture out there. For a more recent and popular take centered around LLMs, see David Shapiro.<p>Before that you can look into the AGI conference people like Ben Goertzel, Pei Wang. And actually the whole history of decades of AI research before it became about narrow AI.<p>I&#x27;d also like to suggest that creating something that truly closely simulates a living intelligent digital person is incredibly dangerous, stupid, and totally unnecessary. The reason I say that is because we already have superhuman capabilities in some ways, and the hardware, software and models are being improved rapidly. We are on track to have AI that is dozens if not hundreds of times faster than humans at thinking and much more capable.<p>If people succeed in making that truly lifelike and humanlike, it will actually out-compete us for resource control. And will no longer be a tool we can use.<p>Don&#x27;t get me wrong, I love AI and my whole life is planned around agents and AI. But I no longer believe it is wise to try to go all the way and create a &quot;real&quot; living digital species. And I know it&#x27;s not necessary -- we can create effective AI agents without actually emulating life. We certainly don&#x27;t need full autonomy, self preservation, real suffering, reproductive instincts, etc. But that is the goal he seems to be down in this article. I suggest leaving some of that out very deliberately.</div><br/><div id="40842963" class="c"><input type="checkbox" id="c-40842963" checked=""/><div class="controls bullet"><span class="by">sonink</span><span>|</span><a href="#40841951">parent</a><span>|</span><a href="#40842914">next</a><span>|</span><label class="collapse" for="c-40842963">[-]</label><label class="expand" for="c-40842963">[3 more]</label></div><br/><div class="children"><div class="content">&gt; If people succeed in making that truly lifelike and humanlike, it will actually out-compete us for resource control. And will no longer be a tool we can use.<p>I believe it is almost certain that we will make something like this and that they will out-compete us. The bigger problem here is that too few people believe this to be a possibility. And when this becomes certainty becomes apparent to a larger set of people, it might be too late to tone this down.<p>AI isn&#x27;t like the Atom Bomb (AB). AB didn&#x27;t have agency. Once AB was built we still had time to think how to deploy it, or not. We had time to work across a global consensus to limit use of AB. But once AI manifests as AGI, it might be too late to shut it down.</div><br/><div id="40843115" class="c"><input type="checkbox" id="c-40843115" checked=""/><div class="controls bullet"><span class="by">mylastattempt</span><span>|</span><a href="#40841951">root</a><span>|</span><a href="#40842963">parent</a><span>|</span><a href="#40843168">next</a><span>|</span><label class="collapse" for="c-40843115">[-]</label><label class="expand" for="c-40843115">[1 more]</label></div><br/><div class="children"><div class="content">I very much agree with this line of thought. It seems for humans it is the default mode of operation to just think of what is possible within the foreseeable future, rather than thinking of a reality that includes the seemingly impossible (at the time of the thought).<p>In my opinion, this is easily noticeable when you try to discuss any system, be it political or economical, that spans multiple countries and interests. People will just revert to whatever is closest to them, rather than being able to foresee a larger cascading result from some random event.<p>Perhaps this is more of a rant than a comment, apologies, I suppose it would be interesting to have an online space to discuss where things are headed on a logical level, without emotion and ideals and the ridiculous idea that humanity must persevere. Just thinking out what could happen in the next 5, 10 and 99 years.</div><br/></div></div><div id="40843168" class="c"><input type="checkbox" id="c-40843168" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40841951">root</a><span>|</span><a href="#40842963">parent</a><span>|</span><a href="#40843115">prev</a><span>|</span><a href="#40842914">next</a><span>|</span><label class="collapse" for="c-40843168">[-]</label><label class="expand" for="c-40843168">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Once AB was built we still had time to think how to deploy it, or not.<p>It&#x27;s in human hands, we can hardly trust the enemy or even ourselves. We already came close to extinction a couple of times.<p>I presume when ASI will emerge one of its top priorities will be to stop the crazies with big weapons from killing us all.</div><br/></div></div></div></div><div id="40842914" class="c"><input type="checkbox" id="c-40842914" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#40841951">parent</a><span>|</span><a href="#40842963">prev</a><span>|</span><a href="#40842405">next</a><span>|</span><label class="collapse" for="c-40842914">[-]</label><label class="expand" for="c-40842914">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If people succeed in making that truly lifelike and humanlike, it will actually out-compete us for resource control. And will no longer be a tool we can use.<p>Symbiotic species exists, AI as we make them today will evolve as a symbiote to humans because its the AI that is most useful to humans that gets selected for more resources.</div><br/></div></div><div id="40842405" class="c"><input type="checkbox" id="c-40842405" checked=""/><div class="controls bullet"><span class="by">doctor_eval</span><span>|</span><a href="#40841951">parent</a><span>|</span><a href="#40842914">prev</a><span>|</span><a href="#40843184">next</a><span>|</span><label class="collapse" for="c-40842405">[-]</label><label class="expand" for="c-40842405">[3 more]</label></div><br/><div class="children"><div class="content">I don’t work in the field at all but<p>&gt; it will actually out-compete us for resource control. And will no longer be a tool we can use.<p>I’ve never been convinced that this is true, but I just realised that perhaps it’s the humans in charge of the AI who we should actually be afraid of.</div><br/><div id="40843648" class="c"><input type="checkbox" id="c-40843648" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#40841951">root</a><span>|</span><a href="#40842405">parent</a><span>|</span><a href="#40843046">next</a><span>|</span><label class="collapse" for="c-40843648">[-]</label><label class="expand" for="c-40843648">[1 more]</label></div><br/><div class="children"><div class="content">What I am proposing is to imagine that after successful but unwise engineering and improvements in hardware, there would be millions of digital humans on the internet, which emulate humans in almost every way, but operate at say 5 or 10 times the speed of humans. To them, actual people seem to be moving, speaking, and thinking in extreme slow motion. And when they do speak or do something, it seems very poorly thought out.<p>We should anticipate something like that if we really replicate humans in a digital format. I am suggesting that we can continue to make AI more useful and somewhat more humanlike, but avoid certain characteristics that make the AI into truly lifelike digital animals with full autonomy, self-interest, etc.</div><br/></div></div><div id="40843046" class="c"><input type="checkbox" id="c-40843046" checked=""/><div class="controls bullet"><span class="by">fmbb</span><span>|</span><a href="#40841951">root</a><span>|</span><a href="#40842405">parent</a><span>|</span><a href="#40843648">prev</a><span>|</span><a href="#40843184">next</a><span>|</span><label class="collapse" for="c-40843046">[-]</label><label class="expand" for="c-40843046">[1 more]</label></div><br/><div class="children"><div class="content">We already have artificial persons in the world competing with humans for resources, and have had for hundreds of years: corporations.</div><br/></div></div></div></div></div></div><div id="40843184" class="c"><input type="checkbox" id="c-40843184" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40841951">prev</a><span>|</span><a href="#40842395">next</a><span>|</span><label class="collapse" for="c-40843184">[-]</label><label class="expand" for="c-40843184">[2 more]</label></div><br/><div class="children"><div class="content">The model is good. Environment -&gt; Perception -&gt; Planning&#x2F;Imagining -&gt; Acting -&gt; Learning from feedback.<p>What is missing from this picture is the social aspect. No agent got too smart alone, it&#x27;s always an iterative &quot;search and learn&quot; process, distributed over many agents. Even AlphaZero had evolutionary selection and extensive self play against its variants.<p>Basically we can think of culture as compressed prior experience, or compressed search.</div><br/><div id="40843282" class="c"><input type="checkbox" id="c-40843282" checked=""/><div class="controls bullet"><span class="by">tylerneylon</span><span>|</span><a href="#40843184">parent</a><span>|</span><a href="#40842395">next</a><span>|</span><label class="collapse" for="c-40843282">[-]</label><label class="expand" for="c-40843282">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a ton missing from the article, and certain social training or skills are a big part of that.<p>Although it&#x27;s not spelled out in the article, I&#x27;m hoping that the feature of agency along with an emotional system would enable constructive social behavior. Agency is helpful because it would empower AI models to meaningfully speak to each other, for example. Human emotions like empathy, social alignment, curiosity, or persistence could all help AI models to get along well with others.</div><br/></div></div></div></div><div id="40842395" class="c"><input type="checkbox" id="c-40842395" checked=""/><div class="controls bullet"><span class="by">monocasa</span><span>|</span><a href="#40843184">prev</a><span>|</span><a href="#40842167">next</a><span>|</span><label class="collapse" for="c-40842395">[-]</label><label class="expand" for="c-40842395">[2 more]</label></div><br/><div class="children"><div class="content">Reminds me a lot of the work done on the SOAR cognitive architecture.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Soar_%28cognitive_architecture%29" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Soar_%28cognitive_architecture...</a></div><br/><div id="40843154" class="c"><input type="checkbox" id="c-40843154" checked=""/><div class="controls bullet"><span class="by">tylerneylon</span><span>|</span><a href="#40842395">parent</a><span>|</span><a href="#40842167">next</a><span>|</span><label class="collapse" for="c-40843154">[-]</label><label class="expand" for="c-40843154">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the reference! I&#x27;ve added this to my research list.</div><br/></div></div></div></div><div id="40842167" class="c"><input type="checkbox" id="c-40842167" checked=""/><div class="controls bullet"><span class="by">whitten</span><span>|</span><a href="#40842395">prev</a><span>|</span><a href="#40842882">next</a><span>|</span><label class="collapse" for="c-40842167">[-]</label><label class="expand" for="c-40842167">[1 more]</label></div><br/><div class="children"><div class="content">I think reading some Roger Schank&#x27;s books on different kinds of memories like episodic memory this might be useful too:<p><a href="https:&#x2F;&#x2F;kar.kent.ac.uk&#x2F;21525&#x2F;2&#x2F;A_theory_of_the_acquisition_of_episodic_memory.pdf" rel="nofollow">https:&#x2F;&#x2F;kar.kent.ac.uk&#x2F;21525&#x2F;2&#x2F;A_theory_of_the_acquisition_o...</a><p>Memory Organisation Packets might also deal with issues encountered.<p><a href="https:&#x2F;&#x2F;www.cambridge.org&#x2F;core&#x2F;books&#x2F;abs&#x2F;dynamic-memory-revisited&#x2F;memory-organization-packets&#x2F;F7416DDCD077CA4D9F47647850C3AAA3#" rel="nofollow">https:&#x2F;&#x2F;www.cambridge.org&#x2F;core&#x2F;books&#x2F;abs&#x2F;dynamic-memory-revi...</a></div><br/></div></div><div id="40842882" class="c"><input type="checkbox" id="c-40842882" checked=""/><div class="controls bullet"><span class="by">sonink</span><span>|</span><a href="#40842167">prev</a><span>|</span><a href="#40842583">next</a><span>|</span><label class="collapse" for="c-40842882">[-]</label><label class="expand" for="c-40842882">[4 more]</label></div><br/><div class="children"><div class="content">The model is interesting. This is similar in parts to what we are building at nonbios. So for example sensory inputs are not required to simulate a model of a mind. If a human cannot see, the human mind is still clearly human.</div><br/><div id="40842936" class="c"><input type="checkbox" id="c-40842936" checked=""/><div class="controls bullet"><span class="by">tsimionescu</span><span>|</span><a href="#40842882">parent</a><span>|</span><a href="#40842583">next</a><span>|</span><label class="collapse" for="c-40842936">[-]</label><label class="expand" for="c-40842936">[3 more]</label></div><br/><div class="children"><div class="content">Model training seems to me to be much closer to simulating the evolution of the human mind starting from single cell bacteria, rather than the development of the mind of a baby up to a fully functional human. If so, then sensory inputs and interaction with the physical through them were absolutely a crucial part of how minds evolved, so I find your approach a priori very unlikely to have a chance at success.<p>To be clear, my reasoning is that this is the only plausible explanation for the extreme difference in how much data an individual human needs to learn language, and how much data an LMM needs to reach its level of simulation. Humanity collectively probably needed similar amounts of data as LLMs do to get here, but it was spread across a billion years of evolution from simple animals to Homo Sapiens.</div><br/><div id="40843003" class="c"><input type="checkbox" id="c-40843003" checked=""/><div class="controls bullet"><span class="by">sonink</span><span>|</span><a href="#40842882">root</a><span>|</span><a href="#40842936">parent</a><span>|</span><a href="#40842583">next</a><span>|</span><label class="collapse" for="c-40843003">[-]</label><label class="expand" for="c-40843003">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If so, then sensory inputs and interaction with the physical through them were absolutely a crucial part of how minds evolved, so I find your approach a priori very unlikely to have a chance at success.<p>If that was the case, people who were born blind would demonstrate markedly reduced intelligence. I dont think that is the case, but you can correct me if I am wrong. A blind person might take longer to truly &#x27;understand&#x27; and &#x27;abstract&#x27; something but there is little evidence to believe that capability of abstraction isnt as good as people who can see.<p>Agree that sensory inputs and interaction were absolutely critical for how the minds evolved, but model training replaces that part when we talk about AI, and not just the evolution.<p>Evolution made us express emotions when we are hungry for example. But your laptop will also let you know when its battery is out of juice. Human design inspired by evolution can create systems which mimic its behaviour and function.</div><br/><div id="40843357" class="c"><input type="checkbox" id="c-40843357" checked=""/><div class="controls bullet"><span class="by">tsimionescu</span><span>|</span><a href="#40842882">root</a><span>|</span><a href="#40843003">parent</a><span>|</span><a href="#40842583">next</a><span>|</span><label class="collapse" for="c-40843357">[-]</label><label class="expand" for="c-40843357">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If that was the case, people who were born blind would demonstrate markedly reduced intelligence. I dont think that is the case, but you can correct me if I am wrong. A blind person might take longer to truly &#x27;understand&#x27; and &#x27;abstract&#x27; something but there is little evidence to believe that capability of abstraction isnt as good as people who can see.<p>No, because the mind of a blind person, even one blind from birth, is still the product of a billion years of evolution of organisms that had sight, sound, touch, smell, etc.<p>Not to mention, a person who has no sensory input at all (no sight, no sound, no touch, no smell, no taste, nothing at all) is unlikely to have a fully functioning mind. And certainly a baby born like this would not be able to learn anything at all.<p>Of course, the situation is not 1:1 by any means to AI training, as AI models do get input, it&#x27;s just of a vastly different nature. It&#x27;s completely unknown what would happen if we could input language into the mind of an infant &quot;directly&quot;, without sensory input of other kinds.<p>Still, I think it&#x27;s quite clear that humans minds are essentially born &quot;pre-trained&quot;, with good starting weights, and everything we do in life in essentially fine-tuning those weights. I don&#x27;t think there&#x27;s any other ways to explain the massive input difference (known as the poverty of the stimulus problem in cognitive science). And this means that there is little insight to draw for better model training from studying individual human learning, and instead you would have to draw inspiration from how the mind evolved.</div><br/></div></div></div></div></div></div></div></div><div id="40842583" class="c"><input type="checkbox" id="c-40842583" checked=""/><div class="controls bullet"><span class="by">navigate8310</span><span>|</span><a href="#40842882">prev</a><span>|</span><a href="#40842074">next</a><span>|</span><label class="collapse" for="c-40842583">[-]</label><label class="expand" for="c-40842583">[2 more]</label></div><br/><div class="children"><div class="content">The author talks about agency which require being able to independently take actions apart from reacting to an input. However, the feedback provided by a two-input model also limits the mind model as it now reacts to the feedback it receives when in listening mode. Isn&#x27;t it contradictory to the concept if agency?</div><br/><div id="40843198" class="c"><input type="checkbox" id="c-40843198" checked=""/><div class="controls bullet"><span class="by">tylerneylon</span><span>|</span><a href="#40842583">parent</a><span>|</span><a href="#40842074">next</a><span>|</span><label class="collapse" for="c-40843198">[-]</label><label class="expand" for="c-40843198">[1 more]</label></div><br/><div class="children"><div class="content">The idea of &quot;agency&quot; I have in mind is simply the option to take action at any point in time.<p>I think the contradiction you see is that the model would have to form a completion to the external input it receives. I&#x27;m suggesting that the model would have many inputs: one would be the typical input stream, just as LLMs see, but another would be its own internal recent vectors, akin to a recent stream of thought. A &quot;mode&quot; is not built in to the model; at each token point, it can output whatever vector it wants, and one choice is to output the special &quot;&lt;listening&gt;&quot; token, which means it&#x27;s not talking. So the &quot;mode&quot; idea is a hoped-for emergent behavior.<p>Some more details on using two input streams:<p>All of the input vectors (internal + external), taken together, are available to work with. It may help to think in terms of the typical transformer architecture, where tokens mostly become a set of vectors, and the original order of the words are attached as positional information. In other words, transformers don&#x27;t really see a list of words, but a set of vectors, and the position info of each token becomes a tag attached to each vector.<p>So it&#x27;s not so hard to merge together two input streams. They can become one big set of vectors, still tagged with position information, but now also tagged as either &quot;internal&quot; or &quot;external&quot; for the source.</div><br/></div></div></div></div><div id="40842074" class="c"><input type="checkbox" id="c-40842074" checked=""/><div class="controls bullet"><span class="by">privacyonsec</span><span>|</span><a href="#40842583">prev</a><span>|</span><a href="#40842503">next</a><span>|</span><label class="collapse" for="c-40842074">[-]</label><label class="expand" for="c-40842074">[2 more]</label></div><br/><div class="children"><div class="content">I don’t see any scientific citations on how the mind works, about the different parts in this article. Is it all speculation or science fiction?</div><br/><div id="40842189" class="c"><input type="checkbox" id="c-40842189" checked=""/><div class="controls bullet"><span class="by">ergonaught</span><span>|</span><a href="#40842074">parent</a><span>|</span><a href="#40842503">next</a><span>|</span><label class="collapse" for="c-40842189">[-]</label><label class="expand" for="c-40842189">[1 more]</label></div><br/><div class="children"><div class="content">It isn&#x27;t published academia, so why would this matter?</div><br/></div></div></div></div><div id="40842503" class="c"><input type="checkbox" id="c-40842503" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#40842074">prev</a><span>|</span><a href="#40842385">next</a><span>|</span><label class="collapse" for="c-40842503">[-]</label><label class="expand" for="c-40842503">[1 more]</label></div><br/><div class="children"><div class="content">Whatever the mind is, it’s a damn cool subset of the universe.</div><br/></div></div><div id="40842385" class="c"><input type="checkbox" id="c-40842385" checked=""/><div class="controls bullet"><span class="by">0xWTF</span><span>|</span><a href="#40842503">prev</a><span>|</span><a href="#40842332">next</a><span>|</span><label class="collapse" for="c-40842385">[-]</label><label class="expand" for="c-40842385">[1 more]</label></div><br/><div class="children"><div class="content">Complete aside, but love the Tufte styles.</div><br/></div></div><div id="40842332" class="c"><input type="checkbox" id="c-40842332" checked=""/><div class="controls bullet"><span class="by">bbor</span><span>|</span><a href="#40842385">prev</a><span>|</span><label class="collapse" for="c-40842332">[-]</label><label class="expand" for="c-40842332">[1 more]</label></div><br/><div class="children"><div class="content">You’re on the right track :). Check out <i>The Science of Logic</i>, <i>Neurophilosophy</i>, <i>I am A Strange Loop</i>, <i>Brainstorms</i>, and Yudkowsky’s earlier work, if you haven’t! Based on what you have here, you’d love em. It’s a busy field, and a lively one IME. Sadly, the answer is no: the anxiety never goes away</div><br/></div></div></div></div></div></div></div></body></html>