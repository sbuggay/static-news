<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1724922069026" as="style"/><link rel="stylesheet" href="styles.css?v=1724922069026"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://gamengen.github.io">Diffusion models are real-time game engines</a> <span class="domain">(<a href="https://gamengen.github.io">gamengen.github.io</a>)</span></div><div class="subtext"><span>jmorgan</span> | <span>332 comments</span></div><br/><div><div id="41375657" class="c"><input type="checkbox" id="c-41375657" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#41380908">next</a><span>|</span><label class="collapse" for="c-41375657">[-]</label><label class="expand" for="c-41375657">[86 more]</label></div><br/><div class="children"><div class="content">So, this is surprising. Apparently there’s more cause, effect, and sequencing in diffusion models than what I expected, which would be roughly ‘none’. Google here uses SD 1.4, as the core of the diffusion model, which is a nice reminder that open models are useful to even giant cloud monopolies.<p>The two main things of note I took away from the summary were: 1) they got infinite training data using agents playing doom (makes sense), and 2) they added Gaussian noise to source frames and rewarded the agent for ‘correcting’ sequential frames back, and said this was critical to get long range stable ‘rendering’ out of the model.<p>That last is intriguing — they explain the intuition as teaching the model to do error correction &#x2F; guide it to be stable.<p>Finally, I wonder if this model would be easy to fine tune for ‘photo realistic’ &#x2F; ray traced restyling — I’d be super curious to see how hard it would be to get a ‘nicer’ rendering out of this model, treating it as a doom foundation model of sorts.<p>Anyway, a fun idea that worked! Love those.</div><br/><div id="41378523" class="c"><input type="checkbox" id="c-41378523" checked=""/><div class="controls bullet"><span class="by">wavemode</span><span>|</span><a href="#41375657">parent</a><span>|</span><a href="#41376240">next</a><span>|</span><label class="collapse" for="c-41378523">[-]</label><label class="expand" for="c-41378523">[57 more]</label></div><br/><div class="children"><div class="content">&gt; Apparently there’s more cause, effect, and sequencing in diffusion models than what I expected<p>To temper this a bit, you may want to pay close attention to the demo videos. The player rarely backtracks, and for good reason - the few times the character does turn around and look back at something a second time, it has changed significantly (the most noticeable I think is the room with the grey wall and triangle sign).<p>This falls in line with how we&#x27;d expect a diffusion model to behave - it&#x27;s trained on many billions of frames of gameplay, so it&#x27;s very good at generating a plausible -next- frame of gameplay based on some previous frames. But it doesn&#x27;t deeply understand logical gameplay constraints, like remembering level geometry.</div><br/><div id="41378928" class="c"><input type="checkbox" id="c-41378928" checked=""/><div class="controls bullet"><span class="by">dewarrn1</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378523">parent</a><span>|</span><a href="#41379004">next</a><span>|</span><label class="collapse" for="c-41378928">[-]</label><label class="expand" for="c-41378928">[16 more]</label></div><br/><div class="children"><div class="content">Great observation. And not entirely unlike normal human visual perception which is notoriously vulnerable to missing highly salient information; I&#x27;m reminded of the &quot;gorillas in our midst&quot; work by Dan Simons and Christopher Chabris [0].<p>[0]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Inattentional_blindness#Invisible_Gorilla_Test" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Inattentional_blindness#Invisi...</a></div><br/><div id="41379746" class="c"><input type="checkbox" id="c-41379746" checked=""/><div class="controls bullet"><span class="by">lawlessone</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378928">parent</a><span>|</span><a href="#41379912">next</a><span>|</span><label class="collapse" for="c-41379746">[-]</label><label class="expand" for="c-41379746">[1 more]</label></div><br/><div class="children"><div class="content">I reminds me of dreaming. When you do something and turn back to check it has turned into something completely different.<p>edit: someone should train it on MyHouse.wad</div><br/></div></div><div id="41379912" class="c"><input type="checkbox" id="c-41379912" checked=""/><div class="controls bullet"><span class="by">robotresearcher</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378928">parent</a><span>|</span><a href="#41379746">prev</a><span>|</span><a href="#41379736">next</a><span>|</span><label class="collapse" for="c-41379912">[-]</label><label class="expand" for="c-41379912">[2 more]</label></div><br/><div class="children"><div class="content">Not noticing to a gorilla that ‘shouldn’t’ be there is not the same thing as object permanence. Even quite young babies are surprised by objects that go missing.</div><br/><div id="41380143" class="c"><input type="checkbox" id="c-41380143" checked=""/><div class="controls bullet"><span class="by">dewarrn1</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379912">parent</a><span>|</span><a href="#41379736">next</a><span>|</span><label class="collapse" for="c-41380143">[-]</label><label class="expand" for="c-41380143">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s absolutely true. It&#x27;s also well-established by Simons et al. and others that healthy normal adults maintain only a very sparse visual representation of their surroundings, anchored but not perfectly predicted by attention, and this drives the unattended gorilla phenomenon (along with many others). I don&#x27;t work in this domain, but I would suggest that object permanence probably starts with attending and perceiving an object, whereas the inattentional or change blindness phenomena mostly (but not exclusively) occur when an object is not attended (or only briefly attended) <i>or</i> attention is divided by some competing task.</div><br/></div></div></div></div><div id="41379736" class="c"><input type="checkbox" id="c-41379736" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378928">parent</a><span>|</span><a href="#41379912">prev</a><span>|</span><a href="#41381447">next</a><span>|</span><label class="collapse" for="c-41379736">[-]</label><label class="expand" for="c-41379736">[9 more]</label></div><br/><div class="children"><div class="content">Are you saying if I turn around, I’ll be surprised at what I find ? I don’t feel like this is accurate at all.</div><br/><div id="41380209" class="c"><input type="checkbox" id="c-41380209" checked=""/><div class="controls bullet"><span class="by">dewarrn1</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379736">parent</a><span>|</span><a href="#41380408">next</a><span>|</span><label class="collapse" for="c-41380209">[-]</label><label class="expand" for="c-41380209">[3 more]</label></div><br/><div class="children"><div class="content">Not exactly, but our representation of what&#x27;s behind us is a lot more sparse than we would assume.  That is, I might not be surprised by what I see when I turn around, but it could have changed pretty radically since I last looked, and I might not notice.  In fact, an observer might be quite surprised that I missed the change.<p>Objectively, Simons and Chabris (and many others) have a lot of data to support these ideas. Subjectively, I can say that these types of tasks (inattentional blindness, change blindness, etc.) are humbling.</div><br/><div id="41380917" class="c"><input type="checkbox" id="c-41380917" checked=""/><div class="controls bullet"><span class="by">jerf</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41380209">parent</a><span>|</span><a href="#41380408">next</a><span>|</span><label class="collapse" for="c-41380917">[-]</label><label class="expand" for="c-41380917">[2 more]</label></div><br/><div class="children"><div class="content">Well, it&#x27;s a bit of a spoiler to encounter this video in this context, but this is a very good video: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=LRFMuGBP15U" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=LRFMuGBP15U</a><p>Even having a clue why I&#x27;m linking this, I virtually guarantee you won&#x27;t catch everything.<p>And even if you do catch everything... the <i>real</i> thing to notice is that you had to <i>look</i>. Your brain does not flag these things naturally. Dreams are notorious for this sort of thing, but even in the waking world your model of the world is much less rich than you think. Magic tricks like to hide in this space, for instance.</div><br/><div id="41382646" class="c"><input type="checkbox" id="c-41382646" checked=""/><div class="controls bullet"><span class="by">dewarrn1</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41380917">parent</a><span>|</span><a href="#41380408">next</a><span>|</span><label class="collapse" for="c-41382646">[-]</label><label class="expand" for="c-41382646">[1 more]</label></div><br/><div class="children"><div class="content">Yup, great example!  Simons&#x27;s lab has done some things along exactly these lines [0], too.<p>[0]: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=wBoMjORwA-4" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=wBoMjORwA-4</a></div><br/></div></div></div></div></div></div><div id="41380408" class="c"><input type="checkbox" id="c-41380408" checked=""/><div class="controls bullet"><span class="by">ajuc</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379736">parent</a><span>|</span><a href="#41380209">prev</a><span>|</span><a href="#41379951">next</a><span>|</span><label class="collapse" for="c-41380408">[-]</label><label class="expand" for="c-41380408">[2 more]</label></div><br/><div class="children"><div class="content">The opposite - if you turn around and there&#x27;s something that wasn&#x27;t there the last time - you&#x27;ll likely not notice if it&#x27;s not out of place. You&#x27;ll just assume it was there and you weren&#x27;t paying attention.<p>We don&#x27;t memorize things that the environment remembers for us if they aren&#x27;t relevant for other reasons.</div><br/><div id="41386572" class="c"><input type="checkbox" id="c-41386572" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41380408">parent</a><span>|</span><a href="#41379951">next</a><span>|</span><label class="collapse" for="c-41386572">[-]</label><label class="expand" for="c-41386572">[1 more]</label></div><br/><div class="children"><div class="content">We also don&#x27;t just work on images. We work on a lot of sensory data. So i think images of the environment are just one part of it.</div><br/></div></div></div></div><div id="41379951" class="c"><input type="checkbox" id="c-41379951" checked=""/><div class="controls bullet"><span class="by">matheusd</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379736">parent</a><span>|</span><a href="#41380408">prev</a><span>|</span><a href="#41381447">next</a><span>|</span><label class="collapse" for="c-41379951">[-]</label><label class="expand" for="c-41379951">[3 more]</label></div><br/><div class="children"><div class="content">If a generic human glances at an unfamiliar screen&#x2F;wall&#x2F;room, can they accurately, pixel-perfectly reconstruct every single element of it? Can they do it for every single screen they have seen in their entire lives?</div><br/><div id="41380107" class="c"><input type="checkbox" id="c-41380107" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379951">parent</a><span>|</span><a href="#41381447">next</a><span>|</span><label class="collapse" for="c-41380107">[-]</label><label class="expand" for="c-41380107">[2 more]</label></div><br/><div class="children"><div class="content">I never said pixel perfect, but I would be surprised if whole objects , like flaming lanterns suddenly appeared.<p>What this demo demonstrates to me is how incredible willing we are to accept what seems familiar to us as accurate.<p>I bet if you look closely and objectively you will see even more anomalies. But at first watch, I didn’t see most errors because I think accepting something is more efficient for the brain.</div><br/><div id="41381248" class="c"><input type="checkbox" id="c-41381248" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41380107">parent</a><span>|</span><a href="#41381447">next</a><span>|</span><label class="collapse" for="c-41381248">[-]</label><label class="expand" for="c-41381248">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;d likely be surprised by a flaming lantern unless you were in Flaming Lanterns &#x27;R Us, but if you were watching a video of a card trick and the two participants changed clothes while the camera wasn&#x27;t focused on them, you may well miss that and the other five changes that came with that.</div><br/></div></div></div></div></div></div></div></div><div id="41381447" class="c"><input type="checkbox" id="c-41381447" checked=""/><div class="controls bullet"><span class="by">throwway_278314</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378928">parent</a><span>|</span><a href="#41379736">prev</a><span>|</span><a href="#41379004">next</a><span>|</span><label class="collapse" for="c-41381447">[-]</label><label class="expand" for="c-41381447">[3 more]</label></div><br/><div class="children"><div class="content">Work which exaggerates the blindness.<p>The people were told to focus very deeply on a certain aspect of the scene. Maintaining that focus means explicitly blocking things not related to that focus. Also, there is social pressure at the end to have peformed well at the task; evaluating them on a task which is intentionally completely different than the one explicitly given is going to bias people away from reporting gorillas.<p>And also, &quot;notice anything unusual&quot; is a pretty vague prompt. No-one in the video thought the gorillas were unusual, so if the PEOPLE IN THE SCENE thought gorillas were normal, why would I think they were strange? Look at any TV show, they are all full of things which are pretty crazy unusual in normal life, yet not unusual in terms of the plot.<p>Why would you think the gorillas were unusual?</div><br/><div id="41388429" class="c"><input type="checkbox" id="c-41388429" checked=""/><div class="controls bullet"><span class="by">InDubioProRubio</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41381447">parent</a><span>|</span><a href="#41381605">next</a><span>|</span><label class="collapse" for="c-41388429">[-]</label><label class="expand" for="c-41388429">[1 more]</label></div><br/><div class="children"><div class="content">Does that work on autistic people? Having no filters or fewer filters, should allow them to be more efficient &quot;on guard duty&quot; looking for unexpected things.</div><br/></div></div><div id="41381605" class="c"><input type="checkbox" id="c-41381605" checked=""/><div class="controls bullet"><span class="by">dewarrn1</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41381447">parent</a><span>|</span><a href="#41388429">prev</a><span>|</span><a href="#41379004">next</a><span>|</span><label class="collapse" for="c-41381605">[-]</label><label class="expand" for="c-41381605">[1 more]</label></div><br/><div class="children"><div class="content">I understand what you mean.  I believe that the authors would contend that what you&#x27;re describing is a typical attentional state for an awake&#x2F;aware human: focused mostly on one thing, and with surprisingly little awareness of most other things (until&#x2F;unless they are in turn attended).<p>Furthermore, even what we attend to isn&#x27;t always represented with all that much detail.  Simons has a whole series of cool demonstration experiments where they show that they can swap out someone you&#x27;re speaking with (an unfamiliar conversational partner like a store clerk or someone asking for directions), and you may not even notice [0].  It&#x27;s rather eerie.<p>[0]: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=FWSxSQsspiQ&amp;t=5s" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=FWSxSQsspiQ&amp;t=5s</a></div><br/></div></div></div></div></div></div><div id="41379004" class="c"><input type="checkbox" id="c-41379004" checked=""/><div class="controls bullet"><span class="by">nmstoker</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378523">parent</a><span>|</span><a href="#41378928">prev</a><span>|</span><a href="#41378844">next</a><span>|</span><label class="collapse" for="c-41379004">[-]</label><label class="expand" for="c-41379004">[11 more]</label></div><br/><div class="children"><div class="content">I saw a longer video of this that Ethan Mollick posted and in that one, the sequences are longer and they do appear to demonstrate a fair amount of consistency. The clips don&#x27;t backtrack in the summary video on the paper&#x27;s home page because they&#x27;re showing a number of district environments but you only get a few seconds of each.<p>If I studied the longer one more closely, I&#x27;m sure inconsistencies would be seen but it seemed able to recall presence&#x2F;absence of destroyed items, dead monsters etc on subsequent loops around a central obstruction that completely obscured them for quite a while. This did seem pretty odd to me, as I expected it to match how you&#x27;d described it.</div><br/><div id="41379160" class="c"><input type="checkbox" id="c-41379160" checked=""/><div class="controls bullet"><span class="by">wavemode</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379004">parent</a><span>|</span><a href="#41378844">next</a><span>|</span><label class="collapse" for="c-41379160">[-]</label><label class="expand" for="c-41379160">[10 more]</label></div><br/><div class="children"><div class="content">Yes it definitely is very good for simulating gameplay footage, don&#x27;t get me wrong. Its input for predicting the next frame is not just the previous frame, it has access to a whole sequence of prior frames.<p>But to say the model is simulating actual gameplay (i.e. that a person could actually play Doom in this) is far fetched. It&#x27;s definitely great that the model was able to remember that the gray wall was still there after we turned around, but it&#x27;s untenable for actual gameplay that the wall completely changed location and orientation.</div><br/><div id="41379645" class="c"><input type="checkbox" id="c-41379645" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379160">parent</a><span>|</span><a href="#41379375">next</a><span>|</span><label class="collapse" for="c-41379645">[-]</label><label class="expand" for="c-41379645">[8 more]</label></div><br/><div class="children"><div class="content">&gt; <i>it&#x27;s untenable for actual gameplay that the wall completely changed location and orientation.</i><p>It would in an SCP-themed game. Or dreamscape&#x2F;Inception themed one.<p>Hell, &quot;you&#x27;re trapped in Doom-like dreamscape, escape before you lose your mind&quot; is a very interesting pitch for a game. Basically take this Doom thing and make walking though a specific, unique-looking doorway from the original game to be the victory condition - the player&#x27;s job would be to coerce the model to generate it, while also not dying in the Doom fever dream game itself. I&#x27;d play the hell out of this.<p>(Implementation-wise, just loop in a simple recognition model to continously evaluate victory condiiton from last few frames, and some OCR to detect when player&#x27;s hit points indicator on the HUD drops to zero.)<p>(I&#x27;ll happily pay $100 this year to the first project that gets this to work. I bet I&#x27;m not the only one. Doesn&#x27;t have to be Doom specifically, just has to be interesting.)</div><br/><div id="41380666" class="c"><input type="checkbox" id="c-41380666" checked=""/><div class="controls bullet"><span class="by">kridsdale1</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379645">parent</a><span>|</span><a href="#41379886">next</a><span>|</span><label class="collapse" for="c-41380666">[-]</label><label class="expand" for="c-41380666">[2 more]</label></div><br/><div class="children"><div class="content">Check out the actual modern DOOM WAD MyHouse which implements these ideas. It totally breaks our preconceptions of what the DOOM engine is capable of.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;MyHouse.wad" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;MyHouse.wad</a></div><br/><div id="41381686" class="c"><input type="checkbox" id="c-41381686" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41380666">parent</a><span>|</span><a href="#41379886">next</a><span>|</span><label class="collapse" for="c-41381686">[-]</label><label class="expand" for="c-41381686">[1 more]</label></div><br/><div class="children"><div class="content">MyHouse is excellent, but it mostly breaks our perception of what the Doom engine is capable of by not <i>really</i> using the Doom engine. It leans heavily on engine features which were embellishments by the GZDoom project, and never existed in the original Doom codebase.</div><br/></div></div></div></div><div id="41379886" class="c"><input type="checkbox" id="c-41379886" checked=""/><div class="controls bullet"><span class="by">wavemode</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379645">parent</a><span>|</span><a href="#41380666">prev</a><span>|</span><a href="#41379375">next</a><span>|</span><label class="collapse" for="c-41379886">[-]</label><label class="expand" for="c-41379886">[5 more]</label></div><br/><div class="children"><div class="content">To be honest, I agree! That would be an interesting gameplay concept for sure.<p>Mainly just wanted to temper expectations I&#x27;m seeing throughout this thread that the model is actually simulating Doom. I don&#x27;t know what will be required to get from here to there, but we&#x27;re definitely not there yet.</div><br/><div id="41380137" class="c"><input type="checkbox" id="c-41380137" checked=""/><div class="controls bullet"><span class="by">KajMagnus</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379886">parent</a><span>|</span><a href="#41380013">next</a><span>|</span><label class="collapse" for="c-41380137">[-]</label><label class="expand" for="c-41380137">[1 more]</label></div><br/><div class="children"><div class="content">Or if training the model on many FPS games? Surviving in one nightmare that morphs into another, into another, into another ...</div><br/></div></div><div id="41380013" class="c"><input type="checkbox" id="c-41380013" checked=""/><div class="controls bullet"><span class="by">ValentinA23</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379886">parent</a><span>|</span><a href="#41380137">prev</a><span>|</span><a href="#41379375">next</a><span>|</span><label class="collapse" for="c-41380013">[-]</label><label class="expand" for="c-41380013">[3 more]</label></div><br/><div class="children"><div class="content">What you&#x27;re pointing at mirrors the same kind of limitation in using LLMs for role-play&#x2F;interactive fictions.</div><br/><div id="41381281" class="c"><input type="checkbox" id="c-41381281" checked=""/><div class="controls bullet"><span class="by">lawlessone</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41380013">parent</a><span>|</span><a href="#41379375">next</a><span>|</span><label class="collapse" for="c-41381281">[-]</label><label class="expand" for="c-41381281">[2 more]</label></div><br/><div class="children"><div class="content">Maybe a hybrid approach would work. Certain things like inventory being stored as variables, lists etc.<p>Wouldn&#x27;t be as pure though.</div><br/><div id="41385030" class="c"><input type="checkbox" id="c-41385030" checked=""/><div class="controls bullet"><span class="by">crooked-v</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41381281">parent</a><span>|</span><a href="#41379375">next</a><span>|</span><label class="collapse" for="c-41385030">[-]</label><label class="expand" for="c-41385030">[1 more]</label></div><br/><div class="children"><div class="content">Give it state by having a rendered-but-offscreen pixel area that&#x27;s fed back in as byte data for the next frame.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41379375" class="c"><input type="checkbox" id="c-41379375" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379160">parent</a><span>|</span><a href="#41379645">prev</a><span>|</span><a href="#41378844">next</a><span>|</span><label class="collapse" for="c-41379375">[-]</label><label class="expand" for="c-41379375">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s an empirical question, right? But they didn&#x27;t do it...</div><br/></div></div></div></div></div></div><div id="41378844" class="c"><input type="checkbox" id="c-41378844" checked=""/><div class="controls bullet"><span class="by">whiteboardr</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378523">parent</a><span>|</span><a href="#41379004">prev</a><span>|</span><a href="#41378617">next</a><span>|</span><label class="collapse" for="c-41378844">[-]</label><label class="expand" for="c-41378844">[9 more]</label></div><br/><div class="children"><div class="content">But does it need to be frame-based?<p>What if you combine this with an engine in parallel that provides all geometry including characters and objects with their respective behavior, recording changes made through interactions the other model generates, talking back to it?<p>A dialogue between two parties with different functionality so to speak.<p>(Non technical person here - just fantasizing)</div><br/><div id="41379109" class="c"><input type="checkbox" id="c-41379109" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378844">parent</a><span>|</span><a href="#41379975">next</a><span>|</span><label class="collapse" for="c-41379109">[-]</label><label class="expand" for="c-41379109">[1 more]</label></div><br/><div class="children"><div class="content">In that case, the title of the article wouldn’t be true anymore. It seems like a better plan, though.</div><br/></div></div><div id="41379975" class="c"><input type="checkbox" id="c-41379975" checked=""/><div class="controls bullet"><span class="by">robotresearcher</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378844">parent</a><span>|</span><a href="#41379109">prev</a><span>|</span><a href="#41379251">next</a><span>|</span><label class="collapse" for="c-41379975">[-]</label><label class="expand" for="c-41379975">[3 more]</label></div><br/><div class="children"><div class="content">In that scheme what is the NN providing that a classical renderer would not? DOOM ran great on an Intel 486, which is not a lot of computer.</div><br/><div id="41381181" class="c"><input type="checkbox" id="c-41381181" checked=""/><div class="controls bullet"><span class="by">Sohcahtoa82</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379975">parent</a><span>|</span><a href="#41380018">next</a><span>|</span><label class="collapse" for="c-41381181">[-]</label><label class="expand" for="c-41381181">[1 more]</label></div><br/><div class="children"><div class="content">&gt; DOOM ran great on an Intel 486<p>It always blew my mind how well it worked on a 33 Mhz 486.  I&#x27;m fairly sure it ran at 30 fps in 320x200.  That gives it just over 17 clock cycles per pixel, and that doesn&#x27;t even include time for game logic.<p>My memory could be wrong, though, but even if it required a 66 Mhz to reach 30 fps, that&#x27;s still only 34 clocks per pixel on an architecture that required multiple clocks for a simple integer add instruction.</div><br/></div></div><div id="41380018" class="c"><input type="checkbox" id="c-41380018" checked=""/><div class="controls bullet"><span class="by">whiteboardr</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379975">parent</a><span>|</span><a href="#41381181">prev</a><span>|</span><a href="#41379251">next</a><span>|</span><label class="collapse" for="c-41380018">[-]</label><label class="expand" for="c-41380018">[1 more]</label></div><br/><div class="children"><div class="content">An experience that isn’t asset- but rule-based.</div><br/></div></div></div></div><div id="41379251" class="c"><input type="checkbox" id="c-41379251" checked=""/><div class="controls bullet"><span class="by">beepbooptheory</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378844">parent</a><span>|</span><a href="#41379975">prev</a><span>|</span><a href="#41378617">next</a><span>|</span><label class="collapse" for="c-41379251">[-]</label><label class="expand" for="c-41379251">[4 more]</label></div><br/><div class="children"><div class="content">What would the model provide if not what we see on the screen?</div><br/><div id="41379695" class="c"><input type="checkbox" id="c-41379695" checked=""/><div class="controls bullet"><span class="by">whiteboardr</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379251">parent</a><span>|</span><a href="#41378617">next</a><span>|</span><label class="collapse" for="c-41379695">[-]</label><label class="expand" for="c-41379695">[3 more]</label></div><br/><div class="children"><div class="content">The environment and everything in it.<p>“Everything” would mean all objects and the elements they’re made of, their rules on how they interact and decay.<p>A modularized ecosystem i guess, comprised of “sub-systems” of sorts.<p>The other model, that provides all interaction (cause for effect) could either be run artificially or be used interactively by a human - opening up the possibility for being a tree : )<p>This all would need an interfacing agent that in principle would be an engine simulating the second law of thermodynamics and at the same time recording every state that has changed and diverged off the driving actor’s vector in time.<p>Basically the “effects” model keeping track of everyones history.<p>In the end a system with an “everything” model (that can grow overtime), a “cause” model messing with it, brought together and documented by the “effect” model.<p>(Again … non technical person, just fantasizing)  : )</div><br/><div id="41385167" class="c"><input type="checkbox" id="c-41385167" checked=""/><div class="controls bullet"><span class="by">HappMacDonald</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379695">parent</a><span>|</span><a href="#41383790">next</a><span>|</span><label class="collapse" for="c-41385167">[-]</label><label class="expand" for="c-41385167">[1 more]</label></div><br/><div class="children"><div class="content">So you&#x27;re basically just talking about upgrading &quot;enemy AI&quot; to a more complex form of AI :)</div><br/></div></div><div id="41383790" class="c"><input type="checkbox" id="c-41383790" checked=""/><div class="controls bullet"><span class="by">mplewis</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379695">parent</a><span>|</span><a href="#41385167">prev</a><span>|</span><a href="#41378617">next</a><span>|</span><label class="collapse" for="c-41383790">[-]</label><label class="expand" for="c-41383790">[1 more]</label></div><br/><div class="children"><div class="content">What you’re asking for doesn’t make sense.</div><br/></div></div></div></div></div></div></div></div><div id="41378617" class="c"><input type="checkbox" id="c-41378617" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378523">parent</a><span>|</span><a href="#41378844">prev</a><span>|</span><a href="#41379104">next</a><span>|</span><label class="collapse" for="c-41378617">[-]</label><label class="expand" for="c-41378617">[2 more]</label></div><br/><div class="children"><div class="content">That is kind of cool though, I would play like being lost in a dream.<p>If on the backend you could record the level layouts in memory you could have exploration teams that try to find new areas to explore.</div><br/><div id="41378766" class="c"><input type="checkbox" id="c-41378766" checked=""/><div class="controls bullet"><span class="by">debo_</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378617">parent</a><span>|</span><a href="#41379104">next</a><span>|</span><label class="collapse" for="c-41378766">[-]</label><label class="expand" for="c-41378766">[1 more]</label></div><br/><div class="children"><div class="content">It would be cool for dream sequences in games to feel more like dreams. This is probably an expensive way to do it, but it would be neat!</div><br/></div></div></div></div><div id="41379104" class="c"><input type="checkbox" id="c-41379104" checked=""/><div class="controls bullet"><span class="by">hoosieree</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378523">parent</a><span>|</span><a href="#41378617">prev</a><span>|</span><a href="#41378730">next</a><span>|</span><label class="collapse" for="c-41379104">[-]</label><label class="expand" for="c-41379104">[1 more]</label></div><br/><div class="children"><div class="content">Small objects like powerups appear and disappear as the player moves (even without backtracking), the ammo count is constantly varying, getting shot doesn&#x27;t deplete health or armor, etc.</div><br/></div></div><div id="41378730" class="c"><input type="checkbox" id="c-41378730" checked=""/><div class="controls bullet"><span class="by">codeflo</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378523">parent</a><span>|</span><a href="#41379104">prev</a><span>|</span><a href="#41379237">next</a><span>|</span><label class="collapse" for="c-41378730">[-]</label><label class="expand" for="c-41378730">[2 more]</label></div><br/><div class="children"><div class="content">Even purely going forward, specks on wall textures morph into opponents and so on. All the diffusion-generated videos I’ve seen so far have this kind of unsettling feature.</div><br/><div id="41379134" class="c"><input type="checkbox" id="c-41379134" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378730">parent</a><span>|</span><a href="#41379237">next</a><span>|</span><label class="collapse" for="c-41379134">[-]</label><label class="expand" for="c-41379134">[1 more]</label></div><br/><div class="children"><div class="content">It it like some kind of weird dream doom.</div><br/></div></div></div></div><div id="41379237" class="c"><input type="checkbox" id="c-41379237" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378523">parent</a><span>|</span><a href="#41378730">prev</a><span>|</span><a href="#41378693">next</a><span>|</span><label class="collapse" for="c-41379237">[-]</label><label class="expand" for="c-41379237">[1 more]</label></div><br/><div class="children"><div class="content">So for the next iteration, they should add a minimap overlay (perhaps on a side channel) -  it should help the model give more consistent output in any given location. Right now, the game is very much like a lucid dream - the universe makes sense from moment to moment, but without outside reference, everything that falls out of short-term memory (few frames here) gets reimagined.</div><br/></div></div><div id="41378693" class="c"><input type="checkbox" id="c-41378693" checked=""/><div class="controls bullet"><span class="by">Groxx</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378523">parent</a><span>|</span><a href="#41379237">prev</a><span>|</span><a href="#41379575">next</a><span>|</span><label class="collapse" for="c-41378693">[-]</label><label class="expand" for="c-41378693">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s an example right at the beginning too - the ammo drop on the right changes to something green (I think that&#x27;s a body?)</div><br/></div></div><div id="41379575" class="c"><input type="checkbox" id="c-41379575" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378523">parent</a><span>|</span><a href="#41378693">prev</a><span>|</span><a href="#41381948">next</a><span>|</span><label class="collapse" for="c-41379575">[-]</label><label class="expand" for="c-41379575">[9 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t see this as something that would be hard to overcome. Sora for instance has already shown the ability for a diffusion model to maintain object permanence. Flux recently too has shown the ability to render the same person in many different poses or images.</div><br/><div id="41379642" class="c"><input type="checkbox" id="c-41379642" checked=""/><div class="controls bullet"><span class="by">idunnoman1222</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379575">parent</a><span>|</span><a href="#41379664">next</a><span>|</span><label class="collapse" for="c-41379642">[-]</label><label class="expand" for="c-41379642">[1 more]</label></div><br/><div class="children"><div class="content">Where does a sora video turn around backwards? I don’t even maintain such consistency in my dreams.</div><br/></div></div><div id="41379664" class="c"><input type="checkbox" id="c-41379664" checked=""/><div class="controls bullet"><span class="by">idunnoman1222</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379575">parent</a><span>|</span><a href="#41379642">prev</a><span>|</span><a href="#41381948">next</a><span>|</span><label class="collapse" for="c-41379664">[-]</label><label class="expand" for="c-41379664">[7 more]</label></div><br/><div class="children"><div class="content">Where does a sora video turn around backwards? I can’t maintain such consistency in my own dreams.</div><br/><div id="41380256" class="c"><input type="checkbox" id="c-41380256" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379664">parent</a><span>|</span><a href="#41381948">next</a><span>|</span><label class="collapse" for="c-41380256">[-]</label><label class="expand" for="c-41380256">[6 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know of an example (not to say it doesn&#x27;t exist) but the problem is fundamentally the same as things moving out of sight&#x2F;out of frame and coming back again.</div><br/><div id="41381356" class="c"><input type="checkbox" id="c-41381356" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41380256">parent</a><span>|</span><a href="#41381948">next</a><span>|</span><label class="collapse" for="c-41381356">[-]</label><label class="expand" for="c-41381356">[5 more]</label></div><br/><div class="children"><div class="content">&gt; the problem is fundamentally the same as things moving out of sight&#x2F;out of frame and coming back again<p>Maybe it is, but doing that with the entire scene instead of just a small part of it makes the problem massively harder, as the model needs to grow exponentially to remember more things. It isn&#x27;t something that we will manage anytime soon, maybe 10-20 years with current architecture and same compute progress.<p>Then you make that even harder by remembering a whole game level? No, ain&#x27;t gonna happen in our lifetimes without massive changes to the architecture. They would need to make a different model keep track of level state etc, not just an image to image model.</div><br/><div id="41384612" class="c"><input type="checkbox" id="c-41384612" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41381356">parent</a><span>|</span><a href="#41381948">next</a><span>|</span><label class="collapse" for="c-41384612">[-]</label><label class="expand" for="c-41384612">[4 more]</label></div><br/><div class="children"><div class="content">10 to 20 years sounds wildly pessimistic<p>In this sora video the dragon covers half the scene, and its basically identical when it is revealed again ~5 seconds later, or about 150 frames later. The is lots of evidence (and some studies) that these models are in fact building internal world models.<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=LXJ-yLiktDU" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=LXJ-yLiktDU</a><p>Buckle in, the train is moving way faster. I don&#x27;t think there would be much surprise if this is solved in the next few generations of video generators. The first generation is already doing very well.</div><br/><div id="41385003" class="c"><input type="checkbox" id="c-41385003" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41384612">parent</a><span>|</span><a href="#41381948">next</a><span>|</span><label class="collapse" for="c-41385003">[-]</label><label class="expand" for="c-41385003">[3 more]</label></div><br/><div class="children"><div class="content">Did you watch the video, it is completely different after the dragon goes past? Its still a flag there, but everything else changed. Even the stores in the background changed, the mass of people is completely different with no hint of anyone moving there etc.<p>You always get this from AI enthusiast, they come and post &quot;proof&quot; that disproves their own point.</div><br/><div id="41385268" class="c"><input type="checkbox" id="c-41385268" checked=""/><div class="controls bullet"><span class="by">HappMacDonald</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41385003">parent</a><span>|</span><a href="#41381948">next</a><span>|</span><label class="collapse" for="c-41385268">[-]</label><label class="expand" for="c-41385268">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not GP, but running over that video I&#x27;m actually having a hard time finding any detail present before the dragon obscures them not either exit frame right when the camera pans left slightly near the end or not re-appear with reasonably crisp detail after the dragon gets out of the way.<p>Most of the mob of people are indistinct, but there is a woman in a lime green coat who is visible, and then obstructed by the dragon twice (beard and ribbon) and reappears fine. Unfortunately when dragon fully moves past she has been lost to frame right.<p>There is another person in black holding a red satchel which is visible both before and after the dragon has passed.<p>Nothing about the storefronts appear to change. The complex sign full of Chinese text (which might be gibberish text: it&#x27;s highly stylized and I don&#x27;t know Chinese) appears to survive the dragon passing without even any changes to the individual ideograms.<p>There is also a red box shaped like a Chinese paper lantern with a single gold ideogram on it at the store entrance which spends most of the video obscured by the dragon and is still in the same location after it passes (though video artifacting makes it more challenging to verify that that ideogram is unchanged it certainly does not appear substantially different)<p>What detail are you seeing that is different before and after the obstruction?</div><br/><div id="41386445" class="c"><input type="checkbox" id="c-41386445" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41385268">parent</a><span>|</span><a href="#41381948">next</a><span>|</span><label class="collapse" for="c-41386445">[-]</label><label class="expand" for="c-41386445">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What detail are you seeing that is different before and after the obstruction?<p>First frame, guy in blue hat next to a flag. That flag and the guy is then gone afterwards.<p>The two flags near the wall are gone, there is something triangular there but there was two flags before the dragon went past.<p>Then not to mention that the crowd is 6 people deep after the dragon went past, while just 4 people deep before, it is way more crowded.<p>Instead of the flag that was there before the dragon, it put in 2 more flags afterwards far more to the left.<p>Third second a guy was out of frame for a few frames, and suddenly gained a blue scarf. AFter dragon went by he turned into a woman. Next to that person was a guy with a blue cap, he completely disappears.<p>&gt; Most of the mob of people are indistinct<p>No they aren&#x27;t, they are mostly distinct and basically all of them changes. If you ignore that the entire mob totally changes both in number and appearance and where it is, sure it is pretty good, except it forgot the flags, but how can you ignore the mob when we talk about the model remembering details? The wall is much less information dense than the mob, so that is much easier to remember for the model, the difficulty is in the mob.<p>&gt; but there is a woman in a lime green coat who is visible,<p>She was just out of frame for a fraction of a second, not the big bit where the dragon moves past. The guy in blue jacket and blue cap behind her disappears though, or merges with another person and becomes a woman with a muffler after the dragon moved past.<p>So, in the end some big strokes were kept, and that was a very tiny part of the image that was both there before and after the dragon moved past so it was far from a whole image with full details. Almost all details are wrong.<p>Maybe he meant that the house looked mostly the same, I agree the upper parts does, but I looked at the windows and they were completely different, it is full of people heads after the dragon moved past while before it was just clean walls.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="41381948" class="c"><input type="checkbox" id="c-41381948" checked=""/><div class="controls bullet"><span class="by">nielsbot</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378523">parent</a><span>|</span><a href="#41379575">prev</a><span>|</span><a href="#41378967">next</a><span>|</span><label class="collapse" for="c-41381948">[-]</label><label class="expand" for="c-41381948">[1 more]</label></div><br/><div class="children"><div class="content">You can also notice in the first part of the video the ammo numbers fluctuate a bit randomly.</div><br/></div></div><div id="41378967" class="c"><input type="checkbox" id="c-41378967" checked=""/><div class="controls bullet"><span class="by">alickz</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378523">parent</a><span>|</span><a href="#41381948">prev</a><span>|</span><a href="#41376240">next</a><span>|</span><label class="collapse" for="c-41378967">[-]</label><label class="expand" for="c-41378967">[3 more]</label></div><br/><div class="children"><div class="content">is that something that can be solved with more memory&#x2F;attention&#x2F;context?<p>or do we believe it&#x27;s an inherent limitation in the approach?</div><br/><div id="41379058" class="c"><input type="checkbox" id="c-41379058" checked=""/><div class="controls bullet"><span class="by">noiv</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378967">parent</a><span>|</span><a href="#41376240">next</a><span>|</span><label class="collapse" for="c-41379058">[-]</label><label class="expand" for="c-41379058">[2 more]</label></div><br/><div class="children"><div class="content">I think the real question is does the player get shot from behind?</div><br/><div id="41379421" class="c"><input type="checkbox" id="c-41379421" checked=""/><div class="controls bullet"><span class="by">alickz</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379058">parent</a><span>|</span><a href="#41376240">next</a><span>|</span><label class="collapse" for="c-41379421">[-]</label><label class="expand" for="c-41379421">[1 more]</label></div><br/><div class="children"><div class="content">great question<p>tangentially related but Grand Theft Auto speedrunners often point the camera behind them while driving so cars don&#x27;t spawn &quot;behind&quot; them (aka in front of the car)</div><br/></div></div></div></div></div></div></div></div><div id="41376240" class="c"><input type="checkbox" id="c-41376240" checked=""/><div class="controls bullet"><span class="by">refibrillator</span><span>|</span><a href="#41375657">parent</a><span>|</span><a href="#41378523">prev</a><span>|</span><a href="#41377529">next</a><span>|</span><label class="collapse" for="c-41376240">[-]</label><label class="expand" for="c-41376240">[14 more]</label></div><br/><div class="children"><div class="content">Just want to clarify a couple possible misconceptions:<p>The diffusion model doesn’t maintain any state itself, though its weights may encode some notion of cause&#x2F;effect. It just renders one frame at a time (after all it’s a text to image model, not text to video). Instead of text, the previous states and frames are provided as inputs to the model to predict the next frame.<p>Noise is added to the previous frames before being passed into the SD model, so the RL agents were not involved with “correcting” it.<p>De-noising objectives are widespread in ML, intuitively it forces a predictive model to leverage context, ie surrounding frames&#x2F;words&#x2F;etc.<p>In this case it helps prevent auto-regressive drift due to the accumulation of small errors from the randomness inherent in generative diffusion models. Figure 4 shows such drift happening when a player is standing still.</div><br/><div id="41378525" class="c"><input type="checkbox" id="c-41378525" checked=""/><div class="controls bullet"><span class="by">rvnx</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41376240">parent</a><span>|</span><a href="#41377529">next</a><span>|</span><label class="collapse" for="c-41378525">[-]</label><label class="expand" for="c-41378525">[13 more]</label></div><br/><div class="children"><div class="content">The concept is that if you train a Diffusion model by feeding all the possible frames seen in the game.<p>The training was over almost 1 billion frames, 20 days of full-time play-time, taking a screenshot of every single inch of the map.<p>Now you show him N frames as input, and ask it &quot;give me frame N+1&quot;, then it gives you the frame n. N+1 back based on how it was originally seen during training.<p>But it is not frame N+1 from a mysterious intelligence, it&#x27;s simply frame N+1 given back from past database.<p>The drift you mentioned is actually a clear (but sad) proof that the model does not work at inventing new frames, and can only spit out an answer from the past dataset.<p>It&#x27;s a bit like if you train stable diffusion on Simpsons episodes, and that it outputs the next frame of an existing episode that was in the training set, but few frames later goes wild and buggy.</div><br/><div id="41378624" class="c"><input type="checkbox" id="c-41378624" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378525">parent</a><span>|</span><a href="#41378605">next</a><span>|</span><label class="collapse" for="c-41378624">[-]</label><label class="expand" for="c-41378624">[2 more]</label></div><br/><div class="children"><div class="content">Research is the acquisition of knowledge that may or may not have practical applications.<p>They succeeded in the research, gained knowledge, and might be able to do something awesome with it.<p>It’s a success even if they don’t sell anything.</div><br/></div></div><div id="41378605" class="c"><input type="checkbox" id="c-41378605" checked=""/><div class="controls bullet"><span class="by">jetrink</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378525">parent</a><span>|</span><a href="#41378624">prev</a><span>|</span><a href="#41377529">next</a><span>|</span><label class="collapse" for="c-41378605">[-]</label><label class="expand" for="c-41378605">[10 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think you&#x27;ve understood the project completely. The model accepts player input, so frame 601 could be quite different if the player decided to turn left rather than right, or chose that moment to fire at an exploding barrel.</div><br/><div id="41378658" class="c"><input type="checkbox" id="c-41378658" checked=""/><div class="controls bullet"><span class="by">rvnx</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378605">parent</a><span>|</span><a href="#41377529">next</a><span>|</span><label class="collapse" for="c-41378658">[-]</label><label class="expand" for="c-41378658">[9 more]</label></div><br/><div class="children"><div class="content">1 billion frames in memory... With such dataset, you have seen practically all realistic possibilities in the short-term.<p>If it would be able to invent action and maps and let the user play &quot;infinite doom&quot;, then it would be very different (and impressive!).</div><br/><div id="41379524" class="c"><input type="checkbox" id="c-41379524" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378658">parent</a><span>|</span><a href="#41378857">next</a><span>|</span><label class="collapse" for="c-41379524">[-]</label><label class="expand" for="c-41379524">[3 more]</label></div><br/><div class="children"><div class="content">Like many people in case of LLMs, you&#x27;re just demonstrating unawareness of - or disbelief in - the fact that the model doesn&#x27;t record training data vetbatim, but smears it out in high-dimensional space, from which it then samples. The model then doesn&#x27;t recall past inputs (which are effectively under extreme lossy compression), but samples from that high-dimensional space to produce output. The high-dimensional representation by necessity captures semantic understanding of the training data.<p>Generating &quot;infinite Doom&quot; is exactly what this model is doing, as it does not capture the larger map layout well enough to stay consistent with it.</div><br/><div id="41379689" class="c"><input type="checkbox" id="c-41379689" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379524">parent</a><span>|</span><a href="#41379981">next</a><span>|</span><label class="collapse" for="c-41379689">[-]</label><label class="expand" for="c-41379689">[1 more]</label></div><br/><div class="children"><div class="content">Whether or not a judge understands this will probably form the basis of any precedent set about the legality of image models and copyright.</div><br/></div></div><div id="41379981" class="c"><input type="checkbox" id="c-41379981" checked=""/><div class="controls bullet"><span class="by">znx_0</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379524">parent</a><span>|</span><a href="#41379689">prev</a><span>|</span><a href="#41378857">next</a><span>|</span><label class="collapse" for="c-41379981">[-]</label><label class="expand" for="c-41379981">[1 more]</label></div><br/><div class="children"><div class="content">I like &quot;conditioned brute force&quot; better term.</div><br/></div></div></div></div><div id="41378857" class="c"><input type="checkbox" id="c-41378857" checked=""/><div class="controls bullet"><span class="by">OskarS</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378658">parent</a><span>|</span><a href="#41379524">prev</a><span>|</span><a href="#41377529">next</a><span>|</span><label class="collapse" for="c-41378857">[-]</label><label class="expand" for="c-41378857">[5 more]</label></div><br/><div class="children"><div class="content">&gt; 1 billion frames in memory... With such dataset, you have seen practically all realistic possibilities in the short-term.<p>I mean... no? Not even close? Multiply the number of game states with the number of inputs at any given frame gives you a number vastly bigger than 1 billion, not even comparable. Even with 20 days of play time to train no, it&#x27;s entirely likely that at no point did someone stop at a certain location and look to the left from that angle. They might have done from similar angles, but the model then has to reconstruct some sense of the geometry of the level to synthesize the frame. They might also not have arrived there from the same direction, which again the model needs some smarts to understand.<p>I get your point, it&#x27;s very overtrained on these particular levels of Doom, which means you might as well just play Doom. But this is not a hash table lookup we&#x27;re talking about, it&#x27;s pretty impressive work.</div><br/><div id="41379045" class="c"><input type="checkbox" id="c-41379045" checked=""/><div class="controls bullet"><span class="by">rvnx</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41378857">parent</a><span>|</span><a href="#41377529">next</a><span>|</span><label class="collapse" for="c-41379045">[-]</label><label class="expand" for="c-41379045">[4 more]</label></div><br/><div class="children"><div class="content">This was the basis for the reasoning:<p>The map 1 has 2&#x27;518 walkable map units.
There are 65536 angles.<p>2&#x27;518*65&#x27;536=165&#x27;019&#x27;648<p>If you capture 165M frames, you already cover all the possibilities in terms of camera &#x2F; player view, but probably the diffusion models don&#x27;t even need to have all the frames (the same way that LLMs don&#x27;t).</div><br/><div id="41379377" class="c"><input type="checkbox" id="c-41379377" checked=""/><div class="controls bullet"><span class="by">commodoreboxer</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379045">parent</a><span>|</span><a href="#41379166">next</a><span>|</span><label class="collapse" for="c-41379377">[-]</label><label class="expand" for="c-41379377">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s also enemy motion, enemy attacks, shooting, and UI considerations, which make the combinatorials explode.<p>And Doom movement isn&#x27;t tile based. The map may be, but you can be in many many places on a tile.</div><br/></div></div><div id="41379166" class="c"><input type="checkbox" id="c-41379166" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379045">parent</a><span>|</span><a href="#41379377">prev</a><span>|</span><a href="#41379113">next</a><span>|</span><label class="collapse" for="c-41379166">[-]</label><label class="expand" for="c-41379166">[1 more]</label></div><br/><div class="children"><div class="content">Do you have to be exactly on a tile in Doom? I thought the guy walked smoothly around the map.</div><br/></div></div><div id="41379113" class="c"><input type="checkbox" id="c-41379113" checked=""/><div class="controls bullet"><span class="by">znx_0</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41379045">parent</a><span>|</span><a href="#41379166">prev</a><span>|</span><a href="#41377529">next</a><span>|</span><label class="collapse" for="c-41379113">[-]</label><label class="expand" for="c-41379113">[1 more]</label></div><br/><div class="children"><div class="content">I think enemy and effects are probably in there</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="41377529" class="c"><input type="checkbox" id="c-41377529" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#41375657">parent</a><span>|</span><a href="#41376240">prev</a><span>|</span><a href="#41379654">next</a><span>|</span><label class="collapse" for="c-41377529">[-]</label><label class="expand" for="c-41377529">[9 more]</label></div><br/><div class="children"><div class="content">But it&#x27;s not a game. It&#x27;s a memory of a game video, predicting the next frame based on the few previous frames, like &quot;I can imagine what happened next&quot;.<p>I would call it the world&#x27;s least efficient video compression.<p>What I would like to see is the actual <i>predictive</i> strength, aka imagination, which I did not notice mentioned in the abstract. The model is trained on a set of classic maps. What would it do, given a few frames of gameplay on an unfamiliar map as input? How well could it imagine what happens next?</div><br/><div id="41377910" class="c"><input type="checkbox" id="c-41377910" checked=""/><div class="controls bullet"><span class="by">PoignardAzur</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41377529">parent</a><span>|</span><a href="#41378842">next</a><span>|</span><label class="collapse" for="c-41377910">[-]</label><label class="expand" for="c-41377910">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>But it&#x27;s not a game. It&#x27;s a memory of a game video, predicting the next frame based on the few previous frames, like &quot;I can imagine what happened next&quot;.</i><p>It&#x27;s not super clear from the landing page, but I <i>think</i> it&#x27;s an engine? Like, its input is both previous images <i>and</i> input for the next frame.<p>So as a player, if you press &quot;shoot&quot;, the diffusion engine need to output an image where the monster in front of you takes damage&#x2F;dies.</div><br/><div id="41378565" class="c"><input type="checkbox" id="c-41378565" checked=""/><div class="controls bullet"><span class="by">bergen</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41377910">parent</a><span>|</span><a href="#41378842">next</a><span>|</span><label class="collapse" for="c-41378565">[-]</label><label class="expand" for="c-41378565">[1 more]</label></div><br/><div class="children"><div class="content">How is what you think they say not clear?<p>We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction with a complex environment over long trajectories at high quality.</div><br/></div></div></div></div><div id="41378842" class="c"><input type="checkbox" id="c-41378842" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41377529">parent</a><span>|</span><a href="#41377910">prev</a><span>|</span><a href="#41379747">next</a><span>|</span><label class="collapse" for="c-41378842">[-]</label><label class="expand" for="c-41378842">[1 more]</label></div><br/><div class="children"><div class="content">No, it’s predicting the next frame conditioned on past frames <i>AND player actions!</i> This is clear from the article. Mere video generation would be nothing new.</div><br/></div></div><div id="41379747" class="c"><input type="checkbox" id="c-41379747" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41377529">parent</a><span>|</span><a href="#41378842">prev</a><span>|</span><a href="#41378645">next</a><span>|</span><label class="collapse" for="c-41379747">[-]</label><label class="expand" for="c-41379747">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a memory of a video looped to controls, so frame 1 is &quot;I wonder how would it look if the player pressed D instead of W&quot;, then the frame 2 is based on frame 1, etc. and couple frames in, it&#x27;s already not remembering, but <i>imagining</i> the gameplay on the fly. It&#x27;s not prerecorded, it responds to inputs during generation. That&#x27;s what makes it a game engine.</div><br/></div></div><div id="41378645" class="c"><input type="checkbox" id="c-41378645" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41377529">parent</a><span>|</span><a href="#41379747">prev</a><span>|</span><a href="#41378361">next</a><span>|</span><label class="collapse" for="c-41378645">[-]</label><label class="expand" for="c-41378645">[1 more]</label></div><br/><div class="children"><div class="content">They could down convert the entire model to only utilize the subset of matrix components from stable diffusion. This approach may be able to improve internet bandwidth efficiency assuming consumers in the future have powerful enough computers.</div><br/></div></div><div id="41378361" class="c"><input type="checkbox" id="c-41378361" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41377529">parent</a><span>|</span><a href="#41378645">prev</a><span>|</span><a href="#41377583">next</a><span>|</span><label class="collapse" for="c-41378361">[-]</label><label class="expand" for="c-41378361">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more like the Tetris Effect, where the model has seen so much Doom that it confabulates gameplay.</div><br/></div></div><div id="41377583" class="c"><input type="checkbox" id="c-41377583" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41377529">parent</a><span>|</span><a href="#41378361">prev</a><span>|</span><a href="#41379654">next</a><span>|</span><label class="collapse" for="c-41377583">[-]</label><label class="expand" for="c-41377583">[2 more]</label></div><br/><div class="children"><div class="content">If it&#x27;s trained on absolute player coordinates then it would likely just morph into the known map at those coordinates.</div><br/><div id="41377625" class="c"><input type="checkbox" id="c-41377625" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41377583">parent</a><span>|</span><a href="#41379654">next</a><span>|</span><label class="collapse" for="c-41377625">[-]</label><label class="expand" for="c-41377625">[1 more]</label></div><br/><div class="children"><div class="content">But it&#x27;s trained on the actual screen pixel data, AFAICT. It&#x27;s literally a visual imagination model, not gameplay &#x2F; geometry imagination model. They had to make special provisions to the pixel data on the HUD which by its nature different than the pictures of a 3D world.</div><br/></div></div></div></div></div></div><div id="41379654" class="c"><input type="checkbox" id="c-41379654" checked=""/><div class="controls bullet"><span class="by">raghavbali</span><span>|</span><a href="#41375657">parent</a><span>|</span><a href="#41377529">prev</a><span>|</span><a href="#41380539">next</a><span>|</span><label class="collapse" for="c-41379654">[-]</label><label class="expand" for="c-41379654">[1 more]</label></div><br/><div class="children"><div class="content">Nicely summarised. Another important thing that clearly standsout (not to undermine the efforts and work gone into this) is the fact that more and more we are now seeing larger and more complex building blocks emerging (first it was embedding models then encoder decoder layers and now whole models are being duck-taped for even powerful pipelines). AI&#x2F;DL ecosystem is growing on a nice trajectory.<p>Though I wonder if 10 years down the line folks wouldn&#x27;t even care about underlying model details (no more than a current day web-developer needs to know about network packets).<p>PS: Not great examples, but I hope you get the idea ;)</div><br/></div></div><div id="41380539" class="c"><input type="checkbox" id="c-41380539" checked=""/><div class="controls bullet"><span class="by">pradn</span><span>|</span><a href="#41375657">parent</a><span>|</span><a href="#41379654">prev</a><span>|</span><a href="#41378041">next</a><span>|</span><label class="collapse" for="c-41380539">[-]</label><label class="expand" for="c-41380539">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Google here uses SD 1.4, as the core of the diffusion model, which is a nice reminder that open models are useful to even giant cloud monopolies.<p>A mistake people make all the time is that massive companies will put all their resources toward every project. This paper was written by four co-authors. They probably got a good amount of resources, but they still had to share in the pool allocated to their research department.<p>Even Google only has one Gemini (in a few versions).</div><br/></div></div><div id="41386219" class="c"><input type="checkbox" id="c-41386219" checked=""/><div class="controls bullet"><span class="by">bubaumba</span><span>|</span><a href="#41375657">parent</a><span>|</span><a href="#41378041">prev</a><span>|</span><a href="#41380908">next</a><span>|</span><label class="collapse" for="c-41386219">[-]</label><label class="expand" for="c-41386219">[2 more]</label></div><br/><div class="children"><div class="content">&gt; nice reminder that open models are useful to<p>You didn&#x27;t say open _what_  models. Was that intentional?</div><br/><div id="41386665" class="c"><input type="checkbox" id="c-41386665" checked=""/><div class="controls bullet"><span class="by">Philpax</span><span>|</span><a href="#41375657">root</a><span>|</span><a href="#41386219">parent</a><span>|</span><a href="#41380908">next</a><span>|</span><label class="collapse" for="c-41386665">[-]</label><label class="expand" for="c-41386665">[1 more]</label></div><br/><div class="children"><div class="content">They did, SD 1.4</div><br/></div></div></div></div></div></div><div id="41380908" class="c"><input type="checkbox" id="c-41380908" checked=""/><div class="controls bullet"><span class="by">SeanAnderson</span><span>|</span><a href="#41375657">prev</a><span>|</span><a href="#41388703">next</a><span>|</span><label class="collapse" for="c-41380908">[-]</label><label class="expand" for="c-41380908">[22 more]</label></div><br/><div class="children"><div class="content">After some discussion in this thread, I found it worth pointing out that this paper is NOT describing a system which receives real-time user input and adjusts its output accordingly, but, to me, the way the abstract is worded heavily implied this was occurring.<p>It&#x27;s trained on a large set of data in which agents played DOOM and video samples are given to users for evaluation, but users are not feeding inputs into the simulation in real-time in such a way as to be &quot;playing DOOM&quot; at ~20FPS.<p>There are some key phrases within the paper that hint at this such as &quot;Key questions remain, such as ... how games would be effectively created in the first place, including how to best leverage human inputs&quot; and &quot;Our end goal is to have human players interact with our simulation.&quot;, but mostly it&#x27;s just the omission of a section describing real-time user gameplay.</div><br/><div id="41384433" class="c"><input type="checkbox" id="c-41384433" checked=""/><div class="controls bullet"><span class="by">ollin</span><span>|</span><a href="#41380908">parent</a><span>|</span><a href="#41383197">next</a><span>|</span><label class="collapse" for="c-41384433">[-]</label><label class="expand" for="c-41384433">[1 more]</label></div><br/><div class="children"><div class="content">We can&#x27;t assess the quality of gameplay ourselves of course (since the model wasn&#x27;t released), but one author said &quot;It&#x27;s playable, the videos on our project page are actual  game play.&quot; (<a href="https:&#x2F;&#x2F;x.com&#x2F;shlomifruchter&#x2F;status&#x2F;1828850796840268009" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;shlomifruchter&#x2F;status&#x2F;1828850796840268009</a>) and the video on top of <a href="https:&#x2F;&#x2F;gamengen.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;gamengen.github.io&#x2F;</a> starts out with &quot;these are real-time recordings of people playing the game&quot;. Based on those claims, it seems likely that they did get a playable system in front of humans by the end of the project (though perhaps not by the time the draft was uploaded to arXiv).</div><br/></div></div><div id="41383197" class="c"><input type="checkbox" id="c-41383197" checked=""/><div class="controls bullet"><span class="by">dewarrn1</span><span>|</span><a href="#41380908">parent</a><span>|</span><a href="#41384433">prev</a><span>|</span><a href="#41381509">next</a><span>|</span><label class="collapse" for="c-41383197">[-]</label><label class="expand" for="c-41383197">[1 more]</label></div><br/><div class="children"><div class="content">The paper should definitely be more clear on this point, but there&#x27;s a sentence in section 5.2.3 that makes me think that this was playable and played: &quot;When playing with the model manually, we observe that some areas are very easy for both, some areas are very hard for both, and in some the agent performs much better.&quot;  It may be a failure of imagination, but I can&#x27;t think of another reasonable way of interpreting &quot;playing with the model manually&quot;.</div><br/></div></div><div id="41381509" class="c"><input type="checkbox" id="c-41381509" checked=""/><div class="controls bullet"><span class="by">Chance-Device</span><span>|</span><a href="#41380908">parent</a><span>|</span><a href="#41383197">prev</a><span>|</span><a href="#41382425">next</a><span>|</span><label class="collapse" for="c-41381509">[-]</label><label class="expand" for="c-41381509">[7 more]</label></div><br/><div class="children"><div class="content">I also thought this, but refer back to the paper, not the abstract:<p>&gt; A is the set of key presses and mouse movements…<p>&gt; …to condition on actions, we simply learn an embedding A_emb for each action<p>So, it’s clear that in this model the diffusion process is conditioned by embedding A that is derived from user actions rather than words.<p>Then a noised start frame is encoded into latents and concatenated on to the noise latents as a second conditioning.<p>So we have a diffusion model which is trained solely on images of doom, and which is conditioned on current doom frames and user actions to produce subsequent frames.<p>So yes, the users are playing it.<p>However, it should be unsurprising that this is possible. This is effectively just a neural recording of the game. But it’s a cool tech demo.</div><br/><div id="41384119" class="c"><input type="checkbox" id="c-41384119" checked=""/><div class="controls bullet"><span class="by">psb217</span><span>|</span><a href="#41380908">root</a><span>|</span><a href="#41381509">parent</a><span>|</span><a href="#41382611">next</a><span>|</span><label class="collapse" for="c-41384119">[-]</label><label class="expand" for="c-41384119">[1 more]</label></div><br/><div class="children"><div class="content">The agent never interacts with the simulator during training or evaluation. There is no user, there is only an agent which trained to play the real game and which produced the sequences of game frames and actions that were used to train the simulator and to provide ground truth sequences of game experience for evaluation. Their evaluation metrics are all based on running short simulations in the diffusion model which are initiated with some number of conditioning frames taken from the real game engine. Statements in the paper like: &quot;GameNGen shows that an architecture and model weights exist such that a neural model can effectively run a complex game (DOOM) interactively on existing hardware.&quot; are wildly misleading.</div><br/></div></div><div id="41382611" class="c"><input type="checkbox" id="c-41382611" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#41380908">root</a><span>|</span><a href="#41381509">parent</a><span>|</span><a href="#41384119">prev</a><span>|</span><a href="#41382425">next</a><span>|</span><label class="collapse" for="c-41382611">[-]</label><label class="expand" for="c-41382611">[5 more]</label></div><br/><div class="children"><div class="content">I wonder if they could somehow feed in a trained Gaussian splats model to this to get better images?<p>Since the splats are specifically designed for rendering it seems like it would be an efficient way for the image model to learn the geometry without having to encode it on the image model itself.</div><br/><div id="41383696" class="c"><input type="checkbox" id="c-41383696" checked=""/><div class="controls bullet"><span class="by">Chance-Device</span><span>|</span><a href="#41380908">root</a><span>|</span><a href="#41382611">parent</a><span>|</span><a href="#41382425">next</a><span>|</span><label class="collapse" for="c-41383696">[-]</label><label class="expand" for="c-41383696">[4 more]</label></div><br/><div class="children"><div class="content">I’m not sure how that would help vs just training the model with the conditionings described in the paper.<p>I’m not very familiar with Gaussian  splats models, but aren’t they just a way of constructing images using multiple superimposed parameterized Gaussian distributions, sort of like the Fourier series does with waveforms using sine and cosine waves?<p>I’m not seeing how that would apply here but I’d be interested in hearing how you would do it.</div><br/><div id="41385705" class="c"><input type="checkbox" id="c-41385705" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#41380908">root</a><span>|</span><a href="#41383696">parent</a><span>|</span><a href="#41382425">next</a><span>|</span><label class="collapse" for="c-41385705">[-]</label><label class="expand" for="c-41385705">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not certain where it would fit in, but my thinking is this.<p>There&#x27;s been a bunch of work on making splats efficient and good at representing geometry. Reading more, perhaps NERFs would be a better fit, since they&#x27;re an actual neutral network.<p>My thinking is that if you trained a NERF ahead of time to represent the geometry and layout of the levels, and plug that in to the diffusion model (as a part of computing the latents, and then also on the other side so it can be used to improve the rendering) then the diffusion model could focus on learning how actions manipulate the world without having to learn the geometry representation.</div><br/><div id="41385915" class="c"><input type="checkbox" id="c-41385915" checked=""/><div class="controls bullet"><span class="by">Chance-Device</span><span>|</span><a href="#41380908">root</a><span>|</span><a href="#41385705">parent</a><span>|</span><a href="#41382425">next</a><span>|</span><label class="collapse" for="c-41385915">[-]</label><label class="expand" for="c-41385915">[2 more]</label></div><br/><div class="children"><div class="content">I don’t know if that would really help, I have a hard time imagining exactly what that model would be doing in practise.<p>To be honest none of the stuff in the paper is very practical, you almost certainly do not want a diffusion model trying to be an entire game under any circumstances.<p>What you might want to do is use a diffusion model to transform a low poly, low fidelity game world into something photorealistic. So the geometry, player movement and physics etc would all make sense, and then the model paints over it something that looks like reality based on some primitive texture cues in the low fidelity render.<p>I’d bet money that something like that will happen and it is the future of games and video.</div><br/><div id="41386187" class="c"><input type="checkbox" id="c-41386187" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#41380908">root</a><span>|</span><a href="#41385915">parent</a><span>|</span><a href="#41382425">next</a><span>|</span><label class="collapse" for="c-41386187">[-]</label><label class="expand" for="c-41386187">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I realize this will never be useful for much in practice (although maybe as some kind of client side prediction for cloud gaming? But likely if you could run this in real time you might as well run whatever game there is in real time as well, unless there&#x27;s some kind of massive world running on the server that&#x27;s too large to stream the geometry for effectively), I was mostly just trying to think of a way to avoid the issues with fake looking frames or forgetting what the level looks like when you turn around that someone mentioned.<p>Not exactly that, but Nvidia does something like this already, they call it DLSS. It uses previous frames and motion vector to render a next frame using machine learning.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41382425" class="c"><input type="checkbox" id="c-41382425" checked=""/><div class="controls bullet"><span class="by">7734128</span><span>|</span><a href="#41380908">parent</a><span>|</span><a href="#41381509">prev</a><span>|</span><a href="#41381190">next</a><span>|</span><label class="collapse" for="c-41382425">[-]</label><label class="expand" for="c-41382425">[1 more]</label></div><br/><div class="children"><div class="content">What you&#x27;re describing reminded me of this cool project:<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=udPY5rQVoW0" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=udPY5rQVoW0</a>
&quot;Playing a Neural Network&#x27;s version of GTA V: GAN Theft Auto&quot;</div><br/></div></div><div id="41381190" class="c"><input type="checkbox" id="c-41381190" checked=""/><div class="controls bullet"><span class="by">refibrillator</span><span>|</span><a href="#41380908">parent</a><span>|</span><a href="#41382425">prev</a><span>|</span><a href="#41381562">next</a><span>|</span><label class="collapse" for="c-41381190">[-]</label><label class="expand" for="c-41381190">[4 more]</label></div><br/><div class="children"><div class="content">You are incorrect, this is an <i>interactive</i> simulation that is playable by humans.<p>&gt; Figure 1: a human player is playing DOOM on GameNGen at 20 FPS.<p>The abstract is ambiguously worded which has caused a lot of confusion here, but the paper is unmistakably clear about this point.<p>Kind of disappointing to see this misinformation upvoted so highly on a forum full of tech experts.</div><br/><div id="41384170" class="c"><input type="checkbox" id="c-41384170" checked=""/><div class="controls bullet"><span class="by">psb217</span><span>|</span><a href="#41380908">root</a><span>|</span><a href="#41381190">parent</a><span>|</span><a href="#41381336">next</a><span>|</span><label class="collapse" for="c-41384170">[-]</label><label class="expand" for="c-41384170">[1 more]</label></div><br/><div class="children"><div class="content">If the generative model&#x2F;simulator can run at 20FPS, then obviously in principle a human could play the game in simulation at 20 FPS. However, they do no evaluation of human play in the paper. My guess is that they limited human evals to watching short clips of play in the real engine vs the simulator (which conditions on some number of initial frames from the engine when starting each clip...) since the actual &quot;playability&quot; is not great.</div><br/></div></div><div id="41381336" class="c"><input type="checkbox" id="c-41381336" checked=""/><div class="controls bullet"><span class="by">FrustratedMonky</span><span>|</span><a href="#41380908">root</a><span>|</span><a href="#41381190">parent</a><span>|</span><a href="#41384170">prev</a><span>|</span><a href="#41381562">next</a><span>|</span><label class="collapse" for="c-41381336">[-]</label><label class="expand" for="c-41381336">[2 more]</label></div><br/><div class="children"><div class="content">Yeah. If isn&#x27;t doing this, then what could it be doing that is worth a paper? &quot;real-time user input and adjusts its output accordingly&quot;</div><br/><div id="41381426" class="c"><input type="checkbox" id="c-41381426" checked=""/><div class="controls bullet"><span class="by">rvnx</span><span>|</span><a href="#41380908">root</a><span>|</span><a href="#41381336">parent</a><span>|</span><a href="#41381562">next</a><span>|</span><label class="collapse" for="c-41381426">[-]</label><label class="expand" for="c-41381426">[1 more]</label></div><br/><div class="children"><div class="content">There is a hint in the paper itself:<p>It says in a shy way that it is based on:
&quot;Ha &amp; Schmidhuber (2018) who train a Variational Auto-Encoder (Kingma &amp; Welling, 2014) to encode game frames into a
latent vector&quot;<p>So it means they most likely took <a href="https:&#x2F;&#x2F;worldmodels.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;worldmodels.github.io&#x2F;</a> (that is actually open-source) or something similar and swapped the frame generation by Stable Diffusion that was released in 2022.</div><br/></div></div></div></div></div></div><div id="41381562" class="c"><input type="checkbox" id="c-41381562" checked=""/><div class="controls bullet"><span class="by">teamonkey</span><span>|</span><a href="#41380908">parent</a><span>|</span><a href="#41381190">prev</a><span>|</span><a href="#41381754">next</a><span>|</span><label class="collapse" for="c-41381562">[-]</label><label class="expand" for="c-41381562">[1 more]</label></div><br/><div class="children"><div class="content">I think <i>someone</i> is playing it, but it has a reduced set of inputs and they&#x27;re playing it in a very specific way (slowly, avoiding looking back to places they&#x27;ve been) so as not to show off the flaws in the system.<p>The people surveyed in this study are not playing the game, they are watching extremely short video clips of the game being played and comparing them to equally short videos of the original Doom being played, to see if they can spot the difference.<p>I may be wrong with how it works, but I think this is just hallucinating in real time. It has no internal state per se, it knows what was on screen in the previous few frames and it knows what inputs the user is pressing, and so it generates the next frame. Like with video compression, it probably doesn&#x27;t need to generate a full frame every time, just &quot;differences&quot;.<p>As with all the previous AI game research, these are not games in any real sense. They fall apart when played beyond any meaningful length of time (seconds). Crucially, they are not playable by anyone other than the developers in very controlled settings. A defining attribute of any game is that it can be played.</div><br/></div></div><div id="41381754" class="c"><input type="checkbox" id="c-41381754" checked=""/><div class="controls bullet"><span class="by">lewhoo</span><span>|</span><a href="#41380908">parent</a><span>|</span><a href="#41381562">prev</a><span>|</span><a href="#41386325">next</a><span>|</span><label class="collapse" for="c-41381754">[-]</label><label class="expand" for="c-41381754">[1 more]</label></div><br/><div class="children"><div class="content">The movement of the player seems jittery a bit so I inferred something similar on that basis.</div><br/></div></div><div id="41386325" class="c"><input type="checkbox" id="c-41386325" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41380908">parent</a><span>|</span><a href="#41381754">prev</a><span>|</span><a href="#41381141">next</a><span>|</span><label class="collapse" for="c-41386325">[-]</label><label class="expand" for="c-41386325">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I found it worth pointing out that this paper is NOT describing a system which receives real-time user input and adjusts its output accordingly<p>Well you&#x27;re wrong as specified in the first video and by the authors themselves, maybe next time check better instead of writing comments with such authoritative tone of things you don&#x27;t actually know.</div><br/></div></div><div id="41381141" class="c"><input type="checkbox" id="c-41381141" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#41380908">parent</a><span>|</span><a href="#41386325">prev</a><span>|</span><a href="#41382706">next</a><span>|</span><label class="collapse" for="c-41381141">[-]</label><label class="expand" for="c-41381141">[1 more]</label></div><br/><div class="children"><div class="content">Were the agents playing at 20 real FPS, or did this occur like a Pixar movie offline?</div><br/></div></div><div id="41382706" class="c"><input type="checkbox" id="c-41382706" checked=""/><div class="controls bullet"><span class="by">SeanAnderson</span><span>|</span><a href="#41380908">parent</a><span>|</span><a href="#41381141">prev</a><span>|</span><a href="#41381413">next</a><span>|</span><label class="collapse" for="c-41382706">[-]</label><label class="expand" for="c-41382706">[2 more]</label></div><br/><div class="children"><div class="content">Ehhh okay, I&#x27;m not as convinced as I was earlier. Sorry for misleading. There&#x27;s been a lot of back-and-forth.<p>I would&#x27;ve really liked to see a section of the paper explicitly call out that they used humans in real time. There&#x27;s a lot of sentences that led me to believe otherwise. It&#x27;s clear that they used a bunch of agents to simulate gameplay where those agents submitted user inputs to affect the gameplay and they captured those inputs in their model. This made it a bit murky as to whether humans ever actually got involved.<p>This statement, &quot;Our end goal is to have human players interact with our simulation. To that end, the policy π as in Section 2 is that of human gameplay. Since we cannot sample from that directly at scale, we start by approximating it via teaching an automatic agent to play&quot;<p>led me to believe that while they had an ultimate goal of user input (why wouldn&#x27;t they) they sufficed by approximating human input.<p>I was looking to refute that assumption later in the paper by hopefully reading some words on the human gameplay experience, but instead, under Results, I found:<p>&quot;Human Evaluation. As another measurement of simulation quality, we provided 10 human raters with 130 random short clips (of lengths 1.6 seconds and 3.2 seconds) of our simulation side by side with the real game. The raters were tasked with recognizing the real game (see Figure 14 in Appendix A.6). The raters only choose the actual game over the simulation in 58% or 60% of the
time (for the 1.6 seconds and 3.2 seconds clips, respectively).&quot;<p>and it&#x27;s like.. okay.. if you have a section in results on human evaluation, and your goal is to have humans play, then why are you talking just about humans reviewing video rather than giving some sort of feedback on the human gameplay experience - even if it&#x27;s not especially positive?<p>Still, in the Discussion section, it mentions, &quot;The second important limitation are the remaining differences between the agent’s behavior and those of human players. For example, our agent, even at the end of training, still does not explore all of the game’s locations and interactions, leading to erroneous behavior in those cases.&quot; which makes it more clear that humans gave input which went outside the bounds of the automatic agents. It doesn&#x27;t seem like this would occur if it were agents simulating more input.<p>Ultimately, I think that the paper itself could&#x27;ve been more clear in this regard, but clearly the publishing website tries to be very explicit by saying upfront - &quot;Real-time recordings of people playing the game DOOM&quot; and it&#x27;s pretty hard to argue against that.<p>Anyway. I repent! It was a learning experience going back and forth on my belief here. Very cool tech overall.</div><br/><div id="41384216" class="c"><input type="checkbox" id="c-41384216" checked=""/><div class="controls bullet"><span class="by">psb217</span><span>|</span><a href="#41380908">root</a><span>|</span><a href="#41382706">parent</a><span>|</span><a href="#41381413">next</a><span>|</span><label class="collapse" for="c-41384216">[-]</label><label class="expand" for="c-41384216">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s funny how academic writing works. Authors rarely produce many unclear or ambiguous statements where the most likely interpretation undersells their work...</div><br/></div></div></div></div><div id="41381413" class="c"><input type="checkbox" id="c-41381413" checked=""/><div class="controls bullet"><span class="by">pajeets</span><span>|</span><a href="#41380908">parent</a><span>|</span><a href="#41382706">prev</a><span>|</span><a href="#41388703">next</a><span>|</span><label class="collapse" for="c-41381413">[-]</label><label class="expand" for="c-41381413">[1 more]</label></div><br/><div class="children"><div class="content">I knew it was too good be true but seems like real time video generation can be good enough to get to a point where it feels like a truly interactive video&#x2F;game<p>Imagine if text2game was possible. there would be some sort of network generating each frame from an image generated by text, with some underlying 3d physics simulation to keep all the multiplayer screens sync&#x27;d<p>this paper does not seem to be of that possibility rather some cleverly words to make you think people were playing a real time video. we can&#x27;t even generate more than 5~10 second of video without it hallucinating. something this persistent would require an extreme amount of gameplay video training. it can be done but the video shown by this paper is not true to its words.</div><br/></div></div></div></div><div id="41388703" class="c"><input type="checkbox" id="c-41388703" checked=""/><div class="controls bullet"><span class="by">rldjbpin</span><span>|</span><a href="#41380908">prev</a><span>|</span><a href="#41375719">next</a><span>|</span><label class="collapse" for="c-41388703">[-]</label><label class="expand" for="c-41388703">[1 more]</label></div><br/><div class="children"><div class="content">this is truly a cool demo, but a very misleading title.<p>to me it seems like a very bruteforce or greedy way to give the impression to a user that they are &quot;playing&quot; a game. the difference being that you already own the game to make this possible, but cannot let the user use that copy!<p>using generative AI for game creation is at a nascent stage but there are much more elegant ways to go about the end goal. perhaps in the future with computing so far ahead that we moved beyond the current architecture, this might be worth doing instead of emulation perhaps.</div><br/></div></div><div id="41375719" class="c"><input type="checkbox" id="c-41375719" checked=""/><div class="controls bullet"><span class="by">wkcheng</span><span>|</span><a href="#41388703">prev</a><span>|</span><a href="#41375667">next</a><span>|</span><label class="collapse" for="c-41375719">[-]</label><label class="expand" for="c-41375719">[42 more]</label></div><br/><div class="children"><div class="content">It&#x27;s insane that that this works, and that it works fast enough to render at 20 fps.  It seems like they almost made a cross between a diffusion model and an RNN, since they had to encode the previous frames and actions and feed it into the model at each step.<p>Abstractly, it&#x27;s like the model is dreaming of a game that it played a lot of, and real time inputs just change the state of the dream.  It makes me wonder if humans are just next moment prediction machines, with just a little bit more memory built in.</div><br/><div id="41376075" class="c"><input type="checkbox" id="c-41376075" checked=""/><div class="controls bullet"><span class="by">lokimedes</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41378224">next</a><span>|</span><label class="collapse" for="c-41376075">[-]</label><label class="expand" for="c-41376075">[9 more]</label></div><br/><div class="children"><div class="content">It makes good sense for humans to have this ability. If we flip the argument, and see the next frame as a hypothesis for what is expected as the outcome of the current frame, then comparing this &quot;hypothesis&quot; with what is sensed makes it easier to process the differences, rather than the totality of the sensory input.<p>As Richard Dawkins recently put it in a podcast[1], our genes are great prediction machines, as their continued survival rests on it. Being able to generate a visual prediction fits perfectly with the amount of resources we dedicate to sight.<p>If that is the case, what does aphantasia tell us?<p>[1] <a href="https:&#x2F;&#x2F;podcasts.apple.com&#x2F;dk&#x2F;podcast&#x2F;into-the-impossible-with-brian-keating&#x2F;id1169885840?i=1000665776221" rel="nofollow">https:&#x2F;&#x2F;podcasts.apple.com&#x2F;dk&#x2F;podcast&#x2F;into-the-impossible-wi...</a></div><br/><div id="41377407" class="c"><input type="checkbox" id="c-41377407" checked=""/><div class="controls bullet"><span class="by">dbspin</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376075">parent</a><span>|</span><a href="#41377318">next</a><span>|</span><label class="collapse" for="c-41377407">[-]</label><label class="expand" for="c-41377407">[5 more]</label></div><br/><div class="children"><div class="content">Worth noting that aphantasia doesn&#x27;t necessarily extend to dreams. Anecdotally - I have pretty severe aphantasia (I can conjure milisecond glimpses of barely tangible imagery that I can&#x27;t quite perceive before it&#x27;s gone - but only since learning that visualisation wasn&#x27;t a linguistic metaphor). I can&#x27;t really simulate object rotation. I can&#x27;t really &#x27;picture&#x27; how things will look before they&#x27;re drawn &#x2F; built etc. However I often have highly vivid dream imagery. I also have excellent recognition of faces and places (e.g.: can&#x27;t get lost in a new city). So there clearly is a lot of preconscious visualisation and image matching going on in some aphantasia cases, even where the explicit visual screen is all but absent.</div><br/><div id="41378107" class="c"><input type="checkbox" id="c-41378107" checked=""/><div class="controls bullet"><span class="by">lokimedes</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41377407">parent</a><span>|</span><a href="#41377880">next</a><span>|</span><label class="collapse" for="c-41378107">[-]</label><label class="expand" for="c-41378107">[3 more]</label></div><br/><div class="children"><div class="content">I fabulate about this in another comment below:<p>&gt; Many people with aphantasia reports being able to visualize in their dreams, meaning that they don&#x27;t lack the ability to generate visuals. So it may be that the [aphantasia] brain has an affinity to rely on the abstract representation when &quot;thinking&quot;, while dreaming still uses the &quot;stable diffusion mode&quot;.<p>(I obviously don&#x27;t know what I&#x27;m talking about, just a fellow aphant)</div><br/><div id="41379195" class="c"><input type="checkbox" id="c-41379195" checked=""/><div class="controls bullet"><span class="by">dbspin</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41378107">parent</a><span>|</span><a href="#41379828">next</a><span>|</span><label class="collapse" for="c-41379195">[-]</label><label class="expand" for="c-41379195">[1 more]</label></div><br/><div class="children"><div class="content">Obviously we&#x27;re all introspecting here - but my guess is that there&#x27;s some kind of cross talk in aphantasic brains between the conscious narrating semantic brain and the visual module. Such that default mode visualisation is impaired. It&#x27;s specifically the loss of reflexive consciousness that allows visuals to emerge. Not sure if this is related, but I have pretty severe chronic insomnia, and I often wonder if this in part relates to the inability to drift off into imagery.</div><br/></div></div><div id="41379828" class="c"><input type="checkbox" id="c-41379828" checked=""/><div class="controls bullet"><span class="by">drowsspa</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41378107">parent</a><span>|</span><a href="#41379195">prev</a><span>|</span><a href="#41377880">next</a><span>|</span><label class="collapse" for="c-41379828">[-]</label><label class="expand" for="c-41379828">[1 more]</label></div><br/><div class="children"><div class="content">Yeah. In my head it&#x27;s like I&#x27;m manipulating SVG paths instead of raw pixels</div><br/></div></div></div></div><div id="41377880" class="c"><input type="checkbox" id="c-41377880" checked=""/><div class="controls bullet"><span class="by">zimpenfish</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41377407">parent</a><span>|</span><a href="#41378107">prev</a><span>|</span><a href="#41377318">next</a><span>|</span><label class="collapse" for="c-41377880">[-]</label><label class="expand" for="c-41377880">[1 more]</label></div><br/><div class="children"><div class="content">Pretty much the same for me.  My aphantasia is total (no images at all) but still ludicrously vivid dreams and not too bad at recognising people and places.</div><br/></div></div></div></div><div id="41377318" class="c"><input type="checkbox" id="c-41377318" checked=""/><div class="controls bullet"><span class="by">jonplackett</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376075">parent</a><span>|</span><a href="#41377407">prev</a><span>|</span><a href="#41376733">next</a><span>|</span><label class="collapse" for="c-41377318">[-]</label><label class="expand" for="c-41377318">[2 more]</label></div><br/><div class="children"><div class="content">What’s the aphantasia link? I’ve got aphantasia. I’m convinced though that the bit of my brain that should be making images is used for letting me ‘see’ how things are connected together very easily in my head. Also I still love games like Pictionary and can somehow draw things onto paper than I don’t really know what they look like in my head. It’s often a surprise when pen meets paper.</div><br/><div id="41377399" class="c"><input type="checkbox" id="c-41377399" checked=""/><div class="controls bullet"><span class="by">lokimedes</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41377318">parent</a><span>|</span><a href="#41376733">next</a><span>|</span><label class="collapse" for="c-41377399">[-]</label><label class="expand" for="c-41377399">[1 more]</label></div><br/><div class="children"><div class="content">I agree, it is my own experience as well. Craig Venter In one of his books also credit this way of representing knowledge as abstractions as his strength in inventing new concepts.<p>The link may be that we actually see differences between “frames”, rather than the frames directly. That in itself would imply that a from of sub-visual representation is being processed by our brain. For aphantasia, it could be that we work directly on this representation instead of recalling imagery through the visual system.<p>Many people with aphantasia reports being able to visualize in their dreams, meaning that they don&#x27;t lack the ability to generate visuals. So it may be that the brain has an affinity to rely on the abstract representation when &quot;thinking&quot;, while dreaming still uses the &quot;stable diffusion mode&quot;.<p>I’m no where near qualified to speak of this with certainty, but it seems plausible to me.</div><br/></div></div></div></div><div id="41376733" class="c"><input type="checkbox" id="c-41376733" checked=""/><div class="controls bullet"><span class="by">quickestpoint</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376075">parent</a><span>|</span><a href="#41377318">prev</a><span>|</span><a href="#41378224">next</a><span>|</span><label class="collapse" for="c-41376733">[-]</label><label class="expand" for="c-41376733">[1 more]</label></div><br/><div class="children"><div class="content">As Richard Dawkins theorized, would be more accurate and less LLM like :)</div><br/></div></div></div></div><div id="41378224" class="c"><input type="checkbox" id="c-41378224" checked=""/><div class="controls bullet"><span class="by">nsbk</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41376075">prev</a><span>|</span><a href="#41384895">next</a><span>|</span><label class="collapse" for="c-41378224">[-]</label><label class="expand" for="c-41378224">[2 more]</label></div><br/><div class="children"><div class="content">We are. At least that&#x27;s what Lisa Feldman Barrett [1] thinks. It is worth listening to this Lex Fridman podcast: Counterintuitive Ideas About How the Brain Works [2], where she explains among other ideas how constant prediction is the most efficient way of running a brain as opposed to reaction. I never get tired of listening to her, she&#x27;s such a great science communicator.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lisa_Feldman_Barrett" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lisa_Feldman_Barrett</a><p>[2] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=NbdRIVCBqNI&amp;t=1443s" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=NbdRIVCBqNI&amp;t=1443s</a></div><br/><div id="41380128" class="c"><input type="checkbox" id="c-41380128" checked=""/><div class="controls bullet"><span class="by">PunchTornado</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41378224">parent</a><span>|</span><a href="#41384895">next</a><span>|</span><label class="collapse" for="c-41380128">[-]</label><label class="expand" for="c-41380128">[1 more]</label></div><br/><div class="children"><div class="content">Interesting talk about the brain, but the stuff she says about free will is not a very good argument. Basically it is sort of the argument that the ancient greeks made which brings the discussion into a point where you can take both directions.</div><br/></div></div></div></div><div id="41384895" class="c"><input type="checkbox" id="c-41384895" checked=""/><div class="controls bullet"><span class="by">bangaladore</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41378224">prev</a><span>|</span><a href="#41380493">next</a><span>|</span><label class="collapse" for="c-41384895">[-]</label><label class="expand" for="c-41384895">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s insane that that this works, and that it works fast enough to render at 20 fps.<p>It is running on an entire v5 TPU (<a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;ai-machine-learning&#x2F;introducing-cloud-tpu-v5p-and-ai-hypercomputer" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;products&#x2F;ai-machine-learning&#x2F;i...</a>)<p>It&#x27;s unclear how that compares to a high-end consumer GPU like a 3090, but they seem to have similar INT8 TFLOPS. The TPU has less memory (16 vs. 24), and I&#x27;m unsure of the other specs.<p>Something doesn&#x27;t add up, in my opinion, though. SD usually takes (at minimum) seconds to produce a high-quality result on a 3090, so I can&#x27;t comprehend how they are like 2 orders of magnitudes faster—indicating that the TPU vastly outperforms a GPU for this task. They seem to be producing low-res (320x240) images, but it still seems too fast.</div><br/><div id="41386678" class="c"><input type="checkbox" id="c-41386678" checked=""/><div class="controls bullet"><span class="by">Philpax</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41384895">parent</a><span>|</span><a href="#41380493">next</a><span>|</span><label class="collapse" for="c-41386678">[-]</label><label class="expand" for="c-41386678">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s been a lot of work in optimising inference speed of SD - SD Turbo, latent consistency models, Hyper-SD, etc. It is very possible to hit these frame rates now.</div><br/></div></div></div></div><div id="41380493" class="c"><input type="checkbox" id="c-41380493" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41384895">prev</a><span>|</span><a href="#41379859">next</a><span>|</span><label class="collapse" for="c-41380493">[-]</label><label class="expand" for="c-41380493">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It makes me wonder if humans are just next moment prediction machines, with just a little bit more memory built in.<p>This, to me, seems extremely reductionist. 
Like you start with AI and work backwards until you frame all cognition as next something predictors.<p>It’s just the stochastic parrot argument again.</div><br/></div></div><div id="41379859" class="c"><input type="checkbox" id="c-41379859" checked=""/><div class="controls bullet"><span class="by">wrsh07</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41380493">prev</a><span>|</span><a href="#41376410">next</a><span>|</span><label class="collapse" for="c-41379859">[-]</label><label class="expand" for="c-41379859">[1 more]</label></div><br/><div class="children"><div class="content">Makes me wonder when an update to the world models paper comes out where they drop in diffusion models: <a href="https:&#x2F;&#x2F;worldmodels.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;worldmodels.github.io&#x2F;</a></div><br/></div></div><div id="41376410" class="c"><input type="checkbox" id="c-41376410" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41379859">prev</a><span>|</span><a href="#41378669">next</a><span>|</span><label class="collapse" for="c-41376410">[-]</label><label class="expand" for="c-41376410">[3 more]</label></div><br/><div class="children"><div class="content">&gt; It makes me wonder if humans are just next moment prediction machines, with just a little bit more memory built in.<p>Yup, see <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Predictive_coding" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Predictive_coding</a></div><br/><div id="41376704" class="c"><input type="checkbox" id="c-41376704" checked=""/><div class="controls bullet"><span class="by">quickestpoint</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376410">parent</a><span>|</span><a href="#41378669">next</a><span>|</span><label class="collapse" for="c-41376704">[-]</label><label class="expand" for="c-41376704">[2 more]</label></div><br/><div class="children"><div class="content">Umm, that’s a theory.</div><br/><div id="41378901" class="c"><input type="checkbox" id="c-41378901" checked=""/><div class="controls bullet"><span class="by">mind-blight</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376704">parent</a><span>|</span><a href="#41378669">next</a><span>|</span><label class="collapse" for="c-41378901">[-]</label><label class="expand" for="c-41378901">[1 more]</label></div><br/><div class="children"><div class="content">So are gravity and friction. I don&#x27;t know how well tested or accepted it is, but being just a theory doesn&#x27;t tell you much about how true it is without more info</div><br/></div></div></div></div></div></div><div id="41378669" class="c"><input type="checkbox" id="c-41378669" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41376410">prev</a><span>|</span><a href="#41376538">next</a><span>|</span><label class="collapse" for="c-41378669">[-]</label><label class="expand" for="c-41378669">[2 more]</label></div><br/><div class="children"><div class="content">Penrose (Nobel prize in physics) stipulates that quantum effects in the brain may allow a certain amount of time travel and back propagation to accomplish this.</div><br/><div id="41379846" class="c"><input type="checkbox" id="c-41379846" checked=""/><div class="controls bullet"><span class="by">wrsh07</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41378669">parent</a><span>|</span><a href="#41376538">next</a><span>|</span><label class="collapse" for="c-41379846">[-]</label><label class="expand" for="c-41379846">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need back propagation to learn<p>This is an incredibly complex hypothesis that doesn&#x27;t really seem justified by the evidence</div><br/></div></div></div></div><div id="41376538" class="c"><input type="checkbox" id="c-41376538" checked=""/><div class="controls bullet"><span class="by">richard___</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41378669">prev</a><span>|</span><a href="#41376072">next</a><span>|</span><label class="collapse" for="c-41376538">[-]</label><label class="expand" for="c-41376538">[1 more]</label></div><br/><div class="children"><div class="content">Did they take in the entire history as context?</div><br/></div></div><div id="41376072" class="c"><input type="checkbox" id="c-41376072" checked=""/><div class="controls bullet"><span class="by">Teever</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41376538">prev</a><span>|</span><a href="#41376153">next</a><span>|</span><label class="collapse" for="c-41376072">[-]</label><label class="expand" for="c-41376072">[1 more]</label></div><br/><div class="children"><div class="content">Also recursion and nested virtualization.  We can dream about dreaming and imagine different scenarios, some completely fictional or simply possible future scenarios all while doing day to day stuff.</div><br/></div></div><div id="41376153" class="c"><input type="checkbox" id="c-41376153" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41376072">prev</a><span>|</span><a href="#41375667">next</a><span>|</span><label class="collapse" for="c-41376153">[-]</label><label class="expand" for="c-41376153">[19 more]</label></div><br/><div class="children"><div class="content">Image is 2D. Video is 3D. The mathematical extension is obvious. In this case, low resolution 2D (pixels), and the third dimension is just frame rate (discrete steps). So rather simple.</div><br/><div id="41376177" class="c"><input type="checkbox" id="c-41376177" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376153">parent</a><span>|</span><a href="#41376997">next</a><span>|</span><label class="collapse" for="c-41376177">[-]</label><label class="expand" for="c-41376177">[16 more]</label></div><br/><div class="children"><div class="content">This is not &quot;just&quot; video, however. It&#x27;s interactive in real time. Sure, you can say that playing is simply video with some extra parameters thrown in to encode player input, but still.</div><br/><div id="41376270" class="c"><input type="checkbox" id="c-41376270" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376177">parent</a><span>|</span><a href="#41376997">next</a><span>|</span><label class="collapse" for="c-41376270">[-]</label><label class="expand" for="c-41376270">[15 more]</label></div><br/><div class="children"><div class="content">It is just video. There are no external interactions.<p>Heck, it is far simpler than video, because the point of view and frame is fixed.</div><br/><div id="41376391" class="c"><input type="checkbox" id="c-41376391" checked=""/><div class="controls bullet"><span class="by">SeanAnderson</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376270">parent</a><span>|</span><a href="#41376359">next</a><span>|</span><label class="collapse" for="c-41376391">[-]</label><label class="expand" for="c-41376391">[12 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re mistaken. The abstract says it&#x27;s interactive, &quot;We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction&quot;<p>Further - &quot;a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions.&quot; specifically &quot;and actions&quot;<p>User input is being fed into this system and subsequent frames take that into account. The user is &quot;actually&quot; firing a gun.</div><br/><div id="41380746" class="c"><input type="checkbox" id="c-41380746" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376391">parent</a><span>|</span><a href="#41378006">next</a><span>|</span><label class="collapse" for="c-41380746">[-]</label><label class="expand" for="c-41380746">[4 more]</label></div><br/><div class="children"><div class="content">No, I am not. The interaction is part of the training, and is used during inference, but it is not including during the process of generation.</div><br/><div id="41380850" class="c"><input type="checkbox" id="c-41380850" checked=""/><div class="controls bullet"><span class="by">SeanAnderson</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41380746">parent</a><span>|</span><a href="#41378006">next</a><span>|</span><label class="collapse" for="c-41380850">[-]</label><label class="expand" for="c-41380850">[3 more]</label></div><br/><div class="children"><div class="content">Okay, I think you&#x27;re right. My mistake. I read through the paper more closely and I found the abstract to be a bit misleading compared to the contents. Sorry.</div><br/><div id="41381579" class="c"><input type="checkbox" id="c-41381579" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41380850">parent</a><span>|</span><a href="#41378006">next</a><span>|</span><label class="collapse" for="c-41381579">[-]</label><label class="expand" for="c-41381579">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t worry. The paper is not very well written.</div><br/><div id="41384294" class="c"><input type="checkbox" id="c-41384294" checked=""/><div class="controls bullet"><span class="by">psb217</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41381579">parent</a><span>|</span><a href="#41378006">next</a><span>|</span><label class="collapse" for="c-41384294">[-]</label><label class="expand" for="c-41384294">[1 more]</label></div><br/><div class="children"><div class="content">Academic authors are consistently better at editing away unclear and ambiguous statements which make their work seem less impressive compared to ones which make their work seem more impressive. Maybe it&#x27;s just a coincidence, lol.</div><br/></div></div></div></div></div></div></div></div><div id="41378006" class="c"><input type="checkbox" id="c-41378006" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376391">parent</a><span>|</span><a href="#41380746">prev</a><span>|</span><a href="#41378611">next</a><span>|</span><label class="collapse" for="c-41378006">[-]</label><label class="expand" for="c-41378006">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s interactive but can it go beyond what it learned from the videos. As in, can the camera break free and roam around the map from different angles? I don&#x27;t think it will be able to do that at all. There are still a few hallucinations in this rendering, it doesn&#x27;t look it understands 3d.</div><br/><div id="41379048" class="c"><input type="checkbox" id="c-41379048" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41378006">parent</a><span>|</span><a href="#41378611">next</a><span>|</span><label class="collapse" for="c-41379048">[-]</label><label class="expand" for="c-41379048">[1 more]</label></div><br/><div class="children"><div class="content">You might be surprised. Generating views from novel angles based on a single image is not novel, and if anything, this model has more than a single frame as input. I’d wager that it’s quite able to extrapolate DOOM-like corridors and rooms even if it hasn’t seen the exact place during training. And sure, it’s imperfect but on the other hand <i>it works in real time</i> on a single TPU.</div><br/></div></div></div></div><div id="41378611" class="c"><input type="checkbox" id="c-41378611" checked=""/><div class="controls bullet"><span class="by">hypertele-Xii</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376391">parent</a><span>|</span><a href="#41378006">prev</a><span>|</span><a href="#41376679">next</a><span>|</span><label class="collapse" for="c-41378611">[-]</label><label class="expand" for="c-41378611">[3 more]</label></div><br/><div class="children"><div class="content">Then why do monsters become blurry smudgy messes when shot? That looks like a video compression artifact of a neural network attempting to replicate low-structure image (source material contains guts exploding, very un-structured visual).</div><br/><div id="41378801" class="c"><input type="checkbox" id="c-41378801" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41378611">parent</a><span>|</span><a href="#41376679">next</a><span>|</span><label class="collapse" for="c-41378801">[-]</label><label class="expand" for="c-41378801">[2 more]</label></div><br/><div class="children"><div class="content">Uh, maybe because monster death animations make up a small part of the training material (ie. gameplay) so the model has not learned to reproduce them very well?<p>There cannot be &quot;video compression artifacts&quot; because it hasn’t even seen any compressed video during training, as far as I can see.<p>Seriously, how is this even a discussion? The article is  clear that the novel thing is that this is real-time frame generation conditioned on the previous frame(s) <i>AND player actions.</i> Just generating video would be nothing new.</div><br/><div id="41384460" class="c"><input type="checkbox" id="c-41384460" checked=""/><div class="controls bullet"><span class="by">psb217</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41378801">parent</a><span>|</span><a href="#41376679">next</a><span>|</span><label class="collapse" for="c-41384460">[-]</label><label class="expand" for="c-41384460">[1 more]</label></div><br/><div class="children"><div class="content">In a sense, poorly reproducing rare content is a form of compression artifact. Ie, since this content occurs rarely in the training set, it will have less impact on the gradients and thus less impact on the final form of the model. Roughly speaking, the model is allocating fewer bits to this content, by storing less information about this content in its parameters, compared to content which it sees more often during training. I think this isn&#x27;t too different from certain aspects of images, videos, music, etc., being distorted in different ways based on how a particular codec allocates its available bits.</div><br/></div></div></div></div></div></div><div id="41376679" class="c"><input type="checkbox" id="c-41376679" checked=""/><div class="controls bullet"><span class="by">nopakos</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376391">parent</a><span>|</span><a href="#41378611">prev</a><span>|</span><a href="#41376359">next</a><span>|</span><label class="collapse" for="c-41376679">[-]</label><label class="expand" for="c-41376679">[2 more]</label></div><br/><div class="children"><div class="content">Maybe it&#x27;s so advanced, it knows the players&#x27; next moves, so it is a video!</div><br/><div id="41382977" class="c"><input type="checkbox" id="c-41382977" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376679">parent</a><span>|</span><a href="#41376359">next</a><span>|</span><label class="collapse" for="c-41382977">[-]</label><label class="expand" for="c-41382977">[1 more]</label></div><br/><div class="children"><div class="content">I guess you are being sarcastic, except this is precisely what it is doing. And it&#x27;s not hard: player movement is low information and probably not the hardest part of the model.</div><br/></div></div></div></div></div></div><div id="41376359" class="c"><input type="checkbox" id="c-41376359" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376270">parent</a><span>|</span><a href="#41376391">prev</a><span>|</span><a href="#41376997">next</a><span>|</span><label class="collapse" for="c-41376359">[-]</label><label class="expand" for="c-41376359">[2 more]</label></div><br/><div class="children"><div class="content">?<p>I highly suggest you to read the paper briefly before commenting on the topic. The whole point is that it&#x27;s not just generating a video.</div><br/><div id="41380758" class="c"><input type="checkbox" id="c-41380758" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376359">parent</a><span>|</span><a href="#41376997">next</a><span>|</span><label class="collapse" for="c-41380758">[-]</label><label class="expand" for="c-41380758">[1 more]</label></div><br/><div class="children"><div class="content">I did. It is generating a video, using latent information on player actions during the process (which it also predicts). It is not interactive.</div><br/></div></div></div></div></div></div></div></div><div id="41376997" class="c"><input type="checkbox" id="c-41376997" checked=""/><div class="controls bullet"><span class="by">InDubioProRubio</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376153">parent</a><span>|</span><a href="#41376177">prev</a><span>|</span><a href="#41375667">next</a><span>|</span><label class="collapse" for="c-41376997">[-]</label><label class="expand" for="c-41376997">[2 more]</label></div><br/><div class="children"><div class="content">Video is also higher resolution, as the pixels flip for the high resolution world by moving through it. Swivelling your head without glasses, even the blurry world contains more information in the curve of pixelchange.</div><br/><div id="41384572" class="c"><input type="checkbox" id="c-41384572" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376997">parent</a><span>|</span><a href="#41375667">next</a><span>|</span><label class="collapse" for="c-41384572">[-]</label><label class="expand" for="c-41384572">[1 more]</label></div><br/><div class="children"><div class="content">Correct, for the sprites. However, the walls in Doom are texture mapped, and so have the same issue as videos. Interesting, though, because I assume the antialiasing is something approximate, given the extreme demands on CPUs of the era.</div><br/></div></div></div></div></div></div></div></div><div id="41375667" class="c"><input type="checkbox" id="c-41375667" checked=""/><div class="controls bullet"><span class="by">zzanz</span><span>|</span><a href="#41375719">prev</a><span>|</span><a href="#41381454">next</a><span>|</span><label class="collapse" for="c-41375667">[-]</label><label class="expand" for="c-41375667">[21 more]</label></div><br/><div class="children"><div class="content">The quest to run doom on everything continues. 
Technically speaking, isn&#x27;t this the greatest possible anti-Doom, the Doom with the highest possible hardware requirement? 
I just find it funny that on a linear scale of hardware specification, Doom now finds itself on both ends.</div><br/><div id="41375717" class="c"><input type="checkbox" id="c-41375717" checked=""/><div class="controls bullet"><span class="by">fngjdflmdflg</span><span>|</span><a href="#41375667">parent</a><span>|</span><a href="#41388667">next</a><span>|</span><label class="collapse" for="c-41375717">[-]</label><label class="expand" for="c-41375717">[11 more]</label></div><br/><div class="children"><div class="content">&gt;Technically speaking, isn&#x27;t this the greatest possible anti-Doom<p>When I read this part I thought you were going to say because you&#x27;re technically <i>not</i> running Doom at all. That is, instead of running Doom without Doom&#x27;s original hardware&#x2F;software environment (by porting it), you&#x27;re running Doom without Doom itself.</div><br/><div id="41375977" class="c"><input type="checkbox" id="c-41375977" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375717">parent</a><span>|</span><a href="#41375899">next</a><span>|</span><label class="collapse" for="c-41375977">[-]</label><label class="expand" for="c-41375977">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s <i>dreaming</i> Doom.</div><br/><div id="41377696" class="c"><input type="checkbox" id="c-41377696" checked=""/><div class="controls bullet"><span class="by">birracerveza</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375977">parent</a><span>|</span><a href="#41385509">next</a><span>|</span><label class="collapse" for="c-41377696">[-]</label><label class="expand" for="c-41377696">[2 more]</label></div><br/><div class="children"><div class="content">We made machines dream of Doom. Insane.</div><br/><div id="41377843" class="c"><input type="checkbox" id="c-41377843" checked=""/><div class="controls bullet"><span class="by">daemin</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41377696">parent</a><span>|</span><a href="#41385509">next</a><span>|</span><label class="collapse" for="c-41377843">[-]</label><label class="expand" for="c-41377843">[1 more]</label></div><br/><div class="children"><div class="content">Time to make a sheep mod for Doom.</div><br/></div></div></div></div><div id="41385509" class="c"><input type="checkbox" id="c-41385509" checked=""/><div class="controls bullet"><span class="by">elwell</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375977">parent</a><span>|</span><a href="#41377696">prev</a><span>|</span><a href="#41382517">next</a><span>|</span><label class="collapse" for="c-41385509">[-]</label><label class="expand" for="c-41385509">[1 more]</label></div><br/><div class="children"><div class="content">Droom</div><br/></div></div><div id="41382517" class="c"><input type="checkbox" id="c-41382517" checked=""/><div class="controls bullet"><span class="by">qingcharles</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375977">parent</a><span>|</span><a href="#41385509">prev</a><span>|</span><a href="#41375899">next</a><span>|</span><label class="collapse" for="c-41382517">[-]</label><label class="expand" for="c-41382517">[1 more]</label></div><br/><div class="children"><div class="content"><i>Do Robots Dream of E1M1?</i></div><br/></div></div></div></div><div id="41375899" class="c"><input type="checkbox" id="c-41375899" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375717">parent</a><span>|</span><a href="#41375977">prev</a><span>|</span><a href="#41388667">next</a><span>|</span><label class="collapse" for="c-41375899">[-]</label><label class="expand" for="c-41375899">[5 more]</label></div><br/><div class="children"><div class="content">Pierre Menard, Author of Doom.</div><br/><div id="41376168" class="c"><input type="checkbox" id="c-41376168" checked=""/><div class="controls bullet"><span class="by">el_memorioso</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375899">parent</a><span>|</span><a href="#41378638">next</a><span>|</span><label class="collapse" for="c-41376168">[-]</label><label class="expand" for="c-41376168">[1 more]</label></div><br/><div class="children"><div class="content">I applaud your erudition.</div><br/></div></div><div id="41378638" class="c"><input type="checkbox" id="c-41378638" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375899">parent</a><span>|</span><a href="#41376168">prev</a><span>|</span><a href="#41379525">next</a><span>|</span><label class="collapse" for="c-41378638">[-]</label><label class="expand" for="c-41378638">[1 more]</label></div><br/><div class="children"><div class="content">OK, this is the single most perfect comment someone could make on this thread. Diffusion me impressed.</div><br/></div></div><div id="41379525" class="c"><input type="checkbox" id="c-41379525" checked=""/><div class="controls bullet"><span class="by">jl6</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375899">parent</a><span>|</span><a href="#41378638">prev</a><span>|</span><a href="#41376563">next</a><span>|</span><label class="collapse" for="c-41379525">[-]</label><label class="expand" for="c-41379525">[1 more]</label></div><br/><div class="children"><div class="content">Knee Deep in the Death of the Author.</div><br/></div></div><div id="41376563" class="c"><input type="checkbox" id="c-41376563" checked=""/><div class="controls bullet"><span class="by">1attice</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375899">parent</a><span>|</span><a href="#41379525">prev</a><span>|</span><a href="#41388667">next</a><span>|</span><label class="collapse" for="c-41376563">[-]</label><label class="expand" for="c-41376563">[1 more]</label></div><br/><div class="children"><div class="content">that took a moment, thank you</div><br/></div></div></div></div></div></div><div id="41388667" class="c"><input type="checkbox" id="c-41388667" checked=""/><div class="controls bullet"><span class="by">rldjbpin</span><span>|</span><a href="#41375667">parent</a><span>|</span><a href="#41375717">prev</a><span>|</span><a href="#41376587">next</a><span>|</span><label class="collapse" for="c-41388667">[-]</label><label class="expand" for="c-41388667">[1 more]</label></div><br/><div class="children"><div class="content">to me the closer analogy here is the &quot;running minecraft inside minecraft&quot; (<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32901461">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32901461</a>)</div><br/></div></div><div id="41376587" class="c"><input type="checkbox" id="c-41376587" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#41375667">parent</a><span>|</span><a href="#41388667">prev</a><span>|</span><a href="#41376099">next</a><span>|</span><label class="collapse" for="c-41376587">[-]</label><label class="expand" for="c-41376587">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the Doom with the highest possible hardware requirement?<p>Isn&#x27;t that possible by setting arbitrarily high goals for ray-cast rendering?</div><br/></div></div><div id="41376099" class="c"><input type="checkbox" id="c-41376099" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#41375667">parent</a><span>|</span><a href="#41376587">prev</a><span>|</span><a href="#41375915">next</a><span>|</span><label class="collapse" for="c-41376099">[-]</label><label class="expand" for="c-41376099">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the No-Doom.</div><br/><div id="41377084" class="c"><input type="checkbox" id="c-41377084" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41376099">parent</a><span>|</span><a href="#41375915">next</a><span>|</span><label class="collapse" for="c-41377084">[-]</label><label class="expand" for="c-41377084">[3 more]</label></div><br/><div class="children"><div class="content">Undoom?</div><br/><div id="41377220" class="c"><input type="checkbox" id="c-41377220" checked=""/><div class="controls bullet"><span class="by">riwsky</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41377084">parent</a><span>|</span><a href="#41377386">next</a><span>|</span><label class="collapse" for="c-41377220">[-]</label><label class="expand" for="c-41377220">[1 more]</label></div><br/><div class="children"><div class="content">It’s a mood.</div><br/></div></div><div id="41377386" class="c"><input type="checkbox" id="c-41377386" checked=""/><div class="controls bullet"><span class="by">jeffhuys</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41377084">parent</a><span>|</span><a href="#41377220">prev</a><span>|</span><a href="#41375915">next</a><span>|</span><label class="collapse" for="c-41377386">[-]</label><label class="expand" for="c-41377386">[1 more]</label></div><br/><div class="children"><div class="content">Bliss</div><br/></div></div></div></div></div></div><div id="41375915" class="c"><input type="checkbox" id="c-41375915" checked=""/><div class="controls bullet"><span class="by">x-complexity</span><span>|</span><a href="#41375667">parent</a><span>|</span><a href="#41376099">prev</a><span>|</span><a href="#41381454">next</a><span>|</span><label class="collapse" for="c-41375915">[-]</label><label class="expand" for="c-41375915">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Technically speaking, isn&#x27;t this the greatest possible anti-Doom, the Doom with the highest possible hardware requirement?<p>Not really? The greatest anti-Doom would be an infinite nest of these types of models predicting models predicting Doom at the very end of the chain.<p>The next step of anti-Doom would be a model generating the model, generating the Doom output.</div><br/><div id="41376328" class="c"><input type="checkbox" id="c-41376328" checked=""/><div class="controls bullet"><span class="by">nurettin</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375915">parent</a><span>|</span><a href="#41376399">next</a><span>|</span><label class="collapse" for="c-41376328">[-]</label><label class="expand" for="c-41376328">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this technically a model (training step) generating a model (a neural network) generating Doom output?</div><br/></div></div><div id="41376399" class="c"><input type="checkbox" id="c-41376399" checked=""/><div class="controls bullet"><span class="by">yuchi</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375915">parent</a><span>|</span><a href="#41376328">prev</a><span>|</span><a href="#41381454">next</a><span>|</span><label class="collapse" for="c-41376399">[-]</label><label class="expand" for="c-41376399">[1 more]</label></div><br/><div class="children"><div class="content">“…now it can <i>implement</i> Doom!”</div><br/></div></div></div></div></div></div><div id="41381454" class="c"><input type="checkbox" id="c-41381454" checked=""/><div class="controls bullet"><span class="by">Sohcahtoa82</span><span>|</span><a href="#41375667">prev</a><span>|</span><a href="#41377398">next</a><span>|</span><label class="collapse" for="c-41381454">[-]</label><label class="expand" for="c-41381454">[12 more]</label></div><br/><div class="children"><div class="content">It&#x27;s always fun reading the dead comments on a post like this.  People love to point how how pointless this is.<p>Some of ya&#x27;ll need to learn how to make things <i>for the fun of making things</i>.  Is this useful?  No, not really.  Is it interesting?  Absolutely.<p>Not everything has to be made for profit.  Not everything has to be made to make the world a better place.  Sometimes, people create things just for the learning experience, the challenge, or they&#x27;re curious to see if something is possible.<p>Time spent enjoying yourself is never time wasted.  Some of ya&#x27;ll are going to be on your death beds wishing you had allowed yourself to have more fun.</div><br/><div id="41382996" class="c"><input type="checkbox" id="c-41382996" checked=""/><div class="controls bullet"><span class="by">ploxiln</span><span>|</span><a href="#41381454">parent</a><span>|</span><a href="#41381493">next</a><span>|</span><label class="collapse" for="c-41382996">[-]</label><label class="expand" for="c-41382996">[2 more]</label></div><br/><div class="children"><div class="content">The skepticism and criticism in this thread is against the hype of AI, it&#x27;s implied by people saying &quot;this is so amazing&quot; that they think that in some near future you can create any video game experience you can imagine by just replacing all the software with some AI models, rendering the whole game.<p>When in reality this is the least efficient and reliable form of Doom yet created, using literally millions of times the computation used by the first x86 PCs that were able to render and play doom in real-time.<p>But it&#x27;s a funny party trick, sure.</div><br/><div id="41385910" class="c"><input type="checkbox" id="c-41385910" checked=""/><div class="controls bullet"><span class="by">joegibbs</span><span>|</span><a href="#41381454">root</a><span>|</span><a href="#41382996">parent</a><span>|</span><a href="#41381493">next</a><span>|</span><label class="collapse" for="c-41385910">[-]</label><label class="expand" for="c-41385910">[1 more]</label></div><br/><div class="children"><div class="content">Yes it&#x27;s less efficient and reliable than regular Doom, but it&#x27;s not just limited to Doom. You could have it simulate a very graphically advanced game that barely runs on current hardware and it would run at the exact same speed as Doom.</div><br/></div></div></div></div><div id="41381493" class="c"><input type="checkbox" id="c-41381493" checked=""/><div class="controls bullet"><span class="by">Gooblebrai</span><span>|</span><a href="#41381454">parent</a><span>|</span><a href="#41382996">prev</a><span>|</span><a href="#41381537">next</a><span>|</span><label class="collapse" for="c-41381493">[-]</label><label class="expand" for="c-41381493">[1 more]</label></div><br/><div class="children"><div class="content">So true. The hustle culture is an spreading disease that has replaced the fun maker culture from the 80s&#x2F;90s.<p>It&#x27;s unavoidable though. Cost of living being increasingly expensive and romantization of entrepreneurs like they are rock stars leads towards this hustle mindset.</div><br/></div></div><div id="41381537" class="c"><input type="checkbox" id="c-41381537" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#41381454">parent</a><span>|</span><a href="#41381493">prev</a><span>|</span><a href="#41381547">next</a><span>|</span><label class="collapse" for="c-41381537">[-]</label><label class="expand" for="c-41381537">[7 more]</label></div><br/><div class="children"><div class="content">I don’t think this is not useful. This is a stepping stone for generating entire novel games.</div><br/><div id="41381739" class="c"><input type="checkbox" id="c-41381739" checked=""/><div class="controls bullet"><span class="by">Sohcahtoa82</span><span>|</span><a href="#41381454">root</a><span>|</span><a href="#41381537">parent</a><span>|</span><a href="#41381547">next</a><span>|</span><label class="collapse" for="c-41381739">[-]</label><label class="expand" for="c-41381739">[6 more]</label></div><br/><div class="children"><div class="content">&gt; This is a stepping stone for generating entire novel games.<p>I don&#x27;t see how.<p>This game &quot;engine&quot; is purely mapping [pixels, input] -&gt; new pixels.  It has no notion of game state (so you can kill an enemy, turn your back, then turn around again, and the enemy could be alive again), not to mention that it requires the game to <i>already exist</i> in order to train it.<p>I suppose, in theory, you could train the network to include game state in the input and output, or potentially even handle game state outside the network entirely and just make it one of the inputs, but the output would be incredibly noisy and nigh unplayable.<p>And like I said, all of it requires the game to already exist in order to train the network.</div><br/><div id="41381880" class="c"><input type="checkbox" id="c-41381880" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#41381454">root</a><span>|</span><a href="#41381739">parent</a><span>|</span><a href="#41381794">next</a><span>|</span><label class="collapse" for="c-41381880">[-]</label><label class="expand" for="c-41381880">[3 more]</label></div><br/><div class="children"><div class="content">&gt;It has no notion of game state (so you can kill an enemy, turn your back, then turn around again)<p>Well you see a wall you turn around then turn back the wall is still there. With enough training data the model will be able to pick up the state of the enemy because it has ALREADY learned the state of the wall due to much more numerous data on the wall. It&#x27;s probably impractical to do this, but this is only a stepping stone like said.<p>&gt; not to mention that it requires the game to already exist in order to train it.<p>Is this a problem? Do games not exist? Not only due we have tons of games, but we also have in theory unlimited amounts of training data for each game.</div><br/><div id="41382149" class="c"><input type="checkbox" id="c-41382149" checked=""/><div class="controls bullet"><span class="by">Sohcahtoa82</span><span>|</span><a href="#41381454">root</a><span>|</span><a href="#41381880">parent</a><span>|</span><a href="#41381794">next</a><span>|</span><label class="collapse" for="c-41382149">[-]</label><label class="expand" for="c-41382149">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Well you see a wall you turn around then turn back the wall is still there. With enough training data the model will be able to pick up the state of the enemy because it has ALREADY learned the state of the wall due to much more numerous data on the wall.<p>It&#x27;s really important to understand that <i>ALL THE MODEL KNOWS</i> is a mapping of [pixels, input] -&gt; new pixels.  It has zero knowledge of game state.  The wall is still there after spinning 360 degrees simply because it knows that the image of a view facing away from the wall while holding the key to turn right eventually becomes an image of a view of the wall.<p>The only &quot;state&quot; that is known is the last few frames of the game screen.  Because of this, it&#x27;s simply not possible for the game model to know if an enemy should be shown as dead or alive once it has been off-screen for longer than those few frames.  It also means that if you keeping turning away and towards an enemy, it could teleport around.  Once it&#x27;s off the screen for those few frames, the model will have forgotten about it.<p>&gt; Is this a problem? Do games not exist?<p>If you&#x27;re trying to make a <i>new</i> game, then you need <i>new</i> frames to train the model on.</div><br/><div id="41383760" class="c"><input type="checkbox" id="c-41383760" checked=""/><div class="controls bullet"><span class="by">ninetyninenine</span><span>|</span><a href="#41381454">root</a><span>|</span><a href="#41382149">parent</a><span>|</span><a href="#41381794">next</a><span>|</span><label class="collapse" for="c-41383760">[-]</label><label class="expand" for="c-41383760">[1 more]</label></div><br/><div class="children"><div class="content">&gt;It&#x27;s really important to understand that ALL THE MODEL KNOWS is a mapping of [pixels, input] -&gt; new pixels. It has zero knowledge of game state.<p>This is false. What occurs in inside the model is unknown. It arranges pixel input and produces pixel output as if it actually understands game state. Like LLMs we don&#x27;t actually fully understand what&#x27;s going on internally. You can&#x27;t assume that models don&#x27;t &quot;understand&quot; things just because the high level training methodology only includes pixel input and output.<p>&gt;The only &quot;state&quot; that is known is the last few frames of the game screen. Because of this, it&#x27;s simply not possible for the game model to know if an enemy should be shown as dead or alive once it has been off-screen for longer than those few frames. It also means that if you keeping turning away and towards an enemy, it could teleport around. Once it&#x27;s off the screen for those few frames, the model will have forgotten about it.<p>This is true. But then one could say it knows game state for up to a few frames.  That&#x27;s different from saying the model ONLY knows pixel input and pixel output. Very different.<p>There are other tricks for long term memory storage as well. Think Radar. Radar will capture the state of the enemy beyond just visual frames so the model won&#x27;t forget an enemy was behind them.<p>Game state can also be encoded into some frame pixels at the bottom lines. The Model can pick up on these associations.<p>edit: someone mentioned that the game state lasts past a few frames.<p>&gt;If you&#x27;re trying to make a new game, then you need new frames to train the model on.<p>Right so for a generative model you would instead of training the model on one game you would train it on multitudes of games. The model would then based off of a seed number output a new type of game.<p>Alternatively you could have a model generate a model.<p>All of what I&#x27;m saying is of course speculative. As I said, this model is a stepping stone for the future. Just like the LLM which is only trivially helpful now, the LLM can be a stepping stone for replacing programmers all together.</div><br/></div></div></div></div></div></div><div id="41381794" class="c"><input type="checkbox" id="c-41381794" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#41381454">root</a><span>|</span><a href="#41381739">parent</a><span>|</span><a href="#41381880">prev</a><span>|</span><a href="#41382726">next</a><span>|</span><label class="collapse" for="c-41381794">[-]</label><label class="expand" for="c-41381794">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; (so you can kill an enemy, turn your back, then turn around again, and the enemy could be alive again)</i><p>Sounds like a great game.<p><i>&gt; not to mention that it requires the game to already exist in order to train it</i><p>Diffusion models create new images that did not previously exist all of the time, so I&#x27;m not sure how that follows. It&#x27;s not hard to extrapolate from TFA to a model that generically creates games based on some input</div><br/></div></div><div id="41382726" class="c"><input type="checkbox" id="c-41382726" checked=""/><div class="controls bullet"><span class="by">throwthrowuknow</span><span>|</span><a href="#41381454">root</a><span>|</span><a href="#41381739">parent</a><span>|</span><a href="#41381794">prev</a><span>|</span><a href="#41381547">next</a><span>|</span><label class="collapse" for="c-41382726">[-]</label><label class="expand" for="c-41382726">[1 more]</label></div><br/><div class="children"><div class="content">Read the paper. It is capable of maintaining state for a fairly long time including updating the UI elements.</div><br/></div></div></div></div></div></div><div id="41381547" class="c"><input type="checkbox" id="c-41381547" checked=""/><div class="controls bullet"><span class="by">msk-lywenn</span><span>|</span><a href="#41381454">parent</a><span>|</span><a href="#41381537">prev</a><span>|</span><a href="#41377398">next</a><span>|</span><label class="collapse" for="c-41381547">[-]</label><label class="expand" for="c-41381547">[1 more]</label></div><br/><div class="children"><div class="content">I’d like to now to carbon footprint of that fun.</div><br/></div></div></div></div><div id="41377398" class="c"><input type="checkbox" id="c-41377398" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41381454">prev</a><span>|</span><a href="#41388679">next</a><span>|</span><label class="collapse" for="c-41377398">[-]</label><label class="expand" for="c-41377398">[14 more]</label></div><br/><div class="children"><div class="content">Doom system requirements:<p><pre><code>  - 4 MB RAM
  - 12 MB disk space 
</code></pre>
Stable diffusion v1<p><pre><code>  &gt; 860M UNet and CLIP ViT-L&#x2F;14 (540M)
  Checkpoint size:
    4.27 Gb 
    7.7 GB (full EMA)
  Running on a TPU-v5e
    Peak compute per chip (bf16)  197 TFLOPs
    Peak compute per chip (Int8)  393 TFLOPs
    HBM2 capacity and bandwidth  16 GB, 819 GBps
    Interchip Interconnect BW  1600 Gbps
</code></pre>
This is quite impressive, especially considering the speed. But there&#x27;s still a ton of room for improvement. It seems it didn&#x27;t even memorize the game despite having the capacity to do so hundreds of times over. So we definitely have lots of room for optimization methods. Though who knows how such things would affect existing tech since the goal here is to memorize.<p>What&#x27;s also interesting about this work is it&#x27;s basically saying you can rip a game if you&#x27;re willing to &quot;play&quot; (automate) it enough times and spend a lot more on storage and compute. I&#x27;m curious what the comparison in cost and time would be if you hired an engineer to reverse engineer Doom (how much prior knowledge do they get considering pertained models and visdoom environment. Was doom source code in T5? And which vit checkpoint was used? I can&#x27;t keep track of Google vit checkpoints).<p>I would love to see the checkpoint of this model. I think people would find some really interesting stuff taking it apart.<p>- <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;gaming&#x2F;comments&#x2F;a4yi5t&#x2F;original_doom_system_requirements_from_my_25_year&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;gaming&#x2F;comments&#x2F;a4yi5t&#x2F;original_doo...</a><p>- <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;CompVis&#x2F;stable-diffusion-v-1-4-original&#x2F;tree&#x2F;main" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;CompVis&#x2F;stable-diffusion-v-1-4-origin...</a><p>- <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;tpu&#x2F;docs&#x2F;v5e" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;tpu&#x2F;docs&#x2F;v5e</a><p>- <a href="https:&#x2F;&#x2F;github.com&#x2F;Farama-Foundation&#x2F;ViZDoom">https:&#x2F;&#x2F;github.com&#x2F;Farama-Foundation&#x2F;ViZDoom</a><p>- <a href="https:&#x2F;&#x2F;zdoom.org&#x2F;index" rel="nofollow">https:&#x2F;&#x2F;zdoom.org&#x2F;index</a></div><br/><div id="41377447" class="c"><input type="checkbox" id="c-41377447" checked=""/><div class="controls bullet"><span class="by">snickmy</span><span>|</span><a href="#41377398">parent</a><span>|</span><a href="#41377933">next</a><span>|</span><label class="collapse" for="c-41377447">[-]</label><label class="expand" for="c-41377447">[8 more]</label></div><br/><div class="children"><div class="content">Those are valid points, but irrelevant for the context of this research.<p>Yes, the computational cost is ridicolous compared to the original game, and yes, it lacks basic things like pre-computing, storing, etc. That said, you could assume that all that can be either done at the margin of this discovery OR over time will naturally improve OR will become less important as a blocker.<p>The fact that you can model a sequence of frames with such contextual awareness without explictly having to encode it, is the real breakthrough here. Both from a pure gaming standpoint, but on simulation in general.</div><br/><div id="41377565" class="c"><input type="checkbox" id="c-41377565" checked=""/><div class="controls bullet"><span class="by">pickledoyster</span><span>|</span><a href="#41377398">root</a><span>|</span><a href="#41377447">parent</a><span>|</span><a href="#41377571">next</a><span>|</span><label class="collapse" for="c-41377565">[-]</label><label class="expand" for="c-41377565">[1 more]</label></div><br/><div class="children"><div class="content">&gt;you could assume that all that can be either done at the margin of this discovery OR over time will naturally improve OR will become less important as a blocker.<p>OR one can hope it will be thrown to the heap of nonviable tech with the rest of spam waste</div><br/></div></div><div id="41377571" class="c"><input type="checkbox" id="c-41377571" checked=""/><div class="controls bullet"><span class="by">tobr</span><span>|</span><a href="#41377398">root</a><span>|</span><a href="#41377447">parent</a><span>|</span><a href="#41377565">prev</a><span>|</span><a href="#41382701">next</a><span>|</span><label class="collapse" for="c-41377571">[-]</label><label class="expand" for="c-41377571">[2 more]</label></div><br/><div class="children"><div class="content">I suppose it also doesn&#x27;t really matter what kinds of resources the game originally requires. The diffusion model isn&#x27;t going to require twice as much memory just because the game does. Presumably you wouldn&#x27;t even necessarily need to be able to render the original game in real time - I would imagine the basic technique would work even if you used a state of the Hollywood-quality offline renderer to render each input frame, and that the performance of the diffusion model would be similar?</div><br/><div id="41382049" class="c"><input type="checkbox" id="c-41382049" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41377398">root</a><span>|</span><a href="#41377571">parent</a><span>|</span><a href="#41382701">next</a><span>|</span><label class="collapse" for="c-41382049">[-]</label><label class="expand" for="c-41382049">[1 more]</label></div><br/><div class="children"><div class="content">Well the majority of ML systems are compression machines (entropy minimizers), so ideally you&#x27;d want to see if you can learn the assets and game mechanics through play alone (what this paper shows). Better would be to do so more efficiently than that devs themselves, finding better compression. Certainly the game is not perfectly optimized. But still, this is a step in that direction. I mean no one has accomplished this before so even with a model with far higher capacity it&#x27;s progress. (I think people are interpreting my comment as dismissive. I&#x27;m critiquing but the key point I was making was about how there&#x27;s likely better architectures, training methods, and all sorts of stuff to still research. Personally I&#x27;m glad there&#x27;s still more to research. That&#x27;s the fun part)</div><br/></div></div></div></div><div id="41382701" class="c"><input type="checkbox" id="c-41382701" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#41377398">root</a><span>|</span><a href="#41377447">parent</a><span>|</span><a href="#41377571">prev</a><span>|</span><a href="#41377542">next</a><span>|</span><label class="collapse" for="c-41382701">[-]</label><label class="expand" for="c-41382701">[1 more]</label></div><br/><div class="children"><div class="content">Is it a breakthrough? Weather models are miles ahead of this as far as I can tell.</div><br/></div></div><div id="41377542" class="c"><input type="checkbox" id="c-41377542" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41377398">root</a><span>|</span><a href="#41377447">parent</a><span>|</span><a href="#41382701">prev</a><span>|</span><a href="#41377933">next</a><span>|</span><label class="collapse" for="c-41377542">[-]</label><label class="expand" for="c-41377542">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure what you&#x27;re saying is irrelevant.<p>1) the model has enough memory to store not only all game assets and engine but even hundreds of &quot;plays&quot;.<p>2) me mentioning that there&#x27;s still a lot of room to make these things better (seems you think so too so maybe not this one?)<p>3) an interesting point I was wondering to compare current state of things (I mean I&#x27;ll give you this but it&#x27;s just a random thought and I&#x27;m not reviewing this paper in an academic setting. This is HN, not NeurIPS. I&#x27;m just curious ¯ \ _ ( ツ ) _ &#x2F; ¯)<p>4) the point that you can rip a game<p>I&#x27;m really not sure what you&#x27;re contesting to because I said several things.<p><pre><code>  &gt; it lacks basic things like pre-computing, storing, etc.
</code></pre>
It does? Last I checked neural nets store information. I guess I need to return my PhD because last I checked there&#x27;s a UNet in SD 1.4 and that contains a decoder.</div><br/><div id="41377869" class="c"><input type="checkbox" id="c-41377869" checked=""/><div class="controls bullet"><span class="by">snickmy</span><span>|</span><a href="#41377398">root</a><span>|</span><a href="#41377542">parent</a><span>|</span><a href="#41377933">next</a><span>|</span><label class="collapse" for="c-41377869">[-]</label><label class="expand" for="c-41377869">[2 more]</label></div><br/><div class="children"><div class="content">Sorry, probably didn&#x27;t explain myself well enough<p>1) yes you are correct. the point i was making is that, in the context of the discovery&#x2F;research, that&#x27;s outside the scope, and &#x27;easier&#x27; to do, as it has been done in other verticals (ie.: e2e self driving)<p>2) yep, aligned here<p>3) I&#x27;m not fully following here, but agree this is not NeurIPS, and no Schmidhuber&#x27;s bickering.<p>4) The network does store information, it just doesn&#x27;t store a gameplay information, which could be forced, but as per point 1, it is , and I think it is the right approach, beyond the scope of this research</div><br/><div id="41381734" class="c"><input type="checkbox" id="c-41381734" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41377398">root</a><span>|</span><a href="#41377869">parent</a><span>|</span><a href="#41377933">next</a><span>|</span><label class="collapse" for="c-41381734">[-]</label><label class="expand" for="c-41381734">[1 more]</label></div><br/><div class="children"><div class="content">1) I&#x27;m not sure this is outside scope. It&#x27;s also not something I&#x27;d use to reject a paper were I to review this in a conference. I mean you got to start somewhere and unlike reviewer 2 I don&#x27;t think any criticism is rejection criteria. That&#x27;d be silly since lack of globally optimal solutions. But I&#x27;m also unconvinced this is proven my self-driving vehicles but I&#x27;m also not an RL expert.<p>3) It&#x27;s always hard to evaluate. I was thinking about the ripping the game and so a reasonable metric is a comparison of ability to perform the task by a human. Of course I&#x27;m A LOT faster than my dishwasher at cleaning dishes but I&#x27;m not occupied while it is going, so it still has high utility. (Someone tell reviewer 2 lol)<p>4) Why should we believe that it doesn&#x27;t store gameplay? The model was fed &quot;user&quot; inputs and frames. So it has this information and this information appears useful for learning the task.</div><br/></div></div></div></div></div></div></div></div><div id="41377933" class="c"><input type="checkbox" id="c-41377933" checked=""/><div class="controls bullet"><span class="by">dTal</span><span>|</span><a href="#41377398">parent</a><span>|</span><a href="#41377447">prev</a><span>|</span><a href="#41388679">next</a><span>|</span><label class="collapse" for="c-41377933">[-]</label><label class="expand" for="c-41377933">[5 more]</label></div><br/><div class="children"><div class="content">&gt;What&#x27;s also interesting about this work is it&#x27;s basically saying you can rip a game if you&#x27;re willing to &quot;play&quot; (automate) it enough times and spend a lot more on storage and compute<p>That&#x27;s the least of it. It means you can <i>generate</i> a game from real footage. Want a perfect flight sim? Put a GoPro in the cockpit of every airliner for a year.</div><br/><div id="41380544" class="c"><input type="checkbox" id="c-41380544" checked=""/><div class="controls bullet"><span class="by">phh</span><span>|</span><a href="#41377398">root</a><span>|</span><a href="#41377933">parent</a><span>|</span><a href="#41378629">next</a><span>|</span><label class="collapse" for="c-41380544">[-]</label><label class="expand" for="c-41380544">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Want a perfect flight sim? Put a GoPro in the cockpit of every airliner for a year.<p>I guess that&#x27;s the occasion to remind that ML is splendid at interpolating, but extrapolating, maybe don&#x27;t keep your hopes too high.<p>Namely, to have a &quot;perfect flight sim&quot; using GoPros, you&#x27;ll need to record hundreds of stalls and crashs.</div><br/></div></div><div id="41378629" class="c"><input type="checkbox" id="c-41378629" checked=""/><div class="controls bullet"><span class="by">camtarn</span><span>|</span><a href="#41377398">root</a><span>|</span><a href="#41377933">parent</a><span>|</span><a href="#41380544">prev</a><span>|</span><a href="#41378330">next</a><span>|</span><label class="collapse" for="c-41378629">[-]</label><label class="expand" for="c-41378629">[1 more]</label></div><br/><div class="children"><div class="content">Plus, presumably, either training it on pilot inputs (and being able to map those to joystick inputs and mouse clicks) or having the user have an identical fake cockpit to play in and a camera to pick up their movements.<p>And, unless you wanted a simulator that only allowed perfectly normal flight, you&#x27;d have to have those airliners go through every possible situation that you wanted to reproduce: warnings, malfunctions, emergencies, pilots pushing the airliner out of its normal flight envelope, etc.</div><br/></div></div><div id="41378330" class="c"><input type="checkbox" id="c-41378330" checked=""/><div class="controls bullet"><span class="by">isaacfung</span><span>|</span><a href="#41377398">root</a><span>|</span><a href="#41377933">parent</a><span>|</span><a href="#41378629">prev</a><span>|</span><a href="#41385395">next</a><span>|</span><label class="collapse" for="c-41378330">[-]</label><label class="expand" for="c-41378330">[1 more]</label></div><br/><div class="children"><div class="content">The possibility seems far beyond gaming(given enough computation resources).<p>You can feed it with videos of usage of any software or real world footage recorded by a Go Pro mounted on your shoulder(with body motion measured by some sesnors though the action space would be much larger).<p>Such a &quot;game engine&quot; can potentially be used as a simulation gym environment to train RL agents.</div><br/></div></div><div id="41385395" class="c"><input type="checkbox" id="c-41385395" checked=""/><div class="controls bullet"><span class="by">dvngnt_</span><span>|</span><a href="#41377398">root</a><span>|</span><a href="#41377933">parent</a><span>|</span><a href="#41378330">prev</a><span>|</span><a href="#41388679">next</a><span>|</span><label class="collapse" for="c-41385395">[-]</label><label class="expand" for="c-41385395">[1 more]</label></div><br/><div class="children"><div class="content">wouldnt make more sense to train using microsoft flight simulator the same way they did DOOM, but im not sure what the point is if the game already exists</div><br/></div></div></div></div></div></div><div id="41388679" class="c"><input type="checkbox" id="c-41388679" checked=""/><div class="controls bullet"><span class="by">mobiuscog</span><span>|</span><a href="#41377398">prev</a><span>|</span><a href="#41379271">next</a><span>|</span><label class="collapse" for="c-41388679">[-]</label><label class="expand" for="c-41388679">[1 more]</label></div><br/><div class="children"><div class="content">Video Game streamers are next in line to be replaced by AI I guess.</div><br/></div></div><div id="41379271" class="c"><input type="checkbox" id="c-41379271" checked=""/><div class="controls bullet"><span class="by">alkonaut</span><span>|</span><a href="#41388679">prev</a><span>|</span><a href="#41379041">next</a><span>|</span><label class="collapse" for="c-41379271">[-]</label><label class="expand" for="c-41379271">[1 more]</label></div><br/><div class="children"><div class="content">The job of the game engine is also to render the world given only the worlds properties (textures, geometries, physics rules, ...), and not given &quot;training data that had to be supplied from an already written engine&quot;.<p>I&#x27;m guessing that the &quot;This door requires a blue key&quot; doesn&#x27;t mean that the user can run around, the engine dreams up a blue key in some other corner of the map, and the user can then return to the door and the engine now opens the door? THAT would be impressive. It&#x27;s interesting to think that all that would be required for that task to go from really hard to quite doable, would be that the door requiring the blue key is blue, and the UI showing some icon indicating the user possesses the blue key. Without that, it becomes (old) hidden state.</div><br/></div></div><div id="41379041" class="c"><input type="checkbox" id="c-41379041" checked=""/><div class="controls bullet"><span class="by">dtagames</span><span>|</span><a href="#41379271">prev</a><span>|</span><a href="#41377336">next</a><span>|</span><label class="collapse" for="c-41379041">[-]</label><label class="expand" for="c-41379041">[17 more]</label></div><br/><div class="children"><div class="content">A diffusion model cannot be a game engine because a game engine can be used to create <i>new</i> games and modify the rules of existing games in real time -- even rules which are not visible on-screen.<p>These tools are fascinating but, as with all AI hype, they need a disclaimer: The tool didn&#x27;t create the game. It simply generated frames and the appearance of play mechanics from a game it sampled (which humans created).</div><br/><div id="41379127" class="c"><input type="checkbox" id="c-41379127" checked=""/><div class="controls bullet"><span class="by">kqr</span><span>|</span><a href="#41379041">parent</a><span>|</span><a href="#41382758">next</a><span>|</span><label class="collapse" for="c-41379127">[-]</label><label class="expand" for="c-41379127">[5 more]</label></div><br/><div class="children"><div class="content">&gt; even rules which are not visible on-screen.<p>If a rule was changed but it&#x27;s never visible on the screen, did it really change?<p>&gt; It simply generated frames and the appearance of play mechanics from a game it sampled (which humans created).<p>Simply?! I understand it&#x27;s mechanically trivial but the fact that it&#x27;s compressed such a rich conditional distribution seems far from simple to me.</div><br/><div id="41379248" class="c"><input type="checkbox" id="c-41379248" checked=""/><div class="controls bullet"><span class="by">znx_0</span><span>|</span><a href="#41379041">root</a><span>|</span><a href="#41379127">parent</a><span>|</span><a href="#41379188">next</a><span>|</span><label class="collapse" for="c-41379248">[-]</label><label class="expand" for="c-41379248">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If a rule was changed but it&#x27;s never visible on the screen, did it really change?<p>Well for &quot;some&quot; games it does really change</div><br/></div></div><div id="41379188" class="c"><input type="checkbox" id="c-41379188" checked=""/><div class="controls bullet"><span class="by">darby_nine</span><span>|</span><a href="#41379041">root</a><span>|</span><a href="#41379127">parent</a><span>|</span><a href="#41379248">prev</a><span>|</span><a href="#41382758">next</a><span>|</span><label class="collapse" for="c-41379188">[-]</label><label class="expand" for="c-41379188">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Simply?! I understand it&#x27;s mechanically trivial but the fact that it&#x27;s compressed such a rich conditional distribution seems far from simple to me.<p>It&#x27;s much simpler than actually creating a game....</div><br/><div id="41380567" class="c"><input type="checkbox" id="c-41380567" checked=""/><div class="controls bullet"><span class="by">stnmtn</span><span>|</span><a href="#41379041">root</a><span>|</span><a href="#41379188">parent</a><span>|</span><a href="#41382758">next</a><span>|</span><label class="collapse" for="c-41380567">[-]</label><label class="expand" for="c-41380567">[2 more]</label></div><br/><div class="children"><div class="content">If someone told you 10 years ago that they were going to create something where you could play a whole new level of Doom, without them writing a single line of game logic&#x2F;rendering code, would you say that that is simpler than creating a demo by writing the game themselves?</div><br/><div id="41382399" class="c"><input type="checkbox" id="c-41382399" checked=""/><div class="controls bullet"><span class="by">darby_nine</span><span>|</span><a href="#41379041">root</a><span>|</span><a href="#41380567">parent</a><span>|</span><a href="#41382758">next</a><span>|</span><label class="collapse" for="c-41382399">[-]</label><label class="expand" for="c-41382399">[1 more]</label></div><br/><div class="children"><div class="content">There are two things at play here: the complexity of the underlying mechanism, and the complexity of detailed creation. This is obviously a complicated mechanism, but in another sense it&#x27;s a trivial result compared to actually reproducing the game itself in its original intended state.</div><br/></div></div></div></div></div></div></div></div><div id="41382758" class="c"><input type="checkbox" id="c-41382758" checked=""/><div class="controls bullet"><span class="by">throwthrowuknow</span><span>|</span><a href="#41379041">parent</a><span>|</span><a href="#41379127">prev</a><span>|</span><a href="#41380248">next</a><span>|</span><label class="collapse" for="c-41382758">[-]</label><label class="expand" for="c-41382758">[1 more]</label></div><br/><div class="children"><div class="content">They only trained it on one game and only embedded the control inputs. You could train it on many games and embed a lot more information about each of them which could possibly allow you to specify a prompt that would describe the game and then play it.</div><br/></div></div><div id="41380248" class="c"><input type="checkbox" id="c-41380248" checked=""/><div class="controls bullet"><span class="by">calebh</span><span>|</span><a href="#41379041">parent</a><span>|</span><a href="#41382758">prev</a><span>|</span><a href="#41379439">next</a><span>|</span><label class="collapse" for="c-41380248">[-]</label><label class="expand" for="c-41380248">[1 more]</label></div><br/><div class="children"><div class="content">One thing I&#x27;d like to see is to take a game rendered with low poly assets (or segmented in some way) and use a diffusion model to add realistic or stylized art details. This would fix the consistency problem while still providing tangible benefits.</div><br/></div></div><div id="41381556" class="c"><input type="checkbox" id="c-41381556" checked=""/><div class="controls bullet"><span class="by">momojo</span><span>|</span><a href="#41379041">parent</a><span>|</span><a href="#41379439">prev</a><span>|</span><a href="#41379212">next</a><span>|</span><label class="collapse" for="c-41381556">[-]</label><label class="expand" for="c-41381556">[1 more]</label></div><br/><div class="children"><div class="content">The title should be &quot;Diffusion Models can be used to render frames given user input&quot;</div><br/></div></div><div id="41379212" class="c"><input type="checkbox" id="c-41379212" checked=""/><div class="controls bullet"><span class="by">sharpshadow</span><span>|</span><a href="#41379041">parent</a><span>|</span><a href="#41381556">prev</a><span>|</span><a href="#41377336">next</a><span>|</span><label class="collapse" for="c-41379212">[-]</label><label class="expand" for="c-41379212">[7 more]</label></div><br/><div class="children"><div class="content">So all it did is generate a video of the gameplay which is slightly different from the video it used for training?</div><br/><div id="41379272" class="c"><input type="checkbox" id="c-41379272" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#41379041">root</a><span>|</span><a href="#41379212">parent</a><span>|</span><a href="#41377336">next</a><span>|</span><label class="collapse" for="c-41379272">[-]</label><label class="expand" for="c-41379272">[6 more]</label></div><br/><div class="children"><div class="content">No, it implements a 3D FPS that&#x27;s interactive, and renders each frame based on your input and a lot of memorized gameplay.</div><br/><div id="41379551" class="c"><input type="checkbox" id="c-41379551" checked=""/><div class="controls bullet"><span class="by">sharpshadow</span><span>|</span><a href="#41379041">root</a><span>|</span><a href="#41379272">parent</a><span>|</span><a href="#41377336">next</a><span>|</span><label class="collapse" for="c-41379551">[-]</label><label class="expand" for="c-41379551">[5 more]</label></div><br/><div class="children"><div class="content">But is it playing the actual game or just making a interactive video of it?</div><br/><div id="41381405" class="c"><input type="checkbox" id="c-41381405" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#41379041">root</a><span>|</span><a href="#41379551">parent</a><span>|</span><a href="#41379731">next</a><span>|</span><label class="collapse" for="c-41381405">[-]</label><label class="expand" for="c-41381405">[2 more]</label></div><br/><div class="children"><div class="content">Yes.<p>All video games are, by definition, interactive videos.<p>What I imagine you&#x27;re asking about is, a typical game like Doom is effectively a function:<p><pre><code>  f(internal state, player input) -&gt; (new frame, new internal state)
</code></pre>
where internal state is the shape and looks of loaded map, positions and behaviors and stats of enemies, player, items, etc.<p>A typical AI that plays Doom, which is <i>not</i> what&#x27;s happening here, is (at runtime):<p><pre><code>  f(last frame) -&gt; new player input
</code></pre>
and is attached in a loop to the previous case in the obvious way.<p>What we have here, however, is a game you can play but implemented in a diffusion model, and it 
works like this:<p><pre><code>  f(player input, N last frames) -&gt; new frame
</code></pre>
Of note here is the <i>lack of game state</i> - the state is implicit in the contents of the N previous frames, and is otherwise not represented or mutated explicitly. The diffusion model has seen so much Doom that it, in a way, <i>internalized</i> most of the state and its evolution, so it can look at what&#x27;s going on and guess what&#x27;s about to happen. Which is what it does: it renders the next frame by predicting it, based on current user input and last N frames. And then that frame becomes the input for the next prediction, and so on, and so on.<p>So yes, it&#x27;s totally an interactive video <i>and</i> a game <i>and</i> a third thing - a probabilistic emulation of Doom on a generative ML model.</div><br/><div id="41383128" class="c"><input type="checkbox" id="c-41383128" checked=""/><div class="controls bullet"><span class="by">sharpshadow</span><span>|</span><a href="#41379041">root</a><span>|</span><a href="#41381405">parent</a><span>|</span><a href="#41379731">next</a><span>|</span><label class="collapse" for="c-41383128">[-]</label><label class="expand" for="c-41383128">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for the further explanation, that’s what I thought in the meantime and intended to find out with my question.<p>That opens up a new branch of possibilities.</div><br/></div></div></div></div><div id="41379731" class="c"><input type="checkbox" id="c-41379731" checked=""/><div class="controls bullet"><span class="by">Maxatar</span><span>|</span><a href="#41379041">root</a><span>|</span><a href="#41379551">parent</a><span>|</span><a href="#41381405">prev</a><span>|</span><a href="#41379805">next</a><span>|</span><label class="collapse" for="c-41379731">[-]</label><label class="expand" for="c-41379731">[1 more]</label></div><br/><div class="children"><div class="content">Making an interactive video of it. It is not playing the game, a human does that.<p>With that said, I wholly disagree that this is not an engine. This is absolutely a game engine and while this particular demo uses the engine to recreate DOOM, an existing game, you could certainly use this engine to produce new games in addition to extrapolating existing games in novel ways.</div><br/></div></div><div id="41379805" class="c"><input type="checkbox" id="c-41379805" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#41379041">root</a><span>|</span><a href="#41379551">parent</a><span>|</span><a href="#41379731">prev</a><span>|</span><a href="#41377336">next</a><span>|</span><label class="collapse" for="c-41379805">[-]</label><label class="expand" for="c-41379805">[1 more]</label></div><br/><div class="children"><div class="content">What is the difference?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41377336" class="c"><input type="checkbox" id="c-41377336" checked=""/><div class="controls bullet"><span class="by">HellDunkel</span><span>|</span><a href="#41379041">prev</a><span>|</span><a href="#41375811">next</a><span>|</span><label class="collapse" for="c-41377336">[-]</label><label class="expand" for="c-41377336">[4 more]</label></div><br/><div class="children"><div class="content">Although impressive i must disagree. Diffusion models are not game engines. A game engine is a component to propell your game (along the time axis?). In that sense it is similar to the engine of the car, hence the name. It does not need a single working car nor a road to drive on do its job.
The above is a dynamic, interactive replication of what happens when you put a car on a given road, requiring a million test drives with working vehicles. An engine would also work offroad.</div><br/><div id="41377539" class="c"><input type="checkbox" id="c-41377539" checked=""/><div class="controls bullet"><span class="by">MasterScrat</span><span>|</span><a href="#41377336">parent</a><span>|</span><a href="#41375811">next</a><span>|</span><label class="collapse" for="c-41377539">[-]</label><label class="expand" for="c-41377539">[3 more]</label></div><br/><div class="children"><div class="content">Interesting point.<p>In a way this is a &quot;simulated game engine&quot;, trained from actual game engine data. But I would argue a working simulated game engine becomes a game engine of its own, as it is then able to &quot;propell the game&quot; as you say. The way it achieves this becomes irrelevant, in one case the content was crafted by humans, in the other case it mimics existing game content, the player really doesn&#x27;t care!<p>&gt; An engine would also work offroad.<p>Here you could imagine that such a &quot;generative game engine&quot; could <i>also</i> go offroad, extrapolating what would happen if you go to unseen places. I&#x27;d even say extrapolation capabilities of such a model could be better than a traditional game engine, as it can make things up as it goes, while if you accidentally cross a wall in a typical game engine the screen goes blank.</div><br/><div id="41378566" class="c"><input type="checkbox" id="c-41378566" checked=""/><div class="controls bullet"><span class="by">HellDunkel</span><span>|</span><a href="#41377336">root</a><span>|</span><a href="#41377539">parent</a><span>|</span><a href="#41378477">next</a><span>|</span><label class="collapse" for="c-41378566">[-]</label><label class="expand" for="c-41378566">[1 more]</label></div><br/><div class="children"><div class="content">The game doom is more than a game engine, isnt it? I‘d be okay with calling the above a „simulated game“ or a „game“. My point is: let‘s not conflate the idea of a „game engine“ which is a construct of intellectual concepts put together to create a simulation of „things happening in time“ and deriving output (audio and visual). the engine is fed with input and data (levels and other assets) and then drives(EDIT) a „game“.<p>training the model with a final game will never give you an engine. maybe a „simulated game“ or even a „game“ but certainly not an „engine“. the latter would mean the model would be capable to derive and extract the technical and intellectual concepts and apply them elsewhere.</div><br/></div></div><div id="41378477" class="c"><input type="checkbox" id="c-41378477" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#41377336">root</a><span>|</span><a href="#41377539">parent</a><span>|</span><a href="#41378566">prev</a><span>|</span><a href="#41375811">next</a><span>|</span><label class="collapse" for="c-41378477">[-]</label><label class="expand" for="c-41378477">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Here you could imagine that such a &quot;generative game engine&quot; could also go offroad, extrapolating what would happen if you go to unseen places.<p>They easily could have demonstrated this by seeding the model with images of Doom maps which weren&#x27;t in the training set, but they chose not to. I&#x27;m sure they tried it and the results just weren&#x27;t good, probably morphing the map into one of the ones it was trained on at the first opportunity.</div><br/></div></div></div></div></div></div><div id="41375811" class="c"><input type="checkbox" id="c-41375811" checked=""/><div class="controls bullet"><span class="by">refibrillator</span><span>|</span><a href="#41377336">prev</a><span>|</span><a href="#41375678">next</a><span>|</span><label class="collapse" for="c-41375811">[-]</label><label class="expand" for="c-41375811">[24 more]</label></div><br/><div class="children"><div class="content">There is no text conditioning provided to the SD model because they removed it, but one can imagine a near future where text prompts are enough to create a fun new game!<p>Yes they had to use RL to learn what DOOM looks like and how it works, but this doesn’t necessarily pose a chicken vs egg problem. In the same way that LLMs can write a novel story, despite only being trained on existing text.<p>IMO one of the biggest challenges with this approach will be open world games with essentially an infinite number of possible states. The paper mentions that they had trouble getting RL agents to completely explore every nook and corner of DOOM. Factorio or Dwarf Fortress probably won’t be simulated anytime soon…I think.</div><br/><div id="41376053" class="c"><input type="checkbox" id="c-41376053" checked=""/><div class="controls bullet"><span class="by">mlsu</span><span>|</span><a href="#41375811">parent</a><span>|</span><a href="#41376167">next</a><span>|</span><label class="collapse" for="c-41376053">[-]</label><label class="expand" for="c-41376053">[13 more]</label></div><br/><div class="children"><div class="content">With enough computation, your neural net weights would converge to some very compressed latent representation of the source code of DOOM. Maybe smaller even than the source code itself? Someone in the field could probably correct me on that.<p>At which point, you effectively would be interpolating in latent space through the source code to actually &quot;render&quot; the game. You&#x27;d have an entire latent space computer, with an engine, assets, textures, a software renderer.<p>With a sufficiently powerful computer, one could imagine what interpolating in this latent space between, say Factorio and TF2 (2 of my favorites). And tweaking this latent space to your liking by conditioning it on any number of gameplay aspects.<p>This future comes very quickly for subsets of the pipeline, like the very end stage of rendering -- DLSS is already in production, for example. Maybe Nvidia&#x27;s revenue wraps back to gaming once again, as we all become bolted into a neural metaverse.<p>God I love that they chose DOOM.</div><br/><div id="41376699" class="c"><input type="checkbox" id="c-41376699" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376053">parent</a><span>|</span><a href="#41377468">next</a><span>|</span><label class="collapse" for="c-41376699">[-]</label><label class="expand" for="c-41376699">[2 more]</label></div><br/><div class="children"><div class="content">&gt; With enough computation, your neural net weights would converge to some very compressed latent representation of the source code of DOOM. Maybe smaller even than the source code itself? Someone in the field could probably correct me on that.<p>Neural nets are not guaranteed to converge to anything even remotely optimal, so no that isn&#x27;t how it works. Also even though neural nets can approximate any function they usually can&#x27;t do it in a time or space efficient manner, resulting in much larger programs than the human written code.</div><br/><div id="41382504" class="c"><input type="checkbox" id="c-41382504" checked=""/><div class="controls bullet"><span class="by">mlsu</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376699">parent</a><span>|</span><a href="#41377468">next</a><span>|</span><label class="collapse" for="c-41382504">[-]</label><label class="expand" for="c-41382504">[1 more]</label></div><br/><div class="children"><div class="content">Could is certainly a better word, yes. There is no guarantee that it will happen, only that it could. The existence of LLMs is proof of that; imagine how large and inefficient a handwritten computer program to generate the next token would be. On the flipside, human beings very effectively predicting the next token, and much more, on 5 watts is proof that LLM in their current form certainly are not the most efficient method for generating next token.<p>I don&#x27;t really know why everyone is piling on me here. Sorry for a bit of fun speculating! This model is on the continuum. There <i>is</i> a latent representation of Doom in weights. <i>some</i> weights, not <i>these</i> weights. Therefore <i>some</i> representation of doom in a neural net <i>could</i> become more efficient over time. That&#x27;s really the point I&#x27;m trying to make.</div><br/></div></div></div></div><div id="41377468" class="c"><input type="checkbox" id="c-41377468" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376053">parent</a><span>|</span><a href="#41376699">prev</a><span>|</span><a href="#41376118">next</a><span>|</span><label class="collapse" for="c-41377468">[-]</label><label class="expand" for="c-41377468">[5 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; With enough computation, your neural net weights would converge to some very compressed latent representation of the source code of DOOM. 
</code></pre>
You and I have very different definitions of compression<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41377398">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41377398</a><p><pre><code>  &gt; Someone in the field could probably correct me on that.
</code></pre>
^__^</div><br/><div id="41378147" class="c"><input type="checkbox" id="c-41378147" checked=""/><div class="controls bullet"><span class="by">_hark</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41377468">parent</a><span>|</span><a href="#41376118">next</a><span>|</span><label class="collapse" for="c-41378147">[-]</label><label class="expand" for="c-41378147">[4 more]</label></div><br/><div class="children"><div class="content">The raw capacity of the network doesn&#x27;t tell you how complex the weights actually are. The capacity is only an upper bound on the complexity.<p>It&#x27;s easy to see this by noting that you can often prune networks quite a bit without any loss in performance. I.e. the effective dimension of the manifold the weights live on can be much, much smaller than the total capacity allows for. In fact, good regularization is exactly that which encourages the model itself to be compressible.</div><br/><div id="41381531" class="c"><input type="checkbox" id="c-41381531" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41378147">parent</a><span>|</span><a href="#41376118">next</a><span>|</span><label class="collapse" for="c-41381531">[-]</label><label class="expand" for="c-41381531">[3 more]</label></div><br/><div class="children"><div class="content">I think your confusing capacity with the training dynamics.<p>Capacity is autological. The amount of information it can express.<p>Training dynamics are the way the model learns, the optimization process, etc. So this is where things like regularization come into play.<p>There&#x27;s also architecture which affects the training dynamics as well as model capacity. Which makes no guarantee that you get the most information dense representation.<p>Fwiw, the authors did also try distillation.</div><br/><div id="41385712" class="c"><input type="checkbox" id="c-41385712" checked=""/><div class="controls bullet"><span class="by">_hark</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41381531">parent</a><span>|</span><a href="#41376118">next</a><span>|</span><label class="collapse" for="c-41385712">[-]</label><label class="expand" for="c-41385712">[2 more]</label></div><br/><div class="children"><div class="content">Sorry I wasn&#x27;t more clear! I&#x27;m referring to the Kolmogorov complexity of the network. The OP said:<p>&gt; With enough computation, your neural net weights would converge to some very compressed latent representation of the source code of DOOM. Maybe smaller even than the source code itself? Someone in the field could probably correct me on that.<p>And they&#x27;re not wrong! An ideally trained network could, in principle, learn the data-generating program, if that program is within its class of representable functions. I might have a NN that naively looks like it takes up GBs of space, but it might actually be parameterizing a much simpler function (hence our ability to prune&#x2F;compress the weights without performance loss - most of the capacity wasn&#x27;t being used for any interesting computation).<p>You&#x27;re right that there&#x27;s no guarantee that the model finds the most &quot;dense&quot; representation. The goal of regularization is to encourage that, though!<p>All over the place in ML there are bounds like:<p>test loss &lt;= train loss + model complexity<p>Hence minimizing model complexity improves generalization performance. This is a kind of Occam&#x27;s Razor: the simplest model generalizes best. So the OP is on the right track - we definitely want networks to learn the &quot;underlying&quot; process that explains the data, which in this case would be a latent representation of the source code (well, except that doesn&#x27;t really make sense since you&#x27;d need the whole rest of the compute stack that code runs on - the neural net has no external resources&#x2F;embodied complexity it calls, unlike the source code which gets to rely on drivers, hardware, operating systems, etc.)</div><br/><div id="41387736" class="c"><input type="checkbox" id="c-41387736" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41385712">parent</a><span>|</span><a href="#41376118">next</a><span>|</span><label class="collapse" for="c-41387736">[-]</label><label class="expand" for="c-41387736">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; An ideally trained network could, in principle, learn the data-generating program
</code></pre>
No disagreement<p><pre><code>  &gt; I might have a NN that naively looks like it takes up GBs of space, but it might actually be parameterizing a much simpler function (hence our ability to prune&#x2F;compress the weights without performance loss - most of the capacity wasn&#x27;t being used for any interesting computation).
</code></pre>
Also no disagreement.<p>I suggested that this probably isn&#x27;t the case here since they tried distillation and saw no effect. While this isn&#x27;t proof that this particular model can&#x27;t be compressed more it does suggest that it is non-trivial. This is especially true given the huge difference in size. I mean we&#x27;re talking about 700x...<p>Where I think our disagreement is in that I read the OP as saying __this__ network. If we&#x27;re talking about a theoretical network, well... nothing I said anywhere is in any disagreement with that. I even said in the post I linked to that the difference shows that there&#x27;s still a long way to go but that this is still cool. Why did I assume OP was talking about __this__ network? Well because we&#x27;re in a thread talking about a paper and well... yes, we&#x27;re talking about compression machines so theoretically (well not actually supported by any math theory) this is true for so many things and that is a bit elementary. So makes more sense (imo) that we&#x27;re talking about this network. And I wanted to make it clear that this network is nowhere near compression. Can further research later result in something that is better than the source code? Who knows? For all the reasons we&#x27;ve both mentioned. We know they are universal approximators (which are not universal mimicers and have limits) but we have no guarantee of global convergence (let alone proof such a thing exists in many problems).<p>And I&#x27;m not sure why you&#x27;re trying to explain the basic concepts to me. I mentioned I was an ML researcher. I see you&#x27;re a PhD at Oxford. I&#x27;m sure you would be annoyed if I was doing the same to you. We can talk at a different level.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41376118" class="c"><input type="checkbox" id="c-41376118" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376053">parent</a><span>|</span><a href="#41377468">prev</a><span>|</span><a href="#41376148">next</a><span>|</span><label class="collapse" for="c-41376118">[-]</label><label class="expand" for="c-41376118">[4 more]</label></div><br/><div class="children"><div class="content">The source code lacks information required to render the game. Textures for example.</div><br/><div id="41376581" class="c"><input type="checkbox" id="c-41376581" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376118">parent</a><span>|</span><a href="#41376929">next</a><span>|</span><label class="collapse" for="c-41376581">[-]</label><label class="expand" for="c-41376581">[2 more]</label></div><br/><div class="children"><div class="content">Obviously assets would get encoded too, in some form. Not necessarily corresponding to the original bitmaps, if the game does some consistent post-processing, the encoded thing would more likely be (equivalent to) the post-processed state.</div><br/><div id="41376701" class="c"><input type="checkbox" id="c-41376701" checked=""/><div class="controls bullet"><span class="by">hoseja</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376581">parent</a><span>|</span><a href="#41376929">next</a><span>|</span><label class="collapse" for="c-41376701">[-]</label><label class="expand" for="c-41376701">[1 more]</label></div><br/><div class="children"><div class="content">Finally, the AI superoptimizing compiler.</div><br/></div></div></div></div><div id="41376929" class="c"><input type="checkbox" id="c-41376929" checked=""/><div class="controls bullet"><span class="by">mistercheph</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376118">parent</a><span>|</span><a href="#41376581">prev</a><span>|</span><a href="#41376148">next</a><span>|</span><label class="collapse" for="c-41376929">[-]</label><label class="expand" for="c-41376929">[1 more]</label></div><br/><div class="children"><div class="content">That’s just an artifact of the language we use to describe an implementation detail, in the sense GP means it, the data payload bits are not essentially distinct from the executable instruction bits</div><br/></div></div></div></div><div id="41376148" class="c"><input type="checkbox" id="c-41376148" checked=""/><div class="controls bullet"><span class="by">electrondood</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376053">parent</a><span>|</span><a href="#41376118">prev</a><span>|</span><a href="#41376167">next</a><span>|</span><label class="collapse" for="c-41376148">[-]</label><label class="expand" for="c-41376148">[1 more]</label></div><br/><div class="children"><div class="content">The Holographic Principle is the idea that our universe is a projection of a higher dimensional space, which sounds an awful lot like the total simulation of an interactive environment, encoded in the parameter space of a neural network.<p>The first thing I thought when I saw this was: couldn&#x27;t my immediate experience be exactly the same thing? Including the illusion of a separate main character to whom events are occurring?</div><br/></div></div></div></div><div id="41376167" class="c"><input type="checkbox" id="c-41376167" checked=""/><div class="controls bullet"><span class="by">basch</span><span>|</span><a href="#41375811">parent</a><span>|</span><a href="#41376053">prev</a><span>|</span><a href="#41376260">next</a><span>|</span><label class="collapse" for="c-41376167">[-]</label><label class="expand" for="c-41376167">[1 more]</label></div><br/><div class="children"><div class="content">Similarly, you could run a very very simple game engine, that outputs little more than a low resolution wireframe, and upscale it.  Put all of the effort into game mechanics and none into visual quality.<p>I would expect something in this realm to be a little better at not being visually inconsistent when you look away and look back.  A red monster turning into a blue friendly etc.</div><br/></div></div><div id="41376260" class="c"><input type="checkbox" id="c-41376260" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41375811">parent</a><span>|</span><a href="#41376167">prev</a><span>|</span><a href="#41376879">next</a><span>|</span><label class="collapse" for="c-41376260">[-]</label><label class="expand" for="c-41376260">[1 more]</label></div><br/><div class="children"><div class="content">&gt; where text prompts are enough to create a fun new game!<p>Not really. This is a reproduction of the first level of Doom. Nothing original is being created.</div><br/></div></div><div id="41376879" class="c"><input type="checkbox" id="c-41376879" checked=""/><div class="controls bullet"><span class="by">SomewhatLikely</span><span>|</span><a href="#41375811">parent</a><span>|</span><a href="#41376260">prev</a><span>|</span><a href="#41376586">next</a><span>|</span><label class="collapse" for="c-41376879">[-]</label><label class="expand" for="c-41376879">[2 more]</label></div><br/><div class="children"><div class="content">Video games are gonna be wild in the near future.  You could have one person talking to a model producing something that&#x27;s on par with a AAA title from today.  Imagine the 2d sidescroller boom on Steam but with immersive photorealistic 3d games with hyper-realistic physics (water flow, fire that spreads, tornados) and full deformability and buildability because the model is pretrained with real world videos.  Your game is just a &quot;style&quot; that tweaks some priors on look, settings, and story.</div><br/><div id="41377922" class="c"><input type="checkbox" id="c-41377922" checked=""/><div class="controls bullet"><span class="by">user432678</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376879">parent</a><span>|</span><a href="#41376586">next</a><span>|</span><label class="collapse" for="c-41377922">[-]</label><label class="expand" for="c-41377922">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, no offence, but you sound like those EA execs wearing expensive suits and never played a single video game in their entire life. There’s a great documentary on how Half Life was made. Gabe Newell was interviewed by someone asking “why you did that and this, it’s not realistic”, where he answered “because it’s more fun this way, you want realism — just go outside”.</div><br/></div></div></div></div><div id="41376586" class="c"><input type="checkbox" id="c-41376586" checked=""/><div class="controls bullet"><span class="by">radarsat1</span><span>|</span><a href="#41375811">parent</a><span>|</span><a href="#41376879">prev</a><span>|</span><a href="#41377286">next</a><span>|</span><label class="collapse" for="c-41376586">[-]</label><label class="expand" for="c-41376586">[1 more]</label></div><br/><div class="children"><div class="content">Most games are conditioned on text, it&#x27;s just that we call it &quot;source code&quot; :).<p>(Jk of course I know what you mean, but you can seriously see text prompts as compressed forms of programming that leverage the model&#x27;s prior knowledge)</div><br/></div></div><div id="41377286" class="c"><input type="checkbox" id="c-41377286" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#41375811">parent</a><span>|</span><a href="#41376586">prev</a><span>|</span><a href="#41376707">next</a><span>|</span><label class="collapse" for="c-41377286">[-]</label><label class="expand" for="c-41377286">[1 more]</label></div><br/><div class="children"><div class="content">This got me thinking. Anyone tried using SD or similar to create graphics for the old classic text adventure games?</div><br/></div></div><div id="41376707" class="c"><input type="checkbox" id="c-41376707" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#41375811">parent</a><span>|</span><a href="#41377286">prev</a><span>|</span><a href="#41377334">next</a><span>|</span><label class="collapse" for="c-41376707">[-]</label><label class="expand" for="c-41376707">[3 more]</label></div><br/><div class="children"><div class="content">&gt; one can imagine a near future where text prompts are enough to create a fun new game<p>Sit down and write down a text prompt for a &quot;fun new game&quot;. You can start with something relatively simple like a Mario-like platformer.<p>By page 300, when you&#x27;re about halfway through describing what you mean, you might understand why this is wishful thinking</div><br/><div id="41377372" class="c"><input type="checkbox" id="c-41377372" checked=""/><div class="controls bullet"><span class="by">reverius42</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376707">parent</a><span>|</span><a href="#41377334">next</a><span>|</span><label class="collapse" for="c-41377372">[-]</label><label class="expand" for="c-41377372">[2 more]</label></div><br/><div class="children"><div class="content">If it can be trained on (many) existing games, then it might work similarly to how you don&#x27;t need to describe every possible detail of a generated image in order to get something that looks like what you&#x27;re asking for (and looks like a plausible image for the underspecified parts).</div><br/><div id="41377701" class="c"><input type="checkbox" id="c-41377701" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41377372">parent</a><span>|</span><a href="#41377334">next</a><span>|</span><label class="collapse" for="c-41377701">[-]</label><label class="expand" for="c-41377701">[1 more]</label></div><br/><div class="children"><div class="content">Things that might work plausible in a static image will not look plausible when things are moving, especially in the game.<p>Also: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41376722">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41376722</a><p>Also: define &quot;fun&quot; and &quot;new&quot; in a &quot;simple text prompt&quot;. Current image generators suck at properly reflecting what you want exactly, because they regurgitate existing things and styles.</div><br/></div></div></div></div></div></div></div></div><div id="41375678" class="c"><input type="checkbox" id="c-41375678" checked=""/><div class="controls bullet"><span class="by">danjl</span><span>|</span><a href="#41375811">prev</a><span>|</span><a href="#41376434">next</a><span>|</span><label class="collapse" for="c-41375678">[-]</label><label class="expand" for="c-41375678">[30 more]</label></div><br/><div class="children"><div class="content">So, diffusion models are game engines as long as you already built the game? You need the game to train the model. Chicken. Egg?</div><br/><div id="41375727" class="c"><input type="checkbox" id="c-41375727" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#41375678">parent</a><span>|</span><a href="#41376119">next</a><span>|</span><label class="collapse" for="c-41375727">[-]</label><label class="expand" for="c-41375727">[21 more]</label></div><br/><div class="children"><div class="content">here are some ideas:<p>- you could build a non-real-time version of the game engine and use the neural net as a real-time approximation<p>- you could edit videos shot in real life to have huds or whatever and train the neural net to simulate reality rather than doom.  (this paper used 900 million frames which i think is about a year of video if it&#x27;s 30fps, but maybe algorithmic improvements can cut the training requirements down) and a year of video isn&#x27;t actually all that much—like, maybe you could recruit 500 people to play paintball while wearing gopro cameras with accelerometers and gyros on their heads and paintball guns, so that you could get a year of video in a weekend?</div><br/><div id="41376426" class="c"><input type="checkbox" id="c-41376426" checked=""/><div class="controls bullet"><span class="by">injidup</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375727">parent</a><span>|</span><a href="#41375825">next</a><span>|</span><label class="collapse" for="c-41376426">[-]</label><label class="expand" for="c-41376426">[5 more]</label></div><br/><div class="children"><div class="content">Why games? I will train it on 1 years worth of me attending Microsoft teams meetings. Then I will go surfing.</div><br/><div id="41378184" class="c"><input type="checkbox" id="c-41378184" checked=""/><div class="controls bullet"><span class="by">kqr</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41376426">parent</a><span>|</span><a href="#41377618">next</a><span>|</span><label class="collapse" for="c-41378184">[-]</label><label class="expand" for="c-41378184">[2 more]</label></div><br/><div class="children"><div class="content">Even if you spend 40 hours a week in video conferences, you&#x27;ll have to work for over four years to get one years&#x27; worth of footage. Of course, by then the models will be even better and so you might actually have a chance of going surfing.<p>I guess I should start hoarding video of myself now.</div><br/><div id="41378573" class="c"><input type="checkbox" id="c-41378573" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41378184">parent</a><span>|</span><a href="#41377618">next</a><span>|</span><label class="collapse" for="c-41378573">[-]</label><label class="expand" for="c-41378573">[1 more]</label></div><br/><div class="children"><div class="content">the neural net doesn&#x27;t need a year of video to train to simulate your face; it can do that from a single photo.  the year of video is to learn how to play the game, and in most cases lots of people are playing the same game, so you can dump all their video in the same training set</div><br/></div></div></div></div><div id="41377618" class="c"><input type="checkbox" id="c-41377618" checked=""/><div class="controls bullet"><span class="by">akie</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41376426">parent</a><span>|</span><a href="#41378184">prev</a><span>|</span><a href="#41378144">next</a><span>|</span><label class="collapse" for="c-41377618">[-]</label><label class="expand" for="c-41377618">[1 more]</label></div><br/><div class="children"><div class="content">Ready to pay for this</div><br/></div></div><div id="41378144" class="c"><input type="checkbox" id="c-41378144" checked=""/><div class="controls bullet"><span class="by">ccozan</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41376426">parent</a><span>|</span><a href="#41377618">prev</a><span>|</span><a href="#41375825">next</a><span>|</span><label class="collapse" for="c-41378144">[-]</label><label class="expand" for="c-41378144">[1 more]</label></div><br/><div class="children"><div class="content">most underrated comment here!</div><br/></div></div></div></div><div id="41375825" class="c"><input type="checkbox" id="c-41375825" checked=""/><div class="controls bullet"><span class="by">w_for_wumbo</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375727">parent</a><span>|</span><a href="#41376426">prev</a><span>|</span><a href="#41376527">next</a><span>|</span><label class="collapse" for="c-41375825">[-]</label><label class="expand" for="c-41375825">[13 more]</label></div><br/><div class="children"><div class="content">That feels like the endgame of video game generation.
You select an art style, a video and the type of game you&#x27;d like to play.
The game is then generated in real-time responding to each action with respect to the existing rule engine.<p>I imagine a game like that could get so convincing in its details and immersiveness that one could forget they&#x27;re playing a game.</div><br/><div id="41376033" class="c"><input type="checkbox" id="c-41376033" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375825">parent</a><span>|</span><a href="#41376673">next</a><span>|</span><label class="collapse" for="c-41376033">[-]</label><label class="expand" for="c-41376033">[3 more]</label></div><br/><div class="children"><div class="content">IIRC, both <i>2001</i>(1968) and <i>Solaris</i>(1972) depict that kind of things as part of alien euthanasia process, not as happy endings</div><br/><div id="41384382" class="c"><input type="checkbox" id="c-41384382" checked=""/><div class="controls bullet"><span class="by">catanama</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41376033">parent</a><span>|</span><a href="#41378637">next</a><span>|</span><label class="collapse" for="c-41384382">[-]</label><label class="expand" for="c-41384382">[1 more]</label></div><br/><div class="children"><div class="content">Well, 2001 is actually a happy ending, as Dave is reborn as a cosmic being. Solaris, at least in the book, is an attempt by the sentient ocean to communicate with researchers through mimics.</div><br/></div></div><div id="41378637" class="c"><input type="checkbox" id="c-41378637" checked=""/><div class="controls bullet"><span class="by">hypertele-Xii</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41376033">parent</a><span>|</span><a href="#41384382">prev</a><span>|</span><a href="#41376673">next</a><span>|</span><label class="collapse" for="c-41378637">[-]</label><label class="expand" for="c-41378637">[1 more]</label></div><br/><div class="children"><div class="content">Also The Matrix, Oblivion, etc.</div><br/></div></div></div></div><div id="41376673" class="c"><input type="checkbox" id="c-41376673" checked=""/><div class="controls bullet"><span class="by">aithrowaway1987</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375825">parent</a><span>|</span><a href="#41376033">prev</a><span>|</span><a href="#41376722">next</a><span>|</span><label class="collapse" for="c-41376673">[-]</label><label class="expand" for="c-41376673">[5 more]</label></div><br/><div class="children"><div class="content">Have you ever played a video game? This is unbelievably depressing. This is a future where games like Slay the Spire, with a unique art style and innovative gameplay simply are not being made.<p>Not to mention this childish nonsense about &quot;forget they&#x27;re playing a game,&quot; as if every game needs to be lifelike VR and there&#x27;s no room for stylization or imagination. I am worried for the future that people think they want these things.</div><br/><div id="41380234" class="c"><input type="checkbox" id="c-41380234" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41376673">parent</a><span>|</span><a href="#41379046">next</a><span>|</span><label class="collapse" for="c-41380234">[-]</label><label class="expand" for="c-41380234">[2 more]</label></div><br/><div class="children"><div class="content">The problem is quite the opposite, that AI will be able to generate games so many game with so many play styles that it will totally dilute the value of all games.<p>Compare it to music gen algo&#x27;s that can now produce music that is 100% indiscernible from generic crappy music. Which is insane given that 5 years ago it could maybe create the sound of something that maybe someone would describe as &quot;sort of guitar-like&quot;. At this rate of progress it&#x27;s probably not going to be long before AI is making better music than humans. And it&#x27;s infinitely available too.</div><br/></div></div><div id="41379046" class="c"><input type="checkbox" id="c-41379046" checked=""/><div class="controls bullet"><span class="by">idiotsecant</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41376673">parent</a><span>|</span><a href="#41380234">prev</a><span>|</span><a href="#41376722">next</a><span>|</span><label class="collapse" for="c-41379046">[-]</label><label class="expand" for="c-41379046">[2 more]</label></div><br/><div class="children"><div class="content">Its a good thing. When the printing press was invented there were probably monks and scribes who thought that this new mechanical monster that took all the individual flourish out of reading was the end of literature. Instead it became a tool to make literature better and just removed a lot of drudgery. Games with individual style and design made by people will of course still exist. They&#x27;ll just be easier to make.</div><br/></div></div></div></div><div id="41376722" class="c"><input type="checkbox" id="c-41376722" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375825">parent</a><span>|</span><a href="#41376673">prev</a><span>|</span><a href="#41375903">next</a><span>|</span><label class="collapse" for="c-41376722">[-]</label><label class="expand" for="c-41376722">[1 more]</label></div><br/><div class="children"><div class="content">There are thousands of games that mimic each other, and only a handful of them are any good.<p>What makes you think a mechanical &quot;predict next frame based on existing games&quot; will be any good?</div><br/></div></div><div id="41375903" class="c"><input type="checkbox" id="c-41375903" checked=""/><div class="controls bullet"><span class="by">omegaworks</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375825">parent</a><span>|</span><a href="#41376722">prev</a><span>|</span><a href="#41375892">next</a><span>|</span><label class="collapse" for="c-41375903">[-]</label><label class="expand" for="c-41375903">[1 more]</label></div><br/><div class="children"><div class="content">EXISTENZ IS PAUSED!</div><br/></div></div><div id="41375892" class="c"><input type="checkbox" id="c-41375892" checked=""/><div class="controls bullet"><span class="by">THBC</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375825">parent</a><span>|</span><a href="#41375903">prev</a><span>|</span><a href="#41376527">next</a><span>|</span><label class="collapse" for="c-41375892">[-]</label><label class="expand" for="c-41375892">[2 more]</label></div><br/><div class="children"><div class="content">Holodeck is just around the corner</div><br/><div id="41377537" class="c"><input type="checkbox" id="c-41377537" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375892">parent</a><span>|</span><a href="#41376527">next</a><span>|</span><label class="collapse" for="c-41377537">[-]</label><label class="expand" for="c-41377537">[1 more]</label></div><br/><div class="children"><div class="content">Except for haptics.</div><br/></div></div></div></div></div></div><div id="41376527" class="c"><input type="checkbox" id="c-41376527" checked=""/><div class="controls bullet"><span class="by">qznc</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375727">parent</a><span>|</span><a href="#41375825">prev</a><span>|</span><a href="#41376140">next</a><span>|</span><label class="collapse" for="c-41376527">[-]</label><label class="expand" for="c-41376527">[1 more]</label></div><br/><div class="children"><div class="content">The Cloud Gaming platforms could record things for training data.</div><br/></div></div></div></div><div id="41376119" class="c"><input type="checkbox" id="c-41376119" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#41375678">parent</a><span>|</span><a href="#41375727">prev</a><span>|</span><a href="#41386183">next</a><span>|</span><label class="collapse" for="c-41376119">[-]</label><label class="expand" for="c-41376119">[3 more]</label></div><br/><div class="children"><div class="content">If you train it on multiple games then you could produce new games that have never existed before, in the same way image generation models can produce new images that have never existed before.</div><br/><div id="41378137" class="c"><input type="checkbox" id="c-41378137" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41376119">parent</a><span>|</span><a href="#41377416">next</a><span>|</span><label class="collapse" for="c-41378137">[-]</label><label class="expand" for="c-41378137">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s unlikely that such a procedurally generated mashup would be perfectly coherent, stable and most importantly <i>fun</i> right out of the gate, so you would need some way to reach into the guts of the generated game and refine it. If properties as simple as &quot;how much health this enemy type has&quot; are scattered across an enormous inscrutable neural network, and may not even have a single consistent definition in all contexts, that&#x27;s going to be quite a challenge. Nevermind if the game just catastrophically implodes and you have to &quot;debug&quot; the model.</div><br/></div></div><div id="41377416" class="c"><input type="checkbox" id="c-41377416" checked=""/><div class="controls bullet"><span class="by">lewhoo</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41376119">parent</a><span>|</span><a href="#41378137">prev</a><span>|</span><a href="#41386183">next</a><span>|</span><label class="collapse" for="c-41377416">[-]</label><label class="expand" for="c-41377416">[1 more]</label></div><br/><div class="children"><div class="content">From what I understand that could make the engine much less stable. The key here is repetitiveness.</div><br/></div></div></div></div><div id="41386183" class="c"><input type="checkbox" id="c-41386183" checked=""/><div class="controls bullet"><span class="by">notfed</span><span>|</span><a href="#41375678">parent</a><span>|</span><a href="#41376119">prev</a><span>|</span><a href="#41375689">next</a><span>|</span><label class="collapse" for="c-41386183">[-]</label><label class="expand" for="c-41386183">[1 more]</label></div><br/><div class="children"><div class="content">I think the same comment could be said about generative images, no?</div><br/></div></div><div id="41375689" class="c"><input type="checkbox" id="c-41375689" checked=""/><div class="controls bullet"><span class="by">billconan</span><span>|</span><a href="#41375678">parent</a><span>|</span><a href="#41386183">prev</a><span>|</span><a href="#41376286">next</a><span>|</span><label class="collapse" for="c-41375689">[-]</label><label class="expand" for="c-41375689">[1 more]</label></div><br/><div class="children"><div class="content">maybe the next step is adding text guidance and generating non-existing games.</div><br/></div></div><div id="41376286" class="c"><input type="checkbox" id="c-41376286" checked=""/><div class="controls bullet"><span class="by">passion__desire</span><span>|</span><a href="#41375678">parent</a><span>|</span><a href="#41375689">prev</a><span>|</span><a href="#41376182">next</a><span>|</span><label class="collapse" for="c-41376286">[-]</label><label class="expand" for="c-41376286">[1 more]</label></div><br/><div class="children"><div class="content">Maybe, in future, techniques of Scientific Machine Learning  which can encode physics and other known laws into a model would form a base model. And then other models on top could just fine tune aspects to customise a game.</div><br/></div></div><div id="41376182" class="c"><input type="checkbox" id="c-41376182" checked=""/><div class="controls bullet"><span class="by">attilakun</span><span>|</span><a href="#41375678">parent</a><span>|</span><a href="#41376286">prev</a><span>|</span><a href="#41376163">next</a><span>|</span><label class="collapse" for="c-41376182">[-]</label><label class="expand" for="c-41376182">[1 more]</label></div><br/><div class="children"><div class="content">If only there was a rich 3-dimensional physical environment we could draw training data from.</div><br/></div></div><div id="41376163" class="c"><input type="checkbox" id="c-41376163" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41375678">parent</a><span>|</span><a href="#41376182">prev</a><span>|</span><a href="#41376434">next</a><span>|</span><label class="collapse" for="c-41376163">[-]</label><label class="expand" for="c-41376163">[1 more]</label></div><br/><div class="children"><div class="content">Well, yeah. Image diffusion models only work because you can provide large amounts of training data. For Doom it is even simpler, since you don&#x27;t need to deal with compositing.</div><br/></div></div></div></div><div id="41376434" class="c"><input type="checkbox" id="c-41376434" checked=""/><div class="controls bullet"><span class="by">helloplanets</span><span>|</span><a href="#41375678">prev</a><span>|</span><a href="#41377191">next</a><span>|</span><label class="collapse" for="c-41376434">[-]</label><label class="expand" for="c-41376434">[3 more]</label></div><br/><div class="children"><div class="content">So, any given sequence of inputs is rebuilt into a corresponding image, twenty times per second. I wonder how separate the game logic and the generated graphics are in the fully trained model.<p>Given a sufficient enough separation between these two, couldn&#x27;t you basically boil the game&#x2F;input logic down to an abstract game template? Meaning, you could just output a hash that corresponds to a specific combination of inputs, and then treat the resulting mapping as a representation of a specific game&#x27;s inner workings.<p>To make it less abstract, you could save some small enough snapshot of the game engine&#x27;s state for all given input sequences. This could make it much less dependent to what&#x27;s recorded off of the agents&#x27; screens. And you could map the objects that appear in the saved states to graphics, in a separate step.<p>I imagine this whole system would work especially well for games that only update when player input is given: Games like Myst, Sokoban, etc.</div><br/><div id="41376668" class="c"><input type="checkbox" id="c-41376668" checked=""/><div class="controls bullet"><span class="by">toppy</span><span>|</span><a href="#41376434">parent</a><span>|</span><a href="#41377191">next</a><span>|</span><label class="collapse" for="c-41376668">[-]</label><label class="expand" for="c-41376668">[2 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;ve just encoded the title of the paper</div><br/></div></div></div></div><div id="41377191" class="c"><input type="checkbox" id="c-41377191" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#41376434">prev</a><span>|</span><a href="#41377364">next</a><span>|</span><label class="collapse" for="c-41377191">[-]</label><label class="expand" for="c-41377191">[5 more]</label></div><br/><div class="children"><div class="content">What I understand is the folloeing: If this works so well, why didn&#x27;t we have good video generation much earlier? After diffusion models were seen to work the most obvious thing to do was to generate the next frame based on previous framrs but... it took 1-2 years for good video models to appear. For example compare Sora generating minecraft video versus this method generating minecraft video. Say in both cases the player is standing on a meadow with fee inputs and watching some pigs. In the Sora video you&#x27;d expect the typical glitched to appear, like erratic, sliding movement, overlapping legs, multiplication of pigs etc. Would these glitches not appear in the GameNGen video? Why?</div><br/><div id="41377208" class="c"><input type="checkbox" id="c-41377208" checked=""/><div class="controls bullet"><span class="by">Closi</span><span>|</span><a href="#41377191">parent</a><span>|</span><a href="#41377210">next</a><span>|</span><label class="collapse" for="c-41377208">[-]</label><label class="expand" for="c-41377208">[3 more]</label></div><br/><div class="children"><div class="content">Because video is much more difficult than images (it&#x27;s lots of images that have to be consistent across time, with motion following laws of physics etc), and this is much more limited in terms of scope than pure arbitrary video generation.</div><br/><div id="41377648" class="c"><input type="checkbox" id="c-41377648" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#41377191">root</a><span>|</span><a href="#41377208">parent</a><span>|</span><a href="#41377210">next</a><span>|</span><label class="collapse" for="c-41377648">[-]</label><label class="expand" for="c-41377648">[2 more]</label></div><br/><div class="children"><div class="content">This misses the point, I&#x27;m comparing two methods of generating minecraft videos.</div><br/><div id="41377705" class="c"><input type="checkbox" id="c-41377705" checked=""/><div class="controls bullet"><span class="by">soulofmischief</span><span>|</span><a href="#41377191">root</a><span>|</span><a href="#41377648">parent</a><span>|</span><a href="#41377210">next</a><span>|</span><label class="collapse" for="c-41377705">[-]</label><label class="expand" for="c-41377705">[1 more]</label></div><br/><div class="children"><div class="content">By simplifying the problem, we are better able to focus on researching specific aspects of generation. In this case, they synthetically created a large, highly domain-specific training set and then used this to train a diffusion model which encodes input parameters instead of text.<p>Sora was trained on a much more diverse dataset, and so has to learn more general solutions in order to maintain consistency, which is harder. The low resolution and simple, highly repetitive textures of doom definitely help as well.<p>In general, this is just an easier problem to approach because of the more focused constraints. It&#x27;s also worth mentioning that noise was added during the process in order to make the model robust to small perturbations.</div><br/></div></div></div></div></div></div><div id="41377210" class="c"><input type="checkbox" id="c-41377210" checked=""/><div class="controls bullet"><span class="by">pantalaimon</span><span>|</span><a href="#41377191">parent</a><span>|</span><a href="#41377208">prev</a><span>|</span><a href="#41377364">next</a><span>|</span><label class="collapse" for="c-41377210">[-]</label><label class="expand" for="c-41377210">[1 more]</label></div><br/><div class="children"><div class="content">I would have thought it is much easier to generate huge amounts of game footage for training, but as I understand this is not what was done here.</div><br/></div></div></div></div><div id="41377364" class="c"><input type="checkbox" id="c-41377364" checked=""/><div class="controls bullet"><span class="by">panki27</span><span>|</span><a href="#41377191">prev</a><span>|</span><a href="#41376371">next</a><span>|</span><label class="collapse" for="c-41377364">[-]</label><label class="expand" for="c-41377364">[11 more]</label></div><br/><div class="children"><div class="content">&gt; Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation.<p>I can hardly believe this claim, anyone who has played some amount of DOOM before should notice the viewport and textures not &quot;feeling right&quot;, or the usually static objects moving slightly.</div><br/><div id="41378334" class="c"><input type="checkbox" id="c-41378334" checked=""/><div class="controls bullet"><span class="by">arc-in-space</span><span>|</span><a href="#41377364">parent</a><span>|</span><a href="#41378643">next</a><span>|</span><label class="collapse" for="c-41378334">[-]</label><label class="expand" for="c-41378334">[1 more]</label></div><br/><div class="children"><div class="content">This, watching the generated clips feels uncomfortable, like a nightmare. Geometry is &quot;swimming&quot; with camera movement, objects randomly appear and disappear, damage is inconsistent.<p>The entire thing would probably crash and burn if you did something just slightly unusual compared to the training data, too. People talking about &#x27;generated&#x27; games often seem to fantasize about an AI that will make up new outcomes for players that go off the beaten path, but a large part of the fun of real games is figuring out what you can do within the predetermined constraints set by the game&#x27;s code. (Pen-and-paper RPGs are highly open-ended, but even a Game Master needs to sometimes protects the players from themselves; whereas the current generation of AI is famously incapable of saying no.)</div><br/></div></div><div id="41378643" class="c"><input type="checkbox" id="c-41378643" checked=""/><div class="controls bullet"><span class="by">aithrowaway1987</span><span>|</span><a href="#41377364">parent</a><span>|</span><a href="#41378334">prev</a><span>|</span><a href="#41379213">next</a><span>|</span><label class="collapse" for="c-41378643">[-]</label><label class="expand" for="c-41378643">[1 more]</label></div><br/><div class="children"><div class="content">I also noticed that they played AI DOOM very slowly: in an actual game you are running around like a madman, but in the video clips the player is moving in a very careful, halting manner. In particular the player only moves in straight lines or turns while stationary, they almost never turn while running. Also didn&#x27;t see much <i>strafing.</i><p>I suspect there is a reason for this: running while turning doesn&#x27;t work properly and makes it very obvious that the system doesn&#x27;t have a consistent internal 3D view of the world. I&#x27;m already getting motion sickness from the inconsistencies in straight-line movement, I can&#x27;t imagine turning is any better.</div><br/></div></div><div id="41379213" class="c"><input type="checkbox" id="c-41379213" checked=""/><div class="controls bullet"><span class="by">freestyle24147</span><span>|</span><a href="#41377364">parent</a><span>|</span><a href="#41378643">prev</a><span>|</span><a href="#41377557">next</a><span>|</span><label class="collapse" for="c-41379213">[-]</label><label class="expand" for="c-41379213">[1 more]</label></div><br/><div class="children"><div class="content">It made me laugh. Maybe they pulled random people from the hallway who had never seen the original Doom (or any FPS), or maybe only selected people who wore glasses and forgot them at their desk.</div><br/></div></div><div id="41377557" class="c"><input type="checkbox" id="c-41377557" checked=""/><div class="controls bullet"><span class="by">meheleventyone</span><span>|</span><a href="#41377364">parent</a><span>|</span><a href="#41379213">prev</a><span>|</span><a href="#41376371">next</a><span>|</span><label class="collapse" for="c-41377557">[-]</label><label class="expand" for="c-41377557">[7 more]</label></div><br/><div class="children"><div class="content">It&#x27;s telling IMO that they only want people opinions based on our notoriously faulty memories rather than sitting comparable situations next to one another in the game and simulation then analyzing them. Several things jump out watching the example video.</div><br/><div id="41378486" class="c"><input type="checkbox" id="c-41378486" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41377364">root</a><span>|</span><a href="#41377557">parent</a><span>|</span><a href="#41376371">next</a><span>|</span><label class="collapse" for="c-41378486">[-]</label><label class="expand" for="c-41378486">[6 more]</label></div><br/><div class="children"><div class="content">&gt;rather than sitting comparable situations next to one another in the game and simulation then analyzing them.<p>That&#x27;s literally how the human rating was setup if you read the paper.</div><br/><div id="41379368" class="c"><input type="checkbox" id="c-41379368" checked=""/><div class="controls bullet"><span class="by">meheleventyone</span><span>|</span><a href="#41377364">root</a><span>|</span><a href="#41378486">parent</a><span>|</span><a href="#41376371">next</a><span>|</span><label class="collapse" for="c-41379368">[-]</label><label class="expand" for="c-41379368">[5 more]</label></div><br/><div class="children"><div class="content">I think you misunderstand me. I don&#x27;t mean a snap evaluation and deciding between two very-short competing videos which is what the participants were doing. I mean doing an actual analysis of how well the simulation matches the ground truth of the game.<p>What I&#x27;d posit is that it&#x27;s not actually a very good replication of the game but very good a replicating short clips that almost look like the game and the short time horizons are deliberately chosen because the authors know the model lacks coherence beyond that.</div><br/><div id="41379745" class="c"><input type="checkbox" id="c-41379745" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41377364">root</a><span>|</span><a href="#41379368">parent</a><span>|</span><a href="#41376371">next</a><span>|</span><label class="collapse" for="c-41379745">[-]</label><label class="expand" for="c-41379745">[4 more]</label></div><br/><div class="children"><div class="content">&gt;I mean doing an actual analysis of how well the simulation matches the ground truth of the game.<p>Do you mean the PSNR and LPIPS metrics used in paper?</div><br/><div id="41380430" class="c"><input type="checkbox" id="c-41380430" checked=""/><div class="controls bullet"><span class="by">meheleventyone</span><span>|</span><a href="#41377364">root</a><span>|</span><a href="#41379745">parent</a><span>|</span><a href="#41376371">next</a><span>|</span><label class="collapse" for="c-41380430">[-]</label><label class="expand" for="c-41380430">[3 more]</label></div><br/><div class="children"><div class="content">No, I think I&#x27;ve been pretty clear that I&#x27;m interested in how mechanically sound the simulation is. Also those measures are over an even shorter duration so even less relevant to how coherent it is at real game scales.</div><br/><div id="41380610" class="c"><input type="checkbox" id="c-41380610" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#41377364">root</a><span>|</span><a href="#41380430">parent</a><span>|</span><a href="#41376371">next</a><span>|</span><label class="collapse" for="c-41380610">[-]</label><label class="expand" for="c-41380610">[2 more]</label></div><br/><div class="children"><div class="content">How should this be concretely evaluated and measured? A vibe check?</div><br/><div id="41382968" class="c"><input type="checkbox" id="c-41382968" checked=""/><div class="controls bullet"><span class="by">meheleventyone</span><span>|</span><a href="#41377364">root</a><span>|</span><a href="#41380610">parent</a><span>|</span><a href="#41376371">next</a><span>|</span><label class="collapse" for="c-41382968">[-]</label><label class="expand" for="c-41382968">[1 more]</label></div><br/><div class="children"><div class="content">I think the studies evaluation using very short video and humans is much more of a vibe check than what I’ve suggested.<p>Off the top of my head DOOM is open source so it should be reasonable to setup repeatable scenarios and use some frames from the game to create a starting scenario for the simulation that is the same. Then the input from the player of the game could be used to drive the simulated version. You could go further and instrument events occurring in the game for direct comparison to the simulation. I’d be interested in setting a baseline for playtime of the level in question and using sessions of around that length as an ultimate test.<p>There are some on obvious mechanical deficiencies seen in the videos they’ve published. One that really stood out to me was the damage taken when in the radioactive slime. So I don’t think the analysis would need to particularly deep to find differences.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="41376371" class="c"><input type="checkbox" id="c-41376371" checked=""/><div class="controls bullet"><span class="by">mo_42</span><span>|</span><a href="#41377364">prev</a><span>|</span><a href="#41377172">next</a><span>|</span><label class="collapse" for="c-41376371">[-]</label><label class="expand" for="c-41376371">[4 more]</label></div><br/><div class="children"><div class="content">An implementation of the game engine in the model itself is theoretically the most accurate solution for predicting the next frame.<p>I&#x27;m wondering when people will apply this to other areas like the real world. Would it learn the game engine of the universe (ie physics)?</div><br/><div id="41376579" class="c"><input type="checkbox" id="c-41376579" checked=""/><div class="controls bullet"><span class="by">radarsat1</span><span>|</span><a href="#41376371">parent</a><span>|</span><a href="#41376728">next</a><span>|</span><label class="collapse" for="c-41376579">[-]</label><label class="expand" for="c-41376579">[1 more]</label></div><br/><div class="children"><div class="content">There has definitely been research for simulating physics based on observation, especially in fluid dynamics but also for rigid body motion and collision. It&#x27;s important for robotics applications actually. You can bet people will be applying this technique in those contexts.<p>I think for real world application one challenge is going to be the &quot;action&quot; signal which is a necessary component of the conditioning signal that makes the simulation reactive. In video games you can just record the buttons, but for real world scenarios you need difficult and intrusive sensor setups for recording force signals.<p>(Again for robotics though maybe it&#x27;s enough to record the motor commands, just that you can&#x27;t easily record the &quot;motor commands&quot; for humans, for example)</div><br/></div></div><div id="41376728" class="c"><input type="checkbox" id="c-41376728" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#41376371">parent</a><span>|</span><a href="#41376579">prev</a><span>|</span><a href="#41377172">next</a><span>|</span><label class="collapse" for="c-41376728">[-]</label><label class="expand" for="c-41376728">[2 more]</label></div><br/><div class="children"><div class="content">A popular theory in neuroscience is that this is what the brain does:<p><a href="https:&#x2F;&#x2F;slatestarcodex.com&#x2F;2017&#x2F;09&#x2F;05&#x2F;book-review-surfing-uncertainty&#x2F;" rel="nofollow">https:&#x2F;&#x2F;slatestarcodex.com&#x2F;2017&#x2F;09&#x2F;05&#x2F;book-review-surfing-un...</a><p>It&#x27;s called predictive coding. By trying to predict sensory stimuli, the brain creates a simplified model of the world, including common sense physics. Yann LeCun says that this is a major key to AGI. Another one is effective planning.<p>But while current predictive models (autoregressive LLMs) work well on text, they don&#x27;t work well on video data, because of the large outcome space. In an LLM, text prediction boils down to a probability distribution over a few thousand possible next tokens, while there are several orders of magnitude more possible &quot;next frames&quot; in a video. Diffusion models work better on video data, but they are not inherently predictive like causal LLMs. Apparently this new Doom model made some progress on that front though.</div><br/><div id="41378167" class="c"><input type="checkbox" id="c-41378167" checked=""/><div class="controls bullet"><span class="by">ccozan</span><span>|</span><a href="#41376371">root</a><span>|</span><a href="#41376728">parent</a><span>|</span><a href="#41377172">next</a><span>|</span><label class="collapse" for="c-41378167">[-]</label><label class="expand" for="c-41378167">[1 more]</label></div><br/><div class="children"><div class="content">Howver, this is due how we actually digitize video. From a human point a view, looking in my room reduces the load to the _objects_ in the room and everyhing else is just noise ( like the color of the wall could be just a single item to remember, while otherwise in the digital world, it needs to remember all the pixels )</div><br/></div></div></div></div></div></div><div id="41377172" class="c"><input type="checkbox" id="c-41377172" checked=""/><div class="controls bullet"><span class="by">lIl-IIIl</span><span>|</span><a href="#41376371">prev</a><span>|</span><a href="#41378058">next</a><span>|</span><label class="collapse" for="c-41377172">[-]</label><label class="expand" for="c-41377172">[4 more]</label></div><br/><div class="children"><div class="content">How does it know how many times it needs to shoot the zombie before it dies?<p>Most enemies have enough hit points to survive the first shot. If the model is only trained on the previous frame, it doesn&#x27;t know how many times the enemy was already shot at.<p>From the video it seems like it is probability based - they may die right away or it might take way longer than it should.<p>I love how the player&#x27;s health goes down when he stands in the radioactive green water.<p>In Doom the enemies fight with each other if they accidentally incur &quot;friendly fire&quot;. It would be interesting to see it play out in this version.</div><br/><div id="41377566" class="c"><input type="checkbox" id="c-41377566" checked=""/><div class="controls bullet"><span class="by">meheleventyone</span><span>|</span><a href="#41377172">parent</a><span>|</span><a href="#41377202">next</a><span>|</span><label class="collapse" for="c-41377566">[-]</label><label class="expand" for="c-41377566">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I love how the player&#x27;s health goes down when he stands in the radioactive green water.<p>This is one of the bits that was weird to me, it doesn&#x27;t work correctly. In the real game you take damage at a consistent rate, in the video the player doesn&#x27;t and whether the player takes damage or not seems highly dependent on some factor that isn&#x27;t whether or not the player is in the radioactive slime. My thought is that its learnt something else that correlates poorly.</div><br/></div></div><div id="41377202" class="c"><input type="checkbox" id="c-41377202" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#41377172">parent</a><span>|</span><a href="#41377566">prev</a><span>|</span><a href="#41377702">next</a><span>|</span><label class="collapse" for="c-41377202">[-]</label><label class="expand" for="c-41377202">[1 more]</label></div><br/><div class="children"><div class="content">It gets a number of previous frame<i>s</i> as input I think.</div><br/></div></div><div id="41377702" class="c"><input type="checkbox" id="c-41377702" checked=""/><div class="controls bullet"><span class="by">lupusreal</span><span>|</span><a href="#41377172">parent</a><span>|</span><a href="#41377202">prev</a><span>|</span><a href="#41378058">next</a><span>|</span><label class="collapse" for="c-41377702">[-]</label><label class="expand" for="c-41377702">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>In Doom the enemies fight with each other if they accidentally incur &quot;friendly fire&quot;. It would be interesting to see it play out in this version.</i><p>They trained this thing on bot gameplay, so I bet it does poorly when advanced strategies like deliberately inducing mob infighting are employed (the bots probably didn&#x27;t do that a lot, of at all.)</div><br/></div></div></div></div><div id="41378058" class="c"><input type="checkbox" id="c-41378058" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#41377172">prev</a><span>|</span><a href="#41377357">next</a><span>|</span><label class="collapse" for="c-41378058">[-]</label><label class="expand" for="c-41378058">[3 more]</label></div><br/><div class="children"><div class="content">Has this model actually learned the 3d space of the game? Is it possible to break the camera free and roam around the map freely and view it from different angles?<p>I noticed a few hallucinations e.g. when it picked green jacket from a corner, walking back it generated another corner. Therefore I don&#x27;t think it has any clue about the 3D world of the game at all.</div><br/><div id="41378164" class="c"><input type="checkbox" id="c-41378164" checked=""/><div class="controls bullet"><span class="by">kqr</span><span>|</span><a href="#41378058">parent</a><span>|</span><a href="#41381344">next</a><span>|</span><label class="collapse" for="c-41378164">[-]</label><label class="expand" for="c-41378164">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Is it possible to break the camera free and roam around the map freely and view it from different angles?<p>I would assume only if the training data contained this type of imagery, which it did not. The training data (from what I understand) consisted only of input+video of actual gameplay, so that is what the model is trained to mimick.<p>This is like a dog that has been trained to form English words – what&#x27;s impressive is not that it does it well, but that it does it at all.</div><br/></div></div><div id="41381344" class="c"><input type="checkbox" id="c-41381344" checked=""/><div class="controls bullet"><span class="by">Sohcahtoa82</span><span>|</span><a href="#41378058">parent</a><span>|</span><a href="#41378164">prev</a><span>|</span><a href="#41377357">next</a><span>|</span><label class="collapse" for="c-41381344">[-]</label><label class="expand" for="c-41381344">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Therefore I don&#x27;t think it has any clue about the 3D world of the game at all.<p>AI models don&#x27;t &quot;know&quot; things at all.<p>At best, they&#x27;re just very fuzzy predictors.  In this case, given the last couple frames of video and a user input, it predicts the next frame.<p>It has zero knowledge of the game world, game rules, interactions, etc.  It&#x27;s merely a mapping of [pixels, input] -&gt; pixels.</div><br/></div></div></div></div><div id="41377357" class="c"><input type="checkbox" id="c-41377357" checked=""/><div class="controls bullet"><span class="by">icoder</span><span>|</span><a href="#41378058">prev</a><span>|</span><a href="#41376855">next</a><span>|</span><label class="collapse" for="c-41377357">[-]</label><label class="expand" for="c-41377357">[3 more]</label></div><br/><div class="children"><div class="content">This is impressive. But at the same time, it can&#x27;t count. We see this every time, and I understand why it happens, but it is still intriguing. We are so close or in some ways even way beyond, and yet at the same time so extremely far away, from &#x27;our&#x27; intelligence.<p>(I say it can&#x27;t count because there are numerous examples where the bullet count glitches, it goes right impressively often, but still, counting, being up or down, is something computers have been able to do flawlessly basically since forever)<p>(It is the same with chess, where the LLM models are becoming really good, yet sometimes make mistakes that even my 8yo niece would not make)</div><br/><div id="41377504" class="c"><input type="checkbox" id="c-41377504" checked=""/><div class="controls bullet"><span class="by">marci</span><span>|</span><a href="#41377357">parent</a><span>|</span><a href="#41376855">next</a><span>|</span><label class="collapse" for="c-41377504">[-]</label><label class="expand" for="c-41377504">[2 more]</label></div><br/><div class="children"><div class="content">&#x27;our&#x27; intelligence may not be the best thing we can make. It would be like trying to only make planes that flaps wings or trucks with legs. A bit like using a llm to do multiplication. Not the best tool. Biomimcry is great for inspiration, but shouldn&#x27;t be a 1-to-1 copy, especialy in different scale and medium.</div><br/><div id="41377826" class="c"><input type="checkbox" id="c-41377826" checked=""/><div class="controls bullet"><span class="by">icoder</span><span>|</span><a href="#41377357">root</a><span>|</span><a href="#41377504">parent</a><span>|</span><a href="#41376855">next</a><span>|</span><label class="collapse" for="c-41377826">[-]</label><label class="expand" for="c-41377826">[1 more]</label></div><br/><div class="children"><div class="content">Sure, although I still think a system with less of a contrast between how well it performs &#x27;modally&#x27; and how bad it performs incidentally, would be more practical.<p>What I wonder is whether LLM&#x27;s will inherently always have this dichotomy and we need something &#x27;extra&#x27; (reasoning, attention or something les biomimicried), or whether this will eventually resolves itself (to an acceptable extend) when they improve even further.</div><br/></div></div></div></div></div></div><div id="41376855" class="c"><input type="checkbox" id="c-41376855" checked=""/><div class="controls bullet"><span class="by">nolist_policy</span><span>|</span><a href="#41377357">prev</a><span>|</span><a href="#41375765">next</a><span>|</span><label class="collapse" for="c-41376855">[-]</label><label class="expand" for="c-41376855">[2 more]</label></div><br/><div class="children"><div class="content">Makes me wonder... If you stand still in front of a door so all past observations only contain that door, will the model teleport you to another level when opening the door?</div><br/><div id="41377124" class="c"><input type="checkbox" id="c-41377124" checked=""/><div class="controls bullet"><span class="by">zbendefy</span><span>|</span><a href="#41376855">parent</a><span>|</span><a href="#41375765">next</a><span>|</span><label class="collapse" for="c-41377124">[-]</label><label class="expand" for="c-41377124">[1 more]</label></div><br/><div class="children"><div class="content">I think some state is also being given (or if its not, it could be given) to the network, like 3d world position&#x2F;orientation of the player, that could help the neural network anchor the player in the world.</div><br/></div></div></div></div><div id="41375765" class="c"><input type="checkbox" id="c-41375765" checked=""/><div class="controls bullet"><span class="by">masterspy7</span><span>|</span><a href="#41376855">prev</a><span>|</span><a href="#41387985">next</a><span>|</span><label class="collapse" for="c-41375765">[-]</label><label class="expand" for="c-41375765">[5 more]</label></div><br/><div class="children"><div class="content">There&#x27;s been a ton of work to generate assets for games using AI: 3d models, textures, code, etc. None of that may even be necessary with a generative game engine like this! If you could scale this up, train on all games in existence, etc. I bet some interesting things would happen</div><br/><div id="41375916" class="c"><input type="checkbox" id="c-41375916" checked=""/><div class="controls bullet"><span class="by">rererereferred</span><span>|</span><a href="#41375765">parent</a><span>|</span><a href="#41375987">next</a><span>|</span><label class="collapse" for="c-41375916">[-]</label><label class="expand" for="c-41375916">[3 more]</label></div><br/><div class="children"><div class="content">But can you grab what this Ai has learned and generate the 3d models, maps and code to turn it into an actual game that can run on a user&#x27;s PC? That would be amazing.</div><br/><div id="41376292" class="c"><input type="checkbox" id="c-41376292" checked=""/><div class="controls bullet"><span class="by">passion__desire</span><span>|</span><a href="#41375765">root</a><span>|</span><a href="#41375916">parent</a><span>|</span><a href="#41376614">next</a><span>|</span><label class="collapse" for="c-41376292">[-]</label><label class="expand" for="c-41376292">[1 more]</label></div><br/><div class="children"><div class="content">Jensen Huang&#x27;s vision that future games will be generated rather than rendered is coming true.</div><br/></div></div><div id="41376614" class="c"><input type="checkbox" id="c-41376614" checked=""/><div class="controls bullet"><span class="by">kleiba</span><span>|</span><a href="#41375765">root</a><span>|</span><a href="#41375916">parent</a><span>|</span><a href="#41376292">prev</a><span>|</span><a href="#41375987">next</a><span>|</span><label class="collapse" for="c-41376614">[-]</label><label class="expand" for="c-41376614">[1 more]</label></div><br/><div class="children"><div class="content">What would be the point? This model has been trained on an existing game, so turning it back into assets, maps, and code would just give you a copy of the original game you started with. I suppose you could create variations of it then... but:<p>You don&#x27;t even need to do all of that - this trained model <i>already is</i> the game, i.e., it&#x27;s interactive, you can play the game.</div><br/></div></div></div></div><div id="41375987" class="c"><input type="checkbox" id="c-41375987" checked=""/><div class="controls bullet"><span class="by">whamlastxmas</span><span>|</span><a href="#41375765">parent</a><span>|</span><a href="#41375916">prev</a><span>|</span><a href="#41387985">next</a><span>|</span><label class="collapse" for="c-41375987">[-]</label><label class="expand" for="c-41375987">[1 more]</label></div><br/><div class="children"><div class="content">I would absolutely love if they could take this demo, add a new door that isn’t in the original, and see what it generates behind that door</div><br/></div></div></div></div><div id="41387985" class="c"><input type="checkbox" id="c-41387985" checked=""/><div class="controls bullet"><span class="by">nicman23</span><span>|</span><a href="#41375765">prev</a><span>|</span><a href="#41382658">next</a><span>|</span><label class="collapse" for="c-41387985">[-]</label><label class="expand" for="c-41387985">[1 more]</label></div><br/><div class="children"><div class="content">what i want from something like this is a mix. a model that can infinitely &quot;zoom&quot; into an object&#x27;s texture which even if not perfect it would be fine and a model that would create 3d geometry from bump maps &#x2F; normals</div><br/></div></div><div id="41382658" class="c"><input type="checkbox" id="c-41382658" checked=""/><div class="controls bullet"><span class="by">throwthrowuknow</span><span>|</span><a href="#41387985">prev</a><span>|</span><a href="#41379321">next</a><span>|</span><label class="collapse" for="c-41382658">[-]</label><label class="expand" for="c-41382658">[1 more]</label></div><br/><div class="children"><div class="content">Several thoughts for future work:<p>1. Continue training on all of the games that used the Doom engine to see if it is capable of creating new graphics, enemies, weapons, etc. I think you would need to embed more details for this perhaps information about what is present in the current level so that you could prompt it to produce a new level from some combination.<p>2. Could embedding information from the map view or a raytrace of the surroundings of the player position help with consistency? I suppose the model would need to predict this information as the neural simulation progressed.<p>3. Can this technique be applied to generating videos with consistent subjects and environments by training on a camera view of a 3D scene and embedding the camera position and the position and animation states of objects and avatars within the scene?<p>4. What would the result of training on a variety of game engines and games with different mechanics and inputs be? The space of possible actions is limited by the available keys on a keyboard or buttons on a controller but the labelling of the characteristics of each game may prove a challenge if you wanted to be able to prompt for specific details.</div><br/></div></div><div id="41379321" class="c"><input type="checkbox" id="c-41379321" checked=""/><div class="controls bullet"><span class="by">dabochen</span><span>|</span><a href="#41382658">prev</a><span>|</span><a href="#41376166">next</a><span>|</span><label class="collapse" for="c-41379321">[-]</label><label class="expand" for="c-41379321">[2 more]</label></div><br/><div class="children"><div class="content">So there is no interactivity, but the generated content is not the exact view in the training data, is this the correct understanding?<p>If so, is it more like imagination&#x2F;hallucination rather than rendering?</div><br/><div id="41380712" class="c"><input type="checkbox" id="c-41380712" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#41379321">parent</a><span>|</span><a href="#41376166">next</a><span>|</span><label class="collapse" for="c-41380712">[-]</label><label class="expand" for="c-41380712">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s conditioned on previous frames AND player actions so it&#x27;s interactive.</div><br/></div></div></div></div><div id="41376166" class="c"><input type="checkbox" id="c-41376166" checked=""/><div class="controls bullet"><span class="by">arduinomancer</span><span>|</span><a href="#41379321">prev</a><span>|</span><a href="#41375704">next</a><span>|</span><label class="collapse" for="c-41376166">[-]</label><label class="expand" for="c-41376166">[7 more]</label></div><br/><div class="children"><div class="content">How does the model “remember” the whole state of the world?<p>Like if I kill an enemy in some room and walk all the way across the map and come back, would the body still be there?</div><br/><div id="41376180" class="c"><input type="checkbox" id="c-41376180" checked=""/><div class="controls bullet"><span class="by">a_e_k</span><span>|</span><a href="#41376166">parent</a><span>|</span><a href="#41376724">next</a><span>|</span><label class="collapse" for="c-41376180">[-]</label><label class="expand" for="c-41376180">[2 more]</label></div><br/><div class="children"><div class="content">Watch closely in the videos and you&#x27;ll see that enemies often respawn when offscreen and sometimes when onscreen.  Destroyed barrels come back, ammo count and health fluctuates weirdly, etc.  It&#x27;s still impressive, but its not perfect in that regard.</div><br/><div id="41376197" class="c"><input type="checkbox" id="c-41376197" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#41376166">root</a><span>|</span><a href="#41376180">parent</a><span>|</span><a href="#41376724">next</a><span>|</span><label class="collapse" for="c-41376197">[-]</label><label class="expand" for="c-41376197">[1 more]</label></div><br/><div class="children"><div class="content">Not unlike in (human) dreams.</div><br/></div></div></div></div><div id="41376724" class="c"><input type="checkbox" id="c-41376724" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#41376166">parent</a><span>|</span><a href="#41376180">prev</a><span>|</span><a href="#41376347">next</a><span>|</span><label class="collapse" for="c-41376724">[-]</label><label class="expand" for="c-41376724">[3 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t even remember the state of the game you look at. Doors spawning right in front of you, particle effects turning into enemies mid flight etc, so just regular gen AI issues.<p>Edit: Can see this in the first 10 seconds of the first video under &quot;Full Gameplay Videos&quot;, stairs turning to corridor turning to closed door for no reason without looking away.</div><br/><div id="41377707" class="c"><input type="checkbox" id="c-41377707" checked=""/><div class="controls bullet"><span class="by">csmattryder</span><span>|</span><a href="#41376166">root</a><span>|</span><a href="#41376724">parent</a><span>|</span><a href="#41378293">next</a><span>|</span><label class="collapse" for="c-41377707">[-]</label><label class="expand" for="c-41377707">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s also the case in the video (0:59) where the player jumps into the poison but doesn&#x27;t take damage for a few seconds then takes two doses back-to-back - they should&#x27;ve taken a hit of damage every ~500-1000ms(?)<p>Guessing the model hasn&#x27;t been taught enough about that, because most people don&#x27;t jump into hazards.</div><br/></div></div></div></div><div id="41376347" class="c"><input type="checkbox" id="c-41376347" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#41376166">parent</a><span>|</span><a href="#41376724">prev</a><span>|</span><a href="#41375704">next</a><span>|</span><label class="collapse" for="c-41376347">[-]</label><label class="expand" for="c-41376347">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t. You need to put the world state in the input (the &quot;prompt&quot;, even it doesn&#x27;t look like prompt in this case). Whatever not in the prompt is lost.</div><br/></div></div></div></div><div id="41375704" class="c"><input type="checkbox" id="c-41375704" checked=""/><div class="controls bullet"><span class="by">ravetcofx</span><span>|</span><a href="#41376166">prev</a><span>|</span><a href="#41376927">next</a><span>|</span><label class="collapse" for="c-41375704">[-]</label><label class="expand" for="c-41375704">[1 more]</label></div><br/><div class="children"><div class="content">There is going to be a flood of these dreamlike &quot;games&quot; in the next few years. This feels likes a bit of a breakthrough in the engineering of these systems.</div><br/></div></div><div id="41376927" class="c"><input type="checkbox" id="c-41376927" checked=""/><div class="controls bullet"><span class="by">jamilton</span><span>|</span><a href="#41375704">prev</a><span>|</span><a href="#41381892">next</a><span>|</span><label class="collapse" for="c-41376927">[-]</label><label class="expand" for="c-41376927">[2 more]</label></div><br/><div class="children"><div class="content">I wonder if the MineRL (<a href="https:&#x2F;&#x2F;www.ijcai.org&#x2F;proceedings&#x2F;2019&#x2F;0339.pdf" rel="nofollow">https:&#x2F;&#x2F;www.ijcai.org&#x2F;proceedings&#x2F;2019&#x2F;0339.pdf</a> and minerl.io) dataset would be sufficient to reproduce this work with Minecraft.<p>Any other similar existing datasets?<p>A really goofy way I can think of to get a bunch of data would be to get videos from youtube and try to detect keyboard sounds to determine what keys they&#x27;re pressing.</div><br/><div id="41377550" class="c"><input type="checkbox" id="c-41377550" checked=""/><div class="controls bullet"><span class="by">jamilton</span><span>|</span><a href="#41376927">parent</a><span>|</span><a href="#41381892">next</a><span>|</span><label class="collapse" for="c-41377550">[-]</label><label class="expand" for="c-41377550">[1 more]</label></div><br/><div class="children"><div class="content">Although ideally a follow up work would be something where there won’t be any potential legal trouble with releasing the complete model so people can play it.<p>A similar approach but with a game where the exact input is obvious and unambiguous from the graphics alone so that you can use unannotated data might work. You’d just have to create a model to create the action annotations. I’m not sure what the point would be, but it sounds like it’d be interesting.</div><br/></div></div></div></div><div id="41381892" class="c"><input type="checkbox" id="c-41381892" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#41376927">prev</a><span>|</span><a href="#41387170">next</a><span>|</span><label class="collapse" for="c-41381892">[-]</label><label class="expand" for="c-41381892">[1 more]</label></div><br/><div class="children"><div class="content">NVIDIA did something similar with GANs in 2020 [1], except users <i>could</i> actually play those games (unlike in this diffusion work which just plays back simulated video). Sentdex later adapted this to play GTA with a really cool demo [2].<p>[1] <a href="https:&#x2F;&#x2F;research.nvidia.com&#x2F;labs&#x2F;toronto-ai&#x2F;gameGAN&#x2F;" rel="nofollow">https:&#x2F;&#x2F;research.nvidia.com&#x2F;labs&#x2F;toronto-ai&#x2F;gameGAN&#x2F;</a><p>[2] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=udPY5rQVoW0" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=udPY5rQVoW0</a></div><br/></div></div><div id="41387170" class="c"><input type="checkbox" id="c-41387170" checked=""/><div class="controls bullet"><span class="by">maxglute</span><span>|</span><a href="#41381892">prev</a><span>|</span><label class="collapse" for="c-41387170">[-]</label><label class="expand" for="c-41387170">[1 more]</label></div><br/><div class="children"><div class="content">RL tetris effect hallucination.<p>Wish there was 1000s of hours of hardcore henry to train. Maybe scrape gopro war cams.</div><br/></div></div></div></div></div></div></div></body></html>