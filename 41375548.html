<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1724835687388" as="style"/><link rel="stylesheet" href="styles.css?v=1724835687388"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://gamengen.github.io">Diffusion Models Are Real-Time Game Engines</a> <span class="domain">(<a href="https://gamengen.github.io">gamengen.github.io</a>)</span></div><div class="subtext"><span>jmorgan</span> | <span>119 comments</span></div><br/><div><div id="41375657" class="c"><input type="checkbox" id="c-41375657" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#41375719">next</a><span>|</span><label class="collapse" for="c-41375657">[-]</label><label class="expand" for="c-41375657">[2 more]</label></div><br/><div class="children"><div class="content">So, this is surprising. Apparently there’s more cause, effect, and sequencing in diffusion models than what I expected, which would be roughly ‘none’. Google here uses SD 1.4, as the core of the diffusion model, which is a nice reminder that open models are useful to even giant cloud monopolies.<p>The two main things of note I took away from the summary were: 1) they got infinite training data using agents playing doom (makes sense), and 2) they added Gaussian noise to source frames and rewarded the agent for ‘correcting’ sequential frames back, and said this was critical to get long range stable ‘rendering’ out of the model.<p>That last is intriguing — they explain the intuition as teaching the model to do error correction &#x2F; guide it to be stable.<p>Finally, I wonder if this model would be easy to fine tune for ‘photo realistic’ &#x2F; ray traced restyling — I’d be super curious to see how hard it would be to get a ‘nicer’ rendering out of this model, treating it as a doom foundation model of sorts.<p>Anyway, a fun idea that worked! Love those.</div><br/><div id="41376240" class="c"><input type="checkbox" id="c-41376240" checked=""/><div class="controls bullet"><span class="by">refibrillator</span><span>|</span><a href="#41375657">parent</a><span>|</span><a href="#41375719">next</a><span>|</span><label class="collapse" for="c-41376240">[-]</label><label class="expand" for="c-41376240">[1 more]</label></div><br/><div class="children"><div class="content">Just want to clarify a couple possible misconceptions:<p>The diffusion model doesn’t maintain any state itself, though its weights may encode some notion of cause&#x2F;effect. It just renders one frame at a time (after all it’s a text to image model, not text to video). Instead of text, the previous states and frames are provided as inputs to the model to predict the next frame.<p>Noise is added to the previous frames before being passed into the SD model, so the RL agents were not involved with “correcting” it.<p>De-noising objectives are widespread in ML, intuitively it forces a predictive model to leverage context, ie surrounding frames&#x2F;words&#x2F;etc.<p>In this case it helps prevent auto-regressive drift due to the accumulation of small errors from the randomness inherent in generative diffusion models. Figure 4 shows such drift happening when a player is standing still.</div><br/></div></div></div></div><div id="41375719" class="c"><input type="checkbox" id="c-41375719" checked=""/><div class="controls bullet"><span class="by">wkcheng</span><span>|</span><a href="#41375657">prev</a><span>|</span><a href="#41377398">next</a><span>|</span><label class="collapse" for="c-41375719">[-]</label><label class="expand" for="c-41375719">[17 more]</label></div><br/><div class="children"><div class="content">It&#x27;s insane that that this works, and that it works fast enough to render at 20 fps.  It seems like they almost made a cross between a diffusion model and an RNN, since they had to encode the previous frames and actions and feed it into the model at each step.<p>Abstractly, it&#x27;s like the model is dreaming of a game that it played a lot of, and real time inputs just change the state of the dream.  It makes me wonder if humans are just next moment prediction machines, with just a little bit more memory built in.</div><br/><div id="41376075" class="c"><input type="checkbox" id="c-41376075" checked=""/><div class="controls bullet"><span class="by">lokimedes</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41376410">next</a><span>|</span><label class="collapse" for="c-41376075">[-]</label><label class="expand" for="c-41376075">[5 more]</label></div><br/><div class="children"><div class="content">It makes good sense for humans to have this ability. If we flip the argument, and see the next frame as a hypothesis for what is expected as the outcome of the current frame, then comparing this &quot;hypothesis&quot; with what is sensed makes it easier to process the differences, rather than the totality of the sensory input.<p>As Richard Dawkins recently put it in a podcast[1], our genes are great prediction machines, as their continued survival rests on it. Being able to generate a visual prediction fits perfectly with the amount of resources we dedicate to sight.<p>If that is the case, what does aphantasia tell us?<p>[1] <a href="https:&#x2F;&#x2F;podcasts.apple.com&#x2F;dk&#x2F;podcast&#x2F;into-the-impossible-with-brian-keating&#x2F;id1169885840?i=1000665776221" rel="nofollow">https:&#x2F;&#x2F;podcasts.apple.com&#x2F;dk&#x2F;podcast&#x2F;into-the-impossible-wi...</a></div><br/><div id="41377407" class="c"><input type="checkbox" id="c-41377407" checked=""/><div class="controls bullet"><span class="by">dbspin</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376075">parent</a><span>|</span><a href="#41377318">next</a><span>|</span><label class="collapse" for="c-41377407">[-]</label><label class="expand" for="c-41377407">[1 more]</label></div><br/><div class="children"><div class="content">Worth noting that aphantasia doesn&#x27;t necessarily extend to dreams. Anecdotally - I have pretty severe aphantasia (I can conjure milisecond glimpses of barely tangible imagery that I can&#x27;t quite perceive before it&#x27;s gone - but only since learning that visualisation wasn&#x27;t a linguistic metaphor). I can&#x27;t really simulate object rotation. I can&#x27;t really &#x27;picture&#x27; how things will look before they&#x27;re drawn &#x2F; built etc. However I often have highly vivid dream imagery. I also have excellent recognition of faces and places (e.g.: can&#x27;t get lost in a new city). So there clearly is a lot of preconscious visualisation and image matching going on in some aphantasia cases, even where the explicit visual screen is all but absent.</div><br/></div></div><div id="41377318" class="c"><input type="checkbox" id="c-41377318" checked=""/><div class="controls bullet"><span class="by">jonplackett</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376075">parent</a><span>|</span><a href="#41377407">prev</a><span>|</span><a href="#41376733">next</a><span>|</span><label class="collapse" for="c-41377318">[-]</label><label class="expand" for="c-41377318">[2 more]</label></div><br/><div class="children"><div class="content">What’s the aphantasia link? I’ve got aphantasia. I’m convinced though that the bit of my brain that should be making images is used for letting me ‘see’ how things are connected together very easily in my head. Also I still love games like Pictionary and can somehow draw things onto paper than I don’t really know what they look like in my head. It’s often a surprise when pen meets paper.</div><br/><div id="41377399" class="c"><input type="checkbox" id="c-41377399" checked=""/><div class="controls bullet"><span class="by">lokimedes</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41377318">parent</a><span>|</span><a href="#41376733">next</a><span>|</span><label class="collapse" for="c-41377399">[-]</label><label class="expand" for="c-41377399">[1 more]</label></div><br/><div class="children"><div class="content">I agree, it is my own experience as well. Craig Venter In one of his books also credit this way of representing knowledge as abstractions as his strength in inventing new concepts.<p>The link may be that we actually see differences between “frames”, rather than the frames directly. That in itself would imply that a from of sub-visual representation is being processed by our brain. For aphantasia, it could be that we work directly on this representation instead of recalling imagery through the visual system.<p>I’m no where near qualified to speak of this with certainty, but it seems plausible to me.</div><br/></div></div></div></div><div id="41376733" class="c"><input type="checkbox" id="c-41376733" checked=""/><div class="controls bullet"><span class="by">quickestpoint</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376075">parent</a><span>|</span><a href="#41377318">prev</a><span>|</span><a href="#41376410">next</a><span>|</span><label class="collapse" for="c-41376733">[-]</label><label class="expand" for="c-41376733">[1 more]</label></div><br/><div class="children"><div class="content">As Richard Dawkins theorized, would be more accurate and less LLM like :)</div><br/></div></div></div></div><div id="41376410" class="c"><input type="checkbox" id="c-41376410" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41376075">prev</a><span>|</span><a href="#41376538">next</a><span>|</span><label class="collapse" for="c-41376410">[-]</label><label class="expand" for="c-41376410">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It makes me wonder if humans are just next moment prediction machines, with just a little bit more memory built in.<p>Yup, see <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Predictive_coding" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Predictive_coding</a></div><br/><div id="41376704" class="c"><input type="checkbox" id="c-41376704" checked=""/><div class="controls bullet"><span class="by">quickestpoint</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376410">parent</a><span>|</span><a href="#41376538">next</a><span>|</span><label class="collapse" for="c-41376704">[-]</label><label class="expand" for="c-41376704">[1 more]</label></div><br/><div class="children"><div class="content">Umm, that’s a theory.</div><br/></div></div></div></div><div id="41376538" class="c"><input type="checkbox" id="c-41376538" checked=""/><div class="controls bullet"><span class="by">richard___</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41376410">prev</a><span>|</span><a href="#41376072">next</a><span>|</span><label class="collapse" for="c-41376538">[-]</label><label class="expand" for="c-41376538">[1 more]</label></div><br/><div class="children"><div class="content">Did they take in the entire history as context?</div><br/></div></div><div id="41376072" class="c"><input type="checkbox" id="c-41376072" checked=""/><div class="controls bullet"><span class="by">Teever</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41376538">prev</a><span>|</span><a href="#41376153">next</a><span>|</span><label class="collapse" for="c-41376072">[-]</label><label class="expand" for="c-41376072">[1 more]</label></div><br/><div class="children"><div class="content">Also recursion and nested virtualization.  We can dream about dreaming and imagine different scenarios, some completely fictional or simply possible future scenarios all while doing day to day stuff.</div><br/></div></div><div id="41376153" class="c"><input type="checkbox" id="c-41376153" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41375719">parent</a><span>|</span><a href="#41376072">prev</a><span>|</span><a href="#41377398">next</a><span>|</span><label class="collapse" for="c-41376153">[-]</label><label class="expand" for="c-41376153">[7 more]</label></div><br/><div class="children"><div class="content">Image is 2D. Video is 3D. The mathematical extension is obvious. In this case, low resolution 2D (pixels), and the third dimension is just frame rate (discrete steps). So rather simple.</div><br/><div id="41376177" class="c"><input type="checkbox" id="c-41376177" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376153">parent</a><span>|</span><a href="#41376997">next</a><span>|</span><label class="collapse" for="c-41376177">[-]</label><label class="expand" for="c-41376177">[5 more]</label></div><br/><div class="children"><div class="content">This is not &quot;just&quot; video, however. It&#x27;s interactive in real time. Sure, you can say that playing is simply video with some extra parameters thrown in to encode player input, but still.</div><br/><div id="41376270" class="c"><input type="checkbox" id="c-41376270" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376177">parent</a><span>|</span><a href="#41376997">next</a><span>|</span><label class="collapse" for="c-41376270">[-]</label><label class="expand" for="c-41376270">[4 more]</label></div><br/><div class="children"><div class="content">It is just video. There are no external interactions.<p>Heck, it is far simpler than video, because the point of view and frame is fixed.</div><br/><div id="41376391" class="c"><input type="checkbox" id="c-41376391" checked=""/><div class="controls bullet"><span class="by">SeanAnderson</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376270">parent</a><span>|</span><a href="#41376359">next</a><span>|</span><label class="collapse" for="c-41376391">[-]</label><label class="expand" for="c-41376391">[2 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re mistaken. The abstract says it&#x27;s interactive, &quot;We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction&quot;<p>Further - &quot;a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions.&quot; specifically &quot;and actions&quot;<p>User input is being fed into this system and subsequent frames take that into account. The user is &quot;actually&quot; firing a gun.</div><br/><div id="41376679" class="c"><input type="checkbox" id="c-41376679" checked=""/><div class="controls bullet"><span class="by">nopakos</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376391">parent</a><span>|</span><a href="#41376359">next</a><span>|</span><label class="collapse" for="c-41376679">[-]</label><label class="expand" for="c-41376679">[1 more]</label></div><br/><div class="children"><div class="content">Maybe it&#x27;s so advanced, it knows the players&#x27; next moves, so it is a video!</div><br/></div></div></div></div><div id="41376359" class="c"><input type="checkbox" id="c-41376359" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376270">parent</a><span>|</span><a href="#41376391">prev</a><span>|</span><a href="#41376997">next</a><span>|</span><label class="collapse" for="c-41376359">[-]</label><label class="expand" for="c-41376359">[1 more]</label></div><br/><div class="children"><div class="content">?<p>I highly suggest you to read the paper briefly before commenting on the topic. The whole point is that it&#x27;s not just generating a video.</div><br/></div></div></div></div></div></div><div id="41376997" class="c"><input type="checkbox" id="c-41376997" checked=""/><div class="controls bullet"><span class="by">InDubioProRubio</span><span>|</span><a href="#41375719">root</a><span>|</span><a href="#41376153">parent</a><span>|</span><a href="#41376177">prev</a><span>|</span><a href="#41377398">next</a><span>|</span><label class="collapse" for="c-41376997">[-]</label><label class="expand" for="c-41376997">[1 more]</label></div><br/><div class="children"><div class="content">Video is also higher resolution, as the pixels flip for the high resolution world by moving through it. Swivelling your head without glasses, even the blurry world contains more information in the curve of pixelchange.</div><br/></div></div></div></div></div></div><div id="41377398" class="c"><input type="checkbox" id="c-41377398" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41375719">prev</a><span>|</span><a href="#41375667">next</a><span>|</span><label class="collapse" for="c-41377398">[-]</label><label class="expand" for="c-41377398">[2 more]</label></div><br/><div class="children"><div class="content">Doom system requirements:<p><pre><code>  - 4 MB RAM
  - 12 MB disk space 
</code></pre>
Stable diffusion v1<p><pre><code>  &gt; 860M UNet and CLIP ViT-L&#x2F;14 (540M)
  Checkpoint size:
    4.27 Gb 
    7.7 GB (full EMA)
  Running on a TPU-v5e
    Peak compute per chip (bf16)  197 TFLOPs
    Peak compute per chip (Int8)  393 TFLOPs
    HBM2 capacity and bandwidth  16 GB, 819 GBps
    Interchip Interconnect BW  1600 Gbps
</code></pre>
This is quite impressive, especially considering the speed. But there&#x27;s still a ton of room for improvement. It seems it didn&#x27;t even memorize the game despite having the capacity to do so hundreds of times over. So we definitely have lots of room for optimization methods. Though who knows how such things would affect existing tech since the goal here is to memorize.<p>What&#x27;s also interesting about this work is it&#x27;s basically saying you can rip a game if you&#x27;re willing to &quot;play&quot; (automate) it enough times and spend a lot more on storage and compute. I&#x27;m curious what the comparison in cost and time would be if you hired an engineer to reverse engineer Doom (how much prior knowledge do they get considering pertained models and visdoom environment. Was doom source code in T5? And which vit checkpoint was used? I can&#x27;t keep track of Google vit checkpoints).<p>I would love to see the checkpoint of this model. I think people would find some really interesting stuff taking it apart.<p>- <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;gaming&#x2F;comments&#x2F;a4yi5t&#x2F;original_doom_system_requirements_from_my_25_year&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;gaming&#x2F;comments&#x2F;a4yi5t&#x2F;original_doo...</a><p>- <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;CompVis&#x2F;stable-diffusion-v-1-4-original&#x2F;tree&#x2F;main" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;CompVis&#x2F;stable-diffusion-v-1-4-origin...</a><p>- <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;tpu&#x2F;docs&#x2F;v5e" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;tpu&#x2F;docs&#x2F;v5e</a><p>- <a href="https:&#x2F;&#x2F;github.com&#x2F;Farama-Foundation&#x2F;ViZDoom">https:&#x2F;&#x2F;github.com&#x2F;Farama-Foundation&#x2F;ViZDoom</a><p>- <a href="https:&#x2F;&#x2F;zdoom.org&#x2F;index" rel="nofollow">https:&#x2F;&#x2F;zdoom.org&#x2F;index</a></div><br/><div id="41377447" class="c"><input type="checkbox" id="c-41377447" checked=""/><div class="controls bullet"><span class="by">snickmy</span><span>|</span><a href="#41377398">parent</a><span>|</span><a href="#41375667">next</a><span>|</span><label class="collapse" for="c-41377447">[-]</label><label class="expand" for="c-41377447">[1 more]</label></div><br/><div class="children"><div class="content">Those are valid points, but irrelevant for the context of this research.<p>Yes, the computational cost is ridicolous compared to the original game, and yes, it lacks basic things like pre-computing, storing, etc. That said, you could assume that all that can be either done at the margin of this discovery OR over time will naturally improve OR will become less important as a blocker.<p>The fact that you can model a sequence of frames with such contextual awareness without explictly having to encode it, is the real breakthrough here. Both from a pure gaming standpoint, but on simulation in general.</div><br/></div></div></div></div><div id="41375667" class="c"><input type="checkbox" id="c-41375667" checked=""/><div class="controls bullet"><span class="by">zzanz</span><span>|</span><a href="#41377398">prev</a><span>|</span><a href="#41375811">next</a><span>|</span><label class="collapse" for="c-41375667">[-]</label><label class="expand" for="c-41375667">[14 more]</label></div><br/><div class="children"><div class="content">The quest to run doom on everything continues. 
Technically speaking, isn&#x27;t this the greatest possible anti-Doom, the Doom with the highest possible hardware requirement? 
I just find it funny that on a linear scale of hardware specification, Doom now finds itself on both ends.</div><br/><div id="41375717" class="c"><input type="checkbox" id="c-41375717" checked=""/><div class="controls bullet"><span class="by">fngjdflmdflg</span><span>|</span><a href="#41375667">parent</a><span>|</span><a href="#41376587">next</a><span>|</span><label class="collapse" for="c-41375717">[-]</label><label class="expand" for="c-41375717">[5 more]</label></div><br/><div class="children"><div class="content">&gt;Technically speaking, isn&#x27;t this the greatest possible anti-Doom<p>When I read this part I thought you were going to say because you&#x27;re technically <i>not</i> running Doom at all. That is, instead of running Doom without Doom&#x27;s original hardware&#x2F;software environment (by porting it), you&#x27;re running Doom without Doom itself.</div><br/><div id="41375977" class="c"><input type="checkbox" id="c-41375977" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375717">parent</a><span>|</span><a href="#41375899">next</a><span>|</span><label class="collapse" for="c-41375977">[-]</label><label class="expand" for="c-41375977">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s <i>dreaming</i> Doom.</div><br/></div></div><div id="41375899" class="c"><input type="checkbox" id="c-41375899" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375717">parent</a><span>|</span><a href="#41375977">prev</a><span>|</span><a href="#41376587">next</a><span>|</span><label class="collapse" for="c-41375899">[-]</label><label class="expand" for="c-41375899">[3 more]</label></div><br/><div class="children"><div class="content">Pierre Menard, Author of Doom.</div><br/><div id="41376168" class="c"><input type="checkbox" id="c-41376168" checked=""/><div class="controls bullet"><span class="by">el_memorioso</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375899">parent</a><span>|</span><a href="#41376563">next</a><span>|</span><label class="collapse" for="c-41376168">[-]</label><label class="expand" for="c-41376168">[1 more]</label></div><br/><div class="children"><div class="content">I applaud your erudition.</div><br/></div></div><div id="41376563" class="c"><input type="checkbox" id="c-41376563" checked=""/><div class="controls bullet"><span class="by">1attice</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375899">parent</a><span>|</span><a href="#41376168">prev</a><span>|</span><a href="#41376587">next</a><span>|</span><label class="collapse" for="c-41376563">[-]</label><label class="expand" for="c-41376563">[1 more]</label></div><br/><div class="children"><div class="content">that took a moment, thank you</div><br/></div></div></div></div></div></div><div id="41376587" class="c"><input type="checkbox" id="c-41376587" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#41375667">parent</a><span>|</span><a href="#41375717">prev</a><span>|</span><a href="#41376099">next</a><span>|</span><label class="collapse" for="c-41376587">[-]</label><label class="expand" for="c-41376587">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the Doom with the highest possible hardware requirement?<p>Isn&#x27;t that possible by setting arbitrarily high goals for ray-cast rendering?</div><br/></div></div><div id="41376099" class="c"><input type="checkbox" id="c-41376099" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#41375667">parent</a><span>|</span><a href="#41376587">prev</a><span>|</span><a href="#41375915">next</a><span>|</span><label class="collapse" for="c-41376099">[-]</label><label class="expand" for="c-41376099">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the No-Doom.</div><br/><div id="41377084" class="c"><input type="checkbox" id="c-41377084" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41376099">parent</a><span>|</span><a href="#41375915">next</a><span>|</span><label class="collapse" for="c-41377084">[-]</label><label class="expand" for="c-41377084">[3 more]</label></div><br/><div class="children"><div class="content">Undoom?</div><br/><div id="41377386" class="c"><input type="checkbox" id="c-41377386" checked=""/><div class="controls bullet"><span class="by">jeffhuys</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41377084">parent</a><span>|</span><a href="#41377220">next</a><span>|</span><label class="collapse" for="c-41377386">[-]</label><label class="expand" for="c-41377386">[1 more]</label></div><br/><div class="children"><div class="content">Bliss</div><br/></div></div><div id="41377220" class="c"><input type="checkbox" id="c-41377220" checked=""/><div class="controls bullet"><span class="by">riwsky</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41377084">parent</a><span>|</span><a href="#41377386">prev</a><span>|</span><a href="#41375915">next</a><span>|</span><label class="collapse" for="c-41377220">[-]</label><label class="expand" for="c-41377220">[1 more]</label></div><br/><div class="children"><div class="content">It’s a mood.</div><br/></div></div></div></div></div></div><div id="41375915" class="c"><input type="checkbox" id="c-41375915" checked=""/><div class="controls bullet"><span class="by">x-complexity</span><span>|</span><a href="#41375667">parent</a><span>|</span><a href="#41376099">prev</a><span>|</span><a href="#41375811">next</a><span>|</span><label class="collapse" for="c-41375915">[-]</label><label class="expand" for="c-41375915">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Technically speaking, isn&#x27;t this the greatest possible anti-Doom, the Doom with the highest possible hardware requirement?<p>Not really? The greatest anti-Doom would be an infinite nest of these types of models predicting models predicting Doom at the very end of the chain.<p>The next step of anti-Doom would be a model generating the model, generating the Doom output.</div><br/><div id="41376328" class="c"><input type="checkbox" id="c-41376328" checked=""/><div class="controls bullet"><span class="by">nurettin</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375915">parent</a><span>|</span><a href="#41376399">next</a><span>|</span><label class="collapse" for="c-41376328">[-]</label><label class="expand" for="c-41376328">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this technically a model (training step) generating a model (a neural network) generating Doom output?</div><br/></div></div><div id="41376399" class="c"><input type="checkbox" id="c-41376399" checked=""/><div class="controls bullet"><span class="by">yuchi</span><span>|</span><a href="#41375667">root</a><span>|</span><a href="#41375915">parent</a><span>|</span><a href="#41376328">prev</a><span>|</span><a href="#41375811">next</a><span>|</span><label class="collapse" for="c-41376399">[-]</label><label class="expand" for="c-41376399">[1 more]</label></div><br/><div class="children"><div class="content">“…now it can <i>implement</i> Doom!”</div><br/></div></div></div></div></div></div><div id="41375811" class="c"><input type="checkbox" id="c-41375811" checked=""/><div class="controls bullet"><span class="by">refibrillator</span><span>|</span><a href="#41375667">prev</a><span>|</span><a href="#41376434">next</a><span>|</span><label class="collapse" for="c-41375811">[-]</label><label class="expand" for="c-41375811">[17 more]</label></div><br/><div class="children"><div class="content">There is no text conditioning provided to the SD model because they removed it, but one can imagine a near future where text prompts are enough to create a fun new game!<p>Yes they had to use RL to learn what DOOM looks like and how it works, but this doesn’t necessarily pose a chicken vs egg problem. In the same way that LLMs can write a novel story, despite only being trained on existing text.<p>IMO one of the biggest challenges with this approach will be open world games with essentially an infinite number of possible states. The paper mentions that they had trouble getting RL agents to completely explore every nook and corner of DOOM. Factorio or Dwarf Fortress probably won’t be simulated anytime soon…I think.</div><br/><div id="41376053" class="c"><input type="checkbox" id="c-41376053" checked=""/><div class="controls bullet"><span class="by">mlsu</span><span>|</span><a href="#41375811">parent</a><span>|</span><a href="#41376167">next</a><span>|</span><label class="collapse" for="c-41376053">[-]</label><label class="expand" for="c-41376053">[8 more]</label></div><br/><div class="children"><div class="content">With enough computation, your neural net weights would converge to some very compressed latent representation of the source code of DOOM. Maybe smaller even than the source code itself? Someone in the field could probably correct me on that.<p>At which point, you effectively would be interpolating in latent space through the source code to actually &quot;render&quot; the game. You&#x27;d have an entire latent space computer, with an engine, assets, textures, a software renderer.<p>With a sufficiently powerful computer, one could imagine what interpolating in this latent space between, say Factorio and TF2 (2 of my favorites). And tweaking this latent space to your liking by conditioning it on any number of gameplay aspects.<p>This future comes very quickly for subsets of the pipeline, like the very end stage of rendering -- DLSS is already in production, for example. Maybe Nvidia&#x27;s revenue wraps back to gaming once again, as we all become bolted into a neural metaverse.<p>God I love that they chose DOOM.</div><br/><div id="41377468" class="c"><input type="checkbox" id="c-41377468" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376053">parent</a><span>|</span><a href="#41376699">next</a><span>|</span><label class="collapse" for="c-41377468">[-]</label><label class="expand" for="c-41377468">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; With enough computation, your neural net weights would converge to some very compressed latent representation of the source code of DOOM. 
</code></pre>
You and I have very different definitions of compression<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41377398">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41377398</a><p><pre><code>  &gt; Someone in the field could probably correct me on that.
</code></pre>
^__^</div><br/></div></div><div id="41376699" class="c"><input type="checkbox" id="c-41376699" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376053">parent</a><span>|</span><a href="#41377468">prev</a><span>|</span><a href="#41376118">next</a><span>|</span><label class="collapse" for="c-41376699">[-]</label><label class="expand" for="c-41376699">[1 more]</label></div><br/><div class="children"><div class="content">&gt; With enough computation, your neural net weights would converge to some very compressed latent representation of the source code of DOOM. Maybe smaller even than the source code itself? Someone in the field could probably correct me on that.<p>Neural nets are not guaranteed to converge to anything even remotely optimal, so no that isn&#x27;t how it works. Also even though neural nets can approximate any function they usually can&#x27;t do it in a time or space efficient manner, resulting in much larger programs than the human written code.</div><br/></div></div><div id="41376118" class="c"><input type="checkbox" id="c-41376118" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376053">parent</a><span>|</span><a href="#41376699">prev</a><span>|</span><a href="#41376148">next</a><span>|</span><label class="collapse" for="c-41376118">[-]</label><label class="expand" for="c-41376118">[4 more]</label></div><br/><div class="children"><div class="content">The source code lacks information required to render the game. Textures for example.</div><br/><div id="41376581" class="c"><input type="checkbox" id="c-41376581" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376118">parent</a><span>|</span><a href="#41376929">next</a><span>|</span><label class="collapse" for="c-41376581">[-]</label><label class="expand" for="c-41376581">[2 more]</label></div><br/><div class="children"><div class="content">Obviously assets would get encoded too, in some form. Not necessarily corresponding to the original bitmaps, if the game does some consistent post-processing, the encoded thing would more likely be (equivalent to) the post-processed state.</div><br/><div id="41376701" class="c"><input type="checkbox" id="c-41376701" checked=""/><div class="controls bullet"><span class="by">hoseja</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376581">parent</a><span>|</span><a href="#41376929">next</a><span>|</span><label class="collapse" for="c-41376701">[-]</label><label class="expand" for="c-41376701">[1 more]</label></div><br/><div class="children"><div class="content">Finally, the AI superoptimizing compiler.</div><br/></div></div></div></div><div id="41376929" class="c"><input type="checkbox" id="c-41376929" checked=""/><div class="controls bullet"><span class="by">mistercheph</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376118">parent</a><span>|</span><a href="#41376581">prev</a><span>|</span><a href="#41376148">next</a><span>|</span><label class="collapse" for="c-41376929">[-]</label><label class="expand" for="c-41376929">[1 more]</label></div><br/><div class="children"><div class="content">That’s just an artifact of the language we use to describe an implementation detail, in the sense GP means it, the data payload bits are not essentially distinct from the executable instruction bits</div><br/></div></div></div></div><div id="41376148" class="c"><input type="checkbox" id="c-41376148" checked=""/><div class="controls bullet"><span class="by">electrondood</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376053">parent</a><span>|</span><a href="#41376118">prev</a><span>|</span><a href="#41376167">next</a><span>|</span><label class="collapse" for="c-41376148">[-]</label><label class="expand" for="c-41376148">[1 more]</label></div><br/><div class="children"><div class="content">The Holographic Principle is the idea that our universe is a projection of a higher dimensional space, which sounds an awful lot like the total simulation of an interactive environment, encoded in the parameter space of a neural network.<p>The first thing I thought when I saw this was: couldn&#x27;t my immediate experience be exactly the same thing? Including the illusion of a separate main character to whom events are occurring?</div><br/></div></div></div></div><div id="41376167" class="c"><input type="checkbox" id="c-41376167" checked=""/><div class="controls bullet"><span class="by">basch</span><span>|</span><a href="#41375811">parent</a><span>|</span><a href="#41376053">prev</a><span>|</span><a href="#41376879">next</a><span>|</span><label class="collapse" for="c-41376167">[-]</label><label class="expand" for="c-41376167">[1 more]</label></div><br/><div class="children"><div class="content">Similarly, you could run a very very simple game engine, that outputs little more than a low resolution wireframe, and upscale it.  Put all of the effort into game mechanics and none into visual quality.<p>I would expect something in this realm to be a little better at not being visually inconsistent when you look away and look back.  A red monster turning into a blue friendly etc.</div><br/></div></div><div id="41376879" class="c"><input type="checkbox" id="c-41376879" checked=""/><div class="controls bullet"><span class="by">SomewhatLikely</span><span>|</span><a href="#41375811">parent</a><span>|</span><a href="#41376167">prev</a><span>|</span><a href="#41377286">next</a><span>|</span><label class="collapse" for="c-41376879">[-]</label><label class="expand" for="c-41376879">[1 more]</label></div><br/><div class="children"><div class="content">Video games are gonna be wild in the near future.  You could have one person talking to a model producing something that&#x27;s on par with a AAA title from today.  Imagine the 2d sidescroller boom on Steam but with immersive photorealistic 3d games with hyper-realistic physics (water flow, fire that spreads, tornados) and full deformability and buildability because the model is pretrained with real world videos.  Your game is just a &quot;style&quot; that tweaks some priors on look, settings, and story.</div><br/></div></div><div id="41377286" class="c"><input type="checkbox" id="c-41377286" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#41375811">parent</a><span>|</span><a href="#41376879">prev</a><span>|</span><a href="#41377334">next</a><span>|</span><label class="collapse" for="c-41377286">[-]</label><label class="expand" for="c-41377286">[1 more]</label></div><br/><div class="children"><div class="content">This got me thinking. Anyone tried using SD or similar to create graphics for the old classic text adventure games?</div><br/></div></div><div id="41376260" class="c"><input type="checkbox" id="c-41376260" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41375811">parent</a><span>|</span><a href="#41377334">prev</a><span>|</span><a href="#41376586">next</a><span>|</span><label class="collapse" for="c-41376260">[-]</label><label class="expand" for="c-41376260">[1 more]</label></div><br/><div class="children"><div class="content">&gt; where text prompts are enough to create a fun new game!<p>Not really. This is a reproduction of the first level of Doom. Nothing original is being created.</div><br/></div></div><div id="41376586" class="c"><input type="checkbox" id="c-41376586" checked=""/><div class="controls bullet"><span class="by">radarsat1</span><span>|</span><a href="#41375811">parent</a><span>|</span><a href="#41376260">prev</a><span>|</span><a href="#41376707">next</a><span>|</span><label class="collapse" for="c-41376586">[-]</label><label class="expand" for="c-41376586">[1 more]</label></div><br/><div class="children"><div class="content">Most games are conditioned on text, it&#x27;s just that we call it &quot;source code&quot; :).<p>(Jk of course I know what you mean, but you can seriously see text prompts as compressed forms of programming that leverage the model&#x27;s prior knowledge)</div><br/></div></div><div id="41376707" class="c"><input type="checkbox" id="c-41376707" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#41375811">parent</a><span>|</span><a href="#41376586">prev</a><span>|</span><a href="#41376434">next</a><span>|</span><label class="collapse" for="c-41376707">[-]</label><label class="expand" for="c-41376707">[2 more]</label></div><br/><div class="children"><div class="content">&gt; one can imagine a near future where text prompts are enough to create a fun new game<p>Sit down and write down a text prompt for a &quot;fun new game&quot;. You can start with something relatively simple like a Mario-like platformer.<p>By page 300, when you&#x27;re about halfway through describing what you mean, you might understand why this is wishful thinking</div><br/><div id="41377372" class="c"><input type="checkbox" id="c-41377372" checked=""/><div class="controls bullet"><span class="by">reverius42</span><span>|</span><a href="#41375811">root</a><span>|</span><a href="#41376707">parent</a><span>|</span><a href="#41376434">next</a><span>|</span><label class="collapse" for="c-41377372">[-]</label><label class="expand" for="c-41377372">[1 more]</label></div><br/><div class="children"><div class="content">If it can be trained on (many) existing games, then it might work similarly to how you don&#x27;t need to describe every possible detail of a generated image in order to get something that looks like what you&#x27;re asking for (and looks like a plausible image for the underspecified parts).</div><br/></div></div></div></div></div></div><div id="41376434" class="c"><input type="checkbox" id="c-41376434" checked=""/><div class="controls bullet"><span class="by">helloplanets</span><span>|</span><a href="#41375811">prev</a><span>|</span><a href="#41377357">next</a><span>|</span><label class="collapse" for="c-41376434">[-]</label><label class="expand" for="c-41376434">[3 more]</label></div><br/><div class="children"><div class="content">So, any given sequence of inputs is rebuilt into a corresponding image, twenty times per second. I wonder how separate the game logic and the generated graphics are in the fully trained model.<p>Given a sufficient enough separation between these two, couldn&#x27;t you basically boil the game&#x2F;input logic down to an abstract game template? Meaning, you could just output a hash that corresponds to a specific combination of inputs, and then treat the resulting mapping as a representation of a specific game&#x27;s inner workings.<p>To make it less abstract, you could save some small enough snapshot of the game engine&#x27;s state for all given input sequences. This could make it much less dependent to what&#x27;s recorded off of the agents&#x27; screens. And you could map the objects that appear in the saved states to graphics, in a separate step.<p>I imagine this whole system would work especially well for games that only update when player input is given: Games like Myst, Sokoban, etc.</div><br/><div id="41376668" class="c"><input type="checkbox" id="c-41376668" checked=""/><div class="controls bullet"><span class="by">toppy</span><span>|</span><a href="#41376434">parent</a><span>|</span><a href="#41377357">next</a><span>|</span><label class="collapse" for="c-41376668">[-]</label><label class="expand" for="c-41376668">[2 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;ve just encoded the title of the paper</div><br/></div></div></div></div><div id="41377357" class="c"><input type="checkbox" id="c-41377357" checked=""/><div class="controls bullet"><span class="by">icoder</span><span>|</span><a href="#41376434">prev</a><span>|</span><a href="#41377191">next</a><span>|</span><label class="collapse" for="c-41377357">[-]</label><label class="expand" for="c-41377357">[1 more]</label></div><br/><div class="children"><div class="content">This is impressive. But at the same time, it can&#x27;t count. We see this every time, and I understand why it happens, but it is still intriguing. We are so close or in some ways even way beyond, and yet at the same time so extremely far away, from &#x27;our&#x27; intelligence.<p>(I say it can&#x27;t count because there are numerous examples where the bullet count glitches, it goes right impressively often, but still, counting, being up or down, is something computers have been able to do flawlessly basically since forever)<p>(It is the same with chess, where the LLM models are becoming really good, yet sometimes make mistakes that even my 8yo niece would not make)</div><br/></div></div><div id="41377191" class="c"><input type="checkbox" id="c-41377191" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#41377357">prev</a><span>|</span><a href="#41376371">next</a><span>|</span><label class="collapse" for="c-41377191">[-]</label><label class="expand" for="c-41377191">[3 more]</label></div><br/><div class="children"><div class="content">What I understand is the folloeing: If this works so well, why didn&#x27;t we have good video generation much earlier? After diffusion models were seen to work the most obvious thing to do was to generate the next frame based on previous framrs but... it took 1-2 years for good video models to appear. For example compare Sora generating minecraft video versus this method generating minecraft video. Say in both cases the player is standing on a meadow with fee inputs and watching some pigs. In the Sora video you&#x27;d expect the typical glitched to appear, like erratic, sliding movement, overlapping legs, multiplication of pigs etc. Would these glitches not appear in the GameNGen video? Why?</div><br/><div id="41377208" class="c"><input type="checkbox" id="c-41377208" checked=""/><div class="controls bullet"><span class="by">Closi</span><span>|</span><a href="#41377191">parent</a><span>|</span><a href="#41377210">next</a><span>|</span><label class="collapse" for="c-41377208">[-]</label><label class="expand" for="c-41377208">[1 more]</label></div><br/><div class="children"><div class="content">Because video is much more difficult than images (it&#x27;s lots of images that have to be consistent across time, with motion following laws of physics etc), and this is much more limited in terms of scope than pure arbitrary video generation.</div><br/></div></div><div id="41377210" class="c"><input type="checkbox" id="c-41377210" checked=""/><div class="controls bullet"><span class="by">pantalaimon</span><span>|</span><a href="#41377191">parent</a><span>|</span><a href="#41377208">prev</a><span>|</span><a href="#41376371">next</a><span>|</span><label class="collapse" for="c-41377210">[-]</label><label class="expand" for="c-41377210">[1 more]</label></div><br/><div class="children"><div class="content">I would have thought it is much easier to generate huge amounts of game footage for training, but as I understand this is not what was done here.</div><br/></div></div></div></div><div id="41376371" class="c"><input type="checkbox" id="c-41376371" checked=""/><div class="controls bullet"><span class="by">mo_42</span><span>|</span><a href="#41377191">prev</a><span>|</span><a href="#41377364">next</a><span>|</span><label class="collapse" for="c-41376371">[-]</label><label class="expand" for="c-41376371">[3 more]</label></div><br/><div class="children"><div class="content">An implementation of the game engine in the model itself is theoretically the most accurate solution for predicting the next frame.<p>I&#x27;m wondering when people will apply this to other areas like the real world. Would it learn the game engine of the universe (ie physics)?</div><br/><div id="41376579" class="c"><input type="checkbox" id="c-41376579" checked=""/><div class="controls bullet"><span class="by">radarsat1</span><span>|</span><a href="#41376371">parent</a><span>|</span><a href="#41376728">next</a><span>|</span><label class="collapse" for="c-41376579">[-]</label><label class="expand" for="c-41376579">[1 more]</label></div><br/><div class="children"><div class="content">There has definitely been research for simulating physics based on observation, especially in fluid dynamics but also for rigid body motion and collision. It&#x27;s important for robotics applications actually. You can bet people will be applying this technique in those contexts.<p>I think for real world application one challenge is going to be the &quot;action&quot; signal which is a necessary component of the conditioning signal that makes the simulation reactive. In video games you can just record the buttons, but for real world scenarios you need difficult and intrusive sensor setups for recording force signals.<p>(Again for robotics though maybe it&#x27;s enough to record the motor commands, just that you can&#x27;t easily record the &quot;motor commands&quot; for humans, for example)</div><br/></div></div><div id="41376728" class="c"><input type="checkbox" id="c-41376728" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#41376371">parent</a><span>|</span><a href="#41376579">prev</a><span>|</span><a href="#41377364">next</a><span>|</span><label class="collapse" for="c-41376728">[-]</label><label class="expand" for="c-41376728">[1 more]</label></div><br/><div class="children"><div class="content">A popular theory in neuroscience is that this is what the brain does:<p><a href="https:&#x2F;&#x2F;slatestarcodex.com&#x2F;2017&#x2F;09&#x2F;05&#x2F;book-review-surfing-uncertainty&#x2F;" rel="nofollow">https:&#x2F;&#x2F;slatestarcodex.com&#x2F;2017&#x2F;09&#x2F;05&#x2F;book-review-surfing-un...</a><p>It&#x27;s called predictive coding. By trying to predict sensory stimuli, the brain creates a simplified model of the world, including common sense physics. Yann LeCun says that this is a major key to AGI. Another one is effective planning.<p>But while current predictive models (autoregressive LLMs) work well on text, they don&#x27;t work well on video data, because of the large outcome space. In an LLM, text prediction boils down to a probability distribution over a few thousand possible next tokens, while there are several orders of magnitude more possible &quot;next frames&quot; in a video. Diffusion models work better on video data, but they are not inherently predictive like causal LLMs. Apparently this new Doom model made some progress on that front though.</div><br/></div></div></div></div><div id="41377364" class="c"><input type="checkbox" id="c-41377364" checked=""/><div class="controls bullet"><span class="by">panki27</span><span>|</span><a href="#41376371">prev</a><span>|</span><a href="#41375678">next</a><span>|</span><label class="collapse" for="c-41377364">[-]</label><label class="expand" for="c-41377364">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation.<p>I can hardly believe this claim, anyone who has played some amount of DOOM before should notice the viewport and textures not &quot;feeling right&quot;, or the usually static objects moving slightly.</div><br/></div></div><div id="41375678" class="c"><input type="checkbox" id="c-41375678" checked=""/><div class="controls bullet"><span class="by">danjl</span><span>|</span><a href="#41377364">prev</a><span>|</span><a href="#41376855">next</a><span>|</span><label class="collapse" for="c-41375678">[-]</label><label class="expand" for="c-41375678">[17 more]</label></div><br/><div class="children"><div class="content">So, diffusion models are game engines as long as you already built the game? You need the game to train the model. Chicken. Egg?</div><br/><div id="41375727" class="c"><input type="checkbox" id="c-41375727" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#41375678">parent</a><span>|</span><a href="#41376119">next</a><span>|</span><label class="collapse" for="c-41375727">[-]</label><label class="expand" for="c-41375727">[10 more]</label></div><br/><div class="children"><div class="content">here are some ideas:<p>- you could build a non-real-time version of the game engine and use the neural net as a real-time approximation<p>- you could edit videos shot in real life to have huds or whatever and train the neural net to simulate reality rather than doom.  (this paper used 900 million frames which i think is about a year of video if it&#x27;s 30fps, but maybe algorithmic improvements can cut the training requirements down) and a year of video isn&#x27;t actually all that much—like, maybe you could recruit 500 people to play paintball while wearing gopro cameras with accelerometers and gyros on their heads and paintball guns, so that you could get a year of video in a weekend?</div><br/><div id="41375825" class="c"><input type="checkbox" id="c-41375825" checked=""/><div class="controls bullet"><span class="by">w_for_wumbo</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375727">parent</a><span>|</span><a href="#41376426">next</a><span>|</span><label class="collapse" for="c-41375825">[-]</label><label class="expand" for="c-41375825">[6 more]</label></div><br/><div class="children"><div class="content">That feels like the endgame of video game generation.
You select an art style, a video and the type of game you&#x27;d like to play.
The game is then generated in real-time responding to each action with respect to the existing rule engine.<p>I imagine a game like that could get so convincing in its details and immersiveness that one could forget they&#x27;re playing a game.</div><br/><div id="41376673" class="c"><input type="checkbox" id="c-41376673" checked=""/><div class="controls bullet"><span class="by">aithrowaway1987</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375825">parent</a><span>|</span><a href="#41376033">next</a><span>|</span><label class="collapse" for="c-41376673">[-]</label><label class="expand" for="c-41376673">[1 more]</label></div><br/><div class="children"><div class="content">Have you ever played a video game? This is unbelievably depressing. This is a future where games like Slay the Spire, with a unique art style and innovative gameplay simply are not being made.<p>Not to mention this childish nonsense about &quot;forget they&#x27;re playing a game,&quot; as if every game needs to be lifelike VR and there&#x27;s no room for stylization or imagination. I am worried for the future that people think they want these things.</div><br/></div></div><div id="41376033" class="c"><input type="checkbox" id="c-41376033" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375825">parent</a><span>|</span><a href="#41376673">prev</a><span>|</span><a href="#41375903">next</a><span>|</span><label class="collapse" for="c-41376033">[-]</label><label class="expand" for="c-41376033">[1 more]</label></div><br/><div class="children"><div class="content">IIRC, both <i>2001</i>(1968) and <i>Solaris</i>(1972) depict that kind of things as part of alien euthanasia process, not as happy endings</div><br/></div></div><div id="41375903" class="c"><input type="checkbox" id="c-41375903" checked=""/><div class="controls bullet"><span class="by">omegaworks</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375825">parent</a><span>|</span><a href="#41376033">prev</a><span>|</span><a href="#41375892">next</a><span>|</span><label class="collapse" for="c-41375903">[-]</label><label class="expand" for="c-41375903">[1 more]</label></div><br/><div class="children"><div class="content">EXISTENZ IS PAUSED!</div><br/></div></div><div id="41375892" class="c"><input type="checkbox" id="c-41375892" checked=""/><div class="controls bullet"><span class="by">THBC</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375825">parent</a><span>|</span><a href="#41375903">prev</a><span>|</span><a href="#41376722">next</a><span>|</span><label class="collapse" for="c-41375892">[-]</label><label class="expand" for="c-41375892">[1 more]</label></div><br/><div class="children"><div class="content">Holodeck is just around the corner</div><br/></div></div><div id="41376722" class="c"><input type="checkbox" id="c-41376722" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375825">parent</a><span>|</span><a href="#41375892">prev</a><span>|</span><a href="#41376426">next</a><span>|</span><label class="collapse" for="c-41376722">[-]</label><label class="expand" for="c-41376722">[1 more]</label></div><br/><div class="children"><div class="content">There are thousands of games that mimic each other, and only a handful of them are any good.<p>What makes you think a mechanical &quot;predict next frame based on existing games&quot; will be any good?</div><br/></div></div></div></div><div id="41376426" class="c"><input type="checkbox" id="c-41376426" checked=""/><div class="controls bullet"><span class="by">injidup</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375727">parent</a><span>|</span><a href="#41375825">prev</a><span>|</span><a href="#41376527">next</a><span>|</span><label class="collapse" for="c-41376426">[-]</label><label class="expand" for="c-41376426">[1 more]</label></div><br/><div class="children"><div class="content">Why games? I will train it on 1 years worth of me attending Microsoft teams meetings. Then I will go surfing.</div><br/></div></div><div id="41376527" class="c"><input type="checkbox" id="c-41376527" checked=""/><div class="controls bullet"><span class="by">qznc</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41375727">parent</a><span>|</span><a href="#41376426">prev</a><span>|</span><a href="#41376140">next</a><span>|</span><label class="collapse" for="c-41376527">[-]</label><label class="expand" for="c-41376527">[1 more]</label></div><br/><div class="children"><div class="content">The Cloud Gaming platforms could record things for training data.</div><br/></div></div></div></div><div id="41376119" class="c"><input type="checkbox" id="c-41376119" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#41375678">parent</a><span>|</span><a href="#41375727">prev</a><span>|</span><a href="#41375689">next</a><span>|</span><label class="collapse" for="c-41376119">[-]</label><label class="expand" for="c-41376119">[2 more]</label></div><br/><div class="children"><div class="content">If you train it on multiple games then you could produce new games that have never existed before, in the same way image generation models can produce new images that have never existed before.</div><br/><div id="41377416" class="c"><input type="checkbox" id="c-41377416" checked=""/><div class="controls bullet"><span class="by">lewhoo</span><span>|</span><a href="#41375678">root</a><span>|</span><a href="#41376119">parent</a><span>|</span><a href="#41375689">next</a><span>|</span><label class="collapse" for="c-41377416">[-]</label><label class="expand" for="c-41377416">[1 more]</label></div><br/><div class="children"><div class="content">From what I understand that could make the engine much less stable. The key here is repetitiveness.</div><br/></div></div></div></div><div id="41375689" class="c"><input type="checkbox" id="c-41375689" checked=""/><div class="controls bullet"><span class="by">billconan</span><span>|</span><a href="#41375678">parent</a><span>|</span><a href="#41376119">prev</a><span>|</span><a href="#41376286">next</a><span>|</span><label class="collapse" for="c-41375689">[-]</label><label class="expand" for="c-41375689">[1 more]</label></div><br/><div class="children"><div class="content">maybe the next step is adding text guidance and generating non-existing games.</div><br/></div></div><div id="41376286" class="c"><input type="checkbox" id="c-41376286" checked=""/><div class="controls bullet"><span class="by">passion__desire</span><span>|</span><a href="#41375678">parent</a><span>|</span><a href="#41375689">prev</a><span>|</span><a href="#41376182">next</a><span>|</span><label class="collapse" for="c-41376286">[-]</label><label class="expand" for="c-41376286">[1 more]</label></div><br/><div class="children"><div class="content">Maybe, in future, techniques of Scientific Machine Learning  which can encode physics and other known laws into a model would form a base model. And then other models on top could just fine tune aspects to customise a game.</div><br/></div></div><div id="41376182" class="c"><input type="checkbox" id="c-41376182" checked=""/><div class="controls bullet"><span class="by">attilakun</span><span>|</span><a href="#41375678">parent</a><span>|</span><a href="#41376286">prev</a><span>|</span><a href="#41376163">next</a><span>|</span><label class="collapse" for="c-41376182">[-]</label><label class="expand" for="c-41376182">[1 more]</label></div><br/><div class="children"><div class="content">If only there was a rich 3-dimensional physical environment we could draw training data from.</div><br/></div></div><div id="41376163" class="c"><input type="checkbox" id="c-41376163" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41375678">parent</a><span>|</span><a href="#41376182">prev</a><span>|</span><a href="#41376855">next</a><span>|</span><label class="collapse" for="c-41376163">[-]</label><label class="expand" for="c-41376163">[1 more]</label></div><br/><div class="children"><div class="content">Well, yeah. Image diffusion models only work because you can provide large amounts of training data. For Doom it is even simpler, since you don&#x27;t need to deal with compositing.</div><br/></div></div></div></div><div id="41376855" class="c"><input type="checkbox" id="c-41376855" checked=""/><div class="controls bullet"><span class="by">nolist_policy</span><span>|</span><a href="#41375678">prev</a><span>|</span><a href="#41377172">next</a><span>|</span><label class="collapse" for="c-41376855">[-]</label><label class="expand" for="c-41376855">[2 more]</label></div><br/><div class="children"><div class="content">Makes me wonder... If you stand still in front of a door so all past observations only contain that door, will the model teleport you to another level when opening the door?</div><br/><div id="41377124" class="c"><input type="checkbox" id="c-41377124" checked=""/><div class="controls bullet"><span class="by">zbendefy</span><span>|</span><a href="#41376855">parent</a><span>|</span><a href="#41377172">next</a><span>|</span><label class="collapse" for="c-41377124">[-]</label><label class="expand" for="c-41377124">[1 more]</label></div><br/><div class="children"><div class="content">I think some state is also being given (or if its not, it could be given) to the network, like 3d world position&#x2F;orientation of the player, that could help the neural network anchor the player in the world.</div><br/></div></div></div></div><div id="41377172" class="c"><input type="checkbox" id="c-41377172" checked=""/><div class="controls bullet"><span class="by">lIl-IIIl</span><span>|</span><a href="#41376855">prev</a><span>|</span><a href="#41377322">next</a><span>|</span><label class="collapse" for="c-41377172">[-]</label><label class="expand" for="c-41377172">[2 more]</label></div><br/><div class="children"><div class="content">How does it know how many times it needs to shoot the zombie before it dies?<p>Most enemies have enough hit points to survive the first shot. If the model is only trained on the previous frame, it doesn&#x27;t know how many times the enemy was already shot at.<p>From the video it seems like it is probability based - they may die right away or it might take way longer than it should.<p>I love how the player&#x27;s health goes down when he stands in the radioactive green water.<p>In Doom the enemies fight with each other if they accidentally incur &quot;friendly fire&quot;. It would be interesting to see it play out in this version.</div><br/><div id="41377202" class="c"><input type="checkbox" id="c-41377202" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#41377172">parent</a><span>|</span><a href="#41377322">next</a><span>|</span><label class="collapse" for="c-41377202">[-]</label><label class="expand" for="c-41377202">[1 more]</label></div><br/><div class="children"><div class="content">It gets a number of previous frame<i>s</i> as input I think.</div><br/></div></div></div></div><div id="41377322" class="c"><input type="checkbox" id="c-41377322" checked=""/><div class="controls bullet"><span class="by">jumploops</span><span>|</span><a href="#41377172">prev</a><span>|</span><a href="#41377336">next</a><span>|</span><label class="collapse" for="c-41377322">[-]</label><label class="expand" for="c-41377322">[1 more]</label></div><br/><div class="children"><div class="content">This seems similar to how we use LLMs to generate code: generate, run, fix, generate.<p>Instead of working through a game, it’s building generic UI components and using common abstractions.</div><br/></div></div><div id="41377336" class="c"><input type="checkbox" id="c-41377336" checked=""/><div class="controls bullet"><span class="by">HellDunkel</span><span>|</span><a href="#41377322">prev</a><span>|</span><a href="#41377219">next</a><span>|</span><label class="collapse" for="c-41377336">[-]</label><label class="expand" for="c-41377336">[1 more]</label></div><br/><div class="children"><div class="content">Although impressive i must disagree. Diffusion models are not game engines. A game engine is a component to propell your game (along the time axis?). In that sense it is similar to the engine of the car, hence the name. It does not need a single working car nor a road to drive on do its job.
The above is a dynamic, interactive replication of what happens when you put a car on a given road, requiring a million test drives with working vehicles. An engine would also work offroad.</div><br/></div></div><div id="41377219" class="c"><input type="checkbox" id="c-41377219" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#41377336">prev</a><span>|</span><a href="#41376927">next</a><span>|</span><label class="collapse" for="c-41377219">[-]</label><label class="expand" for="c-41377219">[1 more]</label></div><br/><div class="children"><div class="content">Certain categories of youtube videos can also be viewed as some sort of game where the actions are the audio&#x2F;transcript advanced a couple of seconds. Add two eggs. Fetch the ball. I&#x27;m walking in the park.</div><br/></div></div><div id="41376927" class="c"><input type="checkbox" id="c-41376927" checked=""/><div class="controls bullet"><span class="by">jamilton</span><span>|</span><a href="#41377219">prev</a><span>|</span><a href="#41375765">next</a><span>|</span><label class="collapse" for="c-41376927">[-]</label><label class="expand" for="c-41376927">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if the MineRL (<a href="https:&#x2F;&#x2F;www.ijcai.org&#x2F;proceedings&#x2F;2019&#x2F;0339.pdf" rel="nofollow">https:&#x2F;&#x2F;www.ijcai.org&#x2F;proceedings&#x2F;2019&#x2F;0339.pdf</a> and minerl.io) dataset would be sufficient to reproduce this work with Minecraft.<p>Any other similar existing datasets?<p>A really goofy way I can think of to get a bunch of data would be to get videos from youtube and try to detect keyboard sounds to determine what keys they&#x27;re pressing.</div><br/></div></div><div id="41375765" class="c"><input type="checkbox" id="c-41375765" checked=""/><div class="controls bullet"><span class="by">masterspy7</span><span>|</span><a href="#41376927">prev</a><span>|</span><a href="#41376913">next</a><span>|</span><label class="collapse" for="c-41375765">[-]</label><label class="expand" for="c-41375765">[5 more]</label></div><br/><div class="children"><div class="content">There&#x27;s been a ton of work to generate assets for games using AI: 3d models, textures, code, etc. None of that may even be necessary with a generative game engine like this! If you could scale this up, train on all games in existence, etc. I bet some interesting things would happen</div><br/><div id="41375916" class="c"><input type="checkbox" id="c-41375916" checked=""/><div class="controls bullet"><span class="by">rererereferred</span><span>|</span><a href="#41375765">parent</a><span>|</span><a href="#41375987">next</a><span>|</span><label class="collapse" for="c-41375916">[-]</label><label class="expand" for="c-41375916">[3 more]</label></div><br/><div class="children"><div class="content">But can you grab what this Ai has learned and generate the 3d models, maps and code to turn it into an actual game that can run on a user&#x27;s PC? That would be amazing.</div><br/><div id="41376292" class="c"><input type="checkbox" id="c-41376292" checked=""/><div class="controls bullet"><span class="by">passion__desire</span><span>|</span><a href="#41375765">root</a><span>|</span><a href="#41375916">parent</a><span>|</span><a href="#41376614">next</a><span>|</span><label class="collapse" for="c-41376292">[-]</label><label class="expand" for="c-41376292">[1 more]</label></div><br/><div class="children"><div class="content">Jensen Huang&#x27;s vision that future games will be generated rather than rendered is coming true.</div><br/></div></div><div id="41376614" class="c"><input type="checkbox" id="c-41376614" checked=""/><div class="controls bullet"><span class="by">kleiba</span><span>|</span><a href="#41375765">root</a><span>|</span><a href="#41375916">parent</a><span>|</span><a href="#41376292">prev</a><span>|</span><a href="#41375987">next</a><span>|</span><label class="collapse" for="c-41376614">[-]</label><label class="expand" for="c-41376614">[1 more]</label></div><br/><div class="children"><div class="content">What would be the point? This model has been trained on an existing game, so turning it back into assets, maps, and code would just give you a copy of the original game you started with. I suppose you could create variations of it then... but:<p>You don&#x27;t even need to do all of that - this trained model <i>already is</i> the game, i.e., it&#x27;s interactive, you can play the game.</div><br/></div></div></div></div><div id="41375987" class="c"><input type="checkbox" id="c-41375987" checked=""/><div class="controls bullet"><span class="by">whamlastxmas</span><span>|</span><a href="#41375765">parent</a><span>|</span><a href="#41375916">prev</a><span>|</span><a href="#41376913">next</a><span>|</span><label class="collapse" for="c-41375987">[-]</label><label class="expand" for="c-41375987">[1 more]</label></div><br/><div class="children"><div class="content">I would absolutely love if they could take this demo, add a new door that isn’t in the original, and see what it generates behind that door</div><br/></div></div></div></div><div id="41376913" class="c"><input type="checkbox" id="c-41376913" checked=""/><div class="controls bullet"><span class="by">lukol</span><span>|</span><a href="#41375765">prev</a><span>|</span><a href="#41375704">next</a><span>|</span><label class="collapse" for="c-41376913">[-]</label><label class="expand" for="c-41376913">[1 more]</label></div><br/><div class="children"><div class="content">I believe future game engines will be state machines with deterministic algorithms that can be reproduced at any time. However, rendering said state into visual &#x2F; auditory &#x2F; etc. experiences will be taken over by AI models.<p>This will also allow players to easily customize what they experience without changing the core game loop.</div><br/></div></div><div id="41375704" class="c"><input type="checkbox" id="c-41375704" checked=""/><div class="controls bullet"><span class="by">ravetcofx</span><span>|</span><a href="#41376913">prev</a><span>|</span><a href="#41377240">next</a><span>|</span><label class="collapse" for="c-41375704">[-]</label><label class="expand" for="c-41375704">[1 more]</label></div><br/><div class="children"><div class="content">There is going to be a flood of these dreamlike &quot;games&quot; in the next few years. This feels likes a bit of a breakthrough in the engineering of these systems.</div><br/></div></div><div id="41377240" class="c"><input type="checkbox" id="c-41377240" checked=""/><div class="controls bullet"><span class="by">thegabriele</span><span>|</span><a href="#41375704">prev</a><span>|</span><a href="#41376977">next</a><span>|</span><label class="collapse" for="c-41377240">[-]</label><label class="expand" for="c-41377240">[1 more]</label></div><br/><div class="children"><div class="content">Wow, I bet Boston Dynamics and such are quite interested</div><br/></div></div><div id="41376977" class="c"><input type="checkbox" id="c-41376977" checked=""/><div class="controls bullet"><span class="by">qnleigh</span><span>|</span><a href="#41377240">prev</a><span>|</span><a href="#41376249">next</a><span>|</span><label class="collapse" for="c-41376977">[-]</label><label class="expand" for="c-41376977">[3 more]</label></div><br/><div class="children"><div class="content">Could a similar scheme be used to drastically improve the visual quality of a video game? You would train the model on gameplay rendered at low and high quality (say with and without ray tracing, and with low and high density meshing), and try to get it to convert a quick render into something photorealistic on the fly.<p>When things like DALL-E first came out, I was expecting something like the above to make it into mainstream games within a few years. But that was either too optimistic or I&#x27;m not up to speed on this sort of thing.</div><br/><div id="41377005" class="c"><input type="checkbox" id="c-41377005" checked=""/><div class="controls bullet"><span class="by">agys</span><span>|</span><a href="#41376977">parent</a><span>|</span><a href="#41376249">next</a><span>|</span><label class="collapse" for="c-41377005">[-]</label><label class="expand" for="c-41377005">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that what Nvidia’s Ray Reconstruction and DLSS (frame generation and upscaler) are doing, more or less?</div><br/><div id="41377388" class="c"><input type="checkbox" id="c-41377388" checked=""/><div class="controls bullet"><span class="by">qnleigh</span><span>|</span><a href="#41376977">root</a><span>|</span><a href="#41377005">parent</a><span>|</span><a href="#41376249">next</a><span>|</span><label class="collapse" for="c-41377388">[-]</label><label class="expand" for="c-41377388">[1 more]</label></div><br/><div class="children"><div class="content">At a high level I guess so. I don&#x27;t know enough about Ray Reconstruction (though the results are impressive), but I was thinking of something more drastic than DLSS. Diffusion models on static images can turn a cartoon into a photorealistic image. Doing something similar for a game, where a low-quality render is turned into something that would otherwise take seconds to render, seems qualitatively quite different from DLSS. In principle a model could fill in huge amounts of detail, like increasing the number of particles in a particle-based effect, adding shading&#x2F;lighting effects...</div><br/></div></div></div></div></div></div><div id="41376249" class="c"><input type="checkbox" id="c-41376249" checked=""/><div class="controls bullet"><span class="by">dysoco</span><span>|</span><a href="#41376977">prev</a><span>|</span><a href="#41376036">next</a><span>|</span><label class="collapse" for="c-41376249">[-]</label><label class="expand" for="c-41376249">[2 more]</label></div><br/><div class="children"><div class="content">Ah finally we are starting to see something gaming related. I&#x27;m curious as to why we haven&#x27;t seen more of neural networks applied to games even in a completely experimental fashion; we used to have a lot of little experimental indie games such as Façade (2005) and I&#x27;m surprised we don&#x27;t have something similar years after the advent of LLMs.<p>We could have mods for old games that generate voices for the characters for example. Maybe it&#x27;s unfeasible from a computing perspective? There are people running local LLMs, no?</div><br/><div id="41376400" class="c"><input type="checkbox" id="c-41376400" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#41376249">parent</a><span>|</span><a href="#41376036">next</a><span>|</span><label class="collapse" for="c-41376400">[-]</label><label class="expand" for="c-41376400">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We could have mods for old games that generate voices for the characters for example<p>You mean in real time? Or just in general?<p>There are <i>a lot</i> of mods that use AI-generated voices. I&#x27;ll say it&#x27;s the norm of modding community now.</div><br/></div></div></div></div><div id="41376036" class="c"><input type="checkbox" id="c-41376036" checked=""/><div class="controls bullet"><span class="by">bufferoverflow</span><span>|</span><a href="#41376249">prev</a><span>|</span><a href="#41376166">next</a><span>|</span><label class="collapse" for="c-41376036">[-]</label><label class="expand" for="c-41376036">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s probably how our reality is rendered.</div><br/></div></div><div id="41376166" class="c"><input type="checkbox" id="c-41376166" checked=""/><div class="controls bullet"><span class="by">arduinomancer</span><span>|</span><a href="#41376036">prev</a><span>|</span><a href="#41376183">next</a><span>|</span><label class="collapse" for="c-41376166">[-]</label><label class="expand" for="c-41376166">[5 more]</label></div><br/><div class="children"><div class="content">How does the model “remember” the whole state of the world?<p>Like if I kill an enemy in some room and walk all the way across the map and come back, would the body still be there?</div><br/><div id="41376180" class="c"><input type="checkbox" id="c-41376180" checked=""/><div class="controls bullet"><span class="by">a_e_k</span><span>|</span><a href="#41376166">parent</a><span>|</span><a href="#41376724">next</a><span>|</span><label class="collapse" for="c-41376180">[-]</label><label class="expand" for="c-41376180">[2 more]</label></div><br/><div class="children"><div class="content">Watch closely in the videos and you&#x27;ll see that enemies often respawn when offscreen and sometimes when onscreen.  Destroyed barrels come back, ammo count and health fluctuates weirdly, etc.  It&#x27;s still impressive, but its not perfect in that regard.</div><br/><div id="41376197" class="c"><input type="checkbox" id="c-41376197" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#41376166">root</a><span>|</span><a href="#41376180">parent</a><span>|</span><a href="#41376724">next</a><span>|</span><label class="collapse" for="c-41376197">[-]</label><label class="expand" for="c-41376197">[1 more]</label></div><br/><div class="children"><div class="content">Not unlike in (human) dreams.</div><br/></div></div></div></div><div id="41376724" class="c"><input type="checkbox" id="c-41376724" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#41376166">parent</a><span>|</span><a href="#41376180">prev</a><span>|</span><a href="#41376347">next</a><span>|</span><label class="collapse" for="c-41376724">[-]</label><label class="expand" for="c-41376724">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t even remember the state of the game you look at. Doors spawning right in front of you, particle effects turning into enemies mid flight etc, so just regular gen AI issues.<p>Edit: Can see this in the first 10 seconds of the first video under &quot;Full Gameplay Videos&quot;, stairs turning to corridor turning to closed door for no reason without looking away.</div><br/></div></div><div id="41376347" class="c"><input type="checkbox" id="c-41376347" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#41376166">parent</a><span>|</span><a href="#41376724">prev</a><span>|</span><a href="#41376183">next</a><span>|</span><label class="collapse" for="c-41376347">[-]</label><label class="expand" for="c-41376347">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t. You need to put the world state in the input (the &quot;prompt&quot;, even it doesn&#x27;t look like prompt in this case). Whatever not in the prompt is lost.</div><br/></div></div></div></div><div id="41376183" class="c"><input type="checkbox" id="c-41376183" checked=""/><div class="controls bullet"><span class="by">broast</span><span>|</span><a href="#41376166">prev</a><span>|</span><a href="#41376001">next</a><span>|</span><label class="collapse" for="c-41376183">[-]</label><label class="expand" for="c-41376183">[2 more]</label></div><br/><div class="children"><div class="content">Maybe one day this will be how operating systems work.</div><br/><div id="41377153" class="c"><input type="checkbox" id="c-41377153" checked=""/><div class="controls bullet"><span class="by">misterflibble</span><span>|</span><a href="#41376183">parent</a><span>|</span><a href="#41376001">next</a><span>|</span><label class="collapse" for="c-41377153">[-]</label><label class="expand" for="c-41377153">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t give them ideas lol terrifying stuff if that happens!</div><br/></div></div></div></div><div id="41376001" class="c"><input type="checkbox" id="c-41376001" checked=""/><div class="controls bullet"><span class="by">kcaj</span><span>|</span><a href="#41376183">prev</a><span>|</span><a href="#41377385">next</a><span>|</span><label class="collapse" for="c-41376001">[-]</label><label class="expand" for="c-41376001">[1 more]</label></div><br/><div class="children"><div class="content">Take a bunch of videos of the real world and calculate the differential camera motion with optical flow or feature tracking. Call this the video’s control input. Now we can play SORA.</div><br/></div></div><div id="41377385" class="c"><input type="checkbox" id="c-41377385" checked=""/><div class="controls bullet"><span class="by">holoduke</span><span>|</span><a href="#41376001">prev</a><span>|</span><a href="#41375925">next</a><span>|</span><label class="collapse" for="c-41377385">[-]</label><label class="expand" for="c-41377385">[1 more]</label></div><br/><div class="children"><div class="content">I saw a video a while ago where they recreated actual doom footage with a diffusion technique so it looked like a jungle or anything you liked. Cant find it anymore,  but looked impressive.</div><br/></div></div><div id="41375925" class="c"><input type="checkbox" id="c-41375925" checked=""/><div class="controls bullet"><span class="by">darrinm</span><span>|</span><a href="#41377385">prev</a><span>|</span><a href="#41375846">next</a><span>|</span><label class="collapse" for="c-41375925">[-]</label><label class="expand" for="c-41375925">[6 more]</label></div><br/><div class="children"><div class="content">So… is it interactive? Playable? Or just generating a video of gameplay?</div><br/><div id="41375932" class="c"><input type="checkbox" id="c-41375932" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#41375925">parent</a><span>|</span><a href="#41375846">next</a><span>|</span><label class="collapse" for="c-41375932">[-]</label><label class="expand" for="c-41375932">[5 more]</label></div><br/><div class="children"><div class="content">From the article: <i>We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction with a complex environment over long trajectories at high quality</i>.<p>The demo is actual gameplay at ~20 FPS.</div><br/><div id="41375989" class="c"><input type="checkbox" id="c-41375989" checked=""/><div class="controls bullet"><span class="by">darrinm</span><span>|</span><a href="#41375925">root</a><span>|</span><a href="#41375932">parent</a><span>|</span><a href="#41375846">next</a><span>|</span><label class="collapse" for="c-41375989">[-]</label><label class="expand" for="c-41375989">[4 more]</label></div><br/><div class="children"><div class="content">It confused me that their stated evaluations by humans are comparing video clips rather than evaluating game play.</div><br/><div id="41376188" class="c"><input type="checkbox" id="c-41376188" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#41375925">root</a><span>|</span><a href="#41375989">parent</a><span>|</span><a href="#41375846">next</a><span>|</span><label class="collapse" for="c-41376188">[-]</label><label class="expand" for="c-41376188">[3 more]</label></div><br/><div class="children"><div class="content">Short clips are the only way a human will make any errors determining which is which.</div><br/><div id="41376261" class="c"><input type="checkbox" id="c-41376261" checked=""/><div class="controls bullet"><span class="by">darrinm</span><span>|</span><a href="#41375925">root</a><span>|</span><a href="#41376188">parent</a><span>|</span><a href="#41375846">next</a><span>|</span><label class="collapse" for="c-41376261">[-]</label><label class="expand" for="c-41376261">[2 more]</label></div><br/><div class="children"><div class="content">More relevant is if by _playing_ it they couldn’t tell which is which.</div><br/><div id="41376751" class="c"><input type="checkbox" id="c-41376751" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#41375925">root</a><span>|</span><a href="#41376261">parent</a><span>|</span><a href="#41375846">next</a><span>|</span><label class="collapse" for="c-41376751">[-]</label><label class="expand" for="c-41376751">[1 more]</label></div><br/><div class="children"><div class="content">They obviously can within seconds, so it wouldn&#x27;t be a result. Being able to generate gameplay that looks right even if it doesn&#x27;t play right is one step.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41375846" class="c"><input type="checkbox" id="c-41375846" checked=""/><div class="controls bullet"><span class="by">throwmeaway222</span><span>|</span><a href="#41375925">prev</a><span>|</span><label class="collapse" for="c-41375846">[-]</label><label class="expand" for="c-41375846">[1 more]</label></div><br/><div class="children"><div class="content">You know how when you&#x27;re dreaming and you walk into a room at your house and you&#x27;re suddenly naked at school?<p>I&#x27;m convinced this is the code that gives Data (ST TNG) his dreaming capabilities.</div><br/></div></div></div></div></div></div></div></body></html>