<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1721206870077" as="style"/><link rel="stylesheet" href="styles.css?v=1721206870077"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2407.10240">XLSTMTime: Long-Term Time Series Forecasting with xLSTM</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>beefman</span> | <span>50 comments</span></div><br/><div><div id="40978739" class="c"><input type="checkbox" id="c-40978739" checked=""/><div class="controls bullet"><span class="by">carbocation</span><span>|</span><a href="#40981811">next</a><span>|</span><label class="collapse" for="c-40978739">[-]</label><label class="expand" for="c-40978739">[18 more]</label></div><br/><div class="children"><div class="content">&gt; In recent years, transformer-based models have gained prominence in multivariate long-term time series forecasting<p>Prominence, yes. But are they generally better than non-deep learning models? My understanding was that this is not the case, but I don&#x27;t follow this field closely.</div><br/><div id="40980151" class="c"><input type="checkbox" id="c-40980151" checked=""/><div class="controls bullet"><span class="by">dongobread</span><span>|</span><a href="#40978739">parent</a><span>|</span><a href="#40978995">next</a><span>|</span><label class="collapse" for="c-40980151">[-]</label><label class="expand" for="c-40980151">[6 more]</label></div><br/><div class="children"><div class="content">From experience in payments&#x2F;spending forecasting, I&#x27;ve found that deep learning generally underperform gradient-boosted tree models. Deep learning models tend to be good at learning seasonality but do not handle complex trends or shocks very well. Economic&#x2F;financial data tends to have straightforward seasonality with complex trends, so deep learning tends to do quite poorly.<p>I do agree with this paper - all of the good deep learning time series architectures I&#x27;ve tried are simple extensions of MLPs or RNNs (e.g. DeepAR or N-BEATS). The transformer-based architectures I&#x27;ve used have been absolutely awful, especially the endless stream of transformer-based &quot;foundational models&quot; that are coming out these days.</div><br/><div id="40980231" class="c"><input type="checkbox" id="c-40980231" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#40978739">root</a><span>|</span><a href="#40980151">parent</a><span>|</span><a href="#40978995">next</a><span>|</span><label class="collapse" for="c-40980231">[-]</label><label class="expand" for="c-40980231">[5 more]</label></div><br/><div class="children"><div class="content">Transformers are just MLPs with extra steps. So in theory they should be just as powerful. The problem with transformers is simultaneously their big advantage: They scale extremely well with larger networks and more training data. Better so than any other architecture out there. So if you had enormous datasets and unlimited compute budget, you could probably do amazing things in this regard as well. But if you&#x27;re just a mortal data scientist without extra funding, you will be better off with more traditional approaches.</div><br/><div id="40980574" class="c"><input type="checkbox" id="c-40980574" checked=""/><div class="controls bullet"><span class="by">dongobread</span><span>|</span><a href="#40978739">root</a><span>|</span><a href="#40980231">parent</a><span>|</span><a href="#40978995">next</a><span>|</span><label class="collapse" for="c-40980574">[-]</label><label class="expand" for="c-40980574">[4 more]</label></div><br/><div class="children"><div class="content">I think what you say is true when comparing transformers to CNNs&#x2F;RNNs, but not to MLPs.<p>Transformers, RNNs, and CNNs are all techniques to reduce parameter count compared to a pure-MLP model. If you took a transformer model and replaced each self-attention layer with a linear layer+activation function, you&#x27;d have a pure MLP model that can model every relationship the transformer does, but can model more possible relationships as well (but at the cost of tons more parameters). MLPs are more powerful&#x2F;scalable but transformers are more efficient.<p>Compared to MLPs, transformers save on parameter count by skimping on the number of parameters devoted to modeling the relationship between tokens. This works in language modeling, where relationships between tokens isn&#x27;t <i>that</i> important - you can jumble up the words in this sentence and it still mostly makes sense. This doesn&#x27;t work in time series, where relationships between tokens (timesteps) is the most important thing of all. The LTSF paper linked in the OP paper also mentions this same problem: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2205.13504" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2205.13504</a> (see section 1)</div><br/><div id="40983213" class="c"><input type="checkbox" id="c-40983213" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#40978739">root</a><span>|</span><a href="#40980574">parent</a><span>|</span><a href="#40982356">next</a><span>|</span><label class="collapse" for="c-40983213">[-]</label><label class="expand" for="c-40983213">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Compared to MLPs, transformers save on parameter count by skimping on the number of parameters<p>That is only correct if you look at models with equal parameter count from a purely theoretical perspective. In practice, it is possible to train transformers to orders of magnitude bigger scales than MLPs because they are so much more efficient. That&#x27;s why I said a modern transformer will easily beat these puny modern MLPs, but only in cases where data and compute budgets allow it. That is not even a question. If you look at recent time series forecasting leaderboard entries, you&#x27;ll almost always see transformers playing along at the top of it: <a href="https:&#x2F;&#x2F;github.com&#x2F;thuml&#x2F;Time-Series-Library">https:&#x2F;&#x2F;github.com&#x2F;thuml&#x2F;Time-Series-Library</a></div><br/></div></div><div id="40982356" class="c"><input type="checkbox" id="c-40982356" checked=""/><div class="controls bullet"><span class="by">newrotik</span><span>|</span><a href="#40978739">root</a><span>|</span><a href="#40980574">parent</a><span>|</span><a href="#40983213">prev</a><span>|</span><a href="#40981299">next</a><span>|</span><label class="collapse" for="c-40982356">[-]</label><label class="expand" for="c-40982356">[1 more]</label></div><br/><div class="children"><div class="content">Though I agree with the idea that MLPs are theoretically more &quot;capable&quot; than transformers, I think seeing them just as a parameter reduction technique is also excessively reductive.<p>Many have tried to build deep and large MLPs for a long time, but at some point adding more parameters wouldn&#x27;t increase models&#x27; performance.<p>In contrast, transformers became so popular because their modelling power just kept scaling with more and more data and more and more parameters. It seems like the &#x27;restriction&#x27; imposed on transformaters (the attention structure) is a verg good functional form for modelling language (and, more and more, some tasks in vision and audio).<p>They did not become popular because they were modest with respect to the parameters used.</div><br/></div></div><div id="40981299" class="c"><input type="checkbox" id="c-40981299" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#40978739">root</a><span>|</span><a href="#40980574">parent</a><span>|</span><a href="#40982356">prev</a><span>|</span><a href="#40978995">next</a><span>|</span><label class="collapse" for="c-40981299">[-]</label><label class="expand" for="c-40981299">[1 more]</label></div><br/><div class="children"><div class="content">Transformers reduce the number of relationships between tokens that must be learned, too. An MLP has to separately learn all possible relationships between token 1 and 2, and 2 and 3, and 3 and 4. A transformer can learn relationships between specific values regardless of position.</div><br/></div></div></div></div></div></div></div></div><div id="40978995" class="c"><input type="checkbox" id="c-40978995" checked=""/><div class="controls bullet"><span class="by">techwizrd</span><span>|</span><a href="#40978739">parent</a><span>|</span><a href="#40980151">prev</a><span>|</span><a href="#40978947">next</a><span>|</span><label class="collapse" for="c-40978995">[-]</label><label class="expand" for="c-40978995">[7 more]</label></div><br/><div class="children"><div class="content">In my aviation safety work, deep learning outperforms traditional non-DL models for multivariate time-series forecasting. Between deep learning models, I&#x27;ve had a wide variance in performance between transformers, Bi-LSTMs, regular MLPs, VAEs, and so on.</div><br/><div id="40979619" class="c"><input type="checkbox" id="c-40979619" checked=""/><div class="controls bullet"><span class="by">montereynack</span><span>|</span><a href="#40978739">root</a><span>|</span><a href="#40978995">parent</a><span>|</span><a href="#40979055">next</a><span>|</span><label class="collapse" for="c-40979619">[-]</label><label class="expand" for="c-40979619">[1 more]</label></div><br/><div class="children"><div class="content">Seconding the other question, would be curious to know</div><br/></div></div><div id="40979055" class="c"><input type="checkbox" id="c-40979055" checked=""/><div class="controls bullet"><span class="by">theLiminator</span><span>|</span><a href="#40978739">root</a><span>|</span><a href="#40978995">parent</a><span>|</span><a href="#40979619">prev</a><span>|</span><a href="#40980991">next</a><span>|</span><label class="collapse" for="c-40979055">[-]</label><label class="expand" for="c-40979055">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s your go-to model that generally performs well with little tuning?</div><br/><div id="40979886" class="c"><input type="checkbox" id="c-40979886" checked=""/><div class="controls bullet"><span class="by">techwizrd</span><span>|</span><a href="#40978739">root</a><span>|</span><a href="#40979055">parent</a><span>|</span><a href="#40980991">next</a><span>|</span><label class="collapse" for="c-40979886">[-]</label><label class="expand" for="c-40979886">[1 more]</label></div><br/><div class="children"><div class="content">If you have short time-series with low variance, noise and outliers, strong prior knowledge, or limited resources to train and maintain a model, I would stick with simpler traditional models.<p>If DL is a good fit for your use-case, then I tend to like transformers or combining CNNs with recurrent models (e.g., BiGRU, GRU, BiLSTM, LSTM) and optional attention.</div><br/></div></div></div></div><div id="40980991" class="c"><input type="checkbox" id="c-40980991" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#40978739">root</a><span>|</span><a href="#40978995">parent</a><span>|</span><a href="#40979055">prev</a><span>|</span><a href="#40980842">next</a><span>|</span><label class="collapse" for="c-40980991">[-]</label><label class="expand" for="c-40980991">[2 more]</label></div><br/><div class="children"><div class="content">What are you doing in aviation safety that requires time series modeling? Weather?</div><br/><div id="40982679" class="c"><input type="checkbox" id="c-40982679" checked=""/><div class="controls bullet"><span class="by">all2</span><span>|</span><a href="#40978739">root</a><span>|</span><a href="#40980991">parent</a><span>|</span><a href="#40980842">next</a><span>|</span><label class="collapse" for="c-40982679">[-]</label><label class="expand" for="c-40982679">[1 more]</label></div><br/><div class="children"><div class="content">My best guess would be accident occurrence prediction.</div><br/></div></div></div></div><div id="40980842" class="c"><input type="checkbox" id="c-40980842" checked=""/><div class="controls bullet"><span class="by">ramon156</span><span>|</span><a href="#40978739">root</a><span>|</span><a href="#40978995">parent</a><span>|</span><a href="#40980991">prev</a><span>|</span><a href="#40978947">next</a><span>|</span><label class="collapse" for="c-40980842">[-]</label><label class="expand" for="c-40980842">[1 more]</label></div><br/><div class="children"><div class="content">Now take into account that it has to be lightweight and DL falls shirt</div><br/></div></div></div></div><div id="40978947" class="c"><input type="checkbox" id="c-40978947" checked=""/><div class="controls bullet"><span class="by">Pandabob</span><span>|</span><a href="#40978739">parent</a><span>|</span><a href="#40978995">prev</a><span>|</span><a href="#40980508">next</a><span>|</span><label class="collapse" for="c-40978947">[-]</label><label class="expand" for="c-40978947">[1 more]</label></div><br/><div class="children"><div class="content">While I don&#x27;t have firsthand experience with these models, I recently discussed this topic with a friend who has used tree-based models like XGBoost for time series analysis. They noted that transformer-based architectures tend to yield decent performance on time series tasks with relatively little effort compared to tree models.<p>From what I understood, tree-based models can usually outperform transformers when given sufficient parameter tuning. However, models like TimeGPT offer decent performance without extensive tuning, making them an attractive option for quicker implementations.</div><br/></div></div><div id="40980508" class="c"><input type="checkbox" id="c-40980508" checked=""/><div class="controls bullet"><span class="by">svnt</span><span>|</span><a href="#40978739">parent</a><span>|</span><a href="#40978947">prev</a><span>|</span><a href="#40980479">next</a><span>|</span><label class="collapse" for="c-40980508">[-]</label><label class="expand" for="c-40980508">[1 more]</label></div><br/><div class="children"><div class="content">The paper says this in the next paragraph. xLSTMTime is not transformer-based either.</div><br/></div></div><div id="40980479" class="c"><input type="checkbox" id="c-40980479" checked=""/><div class="controls bullet"><span class="by">rjurney</span><span>|</span><a href="#40978739">parent</a><span>|</span><a href="#40980508">prev</a><span>|</span><a href="#40981135">next</a><span>|</span><label class="collapse" for="c-40980479">[-]</label><label class="expand" for="c-40980479">[1 more]</label></div><br/><div class="children"><div class="content">They arenât so hot, but recent efforts at transfer learning were promising.</div><br/></div></div></div></div><div id="40981811" class="c"><input type="checkbox" id="c-40981811" checked=""/><div class="controls bullet"><span class="by">dkga</span><span>|</span><a href="#40978739">prev</a><span>|</span><a href="#40980621">next</a><span>|</span><label class="collapse" for="c-40981811">[-]</label><label class="expand" for="c-40981811">[2 more]</label></div><br/><div class="children"><div class="content">A part of my work is literally building nowcasting and other types of prediction models in economics (inflation, GDP etc) and finance (market liquidity, etc). I havenât yet had a chance to read the paper but overall the tone of âtransformers are great for what they do but LSTM-type of models are very valuable stillâ completely resonates with me.</div><br/><div id="40982226" class="c"><input type="checkbox" id="c-40982226" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#40981811">parent</a><span>|</span><a href="#40980621">next</a><span>|</span><label class="collapse" for="c-40982226">[-]</label><label class="expand" for="c-40982226">[1 more]</label></div><br/><div class="children"><div class="content">Have you had the chance to apply Mamba to your work at all? Thoughts?</div><br/></div></div></div></div><div id="40980621" class="c"><input type="checkbox" id="c-40980621" checked=""/><div class="controls bullet"><span class="by">dlojudice</span><span>|</span><a href="#40981811">prev</a><span>|</span><a href="#40978846">next</a><span>|</span><label class="collapse" for="c-40980621">[-]</label><label class="expand" for="c-40980621">[2 more]</label></div><br/><div class="children"><div class="content">Is this somehow related to the Google weather prediction model using AI [1]?<p><a href="https:&#x2F;&#x2F;deepmind.google&#x2F;discover&#x2F;blog&#x2F;graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting&#x2F;" rel="nofollow">https:&#x2F;&#x2F;deepmind.google&#x2F;discover&#x2F;blog&#x2F;graphcast-ai-model-for...</a></div><br/><div id="40983705" class="c"><input type="checkbox" id="c-40983705" checked=""/><div class="controls bullet"><span class="by">scellus</span><span>|</span><a href="#40980621">parent</a><span>|</span><a href="#40978846">next</a><span>|</span><label class="collapse" for="c-40983705">[-]</label><label class="expand" for="c-40983705">[1 more]</label></div><br/><div class="children"><div class="content">No, Graphcast is a graph transformer trained on ERA5 weather reconstructions of the atmosphere, not a general time series prediction model. It by the way outperforms all traditional global point forecasts (non-ensembles), at least on predicting large-scale global patterns (Z500 and such, on the lag of 3â10 days or so). ECMWF has AIFS that is a derivate of Graphcast, they&#x27;ll probably get it or something similar to production in a couple of years.</div><br/></div></div></div></div><div id="40978846" class="c"><input type="checkbox" id="c-40978846" checked=""/><div class="controls bullet"><span class="by">Dowwie</span><span>|</span><a href="#40980621">prev</a><span>|</span><a href="#40979019">next</a><span>|</span><label class="collapse" for="c-40978846">[-]</label><label class="expand" for="c-40978846">[3 more]</label></div><br/><div class="children"><div class="content">marketed as a forecasting tool, so is this not applicable to event classification in time series?</div><br/><div id="40979169" class="c"><input type="checkbox" id="c-40979169" checked=""/><div class="controls bullet"><span class="by">RamblingCTO</span><span>|</span><a href="#40978846">parent</a><span>|</span><a href="#40980077">next</a><span>|</span><label class="collapse" for="c-40979169">[-]</label><label class="expand" for="c-40979169">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say that&#x27;s kind of a different task. I&#x27;m not a pro in this, but you could maybe treat it as a multi-variate forecast problem where the targets are probabilities per event if n is really small?</div><br/></div></div><div id="40980077" class="c"><input type="checkbox" id="c-40980077" checked=""/><div class="controls bullet"><span class="by">jimmySixDOF</span><span>|</span><a href="#40978846">parent</a><span>|</span><a href="#40979169">prev</a><span>|</span><a href="#40979019">next</a><span>|</span><label class="collapse" for="c-40980077">[-]</label><label class="expand" for="c-40980077">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I would be interested where this (and any Transformer&#x2F;LLM based approach) is improving anomaly detection for example.</div><br/></div></div></div></div><div id="40979019" class="c"><input type="checkbox" id="c-40979019" checked=""/><div class="controls bullet"><span class="by">greatpostman</span><span>|</span><a href="#40978846">prev</a><span>|</span><a href="#40981422">next</a><span>|</span><label class="collapse" for="c-40979019">[-]</label><label class="expand" for="c-40979019">[11 more]</label></div><br/><div class="children"><div class="content">The best deep learning time series models are closed source inside hedge funds.</div><br/><div id="40980788" class="c"><input type="checkbox" id="c-40980788" checked=""/><div class="controls bullet"><span class="by">fermisea</span><span>|</span><a href="#40979019">parent</a><span>|</span><a href="#40982062">next</a><span>|</span><label class="collapse" for="c-40980788">[-]</label><label class="expand" for="c-40980788">[1 more]</label></div><br/><div class="children"><div class="content">Most of the hard work is actually feature construction rather than monolithic models. And afaik gradient boosting still rules the world</div><br/></div></div><div id="40982062" class="c"><input type="checkbox" id="c-40982062" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#40979019">parent</a><span>|</span><a href="#40980788">prev</a><span>|</span><a href="#40982056">next</a><span>|</span><label class="collapse" for="c-40982062">[-]</label><label class="expand" for="c-40982062">[1 more]</label></div><br/><div class="children"><div class="content">There is no such thing as a generally best model due to the no free lunch theorem. What works in hedge funds will be bad in other areas that need less or different inductive biases due to having more or less data and different data.</div><br/></div></div><div id="40979362" class="c"><input type="checkbox" id="c-40979362" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#40979019">parent</a><span>|</span><a href="#40982056">prev</a><span>|</span><a href="#40981422">next</a><span>|</span><label class="collapse" for="c-40979362">[-]</label><label class="expand" for="c-40979362">[7 more]</label></div><br/><div class="children"><div class="content">I think hedge funds, at least the advanced once, definitely don&#x27;t use time series modelling anymore. That&#x27;s quit outdated nowadays.</div><br/><div id="40980515" class="c"><input type="checkbox" id="c-40980515" checked=""/><div class="controls bullet"><span class="by">rjurney</span><span>|</span><a href="#40979019">root</a><span>|</span><a href="#40979362">parent</a><span>|</span><a href="#40979449">next</a><span>|</span><label class="collapse" for="c-40980515">[-]</label><label class="expand" for="c-40980515">[1 more]</label></div><br/><div class="children"><div class="content">There are many ways of approaching quantitative trading and many people do employ time series analysis, especially for high frequency trading.</div><br/></div></div><div id="40979449" class="c"><input type="checkbox" id="c-40979449" checked=""/><div class="controls bullet"><span class="by">max_</span><span>|</span><a href="#40979019">root</a><span>|</span><a href="#40979362">parent</a><span>|</span><a href="#40980515">prev</a><span>|</span><a href="#40981422">next</a><span>|</span><label class="collapse" for="c-40979449">[-]</label><label class="expand" for="c-40979449">[5 more]</label></div><br/><div class="children"><div class="content">What do you suspect they are using?</div><br/><div id="40980965" class="c"><input type="checkbox" id="c-40980965" checked=""/><div class="controls bullet"><span class="by">nextos</span><span>|</span><a href="#40979019">root</a><span>|</span><a href="#40979449">parent</a><span>|</span><a href="#40979831">next</a><span>|</span><label class="collapse" for="c-40980965">[-]</label><label class="expand" for="c-40980965">[1 more]</label></div><br/><div class="children"><div class="content">Some funds that tried to recruit me were really interested in classical generative models (ARMA, GARCH, HMMs with heavy-tailed emissions, etc.) extended with deep components to make them more flexible. Pyro and Kevin Murphy&#x27;s ProbML vol II are a good starting point to learn more about these topics.<p>The key is to understand that in some of these problems, data is relatively scarce, and it is really important to quantify uncertainty.</div><br/></div></div><div id="40979831" class="c"><input type="checkbox" id="c-40979831" checked=""/><div class="controls bullet"><span class="by">meowkit</span><span>|</span><a href="#40979019">root</a><span>|</span><a href="#40979449">parent</a><span>|</span><a href="#40980965">prev</a><span>|</span><a href="#40981422">next</a><span>|</span><label class="collapse" for="c-40979831">[-]</label><label class="expand" for="c-40979831">[3 more]</label></div><br/><div class="children"><div class="content">They pull data from all kinds of things now.<p>For example, satellite imagery of trucking activity correlated to specific companies or industries.<p>Its all signal processing at some level, but directly modeling the time series of price or other asset metrics doesnât have the alpha it may have had decades ago.</div><br/><div id="40980523" class="c"><input type="checkbox" id="c-40980523" checked=""/><div class="controls bullet"><span class="by">greatpostman</span><span>|</span><a href="#40979019">root</a><span>|</span><a href="#40979831">parent</a><span>|</span><a href="#40981422">next</a><span>|</span><label class="collapse" for="c-40980523">[-]</label><label class="expand" for="c-40980523">[2 more]</label></div><br/><div class="children"><div class="content">Alternative data is passed into time series models. They are features.<p>You donât know as much about this as you think</div><br/><div id="40980671" class="c"><input type="checkbox" id="c-40980671" checked=""/><div class="controls bullet"><span class="by">myhf</span><span>|</span><a href="#40979019">root</a><span>|</span><a href="#40980523">parent</a><span>|</span><a href="#40981422">next</a><span>|</span><label class="collapse" for="c-40980671">[-]</label><label class="expand" for="c-40980671">[1 more]</label></div><br/><div class="children"><div class="content">emoji hand pointing up</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="40981422" class="c"><input type="checkbox" id="c-40981422" checked=""/><div class="controls bullet"><span class="by">localfirst</span><span>|</span><a href="#40979019">prev</a><span>|</span><a href="#40980524">next</a><span>|</span><label class="collapse" for="c-40981422">[-]</label><label class="expand" for="c-40981422">[1 more]</label></div><br/><div class="children"><div class="content">time series forecasting works best with deterministic domains. none of the published LLM&#x2F;AI&#x2F;Deep&#x2F;Machine techniques do well in the stock market. Absolutely none. we&#x27;ve tried them all.</div><br/></div></div><div id="40980524" class="c"><input type="checkbox" id="c-40980524" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#40981422">prev</a><span>|</span><a href="#40980391">next</a><span>|</span><label class="collapse" for="c-40980524">[-]</label><label class="expand" for="c-40980524">[4 more]</label></div><br/><div class="children"><div class="content">Reminder: If someone&#x27;s time series forecasting method worked, they wouldn&#x27;t be publishing it.</div><br/><div id="40980855" class="c"><input type="checkbox" id="c-40980855" checked=""/><div class="controls bullet"><span class="by">dongobread</span><span>|</span><a href="#40980524">parent</a><span>|</span><a href="#40981196">next</a><span>|</span><label class="collapse" for="c-40980855">[-]</label><label class="expand" for="c-40980855">[1 more]</label></div><br/><div class="children"><div class="content">They definitely would and do, the vast majority of time series work is not about asset prices or beating the stock market</div><br/></div></div><div id="40981196" class="c"><input type="checkbox" id="c-40981196" checked=""/><div class="controls bullet"><span class="by">musleh2</span><span>|</span><a href="#40980524">parent</a><span>|</span><a href="#40980855">prev</a><span>|</span><a href="#40980391">next</a><span>|</span><label class="collapse" for="c-40981196">[-]</label><label class="expand" for="c-40981196">[2 more]</label></div><br/><div class="children"><div class="content">The Transformer model, despite being one of the most successful in AI history, was still being published.</div><br/><div id="40983179" class="c"><input type="checkbox" id="c-40983179" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#40980524">root</a><span>|</span><a href="#40981196">parent</a><span>|</span><a href="#40980391">next</a><span>|</span><label class="collapse" for="c-40983179">[-]</label><label class="expand" for="c-40983179">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a sequence model, not a time-series model. All time series are sequences but not all sequences are time series.</div><br/></div></div></div></div></div></div><div id="40980391" class="c"><input type="checkbox" id="c-40980391" checked=""/><div class="controls bullet"><span class="by">nyanpasu64</span><span>|</span><a href="#40980524">prev</a><span>|</span><a href="#40980293">next</a><span>|</span><label class="collapse" for="c-40980391">[-]</label><label class="expand" for="c-40980391">[4 more]</label></div><br/><div class="children"><div class="content">I misread this as XSLT :&#x27;)</div><br/><div id="40981014" class="c"><input type="checkbox" id="c-40981014" checked=""/><div class="controls bullet"><span class="by">selimnairb</span><span>|</span><a href="#40980391">parent</a><span>|</span><a href="#40981384">next</a><span>|</span><label class="collapse" for="c-40981014">[-]</label><label class="expand" for="c-40981014">[2 more]</label></div><br/><div class="children"><div class="content">Same. I am old?</div><br/><div id="40981855" class="c"><input type="checkbox" id="c-40981855" checked=""/><div class="controls bullet"><span class="by">ThomasBHickey</span><span>|</span><a href="#40980391">root</a><span>|</span><a href="#40981014">parent</a><span>|</span><a href="#40981384">next</a><span>|</span><label class="collapse" for="c-40981855">[-]</label><label class="expand" for="c-40981855">[1 more]</label></div><br/><div class="children"><div class="content">Me too (and yes, I&#x27;m old)</div><br/></div></div></div></div></div></div><div id="40980293" class="c"><input type="checkbox" id="c-40980293" checked=""/><div class="controls bullet"><span class="by">thedudeabides5</span><span>|</span><a href="#40980391">prev</a><span>|</span><a href="#40980750">next</a><span>|</span><label class="collapse" for="c-40980293">[-]</label><label class="expand" for="c-40980293">[2 more]</label></div><br/><div class="children"><div class="content">cant wait for someone to lose all their money trying to predict stocks with this thing</div><br/></div></div><div id="40980750" class="c"><input type="checkbox" id="c-40980750" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#40980293">prev</a><span>|</span><label class="collapse" for="c-40980750">[-]</label><label class="expand" for="c-40980750">[2 more]</label></div><br/><div class="children"><div class="content">Wow, is there a way to apply this to financial trading?</div><br/><div id="40981319" class="c"><input type="checkbox" id="c-40981319" checked=""/><div class="controls bullet"><span class="by">musleh2</span><span>|</span><a href="#40980750">parent</a><span>|</span><label class="collapse" for="c-40981319">[-]</label><label class="expand" for="c-40981319">[1 more]</label></div><br/><div class="children"><div class="content">If you have dataset in financial , I can try it for you</div><br/></div></div></div></div></div></div></div></div></div></body></html>