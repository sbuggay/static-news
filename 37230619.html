<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1692781262601" as="style"/><link rel="stylesheet" href="styles.css?v=1692781262601"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://mastodon.social/@chromakode/110936177254839251">I only lost 10 minutes of data, thanks to ZFS</a>Â <span class="domain">(<a href="https://mastodon.social">mastodon.social</a>)</span></div><div class="subtext"><span>chromakode</span> | <span>185 comments</span></div><br/><div><div id="37231598" class="c"><input type="checkbox" id="c-37231598" checked=""/><div class="controls bullet"><span class="by">xet7</span><span>|</span><a href="#37230879">next</a><span>|</span><label class="collapse" for="c-37231598">[-]</label><label class="expand" for="c-37231598">[21 more]</label></div><br/><div class="children"><div class="content">With ZFS, he is better prepared than other WD and Sandisk SSD users.<p><a href="https:&#x2F;&#x2F;petapixel.com&#x2F;2023&#x2F;08&#x2F;08&#x2F;sandisk-portable-ssds-are-failing-so-frequently-we-can-no-longer-recommend-them&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;petapixel.com&#x2F;2023&#x2F;08&#x2F;08&#x2F;sandisk-portable-ssds-are-f...</a><p><a href="https:&#x2F;&#x2F;www.theverge.com&#x2F;22291828&#x2F;sandisk-extreme-pro-portable-my-passport-failure-continued" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.theverge.com&#x2F;22291828&#x2F;sandisk-extreme-pro-portab...</a><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37042587">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37042587</a><p><a href="https:&#x2F;&#x2F;www.theverge.com&#x2F;23837513&#x2F;western-digital-sandisk-ssd-corrupted-deleted-questions" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.theverge.com&#x2F;23837513&#x2F;western-digital-sandisk-ss...</a><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37188736">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37188736</a></div><br/><div id="37231725" class="c"><input type="checkbox" id="c-37231725" checked=""/><div class="controls bullet"><span class="by">anonuser123456</span><span>|</span><a href="#37231598">parent</a><span>|</span><a href="#37230879">next</a><span>|</span><label class="collapse" for="c-37231725">[-]</label><label class="expand" for="c-37231725">[20 more]</label></div><br/><div class="children"><div class="content">I had two drives in my mirrored zpool die within 8 minutes of one another.<p>Both HGST drives too.  A very sad day.<p>Thankfully I had been regularly zfs sending my contents to another site and lost very little data.<p>ZFS is rad.</div><br/><div id="37231796" class="c"><input type="checkbox" id="c-37231796" checked=""/><div class="controls bullet"><span class="by">anotherhue</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37231725">parent</a><span>|</span><a href="#37232328">next</a><span>|</span><label class="collapse" for="c-37231796">[-]</label><label class="expand" for="c-37231796">[5 more]</label></div><br/><div class="children"><div class="content">&gt; ZFS is rad.
Typo: RAID</div><br/><div id="37232213" class="c"><input type="checkbox" id="c-37232213" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37231796">parent</a><span>|</span><a href="#37233224">next</a><span>|</span><label class="collapse" for="c-37232213">[-]</label><label class="expand" for="c-37232213">[1 more]</label></div><br/><div class="children"><div class="content">Redundant Array of Disks (cost not specified)</div><br/></div></div><div id="37233224" class="c"><input type="checkbox" id="c-37233224" checked=""/><div class="controls bullet"><span class="by">josephg</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37231796">parent</a><span>|</span><a href="#37232213">prev</a><span>|</span><a href="#37232170">next</a><span>|</span><label class="collapse" for="c-37233224">[-]</label><label class="expand" for="c-37233224">[1 more]</label></div><br/><div class="children"><div class="content">Repeat after me: RAID is not a backup.<p>Sounds like it was the backup which saved the day here, not the raid array.</div><br/></div></div><div id="37232170" class="c"><input type="checkbox" id="c-37232170" checked=""/><div class="controls bullet"><span class="by">btown</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37231796">parent</a><span>|</span><a href="#37233224">prev</a><span>|</span><a href="#37231800">next</a><span>|</span><label class="collapse" for="c-37232170">[-]</label><label class="expand" for="c-37232170">[1 more]</label></div><br/><div class="children"><div class="content">Only 1&#x2F;4 of data lost! Meaning still recoverable!</div><br/></div></div><div id="37231800" class="c"><input type="checkbox" id="c-37231800" checked=""/><div class="controls bullet"><span class="by">anonuser123456</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37231796">parent</a><span>|</span><a href="#37232170">prev</a><span>|</span><a href="#37232328">next</a><span>|</span><label class="collapse" for="c-37231800">[-]</label><label class="expand" for="c-37231800">[1 more]</label></div><br/><div class="children"><div class="content">You deserve all my upvotes :)</div><br/></div></div></div></div><div id="37232328" class="c"><input type="checkbox" id="c-37232328" checked=""/><div class="controls bullet"><span class="by">erhaetherth</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37231725">parent</a><span>|</span><a href="#37231796">prev</a><span>|</span><a href="#37231895">next</a><span>|</span><label class="collapse" for="c-37232328">[-]</label><label class="expand" for="c-37232328">[6 more]</label></div><br/><div class="children"><div class="content">Stop, you&#x27;re scarring me! I have mirrored drives in a zpool. If a pair dies I lose 18 TB. My most important stuff is cloud-replicated but still...</div><br/><div id="37233411" class="c"><input type="checkbox" id="c-37233411" checked=""/><div class="controls bullet"><span class="by">csydas</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37232328">parent</a><span>|</span><a href="#37233062">next</a><span>|</span><label class="collapse" for="c-37233411">[-]</label><label class="expand" for="c-37233411">[1 more]</label></div><br/><div class="children"><div class="content">Ah, don&#x27;t be scared. You&#x27;re at least starting to think on your data and replicating your most important stuff elsewhere. I&#x27;d still recommend a non-cloud copy also, but you&#x27;re probably okay :)<p>Take it as a good impetus to catalog your data and find those extra replication options. Data protection does cost you a bit to do it &quot;fully&quot;, but since I&#x27;ve worked on a backup solution before in a client facing way, trust me when I tell you that I&#x27;ve seen rather large businesses (a few you might even know as a household name) who have less consideration for their data than you&#x27;ve expressed in your 3 sentences :)<p>So just figure out which of your data _truly_ needs to survive at all costs, get a solid setup with personally owned storage for the backups in combination with cloud storage, and you&#x27;re probably fine.</div><br/></div></div><div id="37233062" class="c"><input type="checkbox" id="c-37233062" checked=""/><div class="controls bullet"><span class="by">teekert</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37232328">parent</a><span>|</span><a href="#37233411">prev</a><span>|</span><a href="#37232423">next</a><span>|</span><label class="collapse" for="c-37233062">[-]</label><label class="expand" for="c-37233062">[1 more]</label></div><br/><div class="children"><div class="content">3 2 1 backups, ddg it ;)</div><br/></div></div><div id="37232423" class="c"><input type="checkbox" id="c-37232423" checked=""/><div class="controls bullet"><span class="by">densh</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37232328">parent</a><span>|</span><a href="#37233062">prev</a><span>|</span><a href="#37232940">next</a><span>|</span><label class="collapse" for="c-37232423">[-]</label><label class="expand" for="c-37232423">[1 more]</label></div><br/><div class="children"><div class="content">My pairs of mirrored drives come from 2 different manufacturers to prevent a common fault that happens at once to both drives.</div><br/></div></div><div id="37232940" class="c"><input type="checkbox" id="c-37232940" checked=""/><div class="controls bullet"><span class="by">orangepurple</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37232328">parent</a><span>|</span><a href="#37232423">prev</a><span>|</span><a href="#37231895">next</a><span>|</span><label class="collapse" for="c-37232940">[-]</label><label class="expand" for="c-37232940">[2 more]</label></div><br/><div class="children"><div class="content">You should probably be using RAID Z2 in that case which supports simultaneous 2 drive failure without a problem</div><br/><div id="37233413" class="c"><input type="checkbox" id="c-37233413" checked=""/><div class="controls bullet"><span class="by">benoliver999</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37232940">parent</a><span>|</span><a href="#37231895">next</a><span>|</span><label class="collapse" for="c-37233413">[-]</label><label class="expand" for="c-37233413">[1 more]</label></div><br/><div class="children"><div class="content">I actually had this happen, and RAID Z2 saved me from a very long recovery process.<p>I thought it might be the controller but a year on I&#x27;ve had no further issues. Sometimes drives do just go like lightbulbs.</div><br/></div></div></div></div></div></div><div id="37231895" class="c"><input type="checkbox" id="c-37231895" checked=""/><div class="controls bullet"><span class="by">vardump</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37231725">parent</a><span>|</span><a href="#37232328">prev</a><span>|</span><a href="#37232722">next</a><span>|</span><label class="collapse" for="c-37231895">[-]</label><label class="expand" for="c-37231895">[7 more]</label></div><br/><div class="children"><div class="content">That&#x27;s extremely unlikely. Could it have been the controller instead? Which HGST drives?</div><br/><div id="37232025" class="c"><input type="checkbox" id="c-37232025" checked=""/><div class="controls bullet"><span class="by">repiret</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37231895">parent</a><span>|</span><a href="#37231946">next</a><span>|</span><label class="collapse" for="c-37232025">[-]</label><label class="expand" for="c-37232025">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve frequently had drives in a RAID fail in rapid succession.  If you buy a bunch of identical drives at the same time and put them in a RAID, then you can end up with:<p>* They were manufactured in the same batch, maybe even one right after another on the same line.<p>* As they were transported from manufacturer to OEM to you they were exposed the the same environmental conditions, right down to vibrations, humidity, and ambient EM environment.<p>* As you use them, they continue to be exposed to the same environmental conditions, including power supply fluctuations and power inductively coupled into places it doesn&#x27;t belong.<p>* They see the same usage patterns.  Depending on the RAID specifics, that might be right down to seeing the same disk locations seeing the same read and write volume.<p>Its then not surprising if they fail at about the same time.<p>The last machine I put together that I wanted to have high availability, I intentionally bought two  different brand drives to put in the mirror to maximize the likelihood that they fail at very different times.<p>Many years ago (c. 2003) the group I was working in inherited a massive 6U storage server with an insane number of 10k SCSI (it was before SAS was a thing) drives.  We named it &quot;hurricane&quot; for the sound it made.  After a few weeks of using it, the first drive failed.  It rebuilt to a hot spare and we ordered and eventually installed a replacement.  A few weeks later, another drive failed, and this time before it could finish rebuilding, two more drives in the RAID failed and its contents lost (but we had a good backup).  We never used it again.  For a while I used it as a coffee table, but then someone convinced me that was too tacky, and it got ewasted.</div><br/><div id="37232707" class="c"><input type="checkbox" id="c-37232707" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37232025">parent</a><span>|</span><a href="#37232347">next</a><span>|</span><label class="collapse" for="c-37232707">[-]</label><label class="expand" for="c-37232707">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Its then not surprising if they fail at about the same time.</i><p>It is, but in a different way. It is a testament to the depth and precision of manufacturing process control, that two insanely complex machines will behave nearly identically for years, up to the point of failing at about the same time, if they&#x27;ve been made in the same batch, and exposed to about the same environment and usage patterns over those years. You&#x27;d expect any number of random factors to cause one drive fail way before the other, but no - not only there is very little variation between drives in a batch, tiny variations in usage are damped down instead of amplified.<p>It truly is amazing.</div><br/></div></div><div id="37232347" class="c"><input type="checkbox" id="c-37232347" checked=""/><div class="controls bullet"><span class="by">jjav</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37232025">parent</a><span>|</span><a href="#37232707">prev</a><span>|</span><a href="#37231946">next</a><span>|</span><label class="collapse" for="c-37232347">[-]</label><label class="expand" for="c-37232347">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If you buy a bunch of identical drives at the same time and put them in a RAID<p>When setting up a new machine with zfs I intentionally buy drives from as many different brands and models as possible to spread the manufacturing defect risk.</div><br/></div></div></div></div><div id="37231946" class="c"><input type="checkbox" id="c-37231946" checked=""/><div class="controls bullet"><span class="by">jms</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37231895">parent</a><span>|</span><a href="#37232025">prev</a><span>|</span><a href="#37232050">next</a><span>|</span><label class="collapse" for="c-37231946">[-]</label><label class="expand" for="c-37231946">[1 more]</label></div><br/><div class="children"><div class="content">Not extremely unlikely if they were identical drives from the same manufacturing batch.  It&#x27;s good practise to use diverse manufacturers or at least batches when adding disks to a raid array for just this reason.</div><br/></div></div><div id="37232050" class="c"><input type="checkbox" id="c-37232050" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37231895">parent</a><span>|</span><a href="#37231946">prev</a><span>|</span><a href="#37232724">next</a><span>|</span><label class="collapse" for="c-37232050">[-]</label><label class="expand" for="c-37232050">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not unreasonable to believe that if you pick two identical products off the same shelf at the same time (as one would logically do when purchasing 2 of a single item), that the two products were manufactured at similar times and in similar conditions.<p>Your model isn&#x27;t exactly bad, but there is an assumption being made that you haven&#x27;t accounted for. Which to be fair, is frequently not stated. The assumption is that the drives defects are independent of one another. This is a poor assumption when manufactured back to back.</div><br/></div></div><div id="37232724" class="c"><input type="checkbox" id="c-37232724" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37231895">parent</a><span>|</span><a href="#37232050">prev</a><span>|</span><a href="#37232722">next</a><span>|</span><label class="collapse" for="c-37232724">[-]</label><label class="expand" for="c-37232724">[1 more]</label></div><br/><div class="children"><div class="content">Or could be that the tolerances and environmental factors were so tighly matched between the drives.</div><br/></div></div></div></div><div id="37232722" class="c"><input type="checkbox" id="c-37232722" checked=""/><div class="controls bullet"><span class="by">nicman23</span><span>|</span><a href="#37231598">root</a><span>|</span><a href="#37231725">parent</a><span>|</span><a href="#37231895">prev</a><span>|</span><a href="#37230879">next</a><span>|</span><label class="collapse" for="c-37232722">[-]</label><label class="expand" for="c-37232722">[1 more]</label></div><br/><div class="children"><div class="content">i hope me mixing the models and buy times (i have been using the raidz expansion branch) will keep that raidz alive.<p>it has become non economical &#x2F; practical for me to backup everything</div><br/></div></div></div></div></div></div><div id="37230879" class="c"><input type="checkbox" id="c-37230879" checked=""/><div class="controls bullet"><span class="by">NoZebra120vClip</span><span>|</span><a href="#37231598">prev</a><span>|</span><a href="#37232079">next</a><span>|</span><label class="collapse" for="c-37230879">[-]</label><label class="expand" for="c-37230879">[45 more]</label></div><br/><div class="children"><div class="content">Just before I exited the Linux world entirely, I was beginning to chip away at the iceberg known as btrfs, and it was fascinating. I saw so much promise in many of its features, for revolutionizing backups and organizing my disks and everything.<p>Now btrfs isn&#x27;t ZFS, but it has some feature parity and perhaps the &quot;poor man&#x27;s ZFS&quot;. It&#x27;s also much more reasonable to run on certain OS, due to the licensing, packaging, and in-kernel status of ZFS being kind of weird.<p>One memorable time I was encouraged to use ZFS was when I mentioned to the Linux User&#x27;s Group that I&#x27;d had to pull the power cord to reboot my computer, and I was roundly scorned for this foolish maneuver. But you may change your mind about the wisdom of doing either one when you consider that the system in question was a Raspberry Pi. Heh.</div><br/><div id="37230999" class="c"><input type="checkbox" id="c-37230999" checked=""/><div class="controls bullet"><span class="by">eternityforest</span><span>|</span><a href="#37230879">parent</a><span>|</span><a href="#37230957">next</a><span>|</span><label class="collapse" for="c-37230999">[-]</label><label class="expand" for="c-37230999">[37 more]</label></div><br/><div class="children"><div class="content">The pi supposedly can get FS corruption, but I&#x27;ve never seen it because every time I install an image, I run a script that puts tmpfses everywhere and turns off mostly useless logging.   Those things just run for ever, very reliably, as long as you don&#x27;t hammer the SD card.<p>ZFS looks so cool! Unfortunately when I eventually get a NAS I doubt I&#x27;ll want to pay for anything that can run it, so I suspect I&#x27;ll just be doing RAID and ext4.<p>I always stayed away from BTRFS, because every few months I&#x27;d see a &quot;BTRFS destroyed my data&quot; post, followed by an argument about if it was BTRFSes fault.  I see them less now, perhaps it&#x27;s time to revisit?</div><br/><div id="37231194" class="c"><input type="checkbox" id="c-37231194" checked=""/><div class="controls bullet"><span class="by">snailmailman</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37230999">parent</a><span>|</span><a href="#37232807">next</a><span>|</span><label class="collapse" for="c-37231194">[-]</label><label class="expand" for="c-37231194">[6 more]</label></div><br/><div class="children"><div class="content">I had multiple pi systems âbrickedâ after a power outage. Presumably it was file system corruption? I never looked into the issues further. I just wiped the drives and reinstalled. These were vanilla raspbian installs at the time. It happened a few times when I was first trying out my pi. These were a mix of me cutting the power and actual power outages.<p>I only ever had these issues with SD cards. I quickly switched to running my pi off a USB external SSD, and havenât had any problems since then. Now when the power goes out, it boots back up properly and all my services start. All of this on ext3 I think?<p>Planning to redo things for ZFS at some point, but havenât gotten around to it yet.</div><br/><div id="37231220" class="c"><input type="checkbox" id="c-37231220" checked=""/><div class="controls bullet"><span class="by">cwillu</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231194">parent</a><span>|</span><a href="#37231241">next</a><span>|</span><label class="collapse" for="c-37231220">[-]</label><label class="expand" for="c-37231220">[3 more]</label></div><br/><div class="children"><div class="content">Yeah, sd cards that you haven&#x27;t put through extensive crash testing simply can&#x27;t be trusted.  I used to have a jar of sd cards that didn&#x27;t survive testing.</div><br/><div id="37231340" class="c"><input type="checkbox" id="c-37231340" checked=""/><div class="controls bullet"><span class="by">ronjouch</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231220">parent</a><span>|</span><a href="#37231241">next</a><span>|</span><label class="collapse" for="c-37231340">[-]</label><label class="expand" for="c-37231340">[2 more]</label></div><br/><div class="children"><div class="content">The most noticeable raspberrypi SD card life lengthener for me has been to write logs to RAM (assuming you have a stable setup and don&#x27;t count on them to survive a reboot!).<p>Our $job dashboards used to nuke an SD card every couple weeks&#x2F;months, but since the move to logs-in-RAM we&#x27;ve been running the same SDs for years.<p>DIY via {fs,journalctl} config , or using <a href="https:&#x2F;&#x2F;github.com&#x2F;azlux&#x2F;log2ram">https:&#x2F;&#x2F;github.com&#x2F;azlux&#x2F;log2ram</a><p>Also, mount the SD with the `noatime` flag of course: <a href="https:&#x2F;&#x2F;wiki.archlinux.org&#x2F;title&#x2F;Ext4#Disabling_access_time_update" rel="nofollow noreferrer">https:&#x2F;&#x2F;wiki.archlinux.org&#x2F;title&#x2F;Ext4#Disabling_access_time_...</a></div><br/><div id="37232267" class="c"><input type="checkbox" id="c-37232267" checked=""/><div class="controls bullet"><span class="by">eternityforest</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231340">parent</a><span>|</span><a href="#37231241">next</a><span>|</span><label class="collapse" for="c-37232267">[-]</label><label class="expand" for="c-37232267">[1 more]</label></div><br/><div class="children"><div class="content">Noatime, disable swap, logs in RAM, &#x2F;tmp in RAM, .xsession errors in RAM(With logrotate or it will fill up! So many random problems can make a problem there), chromium profile folder in RAM if you do kiosk work(Browser makers must hate flash!).<p>From what I hear Home Assistant is still not the easiest if you want to run for years on a card, not sure if that&#x27;s fixed now, but it&#x27;s one of the big factors blocking me from moving to HA.</div><br/></div></div></div></div></div></div><div id="37231241" class="c"><input type="checkbox" id="c-37231241" checked=""/><div class="controls bullet"><span class="by">ekianjo</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231194">parent</a><span>|</span><a href="#37231220">prev</a><span>|</span><a href="#37232807">next</a><span>|</span><label class="collapse" for="c-37231241">[-]</label><label class="expand" for="c-37231241">[2 more]</label></div><br/><div class="children"><div class="content">If you automate a fsck at every boot you can make these kind of issues go away on the pi</div><br/><div id="37233314" class="c"><input type="checkbox" id="c-37233314" checked=""/><div class="controls bullet"><span class="by">dspillett</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231241">parent</a><span>|</span><a href="#37232807">next</a><span>|</span><label class="collapse" for="c-37233314">[-]</label><label class="expand" for="c-37233314">[1 more]</label></div><br/><div class="children"><div class="content">Vjjnba</div><br/></div></div></div></div></div></div><div id="37232807" class="c"><input type="checkbox" id="c-37232807" checked=""/><div class="controls bullet"><span class="by">benlivengood</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37230999">parent</a><span>|</span><a href="#37231194">prev</a><span>|</span><a href="#37231136">next</a><span>|</span><label class="collapse" for="c-37232807">[-]</label><label class="expand" for="c-37232807">[1 more]</label></div><br/><div class="children"><div class="content">&gt; ZFS looks so cool! Unfortunately when I eventually get a NAS I doubt I&#x27;ll want to pay for anything that can run it, so I suspect I&#x27;ll just be doing RAID and ext4.<p>I ran ZFS on a Raspberry Pi 4 with 8GB of RAM just fine (under debian arm64), and I&#x27;ve used ZFS on a machine with 4GB of RAM for receiving snapshots.</div><br/></div></div><div id="37231136" class="c"><input type="checkbox" id="c-37231136" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37230999">parent</a><span>|</span><a href="#37232807">prev</a><span>|</span><a href="#37232703">next</a><span>|</span><label class="collapse" for="c-37231136">[-]</label><label class="expand" for="c-37231136">[11 more]</label></div><br/><div class="children"><div class="content">&gt; ZFS looks so cool! Unfortunately when I eventually get a NAS I doubt I&#x27;ll want to pay for anything that can run it, so I suspect I&#x27;ll just be doing RAID and ext4.<p>FreeBSD won&#x27;t break your wallet.</div><br/><div id="37231332" class="c"><input type="checkbox" id="c-37231332" checked=""/><div class="controls bullet"><span class="by">boomboomsubban</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231136">parent</a><span>|</span><a href="#37232703">next</a><span>|</span><label class="collapse" for="c-37231332">[-]</label><label class="expand" for="c-37231332">[10 more]</label></div><br/><div class="children"><div class="content">I assumed they are talking about the commonly spread myth that you need a gig of RAM for every terabyte of storage. I think that&#x27;s recommended when doing deduplication, but for a simple NAS ZFS would use a comparable amount of memory as ext4 on RAID.</div><br/><div id="37231469" class="c"><input type="checkbox" id="c-37231469" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231332">parent</a><span>|</span><a href="#37232077">next</a><span>|</span><label class="collapse" for="c-37231469">[-]</label><label class="expand" for="c-37231469">[1 more]</label></div><br/><div class="children"><div class="content">Oh yeah. That was always a sketchy recomendation, but it sure did make its way around. I think deduplication is remarkably seductive, but doesn&#x27;t seem worth the cost for almost anyone, given how it&#x27;s implemented. IIRC, btrfs has a dedup option where you can link up duplicates later, and then you don&#x27;t have to hold a dedupe table of everything all the time, and don&#x27;t need to collect writes to check the dedupe table, etc. But rewriting data isn&#x27;t how zfs rolls, and I get that.</div><br/></div></div><div id="37232077" class="c"><input type="checkbox" id="c-37232077" checked=""/><div class="controls bullet"><span class="by">pseudalopex</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231332">parent</a><span>|</span><a href="#37231469">prev</a><span>|</span><a href="#37231399">next</a><span>|</span><label class="collapse" for="c-37232077">[-]</label><label class="expand" for="c-37232077">[4 more]</label></div><br/><div class="children"><div class="content">Or the myth ZFS without ECC is more dangerous than anything else without ECC.</div><br/><div id="37232275" class="c"><input type="checkbox" id="c-37232275" checked=""/><div class="controls bullet"><span class="by">fho</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37232077">parent</a><span>|</span><a href="#37231399">next</a><span>|</span><label class="collapse" for="c-37232275">[-]</label><label class="expand" for="c-37232275">[3 more]</label></div><br/><div class="children"><div class="content">Not sure if a myth, iirc it was literally in the ZFS manual last time I looked into ZFS (which to be fair was 10+ years ago).</div><br/><div id="37232402" class="c"><input type="checkbox" id="c-37232402" checked=""/><div class="controls bullet"><span class="by">pseudalopex</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37232275">parent</a><span>|</span><a href="#37232352">next</a><span>|</span><label class="collapse" for="c-37232402">[-]</label><label class="expand" for="c-37232402">[1 more]</label></div><br/><div class="children"><div class="content">The documentation author refuted it 9 years ago.[1] Probably your understanding or memory was incorrect.<p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=8438239">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=8438239</a></div><br/></div></div><div id="37232352" class="c"><input type="checkbox" id="c-37232352" checked=""/><div class="controls bullet"><span class="by">boomboomsubban</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37232275">parent</a><span>|</span><a href="#37232402">prev</a><span>|</span><a href="#37231399">next</a><span>|</span><label class="collapse" for="c-37232352">[-]</label><label class="expand" for="c-37232352">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;openzfs.org&#x2F;wiki&#x2F;System_Administration#Data_Integrity" rel="nofollow noreferrer">https:&#x2F;&#x2F;openzfs.org&#x2F;wiki&#x2F;System_Administration#Data_Integrit...</a><p>&gt;Misinformation has been circulated on the FreeNAS forums that ZFS data integrity features are somehow worse than those of other filesystems when ECC RAM is not used. That has been thoroughly debunked. All software needs ECC RAM for reliable operation and ZFS is no different from any other filesystem in that regard</div><br/></div></div></div></div></div></div><div id="37231399" class="c"><input type="checkbox" id="c-37231399" checked=""/><div class="controls bullet"><span class="by">tambourine_man</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231332">parent</a><span>|</span><a href="#37232077">prev</a><span>|</span><a href="#37232703">next</a><span>|</span><label class="collapse" for="c-37231399">[-]</label><label class="expand" for="c-37231399">[4 more]</label></div><br/><div class="children"><div class="content">Is that really the case? Can you point to some references backing that up? (Hehe, unintentional pun)</div><br/><div id="37231766" class="c"><input type="checkbox" id="c-37231766" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231399">parent</a><span>|</span><a href="#37231642">next</a><span>|</span><label class="collapse" for="c-37231766">[-]</label><label class="expand" for="c-37231766">[2 more]</label></div><br/><div class="children"><div class="content">ZFS will happily use a large amount of RAM for caching if you have it, but it&#x27;ll run fine on a recent Pi (3 or 4, or not raspberry at all).<p>It&#x27;ll run fine but more sadly on older Pis running 32-bit kernels, since it does a looooooooot of 64-bit and wider operations, so you pay a nasty tax on that on 32-bit things. (Though the virtual address space limits might actually be sadder than the 64-bit operation penalty there, really...)</div><br/><div id="37232787" class="c"><input type="checkbox" id="c-37232787" checked=""/><div class="controls bullet"><span class="by">waynesonfire</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231766">parent</a><span>|</span><a href="#37231642">next</a><span>|</span><label class="collapse" for="c-37232787">[-]</label><label class="expand" for="c-37232787">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m running ZFS on the smallest AWS instance, running FreeBSD, and it does what i want it to do.</div><br/></div></div></div></div><div id="37231642" class="c"><input type="checkbox" id="c-37231642" checked=""/><div class="controls bullet"><span class="by">boomboomsubban</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231399">parent</a><span>|</span><a href="#37231766">prev</a><span>|</span><a href="#37232703">next</a><span>|</span><label class="collapse" for="c-37231642">[-]</label><label class="expand" for="c-37231642">[1 more]</label></div><br/><div class="children"><div class="content">TrueNAS&#x27; general requirements are 8GB, and they spell out when you want more. Most of the situations you&#x27;d want more you&#x27;d also want more on RAID. <a href="https:&#x2F;&#x2F;www.truenas.com&#x2F;docs&#x2F;core&#x2F;gettingstarted&#x2F;corehardwareguide&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.truenas.com&#x2F;docs&#x2F;core&#x2F;gettingstarted&#x2F;corehardwar...</a></div><br/></div></div></div></div></div></div></div></div><div id="37232703" class="c"><input type="checkbox" id="c-37232703" checked=""/><div class="controls bullet"><span class="by">8fingerlouie</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37230999">parent</a><span>|</span><a href="#37231136">prev</a><span>|</span><a href="#37231156">next</a><span>|</span><label class="collapse" for="c-37232703">[-]</label><label class="expand" for="c-37232703">[3 more]</label></div><br/><div class="children"><div class="content">&gt;  I&#x27;ll just be doing RAID and ext4<p>Why do you need&#x2F;want RAID ?<p>RAID is for availability, but is your data really that important that you cannot wait for a restore ? Most people would be much better off using that 1..N parity drives as versioned backups instead of running RAID.<p>I&#x27;ve run NAS boxes for years, but these days i&#x27;m only using single drives.<p>My setup these days consists of laptops that synchronizes data (encrypted) to the cloud, and a small ARM machine that synchronizes cloud contents locally, and makes a versioned backup to a single drive as well as a versioned cloud backup.<p>As for cost, it&#x27;s cheaper (for me) to store my data in the cloud, than the cost of electricity required to run my NAS.</div><br/><div id="37233112" class="c"><input type="checkbox" id="c-37233112" checked=""/><div class="controls bullet"><span class="by">eternityforest</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37232703">parent</a><span>|</span><a href="#37231156">next</a><span>|</span><label class="collapse" for="c-37233112">[-]</label><label class="expand" for="c-37233112">[2 more]</label></div><br/><div class="children"><div class="content">I was going to do RAID for disk failure tolerance, either with a consumer NAS box, or a disk enclosure that does hardware RAID, and something very low power like a Zero 2 W.<p>Isn&#x27;t RAID parity slightly more space efficient than versioned backups? Or is there a better way to do redundancy that doesn&#x27;t involve just replicating entire files to multiple disks? Or some kind of automated manager that puts each individual file on N different disks out of M?<p>I mostly do embedded so reliable data storage isn&#x27;t generally something I deal with, we usually leave that to the cloud or to the user, and I&#x27;m not quite familiar with what&#x27;s out there.</div><br/><div id="37233346" class="c"><input type="checkbox" id="c-37233346" checked=""/><div class="controls bullet"><span class="by">8fingerlouie</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37233112">parent</a><span>|</span><a href="#37231156">next</a><span>|</span><label class="collapse" for="c-37233346">[-]</label><label class="expand" for="c-37233346">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Isn&#x27;t RAID parity slightly more space efficient than versioned backups?<p>It depends on your storage array. The more drives, the more space efficient RAID becomes, but RAID is still only a single copy of your data.<p>&gt;Or is there a better way to do redundancy that doesn&#x27;t involve just replicating entire files to multiple disks<p>Most of the industry is using erasure coding these days (<a href="https:&#x2F;&#x2F;blog.min.io&#x2F;erasure-coding&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;blog.min.io&#x2F;erasure-coding&#x2F;</a>) which allows for spreading your parity and data across multiple sites. Erasure coding usually runs a layer above the filesystem, as opposed to RAID which typically runs below the filesystem (Snapraid, Mergerfs and others excluded).<p>My personal &quot;backup vault&quot; is a Raspberry Pi 4 with a single 4TB external drive attached. The RPi runs Minio, and all backups are done through the S3 interface or SFTP&#x2F;SMB. It is not the fastest box in the world, but it backs up (incremental) ~2TB in 30 minutes, which is &quot;fast enough&quot;.<p>It consumes on average 4W, which means even with worst case electricity prices of â¬1&#x2F;kWh (which we saw last winter), it costs less than â¬3&#x2F;month.<p>For comparison, my NAS consumed around 50W, and at â¬1&#x2F;kWh, that would cost â¬37&#x2F;month in electricity alone, and then you need to add the cost of the actual hardware itself.<p>I switched off the NAS, and purchased ~10TB of cloud storage (main storage and backup storage at two different locations) for â¬20&#x2F;month, and keep sensitive stuff encrypted with Cryptomator.</div><br/></div></div></div></div></div></div><div id="37231156" class="c"><input type="checkbox" id="c-37231156" checked=""/><div class="controls bullet"><span class="by">TillE</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37230999">parent</a><span>|</span><a href="#37232703">prev</a><span>|</span><a href="#37232204">next</a><span>|</span><label class="collapse" for="c-37231156">[-]</label><label class="expand" for="c-37231156">[4 more]</label></div><br/><div class="children"><div class="content">The only filesystem I&#x27;ve ever completely lost to corruption was btrfs, and that was about a year ago. btrfs-restore completely failed, so if I really needed that data I guess I&#x27;d have to do some manual surgery. I got to the point in the documentation where the only recourse was &quot;idk go ask someone on IRC&quot;.<p>Of course if you have good backups, you can use whatever and not really worry too much about it.</div><br/><div id="37231880" class="c"><input type="checkbox" id="c-37231880" checked=""/><div class="controls bullet"><span class="by">mattpallissard</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231156">parent</a><span>|</span><a href="#37232205">next</a><span>|</span><label class="collapse" for="c-37231880">[-]</label><label class="expand" for="c-37231880">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve used it extensively for many years, both professionally and personally.   Historically it&#x27;s been something that users needed to be paying attention to the mailing list and wikis.<p>For the most part though, sticking to standalone or mirrored disks is pretty rock solid and has been for a long time.  Ditto for subvolumes, snapshots and, send&#x2F;receive.  My laptop has been snapshotted and sent from one piece of hardware to the next for many years now.<p>That said, I&#x27;m with you on the backups.  Anyone who uses btrfs and doesn&#x27;t have a rock solid backups is a mad man.</div><br/></div></div><div id="37232205" class="c"><input type="checkbox" id="c-37232205" checked=""/><div class="controls bullet"><span class="by">eternityforest</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231156">parent</a><span>|</span><a href="#37231880">prev</a><span>|</span><a href="#37231518">next</a><span>|</span><label class="collapse" for="c-37232205">[-]</label><label class="expand" for="c-37232205">[1 more]</label></div><br/><div class="children"><div class="content">Well, I guess that resets my &quot;Has it been long enough since a BTRFS horror story that I can trust it&quot; counter.</div><br/></div></div><div id="37231518" class="c"><input type="checkbox" id="c-37231518" checked=""/><div class="controls bullet"><span class="by">cozzyd</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231156">parent</a><span>|</span><a href="#37232205">prev</a><span>|</span><a href="#37232204">next</a><span>|</span><label class="collapse" for="c-37231518">[-]</label><label class="expand" for="c-37231518">[1 more]</label></div><br/><div class="children"><div class="content">I had some ram fail in my laptop and that killed my btrfs filesystem. Though btrfs restore was able to recover almost everything eventually (which is good because my last backup was a few weeks before since I had been traveling). Decided to go back to boring old ext4.</div><br/></div></div></div></div><div id="37232204" class="c"><input type="checkbox" id="c-37232204" checked=""/><div class="controls bullet"><span class="by">chillfox</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37230999">parent</a><span>|</span><a href="#37231156">prev</a><span>|</span><a href="#37231068">next</a><span>|</span><label class="collapse" for="c-37232204">[-]</label><label class="expand" for="c-37232204">[1 more]</label></div><br/><div class="children"><div class="content">I am using USB disks with ZFS connected to a standard cheap office PC as my storage server. It still provides plenty of benefits over ext4.</div><br/></div></div><div id="37231068" class="c"><input type="checkbox" id="c-37231068" checked=""/><div class="controls bullet"><span class="by">upon_drumhead</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37230999">parent</a><span>|</span><a href="#37232204">prev</a><span>|</span><a href="#37231505">next</a><span>|</span><label class="collapse" for="c-37231068">[-]</label><label class="expand" for="c-37231068">[6 more]</label></div><br/><div class="children"><div class="content">Ubuntu ships with ZFS support, as well as TrueNAS.<p>I personally run my nas with Ubuntu and ZFS and love it.</div><br/><div id="37231929" class="c"><input type="checkbox" id="c-37231929" checked=""/><div class="controls bullet"><span class="by">justinclift</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231068">parent</a><span>|</span><a href="#37231088">next</a><span>|</span><label class="collapse" for="c-37231929">[-]</label><label class="expand" for="c-37231929">[4 more]</label></div><br/><div class="children"><div class="content">Apparently, recent releases of Ubuntu have dropped ZFS filesystem support after the person driving that effort left. :&#x2F;</div><br/><div id="37231995" class="c"><input type="checkbox" id="c-37231995" checked=""/><div class="controls bullet"><span class="by">upon_drumhead</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231929">parent</a><span>|</span><a href="#37231088">next</a><span>|</span><label class="collapse" for="c-37231995">[-]</label><label class="expand" for="c-37231995">[3 more]</label></div><br/><div class="children"><div class="content">Reading up on it, recent releases of Ubuntu have dropped using ZFS for your boot and root volumes, which isn&#x27;t ideal, however, they still support ZFS for any other volumes, which I&#x27;d venture is it&#x27;s primary usage anyway, and they don&#x27;t plan on removing support for anything other then zsys&#x2F;zfs root&#x2F;zfs boot.<p>But thanks for bringing this to my attention. I had missed the changes in 23.04.</div><br/><div id="37232112" class="c"><input type="checkbox" id="c-37232112" checked=""/><div class="controls bullet"><span class="by">chromakode</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231995">parent</a><span>|</span><a href="#37231088">next</a><span>|</span><label class="collapse" for="c-37232112">[-]</label><label class="expand" for="c-37232112">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s been a recent series of PRs which appear to be adding ZFS root support to subiquity, the new Ubuntu installer:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;canonical&#x2F;subiquity&#x2F;pull&#x2F;1689">https:&#x2F;&#x2F;github.com&#x2F;canonical&#x2F;subiquity&#x2F;pull&#x2F;1689</a></div><br/><div id="37232459" class="c"><input type="checkbox" id="c-37232459" checked=""/><div class="controls bullet"><span class="by">justinclift</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37232112">parent</a><span>|</span><a href="#37231088">next</a><span>|</span><label class="collapse" for="c-37232459">[-]</label><label class="expand" for="c-37232459">[1 more]</label></div><br/><div class="children"><div class="content">Cool, hopefully that means it&#x27;s being kept as an option after all. :)</div><br/></div></div></div></div></div></div></div></div><div id="37231088" class="c"><input type="checkbox" id="c-37231088" checked=""/><div class="controls bullet"><span class="by">chromakode</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231068">parent</a><span>|</span><a href="#37231929">prev</a><span>|</span><a href="#37231505">next</a><span>|</span><label class="collapse" for="c-37231088">[-]</label><label class="expand" for="c-37231088">[1 more]</label></div><br/><div class="children"><div class="content">Ditto!</div><br/></div></div></div></div><div id="37231505" class="c"><input type="checkbox" id="c-37231505" checked=""/><div class="controls bullet"><span class="by">akeck</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37230999">parent</a><span>|</span><a href="#37231068">prev</a><span>|</span><a href="#37231658">next</a><span>|</span><label class="collapse" for="c-37231505">[-]</label><label class="expand" for="c-37231505">[2 more]</label></div><br/><div class="children"><div class="content">Are you willing to share your script? I&#x27;d like to compare it to mine to see if I&#x27;ve missed anything. Thanks!</div><br/><div id="37232227" class="c"><input type="checkbox" id="c-37232227" checked=""/><div class="controls bullet"><span class="by">eternityforest</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231505">parent</a><span>|</span><a href="#37231658">next</a><span>|</span><label class="collapse" for="c-37232227">[-]</label><label class="expand" for="c-37232227">[1 more]</label></div><br/><div class="children"><div class="content">The version I&#x27;m using now is all tangled up with an installer and setup script(When I&#x27;m doing interactive installations I tent to try to reuse the same setup for everything, but there&#x27;s a big ASCII art banner for most of the relevant stuff for the SD card.<p>Note that this doesn&#x27;t have the Apache logfile hack so Apache probably won&#x27;t run if you try this and don&#x27;t add something to make it&#x27;s fussy logfile.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;EternityForest&#x2F;KaithemAutomation&#x2F;blob&#x2F;develop&#x2F;kaithem-kioskify.sh">https:&#x2F;&#x2F;github.com&#x2F;EternityForest&#x2F;KaithemAutomation&#x2F;blob&#x2F;dev...</a></div><br/></div></div></div></div><div id="37231658" class="c"><input type="checkbox" id="c-37231658" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37230999">parent</a><span>|</span><a href="#37231505">prev</a><span>|</span><a href="#37230957">next</a><span>|</span><label class="collapse" for="c-37231658">[-]</label><label class="expand" for="c-37231658">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the point of using ext4 on a NAS, and not XFS?</div><br/><div id="37232884" class="c"><input type="checkbox" id="c-37232884" checked=""/><div class="controls bullet"><span class="by">eternityforest</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231658">parent</a><span>|</span><a href="#37230957">next</a><span>|</span><label class="collapse" for="c-37232884">[-]</label><label class="expand" for="c-37232884">[1 more]</label></div><br/><div class="children"><div class="content">XFS is not the default on most systems and I hardly ever hear about it in general, so I really never paid much attention to it in.<p>Seems like people say it is more CPU heavy than EXT4, so unless it&#x27;s way more reliable, would it really be the best choice on a pi&#x2F;router&#x2F;subGHz commercial NAS chip?</div><br/></div></div></div></div></div></div><div id="37230957" class="c"><input type="checkbox" id="c-37230957" checked=""/><div class="controls bullet"><span class="by">KennyFromIT</span><span>|</span><a href="#37230879">parent</a><span>|</span><a href="#37230999">prev</a><span>|</span><a href="#37232037">next</a><span>|</span><label class="collapse" for="c-37230957">[-]</label><label class="expand" for="c-37230957">[5 more]</label></div><br/><div class="children"><div class="content">Alright, I&#x27;ll bite... What led you to leaving the Linux world entirely? What can &quot;the community&quot; learn from your experience to make it better for others?</div><br/><div id="37231063" class="c"><input type="checkbox" id="c-37231063" checked=""/><div class="controls bullet"><span class="by">NoZebra120vClip</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37230957">parent</a><span>|</span><a href="#37232037">next</a><span>|</span><label class="collapse" for="c-37231063">[-]</label><label class="expand" for="c-37231063">[4 more]</label></div><br/><div class="children"><div class="content">Linux is a great fantastic experience, and I have no qualms or ill will about it. I simply had no use for it anymore, and I needed to simplify. I&#x27;ve said before, I&#x27;m not a sysadmin anymore, I don&#x27;t tinker with systems, I need stuff to be operational and in production.<p>I still love Linux and I&#x27;d use it for any given server or Raspi if that were part of my job. I do use it daily in my job, but to a very minimal extent.</div><br/><div id="37231565" class="c"><input type="checkbox" id="c-37231565" checked=""/><div class="controls bullet"><span class="by">theaiquestion</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231063">parent</a><span>|</span><a href="#37232037">next</a><span>|</span><label class="collapse" for="c-37231565">[-]</label><label class="expand" for="c-37231565">[3 more]</label></div><br/><div class="children"><div class="content">Did you switch to a mac or to a windows machine?</div><br/><div id="37231778" class="c"><input type="checkbox" id="c-37231778" checked=""/><div class="controls bullet"><span class="by">worthless-trash</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231565">parent</a><span>|</span><a href="#37232037">next</a><span>|</span><label class="collapse" for="c-37231778">[-]</label><label class="expand" for="c-37231778">[2 more]</label></div><br/><div class="children"><div class="content">They said &quot;Operational and in production&quot; :)</div><br/><div id="37232561" class="c"><input type="checkbox" id="c-37232561" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#37230879">root</a><span>|</span><a href="#37231778">parent</a><span>|</span><a href="#37232037">next</a><span>|</span><label class="collapse" for="c-37232561">[-]</label><label class="expand" for="c-37232561">[1 more]</label></div><br/><div class="children"><div class="content">I honestly don&#x27;t know which one you implied.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37232732" class="c"><input type="checkbox" id="c-37232732" checked=""/><div class="controls bullet"><span class="by">nicman23</span><span>|</span><a href="#37230879">parent</a><span>|</span><a href="#37232037">prev</a><span>|</span><a href="#37232079">next</a><span>|</span><label class="collapse" for="c-37232732">[-]</label><label class="expand" for="c-37232732">[1 more]</label></div><br/><div class="children"><div class="content">btrfs only thing that i do not like at all is the fragmentation that it is prone to having. especially with sparse VMs images.<p>zvols are so much better for that</div><br/></div></div></div></div><div id="37232079" class="c"><input type="checkbox" id="c-37232079" checked=""/><div class="controls bullet"><span class="by">not_your_vase</span><span>|</span><a href="#37230879">prev</a><span>|</span><a href="#37230827">next</a><span>|</span><label class="collapse" for="c-37232079">[-]</label><label class="expand" for="c-37232079">[11 more]</label></div><br/><div class="children"><div class="content">A few years ago we had capacitor plague. Are we living now the storage plague? It&#x27;s getting ridiculous that all storage is getting worse and worse. WD is making HDDs crappy  with SMR, manufacturers says that 3 years operating time is already too much for SSDs and HDDs, and they don&#x27;t joke. I just had a Kingston SSD (okay, that was like 8 years old) and a portable WD HDD (~2 years old) die just this year.<p>The internet is full of problems lately about data loss and longevity issues.<p>I remember 20 years ago HDDs were not meant for eternity either, but they definitely outlived the usefulness of the computer that they were bought with...</div><br/><div id="37232139" class="c"><input type="checkbox" id="c-37232139" checked=""/><div class="controls bullet"><span class="by">kalleboo</span><span>|</span><a href="#37232079">parent</a><span>|</span><a href="#37232439">next</a><span>|</span><label class="collapse" for="c-37232139">[-]</label><label class="expand" for="c-37232139">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had a lot more luck with hard disks these days than I did 20 years ago. Remember 20 years ago was the era of the infamous IBM Deathstar drives where the magnetic coating would literally start sprinkling off the platters. Also the era of terrible terrible Maxtor drives that died in 1-2 years, which Seagate then bought and made their drives also unstable for a while. I ran a server with around 8 drives and had to keep replacing disks at the rate of about one per year.<p>Meanwhile today I&#x27;m helping admin a ZFS server with 20+ drives and drives have about a 4-5 year lifespan.<p>&gt; <i>but they definitely outlived the usefulness of the computer that they were bought with</i><p>Computers were also much more quickly obsoleted back then. When today a 6 year old computer is totally useable, back then you really felt it if your machine was just 3 years old.</div><br/><div id="37233260" class="c"><input type="checkbox" id="c-37233260" checked=""/><div class="controls bullet"><span class="by">olavgg</span><span>|</span><a href="#37232079">root</a><span>|</span><a href="#37232139">parent</a><span>|</span><a href="#37232512">next</a><span>|</span><label class="collapse" for="c-37233260">[-]</label><label class="expand" for="c-37233260">[1 more]</label></div><br/><div class="children"><div class="content">4-5 years? My old backup server has almost been running for 15 years now, 6x 500GB hard drives, one is even running on PATA. 8GB ECC ram, Athlon II. I could save some money replacing those 6 drives with 2x 14TB hard drives today. But as long it is working fine I ain&#x27;t gonna do something before I run out of hard drive space.</div><br/></div></div><div id="37232512" class="c"><input type="checkbox" id="c-37232512" checked=""/><div class="controls bullet"><span class="by">squarefoot</span><span>|</span><a href="#37232079">root</a><span>|</span><a href="#37232139">parent</a><span>|</span><a href="#37233260">prev</a><span>|</span><a href="#37232456">next</a><span>|</span><label class="collapse" for="c-37232512">[-]</label><label class="expand" for="c-37232512">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Remember 20 years ago was the era of the infamous IBM Deathstar drives where the magnetic coating would literally start sprinkling off the platters.<p>I have even older memories of problematic IBM drives. During the early 90s the shop I briefly worked with, found a supplier for IBM SCSI drives at a very convenient price, so they ordered a good lot of them. They worked great on PCs, but some of us also had Amiga machines and of course would love to benefit from the offer. So we tried one, but it didn&#x27;t work; then another, and another; nothing, they were normal SCSI drives but refused to work on any Amiga with a SCSI controller, although any other drive would work in there. In the end we abandoned all hopes and took the drives for a reformat to be used on PCs, but... they were all dead. Completely, not even detectable by any controller; the mere connection to an Amiga SCSI controller destroyed them instantly. We never discovered where the problem was; those drives worked perfectly on all PCs, while we could install any other drive on every Amiga and expect it to work, but no way to put <i>those</i> in an Amiga and expect it to survive. Good old times indeed:)</div><br/></div></div><div id="37232456" class="c"><input type="checkbox" id="c-37232456" checked=""/><div class="controls bullet"><span class="by">intothemild</span><span>|</span><a href="#37232079">root</a><span>|</span><a href="#37232139">parent</a><span>|</span><a href="#37232512">prev</a><span>|</span><a href="#37232439">next</a><span>|</span><label class="collapse" for="c-37232456">[-]</label><label class="expand" for="c-37232456">[2 more]</label></div><br/><div class="children"><div class="content">I remember those IBM Deathstar drives...<p>Was at the Aussie Tribes 2 launch LAN and there was a guy who had one die on him.<p>At that time in the LAN scene there would always be someone who had a Deskstar die ... You could hear the clicking over the noise of the LAN.<p>I realised back then, I can only trust Seagate.</div><br/><div id="37232589" class="c"><input type="checkbox" id="c-37232589" checked=""/><div class="controls bullet"><span class="by">chromakode</span><span>|</span><a href="#37232079">root</a><span>|</span><a href="#37232456">parent</a><span>|</span><a href="#37232439">next</a><span>|</span><label class="collapse" for="c-37232589">[-]</label><label class="expand" for="c-37232589">[1 more]</label></div><br/><div class="children"><div class="content">Hah, I had a Deathstar die on me back in the early 00s too. Surprisingly, about a decade later I hammered it with ddrescue and was able to get almost all the data off it!</div><br/></div></div></div></div></div></div><div id="37232439" class="c"><input type="checkbox" id="c-37232439" checked=""/><div class="controls bullet"><span class="by">squarefoot</span><span>|</span><a href="#37232079">parent</a><span>|</span><a href="#37232139">prev</a><span>|</span><a href="#37232114">next</a><span>|</span><label class="collapse" for="c-37232439">[-]</label><label class="expand" for="c-37232439">[1 more]</label></div><br/><div class="children"><div class="content">All WD drives I bought in the last decade work as expected (0), including the NAS ones bought after the introduction of that SMR thing; I just made sure they are either Plus or Pro, not the plain Red ones, which are SMR-plagued. I was also lucky with SSDs, but especially on desktops I use small ones as I still prefer to keep &#x2F;home dirs and RAID arrays on old rusty drives.<p>0: A couple exceptions: Two WD Red (before SMR) which I took out from my old NAS to put bigger drives in place, and put in a drawer while they were still perfectly healthy. After like 2.5 years in their anti static bags and normal conditions, no excessive heat, no moisture, no magnetic fields etc, I took them out because I needed a spare disk and checked them: both were not working, one completely dead and the other barely recognizable but unreadable. The first didn&#x27;t even show up once connected; I tried to clean all contacts, including the pins on the controller pcb to no avail, and eventually had to ditch it; the 2nd one could be reused only after a full reformat; no way to recover old data, not even using testdisk.
I never experienced nor expected anything like that, and frankly it worries me quite a lot.</div><br/></div></div><div id="37232114" class="c"><input type="checkbox" id="c-37232114" checked=""/><div class="controls bullet"><span class="by">andromeduck</span><span>|</span><a href="#37232079">parent</a><span>|</span><a href="#37232439">prev</a><span>|</span><a href="#37233020">next</a><span>|</span><label class="collapse" for="c-37232114">[-]</label><label class="expand" for="c-37232114">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t worry, your chips will start glitching after a few soon too.<p>We&#x27;re hitting scaling limits. Exponential growth is slowing.</div><br/><div id="37232758" class="c"><input type="checkbox" id="c-37232758" checked=""/><div class="controls bullet"><span class="by">nicman23</span><span>|</span><a href="#37232079">root</a><span>|</span><a href="#37232114">parent</a><span>|</span><a href="#37233020">next</a><span>|</span><label class="collapse" for="c-37232758">[-]</label><label class="expand" for="c-37232758">[1 more]</label></div><br/><div class="children"><div class="content">that does not make any sense if you are talking about the controllers.</div><br/></div></div></div></div><div id="37233020" class="c"><input type="checkbox" id="c-37233020" checked=""/><div class="controls bullet"><span class="by">boomboomsubban</span><span>|</span><a href="#37232079">parent</a><span>|</span><a href="#37232114">prev</a><span>|</span><a href="#37232314">next</a><span>|</span><label class="collapse" for="c-37233020">[-]</label><label class="expand" for="c-37233020">[1 more]</label></div><br/><div class="children"><div class="content">~20 years ago was the HGST &quot;deathstar,&quot; another drive so bad there was a class action lawsuit filed. Disks have always randomly died, that&#x27;s part of the reason Sun made zfs.</div><br/></div></div><div id="37232314" class="c"><input type="checkbox" id="c-37232314" checked=""/><div class="controls bullet"><span class="by">caskstrength</span><span>|</span><a href="#37232079">parent</a><span>|</span><a href="#37233020">prev</a><span>|</span><a href="#37230827">next</a><span>|</span><label class="collapse" for="c-37232314">[-]</label><label class="expand" for="c-37232314">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I remember 20 years ago HDDs were not meant for eternity either, but they definitely outlived the usefulness of the computer that they were bought with...<p>Anecdotally I remember HDDs failing sometimes for me and my friends&#x2F;relatives back in the day. Now it barely happens for SSDs. Hell, even supposedly problematic old Intel from late 2000s still works fine in the same old MacBook I gave to my mother after using it for years.<p>I wonder what it the actual data regarding this.</div><br/></div></div></div></div><div id="37230827" class="c"><input type="checkbox" id="c-37230827" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#37232079">prev</a><span>|</span><a href="#37230873">next</a><span>|</span><label class="collapse" for="c-37230827">[-]</label><label class="expand" for="c-37230827">[5 more]</label></div><br/><div class="children"><div class="content">Very quick summary: The mastodon thread refers to <a href="https:&#x2F;&#x2F;zrepl.github.io" rel="nofollow noreferrer">https:&#x2F;&#x2F;zrepl.github.io</a> &quot;zrepl is a one-stop, integrated solution for ZFS replication.&quot;</div><br/><div id="37231165" class="c"><input type="checkbox" id="c-37231165" checked=""/><div class="controls bullet"><span class="by">istjohn</span><span>|</span><a href="#37230827">parent</a><span>|</span><a href="#37230873">next</a><span>|</span><label class="collapse" for="c-37231165">[-]</label><label class="expand" for="c-37231165">[4 more]</label></div><br/><div class="children"><div class="content">Does anyone know how zrepl compares to sanoid&#x2F;syncoid other than that zrepl is written in Go and sanoid&#x2F;syncoid are Perl scripts?</div><br/><div id="37231268" class="c"><input type="checkbox" id="c-37231268" checked=""/><div class="controls bullet"><span class="by">etherael</span><span>|</span><a href="#37230827">root</a><span>|</span><a href="#37231165">parent</a><span>|</span><a href="#37230873">next</a><span>|</span><label class="collapse" for="c-37231268">[-]</label><label class="expand" for="c-37231268">[3 more]</label></div><br/><div class="children"><div class="content">I use sanoid to do basically the same thing as this, and was interested in giving it a shot to see if it was more hands off but it&#x27;s definitely a more complex setup to begin with, given you have to setup your own SSL certs etc, not sure why they wouldn&#x27;t just use SSH transport for this like everything else.</div><br/><div id="37231344" class="c"><input type="checkbox" id="c-37231344" checked=""/><div class="controls bullet"><span class="by">chromakode</span><span>|</span><a href="#37230827">root</a><span>|</span><a href="#37231268">parent</a><span>|</span><a href="#37230873">next</a><span>|</span><label class="collapse" for="c-37231344">[-]</label><label class="expand" for="c-37231344">[2 more]</label></div><br/><div class="children"><div class="content">I use Wireguard to secure and authenticate the transport. Much easier to set up! SSH is also an option.</div><br/><div id="37231368" class="c"><input type="checkbox" id="c-37231368" checked=""/><div class="controls bullet"><span class="by">etherael</span><span>|</span><a href="#37230827">root</a><span>|</span><a href="#37231344">parent</a><span>|</span><a href="#37230873">next</a><span>|</span><label class="collapse" for="c-37231368">[-]</label><label class="expand" for="c-37231368">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. Good to know that&#x27;s possible, it&#x27;s exactly what I use for sanoid also, so I guess the quickstart just assumes that layer isn&#x27;t available.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37230873" class="c"><input type="checkbox" id="c-37230873" checked=""/><div class="controls bullet"><span class="by">ChrisMarshallNY</span><span>|</span><a href="#37230827">prev</a><span>|</span><a href="#37230820">next</a><span>|</span><label class="collapse" for="c-37230873">[-]</label><label class="expand" for="c-37230873">[6 more]</label></div><br/><div class="children"><div class="content">I tend to use Appleâs Time Machine incremental backup to a Synology spinning rust server. I also have an external SSD that Iâll mirror the internal drive to, if Iâll be doing anything dodgy, or upgrading my machine.<p>That works. TM restores can be quite slow, but almost all my important data is in Git (and hosted storage), so itâs not really been an issue. I just use TM every now and then, if I have a single file I want to backtrack.<p>I also have one of the notorious[0] SanDisk drives. I donât use it for anything important. It just has some game storage. Since Iâm a Mac user, games arenât really much of a factor for me, and I wonât cry, if they croak.<p>[0] <a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2023&#x2F;08&#x2F;sandisk-extreme-ssds-are-worthless-multiple-lawsuits-against-wd-say&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2023&#x2F;08&#x2F;sandisk-extreme-ssds...</a></div><br/><div id="37231349" class="c"><input type="checkbox" id="c-37231349" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#37230873">parent</a><span>|</span><a href="#37231079">next</a><span>|</span><label class="collapse" for="c-37231349">[-]</label><label class="expand" for="c-37231349">[1 more]</label></div><br/><div class="children"><div class="content">I use TM as well.  I got an app (<a href="https:&#x2F;&#x2F;tclementdev.com&#x2F;timemachineeditor&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;tclementdev.com&#x2F;timemachineeditor&#x2F;</a>) that will manually trigger TM backups whenever the machine is idle.  Seems to work a lot better than the Apple automatic or timed backups.</div><br/></div></div><div id="37231079" class="c"><input type="checkbox" id="c-37231079" checked=""/><div class="controls bullet"><span class="by">op00to</span><span>|</span><a href="#37230873">parent</a><span>|</span><a href="#37231349">prev</a><span>|</span><a href="#37230820">next</a><span>|</span><label class="collapse" for="c-37231079">[-]</label><label class="expand" for="c-37231079">[4 more]</label></div><br/><div class="children"><div class="content">Iâve given up on Time Machine. It never seems to work past a month or two for me on my Synology w&#x2F; atalk etc.</div><br/><div id="37231338" class="c"><input type="checkbox" id="c-37231338" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#37230873">root</a><span>|</span><a href="#37231079">parent</a><span>|</span><a href="#37232230">next</a><span>|</span><label class="collapse" for="c-37231338">[-]</label><label class="expand" for="c-37231338">[2 more]</label></div><br/><div class="children"><div class="content">It was always breaking on my Synology too.  I ended up just attaching an 8TB spinning rust directly to the Mac and it&#x27;s been flawless since.<p>Time Machine really doesn&#x27;t like using remote disks that aren&#x27;t official Apple gear.</div><br/><div id="37232224" class="c"><input type="checkbox" id="c-37232224" checked=""/><div class="controls bullet"><span class="by">kalleboo</span><span>|</span><a href="#37230873">root</a><span>|</span><a href="#37231338">parent</a><span>|</span><a href="#37232230">next</a><span>|</span><label class="collapse" for="c-37232224">[-]</label><label class="expand" for="c-37232224">[1 more]</label></div><br/><div class="children"><div class="content">Even when I had an Apple Time Capsule, it would break about once a year. It&#x27;s just a flakey system. Wish they&#x27;d add the equivalent of zsend to APFS instead of using the weird &quot;gigantic sparse disk image with hard links in it&quot; system</div><br/></div></div></div></div><div id="37232230" class="c"><input type="checkbox" id="c-37232230" checked=""/><div class="controls bullet"><span class="by">kalleboo</span><span>|</span><a href="#37230873">root</a><span>|</span><a href="#37231079">parent</a><span>|</span><a href="#37231338">prev</a><span>|</span><a href="#37230820">next</a><span>|</span><label class="collapse" for="c-37232230">[-]</label><label class="expand" for="c-37232230">[1 more]</label></div><br/><div class="children"><div class="content">FWIW Apple has deprecated AFP&#x2F;AppleTalk and you should disable it on the Synology. It&#x27;s far more stable with SMB (but still not great)</div><br/></div></div></div></div></div></div><div id="37230820" class="c"><input type="checkbox" id="c-37230820" checked=""/><div class="controls bullet"><span class="by">windows2020</span><span>|</span><a href="#37230873">prev</a><span>|</span><a href="#37231916">next</a><span>|</span><label class="collapse" for="c-37230820">[-]</label><label class="expand" for="c-37230820">[24 more]</label></div><br/><div class="children"><div class="content">Could snapshotting the filesystem every 10 minutes have contributed to its death?</div><br/><div id="37230909" class="c"><input type="checkbox" id="c-37230909" checked=""/><div class="controls bullet"><span class="by">codetrotter</span><span>|</span><a href="#37230820">parent</a><span>|</span><a href="#37230914">next</a><span>|</span><label class="collapse" for="c-37230909">[-]</label><label class="expand" for="c-37230909">[3 more]</label></div><br/><div class="children"><div class="content">ZFS is very special, and it is cheap to make snapshots with ZFS, because ZFS uses copy-on-write.<p>Intuitively I would think that the amount of extra writes is pretty low, even if you snapshot very frequently.<p>But scientific measurements would be nice.<p>I used to do snapshots every minute, every hour and every day with ZFS on some servers I administered. Iâd purge the minute snapshots after 60 minutes. And I had cron jobs on other machines to backup the hourly and daily snapshots. I had it set up so that hourly snapshots were kept for something like 72 hours. And the daily snapshots were kept forever.<p>The idea with the every minute snapshots being that they were for undoing manually made mistakes during SQL migrations etc.<p>It worked well for me.<p>I still use ZFS on my FreeBSD servers. But at the moment my projects are low traffic and the data only changes in important ways some rare times. So with my current personal servers I manually snapshot about once a week and manually trigger a backup of that from another server.<p>Another thing Iâve changed is that now I only snapshot the parts of the file system where I store PostgreSQL databases and other application data. I no longer care so much about snapshotting the operating system data and such. If I have a serious hardware malfunction I will do a fresh install of the OS, and I have a log of what important config values are used and so on, that my backup scripts copy when I run them, without copying all of the other things.</div><br/><div id="37232147" class="c"><input type="checkbox" id="c-37232147" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#37230820">root</a><span>|</span><a href="#37230909">parent</a><span>|</span><a href="#37232635">next</a><span>|</span><label class="collapse" for="c-37232147">[-]</label><label class="expand" for="c-37232147">[1 more]</label></div><br/><div class="children"><div class="content">This sounds overkill even for production data, much less personal data, particularly the every minute and the fact you keep dailies forever.<p>Unless you&#x27;re a custodian of some secret society&#x27;s files!</div><br/></div></div><div id="37232635" class="c"><input type="checkbox" id="c-37232635" checked=""/><div class="controls bullet"><span class="by">oefrha</span><span>|</span><a href="#37230820">root</a><span>|</span><a href="#37230909">parent</a><span>|</span><a href="#37232147">prev</a><span>|</span><a href="#37230914">next</a><span>|</span><label class="collapse" for="c-37232635">[-]</label><label class="expand" for="c-37232635">[1 more]</label></div><br/><div class="children"><div class="content">Copy-on-write and cheap snapshots was quite special when ZFS was created. Itâs hardly special in 2023, when every single non-vintage iPhone, iPad and Mac has that.</div><br/></div></div></div></div><div id="37230914" class="c"><input type="checkbox" id="c-37230914" checked=""/><div class="controls bullet"><span class="by">georgyo</span><span>|</span><a href="#37230820">parent</a><span>|</span><a href="#37230909">prev</a><span>|</span><a href="#37230958">next</a><span>|</span><label class="collapse" for="c-37230914">[-]</label><label class="expand" for="c-37230914">[5 more]</label></div><br/><div class="children"><div class="content">Not likely.<p>The snapshot doesn&#x27;t write much, and both SSDs and ZFS are copy on write. Which means the cost of writing after a snapshot is the same as before the snapshot.<p>On the other hand context is missing. Both SSDs and ZFS don&#x27;t like being full or even close to full. The working set was ~650GB, of the drive was 1TB, then those snapshots could have easily made the drive over 90% full. This could have made ZFS unhappy all by itself.</div><br/><div id="37230992" class="c"><input type="checkbox" id="c-37230992" checked=""/><div class="controls bullet"><span class="by">chromakode</span><span>|</span><a href="#37230820">root</a><span>|</span><a href="#37230914">parent</a><span>|</span><a href="#37231629">next</a><span>|</span><label class="collapse" for="c-37230992">[-]</label><label class="expand" for="c-37230992">[1 more]</label></div><br/><div class="children"><div class="content">I agree that it was unlikely. The total size of all data and snapshots was 625 GiB on a 2 TB drive (which had seen less than 2 years of moderate use). It was a pretty unexpected failure.</div><br/></div></div><div id="37231629" class="c"><input type="checkbox" id="c-37231629" checked=""/><div class="controls bullet"><span class="by">Xaiph_Rahci</span><span>|</span><a href="#37230820">root</a><span>|</span><a href="#37230914">parent</a><span>|</span><a href="#37230992">prev</a><span>|</span><a href="#37230958">next</a><span>|</span><label class="collapse" for="c-37231629">[-]</label><label class="expand" for="c-37231629">[3 more]</label></div><br/><div class="children"><div class="content">&gt; cost of writing after a snapshot is the same as before the snapshot<p>I didn&#x27;t understand this, could you please clarify?<p>If there was no snapshot, there would be only one write operation, the actual write. However, with snapshot in place, in addition to actual write, there is a copy operation which copies the original data and writes to snapshot location. So, there should be two write operations (actual + copy).</div><br/><div id="37231687" class="c"><input type="checkbox" id="c-37231687" checked=""/><div class="controls bullet"><span class="by">boomboomsubban</span><span>|</span><a href="#37230820">root</a><span>|</span><a href="#37231629">parent</a><span>|</span><a href="#37231799">next</a><span>|</span><label class="collapse" for="c-37231687">[-]</label><label class="expand" for="c-37231687">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no copy operation, the previous data isn&#x27;t overwritten and the new data is written to a new block. It&#x27;s &quot;copy-on-write.&quot;</div><br/></div></div><div id="37231799" class="c"><input type="checkbox" id="c-37231799" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#37230820">root</a><span>|</span><a href="#37231629">parent</a><span>|</span><a href="#37231687">prev</a><span>|</span><a href="#37230958">next</a><span>|</span><label class="collapse" for="c-37231799">[-]</label><label class="expand" for="c-37231799">[1 more]</label></div><br/><div class="children"><div class="content">ZFS is never overwriting in place in either case, you&#x27;re just not freeing the old one if it&#x27;s in a snapshot, and a snapshot is just a note that &quot;nickname this point in time &#x27;mysnapshot&#x27;, and don&#x27;t clean up anything referenced at this point in time&quot;, so it&#x27;s very cheap to make, and you just check it later when you would be cleaning things up.</div><br/></div></div></div></div></div></div><div id="37230958" class="c"><input type="checkbox" id="c-37230958" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#37230820">parent</a><span>|</span><a href="#37230914">prev</a><span>|</span><a href="#37230848">next</a><span>|</span><label class="collapse" for="c-37230958">[-]</label><label class="expand" for="c-37230958">[1 more]</label></div><br/><div class="children"><div class="content">As I understand it, taking a snapshot with ZFS involves writing a metadata object and some data references. Assuming 100 GB of data, 128K block size, and 64 bit pointers, I&#x27;d guesstimate * that <i>new</i> data written during a snapshot would be in the ballpark of 5 MB. Is doing that 6 times per hour (52,560 times per year) enough to cause premature wear on the drive? That would be ~256 GB per year. This is likely under 1% of an SSD&#x27;s write endurance. So, I&#x27;d be surprised if taking 10 minute snapshots was a significant causal factor.<p>* I could be wrong, I asked for some help from not the most reliable sources. Happy to be corrected. Still, if my estimate is <i>higher</i> than actual and yet still unlikely to affect drive longevity, it may be moot.</div><br/></div></div><div id="37230848" class="c"><input type="checkbox" id="c-37230848" checked=""/><div class="controls bullet"><span class="by">HankB99</span><span>|</span><a href="#37230820">parent</a><span>|</span><a href="#37230958">prev</a><span>|</span><a href="#37231665">next</a><span>|</span><label class="collapse" for="c-37230848">[-]</label><label class="expand" for="c-37230848">[1 more]</label></div><br/><div class="children"><div class="content">Not likely. A snapshot just marks the most recently written block and prevents previous blocks from being altered. (More or less.) Since ZFS is copy on write, any changes to files will involve the same writes and some previously written data will not be deleted.</div><br/></div></div><div id="37231665" class="c"><input type="checkbox" id="c-37231665" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#37230820">parent</a><span>|</span><a href="#37230848">prev</a><span>|</span><a href="#37231176">next</a><span>|</span><label class="collapse" for="c-37231665">[-]</label><label class="expand" for="c-37231665">[4 more]</label></div><br/><div class="children"><div class="content">Minimum write size of a modern Flash chip can be ~100MB(!) according to a comment found in a random orange website[1]. So 5MB write every 10 minutes can be 600MB&#x2F;hr, which is 4.8TB&#x2F;8-hr-day, which is 24TB&#x2F;40-hour-week, which is 3.43 DWPD real time for a 1TB drive, and 2500 TBW in 2 years real time[2].<p>Official quoted specification for SN850 is 600 TBW of write endurance, likely after derating for obvious warranty implications. Incidentally, 2500TB is also a typical endurance figure for many SSDs in this market. Overall, to me, sounds not entirely impossible.<p>I kind of wonder what&#x27;s the controller says in SMART data, if still alive. On Linux the command is `apt install smartmontools; smartctl -s on &#x2F;dev&#x2F;sda; smartctl -A &#x2F;dev&#x2F;sda`, and it shall print out a table[4]. On Windows, just install CrystalDiskInfo[3].<p>1: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29165202">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29165202</a><p>2: DWPD: drive writes per day, TBW: Total Bytes Written - in terabytes<p>3: <a href="https:&#x2F;&#x2F;crystalmark.info&#x2F;en&#x2F;software&#x2F;crystaldiskinfo&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;crystalmark.info&#x2F;en&#x2F;software&#x2F;crystaldiskinfo&#x2F;</a><p>4: Note that &quot;Pre-fail&quot; means the value is supposed to <i>change when about to fail</i> and &quot;Old_age&quot; means the value is supposed to <i>indicate age</i>, NOT &quot;this is bad and about to fail&quot; and &quot;this drive is old&quot;. It always says all Pre-fail and Old_age. Someone should have changed it to &quot;somewhat_boolean&quot; and &quot;life_remain&quot; long time ago in my opinion.</div><br/><div id="37231746" class="c"><input type="checkbox" id="c-37231746" checked=""/><div class="controls bullet"><span class="by">chromakode</span><span>|</span><a href="#37230820">root</a><span>|</span><a href="#37231665">parent</a><span>|</span><a href="#37232283">next</a><span>|</span><label class="collapse" for="c-37231746">[-]</label><label class="expand" for="c-37231746">[1 more]</label></div><br/><div class="children"><div class="content">Unfortunately the drive didn&#x27;t appear accessible at all via nvme-cli. Interestingly it shows up in lspci but doesn&#x27;t get a &#x2F;dev&#x2F;nvme. It tends to hang the UEFIs of the two systems I tried it in when they try to read it.</div><br/></div></div><div id="37232283" class="c"><input type="checkbox" id="c-37232283" checked=""/><div class="controls bullet"><span class="by">kalleboo</span><span>|</span><a href="#37230820">root</a><span>|</span><a href="#37231665">parent</a><span>|</span><a href="#37231746">prev</a><span>|</span><a href="#37232107">next</a><span>|</span><label class="collapse" for="c-37232283">[-]</label><label class="expand" for="c-37232283">[1 more]</label></div><br/><div class="children"><div class="content">My machines have always been just constantly writing logs, like every couple seconds (macOS does this), and the write wear has never been anywhere near that bad. The advertised endurance must take into account write amplification for typical loads.</div><br/></div></div><div id="37232107" class="c"><input type="checkbox" id="c-37232107" checked=""/><div class="controls bullet"><span class="by">pseudalopex</span><span>|</span><a href="#37230820">root</a><span>|</span><a href="#37231665">parent</a><span>|</span><a href="#37232283">prev</a><span>|</span><a href="#37231176">next</a><span>|</span><label class="collapse" for="c-37232107">[-]</label><label class="expand" for="c-37232107">[1 more]</label></div><br/><div class="children"><div class="content">Minimum write size is not erase block size.</div><br/></div></div></div></div><div id="37231176" class="c"><input type="checkbox" id="c-37231176" checked=""/><div class="controls bullet"><span class="by">E39M5S62</span><span>|</span><a href="#37230820">parent</a><span>|</span><a href="#37231665">prev</a><span>|</span><a href="#37230846">next</a><span>|</span><label class="collapse" for="c-37231176">[-]</label><label class="expand" for="c-37231176">[2 more]</label></div><br/><div class="children"><div class="content">Nope. As others have mentioned, ZFS is CoW. Snapshots are &quot;free&quot; in that they (basically) point to a transaction group in the filesystem. They record a small amount of metadata to disk on each snapshot - on the order of a few MB. This is much much lower than an rclone&#x2F;sync style backup.</div><br/><div id="37231690" class="c"><input type="checkbox" id="c-37231690" checked=""/><div class="controls bullet"><span class="by">Izkata</span><span>|</span><a href="#37230820">root</a><span>|</span><a href="#37231176">parent</a><span>|</span><a href="#37230846">next</a><span>|</span><label class="collapse" for="c-37231690">[-]</label><label class="expand" for="c-37231690">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s also the least interesting part of comparing a ZFS backup to rsync&#x2F;rclone:  The rsync way is to crawl the entire tree being backed up to diff over the network then copy the differences.  Because of snapshots, ZFS already knows all the changes that occurred between snapshot A and snapshot B, and (provided state up to A has already been backed up) can update the backup by pushing all changes between A and B as one big binary blob without having to scan or diff anything.</div><br/></div></div></div></div><div id="37230846" class="c"><input type="checkbox" id="c-37230846" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#37230820">parent</a><span>|</span><a href="#37231176">prev</a><span>|</span><a href="#37231473">next</a><span>|</span><label class="collapse" for="c-37230846">[-]</label><label class="expand" for="c-37230846">[3 more]</label></div><br/><div class="children"><div class="content">No. Individual snapshots are a matter of kilobytes; desktop environments write considerably more.</div><br/><div id="37231102" class="c"><input type="checkbox" id="c-37231102" checked=""/><div class="controls bullet"><span class="by">istjohn</span><span>|</span><a href="#37230820">root</a><span>|</span><a href="#37230846">parent</a><span>|</span><a href="#37231473">next</a><span>|</span><label class="collapse" for="c-37231102">[-]</label><label class="expand" for="c-37231102">[2 more]</label></div><br/><div class="children"><div class="content">I read somewhere that snapshots are actually around 5 MB. Still not a lot, but a lot more than a few KB. A year&#x27;s worth of hourly snapshots comes to over 40 GB just in snapshot overhead.</div><br/><div id="37231164" class="c"><input type="checkbox" id="c-37231164" checked=""/><div class="controls bullet"><span class="by">chromakode</span><span>|</span><a href="#37230820">root</a><span>|</span><a href="#37231102">parent</a><span>|</span><a href="#37231473">next</a><span>|</span><label class="collapse" for="c-37231164">[-]</label><label class="expand" for="c-37231164">[1 more]</label></div><br/><div class="children"><div class="content">Another factor to weigh in my case is this laptop probably spends at least 50% of its life suspended. The overhead should be measured in MB per hour of uptime.</div><br/></div></div></div></div></div></div><div id="37230850" class="c"><input type="checkbox" id="c-37230850" checked=""/><div class="controls bullet"><span class="by">baby_souffle</span><span>|</span><a href="#37230820">parent</a><span>|</span><a href="#37231473">prev</a><span>|</span><a href="#37230880">next</a><span>|</span><label class="collapse" for="c-37230850">[-]</label><label class="expand" for="c-37230850">[1 more]</label></div><br/><div class="children"><div class="content">Nah, changes are COW so most snapshots are tiny.</div><br/></div></div><div id="37230880" class="c"><input type="checkbox" id="c-37230880" checked=""/><div class="controls bullet"><span class="by">abrookewood</span><span>|</span><a href="#37230820">parent</a><span>|</span><a href="#37230850">prev</a><span>|</span><a href="#37231166">next</a><span>|</span><label class="collapse" for="c-37230880">[-]</label><label class="expand" for="c-37230880">[1 more]</label></div><br/><div class="children"><div class="content">I guess it could contribute somewhat, but I don&#x27;t think it is that much additional work: for every new write (since the last snapshot), there is one additional read as the data is sent. It isn&#x27;t reading the whole file system, just the incremental data.</div><br/></div></div><div id="37231166" class="c"><input type="checkbox" id="c-37231166" checked=""/><div class="controls bullet"><span class="by">endisneigh</span><span>|</span><a href="#37230820">parent</a><span>|</span><a href="#37230880">prev</a><span>|</span><a href="#37231916">next</a><span>|</span><label class="collapse" for="c-37231166">[-]</label><label class="expand" for="c-37231166">[1 more]</label></div><br/><div class="children"><div class="content">Of course it contributed. But it probably wasnât the main reason or a significant contributor.</div><br/></div></div></div></div><div id="37231916" class="c"><input type="checkbox" id="c-37231916" checked=""/><div class="controls bullet"><span class="by">justinclift</span><span>|</span><a href="#37230820">prev</a><span>|</span><a href="#37231470">next</a><span>|</span><label class="collapse" for="c-37231916">[-]</label><label class="expand" for="c-37231916">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Not gonna lie, it was pretty terrifying until I had my first confirmation I could decrypt the data.<p>Sounds like the process was a bit less tested and documented than optimal.  For a home system or personal desktop that&#x27;s not super unusual.<p>You don&#x27;t want to be working out your restore procedure on the fly for production servers though. ;)</div><br/><div id="37232506" class="c"><input type="checkbox" id="c-37232506" checked=""/><div class="controls bullet"><span class="by">chromakode</span><span>|</span><a href="#37231916">parent</a><span>|</span><a href="#37231470">next</a><span>|</span><label class="collapse" for="c-37232506">[-]</label><label class="expand" for="c-37232506">[1 more]</label></div><br/><div class="children"><div class="content">For sure. I knew I had all the right ingredients to restore, and had kicked the tires 6 months back, but I initially copied over the wrong key. When it failed to load I had a sad moment until I realized my mistake. In such fail moments there&#x27;s a flash of clarity where every process gap becomes blindingly obvious.</div><br/></div></div></div></div><div id="37231470" class="c"><input type="checkbox" id="c-37231470" checked=""/><div class="controls bullet"><span class="by">DavidSJ</span><span>|</span><a href="#37231916">prev</a><span>|</span><a href="#37233193">next</a><span>|</span><label class="collapse" for="c-37231470">[-]</label><label class="expand" for="c-37231470">[5 more]</label></div><br/><div class="children"><div class="content"><i>My snapshots are encrypted by the original computer (this is cool because the NAS canât read them!). So I also needed to restore the encryption âwrapper keyâ to be able to use the backups.<p>Not gonna lie, it was pretty terrifying until I had my first confirmation I could decrypt the data.</i><p>Note: no bespoke backup method should be assumed functional unless you actually periodically check that you can restore data from it.</div><br/><div id="37231578" class="c"><input type="checkbox" id="c-37231578" checked=""/><div class="controls bullet"><span class="by">raisin_churn</span><span>|</span><a href="#37231470">parent</a><span>|</span><a href="#37231774">next</a><span>|</span><label class="collapse" for="c-37231578">[-]</label><label class="expand" for="c-37231578">[3 more]</label></div><br/><div class="children"><div class="content">Or non-bespoke.</div><br/><div id="37232155" class="c"><input type="checkbox" id="c-37232155" checked=""/><div class="controls bullet"><span class="by">DavidSJ</span><span>|</span><a href="#37231470">root</a><span>|</span><a href="#37231578">parent</a><span>|</span><a href="#37231663">next</a><span>|</span><label class="collapse" for="c-37232155">[-]</label><label class="expand" for="c-37232155">[1 more]</label></div><br/><div class="children"><div class="content">I added that word because at least with non-bespoke backups, you have probably thousands of users each day testing restoration out of necessity, and word might get around if the method failed to restore. But nevertheless, one should still test even then.</div><br/></div></div><div id="37231663" class="c"><input type="checkbox" id="c-37231663" checked=""/><div class="controls bullet"><span class="by">justinjlynn</span><span>|</span><a href="#37231470">root</a><span>|</span><a href="#37231578">parent</a><span>|</span><a href="#37232155">prev</a><span>|</span><a href="#37231774">next</a><span>|</span><label class="collapse" for="c-37231663">[-]</label><label class="expand" for="c-37231663">[1 more]</label></div><br/><div class="children"><div class="content">In short, backups aren&#x27;t actually taken until they have been verifiably restored.</div><br/></div></div></div></div><div id="37231774" class="c"><input type="checkbox" id="c-37231774" checked=""/><div class="controls bullet"><span class="by">totetsu</span><span>|</span><a href="#37231470">parent</a><span>|</span><a href="#37231578">prev</a><span>|</span><a href="#37233193">next</a><span>|</span><label class="collapse" for="c-37231774">[-]</label><label class="expand" for="c-37231774">[1 more]</label></div><br/><div class="children"><div class="content">I always meant to keep a copy of my LUKS header on separate disk, just in case, but..</div><br/></div></div></div></div><div id="37233193" class="c"><input type="checkbox" id="c-37233193" checked=""/><div class="controls bullet"><span class="by">copirate</span><span>|</span><a href="#37231470">prev</a><span>|</span><a href="#37232176">next</a><span>|</span><label class="collapse" for="c-37233193">[-]</label><label class="expand" for="c-37233193">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve read that ZFS is less safe than other Linux filesystems if you don&#x27;t use ECC RAM, because it assumes that there are no memory errors and therefore doesn&#x27;t provide a tool to repair a filesystem corrupted by such errors. Is this true?</div><br/><div id="37233309" class="c"><input type="checkbox" id="c-37233309" checked=""/><div class="controls bullet"><span class="by">Modified3019</span><span>|</span><a href="#37233193">parent</a><span>|</span><a href="#37232176">next</a><span>|</span><label class="collapse" for="c-37233309">[-]</label><label class="expand" for="c-37233309">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not true. That&#x27;s basically ancient forum myth, alongside the also incorrect &quot;ZFS needs 1GB memory per TB of HDD&quot; nonsense that has thankfully mostly died out finally. ZFS makes no additional assumptions when using ECC vs non-ECC memory.<p>It is theoretically possibly to construct a scenario where evil ram does all the exactly right things needed fool ZFS and corrupt your filesystem. Any pearl clutching about this thing which has never happened somehow also ignores that <i>every</i> filesystem is going to get corrupted.<p>In reality, while ECC memory is always nice to have, it&#x27;s no more required than any other filesystem. Though personally now that amounts of +32gb are common, I generally prefer error correction&#x2F;detection over ultimate speed these days. Though ironically ECC memory is actually really nice to overclock, because I can actually just check my logs and prove if my system is actually stable.<p>There so many actual dangers to your data in comparison that it&#x27;s laughable. The biggest one being <i>you</i>. Followed by hardware failure, malware, and genuine ZFS bugs. I&#x27;d stay far away from raw sends of encrypted datasets in ZFS for a while, there are edge cases that haven&#x27;t been resolved yet.<p><i>Edit</i> Longer article saying the same thing: <a href="https:&#x2F;&#x2F;jrs-s.net&#x2F;2015&#x2F;02&#x2F;03&#x2F;will-zfs-and-non-ecc-ram-kill-your-data&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;jrs-s.net&#x2F;2015&#x2F;02&#x2F;03&#x2F;will-zfs-and-non-ecc-ram-kill-y...</a></div><br/></div></div></div></div><div id="37232176" class="c"><input type="checkbox" id="c-37232176" checked=""/><div class="controls bullet"><span class="by">makach</span><span>|</span><a href="#37233193">prev</a><span>|</span><a href="#37231233">next</a><span>|</span><label class="collapse" for="c-37232176">[-]</label><label class="expand" for="c-37232176">[3 more]</label></div><br/><div class="children"><div class="content">I read this piece as someone who just got lucky. He never tested it until he needed it. 10 minutes is good. Donât let this story fool you to not take backups.</div><br/><div id="37232192" class="c"><input type="checkbox" id="c-37232192" checked=""/><div class="controls bullet"><span class="by">mijoharas</span><span>|</span><a href="#37232176">parent</a><span>|</span><a href="#37232474">next</a><span>|</span><label class="collapse" for="c-37232192">[-]</label><label class="expand" for="c-37232192">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Donât let this story fool you to not take backups.<p>Do you mean don&#x27;t let this story fool you to not test your backups? Because the whole point of the story is he was saved by having his backups. (Though you&#x27;re right that he lucked out by having it work when he hadn&#x27;t tested it)</div><br/></div></div><div id="37232474" class="c"><input type="checkbox" id="c-37232474" checked=""/><div class="controls bullet"><span class="by">chromakode</span><span>|</span><a href="#37232176">parent</a><span>|</span><a href="#37232192">prev</a><span>|</span><a href="#37231233">next</a><span>|</span><label class="collapse" for="c-37232474">[-]</label><label class="expand" for="c-37232474">[1 more]</label></div><br/><div class="children"><div class="content">I tested my backups about 6 months ago when I set up zrepl. When I mentioned it was scary until I could decrypt the data, that wasn&#x27;t the whole story, actually: initially I restored the wrong wrapper key and it failed to load!<p>It&#x27;s also scary in general to go from 2 copies to only 1 copy of data. A friend and I have been planning to trade replicas but haven&#x27;t set it up yet. There&#x27;s definitely still room for improvement in my setup.</div><br/></div></div></div></div><div id="37231233" class="c"><input type="checkbox" id="c-37231233" checked=""/><div class="controls bullet"><span class="by">KyleSanderson</span><span>|</span><a href="#37232176">prev</a><span>|</span><a href="#37232656">next</a><span>|</span><label class="collapse" for="c-37231233">[-]</label><label class="expand" for="c-37231233">[9 more]</label></div><br/><div class="children"><div class="content">bcachefs is the near future for Linux here.
<a href="https:&#x2F;&#x2F;bcachefs.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;bcachefs.org&#x2F;</a></div><br/><div id="37231522" class="c"><input type="checkbox" id="c-37231522" checked=""/><div class="controls bullet"><span class="by">arjvik</span><span>|</span><a href="#37231233">parent</a><span>|</span><a href="#37231484">next</a><span>|</span><label class="collapse" for="c-37231522">[-]</label><label class="expand" for="c-37231522">[2 more]</label></div><br/><div class="children"><div class="content">What does bcachefs do better than BTRFS or ZFS?</div><br/><div id="37231674" class="c"><input type="checkbox" id="c-37231674" checked=""/><div class="controls bullet"><span class="by">amarshall</span><span>|</span><a href="#37231233">root</a><span>|</span><a href="#37231522">parent</a><span>|</span><a href="#37231484">next</a><span>|</span><label class="collapse" for="c-37231674">[-]</label><label class="expand" for="c-37231674">[1 more]</label></div><br/><div class="children"><div class="content">Than both? Tiered storage. Than btrfs? Hopefully parity RAID, and performance. Than ZFS? Being GPL compliant, so viable for being in-tree in the Kernel.<p>(list most likely incomplete)</div><br/></div></div></div></div><div id="37231484" class="c"><input type="checkbox" id="c-37231484" checked=""/><div class="controls bullet"><span class="by">ac29</span><span>|</span><a href="#37231233">parent</a><span>|</span><a href="#37231522">prev</a><span>|</span><a href="#37231421">next</a><span>|</span><label class="collapse" for="c-37231484">[-]</label><label class="expand" for="c-37231484">[2 more]</label></div><br/><div class="children"><div class="content">Near future? Its been in development for 7 years and still hasnt been accepted upstream (an upstreaming effort is in progress, though).</div><br/><div id="37231703" class="c"><input type="checkbox" id="c-37231703" checked=""/><div class="controls bullet"><span class="by">KyleSanderson</span><span>|</span><a href="#37231233">root</a><span>|</span><a href="#37231484">parent</a><span>|</span><a href="#37231421">next</a><span>|</span><label class="collapse" for="c-37231703">[-]</label><label class="expand" for="c-37231703">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s just silliness (attitudes, versus technical merit) blocking it now. The only thing stopping it is if something happens to Kent.<p><a href="https:&#x2F;&#x2F;www.phoronix.com&#x2F;news&#x2F;Linux-Torvalds-Bcachefs-Review" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.phoronix.com&#x2F;news&#x2F;Linux-Torvalds-Bcachefs-Review</a></div><br/></div></div></div></div><div id="37231421" class="c"><input type="checkbox" id="c-37231421" checked=""/><div class="controls bullet"><span class="by">brunoqc</span><span>|</span><a href="#37231233">parent</a><span>|</span><a href="#37231484">prev</a><span>|</span><a href="#37232656">next</a><span>|</span><label class="collapse" for="c-37231421">[-]</label><label class="expand" for="c-37231421">[4 more]</label></div><br/><div class="children"><div class="content">Any tldr about why we should be looking forward to this?<p>Any cool things?</div><br/><div id="37231716" class="c"><input type="checkbox" id="c-37231716" checked=""/><div class="controls bullet"><span class="by">KyleSanderson</span><span>|</span><a href="#37231233">root</a><span>|</span><a href="#37231421">parent</a><span>|</span><a href="#37232656">next</a><span>|</span><label class="collapse" for="c-37231716">[-]</label><label class="expand" for="c-37231716">[3 more]</label></div><br/><div class="children"><div class="content">Erasure Coding when it lands should be pretty solid.<p>Until then per-directory data replicas is the killer feature for me (Music has 3, Documents has 5, Downloads has 1). Something to be very excited about with full compression and encryption.</div><br/><div id="37232865" class="c"><input type="checkbox" id="c-37232865" checked=""/><div class="controls bullet"><span class="by">vladvasiliu</span><span>|</span><a href="#37231233">root</a><span>|</span><a href="#37231716">parent</a><span>|</span><a href="#37232045">next</a><span>|</span><label class="collapse" for="c-37232865">[-]</label><label class="expand" for="c-37232865">[1 more]</label></div><br/><div class="children"><div class="content">You can do that with ZFS at the cost of defining separate filesystems per directory.<p>I don&#x27;t use multiple replicas, but I use that to tailor my backups per directory. ~&#x2F;documents is snapshotted and backed up on the regular, with long-lived snapshots. Code is snapshotted regularly, but snapshots don&#x27;t live too long, and they&#x27;re not shipped to a different drive. I don&#x27;t care for ~&#x2F;tmp so no snapshots.<p>---<p>edit: the cost, besides having to actually create the file systems, is that moving data between them isn&#x27;t instant.</div><br/></div></div><div id="37232045" class="c"><input type="checkbox" id="c-37232045" checked=""/><div class="controls bullet"><span class="by">brunoqc</span><span>|</span><a href="#37231233">root</a><span>|</span><a href="#37231716">parent</a><span>|</span><a href="#37232865">prev</a><span>|</span><a href="#37232656">next</a><span>|</span><label class="collapse" for="c-37232045">[-]</label><label class="expand" for="c-37232045">[1 more]</label></div><br/><div class="children"><div class="content">Nice, thanks!</div><br/></div></div></div></div></div></div></div></div><div id="37232656" class="c"><input type="checkbox" id="c-37232656" checked=""/><div class="controls bullet"><span class="by">sandreas</span><span>|</span><a href="#37231233">prev</a><span>|</span><a href="#37231832">next</a><span>|</span><label class="collapse" for="c-37232656">[-]</label><label class="expand" for="c-37232656">[2 more]</label></div><br/><div class="children"><div class="content">For every ZFS fan, I can recommend zfs-auto-snapshot[1]. I use it on my proxmox server[2] to auto manage snapshots incl throwing away old ones.<p>[1]: <a href="https:&#x2F;&#x2F;github.com&#x2F;zfsonlinux&#x2F;zfs-auto-snapshot">https:&#x2F;&#x2F;github.com&#x2F;zfsonlinux&#x2F;zfs-auto-snapshot</a><p>[2]: <a href="https:&#x2F;&#x2F;pilabor.com&#x2F;series&#x2F;proxmox&#x2F;restore-virtual-machine-via-zfs-snapshot&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;pilabor.com&#x2F;series&#x2F;proxmox&#x2F;restore-virtual-machine-v...</a></div><br/><div id="37232774" class="c"><input type="checkbox" id="c-37232774" checked=""/><div class="controls bullet"><span class="by">nicman23</span><span>|</span><a href="#37232656">parent</a><span>|</span><a href="#37231832">next</a><span>|</span><label class="collapse" for="c-37232774">[-]</label><label class="expand" for="c-37232774">[1 more]</label></div><br/><div class="children"><div class="content">i actually made 2 scripts to automate the sending and deletion of old snapshots along with one that calls auto snapshot on a vm rebooting &#x2F; shutting down. I was thinking that they might be useful to other people.</div><br/></div></div></div></div><div id="37231832" class="c"><input type="checkbox" id="c-37231832" checked=""/><div class="controls bullet"><span class="by">minimalist</span><span>|</span><a href="#37232656">prev</a><span>|</span><a href="#37230954">next</a><span>|</span><label class="collapse" for="c-37231832">[-]</label><label class="expand" for="c-37231832">[1 more]</label></div><br/><div class="children"><div class="content">For people who don&#x27;t want to use ZFS but are okay with LVM: wyng-backup (formerly sparsebak)<p><a href="https:&#x2F;&#x2F;github.com&#x2F;tasket&#x2F;wyng-backup">https:&#x2F;&#x2F;github.com&#x2F;tasket&#x2F;wyng-backup</a></div><br/></div></div><div id="37230954" class="c"><input type="checkbox" id="c-37230954" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#37231832">prev</a><span>|</span><a href="#37231008">next</a><span>|</span><label class="collapse" for="c-37230954">[-]</label><label class="expand" for="c-37230954">[10 more]</label></div><br/><div class="children"><div class="content">I just have rsync running in a cronjob. How is this significantly different?<p>I imagine it is, but I don&#x27;t know how.</div><br/><div id="37230981" class="c"><input type="checkbox" id="c-37230981" checked=""/><div class="controls bullet"><span class="by">kadoban</span><span>|</span><a href="#37230954">parent</a><span>|</span><a href="#37231048">next</a><span>|</span><label class="collapse" for="c-37230981">[-]</label><label class="expand" for="c-37230981">[6 more]</label></div><br/><div class="children"><div class="content">Mostly different in terms of performance and wear on the drives. If you rsync over and over, it has to scan basically the whole filesystem for changes each time. Zfs snapshots don&#x27;t. The snapshot is ~instant and the calculation of what to send has no need to examine any files.<p>I don&#x27;t think the performance and drive-lifetime hit of running rsync every 10 minutes would be good.<p>Zfs should have an edge in terms of atomicity as well, but in practice I&#x27;m not sure how much that matters. I _think_ it does matter but isn&#x27;t perfect (zfs can&#x27;t trick applications into doing atomic writes if they&#x27;re not already, but it won&#x27;t have _another_ worse layer of breaking atomicity like rsync must).</div><br/><div id="37231082" class="c"><input type="checkbox" id="c-37231082" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#37230954">root</a><span>|</span><a href="#37230981">parent</a><span>|</span><a href="#37232908">next</a><span>|</span><label class="collapse" for="c-37231082">[-]</label><label class="expand" for="c-37231082">[4 more]</label></div><br/><div class="children"><div class="content">Depends what else the machines are doing, and how much ram, and the rsync settings.<p>If you read all the files on both sides, every time and don&#x27;t have more ram than disk, it&#x27;s going to be a lot of work. If you&#x27;re just looking at directory entries most of the time, there&#x27;s a good chance that&#x27;s all cached and it&#x27;s no disk load, other than the small changes.<p>I ageee with you though that atomicity is a big difference, if it matters, and in most cases, it probably doesn&#x27;t.<p>Personally, I&#x27;ve mostly stopped doing rsync backups in favor of zfs send, but I&#x27;ve still got one I need to get around to changing. Sanoid&#x2F;syncoid is pretty decent for less effort snapshotting and syncing snapshots; but I haven&#x27;t done anything with encrypted datasets. For most of my systems, I&#x27;d prefer recovery over security. For the one system in iffy hosting, it runs full disk encryption as a layer below zfs, so it&#x27;s zfs sends are cleartext, too. (The hosting facility has given me other customer&#x27;s unwiped disks; better for me to assume my disks won&#x27;t be wiped)</div><br/><div id="37231150" class="c"><input type="checkbox" id="c-37231150" checked=""/><div class="controls bullet"><span class="by">kadoban</span><span>|</span><a href="#37230954">root</a><span>|</span><a href="#37231082">parent</a><span>|</span><a href="#37231240">next</a><span>|</span><label class="collapse" for="c-37231150">[-]</label><label class="expand" for="c-37231150">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If you read all the files on both sides, every time and don&#x27;t have more ram than disk, it&#x27;s going to be a lot of work. If you&#x27;re just looking at directory entries most of the time, there&#x27;s a good chance that&#x27;s all cached and it&#x27;s no disk load, other than smthe dmall changes.<p>Yeah that&#x27;s a good point. I know that rsync is _quite_ clever, but at least any incantations I&#x27;ve ever done it still hits the drives a good amount. I&#x27;d ballpark guess a couple of orders of magnitude better than just &quot;cp -r&quot; or something, but still a couple of orders of magnitude worse than zfs snapshots.<p>Yeah you&#x27;re 100% right it&#x27;ll depend on bunch of variables though.<p>&gt; Sanoid&#x2F;syncoid is pretty decent for less effor snapshotting and syncing snapshots; but I haven&#x27;t done anything with encrypted datasets.<p>I&#x27;m not sure I&#x27;d recommend it, but I use both directly on encrypted datasets. I have tested recovery a couple times and it works fine, but I&#x27;ve read some cautionary tales too. I _think_ they&#x27;re all old issues?</div><br/></div></div><div id="37231240" class="c"><input type="checkbox" id="c-37231240" checked=""/><div class="controls bullet"><span class="by">livueta</span><span>|</span><a href="#37230954">root</a><span>|</span><a href="#37231082">parent</a><span>|</span><a href="#37231150">prev</a><span>|</span><a href="#37232908">next</a><span>|</span><label class="collapse" for="c-37231240">[-]</label><label class="expand" for="c-37231240">[2 more]</label></div><br/><div class="children"><div class="content">Regarding atomicity, do you rsync from the live filesystem or from the hidden .zfs&#x2F;snapshot dir?<p>My impetus for swapping to a send-based approach from an rsync-based approach of running an incremental (just from the normal non-snapshot filesystem view) then taking a snap on the far side was encountering corrupted encrypted containers. If rsync ran while a container was mounted and being written to, it&#x27;d get an inconsistent view of the underlying file and produce a nonsense diff, resulting in an unmountable container in the backup. This doesn&#x27;t happen with send because it has a consistent view of the blocks it needs to replicate, but as I was writing this I realized that running rsync from zfs&#x27;s view of the snapshot might get around that. Of course, at that point it&#x27;s probably easiest to just use send.</div><br/><div id="37231445" class="c"><input type="checkbox" id="c-37231445" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#37230954">root</a><span>|</span><a href="#37231240">parent</a><span>|</span><a href="#37232908">next</a><span>|</span><label class="collapse" for="c-37231445">[-]</label><label class="expand" for="c-37231445">[1 more]</label></div><br/><div class="children"><div class="content">My rsync backup goes from the live filesystem (and then does some snapshot-like things on the other end with hard links and what not), but it&#x27;s for my househould&#x27;s shared network drive, so atomicity isn&#x27;t super important. Mostly there&#x27;s not many changes, and if there are, it&#x27;s ok if it takes a couple snapshots to settle.<p>I didn&#x27;t originally have that area as its own zfs filesystem, and it wasn&#x27;t even originally on zfs, but I moved things around when setting up a new, offsite, backup system... Just haven&#x27;t gotten around to redoing the old backups. I don&#x27;t think I&#x27;d spend any more time on rsync based backups, given how I use things now; incremental zfs means no need to compare, which makes me have good feelings.</div><br/></div></div></div></div></div></div><div id="37232908" class="c"><input type="checkbox" id="c-37232908" checked=""/><div class="controls bullet"><span class="by">vladvasiliu</span><span>|</span><a href="#37230954">root</a><span>|</span><a href="#37230981">parent</a><span>|</span><a href="#37231082">prev</a><span>|</span><a href="#37231048">next</a><span>|</span><label class="collapse" for="c-37232908">[-]</label><label class="expand" for="c-37232908">[1 more]</label></div><br/><div class="children"><div class="content">Is <i>reading</i> a drive actually that big a hit on the drive&#x27;s lifetime? At least for SSDs, I always see write endurance quoted, never read.<p>Sure, having to scan a whole tree of files can take a toll on the performance as perceived by other apps trying to use the drive.</div><br/></div></div></div></div><div id="37231048" class="c"><input type="checkbox" id="c-37231048" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#37230954">parent</a><span>|</span><a href="#37230981">prev</a><span>|</span><a href="#37231008">next</a><span>|</span><label class="collapse" for="c-37231048">[-]</label><label class="expand" for="c-37231048">[3 more]</label></div><br/><div class="children"><div class="content">While rsync does incremental backups fine, it doesn&#x27;t offer deduplication. If you want that, have a look around at newer options like restic or others such as borg (see a comprehensive list at <a href="https:&#x2F;&#x2F;github.com&#x2F;restic&#x2F;others">https:&#x2F;&#x2F;github.com&#x2F;restic&#x2F;others</a>)</div><br/><div id="37231107" class="c"><input type="checkbox" id="c-37231107" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#37230954">root</a><span>|</span><a href="#37231048">parent</a><span>|</span><a href="#37231008">next</a><span>|</span><label class="collapse" for="c-37231107">[-]</label><label class="expand" for="c-37231107">[2 more]</label></div><br/><div class="children"><div class="content">rsync does have --link-dest which you can sometimes use to get a (file level) dedup effect.<p>zfs dedupe is pretty expensive and doesn&#x27;t often work out like people might expect...</div><br/><div id="37231717" class="c"><input type="checkbox" id="c-37231717" checked=""/><div class="controls bullet"><span class="by">Izkata</span><span>|</span><a href="#37230954">root</a><span>|</span><a href="#37231107">parent</a><span>|</span><a href="#37231008">next</a><span>|</span><label class="collapse" for="c-37231717">[-]</label><label class="expand" for="c-37231717">[1 more]</label></div><br/><div class="children"><div class="content">&gt; rsync does have --link-dest which you can sometimes use to get a (file level) dedup effect.<p>Sadly for safety&#x27;s sake directory hardlinks pretty much don&#x27;t exist, so this doesn&#x27;t save as much as it could.  Apple hacked in an exception for Time Machine so they could get these additional savings.</div><br/></div></div></div></div></div></div></div></div><div id="37231008" class="c"><input type="checkbox" id="c-37231008" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#37230954">prev</a><span>|</span><a href="#37232399">next</a><span>|</span><label class="collapse" for="c-37231008">[-]</label><label class="expand" for="c-37231008">[3 more]</label></div><br/><div class="children"><div class="content">For those that don&#x27;t know, there are many wonderful incremental backup solutions that don&#x27;t require ZFS. * For one, I personally recommend Restic (<a href="https:&#x2F;&#x2F;restic.net" rel="nofollow noreferrer">https:&#x2F;&#x2F;restic.net</a>) because of its deduplication.<p>* People on macOS don&#x27;t have ZFS, well... maybe they could? See <a href="https:&#x2F;&#x2F;github.com&#x2F;spl&#x2F;zfs-on-mac">https:&#x2F;&#x2F;github.com&#x2F;spl&#x2F;zfs-on-mac</a></div><br/><div id="37232967" class="c"><input type="checkbox" id="c-37232967" checked=""/><div class="controls bullet"><span class="by">drexlspivey</span><span>|</span><a href="#37231008">parent</a><span>|</span><a href="#37231059">next</a><span>|</span><label class="collapse" for="c-37232967">[-]</label><label class="expand" for="c-37232967">[1 more]</label></div><br/><div class="children"><div class="content">macOS Time machine does incremental backups, is there any other reason wou might need ZFS?</div><br/></div></div><div id="37231059" class="c"><input type="checkbox" id="c-37231059" checked=""/><div class="controls bullet"><span class="by">chromakode</span><span>|</span><a href="#37231008">parent</a><span>|</span><a href="#37232967">prev</a><span>|</span><a href="#37232399">next</a><span>|</span><label class="collapse" for="c-37231059">[-]</label><label class="expand" for="c-37231059">[1 more]</label></div><br/><div class="children"><div class="content">bupstash.io was my favored option other than ZFS. It&#x27;s a beautiful and performant solution. Being filesystem agnostic is an advantage in many contexts.<p>In the end I chose ZFS for the efficiency of snapshots (vs. a full disk scan) and atomicity. Both enable more frequent, smaller syncs, which is perfect for a laptop.</div><br/></div></div></div></div><div id="37232399" class="c"><input type="checkbox" id="c-37232399" checked=""/><div class="controls bullet"><span class="by">Helmut10001</span><span>|</span><a href="#37231008">prev</a><span>|</span><a href="#37230886">next</a><span>|</span><label class="collapse" for="c-37232399">[-]</label><label class="expand" for="c-37232399">[3 more]</label></div><br/><div class="children"><div class="content">I wonder if zrepl could be run in WSL2 - would be nice to backup Windows computers as well using this approach.<p>At the moment, I use Nextcloud to sync data to my server. It is a more selective approach and Nextcloud is, per se, not a backup solution because not all files can be backed up.. and live-sync is always a half-baked backup solution.</div><br/><div id="37232892" class="c"><input type="checkbox" id="c-37232892" checked=""/><div class="controls bullet"><span class="by">vladvasiliu</span><span>|</span><a href="#37232399">parent</a><span>|</span><a href="#37230886">next</a><span>|</span><label class="collapse" for="c-37232892">[-]</label><label class="expand" for="c-37232892">[2 more]</label></div><br/><div class="children"><div class="content">How would that work? Zrepl ships ZFS snapshots. You could probably wrangle wsl2 to install its distro on zfs. If so, I see no reason why zrepl wouldn&#x27;t work <i>with the linux environment</i>. But snapshotting the whole Windows drive? I don&#x27;t think so.<p>If you want similar features, I think ReFS comes close. AFAICT it&#x27;s not supported as a boot drive.</div><br/><div id="37233366" class="c"><input type="checkbox" id="c-37233366" checked=""/><div class="controls bullet"><span class="by">Helmut10001</span><span>|</span><a href="#37232399">root</a><span>|</span><a href="#37232892">parent</a><span>|</span><a href="#37230886">next</a><span>|</span><label class="collapse" for="c-37233366">[-]</label><label class="expand" for="c-37233366">[1 more]</label></div><br/><div class="children"><div class="content">I thought maybe if Windows is installed on a ZFS volume, and WSL2 on another, Zrep from Linux should be able to backup both, snapshots from the Linux and the Windows volume. But a quick search on Google reveals that Windows on ZFS is not a thing, yet.</div><br/></div></div></div></div></div></div><div id="37230886" class="c"><input type="checkbox" id="c-37230886" checked=""/><div class="controls bullet"><span class="by">op00to</span><span>|</span><a href="#37232399">prev</a><span>|</span><a href="#37232432">next</a><span>|</span><label class="collapse" for="c-37230886">[-]</label><label class="expand" for="c-37230886">[6 more]</label></div><br/><div class="children"><div class="content">I wonder how much he gained from entirely restoring the system versus simply reprovisioning (gasp, even manually reinstalling) and restoring needed files a will. I&#x27;m not sure there&#x27;s a lot of value in snapshotting and restoring stuff in a lost ssd situation tthat&#x27;s also available in mirrors across the world.</div><br/><div id="37230964" class="c"><input type="checkbox" id="c-37230964" checked=""/><div class="controls bullet"><span class="by">chromakode</span><span>|</span><a href="#37230886">parent</a><span>|</span><a href="#37231291">next</a><span>|</span><label class="collapse" for="c-37230964">[-]</label><label class="expand" for="c-37230964">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve reflected similarly after this exercise.<p>I&#x27;ve lost data before, but it felt terrible to lose my context and working memory. While I make sure the most important stuff is in git, there&#x27;s a bunch of momentum and working memory in my bash history and system configuration. It&#x27;s also nice to not have to think very hard about a patchwork of backup plans.<p>It&#x27;s nice to get a fresh start every now and then, but not under duress. I was in the middle of a multi day project and was gonna lose time either way. It was real nice to boot back into a machine that felt like home.</div><br/></div></div><div id="37231291" class="c"><input type="checkbox" id="c-37231291" checked=""/><div class="controls bullet"><span class="by">etherael</span><span>|</span><a href="#37230886">parent</a><span>|</span><a href="#37230964">prev</a><span>|</span><a href="#37230916">next</a><span>|</span><label class="collapse" for="c-37231291">[-]</label><label class="expand" for="c-37231291">[1 more]</label></div><br/><div class="children"><div class="content">I do a version of this that doesn&#x27;t require restore at all, I have three separate physical systems that all share a VPN across the world, and wherever I am at any given time zfs snapshots are syncing across that VPN to those three physical systems depending on which one is the primary I&#x27;m using at any given point in time. They also use the VPN layer to check if they have peer status on a faster local network like the wifi or LAN and use that instead for the snapshot transfers if so.<p>If for any reason any one of these systems either is destroyed or is no longer master, picking up from where I left off is as simple as picking another system up and marking it &quot;master&quot;. No restore process, no changes, nothing at all, and it picks up from exactly where I left off when I was working on the other system. Means I can just grab my EDC laptop and stuff it in a bag not knowing how long I&#x27;ll be out or where I&#x27;ll be going and also know that it will be completely up to date with my datasets, or I can grab my desktop replacement laptop and its enormous external disk if I am going to be on a different continent for an extended period of time and want full geographic dataset locality. At no point in time does any of the above require the manual running of any process or replication or anything like that.<p>Reprovision and restore would take a whole lot longer than this, wouldn&#x27;t give the abilities that it provides, and the above is only possible because of zfs snapshot replication.<p>I also use a USB C external SSD that is a member of a ZFS mirror and a md raid group that is bootable, so even if my EDC laptop were to spontaneously combust, I could immediately get up and running on any similar laptop with roughly comparable hardware simply by putting that SSD in and booting from it, then adding the SSD on the laptop to the zfs mirror &#x2F; md raid group.</div><br/></div></div><div id="37230916" class="c"><input type="checkbox" id="c-37230916" checked=""/><div class="controls bullet"><span class="by">theossuary</span><span>|</span><a href="#37230886">parent</a><span>|</span><a href="#37231291">prev</a><span>|</span><a href="#37232432">next</a><span>|</span><label class="collapse" for="c-37230916">[-]</label><label class="expand" for="c-37230916">[3 more]</label></div><br/><div class="children"><div class="content">The biggest time save is in time spent recovering. It&#x27;s so much faster to restore the entire system than to reinstall the OS, reconfigure the bootloader, resetup disk encryption, reconfigure user accounts, reinstall all software, manually reload configs, etc.<p>Or put it more directly, full disk backups are a great way to get RTO down.</div><br/><div id="37231065" class="c"><input type="checkbox" id="c-37231065" checked=""/><div class="controls bullet"><span class="by">op00to</span><span>|</span><a href="#37230886">root</a><span>|</span><a href="#37230916">parent</a><span>|</span><a href="#37232432">next</a><span>|</span><label class="collapse" for="c-37231065">[-]</label><label class="expand" for="c-37231065">[2 more]</label></div><br/><div class="children"><div class="content">In this case the author had to do arcane magick to restore his zfs snapshot. This wasnât a routine raw dd restore.</div><br/><div id="37231122" class="c"><input type="checkbox" id="c-37231122" checked=""/><div class="controls bullet"><span class="by">chromakode</span><span>|</span><a href="#37230886">root</a><span>|</span><a href="#37231065">parent</a><span>|</span><a href="#37232432">next</a><span>|</span><label class="collapse" for="c-37231122">[-]</label><label class="expand" for="c-37231122">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. It was a large time investment that happened to pay off. If I ever have to do it again I&#x27;ll be much faster. I hope that with wider ZFS adoption some of the routine tasks will be automated better in the future. I see no reason why in a couple years this couldn&#x27;t be a mature fire and forget user experience.</div><br/></div></div></div></div></div></div></div></div><div id="37232432" class="c"><input type="checkbox" id="c-37232432" checked=""/><div class="controls bullet"><span class="by">Kiro</span><span>|</span><a href="#37230886">prev</a><span>|</span><a href="#37231058">next</a><span>|</span><label class="collapse" for="c-37232432">[-]</label><label class="expand" for="c-37232432">[5 more]</label></div><br/><div class="children"><div class="content">Am I the only one who doesn&#x27;t have any important data? If I lost everything today I would just start afresh and move on.</div><br/><div id="37232536" class="c"><input type="checkbox" id="c-37232536" checked=""/><div class="controls bullet"><span class="by">dkh</span><span>|</span><a href="#37232432">parent</a><span>|</span><a href="#37232738">next</a><span>|</span><label class="collapse" for="c-37232536">[-]</label><label class="expand" for="c-37232536">[3 more]</label></div><br/><div class="children"><div class="content">At all? Like, anywhere? Or do you just have data living on cloud services instead of locally?</div><br/><div id="37232615" class="c"><input type="checkbox" id="c-37232615" checked=""/><div class="controls bullet"><span class="by">Kiro</span><span>|</span><a href="#37232432">root</a><span>|</span><a href="#37232536">parent</a><span>|</span><a href="#37232738">next</a><span>|</span><label class="collapse" for="c-37232615">[-]</label><label class="expand" for="c-37232615">[2 more]</label></div><br/><div class="children"><div class="content">At all. What important data do you have? I really can&#x27;t think of anything that I would miss if everything disappeared.</div><br/><div id="37233409" class="c"><input type="checkbox" id="c-37233409" checked=""/><div class="controls bullet"><span class="by">Symbiote</span><span>|</span><a href="#37232432">root</a><span>|</span><a href="#37232615">parent</a><span>|</span><a href="#37232738">next</a><span>|</span><label class="collapse" for="c-37233409">[-]</label><label class="expand" for="c-37233409">[1 more]</label></div><br/><div class="children"><div class="content">Address book, photographs&#x2F;video of people I care about and holidays, personal diary, hobby projects, old letters&#x2F;emails, stored passwords, archived bank statements&#x2F;contracts&#x2F;insurance and other important documents.<p>I think you are very unusual if you don&#x27;t care about any of this.</div><br/></div></div></div></div></div></div><div id="37232738" class="c"><input type="checkbox" id="c-37232738" checked=""/><div class="controls bullet"><span class="by">unmole</span><span>|</span><a href="#37232432">parent</a><span>|</span><a href="#37232536">prev</a><span>|</span><a href="#37231058">next</a><span>|</span><label class="collapse" for="c-37232738">[-]</label><label class="expand" for="c-37232738">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Am I the only one who doesn&#x27;t have any important data?<p>Quite possibly.</div><br/></div></div></div></div><div id="37231058" class="c"><input type="checkbox" id="c-37231058" checked=""/><div class="controls bullet"><span class="by">shruggedatlas</span><span>|</span><a href="#37232432">prev</a><span>|</span><a href="#37230825">next</a><span>|</span><label class="collapse" for="c-37231058">[-]</label><label class="expand" for="c-37231058">[3 more]</label></div><br/><div class="children"><div class="content">How could I backup using incremental atomic snapshots on Windows?</div><br/><div id="37231189" class="c"><input type="checkbox" id="c-37231189" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#37231058">parent</a><span>|</span><a href="#37231539">next</a><span>|</span><label class="collapse" for="c-37231189">[-]</label><label class="expand" for="c-37231189">[1 more]</label></div><br/><div class="children"><div class="content">Look into the Windows Copy Shadow Service.<p>Unfortunately, I haven&#x27;t had luck with Open Source backup software that uses it (the shadow copy snapshot would fail, the error code would be no help, and finding no resources, I gave up), but commercial software I&#x27;ve used was great. When I was at a big corp, the commercial backup software whose name escapes me at the moment would litterally wait for files to be saved, then do an incremental backup.<p>As of now, I&#x27;m using Veeam on my personal machines, and it runs an incremental backup nightly and saves to an smb share.</div><br/></div></div><div id="37231539" class="c"><input type="checkbox" id="c-37231539" checked=""/><div class="controls bullet"><span class="by">deadbeeves</span><span>|</span><a href="#37231058">parent</a><span>|</span><a href="#37231189">prev</a><span>|</span><a href="#37230825">next</a><span>|</span><label class="collapse" for="c-37231539">[-]</label><label class="expand" for="c-37231539">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using urbackup for years. It does disk image-based backups and&#x2F;or file-based backups. Disk image-based backups are incremental at the block level, and file-based backups are incremental at the file level (so if a single byte of the file has changed, the entire file gets backed up). It uses the Volume Shadow Copy mechanism that the sibling comment mentioned to get atomicity and avoid file locking issues.</div><br/></div></div></div></div><div id="37230825" class="c"><input type="checkbox" id="c-37230825" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#37231058">prev</a><span>|</span><a href="#37231127">next</a><span>|</span><label class="collapse" for="c-37230825">[-]</label><label class="expand" for="c-37230825">[5 more]</label></div><br/><div class="children"><div class="content">spoiler: he has 10 minutes incremental backups.</div><br/><div id="37230897" class="c"><input type="checkbox" id="c-37230897" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37230825">parent</a><span>|</span><a href="#37231127">next</a><span>|</span><label class="collapse" for="c-37230897">[-]</label><label class="expand" for="c-37230897">[4 more]</label></div><br/><div class="children"><div class="content">Its &quot;backups&quot; join &quot;zfs makes snapshots easy&quot; join &quot;snapshots make incremental backups easy&quot; join &quot;backups on device aren&#x27;t a backup&quot; join &quot;I had off-device backups&quot;<p>which reduces to &quot;I had backups&quot; indeed.<p>3-2-1 forever!</div><br/><div id="37231650" class="c"><input type="checkbox" id="c-37231650" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#37230825">root</a><span>|</span><a href="#37230897">parent</a><span>|</span><a href="#37231127">next</a><span>|</span><label class="collapse" for="c-37231650">[-]</label><label class="expand" for="c-37231650">[3 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;backups on device aren&#x27;t a backup&quot;<p>That wasn&#x27;t part of the article.  It was a single drive failure, so RAID would have done fine.</div><br/><div id="37231900" class="c"><input type="checkbox" id="c-37231900" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37230825">root</a><span>|</span><a href="#37231650">parent</a><span>|</span><a href="#37231127">next</a><span>|</span><label class="collapse" for="c-37231900">[-]</label><label class="expand" for="c-37231900">[2 more]</label></div><br/><div class="children"><div class="content">Yes, RAID will get you over some failures. But, it still isn&#x27;t a backup. Backup is what gets you over corrupted RAID, loss of both sides of the mirror stripe, entire disk failure when its not RAID.<p>What he does is run zrep to make a backup. it covers his needs. ZFS snapshot by itself is only transitionally a &quot;backup&quot; for the immediacy of change, it&#x27;s the least safe form of backup if it remains on the same logical drive structure.</div><br/><div id="37232898" class="c"><input type="checkbox" id="c-37232898" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#37230825">root</a><span>|</span><a href="#37231900">parent</a><span>|</span><a href="#37231127">next</a><span>|</span><label class="collapse" for="c-37232898">[-]</label><label class="expand" for="c-37232898">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Yes, RAID will get you over some failures. But, it still isn&#x27;t a backup.<p>backups also can fail, that zrep can start failing after some os&#x2F;kernel update without notifying owner. The question is in probabilities of failures, I kinda would trust industrial raid more than some custom made hobby solution.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37231127" class="c"><input type="checkbox" id="c-37231127" checked=""/><div class="controls bullet"><span class="by">archo</span><span>|</span><a href="#37230825">prev</a><span>|</span><a href="#37230901">next</a><span>|</span><label class="collapse" for="c-37231127">[-]</label><label class="expand" for="c-37231127">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;archive.is&#x2F;VPEAP" rel="nofollow noreferrer">https:&#x2F;&#x2F;archive.is&#x2F;VPEAP</a></div><br/></div></div><div id="37230901" class="c"><input type="checkbox" id="c-37230901" checked=""/><div class="controls bullet"><span class="by">predictabl3</span><span>|</span><a href="#37231127">prev</a><span>|</span><a href="#37231884">next</a><span>|</span><label class="collapse" for="c-37230901">[-]</label><label class="expand" for="c-37230901">[6 more]</label></div><br/><div class="children"><div class="content">Zrepl is a big part of why I feel secure doing the digital nomad thing. A script, run nightlyish, opens a separate-headered LUKS-protected ZFS pool and then copies all snapshots over. That NVME enclosure lives in my &quot;purse&quot; that never leaves me sight&#x2F;body.<p>Between this and NixOS, I can provision a new <i>identical</i> laptop in about 10 minutes.<p>I recently added off-site replication as well, so even if I get completely devastatingly mugged, there&#x27;s still about zero chance of serious data loss.<p>Zrepl is absolutely brilliant software. Easy to run with, but incredibly sophisticated and powerful if you need all the knobs. I can&#x27;t praise it enough.</div><br/><div id="37231086" class="c"><input type="checkbox" id="c-37231086" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#37230901">parent</a><span>|</span><a href="#37231270">next</a><span>|</span><label class="collapse" for="c-37231086">[-]</label><label class="expand" for="c-37231086">[1 more]</label></div><br/><div class="children"><div class="content">Kudos. You&#x27;re probably safer than most people who are only one theft, fire, flood, or other disast</div><br/></div></div><div id="37231270" class="c"><input type="checkbox" id="c-37231270" checked=""/><div class="controls bullet"><span class="by">istjohn</span><span>|</span><a href="#37230901">parent</a><span>|</span><a href="#37231086">prev</a><span>|</span><a href="#37230961">next</a><span>|</span><label class="collapse" for="c-37231270">[-]</label><label class="expand" for="c-37231270">[1 more]</label></div><br/><div class="children"><div class="content">Do you have any experience with sanoid&#x2F;syncoid? What does Zrepl give you over them?</div><br/></div></div><div id="37230961" class="c"><input type="checkbox" id="c-37230961" checked=""/><div class="controls bullet"><span class="by">tmountain</span><span>|</span><a href="#37230901">parent</a><span>|</span><a href="#37231270">prev</a><span>|</span><a href="#37231884">next</a><span>|</span><label class="collapse" for="c-37230961">[-]</label><label class="expand" for="c-37230961">[3 more]</label></div><br/><div class="children"><div class="content">You should do a write up about how this works. It sounds very interesting.</div><br/><div id="37230988" class="c"><input type="checkbox" id="c-37230988" checked=""/><div class="controls bullet"><span class="by">predictabl3</span><span>|</span><a href="#37230901">root</a><span>|</span><a href="#37230961">parent</a><span>|</span><a href="#37231884">next</a><span>|</span><label class="collapse" for="c-37230988">[-]</label><label class="expand" for="c-37230988">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s on my list, but... to be honest if you Google &quot;separate header Luks&quot;, you&#x27;ll find it&#x27;s trivial to create a LUKS device with a detached header. Then the default ZRepl quick start will get you going with the basic pool-to-pool local replication. That will get you almost all the way there. :) I used their docs&#x2F;guide to do the remote replication too, though it would make a good write-up as I could throw in how I use sops-nix for securing the Zrepl TLS bits for the remote scenario too...</div><br/><div id="37231446" class="c"><input type="checkbox" id="c-37231446" checked=""/><div class="controls bullet"><span class="by">LWIRVoltage</span><span>|</span><a href="#37230901">root</a><span>|</span><a href="#37230988">parent</a><span>|</span><a href="#37231884">next</a><span>|</span><label class="collapse" for="c-37231446">[-]</label><label class="expand" for="c-37231446">[1 more]</label></div><br/><div class="children"><div class="content">This does sound really cool- and like a way to ultimately set up a secure, not that complex backup method...</div><br/></div></div></div></div></div></div></div></div><div id="37231884" class="c"><input type="checkbox" id="c-37231884" checked=""/><div class="controls bullet"><span class="by">bomewish</span><span>|</span><a href="#37230901">prev</a><span>|</span><a href="#37231553">next</a><span>|</span><label class="collapse" for="c-37231884">[-]</label><label class="expand" for="c-37231884">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the equivalent setup for a Mac user?</div><br/></div></div><div id="37231553" class="c"><input type="checkbox" id="c-37231553" checked=""/><div class="controls bullet"><span class="by">ck2</span><span>|</span><a href="#37231884">prev</a><span>|</span><a href="#37231173">next</a><span>|</span><label class="collapse" for="c-37231553">[-]</label><label class="expand" for="c-37231553">[1 more]</label></div><br/><div class="children"><div class="content">I really figured we&#x27;d have super easy hardware Raid1 in even consumer level PCs by now given how cheap drives are (and unreliable).<p>My SSD boot drive makes me nervous as heck, constantly backing it up.</div><br/></div></div><div id="37231173" class="c"><input type="checkbox" id="c-37231173" checked=""/><div class="controls bullet"><span class="by">SoftTalker</span><span>|</span><a href="#37231553">prev</a><span>|</span><a href="#37232570">next</a><span>|</span><label class="collapse" for="c-37231173">[-]</label><label class="expand" for="c-37231173">[2 more]</label></div><br/><div class="children"><div class="content">tldr: my drive died, and I had a backup.<p>zfs seems incidental to me. I could have a 10 minute cron job rsyncing changes from ext4 and been just as well off.</div><br/><div id="37232492" class="c"><input type="checkbox" id="c-37232492" checked=""/><div class="controls bullet"><span class="by">poisonborz</span><span>|</span><a href="#37231173">parent</a><span>|</span><a href="#37232570">next</a><span>|</span><label class="collapse" for="c-37232492">[-]</label><label class="expand" for="c-37232492">[1 more]</label></div><br/><div class="children"><div class="content">Classic HN comment!</div><br/></div></div></div></div><div id="37231962" class="c"><input type="checkbox" id="c-37231962" checked=""/><div class="controls bullet"><span class="by">BearhatBeer</span><span>|</span><a href="#37232570">prev</a><span>|</span><label class="collapse" for="c-37231962">[-]</label><label class="expand" for="c-37231962">[1 more]</label></div><br/><div class="children"><div class="content">I feel like a computer running ZFS and serving files, is fine. And it should itself be treated as a strage device with a full parallel backup even though this has cost.<p>But your computer shouldn&#x27;t run ZFS, that&#x27;s for the big boys upstairs. Code&#x27;s too big, it&#x27;s too hungry.</div><br/></div></div></div></div></div></div></div></body></html>