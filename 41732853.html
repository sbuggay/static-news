<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1728032476197" as="style"/><link rel="stylesheet" href="styles.css?v=1728032476197"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2410.01201">Were RNNs all we needed?</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>beefman</span> | <span>165 comments</span></div><br/><div><div id="41733414" class="c"><input type="checkbox" id="c-41733414" checked=""/><div class="controls bullet"><span class="by">xnx</span><span>|</span><a href="#41733784">next</a><span>|</span><label class="collapse" for="c-41733414">[-]</label><label class="expand" for="c-41733414">[81 more]</label></div><br/><div class="children"><div class="content">It&#x27;s curse and a blessing that discussion of topics happens in so many different places. I found this comment on Twitter&#x2F;X interesting: <a href="https:&#x2F;&#x2F;x.com&#x2F;fchollet&#x2F;status&#x2F;1841902521717293273" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;fchollet&#x2F;status&#x2F;1841902521717293273</a><p>&quot;Interesting work on reviving RNNs. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2410.01201" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2410.01201</a> -- in general the fact that there are many recent architectures coming from different directions that roughly match Transformers is proof that architectures aren&#x27;t fundamentally important in the curve-fitting paradigm (aka deep learning)<p>Curve-fitting is about embedding a dataset on a curve. The critical factor is the dataset, not the specific hard-coded bells and whistles that constrain the curve&#x27;s shape. As long as your curve is sufficiently expressive all architectures will converge to the same performance in the large-data regime.&quot;</div><br/><div id="41736317" class="c"><input type="checkbox" id="c-41736317" checked=""/><div class="controls bullet"><span class="by">drodgers</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41733619">next</a><span>|</span><label class="collapse" for="c-41736317">[-]</label><label class="expand" for="c-41736317">[10 more]</label></div><br/><div class="children"><div class="content">&gt; The critical factor is the dataset, not the specific hard-coded bells and whistles that constrain the curve&#x27;s shape<p>I have almost the opposite take. We&#x27;ve had a lot of datasets for ages, but all the progress in the last decade has come from advances how curves are architected and fit to the dataset (including applying more computing power).<p>Maybe there&#x27;s some theoretical sense in which older models could have solved newer problems just as well if only we applied 1000000x the computing power, so the new models are &#x27;just&#x27; an optimisation, but that&#x27;s like dismissing the importance of complexity analysis in algorithm design, and thus insisting that bogosort and quicksort are equivalent.<p>When you start layering in normalisation techniques to minimise overfitting, and especially once you start thinking about more agentic architectures (eg. Deep Q Learning, some of the search space design going into OpenAI&#x27;s o1), then I don&#x27;t think the just-an-optimisation perspective can hold much water at all - more computing power simply couldn&#x27;t solve those problems with older architectures.</div><br/><div id="41739219" class="c"><input type="checkbox" id="c-41739219" checked=""/><div class="controls bullet"><span class="by">tsimionescu</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736317">parent</a><span>|</span><a href="#41738554">next</a><span>|</span><label class="collapse" for="c-41739219">[-]</label><label class="expand" for="c-41739219">[1 more]</label></div><br/><div class="children"><div class="content">I think by far the biggest advances are related to compute power. The amount of processing needed to run training algorithms on the amounts of data needed for the latest models was just not possible even five years ago, and definitely not ten years ago.<p>I&#x27;m sure there are optimizations from the model shape as well, but I don&#x27;t think that running the best algorithms we have today with hardware from five-ten years ago would have worked in any reasonable amount of time&#x2F;money.</div><br/></div></div><div id="41738554" class="c"><input type="checkbox" id="c-41738554" checked=""/><div class="controls bullet"><span class="by">f1shy</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736317">parent</a><span>|</span><a href="#41739219">prev</a><span>|</span><a href="#41737062">next</a><span>|</span><label class="collapse" for="c-41738554">[-]</label><label class="expand" for="c-41738554">[2 more]</label></div><br/><div class="children"><div class="content">Wait! We certainly did NOT have huge datasets (like current internet) for ages. Not even decades. I’ve seen a lecture by a MIT professor (which I cannot find now) where he asserted categorically, that the advances in AI are mostly because of the huge data that we now have and we didn’t before. And that was an <i>old</i> video.</div><br/><div id="41739129" class="c"><input type="checkbox" id="c-41739129" checked=""/><div class="controls bullet"><span class="by">yosefk</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41738554">parent</a><span>|</span><a href="#41737062">next</a><span>|</span><label class="collapse" for="c-41739129">[-]</label><label class="expand" for="c-41739129">[1 more]</label></div><br/><div class="children"><div class="content">Whichever way it&#x27;s true in, it&#x27;s not true in the sense that eg you can approximate any curve with a single layer neural net, and you&#x27;re not actually going to be able to do it for problems CNNs or transformers work decently on. And Google indexed all of the public Internet way before its researchers came up with transformers.<p>Another way to look at it is that like you say, it was an old video but there has been progress since though we had large datasets when it came out by its own definition</div><br/></div></div></div></div><div id="41737062" class="c"><input type="checkbox" id="c-41737062" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736317">parent</a><span>|</span><a href="#41738554">prev</a><span>|</span><a href="#41738187">next</a><span>|</span><label class="collapse" for="c-41737062">[-]</label><label class="expand" for="c-41737062">[5 more]</label></div><br/><div class="children"><div class="content">I see what you are saying, and I made a similar comment.<p>However it&#x27;s still an interesting observation that many architectures can arrive at the same performance (even though the training requirements are different).<p>Naively, you wouldn&#x27;t expect eg &#x27;x -&gt; a * x + b&#x27; to fit the same data as &#x27;x -&gt; a * sin x + b&#x27; about equally well.  But that&#x27;s an observation from low dimensions.  It seems once you add enough parameters, the exact model doesn&#x27;t matter too much for practical expressiveness.<p>I&#x27;m faintly reminded of the Church-Turing Thesis; the differences between different computing architectures are both &#x27;real&#x27; but also &#x27;just an optimisation&#x27;.<p>&gt; When you start layering in normalisation techniques to minimise overfitting, and especially once you start thinking about more agentic architectures (eg. Deep Q Learning, some of the search space design going into OpenAI&#x27;s o1), then I don&#x27;t think the just-an-optimisation perspective can hold much water at all - more computing power simply couldn&#x27;t solve those problems with older architectures.<p>You are right, these normalisation techniques help you economise on training data, not just on compute.  Some of these techniques can be done independent of the model, eg augmenting your training data with noise.  But some others are very model dependent.<p>I&#x27;m not sure how the &#x27;agentic&#x27; approaches fit here.</div><br/><div id="41737234" class="c"><input type="checkbox" id="c-41737234" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41737062">parent</a><span>|</span><a href="#41738187">next</a><span>|</span><label class="collapse" for="c-41737234">[-]</label><label class="expand" for="c-41737234">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Naively, you wouldn&#x27;t expect</i><p>I, a nave, expected this.<p>Is multiplication versus sine in the analogy hiding it, perhaps?<p>I&#x27;ve always pictured it as just &quot;needing to learn&quot; the function terms and the function guts are an abstraction that is learned.<p>Might just be because I&#x27;m a physics dropout with a bunch of whacky half-remembered probably-wrong stuff about how any function can be approximated by ex. fourier series.</div><br/><div id="41737429" class="c"><input type="checkbox" id="c-41737429" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41737234">parent</a><span>|</span><a href="#41738187">next</a><span>|</span><label class="collapse" for="c-41737429">[-]</label><label class="expand" for="c-41737429">[3 more]</label></div><br/><div class="children"><div class="content">So (most) neural nets can be seen as a function of a _fixed_ form with some inputs and lots and lots of parameters.<p>In my example, a and b were the parameters.  The kinds of data you can approximate well with a simple sine wave and the kinds of data you can approximate with a straight line are rather different.<p>Training your neural net only fiddles with the parameters like a and b.  It doesn&#x27;t do anything about the shape of the function.  It doesn&#x27;t change sine into multiplication etc.<p>&gt; [...] about how any function can be approximated by ex. fourier series.<p>Fourier series are an interesting example to bring up!  I think I see what you mean.<p>In theory they work well to approximate any function over either a periodic domain or some finite interval.  But unless you take special care, when you apply Fourier analysis naively it becomes extremely sensitive to errors in the phase parameters.<p>(Special care could eg be done by hacking up your input domain into &#x27;boxes&#x27;.  That works well for eg audio or video compression, but gives up on any model generalisation between &#x27;boxes&#x27;, especially for what would happen in a later box.)<p>Another interesting example is Taylor series.  For many simple functions Taylor series are great, but for even moderately complicated ones you need to be careful.  See eg how the Taylor serious for the logarithm around x=1 works well, but if you tried it around x=0, you are in for a bad time.<p>The interesting observation isn&#x27;t just that there are multiple universal approximators, but that at high enough parameter count, they seem to perform about equally well in how good they are at approximating in practice (but differ in how well they can be trained).</div><br/><div id="41738099" class="c"><input type="checkbox" id="c-41738099" checked=""/><div class="controls bullet"><span class="by">leereeves</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41737429">parent</a><span>|</span><a href="#41738187">next</a><span>|</span><label class="collapse" for="c-41738099">[-]</label><label class="expand" for="c-41738099">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Training your neural net only fiddles with the parameters like a and b. It doesn&#x27;t do anything about the shape of the function. It doesn&#x27;t change sine into multiplication etc.<p>It definitely can. The output will always be piecewise linear (with ReLU), but the overall shape can change completely.</div><br/><div id="41738332" class="c"><input type="checkbox" id="c-41738332" checked=""/><div class="controls bullet"><span class="by">ziofill</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41738099">parent</a><span>|</span><a href="#41738187">next</a><span>|</span><label class="collapse" for="c-41738332">[-]</label><label class="expand" for="c-41738332">[1 more]</label></div><br/><div class="children"><div class="content">You can fit any data with enough parameters. What’s tricky is to constrain a model so that it approximates the ground truth well where there are no data points. If a family of functions is extremely flexible and can fit all kinds of data very efficiently I would argue it makes it harder for those functions to have correct values out of distribution.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41738187" class="c"><input type="checkbox" id="c-41738187" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736317">parent</a><span>|</span><a href="#41737062">prev</a><span>|</span><a href="#41733619">next</a><span>|</span><label class="collapse" for="c-41738187">[-]</label><label class="expand" for="c-41738187">[1 more]</label></div><br/><div class="children"><div class="content">Isn’t bogosort transformer and quicksort proposed modified rnn (175 times faster training for 500 seq) here?</div><br/></div></div></div></div><div id="41733619" class="c"><input type="checkbox" id="c-41733619" checked=""/><div class="controls bullet"><span class="by">islewis</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41736317">prev</a><span>|</span><a href="#41734740">next</a><span>|</span><label class="collapse" for="c-41733619">[-]</label><label class="expand" for="c-41733619">[10 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;As long as your curve is sufficiently expressive all architectures will converge to the same performance in the large-data regime.&quot;<p>I haven&#x27;t fully ingested the paper yet, but it looks like it&#x27;s focused more on compute optimization than the size of the dataset:<p>&gt; ... and (2) are fully parallelizable during training (175x faster for a sequence of length 512<p>Even if many types of architectures converge to the same loss over time, finding the one that converges the fastest is quite valuable given the cost of running GPU&#x27;s at scale.</div><br/><div id="41733970" class="c"><input type="checkbox" id="c-41733970" checked=""/><div class="controls bullet"><span class="by">teruakohatu</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41733619">parent</a><span>|</span><a href="#41735063">next</a><span>|</span><label class="collapse" for="c-41733970">[-]</label><label class="expand" for="c-41733970">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Even if many types of architectures converge to the same loss over time, finding the one that converges the fastest is quite valuable given the cost of running GPU&#x27;s at scale.<p>This! Not just fastest but with the lowest resources in total.<p>Fully connected neural networks are universal functions. Technically we don’t need anything but a FNN, but memory requirements and speed would be abysmal far beyond the realm of practicality.</div><br/><div id="41735837" class="c"><input type="checkbox" id="c-41735837" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41733970">parent</a><span>|</span><a href="#41735063">next</a><span>|</span><label class="collapse" for="c-41735837">[-]</label><label class="expand" for="c-41735837">[7 more]</label></div><br/><div class="children"><div class="content">Unless we could build chips in 3D?</div><br/><div id="41736038" class="c"><input type="checkbox" id="c-41736038" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41735837">parent</a><span>|</span><a href="#41738257">next</a><span>|</span><label class="collapse" for="c-41736038">[-]</label><label class="expand" for="c-41736038">[4 more]</label></div><br/><div class="children"><div class="content">Not even then, a truly fully connected network would have super exponential runtime (it would take N^N time to evaluate)</div><br/><div id="41738130" class="c"><input type="checkbox" id="c-41738130" checked=""/><div class="controls bullet"><span class="by">mvkel</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736038">parent</a><span>|</span><a href="#41736146">next</a><span>|</span><label class="collapse" for="c-41738130">[-]</label><label class="expand" for="c-41738130">[1 more]</label></div><br/><div class="children"><div class="content">Wetware is the future.</div><br/></div></div><div id="41736146" class="c"><input type="checkbox" id="c-41736146" checked=""/><div class="controls bullet"><span class="by">ivan_gammel</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736038">parent</a><span>|</span><a href="#41738130">prev</a><span>|</span><a href="#41738257">next</a><span>|</span><label class="collapse" for="c-41736146">[-]</label><label class="expand" for="c-41736146">[2 more]</label></div><br/><div class="children"><div class="content">We need quantum computing there. I remember seeing a recent article about quantum processes in the brain. If that’s true, QC may be the missing part.</div><br/><div id="41737066" class="c"><input type="checkbox" id="c-41737066" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736146">parent</a><span>|</span><a href="#41738257">next</a><span>|</span><label class="collapse" for="c-41737066">[-]</label><label class="expand" for="c-41737066">[1 more]</label></div><br/><div class="children"><div class="content">Compare and contrast <a href="https:&#x2F;&#x2F;www.smbc-comics.com&#x2F;comic&#x2F;the-talk-3" rel="nofollow">https:&#x2F;&#x2F;www.smbc-comics.com&#x2F;comic&#x2F;the-talk-3</a><p>(Summary: quantum computing is unlikely to help.)</div><br/></div></div></div></div></div></div><div id="41738257" class="c"><input type="checkbox" id="c-41738257" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41735837">parent</a><span>|</span><a href="#41736038">prev</a><span>|</span><a href="#41737003">next</a><span>|</span><label class="collapse" for="c-41738257">[-]</label><label class="expand" for="c-41738257">[1 more]</label></div><br/><div class="children"><div class="content">We are already doing this.</div><br/></div></div><div id="41737003" class="c"><input type="checkbox" id="c-41737003" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41735837">parent</a><span>|</span><a href="#41738257">prev</a><span>|</span><a href="#41735063">next</a><span>|</span><label class="collapse" for="c-41737003">[-]</label><label class="expand" for="c-41737003">[1 more]</label></div><br/><div class="children"><div class="content">Heat extraction.</div><br/></div></div></div></div></div></div><div id="41735063" class="c"><input type="checkbox" id="c-41735063" checked=""/><div class="controls bullet"><span class="by">byearthithatius</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41733619">parent</a><span>|</span><a href="#41733970">prev</a><span>|</span><a href="#41734740">next</a><span>|</span><label class="collapse" for="c-41735063">[-]</label><label class="expand" for="c-41735063">[1 more]</label></div><br/><div class="children"><div class="content">&gt; finding the one that converges the fastest is quite valuable given the cost of running GPU&#x27;s at scale<p>Not to him, he runs the ARC challenge. He wants a new approach entirely. Something capable of few-shot learning out of distribution patterns .... somehow</div><br/></div></div></div></div><div id="41734740" class="c"><input type="checkbox" id="c-41734740" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41733619">prev</a><span>|</span><a href="#41735402">next</a><span>|</span><label class="collapse" for="c-41734740">[-]</label><label class="expand" for="c-41734740">[3 more]</label></div><br/><div class="children"><div class="content">One big thing that bells and whistles do is limit the training space.<p>For example when CNNs took over computer vision that wasn&#x27;t because they were doing something that dense networks couldn&#x27;t do. It was because they removed a lot of edges that didn&#x27;t really matter, allowing us to spend our training budget on deeper networks. Similarly transformers are great because they allow us to train gigantic networks somewhat efficiently. And this paper finds that if we make RNNs a lot faster to train they are actually pretty good. Training speed and efficiency remains the big bottleneck, not the actual expressiveness of the architecture</div><br/><div id="41738723" class="c"><input type="checkbox" id="c-41738723" checked=""/><div class="controls bullet"><span class="by">nutanc</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734740">parent</a><span>|</span><a href="#41735455">next</a><span>|</span><label class="collapse" for="c-41738723">[-]</label><label class="expand" for="c-41738723">[1 more]</label></div><br/><div class="children"><div class="content">This is true. This is the reason, in many of our experiments we find that using a new algorithm, KESieve, we actually find the planes much faster than the traditional deep learning training approaches. The premise is, a neaural network builds planes which separate the data and adjusts these planes through an iterative learning process. What if we can find a non iterative method which can draw these same planes. We have been trying this and so far we have been able to replace most network layers using this approach. haven&#x27;t tried for transformers though yet.<p>Some links if interested:<p>[1] <a href="https:&#x2F;&#x2F;gpt3experiments.substack.com&#x2F;p&#x2F;understanding-neural-networks-and" rel="nofollow">https:&#x2F;&#x2F;gpt3experiments.substack.com&#x2F;p&#x2F;understanding-neural-...</a><p>[2] <a href="https:&#x2F;&#x2F;gpt3experiments.substack.com&#x2F;p&#x2F;building-a-vector-database-in-2gb" rel="nofollow">https:&#x2F;&#x2F;gpt3experiments.substack.com&#x2F;p&#x2F;building-a-vector-dat...</a></div><br/></div></div></div></div><div id="41735402" class="c"><input type="checkbox" id="c-41735402" checked=""/><div class="controls bullet"><span class="by">sakras</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41734740">prev</a><span>|</span><a href="#41734634">next</a><span>|</span><label class="collapse" for="c-41735402">[-]</label><label class="expand" for="c-41735402">[1 more]</label></div><br/><div class="children"><div class="content">I figured this was pretty obvious given that MLPs are universal function approximators. A giant MLP could achieve the same results as a transformer. The problem is the scale - we can’t train a big enough MLP. Transformers are a performance optimization, and that’s why they’re useful.</div><br/></div></div><div id="41734634" class="c"><input type="checkbox" id="c-41734634" checked=""/><div class="controls bullet"><span class="by">Lerc</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41735402">prev</a><span>|</span><a href="#41733758">next</a><span>|</span><label class="collapse" for="c-41734634">[-]</label><label class="expand" for="c-41734634">[2 more]</label></div><br/><div class="children"><div class="content">I remember one of the initial transformer people saying in an interview that they didn&#x27;t think this was the &quot;one true architecture&quot; but a lot of the performance came from people rallying around it and pushing in the one direction.<p>On the other hand, while <i>&quot;As long as your curve is sufficiently expressive all architectures will converge to the same performance in the large-data regime.&quot;</i>  is true,  a sufficiently expressive mechanism may not be computationally or memory efficient.  As both are constraints on what you can actually build,  it&#x27;s not whether the architecture can produce the result, but whether a feasible&#x2F;practical instantiation of that architecture can produce the result.</div><br/><div id="41737265" class="c"><input type="checkbox" id="c-41737265" checked=""/><div class="controls bullet"><span class="by">viktor_von</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734634">parent</a><span>|</span><a href="#41733758">next</a><span>|</span><label class="collapse" for="c-41737265">[-]</label><label class="expand" for="c-41737265">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I remember one of the initial transformer people saying in an interview that they didn&#x27;t think this was the &quot;one true architecture&quot; but a lot of the performance came from people rallying around it and pushing in the one direction.<p>You may be referring to Aidan Gomez (CEO of Cohere and contributor to the transformer architecture) during his Machine Learning Street Talk podcast interview. I agree, if as much attention had been put towards the RNN during the initial transformer hype, we may have very well seen these advancements earlier.</div><br/></div></div></div></div><div id="41733758" class="c"><input type="checkbox" id="c-41733758" checked=""/><div class="controls bullet"><span class="by">acchow</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41734634">prev</a><span>|</span><a href="#41734680">next</a><span>|</span><label class="collapse" for="c-41733758">[-]</label><label class="expand" for="c-41733758">[4 more]</label></div><br/><div class="children"><div class="content">What it will come down to is computational efficiencies. We don’t want to retrain once a month - we want to retrain continuously. We don’t want one agent talking to 5 LLMs. We want thousands of LLMs all working in concert.</div><br/><div id="41734635" class="c"><input type="checkbox" id="c-41734635" checked=""/><div class="controls bullet"><span class="by">ActorNightly</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41733758">parent</a><span>|</span><a href="#41736041">next</a><span>|</span><label class="collapse" for="c-41734635">[-]</label><label class="expand" for="c-41734635">[1 more]</label></div><br/><div class="children"><div class="content">This and also the way models are trained has to be rethought. BPP is good for figuring out complex function mappings, but not for storing information.</div><br/></div></div><div id="41736041" class="c"><input type="checkbox" id="c-41736041" checked=""/><div class="controls bullet"><span class="by">pbhjpbhj</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41733758">parent</a><span>|</span><a href="#41734635">prev</a><span>|</span><a href="#41734680">next</a><span>|</span><label class="collapse" for="c-41736041">[-]</label><label class="expand" for="c-41736041">[2 more]</label></div><br/><div class="children"><div class="content">Sounds like something that has unsustainable energy costs.</div><br/></div></div></div></div><div id="41734680" class="c"><input type="checkbox" id="c-41734680" checked=""/><div class="controls bullet"><span class="by">ants_everywhere</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41733758">prev</a><span>|</span><a href="#41737020">next</a><span>|</span><label class="collapse" for="c-41734680">[-]</label><label class="expand" for="c-41734680">[6 more]</label></div><br/><div class="children"><div class="content">&gt; is proof that architectures aren&#x27;t fundamentally important in the curve-fitting paradigm (aka deep learning)<p>(Somewhat) fun and (somewhat) related fact: there&#x27;s a whole cottage industry of &quot;is all you need&quot; papers <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;search&#x2F;?query=%22is+all+you+need%22&amp;searchtype=all&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;search&#x2F;?query=%22is+all+you+need%22&amp;search...</a></div><br/><div id="41734751" class="c"><input type="checkbox" id="c-41734751" checked=""/><div class="controls bullet"><span class="by">TaurenHunter</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734680">parent</a><span>|</span><a href="#41737020">next</a><span>|</span><label class="collapse" for="c-41734751">[-]</label><label class="expand" for="c-41734751">[5 more]</label></div><br/><div class="children"><div class="content">Reminds me of the &quot;Considered Harmful&quot; articles:<p><a href="https:&#x2F;&#x2F;meyerweb.com&#x2F;eric&#x2F;comment&#x2F;chech.html" rel="nofollow">https:&#x2F;&#x2F;meyerweb.com&#x2F;eric&#x2F;comment&#x2F;chech.html</a></div><br/><div id="41736521" class="c"><input type="checkbox" id="c-41736521" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734751">parent</a><span>|</span><a href="#41734890">next</a><span>|</span><label class="collapse" for="c-41736521">[-]</label><label class="expand" for="c-41736521">[2 more]</label></div><br/><div class="children"><div class="content">Quick, somebody write “All you need Considered Harmful” and “Considered Harmful all you need.”<p>Which seems closer to true?</div><br/><div id="41737014" class="c"><input type="checkbox" id="c-41737014" checked=""/><div class="controls bullet"><span class="by">cozzyd</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736521">parent</a><span>|</span><a href="#41734890">next</a><span>|</span><label class="collapse" for="c-41737014">[-]</label><label class="expand" for="c-41737014">[1 more]</label></div><br/><div class="children"><div class="content">All you need is all you need.</div><br/></div></div></div></div><div id="41734890" class="c"><input type="checkbox" id="c-41734890" checked=""/><div class="controls bullet"><span class="by">jprete</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734751">parent</a><span>|</span><a href="#41736521">prev</a><span>|</span><a href="#41737020">next</a><span>|</span><label class="collapse" for="c-41734890">[-]</label><label class="expand" for="c-41734890">[2 more]</label></div><br/><div class="children"><div class="content">I wonder if there&#x27;s something about tech culture - or tech people - that encourages them to really, really like snowclones.</div><br/><div id="41734978" class="c"><input type="checkbox" id="c-41734978" checked=""/><div class="controls bullet"><span class="by">observationist</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734890">parent</a><span>|</span><a href="#41737020">next</a><span>|</span><label class="collapse" for="c-41734978">[-]</label><label class="expand" for="c-41734978">[1 more]</label></div><br/><div class="children"><div class="content">Yes. Do stuff that other people have been successful doing. Monkey see, monkey do - it&#x27;s not a tech people thing, it&#x27;s a human thing.<p>Tech just happens to be most on display at the moment - because tech people are building the tools and the parameters and the infrastructure handling all our interactions.</div><br/></div></div></div></div></div></div></div></div><div id="41737020" class="c"><input type="checkbox" id="c-41737020" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41734680">prev</a><span>|</span><a href="#41738171">next</a><span>|</span><label class="collapse" for="c-41737020">[-]</label><label class="expand" for="c-41737020">[1 more]</label></div><br/><div class="children"><div class="content">Well, you also need an approach to &#x27;curve fitting&#x27; where it&#x27;s actually computationally feasible to fit the curve.  The approach of mixing layers of matrix multiplication with a simple non-linearity like max(0, x) (ReLU) works really well for that.  Earlier on they tried more complicated non-linearities, like sigmoids, or you could try an arbitrary curve that&#x27;s not split into layers at all, you would probably find it harder.  (But I&#x27;m fairly sure in the end you might end up in the same place, just after lots more computation spent on fitting.)</div><br/></div></div><div id="41738171" class="c"><input type="checkbox" id="c-41738171" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41737020">prev</a><span>|</span><a href="#41736074">next</a><span>|</span><label class="collapse" for="c-41738171">[-]</label><label class="expand" for="c-41738171">[1 more]</label></div><br/><div class="children"><div class="content">well yes but actually no I guess: the transformers benefit at the time was that they were more stable while learning, enabling larger and larger network and dataset to be learnt.</div><br/></div></div><div id="41736074" class="c"><input type="checkbox" id="c-41736074" checked=""/><div class="controls bullet"><span class="by">ctur</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41738171">prev</a><span>|</span><a href="#41737213">next</a><span>|</span><label class="collapse" for="c-41736074">[-]</label><label class="expand" for="c-41736074">[3 more]</label></div><br/><div class="children"><div class="content">Architecture matters because while deep learning can conceivably fit a curve with a single, huge layer (in theory... Universal approximation theorem), the amount of compute and data needed to get there is prohibitive.  Having a good architecture means the theoretical possibility of deep learning finding the right N dimensional curve becomes a practical reality.<p>Another thing about the architecture is we inherently bias it with the way we structure the data.  For instance, take a dataset of (car) traffic patterns.  If you only track the date as a feature, you miss that some events follow not just the day-of-year pattern but also holiday patterns.  You could learn this with deep learning with enough data, but if we bake it into the dataset, you can build a model on it _much_ simpler and faster.<p>So, architecture matters.  Data&#x2F;feature representation matters.</div><br/><div id="41736639" class="c"><input type="checkbox" id="c-41736639" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736074">parent</a><span>|</span><a href="#41737213">next</a><span>|</span><label class="collapse" for="c-41736639">[-]</label><label class="expand" for="c-41736639">[2 more]</label></div><br/><div class="children"><div class="content">&gt; can conceivably fit a curve with a single, huge layer<p>I think you need a hidden layer.  I’ve never seen a universal approximation theorem for a single layer network.</div><br/><div id="41739235" class="c"><input type="checkbox" id="c-41739235" checked=""/><div class="controls bullet"><span class="by">dongecko</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736639">parent</a><span>|</span><a href="#41737213">next</a><span>|</span><label class="collapse" for="c-41739235">[-]</label><label class="expand" for="c-41739235">[1 more]</label></div><br/><div class="children"><div class="content">I second that thought. There is a pretty well cited paper from the late eighties called &quot;Multilayer Feedforward Networks are Universal Approximators&quot;. It shows that a feedforward network with a single hidden layer containing a finite number of neurons can approximate any continuous function. For non continous function additional layers are needed.</div><br/></div></div></div></div></div></div><div id="41737213" class="c"><input type="checkbox" id="c-41737213" checked=""/><div class="controls bullet"><span class="by">tippytippytango</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41736074">prev</a><span>|</span><a href="#41734837">next</a><span>|</span><label class="collapse" for="c-41737213">[-]</label><label class="expand" for="c-41737213">[1 more]</label></div><br/><div class="children"><div class="content">Inductive bias matters. A lot.</div><br/></div></div><div id="41734837" class="c"><input type="checkbox" id="c-41734837" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41737213">prev</a><span>|</span><a href="#41736694">next</a><span>|</span><label class="collapse" for="c-41734837">[-]</label><label class="expand" for="c-41734837">[7 more]</label></div><br/><div class="children"><div class="content">I mean, transformer-based LLMs are RNNs, just really really really big ones with very wide inputs that maintain large amounts of context.</div><br/><div id="41734868" class="c"><input type="checkbox" id="c-41734868" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734837">parent</a><span>|</span><a href="#41736694">next</a><span>|</span><label class="collapse" for="c-41734868">[-]</label><label class="expand" for="c-41734868">[6 more]</label></div><br/><div class="children"><div class="content">No. An RNN has an arbitrarily-long path from old inputs to new outputs, even if in practice it can&#x27;t exploit that path. Transformers have fixed-size input windows.</div><br/><div id="41734919" class="c"><input type="checkbox" id="c-41734919" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734868">parent</a><span>|</span><a href="#41735010">next</a><span>|</span><label class="collapse" for="c-41734919">[-]</label><label class="expand" for="c-41734919">[4 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t have a fixed state and have arbitrarily-long path from input. Well you can but then it&#x27;s just meaningless because you fundamentally cannot keep stuffing information of arbitrary length into a fixed state. RNNs effectively have fixed-size input windows.</div><br/><div id="41734969" class="c"><input type="checkbox" id="c-41734969" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734919">parent</a><span>|</span><a href="#41735010">next</a><span>|</span><label class="collapse" for="c-41734969">[-]</label><label class="expand" for="c-41734969">[3 more]</label></div><br/><div class="children"><div class="content">The <i>path</i> is arbitrarily <i>long</i>, not wide. It is <i>possible</i> for an RNN to be made that remembers the first word of the input, no longer how long the input is. This is not possible with a transformer, so we know they are fundamentally different.</div><br/><div id="41735291" class="c"><input type="checkbox" id="c-41735291" checked=""/><div class="controls bullet"><span class="by">quotemstr</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734969">parent</a><span>|</span><a href="#41735010">next</a><span>|</span><label class="collapse" for="c-41735291">[-]</label><label class="expand" for="c-41735291">[2 more]</label></div><br/><div class="children"><div class="content">But an RNN isn&#x27;t <i>going</i> to remember the first token of input. It won&#x27;t know until it sees the last token whether that first token was relevant after all, so it has to learn token-specific update rules that let it guess how long to hold what kinds of information. (In multi-layer systems, the network uses ineffable abstractions rather than tokens, but the same idea applies.)<p>What the RNN must be doing reminds me of &quot;sliding window attention&quot; --- the model learns how to partition its state between short- and long-range memories to minimize overall loss. The two approaches seem related, perhaps even equivalent up to implementation details.</div><br/><div id="41735438" class="c"><input type="checkbox" id="c-41735438" checked=""/><div class="controls bullet"><span class="by">OkayPhysicist</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41735291">parent</a><span>|</span><a href="#41735010">next</a><span>|</span><label class="collapse" for="c-41735438">[-]</label><label class="expand" for="c-41735438">[1 more]</label></div><br/><div class="children"><div class="content">The most popular RNNs (the ones that were successful enough for Google translate and the like) actually had this behavior baked in to the architecture, called &quot;LSTMs&quot;, &quot;Long-Short Term Memory&quot;</div><br/></div></div></div></div></div></div></div></div><div id="41735010" class="c"><input type="checkbox" id="c-41735010" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734868">parent</a><span>|</span><a href="#41734919">prev</a><span>|</span><a href="#41736694">next</a><span>|</span><label class="collapse" for="c-41735010">[-]</label><label class="expand" for="c-41735010">[1 more]</label></div><br/><div class="children"><div class="content">A chunk of the output still goes into the transformer input, so the arbitrarily-long path still exists, it just goes through a decoding&#x2F;encoding step.</div><br/></div></div></div></div></div></div><div id="41736694" class="c"><input type="checkbox" id="c-41736694" checked=""/><div class="controls bullet"><span class="by">_giorgio_</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41734837">prev</a><span>|</span><a href="#41734127">next</a><span>|</span><label class="collapse" for="c-41736694">[-]</label><label class="expand" for="c-41736694">[1 more]</label></div><br/><div class="children"><div class="content">Chollet is just a philosopher. 
He also thinks that keras and tensorflow are important, when nobody uses those. And he punished false days about their usage.</div><br/></div></div><div id="41734127" class="c"><input type="checkbox" id="c-41734127" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41736694">prev</a><span>|</span><a href="#41735261">next</a><span>|</span><label class="collapse" for="c-41734127">[-]</label><label class="expand" for="c-41734127">[9 more]</label></div><br/><div class="children"><div class="content">after reading this paper, I am now convinced we will need more than curve fitting to build AGI:<a href="https:&#x2F;&#x2F;medium.com&#x2F;@fsndzomga&#x2F;there-will-be-no-agi-d9be9af4428d" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@fsndzomga&#x2F;there-will-be-no-agi-d9be9af44...</a></div><br/><div id="41734557" class="c"><input type="checkbox" id="c-41734557" checked=""/><div class="controls bullet"><span class="by">josh-sematic</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734127">parent</a><span>|</span><a href="#41735522">next</a><span>|</span><label class="collapse" for="c-41734557">[-]</label><label class="expand" for="c-41734557">[1 more]</label></div><br/><div class="children"><div class="content">One reason why I&#x27;m excited about o1 is that it seems like OpenAI have cracked the nut of effective RL during training time, which takes us out of the domain of just fitting to the curve of &quot;what a human would have said next.&quot; I just finished writing a couple blog posts about this; the first [1] covers some problems with that approach and the second [2] talks about what alternatives might look like.<p>[1] <a href="https:&#x2F;&#x2F;www.airtrain.ai&#x2F;blog&#x2F;how-openai-o1-changes-the-llm-training-picture-part-1">https:&#x2F;&#x2F;www.airtrain.ai&#x2F;blog&#x2F;how-openai-o1-changes-the-llm-t...</a>
[2] <a href="https:&#x2F;&#x2F;www.airtrain.ai&#x2F;blog&#x2F;how-openai-o1-changes-the-llm-training-picture-part-2">https:&#x2F;&#x2F;www.airtrain.ai&#x2F;blog&#x2F;how-openai-o1-changes-the-llm-t...</a></div><br/></div></div><div id="41735522" class="c"><input type="checkbox" id="c-41735522" checked=""/><div class="controls bullet"><span class="by">acchow</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734127">parent</a><span>|</span><a href="#41734557">prev</a><span>|</span><a href="#41734494">next</a><span>|</span><label class="collapse" for="c-41735522">[-]</label><label class="expand" for="c-41735522">[1 more]</label></div><br/><div class="children"><div class="content">&gt; After reading this paper, I am now<p>Is this your paper?</div><br/></div></div><div id="41734494" class="c"><input type="checkbox" id="c-41734494" checked=""/><div class="controls bullet"><span class="by">ahzhou</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734127">parent</a><span>|</span><a href="#41735522">prev</a><span>|</span><a href="#41734291">next</a><span>|</span><label class="collapse" for="c-41734494">[-]</label><label class="expand" for="c-41734494">[1 more]</label></div><br/><div class="children"><div class="content">Author: @fandzomga
Username: fsndz<p>Why try to funnel us to your paywalled article?</div><br/></div></div><div id="41734291" class="c"><input type="checkbox" id="c-41734291" checked=""/><div class="controls bullet"><span class="by">xpl</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734127">parent</a><span>|</span><a href="#41734494">prev</a><span>|</span><a href="#41734217">next</a><span>|</span><label class="collapse" for="c-41734291">[-]</label><label class="expand" for="c-41734291">[2 more]</label></div><br/><div class="children"><div class="content">I would like to read it, but it&#x27;s under a paywall.</div><br/><div id="41734319" class="c"><input type="checkbox" id="c-41734319" checked=""/><div class="controls bullet"><span class="by">alwa</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734291">parent</a><span>|</span><a href="#41734217">next</a><span>|</span><label class="collapse" for="c-41734319">[-]</label><label class="expand" for="c-41734319">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;archive.is&#x2F;nGaiU" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;nGaiU</a></div><br/></div></div></div></div><div id="41734217" class="c"><input type="checkbox" id="c-41734217" checked=""/><div class="controls bullet"><span class="by">swolchok</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734127">parent</a><span>|</span><a href="#41734291">prev</a><span>|</span><a href="#41734581">next</a><span>|</span><label class="collapse" for="c-41734217">[-]</label><label class="expand" for="c-41734217">[2 more]</label></div><br/><div class="children"><div class="content">paper is paywalled; just logging into Medium won&#x27;t do it</div><br/><div id="41735372" class="c"><input type="checkbox" id="c-41735372" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734217">parent</a><span>|</span><a href="#41734581">next</a><span>|</span><label class="collapse" for="c-41735372">[-]</label><label class="expand" for="c-41735372">[1 more]</label></div><br/><div class="children"><div class="content">sorry for the paywall, you can read the free version here: <a href="https:&#x2F;&#x2F;www.lycee.ai&#x2F;blog&#x2F;why-no-agi-openai" rel="nofollow">https:&#x2F;&#x2F;www.lycee.ai&#x2F;blog&#x2F;why-no-agi-openai</a></div><br/></div></div></div></div><div id="41734581" class="c"><input type="checkbox" id="c-41734581" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41734127">parent</a><span>|</span><a href="#41734217">prev</a><span>|</span><a href="#41735261">next</a><span>|</span><label class="collapse" for="c-41734581">[-]</label><label class="expand" for="c-41734581">[1 more]</label></div><br/><div class="children"><div class="content">TLDR: “statistically fitting token output is not the same as human intelligence, and human intelligence and AGI are contradictory anyways (because humans make mistakes)”<p>Saved you the paywall click to the poorly structured medium article :)</div><br/></div></div></div></div><div id="41735261" class="c"><input type="checkbox" id="c-41735261" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41733414">parent</a><span>|</span><a href="#41734127">prev</a><span>|</span><a href="#41733784">next</a><span>|</span><label class="collapse" for="c-41735261">[-]</label><label class="expand" for="c-41735261">[21 more]</label></div><br/><div class="children"><div class="content">Most LLMs aren&#x27;t even using a &quot;curve&quot; yet at all, right? All they&#x27;re using is a series of linear equations because the model weights are a simple multiply and add (i.e. basic NN Perceptron). Sure there&#x27;s a squashing function on the output to keep it in a range from 0 to 1 but that&#x27;s done BECAUSE we&#x27;re just adding up stuff.<p>I think probably future NNs will be maybe more adaptive than this perhaps where some Perceptrons use sine wave functions, or other kinds of math functions, beyond just linear &quot;y=mx+b&quot;<p>It&#x27;s astounding that we DID get the emergent intelligence from just doing this &quot;curve fitting&quot; onto &quot;lines&quot; rather than actual &quot;curves&quot;.</div><br/><div id="41735396" class="c"><input type="checkbox" id="c-41735396" checked=""/><div class="controls bullet"><span class="by">OkayPhysicist</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41735261">parent</a><span>|</span><a href="#41736755">next</a><span>|</span><label class="collapse" for="c-41735396">[-]</label><label class="expand" for="c-41735396">[18 more]</label></div><br/><div class="children"><div class="content">The &quot;squashing function&quot; necessarily is nonlinear in multilayer nueral networks. A single layer of a neural network can be quite simply written a weight matrix, times an input vector, equalling an output vector, like so<p>Ax = y<p>Adding another layer is just multiplying a different set of weights times the output of the first, so<p>B(Ax)= y<p>If you remember your linear algebra course, you might see the problem: that can be simplified<p>(BA)x = y<p>Cx = y<p>Completely indistinguishable from a single layer, thus only capable of modeling linear relationships.<p>To prevent this collapse, a non linear function must be introduced between each layer.</div><br/><div id="41735620" class="c"><input type="checkbox" id="c-41735620" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41735396">parent</a><span>|</span><a href="#41736755">next</a><span>|</span><label class="collapse" for="c-41735620">[-]</label><label class="expand" for="c-41735620">[17 more]</label></div><br/><div class="children"><div class="content">Right. All the squashing is doing is keeping the output of any neuron in a range of below 1.<p>But the entire NN itself (Perceptron ones, which most LLMs are) is still completely using nothing but linearity to store all the knowledge from the training process. All the weights are just an &#x27;m&#x27; in the basic line equation &#x27;y=m*x+b&#x27;. The entire training process does nothing but adjust a bunch of slopes of a bunch of lines. It&#x27;s totally linear. No non-linearity at all.</div><br/><div id="41735776" class="c"><input type="checkbox" id="c-41735776" checked=""/><div class="controls bullet"><span class="by">nazgul17</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41735620">parent</a><span>|</span><a href="#41736755">next</a><span>|</span><label class="collapse" for="c-41735776">[-]</label><label class="expand" for="c-41735776">[16 more]</label></div><br/><div class="children"><div class="content">The non linearities are fundamental. Without them, any arbitrarily deep NN is equivalent to a shallow NN (easily computable, as GP was saying), and we know those can&#x27;t even solve the XOR problem.<p>&gt; nothing but linearity<p>No, if you have non linearities, the NN itself is <i>not</i> linear. 
The non linearities are not there primarily to keep the outputs in a given range, though that&#x27;s important, too.</div><br/><div id="41736117" class="c"><input type="checkbox" id="c-41736117" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41735776">parent</a><span>|</span><a href="#41736755">next</a><span>|</span><label class="collapse" for="c-41736117">[-]</label><label class="expand" for="c-41736117">[15 more]</label></div><br/><div class="children"><div class="content">&gt; The non linearities are not there primarily to keep the outputs in a given range<p>Precisely what the `Activation Function` does is to squash an output into a range (normally below one, like tanh). That&#x27;s the only non-linearity I&#x27;m aware of. What other non-linearities are there?<p>All the training does is adjust linear weights tho, like I said. All the training is doing is adjusting the slopes of lines.</div><br/><div id="41736984" class="c"><input type="checkbox" id="c-41736984" checked=""/><div class="controls bullet"><span class="by">uh_uh</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736117">parent</a><span>|</span><a href="#41736189">next</a><span>|</span><label class="collapse" for="c-41736984">[-]</label><label class="expand" for="c-41736984">[2 more]</label></div><br/><div class="children"><div class="content">&gt; That&#x27;s the only non-linearity I&#x27;m aware of.<p>&quot;only&quot; is doing a lot work here because that non-linearity is enough to vastly expand the landscape of functions that an NN can approximate. If the NN was linear, you could greatly simplify the computational needs of the whole thing (as was implied by another commenter above) but you&#x27;d also not get a GPT out of it.</div><br/><div id="41737967" class="c"><input type="checkbox" id="c-41737967" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736984">parent</a><span>|</span><a href="#41736189">next</a><span>|</span><label class="collapse" for="c-41737967">[-]</label><label class="expand" for="c-41737967">[1 more]</label></div><br/><div class="children"><div class="content">All the trainable parameters are just slopes of lines tho. Training NNs doesn&#x27;t involve adjusting any inputs to non-linear functions. The tanh smashing function just makes sure nothing can blow up into large numbers and all outputs are in a range of less than 1. There&#x27;s no &quot;magic&quot; or &quot;knowledge&quot; in the tanh smashing. All the magic is 100% in the weights. They&#x27;re all linear. The amazing thing is that all weights are linear slopes of lines.</div><br/></div></div></div></div><div id="41736189" class="c"><input type="checkbox" id="c-41736189" checked=""/><div class="controls bullet"><span class="by">jcparkyn</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736117">parent</a><span>|</span><a href="#41736984">prev</a><span>|</span><a href="#41737140">next</a><span>|</span><label class="collapse" for="c-41736189">[-]</label><label class="expand" for="c-41736189">[10 more]</label></div><br/><div class="children"><div class="content">&gt; squash an output into a range<p>This isn&#x27;t the primary purpose of the activation function, and in fact it&#x27;s not even necessary. For example see ReLU (probably the most common activation function), leaky ReLU, or for a sillier example: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;Ae9EKCyI1xU?si=KgjhMrOsFEVo2yCe" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;Ae9EKCyI1xU?si=KgjhMrOsFEVo2yCe</a></div><br/><div id="41736475" class="c"><input type="checkbox" id="c-41736475" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736189">parent</a><span>|</span><a href="#41737140">next</a><span>|</span><label class="collapse" for="c-41736475">[-]</label><label class="expand" for="c-41736475">[9 more]</label></div><br/><div class="children"><div class="content">You can change the subject by bringing up as many different NN architectures, Activation Functions, etc. as you want. I&#x27;m telling you the basic NN Perceptron design (what everyone means when they refer to Perceptrons in general), has something like a `tanh` and not only is it&#x27;s PRIMARY function to squash a number, that&#x27;s it&#x27;s ONLY function.</div><br/><div id="41736937" class="c"><input type="checkbox" id="c-41736937" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736475">parent</a><span>|</span><a href="#41736838">next</a><span>|</span><label class="collapse" for="c-41736937">[-]</label><label class="expand" for="c-41736937">[4 more]</label></div><br/><div class="children"><div class="content">You need a non-linear activation function for the universal approximation theorem to hold.  Otherwise, as others have said the model just collapses to a single layer.<p>Technically the output is still what a statistician would call “linear in the parameters”, but due to the universal approximation theorem it can <i>approximate</i> any non-linear function.<p><a href="https:&#x2F;&#x2F;stats.stackexchange.com&#x2F;questions&#x2F;275358&#x2F;why-is-increasing-the-non-linearity-of-neural-networks-desired#335972" rel="nofollow">https:&#x2F;&#x2F;stats.stackexchange.com&#x2F;questions&#x2F;275358&#x2F;why-is-incr...</a></div><br/><div id="41737216" class="c"><input type="checkbox" id="c-41737216" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736937">parent</a><span>|</span><a href="#41736838">next</a><span>|</span><label class="collapse" for="c-41737216">[-]</label><label class="expand" for="c-41737216">[3 more]</label></div><br/><div class="children"><div class="content">As you can see in what I just posted about an inch below this, my point is that the process of training a NN does not involve adjusting any parameter to any non-linear functions. What goes into an activation function is a pure sum of linear multiplications and an add, but there&#x27;s no &quot;tunable&quot; parameter (i.e. adjusted during training) that&#x27;s fed into the activation function.</div><br/><div id="41737569" class="c"><input type="checkbox" id="c-41737569" checked=""/><div class="controls bullet"><span class="by">beckhamc</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41737216">parent</a><span>|</span><a href="#41736838">next</a><span>|</span><label class="collapse" for="c-41737569">[-]</label><label class="expand" for="c-41737569">[2 more]</label></div><br/><div class="children"><div class="content">Learnable parameters on activations <i>do</i> exist, look up parametric activation functions.</div><br/><div id="41737785" class="c"><input type="checkbox" id="c-41737785" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41737569">parent</a><span>|</span><a href="#41736838">next</a><span>|</span><label class="collapse" for="c-41737785">[-]</label><label class="expand" for="c-41737785">[1 more]</label></div><br/><div class="children"><div class="content">If course they do exist. A parameterized activation function is the most obvious thing to <i>try</i> in NN design, and has certainly been invented&#x2F;studied by 1000s of researchers.</div><br/></div></div></div></div></div></div></div></div><div id="41736838" class="c"><input type="checkbox" id="c-41736838" checked=""/><div class="controls bullet"><span class="by">beckhamc</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736475">parent</a><span>|</span><a href="#41736937">prev</a><span>|</span><a href="#41737140">next</a><span>|</span><label class="collapse" for="c-41736838">[-]</label><label class="expand" for="c-41736838">[4 more]</label></div><br/><div class="children"><div class="content">How was that person derailing the convo? Nothing says an activation function has to &quot;squash&quot; a number to be in some range. Leaky ReLUs for instance do `f(x) = x if x &gt; 0 else ax` (for some coefficient `a != 0`), that doesn&#x27;t squash `x` to be in any range (unless you want to be peculiar about your precise definition of what it means to squash a number). The function takes a real in `[-inf, inf]` and produces a number in `[-inf, inf]`.<p>&gt; Sure there&#x27;s a squashing function on the output to keep it in a range from 0 to 1 but that&#x27;s done BECAUSE we&#x27;re just adding up stuff.<p>It&#x27;s not because you&#x27;re &quot;adding up stuff&quot;, there is specific mathematical or statistical reason why it is used. For neural networks it&#x27;s there to stop your multi layer network collapsing to a single layer one (i.e. a linear algebra reason). You can choose whatever function you want, for hidden layers tanh generally isn&#x27;t used anymore, it&#x27;s usually some variant of a ReLU. In fact Leaky ReLUs are very commonly used so OP isn&#x27;t changing the subject.<p>If you define a &quot;perceptron&quot; (`g(Wx+b)` and `W` is a `Px1` matrix) and train it as a logistic regression model then you want `g` to be sigmoid. Its purpose is to ensure that the output can be interpreted as a probability (given that use the correct statistical loss), which means squashing the number. The inverse isn&#x27;t true, if I take random numbers from the internet and squash them to `[0,1]` I don&#x27;t go call them probabilities.<p>&gt; and not only is it&#x27;s PRIMARY function to squash a number, that&#x27;s it&#x27;s ONLY function.<p>Squashing the number isn&#x27;t the reason, it&#x27;s the side effect. And even then, I just said that not all activation functions squash numbers.<p>&gt; All the training does is adjust linear weights tho, like I said.<p>Not sure what your point is. What is a &quot;linear weight&quot;?<p>We call layers of the form `g(Wx+b)` &quot;linear&quot; layers but that&#x27;s an abused term, if g() is non-linear then the output is not linear. Who cares if the inner term `Wx + b` is linear? With enough of these layers you can approximate fairly complicated functions. If you&#x27;re arguing as to whether there is a better fundamental building block then that is another discussion.</div><br/><div id="41737188" class="c"><input type="checkbox" id="c-41737188" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736838">parent</a><span>|</span><a href="#41737140">next</a><span>|</span><label class="collapse" for="c-41737188">[-]</label><label class="expand" for="c-41737188">[3 more]</label></div><br/><div class="children"><div class="content">&gt; What is a &quot;linear weight&quot;?<p>In the context of discussing linearity v.s. non-linearity adding the word &quot;linear&quot; in front of &quot;weight&quot; is more clear, which is what my top level post on this thread was all about too.<p>It&#x27;s astounding to me (and everyone else who&#x27;s being honest) that LLMs can accomplish what they do when it&#x27;s only linear &quot;factors&quot; (i.e. weights) that are all that&#x27;s required to be adjusted during training, to achieve genuine reasoning. During training we&#x27;re not [normally] adjusting any parameters or weights on any non-linear functions. I include the caveat &quot;normally&quot;, because I&#x27;m speaking of the basic Perceptron NN using a squashing-type activation function.</div><br/><div id="41737788" class="c"><input type="checkbox" id="c-41737788" checked=""/><div class="controls bullet"><span class="by">viktor_von</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41737188">parent</a><span>|</span><a href="#41737140">next</a><span>|</span><label class="collapse" for="c-41737788">[-]</label><label class="expand" for="c-41737788">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s astounding to me (and everyone else who&#x27;s being honest) that LLMs can accomplish what they do when it&#x27;s only linear &quot;factors&quot; (i.e. weights) that are all that&#x27;s required to be adjusted during training, to achieve genuine reasoning.<p>When such basic perceptrons are scaled enormously, it becomes less surprising that they can achieve some level of &#x27;genuine reasoning&#x27; (e.g., accurate next-word prediction), since the goal with such networks at the end of the day is just function approximation. What is more surprising to me is how we found ways to train such models i.e., advances in hardware accelerators, combined with massive data, which are factors just as significant in my opinion.</div><br/><div id="41738173" class="c"><input type="checkbox" id="c-41738173" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41737788">parent</a><span>|</span><a href="#41737140">next</a><span>|</span><label class="collapse" for="c-41738173">[-]</label><label class="expand" for="c-41738173">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, no one is surprised that LLMs do what they&#x27;re trained to do: predict tokens. The surprise comes from the fact that merely training to predict tokens ends up with model weights that generate emergent reasoning.<p>If you want to say reasoning and token prediction are just the same thing at scale you can say that, but I don&#x27;t fall into that camp. I think there&#x27;s MUCH more to learn, and indeed a new field of math or even physics that we haven&#x27;t even discovered yet. Like a step change in mathematical understanding analogous to the invention of Calculus.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41737140" class="c"><input type="checkbox" id="c-41737140" checked=""/><div class="controls bullet"><span class="by">wrs</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736117">parent</a><span>|</span><a href="#41736189">prev</a><span>|</span><a href="#41736755">next</a><span>|</span><label class="collapse" for="c-41737140">[-]</label><label class="expand" for="c-41737140">[2 more]</label></div><br/><div class="children"><div class="content">With a ReLU activation function, rather than a simple linear function of the inputs, you get a <i>piecewise linear approximation</i> of a nonlinear function.<p>ReLU enables this by being nonlinear in a simple way, specifically by outputting zero for negative inputs, so each linear unit can then limit its contribution to a portion of the output curve.<p>(This is a lot easier to see on a whiteboard!)</div><br/><div id="41738223" class="c"><input type="checkbox" id="c-41738223" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41737140">parent</a><span>|</span><a href="#41736755">next</a><span>|</span><label class="collapse" for="c-41738223">[-]</label><label class="expand" for="c-41738223">[1 more]</label></div><br/><div class="children"><div class="content">ReLU technically has a non-linearity at zero, but in some sense it&#x27;s still even MORE linear than tanh or sigmoid, so it just demonstrates even better than tanh-type squashing that all this LLM stuff is being done ultimately with straight line math. All a ReLU function does is choose which line to use, a sloped one or a zero one.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41736755" class="c"><input type="checkbox" id="c-41736755" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41735261">parent</a><span>|</span><a href="#41735396">prev</a><span>|</span><a href="#41733784">next</a><span>|</span><label class="collapse" for="c-41736755">[-]</label><label class="expand" for="c-41736755">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s astounding that we DID get the emergent intelligence from just doing this &quot;curve fitting&quot; onto &quot;lines&quot; rather than actual &quot;curves&quot;.<p>In Ye Olden days (the 90’s) we used to approximate non-linear models using splines or seperate slopes models - fit by hand.  They were still linear, but with the right choice of splines you could approximate a non-linear model to whatever degree of accuracy you wanted.<p>Neural networks “just” do this automatically, and faster.</div><br/><div id="41738258" class="c"><input type="checkbox" id="c-41738258" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41733414">root</a><span>|</span><a href="#41736755">parent</a><span>|</span><a href="#41733784">next</a><span>|</span><label class="collapse" for="c-41738258">[-]</label><label class="expand" for="c-41738258">[1 more]</label></div><br/><div class="children"><div class="content">In college (BSME) I wrote a computer program to generate cam profiles from Bezier curves. It&#x27;s just a programming trick to generate curves from straight lines at any level of accuracy you want just by letting the computer take smaller and smaller steps.<p>It&#x27;s an interesting concept to think of how NNs might be able to exploit this effect in some way based on straight lines in the weights, because a very small number of points can identify avery precise and smooth curves, where directions on the curve might equate to Semantic Space Vectors.</div><br/></div></div></div></div></div></div></div></div><div id="41733784" class="c"><input type="checkbox" id="c-41733784" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#41733414">prev</a><span>|</span><a href="#41739313">next</a><span>|</span><label class="collapse" for="c-41733784">[-]</label><label class="expand" for="c-41733784">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Transformers required ~2.5x more training steps to achieve comparable performance, overfitting eventually.<p>&gt; RNNs are particularly suitable for sequence modelling settings such as those involving time series, natural language processing, and other sequential tasks where context from previous steps informs the current prediction.<p>I would like to draw an analogy to digital signal processing. If you think of the recurrent-style architectures as IIR filters and feedforward-only architectures as FIR filters, you will likely find many parallels.<p>The most obvious to me being that IIR filters typically require far fewer elements to produce the same response as an equivalent FIR filter. Granted, the FIR filter is often easier to implement&#x2F;control&#x2F;measure in practical terms (fixed-point arithmetic hardware == ML architectures that can run on GPUs).<p>I don&#x27;t think we get to the exponential scary part of AI without some fundamentally recurrent architecture. I think things like LSTM are kind of an in-between hack in this DSP analogy - You could look at it as FIR with dynamic coefficients. Neuromorphic approaches seem like the best long term bet to me in terms of efficiency.</div><br/></div></div><div id="41739313" class="c"><input type="checkbox" id="c-41739313" checked=""/><div class="controls bullet"><span class="by">scotty79</span><span>|</span><a href="#41733784">prev</a><span>|</span><a href="#41738709">next</a><span>|</span><label class="collapse" for="c-41739313">[-]</label><label class="expand" for="c-41739313">[1 more]</label></div><br/><div class="children"><div class="content">The only strength of transformers is that they can run once for each token and they can pass to themselves intermediate state as they solve your problems. They have to conceal it in tokens that look to humans like a part of the response.<p>It&#x27;s obvious why the newest toy from openai can solve problems better mostly by just being allowed to &quot;talk to itself&quot; for a moment before starting the answer that human sees.<p>Given that, modern incarnation of RNN can be vastly cheaper than transformers provided that they can be trained.<p>Convolutional neural networks get more visual understanding by &quot;reusing&quot; their capacity across the area of the image. RNN&#x27;s and transformers can have better understanding of a given problem by &quot;reusing&quot; their capacity to learn and infer across time (across steps of iterative process really).</div><br/></div></div><div id="41738709" class="c"><input type="checkbox" id="c-41738709" checked=""/><div class="controls bullet"><span class="by">theanonymousone</span><span>|</span><a href="#41739313">prev</a><span>|</span><a href="#41734720">next</a><span>|</span><label class="collapse" for="c-41738709">[-]</label><label class="expand" for="c-41738709">[1 more]</label></div><br/><div class="children"><div class="content">I remember that, the way I understood it, Transformers solved two major &quot;issues&quot; of RNNs that enabled the later boom: Vanishing gradients limiting the context (and model?) size and difficulty in parallelisation limiting the size of the training data.<p>Do we have solutions for these two problems now?</div><br/></div></div><div id="41734720" class="c"><input type="checkbox" id="c-41734720" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#41738709">prev</a><span>|</span><a href="#41735588">next</a><span>|</span><label class="collapse" for="c-41734720">[-]</label><label class="expand" for="c-41734720">[17 more]</label></div><br/><div class="children"><div class="content">My feeling is that the answer is &quot;no&quot;, in the sense that these RNNs wouldn&#x27;t be able to universally replace Transformers in LLMs, even though they might be good enough in some cases and beat them in others.<p>Here&#x27;s why.<p>A user of an LLM <i>might</i> give the model some long text and then say &quot;Translate this into German please&quot;. A Transformer can look back at its whole history. But what is an RNN to do? While the length of its context is unlimited, the amount of information the model retains about it is bounded by whatever is in its hidden state at any given time.<p>Relevant: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.01032" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.01032</a></div><br/><div id="41735836" class="c"><input type="checkbox" id="c-41735836" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41734720">parent</a><span>|</span><a href="#41734734">next</a><span>|</span><label class="collapse" for="c-41735836">[-]</label><label class="expand" for="c-41735836">[8 more]</label></div><br/><div class="children"><div class="content">&gt; the amount of information the model retains about it is bounded by whatever is in its hidden state<p>This is no different than a transformer, which, after all, is bound by a finite state, just organized in a different manner.</div><br/><div id="41736171" class="c"><input type="checkbox" id="c-41736171" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#41734720">root</a><span>|</span><a href="#41735836">parent</a><span>|</span><a href="#41734734">next</a><span>|</span><label class="collapse" for="c-41736171">[-]</label><label class="expand" for="c-41736171">[7 more]</label></div><br/><div class="children"><div class="content">&gt; This is no different than a transformer, which, after all, is bound by a finite state, just organized in a different manner.<p>It&#x27;s not just a matter of organizing things differently. Suppose your network dimension and sequence length are both X.<p>Then your memory usage (per layer) will be O(X^2), while your training update cost will be O(X^3). That&#x27;s for both Transformers and RNNs.<p>However, at the end of the sequence, a Transformer layer can look back see O(X^2) numbers, while an RNN can only see O(X) numbers.</div><br/><div id="41736538" class="c"><input type="checkbox" id="c-41736538" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41734720">root</a><span>|</span><a href="#41736171">parent</a><span>|</span><a href="#41736688">next</a><span>|</span><label class="collapse" for="c-41736538">[-]</label><label class="expand" for="c-41736538">[2 more]</label></div><br/><div class="children"><div class="content">Simplistic thinking. An RNN hidden parameter space of high dimension provides plenty of room for linear projections of token histories. I think people just do not realize just how huge R^N can be.</div><br/><div id="41737056" class="c"><input type="checkbox" id="c-41737056" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#41734720">root</a><span>|</span><a href="#41736538">parent</a><span>|</span><a href="#41736688">next</a><span>|</span><label class="collapse" for="c-41737056">[-]</label><label class="expand" for="c-41737056">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Simplistic thinking. An RNN hidden parameter space of high dimension provides plenty of room for linear projections of token histories. I think people just do not realize just how huge R^N can be.<p>16N bits as hard limit, but more realistically, about 2N bits or less of useful information probably.<p>You&#x27;d need to grow the network dimension in proportion to the maximum sequence length just to avoid the information theoretical limit.</div><br/></div></div></div></div><div id="41736688" class="c"><input type="checkbox" id="c-41736688" checked=""/><div class="controls bullet"><span class="by">f_devd</span><span>|</span><a href="#41734720">root</a><span>|</span><a href="#41736171">parent</a><span>|</span><a href="#41736538">prev</a><span>|</span><a href="#41734734">next</a><span>|</span><label class="collapse" for="c-41736688">[-]</label><label class="expand" for="c-41736688">[4 more]</label></div><br/><div class="children"><div class="content">Transformers actually have an quantifiable state size (see <a href="https:&#x2F;&#x2F;hazyresearch.stanford.edu&#x2F;static&#x2F;posts&#x2F;2024-06-22-ac&#x2F;tradeoff_ac.jpg" rel="nofollow">https:&#x2F;&#x2F;hazyresearch.stanford.edu&#x2F;static&#x2F;posts&#x2F;2024-06-22-ac...</a>) although it&#x27;s anywhere between 200k and 2M floats (for 360M and 1.33B respectively iinm). So a sufficiently sized RNN could have the same state capacity as a transformer.<p>(this is from the Based paper: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2402.18668" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2402.18668</a>)</div><br/><div id="41737276" class="c"><input type="checkbox" id="c-41737276" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#41734720">root</a><span>|</span><a href="#41736688">parent</a><span>|</span><a href="#41734734">next</a><span>|</span><label class="collapse" for="c-41737276">[-]</label><label class="expand" for="c-41737276">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Transformers actually have an quantifiable state size<p>Are you griping about my writing O(X^2) above instead of precisely 2X^2, like this paper? The latter implies the former.<p>&gt; So a sufficiently sized RNN could have the same state capacity as a transformer.<p>Does this contradict anything I&#x27;ve said? If you increase the size of the RNN, while keeping the Transformer fixed, you can match their recurrent state sizes (if you don&#x27;t run out of RAM or funding)</div><br/><div id="41738043" class="c"><input type="checkbox" id="c-41738043" checked=""/><div class="controls bullet"><span class="by">f_devd</span><span>|</span><a href="#41734720">root</a><span>|</span><a href="#41737276">parent</a><span>|</span><a href="#41734734">next</a><span>|</span><label class="collapse" for="c-41738043">[-]</label><label class="expand" for="c-41738043">[2 more]</label></div><br/><div class="children"><div class="content">I was responding to<p>&gt; a Transformer layer can look back see O(X^2) numbers, while an RNN can only see O(X) numbers<p>The thing is RNN can look back infinitely if you don&#x27;t exceed the state capacity. For transformers the state it is defined semi-implicitly (you can change the hidden dims but you cannot extend the look back; ignoring transformer-xl et al.) defined by the amount of tokens, for an RNN it&#x27;s defined explicitly by the state size.<p>The big-O here is irrelevant for the architectures since it&#x27;s all in the configuration &amp; implementation of the model; i.e. there is no relevant asymptote to compare.<p>As an aside this was what was shown in the based paper, the fact that you can have a continuity of state (as with RNN) while have the same associative recall capability as a transformer (the main downfall of recurrent methods at that point).</div><br/><div id="41738520" class="c"><input type="checkbox" id="c-41738520" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#41734720">root</a><span>|</span><a href="#41738043">parent</a><span>|</span><a href="#41734734">next</a><span>|</span><label class="collapse" for="c-41738520">[-]</label><label class="expand" for="c-41738520">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The big-O here is irrelevant for the architectures since it&#x27;s all in the configuration &amp; implementation of the model; i.e. there is no relevant asymptote to compare.<p>?!<p>NNs are like any other algorithm in this regard. Heck, look at the bottom of page 2 of the Were RNNs All We Needed paper. It has big-O notation there and elsewhere.<p>&gt; I was responding to<p>&gt;&gt; a Transformer layer can look back see O(X^2) numbers, while an RNN can only see O(X) numbers<p>In the BASED paper, in Eq. 10, sizeof(s) = 2dN. But I defined d = N = X above. Ergo, sizeof(s) = 2X^2 = O(X^2).<p>For minGRU, sizeof(s) = d. Ergo, sizeof(s) = X = O(X).</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41734734" class="c"><input type="checkbox" id="c-41734734" checked=""/><div class="controls bullet"><span class="by">mkaic</span><span>|</span><a href="#41734720">parent</a><span>|</span><a href="#41735836">prev</a><span>|</span><a href="#41735112">next</a><span>|</span><label class="collapse" for="c-41734734">[-]</label><label class="expand" for="c-41734734">[3 more]</label></div><br/><div class="children"><div class="content">The counterargument here is that you can just scale the size of the hidden state sufficiently such that it can hold compressed representations of whatever-length sequence you like. Ultimately, what I care about is whether RNNs could compete with transformers if FLOPs are held constant—something TFA doesn&#x27;t really investigate.</div><br/><div id="41734804" class="c"><input type="checkbox" id="c-41734804" checked=""/><div class="controls bullet"><span class="by">psb217</span><span>|</span><a href="#41734720">root</a><span>|</span><a href="#41734734">parent</a><span>|</span><a href="#41735112">next</a><span>|</span><label class="collapse" for="c-41734804">[-]</label><label class="expand" for="c-41734804">[2 more]</label></div><br/><div class="children"><div class="content">Well, that&#x27;s what Transformer already does... One problem with the scaling you&#x27;re describing is that there would be a massive amount of redundant information stored in hidden activations during training the RNN. The hidden state at each time step t in the sequence would need to contain all info that (i) could be useful for predicting the token at time t and (ii) that could be useful for predicting tokens at times &gt;t. (i) is obvious and (ii) is since all information about the past is transferred to future predictions through the current hidden state. In principle, Transformers can avoid storing redundant info in multiple hidden states at the cost of having to maintain and access (via attention) a larger hidden state at test&#x2F;eval time.</div><br/><div id="41735005" class="c"><input type="checkbox" id="c-41735005" checked=""/><div class="controls bullet"><span class="by">mkaic</span><span>|</span><a href="#41734720">root</a><span>|</span><a href="#41734804">parent</a><span>|</span><a href="#41735112">next</a><span>|</span><label class="collapse" for="c-41735005">[-]</label><label class="expand" for="c-41735005">[1 more]</label></div><br/><div class="children"><div class="content">&gt; there would be a massive amount of redundant information stored in hidden activations<p>Is there a way to prove this? One potential caveat that comes to mind for me is that perhaps the action of lerping between the old state and the new could be used by the model to perform semantically meaningful transformations on the old state. I guess in my mind it just doesn&#x27;t seem obvious that the hidden state is necessarily a collection of &quot;redundant information&quot; — perhaps the information is culled&#x2F;distilled the further along in the sequence you go? There will always be <i>some</i> redundancy, sure, but I don&#x27;t think that such redundancy necessarily means we <i>have</i> to use superlinear methods like attention.</div><br/></div></div></div></div></div></div><div id="41735112" class="c"><input type="checkbox" id="c-41735112" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#41734720">parent</a><span>|</span><a href="#41734734">prev</a><span>|</span><a href="#41735588">next</a><span>|</span><label class="collapse" for="c-41735112">[-]</label><label class="expand" for="c-41735112">[5 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; A user of an LLM might give the model some long text and then say &quot;Translate this into German please&quot;. A Transformer can look back at its whole history.<p>Which isn&#x27;t necessary. If you say &quot;translate the following to german.&quot; Instead, all it needs is to remember the task at hand and a much smaller amount of recent input. Well, and the ability to output in parallel with processing input.</div><br/><div id="41735307" class="c"><input type="checkbox" id="c-41735307" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#41734720">root</a><span>|</span><a href="#41735112">parent</a><span>|</span><a href="#41735866">next</a><span>|</span><label class="collapse" for="c-41735307">[-]</label><label class="expand" for="c-41735307">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s necessary for arbitrary information processing if you can forget and have no way to &quot;unforget&quot;.<p>A model can decide to forget something that turns out to be important for some future prediction. A human can go back and re-read&#x2F;listen etc, A transformer is always re-reading but a RNN can&#x27;t and is fucked.</div><br/><div id="41735780" class="c"><input type="checkbox" id="c-41735780" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#41734720">root</a><span>|</span><a href="#41735307">parent</a><span>|</span><a href="#41735866">next</a><span>|</span><label class="collapse" for="c-41735780">[-]</label><label class="expand" for="c-41735780">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s just because we twisted it&#x27;s arm. One could for example feed the reversed input after, ie abc|cba where | is a special token. That would allow it to react to any part of the message.</div><br/></div></div></div></div><div id="41735866" class="c"><input type="checkbox" id="c-41735866" checked=""/><div class="controls bullet"><span class="by">DoctorOetker</span><span>|</span><a href="#41734720">root</a><span>|</span><a href="#41735112">parent</a><span>|</span><a href="#41735307">prev</a><span>|</span><a href="#41735505">next</a><span>|</span><label class="collapse" for="c-41735866">[-]</label><label class="expand" for="c-41735866">[1 more]</label></div><br/><div class="children"><div class="content">Also, a lightweight network could do a first pass to identify tasks, instructions, constraints etc, and then a second pass could use the RNN.<p>Consider the flood fill algorithm or union-find algorithm, which feels magical upon first exposure.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hoshen%E2%80%93Kopelman_algorithm" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hoshen%E2%80%93Kopelman_algori...</a><p>Having 2 passes can enable so much more than a single pass.<p>Another alternative could be to have a first pass make notes in a separate buffer while parsing the input. The bandwidth of the note taking and reading can be much much lower than that required for fetching the billions of parameters.</div><br/></div></div><div id="41735505" class="c"><input type="checkbox" id="c-41735505" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#41734720">root</a><span>|</span><a href="#41735112">parent</a><span>|</span><a href="#41735866">prev</a><span>|</span><a href="#41735588">next</a><span>|</span><label class="collapse" for="c-41735505">[-]</label><label class="expand" for="c-41735505">[1 more]</label></div><br/><div class="children"><div class="content">People did something similar to what you are describing 10 years ago: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473</a><p>But it&#x27;s trained on translations, rather than the whole Internet.</div><br/></div></div></div></div></div></div><div id="41735588" class="c"><input type="checkbox" id="c-41735588" checked=""/><div class="controls bullet"><span class="by">charlescurt123</span><span>|</span><a href="#41734720">prev</a><span>|</span><a href="#41736577">next</a><span>|</span><label class="collapse" for="c-41735588">[-]</label><label class="expand" for="c-41735588">[1 more]</label></div><br/><div class="children"><div class="content">I find the entire field lacking when it comes to long-horizon problems. Our current, widely used solution is to scale, but we&#x27;re nowhere near achieving the horizon scales even small mammal brains can handle. Our models can have trillions of parameters, yet a mouse brain would still outperform them on long-horizon tasks and efficiency. It&#x27;s something small, simple, and elegant—an incredible search algorithm that not only finds near-optimal routes but also continuously learns on a fixed computational budget.<p>I&#x27;m honestly a bit envious of future engineers who will be tackling these kinds of problems with a 100-line Jupyter notebook on a laptop years from now. If we discovered the right method or algorithm for these long-horizon problems, a 2B-parameter model might even outperform current models on everything except short, extreme reasoning problems.<p>The only solution I&#x27;ve ever considered for this is expanding a model&#x27;s dimensionality over time, rather than focusing on perfect weights. The higher dimensionality you can provide to a model, the greater its theoretical storage capacity. This could resemble a two-layer model—one layer acting as a superposition of multiple ideal points, and the other layer knowing how to use them.<p>When you think about the loss landscape, imagine it with many minima for a given task. If we could create a method that navigates these minima by reconfiguring the model when needed, we could theoretically develop a single model with near-infinite local minima—and therefore, higher-dimensional memory. This may sound wild, but consider the fact that the human brain potentially creates and disconnects thousands of new connections in a single day. Could it be that these connections steer our internal loss landscape between different minima we need throughout the day?</div><br/></div></div><div id="41736577" class="c"><input type="checkbox" id="c-41736577" checked=""/><div class="controls bullet"><span class="by">vandahm</span><span>|</span><a href="#41735588">prev</a><span>|</span><a href="#41734344">next</a><span>|</span><label class="collapse" for="c-41736577">[-]</label><label class="expand" for="c-41736577">[1 more]</label></div><br/><div class="children"><div class="content">I made a RNN for a college project because I was interested in obsolete historical technology and I thought I needed to seize the opportunity while it lasted, because once I was out of school, I&#x27;d never hear about neural networks ever again.<p>Mine worked, but it was very simple and dog slow, running on my old laptop. Nothing was ever going to run fast on that thing, but I remember my RNN being substantially slower than a feed-forward network would have been.<p>I was <i>so confident</i> that this was dead technology -- an academic curiosity from the 1980s and 1990s. It was bizarre to see how quickly that changed.</div><br/></div></div><div id="41734344" class="c"><input type="checkbox" id="c-41734344" checked=""/><div class="controls bullet"><span class="by">mkaic</span><span>|</span><a href="#41736577">prev</a><span>|</span><a href="#41733288">next</a><span>|</span><label class="collapse" for="c-41734344">[-]</label><label class="expand" for="c-41734344">[4 more]</label></div><br/><div class="children"><div class="content">I strongly enjoy the simplicity of their &quot;minGRU&quot; architecture. It&#x27;s basically just:<p><pre><code>  class MinGRU(nn.Module):
    def __init__(self, token_size, hidden_state_size):
      self.token_to_proposal = nn.Linear(token_size, hidden_size)
      self.token_to_mix_factors = nn.Linear(token_size, hidden_size)

    def forward(self, previous_hidden_state, current_token):
      proposed_hidden_state = self.token_to_proposal(current_token)
      mix_factors = torch.sigmoid(self.token_to_mix_factors(current_token))
      return torch.lerp(proposed_hidden_state, previous_hidden_state, mix_factors)
</code></pre>
And since the proposed hidden states and mix factors for each layer are both only dependent on the current token, you can compute all of them in parallel if you know the whole sequence ahead of time (like during training), and then combine them in linear time using parallel scan.<p>The fact that this is competitive with transformers and state-space models in their small-scale experiments is gratifying to the &quot;best PRs are the ones that delete code&quot; side of me. That said, we won&#x27;t know for sure if this is a capital-B Breakthrough until someone tries scaling it up to parameter and data counts comparable to SOTA models.<p>One detail I found really interesting is that they seem to do all their calculations in log-space, according to the Appendix. They say it&#x27;s for numerical stability, which is curious to me—I&#x27;m not sure I have a good intuition for why running everything in log-space makes the model more stable. Is it because they removed the tanh from the output, making it possible for values to explode if calculations are done in linear space?<p>EDIT: Another thought—it&#x27;s kind of fascinating that this sort of sequence modeling works at all. It&#x27;s like if I gave you all the pages of a book individually torn out and in a random order, and asked you to try to make a vector representation for each page as well as instructions for how to mix that vector with the vector representing all previous pages — except you have zero knowledge of those previous pages. Then, I take all your page vectors, sequentially mix them together in-order, and grade you based on how good of a whole-book summary the final vector represents. Wild stuff.<p>FURTHER EDIT: Yet <i>another</i> thought—right now, they&#x27;re just using two dense linear layers to transform the token into the proposed hidden state and the lerp mix factors. I&#x27;m  curious what would happen if you made those transforms MLPs instead of singular linear layers.</div><br/><div id="41735858" class="c"><input type="checkbox" id="c-41735858" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#41734344">parent</a><span>|</span><a href="#41734956">next</a><span>|</span><label class="collapse" for="c-41735858">[-]</label><label class="expand" for="c-41735858">[1 more]</label></div><br/><div class="children"><div class="content">Log space is important if the token probabilities span a large range of values (powers). There is a reason that maximum likelihood fitting is always performed with log likelihoods.</div><br/></div></div><div id="41734956" class="c"><input type="checkbox" id="c-41734956" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#41734344">parent</a><span>|</span><a href="#41735858">prev</a><span>|</span><a href="#41733288">next</a><span>|</span><label class="collapse" for="c-41734956">[-]</label><label class="expand" for="c-41734956">[2 more]</label></div><br/><div class="children"><div class="content">This architecture, on the surface, seems to preclude the basic function of recognizing sequences of tokens. At the very least, it seems like it should suffer from something like the pumping lemma: if [the ][cat ][is ][black ] results in the output getting close to a certain vector, [the ][cat ][is ][black ][the ][cat ][is ][black ][the ][cat ][is ][black ] should get even closer to that vector and nowhere close to a &quot;why did you just repeat the same sentence three times&quot; vector? Without non-linear mixing between input token and hidden state, there will be a lot of linear similarities between similar token sequences...</div><br/><div id="41735033" class="c"><input type="checkbox" id="c-41735033" checked=""/><div class="controls bullet"><span class="by">mkaic</span><span>|</span><a href="#41734344">root</a><span>|</span><a href="#41734956">parent</a><span>|</span><a href="#41733288">next</a><span>|</span><label class="collapse" for="c-41735033">[-]</label><label class="expand" for="c-41735033">[1 more]</label></div><br/><div class="children"><div class="content">Counterpoint: the hidden state at the beginning of ([the][cat][is][black]) x 3 is (probably) initialized to all zeros, but after seeing those first 4 tokens, it will <i>not</i> be all zeros. Thus, going into the second repetition of the sentence, the model has a different initial hidden state, and should exhibit different behavior. I think this makes it possible for the model to learn to recognize repeated sequences and avoid your proposed pitfall.</div><br/></div></div></div></div></div></div><div id="41733288" class="c"><input type="checkbox" id="c-41733288" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#41734344">prev</a><span>|</span><a href="#41737137">next</a><span>|</span><label class="collapse" for="c-41733288">[-]</label><label class="expand" for="c-41733288">[3 more]</label></div><br/><div class="children"><div class="content">To their credit, the authors (Y. Bengio among them) end the paper with the question, not suggesting they know the answer. These models are very small even by academic standards so any finding would not necessarily extend to current LLM scales. The main conclusion is that RNN class networks can be trained as efficiently as modern alternatives but the resulting performance is only competitive at small scale.</div><br/><div id="41734212" class="c"><input type="checkbox" id="c-41734212" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#41733288">parent</a><span>|</span><a href="#41737137">next</a><span>|</span><label class="collapse" for="c-41734212">[-]</label><label class="expand" for="c-41734212">[2 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; These models are very small even by academic standards so any finding would <i>not necessarily</i> extend to current LLM scales.<p>Emphasis on not necessarily.<p>&gt;&gt; The main conclusion is that RNN class networks can be trained as efficiently as modern alternatives but the resulting performance is only competitive at small scale.<p>Shouldn&#x27;t the conclusion be &quot;the resulting competitive performance has only been confirmed at small scale&quot;?</div><br/><div id="41737970" class="c"><input type="checkbox" id="c-41737970" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#41733288">root</a><span>|</span><a href="#41734212">parent</a><span>|</span><a href="#41737137">next</a><span>|</span><label class="collapse" for="c-41737970">[-]</label><label class="expand" for="c-41737970">[1 more]</label></div><br/><div class="children"><div class="content">yes, that is clearer indeed. However S4 and Mamba class models have also performed well at small scale and started lagging with larger models and larger context sizes, or at particular tasks.</div><br/></div></div></div></div></div></div><div id="41737137" class="c"><input type="checkbox" id="c-41737137" checked=""/><div class="controls bullet"><span class="by">hdivider</span><span>|</span><a href="#41733288">prev</a><span>|</span><a href="#41733016">next</a><span>|</span><label class="collapse" for="c-41737137">[-]</label><label class="expand" for="c-41737137">[2 more]</label></div><br/><div class="children"><div class="content">I still find it remarkable how we need such an extreme amount of electrical energy to power large modern AI models.<p>Compare with one human brain. Far more sophisticated, even beyond our knowledge. What does it take to power it for a day? Some vegetables and rice. Still fine for a while if you supply pure junk food -- it&#x27;ll still perform.<p>Clearly we have a long, long way to go in terms of the energy efficiency of AI approaches. Our so-called <i>neural</i> nets clearly don&#x27;t resemble the energy efficiency of actual biological neurons.</div><br/><div id="41738814" class="c"><input type="checkbox" id="c-41738814" checked=""/><div class="controls bullet"><span class="by">Arch485</span><span>|</span><a href="#41737137">parent</a><span>|</span><a href="#41733016">next</a><span>|</span><label class="collapse" for="c-41738814">[-]</label><label class="expand" for="c-41738814">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s even less! A lot of those vegetables and rice go into powering your heart, muscles, organs, etc. and only a fraction is used for the brain.<p>Maybe the future of AI is in organic neurons?</div><br/></div></div></div></div><div id="41733016" class="c"><input type="checkbox" id="c-41733016" checked=""/><div class="controls bullet"><span class="by">tehsauce</span><span>|</span><a href="#41737137">prev</a><span>|</span><a href="#41738396">next</a><span>|</span><label class="collapse" for="c-41733016">[-]</label><label class="expand" for="c-41733016">[5 more]</label></div><br/><div class="children"><div class="content">I haven’t gone through the paper in detail yet but maybe someone can answer.  
 If you remove the hidden state from an rnn as they say they’ve done, what’s left? An mlp predicting from a single token?</div><br/><div id="41733387" class="c"><input type="checkbox" id="c-41733387" checked=""/><div class="controls bullet"><span class="by">bunderbunder</span><span>|</span><a href="#41733016">parent</a><span>|</span><a href="#41733324">next</a><span>|</span><label class="collapse" for="c-41733387">[-]</label><label class="expand" for="c-41733387">[1 more]</label></div><br/><div class="children"><div class="content">They didn&#x27;t remove the hidden state entirely, they just removed it from the input, forget and update gates. I haven&#x27;t digested the paper either, but I think that in the case of a GRU this means that the hidden state update masking (z_t and r_t in the paper&#x27;s formulas) only depends on the new input, not the input plus the prior hidden state.</div><br/></div></div><div id="41733324" class="c"><input type="checkbox" id="c-41733324" checked=""/><div class="controls bullet"><span class="by">jfcoa</span><span>|</span><a href="#41733016">parent</a><span>|</span><a href="#41733387">prev</a><span>|</span><a href="#41733138">next</a><span>|</span><label class="collapse" for="c-41733324">[-]</label><label class="expand" for="c-41733324">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t completely remove it, it removes certain dependencies on it so that it can be computed by parallel scan, there is still a hidden state. It bears some similarity to what was done with Mamba.</div><br/></div></div><div id="41733138" class="c"><input type="checkbox" id="c-41733138" checked=""/><div class="controls bullet"><span class="by">statusfailed</span><span>|</span><a href="#41733016">parent</a><span>|</span><a href="#41733324">prev</a><span>|</span><a href="#41734457">next</a><span>|</span><label class="collapse" for="c-41733138">[-]</label><label class="expand" for="c-41733138">[1 more]</label></div><br/><div class="children"><div class="content">I only had a quick look, but it looks like they tweaked the state update so the model can be run with parallel scan instead of having to do it sequentially.</div><br/></div></div><div id="41734457" class="c"><input type="checkbox" id="c-41734457" checked=""/><div class="controls bullet"><span class="by">_0ffh</span><span>|</span><a href="#41733016">parent</a><span>|</span><a href="#41733138">prev</a><span>|</span><a href="#41738396">next</a><span>|</span><label class="collapse" for="c-41734457">[-]</label><label class="expand" for="c-41734457">[1 more]</label></div><br/><div class="children"><div class="content">The trick is to make sure the recursive dependency stays linear, that&#x27;s how you enable parallel training.</div><br/></div></div></div></div><div id="41738396" class="c"><input type="checkbox" id="c-41738396" checked=""/><div class="controls bullet"><span class="by">moi2388</span><span>|</span><a href="#41733016">prev</a><span>|</span><a href="#41733471">next</a><span>|</span><label class="collapse" for="c-41738396">[-]</label><label class="expand" for="c-41738396">[1 more]</label></div><br/><div class="children"><div class="content">Yes, and it’s hardly surprising, since the Chinese room thought experiment is completely wrong; that is in fact exactly how you learn something.</div><br/></div></div><div id="41733471" class="c"><input type="checkbox" id="c-41733471" checked=""/><div class="controls bullet"><span class="by">m11a</span><span>|</span><a href="#41738396">prev</a><span>|</span><a href="#41735567">next</a><span>|</span><label class="collapse" for="c-41733471">[-]</label><label class="expand" for="c-41733471">[2 more]</label></div><br/><div class="children"><div class="content">It’d be nice to see more of how this compares to Mamba. Looks like, in performance, they’re not leagues apart and it’s just a <i>different</i> architecture, not necessarily better or worse?</div><br/><div id="41737050" class="c"><input type="checkbox" id="c-41737050" checked=""/><div class="controls bullet"><span class="by">yazzku</span><span>|</span><a href="#41733471">parent</a><span>|</span><a href="#41735567">next</a><span>|</span><label class="collapse" for="c-41737050">[-]</label><label class="expand" for="c-41737050">[1 more]</label></div><br/><div class="children"><div class="content">Look at the memory consumption diagram on page 6. It looks like you&#x27;re basically getting the same running time for less memory usage.</div><br/></div></div></div></div><div id="41736883" class="c"><input type="checkbox" id="c-41736883" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#41735567">prev</a><span>|</span><a href="#41733683">next</a><span>|</span><label class="collapse" for="c-41736883">[-]</label><label class="expand" for="c-41736883">[1 more]</label></div><br/><div class="children"><div class="content">RNNs always had better scaling law curves than transformers.<p>BPTT was their problem</div><br/></div></div><div id="41733683" class="c"><input type="checkbox" id="c-41733683" checked=""/><div class="controls bullet"><span class="by">marcosdumay</span><span>|</span><a href="#41736883">prev</a><span>|</span><a href="#41733653">next</a><span>|</span><label class="collapse" for="c-41733683">[-]</label><label class="expand" for="c-41733683">[14 more]</label></div><br/><div class="children"><div class="content">R == Recurrent<p>From theory the answer to the question should be &quot;yes&quot;, they are Turing complete.<p>The real question is about how to train them, and the paper is about that.</div><br/><div id="41733843" class="c"><input type="checkbox" id="c-41733843" checked=""/><div class="controls bullet"><span class="by">baanist</span><span>|</span><a href="#41733683">parent</a><span>|</span><a href="#41734151">next</a><span>|</span><label class="collapse" for="c-41733843">[-]</label><label class="expand" for="c-41733843">[10 more]</label></div><br/><div class="children"><div class="content">Why aren&#x27;t AI researchers automating the search for efficient architectures?</div><br/><div id="41734736" class="c"><input type="checkbox" id="c-41734736" checked=""/><div class="controls bullet"><span class="by">ActorNightly</span><span>|</span><a href="#41733683">root</a><span>|</span><a href="#41733843">parent</a><span>|</span><a href="#41734403">next</a><span>|</span><label class="collapse" for="c-41734736">[-]</label><label class="expand" for="c-41734736">[5 more]</label></div><br/><div class="children"><div class="content">There has been some work, but the problem is that its such a massive search space. Philosophically speaking, if you look at how humans came into existence, you could make an argument that the process of evolution from basic lifeforms can be represented as one giant compute per minute across of all of earth, where genetic selection happens and computation proceeds to the next minute. Thats a fuckload of compute.<p>In more practical terms, you would imagine that an advanced model contains some semblance of a CPU to be able to truly reason. Given that CPUs can be all NAND gates (which take 2 neurons to represent), and are structured in a recurrent way, you fundamentally have to rethink how to train such a network, because backprop obviously won&#x27;t work to capture things like binary decision points.</div><br/><div id="41735385" class="c"><input type="checkbox" id="c-41735385" checked=""/><div class="controls bullet"><span class="by">baanist</span><span>|</span><a href="#41733683">root</a><span>|</span><a href="#41734736">parent</a><span>|</span><a href="#41734403">next</a><span>|</span><label class="collapse" for="c-41735385">[-]</label><label class="expand" for="c-41735385">[4 more]</label></div><br/><div class="children"><div class="content">I thought the whole point of neural networks was that they were good at searching through these spaces. I&#x27;m pretty sure OpenAI is pruning their models behind the scenes to reduce their costs because that&#x27;s the only way they can keep reducing the cost per token. So their secret sauce at this point is whatever pruning AI they&#x27;re using to whittle the large computation graphs into more cost efficient consumer products.</div><br/><div id="41737411" class="c"><input type="checkbox" id="c-41737411" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#41733683">root</a><span>|</span><a href="#41735385">parent</a><span>|</span><a href="#41734403">next</a><span>|</span><label class="collapse" for="c-41737411">[-]</label><label class="expand" for="c-41737411">[3 more]</label></div><br/><div class="children"><div class="content">When you train a neural network, it is not search, it is descending through a curve.<p>If you were to search for billions of parameters by brute force, you literally could not do it in the lifespan of the universe.<p>A neural network is differentiable, meaning you can take the derivative of it. You train the parameters by taking finding gradient with respect to each parameter, and going in the opposite direction. Hence the name of the popular algorithm, gradient descent.</div><br/><div id="41738393" class="c"><input type="checkbox" id="c-41738393" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#41733683">root</a><span>|</span><a href="#41737411">parent</a><span>|</span><a href="#41738367">prev</a><span>|</span><a href="#41734403">next</a><span>|</span><label class="collapse" for="c-41738393">[-]</label><label class="expand" for="c-41738393">[1 more]</label></div><br/><div class="children"><div class="content">A biological neural network is certainly not differentiable. If the thing we want to build is not realizable with this technique, why can&#x27;t we move on from it?<p>Gradient descent isn&#x27;t the only way to do this. Evolutionary techniques can explore impossibly large, non-linear problem spaces.<p>Being able to define any kind of fitness function you want is sort of like a super power. You don&#x27;t have to think in such constrained ways down this path.</div><br/></div></div></div></div></div></div></div></div><div id="41734403" class="c"><input type="checkbox" id="c-41734403" checked=""/><div class="controls bullet"><span class="by">ks2048</span><span>|</span><a href="#41733683">root</a><span>|</span><a href="#41733843">parent</a><span>|</span><a href="#41734736">prev</a><span>|</span><a href="#41734575">next</a><span>|</span><label class="collapse" for="c-41734403">[-]</label><label class="expand" for="c-41734403">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Neural_architecture_search" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Neural_architecture_search</a></div><br/></div></div><div id="41734575" class="c"><input type="checkbox" id="c-41734575" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#41733683">root</a><span>|</span><a href="#41733843">parent</a><span>|</span><a href="#41734403">prev</a><span>|</span><a href="#41735886">next</a><span>|</span><label class="collapse" for="c-41734575">[-]</label><label class="expand" for="c-41734575">[2 more]</label></div><br/><div class="children"><div class="content">The search space is all off too wide, difficult to parameterize, and there is a wide gap between effective and ineffective architectures - ie: a very small change can make a network effectively DOA.</div><br/><div id="41734895" class="c"><input type="checkbox" id="c-41734895" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#41733683">root</a><span>|</span><a href="#41734575">parent</a><span>|</span><a href="#41735886">next</a><span>|</span><label class="collapse" for="c-41734895">[-]</label><label class="expand" for="c-41734895">[1 more]</label></div><br/><div class="children"><div class="content">Notably architecture search was popular for small vision nets where the cost of many training runs was low enough. I suspect some of the train-then-prune approaches will come back, but even there only by the best funded teams.</div><br/></div></div></div></div></div></div><div id="41734151" class="c"><input type="checkbox" id="c-41734151" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#41733683">parent</a><span>|</span><a href="#41733843">prev</a><span>|</span><a href="#41733653">next</a><span>|</span><label class="collapse" for="c-41734151">[-]</label><label class="expand" for="c-41734151">[3 more]</label></div><br/><div class="children"><div class="content">What are you saying is Turing-complete?</div><br/><div id="41735433" class="c"><input type="checkbox" id="c-41735433" checked=""/><div class="controls bullet"><span class="by">baanist</span><span>|</span><a href="#41733683">root</a><span>|</span><a href="#41734151">parent</a><span>|</span><a href="#41733653">next</a><span>|</span><label class="collapse" for="c-41735433">[-]</label><label class="expand" for="c-41735433">[2 more]</label></div><br/><div class="children"><div class="content">Neural networks are Turing complete, i.e. there is a universal neural network that can compute any effectively computable function¹. Incidentally, when this is combined with Rice&#x27;s theorem² it means that safety research is essentially an unsolvable problem because any non-trivial property of a sufficiently complex neural network, e.g. one that can simulate a Turing machine, will have properties which can not be predicted with finite computation.<p>1: <a href="https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;089396599190080F" rel="nofollow">https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;0893965991...</a><p>2: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rice%27s_theorem?useskin=vector" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rice%27s_theorem?useskin=vecto...</a></div><br/><div id="41738150" class="c"><input type="checkbox" id="c-41738150" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#41733683">root</a><span>|</span><a href="#41735433">parent</a><span>|</span><a href="#41733653">next</a><span>|</span><label class="collapse" for="c-41738150">[-]</label><label class="expand" for="c-41738150">[1 more]</label></div><br/><div class="children"><div class="content">super interesting, and i&#x27;d not seen either reference.   thanks very much.</div><br/></div></div></div></div></div></div></div></div><div id="41733653" class="c"><input type="checkbox" id="c-41733653" checked=""/><div class="controls bullet"><span class="by">dsamarin</span><span>|</span><a href="#41733683">prev</a><span>|</span><a href="#41735072">next</a><span>|</span><label class="collapse" for="c-41733653">[-]</label><label class="expand" for="c-41733653">[3 more]</label></div><br/><div class="children"><div class="content">The name of the paper contrasts with the paper that spawned Transformer architecture, which itself is a reference to the song &quot;All You Need Is Love&quot; by the Beatles. <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Attention_Is_All_You_Need" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Attention_Is_All_You_Need</a></div><br/><div id="41734061" class="c"><input type="checkbox" id="c-41734061" checked=""/><div class="controls bullet"><span class="by">vundercind</span><span>|</span><a href="#41733653">parent</a><span>|</span><a href="#41735072">next</a><span>|</span><label class="collapse" for="c-41734061">[-]</label><label class="expand" for="c-41734061">[2 more]</label></div><br/><div class="children"><div class="content">I eagerly await the backlash to suggesting any one thing is all you need, the first shot of which shall surely be titled: “‘All you need’ Considered Harmful”</div><br/><div id="41734553" class="c"><input type="checkbox" id="c-41734553" checked=""/><div class="controls bullet"><span class="by">ants_everywhere</span><span>|</span><a href="#41733653">root</a><span>|</span><a href="#41734061">parent</a><span>|</span><a href="#41735072">next</a><span>|</span><label class="collapse" for="c-41734553">[-]</label><label class="expand" for="c-41734553">[1 more]</label></div><br/><div class="children"><div class="content">Surely the universe is all you need though</div><br/></div></div></div></div></div></div><div id="41735072" class="c"><input type="checkbox" id="c-41735072" checked=""/><div class="controls bullet"><span class="by">limapedro</span><span>|</span><a href="#41733653">prev</a><span>|</span><a href="#41738755">next</a><span>|</span><label class="collapse" for="c-41735072">[-]</label><label class="expand" for="c-41735072">[1 more]</label></div><br/><div class="children"><div class="content">This is such a interesting paper, sadly they don&#x27;t have big models, I&#x27;d like to see a model trained on TinyStories or even C4 since it should be faster than the transformer variant and see how it compares.</div><br/></div></div><div id="41738755" class="c"><input type="checkbox" id="c-41738755" checked=""/><div class="controls bullet"><span class="by">lccerina</span><span>|</span><a href="#41735072">prev</a><span>|</span><a href="#41736378">next</a><span>|</span><label class="collapse" for="c-41738755">[-]</label><label class="expand" for="c-41738755">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Was all along a scheme by Google to sell more tensor processing units that didn&#x27;t run RNNs well?&quot;</div><br/></div></div><div id="41736378" class="c"><input type="checkbox" id="c-41736378" checked=""/><div class="controls bullet"><span class="by">kgbcia</span><span>|</span><a href="#41738755">prev</a><span>|</span><a href="#41733752">next</a><span>|</span><label class="collapse" for="c-41736378">[-]</label><label class="expand" for="c-41736378">[1 more]</label></div><br/><div class="children"><div class="content">Decision trees is all we needed</div><br/></div></div><div id="41733752" class="c"><input type="checkbox" id="c-41733752" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#41736378">prev</a><span>|</span><a href="#41736930">next</a><span>|</span><label class="collapse" for="c-41733752">[-]</label><label class="expand" for="c-41733752">[2 more]</label></div><br/><div class="children"><div class="content">The model in the paper isn&#x27;t a &quot;real&quot; RNN due making it parallelizable, for same the reasons described in <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.08819" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.08819</a> , and hence is theoretically less powerful than a &quot;real&quot; RNN (struggles at some classes of problems that RNNs traditionally excel at). On the other hand, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.04517" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.04517</a> contains a &quot;real&quot; RNN component, which demonstrates a significant improvement on the kind of state-tracking problems that transformers struggle with.</div><br/><div id="41735113" class="c"><input type="checkbox" id="c-41735113" checked=""/><div class="controls bullet"><span class="by">robertsdionne</span><span>|</span><a href="#41733752">parent</a><span>|</span><a href="#41736930">next</a><span>|</span><label class="collapse" for="c-41735113">[-]</label><label class="expand" for="c-41735113">[1 more]</label></div><br/><div class="children"><div class="content">These are real RNNs, they still depend upon the prior hidden state, it’s just that the gating does not. The basic RNN equation can be parallelized with parallel prefix scan algorithms.</div><br/></div></div></div></div><div id="41736930" class="c"><input type="checkbox" id="c-41736930" checked=""/><div class="controls bullet"><span class="by">Smerity</span><span>|</span><a href="#41733752">prev</a><span>|</span><a href="#41737770">next</a><span>|</span><label class="collapse" for="c-41736930">[-]</label><label class="expand" for="c-41736930">[1 more]</label></div><br/><div class="children"><div class="content">Excited to see more people working on RNNs but wish their citations were better.<p>In 2016 my team from Salesforce Research published our work on the Quasi-Recurrent Neural Network[1] (QRNN). The QRNN variants we describe are near identical (minGRU) or highly similar (minLSTM) to the work here.<p>The QRNN was used, many years ago now, in the first version of Baidu&#x27;s speech recognition system (Deep Voice [6]) and as part of Google&#x27;s handwriting recognition system in Gboard[5] (2019).<p>Even if there are expressivity trade-offs when using parallelizable RNNs they&#x27;ve shown historically they can work well and are low resource and incredibly fast. Very few of the possibilities regarding distillation, hardware optimization, etc, have been explored.<p>Even if you need &quot;exact&quot; recall, various works have shown that even a single layer of attention with a parallelizable RNN can yield strong results. Distillation down to such a model is quite promising.<p>Other recent fast RNN variants such as the RWKV, S4, Mamba et al. include citations to QRNN (2016) and SRU (2017) for a richer history + better context.<p>The SRU work has also had additions in recent years (SRU++), doing well in speech recognition and LM tasks where they found similar speed benefits over Transformers.<p>I note this primarily as the more data points, especially when strongly relevant, the better positioned the research is. A number of the &quot;new&quot; findings from this paper have been previously explored - and do certainly show promise! This makes sure we&#x27;re asking new questions with new insights (with all the benefit of additional research from ~8 years ago) versus missing the work from those earlier.<p>[1] QRNN paper: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1611.01576" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1611.01576</a><p>[2] SRU paper: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1709.02755" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1709.02755</a><p>[3]: SRU++ for speech recognition: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2110.05571" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2110.05571</a><p>[4]: SRU++ for language modeling: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2102.12459" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2102.12459</a><p>[5]: <a href="https:&#x2F;&#x2F;research.google&#x2F;blog&#x2F;rnn-based-handwriting-recognition-in-gboard&#x2F;" rel="nofollow">https:&#x2F;&#x2F;research.google&#x2F;blog&#x2F;rnn-based-handwriting-recogniti...</a><p>[6]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1702.07825" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1702.07825</a></div><br/></div></div><div id="41737770" class="c"><input type="checkbox" id="c-41737770" checked=""/><div class="controls bullet"><span class="by">lettergram</span><span>|</span><a href="#41736930">prev</a><span>|</span><a href="#41734131">next</a><span>|</span><label class="collapse" for="c-41737770">[-]</label><label class="expand" for="c-41737770">[1 more]</label></div><br/><div class="children"><div class="content">In 2016 &amp; 2017 my team at Capital One built several &gt;1B parameter models combining LSTMs with a few other tricks.<p>We were able to build generators that could replicate any dataset they were trained on, and would produce unique deviations, but match the statistical underpinnings of the original datasets.<p><a href="https:&#x2F;&#x2F;medium.com&#x2F;capital-one-tech&#x2F;why-you-dont-necessarily-need-data-for-data-science-48d7bf503074" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;capital-one-tech&#x2F;why-you-dont-necessarily...</a><p>We built several text generators for bots that similarly had very good results. The introduction of the transformer improved the speed and reduced the training &#x2F; data requirements, but honestly the accuracy changed minimal.</div><br/></div></div><div id="41734131" class="c"><input type="checkbox" id="c-41734131" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#41737770">prev</a><span>|</span><a href="#41734918">next</a><span>|</span><label class="collapse" for="c-41734131">[-]</label><label class="expand" for="c-41734131">[1 more]</label></div><br/><div class="children"><div class="content">Yes, all machine learning can be interpreted in terms of approximating the partition function.<p>This is obvious when one considers the connections between Transformers, RNNs, Hopfield networks and the Ising model, a model from statistical mechanics which is solved by calculating the partition function.<p>This interpretation provides us with some very powerful tools that are commonplace in math and physics but which are not talked about in CS &amp; ML.<p>I&#x27;m working on a startup <a href="http:&#x2F;&#x2F;traceoid.ai" rel="nofollow">http:&#x2F;&#x2F;traceoid.ai</a> which takes this exact view. Our approach enables faster training and inference, interpretability and also scalable energy-based models, the Holy Grail of machine learning.<p>Join the discord <a href="https:&#x2F;&#x2F;discord.com&#x2F;invite&#x2F;mr9TAhpyBW" rel="nofollow">https:&#x2F;&#x2F;discord.com&#x2F;invite&#x2F;mr9TAhpyBW</a> or follow me on twitter <a href="https:&#x2F;&#x2F;twitter.com&#x2F;adamnemecek1" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;adamnemecek1</a></div><br/></div></div><div id="41734918" class="c"><input type="checkbox" id="c-41734918" checked=""/><div class="controls bullet"><span class="by">fhdsgbbcaA</span><span>|</span><a href="#41734131">prev</a><span>|</span><a href="#41733170">next</a><span>|</span><label class="collapse" for="c-41734918">[-]</label><label class="expand" for="c-41734918">[3 more]</label></div><br/><div class="children"><div class="content">We really need a [preprint] flag for unreviewed papers.</div><br/><div id="41735960" class="c"><input type="checkbox" id="c-41735960" checked=""/><div class="controls bullet"><span class="by">lgessler</span><span>|</span><a href="#41734918">parent</a><span>|</span><a href="#41733170">next</a><span>|</span><label class="collapse" for="c-41735960">[-]</label><label class="expand" for="c-41735960">[2 more]</label></div><br/><div class="children"><div class="content">IMHO reviews are almost indistinguishable from noise at the AI conferences I&#x27;m familiar with these days anyway, so I don&#x27;t see much of a value add.</div><br/><div id="41737026" class="c"><input type="checkbox" id="c-41737026" checked=""/><div class="controls bullet"><span class="by">fhdsgbbcaA</span><span>|</span><a href="#41734918">root</a><span>|</span><a href="#41735960">parent</a><span>|</span><a href="#41733170">next</a><span>|</span><label class="collapse" for="c-41737026">[-]</label><label class="expand" for="c-41737026">[1 more]</label></div><br/><div class="children"><div class="content">Sad state of affairs, people are incentivized to get more papers and citations at all costs, and quality be damned.<p>An AI Winter is not a great an idea, but an AI Autumn may be beneficial.<p>Just have no major AI conferences for ‘25, perhaps only accept really high tier literature reviews.</div><br/></div></div></div></div></div></div><div id="41733170" class="c"><input type="checkbox" id="c-41733170" checked=""/><div class="controls bullet"><span class="by">hiddencost</span><span>|</span><a href="#41734918">prev</a><span>|</span><a href="#41733156">next</a><span>|</span><label class="collapse" for="c-41733170">[-]</label><label class="expand" for="c-41733170">[8 more]</label></div><br/><div class="children"><div class="content">Note Yoshua Bengio in the author list. This shouldn&#x27;t be taken lightly.</div><br/><div id="41736732" class="c"><input type="checkbox" id="c-41736732" checked=""/><div class="controls bullet"><span class="by">_giorgio_</span><span>|</span><a href="#41733170">parent</a><span>|</span><a href="#41733365">next</a><span>|</span><label class="collapse" for="c-41736732">[-]</label><label class="expand" for="c-41736732">[1 more]</label></div><br/><div class="children"><div class="content">Who cares. Look at Jeffrey Hinton right now. Do you trust him? :-D</div><br/></div></div><div id="41733365" class="c"><input type="checkbox" id="c-41733365" checked=""/><div class="controls bullet"><span class="by">auggierose</span><span>|</span><a href="#41733170">parent</a><span>|</span><a href="#41736732">prev</a><span>|</span><a href="#41733156">next</a><span>|</span><label class="collapse" for="c-41733365">[-]</label><label class="expand" for="c-41733365">[6 more]</label></div><br/><div class="children"><div class="content">And this is where science breaks down.</div><br/><div id="41733624" class="c"><input type="checkbox" id="c-41733624" checked=""/><div class="controls bullet"><span class="by">hotspot_one</span><span>|</span><a href="#41733170">root</a><span>|</span><a href="#41733365">parent</a><span>|</span><a href="#41733156">next</a><span>|</span><label class="collapse" for="c-41733624">[-]</label><label class="expand" for="c-41733624">[5 more]</label></div><br/><div class="children"><div class="content">Not really, because<p>1) Yoshua&#x27;s reputation would take a hit if this paper were bullshit, so he has extrinsic motivation to make it good
2) Yoshua has enough experience in the field to know what is going on in the field, you don&#x27;t have to ask if he forgot about a certain architecture or the work of a certain research group which would contradict his findings-- if such work exists and is credible, it is very likely to be discussed in the paper.
3) This test answers something a leader in the field thinks is important enough for them to work on, else he wouldn&#x27;t be involved.<p>Also note, the poster said the paper shouldn&#x27;t be taken lightly. That doesn&#x27;t mean we need to take it blindly. It only means we cannot dismiss it out of hand, if we have a different view we would need substantive arguments to defend our view.<p>I&#x27;ve overturned the field leader several times in science, but that&#x27;s only because I acknowledged what they got right and that they were indeed the person who got it right.</div><br/><div id="41733966" class="c"><input type="checkbox" id="c-41733966" checked=""/><div class="controls bullet"><span class="by">auggierose</span><span>|</span><a href="#41733170">root</a><span>|</span><a href="#41733624">parent</a><span>|</span><a href="#41733771">next</a><span>|</span><label class="collapse" for="c-41733966">[-]</label><label class="expand" for="c-41733966">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It only means we cannot dismiss it out of hand, if we have a different view we would need substantive arguments to defend our view.<p>You will need to do that anyway, no matter if Yoshua is on the paper, or not. I understand that people have limited bandwidth, and so they need shortcuts, and they need to justify these shortcuts to themselves somehow (of course the justifications are nonsense). Maybe AI will help here.</div><br/></div></div><div id="41733771" class="c"><input type="checkbox" id="c-41733771" checked=""/><div class="controls bullet"><span class="by">DAGdug</span><span>|</span><a href="#41733170">root</a><span>|</span><a href="#41733624">parent</a><span>|</span><a href="#41733966">prev</a><span>|</span><a href="#41733156">next</a><span>|</span><label class="collapse" for="c-41733771">[-]</label><label class="expand" for="c-41733771">[3 more]</label></div><br/><div class="children"><div class="content">“ I&#x27;ve overturned the field leader several times in science”
Either that makes you a field leader yourself, or you did it for trivial things, or you’re BSing. Which one is it?</div><br/><div id="41734388" class="c"><input type="checkbox" id="c-41734388" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#41733170">root</a><span>|</span><a href="#41733771">parent</a><span>|</span><a href="#41733156">next</a><span>|</span><label class="collapse" for="c-41734388">[-]</label><label class="expand" for="c-41734388">[2 more]</label></div><br/><div class="children"><div class="content">there&#x27;s a big space between leader and trivial. it&#x27;s entirely possible to point out the top leader in your field is wrong on ten things over a career, without becoming the top leader yourself.</div><br/><div id="41736509" class="c"><input type="checkbox" id="c-41736509" checked=""/><div class="controls bullet"><span class="by">DAGdug</span><span>|</span><a href="#41733170">root</a><span>|</span><a href="#41734388">parent</a><span>|</span><a href="#41733156">next</a><span>|</span><label class="collapse" for="c-41736509">[-]</label><label class="expand" for="c-41736509">[1 more]</label></div><br/><div class="children"><div class="content">On speculative things or trivial things, sure! On substantive matters (recall: the choice of words is “overturned”), in empirical realms or theory (physics, CS) or math, it’s rather doubtful. Anonymous, self-declared geniuses aren’t to be taken at face value.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41733156" class="c"><input type="checkbox" id="c-41733156" checked=""/><div class="controls bullet"><span class="by">hydrolox</span><span>|</span><a href="#41733170">prev</a><span>|</span><a href="#41734095">next</a><span>|</span><label class="collapse" for="c-41733156">[-]</label><label class="expand" for="c-41733156">[4 more]</label></div><br/><div class="children"><div class="content">Betteridge&#x27;s law of headlines?</div><br/><div id="41733204" class="c"><input type="checkbox" id="c-41733204" checked=""/><div class="controls bullet"><span class="by">woah</span><span>|</span><a href="#41733156">parent</a><span>|</span><a href="#41734095">next</a><span>|</span><label class="collapse" for="c-41733204">[-]</label><label class="expand" for="c-41733204">[3 more]</label></div><br/><div class="children"><div class="content">For paper titles, the law is that the answer is always &quot;yes&quot;</div><br/><div id="41733523" class="c"><input type="checkbox" id="c-41733523" checked=""/><div class="controls bullet"><span class="by">bunderbunder</span><span>|</span><a href="#41733156">root</a><span>|</span><a href="#41733204">parent</a><span>|</span><a href="#41733561">next</a><span>|</span><label class="collapse" for="c-41733523">[-]</label><label class="expand" for="c-41733523">[1 more]</label></div><br/><div class="children"><div class="content">Not always, I think?<p>Opinions probably differ, for example, on John Backus&#x27;s paper &quot;Can programming be liberated from the Von Neumann style?&quot; Many fans of functional programming would say the answer is yes, but Backus himself expressed less enthusiasm in interviews later in his life.<p>I think the important point, though, is that academic papers and newspaper articles are <i>not the same</i>, and titles in the form of questions function differently in the two domains. Journalists tend to use titles like these to dissemble and sensationalize. When academics use these kinds of titles for peer-reviewed articles, it&#x27;s because they really are asking an honest question. Backus was doing it in his paper. The authors of this paper are doing the same. They end the paper by re-iterating the question before launching into a discussion of the limitations that prevent them from reaching any firm conclusions on the answer to this question.</div><br/></div></div><div id="41733561" class="c"><input type="checkbox" id="c-41733561" checked=""/><div class="controls bullet"><span class="by">nephanth</span><span>|</span><a href="#41733156">root</a><span>|</span><a href="#41733204">parent</a><span>|</span><a href="#41733523">prev</a><span>|</span><a href="#41734095">next</a><span>|</span><label class="collapse" for="c-41733561">[-]</label><label class="expand" for="c-41733561">[1 more]</label></div><br/><div class="children"><div class="content">More like &quot;we aren&#x27;t sure, but we have good reasons not to exclude the possibility&quot;</div><br/></div></div></div></div></div></div><div id="41734095" class="c"><input type="checkbox" id="c-41734095" checked=""/><div class="controls bullet"><span class="by">PunchTornado</span><span>|</span><a href="#41733156">prev</a><span>|</span><label class="collapse" for="c-41734095">[-]</label><label class="expand" for="c-41734095">[2 more]</label></div><br/><div class="children"><div class="content">To me this is further evidence that these LLMs learn only to speak English, but there is no reasoning at all in them. If you simplify a lot and obtain the same results and we know how complex the brain is.</div><br/><div id="41735375" class="c"><input type="checkbox" id="c-41735375" checked=""/><div class="controls bullet"><span class="by">quantadev</span><span>|</span><a href="#41734095">parent</a><span>|</span><label class="collapse" for="c-41735375">[-]</label><label class="expand" for="c-41735375">[1 more]</label></div><br/><div class="children"><div class="content">Every LLM expert on the planet agrees LLMs are doing &quot;reasoning&quot;. No one says they have feelings or qualia, but we all know there&#x27;s definitely genuinely artificial reasoning happening.<p>What LLMs have shown both Neuroscience and Computer Science is that reasoning is a mechanical process (or can be simulated by mechanical processes) and is not purely associated only with consciousness.</div><br/></div></div></div></div></div></div></div></div></div></body></html>