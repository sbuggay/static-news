<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1733475653505" as="style"/><link rel="stylesheet" href="styles.css?v=1733475653505"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.apnic.net/2024/05/17/a-transport-protocols-view-of-starlink/">A transport protocol&#x27;s view of Starlink</a>Â <span class="domain">(<a href="https://blog.apnic.net">blog.apnic.net</a>)</span></div><div class="subtext"><span>rolph</span> | <span>23 comments</span></div><br/><div><div id="42337588" class="c"><input type="checkbox" id="c-42337588" checked=""/><div class="controls bullet"><span class="by">teleforce</span><span>|</span><a href="#42333282">next</a><span>|</span><label class="collapse" for="c-42337588">[-]</label><label class="expand" for="c-42337588">[3 more]</label></div><br/><div class="children"><div class="content">Such a quality written article on satellite networking technology, kudos to APNIC.<p>This makes me wonder perhaps TCP is not really suitable or optimized for satellite network.<p>John Ousterhout (of TCL&#x2F;TK fame) has recently proposed a new Homa transport protocol as an alternative for TCP in data center [1]. Perhaps a new more suitable transport protocol for satellite or NTN is also needed. That&#x27;s the beauty of the Internet, the transport protocol is expendable but not network protocol or IP. The fact that IPv6 still a fringe rather than becoming mainstream although it&#x27;s arguably better than IPv4.<p>[1] Homa, a transport protocol to replace TCP for low-latency RPC in data centers:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28204808">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28204808</a></div><br/><div id="42337780" class="c"><input type="checkbox" id="c-42337780" checked=""/><div class="controls bullet"><span class="by">supriyo-biswas</span><span>|</span><a href="#42337588">parent</a><span>|</span><a href="#42333282">next</a><span>|</span><label class="collapse" for="c-42337780">[-]</label><label class="expand" for="c-42337780">[2 more]</label></div><br/><div class="children"><div class="content">Throwing out TCP for a message-oriented layer as Homa does is not really required for addressing this need.<p>Perhaps what would be more useful in this context would be for operating system vendors to perform a HTTP request to a globally distributed endpoint similar to captive portal detection, and then use a more aggressive congestion control algorithm in the case of networks with good throughputs but with high latency.</div><br/><div id="42337810" class="c"><input type="checkbox" id="c-42337810" checked=""/><div class="controls bullet"><span class="by">teleforce</span><span>|</span><a href="#42337588">root</a><span>|</span><a href="#42337780">parent</a><span>|</span><a href="#42333282">next</a><span>|</span><label class="collapse" for="c-42337810">[-]</label><label class="expand" for="c-42337810">[1 more]</label></div><br/><div class="children"><div class="content">I am not saying similar to Homa protocol since it&#x27;s catering for data center traffic but a new transport protocol alternative to cater for variable latency or jitter needs.</div><br/></div></div></div></div></div></div><div id="42333282" class="c"><input type="checkbox" id="c-42333282" checked=""/><div class="controls bullet"><span class="by">NelsonMinar</span><span>|</span><a href="#42337588">prev</a><span>|</span><a href="#42334273">next</a><span>|</span><label class="collapse" for="c-42333282">[-]</label><label class="expand" for="c-42333282">[5 more]</label></div><br/><div class="children"><div class="content">I noticed a huge improvement just switching to stock BBR to my Starlink as well. During a particularly congested time I was bouncing between 5 to 12 Mbps via Starlink. With BBR enabled I got a steady 12. The main problem is that you need BBR on the server for this to work, as a client using Starlink I don&#x27;t have any control over what all the servers I connect to are doing. (Other than my one server I was testing with).<p>I like Huston&#x27;s idea of a Starlink-tuned BBR, I wonder if it&#x27;s a traffic shaping that SpaceX could apply themselves in their ground station datacenters? That&#x27;d involve messing with the TCP stream though, maybe a bad idea.<p>The fact that Starlink has this 15 second switching built in is pretty weird, but you can definitely see it in every continuous latency measure. Even weirder it seems to be globally synchronized: all the hundreds of thousands of dishes are switching to new satellites the same millisecond, globally. Having a customized BBR aware of that 15 second cycle is an interesting idea.</div><br/><div id="42333708" class="c"><input type="checkbox" id="c-42333708" checked=""/><div class="controls bullet"><span class="by">btilly</span><span>|</span><a href="#42333282">parent</a><span>|</span><a href="#42334273">next</a><span>|</span><label class="collapse" for="c-42333708">[-]</label><label class="expand" for="c-42333708">[4 more]</label></div><br/><div class="children"><div class="content">If you use a VPN, wouldn&#x27;t it suffice to just make your VPN connection use BBR?<p>Ditto if you use an https proxy of some kind.</div><br/><div id="42333743" class="c"><input type="checkbox" id="c-42333743" checked=""/><div class="controls bullet"><span class="by">jofla_net</span><span>|</span><a href="#42333282">root</a><span>|</span><a href="#42333708">parent</a><span>|</span><a href="#42333779">next</a><span>|</span><label class="collapse" for="c-42333743">[-]</label><label class="expand" for="c-42333743">[1 more]</label></div><br/><div class="children"><div class="content">I would guess that that would be beneficial, but again only if youre using a TCP vpn, which is suboptimal for other reasons. I think it was called meltdown. 
If that is all you have access to though, im sure it would help.</div><br/></div></div><div id="42333779" class="c"><input type="checkbox" id="c-42333779" checked=""/><div class="controls bullet"><span class="by">Hikikomori</span><span>|</span><a href="#42333282">root</a><span>|</span><a href="#42333708">parent</a><span>|</span><a href="#42333743">prev</a><span>|</span><a href="#42334248">next</a><span>|</span><label class="collapse" for="c-42333779">[-]</label><label class="expand" for="c-42333779">[1 more]</label></div><br/><div class="children"><div class="content">Proxy yes, vpn no. Tcp over tcp vpn is bad, no tcp vpn would make no difference to no vpn.</div><br/></div></div><div id="42334248" class="c"><input type="checkbox" id="c-42334248" checked=""/><div class="controls bullet"><span class="by">Alex-Programs</span><span>|</span><a href="#42333282">root</a><span>|</span><a href="#42333708">parent</a><span>|</span><a href="#42333779">prev</a><span>|</span><a href="#42334273">next</a><span>|</span><label class="collapse" for="c-42334248">[-]</label><label class="expand" for="c-42334248">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;apernet&#x2F;hysteria">https:&#x2F;&#x2F;github.com&#x2F;apernet&#x2F;hysteria</a> has the option to use <a href="https:&#x2F;&#x2F;github.com&#x2F;apernet&#x2F;tcp-brutal">https:&#x2F;&#x2F;github.com&#x2F;apernet&#x2F;tcp-brutal</a>, a deliberately unfair&#x2F;selfish congestion control algorithm.<p>It&#x27;s designed to mitigate certain methods of blocking-via-throttling.<p>I looked into it for a report I wrote a while back, and I was surprised to find that nobody has made something purpose-built for greedy TCP congestion handling in order to improve performance at the expense of others. If there is such a thing, I couldn&#x27;t find it. Perhaps I&#x27;m a little too cynical in my expectations!<p>Maybe TCP-over-TCP is so bad that it&#x27;s not worth it?</div><br/></div></div></div></div></div></div><div id="42334273" class="c"><input type="checkbox" id="c-42334273" checked=""/><div class="controls bullet"><span class="by">kevincox</span><span>|</span><a href="#42333282">prev</a><span>|</span><a href="#42333466">next</a><span>|</span><label class="collapse" for="c-42334273">[-]</label><label class="expand" for="c-42334273">[7 more]</label></div><br/><div class="children"><div class="content">&gt; the endpoints need to use large buffers to hold a copy of all the unacknowledged data, as is required by the TCP protocol.<p>It makes me wonder if anyone has tried to break down the layers to optimize this. In the fairly common case of serving a file off of long-term storage you can jut fetch the old data if needed (likely from the page cache anyways, but still better than duplicating it) and some encryption algorithms are seekable so you can redo the encryption as well.<p>Right now the kernel doesn&#x27;t really have a choice but to buffer all unacknowledged data as the UNIX socket API has no provision for requesting old data from the writer. But a smarter API could put the application in charge of making that data available as required and avoid the need for extra copies in common cases.<p>I know that Netflix did lots of work with the FreeBSD kernel for file to socket and eventually adding in-kernel TLS to remove user space from the equation. But I don&#x27;t know if they went as far to remove the socket buffers.</div><br/><div id="42334851" class="c"><input type="checkbox" id="c-42334851" checked=""/><div class="controls bullet"><span class="by">moandcompany</span><span>|</span><a href="#42334273">parent</a><span>|</span><a href="#42334474">next</a><span>|</span><label class="collapse" for="c-42334851">[-]</label><label class="expand" for="c-42334851">[4 more]</label></div><br/><div class="children"><div class="content">This is an &quot;old&quot; problem that has historically been addressed through things like &quot;Performance Enhancing Proxies (PEPs)&quot; that are defined in RFC 3135 and RFC 3449. (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Performance-enhancing_proxy" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Performance-enhancing_proxy</a>)<p>In internet-style communications, such as routing IP traffic over satellite links to low-earth-orbit or GEO, with much longer round-trip times, link latency is substantially higher than most terrestrial wired or wireless applications and acknowledgements required as part of TCP take much longer to facilitate. PEPs as an example augment the connection allowing end-user&#x2F;client devices in the network with inline-PEPs to retain their normal network settings and perform the task of running or starting sessions with higher TCP-window sizes, as a method for improving overall throughput.<p>The utility of a PEP, or PEP-acting device goes up when you imagine multiple devices, or a network of devices attached to a satellite communications terminal for WAN&#x2F;backhaul connections as the link&#x27;s performance can be managed at one point versus on all downstream client devices.</div><br/><div id="42335866" class="c"><input type="checkbox" id="c-42335866" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#42334273">root</a><span>|</span><a href="#42334851">parent</a><span>|</span><a href="#42334474">next</a><span>|</span><label class="collapse" for="c-42335866">[-]</label><label class="expand" for="c-42335866">[3 more]</label></div><br/><div class="children"><div class="content">TIL that that&#x27;s what they&#x27;re called. Thank you!<p>Do you know if they became obsolete due to modern TCP stacks handling LFNs better or for some other reason?<p>I could imagine them being quite useful for high-loss, high-latency paths (i.e. in addition to LFNs in conjunction with poorly tuned TCP implementation), but most wireless protocols I know (802.11, GPRS and beyond etc.) just implement an ARQ process that masks the loss at the packet or lower layer.<p>So maybe between that and LFN-aware TCPs, there wasn&#x27;t much left for them to improve to justify the additional complexity?</div><br/><div id="42335940" class="c"><input type="checkbox" id="c-42335940" checked=""/><div class="controls bullet"><span class="by">moandcompany</span><span>|</span><a href="#42334273">root</a><span>|</span><a href="#42335866">parent</a><span>|</span><a href="#42334474">next</a><span>|</span><label class="collapse" for="c-42335940">[-]</label><label class="expand" for="c-42335940">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been a long time since I&#x27;ve paid attention to that world, but AFAIK, PEPs are still used and essential equipment for internet-style communication (i.e. TCP over IP) via GEO satellites.<p>It looks like in this 2022 blog post evaluating latency and throughput over Starlink, they concluded that PEPs were not being used in the Starlink network (and probably unnecessary) due to the lower latency characteristics from use of LEO satellites. They also mention that PEPs are (still) commonly employed by GEO-satcom based operators.<p><a href="https:&#x2F;&#x2F;blog.apnic.net&#x2F;2022&#x2F;11&#x2F;28&#x2F;fact-checking-starlinks-performance-figures&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.apnic.net&#x2F;2022&#x2F;11&#x2F;28&#x2F;fact-checking-starlinks-pe...</a></div><br/><div id="42336017" class="c"><input type="checkbox" id="c-42336017" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#42334273">root</a><span>|</span><a href="#42335940">parent</a><span>|</span><a href="#42334474">next</a><span>|</span><label class="collapse" for="c-42336017">[-]</label><label class="expand" for="c-42336017">[1 more]</label></div><br/><div class="children"><div class="content">&gt; they concluded that PEPs were not being used in the Starlink network<p>That makes sense, given that they&#x27;re probably most useful for high-latency networks. But what I find quite surprising is that Starlink does nothing about the 1-2% packet loss, as described in TFA; I&#x27;d really have expected them to fix that using an ARQ at a lower layer.<p>Then again, maybe that&#x27;s a blessing â indiscriminate ARQs like that would be terrible for time critical things like A&#x2F;V, which can usually tolerate packet loss much better than ARQ-induced jitter.<p>Thinking about it, that actually strengthens the case for PEPs: They could improve TCP performance (and maybe things like QUIC?), while leaving non-stream oriented things (like non-QUIC UDP) alone.<p>Maybe Starlink just expects BBR to eventually become the dominant TCP congestion control algorithm and the problem to solve itself that way?</div><br/></div></div></div></div></div></div></div></div><div id="42334474" class="c"><input type="checkbox" id="c-42334474" checked=""/><div class="controls bullet"><span class="by">cyberax</span><span>|</span><a href="#42334273">parent</a><span>|</span><a href="#42334851">prev</a><span>|</span><a href="#42334847">next</a><span>|</span><label class="collapse" for="c-42334474">[-]</label><label class="expand" for="c-42334474">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It makes me wonder if anyone has tried to break down the layers to optimize this.<p>Yep. There was a bunch of proxy servers that optimized HTTP for satellite service. I used Globax back in the day to speed up one-way satellite service: <a href="https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20040602203838&#x2F;http:&#x2F;&#x2F;globax.info&#x2F;shorten.html" rel="nofollow">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20040602203838&#x2F;http:&#x2F;&#x2F;globax.inf...</a><p>Back then traffic was around 10 cents per megabyte in my city, so satellite service was a good way to shave off these costs.</div><br/></div></div><div id="42334847" class="c"><input type="checkbox" id="c-42334847" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#42334273">parent</a><span>|</span><a href="#42334474">prev</a><span>|</span><a href="#42333466">next</a><span>|</span><label class="collapse" for="c-42334847">[-]</label><label class="expand" for="c-42334847">[1 more]</label></div><br/><div class="children"><div class="content">There aren&#x27;t necessarily extra copies even when just using TCP, thanks to sendfile(2) and similar mechanisms.<p>Buffer size isn&#x27;t that much of an issue either, given the relatively low latencies involved and that you can indicate which parts exactly are missing pretty accurately these days with selective TCP acknowledgements, so you&#x27;ll need at most a few round trips to identify these to the sender and eventually receive them.<p>Practically, you&#x27;ll probably not see much loss anyway, for better or worse: TCP historically interpreted packet loss as congestion, instead of actual non-congestion-induced loss on the physical layer. This is why most lower-layer protocols with higher error rates than Ethernet usually implement some sort of lower-layer ARQ to present themselves as &quot;Ethernet-like&quot; to upper layers in terms of error&#x2F;loss rate, and this in turn has made loss-tolerant TCP (such as BBR, as described in the article) less of a research priority, I believe.</div><br/></div></div></div></div><div id="42333466" class="c"><input type="checkbox" id="c-42333466" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#42334273">prev</a><span>|</span><a href="#42335532">next</a><span>|</span><label class="collapse" for="c-42333466">[-]</label><label class="expand" for="c-42333466">[4 more]</label></div><br/><div class="children"><div class="content">Fascinating that the throughput is about 250mbs. Presumably that&#x27;s over the area served by one satellite? I wonder how much cache they put in each one... I vaguely remember a stat that 90% of requests (in data terms) are served from a TB of cache on the consumer internet, perhaps having the satellites gossip for cache hits would work to preserve uplink bandwidth as well. Maybe downlink bandwidth is the thing for this network though and caches just won&#x27;t work.</div><br/><div id="42333679" class="c"><input type="checkbox" id="c-42333679" checked=""/><div class="controls bullet"><span class="by">echoangle</span><span>|</span><a href="#42333466">parent</a><span>|</span><a href="#42334250">next</a><span>|</span><label class="collapse" for="c-42333679">[-]</label><label class="expand" for="c-42333679">[2 more]</label></div><br/><div class="children"><div class="content">I would be surprised if there is a lot or even any cache on the satellites itself. Fast large storage that&#x27;s radiation hardened would be extremely expensive, and they have a lot of satellites. The satellites are low enough that general radiation isn&#x27;t that bad, but every pass through the South Atlantic Anomaly would risk damage if regular flash storage is used.</div><br/><div id="42334277" class="c"><input type="checkbox" id="c-42334277" checked=""/><div class="controls bullet"><span class="by">Alex-Programs</span><span>|</span><a href="#42333466">root</a><span>|</span><a href="#42333679">parent</a><span>|</span><a href="#42334250">next</a><span>|</span><label class="collapse" for="c-42334277">[-]</label><label class="expand" for="c-42334277">[1 more]</label></div><br/><div class="children"><div class="content">Beyond that, how would the cache actually work?<p>Everything is HTTPS nowadays; you can&#x27;t just MITM and stick a caching proxy in front. You could put DNS on the sat I suppose, but other than that you&#x27;d need to have a full Netflix&#x2F;Cloudflare node, and the sats are far too small and numerous for that.</div><br/></div></div></div></div><div id="42334250" class="c"><input type="checkbox" id="c-42334250" checked=""/><div class="controls bullet"><span class="by">vardump</span><span>|</span><a href="#42333466">parent</a><span>|</span><a href="#42333679">prev</a><span>|</span><a href="#42335532">next</a><span>|</span><label class="collapse" for="c-42334250">[-]</label><label class="expand" for="c-42334250">[1 more]</label></div><br/><div class="children"><div class="content">I think a single Starlink v1 satellite has a maximum bandwidth of about 20 Gbps. Newer versions might have a lot more.</div><br/></div></div></div></div><div id="42335532" class="c"><input type="checkbox" id="c-42335532" checked=""/><div class="controls bullet"><span class="by">trebligdivad</span><span>|</span><a href="#42333466">prev</a><span>|</span><a href="#42336205">next</a><span>|</span><label class="collapse" for="c-42335532">[-]</label><label class="expand" for="c-42335532">[2 more]</label></div><br/><div class="children"><div class="content">How different is this behaviour to using a mobile phone in a car or train?  Doesn&#x27;t that also get you odd changes in latency and notice the handoffs between cells?</div><br/><div id="42335806" class="c"><input type="checkbox" id="c-42335806" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#42335532">parent</a><span>|</span><a href="#42336205">next</a><span>|</span><label class="collapse" for="c-42335806">[-]</label><label class="expand" for="c-42335806">[1 more]</label></div><br/><div class="children"><div class="content">The distances involved are orders of magnitude smaller, so you don&#x27;t get these effects nearly as much.</div><br/></div></div></div></div></div></div></div></div></div></body></html>