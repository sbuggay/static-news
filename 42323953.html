<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1733389256624" as="style"/><link rel="stylesheet" href="styles.css?v=1733389256624"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://smcleod.net/2024/12/bringing-k/v-context-quantisation-to-ollama/">Bringing K/V context quantisation to Ollama</a> <span class="domain">(<a href="https://smcleod.net">smcleod.net</a>)</span></div><div class="subtext"><span>mchiang</span> | <span>22 comments</span></div><br/><div><div id="42324030" class="c"><input type="checkbox" id="c-42324030" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#42324163">next</a><span>|</span><label class="collapse" for="c-42324030">[-]</label><label class="expand" for="c-42324030">[1 more]</label></div><br/><div class="children"><div class="content">Shout out to everyone from Ollama and the wider community that helped with the reviews, feedback and assistance along the way. It&#x27;s great to contribute to such a fantastic project.</div><br/></div></div><div id="42324163" class="c"><input type="checkbox" id="c-42324163" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#42324030">prev</a><span>|</span><a href="#42324392">next</a><span>|</span><label class="collapse" for="c-42324163">[-]</label><label class="expand" for="c-42324163">[12 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the best way to use Ollama with a GUI, just OpenWebUI? Any options as well for mobile platforms like Android (or, I don&#x27;t even know if we can run LLMs on the phone in the first place).</div><br/><div id="42326237" class="c"><input type="checkbox" id="c-42326237" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#42324163">parent</a><span>|</span><a href="#42324391">next</a><span>|</span><label class="collapse" for="c-42326237">[-]</label><label class="expand" for="c-42326237">[1 more]</label></div><br/><div class="children"><div class="content">Aa a Windows user, who just wanted something bare bones for playing, I founf this[1] small project useful. It does support multi-modal models which is nice.<p>[1]: <a href="https:&#x2F;&#x2F;github.com&#x2F;jakobhoeg&#x2F;nextjs-ollama-llm-ui">https:&#x2F;&#x2F;github.com&#x2F;jakobhoeg&#x2F;nextjs-ollama-llm-ui</a></div><br/></div></div><div id="42324391" class="c"><input type="checkbox" id="c-42324391" checked=""/><div class="controls bullet"><span class="by">qudat</span><span>|</span><a href="#42324163">parent</a><span>|</span><a href="#42326237">prev</a><span>|</span><a href="#42324186">next</a><span>|</span><label class="collapse" for="c-42324391">[-]</label><label class="expand" for="c-42324391">[2 more]</label></div><br/><div class="children"><div class="content">For hosting a web gui for ollama I use <a href="https:&#x2F;&#x2F;tuns.sh" rel="nofollow">https:&#x2F;&#x2F;tuns.sh</a><p>It really convenient because it&#x27;s just an SSH tunnel and then you get automatic TLS and it protects your home IP.<p>With that you can access it from your mobile phone, just gotta require a password to access it.</div><br/><div id="42324419" class="c"><input type="checkbox" id="c-42324419" checked=""/><div class="controls bullet"><span class="by">satvikpendem</span><span>|</span><a href="#42324163">root</a><span>|</span><a href="#42324391">parent</a><span>|</span><a href="#42324186">next</a><span>|</span><label class="collapse" for="c-42324419">[-]</label><label class="expand" for="c-42324419">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m running OpenWebUI via Docker via OrbStack, it also automatically provides TLS and works pretty well.</div><br/></div></div></div></div><div id="42324186" class="c"><input type="checkbox" id="c-42324186" checked=""/><div class="controls bullet"><span class="by">sadeshmukh</span><span>|</span><a href="#42324163">parent</a><span>|</span><a href="#42324391">prev</a><span>|</span><a href="#42325371">next</a><span>|</span><label class="collapse" for="c-42324186">[-]</label><label class="expand" for="c-42324186">[1 more]</label></div><br/><div class="children"><div class="content">A lot of the UIs, including OpenWebUI have the feature to expose over LAN with users - that&#x27;s what I did to use my GPU while still being on my phone. Not entirely sure about native UIs though.<p>Also, I normally use Groq&#x27;s (with a q) API since it&#x27;s really cheap with no upfront billing info required - it&#x27;s a whole order of magnitude cheaper iirc than OpenAI&#x2F;Claude. They literally have a &#x2F;openai endpoint if you need compatibility.<p>You can look in the direction of Google&#x27;s Gemma if you need a lightweight open weights LLM - there was something there that I forgot.</div><br/></div></div><div id="42325371" class="c"><input type="checkbox" id="c-42325371" checked=""/><div class="controls bullet"><span class="by">huijzer</span><span>|</span><a href="#42324163">parent</a><span>|</span><a href="#42324186">prev</a><span>|</span><a href="#42325499">next</a><span>|</span><label class="collapse" for="c-42325371">[-]</label><label class="expand" for="c-42325371">[1 more]</label></div><br/><div class="children"><div class="content">I have Open WebUI on a Hetzner instance connected to Deep Infra. Works on mobile by turning the web page into an app. I find the web framework that WebUI uses quite bloated&#x2F;slow, but apart from that it does work reliably. Price at Deep Infra is typically about $0.04 per month even when actively asking lots of questions during programming.</div><br/></div></div><div id="42325499" class="c"><input type="checkbox" id="c-42325499" checked=""/><div class="controls bullet"><span class="by">paradite</span><span>|</span><a href="#42324163">parent</a><span>|</span><a href="#42325371">prev</a><span>|</span><a href="#42324210">next</a><span>|</span><label class="collapse" for="c-42325499">[-]</label><label class="expand" for="c-42325499">[3 more]</label></div><br/><div class="children"><div class="content">I built a custom GUI for coding tasks specifically, with built-in code context management and workspaces:<p><a href="https:&#x2F;&#x2F;prompt.16x.engineer&#x2F;" rel="nofollow">https:&#x2F;&#x2F;prompt.16x.engineer&#x2F;</a><p>Should work well if you have 64G vRAM to run SOTA models locally.</div><br/><div id="42325923" class="c"><input type="checkbox" id="c-42325923" checked=""/><div class="controls bullet"><span class="by">throwaway314155</span><span>|</span><a href="#42324163">root</a><span>|</span><a href="#42325499">parent</a><span>|</span><a href="#42325917">next</a><span>|</span><label class="collapse" for="c-42325923">[-]</label><label class="expand" for="c-42325923">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Should work well if you have 64G vRAM to run SOTA models locally.<p>Does anyone have this?<p>edit: Ah, it&#x27;s a Mac app.</div><br/></div></div></div></div><div id="42324210" class="c"><input type="checkbox" id="c-42324210" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#42324163">parent</a><span>|</span><a href="#42325499">prev</a><span>|</span><a href="#42325523">next</a><span>|</span><label class="collapse" for="c-42324210">[-]</label><label class="expand" for="c-42324210">[1 more]</label></div><br/><div class="children"><div class="content">I personally use a mix of Open WebUI, Big AGI, BoltAI, AnythingLLM on the desktop. The mobile space is where things are really lacking at the moment, really I just end up browsing to Open WebUI but that&#x27;s not ideal. I&#x27;d love a iOS native client that&#x27;s well integrated into Siri, Shortcuts, Sharing etc...</div><br/></div></div><div id="42325523" class="c"><input type="checkbox" id="c-42325523" checked=""/><div class="controls bullet"><span class="by">rkwz</span><span>|</span><a href="#42324163">parent</a><span>|</span><a href="#42324210">prev</a><span>|</span><a href="#42325328">next</a><span>|</span><label class="collapse" for="c-42325523">[-]</label><label class="expand" for="c-42325523">[1 more]</label></div><br/><div class="children"><div class="content">If you’re using a Mac, I’ve built a lightweight native app - <a href="https:&#x2F;&#x2F;github.com&#x2F;sheshbabu&#x2F;Chital">https:&#x2F;&#x2F;github.com&#x2F;sheshbabu&#x2F;Chital</a></div><br/></div></div></div></div><div id="42324808" class="c"><input type="checkbox" id="c-42324808" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#42324392">prev</a><span>|</span><label class="collapse" for="c-42324808">[-]</label><label class="expand" for="c-42324808">[7 more]</label></div><br/><div class="children"><div class="content">Nice.<p>That said... I mean...<p>&gt; The journey to integrate K&#x2F;V context cache quantisation into Ollama took around 5 months.<p>??<p>They incorrectly tagged #7926 which is a 2 line change, instead of #6279 where it was implemented, which made me dig a bit deeper and reading the actual change it seems:<p>The commit (1) is:<p><pre><code>    &gt; params := C.llama_context_default_params()
    &gt; ...
    &gt; params.type_k = kvCacheTypeFromStr(strings.ToLower(kvCacheType)) &lt;--- adds this
    &gt; params.type_v = kvCacheTypeFromStr(strings.ToLower(kvCacheType)) &lt;--- adds this
</code></pre>
Which has been part of llama.cpp since Dev 7, 2023 (2).<p>So... mmmm... while this is great, somehow I&#x27;m left feeling kind of  vaguely put-off by the comms around what is really &#x27;we finally support some config flag from llama.cpp that&#x27;s been there for really <i>quite a long time</i>&#x27;.<p>&gt; It took 5 months, but we got there in the end.<p>... I guess... yay? The challenges don&#x27;t seem like they were technical, but I guess, good job getting it across the line in the end?<p>[1] - <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;commit&#x2F;1bdab9fdb19f8a8c73ed85291f9acea5bc1c7075#diff-7c8fcee9a6ef35252c34bdc9910b1e605c5d480ea80d9f2fe1c67dc069e9888cR144">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;commit&#x2F;1bdab9fdb19f8a8c73ed...</a><p>[2] - since <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;commit&#x2F;bcc0eb4591bec5ec02fad3f2bdcb1b265052ea56#diff-201cbc8fd17750764ed4a0862232e81503550c201995e16dc2e2766754eaa57aR907">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;commit&#x2F;bcc0eb4591bec5...</a></div><br/><div id="42324892" class="c"><input type="checkbox" id="c-42324892" checked=""/><div class="controls bullet"><span class="by">meesles</span><span>|</span><a href="#42324808">parent</a><span>|</span><a href="#42326214">next</a><span>|</span><label class="collapse" for="c-42324892">[-]</label><label class="expand" for="c-42324892">[2 more]</label></div><br/><div class="children"><div class="content">Author describes why it took as long as it did in the post, so I don&#x27;t think they&#x27;re trying to be disingenous. Getting minor changes merged upstream in large projects is difficult for newer concepts since you need adoption and support.<p>Full release seems to contain more code[1], and author references the llama.cpp pre-work and that author as well<p>This person is also not a core contributor, so this reads as a hobbyist and fan of AI dev that is writing about their work. Nothing to be ashamed of IMO.<p>[1] - <a href="https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;compare&#x2F;v0.4.7...v0.4.8-rc0">https:&#x2F;&#x2F;github.com&#x2F;ollama&#x2F;ollama&#x2F;compare&#x2F;v0.4.7...v0.4.8-rc0</a></div><br/><div id="42325114" class="c"><input type="checkbox" id="c-42325114" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#42324808">root</a><span>|</span><a href="#42324892">parent</a><span>|</span><a href="#42326214">next</a><span>|</span><label class="collapse" for="c-42325114">[-]</label><label class="expand" for="c-42325114">[1 more]</label></div><br/><div class="children"><div class="content">&gt; this reads as a hobbyist and fan of AI dev that is writing about their work<p>Bingo, that&#x27;s me!<p>I suspect the OP didn&#x27;t actually read the post.<p>1. As you pointed out, it&#x27;s about getting the feature working, enabled and contributed into Ollama, not in llama.cpp<p>2. Digging through git commits isn&#x27;t useful when you work hard to squash commits before merging a PR, there were a _lot_ over the last 5 months.<p>3. While I&#x27;m not a go dev (and the introduction of cgo part way through that threw me a bit) there certainly were technicalities along the way, I suspect they not only didn&#x27;t both to read the post, they also didn&#x27;t bother to read the PR.<p>Also, just to clarify - I didn&#x27;t even share this here, it&#x27;s just my personal blog of things I try to remember I did when I look back at them years later.</div><br/></div></div></div></div><div id="42326214" class="c"><input type="checkbox" id="c-42326214" checked=""/><div class="controls bullet"><span class="by">guywhocodes</span><span>|</span><a href="#42324808">parent</a><span>|</span><a href="#42324892">prev</a><span>|</span><a href="#42325134">next</a><span>|</span><label class="collapse" for="c-42326214">[-]</label><label class="expand" for="c-42326214">[1 more]</label></div><br/><div class="children"><div class="content">This is par for Ollama, look at the log_probs issues&#x2F;prs and you get an idea of how well Ollama is run.<p>Ollama is IMO a model downloader for llama.cpp so you can do roleplay with ease.</div><br/></div></div><div id="42325134" class="c"><input type="checkbox" id="c-42325134" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#42324808">parent</a><span>|</span><a href="#42326214">prev</a><span>|</span><a href="#42324957">next</a><span>|</span><label class="collapse" for="c-42325134">[-]</label><label class="expand" for="c-42325134">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m going to be generous here and assume you didn&#x27;t bother to actually read the post (or even the PR) before writing a snaky, non-constructive comment, but skimming through your HN comment history this appears to be on-brand.</div><br/></div></div></div></div></div></div></div></div></div></body></html>