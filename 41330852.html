<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1724490066257" as="style"/><link rel="stylesheet" href="styles.css?v=1724490066257"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://android-developers.googleblog.com/2024/08/adding-16-kb-page-size-to-android.html">Adding 16 kb page size to Android</a> <span class="domain">(<a href="https://android-developers.googleblog.com">android-developers.googleblog.com</a>)</span></div><div class="subtext"><span>mikece</span> | <span>150 comments</span></div><br/><div><div id="41336354" class="c"><input type="checkbox" id="c-41336354" checked=""/><div class="controls bullet"><span class="by">iam-TJ</span><span>|</span><a href="#41331233">next</a><span>|</span><label class="collapse" for="c-41336354">[-]</label><label class="expand" for="c-41336354">[1 more]</label></div><br/><div class="children"><div class="content">In Debian kernel we&#x27;ve very recently enabled building an ARM64 kernel flavour with 16KiB page size and we&#x27;ve discussed adding a 64KiB flavour at some point as is the case for PowerPC64 already.<p>This will likely reveal bugs that need fixing in some of the 70,000+ packages in the Debian archive.<p>That ARM64 16KiB page size is interesting in respect of the Apple M1 where Asahi [0] identified that the DART IOMMU has a minimum page size of 16KiB so using that page size as a minimum for everything is going to be more efficient.<p>[0] <a href="https:&#x2F;&#x2F;asahilinux.org&#x2F;2021&#x2F;10&#x2F;progress-report-september-2021&#x2F;" rel="nofollow">https:&#x2F;&#x2F;asahilinux.org&#x2F;2021&#x2F;10&#x2F;progress-report-september-202...</a></div><br/></div></div><div id="41331233" class="c"><input type="checkbox" id="c-41331233" checked=""/><div class="controls bullet"><span class="by">a1o</span><span>|</span><a href="#41336354">prev</a><span>|</span><a href="#41332003">next</a><span>|</span><label class="collapse" for="c-41331233">[-]</label><label class="expand" for="c-41331233">[41 more]</label></div><br/><div class="children"><div class="content">&gt; The very first 16 KB enabled Android system will be made available on select devices as a developer option. This is so you can use the developer option to test and fix<p>&gt; once an application is fixed to be page size agnostic, the same application binary can run on both 4 KB and 16 KB devices<p>I am curious about this. When could an app NOT be agnostic to this? Like what an app must be doing to cause this to be noticeable?</div><br/><div id="41331609" class="c"><input type="checkbox" id="c-41331609" checked=""/><div class="controls bullet"><span class="by">o11c</span><span>|</span><a href="#41331233">parent</a><span>|</span><a href="#41331289">next</a><span>|</span><label class="collapse" for="c-41331609">[-]</label><label class="expand" for="c-41331609">[25 more]</label></div><br/><div class="children"><div class="content">The fundamental problem is that system headers don&#x27;t provide enough information. In particular, many programs need both &quot;min runtime page size&quot; and &quot;max runtime page size&quot; (and by this I mean non-huge pages).<p>If you call `mmap` without constraint, you need to assume the result will be aligned to at least &quot;min runtime page size&quot;. In practice it is <i>probably</i> safe to assume 4K for this for &quot;normal&quot; systems, but I&#x27;ve seen it down to 128 bytes on some embedded systems, and I don&#x27;t have much breadth there (this will break many programs though, since there are more errno values than that). I don&#x27;t know enough about SPARC binary compatibility to know if it&#x27;s safe to push this up to 8K for certain targets.<p>But if you want to call `mmap` (etc.) with full constraint, you must work in terms of &quot;max runtime page size&quot;. This is known to be up to at least 64K in the wild (aarch64), but some architectures have &quot;huge&quot; pages not much beyond that so I&#x27;m not sure (256K, 512K, and 1M; beyond that is almost certainly going to be considered huge pages).<p>Besides a C macro, these values also need to be baked into the object file and the linker needs to prevent incompatible assumptions (just in case a new microarchitecture changes them)</div><br/><div id="41331881" class="c"><input type="checkbox" id="c-41331881" checked=""/><div class="controls bullet"><span class="by">lanigone</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41331609">parent</a><span>|</span><a href="#41331821">next</a><span>|</span><label class="collapse" for="c-41331881">[-]</label><label class="expand" for="c-41331881">[15 more]</label></div><br/><div class="children"><div class="content">you can also do 2M and 1G huge pages on x86, it gets kind of silly fast.</div><br/><div id="41333441" class="c"><input type="checkbox" id="c-41333441" checked=""/><div class="controls bullet"><span class="by">ShroudedNight</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41331881">parent</a><span>|</span><a href="#41333671">next</a><span>|</span><label class="collapse" for="c-41333441">[-]</label><label class="expand" for="c-41333441">[1 more]</label></div><br/><div class="children"><div class="content">1G huge pages had (have?) performance benefits on managed runtimes for certain scenarios (Both the JIT code cache and the GC space saw uplift on the SpecJ benchmarks if I recall correctly)<p>If using relatively large quantities of memory 2M should enable much higher TLB hit rates assuming the CPU doesn&#x27;t do something silly like only having 4 slots for pages larger than 4k ¬.¬</div><br/></div></div><div id="41333671" class="c"><input type="checkbox" id="c-41333671" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41331881">parent</a><span>|</span><a href="#41333441">prev</a><span>|</span><a href="#41331821">next</a><span>|</span><label class="collapse" for="c-41333671">[-]</label><label class="expand" for="c-41333671">[13 more]</label></div><br/><div class="children"><div class="content">What? Any pointers on how 1G speeds things up? I&#x27;d have taken a bigger page size to wreak havoc on process scheduling and filesystem.</div><br/><div id="41333894" class="c"><input type="checkbox" id="c-41333894" checked=""/><div class="controls bullet"><span class="by">afr0ck</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41333671">parent</a><span>|</span><a href="#41334436">next</a><span>|</span><label class="collapse" for="c-41333894">[-]</label><label class="expand" for="c-41333894">[10 more]</label></div><br/><div class="children"><div class="content">Because of virtual address translation [1] speed up. When a memory access is made by a program, the CPU must first translate the virtual address to a physical address, by walking a hierarchical data structure called a page table [2]. Walking the page tables is slow, thus CPUs implement a small on-CPU cache of virtual-to-physical translations called a TLB [1]. The TLB has a limited number of entries for each page size. With 4 KiB pages, the contention on this cache is very high, especially if the workload has a very large workingset size, therefore causing frequent cache evictions and slow walk of the page tables. With 2 MiB or 1 GiB pages, there is less contention and more workingset size is covered by the TLB. For example, a TLB with 1024 entries can cover a maximum of 4 MiB of workingset memory. With 2 MiB pages, it can cover up to 2 GiB of workingset memory. Often, the CPU has different number of entries for each page size.<p>However, it is known that larger page sizes have higher internal fragmentation and thus lead to memory wastage. It&#x27;s a trade off. But generally speaking, for modern systems, the overhead of managing memory in 4 KiB is very high and we are at a point where switching to 16&#x2F;64 KiB is almost always a win. 2 MiB is still a bit of a stretch, though, but transparent 2 MiB pages for heap memory is enabled by default on most major Linux distributions, aka THP [2]<p>Source: my PhD is on memory management and address translation on large memory systems, having worked both on hardware architecture of address translation and TLBs as well as the Linux kernel. I&#x27;m happy to talk about this all day!<p>[1] <a href="https:&#x2F;&#x2F;blogs.vmware.com&#x2F;vsphere&#x2F;2020&#x2F;03&#x2F;how-is-virtual-memory-translated-to-physical-memory.html" rel="nofollow">https:&#x2F;&#x2F;blogs.vmware.com&#x2F;vsphere&#x2F;2020&#x2F;03&#x2F;how-is-virtual-memo...</a>
[2] <a href="https:&#x2F;&#x2F;docs.kernel.org&#x2F;admin-guide&#x2F;mm&#x2F;transhuge.html" rel="nofollow">https:&#x2F;&#x2F;docs.kernel.org&#x2F;admin-guide&#x2F;mm&#x2F;transhuge.html</a></div><br/><div id="41334485" class="c"><input type="checkbox" id="c-41334485" checked=""/><div class="controls bullet"><span class="by">versteegen</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41333894">parent</a><span>|</span><a href="#41333987">next</a><span>|</span><label class="collapse" for="c-41334485">[-]</label><label class="expand" for="c-41334485">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m happy to talk about this all day!<p>Oh really :)<p>I&#x27;d like to ask how applications should change their memory allocation or usage patterns to maximise the benefit of THP. Do memory allocators (glibc mainly) need config tweaking to coalesce tiny mallocs into 2MB+ mmaps, will they just always do that automatically, do you need to use a custom pool allocator so you&#x27;re doing large allocations, or are you never going to get the full benefit of huge tables without madvise&#x2F;libhugetlbfs? And does this apply to Mac&#x2F;Windows&#x2F;*BSD at all?<p>[Edit: ouch, I see &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled is default set to &#x27;madvise&#x27; on my system (Slackware) and as a result doing nearly nothing. But I saw it enabled in the past. Well that answers a lot of my questions: got to use madvise&#x2F;libhugetlbfs.]<p>I read you also need to ensure ELF segments are properly aligned to get transparent huge pages for code&#x2F;data.<p>Another question. From your link [2]:<p>&gt; An application may mmap a large region but only touch 1 byte of it, in that case a 2M page might be allocated instead of a 4k page for no good.<p>Do the heuristics used by Linux THP (khugepaged) really allow completely ignoring whether pages have actually been page-faulted in or even initialised? Is a possibility unlikely to happen in practice?</div><br/><div id="41334880" class="c"><input type="checkbox" id="c-41334880" checked=""/><div class="controls bullet"><span class="by">afr0ck</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41334485">parent</a><span>|</span><a href="#41333987">next</a><span>|</span><label class="collapse" for="c-41334880">[-]</label><label class="expand" for="c-41334880">[2 more]</label></div><br/><div class="children"><div class="content">In current Linux systems, there are two main ways to benefit from huge pages. 1) There is the explicit, user-managed approach via hugetlbfs. That&#x27;s not very common. 2) Transparently managed by the kernel via THP (userpsace is completely unaware and any application using mmap() and malloc() can benefit from that).<p>As I mentioned before, most major Linux distributions ship with THP enabled by default. THP automatically allocates huge pages for mmap memory whenever possible (that is when, at least, the region is 2 MiB aligned and is at least 2 MiB in size). There is also a separate kernel thread, khugepaged, that opportunistically tries to coalesce&#x2F;promote base 4K pages into 2 MiB huge pages, whenever possible.<p>Library support is not really required for THP, but could be detrimental for its performance and availability on the long run. A library that is not aware of kernel huge pages may employ suboptimal memory management strategies, resulting in inefficient utilization, for example by unintentionally breaking those huge pages (e.g. via unaligned unmapping), or failing to properly release them to the OS as one full unit, undermining their availability on the long run. Afaik, Tcmalloc from Google is the only library with extensive huge page awareness [1].<p>&gt; Do the heuristics used by Linux THP (khugepaged) really allow completely ignoring whether pages have actually been page-faulted in or even initialised? Is a possibility unlikely to happen in practice?<p>Linux allocates huge pages on first touch. For khugepaged, it only coalesces the pages if all the base pages covering the 2 MiB virtual region exist in some form (not necessary faulted-in. For example, some of those base pages could be in swap space and Linux will first fault them in then migrate them)<p>[1] <a href="https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;osdi21-hunter.pdf" rel="nofollow">https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;osdi21-hunter.pdf</a></div><br/><div id="41336004" class="c"><input type="checkbox" id="c-41336004" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41334880">parent</a><span>|</span><a href="#41333987">next</a><span>|</span><label class="collapse" for="c-41336004">[-]</label><label class="expand" for="c-41336004">[1 more]</label></div><br/><div class="children"><div class="content">Mimalloc has support for huge pages.  It also has an option to reserve 1GB pages on program start, and I&#x27;ve had very good performance results using that setting and replacing factorio&#x27;s allocator.  On windows and linux.</div><br/></div></div></div></div></div></div><div id="41333987" class="c"><input type="checkbox" id="c-41333987" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41333894">parent</a><span>|</span><a href="#41334485">prev</a><span>|</span><a href="#41334509">next</a><span>|</span><label class="collapse" for="c-41333987">[-]</label><label class="expand" for="c-41333987">[3 more]</label></div><br/><div class="children"><div class="content">Thanks!<p>&gt; <i>I&#x27;m happy to talk about this all day!</i><p>With noobs, too? ;)<p>&gt; <i>Often, the CPU has different number of entries for each page size.</i><p>- Does it mean userspace is free to allocate up to a maximum of 1G? I took pages to have a <i>fixed</i> size.<p>- Or, you mean CPUs reserve TLB sizes depending on the requested page size?<p>&gt; <i>With 2 MiB or 1 GiB pages, there is less contention and more workingset size is covered by the TLB</i><p>- Would memory allocators &#x2F; GCs need to be changed to deal with blocks of 1G? Would you say, the current ones found in popular runtimes&#x2F;implementations are adept at doing so?<p>- Does it not adversely affect databases accustomed to smaller page sizes now finding themselves paging in 1G at once?<p>&gt; <i>my PhD is on memory management and address translation on large memory systems</i><p>If the dissertation is public, please do link it, if you&#x27;re comfortable doing so.</div><br/><div id="41334967" class="c"><input type="checkbox" id="c-41334967" checked=""/><div class="controls bullet"><span class="by">afr0ck</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41333987">parent</a><span>|</span><a href="#41334945">next</a><span>|</span><label class="collapse" for="c-41334967">[-]</label><label class="expand" for="c-41334967">[1 more]</label></div><br/><div class="children"><div class="content">&gt; - Does it mean userspace is free to allocate up to a maximum of 1G? I took pages to have a fixed size. &gt; - Or, you mean CPUs reserve TLB sizes depending on the requested page size?<p>The TLB is a hardware cache with a limited number of entries that cannot dynamically change. Your CPU is shipped with a fixed number of entries dedicated for each page size. Translations of base 4 KiB pages could, for example, have 1024 entries. Translations of 2 MiB pages could have 512 entries and those of 1 GiB usually have a very limited number of only 8 or 16. Nowadays, most CPU vendors increased their 2 MiB TLBs to have the same number of entries dedicated for 4 KiB pages.<p>If you&#x27;re wondering why they have to be separate caches, it&#x27;s because, for any page in memory, you can have both mappings at the same time from different processes or different parts of the same process, with possibly different protections.<p>&gt; - Would memory allocators &#x2F; GCs need to be changed to deal with blocks of 1G? Would you say, the current ones found in popular runtimes&#x2F;implementations are adept at doing so?<p>&gt; - Does it not adversely affect databases accustomed to smaller page sizes now finding themselves paging in 1G at once?<p>Runtimes and databases have full control and Linux allows per-process policies via madvise) system call. If a program is not happy with huge pages, it can ask the kernel to be ignored, as it can choose to be cooperative.<p>&gt; If the dissertation is public, please do link it, if you&#x27;re comfortable doing so.<p>I&#x27;m still in the PhD process, so no cookies atm :D</div><br/></div></div></div></div><div id="41334509" class="c"><input type="checkbox" id="c-41334509" checked=""/><div class="controls bullet"><span class="by">CalChris</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41333894">parent</a><span>|</span><a href="#41333987">prev</a><span>|</span><a href="#41334436">next</a><span>|</span><label class="collapse" for="c-41334509">[-]</label><label class="expand" for="c-41334509">[3 more]</label></div><br/><div class="children"><div class="content">Are huge pages expected to share code (X) and data (RW)?</div><br/><div id="41335669" class="c"><input type="checkbox" id="c-41335669" checked=""/><div class="controls bullet"><span class="by">nullindividual</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41334509">parent</a><span>|</span><a href="#41334577">next</a><span>|</span><label class="collapse" for="c-41335669">[-]</label><label class="expand" for="c-41335669">[1 more]</label></div><br/><div class="children"><div class="content">Quoting Windows Internals 7th Edt Part 1:<p><pre><code>    There is an unfortunate side effect of large pages. Each page (whether huge, large, or small) must be mapped with a single protection that applies to the entire page. This is because hardware memory protection is on a per-page basis. If a large page contains, for example, both read-only code and read&#x2F;write data, the page must be marked as read&#x2F;write, meaning that the code will be writable. As a result, device drivers or other kernel-mode code could, either maliciously or due to a bug, modify what is supposed to be read-only operating system or driver code without causing a memory access violation.</code></pre></div><br/></div></div><div id="41334577" class="c"><input type="checkbox" id="c-41334577" checked=""/><div class="controls bullet"><span class="by">versteegen</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41334509">parent</a><span>|</span><a href="#41335669">prev</a><span>|</span><a href="#41334436">next</a><span>|</span><label class="collapse" for="c-41334577">[-]</label><label class="expand" for="c-41334577">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s probably no good reason to put code and data on the same page, it&#x27;s just one extra TLB entry to use two pages instead so the data page can be marked non-executable.</div><br/></div></div></div></div></div></div><div id="41334436" class="c"><input type="checkbox" id="c-41334436" checked=""/><div class="controls bullet"><span class="by">monocasa</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41333671">parent</a><span>|</span><a href="#41333894">prev</a><span>|</span><a href="#41331821">next</a><span>|</span><label class="collapse" for="c-41334436">[-]</label><label class="expand" for="c-41334436">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s nice for type 1 hypervisors when carving up memory for guests.  When page walks for guest virtual to host physical end up taking sixteen levels, a 1G page short circuits that in half to eight.</div><br/><div id="41334984" class="c"><input type="checkbox" id="c-41334984" checked=""/><div class="controls bullet"><span class="by">afr0ck</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41334436">parent</a><span>|</span><a href="#41331821">next</a><span>|</span><label class="collapse" for="c-41334984">[-]</label><label class="expand" for="c-41334984">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what most hypervisors (e.g. Qemu) do on Linux when THP are enabled and allowed for the process.</div><br/></div></div></div></div></div></div></div></div><div id="41331821" class="c"><input type="checkbox" id="c-41331821" checked=""/><div class="controls bullet"><span class="by">dotancohen</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41331609">parent</a><span>|</span><a href="#41331881">prev</a><span>|</span><a href="#41331289">next</a><span>|</span><label class="collapse" for="c-41331821">[-]</label><label class="expand" for="c-41331821">[9 more]</label></div><br/><div class="children"><div class="content">Yes, but the context here is Java or Kotlin running on Android, not embedded C.<p>Or do some Android applications run embedded C with only a Java UI? I&#x27;m not an Android dev.</div><br/><div id="41332990" class="c"><input type="checkbox" id="c-41332990" checked=""/><div class="controls bullet"><span class="by">david_allison</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41331821">parent</a><span>|</span><a href="#41331875">next</a><span>|</span><label class="collapse" for="c-41332990">[-]</label><label class="expand" for="c-41332990">[1 more]</label></div><br/><div class="children"><div class="content">The Android Native Development Kit (NDK) allows building native code libraries for Android (typically C&#x2F;C++, but this can include Rust). These can then be loaded and accessed by JNI on the Java&#x2F;Kotlin side<p>* Brief overview of the NDK: <a href="https:&#x2F;&#x2F;developer.android.com&#x2F;ndk&#x2F;guides" rel="nofollow">https:&#x2F;&#x2F;developer.android.com&#x2F;ndk&#x2F;guides</a><p>* Guide to supporting 16KB page sizes with the NDK <a href="https:&#x2F;&#x2F;developer.android.com&#x2F;guide&#x2F;practices&#x2F;page-sizes" rel="nofollow">https:&#x2F;&#x2F;developer.android.com&#x2F;guide&#x2F;practices&#x2F;page-sizes</a></div><br/></div></div><div id="41331875" class="c"><input type="checkbox" id="c-41331875" checked=""/><div class="controls bullet"><span class="by">orf</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41331821">parent</a><span>|</span><a href="#41332990">prev</a><span>|</span><a href="#41333081">next</a><span>|</span><label class="collapse" for="c-41331875">[-]</label><label class="expand" for="c-41331875">[1 more]</label></div><br/><div class="children"><div class="content">Yes, Android apps can and do have native libraries. Sometimes this can be part of a SDK, or otherwise out of the developers control.</div><br/></div></div><div id="41333081" class="c"><input type="checkbox" id="c-41333081" checked=""/><div class="controls bullet"><span class="by">fpoling</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41331821">parent</a><span>|</span><a href="#41331875">prev</a><span>|</span><a href="#41331872">next</a><span>|</span><label class="collapse" for="c-41333081">[-]</label><label class="expand" for="c-41333081">[1 more]</label></div><br/><div class="children"><div class="content">Chrome browser on Android uses  the same code base as Chrome on desktop including multi-process architecture. But it’s UI is in Java communicating with C++ using JNI.</div><br/></div></div><div id="41331872" class="c"><input type="checkbox" id="c-41331872" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41331821">parent</a><span>|</span><a href="#41333081">prev</a><span>|</span><a href="#41332669">next</a><span>|</span><label class="collapse" for="c-41331872">[-]</label><label class="expand" for="c-41331872">[3 more]</label></div><br/><div class="children"><div class="content">Android apps can call into native code via JNI, which the platform supports.</div><br/><div id="41333685" class="c"><input type="checkbox" id="c-41333685" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41331872">parent</a><span>|</span><a href="#41332669">next</a><span>|</span><label class="collapse" for="c-41333685">[-]</label><label class="expand" for="c-41333685">[2 more]</label></div><br/><div class="children"><div class="content">Wonder if Android apps can also be fully native (C++)?</div><br/><div id="41334052" class="c"><input type="checkbox" id="c-41334052" checked=""/><div class="controls bullet"><span class="by">fensgrim</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41333685">parent</a><span>|</span><a href="#41332669">next</a><span>|</span><label class="collapse" for="c-41334052">[-]</label><label class="expand" for="c-41334052">[1 more]</label></div><br/><div class="children"><div class="content">It is possible to have a project set up with a manifest which contains only a single activity with android.app.NativeActivity pointing to a .so, and zero lines of java&#x2F;kotlin&#x2F;flutter&#x2F;whatever else - though your app initialization will go through usual hoops of spawning a java-based instance.<p>Minimal example would be <a href="https:&#x2F;&#x2F;github.com&#x2F;android&#x2F;ndk-samples&#x2F;blob&#x2F;master&#x2F;native-activity&#x2F;app&#x2F;src&#x2F;main&#x2F;AndroidManifest.xml">https:&#x2F;&#x2F;github.com&#x2F;android&#x2F;ndk-samples&#x2F;blob&#x2F;master&#x2F;native-ac...</a>, though there are well established Qt based apps as well</div><br/></div></div></div></div></div></div><div id="41332669" class="c"><input type="checkbox" id="c-41332669" checked=""/><div class="controls bullet"><span class="by">warkdarrior</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41331821">parent</a><span>|</span><a href="#41331872">prev</a><span>|</span><a href="#41331289">next</a><span>|</span><label class="collapse" for="c-41332669">[-]</label><label class="expand" for="c-41332669">[2 more]</label></div><br/><div class="children"><div class="content">Apps written in Flutter&#x2F;Dart and React Native&#x2F;Javascript both compile to native code with only shims to interface with the Java UI framework.</div><br/><div id="41335417" class="c"><input type="checkbox" id="c-41335417" checked=""/><div class="controls bullet"><span class="by">farmerbb</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41332669">parent</a><span>|</span><a href="#41331289">next</a><span>|</span><label class="collapse" for="c-41335417">[-]</label><label class="expand" for="c-41335417">[1 more]</label></div><br/><div class="children"><div class="content">Flutter&#x2F;Dart, yes, React Native&#x2F;Javascript, no.  With RN the app&#x27;s code runs via an embedded JavaScript engine, and even when, say, Hermes is being used, it&#x27;s still executing bytecode not native machine code.<p>Also important to note that any code that runs on Android&#x27;s ART runtime (i.e. Kotlin and&#x2F;or Java) can get some or all of its code AOT-compiled to machine code by the OS, either upon app install (if the app ships with baseline profiles) or in the background while the device is idle and charging.</div><br/></div></div></div></div></div></div></div></div><div id="41331289" class="c"><input type="checkbox" id="c-41331289" checked=""/><div class="controls bullet"><span class="by">mlmandude</span><span>|</span><a href="#41331233">parent</a><span>|</span><a href="#41331609">prev</a><span>|</span><a href="#41332536">next</a><span>|</span><label class="collapse" for="c-41331289">[-]</label><label class="expand" for="c-41331289">[1 more]</label></div><br/><div class="children"><div class="content">If you use mmap&#x2F;munmap directly within your application you could probably get into trouble by hardcoding the page size.</div><br/></div></div><div id="41332536" class="c"><input type="checkbox" id="c-41332536" checked=""/><div class="controls bullet"><span class="by">growse</span><span>|</span><a href="#41331233">parent</a><span>|</span><a href="#41331289">prev</a><span>|</span><a href="#41331393">next</a><span>|</span><label class="collapse" for="c-41332536">[-]</label><label class="expand" for="c-41332536">[2 more]</label></div><br/><div class="children"><div class="content">If you use a database library that does mmap to create a db file with SC_PAGE_SIZE (4KB) pages, and then upgrade your device to a 16KB one and backup&#x2F;restore the app, now your data isn&#x27;t readable.</div><br/><div id="41336200" class="c"><input type="checkbox" id="c-41336200" checked=""/><div class="controls bullet"><span class="by">phh</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41332536">parent</a><span>|</span><a href="#41331393">next</a><span>|</span><label class="collapse" for="c-41336200">[-]</label><label class="expand" for="c-41336200">[1 more]</label></div><br/><div class="children"><div class="content">Which is the reason you need to format your data to experiment with 16k</div><br/></div></div></div></div><div id="41331393" class="c"><input type="checkbox" id="c-41331393" checked=""/><div class="controls bullet"><span class="by">edflsafoiewq</span><span>|</span><a href="#41331233">parent</a><span>|</span><a href="#41332536">prev</a><span>|</span><a href="#41334555">next</a><span>|</span><label class="collapse" for="c-41331393">[-]</label><label class="expand" for="c-41331393">[1 more]</label></div><br/><div class="children"><div class="content">jemalloc bakes in page size assumptions, see eg <a href="https:&#x2F;&#x2F;github.com&#x2F;jemalloc&#x2F;jemalloc&#x2F;issues&#x2F;467">https:&#x2F;&#x2F;github.com&#x2F;jemalloc&#x2F;jemalloc&#x2F;issues&#x2F;467</a>.</div><br/></div></div><div id="41334555" class="c"><input type="checkbox" id="c-41334555" checked=""/><div class="controls bullet"><span class="by">dataflow</span><span>|</span><a href="#41331233">parent</a><span>|</span><a href="#41331393">prev</a><span>|</span><a href="#41331857">next</a><span>|</span><label class="collapse" for="c-41334555">[-]</label><label class="expand" for="c-41334555">[1 more]</label></div><br/><div class="children"><div class="content">&gt; When could an app NOT be agnostic to this<p>When the app has a custom memory allocator, the allocator might have hardcoded the page size for performance. Otherwise you have to load a static variable (knocks out a cache line you could&#x27;ve used for something else) and then do a multiplication (or bit shift, if you assume power of 2) by a runtime value instead of a shift by a constant, which can be slower.<p>No idea if Android apps are ever this performance sensitive, though.</div><br/></div></div><div id="41331857" class="c"><input type="checkbox" id="c-41331857" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41331233">parent</a><span>|</span><a href="#41334555">prev</a><span>|</span><a href="#41331295">next</a><span>|</span><label class="collapse" for="c-41331857">[-]</label><label class="expand" for="c-41331857">[1 more]</label></div><br/><div class="children"><div class="content">Pages sizes are often important to code that relies on low-level details of the environment it’s running in, like language runtimes. They might do things like mark some sections of code as writable or executable and thus would need to know what the granularity of those requests can be. It’s also of importance to things like allocators that hand out memory backed by mmap pages. If they have, say, a bit field for each 16-byte region of a page that has been used that will change in size in ways they can detect.</div><br/></div></div><div id="41331295" class="c"><input type="checkbox" id="c-41331295" checked=""/><div class="controls bullet"><span class="by">dmytroi</span><span>|</span><a href="#41331233">parent</a><span>|</span><a href="#41331857">prev</a><span>|</span><a href="#41333211">next</a><span>|</span><label class="collapse" for="c-41331295">[-]</label><label class="expand" for="c-41331295">[2 more]</label></div><br/><div class="children"><div class="content">Also ELF segment alignment, which is defaults to 4k.</div><br/><div id="41333090" class="c"><input type="checkbox" id="c-41333090" checked=""/><div class="controls bullet"><span class="by">bri3d</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41331295">parent</a><span>|</span><a href="#41333211">next</a><span>|</span><label class="collapse" for="c-41333090">[-]</label><label class="expand" for="c-41333090">[1 more]</label></div><br/><div class="children"><div class="content">Only on Android, for what it&#x27;s worth; most &quot;vanilla&quot; Linux aarch64 linkers chose 64K defaults several years ago. But yes, most Android applications with native (NDK) binaries will need to be rebuilt with the new 16kb max-page-size.</div><br/></div></div></div></div><div id="41333211" class="c"><input type="checkbox" id="c-41333211" checked=""/><div class="controls bullet"><span class="by">nox101</span><span>|</span><a href="#41331233">parent</a><span>|</span><a href="#41331295">prev</a><span>|</span><a href="#41331290">next</a><span>|</span><label class="collapse" for="c-41333211">[-]</label><label class="expand" for="c-41333211">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know if this fits but I&#x27;ve seen code that allocated say 32 bytes from a function that allocated 1meg under the hood. Not knowing that&#x27;s what was happening the app quickly ran out of memory. It arguably was not the app&#x27;s fault. The API it was calling into was poorly designed and poorly named, such that the fact that you might need to know the block size to use the function was in no way indicated by the name of the function nor the names of any of its parameters.</div><br/></div></div><div id="41331290" class="c"><input type="checkbox" id="c-41331290" checked=""/><div class="controls bullet"><span class="by">vardump</span><span>|</span><a href="#41331233">parent</a><span>|</span><a href="#41333211">prev</a><span>|</span><a href="#41331444">next</a><span>|</span><label class="collapse" for="c-41331290">[-]</label><label class="expand" for="c-41331290">[1 more]</label></div><br/><div class="children"><div class="content">For example use mmap and just assume 4 kB pages.</div><br/></div></div><div id="41331444" class="c"><input type="checkbox" id="c-41331444" checked=""/><div class="controls bullet"><span class="by">sweeter</span><span>|</span><a href="#41331233">parent</a><span>|</span><a href="#41331290">prev</a><span>|</span><a href="#41332003">next</a><span>|</span><label class="collapse" for="c-41331444">[-]</label><label class="expand" for="c-41331444">[5 more]</label></div><br/><div class="children"><div class="content">Wine doesn&#x27;t work on 16 KB page size among other things.</div><br/><div id="41332072" class="c"><input type="checkbox" id="c-41332072" checked=""/><div class="controls bullet"><span class="by">mananaysiempre</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41331444">parent</a><span>|</span><a href="#41332003">next</a><span>|</span><label class="collapse" for="c-41332072">[-]</label><label class="expand" for="c-41332072">[4 more]</label></div><br/><div class="children"><div class="content">This seems especially peculiar given Windows has a 64K mapping granularity.</div><br/><div id="41332384" class="c"><input type="checkbox" id="c-41332384" checked=""/><div class="controls bullet"><span class="by">tredre3</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41332072">parent</a><span>|</span><a href="#41332003">next</a><span>|</span><label class="collapse" for="c-41332384">[-]</label><label class="expand" for="c-41332384">[3 more]</label></div><br/><div class="children"><div class="content">Windows uses 4KB pages.</div><br/><div id="41332602" class="c"><input type="checkbox" id="c-41332602" checked=""/><div class="controls bullet"><span class="by">mananaysiempre</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41332384">parent</a><span>|</span><a href="#41332560">next</a><span>|</span><label class="collapse" for="c-41332602">[-]</label><label class="expand" for="c-41332602">[1 more]</label></div><br/><div class="children"><div class="content">Right (on x86-32 and -64, because you can’t have 64KB pages there, though larger page sizes do exist and get used). You still cannot (e.g.) MapViewOfFile() on an address not divisible by 64KB, because Alpha[1]. As far as I understand, Windows is mostly why the docs for the Blink emulator[2] (a companion project of Cosmopolitan libc) tell you any programs under it need to use sysconf(_SC_PAGESIZE) [aka getpagesize() aka getauxval(AT_PAGESZ)] instead of assuming 4KB.<p>[1] <a href="https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;oldnewthing&#x2F;20031008-00&#x2F;?p=42223" rel="nofollow">https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;oldnewthing&#x2F;20031008-00&#x2F;?p=42...</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;jart&#x2F;blink&#x2F;blob&#x2F;master&#x2F;README.md#compiling-and-running-programs-under-blink">https:&#x2F;&#x2F;github.com&#x2F;jart&#x2F;blink&#x2F;blob&#x2F;master&#x2F;README.md#compilin...</a></div><br/></div></div><div id="41332560" class="c"><input type="checkbox" id="c-41332560" checked=""/><div class="controls bullet"><span class="by">nullindividual</span><span>|</span><a href="#41331233">root</a><span>|</span><a href="#41332384">parent</a><span>|</span><a href="#41332602">prev</a><span>|</span><a href="#41332003">next</a><span>|</span><label class="collapse" for="c-41332560">[-]</label><label class="expand" for="c-41332560">[1 more]</label></div><br/><div class="children"><div class="content">4K, 2M (&quot;large page&quot;), or 1G (&quot;huge page&quot;) on x86-64. A single allocation request can consist of multiple page sizes. From <i>Windows Internal 7th Edt Part 1</i>:<p><pre><code>    On Windows 10 version 1607 x64 and Server 2016 systems, large pages may also be mapped with huge pages, which are 1 GB in size. This is done automatically if the allocation size requested is larger than 1 GB, but it does not have to be a multiple of 1 GB. For example, an allocation of 1040 MB would result in using one huge page (1024 MB) plus 8 “normal” large pages (16 MB divided by 2 MB).</code></pre></div><br/></div></div></div></div></div></div></div></div></div></div><div id="41332003" class="c"><input type="checkbox" id="c-41332003" checked=""/><div class="controls bullet"><span class="by">devit</span><span>|</span><a href="#41331233">prev</a><span>|</span><a href="#41331152">next</a><span>|</span><label class="collapse" for="c-41332003">[-]</label><label class="expand" for="c-41332003">[27 more]</label></div><br/><div class="children"><div class="content">Seems pretty dubious to do this without adding support for having both 4KB and 16KB processes at once to the Linux kernel, since it means all old binaries break and emulators which emulate normal systems with 4KB pages (Wine, console emulators, etc.) might dramatically lose performance if they need to emulate the MMU.<p>Hopefully they don&#x27;t actually ship a 16KB default before supporting 4KB pages as well in the same kernel.<p>Also it would probably be reasonable, along with making the Linux kernel change, to design CPUs where you can configure a 16KB pagetable entry to map at 4KB granularity and pagefault after the first 4KB or 8KB (requires 3 extra bits per PTE or 2 if coalesced with the invalid bit), so that memory can be saved by allocating 4KB&#x2F;8KB pages when 16KB would have wasted padding.</div><br/><div id="41332492" class="c"><input type="checkbox" id="c-41332492" checked=""/><div class="controls bullet"><span class="by">Veserv</span><span>|</span><a href="#41332003">parent</a><span>|</span><a href="#41332970">next</a><span>|</span><label class="collapse" for="c-41332492">[-]</label><label class="expand" for="c-41332492">[13 more]</label></div><br/><div class="children"><div class="content">Having both 4KB and 16KB simultaneously is either easy or hard depending on which hardware feature they are using for 16KB pages.<p>If they are using the configurable granule size, then that is a system-wide hardware configuration option. You literally can not map at smaller granularity while that bit is set.<p>You might be able to design a CPU that allows your idea of partial pages, but there be dragons.<p>If they are not configuring the granule size, instead opting for software enforcement in conjunction with always using the contiguous hint bit, then it might be possible.<p>However, I am pretty sure they are talking about hardware granule size, since the contiguous hint is most commonly used to support 16 contiguous entrys (though the CPU designer is technically allowed to do whatever grouping they want) which would be 64KB.</div><br/><div id="41332905" class="c"><input type="checkbox" id="c-41332905" checked=""/><div class="controls bullet"><span class="by">stingraycharles</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41332492">parent</a><span>|</span><a href="#41333593">next</a><span>|</span><label class="collapse" for="c-41332905">[-]</label><label class="expand" for="c-41332905">[8 more]</label></div><br/><div class="children"><div class="content">I’m a total idiot, how exactly is page size a CPU issue rather than a kernel issue? Is it about memory channel protocols &#x2F; communication?<p>Disks have been slowly migrating away from the 4kb sector size, is this a same thing going on? That you need to actual drive to support it, because of internal structuring (i.e. how exactly the CPU aligns things in RAM), and on some super low level 4kb &#x2F; 16kb being the smallest unit of memory you can allocate?<p>And does that then mean that there’s less overhead in all kinds of memory (pre)fetchers in the CPU, because more can be achieved in less clock cycles?</div><br/><div id="41333655" class="c"><input type="checkbox" id="c-41333655" checked=""/><div class="controls bullet"><span class="by">s_tec</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41332905">parent</a><span>|</span><a href="#41333367">next</a><span>|</span><label class="collapse" for="c-41333655">[-]</label><label class="expand" for="c-41333655">[1 more]</label></div><br/><div class="children"><div class="content">Each OS process has its own virtual address space, which is why one process cannot read another&#x27;s memory. The CPU implements these address spaces in hardware, since literally every memory read or write needs to have its address translated from virtual to physical.<p>The CPU&#x27;s address translation process relies on tables that the OS sets up. For instance, one table entry might say that the 4K memory chunk with virtual address 0x21000-0x21fff maps to physical address 0xf56e3000, and is both executable and read-only. So yes, the OS sets up the tables, but the hardware implements the protection.<p>Since memory protection is a hardware feature, the hardware needs to decide how fine-grained the pages are. It&#x27;s possible to build a CPU with byte-level protection, but this would be crazy-inefficient. Bigger pages mean less translation work, but they can also create more wasted space. Sizes in the 4K-64K range seem to offer good tradeoffs for everyday workloads.</div><br/></div></div><div id="41333367" class="c"><input type="checkbox" id="c-41333367" checked=""/><div class="controls bullet"><span class="by">pwg</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41332905">parent</a><span>|</span><a href="#41333655">prev</a><span>|</span><a href="#41333152">next</a><span>|</span><label class="collapse" for="c-41333367">[-]</label><label class="expand" for="c-41333367">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I’m a total idiot, how exactly is page size a CPU issue rather than a kernel issue?<p>Because the size of a page is a hardware defined size for Intel and ARM CPU&#x27;s (well, more modern Intel and ARM CPU&#x27;s give the OS a choice of sizes from a small set of options).<p>It (page size) is baked into the CPU hardware.<p>&gt; And does that then mean that there’s less overhead in all kinds of memory (pre)fetchers in the CPU, because more can be achieved in less clock cycles?<p>For the same size TLB (Translation Look-aside Buffer -- the CPU hardware that stores the &quot;referencing info&quot; for the currently active set of pages being used by the code running on the CPU) a larger page size allows more total memory to be accessible before taking a page fault and having to replace one or more of the entries in the TLB.  So yes, it means less overhead, because CPU cycles are not used up in replacing as many TLB entries as often.</div><br/></div></div><div id="41333152" class="c"><input type="checkbox" id="c-41333152" checked=""/><div class="controls bullet"><span class="by">fpoling</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41332905">parent</a><span>|</span><a href="#41333367">prev</a><span>|</span><a href="#41333086">next</a><span>|</span><label class="collapse" for="c-41333152">[-]</label><label class="expand" for="c-41333152">[1 more]</label></div><br/><div class="children"><div class="content">Samsung SSD still reports to the system that their logical sector size is 512 bytes. In fact one of the recent models even removed the option to reconfigure the disk to use 4k logical sectors. Presumably Samsung has figured that since the physical sector is much larger and they need complex mapping of logic sectors in any case, they decided not to support 4K option and stick with 512 bytes.</div><br/></div></div><div id="41333086" class="c"><input type="checkbox" id="c-41333086" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41332905">parent</a><span>|</span><a href="#41333152">prev</a><span>|</span><a href="#41333593">next</a><span>|</span><label class="collapse" for="c-41333086">[-]</label><label class="expand" for="c-41333086">[4 more]</label></div><br/><div class="children"><div class="content">The CPU has hardware that does a page table walk automatically when you access an address for which the translation is not cached in the TLB. Otherwise virtual memory would be really slow.<p>Since the CPU hardware itself is doing the page table walk it needs to understand page tables and page table entries etc. including how big pages are.<p>Also you need to know how big pages are for the TLB itself.<p>The value of 4kB itself is pretty much arbitrary. It has to be a small enough number that you don&#x27;t waste a load of memory by mapping memory that isn&#x27;t used (e.g. if you ask for 4.01kB you&#x27;re actually going to get 8kB), but a large enough number that you aren&#x27;t spending all your time managing tiny pages.<p>That&#x27;s why increasing the page size makes things faster but waste more memory.<p>4kB arguably isn&#x27;t optimal anymore since we have way more memory now than when it was de facto standardised so it doesn&#x27;t matter as much if we waste a bit. Maybe.</div><br/><div id="41333222" class="c"><input type="checkbox" id="c-41333222" checked=""/><div class="controls bullet"><span class="by">quotemstr</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41333086">parent</a><span>|</span><a href="#41333593">next</a><span>|</span><label class="collapse" for="c-41333222">[-]</label><label class="expand" for="c-41333222">[3 more]</label></div><br/><div class="children"><div class="content">As an aside, it&#x27;s shame that hardware page table walking won out over software filled TLBs, as some older computers had. I wonder what clever and wonderful hacks we might have been able to invent had we not needed to give the CPU a raw pointer to a data structure the layout of which is fixed forever.</div><br/><div id="41333547" class="c"><input type="checkbox" id="c-41333547" checked=""/><div class="controls bullet"><span class="by">Denvercoder9</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41333222">parent</a><span>|</span><a href="#41333381">next</a><span>|</span><label class="collapse" for="c-41333547">[-]</label><label class="expand" for="c-41333547">[1 more]</label></div><br/><div class="children"><div class="content">Page table layout isn&#x27;t really fixed forever, x86 has changed its multiple times.</div><br/></div></div><div id="41333381" class="c"><input type="checkbox" id="c-41333381" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41333222">parent</a><span>|</span><a href="#41333547">prev</a><span>|</span><a href="#41333593">next</a><span>|</span><label class="collapse" for="c-41333381">[-]</label><label class="expand" for="c-41333381">[1 more]</label></div><br/><div class="children"><div class="content">Yeah maybe, though in practice I think it would be just too slow.</div><br/></div></div></div></div></div></div></div></div><div id="41333593" class="c"><input type="checkbox" id="c-41333593" checked=""/><div class="controls bullet"><span class="by">sweetjuly</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41332492">parent</a><span>|</span><a href="#41332905">prev</a><span>|</span><a href="#41332970">next</a><span>|</span><label class="collapse" for="c-41333593">[-]</label><label class="expand" for="c-41333593">[4 more]</label></div><br/><div class="children"><div class="content">Hmm, I&#x27;m not sure that&#x27;s quite right. ARMv8 supports per TTBR translation granules [1] and so you can have 4K and 16K user processes coexisting under an arbitrary page size kernel by just context switching TCR.TG0 at the same time as TTBR0. There is no such thing as a global granule size.<p>[1]: <a href="https:&#x2F;&#x2F;arm.jonpalmisc.com&#x2F;2023_09_sysreg&#x2F;AArch64-tcr_el2#fieldset_0-15_14" rel="nofollow">https:&#x2F;&#x2F;arm.jonpalmisc.com&#x2F;2023_09_sysreg&#x2F;AArch64-tcr_el2#fi...</a></div><br/><div id="41334215" class="c"><input type="checkbox" id="c-41334215" checked=""/><div class="controls bullet"><span class="by">Veserv</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41333593">parent</a><span>|</span><a href="#41332970">next</a><span>|</span><label class="collapse" for="c-41334215">[-]</label><label class="expand" for="c-41334215">[3 more]</label></div><br/><div class="children"><div class="content">Well, if you want to run headfirst into the magical land of hardware errata, I guess you could go around creating heterogeneous, switched mappings.<p>I doubt the TCRs were ever intended to support rapid runtime switching or that the TLBs were ever intended to support heterogeneous entrys even with ASID tagging.</div><br/><div id="41335910" class="c"><input type="checkbox" id="c-41335910" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41334215">parent</a><span>|</span><a href="#41334690">next</a><span>|</span><label class="collapse" for="c-41335910">[-]</label><label class="expand" for="c-41335910">[1 more]</label></div><br/><div class="children"><div class="content">Apple has been literally doing this for years.</div><br/></div></div><div id="41334690" class="c"><input type="checkbox" id="c-41334690" checked=""/><div class="controls bullet"><span class="by">quotemstr</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41334215">parent</a><span>|</span><a href="#41335910">prev</a><span>|</span><a href="#41332970">next</a><span>|</span><label class="collapse" for="c-41334690">[-]</label><label class="expand" for="c-41334690">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;ve listed things that could go wrong without citing specific errata. Should we just assume that hardware doesn&#x27;t work as documented? It seems premature to deem the feature buggy without having tried it.</div><br/></div></div></div></div></div></div></div></div><div id="41332970" class="c"><input type="checkbox" id="c-41332970" checked=""/><div class="controls bullet"><span class="by">Zefiroj</span><span>|</span><a href="#41332003">parent</a><span>|</span><a href="#41332492">prev</a><span>|</span><a href="#41332312">next</a><span>|</span><label class="collapse" for="c-41332970">[-]</label><label class="expand" for="c-41332970">[1 more]</label></div><br/><div class="children"><div class="content">The support for mTHP exists in upstream Linux, but the swap story is not quite there yet. THP availability also needs work and there are a few competing directions.<p>Supporting multiple page sizes well transparently is non-trivial.<p>For a recent summary on one of the approaches, TAO (THP Allocation Optimization), see this lwn article: <a href="https:&#x2F;&#x2F;lwn.net&#x2F;Articles&#x2F;974636&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lwn.net&#x2F;Articles&#x2F;974636&#x2F;</a></div><br/></div></div><div id="41332312" class="c"><input type="checkbox" id="c-41332312" checked=""/><div class="controls bullet"><span class="by">phh</span><span>|</span><a href="#41332003">parent</a><span>|</span><a href="#41332970">prev</a><span>|</span><a href="#41332078">next</a><span>|</span><label class="collapse" for="c-41332312">[-]</label><label class="expand" for="c-41332312">[2 more]</label></div><br/><div class="children"><div class="content">Google&#x2F;Android doesn&#x27;t care much about backward compatibility and broke programs released on Pixel 3 in Pixel 7. (the interdiction of 32bit-only apps is 2019 on Play Store, Pixel 7 is first 64bits only device, while Google still released 32bits only device in 2023...). They quite regularly break apps in new Android versions (despite their infrastructure to handle backward compatibility), and app developers are used to brace themselves around Android &amp; Pixel releases</div><br/><div id="41333397" class="c"><input type="checkbox" id="c-41333397" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41332312">parent</a><span>|</span><a href="#41332078">next</a><span>|</span><label class="collapse" for="c-41333397">[-]</label><label class="expand" for="c-41333397">[1 more]</label></div><br/><div class="children"><div class="content">Generally I&#x27;ve found Google to care much more about not breaking old apps compared to Apple, which often expects developers to rebuild apps for OS updates or else the apps stop working entirely (or buy entirely new machines to get OS updates at all, e.g. the Intel&#x2F;Apple Silicon transition). Google isn&#x27;t on the level of Windows &quot;we will watch for specific binaries and re-introduce bugs in the kernel specifically for those binaries that they depend on&quot; in terms of backwards compatibility, but I wouldn&#x27;t go so far as to say they don&#x27;t care. I&#x27;m not sure whether that&#x27;s better or worse: there&#x27;s definitely merit to Apple&#x27;s approach, since it keeps them able to iterate quickly on UX and performance by dropping support for the old stuff.</div><br/></div></div></div></div><div id="41332078" class="c"><input type="checkbox" id="c-41332078" checked=""/><div class="controls bullet"><span class="by">username81</span><span>|</span><a href="#41332003">parent</a><span>|</span><a href="#41332312">prev</a><span>|</span><a href="#41332085">next</a><span>|</span><label class="collapse" for="c-41332078">[-]</label><label class="expand" for="c-41332078">[2 more]</label></div><br/><div class="children"><div class="content">Shouldn&#x27;t there be some kind of setting to change the page size per program? AFAIK AMD64 CPUs can do this.</div><br/><div id="41335924" class="c"><input type="checkbox" id="c-41335924" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41332078">parent</a><span>|</span><a href="#41332085">next</a><span>|</span><label class="collapse" for="c-41335924">[-]</label><label class="expand" for="c-41335924">[1 more]</label></div><br/><div class="children"><div class="content">Yes, ARM CPUs can do it too.</div><br/></div></div></div></div><div id="41332085" class="c"><input type="checkbox" id="c-41332085" checked=""/><div class="controls bullet"><span class="by">fouronnes3</span><span>|</span><a href="#41332003">parent</a><span>|</span><a href="#41332078">prev</a><span>|</span><a href="#41332113">next</a><span>|</span><label class="collapse" for="c-41332085">[-]</label><label class="expand" for="c-41332085">[1 more]</label></div><br/><div class="children"><div class="content">Could they upstream that or would that require a fork?</div><br/></div></div><div id="41332113" class="c"><input type="checkbox" id="c-41332113" checked=""/><div class="controls bullet"><span class="by">mgaunard</span><span>|</span><a href="#41332003">parent</a><span>|</span><a href="#41332085">prev</a><span>|</span><a href="#41332901">next</a><span>|</span><label class="collapse" for="c-41332113">[-]</label><label class="expand" for="c-41332113">[5 more]</label></div><br/><div class="children"><div class="content">why does it break userland? if you need to know the page size, you should query sysconf SC_PAGESIZE.</div><br/><div id="41332171" class="c"><input type="checkbox" id="c-41332171" checked=""/><div class="controls bullet"><span class="by">akdev1l</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41332113">parent</a><span>|</span><a href="#41332657">next</a><span>|</span><label class="collapse" for="c-41332171">[-]</label><label class="expand" for="c-41332171">[1 more]</label></div><br/><div class="children"><div class="content">Assumptions in the software.<p>Jemalloc is infamous for this: <a href="https:&#x2F;&#x2F;github.com&#x2F;sigp&#x2F;lighthouse&#x2F;issues&#x2F;5244">https:&#x2F;&#x2F;github.com&#x2F;sigp&#x2F;lighthouse&#x2F;issues&#x2F;5244</a></div><br/></div></div><div id="41332657" class="c"><input type="checkbox" id="c-41332657" checked=""/><div class="controls bullet"><span class="by">fweimer</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41332113">parent</a><span>|</span><a href="#41332171">prev</a><span>|</span><a href="#41332612">next</a><span>|</span><label class="collapse" for="c-41332657">[-]</label><label class="expand" for="c-41332657">[1 more]</label></div><br/><div class="children"><div class="content">It should not break userland. GNU&#x2F;Linux (not necessarily Android though) has supported 64K pages pretty much from the start because that was the originally page size chosen for server-focus kernels and distributions. But there are some things that need to be worked around.<p>Certain build processes determine the page size at compile time and assume it&#x27;s the same at run time, and fail if it is not: <a href="https:&#x2F;&#x2F;github.com&#x2F;jemalloc&#x2F;jemalloc&#x2F;issues&#x2F;467">https:&#x2F;&#x2F;github.com&#x2F;jemalloc&#x2F;jemalloc&#x2F;issues&#x2F;467</a><p>Some memory-mapped files formats have assumptions about page granularity: <a href="https:&#x2F;&#x2F;bugzilla.redhat.com&#x2F;show_bug.cgi?id=1979804" rel="nofollow">https:&#x2F;&#x2F;bugzilla.redhat.com&#x2F;show_bug.cgi?id=1979804</a><p>The file format issue applies to ELF as well. Some people patch their toolchains (or use suitable linker options) to produce slightly smaller binaries that can only be loaded if the page size is 4K, even though the ABI is pretty clear in that you should link for compatibility with up to 64K pages.</div><br/></div></div><div id="41332612" class="c"><input type="checkbox" id="c-41332612" checked=""/><div class="controls bullet"><span class="by">Dwedit</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41332113">parent</a><span>|</span><a href="#41332657">prev</a><span>|</span><a href="#41332688">next</a><span>|</span><label class="collapse" for="c-41332612">[-]</label><label class="expand" for="c-41332612">[1 more]</label></div><br/><div class="children"><div class="content">Emulating a processor with 4K size pages becomes much higher performance if you can use real addresses directly.</div><br/></div></div><div id="41332688" class="c"><input type="checkbox" id="c-41332688" checked=""/><div class="controls bullet"><span class="by">ndesaulniers</span><span>|</span><a href="#41332003">root</a><span>|</span><a href="#41332113">parent</a><span>|</span><a href="#41332612">prev</a><span>|</span><a href="#41332901">next</a><span>|</span><label class="collapse" for="c-41332688">[-]</label><label class="expand" for="c-41332688">[1 more]</label></div><br/><div class="children"><div class="content">Ossification.<p>If the page size has been 4k for decades for most OS&#x27; and architectures, people get sloppy and hard code that literal value, rather than query for it.</div><br/></div></div></div></div><div id="41332901" class="c"><input type="checkbox" id="c-41332901" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#41332003">parent</a><span>|</span><a href="#41332113">prev</a><span>|</span><a href="#41332974">next</a><span>|</span><label class="collapse" for="c-41332901">[-]</label><label class="expand" for="c-41332901">[1 more]</label></div><br/><div class="children"><div class="content">&gt; all old binaries break and emulators which emulate normal systems with 4KB pages<p>Would it actually affect the kind of emulators present on Android, i.e. largely software-only ones, as opposed to hardware virtualizers making use of a CPU&#x27;s vTLB?<p>Wine is famously not an emulator and as such doesn&#x27;t really exist&#x2F;make sense on (non-x86) Android (as it would only be able to execute ARM binaries, not x86 ones).<p>For the downvote: Genuinely curious here on which type of emulator this could affect.</div><br/></div></div></div></div><div id="41331152" class="c"><input type="checkbox" id="c-41331152" checked=""/><div class="controls bullet"><span class="by">twoodfin</span><span>|</span><a href="#41332003">prev</a><span>|</span><a href="#41332194">next</a><span>|</span><label class="collapse" for="c-41331152">[-]</label><label class="expand" for="c-41331152">[26 more]</label></div><br/><div class="children"><div class="content">A little additional background: iOS has used 16KB pages since the 64-bit transition, and ARM Macs have inherited that design.</div><br/><div id="41331334" class="c"><input type="checkbox" id="c-41331334" checked=""/><div class="controls bullet"><span class="by">arghwhat</span><span>|</span><a href="#41331152">parent</a><span>|</span><a href="#41331910">next</a><span>|</span><label class="collapse" for="c-41331334">[-]</label><label class="expand" for="c-41331334">[23 more]</label></div><br/><div class="children"><div class="content">A more relevant bit of background is that 4KB pages lead to quite a lot of overhead due to the sheer number of mappings needing to be configured and cached. Using larger pages reduce overhead, in particular TLB misses as fewer entries are needed to describe the same memory range.<p>While x86 chips mainly supports 4K, 2M and 1G pages, ARM chips tend to support more practical 16K page sizes - a nice balance between performance and wasting memory due to lower allocation granularity.<p>Nothing in particular to do with Apple and iOS.</div><br/><div id="41331452" class="c"><input type="checkbox" id="c-41331452" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331334">parent</a><span>|</span><a href="#41332577">next</a><span>|</span><label class="collapse" for="c-41331452">[-]</label><label class="expand" for="c-41331452">[20 more]</label></div><br/><div class="children"><div class="content">Makes me wonder how much performance Windows is leaving on the table with its primitive support for large pages. It does support them, but it doesn&#x27;t coalesce pages transparently like Linux does, and explicitly allocating them requires special permissions and is very likely to fail due to fragmentation if the system has been running for a while. In practice it&#x27;s scarcely used outside of server software which immediately grabs a big chunk of large pages at boot and holds onto them forever.</div><br/><div id="41331533" class="c"><input type="checkbox" id="c-41331533" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331452">parent</a><span>|</span><a href="#41331520">next</a><span>|</span><label class="collapse" for="c-41331533">[-]</label><label class="expand" for="c-41331533">[12 more]</label></div><br/><div class="children"><div class="content">A lot of low level stuff is a lot slower on Windows, let alone the GUI. There&#x27;s also entire blogs cataloging an abundance of pathological performance issues.<p>The one I notice the most is the filesystem. Running Linux in VirtualBox, I got <i>7x</i> the host speed for many small file operations. (On top of that Explorer itself has its own random lag.)<p>I think a better question is how much performance are they leaving on the table by bloating the OS so much. Like they could have just not touched Explorer for 20 years and it would be 10x snappier now.<p>I think the number is closer to 100x actually. Explorer on XP opens (fully rendered) after a single video frame... also while running virtualized inside Win10.<p>Meanwhile Win10 Explorer opens after a noticeable delay, and then spends the next several hundred milliseconds painting the UI elements one by one...</div><br/><div id="41332138" class="c"><input type="checkbox" id="c-41332138" checked=""/><div class="controls bullet"><span class="by">nullindividual</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331533">parent</a><span>|</span><a href="#41331985">next</a><span>|</span><label class="collapse" for="c-41332138">[-]</label><label class="expand" for="c-41332138">[5 more]</label></div><br/><div class="children"><div class="content">&gt; The one I notice the most is the filesystem.<p>This is due to the extensible file system filter model in place; I&#x27;m not aware of another OS that implements this feature and is primarily used for antivirus, but can be used by any developer for any purpose.<p>It applies to all file systems on Windows.<p>DevDrive[0] is Microsoft&#x27;s current solution to this.<p>&gt; Meanwhile Win10 Explorer opens after a noticeable delay<p>This could be, again, largely due to 3rd party hooks (or 1st party software that doesn&#x27;t ship with Windows) into Explorer.<p>[0] <a href="https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;visualstudio&#x2F;devdrive&#x2F;" rel="nofollow">https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;visualstudio&#x2F;devdrive&#x2F;</a></div><br/><div id="41332705" class="c"><input type="checkbox" id="c-41332705" checked=""/><div class="controls bullet"><span class="by">redleader55</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41332138">parent</a><span>|</span><a href="#41332672">next</a><span>|</span><label class="collapse" for="c-41332705">[-]</label><label class="expand" for="c-41332705">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m not aware of another OS that implements this feature<p>I&#x27;m not sure this is exactly what you mean, but Linux has inotify and all sorts of BPF hooks for filtering various syscalls, for example file operations.</div><br/><div id="41333153" class="c"><input type="checkbox" id="c-41333153" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41332705">parent</a><span>|</span><a href="#41332672">next</a><span>|</span><label class="collapse" for="c-41333153">[-]</label><label class="expand" for="c-41333153">[1 more]</label></div><br/><div class="children"><div class="content">FSFilters are basically a custom kernel module that can and will do anything they want on any filesystem access. (There&#x27;s also network filters, which is how things like WinPcap get implemented.)<p>So yes, you could implement something similar in Linux, but there&#x27;s not, last I looked, a prebuilt toolkit and infrastructure for them, just the generic interfaces you can use to hook anything.<p>(Compare the difference between writing a BPF module to hook all FS operations, and the limitations of eBPF, to having an InterceptFSCalls struct that you define in your custom kernel module to run your own arbitrary code on every access.)</div><br/></div></div></div></div><div id="41332672" class="c"><input type="checkbox" id="c-41332672" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41332138">parent</a><span>|</span><a href="#41332705">prev</a><span>|</span><a href="#41331985">next</a><span>|</span><label class="collapse" for="c-41332672">[-]</label><label class="expand" for="c-41332672">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad you mentioned that. I noticed when running &quot;Hello world&quot; C program on Windows 10 that Windows performs over 100 reads of the Registry before running the program. Same thing when I right click a file...<p>A few of those are 3rd party, but most are not.</div><br/><div id="41332786" class="c"><input type="checkbox" id="c-41332786" checked=""/><div class="controls bullet"><span class="by">nullindividual</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41332672">parent</a><span>|</span><a href="#41331985">next</a><span>|</span><label class="collapse" for="c-41332786">[-]</label><label class="expand" for="c-41332786">[1 more]</label></div><br/><div class="children"><div class="content">Remember that Win32 process creation is expensive[0]. And on NT, processes don&#x27;t run, <i>threads do</i>.<p>The strategy of applications, like olde-tymey Apache using multiple processes to handle incoming connections is fine on UN*X, but terrible on Windows.<p>[0] <a href="https:&#x2F;&#x2F;fourcore.io&#x2F;blogs&#x2F;how-a-windows-process-is-created-part-2" rel="nofollow">https:&#x2F;&#x2F;fourcore.io&#x2F;blogs&#x2F;how-a-windows-process-is-created-p...</a></div><br/></div></div></div></div></div></div><div id="41331985" class="c"><input type="checkbox" id="c-41331985" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331533">parent</a><span>|</span><a href="#41332138">prev</a><span>|</span><a href="#41333436">next</a><span>|</span><label class="collapse" for="c-41331985">[-]</label><label class="expand" for="c-41331985">[3 more]</label></div><br/><div class="children"><div class="content">&gt; The one I notice the most is the filesystem<p>I’m not sure it’s the file system per se, I believe the main reason is the security model.<p>NT kernel has rather sophisticated security. The securable objects have security descriptors with many access control entries and auditing rules, which inherit over file system and other hierarchies according to some simple rules e.g. allow+deny=deny. Trustees are members of multiple security groups, and security groups can include other security groups so it’s not just a list, it’s a graph.<p>This makes access checks in NT relatively expensive. The kernel needs to perform access check every time a process creates or opens a file, that’s why CreateFile API function is relatively slow.</div><br/><div id="41333107" class="c"><input type="checkbox" id="c-41333107" checked=""/><div class="controls bullet"><span class="by">temac</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331985">parent</a><span>|</span><a href="#41333436">next</a><span>|</span><label class="collapse" for="c-41333107">[-]</label><label class="expand" for="c-41333107">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been trying to use auditing rules for a usage that seems completely in scope and obvious to prioritize from a security point of view (tracing access to EFS files and&#x2F;or the keys allowing the access) and my conclusion was that you basically can&#x27;t, the doc is garbage, the implementation is probably ad-hoc with lots of holes, and MS probably hasn&#x27;t prioritised the maintenance of this feature since several decades (too busy adding ads in the start menu I guess)<p>The NT security descriptors are also so complex they are probably a little useless in practice too, because it&#x27;s too hard to use correctly. On top of that the associated Win32 API is also too hard to use correctly to the point that I found an important bug in the usage model described in MSDN, meaning that the doc writer did not know how the function actually work (in tons of cases you probably don&#x27;t hit this case, but if you start digging in all internal and external users, who knows what you could find...)<p>NT was full of good ideas but the execution is often quite poor.</div><br/><div id="41335706" class="c"><input type="checkbox" id="c-41335706" checked=""/><div class="controls bullet"><span class="by">nullindividual</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41333107">parent</a><span>|</span><a href="#41333436">next</a><span>|</span><label class="collapse" for="c-41335706">[-]</label><label class="expand" for="c-41335706">[1 more]</label></div><br/><div class="children"><div class="content">From an NTFS auditing perspective, there’s no difference between auditing a non-EFS file or EFS file. Knowing that file auditing works just fine having done it many times, what makes you say it doesn’t work?</div><br/></div></div></div></div></div></div><div id="41333436" class="c"><input type="checkbox" id="c-41333436" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331533">parent</a><span>|</span><a href="#41331985">prev</a><span>|</span><a href="#41331805">next</a><span>|</span><label class="collapse" for="c-41333436">[-]</label><label class="expand" for="c-41333436">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The one I notice the most is the filesystem. Running Linux in VirtualBox, I got 7x the host speed for many small file operations. (On top of that Explorer itself has its own random lag.)<p>That’s a very old problem. In early days of subversion, the metadata for every directory existed in the directory. The rationale was that you could check out just a directory in svn. It was disastrously slow on Windows and the subversion maintainers had no answer for it, except insulting ones like “turn off virus scanning”. Telling a windows user to turn off virus scanning is equivalent to telling someone to play freeze tag in traffic. You might as well just tell them, “go fuck yourself with a rusty chainsaw”<p>Someone reorganized the data so it all happened at the root directory and the CLI just searched upward until it found the single metadata file. If memory serves that made large checkouts and updates about 2-3 times faster on Linux and 20x faster on windows.</div><br/></div></div><div id="41331805" class="c"><input type="checkbox" id="c-41331805" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331533">parent</a><span>|</span><a href="#41333436">prev</a><span>|</span><a href="#41331520">next</a><span>|</span><label class="collapse" for="c-41331805">[-]</label><label class="expand" for="c-41331805">[2 more]</label></div><br/><div class="children"><div class="content">None of this has to do with page size.</div><br/><div id="41332020" class="c"><input type="checkbox" id="c-41332020" checked=""/><div class="controls bullet"><span class="by">pantalaimon</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331805">parent</a><span>|</span><a href="#41331520">next</a><span>|</span><label class="collapse" for="c-41332020">[-]</label><label class="expand" for="c-41332020">[1 more]</label></div><br/><div class="children"><div class="content">Death by 1000 cuts</div><br/></div></div></div></div></div></div><div id="41331520" class="c"><input type="checkbox" id="c-41331520" checked=""/><div class="controls bullet"><span class="by">arghwhat</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331452">parent</a><span>|</span><a href="#41331533">prev</a><span>|</span><a href="#41332410">next</a><span>|</span><label class="collapse" for="c-41331520">[-]</label><label class="expand" for="c-41331520">[6 more]</label></div><br/><div class="children"><div class="content">Quite a bit, but 2M is an annoying size and the transparent handling is suboptimal. Without userspace cooperating, the kernel might end up having to split the pages at random due to an unfortunate unaligned munmap&#x2F;madvise from an application not realizing it was being served 2M pages.<p>Having Intel&#x2F;AMD add 16-128K page support, or making it common for userspace to explicitly ask for 2M pages for their heap arenas is likely better than the page merging logic. Less fragile.<p>1G pages are practically useless outside specialized server software as it is very difficult to find 1G contiguous memory to back it on a “normal” system that has been running for a while.</div><br/><div id="41333346" class="c"><input type="checkbox" id="c-41333346" checked=""/><div class="controls bullet"><span class="by">bewaretheirs</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331520">parent</a><span>|</span><a href="#41331785">next</a><span>|</span><label class="collapse" for="c-41333346">[-]</label><label class="expand" for="c-41333346">[2 more]</label></div><br/><div class="children"><div class="content">Intel&#x27;s menu of page sizes is an artifact of its page table structure.<p>On x86 in 64-bit mode, page table entries are 64 bits each; the lowest level in the hierarchy (L1) is a 4K page containing 512 64-bit of PTEs which in total map 2M of memory, which is not coincidentally the large page size.<p>The L1 page table pages are themselves found via a PTE in a L2 page table; one L2 page table page maps 512*2M = 1G of virtual address space, which is again, not coincidentally, the huge page size.<p>Large pages are mapped by a L2 PTE (sometimes called a PDE, &quot;page directory entry&quot;) with a particular bit set indicating that the PTE points at the large page rather than a PTE page.   The hardware page table walker just stops at that point.<p>And huge pages are similarly mapped by an L3 PTE with a bit set indicating that the L3 PTE is a huge page.<p>Shoehorning an intermediate size would complicate page table updates or walks or probably both.<p>Note that an OS can, of its own accord independent of hardware maintain allocations as a coarser granularity and sometimes get some savings out of this.  For one historic example, the VAX had a tiny 512-byte page size; IIRC, BSD unix pretended it had a 1K page size and always updated PTEs in pairs.</div><br/><div id="41336196" class="c"><input type="checkbox" id="c-41336196" checked=""/><div class="controls bullet"><span class="by">arghwhat</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41333346">parent</a><span>|</span><a href="#41331785">next</a><span>|</span><label class="collapse" for="c-41336196">[-]</label><label class="expand" for="c-41336196">[1 more]</label></div><br/><div class="children"><div class="content">Hmm? Pretending the page size is larger than it is would not yield the primary performance benefits of reduced TLB misses. Unless I am missing something, that seems more like a hack to save a tiny bit of kernel memory on a constrained system by having two PTE’s backed by the same internal page structure.<p>Unless we can change the size of the smallest page entry on Intel, I doubt there is room to do anything interesting there. If we could do like ARM and just multiply all the page sizes by 4 you would avoid any “shoehorning”.</div><br/></div></div></div></div><div id="41331785" class="c"><input type="checkbox" id="c-41331785" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331520">parent</a><span>|</span><a href="#41333346">prev</a><span>|</span><a href="#41331652">next</a><span>|</span><label class="collapse" for="c-41331785">[-]</label><label class="expand" for="c-41331785">[2 more]</label></div><br/><div class="children"><div class="content">Would a reasonable compromise be to change the base allocation granularity to 2MB, and transparently sub-allocate those 2MB blocks into 64KB blocks (the current Windows allocation granularity) when normal pages are requested? That feels like it should keep 2MB page fragmentation to a minimum without breaking existing software, but given they haven&#x27;t done it there&#x27;s probably some caveat I&#x27;m overlooking.</div><br/><div id="41331925" class="c"><input type="checkbox" id="c-41331925" checked=""/><div class="controls bullet"><span class="by">lanigone</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331785">parent</a><span>|</span><a href="#41331652">next</a><span>|</span><label class="collapse" for="c-41331925">[-]</label><label class="expand" for="c-41331925">[1 more]</label></div><br/><div class="children"><div class="content">you might find this interesting<p><a href="https:&#x2F;&#x2F;www.hudsonrivertrading.com&#x2F;hrtbeat&#x2F;low-latency-optimization-part-2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.hudsonrivertrading.com&#x2F;hrtbeat&#x2F;low-latency-optim...</a></div><br/></div></div></div></div></div></div><div id="41332410" class="c"><input type="checkbox" id="c-41332410" checked=""/><div class="controls bullet"><span class="by">tedunangst</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331452">parent</a><span>|</span><a href="#41331520">prev</a><span>|</span><a href="#41332577">next</a><span>|</span><label class="collapse" for="c-41332410">[-]</label><label class="expand" for="c-41332410">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve lost count of how many blog posts about poor performance ended with the punchline &quot;so then we turned off page coalescing&quot;.</div><br/></div></div></div></div><div id="41332577" class="c"><input type="checkbox" id="c-41332577" checked=""/><div class="controls bullet"><span class="by">daghamm</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331334">parent</a><span>|</span><a href="#41331452">prev</a><span>|</span><a href="#41331910">next</a><span>|</span><label class="collapse" for="c-41332577">[-]</label><label class="expand" for="c-41332577">[2 more]</label></div><br/><div class="children"><div class="content">IIRC, 64-bit ARM can do 4K, 16K, 64K and 2M  pages. But there are some special rules for the last one.<p><a href="https:&#x2F;&#x2F;documentation-service.arm.com&#x2F;static&#x2F;64d5f38f4a92140bb31d7f03" rel="nofollow">https:&#x2F;&#x2F;documentation-service.arm.com&#x2F;static&#x2F;64d5f38f4a92140...</a></div><br/><div id="41335102" class="c"><input type="checkbox" id="c-41335102" checked=""/><div class="controls bullet"><span class="by">sweetjuly</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41332577">parent</a><span>|</span><a href="#41331910">next</a><span>|</span><label class="collapse" for="c-41335102">[-]</label><label class="expand" for="c-41335102">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a little weirder. At least one translation granule is required but it is up to the implementation to choose which one(s) they want. Many older Arm cores only support 4KB and 64KB but newer ones support all three.<p>The size of the translation granule determines the size of the block entries at each level. So 4K granules has super pages of 2MB and 1GB, 16KB granules has 32MB super pages, and 64K has 512MB super pages.</div><br/></div></div></div></div></div></div><div id="41331910" class="c"><input type="checkbox" id="c-41331910" checked=""/><div class="controls bullet"><span class="by">HumblyTossed</span><span>|</span><a href="#41331152">parent</a><span>|</span><a href="#41331334">prev</a><span>|</span><a href="#41332194">next</a><span>|</span><label class="collapse" for="c-41331910">[-]</label><label class="expand" for="c-41331910">[2 more]</label></div><br/><div class="children"><div class="content">How is this &quot;additional background&quot;?  This was a post by Google regarding Android.</div><br/><div id="41335564" class="c"><input type="checkbox" id="c-41335564" checked=""/><div class="controls bullet"><span class="by">Kwpolska</span><span>|</span><a href="#41331152">root</a><span>|</span><a href="#41331910">parent</a><span>|</span><a href="#41332194">next</a><span>|</span><label class="collapse" for="c-41335564">[-]</label><label class="expand" for="c-41335564">[1 more]</label></div><br/><div class="children"><div class="content">That this isn&#x27;t the only 4K→16K transition in recent history? Some programs that assumed 4K had to be fixed as part of the transition, this can provide insights for the work required for Android.</div><br/></div></div></div></div></div></div><div id="41332194" class="c"><input type="checkbox" id="c-41332194" checked=""/><div class="controls bullet"><span class="by">eyalitki</span><span>|</span><a href="#41331152">prev</a><span>|</span><a href="#41331073">next</a><span>|</span><label class="collapse" for="c-41332194">[-]</label><label class="expand" for="c-41332194">[4 more]</label></div><br/><div class="children"><div class="content">RHEL tried that in that past with 64KB on AARCH64, it led to MANY bugs all across the software stack, and they eventually reverted it - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=27513209">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=27513209</a>.<p>I&#x27;m impressed by the effort on Google&#x27;s side, yet I&#x27;ll be surprised if this effort will pay off.</div><br/><div id="41332269" class="c"><input type="checkbox" id="c-41332269" checked=""/><div class="controls bullet"><span class="by">nektro</span><span>|</span><a href="#41332194">parent</a><span>|</span><a href="#41332309">next</a><span>|</span><label class="collapse" for="c-41332269">[-]</label><label class="expand" for="c-41332269">[1 more]</label></div><br/><div class="children"><div class="content">apple&#x27;s m-series chips use a 16kb page size by default so the state of things has improved significantly with software wanting to support asahi and other related endeavors</div><br/></div></div><div id="41332309" class="c"><input type="checkbox" id="c-41332309" checked=""/><div class="controls bullet"><span class="by">kcb</span><span>|</span><a href="#41332194">parent</a><span>|</span><a href="#41332269">prev</a><span>|</span><a href="#41333171">next</a><span>|</span><label class="collapse" for="c-41332309">[-]</label><label class="expand" for="c-41332309">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia is pushing 64KB pages on their Grace-Hopper system.</div><br/></div></div><div id="41333171" class="c"><input type="checkbox" id="c-41333171" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#41332194">parent</a><span>|</span><a href="#41332309">prev</a><span>|</span><a href="#41331073">next</a><span>|</span><label class="collapse" for="c-41333171">[-]</label><label class="expand" for="c-41333171">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t realize they had reverted it, I used to run RHEL builds on Pi systems to test for 64k page bugs because it&#x27;s not like there&#x27;s a POWER SBC I could buy for this.</div><br/></div></div></div></div><div id="41331073" class="c"><input type="checkbox" id="c-41331073" checked=""/><div class="controls bullet"><span class="by">monocasa</span><span>|</span><a href="#41332194">prev</a><span>|</span><a href="#41334716">next</a><span>|</span><label class="collapse" for="c-41331073">[-]</label><label class="expand" for="c-41331073">[25 more]</label></div><br/><div class="children"><div class="content">I wonder how much help they had by asahi doing a lot of the kernel and ecosystem work anablibg 16k pages.<p>RISC-V being fixed to 4k pages seems to be a bit of an oversight as well.</div><br/><div id="41331139" class="c"><input type="checkbox" id="c-41331139" checked=""/><div class="controls bullet"><span class="by">ashkankiani</span><span>|</span><a href="#41331073">parent</a><span>|</span><a href="#41331814">next</a><span>|</span><label class="collapse" for="c-41331139">[-]</label><label class="expand" for="c-41331139">[15 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty cool that I can read &quot;anablibg&quot; and know that means &quot;enabling.&quot; The brain is pretty neat. I wonder if LLMs would get it too. They probably would.</div><br/><div id="41331375" class="c"><input type="checkbox" id="c-41331375" checked=""/><div class="controls bullet"><span class="by">evilduck</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331139">parent</a><span>|</span><a href="#41331366">next</a><span>|</span><label class="collapse" for="c-41331375">[-]</label><label class="expand" for="c-41331375">[8 more]</label></div><br/><div class="children"><div class="content">Question I wrote:<p>&gt; I encountered the typo &quot;anablibg&quot; in the sentence &quot;I wonder how much help they had by asahi doing a lot of the kernel and ecosystem work anablibg 16k pages.&quot; What did they actually mean?<p>GPT-4o and Sonnet 3.5 understood it perfectly. This isn&#x27;t really a problem for the large models.<p>For local small models:<p>* Gemma2 9b did not get it and thought it meant &quot;analyzing&quot;.<p>* Codestral (22b) did not it get it and thought it meant &quot;allocating&quot;.<p>* Phi3 Mini failed spectacularly.<p>* Phi3 14b and Qwen2 did not get it and thought it was &quot;annotating&quot;.<p>* Mistral-nemo thought it was a portmanteau &quot;anabling&quot; as a combination of &quot;an&quot; and &quot;enabling&quot;. Partial credit for being close and some creativity?<p>* Llama3.1 got it perfectly.</div><br/><div id="41333750" class="c"><input type="checkbox" id="c-41333750" checked=""/><div class="controls bullet"><span class="by">treyd</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331375">parent</a><span>|</span><a href="#41331423">next</a><span>|</span><label class="collapse" for="c-41333750">[-]</label><label class="expand" for="c-41333750">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if they&#x27;d do better if there was the context that it&#x27;s in a thread titled &quot;Adding 16 kb page size to Android&quot;?  The &quot;analyzing&quot; interpretation is plausible if you don&#x27;t know what 16k pages, kernels, Asahi, etc are.</div><br/></div></div><div id="41331423" class="c"><input type="checkbox" id="c-41331423" checked=""/><div class="controls bullet"><span class="by">jandrese</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331375">parent</a><span>|</span><a href="#41333750">prev</a><span>|</span><a href="#41331748">next</a><span>|</span><label class="collapse" for="c-41331423">[-]</label><label class="expand" for="c-41331423">[2 more]</label></div><br/><div class="children"><div class="content">Seems like there is a bit of a roll of the dice there.  The ones that got it right may have just been lucky.</div><br/><div id="41332027" class="c"><input type="checkbox" id="c-41332027" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331423">parent</a><span>|</span><a href="#41331748">next</a><span>|</span><label class="collapse" for="c-41332027">[-]</label><label class="expand" for="c-41332027">[1 more]</label></div><br/><div class="children"><div class="content">Ran it a few times in new sessions, 0 failures so far.</div><br/></div></div></div></div><div id="41331748" class="c"><input type="checkbox" id="c-41331748" checked=""/><div class="controls bullet"><span class="by">slaymaker1907</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331375">parent</a><span>|</span><a href="#41331423">prev</a><span>|</span><a href="#41333104">next</a><span>|</span><label class="collapse" for="c-41331748">[-]</label><label class="expand" for="c-41331748">[1 more]</label></div><br/><div class="children"><div class="content">I wonder how much of a test this is for the LLM vs whatever tokenizer&#x2F;preprocessing they&#x27;re doing.</div><br/></div></div><div id="41333104" class="c"><input type="checkbox" id="c-41333104" checked=""/><div class="controls bullet"><span class="by">Alifatisk</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331375">parent</a><span>|</span><a href="#41331748">prev</a><span>|</span><a href="#41331879">next</a><span>|</span><label class="collapse" for="c-41333104">[-]</label><label class="expand" for="c-41333104">[2 more]</label></div><br/><div class="children"><div class="content">Is there any task Gemma is better at compared to others?</div><br/><div id="41334317" class="c"><input type="checkbox" id="c-41334317" checked=""/><div class="controls bullet"><span class="by">evilduck</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41333104">parent</a><span>|</span><a href="#41331879">next</a><span>|</span><label class="collapse" for="c-41334317">[-]</label><label class="expand" for="c-41334317">[1 more]</label></div><br/><div class="children"><div class="content">Local LLM topics are a treadmill of “what’s best and what is preferred” changing basically weekly to monthly, it’s a rapidly evolving field, but right now I actually tend to gravitate to Gemma2 9b for coding assistance for Typescript work or general question and answer stuff. Its embedded knowledge and speed on the computers that I have (32GB M2 Max, 16GB M1 Air, 4080 gaming desktop) make for a good balance while also using the computer for other stuff, bigger models limit what else I can run simultaneously and are slower than my reading speed, smaller models have less utility and the speed increase is pointless if they’re dumb.</div><br/></div></div></div></div><div id="41331879" class="c"><input type="checkbox" id="c-41331879" checked=""/><div class="controls bullet"><span class="by">Retr0id</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331375">parent</a><span>|</span><a href="#41333104">prev</a><span>|</span><a href="#41331366">next</a><span>|</span><label class="collapse" for="c-41331879">[-]</label><label class="expand" for="c-41331879">[1 more]</label></div><br/><div class="children"><div class="content">fwiw I failed to figure it out as a human, I had to check the replies.</div><br/></div></div></div></div><div id="41331366" class="c"><input type="checkbox" id="c-41331366" checked=""/><div class="controls bullet"><span class="by">mrbuttons454</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331139">parent</a><span>|</span><a href="#41331375">prev</a><span>|</span><a href="#41331221">next</a><span>|</span><label class="collapse" for="c-41331366">[-]</label><label class="expand" for="c-41331366">[1 more]</label></div><br/><div class="children"><div class="content">Until I read your comment I didn&#x27;t even notice...</div><br/></div></div><div id="41331221" class="c"><input type="checkbox" id="c-41331221" checked=""/><div class="controls bullet"><span class="by">mrob</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331139">parent</a><span>|</span><a href="#41331366">prev</a><span>|</span><a href="#41331400">next</a><span>|</span><label class="collapse" for="c-41331221">[-]</label><label class="expand" for="c-41331221">[4 more]</label></div><br/><div class="children"><div class="content">LLMs are at a great disadvantage here because they operate on tokens, not letters.</div><br/><div id="41331422" class="c"><input type="checkbox" id="c-41331422" checked=""/><div class="controls bullet"><span class="by">platelminto</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331221">parent</a><span>|</span><a href="#41331400">next</a><span>|</span><label class="collapse" for="c-41331422">[-]</label><label class="expand" for="c-41331422">[3 more]</label></div><br/><div class="children"><div class="content">I remember reading somewhere that LLMs are actually fantastic at reading heavily mistyped sentences! Mistyped to a level where humans actually struggle.<p>(I will update this comment if I find a source)</div><br/><div id="41331493" class="c"><input type="checkbox" id="c-41331493" checked=""/><div class="controls bullet"><span class="by">thanatropism</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331422">parent</a><span>|</span><a href="#41331400">next</a><span>|</span><label class="collapse" for="c-41331493">[-]</label><label class="expand" for="c-41331493">[2 more]</label></div><br/><div class="children"><div class="content">Tihs probably refers to comon mispelllings an typo&#x27;s.</div><br/><div id="41333020" class="c"><input type="checkbox" id="c-41333020" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331493">parent</a><span>|</span><a href="#41331400">next</a><span>|</span><label class="collapse" for="c-41333020">[-]</label><label class="expand" for="c-41333020">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s actually not. You can scramble every letter within words and it can mostly unscramble it. Keep the first letter and it recovers almost 100%.</div><br/></div></div></div></div></div></div></div></div><div id="41331400" class="c"><input type="checkbox" id="c-41331400" checked=""/><div class="controls bullet"><span class="by">im3w1l</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331139">parent</a><span>|</span><a href="#41331221">prev</a><span>|</span><a href="#41331814">next</a><span>|</span><label class="collapse" for="c-41331400">[-]</label><label class="expand" for="c-41331400">[1 more]</label></div><br/><div class="children"><div class="content">I asked chatgpt and it did get it.<p>Personally, when I read the comment my brain kinda skipped over the word since it contained the part &quot;lib&quot; I assumed it was some obscure library that I didn&#x27;t care about. It doesn&#x27;t fit grammatically but I didn&#x27;t give it enough thought to notice.</div><br/></div></div></div></div><div id="41331814" class="c"><input type="checkbox" id="c-41331814" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41331073">parent</a><span>|</span><a href="#41331139">prev</a><span>|</span><a href="#41334588">next</a><span>|</span><label class="collapse" for="c-41331814">[-]</label><label class="expand" for="c-41331814">[4 more]</label></div><br/><div class="children"><div class="content">Probably very little, since the Android ecosystem is quite divorced from the Linux one.</div><br/><div id="41332087" class="c"><input type="checkbox" id="c-41332087" checked=""/><div class="controls bullet"><span class="by">nabla9</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331814">parent</a><span>|</span><a href="#41334588">next</a><span>|</span><label class="collapse" for="c-41332087">[-]</label><label class="expand" for="c-41332087">[3 more]</label></div><br/><div class="children"><div class="content">Android kernel is a mainstream Linux kernel, with additional drivers, and other  functionality.</div><br/><div id="41334666" class="c"><input type="checkbox" id="c-41334666" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41332087">parent</a><span>|</span><a href="#41333173">next</a><span>|</span><label class="collapse" for="c-41334666">[-]</label><label class="expand" for="c-41334666">[1 more]</label></div><br/><div class="children"><div class="content">I am aware. This does not change what I said.</div><br/></div></div><div id="41333173" class="c"><input type="checkbox" id="c-41333173" checked=""/><div class="controls bullet"><span class="by">temac</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41332087">parent</a><span>|</span><a href="#41334666">prev</a><span>|</span><a href="#41334588">next</a><span>|</span><label class="collapse" for="c-41333173">[-]</label><label class="expand" for="c-41333173">[1 more]</label></div><br/><div class="children"><div class="content">The linux kernel already works perfectly fine with various base page sizes.</div><br/></div></div></div></div></div></div><div id="41334588" class="c"><input type="checkbox" id="c-41334588" checked=""/><div class="controls bullet"><span class="by">wren6991</span><span>|</span><a href="#41331073">parent</a><span>|</span><a href="#41331814">prev</a><span>|</span><a href="#41331182">next</a><span>|</span><label class="collapse" for="c-41334588">[-]</label><label class="expand" for="c-41334588">[1 more]</label></div><br/><div class="children"><div class="content">RV64 has some reserved encoding space in satp.mode so there&#x27;s an obvious path to expanding the number of page table formats at a later time. Just requires everyone to agree on the direction (common issue with RISC-V).<p>For RV32 I think we are probably stuck with Sv32 4k pages forever.</div><br/></div></div><div id="41331182" class="c"><input type="checkbox" id="c-41331182" checked=""/><div class="controls bullet"><span class="by">IshKebab</span><span>|</span><a href="#41331073">parent</a><span>|</span><a href="#41334588">prev</a><span>|</span><a href="#41334716">next</a><span>|</span><label class="collapse" for="c-41331182">[-]</label><label class="expand" for="c-41331182">[4 more]</label></div><br/><div class="children"><div class="content">Probably wouldn&#x27;t be too hard to add a 16 kB page size extension. But I think the Svnapot extension is their solution to this problem. If you&#x27;re not familiar it lets you mark a set of pages as being part of a contiguously mapped 64 kB region. No idea how the performance characteristics vary. It relieves TLB pressure, but you still have to create 16 4kB page table entries.</div><br/><div id="41331639" class="c"><input type="checkbox" id="c-41331639" checked=""/><div class="controls bullet"><span class="by">monocasa</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331182">parent</a><span>|</span><a href="#41334716">next</a><span>|</span><label class="collapse" for="c-41331639">[-]</label><label class="expand" for="c-41331639">[3 more]</label></div><br/><div class="children"><div class="content">Svnapot is a poor solution to the problem.<p>On one hand it means that that each page table entry takes up half a cache line for the 16KB case, and two whole cache lines in the 64KB case.  This really cuts down on the page walker hardware&#x27;s ability to effectively prefetch TLB entries, leading to basically the same issues as this classic discussion about why tree based page tables are generally more effective than hash based page tables (shifted forward in time to today&#x27;s gate counts).  <a href="https:&#x2F;&#x2F;yarchive.net&#x2F;comp&#x2F;linux&#x2F;page_tables.html" rel="nofollow">https:&#x2F;&#x2F;yarchive.net&#x2F;comp&#x2F;linux&#x2F;page_tables.html</a>  This is why ARM shifted from a Svnapot like solution to the &quot;translation granule queryable and partially selectable at runtime&quot; solution.<p>Another issue is the fact that a big reason to switch to 16KB or even 64KB pages is to allow for more address range for VIPT caches.  You want to allow high performance implementations to be able to look up the cache line while performing the TLB lookup in parallel, then compare the tag with the result of the TLB lookup.  This means that practically only the untranslated bits of the address can be used by the set selection portion of the cache lookup.  When you have 12 bits untranslated in a address, combined with 64 byte cachelines gives you 64 sets, multiply that by 8 ways and you get the 32KB L1 caches very common in systems with 4KB page sizes (sometimes with some heroic effort to throw a ton of transistors&#x2F;power at the problem to make a 64KB cache by essentially duplicating large parts of the cache lookup hardware for that extra bit of address).  What you really want is for the arch to be able to disallow 4KB pages like on apple silicon which is the main piece that allows their giant 128LB and 192KB L1 caches.</div><br/><div id="41332500" class="c"><input type="checkbox" id="c-41332500" checked=""/><div class="controls bullet"><span class="by">aseipp</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331639">parent</a><span>|</span><a href="#41334574">next</a><span>|</span><label class="collapse" for="c-41332500">[-]</label><label class="expand" for="c-41332500">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What you really want is for the arch to be able to disallow 4KB pages like on apple silicon which is the main piece that allows their giant 128LB and 192KB L1 caches.<p>Minor nit but they allow 4k pages. Linux doesn&#x27;t support 16k and 4k pages at the same time; macOS does but is just very particular about 4k pages being used for scenarios like Rosetta processes or virtual machines e.g. Parallels uses it for Windows-on-ARM, I think. Windows will probably never support non-4k pages I&#x27;d guess.<p>But otherwise, you&#x27;re totally right. I wish RISC-V had gone with the configurable granule approach like ARM did. Major missed opportunity but maybe a fix will get ratified at some point...</div><br/></div></div><div id="41334574" class="c"><input type="checkbox" id="c-41334574" checked=""/><div class="controls bullet"><span class="by">wren6991</span><span>|</span><a href="#41331073">root</a><span>|</span><a href="#41331639">parent</a><span>|</span><a href="#41332500">prev</a><span>|</span><a href="#41334716">next</a><span>|</span><label class="collapse" for="c-41334574">[-]</label><label class="expand" for="c-41334574">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  This means that practically only the untranslated bits of the address can be used by the set selection portion of the cache lookup<p>It&#x27;s true that this makes things difficult, but Arm have been shipping D caches with way size &gt; page size for decades. The problem you get is that virtual synonyms of the same physical cache block can become incoherent with one another. You solve this by extending your coherence protocol to cover the potential synonyms of each index in the set (so for example with 16 kB&#x2F;way and 4 kB pages, there are four potential indices for each physical cache block, and you need to maintain their coherence). It has some cost and the cost scales with the ratio of way size : page size, so it&#x27;s still desirable to stay under the limit, e.g. by just increasing the number of cache ways.</div><br/></div></div></div></div></div></div></div></div><div id="41334716" class="c"><input type="checkbox" id="c-41334716" checked=""/><div class="controls bullet"><span class="by">CalChris</span><span>|</span><a href="#41331073">prev</a><span>|</span><a href="#41333440">next</a><span>|</span><label class="collapse" for="c-41334716">[-]</label><label class="expand" for="c-41334716">[7 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  iOS has had 16K pages since forever.
  OSX switched to 16K pages in 2020 with the M1.
  Windows is stuck on 4K pages, even for AArch64.
  Linux has various page sizes. Asahi is 16K.</code></pre></div><br/><div id="41334819" class="c"><input type="checkbox" id="c-41334819" checked=""/><div class="controls bullet"><span class="by">nullindividual</span><span>|</span><a href="#41334716">parent</a><span>|</span><a href="#41333440">next</a><span>|</span><label class="collapse" for="c-41334819">[-]</label><label class="expand" for="c-41334819">[6 more]</label></div><br/><div class="children"><div class="content">Windows has 4K, 2M, and 1G page sizes on x86-64.</div><br/><div id="41334852" class="c"><input type="checkbox" id="c-41334852" checked=""/><div class="controls bullet"><span class="by">CalChris</span><span>|</span><a href="#41334716">root</a><span>|</span><a href="#41334819">parent</a><span>|</span><a href="#41334882">next</a><span>|</span><label class="collapse" for="c-41334852">[-]</label><label class="expand" for="c-41334852">[3 more]</label></div><br/><div class="children"><div class="content">Normal, large and huge. But default normal pages (which is what Android is changing) are 4K. FWIW, Itanium and Alpha had 8K default pages.<p><a href="https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;oldnewthing&#x2F;20210510-00&#x2F;?p=105200" rel="nofollow">https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;oldnewthing&#x2F;20210510-00&#x2F;?p=10...</a><p>I wonder why Microsoft stayed with 4K for AArch64.</div><br/><div id="41335591" class="c"><input type="checkbox" id="c-41335591" checked=""/><div class="controls bullet"><span class="by">Kwpolska</span><span>|</span><a href="#41334716">root</a><span>|</span><a href="#41334852">parent</a><span>|</span><a href="#41335728">next</a><span>|</span><label class="collapse" for="c-41335591">[-]</label><label class="expand" for="c-41335591">[1 more]</label></div><br/><div class="children"><div class="content">Microsoft wanted to make x86 compatibility as painless as possible. They adopted an ABI in which registers can be generally mapped 1:1 between the two architectures.</div><br/></div></div><div id="41335728" class="c"><input type="checkbox" id="c-41335728" checked=""/><div class="controls bullet"><span class="by">nullindividual</span><span>|</span><a href="#41334716">root</a><span>|</span><a href="#41334852">parent</a><span>|</span><a href="#41335591">prev</a><span>|</span><a href="#41334882">next</a><span>|</span><label class="collapse" for="c-41335728">[-]</label><label class="expand" for="c-41335728">[1 more]</label></div><br/><div class="children"><div class="content">I was confused as to why you were posting incorrect information when this thread already contained the correct information.</div><br/></div></div></div></div><div id="41334882" class="c"><input type="checkbox" id="c-41334882" checked=""/><div class="controls bullet"><span class="by">baby_souffle</span><span>|</span><a href="#41334716">root</a><span>|</span><a href="#41334819">parent</a><span>|</span><a href="#41334852">prev</a><span>|</span><a href="#41333440">next</a><span>|</span><label class="collapse" for="c-41334882">[-]</label><label class="expand" for="c-41334882">[2 more]</label></div><br/><div class="children"><div class="content">&gt; and 1G page sizes on x86-64.<p>I wonder who requested the 1G page size be implemented and what they use it for...</div><br/><div id="41335570" class="c"><input type="checkbox" id="c-41335570" checked=""/><div class="controls bullet"><span class="by">Kwpolska</span><span>|</span><a href="#41334716">root</a><span>|</span><a href="#41334882">parent</a><span>|</span><a href="#41333440">next</a><span>|</span><label class="collapse" for="c-41335570">[-]</label><label class="expand" for="c-41335570">[1 more]</label></div><br/><div class="children"><div class="content">Another thread says virtual machines.</div><br/></div></div></div></div></div></div></div></div><div id="41333440" class="c"><input type="checkbox" id="c-41333440" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#41334716">prev</a><span>|</span><a href="#41332804">next</a><span>|</span><label class="collapse" for="c-41333440">[-]</label><label class="expand" for="c-41333440">[4 more]</label></div><br/><div class="children"><div class="content">Now I wonder: Does increased page size have any negative impacts on I&#x2F;O performance or flash lifetime, e.g. for writebacks of dirty pages of memory-mapped files where only a small part was changed?<p>Or is the write granularity of modern managed flash devices (such as eMMCs as used in Android smartphones) much larger than either 4 or 16 kB anyway?</div><br/><div id="41333629" class="c"><input type="checkbox" id="c-41333629" checked=""/><div class="controls bullet"><span class="by">tadfisher</span><span>|</span><a href="#41333440">parent</a><span>|</span><a href="#41332804">next</a><span>|</span><label class="collapse" for="c-41333629">[-]</label><label class="expand" for="c-41333629">[3 more]</label></div><br/><div class="children"><div class="content">Flash controllers expose blocks of 512B or 4096KB, but the actual NAND chips operate in terms of &quot;erase blocks&quot; which range from 1MB to 8MB (or really anything); in these blocks, an individual bit can be flipped from &quot;0&quot; to &quot;1&quot; once, and flipping any bit back to &quot;0&quot; requires erasing the entire block and flipping the desired bits back to &quot;1&quot; [0].<p>All of this is hidden from the host by the NAND controller, and SSDs employ many strategies (including DRAM caching, heterogeneous NAND dies, wear-leveling and garbage-collection algorithms) to avoid wearing the storage NAND. Effectively you must treat flash storage devices as block devices of their advertised block size because you have no idea where your data ends up physically on the device, so any host-side algorithm is fairly worthless.<p>[0]: <a href="https:&#x2F;&#x2F;spdk.io&#x2F;doc&#x2F;ssd_internals.html" rel="nofollow">https:&#x2F;&#x2F;spdk.io&#x2F;doc&#x2F;ssd_internals.html</a></div><br/><div id="41333672" class="c"><input type="checkbox" id="c-41333672" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#41333440">root</a><span>|</span><a href="#41333629">parent</a><span>|</span><a href="#41332804">next</a><span>|</span><label class="collapse" for="c-41333672">[-]</label><label class="expand" for="c-41333672">[2 more]</label></div><br/><div class="children"><div class="content">Writes on NAND happen at the block, not the page level, though. I believe the ratio between the two is usually something like 1:8 or so.<p>Even blocks might still be larger than 4KB, but if they’re not, presumably a NAND controller could allow such smaller writes to avoid write amplification?<p>The mapping between physical and logical block address is complex anyway because of wear leveling and bad block management, so I don’t think there’s a need for write granularity to be the erase block&#x2F;page or even write block size.</div><br/><div id="41333839" class="c"><input type="checkbox" id="c-41333839" checked=""/><div class="controls bullet"><span class="by">to11mtm</span><span>|</span><a href="#41333440">root</a><span>|</span><a href="#41333672">parent</a><span>|</span><a href="#41332804">next</a><span>|</span><label class="collapse" for="c-41333839">[-]</label><label class="expand" for="c-41333839">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Even blocks might still be larger than 4KB, but if they’re not, presumably a NAND controller could allow such smaller writes to avoid write amplification?<p>Look at what SandForce was doing a decade+ ago. They had hardware compression to lower write amp and some sort of &#x27;battery backup&#x27; to ensure operations completed. Various bits of this sort of tech is in most decent drives now.<p>&gt; The mapping between physical and logical block address is complex anyway because of wear leveling and bad block management, so I don’t think there’s a need for write granularity to be the erase block&#x2F;page or even write block size.<p>The controller needs to know what blocks can get a clean write vs what needs an erase; that&#x27;s part of the trim&#x2F;gc process they do in background.<p>Assuming you have sufficient space, it works kinda like this:<p>- Writes are done to &#x27;free-free&#x27; area, i.e. parts of the flash it can treat like SLC for faster access and less wear. If you have less than 25%-ish of drive free this becomes a problem. Controller is tracking all of this state.<p>- When it&#x27;s got nothing better to do for a bit, controller will work to determine which old blocks to &#x27;rewrite&#x27; with data from the SLC-treated flash into &#x27;longer lived&#x27; but whatever-Level-cell storage. I&#x27;m guessing (hoping?) there&#x27;s a lot of fanciness going on there, i.e. frequently touched files take longer to get a full rewrite.<p>TBH sounds like a fun thing to research more</div><br/></div></div></div></div></div></div></div></div><div id="41332804" class="c"><input type="checkbox" id="c-41332804" checked=""/><div class="controls bullet"><span class="by">taeric</span><span>|</span><a href="#41333440">prev</a><span>|</span><a href="#41332425">next</a><span>|</span><label class="collapse" for="c-41332804">[-]</label><label class="expand" for="c-41332804">[2 more]</label></div><br/><div class="children"><div class="content">I see they have measured improvements in the performance of some things.  In particular, the camera app starts faster.  Small percentage, but still real.<p>Curious if there are any other changes you could do based on some of those learnings?  The camera app, in particular, seems like a good one to optimize to start instantly.  Especially so with the the shortcut &quot;double power key&quot; that many phones&#x2F;people have setup.<p>Specifically, I would expect you should be able to do something like the lisp norm of &quot;dump image?&quot;  Startup should then largely be loading the image, not executing much if any initialization code?  (Honestly, I mostly assume this already happens?)</div><br/><div id="41335946" class="c"><input type="checkbox" id="c-41335946" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41332804">parent</a><span>|</span><a href="#41332425">next</a><span>|</span><label class="collapse" for="c-41335946">[-]</label><label class="expand" for="c-41335946">[1 more]</label></div><br/><div class="children"><div class="content">A big part of the challenge for launching the camera app is getting the hardware ready and quickly freeing up RAM for image processing.</div><br/></div></div></div></div><div id="41332425" class="c"><input type="checkbox" id="c-41332425" checked=""/><div class="controls bullet"><span class="by">daghamm</span><span>|</span><a href="#41332804">prev</a><span>|</span><a href="#41332593">next</a><span>|</span><label class="collapse" for="c-41332425">[-]</label><label class="expand" for="c-41332425">[3 more]</label></div><br/><div class="children"><div class="content">Can someone explain those numbers to me?<p>5-10% performance boost sounds huge. Wouldn&#x27;t we have much larger TLBd if page walk was really this expensive?<p>On the other hand 9% increase in memory usage also sounds huge. How did this affect memory usage that much?</div><br/><div id="41332572" class="c"><input type="checkbox" id="c-41332572" checked=""/><div class="controls bullet"><span class="by">scottlamb</span><span>|</span><a href="#41332425">parent</a><span>|</span><a href="#41332593">next</a><span>|</span><label class="collapse" for="c-41332572">[-]</label><label class="expand" for="c-41332572">[2 more]</label></div><br/><div class="children"><div class="content">&gt; 5-10% performance boost sounds huge. Wouldn&#x27;t we have much larger TLBd if page walk was really this expensive?<p>It&#x27;s pretty typical for large programs to spend 15+% of their &quot;CPU time&quot; waiting for the TLB. [1] So larger pages really help, including changing the base 4 KiB -&gt; 16 KiB (4x reduction in TLB pressure) and using 2 MiB huge pages (512x reduction where it works out).<p>I&#x27;ve also wondered why the TLB isn&#x27;t larger.<p>&gt; On the other hand 9% increase in memory usage also sounds huge. How did this affect memory usage that much?<p>This is the granularity at which physical memory is assigned, and there are a lot of reasons most of a page might be wasted:<p>* The heap allocator will typically cram many things together in a page, but it might say only use a given page for allocations in a certain size range, so not all allocations will snuggle in next to each other.<p>* Program stacks each use at least one distinct page of physical RAM because they&#x27;re placed in distinct virtual address ranges with guard pages between. So if you have 1,024 threads, they used at least 4 MiB of RAM with 4 KiB pages, 16 MiB of RAM with 16 KiB pages.<p>* Anything from the filesystem that is cached in RAM ends up in the page cache, and true to the name, it has page granularity. So caching a 1-byte file would take 4 KiB before, 16 KiB after.<p>[1] If you have an Intel CPU, toplev is particularly nice for pointing this kind of thing out. <a href="https:&#x2F;&#x2F;github.com&#x2F;andikleen&#x2F;pmu-tools">https:&#x2F;&#x2F;github.com&#x2F;andikleen&#x2F;pmu-tools</a></div><br/><div id="41333495" class="c"><input type="checkbox" id="c-41333495" checked=""/><div class="controls bullet"><span class="by">95014_refugee</span><span>|</span><a href="#41332425">root</a><span>|</span><a href="#41332572">parent</a><span>|</span><a href="#41332593">next</a><span>|</span><label class="collapse" for="c-41333495">[-]</label><label class="expand" for="c-41333495">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ve also wondered why the TLB isn&#x27;t larger.<p>Fast CAMs are (relatively) expensive, is the excuse I always hear.</div><br/></div></div></div></div></div></div><div id="41332593" class="c"><input type="checkbox" id="c-41332593" checked=""/><div class="controls bullet"><span class="by">dboreham</span><span>|</span><a href="#41332425">prev</a><span>|</span><a href="#41331647">next</a><span>|</span><label class="collapse" for="c-41332593">[-]</label><label class="expand" for="c-41332593">[1 more]</label></div><br/><div class="children"><div class="content">Time to grab some THP popcorn...</div><br/></div></div><div id="41331647" class="c"><input type="checkbox" id="c-41331647" checked=""/><div class="controls bullet"><span class="by">lostmsu</span><span>|</span><a href="#41332593">prev</a><span>|</span><a href="#41333187">next</a><span>|</span><label class="collapse" for="c-41331647">[-]</label><label class="expand" for="c-41331647">[3 more]</label></div><br/><div class="children"><div class="content">Not entirely related (except the block size), but I am considering making and standardizing a system-wide content-based cache with default block size 16KB.<p>The idea is that you&#x27;d have a system-wide (or not) service that can do two or three things:<p>- read 16KB block by its SHA256 (also return length that can be &lt;16KB), if cached<p>- write a block to cache<p>- maybe pin a block (e.g. make it non-evictable)<p>I would be like a block-level file content dedup + eviction to keep the size limited.<p>Should reduce storage used by various things due to dedup functionality, but may require internet for corresponding apps to work properly.<p>With a peer-to-peer sharing system on top of it may significantly reduce storage requirements.<p>The only disadvantage is the same as with shared website caches prior to cache isolation introduction: apps can poke what you have in your cache and deduce some information about you from it.</div><br/><div id="41331688" class="c"><input type="checkbox" id="c-41331688" checked=""/><div class="controls bullet"><span class="by">monocasa</span><span>|</span><a href="#41331647">parent</a><span>|</span><a href="#41333783">next</a><span>|</span><label class="collapse" for="c-41331688">[-]</label><label class="expand" for="c-41331688">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d probably pick a size greater than 16KB for that.  Windows doesn&#x27;t expose translations less than 64KB in their version of mmap, and internally their file cache works in increments of 256KB.  And these were numbers they picked back in the 90s.</div><br/></div></div><div id="41333783" class="c"><input type="checkbox" id="c-41333783" checked=""/><div class="controls bullet"><span class="by">treyd</span><span>|</span><a href="#41331647">parent</a><span>|</span><a href="#41331688">prev</a><span>|</span><a href="#41333187">next</a><span>|</span><label class="collapse" for="c-41333783">[-]</label><label class="expand" for="c-41333783">[1 more]</label></div><br/><div class="children"><div class="content">I would go for higher than 16K.  I believe BitTorrent&#x27;s default minimum chunk size is 64K, for example.  It really depends on the use case in question though, if you&#x27;re doing random writes then larger chunk sizes quickly waste a ton of bandwidth, especially if you&#x27;re doing recursive rewrites of a tree structure.<p>Would a variable chunk size be acceptable for whatever it is you&#x27;re building?</div><br/></div></div></div></div><div id="41333187" class="c"><input type="checkbox" id="c-41333187" checked=""/><div class="controls bullet"><span class="by">quotemstr</span><span>|</span><a href="#41331647">prev</a><span>|</span><a href="#41332469">next</a><span>|</span><label class="collapse" for="c-41333187">[-]</label><label class="expand" for="c-41333187">[1 more]</label></div><br/><div class="children"><div class="content">Good. It&#x27;s about time. 4KB pages come down to us from 32-bit time immemorial. We didn&#x27;t bump the page size when we doubled the sizes of pointers and longs for the 64-bit transition. 4KB has been way too small for ages, and I&#x27;m glad we&#x27;re biting the minor compatibility bullet and adopting a page size more suited to modern computing.</div><br/></div></div><div id="41332469" class="c"><input type="checkbox" id="c-41332469" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#41333187">prev</a><span>|</span><label class="collapse" for="c-41332469">[-]</label><label class="expand" for="c-41332469">[4 more]</label></div><br/><div class="children"><div class="content">No mention of Apple on the page. Apple has been using 16K pages for years now.</div><br/><div id="41334644" class="c"><input type="checkbox" id="c-41334644" checked=""/><div class="controls bullet"><span class="by">zahlman</span><span>|</span><a href="#41332469">parent</a><span>|</span><label class="collapse" for="c-41334644">[-]</label><label class="expand" for="c-41334644">[3 more]</label></div><br/><div class="children"><div class="content">Why would an &quot;Android news&quot; blog mention what competitors are doing?</div><br/><div id="41334702" class="c"><input type="checkbox" id="c-41334702" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#41332469">root</a><span>|</span><a href="#41334644">parent</a><span>|</span><label class="collapse" for="c-41334702">[-]</label><label class="expand" for="c-41334702">[2 more]</label></div><br/><div class="children"><div class="content">Because the whole thing sounds like they’re doing something new, but they’re just catching up to something Apple has done back when they switched to aarch64.</div><br/><div id="41335292" class="c"><input type="checkbox" id="c-41335292" checked=""/><div class="controls bullet"><span class="by">deadlydose</span><span>|</span><a href="#41332469">root</a><span>|</span><a href="#41334702">parent</a><span>|</span><label class="collapse" for="c-41335292">[-]</label><label class="expand" for="c-41335292">[1 more]</label></div><br/><div class="children"><div class="content">A page right out of Apple&#x27;s playbook then.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>