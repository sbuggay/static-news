<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1737536470554" as="style"/><link rel="stylesheet" href="styles.css?v=1737536470554"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2501.06425">Tensor Product Attention Is All You Need</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>eunos</span> | <span>41 comments</span></div><br/><div><div id="42788722" class="c"><input type="checkbox" id="c-42788722" checked=""/><div class="controls bullet"><span class="by">carbocation</span><span>|</span><a href="#42789970">next</a><span>|</span><label class="collapse" for="c-42788722">[-]</label><label class="expand" for="c-42788722">[4 more]</label></div><br/><div class="children"><div class="content">My kingdom for renaming this paper to something like &quot;Tensor Product Attention is a Memory-Efficient Approach for Long-Sequence Language Modeling&quot;</div><br/><div id="42789287" class="c"><input type="checkbox" id="c-42789287" checked=""/><div class="controls bullet"><span class="by">Zacharias030</span><span>|</span><a href="#42788722">parent</a><span>|</span><a href="#42789970">next</a><span>|</span><label class="collapse" for="c-42789287">[-]</label><label class="expand" for="c-42789287">[3 more]</label></div><br/><div class="children"><div class="content">If you don’t like the title, wait till you see this acronym:
„… we introduce the
Tensor ProducT ATTenTion Transformer (T6), a new model architecture…“</div><br/><div id="42789493" class="c"><input type="checkbox" id="c-42789493" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#42788722">root</a><span>|</span><a href="#42789287">parent</a><span>|</span><a href="#42789970">next</a><span>|</span><label class="collapse" for="c-42789493">[-]</label><label class="expand" for="c-42789493">[2 more]</label></div><br/><div class="children"><div class="content">There is a famous transformer model named T5 from Google, and also S4, S4 and S6 (Mamba) in the LLM space, so it is not unusual naming.</div><br/><div id="42790119" class="c"><input type="checkbox" id="c-42790119" checked=""/><div class="controls bullet"><span class="by">black_puppydog</span><span>|</span><a href="#42788722">root</a><span>|</span><a href="#42789493">parent</a><span>|</span><a href="#42789970">next</a><span>|</span><label class="collapse" for="c-42790119">[-]</label><label class="expand" for="c-42790119">[1 more]</label></div><br/><div class="children"><div class="content">&quot;... is all you need&quot; isn&#x27;t <i>unusual</i> either, and yet GGP isn&#x27;t happy about it (and I understand why)</div><br/></div></div></div></div></div></div></div></div><div id="42789970" class="c"><input type="checkbox" id="c-42789970" checked=""/><div class="controls bullet"><span class="by">bbcc90</span><span>|</span><a href="#42788722">prev</a><span>|</span><a href="#42788709">next</a><span>|</span><label class="collapse" for="c-42789970">[-]</label><label class="expand" for="c-42789970">[2 more]</label></div><br/><div class="children"><div class="content">(trying to move the critique beyond the title...)<p>When trying to deploy llms in with larger context windows constrained environments 2 things start to hurt:
a) increased memory footprint for longer KV cache
b) increased decode speed due to longer context window. 
this paper addresses a) only, which is useful, but we are still left with b) (right?)</div><br/><div id="42790070" class="c"><input type="checkbox" id="c-42790070" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#42789970">parent</a><span>|</span><a href="#42788709">next</a><span>|</span><label class="collapse" for="c-42790070">[-]</label><label class="expand" for="c-42790070">[1 more]</label></div><br/><div class="children"><div class="content">The more meaningful contribution may be (section 3.4)<p>&gt; These variants illustrate TPA’s versatility in balancing memory cost, computational
overhead, and representation power. By choosing which dimensions (heads or tokens) remain contextual and adjusting ranks (RQ, RK, RV ), TPA unifies multiple existing attention mechanisms—
such as MHA, MQA, and GQA—under one framework, while potentially reducing the KV cache
size by an order of magnitude during autoregressive inference.<p>re: the title, it might be the true one if their proofs hold up<p>---<p>I&#x27;m now curious if the Element-wise Attention is All You Need preprint can be fit into this framework. Sadly my math is not currently up to the task. It appears to offer even better computational savings during both training and inference while maintaining accuracy, though only tested with a smaller model<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2501.05730" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2501.05730</a></div><br/></div></div></div></div><div id="42788709" class="c"><input type="checkbox" id="c-42788709" checked=""/><div class="controls bullet"><span class="by">whymauri</span><span>|</span><a href="#42789970">prev</a><span>|</span><a href="#42790103">next</a><span>|</span><label class="collapse" for="c-42788709">[-]</label><label class="expand" for="c-42788709">[14 more]</label></div><br/><div class="children"><div class="content">I really can&#x27;t with these paper titles anymore, man.</div><br/><div id="42788847" class="c"><input type="checkbox" id="c-42788847" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#42788709">parent</a><span>|</span><a href="#42790593">next</a><span>|</span><label class="collapse" for="c-42788847">[-]</label><label class="expand" for="c-42788847">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s an Ask HN thread going[1] asking about what people have done with small LLMs. This seems like a possible application. I asked Granite 3.1 MOE 3B to generate a title based on the abstract and it came up with:<p><i>Tensor Product Attention: A Memory-Efficient Solution for Longer Input Sequences in Language Models</i><p>Maybe a Greasemonkey script to pass arXiv abstracts to a local Ollama could be something...<p>[1]: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42784365">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42784365</a></div><br/></div></div><div id="42790593" class="c"><input type="checkbox" id="c-42790593" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#42788709">parent</a><span>|</span><a href="#42788847">prev</a><span>|</span><a href="#42788968">next</a><span>|</span><label class="collapse" for="c-42790593">[-]</label><label class="expand" for="c-42790593">[1 more]</label></div><br/><div class="children"><div class="content">And they don&#x27;t formally show that the titles are correct.</div><br/></div></div><div id="42788968" class="c"><input type="checkbox" id="c-42788968" checked=""/><div class="controls bullet"><span class="by">wisty</span><span>|</span><a href="#42788709">parent</a><span>|</span><a href="#42790593">prev</a><span>|</span><a href="#42789053">next</a><span>|</span><label class="collapse" for="c-42788968">[-]</label><label class="expand" for="c-42788968">[2 more]</label></div><br/><div class="children"><div class="content">Clickbait paper titles considered harmful?</div><br/><div id="42789056" class="c"><input type="checkbox" id="c-42789056" checked=""/><div class="controls bullet"><span class="by">gbnwl</span><span>|</span><a href="#42788709">root</a><span>|</span><a href="#42788968">parent</a><span>|</span><a href="#42789053">next</a><span>|</span><label class="collapse" for="c-42789056">[-]</label><label class="expand" for="c-42789056">[1 more]</label></div><br/><div class="children"><div class="content">OK I&#x27;ll admit I chuckled</div><br/></div></div></div></div><div id="42789053" class="c"><input type="checkbox" id="c-42789053" checked=""/><div class="controls bullet"><span class="by">anigbrowl</span><span>|</span><a href="#42788709">parent</a><span>|</span><a href="#42788968">prev</a><span>|</span><a href="#42788838">next</a><span>|</span><label class="collapse" for="c-42789053">[-]</label><label class="expand" for="c-42789053">[3 more]</label></div><br/><div class="children"><div class="content">By 2038 all scientific papers will be titled &#x27;Bruh.&#x27; While this might at first seem a recipe for confusion, the fundamental interconnectedness of all things as demonstrated by Ollama(Googol 13) highlight the fact that pretty much any insight is as good as any other and are all descriptions of the same underlying phenomenon. Freed from constraint like survival or the necessity to engage in economic activity, humanity in the 203s will mainly devote itself to contemplating amusing but fundamentally interchangeable perspectives within increasingly comfy pleasure cubes.</div><br/><div id="42789533" class="c"><input type="checkbox" id="c-42789533" checked=""/><div class="controls bullet"><span class="by">01HNNWZ0MV43FF</span><span>|</span><a href="#42788709">root</a><span>|</span><a href="#42789053">parent</a><span>|</span><a href="#42789524">next</a><span>|</span><label class="collapse" for="c-42789533">[-]</label><label class="expand" for="c-42789533">[1 more]</label></div><br/><div class="children"><div class="content">As foretold by Joseph Campbell</div><br/></div></div><div id="42789524" class="c"><input type="checkbox" id="c-42789524" checked=""/><div class="controls bullet"><span class="by">smlacy</span><span>|</span><a href="#42788709">root</a><span>|</span><a href="#42789053">parent</a><span>|</span><a href="#42789533">prev</a><span>|</span><a href="#42788838">next</a><span>|</span><label class="collapse" for="c-42789524">[-]</label><label class="expand" for="c-42789524">[1 more]</label></div><br/><div class="children"><div class="content">Bruh is all you need</div><br/></div></div></div></div><div id="42788838" class="c"><input type="checkbox" id="c-42788838" checked=""/><div class="controls bullet"><span class="by">ilove196884</span><span>|</span><a href="#42788709">parent</a><span>|</span><a href="#42789053">prev</a><span>|</span><a href="#42789581">next</a><span>|</span><label class="collapse" for="c-42788838">[-]</label><label class="expand" for="c-42788838">[5 more]</label></div><br/><div class="children"><div class="content">I hate how paper titles are worded like seo techniques.</div><br/><div id="42788934" class="c"><input type="checkbox" id="c-42788934" checked=""/><div class="controls bullet"><span class="by">spiritplumber</span><span>|</span><a href="#42788709">root</a><span>|</span><a href="#42788838">parent</a><span>|</span><a href="#42789058">next</a><span>|</span><label class="collapse" for="c-42788934">[-]</label><label class="expand" for="c-42788934">[1 more]</label></div><br/><div class="children"><div class="content">Turn something into a metric and it will be misused. Ever always was</div><br/></div></div><div id="42789058" class="c"><input type="checkbox" id="c-42789058" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#42788709">root</a><span>|</span><a href="#42788838">parent</a><span>|</span><a href="#42788934">prev</a><span>|</span><a href="#42789581">next</a><span>|</span><label class="collapse" for="c-42789058">[-]</label><label class="expand" for="c-42789058">[3 more]</label></div><br/><div class="children"><div class="content">This is a riff on the original &quot;attention is all you need&quot; paper, there has been a few of these lately</div><br/><div id="42790004" class="c"><input type="checkbox" id="c-42790004" checked=""/><div class="controls bullet"><span class="by">Matthyze</span><span>|</span><a href="#42788709">root</a><span>|</span><a href="#42789058">parent</a><span>|</span><a href="#42789581">next</a><span>|</span><label class="collapse" for="c-42790004">[-]</label><label class="expand" for="c-42790004">[2 more]</label></div><br/><div class="children"><div class="content">A few? A multitude.</div><br/><div id="42790106" class="c"><input type="checkbox" id="c-42790106" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#42788709">root</a><span>|</span><a href="#42790004">parent</a><span>|</span><a href="#42789581">next</a><span>|</span><label class="collapse" for="c-42790106">[-]</label><label class="expand" for="c-42790106">[1 more]</label></div><br/><div class="children"><div class="content">This one might be right if they have in fact unified multiple attention approaches into a single framework<p>see Section 3.4</div><br/></div></div></div></div></div></div></div></div><div id="42789581" class="c"><input type="checkbox" id="c-42789581" checked=""/><div class="controls bullet"><span class="by">byyoung3</span><span>|</span><a href="#42788709">parent</a><span>|</span><a href="#42788838">prev</a><span>|</span><a href="#42790103">next</a><span>|</span><label class="collapse" for="c-42789581">[-]</label><label class="expand" for="c-42789581">[1 more]</label></div><br/><div class="children"><div class="content">haha same</div><br/></div></div></div></div><div id="42790103" class="c"><input type="checkbox" id="c-42790103" checked=""/><div class="controls bullet"><span class="by">hangonhn</span><span>|</span><a href="#42788709">prev</a><span>|</span><a href="#42789049">next</a><span>|</span><label class="collapse" for="c-42790103">[-]</label><label class="expand" for="c-42790103">[3 more]</label></div><br/><div class="children"><div class="content">For those of us who are lay people outside of machine learning and AI, what was the critical insight that made “attention all you need” in the original Transformer paper?</div><br/><div id="42790563" class="c"><input type="checkbox" id="c-42790563" checked=""/><div class="controls bullet"><span class="by">danielbln</span><span>|</span><a href="#42790103">parent</a><span>|</span><a href="#42790587">next</a><span>|</span><label class="collapse" for="c-42790563">[-]</label><label class="expand" for="c-42790563">[1 more]</label></div><br/><div class="children"><div class="content">I believe the insight was the introduction of the attention mechanisms that allow the NN to look at all words (well, embeddings) in parallel and make connections between them, instead of processing things purely sequential.</div><br/></div></div><div id="42790587" class="c"><input type="checkbox" id="c-42790587" checked=""/><div class="controls bullet"><span class="by">freilanzer</span><span>|</span><a href="#42790103">parent</a><span>|</span><a href="#42790563">prev</a><span>|</span><a href="#42789049">next</a><span>|</span><label class="collapse" for="c-42790587">[-]</label><label class="expand" for="c-42790587">[1 more]</label></div><br/><div class="children"><div class="content">That attention works and is highly parallelisable.</div><br/></div></div></div></div><div id="42789049" class="c"><input type="checkbox" id="c-42789049" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#42790103">prev</a><span>|</span><a href="#42789836">next</a><span>|</span><label class="collapse" for="c-42789049">[-]</label><label class="expand" for="c-42789049">[6 more]</label></div><br/><div class="children"><div class="content">Tensor decomposition has traditionally suffered from high computational complexity. Is it an issue here?</div><br/><div id="42790542" class="c"><input type="checkbox" id="c-42790542" checked=""/><div class="controls bullet"><span class="by">davmre</span><span>|</span><a href="#42789049">parent</a><span>|</span><a href="#42789185">next</a><span>|</span><label class="collapse" for="c-42790542">[-]</label><label class="expand" for="c-42790542">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re not proposing to apply tensor decomposition to an existing collection of weights. It&#x27;s an architecture in which the K, V, and Q tensors are constructed as a product of factors. The model works with the factors directly and you just need to compute their product on the forward pass (and adjoints on the backwards pass), so there&#x27;s no decomposition.</div><br/></div></div><div id="42789185" class="c"><input type="checkbox" id="c-42789185" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#42789049">parent</a><span>|</span><a href="#42790542">prev</a><span>|</span><a href="#42789196">next</a><span>|</span><label class="collapse" for="c-42789185">[-]</label><label class="expand" for="c-42789185">[1 more]</label></div><br/><div class="children"><div class="content">My math is rusty, but it looks to have a higher complexity than the original attention. I cannot say if it is an issue. Generally it seems we are willing to spend more computation at training time if it produces better results at inference time. In this case they are reducing the resources needed at inference time (an order of magnitude for the KV cache) or enabling longer sequences given the same resources.<p>There&#x27;s another paper I saw yesterday, &quot;Element-wise Attention is All You Need&quot; which looks like an early preprint, written by a solo author with a solo A800, and tested on some smaller problems. If the results hold up for language benchmarks, it could reduce resource requirements during training as well. It looks to have a lower complexity when scaling<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2501.05730" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2501.05730</a></div><br/></div></div><div id="42789196" class="c"><input type="checkbox" id="c-42789196" checked=""/><div class="controls bullet"><span class="by">absolutelastone</span><span>|</span><a href="#42789049">parent</a><span>|</span><a href="#42789185">prev</a><span>|</span><a href="#42789241">next</a><span>|</span><label class="collapse" for="c-42789196">[-]</label><label class="expand" for="c-42789196">[2 more]</label></div><br/><div class="children"><div class="content">Looks like it&#x27;s just a matrix decomposition in the paper. I&#x27;m guessing anyway. These attention papers are always a painful mix of mathematical, quasi-mathematical, and information retrieval jargon.<p>There is something in the github repo about higher-order decompositions. Don&#x27;t find where the method for factoring is given.</div><br/><div id="42789213" class="c"><input type="checkbox" id="c-42789213" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#42789049">root</a><span>|</span><a href="#42789196">parent</a><span>|</span><a href="#42789241">next</a><span>|</span><label class="collapse" for="c-42789213">[-]</label><label class="expand" for="c-42789213">[1 more]</label></div><br/><div class="children"><div class="content">I chuckled when I read, in S-3.1<p>&gt; Specifically, for each token t, with a small abuse of notation, we define:</div><br/></div></div></div></div><div id="42789241" class="c"><input type="checkbox" id="c-42789241" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#42789049">parent</a><span>|</span><a href="#42789196">prev</a><span>|</span><a href="#42789836">next</a><span>|</span><label class="collapse" for="c-42789241">[-]</label><label class="expand" for="c-42789241">[1 more]</label></div><br/><div class="children"><div class="content">At a sniff test it would make sense.<p>Trading computational complexity for space.</div><br/></div></div></div></div><div id="42789836" class="c"><input type="checkbox" id="c-42789836" checked=""/><div class="controls bullet"><span class="by">thunkingdeep</span><span>|</span><a href="#42789049">prev</a><span>|</span><a href="#42789079">next</a><span>|</span><label class="collapse" for="c-42789836">[-]</label><label class="expand" for="c-42789836">[3 more]</label></div><br/><div class="children"><div class="content">If you don’t pay to read papers, you don’t get to complain about the titles, imo.<p>I hate ads, but I’m not paying for YouTube Premium either. That’s how it goes. I get ads.</div><br/><div id="42790296" class="c"><input type="checkbox" id="c-42790296" checked=""/><div class="controls bullet"><span class="by">Vampiero</span><span>|</span><a href="#42789836">parent</a><span>|</span><a href="#42790136">next</a><span>|</span><label class="collapse" for="c-42790296">[-]</label><label class="expand" for="c-42790296">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I hate ads, but I’m not paying for YouTube Premium either. That’s how it goes. I get ads.<p>No that&#x27;s not how it goes. You get Ublock Origin and then you don&#x27;t get ads. Simple as that.<p>If you don&#x27;t like ads and don&#x27;t fight against them it means you accept ads and want to see more of them shoved down our collective throat. At least from the perspective of marketers  and industries who rely on ads. That&#x27;s how we ended up in this predicament in the first place. Lazy compliance.<p>If Youtube isn&#x27;t sustainable without ads, it should die so that natural selection can take over and so that a better ecosystem can finally take its place. Every single &quot;Youtuber&quot; hates the platform, mostly because it has zero transparency being a Google product. The viewers hate it too because it constantly takes down their favorite videos and creators and because it&#x27;s full of ads.<p>The only reason it&#x27;s (still) the main site for hosting videos is quite literally just ad-fueled inertia due to the intrinsic cost of hosting videos. If ads didn&#x27;t exist the only sustainable solution would be something less centralized like Peertube. And to me that&#x27;s a desirable outcome.</div><br/></div></div><div id="42790136" class="c"><input type="checkbox" id="c-42790136" checked=""/><div class="controls bullet"><span class="by">black_puppydog</span><span>|</span><a href="#42789836">parent</a><span>|</span><a href="#42790296">prev</a><span>|</span><a href="#42789079">next</a><span>|</span><label class="collapse" for="c-42790136">[-]</label><label class="expand" for="c-42790136">[1 more]</label></div><br/><div class="children"><div class="content">Authors are from, let&#x27;s see, china and california. Then I guess a good chunk of the HN crowd is entitled to bitching about the title?</div><br/></div></div></div></div><div id="42789079" class="c"><input type="checkbox" id="c-42789079" checked=""/><div class="controls bullet"><span class="by">cute_boi</span><span>|</span><a href="#42789836">prev</a><span>|</span><a href="#42789277">next</a><span>|</span><label class="collapse" for="c-42789079">[-]</label><label class="expand" for="c-42789079">[4 more]</label></div><br/><div class="children"><div class="content">&gt; a novel attention mechanism<p>Why do every paper has to mention this word &quot;novel&quot; and these titles are getting crazier day by day.</div><br/><div id="42789198" class="c"><input type="checkbox" id="c-42789198" checked=""/><div class="controls bullet"><span class="by">verdverm</span><span>|</span><a href="#42789079">parent</a><span>|</span><a href="#42789702">next</a><span>|</span><label class="collapse" for="c-42789198">[-]</label><label class="expand" for="c-42789198">[1 more]</label></div><br/><div class="children"><div class="content">There are a number of papers which aim to improve the attention aspect of models, all being some derivation of the original &quot;Attention is All You Need&quot; paper. A pattern of &quot;&#x27;blank&#x27; Attention is All You Need&quot; has emerged</div><br/></div></div><div id="42789702" class="c"><input type="checkbox" id="c-42789702" checked=""/><div class="controls bullet"><span class="by">NitpickLawyer</span><span>|</span><a href="#42789079">parent</a><span>|</span><a href="#42789198">prev</a><span>|</span><a href="#42789207">next</a><span>|</span><label class="collapse" for="c-42789702">[-]</label><label class="expand" for="c-42789702">[1 more]</label></div><br/><div class="children"><div class="content">If your paper is scored &#x2F; gated on &quot;novel factor&quot; by admission committees, then applicants will over-use that term.</div><br/></div></div><div id="42789207" class="c"><input type="checkbox" id="c-42789207" checked=""/><div class="controls bullet"><span class="by">patrick451</span><span>|</span><a href="#42789079">parent</a><span>|</span><a href="#42789702">prev</a><span>|</span><a href="#42789277">next</a><span>|</span><label class="collapse" for="c-42789207">[-]</label><label class="expand" for="c-42789207">[1 more]</label></div><br/><div class="children"><div class="content">Because to publish in a real journal, you typically need both novelty and for your work to be &quot;interesting&quot;. The job of the abstract and introduction of a paper (where the word &quot;novel&quot; normally lives) is to sell the reviewer that the paper should be published and to sell you that you should read and cite it.</div><br/></div></div></div></div><div id="42789277" class="c"><input type="checkbox" id="c-42789277" checked=""/><div class="controls bullet"><span class="by">joshdavham</span><span>|</span><a href="#42789079">prev</a><span>|</span><label class="collapse" for="c-42789277">[-]</label><label class="expand" for="c-42789277">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sorry but can people please stop naming their papers &quot;X is all you need&quot;? It&#x27;s super annoying.</div><br/><div id="42789366" class="c"><input type="checkbox" id="c-42789366" checked=""/><div class="controls bullet"><span class="by">recursive</span><span>|</span><a href="#42789277">parent</a><span>|</span><a href="#42790322">next</a><span>|</span><label class="collapse" for="c-42789366">[-]</label><label class="expand" for="c-42789366">[2 more]</label></div><br/><div class="children"><div class="content">Are you saying... you consider it harmful?</div><br/><div id="42789909" class="c"><input type="checkbox" id="c-42789909" checked=""/><div class="controls bullet"><span class="by">pepinator</span><span>|</span><a href="#42789277">root</a><span>|</span><a href="#42789366">parent</a><span>|</span><a href="#42790322">next</a><span>|</span><label class="collapse" for="c-42789909">[-]</label><label class="expand" for="c-42789909">[1 more]</label></div><br/><div class="children"><div class="content">A more precise title would be better, so, yeah, it&#x27;s harmful.</div><br/></div></div></div></div><div id="42790322" class="c"><input type="checkbox" id="c-42790322" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#42789277">parent</a><span>|</span><a href="#42789366">prev</a><span>|</span><label class="collapse" for="c-42790322">[-]</label><label class="expand" for="c-42790322">[1 more]</label></div><br/><div class="children"><div class="content">Consider submitting an article titled &quot;all you need is considered harmful&quot;</div><br/></div></div></div></div></div></div></div></div></div></body></html>