<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1716973278411" as="style"/><link rel="stylesheet" href="styles.css?v=1716973278411"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/karpathy/llm.c/discussions/481">Reproducing GPT-2 in llm.c</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>tosh</span> | <span>93 comments</span></div><br/><div><div id="40502693" class="c"><input type="checkbox" id="c-40502693" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#40509643">next</a><span>|</span><label class="collapse" for="c-40502693">[-]</label><label class="expand" for="c-40502693">[43 more]</label></div><br/><div class="children"><div class="content">Hi HN the main (more detailed) article is here 
<a href="https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;llm.c&#x2F;discussions&#x2F;481">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;llm.c&#x2F;discussions&#x2F;481</a><p>Happy to answer questions!</div><br/><div id="40509833" class="c"><input type="checkbox" id="c-40509833" checked=""/><div class="controls bullet"><span class="by">sytelus</span><span>|</span><a href="#40502693">parent</a><span>|</span><a href="#40503358">next</a><span>|</span><label class="collapse" for="c-40509833">[-]</label><label class="expand" for="c-40509833">[1 more]</label></div><br/><div class="children"><div class="content">So, NanoGPT took 1.8 days on 8xA100 for 124M model training on 30.7B tokens using flash attention. This would translate to 14.4hr for 10B tokens. With llm.c it is ~1.5 hr which is almost 10X speedup!<p>Does this look ballpark correct? Is there any summary of where majority of this improvement comes from?</div><br/></div></div><div id="40503358" class="c"><input type="checkbox" id="c-40503358" checked=""/><div class="controls bullet"><span class="by">lagrange77</span><span>|</span><a href="#40502693">parent</a><span>|</span><a href="#40509833">prev</a><span>|</span><a href="#40502871">next</a><span>|</span><label class="collapse" for="c-40503358">[-]</label><label class="expand" for="c-40503358">[5 more]</label></div><br/><div class="children"><div class="content">Thank you for the effort you put in your educational work, it helped me and others a lot! In fact, i&#x27;m training my nanoGPT version right now. :)<p>&gt; Ultimately my interest in llm.c is to have a nice, clean, minimal, super dependency-light repo in direct C&#x2F;CUDA implementation, which I find aesthetically pleasing.<p>Also, it&#x27;s awesome that you spend your time on your passion.<p>Any plans on making a video series on llm.c? :D</div><br/><div id="40503512" class="c"><input type="checkbox" id="c-40503512" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40503358">parent</a><span>|</span><a href="#40502871">next</a><span>|</span><label class="collapse" for="c-40503512">[-]</label><label class="expand" for="c-40503512">[4 more]</label></div><br/><div class="children"><div class="content">Yes definitely. Related tweet of mine:<p><a href="https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1760388761349927356?lang=en" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1760388761349927356?lang=en</a><p>1. Build the thing<p>2. Build the ramp<p>Currently on step 1 :). It helps to build it first so you know where you are going, and then you can more easily re-build it when you&#x27;re vector pointed at the end result.</div><br/><div id="40508012" class="c"><input type="checkbox" id="c-40508012" checked=""/><div class="controls bullet"><span class="by">LorenzoGood</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40503512">parent</a><span>|</span><a href="#40503933">next</a><span>|</span><label class="collapse" for="c-40508012">[-]</label><label class="expand" for="c-40508012">[1 more]</label></div><br/><div class="children"><div class="content">I love when you leave your job.</div><br/></div></div><div id="40503933" class="c"><input type="checkbox" id="c-40503933" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40503512">parent</a><span>|</span><a href="#40508012">prev</a><span>|</span><a href="#40503739">next</a><span>|</span><label class="collapse" for="c-40503933">[-]</label><label class="expand" for="c-40503933">[1 more]</label></div><br/><div class="children"><div class="content">Everytime you take gardening leave, you build something new and interesting!</div><br/></div></div><div id="40503739" class="c"><input type="checkbox" id="c-40503739" checked=""/><div class="controls bullet"><span class="by">lagrange77</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40503512">parent</a><span>|</span><a href="#40503933">prev</a><span>|</span><a href="#40502871">next</a><span>|</span><label class="collapse" for="c-40503739">[-]</label><label class="expand" for="c-40503739">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s fantastic. My gradient field is pointing towards it.<p>Thank you again!</div><br/></div></div></div></div></div></div><div id="40502871" class="c"><input type="checkbox" id="c-40502871" checked=""/><div class="controls bullet"><span class="by">ngiyabonga</span><span>|</span><a href="#40502693">parent</a><span>|</span><a href="#40503358">prev</a><span>|</span><a href="#40504486">next</a><span>|</span><label class="collapse" for="c-40502871">[-]</label><label class="expand" for="c-40502871">[4 more]</label></div><br/><div class="children"><div class="content">Hi Andrej!<p>First, thank you for your teaching, it has helped me a lot, didn&#x27;t think I&#x27;d ever have the chance to say thank you, but here you are and I hope this gets to you!<p>Question - what&#x27;s a relevant (05-2024) baseline to compare the performance of c code to? Back when you made nanoGPT you were seeing &quot;the file train.py reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training&quot;. So twice the memory on the c node, but unsure of data size &#x2F;epochs, any other details I may be missing. I.e. what&#x27;s the net uplift of running c vs &quot;legacy&quot; torch code?<p>Thanks again for everything.</div><br/><div id="40503033" class="c"><input type="checkbox" id="c-40503033" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40502871">parent</a><span>|</span><a href="#40504486">next</a><span>|</span><label class="collapse" for="c-40503033">[-]</label><label class="expand" for="c-40503033">[3 more]</label></div><br/><div class="children"><div class="content">The baseline is definitely PyTorch (or JAX), and indeed something like nanoGPT. I just never got nanoGPT &quot;past the finish line&quot; of really crossing the t&#x27;s and dotting the i&#x27;s and reproducing the models with as much care as I did now and here in llm.c, and getting to the point where it&#x27;s a single launch command that just does the thing.<p>I think I&#x27;ll try to develop the `train_gpt2.py` inside llm.c to be that, so that we have the two implementations exactly side by side, and it&#x27;s all nice and comparable.<p>The C&#x2F;CUDA code is currently a little bit faster than PyTorch (last time I measured ~2 weeks ago it was about 6% faster), and I think we can push this further. This is done by manually hard-coding a bunch of fusions&#x2F;optimizations that are non-trivial for torch.compile to find (e.g. our FusedClassifier). But PyTorch has some pending work&#x2F;PRs that will also speed up their side a lot.<p>Ultimately my interest in llm.c is to have a nice, clean, minimal, super dependency-light repo in direct C&#x2F;CUDA implementation, which I find aesthetically pleasing. And on top of that, educational, i.e. using all of the above as an endpoint of an intro LLM course.</div><br/><div id="40506290" class="c"><input type="checkbox" id="c-40506290" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40503033">parent</a><span>|</span><a href="#40506422">next</a><span>|</span><label class="collapse" for="c-40506290">[-]</label><label class="expand" for="c-40506290">[1 more]</label></div><br/><div class="children"><div class="content">Just out of curiosity, how do you feel about Tinygrad? They just released 0.9 and are also on the HN home page today.</div><br/></div></div><div id="40506422" class="c"><input type="checkbox" id="c-40506422" checked=""/><div class="controls bullet"><span class="by">raymond_goo</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40503033">parent</a><span>|</span><a href="#40506290">prev</a><span>|</span><a href="#40504486">next</a><span>|</span><label class="collapse" for="c-40506422">[-]</label><label class="expand" for="c-40506422">[1 more]</label></div><br/><div class="children"><div class="content">Maybe talk to MasterClass...</div><br/></div></div></div></div></div></div><div id="40504486" class="c"><input type="checkbox" id="c-40504486" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#40502693">parent</a><span>|</span><a href="#40502871">prev</a><span>|</span><a href="#40504909">next</a><span>|</span><label class="collapse" for="c-40504486">[-]</label><label class="expand" for="c-40504486">[8 more]</label></div><br/><div class="children"><div class="content">Ok, we&#x27;ve changed the URL to that from <a href="https:&#x2F;&#x2F;twitter.com&#x2F;karpathy&#x2F;status&#x2F;1795484547267834137" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;karpathy&#x2F;status&#x2F;1795484547267834137</a> above. Thanks!</div><br/><div id="40504739" class="c"><input type="checkbox" id="c-40504739" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40504486">parent</a><span>|</span><a href="#40506549">next</a><span>|</span><label class="collapse" for="c-40504739">[-]</label><label class="expand" for="c-40504739">[5 more]</label></div><br/><div class="children"><div class="content">sounds good. both work, (though) I think HN has a bit of an anti-twitter bias.</div><br/><div id="40508339" class="c"><input type="checkbox" id="c-40508339" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40504739">parent</a><span>|</span><a href="#40506439">next</a><span>|</span><label class="collapse" for="c-40508339">[-]</label><label class="expand" for="c-40508339">[1 more]</label></div><br/><div class="children"><div class="content">I agree - Twitter is still the primary source for a lot of original work and original thoughts. Unfortunately it&#x27;s gotten more complicated because (1) the threads there have gotten less accessible and (2) some people have assigned the entire site to one side of the culture war.</div><br/></div></div><div id="40506439" class="c"><input type="checkbox" id="c-40506439" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40504739">parent</a><span>|</span><a href="#40508339">prev</a><span>|</span><a href="#40506549">next</a><span>|</span><label class="collapse" for="c-40506439">[-]</label><label class="expand" for="c-40506439">[3 more]</label></div><br/><div class="children"><div class="content">First, love the videos and other work you&#x27;ve been doing. The micrograd videos are a great way to show people this is all math in the end, and I&#x27;ve linked to specific timestamps in that video and others more times than I can count.<p>For why I think we have a anti-twitter bias...<p>Twitter doesn&#x27;t show replies or any further context without being logged in. Most people will have accounts but I know a lot here deleted theirs or refuse to use it for one reason or another.<p>Also IMO most here are going to want to read the full source so it just cuts out the middleman. This would usually fall under the &quot;Please submit the original source. If a post reports on something found on another site, submit the latter.&quot; guideline which is a little different since the source is yourself, but still the Twitter post doesn&#x27;t add anything new or novel.</div><br/><div id="40507297" class="c"><input type="checkbox" id="c-40507297" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40506439">parent</a><span>|</span><a href="#40506549">next</a><span>|</span><label class="collapse" for="c-40507297">[-]</label><label class="expand" for="c-40507297">[2 more]</label></div><br/><div class="children"><div class="content">fwiw I totally understand the sentiment! it&#x27;s actually a bit sad to me that so much of our content is moving from the shared, open web to platforms like twitter, unfortunately there seems to be too much value add around built-in discoverability, comments, ease of authoring, for many people revenue sharing, etc.</div><br/><div id="40508380" class="c"><input type="checkbox" id="c-40508380" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40507297">parent</a><span>|</span><a href="#40506549">next</a><span>|</span><label class="collapse" for="c-40508380">[-]</label><label class="expand" for="c-40508380">[1 more]</label></div><br/><div class="children"><div class="content">Yes, definitely. I had to double check your age (apologies! feels rude somehow) and yep, we&#x27;re basically the same age. The web was different back then. Maybe not better; maybe that&#x27;s nostalgia. But never before has more creators had as many tools and avenues to promote and monotonize their work as they do now.</div><br/></div></div></div></div></div></div></div></div><div id="40506549" class="c"><input type="checkbox" id="c-40506549" checked=""/><div class="controls bullet"><span class="by">wrboyce</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40504486">parent</a><span>|</span><a href="#40504739">prev</a><span>|</span><a href="#40504909">next</a><span>|</span><label class="collapse" for="c-40506549">[-]</label><label class="expand" for="c-40506549">[2 more]</label></div><br/><div class="children"><div class="content">Could you mention what the link has been changed from too? Sometimes it helps with context when reading the comments. Thanks!</div><br/><div id="40508325" class="c"><input type="checkbox" id="c-40508325" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40506549">parent</a><span>|</span><a href="#40504909">next</a><span>|</span><label class="collapse" for="c-40508325">[-]</label><label class="expand" for="c-40508325">[1 more]</label></div><br/><div class="children"><div class="content">I agree that it helps! but I did mention it, no?  Admittedly &quot;to that from&quot; is a bit of an awkward construction</div><br/></div></div></div></div></div></div><div id="40504909" class="c"><input type="checkbox" id="c-40504909" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#40502693">parent</a><span>|</span><a href="#40504486">prev</a><span>|</span><a href="#40509541">next</a><span>|</span><label class="collapse" for="c-40504909">[-]</label><label class="expand" for="c-40504909">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Keep in mind that here we trained for 10B tokens, while GPT-3 models were all trained for 300B tokens. [...] GPT-3 actually didn&#x27;t change too much at all about the model (context size 1024 -&gt; 2048, I think that&#x27;s it?).<p>Andrej, based on that do you have a rough cost estimate for what it would take to train a GPT-3 Ada (350M)? Do you plan to get there with llm.c ?</div><br/><div id="40504950" class="c"><input type="checkbox" id="c-40504950" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40504909">parent</a><span>|</span><a href="#40509541">next</a><span>|</span><label class="collapse" for="c-40504950">[-]</label><label class="expand" for="c-40504950">[2 more]</label></div><br/><div class="children"><div class="content">The 350M model I trained last night was 30B tokens, 14 hours, ~$200.
Conveniently, 300B is exactly 10X the tokens so ~$2K would be the estimate. You&#x27;d have to wait 140 hours on one box though. Getting an H100 box instead of A100 will already cut the time latency down probably by a factor of 2-3X, for free, even without going to fp8 (which we do plan to support).<p>So TLDR at this model scale, llm.c is already there functionally, I think, it&#x27;s a matter of the compute resources and patience. I currently have this one box from Lambda and I have to look around for a few more boxes and merge the pending PR for multi-node training support. Getting all of this into a nice, stable state is probably a good chunk of the pending work right now.</div><br/></div></div></div></div><div id="40509541" class="c"><input type="checkbox" id="c-40509541" checked=""/><div class="controls bullet"><span class="by">0x1ceb00da</span><span>|</span><a href="#40502693">parent</a><span>|</span><a href="#40504909">prev</a><span>|</span><a href="#40506712">next</a><span>|</span><label class="collapse" for="c-40509541">[-]</label><label class="expand" for="c-40509541">[1 more]</label></div><br/><div class="children"><div class="content">Hi. Is it possible to somehow run llm.c on an amd gpu?</div><br/></div></div><div id="40506712" class="c"><input type="checkbox" id="c-40506712" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#40502693">parent</a><span>|</span><a href="#40509541">prev</a><span>|</span><a href="#40502821">next</a><span>|</span><label class="collapse" for="c-40506712">[-]</label><label class="expand" for="c-40506712">[1 more]</label></div><br/><div class="children"><div class="content">Would you consider switching your interest to protein structure prediction?  In particular, the current most advanced model is a closed-source, closed-weights system that was trained on a proprietary hardware.  It is intentionally kept that way for now to enable deepmind to commercialize their product.<p>The goal here isn&#x27;t to make the best performing model: it&#x27;s ablation.  How much can we remove from protein structure prediction (such as multiple sequence alignments and molecular dynamics, which were two improvements in AF3), while still having a generalized model that can predict novel folds.<p>Then focus on teaching the minimal necessary math and code to reproduce the results to the larger biological community.  All I can say about AF3 is that it literally taught me that everything I learned about protein structure prediction in the last 30 years was misguided, or outright wrong.<p>Don&#x27;t worry about drug discovery or any of the hard stuff.  Just continue to show that all that&#x27;s required to predict novel structures is the existing PDB.</div><br/></div></div><div id="40502821" class="c"><input type="checkbox" id="c-40502821" checked=""/><div class="controls bullet"><span class="by">1024core</span><span>|</span><a href="#40502693">parent</a><span>|</span><a href="#40506712">prev</a><span>|</span><a href="#40505007">next</a><span>|</span><label class="collapse" for="c-40502821">[-]</label><label class="expand" for="c-40502821">[1 more]</label></div><br/><div class="children"><div class="content">Thank you, from an appreciative reader!</div><br/></div></div><div id="40505007" class="c"><input type="checkbox" id="c-40505007" checked=""/><div class="controls bullet"><span class="by">localhost</span><span>|</span><a href="#40502693">parent</a><span>|</span><a href="#40502821">prev</a><span>|</span><a href="#40508631">next</a><span>|</span><label class="collapse" for="c-40505007">[-]</label><label class="expand" for="c-40505007">[3 more]</label></div><br/><div class="children"><div class="content">How large is the set of binaries needed to do this training job? The current pytorch + CUDA ecosystem is so incredibly gigantic and manipulating those container images is painful because they are so large. I was hopeful that this would be the beginnings of a much smaller training&#x2F;fine-tuning stack?</div><br/><div id="40505292" class="c"><input type="checkbox" id="c-40505292" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40505007">parent</a><span>|</span><a href="#40508631">next</a><span>|</span><label class="collapse" for="c-40505292">[-]</label><label class="expand" for="c-40505292">[2 more]</label></div><br/><div class="children"><div class="content">That is 100% my intention and hope and I think we are very close to deleting all of that. Right now on master, I am already only using Python for the tokenization preprocessing. In principle the requirements for llm.c should be extremely minimal. I think this a few days of work that is high on my mind.<p>Biggest problem right now is finding a place that can host the 135GB of tokens for FineWeb100B. Will probably use S3 or something.<p>Related see: 
<a href="https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;llm.c&#x2F;issues&#x2F;482">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;llm.c&#x2F;issues&#x2F;482</a></div><br/><div id="40506672" class="c"><input type="checkbox" id="c-40506672" checked=""/><div class="controls bullet"><span class="by">metadat</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40505292">parent</a><span>|</span><a href="#40508631">next</a><span>|</span><label class="collapse" for="c-40506672">[-]</label><label class="expand" for="c-40506672">[1 more]</label></div><br/><div class="children"><div class="content">Could this be a good case for a torrent?</div><br/></div></div></div></div></div></div><div id="40508631" class="c"><input type="checkbox" id="c-40508631" checked=""/><div class="controls bullet"><span class="by">jonesn11</span><span>|</span><a href="#40502693">parent</a><span>|</span><a href="#40505007">prev</a><span>|</span><a href="#40503046">next</a><span>|</span><label class="collapse" for="c-40508631">[-]</label><label class="expand" for="c-40508631">[1 more]</label></div><br/><div class="children"><div class="content">Like the FAQ, you correctly anticipated my questions.</div><br/></div></div><div id="40503046" class="c"><input type="checkbox" id="c-40503046" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#40502693">parent</a><span>|</span><a href="#40508631">prev</a><span>|</span><a href="#40503519">next</a><span>|</span><label class="collapse" for="c-40503046">[-]</label><label class="expand" for="c-40503046">[4 more]</label></div><br/><div class="children"><div class="content">How big of a perf improvement would result from using the architectural tweaks that Llama3 and others have put in place since GPT-2?</div><br/><div id="40503324" class="c"><input type="checkbox" id="c-40503324" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40503046">parent</a><span>|</span><a href="#40503519">next</a><span>|</span><label class="collapse" for="c-40503324">[-]</label><label class="expand" for="c-40503324">[3 more]</label></div><br/><div class="children"><div class="content">My understanding and suspicion is mostly less than you think. Llama 3 architecture has the following changes on GPT-2:<p>1. delete the absolute positional encoding and replace with RoPE<p>2. delete all biases in all layers (in LayerNorms, they 
turn into RMSNorm)<p>3. GeLU -&gt; SwiGLU non-linearity in the MLP<p>4. longer context length<p>5. architecture hyperparameter changes, e.g. slightly different aspect ratios<p>And there was a paper that I can&#x27;t find the reference to anymore that claimed that if you train long enough, the gap becomes even lower. Possibly because the absolutely positional encoding has enough time to train more fully, where as the RoPE layer benefits from the &quot;inductive bias&quot; it adds in the earlier stages of training.<p>But I don&#x27;t have full confidence on the above claim, maybe someone has tried or has better&#x2F;concrete reference.</div><br/><div id="40504843" class="c"><input type="checkbox" id="c-40504843" checked=""/><div class="controls bullet"><span class="by">jorlow</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40503324">parent</a><span>|</span><a href="#40503519">next</a><span>|</span><label class="collapse" for="c-40504843">[-]</label><label class="expand" for="c-40504843">[2 more]</label></div><br/><div class="children"><div class="content">Note llama&#x27;s feed forward is a bit different too:<p><pre><code>  self.w2(F.silu(self.w1(x)) * self.w3(x))
</code></pre>
I.e. the nonlinearity is a gate.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;meta-llama&#x2F;llama3&#x2F;blob&#x2F;14aab0428d3ec3a9596f1dea06d9c564f9c0e35f&#x2F;llama&#x2F;model.py#L219">https:&#x2F;&#x2F;github.com&#x2F;meta-llama&#x2F;llama3&#x2F;blob&#x2F;14aab0428d3ec3a959...</a></div><br/><div id="40508400" class="c"><input type="checkbox" id="c-40508400" checked=""/><div class="controls bullet"><span class="by">soraki_soladead</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40504843">parent</a><span>|</span><a href="#40503519">next</a><span>|</span><label class="collapse" for="c-40508400">[-]</label><label class="expand" for="c-40508400">[1 more]</label></div><br/><div class="children"><div class="content">Fwiw, that&#x27;s SwiGLU in #3 above. Swi = Swish = silu. GLU is gated linear unit; the gate construction you describe.</div><br/></div></div></div></div></div></div></div></div><div id="40503519" class="c"><input type="checkbox" id="c-40503519" checked=""/><div class="controls bullet"><span class="by">363849473754</span><span>|</span><a href="#40502693">parent</a><span>|</span><a href="#40503046">prev</a><span>|</span><a href="#40502913">next</a><span>|</span><label class="collapse" for="c-40503519">[-]</label><label class="expand" for="c-40503519">[4 more]</label></div><br/><div class="children"><div class="content">You might have covered this topic before, but I&#x27;m curious about the main performance differences between nanoGPT and llm.c. I&#x27;m planning to take your &quot;Zero to Hero&quot; course, and I&#x27;d like to know how capable the nanoGPT chatbot you&#x27;ll build is. Is its quality comparable to GPT-2 when used as a chatbot?</div><br/><div id="40503665" class="c"><input type="checkbox" id="c-40503665" checked=""/><div class="controls bullet"><span class="by">karpathy</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40503519">parent</a><span>|</span><a href="#40502913">next</a><span>|</span><label class="collapse" for="c-40503665">[-]</label><label class="expand" for="c-40503665">[3 more]</label></div><br/><div class="children"><div class="content">Zero To Hero doesn&#x27;t make it all the way to a chatbot, it stops at pretraining, and even that at a fairly small scale or character-level transformer on TinyShakespeare. I think it&#x27;s a good conceptual intro but you don&#x27;t get too too far as a competent chatbot. I think I should be able to improve on this soon.</div><br/><div id="40504285" class="c"><input type="checkbox" id="c-40504285" checked=""/><div class="controls bullet"><span class="by">363849473754</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40503665">parent</a><span>|</span><a href="#40504716">next</a><span>|</span><label class="collapse" for="c-40504285">[-]</label><label class="expand" for="c-40504285">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! So, you are considering expanding the Zero to Hero series to include building a basic GPT-2 toy chatbot? I believe you mentioned in one of the early lectures that you planned to include building a toy version of Dalle. Do you still have plans for that as well?</div><br/></div></div><div id="40504716" class="c"><input type="checkbox" id="c-40504716" checked=""/><div class="controls bullet"><span class="by">maskil</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40503665">parent</a><span>|</span><a href="#40504285">prev</a><span>|</span><a href="#40502913">next</a><span>|</span><label class="collapse" for="c-40504716">[-]</label><label class="expand" for="c-40504716">[1 more]</label></div><br/><div class="children"><div class="content">Please do! It&#x27;s a fantastic series!</div><br/></div></div></div></div></div></div><div id="40502913" class="c"><input type="checkbox" id="c-40502913" checked=""/><div class="controls bullet"><span class="by">sturza</span><span>|</span><a href="#40502693">parent</a><span>|</span><a href="#40503519">prev</a><span>|</span><a href="#40505343">next</a><span>|</span><label class="collapse" for="c-40502913">[-]</label><label class="expand" for="c-40502913">[3 more]</label></div><br/><div class="children"><div class="content">Do you think grokking leads to proper generalized reasoning? <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.15071" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.15071</a></div><br/><div id="40503609" class="c"><input type="checkbox" id="c-40503609" checked=""/><div class="controls bullet"><span class="by">bilsbie</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40502913">parent</a><span>|</span><a href="#40505343">next</a><span>|</span><label class="collapse" for="c-40503609">[-]</label><label class="expand" for="c-40503609">[2 more]</label></div><br/><div class="children"><div class="content">Any tips on understanding grokking?  I’m not following that paper.</div><br/><div id="40503823" class="c"><input type="checkbox" id="c-40503823" checked=""/><div class="controls bullet"><span class="by">sturza</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40503609">parent</a><span>|</span><a href="#40505343">next</a><span>|</span><label class="collapse" for="c-40503823">[-]</label><label class="expand" for="c-40503823">[1 more]</label></div><br/><div class="children"><div class="content">Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. Overfitting and being cool about it and some new behavior might emerge.</div><br/></div></div></div></div></div></div><div id="40504625" class="c"><input type="checkbox" id="c-40504625" checked=""/><div class="controls bullet"><span class="by">m11a</span><span>|</span><a href="#40502693">parent</a><span>|</span><a href="#40505343">prev</a><span>|</span><a href="#40509643">next</a><span>|</span><label class="collapse" for="c-40504625">[-]</label><label class="expand" for="c-40504625">[2 more]</label></div><br/><div class="children"><div class="content">Why write in CUDA and not just use PyTorch etc?<p>if performance, how much faster is it, out of curiosity?</div><br/><div id="40505621" class="c"><input type="checkbox" id="c-40505621" checked=""/><div class="controls bullet"><span class="by">kgwgk</span><span>|</span><a href="#40502693">root</a><span>|</span><a href="#40504625">parent</a><span>|</span><a href="#40509643">next</a><span>|</span><label class="collapse" for="c-40505621">[-]</label><label class="expand" for="c-40505621">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why write in CUDA and not just use PyTorch etc?<p>“LLM training in simple, pure C&#x2F;CUDA. There is no need for 245MB of PyTorch or 107MB of cPython. […] A few more words on what I want this repo to be: First, I want llm.c to be a place for education.”</div><br/></div></div></div></div></div></div><div id="40509643" class="c"><input type="checkbox" id="c-40509643" checked=""/><div class="controls bullet"><span class="by">adeptima</span><span>|</span><a href="#40502693">prev</a><span>|</span><a href="#40502642">next</a><span>|</span><label class="collapse" for="c-40509643">[-]</label><label class="expand" for="c-40509643">[1 more]</label></div><br/><div class="children"><div class="content">Andrej Karpathy karpathy is a magician!<p>But being the coolest kid on the block with pure C&#x2F;CUDA implementation is not enough  
<a href="https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;llm.c">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;llm.c</a><p>Studying a baby Llama 2 model source code in pure Mojo is the next level
<a href="https:&#x2F;&#x2F;github.com&#x2F;tairov&#x2F;llama2.mojo">https:&#x2F;&#x2F;github.com&#x2F;tairov&#x2F;llama2.mojo</a>
<a href="https:&#x2F;&#x2F;github.com&#x2F;tairov&#x2F;llama2.mojo&#x2F;blob&#x2F;master&#x2F;llama2.mojo">https:&#x2F;&#x2F;github.com&#x2F;tairov&#x2F;llama2.mojo&#x2F;blob&#x2F;master&#x2F;llama2.moj...</a><p>Mojo Lang - Tomorrow&#x27;s High Performance Python? (with Chris Lattner) 
<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=JRcXUuQYR90" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=JRcXUuQYR90</a><p>Andrej Karpathy and Chris Lattner collab is on my wishlist ;)</div><br/></div></div><div id="40502642" class="c"><input type="checkbox" id="c-40502642" checked=""/><div class="controls bullet"><span class="by">benterix</span><span>|</span><a href="#40509643">prev</a><span>|</span><a href="#40506547">next</a><span>|</span><label class="collapse" for="c-40502642">[-]</label><label class="expand" for="c-40502642">[29 more]</label></div><br/><div class="children"><div class="content">I just hope than in a couple of years we&#x27;ll see a submission here titled &quot;Reproduce GPT-4 on legacy RTX 4090.&quot;<p>Because currently even with open source (?) models we are still consumers, and the training is still the domain of the rich.</div><br/><div id="40502797" class="c"><input type="checkbox" id="c-40502797" checked=""/><div class="controls bullet"><span class="by">ravetcofx</span><span>|</span><a href="#40502642">parent</a><span>|</span><a href="#40503012">next</a><span>|</span><label class="collapse" for="c-40502797">[-]</label><label class="expand" for="c-40502797">[14 more]</label></div><br/><div class="children"><div class="content">Accessing the dataset to train from scratch will be the biggest hurdle, now a lot of the pile has had ladder pulled since GPT-4</div><br/><div id="40503500" class="c"><input type="checkbox" id="c-40503500" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40502797">parent</a><span>|</span><a href="#40503003">next</a><span>|</span><label class="collapse" for="c-40503500">[-]</label><label class="expand" for="c-40503500">[5 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;HuggingFaceFW&#x2F;fineweb" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;HuggingFaceFW&#x2F;fineweb</a> has 15T cleaned and deduplicated english web data tokens.</div><br/><div id="40503867" class="c"><input type="checkbox" id="c-40503867" checked=""/><div class="controls bullet"><span class="by">ravetcofx</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40503500">parent</a><span>|</span><a href="#40503003">next</a><span>|</span><label class="collapse" for="c-40503867">[-]</label><label class="expand" for="c-40503867">[4 more]</label></div><br/><div class="children"><div class="content">Holy crap, Does huggingface charge for bandwidth if you&#x27;re downloading 45 terabytes??</div><br/><div id="40508432" class="c"><input type="checkbox" id="c-40508432" checked=""/><div class="controls bullet"><span class="by">andersa</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40503867">parent</a><span>|</span><a href="#40504075">next</a><span>|</span><label class="collapse" for="c-40508432">[-]</label><label class="expand" for="c-40508432">[1 more]</label></div><br/><div class="children"><div class="content">Fun trivia: downloading 45TB costs about $60, according to Cloudflare.</div><br/></div></div><div id="40504075" class="c"><input type="checkbox" id="c-40504075" checked=""/><div class="controls bullet"><span class="by">drexlspivey</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40503867">parent</a><span>|</span><a href="#40508432">prev</a><span>|</span><a href="#40503003">next</a><span>|</span><label class="collapse" for="c-40504075">[-]</label><label class="expand" for="c-40504075">[2 more]</label></div><br/><div class="children"><div class="content">I believe they are hosting it on Cloudflare who doesn’t charge for egress</div><br/><div id="40504266" class="c"><input type="checkbox" id="c-40504266" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40504075">parent</a><span>|</span><a href="#40503003">next</a><span>|</span><label class="collapse" for="c-40504266">[-]</label><label class="expand" for="c-40504266">[1 more]</label></div><br/><div class="children"><div class="content">More specifically, Cloudflare R2 doesn&#x27;t charge for egress, and Cloudflare doesn&#x27;t charge for egress to members in the Bandwidth Alliance which include Azure, Google Cloud, Oracle, Alibaba Cloud, and others, though critically not AWS.<p>They very much do charge egress fees elsewhere.</div><br/></div></div></div></div></div></div></div></div><div id="40503003" class="c"><input type="checkbox" id="c-40503003" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40502797">parent</a><span>|</span><a href="#40503500">prev</a><span>|</span><a href="#40503006">next</a><span>|</span><label class="collapse" for="c-40503003">[-]</label><label class="expand" for="c-40503003">[1 more]</label></div><br/><div class="children"><div class="content">Someone will come along and say &quot;Why don&#x27;t you just mirror Anna&#x27;s Archive?&quot; in 3...2...1...</div><br/></div></div><div id="40503006" class="c"><input type="checkbox" id="c-40503006" checked=""/><div class="controls bullet"><span class="by">meiraleal</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40502797">parent</a><span>|</span><a href="#40503003">prev</a><span>|</span><a href="#40503004">next</a><span>|</span><label class="collapse" for="c-40503006">[-]</label><label class="expand" for="c-40503006">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m okay with paying for datasets</div><br/><div id="40503150" class="c"><input type="checkbox" id="c-40503150" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40503006">parent</a><span>|</span><a href="#40503004">next</a><span>|</span><label class="collapse" for="c-40503150">[-]</label><label class="expand" for="c-40503150">[3 more]</label></div><br/><div class="children"><div class="content">Depends on how the courts rule.  If the copyright maximalists prevail, only the wealthiest entities will be able to afford to license a useful data set.<p>Paradoxically enough, this is the outcome that most &quot;Hacker News&quot; denizens seem to be rooting for.</div><br/><div id="40503463" class="c"><input type="checkbox" id="c-40503463" checked=""/><div class="controls bullet"><span class="by">meiraleal</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40503150">parent</a><span>|</span><a href="#40504918">next</a><span>|</span><label class="collapse" for="c-40503463">[-]</label><label class="expand" for="c-40503463">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d still get most of my dataset from torrent but I could pay for specific things like high quality source code.</div><br/></div></div><div id="40504918" class="c"><input type="checkbox" id="c-40504918" checked=""/><div class="controls bullet"><span class="by">groby_b</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40503150">parent</a><span>|</span><a href="#40503463">prev</a><span>|</span><a href="#40503004">next</a><span>|</span><label class="collapse" for="c-40504918">[-]</label><label class="expand" for="c-40504918">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s almost as if people believe in fairness and compensating people for their work.<p>Also, it&#x27;s worth noting that this is only true as long as we&#x27;re stuck in the &quot;must train on the entire sum total of human output ever created&quot; local minimum for machine learning.  Given that most biological entities learn with much less data, this might well be the thing that prods ML research to using an approach that isn&#x27;t &quot;IDK, buy a few containers of GPUs, and half a DC of storage, see if that makes things better&quot;.</div><br/></div></div></div></div></div></div><div id="40503004" class="c"><input type="checkbox" id="c-40503004" checked=""/><div class="controls bullet"><span class="by">exe34</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40502797">parent</a><span>|</span><a href="#40503006">prev</a><span>|</span><a href="#40503012">next</a><span>|</span><label class="collapse" for="c-40503004">[-]</label><label class="expand" for="c-40503004">[3 more]</label></div><br/><div class="children"><div class="content">i suppose you wouldn&#x27;t be able to use it for external services, but internally, I&#x27;m sure you can find some books that fell off the back of a truck...</div><br/><div id="40503065" class="c"><input type="checkbox" id="c-40503065" checked=""/><div class="controls bullet"><span class="by">HeatrayEnjoyer</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40503004">parent</a><span>|</span><a href="#40503012">next</a><span>|</span><label class="collapse" for="c-40503065">[-]</label><label class="expand" for="c-40503065">[2 more]</label></div><br/><div class="children"><div class="content">No reason you can&#x27;t go external. GPT was trained using ebook torrent sites</div><br/><div id="40503937" class="c"><input type="checkbox" id="c-40503937" checked=""/><div class="controls bullet"><span class="by">artninja1988</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40503065">parent</a><span>|</span><a href="#40503012">next</a><span>|</span><label class="collapse" for="c-40503937">[-]</label><label class="expand" for="c-40503937">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI has enough money to hire lawyers to defend it until the end of time though</div><br/></div></div></div></div></div></div></div></div><div id="40503012" class="c"><input type="checkbox" id="c-40503012" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#40502642">parent</a><span>|</span><a href="#40502797">prev</a><span>|</span><a href="#40509436">next</a><span>|</span><label class="collapse" for="c-40503012">[-]</label><label class="expand" for="c-40503012">[4 more]</label></div><br/><div class="children"><div class="content">We won’t ever get there or need to because GPT-4 wasn’t trained on one GPU it was trained on thousands. The (most likely) biggest meaningful difference between -2 and -4 is the number of parameters and the training data&#x2F;duration. I don’t think you’d really learn much more.</div><br/><div id="40506615" class="c"><input type="checkbox" id="c-40506615" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40503012">parent</a><span>|</span><a href="#40509436">next</a><span>|</span><label class="collapse" for="c-40506615">[-]</label><label class="expand" for="c-40506615">[3 more]</label></div><br/><div class="children"><div class="content">It’s not about learning. It’s about owning. Exactly the reason OpenAI stopped being open. Having GPT-4-quality LLMs created by anyone with a gaming PC would be pretty radical.</div><br/><div id="40507320" class="c"><input type="checkbox" id="c-40507320" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40506615">parent</a><span>|</span><a href="#40509436">next</a><span>|</span><label class="collapse" for="c-40507320">[-]</label><label class="expand" for="c-40507320">[2 more]</label></div><br/><div class="children"><div class="content">And you won’t get there. Those models are far too large for a 2024 GPU. Llama-3 70b is arguably close to GPT-4 but is still too large for gaming GPUs (and probably for many years of GPU updates)</div><br/><div id="40508407" class="c"><input type="checkbox" id="c-40508407" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40507320">parent</a><span>|</span><a href="#40509436">next</a><span>|</span><label class="collapse" for="c-40508407">[-]</label><label class="expand" for="c-40508407">[1 more]</label></div><br/><div class="children"><div class="content">“You won’t get there” is a pretty vast statement for all of the future. Two fairly reasonable predictions: 1) the compute needed to get GPT4 performance will decrease. 2) the compute on consumer GPUs will increase.<p>At some point they cross, and you will be able to run a GPT4-quality LLM on a consumer GPU. At some point after that, you’ll be able to run a GPT4-quality LLM on a 2024 consumer GPU if you can find one.<p>Important to emphasize, I’m not saying “GPT-4”. Llama-3 was trained on 24k GPU clusters. “Able to do the exact same processing at 1&#x2F;24k the compute” is different from “Able to get equivalent performance at 1&#x2F;24k compute”. Even then, given a long enough time scale, the former is possible.</div><br/></div></div></div></div></div></div></div></div><div id="40509436" class="c"><input type="checkbox" id="c-40509436" checked=""/><div class="controls bullet"><span class="by">doubloon</span><span>|</span><a href="#40502642">parent</a><span>|</span><a href="#40503012">prev</a><span>|</span><a href="#40505413">next</a><span>|</span><label class="collapse" for="c-40509436">[-]</label><label class="expand" for="c-40509436">[1 more]</label></div><br/><div class="children"><div class="content">Dude im hoping we get rid of Nvidia completely. I can run llama.cpp inference on a 7B model on my 24 core cpu intel machine using just CPU and it only uses about 4gb of ram and is not that slow. If we could have massive parallel arm core or even riscv machines without the Cuda issues with proprietary driver hell  it would be much more open source. And much less wonkage for the normie user</div><br/></div></div><div id="40505413" class="c"><input type="checkbox" id="c-40505413" checked=""/><div class="controls bullet"><span class="by">sabareesh</span><span>|</span><a href="#40502642">parent</a><span>|</span><a href="#40509436">prev</a><span>|</span><a href="#40504133">next</a><span>|</span><label class="collapse" for="c-40505413">[-]</label><label class="expand" for="c-40505413">[1 more]</label></div><br/><div class="children"><div class="content">Well here is a comment on 4090 <a href="https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;llm.c&#x2F;discussions&#x2F;481#discussioncomment-9585564">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;llm.c&#x2F;discussions&#x2F;481#discussion...</a></div><br/></div></div><div id="40504133" class="c"><input type="checkbox" id="c-40504133" checked=""/><div class="controls bullet"><span class="by">anthonix1</span><span>|</span><a href="#40502642">parent</a><span>|</span><a href="#40505413">prev</a><span>|</span><a href="#40503954">next</a><span>|</span><label class="collapse" for="c-40504133">[-]</label><label class="expand" for="c-40504133">[3 more]</label></div><br/><div class="children"><div class="content">FWIW, I&#x27;m seeing ~318,000 toks&#x2F;sec throughput on a 4x AMD 7900 XTX machine (less than $4k worth of GPU), using the same settings as in the post (0.5M batch size etc).</div><br/><div id="40506035" class="c"><input type="checkbox" id="c-40506035" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40504133">parent</a><span>|</span><a href="#40503954">next</a><span>|</span><label class="collapse" for="c-40506035">[-]</label><label class="expand" for="c-40506035">[2 more]</label></div><br/><div class="children"><div class="content">Did you reproduce the evaluation as well?</div><br/><div id="40506887" class="c"><input type="checkbox" id="c-40506887" checked=""/><div class="controls bullet"><span class="by">anthonix1</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40506035">parent</a><span>|</span><a href="#40503954">next</a><span>|</span><label class="collapse" for="c-40506887">[-]</label><label class="expand" for="c-40506887">[1 more]</label></div><br/><div class="children"><div class="content">It converges similarly on smaller datasets.<p>About to kick off a training from scratch run on the same fineweb-10B, which at 324k toks&#x2F;sec should take about 8.6 hours. And with my kWh cost, that is about $2.50 cost to train.<p>Will report back tomorrow when the training has finished..</div><br/></div></div></div></div></div></div><div id="40503954" class="c"><input type="checkbox" id="c-40503954" checked=""/><div class="controls bullet"><span class="by">auspiv</span><span>|</span><a href="#40502642">parent</a><span>|</span><a href="#40504133">prev</a><span>|</span><a href="#40503074">next</a><span>|</span><label class="collapse" for="c-40503954">[-]</label><label class="expand" for="c-40503954">[2 more]</label></div><br/><div class="children"><div class="content">Considering it takes 8x A100 GPUs (80GB VRAM) to train GPT-2, I think it&#x27;ll take far more than a single 4090.</div><br/><div id="40504083" class="c"><input type="checkbox" id="c-40504083" checked=""/><div class="controls bullet"><span class="by">bufo</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40503954">parent</a><span>|</span><a href="#40503074">next</a><span>|</span><label class="collapse" for="c-40504083">[-]</label><label class="expand" for="c-40504083">[1 more]</label></div><br/><div class="children"><div class="content">The RTX 4090 has about the same BF16 Tensor Core TOPs than the A100, assuming 50% MFU (like the A100 40 GB PCIe) it would take 8x longer on 1 RTX 4090 vs 8x A100 80GB SXM, so 12 hours.
Datasheet here for the TOPs <a href="https:&#x2F;&#x2F;images.nvidia.com&#x2F;aem-dam&#x2F;Solutions&#x2F;geforce&#x2F;ada&#x2F;nvidia-ada-gpu-architecture.pdf" rel="nofollow">https:&#x2F;&#x2F;images.nvidia.com&#x2F;aem-dam&#x2F;Solutions&#x2F;geforce&#x2F;ada&#x2F;nvid...</a>
50% MFU should be achievable on the 4090.</div><br/></div></div></div></div><div id="40503074" class="c"><input type="checkbox" id="c-40503074" checked=""/><div class="controls bullet"><span class="by">Invictus0</span><span>|</span><a href="#40502642">parent</a><span>|</span><a href="#40503954">prev</a><span>|</span><a href="#40506547">next</a><span>|</span><label class="collapse" for="c-40503074">[-]</label><label class="expand" for="c-40503074">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not saying this to be rude, but I think you have a deep misunderstanding of how AI training works. You cannot just skip the matrix multiplications necessary to train the model, or get current hardware to do it faster.</div><br/><div id="40504748" class="c"><input type="checkbox" id="c-40504748" checked=""/><div class="controls bullet"><span class="by">xdavidliu</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40503074">parent</a><span>|</span><a href="#40506600">next</a><span>|</span><label class="collapse" for="c-40504748">[-]</label><label class="expand" for="c-40504748">[1 more]</label></div><br/><div class="children"><div class="content">was the first sentence really necessary? The second sentence seems fine by itself.</div><br/></div></div><div id="40506600" class="c"><input type="checkbox" id="c-40506600" checked=""/><div class="controls bullet"><span class="by">nickpsecurity</span><span>|</span><a href="#40502642">root</a><span>|</span><a href="#40503074">parent</a><span>|</span><a href="#40504748">prev</a><span>|</span><a href="#40506547">next</a><span>|</span><label class="collapse" for="c-40506600">[-]</label><label class="expand" for="c-40506600">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s work on replacing multiplication. Here&#x27;s four examples:<p><a href="https:&#x2F;&#x2F;openaccess.thecvf.com&#x2F;content_CVPR_2020&#x2F;papers&#x2F;Chen_AdderNet_Do_We_Really_Need_Multiplications_in_Deep_Learning_CVPR_2020_paper.pdf" rel="nofollow">https:&#x2F;&#x2F;openaccess.thecvf.com&#x2F;content_CVPR_2020&#x2F;papers&#x2F;Chen_...</a><p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2012.03458" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2012.03458</a><p><a href="https:&#x2F;&#x2F;openaccess.thecvf.com&#x2F;content&#x2F;CVPR2021W&#x2F;MAI&#x2F;papers&#x2F;Elhoushi_DeepShift_Towards_Multiplication-Less_Neural_Networks_CVPRW_2021_paper.pdf" rel="nofollow">https:&#x2F;&#x2F;openaccess.thecvf.com&#x2F;content&#x2F;CVPR2021W&#x2F;MAI&#x2F;papers&#x2F;E...</a><p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2106.10860" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2106.10860</a></div><br/></div></div></div></div></div></div><div id="40506547" class="c"><input type="checkbox" id="c-40506547" checked=""/><div class="controls bullet"><span class="by">aliljet</span><span>|</span><a href="#40502642">prev</a><span>|</span><a href="#40502637">next</a><span>|</span><label class="collapse" for="c-40506547">[-]</label><label class="expand" for="c-40506547">[1 more]</label></div><br/><div class="children"><div class="content">Is there a reason you&#x27;re not trying to port this into an even more stack agnostic world without CUDA?</div><br/></div></div><div id="40502637" class="c"><input type="checkbox" id="c-40502637" checked=""/><div class="controls bullet"><span class="by">indigodaddy</span><span>|</span><a href="#40506547">prev</a><span>|</span><a href="#40504905">next</a><span>|</span><label class="collapse" for="c-40502637">[-]</label><label class="expand" for="c-40502637">[4 more]</label></div><br/><div class="children"><div class="content">Looks like this is re: training, but wonder how inference would be on some garbage older machine with no GPU on this model?</div><br/><div id="40504820" class="c"><input type="checkbox" id="c-40504820" checked=""/><div class="controls bullet"><span class="by">ryankrage77</span><span>|</span><a href="#40502637">parent</a><span>|</span><a href="#40504905">next</a><span>|</span><label class="collapse" for="c-40504820">[-]</label><label class="expand" for="c-40504820">[3 more]</label></div><br/><div class="children"><div class="content">Last time I tried GPT-2 on CPU (which I think was shortly before chatGPT was launched), I was getting about 0.2 tokens&#x2F;sec. CPU utilization was low though, so running inference in parralel gave better results. I was using 2 x E5-2660&#x27;s.</div><br/><div id="40505038" class="c"><input type="checkbox" id="c-40505038" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#40502637">root</a><span>|</span><a href="#40504820">parent</a><span>|</span><a href="#40504905">next</a><span>|</span><label class="collapse" for="c-40505038">[-]</label><label class="expand" for="c-40505038">[2 more]</label></div><br/><div class="children"><div class="content">DDR5 helps a lot. You can actually run stuff like LLaMA at &gt;1 tok&#x2F;s on the CPU with high-end gaming hardware these days.</div><br/><div id="40509508" class="c"><input type="checkbox" id="c-40509508" checked=""/><div class="controls bullet"><span class="by">doubloon</span><span>|</span><a href="#40502637">root</a><span>|</span><a href="#40505038">parent</a><span>|</span><a href="#40504905">next</a><span>|</span><label class="collapse" for="c-40509508">[-]</label><label class="expand" for="c-40509508">[1 more]</label></div><br/><div class="children"><div class="content">I have a 24 core Intel cpu and llama3.cpp runs llama3 surprisingly fast in surprisingly little RAM. Yes it becomes a space heater but theres light at the end of the cuda free tunnel</div><br/></div></div></div></div></div></div></div></div><div id="40508125" class="c"><input type="checkbox" id="c-40508125" checked=""/><div class="controls bullet"><span class="by">ls612</span><span>|</span><a href="#40504905">prev</a><span>|</span><a href="#40506665">next</a><span>|</span><label class="collapse" for="c-40508125">[-]</label><label class="expand" for="c-40508125">[1 more]</label></div><br/><div class="children"><div class="content">Is this the sort of thing that a person with curiosity and a 4090 could do? It says he used 8xA100s in the cloud to do this but is it just a matter of the 4090 going 8x slower or will memory constraints kill the whole endeavour?</div><br/></div></div><div id="40506665" class="c"><input type="checkbox" id="c-40506665" checked=""/><div class="controls bullet"><span class="by">zimabluerain</span><span>|</span><a href="#40508125">prev</a><span>|</span><a href="#40505258">next</a><span>|</span><label class="collapse" for="c-40506665">[-]</label><label class="expand" for="c-40506665">[1 more]</label></div><br/><div class="children"><div class="content">the code works well on H100: <a href="https:&#x2F;&#x2F;x.com&#x2F;Yuchenj_UW&#x2F;status&#x2F;1795554739633221804" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;Yuchenj_UW&#x2F;status&#x2F;1795554739633221804</a></div><br/></div></div><div id="40505258" class="c"><input type="checkbox" id="c-40505258" checked=""/><div class="controls bullet"><span class="by">natsucks</span><span>|</span><a href="#40506665">prev</a><span>|</span><a href="#40504685">next</a><span>|</span><label class="collapse" for="c-40505258">[-]</label><label class="expand" for="c-40505258">[4 more]</label></div><br/><div class="children"><div class="content">In your opinion is it important for ML engineers to know C?</div><br/><div id="40509676" class="c"><input type="checkbox" id="c-40509676" checked=""/><div class="controls bullet"><span class="by">adeptima</span><span>|</span><a href="#40505258">parent</a><span>|</span><a href="#40505791">next</a><span>|</span><label class="collapse" for="c-40509676">[-]</label><label class="expand" for="c-40509676">[1 more]</label></div><br/><div class="children"><div class="content">Spend one year to study multiple languages - bash, C, C++, Go, Python ... and even Mojo or Rust. 10-20 hours a week. Being able to read top programming languages is the best investment I ever made. You will become fearless and can see the matrix ;)</div><br/></div></div><div id="40505791" class="c"><input type="checkbox" id="c-40505791" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#40505258">parent</a><span>|</span><a href="#40509676">prev</a><span>|</span><a href="#40505694">next</a><span>|</span><label class="collapse" for="c-40505791">[-]</label><label class="expand" for="c-40505791">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;d have to be deep into ML Infrastructure to use C, probably via CUDA. No-one who develops or uses ML models touches C or even C++. tinygrad and llama.cpp are exceptions.</div><br/></div></div><div id="40505694" class="c"><input type="checkbox" id="c-40505694" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#40505258">parent</a><span>|</span><a href="#40505791">prev</a><span>|</span><a href="#40504685">next</a><span>|</span><label class="collapse" for="c-40505694">[-]</label><label class="expand" for="c-40505694">[1 more]</label></div><br/><div class="children"><div class="content">0% chance</div><br/></div></div></div></div><div id="40504685" class="c"><input type="checkbox" id="c-40504685" checked=""/><div class="controls bullet"><span class="by">celltalk</span><span>|</span><a href="#40505258">prev</a><span>|</span><a href="#40503877">next</a><span>|</span><label class="collapse" for="c-40504685">[-]</label><label class="expand" for="c-40504685">[1 more]</label></div><br/><div class="children"><div class="content">Time for llm videos!</div><br/></div></div><div id="40503877" class="c"><input type="checkbox" id="c-40503877" checked=""/><div class="controls bullet"><span class="by">notg963</span><span>|</span><a href="#40504685">prev</a><span>|</span><a href="#40504925">next</a><span>|</span><label class="collapse" for="c-40503877">[-]</label><label class="expand" for="c-40503877">[1 more]</label></div><br/><div class="children"><div class="content">Do you have plans to create videos for the llm.c?</div><br/></div></div><div id="40504494" class="c"><input type="checkbox" id="c-40504494" checked=""/><div class="controls bullet"><span class="by">anoy8888</span><span>|</span><a href="#40502738">prev</a><span>|</span><label class="collapse" for="c-40504494">[-]</label><label class="expand" for="c-40504494">[1 more]</label></div><br/><div class="children"><div class="content">Can it be done in rust ?</div><br/></div></div></div></div></div></div></div></body></html>