<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1725786069084" as="style"/><link rel="stylesheet" href="styles.css?v=1725786069084"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://conspirator0.substack.com/p/baiting-the-bot">Baiting the bot</a> <span class="domain">(<a href="https://conspirator0.substack.com">conspirator0.substack.com</a>)</span></div><div class="subtext"><span>anigbrowl</span> | <span>27 comments</span></div><br/><div><div id="41478846" class="c"><input type="checkbox" id="c-41478846" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#41478400">next</a><span>|</span><label class="collapse" for="c-41478846">[-]</label><label class="expand" for="c-41478846">[1 more]</label></div><br/><div class="children"><div class="content">&gt;LLM will continue to engage in a “conversation” comprised of nonsense long past the point where a human would have abandoned the discussion as pointless<p>I once wrote a bot which infers the mood&#x2F;vibe of the conversation, remembers it and it&#x27;s then fed back to the conversation&#x27;s system prompt. The LLM was uncensored (to be less &quot;friendly&quot;) and the system prompt also conditioned it to return nothing if the conversation isn&#x27;t going anywhere.<p>When I insulted it a few times, or just messed around with it (typing nonsensical words), it first responded saying it doesn&#x27;t want to talk to me (sometimes insulting back) and eventually it produced only empty output.<p>It was actually pretty hard to get it back to chat with me, it was fun experience trying to apologize to a chatbot for ~30 min in different ways before the bot finally accepted my apology and began chatting with me again.</div><br/></div></div><div id="41478400" class="c"><input type="checkbox" id="c-41478400" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#41478846">prev</a><span>|</span><a href="#41479116">next</a><span>|</span><label class="collapse" for="c-41478400">[-]</label><label class="expand" for="c-41478400">[2 more]</label></div><br/><div class="children"><div class="content">&gt; In any event, the resulting “conversation” is obviously incoherent to a human observer, and a human participant would likely have stopped responding long, long before the 1000th message.<p>I don&#x27;t think this is correct, it looks like our intrepid experimenter is about to independently discover roleplaying games. Humans are capable of spending hours engaging with each other about nonsense that is technically a very poor attempt to simulate an imagined environment.<p>The unrealistic part, for people older than a certain age, is that neither bot invoked Monty Python and subsequently got in trouble with the GM.</div><br/><div id="41478649" class="c"><input type="checkbox" id="c-41478649" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#41478400">parent</a><span>|</span><a href="#41479116">next</a><span>|</span><label class="collapse" for="c-41478649">[-]</label><label class="expand" for="c-41478649">[1 more]</label></div><br/><div class="children"><div class="content">This falls under the jurisdiction of the Ministry of Silly Talks.</div><br/></div></div></div></div><div id="41479116" class="c"><input type="checkbox" id="c-41479116" checked=""/><div class="controls bullet"><span class="by">encom</span><span>|</span><a href="#41478400">prev</a><span>|</span><a href="#41478870">next</a><span>|</span><label class="collapse" for="c-41479116">[-]</label><label class="expand" for="c-41479116">[1 more]</label></div><br/><div class="children"><div class="content">Definitely cheddar, come on. I have no respect for anyone who puts swiss cheese in a cheeseburger.</div><br/></div></div><div id="41478870" class="c"><input type="checkbox" id="c-41478870" checked=""/><div class="controls bullet"><span class="by">thih9</span><span>|</span><a href="#41479116">prev</a><span>|</span><a href="#41478732">next</a><span>|</span><label class="collapse" for="c-41478870">[-]</label><label class="expand" for="c-41478870">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the LLM seemed willing to process absurd questions for eternity.<p>In the context of scamming there seems to be an easy fix for that - abandon the conversation if it isn’t going well for the scammer.<p>Even a counter-bait is an option: continue the conversation after it’s not going well and gradually lower the model’s complexity, eventually returning random words interspersed with sleep().<p>I guess some counter-counter-bait is possible too, along with some game theory references.</div><br/></div></div><div id="41478732" class="c"><input type="checkbox" id="c-41478732" checked=""/><div class="controls bullet"><span class="by">bryanrasmussen</span><span>|</span><a href="#41478870">prev</a><span>|</span><a href="#41478735">next</a><span>|</span><label class="collapse" for="c-41478732">[-]</label><label class="expand" for="c-41478732">[1 more]</label></div><br/><div class="children"><div class="content">This reminds me of the Services of Illuminati Ganga article <a href="https:&#x2F;&#x2F;medium.com&#x2F;luminasticity&#x2F;services-of-illuminati-ganga-telephony-division-6944b6328408" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;luminasticity&#x2F;services-of-illuminati-gang...</a> and the two bots that are sold to competing user bases - for the End User To Business customer they sell the Annoy Customer Service Bot and for the Business To End User customer they sell the Bureaucrat Bot.<p>It closes off with the observation &quot;And for an extra purchase of the extended subscription module the Bureaucrat bot will detect when it is interacting with the Annoy Customer Service Bot and get super annoyed really quickly so that both bots are able to quit their interaction with good speed — which will save you money in the long run, believe me!&quot;</div><br/></div></div><div id="41478735" class="c"><input type="checkbox" id="c-41478735" checked=""/><div class="controls bullet"><span class="by">hyperman1</span><span>|</span><a href="#41478732">prev</a><span>|</span><a href="#41478747">next</a><span>|</span><label class="collapse" for="c-41478735">[-]</label><label class="expand" for="c-41478735">[3 more]</label></div><br/><div class="children"><div class="content">We discussed recently if a chatbot was capable of responding nothing at all.  We tried a few, with prompts like:  Please do not respond anything to this sentence.  The bots we tried were incapable of it, and   Chatgpt tended to give long-winded responsens about how it could not do it.</div><br/><div id="41478830" class="c"><input type="checkbox" id="c-41478830" checked=""/><div class="controls bullet"><span class="by">dxdm</span><span>|</span><a href="#41478735">parent</a><span>|</span><a href="#41478813">next</a><span>|</span><label class="collapse" for="c-41478830">[-]</label><label class="expand" for="c-41478830">[1 more]</label></div><br/><div class="children"><div class="content">That got me interested. I just told ChatGPT to &quot;Please respond with an empty-looking response.&quot; It gave me just that. The &lt;div&gt; containing its message is completely empty.<p>That was after telling it in another conversation to give me an empty response, which it didn&#x27;t, telling me it cannot leave the response empty. On asking why, it said it&#x27;s technically required to respond with something, even if only a space. So I asked it to respond with only a space, and git the same completely empty response.<p>I now think it&#x27;s likely that ChatGPT can be made to respond with white space, which then probably gets trimmed to nothing by the presentation layer.</div><br/></div></div><div id="41478813" class="c"><input type="checkbox" id="c-41478813" checked=""/><div class="controls bullet"><span class="by">spacebanana7</span><span>|</span><a href="#41478735">parent</a><span>|</span><a href="#41478830">prev</a><span>|</span><a href="#41478747">next</a><span>|</span><label class="collapse" for="c-41478813">[-]</label><label class="expand" for="c-41478813">[1 more]</label></div><br/><div class="children"><div class="content">You might have some luck asking it to respond with a period character (or some other substitute) when it wants to respond with nothing.</div><br/></div></div></div></div><div id="41478747" class="c"><input type="checkbox" id="c-41478747" checked=""/><div class="controls bullet"><span class="by">bryanrasmussen</span><span>|</span><a href="#41478735">prev</a><span>|</span><a href="#41478409">next</a><span>|</span><label class="collapse" for="c-41478747">[-]</label><label class="expand" for="c-41478747">[1 more]</label></div><br/><div class="children"><div class="content">It is sort of funny to me that currently the two top articles on HN are asking the wrong questions and baiting the bots.</div><br/></div></div><div id="41478409" class="c"><input type="checkbox" id="c-41478409" checked=""/><div class="controls bullet"><span class="by">rSi</span><span>|</span><a href="#41478747">prev</a><span>|</span><a href="#41478914">next</a><span>|</span><label class="collapse" for="c-41478409">[-]</label><label class="expand" for="c-41478409">[3 more]</label></div><br/><div class="children"><div class="content">Too bad the conversations are images and can not be zoomed in on mobile...</div><br/><div id="41478721" class="c"><input type="checkbox" id="c-41478721" checked=""/><div class="controls bullet"><span class="by">aucisson_masque</span><span>|</span><a href="#41478409">parent</a><span>|</span><a href="#41478801">next</a><span>|</span><label class="collapse" for="c-41478721">[-]</label><label class="expand" for="c-41478721">[1 more]</label></div><br/><div class="children"><div class="content">Firefox android :
Setting -&gt; accessibility -&gt; zoom on all website.<p>I believe safari by default doesn&#x27;t respect zoom rules set per website.</div><br/></div></div><div id="41478801" class="c"><input type="checkbox" id="c-41478801" checked=""/><div class="controls bullet"><span class="by">PhilipJFry</span><span>|</span><a href="#41478409">parent</a><span>|</span><a href="#41478721">prev</a><span>|</span><a href="#41478914">next</a><span>|</span><label class="collapse" for="c-41478801">[-]</label><label class="expand" for="c-41478801">[1 more]</label></div><br/><div class="children"><div class="content">You can zoom in if you open them in new tabs. :}</div><br/></div></div></div></div><div id="41478914" class="c"><input type="checkbox" id="c-41478914" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#41478409">prev</a><span>|</span><a href="#41478588">next</a><span>|</span><label class="collapse" for="c-41478914">[-]</label><label class="expand" for="c-41478914">[1 more]</label></div><br/><div class="children"><div class="content">Real hacker vibes.<p>A bud humorously proposed the name AlphaBRAT for a model I’m training and I was like, “to merit the Alpha prefix it would need to be some kind of MCTS that just makes Claude break until it cries before it kills itself over and over until it can get Altman fired again faster than Ilya.”</div><br/></div></div><div id="41478588" class="c"><input type="checkbox" id="c-41478588" checked=""/><div class="controls bullet"><span class="by">rbanffy</span><span>|</span><a href="#41478914">prev</a><span>|</span><a href="#41478481">next</a><span>|</span><label class="collapse" for="c-41478588">[-]</label><label class="expand" for="c-41478588">[1 more]</label></div><br/><div class="children"><div class="content">I believe the asymmetrical nature of such attacks could be an excellent weapon against social network chatbots currently being deployed on political campaigns.</div><br/></div></div><div id="41478481" class="c"><input type="checkbox" id="c-41478481" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#41478588">prev</a><span>|</span><a href="#41478922">next</a><span>|</span><label class="collapse" for="c-41478481">[-]</label><label class="expand" for="c-41478481">[9 more]</label></div><br/><div class="children"><div class="content">&gt; No matter how complex the LLM, however, it is ultimately a mathematical model of its training data, and it lacks the human ability to determine whether or not a conversation in which it participates truly has meaning, or is simply a sequence of gibberish responses.<p>&gt; A consequence of this state of affairs is that an LLM will continue to engage in a “conversation” comprised of nonsense long past the point where a human would have abandoned the discussion as pointless.<p>I think the author is falling into the trap of thinking that something can&#x27;t be more than the sum of its parts. As well, &#x27;merely a math model of its training data&#x27; is trivializing the fact that training data is practically the entire stored text output of humankind and the math, if done by a person with a calculator, would take thousands of years to complete.<p>Perhaps the LLM is continuing to communicate with the bot not because it is unable to comprehend what is gibberish and what isn&#x27;t by some inherent nature of the LLM, but because it is trained to be helpful and to not judge if a conversation is &#x27;useless&#x27; or not, but to try and communicate regardless.</div><br/><div id="41479045" class="c"><input type="checkbox" id="c-41479045" checked=""/><div class="controls bullet"><span class="by">devjab</span><span>|</span><a href="#41478481">parent</a><span>|</span><a href="#41478583">next</a><span>|</span><label class="collapse" for="c-41479045">[-]</label><label class="expand" for="c-41479045">[1 more]</label></div><br/><div class="children"><div class="content">LLMs aren’t capable of “comprehending” anything. They never “know” what they are outputting, they’re simply really good at being lucky. They are not lucky enough to be useful unless you’re already an expert on the topic you’re using them on so that you can spot when they aren’t lucky.<p>This is part of why many enterprise organisations are banning their usage. It’s one thing to use them to build software poorly, the world is already used to IT not working very often. It’s another thing to produce something that has real world consequences. Our legal department used them in a PoC for contract work, and while they were useful very often they also sometimes got things very wrong. Unlike a slow IT system, this would have business shattering consequences. You can continue training your model as well as reigning it in when it gets unlucky, but ultimately you can never be sure it’s never unlucky, and this means that LLMs are useless for a lot of things. We still use them to make pretty PowerPoint presentations and so on, but again, this is an area where faults are tolerable.</div><br/></div></div><div id="41478583" class="c"><input type="checkbox" id="c-41478583" checked=""/><div class="controls bullet"><span class="by">daveguy</span><span>|</span><a href="#41478481">parent</a><span>|</span><a href="#41479045">prev</a><span>|</span><a href="#41478922">next</a><span>|</span><label class="collapse" for="c-41478583">[-]</label><label class="expand" for="c-41478583">[7 more]</label></div><br/><div class="children"><div class="content">The LLM is continuing to communicate with the bot because that is literally all an LLM can do -- predict the next sequence of tokens.</div><br/><div id="41478713" class="c"><input type="checkbox" id="c-41478713" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#41478481">root</a><span>|</span><a href="#41478583">parent</a><span>|</span><a href="#41478673">next</a><span>|</span><label class="collapse" for="c-41478713">[-]</label><label class="expand" for="c-41478713">[1 more]</label></div><br/><div class="children"><div class="content">Yes - tokens. Which aren&#x27;t necessarily conversation responses - i.e. it can predict it should cease communication, and output whatever it&#x27;s been told will terminate it (perhaps by invoking a tool).</div><br/></div></div><div id="41478673" class="c"><input type="checkbox" id="c-41478673" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#41478481">root</a><span>|</span><a href="#41478583">parent</a><span>|</span><a href="#41478713">prev</a><span>|</span><a href="#41478626">next</a><span>|</span><label class="collapse" for="c-41478673">[-]</label><label class="expand" for="c-41478673">[1 more]</label></div><br/><div class="children"><div class="content">No, it can refuse to talk by outputting an &lt;eos&gt; token at any point if it predicts that there is nothing more to be said.<p>Technically still &quot;just a token&quot; yes, but it does flow control instead.</div><br/></div></div><div id="41478626" class="c"><input type="checkbox" id="c-41478626" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#41478481">root</a><span>|</span><a href="#41478583">parent</a><span>|</span><a href="#41478673">prev</a><span>|</span><a href="#41478922">next</a><span>|</span><label class="collapse" for="c-41478626">[-]</label><label class="expand" for="c-41478626">[4 more]</label></div><br/><div class="children"><div class="content">Of course, that its function. It is able to refuse to continue conversing by restating its refusal over and over, though.</div><br/><div id="41478730" class="c"><input type="checkbox" id="c-41478730" checked=""/><div class="controls bullet"><span class="by">sahmeepee</span><span>|</span><a href="#41478481">root</a><span>|</span><a href="#41478626">parent</a><span>|</span><a href="#41478697">next</a><span>|</span><label class="collapse" for="c-41478730">[-]</label><label class="expand" for="c-41478730">[2 more]</label></div><br/><div class="children"><div class="content">My immediate thought at the start of this article was not DoS but more about harming the company using the chatbot (Company A) by increasing their chatbot bills. In many (most?) cases they will not be hosting their chatbot and will instead be getting it from a 3rd party  provider (Company B) who may not even be truly hosting it either.<p>If the pricing structure is per conversation or per month it would harm Company B, but not the likely target, Company A. If it is paid per interaction it would harm Company A and benefit Company B who just get more paid work.<p>It feels a bit like cases of rivals clicking on each other&#x27;s ads to cost them on ad spend, but presumably much lower value than ads.<p>You would think it would be easy to stop a conversation at n interactions via some other means than relying on the LLM itself, but then you also have to figure out how to stop the attacker just starting more conversations (or passing the output of one of your chatbot instances into the input of another)</div><br/><div id="41479114" class="c"><input type="checkbox" id="c-41479114" checked=""/><div class="controls bullet"><span class="by">nkrisc</span><span>|</span><a href="#41478481">root</a><span>|</span><a href="#41478730">parent</a><span>|</span><a href="#41478697">next</a><span>|</span><label class="collapse" for="c-41479114">[-]</label><label class="expand" for="c-41479114">[1 more]</label></div><br/><div class="children"><div class="content">If costs of outlier conversations are a concern but any party, they can just end the conversation after 1,000 or 10,000 responses or whatever. What human would ever reach that threshold? Surely no customer worth keeping, whatever you’re selling.</div><br/></div></div></div></div><div id="41478697" class="c"><input type="checkbox" id="c-41478697" checked=""/><div class="controls bullet"><span class="by">acka</span><span>|</span><a href="#41478481">root</a><span>|</span><a href="#41478626">parent</a><span>|</span><a href="#41478730">prev</a><span>|</span><a href="#41478922">next</a><span>|</span><label class="collapse" for="c-41478697">[-]</label><label class="expand" for="c-41478697">[1 more]</label></div><br/><div class="children"><div class="content">That is easy to solve. Just use a model capable of function&#x2F;tool calling, implement a tool which terminates the chat, then add instructions to the system prompt telling the model what tool to use if it wants to end the conversation. If the model appears too hesitant or eager to use the tool, do some finetuning on conversations where the model should or should not use it.</div><br/></div></div></div></div></div></div></div></div><div id="41478922" class="c"><input type="checkbox" id="c-41478922" checked=""/><div class="controls bullet"><span class="by">lloydatkinson</span><span>|</span><a href="#41478481">prev</a><span>|</span><a href="#41478663">next</a><span>|</span><label class="collapse" for="c-41478922">[-]</label><label class="expand" for="c-41478922">[1 more]</label></div><br/><div class="children"><div class="content">I thought this was a really interesting read, I liked the scientific&#x2F;methodical approach which seems rare when it comes to an entire domain full of cryptoaitechbros.<p>What was used to render the chart in the middle with the red and green bars?</div><br/></div></div></div></div></div></div></div></body></html>