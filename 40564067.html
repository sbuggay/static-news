<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1717491672767" as="style"/><link rel="stylesheet" href="styles.css?v=1717491672767"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://tridao.me/blog/2024/mamba2-part1-model/">Mamba-2 – State Space Duality</a> <span class="domain">(<a href="https://tridao.me">tridao.me</a>)</span></div><div class="subtext"><span>bratao</span> | <span>26 comments</span></div><br/><div><div id="40564607" class="c"><input type="checkbox" id="c-40564607" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40565424">next</a><span>|</span><label class="collapse" for="c-40564607">[-]</label><label class="expand" for="c-40564607">[11 more]</label></div><br/><div class="children"><div class="content">Dao and Gu show that if you simplify Mamba so its state-space layer uses a diagonal matrix A_t that is a scalar times the identity matrix, i.e., A_t = a_t I, the state-space transformation can be expressed as a form of causal linear attention[a] by compounding coefficients a_1 ... a_t at each time step t. The equivalence of the simplified state-space layer and causal linear attention constitute the duality the authors refer to in the title. By taking advantage of this duality, Mamba-2 can be trained more efficiently, i.e., faster than original Mamba on GPUs.<p>Theoretical stuff aside, Mamba-2&#x27;s performance seems to scale slightly better than original Mamba: <a href="https:&#x2F;&#x2F;tridao.me&#x2F;assets&#x2F;img&#x2F;2024-05-31-mamba-2&#x2F;pile_8k_mamba2-1400.webp" rel="nofollow">https:&#x2F;&#x2F;tridao.me&#x2F;assets&#x2F;img&#x2F;2024-05-31-mamba-2&#x2F;pile_8k_mamb...</a><p>Here&#x27;s the code implementing Mamba-2: <a href="https:&#x2F;&#x2F;github.com&#x2F;state-spaces&#x2F;mamba&#x2F;blob&#x2F;main&#x2F;mamba_ssm&#x2F;modules&#x2F;mamba2.py">https:&#x2F;&#x2F;github.com&#x2F;state-spaces&#x2F;mamba&#x2F;blob&#x2F;main&#x2F;mamba_ssm&#x2F;mo...</a><p>Great work by Tri Dao (of FlashAttention fame) and Albert Gu, as usual.<p>The key question, for me and many others, is whether Mamba, Mamba-2, RWKV, and other linear RNN &#x2F; linear attention models will ever match the performance of standard Softmax attention. My understanding and experience is that all the linear attention models out there [b] still underperform Softmax attention on things like recall tasks.[c]<p>---<p>[a] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.16236" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.16236</a><p>[b] <a href="https:&#x2F;&#x2F;github.com&#x2F;topics&#x2F;linear-attention">https:&#x2F;&#x2F;github.com&#x2F;topics&#x2F;linear-attention</a> &#x2F; <a href="https:&#x2F;&#x2F;github.com&#x2F;topics&#x2F;linear-attention-model">https:&#x2F;&#x2F;github.com&#x2F;topics&#x2F;linear-attention-model</a> -- this list is by no means complete!<p>[c] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.01032" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.01032</a></div><br/><div id="40566025" class="c"><input type="checkbox" id="c-40566025" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#40564607">parent</a><span>|</span><a href="#40572087">next</a><span>|</span><label class="collapse" for="c-40566025">[-]</label><label class="expand" for="c-40566025">[5 more]</label></div><br/><div class="children"><div class="content">Notably, humans also underperform Transformers on recall tasks. And yet we do ok on many others, even with our imperfect recall. So I hope we can identify a set of high value tasks on which these new architectures outperform Transformers and start benchmarking them on Transformers, too. Recall isn’t really “all you need” in this space, although it certainly impresses and helps to plug the capability gaps.</div><br/><div id="40570986" class="c"><input type="checkbox" id="c-40570986" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40564607">root</a><span>|</span><a href="#40566025">parent</a><span>|</span><a href="#40572087">next</a><span>|</span><label class="collapse" for="c-40570986">[-]</label><label class="expand" for="c-40570986">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Notably, humans also underperform Transformers on recall tasks.<p>Source? I don&#x27;t think this is remotely true based on even the most recent benchmarks I&#x27;ve seen.</div><br/><div id="40571554" class="c"><input type="checkbox" id="c-40571554" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#40564607">root</a><span>|</span><a href="#40570986">parent</a><span>|</span><a href="#40572087">next</a><span>|</span><label class="collapse" for="c-40571554">[-]</label><label class="expand" for="c-40571554">[3 more]</label></div><br/><div class="children"><div class="content">Can you memorize “War and Peace” in one sitting and randomly and precisely recall things from it on demand? No? Didn’t think you could. Could you perhaps translate Farsi into Hawaiian and then speak it? Can you give instant, and nearly always correct to almost any textbook question irrespective of the domain? All of that is recall, and all of that GPT4 could do for well over a year.</div><br/><div id="40571916" class="c"><input type="checkbox" id="c-40571916" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#40564607">root</a><span>|</span><a href="#40571554">parent</a><span>|</span><a href="#40572087">next</a><span>|</span><label class="collapse" for="c-40571916">[-]</label><label class="expand" for="c-40571916">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Can you memorize “War and Peace in one sitting - neither can an LLM, unless you weaken it to &quot;well I meant input that fits in context&quot;, not memorize<p>&gt; precisely recall things from it on demand - &quot;precisely&quot; is load-bearing here, <i>and</i> the question at hand. (no, LLMs don&#x27;t have perfect or precise recall, even from in-context material, and it&#x27;s not particularly close)<p>&gt; translate Farsi into Hawaiian and then speak it - 0 models, is this supposed to be a gpt-4o reference? What does this have to do with &quot;perfect recall?&quot; Let&#x27;s pretend it does. Are you claiming that gpt-4o has a 100% success rate translating Farsi to Hawaiian?<p>&gt; Can you give instant, and nearly always correct to almost any textbook question - <i>sobs in just spent 4 days benchmarking</i>. For the record, best non-RAG I have is gpt-4o at 86% on MedQA and LegalBench. With RAG, 93%. RAG gpt-4o just barely scrapes by my cofounders USMLE scores. It&#x27;s excellent. But it&#x27;s not superhuman.<p>&gt; All of that is recall - no, translation is not recall</div><br/><div id="40572109" class="c"><input type="checkbox" id="c-40572109" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40564607">root</a><span>|</span><a href="#40571916">parent</a><span>|</span><a href="#40572087">next</a><span>|</span><label class="collapse" for="c-40572109">[-]</label><label class="expand" for="c-40572109">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re quoting yourself. It&#x27;s hard to understand which quotes are yours or not.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40572087" class="c"><input type="checkbox" id="c-40572087" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40564607">parent</a><span>|</span><a href="#40566025">prev</a><span>|</span><a href="#40566786">next</a><span>|</span><label class="collapse" for="c-40572087">[-]</label><label class="expand" for="c-40572087">[1 more]</label></div><br/><div class="children"><div class="content">It is fundamentally impossible for linear attention to outperform quadratic attention. What you could do instead is have a few quadratic attention layers in the first layers and one in the last and have everything else use linear attention. This would allow you to have a context length of millions of tokens within 24 GB VRAM for a 7b model because you have eliminated the KV cache for the inner layers, while still retaining the ability to perform any token vs any other token attention. The final layer is also somewhat important since it allows the model to see all its reasoning outputs and attend over its previous reasoning steps.</div><br/></div></div><div id="40566786" class="c"><input type="checkbox" id="c-40566786" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#40564607">parent</a><span>|</span><a href="#40572087">prev</a><span>|</span><a href="#40565424">next</a><span>|</span><label class="collapse" for="c-40566786">[-]</label><label class="expand" for="c-40566786">[4 more]</label></div><br/><div class="children"><div class="content">Quadratic transformers outperform weaker forms of attention on recall tasks, but recurrent models are strictly more powerful than transformers (when they don&#x27;t use chain of thought) at state tracking problems. Mamba 2 likely has the same limitations with state tracking as a full transformer due to being parallelizable: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.08819" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.08819</a> .</div><br/><div id="40567198" class="c"><input type="checkbox" id="c-40567198" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40564607">root</a><span>|</span><a href="#40566786">parent</a><span>|</span><a href="#40565424">next</a><span>|</span><label class="collapse" for="c-40567198">[-]</label><label class="expand" for="c-40567198">[3 more]</label></div><br/><div class="children"><div class="content">Thank you for sharing this. I&#x27;ve added it to my reading list!</div><br/><div id="40569620" class="c"><input type="checkbox" id="c-40569620" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#40564607">root</a><span>|</span><a href="#40567198">parent</a><span>|</span><a href="#40565424">next</a><span>|</span><label class="collapse" for="c-40569620">[-]</label><label class="expand" for="c-40569620">[2 more]</label></div><br/><div class="children"><div class="content">The last figure in this paper is a huge disappointment when you consider how reality meets such theoretical arguments.  A typical mamba model would have 100 layers (maybe 60 for snaller ones). That figure scales up to 4 layers and these are sufficient for the problem they consider so the argument goes that another RNN only needed one layer for it.</div><br/><div id="40572147" class="c"><input type="checkbox" id="c-40572147" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#40564607">root</a><span>|</span><a href="#40569620">parent</a><span>|</span><a href="#40565424">next</a><span>|</span><label class="collapse" for="c-40572147">[-]</label><label class="expand" for="c-40572147">[1 more]</label></div><br/><div class="children"><div class="content">Why bother with more layers? The inability of the conventional transformer to not have recurrent layers is mostly a weakness. The final layers of an LLM do very little (except the last) and most of the time just let the token pass through from the layer that actually determined it. A recurrent architecture could dynamically perform as many passes as it needs to produce the next token. This would result in a speedup for easy tokens and a slowdown for hard tokens compared to the fixed &quot;you must go through all layers regardless&quot; architecture of classical LLMs.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40565424" class="c"><input type="checkbox" id="c-40565424" checked=""/><div class="controls bullet"><span class="by">evnc</span><span>|</span><a href="#40564607">prev</a><span>|</span><a href="#40565713">next</a><span>|</span><label class="collapse" for="c-40565424">[-]</label><label class="expand" for="c-40565424">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a bit of a noob here, but if<p>a) a linear SSM (a form of RNN?) is equivalent to Attention without the scaling and softmax; and<p>b) Attention is &quot;all you need&quot; and the thing that made Transformers radically outperform all the previous architectures like LSTMs that used to dominate NLP;<p>does that imply c) the scaling and softmax parts of the attention equation, in particular, is the magic touch that makes Transformers work so well?</div><br/><div id="40566998" class="c"><input type="checkbox" id="c-40566998" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#40565424">parent</a><span>|</span><a href="#40565713">next</a><span>|</span><label class="collapse" for="c-40566998">[-]</label><label class="expand" for="c-40566998">[2 more]</label></div><br/><div class="children"><div class="content">The major difference is that transformer state grows as the sequence gets longer, while recurrent models use a fixed size state. So presumably at sequence length (T) &gt; size of state space (N), the transformer will be better on some very specific tasks. Not all, especially those that require the model to select information from the beginning of the sequence conditional on something at the end of the sequence.  Transformers can refocus any time, while SSNs need to guess right from the start what to keep and what to drop. SSNs could use the old trick of repeating the input twice to allow the end to condition on the beginning as well.<p>An important role is held by the softmax function which normalizes the attention scores, allowing the model to weigh different parts of the input sequence dynamically. This means that, unlike RNNs which sequentially process inputs and update states, Transformers can directly access and prioritize information from any part of the sequence, and they are not slower for T &lt; N.</div><br/><div id="40570403" class="c"><input type="checkbox" id="c-40570403" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#40565424">root</a><span>|</span><a href="#40566998">parent</a><span>|</span><a href="#40565713">next</a><span>|</span><label class="collapse" for="c-40570403">[-]</label><label class="expand" for="c-40570403">[1 more]</label></div><br/><div class="children"><div class="content">Nice description, thanks.</div><br/></div></div></div></div></div></div><div id="40565713" class="c"><input type="checkbox" id="c-40565713" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#40565424">prev</a><span>|</span><a href="#40565034">next</a><span>|</span><label class="collapse" for="c-40565713">[-]</label><label class="expand" for="c-40565713">[1 more]</label></div><br/><div class="children"><div class="content">Has anyone tried training yet and are there any obvious pitfalls for multi GPU training like there were in mamba-1?</div><br/></div></div><div id="40565034" class="c"><input type="checkbox" id="c-40565034" checked=""/><div class="controls bullet"><span class="by">adt</span><span>|</span><a href="#40565713">prev</a><span>|</span><a href="#40565304">next</a><span>|</span><label class="collapse" for="c-40565034">[-]</label><label class="expand" for="c-40565034">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;lifearchitect.ai&#x2F;models-table&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lifearchitect.ai&#x2F;models-table&#x2F;</a></div><br/><div id="40565415" class="c"><input type="checkbox" id="c-40565415" checked=""/><div class="controls bullet"><span class="by">webappguy</span><span>|</span><a href="#40565034">parent</a><span>|</span><a href="#40565304">next</a><span>|</span><label class="collapse" for="c-40565415">[-]</label><label class="expand" for="c-40565415">[1 more]</label></div><br/><div class="children"><div class="content">love this thanks!! Wish we could classify and rank tables and rows (like all OpenAI models).</div><br/></div></div></div></div><div id="40565304" class="c"><input type="checkbox" id="c-40565304" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#40565034">prev</a><span>|</span><a href="#40566685">next</a><span>|</span><label class="collapse" for="c-40565304">[-]</label><label class="expand" for="c-40565304">[2 more]</label></div><br/><div class="children"><div class="content">&quot;From one perspective, Mamba-2 isn’t strictly better than Mamba-1: while it’s a dramatic improvement from a training perspective, Mamba-1 might be better from a pure inference perspective. Since inference speed of SSMs is entirely governed by the state dimension, if one wants to maximize performance for a target inference efficiency (i.e. for a particular state size N), then the increased expressivity of Mamba-1 might be better.&quot;</div><br/><div id="40565967" class="c"><input type="checkbox" id="c-40565967" checked=""/><div class="controls bullet"><span class="by">radarsat1</span><span>|</span><a href="#40565304">parent</a><span>|</span><a href="#40566685">next</a><span>|</span><label class="collapse" for="c-40565967">[-]</label><label class="expand" for="c-40565967">[1 more]</label></div><br/><div class="children"><div class="content">Seems like since Mamba 2 is a constrained version of Mamba 1, maybe Mamba 1 could be used in a fine-tuning stage.</div><br/></div></div></div></div><div id="40566685" class="c"><input type="checkbox" id="c-40566685" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#40565304">prev</a><span>|</span><a href="#40564591">next</a><span>|</span><label class="collapse" for="c-40566685">[-]</label><label class="expand" for="c-40566685">[1 more]</label></div><br/><div class="children"><div class="content">This appears to be huge. Win-win-win for fast LU factorization!</div><br/></div></div><div id="40564591" class="c"><input type="checkbox" id="c-40564591" checked=""/><div class="controls bullet"><span class="by">eranation</span><span>|</span><a href="#40566685">prev</a><span>|</span><a href="#40564865">next</a><span>|</span><label class="collapse" for="c-40564591">[-]</label><label class="expand" for="c-40564591">[4 more]</label></div><br/><div class="children"><div class="content">I’ll bite: can anyone please eli5 to the non PhDs among us?</div><br/><div id="40564768" class="c"><input type="checkbox" id="c-40564768" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#40564591">parent</a><span>|</span><a href="#40564717">next</a><span>|</span><label class="collapse" for="c-40564768">[-]</label><label class="expand" for="c-40564768">[1 more]</label></div><br/><div class="children"><div class="content">restricts freedom in one of the parameters (A) to make training substantially more efficient (easier for a GPU to churn through).<p>the actual flops involved are similar to the original SSM-based version, but that&#x27;s harder to formulate as strictly matrix multiplications</div><br/></div></div><div id="40564717" class="c"><input type="checkbox" id="c-40564717" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#40564591">parent</a><span>|</span><a href="#40564768">prev</a><span>|</span><a href="#40565871">next</a><span>|</span><label class="collapse" for="c-40564717">[-]</label><label class="expand" for="c-40564717">[1 more]</label></div><br/><div class="children"><div class="content">TL;DR: The authors show that if you simplify Mamba so its state-space layer uses a diagonal matrix A that is a scalar times the identity matrix, the state-space transformation can be expressed as a form of causal linear attention.[a] That&#x27;s the duality the authors refer to in the title. The key practical benefit is that it enables more efficient (faster) training on GPUs.<p>---<p>[a] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.16236" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.16236</a></div><br/></div></div><div id="40565871" class="c"><input type="checkbox" id="c-40565871" checked=""/><div class="controls bullet"><span class="by">xcodevn</span><span>|</span><a href="#40564591">parent</a><span>|</span><a href="#40564717">prev</a><span>|</span><a href="#40564865">next</a><span>|</span><label class="collapse" for="c-40565871">[-]</label><label class="expand" for="c-40565871">[1 more]</label></div><br/><div class="children"><div class="content">tldr: mamba is not as good as transformer.</div><br/></div></div></div></div><div id="40564865" class="c"><input type="checkbox" id="c-40564865" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#40564591">prev</a><span>|</span><label class="collapse" for="c-40564865">[-]</label><label class="expand" for="c-40564865">[1 more]</label></div><br/><div class="children"><div class="content">TLDR for non-NLP people: Mamba-2 is <i>much</i> faster to train than Mamba-1.</div><br/></div></div></div></div></div></div></div></body></html>