<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1711098068509" as="style"/><link rel="stylesheet" href="styles.css?v=1711098068509"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.quantamagazine.org/how-chain-of-thought-reasoning-helps-neural-networks-compute-20240321/">How Chain-of-Thought Reasoning Helps Neural Networks Compute</a> <span class="domain">(<a href="https://www.quantamagazine.org">www.quantamagazine.org</a>)</span></div><div class="subtext"><span>amichail</span> | <span>27 comments</span></div><br/><div><div id="39787007" class="c"><input type="checkbox" id="c-39787007" checked=""/><div class="controls bullet"><span class="by">stygiansonic</span><span>|</span><a href="#39787632">next</a><span>|</span><label class="collapse" for="c-39787007">[-]</label><label class="expand" for="c-39787007">[19 more]</label></div><br/><div class="children"><div class="content">A simplified explanation, which I think I heard from Karpathy, is that transformer models only do computation when they generate (decode) a token. So generating more tokens (using CoT) gives the model more time to “think”.<p>Obviously this doesn’t capture all the nuance.</div><br/><div id="39787572" class="c"><input type="checkbox" id="c-39787572" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#39787007">parent</a><span>|</span><a href="#39788452">next</a><span>|</span><label class="collapse" for="c-39787572">[-]</label><label class="expand" for="c-39787572">[5 more]</label></div><br/><div class="children"><div class="content">I have another explanation. LLMs are essentially trained on &quot;A B&quot;, i.e. is it plausible that B follows A.<p>There&#x27;s simply a much larger space of possibilities for shorter completions, A B1, A B2, etc. that are plausible. Like if I ask you to give a short reply to a nuanced question, you could reply with a thoughtful answer, a plausible superficially correct sounding answer, convincing BS, etc.<p>Whereas if you force someone to explain their reasoning, the space of plausible completions reduces. If you start with convincing BS and work through it honestly, you will conclude that you should reverse. (This is similar to how one of the best ways to debunk toxic beliefs with honest people is simply through openly asking them to play out the consequences and walking through the impact of stuff that sounds good without much thought.)<p>This is similar to the reason that loading your prompt with things that reduce the space of plausible completions is effective prompt engineering.</div><br/><div id="39787611" class="c"><input type="checkbox" id="c-39787611" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#39787007">root</a><span>|</span><a href="#39787572">parent</a><span>|</span><a href="#39787609">next</a><span>|</span><label class="collapse" for="c-39787611">[-]</label><label class="expand" for="c-39787611">[1 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re right. I would go a step further and say that all learning is roughly synonymous with reducing the output space, and that humans do the exact same thing. There are more ways to get the wrong answer to a math problem than there are to get the right answer. When you learn someone&#x27;s name, you&#x27;re narrowing your output to be a single name rather than all plausible names.<p>The output of a generative model is practically infinite. I suspect it&#x27;s possible to continually narrow the space of completions and never converge on a single output. If this turns out to be true, it would bode well for the scalability of few-shot learning.</div><br/></div></div><div id="39787609" class="c"><input type="checkbox" id="c-39787609" checked=""/><div class="controls bullet"><span class="by">jorl17</span><span>|</span><a href="#39787007">root</a><span>|</span><a href="#39787572">parent</a><span>|</span><a href="#39787611">prev</a><span>|</span><a href="#39788058">next</a><span>|</span><label class="collapse" for="c-39787609">[-]</label><label class="expand" for="c-39787609">[1 more]</label></div><br/><div class="children"><div class="content">I was going to write pretty much this exact same comment. I am an amateur in how LLMs work, definitely, but I always thought this was the plausible explanation.<p>If I want the &quot;assistant &quot;LLM to tell me &quot;How much 5 times 2 is&quot;, if I feed it the line &quot;5 * 2 = &quot; as if it&#x27;s already started giving that answer, it will very likely write 5*2 = 10.<p>Since LLMs operate on semantic relationships between tokens, the more a bunch of tokens are &quot;close&quot; to a given &quot;semantic topic&quot;, the more the LLM will keep outputting tokens in that topic. It&#x27;s the reason why if you ask an LLM to &quot;review and grade poetry&quot;, eventually it starts saying the same thing even about rather different poems -- the output is so filled with the same words, that it just keeps repeating them.<p>Another example:<p>If I ask the LLM to solve me a riddle, just by itself, the LLM may get it wrong. If, however, I start the answer, unravelling a tiny bit of the problem it will very likely give the right answer, as if it&#x27;s been &quot;guided&quot; onto the right &quot;problem space&quot;.<p>By getting LLMs to &quot;say&quot; how they are going to solve things and checking for errors, each words basically tugs onto the next one, honing in on the correct solution.<p>In other words:<p>If an LLM has to answer a question -- any question --, but right after we ask the question we &quot;populate&quot; its answer with some text, what text is more likely to make the LLM answer incorrectly?<p>- Gibberish nonsense<p>- Something logical and related to the problem?<p>Evidently, the more gibberish we give to it, the more likely it is to get it wrong, since we&#x27;re moving away from the &quot;island of relevant semantic meaning&quot;, so to speak. So if we just get the LLM to feed itself more relevant tokens, it automatically guides itself to a better answer. It&#x27;s kind of like there&#x27;s an &quot;objective, ideal&quot; sequence of tokens, and it can work as an attractor. The more the LLM outputs words, the more it gets attracted to that sequence...that....&quot;island of relevant semantic meaning&quot;.<p>But, again, I know nothing of this. This is just how I view it, conceptually. It&#x27;s probably very wrong.</div><br/></div></div><div id="39788058" class="c"><input type="checkbox" id="c-39788058" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39787007">root</a><span>|</span><a href="#39787572">parent</a><span>|</span><a href="#39787609">prev</a><span>|</span><a href="#39788452">next</a><span>|</span><label class="collapse" for="c-39788058">[-]</label><label class="expand" for="c-39788058">[2 more]</label></div><br/><div class="children"><div class="content">It helps, but it still gets stuck in local optima based on what it started with. I&#x27;ve never seen it turn around and correct its faulty reasoning unless it tried to actually run the code and observed an Exception. If I respond with &quot;but have you considered XYZ?&quot;, my leading question will usually cause it to correct itself, even when it wasn&#x27;t incorrect.<p>We need some way to generate multiple independent thoughts in parallel. Each separate thought is constructed using chain of thought to improve the reliability. Then you have some way to &quot;reduce&quot; these multiple thoughts into a single solution. The analogy would be a human brainstorming session where we try to attack the same problem from multiple angles and we try to decorrelate each idea&#x2F;approach.</div><br/><div id="39788150" class="c"><input type="checkbox" id="c-39788150" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#39787007">root</a><span>|</span><a href="#39788058">parent</a><span>|</span><a href="#39788452">next</a><span>|</span><label class="collapse" for="c-39788150">[-]</label><label class="expand" for="c-39788150">[1 more]</label></div><br/><div class="children"><div class="content">We already have that, it&#x27;s called beam decoding, and there are three of thought solutions as well, for each beam you can pick the one with the best logprob, but it&#x27;s not a given that the result will be better because logprob only capture the model decisiveness not correctness, so it&#x27;ll still fail if a model is confidently wrong.</div><br/></div></div></div></div></div></div><div id="39788452" class="c"><input type="checkbox" id="c-39788452" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#39787007">parent</a><span>|</span><a href="#39787572">prev</a><span>|</span><a href="#39788197">next</a><span>|</span><label class="collapse" for="c-39788452">[-]</label><label class="expand" for="c-39788452">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what I thought at first, but that actually doesn&#x27;t make sense, the amount of work done on a string is the same even if the string is followed by padding due to the mask used in attention. Then I realised that an LLM&#x27;s working memory is limited to its activations, which can be limiting. But it can extend its working memory by writing partial results to the output and reading it in. E.g. if you tell it to &quot;think of a number&quot; without telling you what it is it can&#x27;t do that, there is nowhere to store that number, it has no temporary storage other than the tape. But if you ask it to &quot;think step by step&quot; you let it store intermediate results (thoughts) on the tape, giving it extra storage it can use for thinking.</div><br/></div></div><div id="39788197" class="c"><input type="checkbox" id="c-39788197" checked=""/><div class="controls bullet"><span class="by">earslap</span><span>|</span><a href="#39787007">parent</a><span>|</span><a href="#39788452">prev</a><span>|</span><a href="#39787570">next</a><span>|</span><label class="collapse" for="c-39788197">[-]</label><label class="expand" for="c-39788197">[1 more]</label></div><br/><div class="children"><div class="content">The autoregressive transformer architecture has a constant cost per token, no matter how hard the task is. You can ask the most complicated reasoning question, and it takes the same amount of computation to generate the next token compared to the simplest yes &#x2F; no question. This is due to architectural constraints. Letting the LLM generate &quot;scratch&quot; data to compute (attend to relevant information) is a way of circumventing the constant cost limitation. The harder the task, the more &quot;scratch&quot; you need so more relevant context is available for future tokens.</div><br/></div></div><div id="39787570" class="c"><input type="checkbox" id="c-39787570" checked=""/><div class="controls bullet"><span class="by">XenophileJKO</span><span>|</span><a href="#39787007">parent</a><span>|</span><a href="#39788197">prev</a><span>|</span><a href="#39788095">next</a><span>|</span><label class="collapse" for="c-39787570">[-]</label><label class="expand" for="c-39787570">[2 more]</label></div><br/><div class="children"><div class="content">So my experience creating products on GPT3.5-Turbo is that there is an upper limit to how much instructional complexity the model can handle at a time. It isn&#x27;t really about &quot;adding computation&quot;, though you are doing this. The key is to construct the process so that the model only has to focus on a limited scope to make the decision on.<p>In effect you are kind of creating a tree structure of decisions that build off of each other. By generating intermediate tokens the model can now only pay attention to the smaller set of already collapsed decisions. It is a little more complicated than that as the model will create anticipatory behavior where intermediate steps get biased by an incorrect result that the model anticipates.</div><br/><div id="39787590" class="c"><input type="checkbox" id="c-39787590" checked=""/><div class="controls bullet"><span class="by">XenophileJKO</span><span>|</span><a href="#39787007">root</a><span>|</span><a href="#39787570">parent</a><span>|</span><a href="#39788095">next</a><span>|</span><label class="collapse" for="c-39787590">[-]</label><label class="expand" for="c-39787590">[1 more]</label></div><br/><div class="children"><div class="content">Also I should say it isn&#x27;t just instructional complexity, it is ambiguity which creates the upper limit on capability.</div><br/></div></div></div></div><div id="39788095" class="c"><input type="checkbox" id="c-39788095" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#39787007">parent</a><span>|</span><a href="#39787570">prev</a><span>|</span><a href="#39787060">next</a><span>|</span><label class="collapse" for="c-39788095">[-]</label><label class="expand" for="c-39788095">[2 more]</label></div><br/><div class="children"><div class="content">This begs the question: why is it that giving them more time to &quot;think&quot; yields better answers, and is there any limit to that? If I make them write hundreds of pages of explanation, there must be a diminishing returns of some kind. What influences the optimal amount of thinking?<p>My guess is that good answers are more well reasoned than answers that are short and to the point, and this is picked up in training or fine-tuning or some other step.<p>And probably the optimal amount of thinking has something to do with the training set or the size of the network (wild guesses).</div><br/><div id="39788213" class="c"><input type="checkbox" id="c-39788213" checked=""/><div class="controls bullet"><span class="by">lappa</span><span>|</span><a href="#39787007">root</a><span>|</span><a href="#39788095">parent</a><span>|</span><a href="#39787060">next</a><span>|</span><label class="collapse" for="c-39788213">[-]</label><label class="expand" for="c-39788213">[1 more]</label></div><br/><div class="children"><div class="content">Look at it from an algorithmic perspective. In computer science many algorithms take a non-constant number of steps to execute. However, in transformers models, there are a limited number of decoder blocks, and a limited number of FFN layers in each block. This presents a theoretical upper bound on the complexity of the algorithms a decoder network can solve in a single token generation pass.<p>This explains why GPT4 cannot accurately perform large number multiplication and decimal exponentiation. [0]<p>This example can extend to general natural language generation. While some answers can be immediately retrieved or generated by a &quot;cache&quot; &#x2F; algorithm which exists in latent space, some tokens have better quality when their latent-space algorithm is executed in multiple steps.<p>[0] <a href="https:&#x2F;&#x2F;www.semanticscholar.org&#x2F;reader&#x2F;817e52b815560f95171d8fa60f78dd965e885a65" rel="nofollow">https:&#x2F;&#x2F;www.semanticscholar.org&#x2F;reader&#x2F;817e52b815560f95171d8...</a></div><br/></div></div></div></div><div id="39787060" class="c"><input type="checkbox" id="c-39787060" checked=""/><div class="controls bullet"><span class="by">_boffin_</span><span>|</span><a href="#39787007">parent</a><span>|</span><a href="#39788095">prev</a><span>|</span><a href="#39787592">next</a><span>|</span><label class="collapse" for="c-39787060">[-]</label><label class="expand" for="c-39787060">[1 more]</label></div><br/><div class="children"><div class="content">One of the things I’ve been doing with the models I’ve been using with coding is adding the stack and primary dependencies in the system prompt and then asking or conversing.   It has helped out a lot, or at least feels like it has.</div><br/></div></div><div id="39787592" class="c"><input type="checkbox" id="c-39787592" checked=""/><div class="controls bullet"><span class="by">Zondartul</span><span>|</span><a href="#39787007">parent</a><span>|</span><a href="#39787060">prev</a><span>|</span><a href="#39787426">next</a><span>|</span><label class="collapse" for="c-39787592">[-]</label><label class="expand" for="c-39787592">[2 more]</label></div><br/><div class="children"><div class="content">The tokens are also necessary to store information, or at least off-load it from neuron activations.<p>E.g. if you asked an LLM &quot;think about X and then do Y&quot;, if the &quot;think X&quot; part is silent, the LLM has a high chance of:<p>a) just not doing that, or<p>b) thinking about it but then forgetting, because the capacity of &#x27;RAM&#x27; or neuron activations is unknown but probably less than a few tokens.<p>Actually, has anyone tried to measure how much non-context data (i.e. new data generated from context data) a LLM can keep &quot;in memory&quot; without writing it down?</div><br/><div id="39787975" class="c"><input type="checkbox" id="c-39787975" checked=""/><div class="controls bullet"><span class="by">pgorczak</span><span>|</span><a href="#39787007">root</a><span>|</span><a href="#39787592">parent</a><span>|</span><a href="#39787426">next</a><span>|</span><label class="collapse" for="c-39787975">[-]</label><label class="expand" for="c-39787975">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think commonly used LLM architectures have internal state that carries over between inference steps, so shouldn’t that be none? Unless you mean the previously generated tokens up to the context limit which is well defined.</div><br/></div></div></div></div><div id="39787426" class="c"><input type="checkbox" id="c-39787426" checked=""/><div class="controls bullet"><span class="by">ukuina</span><span>|</span><a href="#39787007">parent</a><span>|</span><a href="#39787592">prev</a><span>|</span><a href="#39787777">next</a><span>|</span><label class="collapse" for="c-39787426">[-]</label><label class="expand" for="c-39787426">[1 more]</label></div><br/><div class="children"><div class="content">This is true. You can get a similar effect by asking the model to plan its path first without writing any code, then asking it to review its plan for deficiencies, and finally asking it to enact the plan and write the code.</div><br/></div></div><div id="39787777" class="c"><input type="checkbox" id="c-39787777" checked=""/><div class="controls bullet"><span class="by">rdedev</span><span>|</span><a href="#39787007">parent</a><span>|</span><a href="#39787426">prev</a><span>|</span><a href="#39787015">next</a><span>|</span><label class="collapse" for="c-39787777">[-]</label><label class="expand" for="c-39787777">[1 more]</label></div><br/><div class="children"><div class="content">Do you think there is a fundamental difference between masked language modelling vs causal language modelling? I feel like most LLMs are decoder only models just cause they are easier to train because their attention mask is fixed</div><br/></div></div><div id="39787015" class="c"><input type="checkbox" id="c-39787015" checked=""/><div class="controls bullet"><span class="by">sadhorse</span><span>|</span><a href="#39787007">parent</a><span>|</span><a href="#39787777">prev</a><span>|</span><a href="#39787632">next</a><span>|</span><label class="collapse" for="c-39787015">[-]</label><label class="expand" for="c-39787015">[2 more]</label></div><br/><div class="children"><div class="content">Does every token requires a full model computation?</div><br/><div id="39787238" class="c"><input type="checkbox" id="c-39787238" checked=""/><div class="controls bullet"><span class="by">onedognight</span><span>|</span><a href="#39787007">root</a><span>|</span><a href="#39787015">parent</a><span>|</span><a href="#39787632">next</a><span>|</span><label class="collapse" for="c-39787238">[-]</label><label class="expand" for="c-39787238">[1 more]</label></div><br/><div class="children"><div class="content">No, you can cache some of the work you did when processing the previous tokens. This is one of the key optimization ideas designed into the architecture.</div><br/></div></div></div></div></div></div><div id="39787632" class="c"><input type="checkbox" id="c-39787632" checked=""/><div class="controls bullet"><span class="by">activatedgeek</span><span>|</span><a href="#39787007">prev</a><span>|</span><a href="#39787773">next</a><span>|</span><label class="collapse" for="c-39787632">[-]</label><label class="expand" for="c-39787632">[2 more]</label></div><br/><div class="children"><div class="content">I want to point out a tweet [1] that is very relevant to the miracle of CoT, and probably a simpler explanation.<p><pre><code>  &gt; Let&#x27;s think &quot;step by step&quot;!

  &gt; Another tidbit I like about data and prompts that miraculously work.
  &gt; Searching for this phrase resulted in this website (among others),  
  &gt; http:&#x2F;&#x2F;geteasysolution.com, containing many math step-by-step solutions. 
  &gt; How common are they? Quite.

  &gt; Makes you think.
</code></pre>
[1]: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;yanaiela&#x2F;status&#x2F;1765077404043952516" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;yanaiela&#x2F;status&#x2F;1765077404043952516</a></div><br/><div id="39787771" class="c"><input type="checkbox" id="c-39787771" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#39787632">parent</a><span>|</span><a href="#39787773">next</a><span>|</span><label class="collapse" for="c-39787771">[-]</label><label class="expand" for="c-39787771">[1 more]</label></div><br/><div class="children"><div class="content">Though that justifies the specific phrase, it doesn&#x27;t really contradict the usual explanations of how CoT works. Like... the phrase directs it into the conceptual space of a website that has lots of CoT examples, but if CoT didn&#x27;t help it think, that wouldn&#x27;t actually result in better outputs.</div><br/></div></div></div></div><div id="39787773" class="c"><input type="checkbox" id="c-39787773" checked=""/><div class="controls bullet"><span class="by">patcon</span><span>|</span><a href="#39787632">prev</a><span>|</span><a href="#39788445">next</a><span>|</span><label class="collapse" for="c-39787773">[-]</label><label class="expand" for="c-39787773">[1 more]</label></div><br/><div class="children"><div class="content">Chain of thought reminds me of &quot;muddling through&quot;, which immediately clicks with my intuition of the right approach to approximations of intelligence: <a href="https:&#x2F;&#x2F;studio.ribbonfarm.com&#x2F;p&#x2F;massed-muddler-intelligence#%C2%A7muddling-through-vs-godding-through" rel="nofollow">https:&#x2F;&#x2F;studio.ribbonfarm.com&#x2F;p&#x2F;massed-muddler-intelligence#...</a></div><br/></div></div><div id="39788445" class="c"><input type="checkbox" id="c-39788445" checked=""/><div class="controls bullet"><span class="by">woopsn</span><span>|</span><a href="#39787773">prev</a><span>|</span><a href="#39788246">next</a><span>|</span><label class="collapse" for="c-39788445">[-]</label><label class="expand" for="c-39788445">[1 more]</label></div><br/><div class="children"><div class="content">In computing we use analogies everywhere: stack, bus, web, garbage collector, parent, container, ...<p>Master became somewhat controversial recently, but overall the main risk our liberal repurposing of terms introduces is that we sometimes follow the &quot;wrong&quot; idea and design a machine that doesn&#x27;t do what it ought to, or is unnecessarily complicated, that we develop systems (and documentation etc) that are inefficient if not dumb.<p>In adopting &quot;thought&quot; terminology and other analogies to psychological processes I fear we&#x27;ll not just misunderstand this technology and how it works, but also degrade the rigour of machine science, damaging our credibility and misleading the public as well.<p>Nobody will ever make the mistake of supposing that &quot;rehydrating&quot; a data structure involves water, or that busy beaver machines are living beings. But the language coming out of the LLM field in particular causes these problems immediately, and they are extreme -- scientists and engineers themselves have trouble telling if it&#x27;s supposed to be an analogy or not.</div><br/></div></div><div id="39788246" class="c"><input type="checkbox" id="c-39788246" checked=""/><div class="controls bullet"><span class="by">MrYellowP</span><span>|</span><a href="#39788445">prev</a><span>|</span><a href="#39787564">next</a><span>|</span><label class="collapse" for="c-39788246">[-]</label><label class="expand" for="c-39788246">[1 more]</label></div><br/><div class="children"><div class="content">I thought this was already obvious.<p>It&#x27;s all just about the awareness of contexts. Want to improve it? Simply add a term to the prompt to unlock more considerations. Assuming we&#x27;ve not reached the edge of the context window, every new word &quot;unlocks&quot; new vectors with more context the language models adds to the considerations.<p>The similarity with how the human brain (seems to) works is so remarkable, it doesn&#x27;t even make sense not to use it as an analogue for how to better use language models.<p>When the results (same way of manipulating an LLM as manipulating a human brain ... using the right words) can be achieved the same way, why believe there&#x27;s a difference?<p>This is stuff one can learn over time by using&#x2F;researching 3B models. While most people seem to shun them, some of them are extremely powerfull, like the &quot;old&quot; orca mini 3B. I am still using that one! All they really need is better prompts and that approach works perfectly fine.<p>The biggest hurdle I&#x27;ve found is the usually small context window of such small models, but there&#x27;s ways of cheating around that without sacrificing too much of the quality using small rope extension, summarizing text, adding context words or leaving out letters of words in the prompt, virtually increasing the size of the context window.<p>If you want to improve the results of your language model, you should become a mentalist&#x2F;con-man&#x2F;magician&#x2F;social engineer. It sounds weird, but it works!</div><br/></div></div><div id="39787564" class="c"><input type="checkbox" id="c-39787564" checked=""/><div class="controls bullet"><span class="by">fzaninotto</span><span>|</span><a href="#39788246">prev</a><span>|</span><label class="collapse" for="c-39787564">[-]</label><label class="expand" for="c-39787564">[2 more]</label></div><br/><div class="children"><div class="content">Great article. Now what happens when you apply this idea and let a LLM continue a chain of thought beyond mere question answering? Some form of artificial consciousness.<p>We&#x27;ve made this experiment: <a href="https:&#x2F;&#x2F;marmelab.com&#x2F;blog&#x2F;2023&#x2F;06&#x2F;06&#x2F;artificial-consciousness.html" rel="nofollow">https:&#x2F;&#x2F;marmelab.com&#x2F;blog&#x2F;2023&#x2F;06&#x2F;06&#x2F;artificial-consciousnes...</a></div><br/><div id="39788625" class="c"><input type="checkbox" id="c-39788625" checked=""/><div class="controls bullet"><span class="by">starbugs</span><span>|</span><a href="#39787564">parent</a><span>|</span><label class="collapse" for="c-39788625">[-]</label><label class="expand" for="c-39788625">[1 more]</label></div><br/><div class="children"><div class="content">Material reductionism at its best. Now you have a stochastic parrot &quot;talking&quot; to itself. How can anyone get to the conclusion that this would even begin to resemble a tiny bit of what we call consciousness?<p>Good luck with this dead end.</div><br/></div></div></div></div></div></div></div></div></div></body></html>