<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1715504454133" as="style"/><link rel="stylesheet" href="styles.css?v=1715504454133"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://openreview.net/forum?id=2dnO3LLiJ1">Vision Transformers Need Registers</a>Â <span class="domain">(<a href="https://openreview.net">openreview.net</a>)</span></div><div class="subtext"><span>cscurmudgeon</span> | <span>17 comments</span></div><br/><div><div id="40330111" class="c"><input type="checkbox" id="c-40330111" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#40330303">next</a><span>|</span><label class="collapse" for="c-40330111">[-]</label><label class="expand" for="c-40330111">[10 more]</label></div><br/><div class="children"><div class="content">According to the paper, the &quot;registers&quot; are additional learnable tokens that are appended to the input sequence of a Vision Transformer model during training.<p>They are added after the patch embedding layer, with a learnable value, similar to the [CLS] token and then at the end of the Vision Transformer, the register tokens are discarded, and only the [CLS] token and patch tokens are used as image representations.<p>The register tokens provide a place for the model to store, process and retrieve global information during the forward pass, without repurposing patch tokens for this role.<p>Adding register tokens removes the artifacts and high-norm &quot;outlier&quot; tokens that otherwise appear in the feature maps of trained Vision Transformer models.<p>Using register tokens leads to smoother feature maps, improved performance on dense prediction tasks, and enables better unsupervised object discovery compared to the same models trained without the additional register tokens.<p>This is a neat result. For just a 2% increase in inference cost, you can significantly improve ViT model performance. Close to a free lunch.</div><br/><div id="40331489" class="c"><input type="checkbox" id="c-40331489" checked=""/><div class="controls bullet"><span class="by">larodi</span><span>|</span><a href="#40330111">parent</a><span>|</span><a href="#40330512">next</a><span>|</span><label class="collapse" for="c-40331489">[-]</label><label class="expand" for="c-40331489">[4 more]</label></div><br/><div class="children"><div class="content">This all token business is very shady, and the whole probability theory. You add token here and there and magic happens. Discreet math people can not take this lightly. Stochastic regexes is one thing, but this on a completely different level of mathematical debauchery.<p>Absolutely amazing this works.</div><br/><div id="40331524" class="c"><input type="checkbox" id="c-40331524" checked=""/><div class="controls bullet"><span class="by">programjames</span><span>|</span><a href="#40330111">root</a><span>|</span><a href="#40331489">parent</a><span>|</span><a href="#40332325">next</a><span>|</span><label class="collapse" for="c-40331524">[-]</label><label class="expand" for="c-40331524">[1 more]</label></div><br/><div class="children"><div class="content">Vision transformers are essentially just JPEG but with learned features rather than the Fourier transform.</div><br/></div></div><div id="40332325" class="c"><input type="checkbox" id="c-40332325" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#40330111">root</a><span>|</span><a href="#40331489">parent</a><span>|</span><a href="#40331524">prev</a><span>|</span><a href="#40331501">next</a><span>|</span><label class="collapse" for="c-40332325">[-]</label><label class="expand" for="c-40332325">[1 more]</label></div><br/><div class="children"><div class="content">I find it wild that the training process can do such things as forcing it repurpose background areas to begin with. The authors just observed abd optimized what the model was already doing by itself.</div><br/></div></div><div id="40331501" class="c"><input type="checkbox" id="c-40331501" checked=""/><div class="controls bullet"><span class="by">akomtu</span><span>|</span><a href="#40330111">root</a><span>|</span><a href="#40331489">parent</a><span>|</span><a href="#40332325">prev</a><span>|</span><a href="#40330512">next</a><span>|</span><label class="collapse" for="c-40331501">[-]</label><label class="expand" for="c-40331501">[1 more]</label></div><br/><div class="children"><div class="content">The modern Alchemy.</div><br/></div></div></div></div><div id="40330512" class="c"><input type="checkbox" id="c-40330512" checked=""/><div class="controls bullet"><span class="by">macleginn</span><span>|</span><a href="#40330111">parent</a><span>|</span><a href="#40331489">prev</a><span>|</span><a href="#40330685">next</a><span>|</span><label class="collapse" for="c-40330512">[-]</label><label class="expand" for="c-40330512">[1 more]</label></div><br/><div class="children"><div class="content">There was an attempt to add several CLS tokens to BERT, with less spectacular results: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2210.05043" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2210.05043</a></div><br/></div></div><div id="40330685" class="c"><input type="checkbox" id="c-40330685" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#40330111">parent</a><span>|</span><a href="#40330512">prev</a><span>|</span><a href="#40330830">next</a><span>|</span><label class="collapse" for="c-40330685">[-]</label><label class="expand" for="c-40330685">[1 more]</label></div><br/><div class="children"><div class="content">are there lessons here for regular (non vision) transformers? sounds close to attention sinks&#x2F;pause tokens?</div><br/></div></div><div id="40330830" class="c"><input type="checkbox" id="c-40330830" checked=""/><div class="controls bullet"><span class="by">johntb86</span><span>|</span><a href="#40330111">parent</a><span>|</span><a href="#40330685">prev</a><span>|</span><a href="#40331418">next</a><span>|</span><label class="collapse" for="c-40330830">[-]</label><label class="expand" for="c-40330830">[1 more]</label></div><br/><div class="children"><div class="content">For these tokens you first need to unembed the result of the final layer, the re-embed the resulting token on the next pass. Has anyone investigated passing the raw output of one pass to the input of the next?</div><br/></div></div><div id="40331418" class="c"><input type="checkbox" id="c-40331418" checked=""/><div class="controls bullet"><span class="by">hasmanean</span><span>|</span><a href="#40330111">parent</a><span>|</span><a href="#40330830">prev</a><span>|</span><a href="#40330393">next</a><span>|</span><label class="collapse" for="c-40331418">[-]</label><label class="expand" for="c-40331418">[1 more]</label></div><br/><div class="children"><div class="content">So is that what all the visual cues are in real life, things like fashion accessories, uniforms etc.?</div><br/></div></div><div id="40330393" class="c"><input type="checkbox" id="c-40330393" checked=""/><div class="controls bullet"><span class="by">kadushka</span><span>|</span><a href="#40330111">parent</a><span>|</span><a href="#40331418">prev</a><span>|</span><a href="#40330303">next</a><span>|</span><label class="collapse" for="c-40330393">[-]</label><label class="expand" for="c-40330393">[1 more]</label></div><br/><div class="children"><div class="content">Interesting. One other potential benefit is an easier quantization of the activations.</div><br/></div></div></div></div><div id="40330303" class="c"><input type="checkbox" id="c-40330303" checked=""/><div class="controls bullet"><span class="by">richdougherty</span><span>|</span><a href="#40330111">prev</a><span>|</span><a href="#40332663">next</a><span>|</span><label class="collapse" for="c-40330303">[-]</label><label class="expand" for="c-40330303">[1 more]</label></div><br/><div class="children"><div class="content">Related? &quot;Let&#x27;s Think Dot by Dot: Hidden Computation in Transformer Language Models&quot;
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.15758" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2404.15758</a><p>&gt;      Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., &#x27;......&#x27;) in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.<p>&gt; In this work, we demonstrate that transformers trained on the next-token prediction objective can achieve improved performance on certain tasks when given filler tokens, achieving perfect accuracy whereas the no-filler, immediate-answer setting achieves only low accuracy.<p>--<p>I wonder if we could get benefits from adding special computation&#x2F;register tokens to text LLMs?<p>More discussion:<p>- <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40182695">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40182695</a><p>- <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1cf2w5a&#x2F;transformers_can_use_meaningless_filler_tokens_eg&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1cf2w5a&#x2F;transfo...</a></div><br/></div></div><div id="40332663" class="c"><input type="checkbox" id="c-40332663" checked=""/><div class="controls bullet"><span class="by">ashvardanian</span><span>|</span><a href="#40330303">prev</a><span>|</span><a href="#40330390">next</a><span>|</span><label class="collapse" for="c-40332663">[-]</label><label class="expand" for="c-40332663">[1 more]</label></div><br/><div class="children"><div class="content">We tested dozens (maybe &gt;100) of papers&#x2F;ideas over the last years in vision and multimodal perception and this is one of the rare cases, where everything worked well! Neat idea and paper!<p>This model, for example, uses 4 register tokens, and combines them with Matryoshka-style losses for training, resulting in super-compact 64-dimensional embeddings, in case anyone is looking for CLIP alternatives: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;unum-cloud&#x2F;uform3-image-text-english-large" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;unum-cloud&#x2F;uform3-image-text-english-...</a></div><br/></div></div><div id="40330390" class="c"><input type="checkbox" id="c-40330390" checked=""/><div class="controls bullet"><span class="by">joaquincabezas</span><span>|</span><a href="#40332663">prev</a><span>|</span><a href="#40331537">next</a><span>|</span><label class="collapse" for="c-40330390">[-]</label><label class="expand" for="c-40330390">[1 more]</label></div><br/><div class="children"><div class="content">I was at ICLR and this was one of the best of this year, it was also evident during the poster session. Congrats to the authors!!</div><br/></div></div><div id="40331537" class="c"><input type="checkbox" id="c-40331537" checked=""/><div class="controls bullet"><span class="by">barbolo</span><span>|</span><a href="#40330390">prev</a><span>|</span><a href="#40330110">next</a><span>|</span><label class="collapse" for="c-40331537">[-]</label><label class="expand" for="c-40331537">[1 more]</label></div><br/><div class="children"><div class="content">Iâve been using DinoV2 for some months now. Iâve tried the models with 4 register tokens along with CLS + patch tokens. Iâve several embeddings (tokens) from previous model (no registers) which are part of my solution, so I didnât adopt the newer âregisterâ models because the CLS tokens are not aligned between 0 registers and 4 registers models. It would be nice if the CLS and patch tokens were somehow aligned between those models.</div><br/></div></div><div id="40329873" class="c"><input type="checkbox" id="c-40329873" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#40330110">prev</a><span>|</span><label class="collapse" for="c-40329873">[-]</label><label class="expand" for="c-40329873">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Attention sinks&quot; for vision models?</div><br/></div></div></div></div></div></div></div></body></html>