<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1712566861305" as="style"/><link rel="stylesheet" href="styles.css?v=1712566861305"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.eetimes.com/groq-ceo-we-no-longer-sell-hardware/">Groq CEO: &#x27;We No Longer Sell Hardware&#x27;</a> <span class="domain">(<a href="https://www.eetimes.com">www.eetimes.com</a>)</span></div><div class="subtext"><span>frozenport</span> | <span>114 comments</span></div><br/><div><div id="39965364" class="c"><input type="checkbox" id="c-39965364" checked=""/><div class="controls bullet"><span class="by">geor9e</span><span>|</span><a href="#39965030">next</a><span>|</span><label class="collapse" for="c-39965364">[-]</label><label class="expand" for="c-39965364">[16 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand why the comments are trash-talking Groq. They are the fastest LLM inference provider by a big margin. Why would they sell their hardware to any other company for any price? Keep it all for themselves and take over the market. 95% of my LLM requests go to Groq these days because it&#x27;s 0.25 seconds round trip for a complete answer. In comparison, &quot;Claude Instant&quot; takes about 4 seconds. The other 5% of my requests go to Claude Opus and GPT-4, when I&#x27;m willing to wait an excruciating 5+ seconds for a better answer. I hate waiting. Latency is king. Groq wins.</div><br/><div id="39965465" class="c"><input type="checkbox" id="c-39965465" checked=""/><div class="controls bullet"><span class="by">fitzn</span><span>|</span><a href="#39965364">parent</a><span>|</span><a href="#39965407">next</a><span>|</span><label class="collapse" for="c-39965465">[-]</label><label class="expand" for="c-39965465">[13 more]</label></div><br/><div class="children"><div class="content">What open source model are you using when you hit groq?<p>I just benchmarked some perf for some of my larger context window queries last week and groq&#x27;s API took 1.6 seconds versus 1.8 to 2.2 for OpenAI GPT-3.5-turbo. So, it wasn&#x27;t much faster. I almost emailed their support to see if I was doing something wrong. Would love to hear any details about your workload or the complexity of your queries.</div><br/><div id="39966573" class="c"><input type="checkbox" id="c-39966573" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#39965364">root</a><span>|</span><a href="#39965465">parent</a><span>|</span><a href="#39966357">next</a><span>|</span><label class="collapse" for="c-39966573">[-]</label><label class="expand" for="c-39966573">[2 more]</label></div><br/><div class="children"><div class="content">What context did I miss that implies they are using an open source model?</div><br/><div id="39967383" class="c"><input type="checkbox" id="c-39967383" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#39965364">root</a><span>|</span><a href="#39966573">parent</a><span>|</span><a href="#39966357">next</a><span>|</span><label class="collapse" for="c-39967383">[-]</label><label class="expand" for="c-39967383">[1 more]</label></div><br/><div class="children"><div class="content">If you go to GroqChat (which is like a demo app), they offer Gemma, Mistral, and LLaMa. These are all open-weights models.</div><br/></div></div></div></div><div id="39966322" class="c"><input type="checkbox" id="c-39966322" checked=""/><div class="controls bullet"><span class="by">laserbeam</span><span>|</span><a href="#39965364">root</a><span>|</span><a href="#39965465">parent</a><span>|</span><a href="#39966357">prev</a><span>|</span><a href="#39965407">next</a><span>|</span><label class="collapse" for="c-39966322">[-]</label><label class="expand" for="c-39966322">[9 more]</label></div><br/><div class="children"><div class="content">&gt; 1.6 vs 1.8-2.2 seconds<p>I believe certain companies would kill for 20% performance improvements on their main product.</div><br/><div id="39966544" class="c"><input type="checkbox" id="c-39966544" checked=""/><div class="controls bullet"><span class="by">gpapilion</span><span>|</span><a href="#39965364">root</a><span>|</span><a href="#39966322">parent</a><span>|</span><a href="#39966567">next</a><span>|</span><label class="collapse" for="c-39966544">[-]</label><label class="expand" for="c-39966544">[5 more]</label></div><br/><div class="children"><div class="content">I have lots of questions about how important latency is since you may be replacing many minutes or hours of a person’s time with undoubtedly a quicker response by any measure. This seems like a knee jerk reaction assuming latency is as important as it’s been with advertising.<p>I’m not convinced latency matters as much as groqs material tries to claim it does.</div><br/><div id="39966707" class="c"><input type="checkbox" id="c-39966707" checked=""/><div class="controls bullet"><span class="by">w-ll</span><span>|</span><a href="#39965364">root</a><span>|</span><a href="#39966544">parent</a><span>|</span><a href="#39966683">next</a><span>|</span><label class="collapse" for="c-39966707">[-]</label><label class="expand" for="c-39966707">[3 more]</label></div><br/><div class="children"><div class="content">When has latency ever not mattered?<p>Let alone &#x27;chat&#x27; use cases, but holding a reponse up for N*1.2 longer than it could holds all sorts of other resources up&#x2F;down stream.</div><br/><div id="39967316" class="c"><input type="checkbox" id="c-39967316" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#39965364">root</a><span>|</span><a href="#39966707">parent</a><span>|</span><a href="#39966683">next</a><span>|</span><label class="collapse" for="c-39967316">[-]</label><label class="expand" for="c-39967316">[2 more]</label></div><br/><div class="children"><div class="content">When it&#x27;s already faster than I can absorb the response, which for me as an organic brain includes the normal token generation rate of the free tier of ChatGPT.<p>If I was using them to process far more text, e.g. summarise long documents, or if I was using it as an inline editing assistant, then I&#x27;d care more about the speed.</div><br/><div id="39967390" class="c"><input type="checkbox" id="c-39967390" checked=""/><div class="controls bullet"><span class="by">qeternity</span><span>|</span><a href="#39965364">root</a><span>|</span><a href="#39967316">parent</a><span>|</span><a href="#39966683">next</a><span>|</span><label class="collapse" for="c-39967390">[-]</label><label class="expand" for="c-39967390">[1 more]</label></div><br/><div class="children"><div class="content">&gt; When it&#x27;s already faster than I can absorb the response<p>Streaming a response from a chatbot is only one use-case of LLMs.<p>I would argue the most interesting applications do not fall into this category.</div><br/></div></div></div></div></div></div><div id="39966683" class="c"><input type="checkbox" id="c-39966683" checked=""/><div class="controls bullet"><span class="by">frozenport</span><span>|</span><a href="#39965364">root</a><span>|</span><a href="#39966544">parent</a><span>|</span><a href="#39966707">prev</a><span>|</span><a href="#39966567">next</a><span>|</span><label class="collapse" for="c-39966683">[-]</label><label class="expand" for="c-39966683">[1 more]</label></div><br/><div class="children"><div class="content">I guess its tool calling? When you chain the LLMs together?</div><br/></div></div></div></div><div id="39966567" class="c"><input type="checkbox" id="c-39966567" checked=""/><div class="controls bullet"><span class="by">robrenaud</span><span>|</span><a href="#39965364">root</a><span>|</span><a href="#39966322">parent</a><span>|</span><a href="#39966544">prev</a><span>|</span><a href="#39966401">next</a><span>|</span><label class="collapse" for="c-39966567">[-]</label><label class="expand" for="c-39966567">[1 more]</label></div><br/><div class="children"><div class="content">Model quality matters a ton too. They aren&#x27;t serving OpenAI or Anthropic models, which are state of the art.</div><br/></div></div><div id="39966401" class="c"><input type="checkbox" id="c-39966401" checked=""/><div class="controls bullet"><span class="by">metadat</span><span>|</span><a href="#39965364">root</a><span>|</span><a href="#39966322">parent</a><span>|</span><a href="#39966567">prev</a><span>|</span><a href="#39965407">next</a><span>|</span><label class="collapse" for="c-39966401">[-]</label><label class="expand" for="c-39966401">[2 more]</label></div><br/><div class="children"><div class="content">&quot;kill&quot;, .. why would anyone kill for a fraction of a second in this case?  Informed folks know that LLM hosters aren&#x27;t raking in the big bucks.<p>They&#x27;re selling dreams and aspirations, and those are what&#x27;s driving the funding.</div><br/><div id="39967280" class="c"><input type="checkbox" id="c-39967280" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#39965364">root</a><span>|</span><a href="#39966401">parent</a><span>|</span><a href="#39965407">next</a><span>|</span><label class="collapse" for="c-39967280">[-]</label><label class="expand" for="c-39967280">[1 more]</label></div><br/><div class="children"><div class="content">Google has used LMs in search for years (just not trendy LLMs), and search is famously optimized to the millisecond. Visa uses LMs to perform fraud detection every time someone makes a transaction, which is also quite latency sensitive. I&#x27;m guessing &quot;informed folks&quot; aren&#x27;t so informed about the broader market.<p>OpenAI and Anthropic&#x27;s APIs are obviously not latency-driven. Same with comparable LLM API resellers like Azure. Most people are likely not expecting tight latency SLOs there. That said, chat experiences (esp. voice ones) would probably be even more valuable if they could react in &quot;human time&quot; instead of with few seconds delay.<p>Integrating specialized hardware that can shave inference to fractions of a second seems like something that could be useful in a variety of latency-sensitive opportunities. Especially if this allows larger language models to be used where traditionally they were too slow.</div><br/></div></div></div></div></div></div></div></div><div id="39965407" class="c"><input type="checkbox" id="c-39965407" checked=""/><div class="controls bullet"><span class="by">huac</span><span>|</span><a href="#39965364">parent</a><span>|</span><a href="#39965465">prev</a><span>|</span><a href="#39965030">next</a><span>|</span><label class="collapse" for="c-39965407">[-]</label><label class="expand" for="c-39965407">[2 more]</label></div><br/><div class="children"><div class="content">why don&#x27;t you stream the results?</div><br/><div id="39967010" class="c"><input type="checkbox" id="c-39967010" checked=""/><div class="controls bullet"><span class="by">tpetry</span><span>|</span><a href="#39965364">root</a><span>|</span><a href="#39965407">parent</a><span>|</span><a href="#39965030">next</a><span>|</span><label class="collapse" for="c-39967010">[-]</label><label class="expand" for="c-39967010">[1 more]</label></div><br/><div class="children"><div class="content">You still have to wait for the end of the streamed response until you can continue with your task.</div><br/></div></div></div></div></div></div><div id="39965030" class="c"><input type="checkbox" id="c-39965030" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965364">prev</a><span>|</span><a href="#39964943">next</a><span>|</span><label class="collapse" for="c-39965030">[-]</label><label class="expand" for="c-39965030">[32 more]</label></div><br/><div class="children"><div class="content">Interesting, I guess that is why I never got a response back from them about buying their stuff.<p>My guess is that they realized that just selling hardware is a lot harder than running it themselves. Deploying this level of compute is non-trivial, with very high rates of failure, as well as huge supply chain issues. If you have to sell the hardware and support people buying it, that is a world of trouble.<p>&gt; no-one wants to take the risk of buying a whole bunch of hardware<p>I do!<p>Nobody has stated it yet, but this is probably great news for tenstorrent.<p>Disclosure: building a cloud compute provider starting with AMD MI300x, and eventually any other high end hardware that our customers are asking for.</div><br/><div id="39966620" class="c"><input type="checkbox" id="c-39966620" checked=""/><div class="controls bullet"><span class="by">gpapilion</span><span>|</span><a href="#39965030">parent</a><span>|</span><a href="#39966108">next</a><span>|</span><label class="collapse" for="c-39966620">[-]</label><label class="expand" for="c-39966620">[2 more]</label></div><br/><div class="children"><div class="content">It’s basically their minimum cluster size for a reasonable model requires 8ish racks of compute.<p>Semi analysis did some cost estimates, and I did some but you’re likely paying somewhere in the 12 million dollar range for the equipment to serve a single query using llama-70b. Compare that to a couple of gpus, and it’s easy to see why they are struggling to sell hardware, they can’t scale down.<p>Since they didn’t use hbm, you need to stich enough cards together to get the memory to hold your model. It takes a lot of 256mb cards to get to 64gb, and there isn’t a good way to try the tech out since a single rack really can’t serve an LLM.<p>The cloud provider path sounds riskier since that’s two capital intensive businesses, chip design and production and running a cloud service provider.</div><br/><div id="39966658" class="c"><input type="checkbox" id="c-39966658" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39966620">parent</a><span>|</span><a href="#39966108">next</a><span>|</span><label class="collapse" for="c-39966658">[-]</label><label class="expand" for="c-39966658">[1 more]</label></div><br/><div class="children"><div class="content">This is some fantastic insight. That would also inflate the opex (power&#x2F;space&#x2F;staffing) needs as well as people need to consider all the networking gear to hook this stuff together. 400G nic&#x2F;switches&#x2F;cables aren&#x27;t cheap and in some cases, very hard to obtain in any sort of quantity.<p>It does seem like an odd move in that case. I liken this to a company like Bitmain. Why sell the miners when you could just run them yourselves? Well, fact is that they do both. But in this case, Groq is turning off the sales. Who knows, maybe it just ends up being a temporary thing until they can sort all of the pieces out.</div><br/></div></div></div></div><div id="39966108" class="c"><input type="checkbox" id="c-39966108" checked=""/><div class="controls bullet"><span class="by">ukd1</span><span>|</span><a href="#39965030">parent</a><span>|</span><a href="#39966620">prev</a><span>|</span><a href="#39965100">next</a><span>|</span><label class="collapse" for="c-39966108">[-]</label><label class="expand" for="c-39966108">[7 more]</label></div><br/><div class="children"><div class="content">How do y&#x27;all compare to <a href="https:&#x2F;&#x2F;tensorwave.com" rel="nofollow">https:&#x2F;&#x2F;tensorwave.com</a>?</div><br/><div id="39966165" class="c"><input type="checkbox" id="c-39966165" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39966108">parent</a><span>|</span><a href="#39965100">next</a><span>|</span><label class="collapse" for="c-39966165">[-]</label><label class="expand" for="c-39966165">[6 more]</label></div><br/><div class="children"><div class="content">To be totally honest, I have no idea. When I first learned about them, I reached out to the CEO privately on LinkedIn, he asked what I was up to, I told him probably more than I should have (I come from an open source and transparent background), then he stopped talking to me entirely.<p>Since then, one of the co-founders blocked me on Twitter for pointing out that despite their claims, they were not the first MI300x to production. Neither were we, ElioVP gets that trophy, then Lamini, then GigaIO. Making us 4th and them 5th. I could go on and on with weird stuff I&#x27;ve seen them do, but it just isn&#x27;t productive here.<p>Anyway, I think we have some overlap since we both are one of the few startups on the planet that actually has MI300x. But beyond that, I believe strongly that this space is large enough for multiple players and I don&#x27;t see a need to be weird with each other. Apparently, I&#x27;m not on the same page though.<p>¯\_(ツ)_&#x2F;¯</div><br/><div id="39966304" class="c"><input type="checkbox" id="c-39966304" checked=""/><div class="controls bullet"><span class="by">ametrau</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39966165">parent</a><span>|</span><a href="#39965100">next</a><span>|</span><label class="collapse" for="c-39966304">[-]</label><label class="expand" for="c-39966304">[5 more]</label></div><br/><div class="children"><div class="content">For what it’s worth, to me, your approach is the one I’d prefer as a customer.</div><br/><div id="39966352" class="c"><input type="checkbox" id="c-39966352" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39966304">parent</a><span>|</span><a href="#39965100">next</a><span>|</span><label class="collapse" for="c-39966352">[-]</label><label class="expand" for="c-39966352">[4 more]</label></div><br/><div class="children"><div class="content">Thank you. I&#x27;ve been on HN since 2009. The most successful people I&#x27;ve seen here, are the ones that are transparent, honest and ethical.<p>I&#x27;m not trying to point fingers, I&#x27;m just focused on building a sustainable business and listening to my customers needs. The only way I can do that is by communicating with everyone around me as clearly and openly as I can. All our customers will know exactly where they stand, at all times.<p>I post a lot of open information on r&#x2F;AMD_Stock and the feedback that I&#x27;ve gotten there has been exceptional. People are excited to see if AMD can claw back a bit of the market. For the safety and success of AI, we don&#x27;t need team blue vs. team red, we need everyone to work towards having as many options as possible.<p>This is one way that I think we are going to differentiate ourselves. We won&#x27;t just have MI300x, we will have every best-of-the-best chunk of hardware that we can get our hands on. No longer will super computers be tied up behind govt&#x2F;edu grants. We want to democratize it. It has long been a goal of mine to build a super computer, and here is my chance. I&#x27;m excited.<p>One thing that sets us apart is that my co-founder and I have a ton of experience deploying, managing and optimizing 150,000 AMD GPUs and 20PB+ of storage. We did it ourselves, all through covid and all of the supply chain issues. I&#x27;m not sure many others have done that and this is something that we are well versed at doing.<p>I&#x27;m also seeing my competitors hiring a ton, while we are staying lean and mean with a very small team. I&#x27;d rather automate everything we deploy and focus all of our investors money on buying compute. We also have a pool of previous people we can hire from, which I think is quite an advantage over blanket hiring.</div><br/><div id="39966558" class="c"><input type="checkbox" id="c-39966558" checked=""/><div class="controls bullet"><span class="by">elbear</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39966352">parent</a><span>|</span><a href="#39965100">next</a><span>|</span><label class="collapse" for="c-39966558">[-]</label><label class="expand" for="c-39966558">[3 more]</label></div><br/><div class="children"><div class="content">Just wanted to say that I like how you see things.
Also, if you need help with infrastructure automation and are interested in using Nix for increased reproducibility, get in touch. I know you said you want to keep lean and already have a pipeline of potential candidates, but just in case.</div><br/><div id="39966590" class="c"><input type="checkbox" id="c-39966590" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39966558">parent</a><span>|</span><a href="#39965100">next</a><span>|</span><label class="collapse" for="c-39966590">[-]</label><label class="expand" for="c-39966590">[2 more]</label></div><br/><div class="children"><div class="content">Hi Lucian, thank you for stepping up to the plate. Recognized and appreciated. We aren&#x27;t quite in the hiring phase today, still bootstrapping and focused on growing with customer demand, but I have absolutely added you to my list for the future. Cheers!</div><br/><div id="39966669" class="c"><input type="checkbox" id="c-39966669" checked=""/><div class="controls bullet"><span class="by">elbear</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39966590">parent</a><span>|</span><a href="#39965100">next</a><span>|</span><label class="collapse" for="c-39966669">[-]</label><label class="expand" for="c-39966669">[1 more]</label></div><br/><div class="children"><div class="content">Sure. Thank you for answering. Good luck with what you&#x27;re doing!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39965100" class="c"><input type="checkbox" id="c-39965100" checked=""/><div class="controls bullet"><span class="by">aleph_minus_one</span><span>|</span><a href="#39965030">parent</a><span>|</span><a href="#39966108">prev</a><span>|</span><a href="#39966442">next</a><span>|</span><label class="collapse" for="c-39965100">[-]</label><label class="expand" for="c-39965100">[11 more]</label></div><br/><div class="children"><div class="content">&gt; If you have to sell the hardware and support people buying it, that is a world of trouble.<p>What is the difference between this and having to sell the cloud access and supporting the people who buy a subscription?</div><br/><div id="39965153" class="c"><input type="checkbox" id="c-39965153" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39965100">parent</a><span>|</span><a href="#39965125">next</a><span>|</span><label class="collapse" for="c-39965153">[-]</label><label class="expand" for="c-39965153">[5 more]</label></div><br/><div class="children"><div class="content">&gt; What is the difference between this and having to sell the cloud access and supporting the people who buy a subscription<p>Margins.<p>Pricing for cloud compute is much higher and servicing and management for the provider is much cheaper.<p>If I sold hardware directly, then I&#x27;m often on the hook for support contracts which can get pricy with hardware and distract from shipping future facing product features, as customers who purchase directly have longer upgrade windows due to logistical overhead.</div><br/><div id="39965199" class="c"><input type="checkbox" id="c-39965199" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39965153">parent</a><span>|</span><a href="#39965125">next</a><span>|</span><label class="collapse" for="c-39965199">[-]</label><label class="expand" for="c-39965199">[4 more]</label></div><br/><div class="children"><div class="content">The pricing isn&#x27;t necessarily much higher. The pricing is set up to amortize the cost of running things over a period of time. If you&#x27;re only going to use it for a short time, you pay more than someone who commits to a 3 year contract.<p>It also isn&#x27;t just the hardware capex, it is everything involved under the covers. Market pricing also factors into that as well. This is something I&#x27;ve struggled with myself quite a bit. I know all of my costs and what I&#x27;d like to charge, but because my offering is so brand new, until my competitors announced their pricing, I wasn&#x27;t sure what the market would tolerate.<p>Selling hardware directly is hard for exactly what you state though. Service contracts are a pain in the butt. All of this latest AI hardware has high rates of failures too. Up until recently with AMD coming to market with a great offering, the only thing people want any sort of quantity on are nvidia products. Groq probably realized that people buying 1-2 cards at a time, wasn&#x27;t going to be profitable.</div><br/><div id="39965213" class="c"><input type="checkbox" id="c-39965213" checked=""/><div class="controls bullet"><span class="by">alephnerd</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39965199">parent</a><span>|</span><a href="#39965125">next</a><span>|</span><label class="collapse" for="c-39965213">[-]</label><label class="expand" for="c-39965213">[3 more]</label></div><br/><div class="children"><div class="content">&gt; it is everything involved under the covers<p>Yep! You explained it better than me!<p>&gt; Groq probably realized that people buying 1-2 cards at a time, wasn&#x27;t going to be profitable<p>Yep! And if would have bogged them down by slowing down R&amp;D cycles and even fulfilling orders as they obviously are not placing orders the same size as Nvidia or AMD.<p>I wonder what this portends for SambaNova and other similar vendors as well.</div><br/><div id="39965284" class="c"><input type="checkbox" id="c-39965284" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39965213">parent</a><span>|</span><a href="#39965832">next</a><span>|</span><label class="collapse" for="c-39965284">[-]</label><label class="expand" for="c-39965284">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>I wonder what this portends for SambaNova and other similar vendors as well.</i><p>Time will tell. This is definitely an interesting development.</div><br/></div></div><div id="39965832" class="c"><input type="checkbox" id="c-39965832" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39965213">parent</a><span>|</span><a href="#39965284">prev</a><span>|</span><a href="#39965125">next</a><span>|</span><label class="collapse" for="c-39965832">[-]</label><label class="expand" for="c-39965832">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I wonder what this portends for SambaNova and other similar vendors as well.<p>They&#x27;re focused on services based full stack deployments afaik.<p>They come in with a rack and sell you on models as well.</div><br/></div></div></div></div></div></div></div></div><div id="39965125" class="c"><input type="checkbox" id="c-39965125" checked=""/><div class="controls bullet"><span class="by">themoonisachees</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39965100">parent</a><span>|</span><a href="#39965153">prev</a><span>|</span><a href="#39965553">next</a><span>|</span><label class="collapse" for="c-39965125">[-]</label><label class="expand" for="c-39965125">[2 more]</label></div><br/><div class="children"><div class="content">A lot. It&#x27;s the same reason amazon doesn&#x27;t sell servers and instead gives you access to a single instance that everyone pretends is the same but in reality is massively transient.</div><br/><div id="39966495" class="c"><input type="checkbox" id="c-39966495" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39965125">parent</a><span>|</span><a href="#39965553">next</a><span>|</span><label class="collapse" for="c-39966495">[-]</label><label class="expand" for="c-39966495">[1 more]</label></div><br/><div class="children"><div class="content">We give full bare metal access. If you just want one GPU, we give you a VM with PCIe pass through. If you take a whole box, we can give BMC and can give you access to the PDU itself, to hard reboot things remotely. It is as if you own the whole machine yourself.</div><br/></div></div></div></div><div id="39965553" class="c"><input type="checkbox" id="c-39965553" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39965100">parent</a><span>|</span><a href="#39965125">prev</a><span>|</span><a href="#39965133">next</a><span>|</span><label class="collapse" for="c-39965553">[-]</label><label class="expand" for="c-39965553">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>What is the difference between this and having to sell the cloud access and supporting the people who buy a subscription</i><p>Knowledge&#x2F;training.<p>If you&#x27;re shipping a brand new hardware arch, exposed as raw hardware, then you&#x27;re on the hook for training everyone in the world and fixing all their weird edge case uses.<p>I.e. are you willing to invest in Intel&#x2F;AMD&#x2F;Nvidia-scale QA and support?<p>If you&#x27;re exposing a PaaS (or even IaaS), then you have some levers you can tweak &#x2F; mask behind the scenes, so only <i>your</i> team need be experts at low-level operations.<p>For a fast-paced company, the latter model makes a lot more sense, at least until hardware+software stabilizes.</div><br/><div id="39965919" class="c"><input type="checkbox" id="c-39965919" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39965553">parent</a><span>|</span><a href="#39965133">next</a><span>|</span><label class="collapse" for="c-39965919">[-]</label><label class="expand" for="c-39965919">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>I.e. are you willing to invest in Intel&#x2F;AMD&#x2F;Nvidia-scale QA and support?</i><p>At least in our experience, the first line of support is from the chassis vendors. You don&#x27;t go to a store and buy MI300x. You buy them from someone like SMCI&#x2F;Dell, who provides the support. Of course, behind the scenes, they might be talking to AMD. Even those chassis companies often have other providers of their gear (like Exxact) as another line of defense as well.<p>In the case of Groq, it would have been death by 1000 cuts to have to support end users directly, especially if they are selling small quantities. It is much easier to just build data centers full of gear, maintain it yourself and then just rent the time on the hardware.</div><br/></div></div></div></div><div id="39965133" class="c"><input type="checkbox" id="c-39965133" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39965100">parent</a><span>|</span><a href="#39965553">prev</a><span>|</span><a href="#39966442">next</a><span>|</span><label class="collapse" for="c-39965133">[-]</label><label class="expand" for="c-39965133">[1 more]</label></div><br/><div class="children"><div class="content">Good question. Not much. If anything, what I&#x27;m doing is even harder because I will have multiple sources for the hardware. I have to deal with all of the hardware and data center issues, as well as the customers who rely on us to provide them access.<p>Good thing that I&#x27;m a glutton for punishment.</div><br/></div></div></div></div><div id="39966442" class="c"><input type="checkbox" id="c-39966442" checked=""/><div class="controls bullet"><span class="by">winwang</span><span>|</span><a href="#39965030">parent</a><span>|</span><a href="#39965100">prev</a><span>|</span><a href="#39965052">next</a><span>|</span><label class="collapse" for="c-39966442">[-]</label><label class="expand" for="c-39966442">[5 more]</label></div><br/><div class="children"><div class="content">How&#x27;s the overall software support for MI300 series? The hardware itself looks great.<p>(also, +100 to valuing honesty and transparency)</div><br/><div id="39966473" class="c"><input type="checkbox" id="c-39966473" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39966442">parent</a><span>|</span><a href="#39965052">next</a><span>|</span><label class="collapse" for="c-39966473">[-]</label><label class="expand" for="c-39966473">[4 more]</label></div><br/><div class="children"><div class="content">The hardware is actually pretty amazing. 192GB (or 1.5TB in a chassis), is a game changer.<p>I&#x27;ll let you know once I get my hands on them again. There really isn&#x27;t enough public information about them at all. So far, my friends at ElioVP [0] have published a blog post. Still with not enough detail for my taste, but I&#x27;m pretty sure he is limited by what he can talk about. Luckily, I am not.<p>I mention in another comment below that my current goal is to get a bunch of people to perform testing on them and then publish blog posts along with open source code. This way, we can start a repository of CI&#x2F;CD tests to see how things improve with time. ROCm 6.1 is rumored to be quite an improvement.<p>[0] <a href="https:&#x2F;&#x2F;www.evp.cloud&#x2F;post&#x2F;diving-deeper-insights-from-our-llm-inference-testing" rel="nofollow">https:&#x2F;&#x2F;www.evp.cloud&#x2F;post&#x2F;diving-deeper-insights-from-our-l...</a></div><br/><div id="39966493" class="c"><input type="checkbox" id="c-39966493" checked=""/><div class="controls bullet"><span class="by">winwang</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39966473">parent</a><span>|</span><a href="#39965052">next</a><span>|</span><label class="collapse" for="c-39966493">[-]</label><label class="expand" for="c-39966493">[3 more]</label></div><br/><div class="children"><div class="content">Nice! I&#x27;m pretty interested in GPGPU applications and MI300A, but I&#x27;m also just glad for more competition. Love that you hit up the LocalLLaMa sub.<p>Do you know if anyone&#x27;s tested CuPy stuff on MI300X?</div><br/><div id="39966516" class="c"><input type="checkbox" id="c-39966516" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39966493">parent</a><span>|</span><a href="#39965052">next</a><span>|</span><label class="collapse" for="c-39966516">[-]</label><label class="expand" for="c-39966516">[2 more]</label></div><br/><div class="children"><div class="content">We haven&#x27;t spec&#x27;d to buy A&#x27;s quite yet as you&#x27;re actually the first person I&#x27;ve heard even suggest them. If you&#x27;re truly interested, hit me up personally.<p>By default, we are putting dual 9754&#x27;s in the chassis, along with 3TB ram and 155TB nvme. A pretty beefy box. However, if you want to work with us, we can customize this to whatever customers need.<p>Effectively, we are the capex&#x2F;opex for something that requires a lot of upfront funding and want to work with businesses that would rather focus on the software side of things.</div><br/><div id="39966875" class="c"><input type="checkbox" id="c-39966875" checked=""/><div class="controls bullet"><span class="by">winwang</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39966516">parent</a><span>|</span><a href="#39965052">next</a><span>|</span><label class="collapse" for="c-39966875">[-]</label><label class="expand" for="c-39966875">[1 more]</label></div><br/><div class="children"><div class="content">Was mostly just checking to see if someone had already tested GPGPU, though I know some HPC labs like the MI300A. While I am starting a business, I&#x27;m not at the point of shipping software just yet (I wish!). Will definitely keep you in mind for if&#x2F;when we get to AMD -- it&#x27;s something I&#x27;d want, though that depends on achieving any modicum of success, haha.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39965052" class="c"><input type="checkbox" id="c-39965052" checked=""/><div class="controls bullet"><span class="by">rnts08</span><span>|</span><a href="#39965030">parent</a><span>|</span><a href="#39966442">prev</a><span>|</span><a href="#39965074">next</a><span>|</span><label class="collapse" for="c-39965052">[-]</label><label class="expand" for="c-39965052">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s interesting, something that I&#x27;ve been really wanting to get into as well, but where I am there is literally no venture capital to raise for this at the moment. I&#x27;d be interested to know more and&#x2F;or bounce some ideas though.</div><br/><div id="39965110" class="c"><input type="checkbox" id="c-39965110" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39965052">parent</a><span>|</span><a href="#39965074">next</a><span>|</span><label class="collapse" for="c-39965110">[-]</label><label class="expand" for="c-39965110">[1 more]</label></div><br/><div class="children"><div class="content">Extremely capital intensive, but also requires relationships in the industry at many levels. Luckily, I happen to have both and they are crazy enough to put their trust in me. I feel very grateful for that.</div><br/></div></div></div></div><div id="39965074" class="c"><input type="checkbox" id="c-39965074" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#39965030">parent</a><span>|</span><a href="#39965052">prev</a><span>|</span><a href="#39964943">next</a><span>|</span><label class="collapse" for="c-39965074">[-]</label><label class="expand" for="c-39965074">[4 more]</label></div><br/><div class="children"><div class="content">that or if you put chips in the hands of your customers, they may start to benchmark it against other equivalent solutions</div><br/><div id="39965099" class="c"><input type="checkbox" id="c-39965099" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39965074">parent</a><span>|</span><a href="#39964943">next</a><span>|</span><label class="collapse" for="c-39965099">[-]</label><label class="expand" for="c-39965099">[3 more]</label></div><br/><div class="children"><div class="content">Funny you should mention that. ;-)<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1bpgrdf&#x2F;wanted_amd_mi300x_benchmarks&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1bpgrdf&#x2F;wanted_...</a><p>I&#x27;ve got about a dozen people signed up. Just working through some hardware issues right now (see above about high rate of failures), and hope to have this resolved next week, so that I can get people onto them and doing their testing.</div><br/><div id="39965841" class="c"><input type="checkbox" id="c-39965841" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39965099">parent</a><span>|</span><a href="#39964943">next</a><span>|</span><label class="collapse" for="c-39965841">[-]</label><label class="expand" for="c-39965841">[2 more]</label></div><br/><div class="children"><div class="content">Shiny! You have you guys gotten MI300X&#x27;s working for non-inference use cases?</div><br/><div id="39965896" class="c"><input type="checkbox" id="c-39965896" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965030">root</a><span>|</span><a href="#39965841">parent</a><span>|</span><a href="#39964943">next</a><span>|</span><label class="collapse" for="c-39965896">[-]</label><label class="expand" for="c-39965896">[1 more]</label></div><br/><div class="children"><div class="content">We got them, then gave them to a customer to use for a week, got them back and now we are having some hardware issues that we are in the process of sorting out. I literally haven&#x27;t had more than a few hours time on them yet.<p>They _should_ work fine for both training and inference, but since nobody has done much in the way of public in-depth benchmarks yet... I was hoping to get people to do it for us in order to stay as unbiased as possible.<p>noticing now: strange that my previous comment was downvoted. Would be nice to understand what someone didn&#x27;t like about what I said!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39964943" class="c"><input type="checkbox" id="c-39964943" checked=""/><div class="controls bullet"><span class="by">gandalfgeek</span><span>|</span><a href="#39965030">prev</a><span>|</span><a href="#39965148">next</a><span>|</span><label class="collapse" for="c-39964943">[-]</label><label class="expand" for="c-39964943">[14 more]</label></div><br/><div class="children"><div class="content">They&#x27;re calling the lie on needing bleeding edge hardware for performance.<p>5 yr old silicon (14 nm!!) and no hbm.<p>Their secret sauce seems to be an ahead-of-time compiler that statically lays out entire computation, enabling zero contention at runtime. Basically, they stamp out all non-determinism.<p><a href="https:&#x2F;&#x2F;wow.groq.com&#x2F;isca-2022-paper" rel="nofollow">https:&#x2F;&#x2F;wow.groq.com&#x2F;isca-2022-paper</a></div><br/><div id="39965063" class="c"><input type="checkbox" id="c-39965063" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#39964943">parent</a><span>|</span><a href="#39964961">next</a><span>|</span><label class="collapse" for="c-39965063">[-]</label><label class="expand" for="c-39965063">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not really a lie though. They require 20x more chips (storing all the weights in sram instead of hbm is expensive!) than Nvidia GPUs for a ~2x speed increase. Overall the power cost is more expensive for groq than GPUs.</div><br/><div id="39965437" class="c"><input type="checkbox" id="c-39965437" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#39964943">root</a><span>|</span><a href="#39965063">parent</a><span>|</span><a href="#39966773">next</a><span>|</span><label class="collapse" for="c-39965437">[-]</label><label class="expand" for="c-39965437">[1 more]</label></div><br/><div class="children"><div class="content">Are power and 20x 14-nm chip capacity limiting factors currently?<p>It&#x27;s not inconceivable that&#x27;s a better trade-off than leading-node and HBM requirements.</div><br/></div></div><div id="39966773" class="c"><input type="checkbox" id="c-39966773" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#39964943">root</a><span>|</span><a href="#39965063">parent</a><span>|</span><a href="#39965437">prev</a><span>|</span><a href="#39965262">next</a><span>|</span><label class="collapse" for="c-39966773">[-]</label><label class="expand" for="c-39966773">[1 more]</label></div><br/><div class="children"><div class="content">Edit: 200x more chips, not 20x.</div><br/></div></div><div id="39965262" class="c"><input type="checkbox" id="c-39965262" checked=""/><div class="controls bullet"><span class="by">rattray</span><span>|</span><a href="#39964943">root</a><span>|</span><a href="#39965063">parent</a><span>|</span><a href="#39966773">prev</a><span>|</span><a href="#39964961">next</a><span>|</span><label class="collapse" for="c-39965262">[-]</label><label class="expand" for="c-39965262">[2 more]</label></div><br/><div class="children"><div class="content">Source? Or how do you know that?</div><br/><div id="39965411" class="c"><input type="checkbox" id="c-39965411" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39964943">root</a><span>|</span><a href="#39965262">parent</a><span>|</span><a href="#39964961">next</a><span>|</span><label class="collapse" for="c-39965411">[-]</label><label class="expand" for="c-39965411">[1 more]</label></div><br/><div class="children"><div class="content">This was discussed extensively in previous threads, e.g. <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39431989">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39431989</a></div><br/></div></div></div></div></div></div><div id="39964961" class="c"><input type="checkbox" id="c-39964961" checked=""/><div class="controls bullet"><span class="by">halflings</span><span>|</span><a href="#39964943">parent</a><span>|</span><a href="#39965063">prev</a><span>|</span><a href="#39966853">next</a><span>|</span><label class="collapse" for="c-39964961">[-]</label><label class="expand" for="c-39964961">[3 more]</label></div><br/><div class="children"><div class="content">No HBM because they use tons of fast SRAM instead. Isn&#x27;t that the main driver for performance here?<p>(the way I understood it =&gt; it&#x27;s still cost effective at scale due to throughput increase this brings)</div><br/><div id="39965273" class="c"><input type="checkbox" id="c-39965273" checked=""/><div class="controls bullet"><span class="by">gandalfgeek</span><span>|</span><a href="#39964943">root</a><span>|</span><a href="#39964961">parent</a><span>|</span><a href="#39965094">next</a><span>|</span><label class="collapse" for="c-39965273">[-]</label><label class="expand" for="c-39965273">[1 more]</label></div><br/><div class="children"><div class="content">&gt; No HBM because they use tons of fast SRAM instead. Isn&#x27;t that the main driver for performance here?<p>No doubt fast SRAM helps, but from a computation pov imho its that they&#x27;ve statically planned computation and eliminated all locks.<p>Short explainer here: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=H77tV1KcWIE" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=H77tV1KcWIE</a> (Based on their paper).</div><br/></div></div><div id="39965094" class="c"><input type="checkbox" id="c-39965094" checked=""/><div class="controls bullet"><span class="by">germanjoey</span><span>|</span><a href="#39964943">root</a><span>|</span><a href="#39964961">parent</a><span>|</span><a href="#39965273">prev</a><span>|</span><a href="#39966853">next</a><span>|</span><label class="collapse" for="c-39965094">[-]</label><label class="expand" for="c-39965094">[1 more]</label></div><br/><div class="children"><div class="content">cost effective in what sense? groq doesn&#x27;t achieve high efficiency, only low latency. but that&#x27;s not done in a cost-effective way. compare sambanova achieving the same performance with 8 chips instead of 568, and with higher precision.</div><br/></div></div></div></div><div id="39966853" class="c"><input type="checkbox" id="c-39966853" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#39964943">parent</a><span>|</span><a href="#39964961">prev</a><span>|</span><a href="#39964979">next</a><span>|</span><label class="collapse" for="c-39966853">[-]</label><label class="expand" for="c-39966853">[2 more]</label></div><br/><div class="children"><div class="content">I wonder if the use of eDRAM (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;EDRAM" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;EDRAM</a>), which is essentially embedding DRAM into a chip made on a logic process would be a good idea here.<p>EDRAM is essentially a tradeoff between SRAM and DRAM, offering much greater density at the cost of somewhat worse throughput and latency.<p>There were a couple of POWER cpus that used EDRAM as L3 cache, but it seems to have fallen out of favor.</div><br/><div id="39967109" class="c"><input type="checkbox" id="c-39967109" checked=""/><div class="controls bullet"><span class="by">grandmczeb</span><span>|</span><a href="#39964943">root</a><span>|</span><a href="#39966853">parent</a><span>|</span><a href="#39964979">next</a><span>|</span><label class="collapse" for="c-39967109">[-]</label><label class="expand" for="c-39967109">[1 more]</label></div><br/><div class="children"><div class="content">It fell out of favor because it lost the density advantage in newer processes.</div><br/></div></div></div></div><div id="39964979" class="c"><input type="checkbox" id="c-39964979" checked=""/><div class="controls bullet"><span class="by">dralley</span><span>|</span><a href="#39964943">parent</a><span>|</span><a href="#39966853">prev</a><span>|</span><a href="#39965148">next</a><span>|</span><label class="collapse" for="c-39964979">[-]</label><label class="expand" for="c-39964979">[3 more]</label></div><br/><div class="children"><div class="content">So Itanium and its &quot;sufficiently smart compiler&quot; but functional?</div><br/><div id="39965502" class="c"><input type="checkbox" id="c-39965502" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#39964943">root</a><span>|</span><a href="#39964979">parent</a><span>|</span><a href="#39965148">next</a><span>|</span><label class="collapse" for="c-39965502">[-]</label><label class="expand" for="c-39965502">[2 more]</label></div><br/><div class="children"><div class="content">From skimming the link above, it seems like they accepted it&#x27;s extremely difficult (maybe impossible) to generate high ILP from a VLIW compiler <i>on complex hardware</i> (what Itanium tried to do).<p>So they attacked the italicized portion and simplified the hardware. Mostly by eliminating memory-layer non-determinism &#x2F; using time-sync&#x27;d global memory instructions as part of the ISA(?).<p>This apparently reduced the difficulty of the compiler problem to something manageable (but no doubt still &quot;fun&quot;)... and voila, performance.</div><br/><div id="39965824" class="c"><input type="checkbox" id="c-39965824" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#39964943">root</a><span>|</span><a href="#39965502">parent</a><span>|</span><a href="#39965148">next</a><span>|</span><label class="collapse" for="c-39965824">[-]</label><label class="expand" for="c-39965824">[1 more]</label></div><br/><div class="children"><div class="content">The problem set groq is restricted to (known size tensor manipulation) lends itself to much easier solutions than full blown ILP. The general problem of compiling arbitrary Turing complete algorithms to the arch is NP hard. Tensor manipulation... That&#x27;s a different story.</div><br/></div></div></div></div></div></div></div></div><div id="39965148" class="c"><input type="checkbox" id="c-39965148" checked=""/><div class="controls bullet"><span class="by">alted</span><span>|</span><a href="#39964943">prev</a><span>|</span><a href="#39965286">next</a><span>|</span><label class="collapse" for="c-39965148">[-]</label><label class="expand" for="c-39965148">[5 more]</label></div><br/><div class="children"><div class="content">Custom state-of-the-art silicon is ridiculously expensive.<p>For a minimum 100 wafers = 10k chips, Groq may have paid $100M = $10k&#x2F;chip purely in amortizing design costs.<p>Chip design (software + engineer time) and fabrication setup (lithography masks) grow exponentially [1][2] with smaller nodes, e.g., maybe $100M for Groq&#x27;s current 14nm chips to ~$500M for their planned 4nm tapeout. Once you reach mass production (&gt;&gt;1000 wafers, which have ~150 large chips each), wafers are $10k each. On top of this, it takes ~1 year to design then have prototypes made. (These same issues still exist on older slower nodes, albeit not as bad.)<p>This could be reduced somewhat if chip design software were cheaper and margins were lower, but maybe 20% of this cost is due to fundamental manufacturing difficulty.<p>(disclosure: I don&#x27;t work with recent tech nodes myself; this is my best guess)<p>[1] <a href="https:&#x2F;&#x2F;www.semianalysis.com&#x2F;p&#x2F;the-dark-side-of-the-semiconductor" rel="nofollow">https:&#x2F;&#x2F;www.semianalysis.com&#x2F;p&#x2F;the-dark-side-of-the-semicond...</a>
[2] <a href="https:&#x2F;&#x2F;www.extremetech.com&#x2F;computing&#x2F;272096-3nm-process-node" rel="nofollow">https:&#x2F;&#x2F;www.extremetech.com&#x2F;computing&#x2F;272096-3nm-process-nod...</a></div><br/><div id="39966550" class="c"><input type="checkbox" id="c-39966550" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965148">parent</a><span>|</span><a href="#39965404">next</a><span>|</span><label class="collapse" for="c-39966550">[-]</label><label class="expand" for="c-39966550">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Custom state-of-the-art silicon is ridiculously expensive.</i><p>Think about the amount of money being dumped into &quot;AI&quot; at this point. If you&#x27;ve got the technology and people to make stuff faster&#x2F;better&#x2F;cheaper, finding investors to dump money into your chip making business is probably not as hard as it was 2 years ago.<p>Groq is making this change for other reasons than the expense of tapping out chips.</div><br/></div></div><div id="39965404" class="c"><input type="checkbox" id="c-39965404" checked=""/><div class="controls bullet"><span class="by">shrubble</span><span>|</span><a href="#39965148">parent</a><span>|</span><a href="#39966550">prev</a><span>|</span><a href="#39965286">next</a><span>|</span><label class="collapse" for="c-39965404">[-]</label><label class="expand" for="c-39965404">[3 more]</label></div><br/><div class="children"><div class="content">The report I read said that latest TSMC is 17K per wafer.  How much less it is for 14nm I don&#x27;t know.</div><br/><div id="39965494" class="c"><input type="checkbox" id="c-39965494" checked=""/><div class="controls bullet"><span class="by">karma_pharmer</span><span>|</span><a href="#39965148">root</a><span>|</span><a href="#39965404">parent</a><span>|</span><a href="#39965286">next</a><span>|</span><label class="collapse" for="c-39965494">[-]</label><label class="expand" for="c-39965494">[2 more]</label></div><br/><div class="children"><div class="content">The masks are the expensive part, not the wafers.</div><br/><div id="39966187" class="c"><input type="checkbox" id="c-39966187" checked=""/><div class="controls bullet"><span class="by">thrtythreeforty</span><span>|</span><a href="#39965148">root</a><span>|</span><a href="#39965494">parent</a><span>|</span><a href="#39965286">next</a><span>|</span><label class="collapse" for="c-39966187">[-]</label><label class="expand" for="c-39966187">[1 more]</label></div><br/><div class="children"><div class="content">They are both fabulously expensive.</div><br/></div></div></div></div></div></div></div></div><div id="39965286" class="c"><input type="checkbox" id="c-39965286" checked=""/><div class="controls bullet"><span class="by">mlazos</span><span>|</span><a href="#39965148">prev</a><span>|</span><a href="#39965060">next</a><span>|</span><label class="collapse" for="c-39965286">[-]</label><label class="expand" for="c-39965286">[7 more]</label></div><br/><div class="children"><div class="content">The smoke and mirrors around groq are finally clearing. Truth is that their system is insanely expensive to maintain.  hundreds (&gt; 500 iirc) of chips to get wild tokens&#x2F;s but the power and maintenance expense is crazy high for that number of chips. TCO just isn’t worth it</div><br/><div id="39966276" class="c"><input type="checkbox" id="c-39966276" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39965286">parent</a><span>|</span><a href="#39966892">next</a><span>|</span><label class="collapse" for="c-39966276">[-]</label><label class="expand" for="c-39966276">[3 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t know that. For one thing, their silicon costs are going to be relatively cheap. It&#x27;s an old reliable, 14nm process, and compared to even Google&#x27;s TPU this is a relatively simple chip. For another they _could_ be putting all that silicon to a good use, and by all indications they are. Because there&#x27;s far less local memory movement, and weights are distributed throughout the system, even this 14nm system could be energy efficient. 9&#x2F;10ths of all power in a conventional system does not go towards compute - it&#x27;s wasted in moving data back and forth. This is especially bad in transformers, which, because of their size, largely defeat the memory hierarchies the architects worked so hard to perfect. IOW, all your caches are useless and you&#x27;re unnecessarily wasting 90% of your energy while also getting worse latency and worse throughput (due to memory bus bandwidth constraints). Oops. These folks seem to be offering something that nobody else does - a feasible, proven way to get out of jail free. I wish them all the success they can get, because all the other currently available architectures are largely unsuitable for high throughput transformer inference, and they work in spite, instead of because, of their design.</div><br/><div id="39966751" class="c"><input type="checkbox" id="c-39966751" checked=""/><div class="controls bullet"><span class="by">mlazos</span><span>|</span><a href="#39965286">root</a><span>|</span><a href="#39966276">parent</a><span>|</span><a href="#39966892">next</a><span>|</span><label class="collapse" for="c-39966751">[-]</label><label class="expand" for="c-39966751">[2 more]</label></div><br/><div class="children"><div class="content">Peak H100 power consumption is 700W. Average power consumption of the groq card (from their own website) is 240W. With 576 chips it just doesn’t look good. How much is that millisecond perf gain worth it to end users?<p>That said I think their arch is super interesting. I just think that demo was way too hype when the actual system is pretty impractical.</div><br/><div id="39967213" class="c"><input type="checkbox" id="c-39967213" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39965286">root</a><span>|</span><a href="#39966751">parent</a><span>|</span><a href="#39966892">next</a><span>|</span><label class="collapse" for="c-39967213">[-]</label><label class="expand" for="c-39967213">[1 more]</label></div><br/><div class="children"><div class="content">So? They aren&#x27;t performing the same computation. You can&#x27;t compare the two. What you can compare is power draw at an equivalent tokens&#x2F;sec on the same model. But you don&#x27;t have that number.</div><br/></div></div></div></div></div></div><div id="39966892" class="c"><input type="checkbox" id="c-39966892" checked=""/><div class="controls bullet"><span class="by">Oribi</span><span>|</span><a href="#39965286">parent</a><span>|</span><a href="#39966276">prev</a><span>|</span><a href="#39965060">next</a><span>|</span><label class="collapse" for="c-39966892">[-]</label><label class="expand" for="c-39966892">[3 more]</label></div><br/><div class="children"><div class="content">Why would they want to run it themselves if the TCO didn’t work out</div><br/><div id="39967244" class="c"><input type="checkbox" id="c-39967244" checked=""/><div class="controls bullet"><span class="by">b-side</span><span>|</span><a href="#39965286">root</a><span>|</span><a href="#39966892">parent</a><span>|</span><a href="#39966997">next</a><span>|</span><label class="collapse" for="c-39967244">[-]</label><label class="expand" for="c-39967244">[1 more]</label></div><br/><div class="children"><div class="content">Because they rather operate at a loss with high revenue rather than have 0 revenue and loss?</div><br/></div></div><div id="39966997" class="c"><input type="checkbox" id="c-39966997" checked=""/><div class="controls bullet"><span class="by">mrkeen</span><span>|</span><a href="#39965286">root</a><span>|</span><a href="#39966892">parent</a><span>|</span><a href="#39967244">prev</a><span>|</span><a href="#39965060">next</a><span>|</span><label class="collapse" for="c-39966997">[-]</label><label class="expand" for="c-39966997">[1 more]</label></div><br/><div class="children"><div class="content">I thought that was par for the course these days.<p>Operate at a loss.  Get a big valuation.  Cash out.</div><br/></div></div></div></div></div></div><div id="39965060" class="c"><input type="checkbox" id="c-39965060" checked=""/><div class="controls bullet"><span class="by">rnts08</span><span>|</span><a href="#39965286">prev</a><span>|</span><a href="#39965911">next</a><span>|</span><label class="collapse" for="c-39965060">[-]</label><label class="expand" for="c-39965060">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like they&#x27;re looking to get bought up to me. I&#x27;m sure they could monetize their current hardware, and build to sell just like other niche hardware vendors. Anyone remember the hype around big &quot;cloud&quot; storage boxes 10 years back?</div><br/></div></div><div id="39965911" class="c"><input type="checkbox" id="c-39965911" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#39965060">prev</a><span>|</span><a href="#39965064">next</a><span>|</span><label class="collapse" for="c-39965911">[-]</label><label class="expand" for="c-39965911">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not able to get consistent replies from the API. It&#x27;s lightening fast for like ten minutes and then starts freezing up for several seconds.<p>I want to use it, but it&#x27;s been very unreliable. I have been using Claude 3 and thinking about together.ai with Mixtral.</div><br/><div id="39966171" class="c"><input type="checkbox" id="c-39966171" checked=""/><div class="controls bullet"><span class="by">QuadrupleA</span><span>|</span><a href="#39965911">parent</a><span>|</span><a href="#39965064">next</a><span>|</span><label class="collapse" for="c-39966171">[-]</label><label class="expand" for="c-39966171">[1 more]</label></div><br/><div class="children"><div class="content">Same, it&#x27;s great when it&#x27;s quick &#x2F; available, but they seem underprovisioned for busy times and I often get long 10-30 second stalls.</div><br/></div></div></div></div><div id="39965064" class="c"><input type="checkbox" id="c-39965064" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#39965911">prev</a><span>|</span><a href="#39964964">next</a><span>|</span><label class="collapse" for="c-39965064">[-]</label><label class="expand" for="c-39965064">[4 more]</label></div><br/><div class="children"><div class="content">Given that their hardware is different I can kinda see how they don’t want to deal with supporting customers.<p>&gt; what do you mean I can’t just drop a CUDA docker image in?</div><br/><div id="39965092" class="c"><input type="checkbox" id="c-39965092" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#39965064">parent</a><span>|</span><a href="#39964964">next</a><span>|</span><label class="collapse" for="c-39965092">[-]</label><label class="expand" for="c-39965092">[3 more]</label></div><br/><div class="children"><div class="content">if you&#x27;re a hardware startup that doesn&#x27;t sell hardware, what are you?</div><br/><div id="39965106" class="c"><input type="checkbox" id="c-39965106" checked=""/><div class="controls bullet"><span class="by">aleph_minus_one</span><span>|</span><a href="#39965064">root</a><span>|</span><a href="#39965092">parent</a><span>|</span><a href="#39967217">next</a><span>|</span><label class="collapse" for="c-39965106">[-]</label><label class="expand" for="c-39965106">[1 more]</label></div><br/><div class="children"><div class="content">&gt; if you&#x27;re a hardware startup that doesn&#x27;t sell hardware, what are you?<p>A hardware startup that sells cloud access to its hardware. :-)</div><br/></div></div><div id="39967217" class="c"><input type="checkbox" id="c-39967217" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#39965064">root</a><span>|</span><a href="#39965092">parent</a><span>|</span><a href="#39965106">prev</a><span>|</span><a href="#39964964">next</a><span>|</span><label class="collapse" for="c-39967217">[-]</label><label class="expand" for="c-39967217">[1 more]</label></div><br/><div class="children"><div class="content">Hardware setup that produces superior hardware and extracts the benefit in house ?</div><br/></div></div></div></div></div></div><div id="39964964" class="c"><input type="checkbox" id="c-39964964" checked=""/><div class="controls bullet"><span class="by">dsrtslnd23</span><span>|</span><a href="#39965064">prev</a><span>|</span><a href="#39964884">next</a><span>|</span><label class="collapse" for="c-39964964">[-]</label><label class="expand" for="c-39964964">[5 more]</label></div><br/><div class="children"><div class="content">So unless there are new Croq datacenters coming, this is only interesting for North American users. Otherwise H100 based latency optimized solutions would be faster - in particular for time-to-first-token sensitive applications.</div><br/><div id="39965101" class="c"><input type="checkbox" id="c-39965101" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#39964964">parent</a><span>|</span><a href="#39964884">next</a><span>|</span><label class="collapse" for="c-39965101">[-]</label><label class="expand" for="c-39965101">[4 more]</label></div><br/><div class="children"><div class="content">&gt; latency optimized solutions would be faster - in particular for time-to-first-token sensitive applications<p>Do you have any idea how fast Groq is? Go try it. Consistently over 400 t&#x2F;s for most of the models that they support, and extremely low latency.</div><br/><div id="39965458" class="c"><input type="checkbox" id="c-39965458" checked=""/><div class="controls bullet"><span class="by">huac</span><span>|</span><a href="#39964964">root</a><span>|</span><a href="#39965101">parent</a><span>|</span><a href="#39964884">next</a><span>|</span><label class="collapse" for="c-39965458">[-]</label><label class="expand" for="c-39965458">[3 more]</label></div><br/><div class="children"><div class="content">time to first token != tokens per second<p>remember that EU -&gt; US is ~150ms unavoidable latency, for example. then your comparison is local H100 vs Grok + 150ms latency to first token.</div><br/><div id="39966464" class="c"><input type="checkbox" id="c-39966464" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#39964964">root</a><span>|</span><a href="#39965458">parent</a><span>|</span><a href="#39965754">next</a><span>|</span><label class="collapse" for="c-39966464">[-]</label><label class="expand" for="c-39966464">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m in Australia. I have 249ms of unavoidable latency and I&#x27;d still use the groq API if I could. It&#x27;s <i>that</i> much faster than other inference solutions.</div><br/></div></div><div id="39965754" class="c"><input type="checkbox" id="c-39965754" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#39964964">root</a><span>|</span><a href="#39965458">parent</a><span>|</span><a href="#39966464">prev</a><span>|</span><a href="#39964884">next</a><span>|</span><label class="collapse" for="c-39965754">[-]</label><label class="expand" for="c-39965754">[1 more]</label></div><br/><div class="children"><div class="content">&gt; time to first token != tokens per second<p>I said &quot;and extremely low latency&quot; because I know they are different. Groq&#x27;s TTFT is still consistently competitive with any other provider, and lower than most of them. Here&#x27;s some benchmarks: <a href="https:&#x2F;&#x2F;github.com&#x2F;ray-project&#x2F;llmperf-leaderboard#70b-models-1">https:&#x2F;&#x2F;github.com&#x2F;ray-project&#x2F;llmperf-leaderboard#70b-model...</a></div><br/></div></div></div></div></div></div></div></div><div id="39964884" class="c"><input type="checkbox" id="c-39964884" checked=""/><div class="controls bullet"><span class="by">scosman</span><span>|</span><a href="#39964964">prev</a><span>|</span><a href="#39965025">next</a><span>|</span><label class="collapse" for="c-39964884">[-]</label><label class="expand" for="c-39964884">[2 more]</label></div><br/><div class="children"><div class="content">Wildly fast inference. And current chips are 14nm so headroom to get a lot better.</div><br/><div id="39964989" class="c"><input type="checkbox" id="c-39964989" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#39964884">parent</a><span>|</span><a href="#39965025">next</a><span>|</span><label class="collapse" for="c-39964989">[-]</label><label class="expand" for="c-39964989">[1 more]</label></div><br/><div class="children"><div class="content">Note that SRAM density doesn&#x27;t scale at the same rate as logic density, and Groqs &quot;secret sauce&quot; is putting a ton of SRAM on their chips. Their stuff won&#x27;t necessarily see the full benefits of switching to denser nodes if the bottleneck is how much SRAM they can pack onto each chip.<p><a href="https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;no-sram-scaling-implies-on-more-expensive-cpus-and-gpus" rel="nofollow">https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;no-sram-scaling-implies-on...</a><p>IIRC the last big jump for SRAM density was at 7nm, so they do still have that card to play, but progress has slowed to a crawl beyond that. TSMC 3nm SRAM is barely denser than TSMC 7nm SRAM.</div><br/></div></div></div></div><div id="39965025" class="c"><input type="checkbox" id="c-39965025" checked=""/><div class="controls bullet"><span class="by">zetazzed</span><span>|</span><a href="#39964884">prev</a><span>|</span><a href="#39965093">next</a><span>|</span><label class="collapse" for="c-39965025">[-]</label><label class="expand" for="c-39965025">[2 more]</label></div><br/><div class="children"><div class="content">Man, I want to appreciate a nice new hardware approach, but they say such BS that it is hard to read about them:<p>&gt; “There might need to be a new term, because by the end of next year we’re going to deploy enough LPUs that compute-wise, it’s going to be the equivalent of all the hyperscalers combined,” he said. “We already have a non-trivial portion of that.”<p>Really? Does anyone seriously believe they are going to be the equivalent of all hyperscalers in compute next year? (Where Meta alone is at 1 million H100 equivalents.) In the same article where they say it&#x27;s too hard for them to sell chips? And when they literally don&#x27;t have a setup to even accept a credit card today?</div><br/><div id="39965386" class="c"><input type="checkbox" id="c-39965386" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39965025">parent</a><span>|</span><a href="#39965093">next</a><span>|</span><label class="collapse" for="c-39965386">[-]</label><label class="expand" for="c-39965386">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t put a million-dollar rack on a credit card. I&#x27;m not sure they want retail customers for their API either.</div><br/></div></div></div></div><div id="39965093" class="c"><input type="checkbox" id="c-39965093" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#39965025">prev</a><span>|</span><a href="#39965050">next</a><span>|</span><label class="collapse" for="c-39965093">[-]</label><label class="expand" for="c-39965093">[8 more]</label></div><br/><div class="children"><div class="content">That sucks. I wanted to save up for a couple years and get some hardware for home, but I guess the &quot;AI&quot; space moves so fast you barely get a couple months</div><br/><div id="39965387" class="c"><input type="checkbox" id="c-39965387" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39965093">parent</a><span>|</span><a href="#39965050">next</a><span>|</span><label class="collapse" for="c-39965387">[-]</label><label class="expand" for="c-39965387">[7 more]</label></div><br/><div class="children"><div class="content">Save up for Tenstorrent instead.</div><br/><div id="39965910" class="c"><input type="checkbox" id="c-39965910" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#39965093">root</a><span>|</span><a href="#39965387">parent</a><span>|</span><a href="#39965050">next</a><span>|</span><label class="collapse" for="c-39965910">[-]</label><label class="expand" for="c-39965910">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll look into it, though seeing &quot;contact us&quot; always makes me think they&#x27;re not going to sell a single unit to a home user. (With that said, Groq probably wouldn&#x27;t either. You can technically buy LPUs for 20k each, without an expectation of support, but it takes tens of them to run Mixtral.)<p>Tenstorrent also looks incredibly Python-specific (as in, everything including their SMI seems mostly Python-based) which doesn&#x27;t seem promising?</div><br/><div id="39966456" class="c"><input type="checkbox" id="c-39966456" checked=""/><div class="controls bullet"><span class="by">ojn</span><span>|</span><a href="#39965093">root</a><span>|</span><a href="#39965910">parent</a><span>|</span><a href="#39966101">next</a><span>|</span><label class="collapse" for="c-39966456">[-]</label><label class="expand" for="c-39966456">[3 more]</label></div><br/><div class="children"><div class="content">Most of the low-level pieces are in Rust, the TUI is written in Python and most of the remaining pieces are getting lowered down to the Rust libraries over time.<p>(It was all Python up until ~6 months ago)<p>EDIT: Oh, and you can buy the Grayskull cards online now, without contacting anyone.</div><br/><div id="39966731" class="c"><input type="checkbox" id="c-39966731" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965093">root</a><span>|</span><a href="#39966456">parent</a><span>|</span><a href="#39966832">next</a><span>|</span><label class="collapse" for="c-39966731">[-]</label><label class="expand" for="c-39966731">[1 more]</label></div><br/><div class="children"><div class="content">Even talking to people there, my experience is that they are super nice!</div><br/></div></div><div id="39966832" class="c"><input type="checkbox" id="c-39966832" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#39965093">root</a><span>|</span><a href="#39966456">parent</a><span>|</span><a href="#39966731">prev</a><span>|</span><a href="#39966101">next</a><span>|</span><label class="collapse" for="c-39966832">[-]</label><label class="expand" for="c-39966832">[1 more]</label></div><br/><div class="children"><div class="content">&gt; EDIT: Oh, and you can buy the Grayskull cards online now, without contacting anyone.<p>I actually don&#x27;t mind having to contact, I only mind if they won&#x27;t want to sell to me due to being a non-bulk order.<p>&gt; Most of the low-level pieces are in Rust<p>That&#x27;s awesome!</div><br/></div></div></div></div><div id="39966101" class="c"><input type="checkbox" id="c-39966101" checked=""/><div class="controls bullet"><span class="by">sipjca</span><span>|</span><a href="#39965093">root</a><span>|</span><a href="#39965910">parent</a><span>|</span><a href="#39966456">prev</a><span>|</span><a href="#39965050">next</a><span>|</span><label class="collapse" for="c-39966101">[-]</label><label class="expand" for="c-39966101">[2 more]</label></div><br/><div class="children"><div class="content">fwiw as a consumer I have a tenstorrent card in my machine</div><br/><div id="39966561" class="c"><input type="checkbox" id="c-39966561" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#39965093">root</a><span>|</span><a href="#39966101">parent</a><span>|</span><a href="#39965050">next</a><span>|</span><label class="collapse" for="c-39966561">[-]</label><label class="expand" for="c-39966561">[1 more]</label></div><br/><div class="children"><div class="content">that is great. please hit me up privately, i&#x27;d love to chat with you about it.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39965050" class="c"><input type="checkbox" id="c-39965050" checked=""/><div class="controls bullet"><span class="by">creato</span><span>|</span><a href="#39965093">prev</a><span>|</span><a href="#39964877">next</a><span>|</span><label class="collapse" for="c-39965050">[-]</label><label class="expand" for="c-39965050">[9 more]</label></div><br/><div class="children"><div class="content">&gt; If customers come with requests for high volumes of chips for very large installations, Groq will instead propose partnering on data center deployment. Ross said that Groq has “signed a deal” with Saudi state-owned oil company Aramco, though he declined to give further details, saying only that the deal involved “a very large deployment of [Groq] LPUs.”<p>What? How does this make sense?</div><br/><div id="39965418" class="c"><input type="checkbox" id="c-39965418" checked=""/><div class="controls bullet"><span class="by">shrubble</span><span>|</span><a href="#39965050">parent</a><span>|</span><a href="#39965058">next</a><span>|</span><label class="collapse" for="c-39965418">[-]</label><label class="expand" for="c-39965418">[1 more]</label></div><br/><div class="children"><div class="content">Oil &amp; gas has large data needs, they had petabyte-scale data 2 decades ago.</div><br/></div></div><div id="39965058" class="c"><input type="checkbox" id="c-39965058" checked=""/><div class="controls bullet"><span class="by">theturtletalks</span><span>|</span><a href="#39965050">parent</a><span>|</span><a href="#39965418">prev</a><span>|</span><a href="#39964877">next</a><span>|</span><label class="collapse" for="c-39965058">[-]</label><label class="expand" for="c-39965058">[7 more]</label></div><br/><div class="children"><div class="content">If you read on, Groq said they would only sell hardware to US companies and outside companies would get cloud services, not the LPUs. I think the US government told them to keep the LPUs in-house since they could be the secret sauce for scale.</div><br/><div id="39965079" class="c"><input type="checkbox" id="c-39965079" checked=""/><div class="controls bullet"><span class="by">creato</span><span>|</span><a href="#39965050">root</a><span>|</span><a href="#39965058">parent</a><span>|</span><a href="#39964877">next</a><span>|</span><label class="collapse" for="c-39965079">[-]</label><label class="expand" for="c-39965079">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not questioning the deployment strategy, I&#x27;m wondering why Saudi Aramco wants to access so much compute power that is highly specialized(?) for generative AI workloads. Or is it more general than that?</div><br/><div id="39965316" class="c"><input type="checkbox" id="c-39965316" checked=""/><div class="controls bullet"><span class="by">thawab</span><span>|</span><a href="#39965050">root</a><span>|</span><a href="#39965079">parent</a><span>|</span><a href="#39965115">next</a><span>|</span><label class="collapse" for="c-39965316">[-]</label><label class="expand" for="c-39965316">[1 more]</label></div><br/><div class="children"><div class="content">It’s about material discovery[0], Aramco built a 250b model to help them improve efficiency [1].<p>[0] <a href="https:&#x2F;&#x2F;deepmind.google&#x2F;discover&#x2F;blog&#x2F;millions-of-new-materials-discovered-with-deep-learning&#x2F;" rel="nofollow">https:&#x2F;&#x2F;deepmind.google&#x2F;discover&#x2F;blog&#x2F;millions-of-new-materi...</a><p>[1] <a href="https:&#x2F;&#x2F;www.aramco.com&#x2F;en&#x2F;news-media&#x2F;speeches&#x2F;2024&#x2F;leap24--remarks-by-amin-h-nasser-saudi-aramco-president-and-ceo" rel="nofollow">https:&#x2F;&#x2F;www.aramco.com&#x2F;en&#x2F;news-media&#x2F;speeches&#x2F;2024&#x2F;leap24--r...</a></div><br/></div></div><div id="39965115" class="c"><input type="checkbox" id="c-39965115" checked=""/><div class="controls bullet"><span class="by">aleph_minus_one</span><span>|</span><a href="#39965050">root</a><span>|</span><a href="#39965079">parent</a><span>|</span><a href="#39965316">prev</a><span>|</span><a href="#39965117">next</a><span>|</span><label class="collapse" for="c-39965115">[-]</label><label class="expand" for="c-39965115">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m wondering why Saudi Aramco wants to access so much compute power that is highly specialized(?) for generative AI workloads. Or is it more general than that?<p>For vanity reasons and because AI is the future (not every company acts that rationally for huge buying decisions).</div><br/><div id="39965625" class="c"><input type="checkbox" id="c-39965625" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#39965050">root</a><span>|</span><a href="#39965115">parent</a><span>|</span><a href="#39965117">next</a><span>|</span><label class="collapse" for="c-39965625">[-]</label><label class="expand" for="c-39965625">[1 more]</label></div><br/><div class="children"><div class="content">Also diversification. They’re smart people; oil isn’t the future. Comparatively small investments to hedge make perfect sense.</div><br/></div></div></div></div><div id="39965117" class="c"><input type="checkbox" id="c-39965117" checked=""/><div class="controls bullet"><span class="by">theturtletalks</span><span>|</span><a href="#39965050">root</a><span>|</span><a href="#39965079">parent</a><span>|</span><a href="#39965115">prev</a><span>|</span><a href="#39966254">next</a><span>|</span><label class="collapse" for="c-39965117">[-]</label><label class="expand" for="c-39965117">[1 more]</label></div><br/><div class="children"><div class="content">Maybe Saudi doesn’t want to rely on OpenAI and other APIs and wants to run a fine-tuned Mixtral model on the cloud or their hardware. International companies will probably opt for an open source model since the data is sensitive and OpenAI could pass that to intelligence.</div><br/></div></div><div id="39966254" class="c"><input type="checkbox" id="c-39966254" checked=""/><div class="controls bullet"><span class="by">recursivecaveat</span><span>|</span><a href="#39965050">root</a><span>|</span><a href="#39965079">parent</a><span>|</span><a href="#39965117">prev</a><span>|</span><a href="#39964877">next</a><span>|</span><label class="collapse" for="c-39966254">[-]</label><label class="expand" for="c-39966254">[1 more]</label></div><br/><div class="children"><div class="content">Partly supply-chain security I imagine. If generative AI does indeed become the <i>next big thing</i> much nicer to have a giant pile of hardware physically in your country than buying a drip feed from a foreign company.</div><br/></div></div></div></div></div></div></div></div><div id="39964877" class="c"><input type="checkbox" id="c-39964877" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#39965050">prev</a><span>|</span><a href="#39965504">next</a><span>|</span><label class="collapse" for="c-39964877">[-]</label><label class="expand" for="c-39964877">[5 more]</label></div><br/><div class="children"><div class="content">Read: We&#x27;re forcing someone&#x27;s hand in acquiring us.<p>Groq is still under a 30 request per minute rate-limit, which drops to <i>10 requests per minute</i> if you have all day usage.<p>Billing has been &quot;coming soon&quot; this whole time, and while they&#x27;ve built out hype enabling features like function calling, somehow they can&#x27;t setup a Stripe webhook to collect money for realistic rate limits.<p>They couldn&#x27;t scream &quot;we can&#x27;t service the tiniest bit of our demand&quot; any louder at this point.<p>_<p>Edit: For anyone looking for fast inference without the smoke and mirrors, I&#x27;ve been using Fireworks.ai in production and it&#x27;s great. 200 tk&#x2F;s - 300 tk&#x2F;s is closer to Groq than it is to OpenAI and co.<p>And as a bonus they support PEFT with serverless pricing.</div><br/><div id="39964936" class="c"><input type="checkbox" id="c-39964936" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#39964877">parent</a><span>|</span><a href="#39964944">next</a><span>|</span><label class="collapse" for="c-39964936">[-]</label><label class="expand" for="c-39964936">[1 more]</label></div><br/><div class="children"><div class="content">they don&#x27;t even let us pay them, it&#x27;s insane<p>I just have free API access with no ability to add a credit card.</div><br/></div></div><div id="39964944" class="c"><input type="checkbox" id="c-39964944" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#39964877">parent</a><span>|</span><a href="#39964936">prev</a><span>|</span><a href="#39964942">next</a><span>|</span><label class="collapse" for="c-39964944">[-]</label><label class="expand" for="c-39964944">[2 more]</label></div><br/><div class="children"><div class="content">What are you using all this for? Whats the product?</div><br/><div id="39964986" class="c"><input type="checkbox" id="c-39964986" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#39964877">root</a><span>|</span><a href="#39964944">parent</a><span>|</span><a href="#39964942">next</a><span>|</span><label class="collapse" for="c-39964986">[-]</label><label class="expand" for="c-39964986">[1 more]</label></div><br/><div class="children"><div class="content">I run an AI story telling site and an AI ideation platform.<p>The story telling site alone averaged 27k requests a day this week, so about double what their current request limit is, and honestly not even that popular of a site.<p>You can&#x27;t run much more than a toy project on their current rate limits.</div><br/></div></div></div></div></div></div><div id="39965504" class="c"><input type="checkbox" id="c-39965504" checked=""/><div class="controls bullet"><span class="by">karma_pharmer</span><span>|</span><a href="#39964877">prev</a><span>|</span><label class="collapse" for="c-39965504">[-]</label><label class="expand" for="c-39965504">[1 more]</label></div><br/><div class="children"><div class="content">Another casualty of AI KYC.</div><br/></div></div></div></div></div></div></div></body></html>